{
  "DisCo: Towards Harmonious Disentanglement and Collaboration between Tabular and Semantic Space for Recommendation": "Kounianhua Du Shanghai Jiao Tong University Shanghai, China kounianhuadu@sjtu.edu.cn Jizheng Chen Shanghai Jiao Tong University Shanghai, China humihuadechengzhi@sjtu.edu.cn Jianghao Lin Shanghai Jiao Tong University Shanghai, China chiangel@sjtu.edu.cn Yunjia Xi Shanghai Jiao Tong University Shanghai, China xiyunjia@sjtu.edu.cn Bo Chen Huawei Noah's Ark Lab Shanghai, China chenbo116@huawei.com",
  "Hangyu Wang": "Shanghai Jiao Tong University Shanghai, China hangyuwang@sjtu.edu.cn Xinyi Dai Huawei Noah's Ark Lab Shanghai, China daixinyi5@huawei.com Ruiming Tang Huawei Noah's Ark Lab Shenzhen, China tangruiming@huawei.com",
  "ABSTRACT": "Recommender systems play important roles in various applications such as e-commerce, social media, etc. Conventional recommendation methods usually model the collaborative signals within the tabular representation space. Despite the personalization modeling and the efficiency, the latent semantic dependencies are omitted. Methods that introduce semantics into recommendation then emerge, injecting knowledge from the semantic representation space where the general language understanding are compressed. However, existing semantic-enhanced recommendation methods focus on aligning the two spaces, during which the representations of the two spaces tend to get close while the unique patterns are discarded and not well explored. In this paper, we propose DisCo to Dis entangle the unique patterns from the two representation spaces and Co llaborate the two spaces for recommendation enhancement, where both the specificity and the consistency of the two spaces are captured. Concretely, we propose 1) a dual-side attentive network to capture the intra-domain patterns and the inter-domain patterns, 2) a sufficiency constraint to preserve the task-relevant information of each representation space and filter out the noise, and 3) a disentanglement constraint to avoid the model from discarding the unique information. These modules strike a balance between disentanglement and collaboration of the two representation spaces to produce informative pattern vectors, which could serve as extra features and be appended to arbitrary recommendation backbones for enhancement. Experiment results validate the superiority of our method against different models and the compatibility of DisCo Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. KDD '24, Aug 25-29, 2024, Barcelona Â© 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-XXXX-X/18/06...$15.00 https://doi.org/XXXXXXX.XXXXXXX Weinan Zhang Shanghai Jiao Tong University Shanghai, China wnzhang@sjtu.edu.cn Figure 1: Illustration of the motivation. Collaborative-based in-domain pattern learning Tabular Data Space Semantic Data Space Semantic dependencies with open world knowledge Near Beer Diaper User 1: Beer -> Diaper -> ... User 2: Diaper -> Beer -> ... Near Ernest Hemingway Raymond Carver 1. Representative minimalist writers. 2. Carver once said: \" Hemingway taught me how to write.\" Noise -> To be eliminated. Aligning Part -> To be ensured. Disenrangling Part -> To be ensured. Union of all potential task-Related information. R over different backbones. Various ablation studies and efficiency analysis are also conducted to justify each model component.",
  "CCS CONCEPTS": "Â· Information systems â†’ Recommender systems .",
  "KEYWORDS": "Recommender Systems, User Modeling, Large Language Model",
  "ACMReference Format:": "Kounianhua Du, Jizheng Chen, Jianghao Lin, Yunjia Xi, Hangyu Wang, Xinyi Dai, Bo Chen, Ruiming Tang, and Weinan Zhang. 2018. DisCo: Towards Harmonious Disentanglement and Collaboration between Tabular and Semantic Space for Recommendation. In Proceedings of KDD (KDD '24). ACM, New York, NY, USA, 11 pages. https://doi.org/XXXXXXX.XXXXXXX",
  "1 INTRODUCTION": "Recommender systems have become an integral part of today's digital ecosystem, enhancing user experiences, boosting engagement, facilitating decision-making, and fostering connections between KDD '24, Aug 25-29, 2024, Barcelona Kounianhua Du et al. users and relevant content or products. They are widely used in various industries, including e-commerce [36], entertainment [5], social media [41], and online streaming platforms [18, 44, 45]. Conventional recommender systems usually only model the collaborative signals within the tabular representation space, where samples consist of multi-field categorical features. These methods focus on mining beneficial interactions among features [19, 27, 37, 43] and modeling user interests using historical user behaviors [9, 30, 32, 44, 45] for accurate and personalized recommendation. While being good at modeling feature interactions and personalized user preferences, these methods fail to learn the latent semantic dependencies of features. For example, as shown in Figure 1, a book written by Ernest Hemingway and a book written by Raymond Carver can be close in semantic space because the two authors are known to be minimalist writing style and Carver said that he borrowed many elements from Hemingway's style. This latent semantic dependency cannot be inferred from the tabular representation space where features are firstly encoded in a one-hot manner. Attempts to incorporate external knowledge within semantic representation space into recommendation then emerge [14, 15, 22], where textual descriptions and their encodings are used to hold the external semantic knowledge. Despite the general language understanding within semantic representation space, the encoding of the large language model has ambiguity and fails to imply the correlations of some features well. For instance, beer and diaper are distant in semantic space but near in tabular space for recommendation where the correlation analysis is done, since users tend to buy beer and diaper together. As discussed above, the relationship among the same set of user behaviors can be different in the two representation spaces. The unique and disentangled patterns from the two representation spaces form a complementary relationship with each other, contributing different information. Therefore, it is vital to effectively and efficiently collaborate the tabular and semantic representation spaces. The existing works either: 1) take only one space into consideration [10] or 2) only focus on aligning the two spaces [22], during which the representations tend to get close (green part in Figure 1), with the disentangled part (yellow and blue parts in Figure 1) being discarded. In this paper, we propose DisCo to Dis entangle and Co llaborate the tabular and semantic representation spaces for user behavior patterns modeling. Therefore, we design 1) a dual-side attentive network (DS-Attn) to capture the intra-domain and inter-domain patterns, 2) a sufficiency constraint to preserve the useful information (yellow, green, and blue parts in Figure 1) and eliminate the noisy information (grey part in Figure 1) from the two representation spaces, and 3) a disentanglement constraint to preserve the disentangling parts from the two representation spaces (yellow and blue part in Figure 1). Together, these modules strike a balance of the collaboration and disentanglement between the tabular and semantic representation space. Concretely, DS-Attn infers the inner patterns within the tabular space and the semantic space for collaborative-based correlations and semantic-based dependencies with the intra-domain attention, and captures the inter patterns between the two spaces for aligned knowledge with the inter-domain attention where a query-key exchange is adopted to make the two spaces attend to each other. The parameters of DS-Attn and embedding networks are regularized by the sufficiency and disentanglement constraints. The sufficiency constraint maximizes the mutual information between the representations of each space with the labels, which preserves the task-relevant information offered by each space. The disentanglement constraint minimizes the mutual information between the output vectors of DS-Attn from the two spaces, which forces the two spaces to offer different information. The resulting vectors output by DS-Attn and regularized by the two constraints preserve both the consistent and the specific knowledge of the two representation spaces, which can then be fed into arbitrary recommendation backbones for prediction enhancement. The main contributions of the paper are summarized as follows: Â· We design a novel and effective framework, DisCo, that harmoniously captures both the consistent and the specific knowledge from tabular representation space and semantic representation space with the dual-side attentive network under the regularization of the proposed sufficiency and disentanglement constraints. Â· We emphasize the importance of unique and disentangled information in both the tabular space and the semantic space, and propose the first work to disentangle the tabular and semantic representation spaces for unique domain knowledge. Â· DisCo is a model-agnostic framework compatible with different recommendation backbones, offering flexibility and generality. The experiment results over a series of recommendation backbones justify the consistent superiority of the proposed method. Ablation studies are also conducted to validate the effectiveness of different model components.",
  "2 RELATED WORK": "",
  "2.1 Tabular-Only Methods": "Early recommendation models focus on digging into interactions among features. FM [35] captures second-order feature interactions. FFM [20] introduces field-aware interactions. Wide & Deep [2] combines the strengths of linear models and deep neural networks. DeepFM [12] replaces the logistic regression layer in [2] with an FM layer. xDeepFM [23] introduces the cross layer for highorder feature interactions. PNN [33] utilizes the product layer to learn the high-order product interactions. DCN [40] proposes to capture both shallow and deep feature interactions effectively. AutoInt [37] utilizes the multi-head self-attention mechanism to learn high-order feature interactions. By modeling user behavior patterns, recommenders provide more personalized recommendations. DIN [45] incorporates the attention mechanism [39] to capture user interests. DIEN [44] uses GRU module to better model the evolving interests of users. DSIN [9] proposes to capture the dynamic interests of users within a session. MIMN [29] leverages a memory network architecture to capture different aspects of user interests. SIM [30] models long-term user behaviors.",
  "2.2 Semantic-Enhanced Methods": "Recently, large language models have shown great impact and shed light on various domains of recommendation systems. There are several attempts to incorporate large language models into recommender systems [24-26, 42]. PTab [28] adopts a classic BERT [6] DisCo: Towards Harmonious Disentanglement and Collaboration between Tabular and Semantic Space for Recommendation KDD '24, Aug 25-29, 2024, Barcelona framework with Modality Transformation(MT), Masked-Language Finetuning(MF), and Classification Fine-tuning(CF) training stages. P5 [10], as well as its variants [11, 16, 17], propose to tune T5 [34] as a unified recommendation model for various downstream tasks ZESRec [7] proposes to obtain universal representations from item descriptions through BERT for zero-shot recommendation. UniSRec [15] learns item representations via a fixed BERT model followed by an MoE-enhanced network. CTRL [22] adopts the contrastive learning methodology to align the tabular space and semantic space for recommendation enhancement. VQ-Rec [14] makes improvements on UnisRec [15], which introduces vector quantization technique.",
  "3 PRELIMINARIES": "",
  "3.1 Problem Formulation": "The click-through rate (CTR) prediction task aims at accurately predicting the probability of a user clicking an item, which is the core task for recommender systems. Therefore, we mainly focus on the CTR prediction task in this work. The conventional CTR prediction task within the tabular domain can be formulated as  where X U i = h ğ‘¥ ğ‘ˆ ğ‘–, 1 , . . . , ğ‘¥ ğ‘ˆ ğ‘–,ğ¹ ğ‘ˆ i is the set of user features, X I i = h ğ‘¥ ğ¼ ğ‘–, 1 , . . . , ğ‘¥ ğ¼ ğ‘–,ğ¹ ğ¼ i is the set of item features, X C i = h ğ‘¥ ğ¶ ğ‘–, 1 , . . . , ğ‘¥ ğ¶ ğ‘–,ğ¹ ğ¶ i is the set of context features for the click prediction event (e.g., device, season, etc.), ğ‘¦ ğ‘– is the label of the data point, and ğœƒ is the model parameter. We use ğ¹ ğ‘ˆ , ğ¹ ğ¼ , ğ¹ ğ¶ to denote the number of user features, item features, and context features, respectively. These methods only focus on modeling feature interactions of the input based on the target sample only but fail to model the user behavior patterns. Modeling user behavior patterns plays an important role in boosting personalized recommendation performance. This line of methods takes users' historical behaviors as extra inputs and models the dependencies between the candidate item and historical items, which can be formulated as  where [âŸ¨ X I ik , ğ‘¦ ğ‘– ğ‘˜ âŸ©] ğ¾ ğ‘˜ = 1 represents the list of user's historical behaviors and their corresponding labels, and ğœƒ is the model parameter. In this paper, we aim to involve the embedded open-world knowledge of large language models to achieve harmonious disentanglement and collaboration between the tabular and semantic space for recommendation enhancement. Hence, the prediction can be formulated as  where Î¦ ğ‘† represents the encoder of a large language model.",
  "3.2 Mutual Information Minimization & Maximization": "Mutual information (MI) is important but hard to compute in neural networks. For mutual information maximization, MINE [1] builds connections between the expectations of variables and mutual information, and proposes a lower bound of the mutual information based on the Donsker-Varadhan representation of KL divergence. DIM [13] points out that we do not necessarily need to obtain the precise value of MI but only need to maximize it. They use the Jensen-Shannon Divergence to estimate the MI and therefore propose a GAN-style loss to maximize it:  For mutual information minimization, CLUB [3] introduces an upper bound for mutual information. When the conditional distribution ğ‘ ( ğ‘¦ | ğ‘¥ ) is known, the upper bound can be represented as  When the conditional distribution is not known, one could use a variational distribution ğ‘ ğœƒ ( ğ‘¦ | ğ‘¥ ) to approximate it and the upper bound then becomes",
  "4 METHODOLOGY": "In this section, we describe the methodology of DisCo, which is model-agnostic and compatible with different backbones.",
  "4.1 Overview": "As illustrated in Figure 2, we first prepare the tabular and semantic embeddings. In this paper, we encode the user behaviors in tabular and semantic representation space and extract patterns from the resulting tabular and semantic embeddings. To make use of the general open-world knowledge from the semantic space in an efficient manner, we pre-store the semantic embedding of each item in an indexed knowledge base, with the item ID being the indexing key. The semantic embeddings are generated from a frozen LLM, with the textual item descriptions as inputs. The tabular embeddings are obtained via tabular embedding layers with one-hot encoded features as inputs. The proposed DisCo mainly consists of three components: the dual-side attentive network, the sufficiency constraint, and the disentanglement constraint. To collaborate the tabular and semantic representation space, we propose a Dual-Side Attentive Network to encode the patterns from user behaviors under collaborative learning based representation space, semantic dependencies-based representation space, and the interactions of the two spaces. The resulting pattern vectors serve as additional features for arbitrary recommendation models. In addition, we propose two constraints to regularize the model, which preserve the useful and unique information from the two representation spaces: 1) A sufficiency constraint to maximize the mutual information between encoded vectors and the labels, which preserves the task related information and eliminates the noise. 2) A disentanglement constraint to minimize the mutual information between vectors from different representation spaces, which forces each space to provide unique domain-specific knowledge. Together, these components strike a harmonious balance between collaboration and disentanglement. KDD '24, Aug 25-29, 2024, Barcelona Kounianhua Du et al. Tabular Embedding Layer Label Embedding Layer I te m Pool Field-Value Prompt Template I ndexed Knowledge Base I te m Features Textual Description Semantic Embedding TT-Attn SS-Attn ST-Attn TS-Attn Target Sample Historical Behaviors LLM Disentanglement Constraint Dual-Side Attentive Newtork Tabular Semantic I ndexed Knowledge Base Sufficiency Constraint Arbitrary Recommendation Model ? ? ? Pattern Vectors Eliminated by Sufficiency Ensured by Sufficiency Ensured by Disentanglement (a) (b) (c) Figure 2: Overview. (a) To extract the semantic knowledge, a textual description is obtained for each item using a field-value prompt template, which is then fed to a LLM for semantic embedding and stored in an indexed knowledge base. (b) The candidate item and the historical behaviors are encoded in tabular and semantic representation spaces, which are then sent to Dual-Side Attentive Network for intra-domain and inter-domain pattern vectors. The resulting pattern vectors serve as extra features and can be appended to arbitrary recommendation model. (c) Two constraints are devised to regularize the model and preserve both the aligning part and the disentangling part of useful information from the two representation spaces. The sufficiency constraint is applied on the behavior vectors and the summarized pattern vectors to preserve the useful information. The disentanglement constraint is applied on the pattern vectors from the two different domains to force the model to capture unique information from both domains.",
  "4.2 Indexed Knowledge Base": "To efficiently utilize the semantic knowledge, we first build an indexed knowledge base to extract and pre-store the semantic embedding of each item. then fed into a large language model Î¦ ğ‘† (Â·) to acquire the semantic embedding, which will be stored in an indexed knowledge base ğ¾ğµ [Â·] for further usage, with the item features being the index key and the semantic embedding being the value. Figure 3: The illustration of the field-value prompt template. Here is a movie, title is Titanic, genre is Romantic, and director is James Cameron. Prompt Template I te m Description Titanic   Romantic   James Cameron Title Genre Director Field Value Movie For each item, we obtain a semantic description for it using the field-value prompt template. As shown in Figure 3, for a movie with title Titanic , genre Romantic , and director James Cameron , we can obtain the item description \"Here is a movie, title is Titanic, genre is Romantic, and director is James Cameron.\" . The item description is",
  "4.3 Dual-Side Attentive Network": "As discussed in Section 1, the relations among the same set of user behaviors can be different in the tabular and semantic domains. In order to capture the distinct and shared patterns among the behaviors in both domains, we design a dual-side attentive network (DS-Attn) module, which consists of intra-domain attention and inter-domain attention. The intra-domain attention models the distinct domain-specific patterns within the tabular domain for collaborative-based correlations and within the semantic domain for semantic-based dependencies, respectively. The inter-domain attention models the shared patterns between the two domains, where a query-key exchange between the two domains is adopted to make the two domains attend to each other. 4.3.1 Behaviors Encoding. For each target sample Xi = [ X U i , X I i , X C i ] , we gather ğ¾ recent historical behaviors {âŸ¨ X I ik , ğ‘¦ ğ‘– ğ‘˜ âŸ©} ğ¾ ğ‘˜ = 1 to assist the prediction. The candidate item and the historical items are transformed into representations in tabular domain and semantic domain DisCo: Towards Harmonious Disentanglement and Collaboration between Tabular and Semantic Space for Recommendation KDD '24, Aug 25-29, 2024, Barcelona as follows, âˆ€ ğ‘— âˆˆ { ğ‘–, ğ‘– 1 , . . . , ğ‘– ğ¾ } :   where Î¦ ğ‘‡ denotes the tabular domain embedding network and ğ¾ğµ [Â·] denotes the indexed knowledge base constructed in the previous stage. ğ‘€ğ¿ğ‘ƒ is used to reduce the dimension of the semantic embedding. h S j and h T j denote the embedding of item features in semantic space and tabular space, respectively. In order to decrease the entanglement of the embeddings for intra-domain and inter-domain pattern extraction, we decouple the corresponding embeddings into chunks. âˆ€ ğ‘— âˆˆ { ğ‘–, ğ‘– 1 , . . . , ğ‘– ğ¾ } :   where h SI j âˆˆ ğ‘… ğ‘‘ 2 , h SC j âˆˆ ğ‘… ğ‘‘ 2 , h TI j âˆˆ ğ‘… ğ‘‘ 2 , and h TC j âˆˆ ğ‘… ğ‘‘ 2 . In addition, following [8, 31], we also encode the historical labels to model accurate click signals, âˆ€ ğ‘˜ âˆˆ { 1 , . . . , ğ¾ } :  where Î¦ ğ‘Œ denotes the label embedding network. 4.3.2 Attention Block of DS-Attn. The dual-side attentive network consists of four attention blocks for tabular, semantic, tabular-tosemantic, and semantic-to-tabular user behavior patterns. We first define the function of the basic attention block as follows:   where ğœƒ denotes the parameters of an attention block. The detail operations for H , L â€² = ğ´ğ‘¡ğ‘¡ğ‘› ğœƒ ( Q , K , V , L ) are:    where Q âˆˆ ğ‘… 1 Ã— ğ‘‘ 2 , K âˆˆ ğ‘… ğ¾ Ã— ğ‘‘ 2 , V âˆˆ ğ‘… ğ¾ Ã— ğ‘‘ 2 , L âˆˆ ğ‘… ğ¾ Ã— ğ‘‘ denote the input for query, key, value, and labels, and WQ , WK , WV âˆˆ ğ‘… ğ‘‘ 2 Ã— ğ‘‘ represent the trainable weights. 4.3.3 Intra-Domain Attention. To capture the inner domain tabular and semantic behavior patterns, we apply the intra-domain attention among the candidate item and the behavior sequences within the tabular and semantic domains. We define the query, key, and value for intra-tabular and intrasemantic behavior patterns as   where Q SS i , K SS i , and V SS i denote the input for intra-semantic attention and Q TT i , K TT i , and V TT i represent the input of intra-tabular attention. Then the user behavior patterns in different spaces can be obtained by   4.3.4 Inter-Domain Attention. Wefurther capture the inter-domain patterns and model the shared knowledge with an inter-domain attention module. We define the query, key, and value for intra-tabular and intrasemantic behavior patterns encoded as   Â« â€¹ Â« â€¹ Then the user behavior patterns in different spaces can be obtained by",
  "4.4 Sufficiency Constraint": "To preserve the task-related information from the two representation spaces and filter out the noises, we maximize the mutual information between the encoded vectors h from each domain and the labels. The difficulty here lies in that the label space is discrete and only consists of two values. Therefore, we follow DIM [13] to use the summarized pattern vectors H Â· from each space to represent the label space, which extends the discrete states into high-dimensional continuous space. To maximize the mutual information, we maximize the distance between the marginal distribution and joint distributions between two variables. Concretely, for tabular domain sufficiency preserving, we sample the positive pairs as I T+ = {âŸ¨ h TI i , H TT i + âŸ©| ğ‘¦ ğ‘– = ğ‘¦ ğ‘– +} and sample the negative pairs as I T-= {âŸ¨ h TI i , H TT i -âŸ©| ğ‘¦ ğ‘– â‰  ğ‘¦ ğ‘– -} . For semantic domain sufficiency preserving, we sample the positive pairs as I S+ = {âŸ¨ h SI i , H SS i + âŸ©| ğ‘¦ ğ‘– = ğ‘¦ ğ‘– +} and sample the negative pairs as I S-= {âŸ¨ h SI i , H SS i -âŸ©| ğ‘¦ ğ‘– â‰  ğ‘¦ ğ‘– -} . The discriminator network D ğœƒ 1 , D ğœƒ 2 are used to distinguish the two pairs. The optimization objective is:   (25)",
  "4.5 Disentanglement Constraint": "To preserve the unique information from the two representation spaces, a two-level disentanglement is applied. Concretely, we minimize the mutual information among the pattern vectors summarized from behavior vectors in the two domains. To achieve this, we minimize the vCLUB [3] upper bound of the mutual information, which is defined as  KDD '24, Aug 25-29, 2024, Barcelona Kounianhua Du et al. where ğ‘ ğœƒ âˆ— ( Y | X ) is a variational distribution with parameter ğœƒ âˆ— to approximate ğ‘ ( Y | X ) . At each iteration of training, the variational approximation network trained to maximize log ğ‘ ğœƒ âˆ— ( Y | X ) is first optimized, and then the main model is optimized. In this way, we train a vCLUB mutual information estimator for each pair of feature vectors from different domains and minimize the mutual information between each pair. The loss objective of the disentanglement module could then be formulated as",
  "4.6 Prediction and Training Objective": "The aggregated feature and label embeddings are then disentangled and appended to any recommendation backbone models for prediction as  where ğ‘“ (Â·) denotes an arbitrary recommendation model. The training objective consists of the prediction loss, the sufficiency loss, and the disentanglement loss, which can be formulated as:   where ğ›¼ and ğ›½ are hyperparameters to scale the loss components.",
  "5 EXPERIMENTS": "In this section, we empirically evaluate the proposed model on three datasets. Five research questions lead the experiment part. RQ1 How does DisCo perform against the baselines? RQ2 Is DisCo compatible with different backbones? RQ3 Does each model component contribute to the performance? RQ4 How is the efficiency of DisCo?",
  "5.1 Setup": "5.1.1 Datasets. We use three public datasets to evaluate DisCo. The statistics of the datasets are summarized in Table 1. Table 1: Dataset statistics. Â· ML-1M 1 is a collection of movie ratings provided by users of the MovieLens website. Â· AZ-Toys 2 gathers product reviews and metadata related to toys and games available in Amazon e-commerce. Â· ML-25M 3 is a popular movie recommendation dataset widely used in machine learning and recommender systems. 1 https://grouplens.org/datasets/movielens/1m/ 2 https://cseweb.ucsd.edu/ jmcauley/datasets.html 3 https://files.grouplens.org/datasets/movielens/ml-25m.zip Samples with ratings greater than 3 are treated as positive samples, with the others being negative samples. The window size of the historical behaviors is 30. Data is split according to the global timestamps. Specifically, the training data lies between [ 0 , ğ‘‡ 0 ) , the validation data lies between [ ğ‘‡ 0 , ğ‘‡ 1 ) , and the test data lies between [ ğ‘‡ 1 , +âˆ) . The ratio of data amount for train/valid/test is 8 : 1 : 1. 5.1.2 Evaluation Metrics. Two widely used metrics including AUC (Area under the ROC curve) and Log Loss (binary cross-entropy loss) are applied to evaluate the performance. 5.1.3 Competing Models. We compare the proposed DisCo with the following methods: 1) conventional tabular methods including DeepFM [12], DCN [40], PNN [33], xDeepFM [23], AutoInt [37], DIN [45], DIEN [44], and 2) semantic-enhanced methods P5 [10], UnisRec [15], CTRL [22], and VQ-Rec [14]. 5.1.4 Implementation Details. We utilize Vicuna-13b [4] released by FastChat 4 for text encoding. For a fair comparison, we fix the embedding size and the hidden layer size to be the same for all backbone models. The embedding size for the tabular domain representation is 32. The hidden layer size used for MLP is [ 128 , 64 ] . We use the bilinear networks to serve as the discriminator network in the sufficiency constraint and vCLUB mutual information estimator in the disentanglement constraint. The coefficients for the sufficiency constraint loss and disentanglement constraint loss are 0 . 02 and 0 . 01, respectively. For each model, the learning rate is searched in the range of { 1 ğ‘’ -4 , 3 ğ‘’ -4 , 5 ğ‘’ -4 , 1 ğ‘’ -3 } , and the weight decay is searched in the range of { 1 ğ‘’ -5 , 3 ğ‘’ -5 , 5 ğ‘’ -5 , 1 ğ‘’ -4 , 3 ğ‘’ -4 } . We use the Adam [21] optimizer during training. The patience of early stop is 10. The code is available 5 6 .",
  "5.2 Overall Performance (RQ1)": "In this- section, we compare our proposed DisCo with various baseline models. The experiment results are displayed in Table 2. From the results, one can draw the following conclusions. 1) Our proposed DisCo can consistently outperform all the baseline models including tabular-only methods and the semantic-enhanced methods. The improvements are statistically significant under p-value < 0 . 001. This shows the effectiveness of the proposed paradigm that disentangles and collaborates tabular and semantic domain knowledge for enhanced recommendation. 2) The methods that involve in semantic knowledge can surpass the conventional tabular-only methods, which shows the effectiveness of introducing external semantic knowledge into recommendation. 3) DisCo outperforms methods that focus on aligning the two representation spaces. For example, CTRL utilizes the contrastive learning methodology to align the two representation spaces. These methodologies tend to make representations in different representation spaces closer, where unique information is discarded during training. This validates the effectiveness of DisCo that preserves the unique information of the two different representation spaces. 4 https://github.com/lm-s 5 https://github.com/KounianhuaDu/DisCo 6 https://github.com/mindspore-lab/models/tree/master/research/huaweinoah/DisCo DisCo: Towards Harmonious Disentanglement and Collaboration between Tabular and Semantic Space for Recommendation KDD '24, Aug 25-29, 2024, Barcelona Table 2: Major results. For all the baselines, we append the user histories and their corresponding ratings/labels for fair comparison. The best result is in bold, while the second-best value is underlined. Rel.Impr denotes the relative AUC improvement of DisCo against each baseline model. The symbol * indicates statistically significant improvement with p-value < 0 . 001 . Table 3: Compatibility experiments. The proposed method offers additional feature fields, which can be followed by different feature interaction operations. We test its compatibility with different CTR backbones.",
  "5.3 Compatibility Study (RQ2)": "Since the extracted patterns obtained from the dual-side attentive network can serve as extra features, they could be appended to arbitrary conventional recommendation models. In this section, we evaluate the compatibility of the proposed framework on different conventional backbones. Thefeature interaction methods of the backbones include productbased, MLP-based, and attention-based operators. We test DisCo on these different operators and justify the effectiveness of the resulting feature fields. The results are displayed in Table 3. From the results, we can see that the proposed method could offer performance gains for various backbone models and operations. The improvements are statistically significant under p-value < 0 . 001, which validates the superior compatibility of DisCo.",
  "5.4 Ablation Studies (RQ3)": "5.4.1 Impact of the Dual-Side Attentive Network. In this section, we validate the effectiveness of the proposed dual-side attentive network module. Concretely, we remove the inter-domain attention within which the two representation spaces attend to each other and only retrain the intra-domain attention. This results in the common two-tower structure in recommender systems, where the semantic and tabular representations are modeled separately for semantic dependencies and collaborative signals respectively. While our dualside attentive network module models both the intra-domain and inter-domain user behavior patterns. The results of the two-tower attention and the dual-side attentive network module are displayed in Table 4. Table 4: Experiment on the Aggregation Module. From the results, we could see that the proposed dual-side attentive network can outperform the two-tower aggregation, which validates the effectiveness of the proposed module that captures both the intra-domain and the inter-domain knowledge. KDD '24, Aug 25-29, 2024, Barcelona Kounianhua Du et al. Intra -domain without disentanglement Inter -domain without disentanglement Intra -domain with disentanglement Inter -domain with disentanglement Figure 4: T-SNE visualization of the representations for the dual-side attentive network output (ML-1M). 5.4.2 Impact of the Constraints. In this section, we study the impact of the proposed constraints. Table 5: Impacts of the two constraints. Firstly, we conduct experiments with and without the constraints, the results of which are displayed in Table 5. From the results, we can see the following conclusions. 1) Both the sufficiency and the disentanglement constraints could offer performance gains to the model, with sufficiency helps to preserve task-relevant information from each space and disentanglement helps to enforce unique information from each space. 2) The two constraints collaborates and boosts performance with each other, which helps to capture both the consistency and specificity information of the two spaces. In addition, we further visualize the representations of the dualside attentive module output with and without the disentanglement constraint to dig into how the disentanglement impacts the distribution of representations. The visualization of ML-1M is displayed in Figure 4. Visualizations of more datasets could be found in the Appendix. Concretely, we visualize the representations of the intra-domain pattern vectors H TT , H SS and inter-domain pattern vectors H TS , H ST with t-sne [38]. The left column of Figure 4 displays the representations without the disentanglement constraint. While the right column of the figure displays those with the disentanglement constraint. From the figure, we can see that under the regularization of the disentanglement constraint, the distributions of representations from different domains are separated better and there are apparently better manifolds existing in the representation spaces with the disentanglement constraint. This illustrates that the disentanglement constraint could well separate and extract different information from the two spaces, which validates the effectiveness of our design.",
  "5.5 Efficiency Analysis (RQ4)": "In this section, we discuss the efficiency of the proposed model. Firstly, the semantic embeddings for items could be pre-computed and stored in the indexed knowledge base, the construction of which can be done offline and only once. In addition, after the MLP used to reduce the dimension of the semantic embedding is trained, we could use the trained MLP to reduce the dimension and further keep a reduced-dimension version of the indexed knowledge base. Therefore, in the inference stage we do not need to deal with the high-dimensional semantic vectors. Table 6: Training and inference time per sample (s). The training and inference time analysis is displayed in Table 6. All the experiments are done on a single V100 GPU with Intel Xeon Gold 6278C 2.60GHz CPU and run three times to get the average time. From the results, we can see that the proposed method does not cause a heavy overhead.",
  "6 CONCLUSION": "Recommender systems play a vital role in our daily life. Conventional recommendation methods focus on modeling feature interactions and user behaviors within the ID-based tabular representation space and fail to capture semantic dependencies among user behaviors. Existing semantic-enhanced methods focus on aligning the tabular and semantic space, while the unique and disentangled parts of the two representation spaces are not well explored. In this paper, we propose DisCo to disentangle and collaborate the tabular and semantic representation spaces to capture both the consistent and the specific knowledge from the two spaces for enhanced recommendations. Concretely, we design three modules, namely dual-side attentive network, the sufficiency constraint, and the disentanglement constraint. To efficiently utilize the semantic knowledge, a textual description for each item is firstly obtained and encoded by LLMs, the embedding of which is then stored into an indexed knowledge. The dual-side attention module models intradomain and inter-domain patterns to offer additional knowledge for arbitrary recommendation backbones, which is constrained by the designed sufficiency and disentanglement constraints. The two constraints force the model to preserve useful information and extract DisCo: Towards Harmonious Disentanglement and Collaboration between Tabular and Semantic Space for Recommendation KDD '24, Aug 25-29, 2024, Barcelona unique information from the two spaces. Extensive experiments and ablation studies on three datasets and various backbone models justify the effectiveness of the proposed method.",
  "REFERENCES": "[1] Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, R. Devon Hjelm, and Aaron C. Courville. 2018. Mutual Information Neural Estimation. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, StockholmsmÃ¤ssan, Stockholm, Sweden, July 10-15, 2018 , Vol. 80. PMLR, 530-539. [2] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In DLRS@RecSys . [3] Pengyu Cheng, Weituo Hao, Shuyang Dai, Jiachang Liu, Zhe Gan, and Lawrence Carin. 2020. CLUB: A Contrastive Log-ratio Upper Bound of Mutual Information. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event (Proceedings of Machine Learning Research, Vol. 119) . PMLR, 1779-1788. [4] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality. https://lmsys.org/blog/2023-03-30-vicuna/ [5] Ingrid A. Christensen and Silvia Schiaffino. 2011. Entertainment recommender systems for group of users. Expert Systems with Applications 38, 11 (2011), 1412714135. https://doi.org/10.1016/j.eswa.2011.04.221 [6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018). [7] Hao Ding, Yifei Ma, Anoop Deoras, Yuyang Wang, and Hao Wang. 2021. ZeroShot Recommender Systems. arXiv:2105.08318 [cs.LG] [8] Kounianhua Du, Weinan Zhang, Ruiwen Zhou, Yangkun Wang, Xilong Zhao, Jiarui Jin, Quan Gan, Zheng Zhang, and David P Wipf. 2022. Learning Enhanced Representation for Tabular Data via Neighborhood Propagation. Advances in Neural Information Processing Systems 35 (2022), 16373-16384. [9] Yufei Feng, Fuyu Lv, Weichen Shen, Menghan Wang, Fei Sun, Yu Zhu, and Keping Yang. 2019. Deep Session Interest Network for Click-Through Rate Prediction. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019 . 2301-2307. [10] Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022. Recommendation as language processing (rlp): A unified pretrain, personalized prompt & predict paradigm (p5). In Proceedings of the 16th ACM Conference on Recommender Systems . 299-315. [11] Shijie Geng, Juntao Tan, Shuchang Liu, Zuohui Fu, and Yongfeng Zhang. 2023. VIP5: Towards Multimodal Foundation Models for Recommendation. arXiv preprint arXiv:2305.14302 (2023). [12] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. In IJCAI . [13] R. Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Philip Bachman, Adam Trischler, and Yoshua Bengio. 2019. Learning deep representations by mutual information estimation and maximization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . [14] Yupeng Hou, Zhankui He, Julian McAuley, and Wayne Xin Zhao. 2023. Learning Vector-Quantized Item Representation for Transferable Sequential Recommenders. arXiv:2210.12316 [cs.IR] [15] Yupeng Hou, Shanlei Mu, Wayne Xin Zhao, Yaliang Li, Bolin Ding, and Ji-Rong Wen. 2022. Towards Universal Sequence Representation Learning for Recommender Systems. arXiv:2206.05941 [cs.IR] [16] Wenyue Hua, Yingqiang Ge, Shuyuan Xu, Jianchao Ji, and Yongfeng Zhang. 2023. UP5: Unbiased Foundation Model for Fairness-aware Recommendation. arXiv preprint arXiv:2305.12090 (2023). [17] Wenyue Hua, Shuyuan Xu, Yingqiang Ge, and Yongfeng Zhang. 2023. How to Index Item IDs for Recommendation Foundation Models. arXiv preprint arXiv:2305.06569 (2023). [18] Yanhua Huang, Hangyu Wang, Yiyun Miao, Ruiwen Xu, Lei Zhang, and Weinan Zhang. 2022. Neural Statistics for Click-Through Rate Prediction. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1849-1853. [19] Yu-Chin Juan, Yong Zhuang, Wei-Sheng Chin, and Chih-Jen Lin. 2016. Fieldaware Factorization Machines for CTR Prediction. In Proceedings of the 10th ACM Conference on Recommender Systems, Boston, MA, USA, September 15-19, 2016 . ACM, 43-50. [20] Yuchin Juan, Yong Zhuang, Wei-Sheng Chin, and Chih-Jen Lin. 2016. Field-aware factorization machines for CTR prediction. In RecSys . [21] Diederik P. Kingma and Jimmy Ba. 2017. Adam: A Method for Stochastic Optimization. arXiv:1412.6980 [cs.LG] [22] Xiangyang Li, Bo Chen, Lu Hou, and Ruiming Tang. 2023. CTRL: Connect Tabular and Language Model for CTR Prediction. arXiv preprint arXiv:2306.02841 (2023). [23] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong Sun. 2018. xdeepfm: Combining explicit and implicit feature interactions for recommender systems. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1754-1763. [24] Jianghao Lin, Bo Chen, Hangyu Wang, Yunjia Xi, Yanru Qu, Xinyi Dai, Kangning Zhang, Ruiming Tang, Yong Yu, and Weinan Zhang. 2024. ClickPrompt: CTR Models are Strong Prompt Generators for Adapting Language Models to CTR Prediction. In Proceedings of the ACM on Web Conference 2024 (WWW '24) . 3319-3330. [25] Jianghao Lin, Xinyi Dai, Yunjia Xi, Weiwen Liu, Bo Chen, Hao Zhang, Yong Liu, Chuhan Wu, Xiangyang Li, Chenxu Zhu, Huifeng Guo, Yong Yu, Ruiming Tang, and Weinan Zhang. 2023. How Can Recommender Systems Benefit from Large Language Models: A Survey. arXiv preprint arXiv:2306.05817 (2023). [26] Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang Quan, Ruiming Tang, Yong Yu, and Weinan Zhang. 2024. ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation. In Proceedings of the ACM on Web Conference 2024 (WWW '24) . 3497-3508. [27] Bin Liu, Chenxu Zhu, Guilin Li, Weinan Zhang, Jincai Lai, Ruiming Tang, Xiuqiang He, Zhenguo Li, and Yong Yu. 2020. AutoFIS: Automatic Feature Interaction Selection in Factorization Models for Click-Through Rate Prediction. In KDD '20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020 . ACM, 2636-2645. [28] Guang Liu, Jie Yang, and Ledell Wu. 2022. PTab: Using the Pre-trained Language Model for Modeling Tabular Data. arXiv preprint arXiv:2209.08060 (2022). [29] Qi Pi, Weijie Bian, Guorui Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Practice on Long Sequential User Behavior Modeling for Click-Through Rate Prediction. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2019, Anchorage, AK, USA, August 4-8, 2019 . ACM, 2671-2679. [30] Qi Pi, Guorui Zhou, Yujing Zhang, Zhe Wang, Lejian Ren, Ying Fan, Xiaoqiang Zhu, and Kun Gai. 2020. Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate Prediction. In CIKM '20: The 29th ACM International Conference on Information and Knowledge Management, Virtual Event, Ireland, October 19-23, 2020 . ACM, 2685-2692. [31] Jiarui Qin, Weinan Zhang, Rong Su, Zhirong Liu, Weiwen Liu, Ruiming Tang, Xiuqiang He, and Yong Yu. 2021. Retrieval & Interaction Machine for Tabular Data Prediction. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining . 1379-1389. [32] Jiarui Qin, Weinan Zhang, Xin Wu, Jiarui Jin, Yuchen Fang, and Yong Yu. 2020. User Behavior Retrieval for Click-Through Rate Prediction. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 . ACM, 2347-2356. [33] Yanru Qu, Bohui Fang, Weinan Zhang, Ruiming Tang, Minzhe Niu, Huifeng Guo, Yong Yu, and Xiuqiang He. 2018. Product-based Neural Networks for User Response Prediction over Multi-field Categorical Data. ACM Transactions on Information Systems (2018). [34] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research 21, 1 (2020), 5485-5551. [35] Steffen Rendle. 2010. Factorization machines. In ICDM . [36] Brent Smith and Greg Linden. 2017. Two decades of recommender systems at Amazon.com. IEEE Internet Computing (2017). https://www.amazon.science/ publications/two-decades-of-recommender-systems-at-amazon-com [37] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, and Jian Tang. 2019. AutoInt: Automatic Feature Interaction Learning via SelfAttentive Neural Networks. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management, CIKM 2019, Beijing, China, November 3-7, 2019 . ACM, 1161-1170. [38] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. Journal of machine learning research 9, 11 (2008). [39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. Attention Is All You Need. arXiv:1706.03762 [cs.CL] [40] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & Cross Network for Ad Click Predictions. In Proceedings of the ADKDD'17, Halifax, NS, Canada, August 13 - 17, 2017 . ACM, 12:1-12:7. [41] Chuhan Wu, Fangzhao Wu, Yongfeng Huang, and Xing Xie. 2023. Personalized news recommendation: Methods and challenges. ACM Transactions on Information Systems 41, 1 (2023), 1-50. KDD '24, Aug 25-29, 2024, Barcelona Kounianhua Du et al. [42] Yunjia Xi, Weiwen Liu, Jianghao Lin, Jieming Zhu, Bo Chen, Ruiming Tang, Weinan Zhang, Rui Zhang, and Yong Yu. 2023. Towards Open-World Recommendation with Knowledge Augmentation from Large Language Models. arXiv:2306.10933 [cs.IR] [43] Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua. 2017. Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017, Melbourne, Australia, August 19-25, 2017 . 3119-3125. [44] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Deep Interest Evolution Network for Click-Through Rate Prediction. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019 . 5941-5948. [45] Guorui Zhou, Xiaoqiang Zhu, Chengru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep Interest Network for Click-Through Rate Prediction. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2018, London, UK, August 19-23, 2018 . 1059-1068.",
  "A VISUALIZATIONS": "Visualizations of the representations for the dual-side attentive network on AZ-Toys and ML-25M are displayed in Figure 5 and Figure 6. Intra -domain without disentanglement Intra -domain with disentanglement Inter -domain without disentanglement Inter -domain with disentanglement Figure 5: T-SNE visualization of the representations for the dual-side attentive network output (AZ-Toys). Intra -domain without disentanglement Intra - domain with disentanglement Inter -domain without disentanglement Figure 6: T-SNE visualization of the representations for the dual-side attentive network output (ML-25M). Inter -domain with disentanglement DisCo: Towards Harmonious Disentanglement and Collaboration between Tabular and Semantic Space for Recommendation KDD '24, Aug 25-29, 2024, Barcelona",
  "Table 7: Items used in the case study.": "",
  "Table 8: Similarities of items in different spaces.": "",
  "B EXPERIMENTS ON LONG-TAIL DATA": "From the results, we can see that In this section, we validate the effectiveness of DisCo on the longtail data where features are less-hit in the tabular representation space. Concretely, we sort items based on their frequency of occurrences in the training set. The bottom 10% in terms of frequency are classified as long-tail items. Then we study the performance of the best-performed tabular-only model DIN and that with the proposed DisCo model.",
  "Table 9: Experiment on the tail Data.": "The performance comparisons are displayed in Table 9. From the results, we can see that the proposed method gives a significant performance boost on the tail data. Since the general knowledge contained in the pretrained semantic embeddings helps to complement the less-trained features in the tabular representation space, where the semantic information is injected through the dual-side attentive network under the regularizations of the constraints.",
  "C CASE STUDY": "In this section, We would like to provide an example to support the claim that encodings from LLMs can capture open-world knowledge as below. Given two items in the movielens dataset: \"Galaxy Quest\" and \"Babe: Pig in the City\". Their genres provided in the dataset and the tags provided by open world are listed in Table 7. For the two items: Â· The two items do not share any common tokens. Â· The two items do not share any common features given in the dataset. Â· But they are actually close as they share a lot common features in the open world (e.g., the tags given by Douban). We then compute the cosine similarities between the above two items in the tabular space and that in the semantic space as in Table 8. (Note that no generation is involved. We use the same genres provided by the movielens dataset to obtain the item encodings for both the tabular encodings and the semantic encodings.) Â· Tabular representations cannot find the relevance between the two items, since no common features exist. Â· Semantic representations by small language models cannot well find the relevance between the two items, since no common tokens exist. Â· Semantic representations by LLMs can well find the relevance between the two items, even there are nearly no common tokens shared between the two items in the dataset. Since there are many anchors about the two items existing in the large open world training corpus (e.g., the common tags given by Douban, the similar descriptions of movies, etc.), as they are trained together, the representations of the two items tend to get close. The above case study can justify that the encodings from LLMs can help to capture open-world knowledge. Received 20 February 2007; revised 12 March 2009; accepted 5 June 2009",
  "keywords_parsed": [
    "Recommender Systems",
    " User Modeling",
    " Large Language Model"
  ]
}