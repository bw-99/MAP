{
  "MvFS: Multi-view Feature Selection for Recommender System": "Youngjune Lee NAVER Corporation, South Korea youngjune.lee93@navercorp.com Yeongjong Jeong NAVER Corporation, South Korea yeongjong.jeong@navercorp.com Keunchan Park NAVER Corporation, South Korea keunchan.park@navercorp.com",
  "ABSTRACT": "Feature selection, which is a technique to select key features in recommender systems, has received increasing research attention. Recently, Adaptive Feature Selection (AdaFS) has shown remarkable performance by adaptively selecting features for each data instance, considering that the importance of a given feature field can vary significantly across data. However, this method still has limitations in that its selection process could be easily biased to major features that frequently occur. To address these problems, we propose Multi-view Feature Selection (MvFS), which selects informative features for each instance more effectively. Most importantly, MvFS employs a multi-view network consisting of multiple sub-networks, each of which learns to measure the feature importance of a part of data with different feature patterns. By doing so, MvFS mitigates the bias problem towards dominant patterns and promotes a more balanced feature selection process. Moreover, MvFS adopts an effective importance score modeling strategy which is applied independently to each field without incurring dependency among features. Experimental results on real-world datasets demonstrate the effectiveness of MvFS compared to state-of-the-art baselines.",
  "CCS CONCEPTS": "· Information systems → Recommender systems .",
  "KEYWORDS": "Recommender System; Feature Selection; CTR Prediction;",
  "ACMReference Format:": "Youngjune Lee, Yeongjong Jeong, Keunchan Park, and SeongKu Kang. 2023. MvFS: Multi-view Feature Selection for Recommender System. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management (CIKM '23), October 21-25, 2023, Birmingham, United Kingdom. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3583780.3615243 ∗ SeongKu Kang is the corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CIKM '23, October 21-25, 2023, Birmingham, United Kingdom © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0124-5/23/10...$15.00 https://doi.org/10.1145/3583780.3615243",
  "SeongKu Kang ∗": "University of Illinois at Urbana-Champaign, USA seongku@illinois.edu",
  "1 INTRODUCTION": "In real-world web platforms, recommender systems (RS) encounter a multitude of features collected from users (e.g., age, gender), items (e.g., brand, price), and their interactions (e.g., dwell time, location), and the effective utilization of these features plays a critical role in the quality of recommendations. To capture the intricate relationships of the features, RS models have employed sophisticated architectures with powerful capacities [2, 6, 7, 9, 10, 24]. However, it has been noted that some of these features lack relevance or exhibit redundancy in the context of user-item interactions, and thereby blindly feeding all features into the model often leads to suboptimal accuracy and slower model optimization [11, 17, 18, 25]. To address this problem, feature selection, which is a technique to select a subset of the most informative features, has recently received increasing attention [11, 12, 14, 16, 17, 25]. In the past decades, a variety of methods has been studied for feature selection, ranging from hand-crafted and statistical methods [3, 4, 27, 28] to traditional machine learning methods [19, 23]. However, they often show limited efficacy as the feature selection is conducted independently from the subsequent RS model, disregarding the model's prediction behaviors [17]. Recent methods [11, 17, 25] have employed the AutoML approach [13, 15] to automatically identify and select the most predictive features during the model optimization, and have shown effectiveness for RS models. A state-of-the-art method, Adaptive Feature Selection (AdaFS) [11], introduces a new strategy that adaptively selects features tailored to each user-item interaction pair, considering that the importance of a given feature field can vary significantly across pairs. To handle such dynamic nature of each pair, AdaFS employs a controller network that computes the importance of each feature field for each pair. Then, it generates weighted feature embeddings by multiplying the importance. With the adaptive selection, AdaFS significantly improves performance over the previous methods that aim to select a globally fixed subset of feature fields [19, 23, 25, 27]. Still, there is much room for improvement in AdaFS. First, it employs a single controller network to select features for all useritem pairs. This makes the selection process could be easily biased in favor of a few large groups of pairs having features that occur frequently, consequently resulting in limited improvement for data with minor features having a relatively low frequency. We argue that the efficacy of the adaptive selection can be further improved by explicitly preventing the bias problem. Second, AdaFS applies a reweighting step to ensure that the sum of the computed importance remains constant before being reflected in the subsequent RS model. However, this reweighting introduces fluctuations in the overall CIKM '23, October 21-25, 2023, Birmingham, United Kingdom Youngjune Lee, Yeongjong Jeong, Keunchan Park, & SeongKu Kang Figure 1: Overview of MvFS. ... Field 1 Field 2 Field N ... ... 𝑰 Input Embedding Controller Output 𝑺 𝑬 𝑿 𝑯 … … G Importance score modeling Multi-view network scale of importance scores according to the number of selected fields, e.g., with 𝑘 selected fields, the average importance scale is proportional to 1 / 𝑘 , which creates unnecessary dependencies among the features resulting in suboptimal performance. In this work, we propose the multi-view feature selection (MvFS), which aims to address the aforementioned shortcomings. To this end, MvFS first employs multiple sub-networks within the controller. Each sub-network in MvFS is designed to specialize in data with different feature patterns, preventing it from being biased to a few dominant patterns and promoting a more balanced feature selection process. Moreover, MvFS adopts a new importance score modeling strategy that is applied independently to each field without the reweighting step. We validate the superiority of MvFS with extensive experiments on real-world datasets, and provide a detailed analysis showing the effectiveness of each proposed component.",
  "2 METHODOLOGY": "We first provide the overview of RS model with feature selection (Sec.2.1). Then, we present our multi-view feature selection strategy (Sec.2.2) and optimization procedure (Sec.2.3).",
  "2.1 RS model with Feature Selection": "In this work, we focus on the adaptive feature selection that adaptively selects informative feature fields for each user-item pair [11]. 2.1.1 Input construction. The raw data consists of several feature fields (e.g., gender, price). The most common way to construct inputs for the RS model is embedding lookup [2, 8, 18, 22, 26]. Formally, given 𝑁 feature fields, we denote each data instance as 𝑿 = [ 𝒙 1 , ..., 𝒙 𝑵 ], where 𝒙 𝒏 is a one-hot vector that encodes the value of 𝑛 𝑡ℎ feature field. Let 𝑷 𝒏 denote a learnable projection matrix of 𝑛 𝑡ℎ field. After projecting each feature field (i.e., the embedding lookup), the data instance 𝑿 is converted to the dense vector 𝑬 as:  where 𝒆 𝒏 = 𝑷 𝒏 𝒙 𝒏 denotes the embedding of each field. 2.1.2 RSmodelwithadaptive feature selection. To selectively exploit informative features for each data, a controller network is employed to compute the importance score of each feature field. Then, a weighted feature vector is generated by multiplying the computed importance score with the corresponding feature embedding. The weighted feature vector 𝑯 is obtained by  where 𝑠 𝑛 is the importance score of 𝑛 𝑡ℎ field computed by the controller. 𝑠 𝑛 can be either a real value (soft selection) or a binary value (hard selection), depending on the design choice [11]. Lastly, the weighted feature vector is fed into the subsequent RS model:  where ˆ 𝑦 is the model prediction. 𝑅𝑆 (·) can be any recommendation model, and it is treated as a black box. It is worth noting that feature selection only affects the model input, allowing it to be flexibly applied to various architectures.",
  "2.2 Multi-view Feature Selection": "We present Muiti-view Feature Selection (MvFS) with a new controller designed to select informative features while avoiding bias toward a few dominant feature patterns (Figure 1). Our controller consists of two components: (1) multi-view network which computes feature importance by incorporating various views from multiple sub-networks, and (2) importance score modeling which decides the final importance score for the feature selection. 2.2.1 Multi-view network . Multi-view network computes the importance of each feature field by taking the feature vector 𝑬 as input. The prior method [11] employs a single network for computing feature importance, and this makes the controller could be easily biased towards a few major features that occur frequently. To tackle this problem, we adopt the idea of the mixture-of-experts [5, 21]. It exploits a divide-and-conquer strategy composed of multiple distinct sub-networks, each of which learns to handle a part of input space. By leveraging multiple sub-networks focusing on data with different feature patterns, we aim to prevent to the controller from being biased and enable a more balanced feature selection. Concretely, the multi-view network consists of (1) 𝐾 sub-networks ( 𝑆𝑁 1 , 𝑆𝑁 2 , ..., 𝑆 𝑁 𝐾 ) quantifying the importance of feature fields, and (2) a gating module 𝑔 that regulates the influence of each subnetwork based on the feature patterns of each data. We first calculate the feature importance by 𝑆𝑁 𝑘 ( 𝑬 ) ∈ R 𝑁 , which yields an importance vector where each element represents the importance of each field. Then, the influences of the sub-networks 𝒓 ∈ R 𝐾 are computed using 𝑔 as follows:  where 𝜎 is the sigmoid function, 𝑾 𝒈 ∈ R 𝐾 × 𝐾𝑁 and 𝒃 𝒈 ∈ R 𝐾 are learnable weight matrix and bias vector of the gating module. We use the outputs of the sub-networks for gating so that we can use the summarized information on the input feature vector 1 . The data sharing similar feature patterns would naturally have similar outputs from the sub-networks, resulting in similar gating results. The final feature importance vector 𝑰 ∈ R 𝑁 is computed by aggregating the output of the sub-networks based on the gating results as follows:  where 𝑟 𝑘 denotes the 𝑘 𝑡ℎ value in 𝒓 . 1 We empirically obtained better results compared to the case of using the feature vector itself as input of the gating module, i.e., 𝐶 = 𝑬 . MvFS: Multi-view Feature Selection for Recommender System CIKM '23, October 21-25, 2023, Birmingham, United Kingdom Table 1: Performance comparison of feature selection methods. * indicates 𝑝 ≤ 0.05 for the two-sided t-test over the best baseline. 2.2.2 Importance score modeling . We utilize the computed importance of each field to model the final importance scores. To strike a balance between exploration and exploitation, we employ a gradual transition from soft selection to hard selection during the training process. In the early stages, the RS model explores various feature combinations through soft selection. As training progresses, we gradually prioritize informative features while disregarding unimportant and redundant ones through hard selection. The final importance score for the 𝑛 𝑡ℎ feature field is defined as follows: Metrics. We use AUC score and Logloss, which are widely used for CTR prediction task, as evaluation metrics. Note that a 0.001-level increase in AUC indicates a significant improvement [1, 2].  where 𝑙 is the threshold for selection. We model the transition using an approximation of the unit step function with the tanh function. We set 𝜏 = 𝑚𝑎𝑥 ( 5 , 1 + 0 . 001 𝑡 ) to control the smoothness of the tanh, where 𝑡 denotes the training step. This choice gradually makes 𝑠 𝑛 a binary value, enabling a smooth transition to hard selection during the training. It is worth noting that our score modeling is applied independently to each field, unlike the prior method [11] that uses the reweighting step across the fields which incurs unnecessary dependencies among the selected features. Lastly, we apply feature selection by multiplying the importance scores 𝒔 with the feature embedding 𝑬 , i.e., 𝑯 = [ 𝑠 1 𝒆 1 , 𝑠 2 𝒆 2 , ..., 𝑠 𝑁 𝒆 𝑵 ] . At the test time, we apply the hard selection with the step function.",
  "2.3 Optimization": "We train the controller and the RS model to predict interactions of user-item pairs. We denote the weighted feature vector for the 𝑚 𝑡ℎ instance as 𝑯 𝑚 , and the corresponding ground truth label as 𝑦 𝑚 . The loss function is defined as follows:  where 𝜽 𝑹𝑺 denotes the parameters of the RS model, including the embedding components and the subsequent layers. 𝜽 𝑪 denotes the parameters of the controller. To ensure reliable feature embeddings for feature selection, we initially warm up the parameters of the RS model for a few epochs without using the controller [11].",
  "3 EXPERIMENTS": "",
  "3.1 Experiment Setup": "Datasets. Weusetwopublicreal-world benchmark datasets: Avazu 2 and Criteo 3 . We randomly split each dataset 8:1:1 for training, validation, and testing. 2 https://www.kaggle.com/c/avazu-ctr-prediction 3 https://ailab.criteo.com/ressources/ Baselines & RS models. Wecompare the MvFS with the following feature selection methods: (1) AutoField [25] selects globally fixed feature fields using neural architecture search techniques [13]. (2) AdaFS [11] adaptively selects feature fields for each data instance via the controller (Sec.2.1). (3) OptFS [17] selects informative global features considering feature interactions. We evaluate the efficacy of the feature selection on the following RS models: MLP [29], DeepFM [2], DCN [24], and IPNN [20]. Implementation details. Our implementation 4 is based on the public PyTorch library for RS 5 . We utilize the official implementation for AdaFS 6 and OptFS 7 . We set the embedding dimension as 16 and the batch size as 4096. For all RS models, we set the MLP layer as two fully-connected layers of size [16, 8]. For all compared methods, we tune the learning rate and L2 regularizer from {1e-3, ... , 1e-6}. For MvFS, the number of sub-network 𝐾 is selected from {3, 4, 5}, and the selection threshold 𝑙 is selected from {0.1, 0.2, 0.3, 0.4}. For the sub-network 𝑆𝑁 𝑘 , we employ a simple linear layer with Softmax activation. We set the warm-up epoch to 3.",
  "3.2 Performance Comparison": "Table 1 presents the performance of various feature selection methods on four different RS models. First, AutoField and OptFS show a limited improvement over the base RS model (i.e., No Selection), and they often show degraded performance. This result shows the limitation of globally fixed feature selection that cannot consider varying feature importance for each data instance. Second, AdaFS, which adaptively selects the features for each data instance, shows performance gains to some extent. However, with a single networkbased controller, its selection process could be easily biased to a few dominant features. Lastly, MvFS shows significant improvement over other baselines with the multi-view network which computes feature importance by incorporating various views from multiple sub-networks, which can effectively prevent the bias problem.",
  "3.3 Study of MvFS": "Transferability analysis. We conduct an analysis to validate the transferability of MvFS. Following [11], we first train MvFS with the MLP model and freeze the controller parameters. Then, we 4 https://github.com/dudwns511/MvFS_CIKM23 5 https://github.com/rixwew/pytorch-fm 6 https://github.com/Applied-Machine-Learning-Lab/AdaFS 7 https://github.com/fuyuanlyu/OptFS CIKM '23, October 21-25, 2023, Birmingham, United Kingdom Youngjune Lee, Yeongjong Jeong, Keunchan Park, & SeongKu Kang Table 2: Transferability analysis on Avazu. * indicates 𝑝 ≤ 0.05 for the two-sided t-test over the No Selection. utilize the controller for feature selection of other RS models (i.e., DeepFM, DCN, and IPNN). The results are presented in Table 2. All models achieve higher performance with the transferred controller of MvFS, which support that MvFS can consistently select the most informative features for different RS models. It is worth noting that the best performance of each RS model is achieved when MvFS is jointly trained with each model (Table 1). This shows that the RS models have different prediction behaviors, and thereby feature selection needs to be tailored for each model, as discussed in [17]. Figure 2: The average feature importance computed by each sub-network. Results with MLP. 0.05 0.04 3 0.04 0 1 2 3 4 5 6 7 8 1011 12 1314 15 16 17 18 19 20 21 Field index (Avazu) 0.03 0.03 0.02 0.02 3 1 2 3 4 5 6 7 8 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 0.02 Field index (Criteo) Figure 3: AUC Improvement over AdaFS. (Left) Field 2 and Field 18 of Avazu. (Right) Field 18 and Field 29 of Criteo. For each dataset, we select two fields that AdaFS shows the most limited performance in the minor feature group (G2). Avazu Criteo 0054 .0036 0048 0042 0030 0036 0027 0030 0024 g 0024 0,0021 Field 2 Field 18 Field 18 Field 29 Effectiveness of controller. We provide empirical analysis to support the effectiveness of our multi-view controller from two perspectives: (a) Whether each sub-network meaningfully captures different feature patterns. (b) Whether MvFS selects features in a more balanced way than a single network (i.e., AdaFS). For (a) , we analyze the differences in the output of sub-networks in Figure 2. In specific, we compute the sum of each sub-network output (i.e., 𝑟 𝑘 · 𝑆𝑁 𝑘 ( 𝑬 ) of Eq.5) on all test data instances and divide by the total number of instances. We observe that each sub-network assigns higher importance to different feature fields. For instance, on Avazu, the sub-network with index 0 tends to consider fields 10, 12, and 18 as important, while the sub-network with index 1 also considers fields 0, 1, and 18 as important. This result shows that each sub-network indeed focuses on different parts of data having distinct feature patterns. For (b) , we analyze the improvement by MvFS on data with minor features having a relatively low frequency. To this end, we split the data into two groups according to the feature frequency: (1) G1 with the top 95% frequent features and (2) G2 with the remaining ones. Then, we present the absolute improvement over AdaFS for each group in Figure 3. We observe that MvFS achieves higher performance than AdaFS on both G1 and G2, with a particularly significant improvement in G2. This result shows that our multi-view controller indeed better prevents it from being biased to the major features and enables a more balanced feature selection. Further, this result supports the superior performance of MvFS in Table 1. Figure 4: Ablation study on Avazu. 0.790 MvFS 0.789 wlo Ism wlo gate 90.788 0.787 0.786 MLP DCN DeepFM IPNN Ablation study. We provide an ablation study of two key components of MvFS in Figure 4. We compare the following ablations: (1) w/o Ism ablates the importance score modeling (Sec.2.2.2). It uses 𝐼 (Eq.5) for the feature selection. (2) w/o gate ablates the gating module which controls the influence of each sub-network. It uses a uniform distribution for 𝑟 𝑘 . We observe that each component brings significant performance improvements across the different RS models. These results support the superiority of our strategy that controls the scale of importance scores without incurring dependency among the selected features ( w/o Ism ), and our strategy that controls the influences of sub-networks based on the feature pattern of each data ( w/o gate ). Figure 5: Effects of the number of sub-network. (Left) AUC, (Right) Logloss results of MLP on Avazu. 7875 0.3780 7870 0.3775 7865 90.3770 0.7860 0.3765 7855 0.3760 0.7850 0.7845 0.3755 Hyperparameter Study. In Figure 5, we report the performance of MvFS with varying 𝐾 , the number of sub-networks in the controller. Compared to the case of a single sub-network ( 𝐾 = 1), the multiview network ( 𝐾 ≥ 2) brings significant performance improvement. Also, the performance tends to converge as 𝐾 increases. The best performance is achieved near 6 -8.",
  "4 CONCLUSION": "We propose MvFS, a multi-view feature selection that selects informative features for each data instance effectively. MvFS uses adaptive feature selection with the multi-view network which computes feature importance by incorporating various views from multiple sub-networks, enabling a more balanced feature selection. Moreover, MvFS adopts a new importance score modeling strategy that computes importance scores without incurring unnecessary dependencies among the selected features. We conduct extensive experiments to validate the effectiveness of MvFS on real-world datasets. Also, we conduct an in-depth study to ascertain the effectiveness of our multi-view approach. MvFS: Multi-view Feature Selection for Recommender System CIKM '23, October 21-25, 2023, Birmingham, United Kingdom",
  "REFERENCES": "[1] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems . 7-10. [2] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: A Factorization-Machine Based Neural Network for CTR Prediction. In Proceedings of the 26th International Joint Conference on Artificial Intelligence . 1725-1731. [3] Mark A Hall. 1999. Correlation-based feature selection for machine learning . Ph. D. Dissertation. The University of Waikato. [4] Arthur E Hoerl and Robert W Kennard. 1970. Ridge regression: Biased estimation for nonorthogonal problems. Technometrics 12, 1 (1970), 55-67. [5] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. 1991. Adaptive mixtures of local experts. Neural computation 3, 1 (1991), 79-87. [6] SeongKu Kang, Junyoung Hwang, Wonbin Kweon, and Hwanjo Yu. 2020. DERRD: A Knowledge Distillation Framework for Recommender System. In CIKM . [7] SeongKu Kang, Junyoung Hwang, Wonbin Kweon, and Hwanjo Yu. 2021. Topology Distillation for Recommender System. In KDD . [8] SeongKu Kang, Junyoung Hwang, Dongha Lee, and Hwanjo Yu. 2019. Semisupervised learning for cross-domain recommendation to cold-start users. In CIKM . [9] SeongKu Kang, Wonbin Kweon, Dongha Lee, Jianxun Lian, Xing Xie, and Hwanjo Yu. 2023. Distillation from Heterogeneous Models for Top-K Recommendation. In WWW . [10] SeongKu Kang, Dongha Lee, Wonbin Kweon, Junyoung Hwang, and Hwanjo Yu. 2022. Consensus Learning from Heterogeneous Objectives for One-Class Collaborative Filtering. In WWW . [11] Weilin Lin, Xiangyu Zhao, Yejing Wang, Tong Xu, and Xian Wu. 2022. AdaFS: Adaptive Feature Selection in Deep Recommender System. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 3309-3317. [12] Bin Liu, Chenxu Zhu, Guilin Li, Weinan Zhang, Jincai Lai, Ruiming Tang, Xiuqiang He, Zhenguo Li, and Yong Yu. 2020. Autofis: Automatic feature interaction selection in factorization models for click-through rate prediction. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 2636-2645. [13] Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2018. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055 (2018). [14] Qiang Liu, Zhaocheng Liu, Haoli Zhang, Yuntian Chen, and Jun Zhu. 2021. Mining cross features for financial credit risk assessment. In Proceedings of the 30th ACM international conference on information & knowledge management . 1069-1078. [15] Renqian Luo, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu. 2018. Neural architecture optimization. Advances in neural information processing systems 31",
  "(2018).": "[16] Fuyuan Lyu, Xing Tang, Huifeng Guo, Ruiming Tang, Xiuqiang He, Rui Zhang, and Xue Liu. 2022. Memorize, Factorize, or be Naive: Learning Optimal Feature Interaction Methods for CTR Prediction. In 2022 IEEE 38th International Conference on Data Engineering (ICDE) . IEEE, 1450-1462. [17] Fuyuan Lyu, Xing Tang, Dugang Liu, Liang Chen, Xiuqiang He, and Xue Liu. 2023. Optimizing feature set for click-through rate prediction. In Proceedings of the ACM Web Conference 2023 . 3386-3395. [18] Fuyuan Lyu, Xing Tang, Hong Zhu, Huifeng Guo, Yingxue Zhang, Ruiming Tang, and Xue Liu. 2022. OptEmbed: Learning Optimal Embedding Table for Clickthrough Rate Prediction. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management . 1399-1409. [19] Alexey Natekin and Alois Knoll. 2013. Gradient boosting machines, a tutorial. Frontiers in neurorobotics 7 (2013), 21. [20] Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, and Jun Wang. 2016. Product-Based Neural Networks for User Response Prediction. In 2016 IEEE 16th International Conference on Data Mining (ICDM) . 1149-1154. [21] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538 (2017). [22] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, and Jian Tang. 2019. Autoint: Automatic feature interaction learning via selfattentive neural networks. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management . 1161-1170. [23] Robert Tibshirani. 1996. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series B (Methodological) 58, 1 (1996), 267-288. [24] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network for ad click predictions. In Proceedings of the ADKDD'17 . 1-7. [25] Yejing Wang, Xiangyu Zhao, Tong Xu, and Xian Wu. 2022. Autofield: Automating feature selection in deep recommender systems. In Proceedings of the ACM Web Conference 2022 . 1977-1986. [26] Zhikun Wei, Xin Wang, and Wenwu Zhu. 2021. Autoias: Automatic integrated architecture searcher for click-trough rate prediction. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management . 21012110. [27] Svante Wold, Kim Esbensen, and Paul Geladi. 1987. Principal component analysis. Chemometrics and intelligent laboratory systems 2, 1-3 (1987), 37-52. [28] Yiming Yang and Jan O Pedersen. 1997. A comparative study on feature selection in text categorization. In Icml , Vol. 97. Citeseer, 35. [29] Weinan Zhang, Tianming Du, and Jun Wang. 2016. Deep Learning over Multifield Categorical Data: -A Case Study on User Response Prediction. In Advances in Information Retrieval: 38th European Conference on IR Research, ECIR 2016, Padua, Italy, March 20-23, 2016. Proceedings 38 . Springer, 45-57.",
  "keywords_parsed": [
    "Recommender System",
    "Feature Selection",
    "CTR Prediction",
    ""
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "Wide & deep learning for recommender systems"
    },
    {
      "ref_id": "b2",
      "title": "DeepFM: A Factorization-Machine Based Neural Network for CTR Prediction"
    },
    {
      "ref_id": "b3",
      "title": "Correlation-based feature selection for machine learning"
    },
    {
      "ref_id": "b4",
      "title": "Ridge regression: Biased estimation for nonorthogonal problems"
    },
    {
      "ref_id": "b5",
      "title": "Adaptive mixtures of local experts"
    },
    {
      "ref_id": "b6",
      "title": "DERRD: A Knowledge Distillation Framework for Recommender System"
    },
    {
      "ref_id": "b7",
      "title": "Topology Distillation for Recommender System"
    },
    {
      "ref_id": "b8",
      "title": "Semisupervised learning for cross-domain recommendation to cold-start users"
    },
    {
      "ref_id": "b9",
      "title": "Distillation from Heterogeneous Models for Top-K Recommendation"
    },
    {
      "ref_id": "b10",
      "title": "Consensus Learning from Heterogeneous Objectives for One-Class Collaborative Filtering"
    },
    {
      "ref_id": "b11",
      "title": "AdaFS: Adaptive Feature Selection in Deep Recommender System"
    },
    {
      "ref_id": "b12",
      "title": "Autofis: Automatic feature interaction selection in factorization models for click-through rate prediction"
    },
    {
      "ref_id": "b13",
      "title": "Darts: Differentiable architecture search"
    },
    {
      "ref_id": "b14",
      "title": "Mining cross features for financial credit risk assessment"
    },
    {
      "ref_id": "b15",
      "title": "Neural architecture optimization"
    }
  ]
}
