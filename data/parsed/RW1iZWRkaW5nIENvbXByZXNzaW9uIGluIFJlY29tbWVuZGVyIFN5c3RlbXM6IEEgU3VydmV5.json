{"Embedding Compression in Recommender Systems: A Survey": "SHIWEI LI \u2217 , Huazhong University of Science and Technology, China HUIFENG GUO \u2217 , Huawei Noah's Ark Lab, China XING TANG \u2020 , Tencent, China RUIMING TANG and LU HOU, Huawei Noah's Ark Lab, China RUIXUAN LI \u2021 , Huazhong University of Science and Technology, China RUI ZHANG \u2021 , ruizhang.info, China To alleviate the problem of information explosion, recommender systems are widely deployed to provide personalized information filtering services. Usually, embedding tables are employed in recommender systems to transform high-dimensional sparse one-hot vectors into dense real-valued embeddings. However, the embedding tables are huge and account for most of the parameters in industrial-scale recommender systems. In order to reduce memory costs and improve efficiency, various approaches are proposed to compress the embedding tables. In this survey, we provide a comprehensive review of embedding compression approaches in recommender systems. We first introduce deep learning recommendation models and the basic concept of embedding compression in recommender systems. Subsequently, we systematically organize existing approaches into three categories, namely low-precision, mixed-dimension, and weight-sharing, respectively. Lastly, we summarize the survey with some general suggestions and provide future prospects for this field.", "CCS Concepts: \u00b7 Information systems \u2192 Recommender systems .": "Additional Key Words and Phrases: recommender systems; embedding tables; model compression; survey", "ACMReference Format:": "Shiwei Li, Huifeng Guo, Xing Tang, Ruiming Tang, Lu Hou, Ruixuan Li, and Rui Zhang. 2024. Embedding Compression in Recommender Systems: A Survey. ACM Comput. Surv. 56, 5, Article 130 (January 2024), 21 pages. https://doi.org/10.1145/3637841", "1 INTRODUCTION": "To alleviate the problem of information explosion, recommender systems [55, 61] are extensively deployed to provide personalized information filtering services, including online shopping [80], advertising systems [46] and so on. Meanwhile, deep learning techniques have shown impressive capabilities in capturing user preferences for candidate items. Thereupon, both the industry and research communities have proposed a variety of deep learning recommendation models (DLRMs) Authors' addresses: Shiwei Li, lishiwei@hust.edu.cn, Huazhong University of Science and Technology, Wuhan, China, 430074; Huifeng Guo, huifeng.guo@huawei.com, Huawei Noah's Ark Lab, Shenzhen, China, 518129; Xing Tang, xing.tang@ hotmail.com, Tencent, Shenzhen, China, 518054; Ruiming Tang, tangruiming@huawei.com; Lu Hou, houlu3@huawei.com, Huawei Noah's Ark Lab, Shenzhen, China, 518129; Ruixuan Li, rxli@hust.edu.cn, Huazhong University of Science and Technology, Wuhan, China, 430074; Rui Zhang, rayteam@yeah.net, ruizhang.info, China. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. 0360-0300/2024/1-ART130 $15.00 https://doi.org/10.1145/3637841 to enhance the performance of recommender systems, such as Wide & Deep [6] in Google Play, DIN [80] in Alibaba and DeepFM [18] in Huawei.", "1.1 Deep Learning Recommendation Models": "Recommender systems are utilized for a diverse range of tasks, such as candidate item matching [30], click-through rate (CTR) prediction [51, 52], and conversion rate (CVR) prediction [44]. For each of these tasks, the employed DLRMs have undergone meticulous design processes to ensure optimal performance. However, without loss of generality, most DLRMs follow the embedding table and neural network paradigm [18, 45, 57, 58], despite the specific design of the neural network component may vary across different model architectures. Neural Prediction Layer Network Feature Deep Interaction Neural Layer Network Embedding Table As illustrated in Figure 1, the embedding table is responsible for converting input rows into dense embedding vectors. It is worth noting that the input rows of DLRMs typically consist of categorical features, which are encoded as high-dimensional one-hot vectors. Each category feature will be referred to as feature for short, and all features under the same category form a feature field. Generally, each feature is associated with a unique embedding stored in the embedding table E \u2208 R \ud835\udc5b \u00d7 \ud835\udc51 , where \ud835\udc5b denotes the total number of features and \ud835\udc51 denotes the embedding dimension. On the other hand, the neural network is primarily engaged in interacting, processing, and analyzing feature embeddings, along with making predictions. Recent studies [18, 29, 36, 40, 65, 70] have consistently focused on optimizing the feature extraction capabilities of the neural networks. For example, [18, 58] utilize product operators to model the feature interactions between different feature fields. [36, 40] employ convolutions on embeddings to capture feature interactions of arbitrary order. [65] introduces an additional attention network to assign varying importance to different feature interactions. Additionally, [29, 70] automatically search for suitable interaction functions using AutoML techniques [21]. In this manuscript, we do not delve into the detailed design of the neural network component. Instead, we recommend referring to the works [61, 74, 75] for a comprehensive understanding of the neural networks used in DLRMs. Despite incorporating various intricate designs, the neural network usually entails relatively shallow layers and a limited number of model parameters. In contrast, the embedding table occupies the vast majority of model parameters. Especially in industrial-scale recommender systems, where there are billions or even trillions of categorical features, the embedding table may take hundreds of GBor even TB to hold [17]. For example, the size of embedding tables in Baidu's advertising systems reaches 10 TB [66]. As the scale of recommender systems perpetually expands, the continuous growth in the number of features will bring greater storage overhead.", "1.2 Embedding Compression in Recommender Systems": "In addition to increasing storage overhead, larger embedding tables will also result in higher latency during table lookup 1 , which will reduce the efficiency of model training and inference. Therefore, to deploy the DLRMs with large embedding tables in real production environment efficiently and economically, it is necessary to compress their embedding tables. However, embedding compression in DLRMs differs significantly from model compression in other fields, such as Computer Vision (CV) [8] and Natural Language Processing (NLP) [19]. These differences primarily manifest in three aspects: model architectures, properties of input data, and model size. Firstly, vision models and language models are usually very deep neural networks stacked by fully-connected layers, convolutional layers, or transformers. Consequently, compression methods designed for these models focus on compressing the aforementioned modules rather than embedding tables. In contrast, DLRMs are typically shallow models, with the majority of parameters concentrated in the embedding tables. Secondly, the input data of vision models and language models are usually images and texts, inherently containing abundant visual and semantic information that can be leveraged for model compression. For example, [4] use the semantic information as a prior knowledge to compress the word embeddings, while [20] exploit the similarity between feature maps derived from image inputs to compress convolution kernels. However, in recommender systems, there is generally limited visual or semantic information available. Fortunately, DLRMs possess unique properties in the input data that can facilitate embedding compression. Specifically, categorical features are organized in feature fields and often follow a highly skewed long-tail distribution, with varying numbers of features in different fields. We can compress embedding tables based on feature frequency and field size. Thirdly, embedding tables of DLRMs are usually hundreds or even thousands of times larger than vision models or language models [53], which presents a more challenging and necessary task for compression. Recently, embedding compression has gained increasing attention in recommender systems, leading to the development and application of various embedding compression techniques for DLRMs. However, there is currently no comprehensive survey summarizing the methods employed for embedding compression. Therefore, the primary objective of this paper is to review and summarize representative research in this field. The embedding table can be regarded as a matrix with three dimensions, that are the precision of weights, the dimension of embeddings, and the number of embeddings. To this end, we summary the embedding compression methods into three categories according to the dimensions they compress, as illustrated in Figure 2. Firstly, low-precision methods reduce the memory of each weight by decreasing its bit width. According to the size of bit width and its corresponding advantages, we further divide the low-precision methods into binarization and quantization. Secondly, mixed-dimension methods reduce the memory of specific embeddings by decreasing their dimensions and using mixed-dimension embeddings. According to the techniques of determining the embedding dimension for different features, we categorize the mixed-dimension methods into rule-based approaches, NAS-based approaches, and pruning. Thirdly, weight-sharing methods reduce the actual parameters of the embedding table by sharing weights among different embeddings. Considering that the number of features is given by the dataset, a solution to reduce the number of embeddings is to reuse embeddings among features. Furthermore, we generalize the sharing to the weight level and define the weight-sharing methods Embedding Compression Low-precision Mixed-dimension Weight-sharing Binarization: [42], [77], DCF [73], DCMF [33], DPR [76], DFM [37] CIGAR [28], HashGNN [54], L 2 Q-GCN [5] Quantization: [16], [69], [66], ALPT [31] Rule-based Approaches: MDE [15], CpRec [53] NAS-based Approaches: NIS [25], ESAPN [39], AutoIAS [59], AutoEmb [78], AutoDim [79], RULE [3], OptEmbed [43] Pruning: DeepLight [10], DNIS [7], AMTL [68], PEP [41], SSEDS [48] Hashing: QR [50], MEmCom [47], BCH [67], FDH [72], LMA [12], ROBE [11] Vector Quantization: Saec [62], MGQE [26], xLightFM [24], LightRec [34], LISA [63] Decomposition: MLET [14], ANT [35], DHE [27], TT-Rec[71], LLRec [56], [64] as generating embeddings with shared weights. According to the way embeddings are generated, we categorize the mixed-dimension methods into hashing, vector quantization, and decomposition. We will introduce the three primary categories in Sections 2, 3 and 4, respectively. Note that embeddings are fed into the neural network as representations of categorical features and form the foundations of DLRMs. Therefore, when compressing embeddings, it may affect the model performance on many aspects, including model accuracy, inference efficiency, training efficiency, and training memory usage. We will discuss the pros and cons of different methods regarding these metrics at the end of each section. In Section 5, the survey is summarized, providing general suggestions for different scenarios and discussing future prospects for this field.", "2 LOW-PRECISION": "As we all know, embedding weights are typically stored in the format of FP32 2 which occupies 32 bits. To reduce the storage of each weight, low-precision approaches are developed to represent a weight with fewer bits. In particular, according to the bit width of weights, low-precision approaches can be further divided into binarization and quantization .", "2.1 Binarization": "Binarization is to compress a full-precision weight into a binary code that only occupy 1 bit. It is widely used in the embedding-based similarity search of recommender systems [28, 54], since the binary embeddings have two distinct advantages compared to the full-precision ones: (1) less memory or disk cost for storing embeddings; (2) higher inference efficiency as the similarity (i.e., inner product) between binary embeddings can be calculated more efficiently through the Hamming distance, which has been proved in [73]. [42, 77] pioneered to obtain binary embeddings in a two-stage (i.e, post-training) manner. Specifically, they first learn a full-precision embedding table while ignoring the binary constraints, and then perform binarization (e.g., \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5b ( \ud835\udc65 ) ) on the full-precision embeddings to get binary embeddings. However, the binarization procedure is not in the training process and thus cannot be optimized by minimizing the training objective, which will bring large irreparable errors and fail to meet an acceptable accuracy. To reduce accuracy degradation, subsequent works have focused on end-to-end approaches to learn the binary embeddings during training. As shown in Figure 3, recent works typically learn binary embeddings following two optimization paradigms, namely direct optimization and indirect optimization. As Figure 3(a) shows, in the direct optimization of binarization, the binary embeddings are maintained as part of the model parameters and will be optimized directly by the training loss. For example, to improve the efficiency of Collaborative Filtering (CF), DCF [73] learns a binary embedding table B \u2208 {\u00b1 1 } \ud835\udc5b \u00d7 \ud835\udc51 . To maximize the information encoded in each binary embedding, DCF further adds a balance-uncorrelation constraint to B (i.e., B \ud835\udc47 1 = 0 , B \ud835\udc47 B = \ud835\udc5b I ), where I is an identity matrix. However, it is NP-hard to optimize the binary embeddings with such constraint. To resolve this problem, DCF also maintains a full-precision embedding table E \u2208 R \ud835\udc5b \u00d7 \ud835\udc51 with the same balance-uncorrelation constraint. The constraint of B is then replaced by adding the mean-square-error (MSE) of ( B -E ) to the objective function. During training, DCF will update B and E alternatively through different optimization algorithms. Specifically, B is updated by Discrete Coordinate Descent (DCD) and E is updated with the aid of Singular Value Decomposition (SVD). This optimization paradigm has been widely used to learn binary embeddings in recommender systems, such as DPR [76], DCMF [33], DFM [37]. DPR changes the objective function of DCF (i.e., rating prediction) to personalized items ranking. DCMF and DFM extends this binarization paradigm to Content-aware Collaborative Filtering [32] and Factorization Machine (FM) [49], respectively. Loss: W(B) +MSE(B-E) backward forwvard propagation propagation loss constraint (a) Direct optimization of binarization Loss: W(B) backward forwvard propagation propagation B ) binarization (b) Indirect optimization of binarization As shown in Figure 3(b), another paradigm is the indirect optimization, where the binary embeddings B are generated from full-precision embeddings E on the fly and will be optimized indirectly by optimizing E . However, it is infeasible to optimize E by the standard gradient descent as the gradients of the binary operations (e.g., \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5b ( \ud835\udc65 ) ) are constantly zero. To solve this problem, CIGAR [28] replaces \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5b ( \ud835\udc65 ) with the scaled tanh function \ud835\udc61\ud835\udc4e\ud835\udc5b\u210e ( \ud835\udefc\ud835\udc65 ) as lim \ud835\udefc \u2192\u221e \ud835\udc61\ud835\udc4e\ud835\udc5b\u210e ( \ud835\udefc\ud835\udc65 ) = \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5b ( \ud835\udc65 ) and \ud835\udc61\ud835\udc4e\ud835\udc5b\u210e ( \ud835\udefc\ud835\udc65 ) has better differential property. In the early stages of training, a smaller value of \ud835\udefc is utilized to yield superior representations, and as the training progresses, its value gradually increases to approximate \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5b () . Another way to solve the non-propagable gradient is straightthrough-estimator (STE) [9], which treats some operations as identity maps during backpropagation. HashGNN [54] employs the STE variant of \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5b () and thus updating E with the gradients of B . However, the huge gap between B and E will cause an imprecise update for E . To solve this issue, HashGNN further develops a dropout-based binarization. Specifically, \u02c6 E = ( 1 -P ) \u2299 E + P \u2299 B will be fed into the following networks, where P \u2208 { 0 , 1 } \ud835\udc5b \u00d7 \ud835\udc51 and \u2299 is the element-wise product. Each element in P is a Bernoulli random value with probability \ud835\udc5d . During backpropagation, only the embeddings that are binarized will be updated through STE and the rest will be updated by standard gradient descent. To ensure convergence, HashGNN adopts a small value for \ud835\udc5d in the initial training phase, gradually increasing it as the training progresses. Similarly, L 2 Q-GCN [5] uses STE to optimize the full-precision embeddings, while introducing a positive scaling factor \ud835\udc60 = \ud835\udc5a\ud835\udc52\ud835\udc4e\ud835\udc5b (| \ud835\udc86 |) for each binary embedding to enhance its presentation capability, where \ud835\udc86 is the full-precision embedding. The comparison of the above three methods is summarized in Algorithm 1.", "Algorithm 1: Comparison between CIGAR [28], HashGNN [54] and L 2 Q-GCN [5].": "", "2.2 Quantization": "Although binarization has better efficiency and less memory cost at the inference stage, it may lead to a significant drop of accuracy, which is not acceptable in several scenarios. As Cheng et al. [6] claim, even 0 . 1% decrease of the prediction accuracy may result in large decline in revenue. To trade off the memory cost and the prediction accuracy, quantization is used to represent each weight with a multi-bit integer. Quantization is the mapping of a 32-bit full-precision weight to an element in the set of quantized values S = { \ud835\udc5e 0 , \ud835\udc5e 1 , ..., \ud835\udc5e \ud835\udc58 } , where \ud835\udc58 = 2 \ud835\udc60 -1 and \ud835\udc60 is the bit width. The most commonly used quantization function is uniform quantization, where the quantized values are uniformly distributed. Specifically, the step size \u0394 = \ud835\udc5e \ud835\udc56 -\ud835\udc5e \ud835\udc56 -1 remains the same for any \ud835\udc56 \u2208 [ 1 , \ud835\udc58 ] . Let \ud835\udc64 be a value clipped into the range [ \ud835\udc5e 0 , \ud835\udc5e \ud835\udc58 ] , we can quantize it into an integer as \u02c6 \ud835\udc64 = \ud835\udc5f\ud835\udc51 (( \ud835\udc64 -\ud835\udc5e 0 )/\u25b3) , where \ud835\udc5f\ud835\udc51 ( \ud835\udc65 ) rounds \ud835\udc65 to an adjacent integer. The integer \u02c6 \ud835\udc64 will be de-quantized into a floating-point value ( \u02c6 \ud835\udc64 \u00d7 \u25b3 + \ud835\udc5e 0 ) when used. Existing work on embedding quantization either performs post-training quantization or trains a quantized embedding table from scratch. Guan et al. [16] studies post-training quantization (PTQ) on the embedding tables and proposes a uniform and a non-uniform quantization algorithm. Specifically, in the uniform quantization, they maintain a quantization range for each embedding and find the best quantization range by a greedy search algorithm. In the non-uniform quantization, they divide similar embeddings into groups and apply k-means clustering on the weights to produce a codebook for each group. The weights in each group will be mapped to the index of a value in the corresponding codebook. These two algorithms improve the accuracy of PTQ, however, they still suffer from accuracy degradation. Loss: W(B) backward forward propagation propagation quantization (a) Quantization-aware training Loss: W(E) backward forward propagation propagation quantization de-quantization (b) Low-precision training To further reduce accuracy degradation, recent works [66, 69] learn quantized weights from scratch. Unlike the well-known quantization-aware training (QAT) [2, 13], [66, 69] use another quantization training framework to exploit the sparsity of the input data, which we term lowprecision training (LPT). As Figure 4(a) shows, QAT quantizes the full-precision weights in the forward pass and updates the full-precision weights with the gradients estimated by STE. As Figure 4(b) shows, in LPT, the weights are stored in the format of integers at training, thereby compressing the training memory. The model takes the de-quantized weights as input and will quantize the weights back into integers after the backward propagation. Since the input one-hot vectors of DLRMs are highly sparse, only extremely small part of the embeddings will be dequantized into floating-point values, whose memory is negligible. Xu et al. [66] uses 16-bit LPT on the embedding table without sacrificing accuracy. To enhance the compression capability of LPT, Yang et al. [69] proposes a mixed-precision scheme where most embeddings are stored in the format of integers, only the most recently or frequently used embeddings are stored in a full-precision cache. With a small cache, they achieve lossless compression with 8-bit or even 4-bit quantization. Li et al. [31] proposes an adaptive low-precision training scheme to learn the quantization step size for better model accuracy.", "2.3 Discussion": "Low-precision is a simple yet effective way for embedding compression. At the inference stage, binarization can reduce the memory usage by 32 \u00d7 and accelerate the inference through Hamming distance. However, the binary embeddings usually cause severe accuracy degradation and need to be trained with the guidance of full-precision embeddings, which requires more memory usage and computing resources at training. In contrast, quantization has a limited compression capability but can achieve a comparable accuracy as the full-precision embeddings. Besides, recent quantization approaches for embedding tables can also compress the memory usage at the training stage and improve the training efficiency by reducing the communication traffic.", "3 MIXED-DIMENSION": "Embedding tables usually assign a uniform dimension to all the embeddings in a heuristic way, and it turns out to be suboptimal in both prediction accuracy and memory usage [25]. As confirmed in [78], a low dimensional embedding is good at handling less frequent features where a high dimensional embedding cannot be well trained. Therefore, to boost the model performance, it is important to assign a appropriate dimension to each feature and use mixed-dimension embeddings. Existing methods can obtain mixed-dimension embeddings in a structured or an unstructured manner. As shown in Figure 5, structured approaches divide the embeddings into groups each of (a) Structured embeddings (b) Unstructured embeddings which has a unique dimension, while unstructured approaches learn a sparse embedding table where the embeddings have various dimensions. However these mixed-dimension embeddings are not friendly for the operations (e.g., inner product) which require the embeddings of the same length. Therefore, the mixed-dimension embeddings need to be transformed into a uniform dimension before feeding into the following networks. Such transformation is usually achieved by linear projection or simply zero padding. Apart from the difference of the embedding structures, existing methods also differ greatly in the way of generating mixed-dimension embeddings. In this section, we will introduce three kinds of mixed-dimension approaches named rule-based approaches, NAS-based approaches and pruning , respectively.", "3.1 Rule-based Approaches": "It is a common understanding that the features with higher frequencies are more informative and the fields with more features occupy more memory. Thus, the embedding dimension can be set with a heuristic rule based on the feature frequency and the field size. To deploy the item embeddings into resource-constraint devices, CpRec [53] divides the items into several groups by frequency and assigns a predefined dimension to each group according to the frequencies of owned features. Similarly, MDE [15] assigns each feature field with a unique dimension according to the number of features included in this field. Specifically, let \ud835\udc8f \u2208 R \ud835\udc5a denotes the number of features in all \ud835\udc5a feature fields and \ud835\udc91 = 1 / \ud835\udc8f , then the embedding dimension of \ud835\udc56 -th field would be \u00af \ud835\udc51 \ud835\udc91 \ud835\udc8a \ud835\udefc /|| \ud835\udc91 | | \ud835\udefc \u221e , where \u00af \ud835\udc51 is the base dimension and \ud835\udefc \u2208 [ 0 , 1 ] denotes the temperature. These rule-based approaches are simple yet effective in reducing the memory usage and alleviating the overfitting problems, however, they suffer from suboptimal performance as the heuristic rules can not be optimized by the ultimate goal of minimizing the training objective.", "3.2 NAS-based Approaches": "NAS was originally proposed to search for the optimal neural network architectures [82]. Recently, it has also been adopted in searching embedding dimensions for different features. Unlike the rule-based approaches where the dimension is set based on a priori, it is now learned. Generally, there are three components in the NAS-based approaches: (1) search space: relaxing the large optimization space of embedding dimensions with heuristic assumptions; (2) controller: usually a neural network or learnable parameters, selecting candidate dimension from the search space in a hard or soft manner; (3) updating algorithm: updating the controller with reinforcement learning (RL) algorithms or differential architecture search (DARTS) [38] techniques and so on. We first introduce the approaches of NIS [25], ESAPN [39] and AutoIAS [59], which adopt a policy network as the controller and update the controller with RL algorithms. NIS [25] and ESAPN [39] are designed to learn embedding dimensions of users and items. In NIS, the authors relax the original search space \ud835\udc5b \u00d7 \ud835\udc51 to \ud835\udc3a \u00d7 \ud835\udc35 ( \ud835\udc3a < \ud835\udc5b and \ud835\udc35 < \ud835\udc51 ) by dividing the features into \ud835\udc3a groups and cutting the embedding dimension of each group into \ud835\udc35 blocks. Then, they use the controller to select several blocks and generate the final embeddings. In ESAPN, the authors predefine the search space as a set of candidate embedding dimensions D = { \ud835\udc51 1 , \ud835\udc51 2 , ..., \ud835\udc51 \ud835\udc58 } , where \ud835\udc51 1 < \ud835\udc51 2 < ... < \ud835\udc51 \ud835\udc58 . Inspired by the fact that the frequencies of features are monotonically increasing in the data stream, they decide to increase or keep the current embedding dimension instead of selecting one in D . The decision is made by the controller based on the feature frequency and the current embedding dimension. Different from NIS and ESAPN, AutoIAS searches not only the embedding dimensions of feature fields but also the following neural network architectures. The authors design a search space for each model component (e.g., the search space of embedding dimensions is similar as D in ESAPN.). To boost the training efficiency, they maintain a supernet at training and use the controller to generate sub-architectures by inheriting parameters from the supernet. The RL-based approaches described above perform a hard selection by selecting only one embedding dimension for each feature or field at a time. Instead, inspired by DARTS, AutoEmb [78] and AutoDim [79] make a soft selection by weighted summing over the embeddings of the candidate dimensions in D = { \ud835\udc51 1 , \ud835\udc51 2 , ..., \ud835\udc51 \ud835\udc58 } . Let \ud835\udf4e \u2208 [ 0 , 1 ] \ud835\udc58 denote the vector composed of the weighting coefficients. AutoEmb searches for the embedding dimensions of individual features, while AutoDim searches for the embedding dimensions of the feature fields. In AutoEmb, the controller is a neural network and generates \ud835\udf4e based on the feature frequency. While AutoDim directly assigns each field with a learnable vector \ud835\udf4e , and it further approximates the hard selection by performing gumbel-softmax [22] on \ud835\udf4e . At training, the controller in AutoEmb and the learnable vectors in AutoDim are optimized through DARTS techniques. After training, the corresponding dimension of the largest weight in \ud835\udf4e is selected and the model will be retrained for a better accuracy. Considering that the training process of the controller is quite time-consuming, recent works [3, 43] search for the optimal embedding dimensions after training the models, without using any controller. They first sample some structures from the search space and then explore the entire search space by using evolutionary search strategies on the sampled structures. Specifically, RULE [3] cuts the embedding table into \ud835\udc3a \u00d7 \ud835\udc35 blocks similar as NIS and adds a diversity regularization to the blocks in the same group for maximizing expressiveness. After training, RULE selects the most suitable embedding blocks under a memory budget (i.e., the maximum number of blocks). OptEmbed [43] trains a supernet while removing non-informative embeddings. After training, it then assigns each field with a binary mask \ud835\udc8e \u2208 { 0 , 1 } \ud835\udc51 to obtain mixed-dimension embeddings, where \ud835\udc51 is the original dimension. The block selections in RULE and the masks in OptEmbed are determined and evolved by the search strategies.", "3.3 Pruning": "Instead of shortening the length of embeddings, pruning can obtain a sparse embedding table and thus get mixed-dimension embeddings. For instance, DeepLight [10] prunes the embedding table in a certain proportion. During training, it prunes and retrains the embedding table alternatively so that the mistakenly pruned weights can grow back. In addition, DeepLight will increase the pruning proportion gradually as training proceeds. Another way to prune the embeddings is to train the embeddings with learnable masks. Specifically, an embedding \ud835\udc86 is pruned as \u02c6 \ud835\udc86 = \ud835\udc8e \u2299 \ud835\udc86 for the forward pass, where \ud835\udc8e is the mask and \u2299 is the element-wise product. DNIS [7] divides features into groups by frequency and assigns each group with a learnable mask \ud835\udc8e \u2208 [ 0 , 1 ] \ud835\udc51 . AMTL [68] develops a network to generate a binary mask \ud835\udc8e \u2208 { 0 , 1 } \ud835\udc51 for each feature based on its frequency. Similarly, PEP [41] generates a binary mask \ud835\udc8e \u2208 { 0 , 1 } \ud835\udc51 for each feature as \ud835\udc8e = I (| \ud835\udc86 | > \ud835\udc54 ( \ud835\udc60 )) , where I (\u00b7) is the indicator function and \ud835\udc54 ( \ud835\udc60 ) (e.g., \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5a\ud835\udc5c\ud835\udc56\ud835\udc51 ( \ud835\udc60 ) ) serves as a learnable threshold. Specially, in PEP, the embeddings should minus \ud835\udc54 ( \ud835\udc60 ) before being pruned by the masks (i.e. \u02c6 \ud835\udc86 = \ud835\udc8e \u2299 ( \ud835\udc86 -\ud835\udc54 ( \ud835\udc60 )) ). At training, the network in AMTL and the learnable threshold in PEP are optimized together with the model parameters by gradient descent, while the learnable masks in DNIS are optimized by DARTS. After training, AMTL and PEP preserve \u02c6 \ud835\udc86 as the final embeddings, while DNIS need pruning \u02c6 \ud835\udc86 with a threshold as the redundant weights in \u02c6 \ud835\udc86 are not exact zero. The differences between the above methods in generating masks are summarized in Algorithm 2.", "Algorithm 2: Comparison between DNIS [7], AMTL [68] and PEP [41].": "To get rid of the extra training process of optimizing the masks, SSEDS [48] develop a single-shot pruning algorithm to prune on the pretrained models. For a pretrained model, SSEDS will prune the columns of the embedding matrix for each field and produce structured embeddings. After training, SSEDS assigns each feature field with a mask and form a mask matrix M = { 1 } \u02c6 \ud835\udc5b \u00d7 \ud835\udc51 , where \u02c6 \ud835\udc5b is the number of fields and \ud835\udc51 is the original embedding dimension. Instead of learning M in the training process, SSEDS use \ud835\udc54 \ud835\udc56 \ud835\udc57 = \ud835\udf15\ud835\udc53 ( M , E )/ \ud835\udf15 M \ud835\udc56 \ud835\udc57 to identify the importance of \ud835\udc57 -th dimension in \ud835\udc56 -th field, where \ud835\udc54 \ud835\udc56 \ud835\udc57 is the gradient of the loss function with respect to M \ud835\udc56 \ud835\udc57 . Specifically, a larger magnitude of | \ud835\udc54 \ud835\udc56 \ud835\udc57 | means that the corresponding dimension has a greater impact on the loss function. Note that all | \ud835\udc54 \ud835\udc56 \ud835\udc57 | can be computed efficiently via only one forward-backward pass. Given a memory budget, SSEDS calculates a saliency score for each dimension as \ud835\udc60 \ud835\udc56 \ud835\udc57 = | \ud835\udc54 \ud835\udc56 \ud835\udc57 |/ \u02dd \u02c6 \ud835\udc5b \ud835\udc56 = 0 \u02dd \ud835\udc51 \ud835\udc57 = 0 | \ud835\udc54 \ud835\udc56 \ud835\udc57 | and prunes the dimensions with the lowest saliency scores.", "3.4 Discussion": "Mixed-dimension approaches can alleviate the overfitting problems and obtain better accuracy, but usually have worse efficiency at the training and the inference stage. At inference, the structured approaches usually suffer from extra computing cost due to the linear transformation and the unstructured approaches store the sparse embedding table using sparse matrix storage , which will cost extra effort to access. At training, NAS-based approaches require extremely long time for searching and pruning usually needs to retrain the pruned models for better accuracy which doubles the training time. In contrast, rule-based approaches have little influence on the efficiency and can save memory also at the training stage. However, they cannot achieve the optimal accuracy.", "4 WEIGHT-SHARING": "Low-precision approaches reduce the number of bits in a weight and mixed-dimension approaches reduce the number of weights in an embedding. Unlike them, weight-sharing approaches share weights among the embedding table, thereby reducing the actual number of parameters within it. Existing weight-sharing approaches usually generate embeddings with several shared vectors. In this section, we relax the definition of weight-sharing and formulate a weight-sharing paradigm based on existing approaches. Specifically, the embedding generation is formulated as \ud835\udc86 = -/ \u02dd \ud835\udc60 \ud835\udc56 = 1 \ud835\udc70 \ud835\udc56 \u00d7 T \ud835\udc56 , where -/ \u02dd denotes concatenation or summation, T \ud835\udc56 is a matrix composed of shared vectors and \ud835\udc70 \ud835\udc56 is an index vector for selecting shared vectors in T \ud835\udc56 . For ease of expression, we refer to the shared vectors as meta-embeddings and the matrices of shared vectors as metatables. According to the principle of constructing the index vectors, we introduce three kinds of weight-sharing methods named hashing , vector quantization , and decomposition , respectively.", "4.1 Hashing": "Hashing methods generate the index vectors by processing the original feature id with hash functions. For instance, the naive hashing method [60] compresses the embedding table with a simple hash function (e.g., the reminder function). Specifically, given the original size of the embedding table \ud835\udc5b \u00d7 \ud835\udc51 , each feature has an embedding \ud835\udc86 = \ud835\udc70 \u00d7 T , where T \u2208 R \ud835\udc5a \u00d7 \ud835\udc51 ( \ud835\udc5a < \ud835\udc5b ) and \ud835\udc70 = one-hot ( id % \ud835\udc5a ) \u2208 { 0 , 1 } \ud835\udc5a . Note that \ud835\udc70 \u00d7 T is actually achieved by table look-up when \ud835\udc70 is a one-hot vector. However, [60] naively maps multiple features to the same embedding. The collisions between features will result in loss of information and drop of accuracy. Algorithm 3: Comparison between QR [50], MEmCom [47], BCH [67] and FDH [72]. To reduce the collisions, existing hashing methods use multiple hash functions to process the feature id. They usually maintain multiple meta-tables ( T 1 , ..., T \ud835\udc60 ) and generate multiple index vectors as \ud835\udc70 \ud835\udc56 = one-hot ( hash \ud835\udc56 ( id )) , where \ud835\udc56 \u2208 [ 1 , \ud835\udc60 ] and { hash \ud835\udc56 } \ud835\udc60 \ud835\udc56 = 1 is a group of hash functions. For example, QR [50] maintain two meta-tables and use the quotient function and the reminder function to generate two index vectors. Similarly, MEmCom [47] also maintain two meta-tables ( T 1 \u2208 R \ud835\udc5a \u00d7 \ud835\udc51 , T 2 \u2208 R \ud835\udc5b \u00d7 1 ) and generate two index vectors as \ud835\udc70 1 = one-hot ( id % \ud835\udc5a ) , \ud835\udc70 2 = one-hot ( id ) . To better distinguish the features, MEmCom multiplies two meta-embeddings as the final embedding. Further, Yan et al. [67] use Binary Code based Hash (BCH) functions to process the feature id at bit level. It divides the 64 bits of a feature id into \ud835\udc60 groups and restructures them into \ud835\udc60 sub-ids ( id 1 , ..., id \ud835\udc60 ). Each sub-id corresponds to an index vector (i.e., \ud835\udc70 \ud835\udc56 = one-hot ( id \ud835\udc56 ) , \ud835\udc56 \u2208 [ 1 , \ud835\udc60 ] ) and obtains a meta-embedding. Additionally, to enhance the compression capability, BCH keeps one single meta-table and shares it among all T \ud835\udc56 , \ud835\udc56 \u2208 [ 1 , \ud835\udc60 ] . Although the hashing methods described above are efficient in memory reduction, they still suffer from accuracy degradation and extra computing cost of the hash functions. To alleviate these problems, Zhang et al. [72] develop a Frequency-based Double Hashing (FDH) method, which only uses hashing on the features with low frequencies. In this way, fewer features need to be processed by the hash function. With a little extra storage for the most frequent features, FDH not only improves the prediction accuracy but also the inference efficiency. The difference between the above methods in generating embeddings is reflected in Algorithm 3. Instead of generating embeddings with meta-embeddings, LMA [12] and ROBE [11] use hash functions to map each weight in the embedding table into a shared memory \ud835\udc40 . For a weight \ud835\udc64 \ud835\udc56,\ud835\udc57 in the embedding table, they take both \ud835\udc56 and \ud835\udc57 as the input of hash functions. LMA utilizes locality sensitive hashing (LSH) to map the weights of each embedding to \ud835\udc40 randomly. ROBE organizes \ud835\udc40 as a circular array and divides the flattened embedding table (i.e. concatenate all rows) into blocks of size \ud835\udc4d . The head of each block is mapped to \ud835\udc40 randomly and the following weights in the block will be mapped to the position next to the head.", "4.2 Vector Quantization": "Hashing methods typically get the index vector by processing the feature id with hash functions, which fail to capture the similarity between features themselves [26]. To capture the similarity, vector quantization (VQ) constructs the index vectors through approximated nearest neighbor search (ANNS). Specifically, for a feature with an original embedding of \ud835\udc86 , VQ gets its index vector as \ud835\udc70 = one-hot ( arg max \ud835\udc58 sim ( \ud835\udc86 , T \ud835\udc58 )) \u2208 { 0 , 1 } \ud835\udc5a , where T \ud835\udc58 is the \ud835\udc58 -th meta-embedding in the metatable T \u2208 R \ud835\udc5a \u00d7 \ud835\udc51 and sim (\u00b7) is a similarity function (e.g., Euclidean distance). In other words, VQ takes the original embedding as input and quantizes it into its most similar meta-embedding. Note that the meta-table and the meta-embedding are commonly referred to codebook and codeword in recent literature on VQ. Here we use meta-table and meta-embedding for consistency. Saec [62] generates a meta-table T by clustering the most frequent embeddings of a pretrained model and then quantizes each original embedding into a meta-embedding in T . However, assigning the same meta-embedding to different features (i.e., collisions) still results in drop of accuracy, even though the features have some similarity. In addition, Saec cannot optimize the meta-table together with the original embeddings, which also results in suboptimal accuracy. To alleviate the collisions, subsequent works adopt product quantization (PQ) [23] and additive quantization (AQ) [1] to quantize an embedding into multiple meta-embeddings. To optimize the meta-table together with the original embeddings, researchers usually quantize the original embeddings into meta-embeddings during training and use the meta-embeddings as the input of the following network, where the original embeddings will be optimized through STE [9]. PQ considers an embedding as a concatenation of several segments (i.e., \ud835\udc86 = -\ud835\udc60 \ud835\udc56 = 1 \ud835\udc86 \ud835\udc56 ). Each segment \ud835\udc86 \ud835\udc56 corresponds to a meta-table T \ud835\udc56 . At training, an embedding \ud835\udc86 is quantized as -\ud835\udc60 \ud835\udc56 = 1 \ud835\udc70 \ud835\udc56 \u00d7 T \ud835\udc56 , where \ud835\udc70 \ud835\udc56 = one-hot ( arg max \ud835\udc58 sim ( \ud835\udc86 \ud835\udc56 , T \ud835\udc56 \ud835\udc58 )) . In other words, PQ quantizes each segment into its most similar meta-embedding in the corresponding meta-table. After training, the original embeddings are discarded and only the meta-tables are preserved. Since the selection of a meta-embedding in a metatable can be compactly encoded by log \ud835\udc41 bits, where \ud835\udc41 is the size of the meta-table, an embedding can now be stored by \ud835\udc60 log \ud835\udc41 bits with the help of the meta-tables. Further, MGQE [26] takes the feature frequency into consideration when using PQ. Specifically, it divides the embeddings of items into \ud835\udc5a groups in ascending order of frequency as G = { E 1 , E 2 , ..., E \ud835\udc5a } and defines N = { \ud835\udc5b 1 , \ud835\udc5b 2 , ..., \ud835\udc5b \ud835\udc5a } where \ud835\udc5b 1 < \ud835\udc5b 2 < ... < \ud835\udc5b \ud835\udc5a . The embeddings in \ud835\udc56 -th group can only be quantized into the first \ud835\udc5b \ud835\udc56 meta-embeddings in each meta-table. Similarly, xLightFM [24] performs PQ in each feature field. Considering that the feature fields have various size, xLightFM searches for the optimal size (i.e., the number of meta-embeddings) of the meta-tables for each field. The search process is achieved by the DARTS algorithm which is similar as the embedding dimension search in Section 3.2. Similar to PQ, AQ considers an embedding as a summation of \ud835\udc60 vectors: \ud835\udc86 = \u02dd \ud835\udc60 \ud835\udc56 = 1 \ud835\udc86 \ud835\udc56 . AQ generates its quantized embeddings by \u02dd \ud835\udc60 \ud835\udc56 = 1 \ud835\udc70 \ud835\udc56 \u00d7 T \ud835\udc56 , \ud835\udc70 \ud835\udc56 = one-hot ( arg max \ud835\udc58 sim ( \ud835\udc86 -\u02dd \ud835\udc56 -1 \ud835\udc63 = 1 T \ud835\udc63 \ud835\udc58 \ud835\udc63 , T \ud835\udc56 \ud835\udc58 )) where \ud835\udc58 \ud835\udc63 is the index of the selected meta-embedding in the \ud835\udc63 -th meta-table. Specifically, the first meta-table takes the embedding \ud835\udc86 as input, and outputs its nearest meta-embedding T 1 \ud835\udc58 1 , the second metatable then quantizes the residual part ( \ud835\udc86 -T 1 \ud835\udc58 1 ) into T 2 \ud835\udc58 2 and so on. The final output embedding \u02c6 \ud835\udc86 = \u02dd \ud835\udc60 \ud835\udc56 = 1 T \ud835\udc56 \ud835\udc58 \ud835\udc56 . LightRec [34] adopts AQ to compress the item embeddings and uses a pretrained model as a teacher to train the meta-tables effectively. LISA [63] utilizes AQ to compress the DLRMs where self-attention is performed for sequence processing. Note that there is a mass of inner product between embeddings in self-attention which suffer from extremely expensive computing costs. To alleviates this problem, LISA pre-calculates the inner product between meta-embeddings in the same meta-table and stores the results in a small table after training. Then, the inner product of embeddings in self-attention can be calculated by summing the inner product of meta-embeddings which can accelerate the inference significantly.", "4.3 Decomposition": "Hashing and vector quantization use one-hot index vectors to perform a hard selection (i.e., selecting only one meta-embedding) in the meta-table and alleviate the collisions between features by maintaining multiply meta-tables. On the contrary, decomposition approaches make a soft selection by summing over all the meta-embeddings in a meta-table T \u2208 R \ud835\udc5a \u00d7 \ud835\udc51 with a real-valued index vector \ud835\udc70 \u2208 R \ud835\udc5a . Due to the wide representation space of the real-valued index vectors, one meta-table is sufficient to resolve the collisions between features. Each feature will have a unique index vector stored in the index matrix I \ud835\udc40 \u2208 R \ud835\udc5b \u00d7 \ud835\udc5a when formulating the decomposition as E = I \ud835\udc40 \u00d7 T . MLET [14] factorizes the embedding table E \u2208 R \ud835\udc5b \u00d7 \ud835\udc51 in terms of I \ud835\udc40 \u2208 R \ud835\udc5b \u00d7 \ud835\udc5a and T \u2208 \ud835\udc45 \ud835\udc5a \u00d7 \ud835\udc51 . Different from the low-rank decomposition where \ud835\udc5a < \ud835\udc51 , MLET decomposes the embedding table into larger matrices (i.e., \ud835\udc5a > \ud835\udc51 ) at training to ensure a larger optimization space. After training, MLET generates the embedding table as E = I \ud835\udc40 \u00d7 T for memory reduction and fast retrieval. ANT [35] adopt a better initialization for T and imposes a sparse constraint on I \ud835\udc40 . Specifically, ANT initializes the meta-table T by clustering the embeddings of a pretrained model. In addition, to reduce redundancy, ANT use an \u2113 1 penalty on I \ud835\udc40 and constrain its domain to be non-negative. Instead of learning and storing the index vectors at training, DHE [27] develop a hash encoder H : N \u2192 R \ud835\udc5a to map each feature id into an index vector \ud835\udc70 \u2208 R \ud835\udc5a on the fly. Specifically, H( \ud835\udc65 ) = [ \u210e 1 ( \ud835\udc65 ) , \u210e 2 ( \ud835\udc65 ) , ..., \u210e \ud835\udc5a ( \ud835\udc65 )] , where { \u210e \ud835\udc56 } \ud835\udc5a \ud835\udc56 = 1 is a group of hash functions. With the hash encoder, DHE can eliminate the storage and optimization of I \ud835\udc40 . Moreover, considering that the index vectors are deterministic and cannot be optimized, DHE further decomposes the meta-table \ud835\udc47 into a multi-layer neural network to enhance its expressive ability. Different from the above naive decomposition, [56, 64, 71] use tensor train decomposition (TTD) to decompose the embedding tables. As shown in Figure 6, the embedding table E \u2208 R \ud835\udc5b \u00d7 \ud835\udc51 will first be reshaped into E \u2208 R ( \ud835\udc5b 1 \ud835\udc51 1 ) \u00d7 ( \ud835\udc5b 2 \ud835\udc51 2 ) \u00d7 ... \u00d7( \ud835\udc5b \ud835\udc60 \ud835\udc51 \ud835\udc60 ) , where \ud835\udc5b = \u02db \ud835\udc60 \ud835\udc56 = 1 \ud835\udc5b \ud835\udc56 and \ud835\udc51 = \u02db \ud835\udc60 \ud835\udc56 = 1 \ud835\udc51 \ud835\udc56 . Then, E will be decomposed as E = G 1 \u00d7 G 2 \u00d7 ... \u00d7 G \ud835\udc60 , where G \ud835\udc56 \u2208 R \ud835\udc5f \ud835\udc56 -1 \u00d7 \ud835\udc5b \ud835\udc56 \ud835\udc51 \ud835\udc56 \u00d7 \ud835\udc5f \ud835\udc56 . {G \ud835\udc56 } \ud835\udc60 \ud835\udc56 = 1 are called TT-cores and { \ud835\udc5f \ud835\udc56 } \ud835\udc60 \ud835\udc56 = 0 are called TT-ranks, in particular \ud835\udc5f 0 = \ud835\udc5f \ud835\udc60 = 1. TT-Rec [71] is the first to use TTD on the embedding tables of DLRMs. It implements optimized kernels of TTD for embedding tables. LLRec [56] uses TTD on the embedding tables while maintaining the prediction accuracy by knowledge distillation. To enhance the compression capability of TTD, [64] further develop semitensor product based tensor train decomposition (STTD). Semi-tensor product is a generalization of reshape nzd ? n,d , { STTD n ,d , n,d,/k matrix product. Specifically, given \ud835\udc82 \u2208 R 1 \u00d7 \ud835\udc5b\ud835\udc5d and \ud835\udc83 \u2208 R \ud835\udc5d , \ud835\udc82 can be cut into \ud835\udc5d blocks { \ud835\udc82 \ud835\udc56 \u2208 R 1 \u00d7 \ud835\udc5b } \ud835\udc5d \ud835\udc56 = 1 and \ud835\udc82 \u22c9 \ud835\udc83 = \u02dd \ud835\udc5d \ud835\udc56 = 1 \ud835\udc82 \ud835\udc56 \u00d7 \ud835\udc83 \ud835\udc56 \u2208 R 1 \u00d7 \ud835\udc5b , where \u22c9 is the left semi-tensor product. For matrices A \u2208 R \u210e \u00d7 \ud835\udc5b\ud835\udc5d and B \u2208 R \ud835\udc5d \u00d7 \ud835\udc5e , A \u22c9 B \u2208 R \u210e \u00d7 \ud835\udc5b\ud835\udc5e contains \u210e \u00d7 \ud835\udc5e blocks and each block is the semi-tensor product between a row of A and a column of B . [64] replaces the conventional matrix tensor product of TTD with the left semi-tensor product. As Figure 6 shows, in STTD, E = \u02c6 G 1 \u22c9 \u02c6 G 2 \u22c9 ... \u22c9 \u02c6 G \ud835\udc60 , where \u02c6 G \ud835\udc56 \u2208 R \ud835\udc5f \ud835\udc56 -1 \ud835\udc58 \u00d7 \ud835\udc5b \ud835\udc56 \ud835\udc51 \ud835\udc56 \ud835\udc58 \u00d7 \ud835\udc5f \ud835\udc56 , \ud835\udc5f 0 = \ud835\udc58 and \ud835\udc5f \ud835\udc60 = 1. In addition, [64] uses self-supervised knowledge distillation to to reduce accuracy loss from compression.", "4.4 Discussion": "Weight sharing approaches usually make remarkable reduction to the memory usage. However, they suffer from low efficiency at training due to extra computing cost for generating embeddings, especially the nearest neighbor search in vector quantization and the matrix multiplication in decomposition approaches. The extra computing cost will also slow down the inference speed except in vector quantization where we can store the results of inner product between meta-embeddings to accelerate the inference. Nevertheless, vector quantization maintains the original embeddings during training which requires extra memory usage. Moreover, these methods usually cannot improve the prediction accuracy, especially hashing usually causes severe drop of accuracy.", "5 SUMMARY": "Embedding tables usually constitute a large portion of model parameters in DLRMs, which need to be compressed for efficient and economical deployment. As recommender systems continue to grow in scale, embedding compression has attracted more and more attention. In this survey, we provide a comprehensive review of the embedding compression methods in recommender systems, accompanied by a systematic and rational organization of existing studies. The embedding table can be conceptualized as a matrix with three dimensions, namely the precision of weights, the dimension of embeddings, and the number of embeddings. Consequently, we classify embedding compression methods into three primary categories according to the dimensions they compress, which are low-precision, mixed-dimension, and weight-sharing, respectively. Low-precision methods reduce the memory of each weight by decreasing its bit width, including binarization and quantization. Mixed-dimension methods reduce the memory of specific embeddings by decreasing their dimensions, including rule-based approaches, NAS-based approaches and pruning. Weight-sharing methods reduce the actual parameters of the embedding table by sharing weights among different embeddings, including hashing, vector quantization and decomposition.", "5.1 General Suggestions": "In the above sections, we have discussed the pros and cons of different compression methods in detail. However, there are no golden criteria to measure which one is the best. How to choose a proper compression method depends greatly on the application scenarios and requirements. Therefore, we offer some general suggestions for the common requirements on the key metrics discussed in Section 1.2, namely model accuracy, inference efficiency, training efficiency, and training memory usage, respectively. \u00b7 Model accuracy. In scenarios that demand high model accuracy, any accuracy degradation caused by compression is deemed unacceptable. In such cases, mixed-dimension methods are recommended as they have been reported to remove redundant parameters and avoid model overfitting. With an appropriate compression ratio, mixed-dimension methods can effectively compress embeddings while maintaining or even improving accuracy. Furthermore, accuracy can also be preserved when compressing embeddings by quantization with a higher bit width. For instance, using a 16-bit representation has been proven to be sufficient for achieving accurate results. On the contrary, for scenarios that do not require high prediction accuracy, quantization with lower bit width or even binarization (1-bit) can be employed to achieve stronger compression. \u00b7 Inference efficiency. In scenarios such as online inference, model inference efficiency is of paramount importance. Generally speaking, most embedding compression methods will not have a great negative impact on the inference speed. However, in several decomposition methods where the embedding table is decomposed into multiple small matrices, the process of recovering embeddings may introduce significant inference latency and should be avoided in this context. To improve inference efficiency while compressing embeddings, vector quantization is suggested, as the feature interaction (e.g., inner-product) of embeddings can be pre-calculated to accelerate the inference process. Additionally, binarization is also worth considering when there is no high requirement on model accuracy. The calculation of feature interactions between binary embeddings is faster compared to that between full-precision embeddings. \u00b7 Training efficiency. In scenarios where the models are supposed to be updated in a timely manner, training efficiency becomes a critical factor. However, it is unfortunate that most embedding compression methods do not contribute to improving the training efficiency. In fact, some of them may significantly reduce training efficiency, particularly NAS-based approaches, pruning, vector quantization, and decomposition. Specifically, NAS-based approaches involve complex calculations to search for optimal embedding dimensions, which can be computationally intensive and time-consuming. Pruning often necessitates retraining to achieve higher accuracy, resulting in additional training overhead. Vector quantization also involves cumbersome calculations for nearest neighbor searches. Decomposition may require multiple matrix multiplications to recover and retrieve embeddings. Therefore, in scenarios that prioritize training efficiency and timely model updates, these methods are not recommended. \u00b7 Training memory usage. In scenarios where computing devices have limited memory, it is desirable to compress the training memory usage of embeddings or, at the very least, avoid increasing it. In such cases, we suggest using rule-based approaches, hashing, or decomposition, as they can compress the embedding table before training. Besides, the low-precision training of quantization is also worth considering, as the embeddings are stored in the format of integers during training. On the contrary, NAS-based approaches and vector quantization are not recommended in this context. They often require storing a significant number of intermediate results to guide the training process, which will consume more memory.", "5.2 Future Prospects": "Embedding compression in recommender systems has witnessed rapid development and notable achievements, although there are still several challenging issues that require attention. We identify several potential directions for further research in this field. \u00b7 Low-precision. The key problem faced by low-precision methods is the severe accuracy degradation at extremely lower bit widths. In view of the extensive and advanced research on quantization and binarization in the deep learning community, we can refer to related techniques to alleviate the accuracy loss when compressing embeddings, which is quite challenging and valuable. \u00b7 Mixed-dimension. In recent advanced mixed-dimension methods, there is a need to enhance the training efficiency of NAS-based approaches and pruning. To address this, we recommend designing lighter NAS frameworks that can efficiently search for the optimal embedding dimension. On the other hand, finding solutions to avoid retraining pruned models is also crucial for enhancing training efficiency. Furthermore, while numerous studies have demonstrated the significant impact of the embedding dimension on model accuracy, there is still a lack of theoretical understanding regarding how the embedding dimension precisely affects model accuracy. Having a solid theoretical basis would be invaluable in guiding the optimal selection of embedding dimensions, enabling more efficient and effective model training. \u00b7 Weight-sharing. To approach the limit of weight sharing methods, we believe that an intriguing direction to explore is the use of embedding generation networks. Considering the powerful representation capabilities of neural networks, we may learn a powerful neural network to generate embeddings, instead of directly learning and maintaining the embeddings themselves. \u00b7 Hybrid approaches. Since the methods within the three primary categories compress the embeddings from different dimensions and enjoy different advantages, we expect future research to establish a unified method for compressing multiple dimensions, or develop hybrid approaches combining these techniques. By integrating the strengths of different compression methods, it is possible to create more powerful and comprehensive compression algorithms. \u00b7 Open benchmarks. This review offers a thorough discussion of embedding compression methods. However, we did not undertake an experimental comparison across these methods. On one hand, distinct methods are applied to different tasks in recommender systems, each of which has unique accuracy metrics. For example, in click-through rate (CTR) prediction, the commonly used metric is the Area Under the Curve (AUC); whereas for rating prediction, Root Mean Square Error (RMSE) and Mean Absolute Error (MAE) are typically employed; for Top-N recommendations, Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG) are commonly utilized as accuracy metrics. On the other hand, a majority of research relies on proprietary datasets without sharing open-source code, presenting obstacles to reproducibility and comparative analyses. Nonetheless, the implementation of these methods is not inherently complex. Given the focus on the embedding tables, a solution involves the definition of a new embedding module during implementation, coupled with the rewriting of the lookup operation for the embedding vector. Therefore, it is necessary to establish a foundational benchmark to evaluate the effectiveness of distinct methods across a spectrum of tasks, like BARS [81], a benchmark designed for recommendations. We posit that this would substantially expedite the application and advancement of this field.", "ACKNOWLEDGMENTS": "This work is supported in part by National Natural Science Foundation of China under grants 62376103, 62302184, 62206102, and Science and Technology Support Program of Hubei Province under grant 2022BAA046.", "REFERENCES": "[1] Artem Babenko and Victor S. Lempitsky. 2014. Additive Quantization for Extreme Vector Compression. In 2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2014, Columbus, OH, USA, June 23-28, 2014 . IEEE Computer Society, Columbus, OH, USA, 931-938. [2] Yash Bhalgat, Jinwon Lee, Markus Nagel, Tijmen Blankevoort, and Nojun Kwak. 2020. LSQ+: Improving low-bit quantization through learnable offsets and better initialization. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR Workshops 2020, Seattle, WA, USA, June 14-19, 2020 . Computer Vision Foundation / IEEE, 2978-2985. [3] Tong Chen, Hongzhi Yin, Yujia Zheng, Zi Huang, Yang Wang, and Meng Wang. 2021. Learning Elastic Embeddings for Customizing On-Device Recommenders. In KDD '21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, Singapore, August 14-18, 2021 . ACM, 138-147. [4] Yunchuan Chen, Lili Mou, Yan Xu, Ge Li, and Zhi Jin. 2016. Compressing Neural Language Models by Sparse Word Representations. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers . The Association for Computer Linguistics. [5] Yankai Chen, Yifei Zhang, Yingxue Zhang, Huifeng Guo, Jingjie Li, Ruiming Tang, Xiuqiang He, and Irwin King. 2021. Towards Low-loss 1-bit Quantization of User-item Representations for Top-K Recommendation. CoRR abs/2112.01944 (2021). [6] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and Hemal Shah. 2016. Wide & Deep Learning for Recommender Systems. In Proceedings of the 1st Workshop on Deep Learning for Recommender Systems, DLRS@RecSys 2016, Boston, MA, USA, September 15, 2016 . ACM, 7-10. [7] Weiyu Cheng, Yanyan Shen, and Linpeng Huang. 2020. Differentiable Neural Input Search for Recommender Systems. CoRR abs/2006.04466 (2020). [8] Tejalal Choudhary, Vipul Kumar Mishra, Anurag Goswami, and Jagannathan Sarangapani. 2020. A comprehensive survey on model compression and acceleration. Artif. Intell. Rev. 53, 7 (2020), 5113-5155. [9] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. 2015. BinaryConnect: Training Deep Neural Networks with binary weights during propagations. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada . 3123-3131. [10] Wei Deng, Junwei Pan, Tian Zhou, Deguang Kong, Aaron Flores, and Guang Lin. 2021. DeepLight: Deep Lightweight Feature Interactions for Accelerating CTR Predictions in Ad Serving. In WSDM '21, The Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel, March 8-12, 2021 . ACM, 922-930. [11] Aditya Desai, Li Chou, and Anshumali Shrivastava. 2022. Random Offset Block Embedding (ROBE) for compressed embedding tables in deep learning recommendation systems. Proceedings of Machine Learning and Systems 4 (2022), 762-778. [12] Aditya Desai, Yanzhou Pan, Kuangyuan Sun, et al. 2021. Semantically Constrained Memory Allocation (SCMA) for Embedding in Efficient Recommendation Systems. CoRR abs/2103.06124 (2021). [13] Steven K. Esser, Jeffrey L. McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmendra S. Modha. 2020. Learned Step Size quantization. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net. [14] Benjamin Ghaemmaghami, Zihao Deng, Benjamin Y. Cho, et al. 2020. Training with Multi-Layer Embeddings for Model Reduction. CoRR abs/2006.05623 (2020). [15] Antonio A. Ginart, Maxim Naumov, Dheevatsa Mudigere, Jiyan Yang, and James Zou. 2021. Mixed Dimension Embeddings with Application to Memory-Efficient Recommendation Systems. In IEEE International Symposium on Information Theory, ISIT 2021, Melbourne, Australia, July 12-20, 2021 . IEEE, 2786-2791. [16] Hui Guan, Andrey Malevich, Jiyan Yang, Jongsoo Park, and Hector Yuen. 2019. Post-Training 4-bit Quantization on Embedding Tables. CoRR abs/1911.02079 (2019). [17] Huifeng Guo, Wei Guo, Yong Gao, Ruiming Tang, Xiuqiang He, and Wenzhi Liu. 2021. ScaleFreeCTR: MixCache-based Distributed Training System for CTR Models with Huge Embedding Table. In SIGIR '21: The 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, Virtual Event, Canada, July 11-15, 2021 . ACM, 1269-1278. [18] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: A Factorization-Machine based Neural Network for CTR Prediction. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017, Melbourne, Australia, August 19-25, 2017 . ijcai.org, 1725-1731. [19] Manish Gupta and Puneet Agrawal. 2022. Compression of Deep Learning Models for Text: A Survey. ACM Trans. Knowl. Discov. Data 16, 4 (2022), 61:1-61:55. [20] Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing Xu, and Chang Xu. 2020. GhostNet: More Features From Cheap Operations. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020 . 1577-1586. [21] Xin He, Kaiyong Zhao, and Xiaowen Chu. 2021. AutoML: A Survey of the State-of-the-Art. Knowledge-Based Systems 212 (2021), 106622. [40] Qiang Liu, Feng Yu, Shu Wu, and Liang Wang. 2015. A Convolutional Click Prediction Model. In Proceedings of the 24th ACM International Conference on Information and Knowledge Management, CIKM 2015, Melbourne, VIC, Australia, October 19 - 23, 2015 . ACM, 1743-1746. [41] Siyi Liu, Chen Gao, Yihong Chen, Depeng Jin, and Yong Li. 2021. Learnable Embedding sizes for Recommender Systems. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenReview.net. [42] Xianglong Liu, Junfeng He, Cheng Deng, and Bo Lang. 2014. Collaborative Hashing. In 2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2014, Columbus, OH, USA, June 23-28, 2014 . IEEE Computer Society, 2147-2154. [43] Fuyuan Lyu, Xing Tang, Hong Zhu, Huifeng Guo, Yingxue Zhang, Ruiming Tang, and Xue Liu. 2022. OptEmbed: Learning Optimal Embedding Table for Click-through Rate Prediction. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management, Atlanta, GA, USA, October 17-21, 2022 . ACM, 1399-1409. [44] Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu, and Kun Gai. 2018. Entire Space MultiTask Model: An Effective Approach for Estimating Post-Click Conversion Rate. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, SIGIR 2018, Ann Arbor, MI, USA, July 08-12, 2018 , Kevyn Collins-Thompson, Qiaozhu Mei, Brian D. Davison, Yiqun Liu, and Emine Yilmaz (Eds.). ACM, 1137-1140. [45] Kelong Mao, Jieming Zhu, Jinpeng Wang, Quanyu Dai, Zhenhua Dong, Xi Xiao, and Xiuqiang He. 2021. SimpleX: A Simple and Strong Baseline for Collaborative Filtering. In CIKM '21: The 30th ACM International Conference on Information and Knowledge Management, Virtual Event, Queensland, Australia, November 1 - 5, 2021 . ACM, 1243-1252. [46] H. Brendan McMahan, Gary Holt, David Sculley, Michael Young, Dietmar Ebner, Julian Grady, Lan Nie, Todd Phillips, Eugene Davydov, Daniel Golovin, Sharat Chikkerur, Dan Liu, Martin Wattenberg, Arnar Mar Hrafnkelsson, Tom Boulos, and Jeremy Kubica. 2013. Ad click prediction: a view from the trenches. In The 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 2013, Chicago, IL, USA, August 11-14, 2013 . ACM, 1222-1230. [47] Niketan Pansare, Jay Katukuri, Aditya Arora, Frank Cipollone, Riyaaz Shaik, Noyan Tokgozoglu, and Chandru Venkataraman. 2022. Learning Compressed Embeddings for On-Device Inference. In Proceedings of Machine Learning and Systems 2022, MLSys 2022, Santa Clara, CA, USA, August 29 - September 1, 2022 . mlsys.org. [48] Liang Qu, Yonghong Ye, Ningzhi Tang, Lixin Zhang, Yuhui Shi, and Hongzhi Yin. 2022. Single-shot Embedding Dimension Search in Recommender System. In SIGIR '22: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . ACM, 513-522. [49] Steffen Rendle. 2010. Factorization Machines. In ICDM 2010, The 10th IEEE International Conference on Data Mining, Sydney, Australia, 14-17 December 2010 . IEEE Computer Society, 995-1000. [50] Hao-Jun Michael Shi, Dheevatsa Mudigere, Maxim Naumov, and Jiyan Yang. 2020. Compositional Embeddings Using Complementary Partitions for Memory-Efficient Recommendation Systems. In KDD '20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020 . ACM, 165-175. [51] Yixin Su, Rui Zhang, Sarah M. Erfani, and Zhenghua Xu. 2021. Detecting Beneficial Feature Interactions for Recommender Systems. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021 . AAAI Press, 4357-4365. [52] Yixin Su, Yunxiang Zhao, Sarah M. Erfani, Junhao Gan, and Rui Zhang. 2022. Detecting Arbitrary Order Beneficial Feature Interactions for Recommender Systems. In KDD '22: The 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Washington, DC, USA, August 14 - 18, 2022 . ACM, 1676-1686. [53] Yang Sun, Fajie Yuan, Min Yang, Guoao Wei, Zhou Zhao, and Duo Liu. 2020. A Generic Network Compression Framework for Sequential Recommender Systems. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 . ACM, 1299-1308. [54] Qiaoyu Tan, Ninghao Liu, Xing Zhao, Hongxia Yang, Jingren Zhou, and Xia Hu. 2020. Learning to Hash with Graph Neural Networks for Recommender Systems. In WWW'20: The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020 . ACM / IW3C2, 1988-1998. [55] Jinpeng Wang, Ziyun Zeng, Yunxiao Wang, Yuting Wang, Xingyu Lu, Tianxiang Li, Jun Yuan, Rui Zhang, Hai-Tao Zheng, and Shu-Tao Xia. 2023. MISSRec: Pre-training and Transferring Multi-modal Interest-aware Sequence Representation for Recommendation. In Proceedings of the 31st ACM International Conference on Multimedia, MM 2023, Ottawa, ON, Canada, 29 October 2023- 3 November 2023 . ACM, 6548-6557. [56] Qinyong Wang, Hongzhi Yin, Tong Chen, Zi Huang, Hao Wang, Yanchang Zhao, and Nguyen Quoc Viet Hung. 2020. Next Point-of-Interest Recommendation on Resource-Constrained Mobile Devices. In WWW'20: The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020 . ACM / IW3C2, 906-916. [57] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & Cross Network for Ad Click Predictions. In Proceedings of the ADKDD'17, Halifax, NS, Canada, August 13 - 17, 2017 . ACM, 12:1-12:7. Francisco, California, USA . AAAI Press, 1669-1675. [77] Zhiwei Zhang, Qifan Wang, Lingyun Ruan, and Luo Si. 2014. Preference preserving hashing for efficient recommendation. In The 37th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '14, Gold Coast , QLD, Australia - July 06 - 11, 2014 . ACM, 183-192. [78] Xiangyu Zhao, Haochen Liu, Wenqi Fan, Hui Liu, Jiliang Tang, Chong Wang, Ming Chen, Xudong Zheng, Xiaobing Liu, and Xiwang Yang. 2021. AutoEmb: Automated Embedding Dimensionality Search in Streaming Recommendations. (2021), 896-905. [79] Xiangyu Zhao, Haochen Liu, Hui Liu, Jiliang Tang, Weiwei Guo, Jun Shi, Sida Wang, Huiji Gao, and Bo Long. 2020. Memory-efficient Embedding for Recommendations. CoRR abs/2006.14827 (2020). [80] Guorui Zhou, Xiaoqiang Zhu, Chengru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep Interest Network for Click-Through Rate Prediction. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2018, London, UK, August 19-23, 2018 . ACM, 1059-1068. [81] Jieming Zhu, Quanyu Dai, Liangcai Su, Rong Ma, Jinyang Liu, Guohao Cai, Xi Xiao, and Rui Zhang. 2022. BARS: Towards Open Benchmarking for Recommender Systems. In SIGIR '22: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . ACM, 2912-2923. [82] Barret Zoph and Quoc V. Le. 2017. Neural Architecture Search with Reinforcement Learning. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings . OpenReview.net."}
