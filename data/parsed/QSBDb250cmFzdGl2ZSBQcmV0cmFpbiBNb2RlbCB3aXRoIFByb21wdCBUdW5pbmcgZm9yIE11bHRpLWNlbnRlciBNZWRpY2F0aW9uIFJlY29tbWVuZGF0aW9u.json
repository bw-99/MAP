{"A Contrastive Pretrain Model with Prompt Tuning for Multi-center Medication Recommendation": "QIDONG LIU, Xi'an Jiaotong University & City University of Hong Kong, China ZHAOPENG QIU, Jarvis Research Center, Tencent YouTu Lab, China XIANGYU ZHAO \u2217 , City University of Hong Kong, China XIAN WU \u2217 , Jarvis Research Center, Tencent YouTu Lab, China ZIJIAN ZHANG, Jilin University & City University of Hong Kong, China TONG XU, University of Science and Technology of China, China FENG TIAN \u2217 , Xi'an Jiaotong Univeristy, China Medication recommendation is one of the most critical health-related applications, which has attracted extensive research interest recently. Most existing works focus on a single hospital with abundant medical data. However, many small hospitals only have a few records, which hinders applying existing medication recommendation works to the real world. Thus, we seek to explore a more practical setting, i.e., multi-center medication recommendation. In this setting, most hospitals have few records, but the total number of records is large. Though small hospitals may benefit from total affluent records, it is also faced with the challenge that the data distributions between various hospitals are much different. In this work, we introduce a novel con T rastive pr E train M odel with P rompt T uning ( TEMPT ) for multi-center medication recommendation, which includes two stages of pretraining and finetuning. We first design two self-supervised tasks for the pretraining stage to learn general medical knowledge. They are mask prediction and contrastive tasks, which extract the intra- and inter-relationships of input diagnosis and procedures. Furthermore, we devise a novel prompt tuning method to capture the specific information of each hospital rather than adopting the common finetuning. On the one hand, the proposed prompt tuning can better learn the heterogeneity of each hospital to fit various distributions. On the other hand, it can also relieve the catastrophic forgetting problem of finetuning. To validate the proposed model, we conduct extensive experiments on the public eICU, a multi-center medical dataset. The experimental results illustrate the effectiveness of our model. The implementation code is available to ease the reproducibility 1 .", "CCS Concepts: \u00b7 Applied computing \u2192 Health informatics ; \u00b7 Information systems \u2192 Recommender systems .": "Additional Key Words and Phrases: Multi-Center; Medication Recommendation; Electronic Health Record; Contrastive Learning; Prompt Tuning", "ACMReference Format:": "Qidong Liu, Zhaopeng Qiu, Xiangyu Zhao, Xian Wu, Zijian Zhang, Tong Xu, and Feng Tian. 2024. A Contrastive Pretrain Model with Prompt Tuning for Multi-center Medication Recommendation. In . ACM, New York, NY, USA, 29 pages. https://doi.org/XXXXXXX.XXXXXXX \u2217 Corresponding Authors. 1 https://github.com/Applied-Machine-Learning-Lab/TEMPT Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. XX, June 03-05,2018, Woodstock, NY \u00a9 2024 Association for Computing Machinery. ACM ISBN 978-1-4503-XXXX-X/18/06...$15.00 https://doi.org/XXXXXXX.XXXXXXX 1 XX, June 03-05,2018, Woodstock, NY Qidong Liu et al. Fig. 1. The histogram of medical record counts of each hospital and the histogram of the average number of prescribed medications by each hospital. 500 1500 2500 3500 4500 # of records 0 2 4 6 8 10 12 # of hospitals (a) Record counts. 0 5 10 15 20 25 30 35 avg. # med per record 0 2 4 6 8 10 12 # of hospitals (b) Average medication number.", "1 INTRODUCTION": "Prescription is an important way for doctors to treat patients, but facing patients with various diagnoses requires tremendous and sophisticated expert efforts. Due to high expertise and complexity, some medical research studies have reported that senior doctors have suffered from heavy workloads of prescriptions [29, 78], while junior doctors are prone to error prescriptions [4, 57]. The automatic medication recommender system could potentially assist doctors and alleviate these problems [1, 52]. Thus, medication recommendation has drawn increasing research attention in recent years [61, 82, 85]. Thanks to the advancement of deep learning, some researchers have explored recommending proper medication sets for a single hospital with abundant medical records. However, in the real world, the number of health records in one hospital is often too few. As shown in Figure 1(a), we analyze the statistics of the eICU [53], which is a real-world multi-center medical dataset. The histogram of record counts of each hospital illustrates that only a few hospitals have more than 2,500 records and most hospitals even own less than 1,500 records. Such few records are insufficient for adequately training a model, but the affluent sum of data from all hospitals gives us a chance to enhance the model. Therefore, we focus on making use of records from several hospitals ( i.e., multi-center) and constructing the model that can recommend proper medication sets for all of these hospitals, especially for benefiting those hospitals with few records. Similar to the process of prescription in the real world, the medication recommendation models give out the proper medication set according to the patient's diagnosis and procedure. The diagnosis often contains the patient's disease, such as 'heart disease', which should be targeted by recommended medications. As for the procedures, such as 'surgery', they need medications for assistance to treat patients, so medications also can be indicated by them. With the growing research interests in this field, numerous deep learning-based models have emerged recently [61, 82], which can be divided into two categories: Longitudinal models and Instance-based models. The longitudinal models [60, 61, 82, 85, 86] recommend medication combinations based on patient's historical records. However, most models in this thread cannot recommend for the patients without historical data, which is more practical in real-world scenarios [65]. In contrast, the instance-based models are based on the current diagnosis and procedures of one patient, and there is no need for historical information, such as Leap [87] and SMR [19]. To be more practical, we will focus on the instance-based setting in this paper. 2 A Contrastive Pretrain Model with Prompt Tuning for Multi-center Medication Recommendation XX, June 03-05,2018, Woodstock, NY Fig. 2. The heatmap to visualize Jensen-Shannon divergence of prescription distribution for all pairs of hospitals in the eICU dataset. 10 20 30 40 50 60 70 80 Hospital ID 10 20 30 40 50 60 70 80 Hospital ID 0.0 0.2 0.4 0.6 0.8 1.0 Existing medication recommendation models are designed for a single hospital [60, 61, 82], but often ignore the multi-center setting. At the same time, the variance of access to treatment resources among multiple hospitals may cause distinct therapies and prescriptions. For example, some research studies have explored how different hospital conditions affect patient outcomes, such as the bronchoscopy [77] and bed size [59]. Besides, whether a hospital is a teaching hospital also affects the ways of treatment [55]. These previous research papers indicate the existence of prescription distribution variance, which may hinder unifying a model for multi-center medication recommendation. To illustrate the distribution variance further, we demonstrate the results of two preliminary experiments. As shown in Figure 1(b), the average prescribed medication number varies much across hospitals, which illustrates that the differences between distributions of various hospitals are large. Besides, we show the Jesen-Shannon divergence of prescription distributions for all pairs of hospitals in the eICU dataset. Jesen-Shannon divergence ( \ud835\udc3d\ud835\udc46\ud835\udc37 ) [39] is an entropy-based method to measure the similarity between two distributions. In specific, \ud835\udc3d\ud835\udc46\ud835\udc37 is bounded between 0 and 1, where \ud835\udc3d\ud835\udc46\ud835\udc37 = 0 means the same two distributions and vice versa. In this experiment, we calculate the \ud835\udc3d\ud835\udc46\ud835\udc37 of prescription distributions between all pairs of hospitals. As shown in Figure 2, the heatmap indicates that the \ud835\udc3d\ud835\udc46\ud835\udc37 values between most hospital pairs are larger than 0 . 4 and some even reach above 0 . 8. The results further demonstrate the great prescription distinction between hospitals. Such a difference will degrade the model trained on multi-center data. Recently, some efforts on multi-domain recommendation (MDR) could relieve the problem: (i) one is multi-task learning related works, such as MMOE [51] and PLE [67]. However, they often work for a small number of domains but are not fit for too many centers, because they need the same number of expert networks as the domains [26]; (ii) the other is about multi-domain Click Through Rate (CTR) prediction, e.g., STAR [63], which devises a star topology learning scheme. The shortcoming of this 3 XX, June 03-05,2018, Woodstock, NY Qidong Liu et al. work is that it is built on a feed-forward network and cannot combine with advanced architectures used in medication recommendation, such as transformer [82]. For the challenge of multi-center medication recommendation mentioned above, we propose a novel con T rastive pr E train M odel with P rompt T uning ( TEMPT ) for multi-center medication recommendation. To extract the medical information from diagnosis and procedures, we construct a transformer-based recommendation model. Specially, this model consists of two stages: pretraining and prompt tuning. At the pretraining stage, we propose two self-supervised tasks to learn general medical knowledge from all hospitals. The diagnosis and procedure contain medical information in themselves, so we design a mask prediction task that reconstructs them to learn the latent intra-relationships. Then, due to the close inter-relationship between diagnosis and procedure sets, we propose a novel contrastive task to capture such many-to-many relationships via aligning the pair of representations. After pretraining, finetuning for each hospital is an instinctive way. However, the general finetuning method faces the problem of catastrophic forgetting [56]. Inspired by the advancement of prompt learning in NLP [36] and CV [24], we design a continuous prompt tuning method for the pretrained model, which can both hold the knowledge in the pretraining stage and better extract specific information for each hospital. The main contributions of this paper can be concluded as follows: \u00b7 To the best of our knowledge, this paper is the first to investigate the multi-center setting in the medication recommendation field. This setting is closer to the situation of the real world, where small hospitals occupy the majority. \u00b7 We propose a two-stage model for multi-center medication recommendation. At the pretraining stage, we devise mask prediction and contrastive self-supervised tasks to learn general medical knowledge. Then, we design an efficient prompt-tuning method to adapt to the distribution of each hospital. \u00b7 Extensive experiments on a multi-center medical dataset eICU have been conducted. The experimental results demonstrate that the proposed model outperforms state-of-the-art medication recommendation models and multi-domain recommendation models. The rest of this paper is organized into five sections. In Section 2, we formulate the multi-center medication recommendation. Then, we detail the proposed model in Section 3. The experiments and analysis are given in Section 4. Finally, we refer to the related works and conclusion in Section 5 and Section 6, respectively.", "2 PROBLEM FORMULATION": "The multi-center Electrical Health Records (EHR) consist of uniform data from several hospitals. Let V = {R ( \u210e ) } \ud835\udc3b \u210e = 1 denote the integrated EHR with \ud835\udc3b hospitals. In the EHR of each hospital, the records can be represented as R ( \u210e ) = [R ( \u210e ) 1 , ..., R ( \u210e ) \ud835\udc60 , ..., R ( \u210e ) \ud835\udc46 ( \u210e ) ] , where \ud835\udc46 ( \u210e ) is the number of records of hospital \u210e . For simplicity, we omit the hospital index \u210e for the single record when we describe the proposed model. We let D \ud835\udc48 = { \ud835\udc51 1 , \ud835\udc51 2 , ..., \ud835\udc51 | D \ud835\udc48 | } denote the diagnosis set, P \ud835\udc48 = { \ud835\udc5d 1 , \ud835\udc5d 2 , ..., \ud835\udc5d | P \ud835\udc48 | } denote the procedure set, and M \ud835\udc48 = { \ud835\udc5a 1 , \ud835\udc5a 2 , ..., \ud835\udc5a | M \ud835\udc48 | } denote the medication set, where |D| , |P| and |M| represent the number of all possible diagnosis, procedures and medications, respectively. Each record includes three types of set, i.e., R \ud835\udc60 = {D , P , M} , where D \u2208 D \ud835\udc48 , P \u2208 P \ud835\udc48 and M \u2208 M \ud835\udc48 . Based on the notations mentioned above, we can formulate the problem of multi-center medication recommendation as: given a patient from any hospital R ( \u210e ) with D and P , we aim to train a model on the multi-center EHR V , which can recommend proper medication set M for the user. The important notations used in this paper are listed in Table 1. 4 A Contrastive Pretrain Model with Prompt Tuning for Multi-center Medication Recommendation XX, June 03-05,2018, Woodstock, NY Table 1. Notations used in TEMPT.", "3 THE PROPOSED MODEL": "In this section, we illustrate the overview of TEMPT first. Then, the input representations and medical encoder are introduced. Finally, we detail the procedure of pretraining and prompt tuning.", "3.1 Overview": "The overview of the proposed TEMPT consists of two stages: pretraining and prompt tuning . For recommending suitable medication combinations, we first use the embedding matrix to get a dense representation of each diagnosis and procedure. Then, two stacked transformer layers, named the medical encoder, are used to encode the set of diagnosis and procedure embeddings. To learn the general knowledge from multi-center, the model is trained on V during the pretraining stage, which is illustrated in Section 3.3. Then, we conduct prompt tuning on EHR R ( \u210e ) to adapt to each hospital at the second stage, which is detailed in Section 3.4.", "3.2 Input Representation and Medical Encoder": "In general, the patients have the records of several diagnosis and procedures simultaneously, which causes complicated combinations of the input. To extract the meticulous medical information of patients, we design the input representation and medical encoder. 3.2.1 Input Representation. We assign two learnable embedding matrices to convert the diagnosis and procedure codes into dense vectors. Each row of the matrices refers to one unique code of diagnosis or procedure. Let E \ud835\udc51 \u2208 R | D \ud835\udc48 | \u00d7 \ud835\udc50 denote the embedding matrix for diagnosis and E \ud835\udc5d \u2208 R | P \ud835\udc48 | \u00d7 \ud835\udc50 for the procedure, where \ud835\udc50 represents the embedding dimension. Then, for the diagnosis and procedures set, we can get the embedding set D = [ e 1 \ud835\udc51 , ..., e \ud835\udc56 \ud835\udc51 , ..., e \ud835\udc41 \ud835\udc51 ] for diagnosis and the embedding set P = [ e 1 \ud835\udc5d , ..., e \ud835\udc57 \ud835\udc5d , ..., e \ud835\udc40 \ud835\udc5d ] for procedure, respectively, via the two embedding 5 XX, June 03-05,2018, Woodstock, NY Qidong Liu et al. matrices, where \ud835\udc41 and \ud835\udc40 are the numbers of diagnosis and procedure in the input sets. The two embedding sets are denoted as input representations and will be input to the medical encoder. 3.2.2 Medical Encoder. The medical encoder consists of two transformer architectures [69] to encode the patients' health conditions from the aspect of diagnosis and procedures. To make use of similar medical knowledge of diagnosis and procedure during the two-stage training, two architectures share the same parameters. Each architecture has \ud835\udc3e transformer layers. We denote the parameters of the medical encoder as \u0398 \ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc5f . To capture the relations hidden in the input diagnosis and procedure sets, the transformer layer has two crucial sequential components, i.e., multi-head attention and point-wise feed-forward network. We will take the input diagnosis representation as an example to illustrate it. In multi-head attention, the attention function is the basic unit, which can be defined as follows:  where Q \u2208 R \ud835\udc3f \ud835\udc44 \u00d7 \ud835\udc50 , K \u2208 R \ud835\udc3f \ud835\udc3e \u00d7 \ud835\udc50 and V \u2208 R \ud835\udc3f \ud835\udc49 \u00d7 \ud835\udc50 are the input matrices. \ud835\udc50 is the dimension of matrices. Then, several identical attentions compose the multi-head attention to obtain different views of the medical information:   where W \ud835\udc44 \ud835\udc4e \u2208 R \ud835\udc50 \u00d7( \ud835\udc50 / \ud835\udc34 ) , W \ud835\udc3e \ud835\udc4e \u2208 R \ud835\udc50 \u00d7( \ud835\udc50 / \ud835\udc34 ) , W \ud835\udc49 \ud835\udc4e \u2208 R \ud835\udc50 \u00d7( \ud835\udc50 / \ud835\udc34 ) and W \ud835\udc42 \u2208 R \ud835\udc50 \u00d7 \ud835\udc50 are all the weight matrices. \ud835\udc34 represents the number of heads. | | is the operation of concatenation. With the residual connection, the medium output M from the multi-head attention can be represented as:  Next, a feed-forward network with residual connection is imposed on the medium output M and we can get the output of the transformer layer as:  where W \ud835\udc39 1 \u2208 R \ud835\udc50 \u00d7 \ud835\udc50 , W \ud835\udc39 2 \u2208 R \ud835\udc50 \u00d7 \ud835\udc50 , b \ud835\udc39 1 \u2208 R \ud835\udc50 and b \ud835\udc39 2 \u2208 R \ud835\udc50 are trainable parameters. LayerNorm (\u00b7) represents the standard layer normalization function. The D ( 1 ) is the output of the first transformer layer for the diagnosis or procedure representation. Similarly, we can get the set of diagnosis and procedure representation vectors from \ud835\udc3e transformer layers and they are denoted as D ( \ud835\udc3e ) and P ( \ud835\udc3e ) . To get the comprehensive representation of the whole diagnosis and procedure sets, we only take out the first row of embedding of D ( \ud835\udc3e ) and P ( \ud835\udc3e ) as the medical representation of diagnosis and procedure and we let r \ud835\udc51 and r \ud835\udc5d denote them, respectively. Note that we insert '[CLS]' token to the first position of diagnosis and procedure set as common BERT [11] does, so r \ud835\udc51 and r \ud835\udc5d are actually corresponding to the '[CLS]' token in the transformer. The transformer embeds the information of the whole sequence in such a special token. Another noteworthy point is that we do not apply the positional embedding as the original transformer, because the elements in diagnosis and procedure sets are not strictly ordered. The outputs of the medical encoder, i.e., r \ud835\udc51 and r \ud835\udc5d , represent the extracted information of the patient's diagnosis and procedure, respectively. Then, they are fed into various tasks for pretraining and tuning, which are referred to in the next two subsections. Besides, they are also the major component for medication recommendation while inference. 6 A Contrastive Pretrain Model with Prompt Tuning for Multi-center Medication Recommendation XX, June 03-05,2018, Woodstock, NY Fig. 3. The pretraining stage for TEMPT. Trasformer Layer K Trasformer Layer K ... ... MLP MLP MLP MLP Medical Encoder Mask Prediction Task Contrastive Task diagnosis procedure", "3.3 Pretraining": "During the pretraining stage, we train the model on the whole multi-center records V to learn the general medical knowledge from all hospitals. However, it is challenging to capture such knowledge because of the data sparsity [94]. Besides, the correlation between diagnosis and procedures has rarely been considered before. Therefore, we design two self-supervised tasks for pretraining the model, i.e., Mask Prediction Task and Contrastive Task , as shown in Figure 3. 3.3.1 Mask Prediction Task. The co-occurrence relationship of medication in EHR has been proven as important information for medication recommendation [61]. However, such intra-relationships between diagnosis and procedure sets are ignored at all. Inspired by the masked language model [11], we devise the Mask Prediction Task for the pretraining stage to capture the co-occurrence relationships of diagnosis and procedures. For the mask prediction task, we randomly mask a proportion of diagnosis and procedures in these two input sets. An embedding e \ud835\udc58 \u2208 R \ud835\udc50 is assigned to the special token '[MASK]'. Then, the input representations are converted to D \u2032 = [ e 1 \ud835\udc51 , e \ud835\udc58 , e 3 \ud835\udc51 , ..., e \ud835\udc41 \ud835\udc51 ] and P \u2032 = [ e 1 \ud835\udc5d , e 2 \ud835\udc5d , e \ud835\udc58 , ..., e \ud835\udc40 \ud835\udc5d ] as an example. After inputting D \u2032 \ud835\udc60 and P \u2032 \ud835\udc60 to the medical encoder, as illustrated in the last subsection, we can get the medical representation of diagnosis and procedure, which are denoted as r \u2032 \ud835\udc51 and r \u2032 \ud835\udc5d , respectively. To predict the masked diagnosis and procedures, respectively, we use two multiple-layer perceptron (MLP) and sigmoid to compute the probability of each unit:  7 XX, June 03-05,2018, Woodstock, NY Qidong Liu et al. where \u02c6 y \ud835\udc51 is the predicted masked probability of each diagnosis. Given the true label of the masked diagnosis, denoted as y \ud835\udc51 , we can get the loss function of the mask prediction task for diagnosis:  Similarly, r \u2032 \ud835\udc5d is fed into an MLP \ud835\udc5a 2 (\u00b7) with trainable parameters and a sigmoid layer, as shown in Eq. (5), to get the predicted probability of masked procedures, which is denoted as \u02c6 y \ud835\udc5d . Then, based on the true label of the masked procedures y \ud835\udc5d , we follow the Eq. (6) and get the loss of the mask prediction task for procedures, denoted as L \ud835\udc5d . Finally, the loss of the mask prediction task is the sum of these two losses and it can be written as follows:  3.3.2 Contrastive Task. It is common that the procedures are scheduled according to the diagnosis, so they have a correspondence relation in some way. For example, the procedure Antiarrhythmic is often used to treat the diagnosis of Tachycardias. In order to capture such an inter-relationship between diagnosis and procedures, we propose a novel contrastive task for pretraining to align a pair of input diagnosis and procedures. For the contrastive task, we firstly adopt two MLPs, also named projector, to project the representation r \u2032 \ud835\udc51 and r \u2032 \ud835\udc5d to another underlying space for a better performance [7, 8]:  Let [ u 1 \ud835\udc51 , ..., u \ud835\udc56 \ud835\udc51 , ..., u \ud835\udc35 \ud835\udc51 ] and [ u 1 \ud835\udc5d , ..., u \ud835\udc57 \ud835\udc5d , ..., u \ud835\udc35 \ud835\udc5d ] denote a batch of projected diagnosis and procedure representations, respectively, where \ud835\udc35 is the batch size. Then, to pull the distance of paired diagnosis and procedures in one identical record, we define u \ud835\udc56 \ud835\udc51 and u \ud835\udc57 \ud835\udc5d as a positive pair when \ud835\udc56 = \ud835\udc57 . By comparison, any other procedure representations in this batch become negatives toward this diagnosis u \ud835\udc56 \ud835\udc51 . Therefore, to push the negatives and pull the positives, we can formulate the contrastive loss function for a batch of diagnosis as follows:  where I [ \ud835\udc56 \u2260 \ud835\udc57 ] \u2208 { 0 , 1 } is an indicator function and \ud835\udf0f represents the temperature parameter. \ud835\udc60\ud835\udc56\ud835\udc5a (\u00b7 , \u00b7) is the function to evaluate the similarity between two vectors and the cosine similarity function is used in this paper. In L \ud835\udc51\ud835\udc5d , the projected diagnosis representations are always in the positive pairs and contrasted with the negative procedures, so there exists another contrastive loss to push distance between the procedures and the corresponding negative diagnosis. We exchange the position between diagnosis and procedure in Eq. (9), and get the contrastive loss L \ud835\udc5d\ud835\udc51 for the procedure. Finally, the loss function for the contrastive task can be formulated as:  3.3.3 Objective. Finally, we optimize the model on all hospital records V . The overall objective function combines the loss functions from the two self-supervised tasks mentioned above. Due to the different scales of mask prediction loss and contrastive loss, we impose a scale weight \ud835\udefe when summing these two losses:  8 A Contrastive Pretrain Model with Prompt Tuning for Multi-center Medication Recommendation XX, June 03-05,2018, Woodstock, NY Fig. 4. The prompt tuning stage for TEMPT. MLP Trasformer Layer K Trasformer Layer K ... ... Medical Encoder Updated Frozen diagnosis procedure prompt prompt In practice, we adopt the gradient descent method to train the whole model. The learnt general medical knowledge are embedded into the parameters E \ud835\udc51 , E \ud835\udc5d and \u0398 \ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc5f .", "3.4 Prompt Tuning": "For recommending a proper medication set, we use the pretrained diagnosis and procedure embedding matrices and medical encoder to get the medical representations of diagnosis and procedures. Then, an MLP and a Sigmoid are adopted to output the recommending probability of each medication.  Aimed at the problem of various hospitals with largely different distributions, finetuning the pretrained model and the MLP \ud835\udc5f on each hospital's EHR R ( \u210e ) is an instinctive idea. The loss function can be written as follows:  where \ud835\udc66 is the true label of prescribed medications. Though the finetuning method can alleviate the problem of various distributions, it cannot capture much specific information for each hospital. Besides, finetuning also faces the challenge of catastrophic forgetting [56], because it will modify the pretrained parameters according to the specific distribution of each hospital. Borrowing the idea of prompt tuning from natural language processing [36] and computer vision [24], we propose a hospital-based prompt tuning method to tackle the problems that exist in the finetuning method, as shown in Figure 4. The process of the devised prompt tuning is detailed here. Firstly, we design prompt embeddings for learning the heterogeneous information for each hospital specifically. Let E \ud835\udc51 \u210e \u2208 R \ud835\udc3b \u00d7( \ud835\udc4f \u00d7 \ud835\udc50 ) and 9 XX, June 03-05,2018, Woodstock, NY Qidong Liu et al.", "Algorithm 1 The Overall Optimization Algorithm of TEMPT": "Input: The multi-center records V = {R ( \u210e ) } \ud835\udc3b \u210e = 1 Output: The well-trained medication recommendation models for each hospital Stage 1: Pretraining Stage 1: while not converge do 2: Sample a mini-batch data instances from records of all hospitals V 3: Random mask diagnosis and procedures for each instance 4: Calculate the loss for the mask prediction task via Eq. (7) 5: Calculate the loss for the contrastive task via Eq. (10) 6: Combine the loss of two tasks, take the gradient and update respective parameters { E \ud835\udc51 , E \ud835\udc5d and \u0398 \ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc5f } 7: end while", "Stage 2: Prompt Tuning Stage": "8: for \u210e = 1 to \ud835\udc3b do 9: while not converge do 10: Sample a mini-batch data instances from \u210e -th hospital's records R ( \u210e ) 11: Insert prompt embedding to the input representation 12: Calculate loss via Eq. (13) 13: Take the gradient and update respective parameters { O ( \u210e ) \ud835\udc51 , O ( \u210e ) \ud835\udc5d , \u0398 ( \u210e ) \ud835\udc40\ud835\udc3f\ud835\udc43 } 14: end while 15: end for E \ud835\udc5d \u210e \u2208 R \ud835\udc3b \u00d7( \ud835\udc4f \u00d7 \ud835\udc50 ) denote the prompt embedding matrix for diagnosis and procedure respectively, where \ud835\udc4f \u2208 { 1 , 2 , 3 ... } represents the number of input prompt embeddings. Each row in the two embedding matrices represents the prompt embedding for a unique hospital. We will split the \ud835\udc4f \u00d7 \ud835\udc50 dimensional embedding into \ud835\udc4f fractions, and each fraction is of \ud835\udc50 dimension. Therefore, the prompt embedding set can be represented as O ( \u210e ) = [ o 1 ( \u210e ) , o 2 ( \u210e ) , ..., o \ud835\udc4f ( \u210e ) ] , i.e., each hospital \u210e has its own corresponding parameters of prompt embedding. Then, the prompt embeddings will be inserted at the start of both the input diagnosis and procedure embedding sets. For simplicity, we omit the subscript ( \u210e ) of prompt embeddings to illustrate. As shown in Figure 4, the input representations are converted to D \u2032\u2032 = [ o 1 \ud835\udc51 , ..., o \ud835\udc4f \ud835\udc51 , e 1 \ud835\udc51 , e 2 \ud835\udc51 , ..., e \ud835\udc41 \ud835\udc51 ] and P \u2032\u2032 = [ o 1 \ud835\udc5d , ..., o \ud835\udc4f \ud835\udc5d , e 1 \ud835\udc5d , e 2 \ud835\udc5d , ..., e \ud835\udc40 \ud835\udc5d ] after insertion. Encoded by the medical encoder and decoded by the MLP layer and sigmoid, we can get the recommending probability for each medication, which is written as follows:  where r \u2032\u2032 \ud835\udc51 and r \u2032\u2032 \ud835\udc5d are the medical outputs of the prompted diagnosis and procedure sets, respectively. The parameters of MLP \ud835\udc5f for hospital \u210e can be represented as \u0398 ( \u210e ) \ud835\udc40\ud835\udc3f\ud835\udc43 . To maintain the general medical knowledge learned at the pretraining stage, we freeze the diagnosis and procedure embedding matrix and medical encoder when tuning the model. The prompt embedding is updated to learn the specific information for each hospital. The MLP is updated for precise recommendations. We tune and save the parameters of the prompt embedding and MLP on R ( \u210e ) to recommend for the hospital \u210e . It is worth noting that we conduct inference for hospital \u210e using corresponding parameters of MLP \u0398 ( \u210e ) \ud835\udc40\ud835\udc3f\ud835\udc43 , and prompt embedding O ( \u210e ) \ud835\udc51 and O ( \u210e ) \ud835\udc5d . 10 A Contrastive Pretrain Model with Prompt Tuning for Multi-center Medication Recommendation XX, June 03-05,2018, Woodstock, NY Table 2. The statistics of pre-processed eICU dataset.", "3.5 Optimization and Inference": "Overall Optimization . Based on Section 3.3 and 3.4, we summarize the optimization of TEMPT in Algorithm 1. Firstly, we pretrain the model on multi-center records V (line 2). During the pretraining, we calculate the losses of two self-supervised tasks (line 4-5), and get the pretrained parameters of embedding matrics and medical encoder (line 6). Then, based on the pretrained parameters, we conduct the prompt tuning (line 11) on each hospital. By calculating the loss of recommending medication (line 12), we get the hospital-specific parameters (line 13). We formulate the optimization problem as follows: Inference . When inference, we recommend proper medications according to hospitals. Given a patient from the hospital \u210e , we combine the pretrained parameter { E \ud835\udc51 , E \ud835\udc5d , \u0398 \ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc5f } and the hospital-specific parameters { O ( \u210e ) \ud835\udc51 , O ( \u210e ) \ud835\udc5d , \u0398 ( \u210e ) \ud835\udc40\ud835\udc3f\ud835\udc43 } to get the model. We follow the procedure of prompt tuning to give out the recommending probability as Equation (12) shows. Then, we impose a threshold \ud835\udc61 on the probability, i.e., taking the medication whose probability is greater than \ud835\udc61 into the recommended medication combination.", "4 EXPERIMENTS": "In this section, we will introduce the comprehensive experiments that are conducted on a real-world multi-center medical dataset. The analysis will be given out from the aspects according to the following Research Problems ( RQ ): \u00b7 RQ1: How does the proposed TEMPT perform compared with the state-of-the-art medication recommendation and multi-domain recommendation models? \u00b7 RQ2: How does each component in TEMPT affect the recommending performance? \u00b7 RQ3: Howdoother parameter-efficient finetuning methods perform, compared with the proposed prompt tuning? \u00b7 RQ4: Does the proposed TEMPT have more storage and computation efficiency? \u00b7 RQ5: How do hyper-parameters affect the performance? \u00b7 RQ6: Can TEMPT serve better for small hospitals? 11 XX, June 03-05,2018, Woodstock, NY Qidong Liu et al.", "4.1 Experiment Settings": "4.1.1 Dataset . We conduct experiments on eICU Collaborative Research Database 2 [53], which is a real-world multi-center medical dataset. It collects the data of patients admitted to critical care units in 2014 and 2015 from several hospitals. As far as we know, it is the only public dataset that can be used to research multi-center medication recommendation. We first remove the diagnosis, procedures and medications without valid codes. Then, for easier training and analysis [82], we retain the top 300 medications and top 1,000 procedures according to the count of occurrence. To guarantee enough instances of each hospital for training and testing, we remove the hospital with under 400 records. Finally, we divide the data into train/validation/test by the ratio of 8:1:1 for each hospital. After pre-processing, the statistics of the dataset are listed in Table 2. The eICU [53] dataset was released in 2018, which collects the records of critical care patients from 208 hospitals. Compared with the most used dataset MIMIC\ud835\udc3c \ud835\udc3c \ud835\udc3c [27] in existing medication recommendation works [61, 82, 86], eICU contains data from multi-center and can support our research. As for the number of records, MIMIC\ud835\udc3c \ud835\udc3c \ud835\udc3c has over 5,000 records for a single hospital, while most hospitals have under 1,500 records in eICU. It means that eICU has much more sparsity and is more difficult to welltrain a medication recommendation model. In our work, we use the diagnosis.csv, treatment.csv, medication.csv and patient.csv in the eICU database to get the input diagnosis, procedures, and true prescribed medications. 4.1.2 Baseline Models . We compare the performance of our model with the following three groups of baseline models: (i) medication recommendation models, (ii) LLM-based Models, (iii) multi-domain recommendation models, and (iv) some variant models of the proposed TEMPT. Medication Recommendation Models. Due to most models in this field devised for the multi-visit situation, we modify them as little as possible to fit our setting. It is worth noting that the input of all medication recommendation baselines is the same as ours, i.e., diagnosis and procedure set, except for Leap [87]. The competing baselines include Leap [87], GAMENet [61], G-Bert [60] and COGNet [82]. \u00b7 Leap [87]. Leap models medication recommendation as a problem of sequential decision-making and adopts reinforcement learning to tune the parameters. We implement it as the original paper and only input the model with the diagnosis set. \u00b7 GAMENet [61]. It uses memory networks and graph neural networks to model the co-occurrence and interactions between medications. Since the medication in eICU is encoded by HICL code, the drug-drug interaction graph (DDI) in the original setting is not available. Therefore, we remove the DDI graph and dynamic memory module in GAMENet. \u00b7 G-Bert [60]. It pretrains the medication and diagnosis encoders on all of the single-visit records and then finetunes the model on multi-visit data. However, under single-visit condition, no historical medication can be used as input. To make full use of the information, we substitute the procedures for medications as inputting with diagnosis. \u00b7 COGNet [82]. COGNet considers recommending for the current visit as combining predicting new ones and copying from historical records. We remove the copy module in COGNet to adapt to the instance-based setting. LLM-based Models . Large language models (LLMs) have revolutionized the research pattern of several fields, including recommender systems [81]. To further verify the effectiveness of our TEMPT, we compare two LLM-based baselines. \u00b7 GPT-4. Wornow et al. [79] have explored that LLMs can conduct the zero-shot clinical trial matching task well given patients' EHR. Following this work, we organize the EHR into texts 2 https://eicu-crd.mit.edu/about/eicu/ 12 A Contrastive Pretrain Model with Prompt Tuning for Multi-center Medication Recommendation XX, June 03-05,2018, Woodstock, NY and adopt the powerful GPT-4 model to give out the recommended medications. The input prompt consists of the diagnosis and procedures of patients' current ICU visits, so the prompt template is as follows, ' In this ICU visit, the patient has diagnoses: <DIAG NAME>, ..., <DIAG NAME>; procedures: <PROC NAME>, ..., <PROC NAME>. Then, the patient should be prescribed: '. The '<DIAG NAME>' and '<PROC NAME>' are the medical terms for diagnosis and procedure, respectively. The response from LLMs should be prescribed medication names. \u00b7 TALLRec [3]. Instead of calling the API of closed-source LLMs, TALLRec supervised finetunes LLaMA-7B [68] to better adapt LLMs to fit for recommendation task. We follow the implementation of TALLRec and adopt the same prompt used for the GPT-4 baseline. Multi-domain Recommendation Models. To comprehensively position our proposed TEMPT in current studies, we also compare several state-of-arts multi-domain recommendation models, including MMOE [51], PLE [67] and STAR [63]. The MMOE and PLE are multi-task based models and can be compatible with many neural architectures, so we apply the proposed medical encoder to them. In terms of the STAR baseline, we implement it according to the original paper completely. For better performance, we train these models following the training scheme in [63], i.e., sampling mini-batch instances for each hospital. \u00b7 MMOE [51]. MMOE designs shared expert networks for modeling various tasks and utilizes the gate networks to control the contribution of each expert. As for the implementation, we regard recommendations for hospitals as multiple tasks. However, since we have 80 hospitals, it is impractical to set up the same number of experts due to the time and space costs. Considering the output dimension of the expert network, we set 15 experts and the structure of each expert is the same as the medical encoder mentioned in Section 3. \u00b7 PLE [67]. Based on MMOE, PLE separates the experts into specific experts and shared experts to relieve the problem of detrimental interactions among different tasks. For the same reason as MMOE, we set up one shared expert and 15 specific experts, which means that each specific expert will correspond to 5 to 6 hospitals. \u00b7 STAR [63]. It devises a centered network to learn the shared knowledge and domain-specific networks to capture the specific information of each domain. We implement it as illustrated in the original paper. Variant Models of TEMPT. To illustrate the efficiency between different training schemes, we build up three variant models equipped with the same model structure as the proposed TEMPT: \u00b7 Full-Train. This model is equipped with the embedding table, medical encoder and the MLP to recommend medications. We train it on the whole multi-center dataset. \u00b7 Single-Train. The model has the same architecture as Full-Train. We train it for every hospital on their own records. \u00b7 TEMPT (finetune) This variant has the same pretraining stage as TEMPT, but it tunes all the parameters and is not equipped with the prompt vector. 4.1.3 Implementation Details . Our implementation is based on PyTorch 1.12 and Python 3.9.6. We train and test the codes on an AMD EPYC 7543 platform with RTX 3090 GPUs. For G-Bert and the proposed TEMPT, the pretrained model and prompt-tuned model are selected by the masked diagnosis prediction and medication recommendation performance, respectively. The other baselines all select hyper-parameters based on the validation performance of medication recommendation. We use Adam optimizer and the early-stopping strategy during the training. To ensure the reproducibility of the model and stability of the results, we repeatedly divide the dataset 5 times using the random seeds in { 42 , 43 , 44 , 45 , 46 } according to Section 4.1 and run the model also with these seeds. The reported results in this paper are all averaged on five runs. For recommendation, we use the threshold \ud835\udc61 = 0 . 3, because this value performs better in total. 13 XX, June 03-05,2018, Woodstock, NY Qidong Liu et al. However, due to the high costs of calling the API of GPT-4 and the extreme training consumption of finetuning LLaMA, we do not conduct repetitive experiments for the LLM-based baselines. The implementation code is available to ease reproducibility 3 . 4.1.4 Evaluation Metrics . We follow the previous works [60, 61, 82, 86] to use the Jaccard Similarity Score (denoted as Jaccard), Average F1 Score (denoted as F1) and Precision-Recall AUC (denoted as PRAUC) as the evaluation metrics. The reported metrics in the following are averaged over all of the records and hospitals. \u00b7 PRAUC , which represents precision-recall area under curve. As mentioned in Section 3, we can get the recommending probability for each medication given a patient record. If the threshold is \ud835\udc61 , then the recommended medications for record \ud835\udc56 are \u02c6 M \ud835\udc56 = { \ud835\udc5a \ud835\udc57 | \u02c6 \ud835\udc66 \ud835\udc57 > \ud835\udc61 } . Based on the recommended medication set, we get the definition of precision and recall:   Wesumthe products of precision and recall under all possible thresholds and then get the PRAUC:  \u00b7 Jaccard , which evaluates the similarity between recommended and true medication sets. It is defined as follows:  \u00b7 F1 , which combines the precision and recall. It is defined as:", "4.2 Overall Performance Comparison (RQ1)": "As Table 3 shows, we repeatedly conduct five experiments with different random seeds on each model and report the averaged performance and standard deviation. Overall, the proposed TEMPT outperforms all competing baselines on three metrics. Besides, we can get several more detailed conclusions from the results as follows: \u00b7 Observing the performance of compared medication recommendation models, they are much worse than the MDR models and the proposed TEMPT. This is because the existing medication recommendation models ignore the situation of multi-center. The problem of the significant difference between hospitals results in the difficulty of training a unified model for all hospitals. \u00b7 Among the medication recommendation models, Leap performs inferior to others, because it only uses the information of diagnosis. It is also worth noting that G-Bert is a pretrain-based model and outperforms others a little, which illustrates that inner medical knowledge can benefit recommending medications for various hospitals. 3 https://github.com/Applied-Machine-Learning-Lab/TEMPT 14 A Contrastive Pretrain Model with Prompt Tuning for Multi-center Medication Recommendation XX, June 03-05,2018, Woodstock, NY Table 3. Overall performance on eICU dataset. The boldface refers to the highest score and the underline indicates the best result of the baselines. ' * ' indicates the statistically significant improvements ( i.e., twosided t-test with \ud835\udc5d < 0 . 05 ) over the best baseline. '-' means that the PRAUC is not applicable to LLM-based baselines, because they do not output the probability of each medication. \u00b7 From the results, we can find that both GPT-4 and TALLRec underperform the proposed TEMPT. The results align with an existing research paper [10], which indicates that LLMs are better at the zero-shot or few-shot setting while inferior to traditional recommendation models with sufficient training data. Furthermore, the general LLMs, e.g., GPT-4 and LLaMA, do not have enough medical knowledge, leading to unsatisfied performance for medication recommendation, a typical medical application. \u00b7 The experimental results also show that MDR models can get the second-best performance. We believe that it is because MDR can learn various distributions of hospitals, which shows the importance of researching the multi-center situation. However, MMOE and PLE still cannot overrun the proposed TEMPT, because they are not fit for too many domains. In general, they need the same number of expert networks with domains, but it is impossible when we have 80 hospitals. Besides, the PLE even has to share a specific expert with some of the hospitals due to too many hospitals, so the performance degrades much. \u00b7 As for STAR, though it can be applied to conditions with many domains, it cannot be compatible with more advanced neural architectures, which limits its performance. \u00b7 Single-Train and Full-Train only learn the specific knowledge of each hospital and the general knowledge of all hospitals, respectively. The results show that both of them cannot satisfy the multi-center situation. \u00b7 A variation of the TEMPT uses common finetuning rather than the proposed prompt, denoted as TEMPT (finetune). It outperforms other medication recommendation and MDR models, which illustrates two-fold facts: (i) pretrain-finetune is a more efficient scheme for multi-center medication recommendation. (ii) the proposed two self-supervised tasks help the model capture useful general medical information. Furthermore, the proposed TEMPT is better than the variation, because it can obtain more specific information about each hospital and relieve the catastrophic forgetting issue via prompt tuning. In summary, we can conclude that the proposed TEMPT outperforms the competing state-of-the-art medication recommendation and MDR models as the answer to RQ1 . 15 XX, June 03-05,2018, Woodstock, NY Qidong Liu et al. Table 4. Ablation study for different components of TEMPT on eICU. The boldface refers to the highest score and the underline indicates the best result of the competitors. ' * ' indicates the statistically significant improvements ( i.e., two-sided t-test with \ud835\udc5d < 0 . 05 ) over the competitor.", "4.3 Ablation Study (RQ2)": "To respond to the RQ2 , we conduct ablation experiments on eICU and the results are shown in Table 4. \u00b7 At first, we analyze the effectiveness of the shared medical encoder and separate MLP. TEMPT-DE represents we use separate medical encoders for diagnosis and procedure, respectively. TEMPTSM is that we use the same MLPs for the pre-training tasks. From the results, we can conclude that using the shared medical encoder and separate MLPs can get the best performance. This shows that it is better to consider diagnosis and procedure as two independent tasks and use the shared medical encoder to learn similar medical knowledge. \u00b7 Then, we discuss how the pretrain tasks affect the performance of TEMPT. TEMPT (finetune) w/o MP represents that we finetune the model only pretrained by contrastive task, while TEMPT (finetune) w/o CL for the model only pretrained by mask prediction task. TEMPT w/o MP and TEMPT w/o CL represent the proposed model without mask prediction and contrastive selfsupervised task. The experimental results show that removing any of the pretrain tasks will cause a performance decrease in both finetuning and prompt tuning, which illustrates that the two pretrain tasks can help the model learn general medical knowledge. More performance decreases on the condition without mask prediction, because of the problem of false negatives [58] in our view. We will explore it in future work. \u00b7 Also, the recommending performance of TEMPT falls when we use the finetuning method rather than the proposed hospital-based prompt tuning. This phenomenon illustrates that prompt tuning relieves the problem of catastrophic forgetting and captures the specific information of each hospital's distribution.", "4.4 Analysis for Parameter-efficient Finetuning (RQ3)": "There are several parameter-efficient finetuning (PEFT) methods that can be adapted to our TEMPT. To verify the effectiveness of the proposed prompt tuning (RQ3), we compare four types of popular PEFT methods and show their performance in Table 5. \u00b7 TEMPT (Adapter) signifies that we employ the same pretraining process as TEMPT, but use the adapter [22] for finetuning. The adapter is integrated into each transformer-based medical encoder of our TEMPT model. The performance comparison reveals that prompt tuning can outperform the adapter. This could be attributed to the fact that the adapter often necessitates more training data [2], but in our multi-center settings, most hospitals have no more than 2,000 samples. 16 A Contrastive Pretrain Model with Prompt Tuning for Multi-center Medication Recommendation XX, June 03-05,2018, Woodstock, NY Table 5. Experiments for comparing different parameter-efficient finetuning methods. The boldface refers to the highest score and the underline indicates the best result of the competitors. ' * ' indicates the statistically significant improvements ( i.e., two-sided t-test with \ud835\udc5d < 0 . 05 ) over the competitor. Fig. 5. The computation and storage cost of each model. FT ST TF TE 0 5 10 15 20 25 30 # (s / epoch) Training Time (a) Computation cost FT ST TF TE 0 50 100 150 200 # (million) parameters Model Size (b) Storage cost. \u00b7 Regarding TEMPT (LoRA) , we implement it using the same pretraining process, but finetune the model by pairing each linear layer in the transformer-based medical encoder with a set of low-rank metrics [23]. The results in Table 5 suggest that LoRA is less effective than our proposed prompt tuning method. LoRA is a type of reparameterization method [37], which implies that it modifies the parameters of the linear layers in the original model with a smaller update step. However, updating too many parameters of the original model may lead to the issue of catastrophic forgetting [56], resulting in suboptimal performance, akin to full finetuning. \u00b7 Prefix tuning introduces prompt vectors into each transformer layer, whereas prompt tuning only inserts them into the input layer. In the experiments, TEMPT (Prefix) denotes that we employ the same pretraining process as TEMPT, but use prefix tuning for finetuning. Our results indicate that our prompt method outperforms TEMPT (Prefix) . This could be because TEMPT (Prefix) has too many prompt parameters to learn, leading to underfitting. This observation aligns with our findings that the optimal prompt number is 2 in hyper-parameter experiments. \u00b7 We also compare with P-Tuning [48]. TEMPT (P-Tuning-1) inputs the pseudo prompts to the prompt encoder, and TEMPT (P-Tuning-2) takes the hospital ID as the input of the prompt encoder. From the results, we find that both these variants achieve inferior performance than TEMPT. It is caused by the difficulty of learning a good prompt encoder, while direct prompt insertion of TEMPT is more effective in learning hospital-specific information. Besides, these three prompt tuning variants surpass TEMPT (finetune) , indicating the effectiveness of the application of prompt tuning for multi-center medication recommendation. 17 XX, June 03-05,2018, Woodstock, NY Qidong Liu et al. 0 5e-4 1e-3 5e-3 1e-2 5e-2 1e-1 0.52 0.53 0.54 0.55 PRAUC (a) Weight of contrastive loss \ud835\udefe Fig. 6. The performance of the model with various values of three hyper-parameters, i.e., the weight of contrastive loss \ud835\udefe , the temperature parameter \ud835\udf0f and the number of prompt \ud835\udc4f . 0.4 0.6 0.8 1 1.2 0.5450 0.5455 0.5460 0.5465 0.5470 PRAUC (b) Temperature parameter \ud835\udf0f 1 2 3 4 5 0.540 0.542 0.544 0.546 0.548 PRAUC (c) Number of prompt \ud835\udc4f", "4.5 Efficiency Analysis (RQ4)": "In this subsection, we aim to explore the computation and storage efficiency of the proposed TEMPT. As shown in Figure 5, we compare the training time and model size between Full-Train (FT), SingleTrain (ST), TEMPT (finetune) (TF) and TEMPT (TE). Full-Train consumes much more training time per epoch than the other three models, because it is trained on the whole multi-center data. Single-Train, TEMPT (finetune) and TEMPT are all trained on the records in a single hospital, but TEMPT is faster than the others. This is because TEMPT only needs to tune the prompt embedding and MLP in the model, while the other two models have to tune the whole model. As for the model size, Full-Train is the smallest because it serves all hospitals with one model, but it also causes poor performance. The model size of TEMPT is much smaller than the others, because all hospitals share one medical encoder and embedding layers, while the other two models have specific parameters of the whole for each hospital. To sum up, the proposed TEMPT has relatively higher computation and storage efficiency.", "4.6 Hyper-parameter Analysis (RQ5)": "The proposed TEMPT owns three important hyper-parameters, i.e., the weight of contrastive loss \ud835\udefe , the temperature parameter \ud835\udf0f and the number of prompt \ud835\udc4f . To explore how they affect the performance of the model, we conduct experiments on various values of them. As shown in Figure 6, the performance increases consistently as \ud835\udefe varies from 0 to 1e-2, which illustrates that the contrastive task helps the model learn the inter-relationship between two inputs and thus improve the quality of representations. However, due to the larger magnitude of the contrastive loss, a continuous increase of \ud835\udefe degrades the performance. As for the temperature parameter, the recommending performance first rises then falls when \ud835\udf0f is ranged from 0 . 4 to 1 . 2, because the smaller temperature can promote the uniformity of representation distribution, but the too small value will cause the difficulty of optimization [70]. Besides, we seek the most proper number of prompts. The result shows that two prompts benefit the proposed model the most. When \ud835\udc4f = 1, the performance is slightly better than without a prompt, because only one prompt vector is not enough to learn the specific information of each hospital. By comparison, more than two prompts are abundant for the model and cause a decrease in performance. To ease the reproduction of all the experimental results in this paper, we also show all the hyper-parameters used in our experiments in Table 6.", "4.7 Analysis for Hospitals of Different Scales (RQ6)": "One important reason to research multi-center medication recommendation is to assist small hospitals, which is much more meaningful for the healthcare system. As we know, small hospitals 18 A Contrastive Pretrain Model with Prompt Tuning for Multi-center Medication Recommendation XX, June 03-05,2018, Woodstock, NY Table 6. The hyper-parameter settings in our experiments. Table 7. Performance comparison on different scales of hospitals. We divide the hospitals into three groups according to the amount of records, i.e., small, medium and large hospitals. 'Impr' indicates the ratio of performance improvement of TEMPT over Single-Train. often own a small number of medical records, which easily causes the problem of underfitting. Therefore, we experiment with the proposed models and competing baselines on different sizes of hospitals. We divide the hospitals into three groups: small ( \ud835\udc5f \u2264 1000), medium (1000 < \ud835\udc5f \u2264 2000) and large ( \ud835\udc5f > 2000), where \ud835\udc5f is the record count of each hospital. It is worth noting that the train-test split metric is the same as before. For evaluation, we test the models on each single hospital and then average the metric values based on the hospital groups. As Table 7 shows, TEMPT outperforms other baselines overall and all three separated groups, which illustrates the efficiency of our TEMPT. In detail, we carefully get four conclusions as follows as the response to the RQ6 : \u00b7 We find that Full-Train not only performs worse overall, but also its accuracy in small hospitals is much lower. This phenomenon illustrates that the unified model tends to fit the most records from the large hospital and the small hospital records are ignored during training. \u00b7 Compared with Full-Train, Single-Train and most medication recommendation models can elevate performance in small hospitals. Single-Train specifies one model for each hospital and thus can 19 XX, June 03-05,2018, Woodstock, NY Qidong Liu et al. benefit small hospitals. Other medication recommendation models elevate the performance by more fabricated architectural designs. \u00b7 MDR baselines even get better performance in small hospitals than large hospitals, which is different from Full-Train and Single-Train. It shows that capturing correlations between various hospitals can help alleviate the data scarcity problem for small hospitals. \u00b7 TEMPT and its variations outperform the other baselines on all hospital groups, because they have learned general medical knowledge that can enhance the model. It is worth noting that freezing parameters when finetuning is better than no freezing in small hospitals, which shows that the problem of catastrophic forgetting leads to a decrease in performance. In contrast, large hospitals need to fit their specific distributions more, so the performance is worse when freezing. TEMPTalleviates the problem of catastrophic forgetting and various distributions simultaneously, so it can get an increase in all three groups of hospitals. \u00b7 It is worth noting that, MDR baselines and TEMPT achieve more accuracy promotion on small hospitals, compared with the other models that do not consider multi-center. Typically, we can find that TEMPT surpasses Single-Train 9 . 20% in the small hospital group, while the improvement is only 6 . 42% for large hospitals. On the one hand, such a phenomenon shows that small hospitals suffer from the problem of little data. On the other hand, it can demonstrate the necessity to formulate the problem as a multi-center setting to benefit small hospitals, which is important for the whole healthcare system. By analysis, the proposed TEMPT serves better for small hospitals than existing methods, due to its better combination of homogeneous and heterogeneous information between all hospitals.", "4.8 Case Study": "In this section, we aim to analyze the performance of several hospitals in detail. As shown in Figure 7, we randomly sample 8 hospitals, and compare the performance of the proposed TEMPT and its variants for further analysis. The results show that the proposed TEMPT often outperforms the variant models in each hospital. Besides, we find that our TEMPT and TEMPT (finetune) exceed the Single-Train much more on hospital 92, 146 and 181. According to the definition in Section 4.6, they are all small hospitals. It can be concluded that the pretrain-finetune scheme can benefit small hospitals by learning general medical knowledge from all records. As for Full-Train, we find that its performance is unstable extremely, because it only learns the distribution of all records. If the hospital's distribution is similar to the total, it can perform well and vice versa. The TEMPT consistently outperforms its finetuning variant, which illustrates that the proposed hospital-based prompt can better capture hospital-specific information.", "5 RELATED WORKS": "In this section, we will introduce recent research studies on the aspects of medication recommendation, multi-domain recommendation and pretrain in the recommendation.", "5.1 Medication Recommendation": "In recent years, with the advancements in recommender systems [31, 38, 40, 41, 43, 44, 47, 50, 72, 89], medication recommendation has attracted much attention due to its practical value [1]. It can assist doctors by giving advice on prescriptions. For most existing works, medication recommendation is formulated as recommending a set of medications given the diagnosis and procedures of the patient. Such a process is similar to the prescription procedure in the real world. The treatments, including medications and procedures, are decided by the diagnosis of patients. For example, if one patient gets 'heart disease', the doctor may prescribe the medication 'Angiotensin' and 'surgery' procedure. At the same time, 'Analgesics' may be prescribed to assist 'surgery', which indicates 20 A Contrastive Pretrain Model with Prompt Tuning for Multi-center Medication Recommendation XX, June 03-05,2018, Woodstock, NY Fig. 7. The performance on randomly sampled 8 hospitals. 'rec' represents the record number of the hospital and 'med' is the average number of prescribed medications. FT, ST, TF and TE refer to Full-Train, Single-Train, TEMPT (finetune) and TEMPT, respectively. The id is the identifier of each hospital in the original eICU database. FT ST TF TE 0.4 0.5 0.6 PRAUC rec=541 med=19.75 (a) id=92 FT ST TF TE 0.4 0.5 0.6 PRAUC rec=594 med=20.69 (b) id=146 FT ST TF TE 0.4 0.5 0.6 PRAUC rec=1057 med=21.72 (c) id=157 FT ST TF TE 0.4 0.5 0.6 PRAUC rec=1869 med=23.72 (d) id=176 FT ST TF TE 0.4 0.5 0.6 PRAUC rec=791 med=29.60 (e) id=181 FT ST TF TE 0.4 0.5 0.6 PRAUC rec=2638 med=22.27 (f) id=199 FT ST TF TE 0.4 0.5 0.6 PRAUC rec=2027 med=27.79 (g) id=252 FT ST TF TE 0.4 0.5 0.6 PRAUC rec=4403 med=23.28 (h) id=264 FT ST TF TE 0.4 0.5 0.6 PRAUC rec=3116 med=19.22 (i) id=443 the medications also need to match the procedure. Therefore, medication recommendation models are conditioned on diagnosis and procedures. Most existing works can be categorized into two groups: instance-based and longitudinal models. Instance-based Models. This thread of models [65, 87, 88] recommend medication sets only based on the patient's current conditions. For instance, Leap [87] models the medication recommendation as a sequential decision-making problem and only takes the diagnosis set as input. It first captures the relationships between several diseases. Recently, Tan et al. [65] propose a symptom-based model. This model only uses the symptom and medication set and recommends medication by 21 XX, June 03-05,2018, Woodstock, NY Qidong Liu et al. comparing the representations of these two sets. Besides, it considers some practical constraints when recommending drug sets, such as safety and size. MedRec [88] introduces a knowledge graph to extract the relationship between symptoms and medication sets. Our work also belongs to this category. Longitudinal Models. Longitudinal models [28, 60, 61, 71, 82, 85, 86, 93] make use of patient's historical records to get the medical information. For example, DMNC [28] adopts a memory network to model the intra-view and inter-view relations of a patient's historical diagnosis and procedures. Metacare++ [66] adopts the meta-learning technique to capture the relationships between patients' clinical visits for cold-start diagnosis prediction. Shang et al. [61] firstly use the graph to capture patients' longitudinal information and drug-drug interactions, which is important for drug safety. In detail, a novel dynamic memory mechanism is devised to utilize the information contained in the graph. To further decrease the drug-drug interaction rate, Yang et al. [86] propose a controllable loss. Besides, for a more precise recommendation, SafeDrug also constructs the drug molecule graph and utilizes the message-passing neural network to extract the relationships between medications further. MICRON [85] and COGNet [82] consider the recommended medication set changed and copied from the patient's historical medication set. In particular, MICRON identifies the medication change between two successive visits and recommends a medication set based on the last visit. Compared to MICRON, COGNet captures the relationship between current and all historical records, and recommends by copying medications from previous prescriptions. It is worth noting that G-Bert [60] also applies the pretrain technique to medication recommendation. In detail, it pretrains a visit encoder with single visit records and finetunes the encoder for modeling multi-visit records. Though it also proposes to pretrain for a single visit, it has two-fold differences from the proposed TEMPT: (i) G-Bert is designed for a single hospital. (ii) G-Bert is a longitudinal model, which is not for instance-based conditions especially. Recently, the advancements in large language models have revolutionized the research in many intelligent healthcare applications [33, 35, 45, 73, 83, 84, 91, 92], including medication recommendation. For example, Wornow et al. [79] propose to construct proper prompts to motivate LLMs for clinical tasks. Besides, Liu et al. [46] modify the output layer of LLMs to fit medication recommendation and further propose a novel distillation method to transfer the semantics of LLMs to a small model. However, the previous works mentioned above aim to recommend medications for doctors from one hospital with abundant medical records, which ignores those small hospitals. To our knowledge, this paper is the first work to focus on multi-center medication recommendations.", "5.2 Multi-domain Recommendation": "Recently, some multi-domain recommendation research studies have emerged [13-17, 25, 32, 34, 7476, 89], aiming to recommend proper items for several domains simultaneously. There are two lines of work in this field, i.e., multi-task learning works and multi-domain CTR prediction works. The first category regards each domain as a task and thus can learn the relationships between various domains. Among these works, MMOE [51] and PLE [67] are the two most popular multitask learning methods. MMOE designs several expert networks to model the specific and shared information of each task, and adopts gate networks to control the contributions of experts. Compared with MMOE, PLE designs specific and shared expert networks to better model task inter-relations. The shortcoming of this line of work is that they do not fit too many domains. The other category is closely related to multi-domain CTR prediction. For example, DisenCDR [6] extracts domainspecific and domain-shared user representations to share similar preferences across various domains. Besides, to alleviate the problem of entanglement of these two types of representation, it also proposes an exclusive regularizer to split them. Similar to DisenCDR, SAR-Net [62] captures the 22 A Contrastive Pretrain Model with Prompt Tuning for Multi-center Medication Recommendation XX, June 03-05,2018, Woodstock, NY user's cross-scenario preference by two designed attention modules and utilizes multi-expert networks to model the information of different scenarios. However, DisenCDR and SAR-Net both require users to have interactions in several domains simultaneously, which is different from the setting of multi-center medication recommendations in this paper. STAR [63] devises the domainspecific and domain-shared feed-forward networks to recommend for users in various scenarios. Since it only inputs features of domain and item, it can be adapted to the problem in this paper.", "5.3 Pretrain in Recommendation": "Pretrained large model has prevailed in computer vision [12, 49] and natural language processing [5, 11, 64] and other fields [90], but it is under different conditions in the field of recommender system (RS). RS is founded on the non-semantic id, which hinders pretraining a universal model that can fit many scenarios. Therefore, existing pretrain works for recommendation are developed from two aspects. One aspect [9, 18, 21] uses semantic information of items, such as review, description, etc., to pretrain a large semantic model. These works can benefit from the advancement in CV and NLP. For example, UniSRec [21] proposes to learn universal item representations by the textual descriptions of each item. M6 [9] transforms the recommendation task to a pure language understanding or generation problem, and pretrain a large model that can be adapted to many downstream recommendation tasks by finetuning. However, such textual information does not exist in most recommendation situations, so the other line of work only utilizes the item's identity to be more practical. These works mainly adopt the pretrain technique to ease the data sparsity problem and get high-quality embeddings for general recommendation models. For example, S3Rec [94] adopts several self-supervised tasks to inject attribute and subsequence correlations into item embeddings. Hao et al. [20] and Wu et al. [80] propose to pretrain the graph-based recommendation model to enhance the long-tail and cold-start embeddings, respectively. Our work belongs to the latter aspect, which only uses the identity of medication. Prompt, as an efficient method, is widely used to tune the large pretrained model [42]. Li et al. [36] propose a continuous prefix prompt to tune the GPT-2 [54] for some NLP tasks. Then, VPT [24] shows that the idea of continuous prompt also works for large vision pretrained models. It sets different learnable prompt vectors when tuning for various downstream vision tasks. Recently, some researchers have also adopted the prompt to recommender systems, such as PEPLER [30] and P5 [18]. Similar to M6, P5 [18] first transfers all data of recommendation to a natural language format and then pretrains a unified model. Based on the pretrained model, it proposes an instruction-based prompt to facilitate the power of zero-shot recommendation. However, P5 is a natural languagebased model, which has many limitations in the general recommendation field. PEPLER [30] is the first to explore the prompt for the typical recommendation. PEPLER devises a prompt generator based on the user's profile and only tunes it after pretraining a general sequential recommendation model. Different from PEPLER, the proposed prompt in TEMPT is devised based on various hospitals and aims at benefiting the recommendation for multi-center. As far as we know, our work is one of the pioneers to adopt prompt to a pure id-based recommendation model, which gives light to this field.", "6 CONCLUSION": "In this paper, we propose a contrastive pretrain model with prompt tuning (TEPMT) for a more practical scenario, i.e., multi-center medication recommendation. Specifically, we first design a transformer-based medical encoder to extract medical information from the input diagnosis and procedure sets. Then, the TEMPT is equipped with two self-supervised tasks to learn general medical knowledge, i.e., mask prediction task and contrastive task for intra- and inter-relationships, respectively. Besides, we design a novel hospital-based prompt tuning to avoid the catastrophic 23 XX, June 03-05,2018, Woodstock, NY Qidong Liu et al. forgetting problem of general finetuning and better fit each hospital's various distributions. To validate our model, we conduct extensive experiments on a multi-center medical dataset, i.e., eICU. The experimental results show that the proposed model outperforms existing medication recommendation models and multi-domain recommendation models. Primarily, we find that the proposed TEMPT is also beneficial for the hospitals on a small scale compared with all baseline models, which is important to the healthcare system. Furthermore, since we only need to train a small proportion of parameters while prompt tuning, the storage and computation costs are much smaller than the general finetuning strategy. Despite the superior performance in multi-center medication recommendation, one possible limitation is that the proposed TEMPT does not take drug-drug interaction into consideration. We leave it to the future work. Besides, due to the privacy requirements of patients and hospitals, the medical data from multi-center cannot be accessed easily in the real world. Therefore, in the future, we will focus on combining federated learning with the proposed pretrain model. Furthermore, most existing research studies only adopt the diagnosis and procedure identity to model the collaborative information, while ignoring patient's personal profiles, such as allergies and age, and semantic relationships between diagnosis, procedure and medications. Thus, exploring how to utilize those informative features by a powerful large language model for medication recommendation is also an interesting and promising direction.", "7 ACKNOWLEDGEMENTS": "This research was partially supported by Research Impact Fund (No.R1015-23), APRC - CityU New Research Initiatives (No.9610565, Start-up Grant for New Faculty of CityU), CityU - HKIDS Early Career Research Grant (No.9360163), Hong Kong ITC Innovation and Technology Fund Midstream Research Programme for Universities Project (No.ITS/034/22MS), Hong Kong Environmental and Conservation Fund (No. 88/2022), and SIRG - CityU Strategic Interdisciplinary Research Grant (No.7020046), Huawei (Huawei Innovation Research Program), Tencent (CCF-Tencent Open Fund, Tencent Rhino-Bird Focused Research Program), Ant Group (CCF-Ant Research Fund, Ant Group Research Fund), Alibaba (CCF-Alimama Tech Kangaroo Fund No. 2024002), CCF-BaiChuan-Ebtech Foundation Model Fund, and Kuaishou, National Natural Science Foundation of China (No.62192781, No.62177038, No.62293551, No.62277042, No.62137002, No.61721002, No.61937001, No.62377038), Project of China Knowledge Centre for Engineering Science and Technology, 'LENOVO-XJTU' Intelligent Industry Joint Laboratory Project.", "REFERENCES": "[1] Zafar Ali, Yi Huang, Irfan Ullah, Junlan Feng, Chao Deng, Nimbeshaho Thierry, Asad Khan, Asim Ullah Jan, Xiaoli Shen, Wu Rui, et al. 2023. Deep learning for medication recommendation: a systematic survey. Data Intelligence 5, 2 (2023), 303-354. [2] Parham Abed Azad and Hamid Beigy. 2024. Multi-BERT: Leveraging Adapters and Prompt Tuning for Low-Resource Multi-Domain Adaptation. arXiv preprint arXiv:2404.02335 (2024). [3] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. Tallrec: An effective and efficient tuning framework to align large language model with recommendation. In Proceedings of the 17th ACM Conference on Recommender Systems . 1007-1014. [4] Nick Barber, M Rawlins, and B Dean Franklin. 2003. Reducing prescribing error: competence, control, and culture. BMJ Quality & Safety 12, suppl 1 (2003), i29-i32. [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877-1901. [6] Jiangxia Cao, Xixun Lin, Xin Cong, Jing Ya, Tingwen Liu, and Bin Wang. 2022. DisenCDR: Learning Disentangled Representations for Cross-Domain Recommendation. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval . 267-277. [7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A simple framework for contrastive learning of visual representations. In International conference on machine learning . PMLR, 1597-1607. 24 A Contrastive Pretrain Model with Prompt Tuning for Multi-center Medication Recommendation XX, June 03-05,2018, Woodstock, NY 25 XX, June 03-05,2018, Woodstock, NY Qidong Liu et al. 26 A Contrastive Pretrain Model with Prompt Tuning for Multi-center Medication Recommendation 27 XX, June 03-05,2018, Woodstock, NY Qidong Liu et al. [90] Zijian Zhang, Xiangyu Zhao, Qidong Liu, Chunxu Zhang, Qian Ma, Wanyu Wang, Hongwei Zhao, Yiqi Wang, and Zitao Liu. 2023. PromptST: Prompt-Enhanced Spatio-Temporal Multi-Attribute Prediction. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management . 3195-3205. 28 A Contrastive Pretrain Model with Prompt Tuning for Multi-center Medication Recommendation XX, June 03-05,2018, Woodstock, NY [91] Zhi Zheng, Zhaopeng Qiu, Hui Xiong, Xian Wu, Tong Xu, Enhong Chen, and Xiangyu Zhao. 2022. Ddr: Dialogue based doctor recommendation for online medical service. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 4592-4600. [92] Zhi Zheng, Zhaopeng Qiu, Tong Xu, Xian Wu, Xiangyu Zhao, Enhong Chen, and Hui Xiong. 2022. CBR: context bias aware recommendation for debiasing user modeling and click prediction. In Proceedings of the ACM Web Conference 2022 . 2268-2276. [93] Zhi Zheng, Chao Wang, Tong Xu, Dazhong Shen, Penggang Qin, Xiangyu Zhao, Baoxing Huai, Xian Wu, and Enhong Chen. 2022. Interaction-aware Drug Package Recommendation via Policy Gradient. ACM Transactions on Information Systems (TOIS) (2022). [94] Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, and Ji-Rong Wen. 2020. S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management . 1893-1902. Received xx xxxx; revised xx xxxx; accepted xx xxxx 29"}
