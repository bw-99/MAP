{"title": "\"Some other poor soul's problems\": a peer recommendation intervention for health-related social support", "authors": "Zachary Levonian; Matthew Zent; Ngan Nguyen; Matthew Mcnamara; Loren Terveen; Svetlana Yarosh", "pub_date": "2022-09", "abstract": "Online health communities (OHCs) offer the promise of connecting with supportive peers. Forming these connections first requires finding relevant peers-a process that can be time-consuming. Peer recommendation systems are a computational approach to make finding peers easier during a health journey. By encouraging OHC users to alter their online social networks, peer recommendations could increase available support. But these benefits are hypothetical and based on mixed, observational evidence. To experimentally evaluate the effect of peer recommendations, we conceptualize these systems as health interventions designed to increase specific beneficial connection behaviors. In this paper, we designed a peer recommendation intervention to increase two behaviors: reading about peer experiences and interacting with peers. We conducted an initial feasibility assessment of this intervention by conducting a 12-week field study in which 79 users of CaringBridge.org received weekly peer recommendations via email. Our results support the usefulness and demand for peer recommendation and suggest benefits to evaluating larger peer recommendation interventions. Our contributions include practical guidance on the development and evaluation of peer recommendation interventions for OHCs.", "sections": [{"heading": "INTRODUCTION", "text": "Social support helps people in health crises cope with stressful circumstances. Access to emotional, informational, and instrumental support is associated with increased quality of life [5], improved psychosocial health [118], and physical health [46]. Support from peers-people who have had similar health experiences-is particularly useful [1,100]. However, a person's existing offline support community may lack peers [113,115]. Online communities provide a place for people to support each other in ways that their existing offline support communities cannot by offering opportunities to connect with peers. While health support is exchanged online on diverse platforms [11,15,99], online health communities (OHCs) are specifically designed for health-related discussion and support [1,133].\nEven in OHCs, however, finding supportive peers can be time-consuming [25,32]. Algorithmic matching systems could enable design features that help OHC users to find peers, but existing approaches are limited or have remained entirely theoretical [41,63]. For example, a common approach to peer finding requires a user to explicitly search or filter for people or topics of interest, a process that support seekers and providers may find labor-intensive and discouraging in healthrelated contexts [32,88]. Recommendation systems for peer finding can incorporate the user's past behaviors as an implicit signal to identify potential matches-people with valuable similar experiences [130]. However, no existing recommendation system has been evaluated for peer matching in OHCs: no experimental evidence links the availability of peer recommendations with hypothesized increases in beneficial peer connection behaviors.\nThe goal of this paper is to explore the idea of connecting peers in online health communities for mutual support. We accomplish this by designing, developing, and evaluating an email-based peer recommendation system for users of the online community CaringBridge. Despite recent research arguing for the potential utility of peer recommendation [17,25,32,41,63,64,71,81,82,130], no study yet describes use of peer recommendation in practice. We conducted a feasibility study to assess the usefulness of and demand for peer recommendation. Feasibility studies are designed to determine if an intervention should be evaluated in a larger or more comprehensive study [12]. We argue that peer recommendation should be conceptualized as a health intervention that can change specific user behaviors-behaviors that are linked to psychosocial or physical health. Our proposed intervention is designed to increase two behaviors: reading about peers' health experiences and increasing interaction among peers. In the rest of this introduction, we summarize the proposed intervention (sec. 1.1), its evaluation in a 12-week field study (sec. 1.2), and the encouraging findings for our core contribution: evidence for the feasibility of using recommendation to connect peers in OHCs (sec. 1.3).", "publication_ref": ["b4", "b45", "b0", "b99", "b112", "b114", "b10", "b14", "b98", "b0", "b132", "b24", "b31", "b40", "b62", "b31", "b87", "b129", "b16", "b24", "b31", "b40", "b62", "b63", "b70", "b80", "b81", "b129", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "What is the proposed intervention?", "text": "The proposed intervention is displaying recommended peers to current OHC users. This design intervention has two aspects: an interface that displays details about recommendations and an algorithm that selects peers to recommend in the interface. The interface shows \"profiles\"-summaries of each individual peer being recommended [42]. By including previews of and links to a user's recent activity within the OHC, we enable prospective peers to evaluate the relevance of the recommended user [2]. Second, the algorithm identifies \"relevant\" peers for a specific user by incorporating their prior activity in the OHC and ranking potential peers based on their mutual activity.\nTable 1. Feasibility assessment of a peer recommendation intervention. We collected evidence in five focus areas (originally described by Bowen et al. [12]). Each point of evidence is associated with a corresponding section (in parentheses). Reading behavior (5.3.1), interaction behavior (5.3.2), second-order effects (5.3.3) blog authors to form connections with peers that they may not be explicitly seeking but for whom they can give or receive meaningful support. We use a weekly email as the interface to display recommendations, producing textual profiles that contain previews of recent blog activity. We use a machine learning-based recommendation system to rank potential peers based on historical interaction behavior on CaringBridge.", "publication_ref": ["b41", "b1", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Feasibility Area", "text": "Description\nAs an intervention, recommending the blogs of peers to OHC users aims to directly manipulate those users' natural social networks. Such manipulations are necessarily complex [108], so a focus on specific user behaviors that the manipulation will induce is important. The intervention is expected to increase two behaviors: reading about the experiences of peers and interacting with peers. Both of these behaviors are associated with benefits, such as reduced stress, useful coping information, and a sense of community [89]-discussed further in sec. 2.1. However, the experimental evidence linking peer connection with benefits is mixed [1], and includes potential risks such as increased distress [89]. These mixed outcomes motivate us to carefully evaluate the feasibility of an intervention to increase these behaviors and produce benefits for participants.", "publication_ref": ["b107", "b88", "b0", "b88"], "figure_ref": [], "table_ref": []}, {"heading": "How did we assess feasibility of the intervention?", "text": "We conducted a field study to assess the feasibility of a peer recommendation intervention and to identify requirements for running a larger randomized controlled trial. Feasibility refers to the ability to use an intervention in reality: a broad and necessarily multifaceted construct. Thus, we collected evidence of feasibility in five focus areas, summarized in Table 1: Demand, Implementation, Practicality, Acceptability, and Efficacy. 1For each focus area, we collected converging lines of evidence to understand the feasibility of the proposed intervention. We derived this evidence from three primary data sources: a log data analysis of CaringBridge user data, surveys of CaringBridge users, and participant feedback during a 12-week field study. We used log data to develop a content-based recommendation model based on implicit feedback from historical peer interactions. We used survey data to identify motivations for peer connection, to characterize interesting peers, and to understand how field study participants engaged with recommendations. We observed usage of our recommendation system during a field study to assess the impact on reading and interaction behavior-as well as to identify any second-order impacts on the usage behavior of both the participants receiving recommendations and the blog authors receiving potentially-unwanted attention from peer strangers.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Contributions", "text": "The primary contribution of this paper is a determination that peer recommendation interventions designed to facilitate OHC user connections are feasible. We offer this feasibility assessment in terms of demand for the intervention, implementation challenges, practicality of administration, acceptability to participants, and efficacy. We present our system design as a model for future peer recommender systems. During the 12-week field study, 79 participants received weekly peer recommendations via email, leading to hundreds of repeat visits to blogs and hundreds of extra peer interactions. Participants clicked 5% of recommendations, although less than half of the participants clicked any recommendation and fewer still chose to visibly interact with recommended blogs. We find no evidence of second-order harms or benefits, and overall find an interest in and willingness to engage with blogs written by peer strangers. We conclude with implications for the further development of peer recommendation systems for OHCs, including considerations for both the context of deployment and design trade-offs. We believe that peer recommendation systems can facilitate connections that authors may not be explicitly seeking-and that those connections can facilitate meaningful support. We offer the first substantive evaluation of a real peer-matching recommendation intervention and its impact on OHC user behavior.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "RELATED WORK", "text": "To design a peer recommendation system for users of online health communities (OHCs), we drew from existing work on OHCs, on peer matching, and on algorithmic recommendation. The most direct precursor to our current work is Hartzler et al. 's study of peer mentor recommendations for cancer patients and caregivers [41], which we discuss as an algorithmic approach to peer matching.", "publication_ref": ["b40"], "figure_ref": [], "table_ref": []}, {"heading": "Online health community use", "text": "People use OHCs in the hope that they will obtain useful support. Both people experiencing a health condition and people caring for a loved one are motivated to use OHCs to help overcome isolation by offsetting deficits in existing relationships and identifying people who have had similar experiences [89]. While the effects of using OHCs are somewhat unclear, feeling socially supported is a key determinant of health linked to OHC use [1,46]. Meta-reviews reveal consistent associations between social support and a variety of health outcomes, e.g. mortality [46,116,117]. OHCs have diverse interfaces and affordances, including forums, listservs, blogs, chatrooms, Q&A sites, and update feeds [89]. The social support available from OHCs is also diverse, including informational support from discussions of treatments and symptoms, emotional support from sympathetic others and being a part of a community, and other forms of support including the instrumental and the spiritual [1,103]. Support can come from many types of users [130], but one particular benefit of OHCs is that they expose you to many peers.\nPeers are people who have similar experiences [100]. Peer relationships differ from professional/patient and mentor/mentee relationships in that no formal role or expectation structures the relationship [100]. Compared to a person's existing offline support networks, peer can provide more useful support [100,113]. A wide variety of theoretical models support the potential benefits of peer connection [1,100,113,115]. We avoid adopting a specific theoretical basis for our current study, as we do not operationalize or measure social support directly, instead focusing on behaviors-reading and interaction-that are compatible with multiple theoretical models.\n2.1.1 Reading about peer experiences. Reading about the experiences of peers can be beneficial even in the absence of interaction [1]. In addition to learning from the valuable information contained in peers' writing e.g. coping strategies [132], reading peer experiences can build a sense of community [96]. Further, reading can reduce loneliness [114], contribute to feelings of normalcy and hope [45,114], reduce uncertainty and anxiety [132], and enable collective sensemaking about one's journey [32]. In general, reading the experiences of others can benefit readers by enabling positive and normalizing social comparisons to the experiences of others [72,102]. But, making social comparisons is not without risk: the negative experiences of others can produce a sense of helplessness or increase distress [56,72].", "publication_ref": ["b88", "b0", "b45", "b45", "b115", "b116", "b88", "b0", "b102", "b129", "b99", "b99", "b99", "b112", "b0", "b99", "b112", "b114", "b0", "b131", "b95", "b113", "b44", "b113", "b131", "b31", "b71", "b101", "b55", "b71"], "figure_ref": [], "table_ref": []}, {"heading": "Interacting with peers.", "text": "Interacting with peers offers many potential benefits-among them are membership in a community, acquisition of new information, normalization of one's experiences, and relief from distress [89]. Online peer interaction can take three general forms: providing support to others, receiving support from others, and forming reciprocal relationships. While receiving support from others has obvious appeal, not all support is perceived as wanted or useful, and in general there is mixed causal evidence for the benefits of online interaction-based social support [1]. A gap between received and perceived support bedevils designers of social support interventions: increased received support is only weakly correlated with perceptions of that support [90,113]. Providing support to peers, on the other hand, may be more beneficial to the provider than the receiver [95]. Support needs differ over the course of a health journey, and providing support presents an opportunity to \"give back\" and enact self-efficacy [51,52]. Reciprocal peer relationships can offer the best of both worlds, but also presents significant risks in health contexts [89]. Stress can increase if online contacts are doing poorly or doing well due to social comparisons [72]. The sudden drop-out of a connection, due to churn or patient death, can also increase distress [6]. Further, peers might be unintentionally unsupportive due to differences in communication style [86]. Due to the risks of interacting with peers, interventions designed to increase interaction cannot be deployed without careful evaluation of the risks and benefits-which motivates us to conduct an initial feasibility study for peer recommendation specifically.", "publication_ref": ["b88", "b0", "b89", "b112", "b94", "b50", "b51", "b88", "b71", "b5", "b85"], "figure_ref": [], "table_ref": []}, {"heading": "Social support interventions", "text": "The archetypal peer support intervention is the support group [20]. Online support groups offer similar approaches using a different medium, although generally still designed for and managed in a clinical setting [57,122]. Other clinical approaches bridge the gap to OHCs-for example, Haldar et al. designed an OHC for people in the same hospital [38]. Peer support interventions have potentially many goals in mind: providing social support, providing health information or education, developing self-efficacy (e.g. by vicarious viewing of peer behavior [96]), adjusting social norms (e.g. use of a particular health behavior), or even facilitating social movements for patient advocacy [100]. In 2004, Cohen expressed skepticism of peer support interventions in general, identifying a string of peer support group studies finding null effects and arguing for a focus on forming weak ties and propping up existing support networks [20]. Nearly 20 years later, the challenges associated with designing effective peer support interventions remain [1]. As an intervention into people's online social networks, we examine recommender systems as a mechanism for encouraging initial interactions that can blossom into weak tie relationships. In general, recommendation is one approach to improving the quality of support received by peers via matching peers by some measure of \"fit\". We discuss prior approaches to peer matching next.", "publication_ref": ["b19", "b56", "b121", "b37", "b95", "b99", "b19", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Health peer matching", "text": "Health peer matching has occurred largely in the context of hospital-attached programs where mentors and mentees are matched by a 3rd-party broker, usually a nurse or program manager [2,76,111]. Consider \"woman-to-woman\", a peer support program for women with gynecologic cancer: when a new participant expresses interest in the program, the program manager selects a match \"of similar diagnosis and age\" from a pool of volunteer mentors [75]. In contrast, online peer recommendation is not constrained to formal mentor/mentee pairings and can draw from a much larger pool of prospective \"volunteers\" at the cost of the clear expectations that come with structure and a human coordinator. In this study, we aim to seriously consider non-coordinated peer matching as a health-related social support intervention.\nLittle explicit guidance exists for peer matching [4,100]. Table 2 lists peer characteristics identified in prior work as salient or important for effective peer matching. We distinguish these characteristics as either proposed as an implication of a particular study, used in practice to match peers in a study or support program, or expressed by participants as preferences for or barriers to effective peer support. 2 While not intended to serve as a rigorous meta-review, this existing literature suggests a wide range of potential characteristics to incorporate in a peer recommender system. A general finding from HCI research is that a shared health condition is not a requirement for peer communication; people perceive value to learning from and communicating with others based on many factors that shift throughout a health journey as needs and communication goals change [25].\nWhile several works have collected empirical data on preferred peer characteristics in support settings, minimal comparative work exists to identify the most important characteristics [100]. Hartzler et al. are a notable exception, running scenario-based sessions in which participants explicitly evaluated five potential peer mentors based on provided health information [41]. Boyes surveyed cancer patients about the importance of specific shared characteristics such as gender, age, and cancer type, although this data is currently unpublished [13]. We extend this body of work by conducting a survey in which participants indicated their preference for specific peer characteristics. While these specific characteristics could be incorporated in future systems, for the purpose of our system we consider machine learning approaches that learn valued peer characteristics from prior peer interactions, discussed next.", "publication_ref": ["b1", "b75", "b110", "b74", "b3", "b99", "b24", "b99", "b40", "b12"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Algorithmic recommendation for peer matching", "text": "Few published works explicitly discuss computational recommendation systems for online health communities. Hartzler et al. matched peer mentors on the basis of shared health interests, language style, and demographics-as extracted from prior posts made in the CancerConnect OHC-although they evaluated these matches in workshop interviews rather than actual use [41]. The other notable example is described only in Diyi Yang's thesis: Yang developed and deployed a recommendation system in the American Cancer Society's Cancer Survivor Network (CSN) forums \"to direct participants to useful and informative threads that they might be interested in\" [129]. They evaluated a model based on implicit feedback from prior commenting behavior by presenting recommended threads and users within the CSN interface, reporting greater thread click-through rate compared to a baseline model recommending recently popular threads. In contrast to the CSN forums, Caring-Bridge is a blogging platform without existing interface recommendation features for discovering other blogs. We present an evaluation for a new CaringBridge recommender that extends beyond click rate into explicit feedback and the effect of the recommendations on a variety of user behaviors.\nOutside of health, a variety of problem formulations and modeling methods have been used for the problem of recommending people. Recommendation models are generally supervised machine learning models that optimize a loss function comparing the model's output to \"ground truth\" labels: explicit or implicit feedback provided by users. Xu et al. present a useful review [128]. Use of implicit behavioral feedback is based on relevance assumptions, e.g. that clicked items are relevant while non-clicked items are not relevant [54], which may not hold true in practice [69,126]. We assume that interactions between OHC users indicates relevance, discussed further in sec. 3.3.1. Given historical user/item pairs, one can then optimize a pointwise loss that rewards high scores for assumed-relevant user/item pairs and low scores for assumed-irrelevant user/item pairs. Input features vary from IDs for the user and item-which gives the classic matrix factorization approach to collaborative filtering [94]-to side information about the context where the recommendation was generated (e.g. the time and place) or content (e.g. prior comments) from the user or item [34,106].", "publication_ref": ["b40", "b128", "b127", "b53", "b68", "b125", "b93", "b33", "b105"], "figure_ref": [], "table_ref": []}, {"heading": "Alternatives to recommendation.", "text": "Recommendation is not the only available mechanism for facilitating online peer connections. Two notable alternatives are improving search and filter tools and designing enriched profile pages to make it easier to represent one's diagnosis, expertise, and support needs [32]. Search is challenging in situations where a user's needs are known only implicitly to the user or are challenging to express in terms the system will understand [10]. We suggest that peer support finding is an exploratory [7] search task (e.g. see Pretorius et al. 's discussion of person-centered help-seekers [88]). Even with rich peer profiles available, it is challenging for searchers to formulate a query that captures their needs and intent [120,121]. Other search systems for finding people-such as expertise-finding systems-were created based on interfaces designed to capture users' needs in a domain-specific query [49]. Mindsets is a recent example of the design work needed to capture domain-specific intents during query formulation [120]; additional research is needed in the peer support context to capture support seekers' and providers' intents. In contrast to search, recommendation offers opportunities to engage with potential peers without explicitly articulating a person's current needs.", "publication_ref": ["b31", "b9", "b6", "b87", "b120", "b48"], "figure_ref": [], "table_ref": []}, {"heading": "Machine learning for matching people.", "text": "Person-to-person recommendation is typically modeled as similar to the conventional user/item recommendation problem. Facebook's deep learning recommendation model (DLRM) represents a common approach, using embeddings for categorical features (including user IDs) and MLPs for creating dense representations of other features, then combining all representations with a final MLP [79]. Less recently, other sites have used approaches based on neighborhoods and similarity of interactions to connect with strangers specifically. Twitter's \"who to follow\" recommendations used an alternative approach similar to PageRank that uses only the existing follow network to make recommendations [35]. Guy et al. explicitly attempted to recommend strangers in an enterprise setting based on number of shared interests and memberships [37]. The modeling problem closest to peer recommendation may be romantic relationship recommendation, a context that aims to encourage interaction between users and values reciprocity [87,110]. We drew on modern deep learning approaches to recommendation-the model we use during the field study is a simplified form of DLRM-to design a system appropriate for the peer support context.", "publication_ref": ["b78", "b34", "b36", "b86", "b109"], "figure_ref": [], "table_ref": []}, {"heading": "SYSTEM DESIGN", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Observed prior use of CaringBridge for peer connection", "text": "CaringBridge.org is an online health community and blogging platform that has been the focus of prior HCI research e.g. [63,70,104]. In collaboration with the CaringBridge organization, we were given access to usage data from the CaringBridge website. Registered users on CaringBridge can  2). Visitors to a site can follow that site to be notified via email when an author of that site publishes a new Journal update. 3 The majority of visitors to a site will be a patient's existing support network [63]. Registered visitors can  leave reactions to Journal updates (a reaction labeled \"Heart\" is the default) to show their support. Alternately, they can leave text-based comments on individual Journal updates or write guestbooks which are comments that appear on a special Well Wishes page. These interface components, and the actions that we track for this study, are shown in Figure 1.\nWe are interested here in interactions (via reactions, comments, and guestbooks) between registered CaringBridge authors. Adopting the terminology used by Levonian et al. [63], an initiation is the first interaction between an author and a site on which they are not an author. In assessing demand for a peer recommendation intervention, we consider the volume of inter-author communication on CaringBridge, studied in detail by Levonian et al. [63]. Between 2010 and 2021, 275K authors initiated with other authors, more than 32.3% of all 852K authors active during that period; this interaction occurs despite minimal existing discovery features to facilitate finding peers. 4  Based on sec. 2.1, we would expect that peer interaction is associated with behavior change for authors. Prior work demonstrates that receiving interactions from visitors-peers and non-peers-is associated with retention on CaringBridge [124], but are peer interactions more impactful than nonpeer interactions? Table 3 shows associational differences between CaringBridge sites based on the interactions (reactions, comments, and guestbooks) received from visitors within 30 days of a site's first published Journal update. Sites for which at least one visitor interaction is left by a peer author will publish on CaringBridge for a median of 3.2 additional months (with 6 additional updates) will add it to the Sites You Visit list. Further complicating matters, removing a site from the Sites You Visit list is unrelated to Follows, and a visitor will automatically Follow a site if they visit and interact with that site. 4 While CaringBridge offers a search feature, authors use this tool to find specific authors, not for general searches for e.g. particular health conditions (see Appendix A).\nPre-print date: September 2022.  compared to sites that receive only interactions from visitors. 5 This positive correlation suggests that peer interaction could change author behavior in ways that are themselves correlated with author benefits [70]. A peer recommendation intervention intervenes in this existing ecosystem of peer interaction: we propose a design appropriate for the CaringBridge context in the next section.  ", "publication_ref": ["b62", "b69", "b103", "b62", "b62", "b62", "b123", "b69"], "figure_ref": ["fig_2", "fig_1"], "table_ref": ["tab_2"]}, {"heading": "Adapting a recommendation system to CaringBridge", "text": "For a peer recommender intervention to be successful, the system must be adapted to the specifics of the context. We summarize three adaptations in our design for the CaringBridge context: recommending blogs rather than peers, using email as the recommendation medium, and designing the email interface.\n3.2.1 Peer recommendation or blog recommendation? While we are motivated by peer-to-peer connection, we are restricted by the interface affordances provided by CaringBridge. CaringBridge offers only a limited public profile view that is minimally exposed in the interface; instead, the sites themselves present a better view into an author's activity. For this reason, we recommend sites rather than authors. The system optimizes for peer connections, but presents those recommendations as site summaries.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Email as recommendation medium.", "text": "As much usage of CaringBridge is motivated by email notifications, we chose to deliver recommendations via email. Author notification emails (depicted in Figure 2b) are sent to site authors when a visitor interacts on a site or a co-author publishes a Journal update. As our recommendations are author-centric and authors are familiar with the design of the author notification emails, we used the same layout and CSS style for our Site Suggestion emails.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Design of the Site Suggestion email.", "text": "The email design can be seen in Figure 2c. Site Suggestion emails contain 5 bullet-pointed site suggestions, a request for feedback and link to a feedback survey, a link to the study FAQ, and an unsubscribe link. Each site recommendation is presented with the site's title (usually the patient's name) and a link to the site's Journal page. We follow Hartzler et al.-who found the most important aspect of evaluating peer mentors is sample posts-by including a preview of the most recent Journal update for each recommended site [41]. We chose to present 5 recommendations as a trade-off between email length and multiple options. We discuss the model used to generate site recommendations in the next section.", "publication_ref": ["b40"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Model development", "text": "To present recommended sites to a recommendation-seeking author, we include the top 5 sites as scored by a recommendation model. We adhered to two key design requirements: First, recommendations should be personalized, focusing on the right connection rather than a popular connection. We should not recommend any one site to a lot of people as that could create a negative experience for that site. Second, recommendations should be available even for authors who have never visited or interacted with another CaringBridge site. Because support-based reading and interaction is our goal, an author's initial \"cold start\" recommendations should be of similar quality to recommendations for long-time authors.\nTo provide recommendations that meet these two criteria, we implemented a content-based recommendation system. Social matching systems require users to disclose sensitive personal information [112], and CaringBridge is a context where authors are already making those sensitive disclosures in the content of the Journal updates they publish [64]. Our modeling task is to use implicit feedback from user activity (sec. 3.3.1) and content-based features (sec. 3.3.2) to create a recommendation model that predicts historical peer connections and, eventually, new site recommendations (sec. 3.3.3). We discuss and compare several recommendation models (sec. 3.3.4), as well as what features (sec. 3.3.5) and resources (sec. 3.3.6) are required to generate peer recommendations using our system.", "publication_ref": ["b111", "b63"], "figure_ref": [], "table_ref": []}, {"heading": "Implicit feedback.", "text": "We processed all historical CaringBridge author interactions since January 1st, 2010. We train our recommendation models on implicit feedback-the behavioral signals that indicate an author is interested in reading a site or interacting with the author of that site [93]. Three potential sources of implicit feedback are available: (1) first visits-when an author visits another author's site for the first time, (2) initiations-when an author interacts with a site for the first time, or (3) reciprocations-when an author initiates with a site and an author of that site subsequently interacts on the initiator's site. 6 Choice of implicit feedback signal can have a significant impact both on what recommendations are shown to users and how those users engage with those recommendations [80]. We choose initiations as our implicit feedback signal, as it is less noisy than site visits (i.e. a visit may not indicate interest) while more plentiful than reciprocations (i.e. reciprocations are less common than unreciprocated initiations [63], which means less data available for model training). By selecting initiations, we assume that leaving a reaction, comment, or guestbook on another author's site indicates a preference for reading that site and interacting with that author relative to other sites. Optimizing for recommendations that increase initiations is likely to increase actual initiations [134], but we acknowledge a semantic gap between the implicit feedback metric and metrics of interest; a construct like \"perceived social support\" may not increase despite receiving more peer interaction [21,58].\nFor each initiation between a source author and a target site, we generate training data. Using initiations presents three complexities: (1) a user may initiate with a site before they become an author, (2) authors may write multiple blogs, and (3) authors may co-write with other authors on a single blog. We address these complexities by ranking author/site pairs rather than sites alone. For each initiation, we generate one positive sample for each author on the target site (target author/site pairs) and each site written by the source author (source author/site pairs). An author/site pair is eligible if the author has published at least three Journal updates on that site-a minimum activity threshold that ensures sufficient data is available during feature extraction. We include in the training data only initiations between eligible author/site pairs. 7 At prediction time, given a recommendation-seeking source author, we score all candidate author/site pairs: eligible author/site pairs that has been active on CaringBridge in the last week and have not previously been interacted with by the source author. 8 As we ultimately recommend sites to authors, we Fig. 4. Features available for recommendation, including 780 features for the recommendation-seeking source author/site pair and the same for each candidate author/site pair. Three dyadic features capture the relationship between the source and candidate within the initiation network. Total features: 1563 convert the ranking of candidate author/site pairs into a ranking of sites. If a site appears among candidate pairs twice (because that site has two authors), we remove all but the highest score from that site. If a recommendation-seeking author is an author of multiple sites, we merge the scores for each source author/site pair by averaging the scores for each candidate pair. 9For each positive sample generated by initiation extraction, we sample an assumed-negative author/site pair to add to the training data. 10 The negative author/site pair is randomly selected from among the candidates. As a history of previous initiations are not required for eligibility, we avoid selection bias issues that arise in models that require user feedback on negative samples [131].", "publication_ref": ["b92", "b79", "b62", "b133", "b20", "b57", "b130"], "figure_ref": [], "table_ref": []}, {"heading": "Model features.", "text": "Given two author/site pairs, we use the recent behavior of the authors on CaringBridge to construct a feature representation of the two, summarized in Figure 4. Different models might use subset of these features, but we describe here the full set included in the model deployed during the field study.\n\u2022 Text (768 \u00d7 2 features): We incorporate context from the three most recent Journal updates written on a site. For each Journal update, we use the pre-trained RoBERTa [68] model available in the HuggingFace Transformers package [127] to compute size-768 contextualized word embeddings. Then, we mean pool the token embeddings and then the update embeddings to produce a single vector representation, an effective general approach to using word embeddings [65,67,92]. 11\u2022 Activity (9 \u00d7 2 features): For each of Journal updates, reactions, comments, and guestbooks, we include the count of that action within the last week and the time elapsed to the most recent action (in hours). In addition, we include the author's current tenure (time elapsed to the author's first published journal update). \u2022 Network (6\u00d72+3 features): During initiation extraction, we maintain the interaction network between authors as described by Levonian et al., in which new edges are created between authors when an initiation occurs [63]. For each author/site pair, we include: indegree, outdegree, and weakly-connected component size. In addition, we include three dyadic features: whether the two authors are weakly connected, whether the candidate author is the friend-of-a-friend of the source author (i.e. this initiation would create triadic closure), and whether the candidate author has previously initiated with the source author (i.e. this initiation would be a reciprocation to a prior initiation).", "publication_ref": ["b67", "b126", "b64", "b66", "b91", "b62"], "figure_ref": [], "table_ref": []}, {"heading": "Offline evaluation.", "text": "Given an initiation, the goal for our recommender system is to rank the site that was actually initiated with as high as possible: ideally in rank 1, above other candidates. We evaluate various modeling approaches by splitting initiations chronologically into a training,  validation, and test set-see Figure 5 for initiation counts. Offline evaluations can diverge substantially from the usage preferences expressed by recommendation consumers [9]; we conduct one here to compare recommendation algorithms in terms of accuracy, coverage, and diversity of predictions on historical initiations.\nDuring evaluation of each initiation, features are captured at the millisecond before the initiation actually occurred: for the source author/site pair (who did the initiation), for the target (who is being initiated with), and for the candidates (who are eligible, active author/site pairs that the source had not previously initiated with). This evaluation approach intends to reward models that rank a site well if the source author/site pair actually initiated with that site at the time the recommendation was made. The number of eligible, active candidates varies according to the time of the initiation; the median number of eligible, active authors during the test period was 13,252.\nWe used two primary evaluation metrics: mean reciprocal rank (MRR) and hit rate (HR). Mean reciprocal rank is computed based on the rank \ud835\udc5f assigned to the target site, where 1 is the best rank given to the highest score. 12 MRR is the mean of the reciprocal ranks (1/\ud835\udc5f ) for every test initiation. Hit rate is the proportion of the time that \ud835\udc5f is less than some threshold. As we provide 5 recommendations in a Site Suggestion email, we report HR@5 (how often would the model recommend the target site in a five-site set) and HR@1 (how often would the model rank the target site first). When we compare multiple hyperparameter configurations for a model, we select the best on the basis of validation MRR and report test metrics as the median from 3 random seeds.\nBeyond accuracy-style metrics, we consider coverage as an important secondary goal [33]. Coverage has two aspects: (1) how many users can receive recommendations or have their sites recommended and (2) the diversity of sites that are recommended in practice. The first aspect of coverage is model agnostic and heavily affected by our decision to require three Journal updates to be eligible. Of 124,051 new authors in 2020, only 51.4% will publish three updates. Further, of those authors that do publish 3 updates, recommendations cannot be delivered until the publication of the 3rd update-a median wait of 2.5 days, but at least 72.4 days for the slowest-publishing 10% of authors. Beyond authors, a hypothetical system that served recommendations to any visitor with an interaction could reach an additional 1.3M new visitors who first interacted in 2020.\nThe second aspect of coverage-the diversity of recommended sites-is model dependent. To evaluate the coverage of recommended sites, we randomly sampled 1000 eligible, active authors at the end of the training period 13 and produced recommendation sets with the 5 highest-scoring sites for each author. We then compared the sites that were actually recommended (R) to the sites that were not recommended (N), among the 12,432 candidate sites available at noon UTC on January 1, 2021. We consider three plausible goals for diverse recommendations, with an associated metric for each goal. The first goal is that a large number of unique sites are recommended rather than a few sites recommended many times; we compute the percentage of unique recommendations given (|R|/5000). The second goal is that newer sites are recommended, a period during which support may Table 4. Offline test performance for various accuracy and coverage metrics. Coverage is reported in terms of number of unique recommended sites (|R|), the percentage of unique recommendations made (out of 5000 total recommendations), and the mean of the minimum site tenure in each rec set (MMST). The final column displays the percentage of recced sites that have no prior connections (i.e. are \"siloed\", |S R |/|R|), the same percentage for non-recced sites, and the ratio of those percentages. The bolded model was used during the field study.", "publication_ref": ["b8", "b32"], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "Network", "text": "MRR HR@1 HR@5 |R| %Unique MMST ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Model comparison.", "text": "We describe the recommendation models we implemented and compared, starting with the model we used during the field study. Table 4 presents the performance of these models in the order we describe them. MLP. We used a multi-layer perceptron (MLP) with two hidden layers as a parsimonious yet effective deep recommender model. All source, candidate, and dyadic features are concatenated into a single input vector. The output layer uses a sigmoid activation to score the inputs and we optimize the standard pointwise binary cross-entropy loss [128]. We trained the model for 1000 epochs over the full training data, holding out a random 1% of the training initiations to compute hold-out loss. Further modeling and optimization details can be found in Appendix B. We report results for two MLP models: MLP Study and MLP Tuned . MLP Study was the result of manual tuning before the study, and best reflects the model configuration used during the field study. MLP Tuned was the result of more systematic hyperparameter tuning (App. B) and adds additional hidden units, weight decay, and dropout.\nWe compare the MLP model to two personalized baselines. PeopleYouKnow uses only the dyadic network features, scoring highest the sites on which authors have already initiated with the source, then sites that are friends-of-friends with the source, then sites that are in the same network component as the source, and finally all other sites. CosSim scores author/site pairs by computing the cosine similarity between source and candidate. Cosine similarity was used by Hartzler et al. to match peers by health interest [41].\nWe include two non-personalized baselines. MostInits scores each site by the number of initiations received by that site within the last week. 15 In other recommendation contexts, popularity baselines like MostInits are strong contenders; not so on CaringBridge, where popularity follows  4 compares the models by accuracy and coverage metrics. The best model (MLP Tuned ) would recommend the site that was actually initiated with more than 20% of the time; the study model (MLP Study ) performs slightly worse. Both MLP models are more likely to recommend newer sites than the baseline models, but less likely to recommend a variety of sites and sites that had not previously received an initiation. PeopleYouKnow achieves impressive MRR and HR metrics, demonstrating the strong utility of the dyadic features; however, when the source and target are not connected, it predicts randomly (hence the high percentage of unique recommendations). CosSim recommends the largest number of siloed candidates, although low MRR indicates that similarity alone is a poor predictor of historical initiation behavior.", "publication_ref": ["b127", "b40"], "figure_ref": [], "table_ref": []}, {"heading": "Feature ablations.", "text": "Providing personalized recommendations requires some form of sensitive data collection from users [112], but privacy and other ethical concerns necessitate collecting as little sensitive data as is required to produce useful recommendations (or in some cases opting not to provide recommendations at all). Pervasive data collection contributes to perceptions of recommendation services as invasive \"little brothers\" [34], changing behavior and undermining the potential benefits of recommendation. Therefore, we compared the relative importance of the different feature sets in order to understand the utility of collecting particular types of sensitive data-results are shown in Table 5. Surprisingly, the model trained without text features performs better on all accuracy metrics and similarly in terms of coverage, and it is also more likely to recommend newer sites. This analysis suggests that the collection of textual data may not be necessary and peer recommendation may still be practical if interaction network data is available: reasonable recommendations could be generated purely based on usage metadata. We still chose to include text features in the model we evaluated during the field study, as otherwise new authors with minimal activity and no peer interactions would all receive the same recommendations, conflicting with our design goal of personalization even in the cold-start setting. 163.3.6 Model deployment & required resources. Figure 6 shows the steps required to generate a batch of Site Suggestion emails. The most time-intensive step was the anonymization and manual transfer of a nightly database snapshot, which led to a 36-hour lag time between the snapshot and  the associated email batch. 17 We created the Site Suggestion emails by retraining the recommender model on the most recent snapshot and scoring all eligible, active candidate sites. Rather than recommending the top 5 sites by score, we decided to limit the total number of times a site was recommended in a single batch to at most 10 times. We made this decision for two reasons: first, as the perception of stranger visits by site authors is unknown, a large influx of strangers might create a negative experience for the recommended site's authors; second, this restriction ensures that a diversity of sites are recommended each week. To achieve this limit, we randomly drafted sites by score until each participant had five recommendations. 18 For ethical reasons, the first author manually inspected all recommended sites, manually excluding sites that would be inappropriate to send to participants. Two sites were removed this way: one for spam and one for COVID-19-related health misinformation. This small percentage of potentially harmful recommendations suggests that after some initial quality validation, not all individual recommendations need to be inspected. For future studies, the actual resource requirements could be reduced by computing recommendations in-house. Disk usage for the weekly snapshot and the feature databases was approximately 96GB. The most RAM-intensive processing is maintaining the author interaction network during feature extraction; all other processing tasks are parallelizable and generally IO-bound. The CPUbased training and inference of the recommendation model could be accelerated with the use of GPUs, although at 1.5s per participant we found inference to be fast enough to serve even a much larger participant population.", "publication_ref": ["b111", "b33"], "figure_ref": ["fig_5"], "table_ref": ["tab_7"]}, {"heading": "METHODS", "text": "To evaluate our recommendation system, we conducted a field study. The previous section summarized aspects of feasibility related to the system design, while the next section summarizes aspects of feasibility related to the field study. Table 6 summarizes these two phases and the associated components, alongside the relevant feasibility aspect. We discuss the assembled evidence by feasibility area in the Discussion (sec. 6).\nThe field study consisted of recruiting CaringBridge authors and sending them 11 weekly Site Suggestion emails. The full analysis timeline is shown in Figure 7. We analyzed field study data using both qualitative and quantitative methods, discussed in subsequent sections. System implementation and analysis code are available on GitHub. 19 Analysis code makes primary use of Python's scikit-learn [85], statsmodels [98], transformers [127], NumPy [40], pandas [73], and Matplotlib [48] packages. The recommendation model was trained using PyTorch [84].  ", "publication_ref": ["b84", "b97", "b126", "b39", "b72", "b47", "b83"], "figure_ref": ["fig_6"], "table_ref": ["tab_9"]}, {"heading": "Recruitment Survey", "text": "We recruited active CaringBridge authors by displaying a banner ad with a link to an opt-in survey.\nThe banner appeared only to logged-in CaringBridge users on the home page of sites on which they are an author, as shown in Figure 1a. 20 The recruitment banner and opt-in survey were active for three weeks in August 2021. The survey asked authors to opt-in to the study and included three optional questions on peer connection: on prior use of CaringBridge for connecting with strangers, on motivations for peer connection, and on characteristics that make a peer connection appealing.\nFull survey text is available in Appendix C.1.\n4.1.1 Participant matching. Figure 8 shows the recruitment pipeline. Of the 100 survey completions, 96 opted-in and met the three study consent criteria: being at least 18 years old, being a current CaringBridge author, and consenting to provide the email address associated with their CaringBridge account in order to receive Site Suggestion emails. We matched survey responses to a specific CaringBridge account based on their provided email. For the 8 cases where we couldn't find an associated account, we sent a follow-up email asking  [63], a majority of the 79 participants both visited and interacted with at least one fellow author's site. This evidence suggests a demand for peer connection, although the field study extends beyond known contacts to peer strangers. To quantify the differences between authors that chose to enroll in the study and other CaringBridge authors, we identified 30K authors who visited their own site while the banner survey was live and thus could have seen the banner, filtering down to 1,759 who had at least 3 Journal updates and thus could have received Site Suggestion emails. Table 7 compares participants to this set of non-enrolled authors, which we term a pseudo-control group, revealing that participants are generally newer to CaringBridge than other eligible authors. While the field study uses an uncontrolled design, we will use this pseudo-control group to estimate the potential impact of the peer recommendation intervention on author behavior.", "publication_ref": ["b62"], "figure_ref": ["fig_1"], "table_ref": ["tab_10"]}, {"heading": "Recommendation Emails", "text": "After we went an initial Site Suggestion email on September 2, 2021, we conducted an initial assessment of interest and determined the study could proceed; we resumed sending emails on September 17, 2021 at a weekly pace. After 11 Site Suggestion emails, a \"thank you\" email was sent with a final request for feedback. To evaluate the effectiveness of the Site Suggestion emails as a recommendation interface, we analyzed the rate of clicks on recommendations, the explicit feedback we received from participants, and the characteristics of the site previews we included in the emails. Click estimates are based on the fusion of multiple data sources, although the primary source was via UTM tags embedded in each email's links. Appendix D presents additional details. Explicit participant feedback was collected from four sources: direct responses to the emails, responses to a feedback survey linked in each email, an unsubscription survey linked in each email, and responses to the final \"thank you\" email sent on December 3, 2021. The Site Suggestion email was previously described in sec. 3.2. The survey texts are presented in Appendix C.2 and C.3. The \"thank you\" email was sent in two versions. To participants that had clicked on none of the recommendations, we asked for feedback on whether they had seen the emails and why they chose not to visit any of the recommended sites. To participants that had clicked on at least one recommendation, we listed up to three random sites they clicked and asked for reflections.\nThe primary information available to participants while they were deciding to click on a recommendation was the content of the recent Journal update preview. To capture the preview characteristics that our participants could see and respond to, we conducted a thematic content analysis on the text of these previews. Two researchers generated open and axial codes independently, then used an affinity mapping process based on the Grounded Theory method [16]. Based on our thematic analysis and on existing work with CaringBridge Journal updates [70,104], we isolated a set of 6 categories in order to produce quantitative prevalence estimates for each of the categories and to identify which categories were associated with clicks-further details in Appendix F.", "publication_ref": ["b15", "b69", "b103"], "figure_ref": [], "table_ref": []}, {"heading": "Observed behavior on CaringBridge", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Reading and interaction behavior.", "text": "To analyze the two primary behavioral outcomes-reading and interaction behavior-we identified measured proxies as indicators. We used repeated visits and site Follows as proxies for interest in reading a site. While a single site visit may still indicate value for the reader, we would need either explicit feedback or some measure of dwell time on the site. Thus, we focus on instances where a participant returns to a site at least once. Follow actions are harder to interpret (see 3.1), but at a minimum indicate an interest in continuing to read updates on the followed site. We use reactions, comments, and guestbooks left on a recommended site as evidence of interaction.\nWe analyzed the textual content of the comments and guestbooks created by participants. Three researchers conducted a qualitative analysis of the participant/site dyads where interaction occurred, writing axial codes and memos based on the Journal updates and comments in which participants and authors of recced sites interacted [16].", "publication_ref": ["b15"], "figure_ref": [], "table_ref": []}, {"heading": "4.3.2", "text": "Second-order effects of recommendations on behavior. In addition to the reading and interaction behavior of participants, receiving Site Suggestion emails and visiting strangers' CaringBridge sites might have second-order effects: harms or benefits that accrue to both participants and the authors of the sites they visit. While we cannot draw firm causal conclusions from our nonexperimental study, we can check for potential harms by estimating the effect of recommendations on two secondary behavior outcomes: (a) publishing Journal updates-the \"primary\" use of Caring-Bridge by authors-and (b) visits to and interactions with fellow authors' sites-a hypothesized outcome of peer interaction. We estimate the effects of two \"treatments\": for authors, we estimate the participation effect of receiving 11 weeks of Site Suggestion emails by comparing participants to Pre-print date: September 2022.\nthe pseudo-control group of eligible unenrolled authors (introduced in sec. 4.1.2); for sites, we estimate the visit effect of receiving site visits from peer strangers by comparing visited recommended sites to both non-visited recommended sites and a pseudo-control group of non-recommended sites. 21We quantify the difference between the \"treated\" authors/sites and the untreated authors/sites, producing three estimates: associational, model-adjusted, and causal. The associational difference for a behavioral outcome \ud835\udc4c is E[\ud835\udc4c |\ud835\udc47 = 1] -E[\ud835\udc4c |\ud835\udc47 = 0]; the raw observed difference in the mean between treated and untreated authors/sites, where E[\ud835\udc4c |\ud835\udc47 = 1] is the mean outcome for the treated group i.e. participant authors or visited sites. The model-adjusted estimate uses linear regression (OLS) to adjust for activity variables \ud835\udc34, adding assumptions about model misspecification to compute the quantity E\n[\ud835\udc4c |\ud835\udc47 = 1, \ud835\udc34] -E[\ud835\udc4c |\ud835\udc47 = 0, \ud835\udc34].\nThe causal estimate requires us to make untestable assumptions about the modeled relationship between the treated and untreated groups [43]-see Appendix G for a detailed discussion of these assumptions. Using potential outcomes notation, we define E[\ud835\udc4c \ud835\udc61 =1 ] as the mean behavioral outcome that would have been observed if all authors in the participant and pseudo-control group had been participants in the study. The true causal effect E[\ud835\udc4c \ud835\udc61 =1 ] -E[\ud835\udc4c \ud835\udc61 =0 ] is the influence on \ud835\udc4c that would occur if Site Suggestion emails were sent to all eligible authors. We use the Bang-Robins doubly robust estimator to compute the causal effect, a modeling approach which combines inverse probability weighting and standardization [8] (see also [43], Ch. 13).\nThe author and site outcomes are measured as number of actions in the 13 weeks post-study and post-visit respectively. As the non-visited comparison sites were not visited by participants, we fabricate a visit time by sampling a random visit time from among the sites that were visited in that same batch. The activity variables \ud835\udc34 are computed based on an equivalent time window before the event of interest-13 weeks before the study for participants and 5 weeks before the site visit respectively. 22 The analysis is not sensitive to the time window over which the pre-study features and post-study outcomes were measured (see Appendix G). The associational and causal estimates require specification of a model that includes all relevant confounds. For example, as we saw in Table 7, pseudo-control authors have been active on CaringBridge longer than participants. Activity variables used are shown in Appendix Table 19, although we cannot measure important confounds such as \"interest in receiving Site Suggestion emails\", which is unobserved and potentially unexplained by the activity variables we do observe.", "publication_ref": ["b42", "b7", "b42"], "figure_ref": [], "table_ref": ["tab_10", "tab_23"]}, {"heading": "4.3.3", "text": "Sample sizes needed for a powered RCT. We use observed participant behavior during our field study to estimate the effect sizes of the peer recommendation intervention's impact on reading and interaction behavior. The structure of our data let us consider two potential future interventions: a one-time recommendation email and a recurring, weekly recommendation email. We estimate the effects of a one-time recommendation email by including only visits and interactions resulting from the first batch of recommendation emails. We compute sample sizes for both a replication (no control group) and an RCT (with control group) from the estimated standardized effect sizes at 80% power with \ud835\udefc=0.05 using G*Power's one-tailed point biserial model [26]. Additional details are available in Appendix H.", "publication_ref": ["b25"], "figure_ref": [], "table_ref": []}, {"heading": "RESULTS", "text": "We recruited authors via a survey (sec. 5.1). Then, we sent 79 participants weekly emails (sec. 5.2). Finally, we observed the subsequent usage of CaringBridge by both the participants and the authors of recommended sites (sec. 5.3). ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Recruitment Survey", "text": "Quantitative survey responses are summarized in Table 8.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_11"]}, {"heading": "5.", "text": "1.1 Self-reported prior use. 40% of participants said they visited the CaringBridge site of an author who they did not know personally. 23 This prior usage provides evidence that the visits and interactions among peers includes strangers, although most interactions are likely between people who already know each other offline [63].", "publication_ref": ["b62"], "figure_ref": [], "table_ref": []}, {"heading": "Interest & motivations.", "text": "We asked participants why they might visit a fellow author's Caring-Bridge site, pre-populating a variety of options based on prior work. The top motivation was to learn from others (80%), then to communicate with peers (46%) and to receive experienced support (44%). A smaller percentage (29%) were motivated by mentoring newer authors. 8 respondents (9%) indicated they were not interested in visiting stranger's sites, although 6 suggested they could be interested in the future. Free response motivations centered on learning from others, including finding inspiration, hearing how others handled similar issues, and learning \"ideas on how to engage readers\" of their own site. \"I would like to see how other CaringBridge authors articulate their reactions and feelings as they undergo the medical treatments for and rehabilitation from whatever medical conditions they experience.\" Two free responses described motivations for not engaging with others' sites, both describing it as a distractor during a busy time. \"it's hard to think about joining in someone else's journey. I can focus on writing this blog discussing our journey because it is letting friends and family know what is happening so it is narrow enough that it doesn't take away from the other things needed to be accomplished during the rest of the day. \" These responses indicate that demand for peer connection is goal-driven and subject to constraints.\nB1 Sep02 B2 Sep17 B3 Sep24 B4 Oct01 B5 Oct08 B6 Oct20 B7 Oct27 B8 Nov02 B9 Nov10 B10 Nov17 B11Nov24", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Peer characteristics.", "text": "We asked participants what characteristics of peer sites would make them want to read and engage with that site. The most selected characteristic was a similar diagnosis or symptoms (84%), a theme echoed in more specific free responses e.g. \"neurological conditions\" and a participant calling shared diagnosis the most important characteristic. A majority of respondents indicated that similar treatments (54%) and high-quality writing (51%) were important. Free responses indicated a desire for \"inspirational\", \"honest\" writing with specific details that shows \"positive and negative aspects\". One participant wanted to see \"multiple posts all the way through death. I wanted to see what I would probably be writing as time progressed. A glimpse into the future if you will.\" Fewer respondents indicated that having the same caregiver relationship with the patient was important, although that checkbox was only relevant for non-patient respondents. 25% of respondents indicated that geography was important; one specified sharing the same hospital as a relevant characteristic. Few selected similar cultural background (14%): a divergence from prior work [24], although the question may have been phrased too euphemistically to elicit specific preferences about e.g. age, gender, and ethnicity. Three participants specifically identified age of the author or patient as a relevant detail. Two participants listed shared social context as important characteristics, such as already knowing the person or having \"common friendships\". One participant said they sought Spanish-language updates, while another said they were looking for sites authored by healthcare professionals. These responses provide evidence for the types of peer recommendations that are perceived as most useful and acceptable by CaringBridge users-as well as indicating that preferences are diverse, necessitating personalization.", "publication_ref": ["b23"], "figure_ref": [], "table_ref": []}, {"heading": "Recommendation Emails", "text": "In total, we sent 4,190 recommendations to 79 participants, with 526 unique sites recommended. To evaluate the effectiveness of the Site Suggestion emails as a recommendation interface, we analyzed the rate of clicks on recommendations (sec. 5.2.1), the explicit feedback we received from participants (sec. 5.2.2), and the characteristics of the site representations we included in the emails (sec. 5.2.3).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Click rate.", "text": "Over the 11 weekly batches of Site Suggestion emails, participants clicked 220 (5.3%) of the 4,190 recommendations. Figure 9 breaks down observed clicks by email batch and by participant. Clicks are approximately log-normal; only 30 of 79 (38%) participants clicked any recommendations, and the most active participant, who we designate P1, was responsible for 54 (24.5%) of the recommendation clicks-and the majority of the interactions, as we will see in sec.  9 summarizes the quantitative feedback received: only 4 of 10 responses found the recommendations interesting in general, while 46% of responses to specific recommendations were deemed relevant compared to 39% irrelevant. 24 Five of the recommendations were deemed \"very irrelevant or offensive\", and associated text feedback can tell us why. Participants objected to sites due to having \"choppy, poor grammar and name calling\", describing the death of a patient (\"don't want to read about ppl dying of cancer\"), being \"too religious\", and describing patients of a very different age than the participant. Objections to Christian religious content may reflect belief misalignment between reader and author [103]; one participant requested the ability to filter sites \"by the number of times Christ is mentioned\". Less severe objections focused on aspects of the writing: \"boring\", \"wordy\", \"no reflection\", \"too much philosophizing\", \"bragging\". One participant desired sites that \"go beyond simply updating the reader\", describing their own efforts as an author to inspire readers and to include \"silver linings\".\nOn the positive side, participants valued the \"wide range of experiences\" present in the recommended sites. One participant was surprised by recommendations for different medical conditions but describes realizing that \"what is important to me is to see and be inspired by how others deal with any difficult situation. \" Two participants valued the recency of the displayed updates. \"I'm finding myself following a lot of these sites. I enjoy the sites that update you often ... I feel like I'm abreast of what's going on and take a personal interest into the person and their health battle. \" Participants valued common ground-such as a patient being treated at the same hospital-and familiarity with issues faced. \"I was more interested in the ones I was kind of familiar with. Understood more.\" One patient gave us a general summary of how they engaged with recommendations: \"Even with all I'm going through ... I've come to care about these people who sites I am following. And I leave a comment every time I login on 95% of them because I know what it's like when nobody comments. So I'm really really grateful you guys have [sent me site recommendations] because it is just helps me personally to take my mind off of things when I can go and pray for some other poor souls problems. So good job!\" 6 participants unsubscribed during the course of the study. 5 provided unsubscription motivations: 2 participants indicated a disinterest in receiving recommendations (\"I thought it was something else\"). 1 indicated that they were no longer using CaringBridge due to the death of their loved one. 2 indicated a lack of time to engage with the recommendations, with one adding in addition: \"it's We received four replies to our final \"thank you\" email. Only one non-clicking participant replied, indicating they had seen a few of the Site Suggestion emails but focused on using CaringBridge to get support from their existing network. \"CaringBridge filled its purpose and functioned as expected. ... I received a lot of care and support from friends. \" We received three replies from participants who clicked at least once, with all responses emphasizing the importance of some kind of common ground. One participant connected first with a site due to common ground-the patient and participant had previously lived in the same small suburb-and then kept following the site due to its use of poetry and song lyrics to process grief. One participant described reading others' sites as \"helpful and informative\", due both to feeling comforted seeing a similar person navigate treatment and to learning strategies when writing their own Journal updates. One participant decided not to follow some of the recommended sites because they had a lot of existing followers. \"Having a Caringbridge site I know what it's like when you don't have a lot of supporters. ... So I try to help by leaving comments 99% of the time on the sites I follow to try and help the caregivers stay positive and give them kudos for all they are handling. \" 5.2.3 Recommendation characteristics. Our thematic analysis identified three high-level themes. We report the first-batch prevalence of categories identified from these themes in Table 10-see further thematic description and quantitative summary in Appendix F.\nDisclosing health status, symptoms, and treatment. Previews that address the question, \"what or how is the patient doing?\" Authors report on both recent heath updates and future plans or expectations. \"Friday is a huge milestone for me. My chemo port is going to be removed. \" Disclosures vary from reporting on processes (\"Sally is getting better day by day\"), to discrete events (\"Bryan met with his neurosurgeon yesterday. \"), to transitions (\"Kristen is finally off the ventilator. \").\nCommunicating emotion and reflection. Previews that express the author's attitudes or reflect on author or patient experiences. Emotions are either linked to specific experiences during the health journey (\"we are immensely happy to finally be home\") or characterize the author's current mental state (\"I wish you could experience this wonderful euphoria I am feeling\"). Reflection is often used as the introduction to a narrative (\"It's been 12 weeks since Beth's accident. I cannot believe it's been that long... \").\nManaging author/audience relationship. Previews that engage with the reader. Includes requests, expressions of gratitude, and context about the update, author, or writing process. Requests ", "publication_ref": ["b102"], "figure_ref": ["fig_7"], "table_ref": ["tab_12", "tab_13"]}, {"heading": "Observed behavior on CaringBridge", "text": "We describe the observed impact of the recommendation intervention on behaviors: reading (sec. 5.3.1), interaction (sec. 5.3.2), and second-order effects on other behaviors (sec. 5.3.3). Participant reading and interaction behaviors are the primary outcomes and are summarized in Table 11.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_14"]}, {"heading": "Reading behavior.", "text": "Table 11 presents counts for both repeat visits and site Follow actions. Follows were rare; only five participants ever followed a site. Repeat visits were more common: 86 (39.1%) of 220 clicked recommendations were visited a second time, and collectively accrued 589 repeat visits during and after the study.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_14"]}, {"heading": "Interaction behavior.", "text": "Nine participants interacted with recommended sites, which is 30% of those who visited at least one recommended site. Collectively, 33 sites received 948 additional interactions as a result of this study, although the majority of these accrue to just a few sites: median interactions with a single site was 6 (M=28.7; SD=56.9). Participants initiated with 38 non-recommended sites during the course of the study as well-likely with sites authored by people they already knew [63]. Thus, participation was associated with a 94.7% increase in total initiations during the study period. 25 71.7% of the interactions were reactions: we discuss text-based interactions next. 26Only four participants interacted using comments or guestbooks, resulting in 20 participant/site dyads in which interaction occurred. Our qualitative analysis identified two areas of interest: firstcontact strategies and potential norm violations. We observed a diversity of first contact strategies, varying from formally introducing the self (\"I am managing a CaringBridge site for my sister. ... Your site was mentioned in an email from CaringBridge, I took interest in your story. \"foot_28 ), to general expressions of support (\"Hope you get some rest soon\"), to establishing common ground by sharing their personal health experiences (\"I kind of know the road you are traveling. [personal health history]\"). A larger study could investigate first-contact strategies that are particularly effective, either at encouraging a response or at providing useful support. In addition, we observed \"potential norm violations\": situations where participant comments diverge from other visitor comments. These divergences include specific behaviors: first-time sympathetic comments on posts announcing the death of the patient, being the first and only commenter on a Journal update, bringing in potentially-unwanted religious messaging (\"God bless you\"), and asking explicit questions of the update author. The commenting norms perceived by \"power user\" peers may diverge from most visitors, enabling them to provide support in situations others may not, but running the risk of producing uniquely negative experiences. P1 provided the only comment on a death announcement update: \"I am so very, very sorry for the passing of your loved one.\" We have no evidence of how these comments were received by authors of recced sites, with no specific objections either in reply to comments or to CaringBridge help/support lines.\nThe author of a recced site responded to a participant comment in only a single instance, 28 and their subsequent interactions expanded into a relationship. While one relationship is insufficient for generalization, we sketch it here as a rich example. P1's first contact with R1 shared personal experiences and offered support. \"You don't know me but you sound like you're handling this all very well. .... I know what it's like to get chemo and I know what it's like after. \" Within weeks, R1 left supportive comments on P1's recent Journal updates (\"I can't even begin to imagine how strong you are\"). Support exchange continued, with evidence that off-CaringBridge communication had been established. Two months into their relationship, R1 explicitly referenced P1 in a Journal update (\"I have a friend, I think she'd agree with that title. She somehow found me through Caring Bridge, and has been a constant support to me and my family since. \") As a frequent commenter, P1 occasionally generated further discussion (Visitor: \"I agree with [P1]. \") and on one occasion was thanked for their support by R1's relative. We take the creation of a new relationship as an important existence proof for the benefits of peer recommendation, although it is difficult to quantify the expected number of relationships per recommendation from this single example. P1 is much more active than other authors, being on the 97th percentile by number of Journal updates and the 99th percentile by number of initiations and interactions. Will authors that are less active still form peer relationships given the opportunity? We estimate the sample sizes needed for a larger trial in sec. 5.3.4.", "publication_ref": ["b62"], "figure_ref": [], "table_ref": []}, {"heading": "5.3.3", "text": "Second-order effects on participant and recommended site behavior. Estimates of the participation effect are shown in Figure 10. 29 While the associational effect of recommendation indicates that participants publish more Journal updates than non-participants, we suspect this is primarily due to participants' shorter tenure, and the effect disappears after adjusting for author tenure. We present a selection of outcomes related to visiting and interacting with others' sites (specifically excluding any sites recommended during the study); none of these estimates suggest a significant positive or negative effect at the 95% significance level. We conclude that receiving recommendation emails had a small or high-variance effect on participant behavior-and no clear harms.\nEstimates of the visit effect are shown in Figure 11. The impact of an individual visit should be small, so we expect and observe in practice small effect sizes. We observe that peer visits are linked to additional author interactions on their own site and on peer sites, although these effects are    11. Visit effect: Estimated impacts of receiving a stranger visit on recommended site behavior in the 90 days after the visit when compared to non-clicked recommendations (square, dark-gray) and pseudo-control recommendations (circle, light-gray). Peer visits and interactions count non-participant author behavior directed at the site. Recommended site author interactions count behavior of authors of the site. The OLS estimate adjusts for pre-study activity on CaringBridge while the doubly robust (DR) estimate adds additional causal identification assumptions. 95% confidence intervals are computed via bootstrapping (1000 iterations). smaller after adjustment. Absent clear evidence of a harmful effect on associated behaviors, and given the successful primary behavior manipulation, we recommend proceeding to an RCT for the intervention.", "publication_ref": [], "figure_ref": ["fig_1", "fig_1", "fig_1"], "table_ref": []}, {"heading": "Raw", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "5.3.4", "text": "Sample sizes needed for a powered RCT. Figure 12 shows the sample sizes needed for an uncontrolled replication of our feasibility study, based on the observed visit and interaction behavior. If sending a one-time recommendation email, at least 314 authors should be included in order to obtain a reliable estimate of total peer interactions with (and visits to) recommended sites. Designing a trial to estimate the effect of recommendation on relationships is more challenging, as no relationships formed due to the first batch of Site Suggestion emails and only one relationship formed during the entire study. Based on that one relationship, a recurring email study would need 478 participants to detect at least 1 relationship per 79 participants (\ud835\udc51=0.11, \ud835\udefc = 0.05, \ud835\udefd=0.2), although 10 times that number of participants would be needed if the proportion of participants that form relationships is closer to 1 in 1000 than 1 in 100-and tens of thousands are needed if relationship formation is uniformly probable across participant/recommendation pairs (as only 1 in 4190 recommendations led to a relationship). We also estimated effect sizes relative to the pseudocontrol group (see full results in Appendix H). Based on unadjusted effect size differences between those groups, a 300-participant recurring-email RCT-with 50% not receiving recommendation emails-should be sufficient to estimate the relative impact of recommendation on total peer visits and interactions. Assuming the same click rate we observed in this study (5.3%), such an RCT would also be sufficiently powered to investigate any potential negative impact of peer visits on recommended site update frequency.", "publication_ref": [], "figure_ref": ["fig_10"], "table_ref": []}, {"heading": "DISCUSSION: FEASIBILITY OF A PEER RECOMMENDATION INTERVENTION", "text": "In this study, we implemented a peer recommendation system in order to assess its feasibility as a behavior-change intervention. We collected evidence for feasibility in five areas (Table 1): Demand, Implementation, Practicality, Acceptability, and Efficacy. Here, we summarize the evidence in each feasibility area and the implications for the future development of peer recommendation interventions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Demand", "text": "Demand refers to interest in the intervention. Specifically, we collected evidence around demand via prior use (sec. 3.1), expressed interest (sec. 5.1.2), and actual use (sec. 5.2.1). Prior use indicates both a large number of peer author interactions and correlations between interaction with peer authors and retention. Peer interaction includes interactions with peer strangers as well: 40% of participants reported previously visiting the site of a stranger. Participants indicated a motivation to connect with peers, although less interest in interaction specifically; 80% reported an interest in learning from the experiences of others, compared to 46% reporting an interest in communication \"with a peer who understands\". These expressions of interest were matched in practice with relatively high recommendation click rates-5% overall, 14% in the first batch of emails.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Implementation", "text": "Implementation refers to the tangible design and engineering required to implement the intervention. Specifically, we collected evidence around implementation requirements for both the recommendation interface (sec. 3.2) and the recommendation model (sec. 3.3). We used email as the recommendation medium to adapt peer recommendation to the CaringBridge context, due to the ubiquity of email notifications on the platform. However, a text-centric email interface may be inappropriate on platforms that emphasize non-text content or want to provide recommendations in \"always-available\" recommendation interfaces that may be more familiar to users. Future studies should adapt the interface design to the platform-integrating recommendations into OHC interfaces as appropriate-although we found that email recommendations were reasonable and accepted by participants. Representing peer profiles in an interface remains an important open question for future work [42]; text previews were effective, but their focus on recency is a trade-off compared to curated profiles or previews that attempt to highlight the expected utility of the recommended site to the viewer e.g. via explanations [130]. The model underlying the interface was based on implicit feedback from historical peer interactions. Future peer recommender systems should carefully consider the available implicit feedback and the implications of optimizing for historical patterns when new interaction patterns are desired. For example, learning from the experiences of others was the most common participant motivation, but learning was likely not the primary motivation for authors' historical interactions-collecting explicit feedback could supplement or replace this implicit feedback to better align the training objective and user motivations.", "publication_ref": ["b41", "b129"], "figure_ref": [], "table_ref": []}, {"heading": "Practicality", "text": "Practicality refers to requirements for administering the intervention in practice. Specifically, we collected evidence of practicality by investigating model quality (sec. 3.3.4), required data (sec. 3.3.5), and compute time (sec. 3.3.6). We presented an offline evaluation based on historical data that enables model comparison. In our context, none of the non-personalized approaches we considered were effective at capturing historical peer interaction dynamics, but such approaches may still be appropriate in other contexts. For example, if only activity data is available, it may be reasonable to form peer cohorts based on sign-up time and activity level [39]. If interaction data is available, our results suggest that traditional interaction-based recommendation models may be effective at recreating historical patterns without the need for elaborate disclosures. The RoBERTa-based similarity approach we considered was generally ineffective; the utility of sensitive health disclosures to peer recommendations needs further consideration, such as using feature extraction approaches targeted to the domain e.g. activity or health role classifiers [63,130]. We did not attempt to compare multiple models during our field study; future investigations will need to link the specific peer connection benefits sought with the type of modeling approach used and conduct appropriate online evaluations. Given our interest in interaction, we recommended only recent sites in order to make candidate scoring more practical-\u224813K active authors vs 1 million total authors-but a system focused on recommending historical information or completed journeys might consider other candidate-generation approaches [67].", "publication_ref": ["b38", "b62", "b129", "b66"], "figure_ref": [], "table_ref": []}, {"heading": "Acceptability", "text": "Acceptability refers to how participants react to the intervention. Specifically, we collected evidence of acceptability via explicit participant preferences (sec. 5.1.3) and feedback (sec. 5.2.2). Pre-study, the characteristics identified as most important for potential peer connections were a similar diagnosis, treatment, and engaging writing. These were the most frequently mentioned characteristics in recommendation feedback as well, with the addition of similar age. In general, the recommendations were only modestly acceptable to participants; 39% of specific recommendations were deemed irrelevant, frequently due to a lack of common ground between the participant and the recommended site. We chose a modeling approach that optimizes for interactions and does not explicitly reward similarity. Future peer recommenders might consider including features that capture the specific aspects of similarity deemed most important to users. For example, users might volunteer information about health condition, or it might be inferred from existing disclosures [66]. We received several explicit requests from users to alter the types of recommendations they were receiving, and one request from a user for the ability to filter the recommendations. Functionally, these requests speak to the coordination role played by a recommendation system. In clinical contexts, a human coordinator can incorporate this feedback to alter their peer matching approach on an individual level [111]. An algorithmic recommendation system faces a much greater challenge soliciting useful feedback on social matches [112]. By offering tooling to provide feedback, recommendation users can self -coordinate. Designing for self-coordination introduces trade-offs between user learning, system explanations, and ease of use [30]. An alternative might be designing peer matching systems with a human coordinator \"in the loop\"; the recommendation system could function as an assistance tool to facilitate matching thousands of peers simultaneously.", "publication_ref": ["b65", "b110", "b111", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "Efficacy", "text": "Efficacy refers to how much the intervention affects the desired behaviors. We collected evidence of efficacy by examining both reading and interaction behavior effects (secs. 5.3.1 and 5.3.2) as well as second-order behavior effects (sec. 5.3.3). Given the small sample and the uncontrolled design, we can only evaluate efficacy in a limited way, although results are promising. 39% of visited sites were visited a second time and a majority (57%) of participants who clicked at least once went on to visit at least one recommended site twice, suggesting that participants were interested in reading about ongoing health journeys. A smaller percentage (30%) of participants interacted with at least one recommended site, and only one reciprocal relationship formed as a result of the recommendations. Unclear social norms around interaction with strangers may present a barrier to greater interaction, such as whether an author is open to receiving unsolicited support [63]. Future design work could explore soliciting indicators of openness to supporter interaction and first-contact writing assistance for peer supporters who are not sure what to say [86,103]. We observed no evidence of second-order harms due to recommendations. These results suggest that efficacy should be evaluated in a larger, controlled trial.\nAll of the interactions we observed in this study-excepting the one relationship-saw our participants providing support to others. Being a peer supporter may by more attractive and accessible than being a support recipient [111]. On one hand, a recommender system that encourages a supporter role runs the risk of generating self-fulfilling prophecies and preventing exploration of other roles [32]-leading users to deprioritise their own needs [1]-as well as contributing to exclusion of new users and reinforcement of existing cliques [77]. On the other hand, exposure to peer recommendations might lead to more exploration than what users choose without this scaffolding [109], and supporting others might serve as the endpoint for a personal transition to working in broader service of a health community [51,64]. In general, the impacts of recommendation on community social dynamics are hard to predict [78]-and it is for that reason that future trials are necessary. Our feasibility study increases our confidence that peer recommendation will have positive benefits, including for equity. While only 1 in 40 of pre-recommender initiations are with sites without prior peer interactions, 1 in 10 of the model recommendations are for these siloed sites. In future studies, recommendation and visit dynamics should be carefully monitored for harmful social dynamics or intervention-generated inequalities [119].", "publication_ref": ["b62", "b85", "b102", "b110", "b31", "b0", "b76", "b50", "b63", "b77", "b118"], "figure_ref": [], "table_ref": []}, {"heading": "ETHICAL CONSIDERATIONS", "text": "This research was deemed exempt from full review by the University of Minnesota Institutional Review Board. All usage data was collected in compliance with the CaringBridge terms of service and privacy policy. To mitigate potential negative impacts from unwanted negative contact, we restricted the pool of recommended sites to include only sites with the lowest privacy settings i.e. those that are indexable by search engines and visible to all visitors.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "LIMITATIONS & FUTURE WORK", "text": "Our initial feasibility study of peer recommendation has significant limitations, although we are excited by the prospect of additional research developing the systems and interventions we consider. We developed and evaluated only a single interface and a single model. Our interface follows existing design patterns in CaringBridge notification emails, but a more iterative design process could produce more useful representations of recommendations [41]. The model we use is not \"state-of-the-art\", but represents a reasonable modern approach. Future studies should conduct more extensive offline and online model comparisons [9]-including with models that are focused on specifically facilitating reciprocal matching [83].\nWe looked at the potential impact of the recommendations for 12 weeks during the field study and 13 weeks post-study. We saw no influence of participants' visits on the journaling behavior of recommended sites in the 13 weeks post-study, but our design prevents us from analyzing any longer-term influence on the authors of recommended sites. In particular, authors that perceive a change in their audience after visits or interactions from peers might change the content of their Journal updates. Writing updates involves self-disclosure, and self-disclosure can lead to vulnerability and potential disappointment in the absence of reciprocation [107]. For an audience of peers, authors may be less likely to disclose negative information [62], which could decrease the potential benefits of expressive writing and self-reflection [70] and, unintentionally, lead to posts that are less useful for both peer and non-peer readers. Future studies should collect explicit feedback from recommended authors to understand the perception of authors toward peer visitors. The relationship between perceived audience and self-disclosure behavior remains an important open question to consider before deployment of peer recommendation systems that may change that audience. As an intervention, the peer recommender system we evaluated was intended to target behaviors correlated with social support benefits. We proposed an RCT focused on quantifying the increase in reading and interaction as a result of exposure to peer recommendations. However, due to the gap between received and perceived support [89], an increase in these behaviors may not produce corresponding increases in perceptions of social support. Future experimental designs could include pre-post self-report measures, either for social support directly (e.g. using the social connectedness scale [61]) or for desired impact on downstream health (e.g. using the perceived stress scale [60] or measures for health-related quality of life [28]).\nWe designed and evaluated the peer recommender system only with users of a single OHC. We argue these results generalize most to OHCs with similar affordances to CaringBridge. Rains argues that communication technologies for social connection have four primary affordances: visibility, availability, control, and reach [89]. Control, for example, is the potential to manage interactions, while availability is the potential to interact at particular times or places, so other text-based asynchronous communities provide similar potential. Visibility is the potential to make one's self known to others or to observe others' behavior; on CaringBridge, visibility is linked to specific blogs and communities that form around those blogs. Forums or listservs offer very different trade-offs around visibility of user-generated content, as do OHCs with rich user profiles [32,41]. Reach is the potential to contact specific individuals, groups, or communities: CaringBridge provides reach only to specific individuals known to the user by name [63]. Peer recommendation might function differently in an ecosystem where other social discovery features are already present [129]. Future work should explore peer recommendation in OHCs with diverging affordances from those provided by CaringBridge-such as Q&A communities.", "publication_ref": ["b40", "b8", "b82", "b106", "b61", "b69", "b88", "b60", "b59", "b27", "b88", "b31", "b40", "b62", "b128"], "figure_ref": [], "table_ref": []}, {"heading": "CONCLUSION", "text": "This paper established the feasibility of peer recommendation interventions for online health communities. We explored peer recommendation as an approach to make finding peers easier, conceptualizing it as an intervention to increase peer connection behavior. Our peer recommendation system meets a demand for learning from the experiences of others and demonstrates the practicality of content-based models implemented in an email interface. During a 12-week field study, peer recommendation emails successfully led participants to read about and interact with peers-with no evidence of harmful second-order effects. We hope that our specific implementation serves to promote future investigation of peer recommendation: including models, interfaces, and outcomes for both individuals and communities.  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A USE OF SEARCH FEATURE ON CARINGBRIDGE", "text": "Do CaringBridge users use search to attempt to find information or supporters? To address this question, we collected a dataset of user-initiated search queries on CaringBridge. These queries were extracted from internal logs collected between July 4, 2021 and July 10, 2021. Our dataset contained 103,830 searches comprising 32,722 unique query strings. We preprocessed the query strings by splitting them into tokens based on whitespace.\nBased on a random sample of 100 queries with 1, 2, or 3 tokens and visual inspection of additional samples, all queries corresponded to either person names or existing site URL strings. Thus, we conclude that a very high percentage of searches are for a specific CaringBridge site. Queries with 4 or more tokens comprise < 1% of the query strings. Visual inspection suggests that the majority of queries with 4+ tokens are help requests or open-domain queries including spam. We could identify no instances of users searching for e.g. a particular health condition, treatment, or symptom; if search is used in this way, it occurs at a low prevalence. Results were identical when isolating to queries conducted by authors rather than by all users.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B MODELING AND OPTIMIZATION DETAILS", "text": "This section provides additional details beyond those provided in sec. 3.3.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.1 MLP model details", "text": "Both MLP Study and MLP Tuned use two hidden layers, ReLU activation functions, and dropout. Optimization occurs over 1000 epochs with a one-cycle learning rate scheduler proposed by Smith and Topin [105] and the Adam optimizer with default hyperparameters. Following best practices to prevent overfitting, we used for prediction the weights from the epoch with the lowest hold-out loss. Hold-out loss was computed over a random 1% of training initiations. We conducted random search experiments (not reported) using the learning rate and Adam hyperparameter distributions given by Sivaprasad et al., finding minimal differences [101]. MLP Study uses 100 hidden units per layer with a dropout \ud835\udc5d of 0.1. MLP Tuned uses 300 hidden units per layer with a dropout \ud835\udc5d of 0.5 and adds a weight decay of 0.0001. Hyperparameter tuning occurred using grid search to set hidden units \u2208 {100, 300, 500}, dropout \ud835\udc5d \u2208 {0.1, 0.5, 0.9}, weight decay \u2208 {0, 0.0001, 0.01}, and maximum learning rate \u2208 [0.008, 0.016]. We fit models from 3 random seeds at each hyperparameter combination and selected parameters based on the median of the 3 models' MRR. We did not explore additional model architectures in depth, and offline metrics could be improved through the use of a model closer to the state-of-the-art or through a larger hyperparameter sweep. One downside to our chronological validation and testing sets is that this setup deviates from the common modeling practice of retraining on regular intervals. Fortunately, Figure 13 indicates that this affect has a minimal impact on our offline evaluation.", "publication_ref": ["b104", "b100"], "figure_ref": ["fig_12"], "table_ref": []}, {"heading": "B.2 Baselines", "text": "Table 12 compares additional baselines beyond those shown in Table 4. We explored CosSim with other feature sets; they all perform worse in terms of MRR and Coverage metrics than using all features. MF is the conventional matrix factorization approach to collaborative filtering, using the dot product to compute similarity and selecting hyperparameters as described by Rendle et al. [94]; 30 note the high hit-rate but low MRR, indicating a strong popularity bias. The new nonpersonalized baselines are RecentInits, which ranks sites by the amount of time since the last initiation with that site, MostJournals/RecentJournals, which mirror MostInits and RecentInits but count published Journal updates rather than initiations, NewestAuthor, which ranks newest sites first, and MostInteractive, which ranks sites by the number of recent interactions made by authors on that site.", "publication_ref": ["b93"], "figure_ref": [], "table_ref": ["tab_16"]}, {"heading": "B.3 Feature ablation discussion", "text": "The results presented in Table 5 suggest that RoBERTa text features are not important for peer recommendation, and that recommendations based solely on text data would require a different approach than the one we use here. We suspect that the relative unimportance of text data reflects a bias in our implicit feedback signal and offline evaluation metrics toward authors already known to the source. Chen et al. observed that social network information was more effective for discovering known contacts, while content similarity was more effective for discovering new connections [17]a dynamic that may be recurring here, as most historical initiations are between known contacts [63]. Even if text-based recommendations are less useful, users may perceive them to be less invasive than network-based recommendations (or vice-versa). Careful assessment of the perceived acceptability of this data collection for peer recommendation is necessary [27] ", "publication_ref": ["b16", "b62", "b26"], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "D CLICK DATA", "text": "We identify recommendation clicks in the Site Suggestion emails via three data sources: (1) Google Analytics counts, (2) CloudFront logs, and (3) logged-in user visits. Links to the recommended sites contain UTM tracking information, including the site, the batch ID, and a unique participant ID.\nEach data source has limitations. The Google Analytics summary can only provide a total count and won't be incremented if is disabled. An event won't be captured in the CloudFront log if UTM tracking tags are stripped and in unknown other cases. 33 Logged-in visits will only be recorded at or near the time of a recommendation click if the participant is already logged in or logs in while on the site.\nGoogle Analytics reports 270 total recommendation clicks, while we have timestamped Cloud-Front log entries for only 232 clicks, suggesting that we are missing timestamps, participant, and site information for 14.1% of clicks. Of the 232 CloudFront clicks, 198 correspond to unique participant/site clicks. We can recover 22 clicks missing from the CloudFront request logs via the logged-in user visits, for a total of 220 total unique participant/site clicks. For subsequent statistical analyses, we assume that any additional clicks are missing at random, although in practice we are more likely to be missing clicks from participants who do not log in. If we assume that the Google Analytics count is authoritative and 14.1% of clicks are missing at random from the CloudFront data, then the expected number of missing participant/site clicks is 10. Thus, we expect only a small impact on our statistical analyses. Logged-in visit data are extracted from daily snapshots of the CaringBridge database. Thus, we only captured the most recent logged-in visit within a 24-hour period, sufficient to identify daily repeat visits but insufficient for fine-grained analysis of browsing behavior.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E SITE RECOMMENDATION ANALYSIS", "text": "In sec. 5.2.3, we describe characteristics of the recommended set of sites. In this section, we provide additional pre-click details and use them to create a pseudo-control comparison set of non-recommended sites.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E.1 Site filtering", "text": "In sec. 3.3.6 we discuss a decision limiting the total number of times a site was recommended in a single batch to at most 10 times. Specifically, we conducted a five-round draft. In each round, a random participant ordering is chosen and participants take turns selecting their highest-scored site that has been picked fewer than 10 times. We analyze the impact of this decision in Figure 14 by plotting the mean of the rank and score distributions for hypothetical recommendation sets generated without filtering. We observe that the maximum model score varies between batches as the model was retrained weekly, and that the filtered sites were similar in score to the sites that would have been recommended without filtering-all filtered sites were still in the top 0.1% of the model's scores. The use of filtering resulted in a 32.5% increase in the number of unique recommended sites, a fair trade-off for the modest decrease in score and rank induced by filtering.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "E.2 Psuedo-control set of non-recommended sites", "text": "The pseudo-control set is composed of the top five sites for each participant within a batch of recommendations that were never recommended during the duration of the study. We visualize this Table 13. Observed recommended pseudo-control USP site behavior from the 35 day window before the point they were clicked (or could have been clicked). Site tenure is in days, Total # of authors is since the sites creation, # of authors and # days visiting peers is for the entire window period, while all other rows are the number of weekly actions.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Recommended", "text": "Pseudo and feelings about personal experiences\" [70]. We adapt this definition to the context of Journal previews by including personal health-related reflection as a component of expressive writing. Using these categories, two researchers separately annotated 658 update previews in three rounds of coding. The researchers met between each round to discuss disagreements and update the codebook. We report pre-discussion IRR scores in Table 17.\nWe used the resulting annotations to model the relationship between these categories and clicks. This problem is the \"post-presentation reward prediction\" problem [31]. Unfortunately, clicks are noisy and it is not possible to directly assess if the presence of a particular category is associated with a greater propensity to click. This impossibility results from confounding: the rank of the recommendation in question, the other recommendations in the same email, when the email was generated and opened, and the specific participant are all confounding factors. Instead, we build multiple models with varied sets of assumptions and look for convergence. Specifically, we assume that clicks are independent-a common assumption in click models [18]-and report analysis for three subsets of recommendations: (a) Clicked Batches Only includes only recommendations in emails for which some but not all of the recs were clicked, so aims to acquire the clearest view of choice among competing options. (b) B1 Only includes only recommendations sent in the first batch of emails, eliminating temporal confounding and reducing bias introduced by varying levels of participant activity. (c) Clickers Only includes only recommendations sent to participants who clicked at least once during the study, reducing bias introduced by never-seen recommendations. We fit logistic regression models to predict individual recommendation clicks. While we report only rec-level logistic regression models here, email-level models, multi-level models for participant and batch, models including subsets of the features, and feature transforms revealed similar patterns.", "publication_ref": ["b69", "b30", "b17"], "figure_ref": [], "table_ref": ["tab_21"]}, {"heading": "F.2 Quantitative content analysis results", "text": "We summarize the relationship between all four categories and click behavior in Table 18. None of the models with any subset of the variables outperforms the null model (F-test \ud835\udc5d > 0.05)-even the Pre-print date: September 2022. Reporting patient/caregiver status Past vs Future News An axis that differentiates past occurrence and future plans.\nThis last week was oh so busy with all of my tests and appointments.\nFriday is a huge milestone for me.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_22"]}, {"heading": "Events", "text": "Those describing events, specifically related to a singular point in time.\nBryan met with his neurosurgeon yesterday. Patient Symptoms, Status, and Health Processes Those detailing how the patient is doing, and health processes related to a prolonged interval of time.\nWell, Sid is getting stronger by the day.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Beginnings, Endings, and Transitions", "text": "Those singular health events that transition the patient into or out of a health process.\nKristen is finally off the ventilator.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Status sentiment", "text": "An axis that defines the sentiment of reported news (as \"good\" or \"bad\").\nToday Ashley had her second day of PT and she did amazing in every way.\nWe waited all night for and unfortunately he'll have to go through another round of chemo. Emotion-laden Reporting Those explicitly stating the author's attitudes towards an update using emotive language.\nTears of joy as I write this. We are sooo happy to announce Andrea is out of the ICU.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Reflection", "text": "Those reflecting on the author's or patient's experiences.\nIt's hard to believe it was only a year ago today that Jen started chemo.\nManaging author/audience relationship Expressions of Gratitude Those that positively acknowledge a specific type of support.\nWe are so thankful for your prayers.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Update Context", "text": "Those that explicitly provide contextual information about the post or its contents for the audience.\nToday is going to be a little different. I don't have any medical updates, I just need to let some stuff out.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Comments on Update Frequency", "text": "Those acknowledging the time between multiple CaringBridge updates.\nFirst, I need to apologize for how long it's been since our last update.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Reflection on Writing Process", "text": "Those reflecting on the author's experiences as a Caring Bridge author. I've really struggled to put my feelings into words in these posts. rank-only model. 34 Expressive writing was associated with a greater probability of being clicked among Clicked Batches (50% of recs with EW clicked vs 36% of recs without) and in B1 (47% of recs with EW clicked vs 29% of recs without), but not when considering all recs sent to clicking participants. Expressions of appreciation were associated with a decreased probability of being clicked among participants who clicked at least once (7% of recs with EOA clicked vs 12% of recs without) but not in the other subsets. Both of these results should be taken with a large grain of salt, and in general these results suggest that participants were making clicking decisions based on factors we did not annotate.  is reasonable to assume that authors that opted to participate in a recommendation intervention are more likely to engage with a recommendation intervention than non-participating authors, independent of e.g. their tenure as an author or their prior peer interaction behavior. If participants are not very different from non-participating authors in terms of their responsiveness to treatment, then it may still be reasonable to assume exchangeability. The same reasoning holds for clicked and non-clicked sites. \u2022 Positivity Positivity requires that the distributions of the covariates for the treated and pseudo-control groups to fully overlap. Positivity is approximately true in our data, i.e. all values of the covariates observed in the treated group are also observed in the pseudo-control group. The positivity assumption is why we omit causal estimates of the impact of deploying Site Suggestion emails to the whole author population in terms of e.g. click rates; we have no ability to estimate the behavior of the pseudo-control population when exposed to Site Suggestion emails. \u2022 Consistency Consistency refers approximately to the assumption that the treatment varies only in ways unrelated to the outcomes. Recommendation varies, so participants will be exposed to different versions of the treatment; different participants are shown different recommendations, so clicked sites will be exposed to visits from different participants. Intuitively, these differences can be causally related to the outcomes: a participant with no prior interaction behavior, for example, may receive less relevant recommendations than a participant with a long history of peer interaction. A site clicked by a participant that visits and leaves a comment is different than a site clicked by a participant that only visits. Joachims et al. argue that recommender systems ought to be viewed as policies that select interventions in order to optimize a desired outcome [55]; it's a policy designed to select variable interventions. By assuming consistency, we assume that the recommendations are of similar quality and that participant visits have similar effects on the visited sites. The degree to which this assumption is violated will bias the resulting estimates. \u2022 No measurement error Measurement error in the observed behaviors is likely non-existent, given the use of a complete database snapshot. \u2022 No model misspecification To adjust for confounding, the models we use need to include all relevant covariates and assume the correct functional form. By fitting linear models, we make a parsimonious but possibly false assumption of functional relationship between the covariates and the outcome of interest. More serious is the assumption that all relevant covariates are available; as discussed, we believe that unmeasured confounders such as an interest in peer connection affect response to treatment in a way that is independent of the measured covariates.\nGiven this discussion, it seems reasonable to object that three of the assumptions are very likely false. In practice, we are assuming that these assumptions are close enough to true, such that we can still attain some insight from computing causal estimates based on these assumptions in order to compare to the observational estimates.", "publication_ref": ["b54"], "figure_ref": [], "table_ref": []}, {"heading": "H SAMPLE SIZE CALCULATIONS", "text": "In sec. 5.3.4, we described a power analysis for an uncontrolled peer recommendation study. In this section, we provide additional details and add estimates for an RCT. In Fig. 12, we showed sample size estimates for two variations of a replication of our feasibility study by computing the effect of two author behaviors associated with receiving recommendations. Here, we extend this analysis to include 5 behaviors: unique repeat visits to recommended sites, interactions with recommended sites, unique repeat visits to all sites, interactions with all sites, and recommended site updates. We Pre-print date: September 2022.", "publication_ref": [], "figure_ref": ["fig_10"], "table_ref": []}, {"heading": "", "text": "We chose to deploy the model that included the text data (MLP Study ) in part because it will still provide personalized recommendations, even to authors who have not yet interacted with peers. We can observe this effect in the coverage predictions: while recommended site diversity is higher for authors who had previously initiated (342 unique sites for 439 source users, each site recommended on average 6.4 times) compared to authors who had no previous interactions (303 unique sites for 561 source users, each site recommended on average 9.3 times), these differences are relatively small. Further, these recommendations are of a similar quality: MRR on test initiations is actually higher for people with no previous initiations than previous initiators (0.190 vs 0.147). Thus, we were satisfied that deploying MLP Study satisfied our cold-start design requirement.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C SURVEY MATERIALS", "text": "All questions other than the eligibility, consent, and email questions are optional. All surveys were hosted on the Qualtrics platform.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.1 Preference Survey", "text": "All registered CaringBridge authors (18+ years old) are invited to take this 1-minute survey, which is part of a research collaboration by CaringBridge and a team of technology researchers at the University of Minnesota.\nThe purpose of CaringBridge is to help people get the support they need during health journeys-and support comes from many places! Many authors choose to make their CaringBridge sites open for anyone to read. We want to send you emails with links to CaringBridge sites that we think you'll want to follow.\nIf you are interested in participating in our study, we'll send you personalized emails with links to CaringBridge sites. (As always, [privacy comes first] 31 on CaringBridge: no one can read your site unless you want them to.) Opting in and sharing your opinion will help CaringBridge and the research community design features that make giving support easy and make a difference in the lives of the patients and caregivers you care about. Complete information and an FAQ for this study are available by clicking here [link].\nClick the arrow in the lower right corner of your screen to take the survey.  set's ranks in Figure 14 along with the resulting effect on the corresponding score assigned to each set by the recommendation algorithm. In Table 13, we see that the set of recommended author/site pairs were significantly different from the group of sites that could have been recommended (pseudo-control). Values were calculated at the time a recommendation was clicked; in the case of non-clicked and pseudo-control sites, a click time was randomly chosen from the set of actual click times from the same recommendation batch.\nIn Tables 14 and15, we extend this analysis to include the subset of clicked recommended author/site pairs. Again, we see many significant differences between the set of clicked recommended author/site pairs and the pseudo-control set. However, we find little significant differences between clicked and non-clicked recommended author/site pairs.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F THEMATIC ANALYSIS OF JOURNAL UPDATE PREVIEWS F.1 Quantitative content analysis methods", "text": "Our thematic content analysis identified three high-level themes (discussed in sec. 5.2.3)-the full set of themes with descriptions and examples is shown in Table 16. Based on these themes, we isolated four categories for quantitative analysis:\n(1) Reporting Health Status, Symptoms, and Treatment. Includes previews that update on specific health events, symptoms, or emotions of the patient. We include two sub-categories for positive and negative disclosures e.g. as used by Yang et al. [130]. Previews can contain both positive and negative disclosures if both are provided or if the author qualifies the news (\"in horrible pain, but finally a reason for hope\").\n(2) Managing Author/Audience Relationship. The author is visible through descriptions of their role as a writer or their relationship with their blog's readers. See thematic description above.\n\"writing these posts is somewhat cleansing. \" (3) Expressions of Appreciation (EOA). This theoretical construct was previously used by Smith ", "publication_ref": ["b129"], "figure_ref": [], "table_ref": []}, {"heading": "G ESTIMATES OF STUDY IMPACT ON BEHAVIOR FOR AUTHORS AND SITES", "text": "In sec. 5.3.3, we estimated the impact of recommendation on second-order behaviors. We provide additional method details and sensitivity analysis in this section. In Figures 10 and11, we showed post-study outcomes by comparing 5 weeks (35 days) of prestudy behavior to 12 weeks (91 days) of post-study behavior. We conducted a sensitivity analysis to determine the impact of two decisions: examining behavior during the post-study period (rather than the \"during study\" period) and choosing a time window of 12 weeks. Figure 15 is a high-detail summary of those decisions, demonstrating that differences are small depending on the selected time window: smaller post-study time windows are generally associated with higher-variance effect size estimates. In pre/post panel data analyses, it is common to use the same time window pre-and post-intervention, which helps to control for longer-term behavioral trends. We tried both approaches, and found no difference except increased variance, so we use data from the full pre-study time window (5 weeks) even if the post-study time window is less than 5 weeks.\nUsing a similar approach, we extend this analysis to investigate post-click outcomes on recommended site behavior. In Figure 11, we showed post-study outcomes by comparing 5 weeks (35 days) of pre-click behavior to 12 weeks (91 days) of post-click behavior. Similar to Appendix E, values were calculated at the time a site was first clicked and in the case of non-clicked and pseudo-control sites, a click time was randomly chosen from the set of actual click times in the first batch that site was/could of be recommended. We conducted a sensitivity analysis to examine the impact a participant visiting a recommended site had on the sites behavior during the post-click period. Figure 16 is a high-detail summary demonstrating we were unable to capture statistically significant effects from participant clicks on recommended site behavior. Moreover, we find that effect estimates based on comparison to the non-clicked and pseudo-control groups are similar.\nAs noted by Hern\u00e1n and Robins, successful causal inference predominantly rests on untestable assumptions [43]. The doubly robust estimates we produce depend on five assumptions:\n\u2022 Exchangeability Exchangeability refers to the probability of being in the treatment group is independent of the causal outcome (depending only on \ud835\udc34). Exchangeability is likely false: it   16. Estimated impacts of Site Suggestion emails on recommended site behavior post click, both using non-click sites (left column) and pseudo-control sites (right column). The estimators shown are as described in sec. 5.3.3: raw (solid, blue), OLS (dashed, orange), and DR (dotted, green). Figure 11 captures the vertical slice at week 13 in the right column. 95% confidence intervals are computed via bootstrapping (1000 iterations).\nconsider the 35 days before and after the first exposure to recommendation or first stranger visit for the one-time email and 35 days before and the study period plus 35 days after for the 12 week recurring email intervention.\nTable 20 outlines the mean, variance, and sample size for each group in terms of weekly actions used in the calculations. The structure of our data let us consider two potential future interventions: a one-time recommendation email and a recurring, weekly recommendation email. We estimate the effects of a one-time recommendation email by including only participant exposures and visits from the first batch of recommendation emails (B1). We use the pseudo-control group (see sec. 4.1.2 and Table 7) and non-clicked recommendations (see Appendix E) to analyze the effect of receiving recommendations and stranger visits. For participants (recommended sites) behaviors, we report weekly actions from a 5 week pre-inflection period to a post-inflection period: 12 weeks + 5 weeks and 5 weeks. For participants (all sites) and recommended sites behaviors, we report the difference in weekly actions from similar pre and post-inflection periods. Effect size calculations were made using 17 decimal places.\nUsing these statistics, Table 21 estimates the sample size required for a replication (same as Fig. 12 and for an RCT, with 50% not receiving recommendation emails. For author behaviors targeted at recommended sites, we compute the effect as simply as \ud835\udc51 = \ud835\udc40 \ud835\udc43 /\ud835\udc46\ud835\udc37. For all other behaviors, we control for prior behavior by computing the effect as \ud835\udc51 = (\ud835\udc40\n)/\ud835\udc46\ud835\udc37 \ud835\udc5d\ud835\udc5c\ud835\udc5c\ud835\udc59\ud835\udc52\ud835\udc51 , where P is the group of participants or clicked recommended sites and C is the control group of pseudo-control participants or non-clicked recommended sites. We present \ud835\udc40 \ud835\udc4f\ud835\udc52 \ud835\udc53 \ud835\udc5c\ud835\udc5f\ud835\udc52 -\ud835\udc40 \ud835\udc4e\ud835\udc53 \ud835\udc61\ud835\udc52\ud835\udc5f because the expected churn of users dictates over time they will become less active. We find this is true in our sample; all users that published at least one journal update in July 2021 subsequently published, on average, 1.32 (5.44) less updates in the following month. Here, a positive effect size means that being a participant results in less of a difference from before and after when compared to the control group. Using G*Power 3.1.9.7, the estimated required sample sizes for an appropriately powered RCT were calculated based on observed effect sizes using a one tailed point biserial model at 80% power with \ud835\udefc = 0.5 [26].\nUnsurprisingly, we see large differences in participant behavior towards recommended sites when compared to the pre-study period. More interesting is the fact that participants, on average, sustained an increase in weekly actions including non-recommended sites after receiving recommendations. While this translates to a seemingly large effect size during calculation, we attribute these differences primarily to the significant differences pre-study between both groups outlined in Table 4.1.2. Here, we believe any differences that exist between groups can largely be attributed to confounding variables (i.e. author/site tenure).  ", "publication_ref": ["b42", "b25"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Logging On, Reaching Out, and Getting By: A Review of Self-reported Psychosocial Impacts of Online Peer Support for People Impacted by Cancer", "journal": "Proc. ACM Hum.-Comput. Interact", "year": "2021-04", "authors": "Kimberley R Allison; Pandora Patterson; Daniel Guilbert; Melissa Noke; Olga Husson"}, {"ref_id": "b1", "title": "Considerations in Designing Digital Peer Support for Mental Health: Interview Study Among Users of a Digital Support System (Buddy Project)", "journal": "JMIR Mental Health", "year": "2021", "authors": "Nazanin Andalibi; Madison K Flood"}, {"ref_id": "b2", "title": "Social Support, Reciprocity, and Anonymity in Responses to Sexual Abuse Disclosures on Social Media", "journal": "ACM Transactions on Computer-Human Interaction (TOCHI)", "year": "2018-10", "authors": "Nazanin Andalibi; Oliver L Haimson; Munmun De Choudhury; Andrea Forte"}, {"ref_id": "b3", "title": "Peer mentorship to improve self-management of hip and knee osteoarthritis: a randomised feasibility trial", "journal": "BMJ Open", "year": "2021-07", "authors": "Anna M Anderson; Elizabeth C Lavender; Esther Dusabe-Richards; Teumzghi F Mebrahtu; Linda Mcgowan; Philip G Conaghan; Sarah R Kingsbury; Gerry Richardson; Deborah Antcliff; Gretl A Mchugh"}, {"ref_id": "b4", "title": "One-on-one peer support and quality of life for breast cancer patients", "journal": "Patient Education and Counseling", "year": "1998-10", "authors": "Cathy Fredrick D Ashbury; Shawna L Cameron; Margaret Mercer; Eleanor Fitch;  Nielsen"}, {"ref_id": "b5", "title": "A thematic analysis of patient communication in Parkinson's disease online support group discussion forums", "journal": "Computers in Human Behavior", "year": "2012-03", "authors": "Angelica Attard; Neil S Coulson"}, {"ref_id": "b6", "title": "Query Formulation in Web Information Search", "journal": "", "year": "2003", "authors": "Anne Aula"}, {"ref_id": "b7", "title": "Doubly Robust Estimation in Missing Data and Causal Inference Models", "journal": "Biometrics", "year": "2005", "authors": "Heejung Bang; James M Robins"}, {"ref_id": "b8", "title": "A Comparison of Offline Evaluations, Online Evaluations, and User Studies in the Context of Research-Paper Recommender Systems", "journal": "Springer International Publishing", "year": "2015", "authors": "Joeran Beel; Stefan Langer"}, {"ref_id": "b9", "title": "Anomalous States of Knowledge as a Basis for Information Retrieval", "journal": "The Canadian Journal of Information Science", "year": "1980", "authors": "Nicholas J Belkin"}, {"ref_id": "b10", "title": "Seeking Support on Facebook: A Content Analysis of Breast Cancer Groups", "journal": "Journal of Medical Internet Research", "year": "2011-02", "authors": "Jacqueline L Bender; Maria-Carolina Jimenez-Marroquin; Alejandro R Jadad"}, {"ref_id": "b11", "title": "How We Design Feasibility Studies", "journal": "American Journal of Preventive Medicine", "year": "2009-05", "authors": "Deborah J Bowen; Matthew Kreuter; Bonnie Spring; Ludmila Cofta-Woerpel; Laura Linnan; Diane Weiner; Suzanne Bakken; Cecilia Patrick Kaplan; Linda Squiers; Cecilia Fabrizio; Maria Fernandez"}, {"ref_id": "b12", "title": "Preferences for models of peer support in the digital era: A cross-sectional survey of people with cancer", "journal": "Psycho-Oncology", "year": "2018", "authors": "Allison Boyes; Heidi Turon; Alix Hall; Rochelle Watson; Anthony Proietto; Robert Sanson-Fisher"}, {"ref_id": "b13", "title": "What is the ideal support group? Views of Australian people with cancer and their carers", "journal": "Psycho-Oncology", "year": "2007", "authors": "Phyllis N Butow; Laura T Kirsten; Jane M Ussher; Gerard V Wain; Mirjana Sandoval; Kim M Hobbs; Katharine Hodgkinson; Annie Stenlake"}, {"ref_id": "b14", "title": "Recovery Amid Pro-Anorexia: Analysis of Recovery in Social Media", "journal": "ACM", "year": "2016", "authors": "Stevie Chancellor; Tanushree Mitra; Munmun De Choudhury"}, {"ref_id": "b15", "title": "Constructing grounded theory: a practical guide through qualitative analysis", "journal": "", "year": "2006", "authors": "Kathy Charmaz"}, {"ref_id": "b16", "title": "Make New Friends, but Keep the Old: Recommending People on Social Networking Sites", "journal": "ACM", "year": "2009-09", "authors": "Jilin Chen; Werner Geyer; Casey Dugan; Michael Muller; Ido Guy"}, {"ref_id": "b17", "title": "Click Models for Web Search", "journal": "Synthesis Lectures on Information Concepts, Retrieval, and Services", "year": "2015-07", "authors": "Aleksandr Chuklin; Ilya Markov; Maarten De Rijke"}, {"ref_id": "b18", "title": "Locating patient expertise in everyday life", "journal": "Association for Computing Machinery", "year": "2009", "authors": "Andrea Civan; David W Mcdonald; Kenton T Unruh; Wanda Pratt"}, {"ref_id": "b19", "title": "Social relationships and health", "journal": "American psychologist", "year": "2004", "authors": "Sheldon Cohen"}, {"ref_id": "b20", "title": "Social support and subjective burden in caregivers of adults and older adults: A meta-analysis", "journal": "PLOS ONE", "year": "2018-01", "authors": "Rafael Del Pino-Casado; Antonio Fr\u00edas-Osuna; Pedro A Palomino-Moral; Mar\u00eda Ruzafa-Mart\u00ednez; Antonio J Ramos-Morcillo"}, {"ref_id": "b21", "title": "Effect of peer support on prevention of postnatal depression among high risk women: multisite randomised controlled trial", "journal": "BMJ", "year": "2009-01", "authors": "C-L Dennis; E Hodnett; L Kenton; J Weston; J Zupancic; D E Stewart;  Kiss"}, {"ref_id": "b22", "title": "Simplify and Robustify Negative Sampling for Implicit Collaborative Filtering", "journal": "Advances in Neural Information Processing Systems", "year": "2020", "authors": "Jingtao Ding; Yuhan Quan; Quanming Yao; Yong Li; Depeng Jin"}, {"ref_id": "b23", "title": "The experience and impact of chronic disease peer support interventions: A qualitative synthesis", "journal": "Patient Education and Counseling", "year": "2013-07", "authors": "Gayathri Embuldeniya; Paula Veinot; Emma Bell; Mary Bell; Joyce Nyhof-Young; Joanna E M Sale; Nicky Britten"}, {"ref_id": "b24", "title": "I'm So Glad I Met You\": Designing Dynamic Collaborative Support for Young Adult Cancer Survivors", "journal": "ACM", "year": "2017", "authors": "Jordan Eschler; Wanda Pratt"}, {"ref_id": "b25", "title": "G*Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences", "journal": "Behavior Research Methods", "year": "2007-05", "authors": "Franz Faul; Edgar Erdfelder; Albert-Georg Lang; Axel Buchner"}, {"ref_id": "b26", "title": "Participant\" Perceptions of Twitter Research Ethics", "journal": "SAGE", "year": "2018-03", "authors": "Casey Fiesler; Nicholas Proferes"}, {"ref_id": "b27", "title": "Measuring Healthy Days", "journal": "CDC", "year": "2000", "authors": ""}, {"ref_id": "b28", "title": "Social Uses of Personal Health Information Within PatientsLikeMe, an Online Patient Community: What Can Happen When Patients Have Access to One Another's Data", "journal": "J Med Internet Res", "year": "2008", "authors": "Jeana Frost; Michael Massagli"}, {"ref_id": "b29", "title": "Do People Engage Cognitively with AI? Impact of AI Assistance on Incidental Learning", "journal": "Association for Computing Machinery", "year": "2022", "authors": "Z Krzysztof; Lena Gajos;  Mamykina"}, {"ref_id": "b30", "title": "Mixed methods for evaluating user satisfaction", "journal": "ACM", "year": "2018", "authors": "Jean Garcia-Gathright; Christine Hosey; Brian St; Ben Thomas; Fernando Carterette;  Diaz"}, {"ref_id": "b31", "title": "How HCI Bridges Health and Design in Online Health Communities: A Systematic Review", "journal": "Association for Computing Machinery", "year": "2021", "authors": "Do\u011fa Gatos; Asl\u0131 G\u00fcnay; G\u00fcncel K\u0131rlang\u0131\u00e7; Kemal Kuscu; Asim Evren; Yantac "}, {"ref_id": "b32", "title": "Beyond accuracy: evaluating recommender systems by coverage and serendipity", "journal": "Association for Computing Machinery", "year": "2010", "authors": "Mouzhi Ge; Carla Delgado-Battenfeld; Dietmar Jannach"}, {"ref_id": "b33", "title": "Shaping the Stream: Techniques and Troubles of Algorithmic Recommendation", "journal": "Cambridge University Press", "year": "2019", "authors": "K E Goldschmitt; Nick Seaver"}, {"ref_id": "b34", "title": "WTF: the who to follow service at Twitter", "journal": "Association for Computing Machinery", "year": "2013", "authors": "Pankaj Gupta; Ashish Goel; Jimmy Lin; Aneesh Sharma; Dong Wang; Reza Zadeh"}, {"ref_id": "b35", "title": "Do You Know? Recommending People to Invite into Your Social Network", "journal": "ACM", "year": "2009", "authors": "Ido Guy; Ronen ; Eric Wilcox"}, {"ref_id": "b36", "title": "Do you want to know? recommending strangers in the enterprise", "journal": "Association for Computing Machinery", "year": "2011-09", "authors": "Ido Guy; Sigalit Ur; Inbal Ronen; Adam Perer; Michal Jacovi"}, {"ref_id": "b37", "title": "The Patient Advice System: A Technology Probe Study to Enable Peer Support in the Hospital", "journal": "Proc. ACM Hum.-Comput. Interact", "year": "2020-10", "authors": "Shefali Haldar; Yoojung Kim; R Sonali; Andrea L Mishra; Ari H Hartzler; Wanda Pollack;  Pratt"}, {"ref_id": "b38", "title": "Supporting social recommendations with activity-balanced clustering", "journal": "Association for Computing Machinery", "year": "2007", "authors": "F Maxwell Harper; Shilad Sen; Dan Frankowski"}, {"ref_id": "b39", "title": "Array programming with NumPy", "journal": "Nature", "year": "2020-09", "authors": "Charles R Harris; K Jarrod Millman; St\u00e9fan J Van Der Walt; Ralf Gommers; Pauli Virtanen; David Cournapeau; Eric Wieser; Julian Taylor; Sebastian Berg; Nathaniel J Smith; Robert Kern; Matti Picus; Stephan Hoyer; Marten H Van Kerkwijk; Matthew Brett; Allan Haldane; Jaime Fern\u00e1ndez Del R\u00edo; Mark Wiebe; Pearu Peterson; Pierre G\u00e9rard-Marchant; Kevin Sheppard; Tyler Reddy; Warren Weckesser; Hameer Abbasi; Christoph Gohlke; Travis E Oliphant"}, {"ref_id": "b40", "title": "Leveraging cues from person-generated health data for peer matching in online communities", "journal": "J Am Med Inform Assoc", "year": "2016-05", "authors": "Andrea L Hartzler; Megan N Taylor; Albert Park; Troy Griffiths; Uba Backonja; David W Mcdonald; Sam Wahbeh; Cory Brown; Wanda Pratt"}, {"ref_id": "b41", "title": "Design and Usability of Interactive User Profiles for Online Health Communities", "journal": "ACM Trans. Comput.-Hum. Interact", "year": "2016-06", "authors": "Andrea L Hartzler; Bridget Weis; Carly Cahill; Wanda Pratt; Albert Park; Uba Backonja; David W Mcdonald"}, {"ref_id": "b42", "title": "Causal Inference: What If", "journal": "Chapman & Hall/CRC", "year": "2020", "authors": "Miguel A Hern\u00e1n; J M Robins"}, {"ref_id": "b43", "title": "Opportunities for Enhancing Access and Efficacy of Peer Sponsorship in Substance Use Disorder Recovery", "journal": "ACM", "year": "2020", "authors": "Jeremy Heyer; Zachary Schmitt; Lynn Dombrowski; Svetlana Yarosh"}, {"ref_id": "b44", "title": "A qualitative investigation of the impact of peer to peer online support for women living with Polycystic Ovary Syndrome", "journal": "BMC Women's Health", "year": "2013-12", "authors": "Sarah Holbrey; Neil S Coulson"}, {"ref_id": "b45", "title": "Social Relationships and Mortality Risk: A Meta-analytic Review", "journal": "PLOS Medicine", "year": "2010-07", "authors": "Julianne Holt-Lunstad; Timothy B Smith; J Bradley Layton"}, {"ref_id": "b46", "title": "Exploring kidney patients' experiences of receiving individual peer support", "journal": "Health Expectations", "year": "2009-12", "authors": "Jane Hughes; Eleri Wood; Gaynor Smith"}, {"ref_id": "b47", "title": "Matplotlib: A 2D graphics environment", "journal": "Computing in Science & Engineering", "year": "2007", "authors": "J D Hunter"}, {"ref_id": "b48", "title": "Expert Finding Systems: A Systematic Review", "journal": "Applied Sciences", "year": "2019-01", "authors": "Omayma Husain; Naomie Salim; Rose Alinda Alias; Samah Abdelsalam; Alzubair Hassan"}, {"ref_id": "b49", "title": "What models of peer support do people with colorectal cancer prefer?", "journal": "European Journal of Cancer Care", "year": "2011", "authors": "S C Ieropoli; V M White; M Jefford; D Akkerman"}, {"ref_id": "b50", "title": "The Narrative Tapestry Design Process: Weaving Online Social Support from Stories of Stigma", "journal": "Proc. ACM Hum.-Comput. Interact", "year": "2021-10", "authors": "Joshua Introne; Isabel Munoz; Bryan Semaan"}, {"ref_id": "b51", "title": "A Cancer Journey Framework: Guiding the Design of Holistic Health Technology", "journal": "ICST", "year": "2016", "authors": "Maia Jacobs; James Clawson; Elizabeth D Mynatt"}, {"ref_id": "b52", "title": "A Re-visit of the Popularity Baseline in Recommender Systems", "journal": "", "year": "2020-07", "authors": "Yitong Ji; Aixin Sun; Jie Zhang; Chenliang Li"}, {"ref_id": "b53", "title": "Optimizing search engines using clickthrough data", "journal": "Association for Computing Machinery", "year": "2002", "authors": "Thorsten Joachims"}, {"ref_id": "b54", "title": "Recommendations as Treatments", "journal": "AI Magazine", "year": "2021-11", "authors": "Thorsten Joachims; Ben London; Yi Su; Adith Swaminathan; Lequn Wang"}, {"ref_id": "b55", "title": "Participation and Aspirations Pre-print date: September 2022. in Chat-based Peer Support for Youth Living with HIV", "journal": "Association for Computing Machinery", "year": "2021", "authors": "Naveena Karusala; David Odhiambo Seeh; Cyrus Mugo; Brandon Guthrie; Megan A Moreno; Grace John-Stewart; Irene Inwani; Richard Anderson; Keshet Ronen"}, {"ref_id": "b56", "title": "Peer support opportunities across the cancer care continuum: a systematic scoping review of recent peer-reviewed literature", "journal": "Support Care Cancer", "year": "2019-01", "authors": "Sarah D Kowitt; Katrina R Ellis; Veronica Carlisle; Nivedita L Bhushan; Kristin Z Black; Kaitlyn Brodar; Nicole M Cranley; Kia L Davis; Eugenia Eng; Michelle Y Martin; Jared Mcguirt; L Rebeccah; Patrick Y Sokol; Anissa I Tang; Jennifer S Vines; Edwin B Walker;  Fisher"}, {"ref_id": "b57", "title": "Social Support and Preventive and Therapeutic Interventions", "journal": "Springer US", "year": "1996", "authors": "Brian Lakey; Catherine J Lutz"}, {"ref_id": "b58", "title": "Exploring the feasibility, acceptability and value of volunteer peer mentors in supporting self-management of osteoarthritis: a qualitative evaluation", "journal": "Disability and Rehabilitation", "year": "2021-09", "authors": "Elizabeth C Lavender; Esther Dusabe-Richards; Anna M Anderson; Deborah Antcliff; Linda Mcgowan; Philip G Conaghan; Sarah R Kingsbury; Gretl A Mchugh"}, {"ref_id": "b59", "title": "Review of the Psychometric Evidence of the Perceived Stress Scale", "journal": "Asian Nursing Research", "year": "2012-12", "authors": "Eun-Hyun Lee"}, {"ref_id": "b60", "title": "Measuring belongingness: The Social Connectedness and the Social Assurance scales", "journal": "Journal of Counseling Psychology", "year": "1995", "authors": "Richard M Lee; Steven B Robbins"}, {"ref_id": "b61", "title": "Comparing Standard Versus Prosocial Internet Support Groups for Patients With Breast Cancer: A Randomized Controlled Trial of the Helper Therapy Principle", "journal": "J Clin Oncol", "year": "2014-12", "authors": "Stephen J Lepore; Joanne S Buzaglo; Morton A Lieberman; Mitch Golant; Judith R Greener; Adam Davey"}, {"ref_id": "b62", "title": "Patterns of Patient and Caregiver Mutual Support Connections in an Online Health Community", "journal": "Proc. ACM Hum.-Comput. Interact", "year": "2021-01", "authors": "Zachary Levonian; Marco Dow; Drew Erikson; Sourojit Ghosh; Hannah Miller Hillberg; Saumik Narayanan; Loren Terveen; Svetlana Yarosh"}, {"ref_id": "b63", "title": "Bridging Qualitative and Quantitative Methods for User Modeling: Tracing Cancer Patient Behavior in an Online Health Community", "journal": "AAAI", "year": "2020", "authors": "Zachary Levonian; Drew Richard Erikson; Wenqi Luo; Saumik Narayanan; Sabirat Rubya; Prateek Vachher; Loren Terveen; Svetlana Yarosh"}, {"ref_id": "b64", "title": "Trade-offs in Sampling and Search for Early-stage Interactive Text Classification", "journal": "Association for Computing Machinery", "year": "2022", "authors": "Zachary Levonian; Chia-Jung Lee; Vanessa Murdock; F Maxwell Harper"}, {"ref_id": "b65", "title": "Condition Unknown: Predicting Patients' Health Conditions in an Online Health Community", "journal": "Association for Computing Machinery", "year": "2018", "authors": "Changye Li; Zachary Levonian; Haiwei Ma; Svetlana Yarosh"}, {"ref_id": "b66", "title": "Pretrained Transformers for Text Ranking: BERT and Beyond", "journal": "", "year": "2020-10", "authors": "Jimmy Lin; Rodrigo Nogueira; Andrew Yates"}, {"ref_id": "b67", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "journal": "", "year": "2019-07", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b68", "title": "Between Clicks and Satisfaction: Study on Multi-Phase User Preferences and Satisfaction for Online News Reading", "journal": "ACM", "year": "2018", "authors": "Hongyu Lu; Min Zhang; Shaoping Ma"}, {"ref_id": "b69", "title": "Write for Life: Persisting in Online Health Communities Through Expressive Writing and Social Support", "journal": "Proc. ACM Hum.-Comput. Interact", "year": "2017-12", "authors": "C Estelle Haiwei Ma; Lu Smith; Saumik He; Robert A Narayanan; Roni Giaquinto; Linda Evans; Svetlana Hanson;  Yarosh"}, {"ref_id": "b70", "title": "Be Grateful You Don't Have a Real Disease\": Understanding Rare Disease Relationships", "journal": "Association for Computing Machinery", "year": "2017", "authors": "Haley Macleod; Grace Bastin; Leslie S Liu; Katie Siek; Kay Connelly"}, {"ref_id": "b71", "title": "Computer-mediated infertility support groups: An exploratory study of online experiences", "journal": "Patient Education and Counseling", "year": "2008-10", "authors": "H Sumaira; Neil S Malik;  Coulson"}, {"ref_id": "b72", "title": "Data Structures for Statistical Computing in Python", "journal": "", "year": "2010", "authors": "Wes Mckinney"}, {"ref_id": "b73", "title": "There's somebody like me\": perspectives of a peer-to-peer gynecologic cancer mentorship program", "journal": "Support Care Cancer", "year": "2021-12", "authors": "Hannah Kang Moran; Joanna Veazey Brooks; Lori Spoozak"}, {"ref_id": "b74", "title": "Woman to Woman: a hospital-based support program for women with gynecologic cancer and their families", "journal": "Routledge", "year": "2016", "authors": "Arden Moulton"}, {"ref_id": "b75", "title": "Woman to Woman: A Peer to Peer Support Program for Women With Gynecologic Cancer", "journal": "Social Work in Health Care", "year": "2013-11", "authors": "Arden Moulton; Amy Balbierz; Stephanie Eisenman; Elizabeth Neustein; Virginia Walther; Irwin Epstein"}, {"ref_id": "b76", "title": "A Park or A Highway: Overcoming Tensions in Designing for Socioemotional and Informational Needs in Online Health Communities", "journal": "ACM", "year": "2017", "authors": "Drashko Nakikj; Lena Mamykina"}, {"ref_id": "b77", "title": "Lost in Migration: Information Management and Community Building in an Online Health Community", "journal": "Association for Computing Machinery", "year": "2018", "authors": "Drashko Nakikj; Lena Mamykina"}, {"ref_id": "b78", "title": "Deep Learning Recommendation Model for Personalization and Recommendation Systems", "journal": "", "year": "2019-05", "authors": "Maxim Naumov; Dheevatsa Mudigere; Michael Hao-Jun; Jianyu Shi; Narayanan Huang; Jongsoo Sundaraman; Xiaodong Park; Udit Wang; Carole-Jean Gupta; Alisson G Wu; Dmytro Azzolini; Andrey Dzhulgakov; Ilia Mallevich; Yinghai Cherniavskii; Raghuraman Lu; Ansha Krishnamoorthi; Volodymyr Yu; Stephanie Kondratenko; Xianjie Pereira; Wenlin Chen; Vijay Chen; Bill Rao; Liang Jia; Misha Xiong;  Smelyanskiy"}, {"ref_id": "b79", "title": "Choice of Implicit Signal Matters: Accounting for User Aspirations in Podcast Recommendations", "journal": "Association for Computing Machinery", "year": "2022", "authors": "Zahra Nazari; Praveen Chandar; Ghazal Fazelnia; Catherine M Edwards; Benjamin Carterette; Mounia Lalmas"}, {"ref_id": "b80", "title": "It's Not That I Don'T Have Problems, I'M Just Not Putting Them on Facebook: Challenges and Opportunities in Using Online Social Networks for Health", "journal": "ACM", "year": "2011", "authors": "Mark W Newman; Debra Lauterbach; Sean A Munson; Paul Resnick; Margaret E Morris"}, {"ref_id": "b81", "title": "Design Opportunities for Mental Health Peer Support Technologies", "journal": "ACM", "year": "2017", "authors": "O' Kathleen; Arpita Leary; Sean A Bhattacharya; Jacob O Munson; Wanda Wobbrock;  Pratt"}, {"ref_id": "b82", "title": "Reciprocal Recommender Systems: Analysis of state-of-art literature, challenges and opportunities towards social recommendation", "journal": "Information Fusion", "year": "2021-05", "authors": "Iv\u00e1n Palomares; James Neve; Carlos Porcel; Luiz Pizzato; Ido Guy; Enrique Herrera-Viedma"}, {"ref_id": "b83", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library", "journal": "Curran Associates, Inc", "year": "2019", "authors": "Adam Paszke; Sam Gross; Francisco Massa; Adam Lerer; James Bradbury; Gregory Chanan; Trevor Killeen; Zeming Lin; Natalia Gimelshein; Luca Antiga; Alban Desmaison; Andreas Kopf; Edward Yang; Zachary Devito; Martin Raison; Alykhan Tejani; Sasank Chilamkurthy; Benoit Steiner; Lu Fang; Junjie Bai; Soumith Chintala"}, {"ref_id": "b84", "title": "Scikit-learn: Machine Learning in Python", "journal": "Journal of Machine Learning Research", "year": "2011", "authors": "F Pedregosa; G Varoquaux; A Gramfort; V Michel; B Thirion; O Grisel; M Blondel; P Prettenhofer; R Weiss; V Dubourg; J Vanderplas; A Passos; D Cournapeau; M Brucher; M Perrot; E Duchesnay"}, {"ref_id": "b85", "title": "Exploring the Effects of Technological Writing Assistance for Support Providers in Online Mental Health Community", "journal": "Association for Computing Machinery", "year": "2020", "authors": "Zhenhui Peng; Qingyu Guo; Ka Wing Tsang; Xiaojuan Ma"}, {"ref_id": "b86", "title": "RECON: a reciprocal recommender for online dating", "journal": "Association for Computing Machinery", "year": "2010", "authors": "Luiz Pizzato; Tomek Rej; Thomas Chung; Irena Koprinska; Judy Kay"}, {"ref_id": "b87", "title": "Searching for Mental Health: A Mixed-Methods Study of Young People's Online Help-seeking", "journal": "Association for Computing Machinery", "year": "2020-09", "authors": "Claudette Pretorius; Darragh Mccashin; Naoise Kavanagh; David Coyle"}, {"ref_id": "b88", "title": "Coping with Illness Digitally", "journal": "EbPkuQEACAAJ", "year": "2018", "authors": "Stephen A Rains"}, {"ref_id": "b89", "title": "The Social Dimension of Blogging about Health: Health Blogging, Social Support, and Well-being", "journal": "Communication Monographs", "year": "2011-12", "authors": "Stephen A Rains; David M Keating"}, {"ref_id": "b90", "title": "Facilitators of peer coaching/support engagement and dissemination among women at risk for and surviving with breast cancer", "journal": "Translational Behavioral Medicine", "year": "2021-01", "authors": "Kathryn Rehberg; Adina Fleischmann; Elana Silber; C Suzanne; Frances Marcus O'neill; Kenneth P Lewis;  Tercyak"}, {"ref_id": "b91", "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks", "journal": "", "year": "2019", "authors": "Nils Reimers; Iryna Gurevych"}, {"ref_id": "b92", "title": "BPR: Bayesian personalized ranking from implicit feedback", "journal": "AUAI Press", "year": "2009", "authors": "Steffen Rendle; Christoph Freudenthaler; Zeno Gantner; Lars Schmidt-Thieme"}, {"ref_id": "b93", "title": "Neural Collaborative Filtering vs. Matrix Factorization Revisited", "journal": "Association for Computing Machinery", "year": "2020", "authors": "Steffen Rendle; Walid Krichene; Li Zhang; John Anderson"}, {"ref_id": "b94", "title": "The \"Helper\" Therapy Principle", "journal": "Soc Work", "year": "1965-04", "authors": "Frank Riessman"}, {"ref_id": "b95", "title": "Algorithmic Patient Matching in Peer Support Systems for Hospital Inpatients", "journal": "", "year": "2021", "authors": "Herman Saksono"}, {"ref_id": "b96", "title": "Peer-to-peer mentoring for individuals with early inflammatory arthritis: feasibility pilot", "journal": "BMJ Open", "year": "2013-01", "authors": "Sharron Sandhu; Paula Veinot; Gayathri Embuldeniya; Sydney Brooks; Joanna Sale; Sicong Huang; Alex Zhao; Dawn Richards; Mary J Bell"}, {"ref_id": "b97", "title": "statsmodels: Econometric and statistical modeling with python", "journal": "", "year": "2010", "authors": "Skipper Seabold; Josef Perktold"}, {"ref_id": "b98", "title": "Mental Health Support and Its Relationship to Linguistic Accommodation in Online Communities", "journal": "ACM", "year": "2018", "authors": "Eva Sharma; Munmun De Choudhury"}, {"ref_id": "b99", "title": "Peer interventions to promote health: Conceptual considerations", "journal": "American Journal of Orthopsychiatry", "year": "2011", "authors": "Jane M Simoni; Julie C Franks; Keren Lehavot; Samantha S Yard"}, {"ref_id": "b100", "title": "Optimizer Benchmarking Needs to Account for Hyperparameter Tuning", "journal": "", "year": "2020", "authors": "Teja Prabhu; Florian Sivaprasad; Thijs Mai; Martin Vogels; Fran\u00e7ois Jaggi;  Fleuret"}, {"ref_id": "b101", "title": "Enabling mutual helping? Examining variable needs for facilitated peer support", "journal": "Patient Education and Counseling", "year": "2011-11", "authors": "C Zo\u00eb; Sara J Skea; Vikki A Maclennan; James N' Entwistle;  Dow"}, {"ref_id": "b102", "title": "What is Spiritual Support and How Might It Impact the Design of Online Communities?", "journal": "Proc. ACM Hum.-Comput. Interact", "year": "2021-04", "authors": "C Estelle Smith; Avleen Kaur; Katie Z Gach; Loren G Terveen; Mary Jo Kreitzer; Susan O' Conner-Von"}, {"ref_id": "b103", "title": "I Cannot Do All of This Alone\": Exploring Instrumental and Prayer Support in Online Health Communities", "journal": "ACM Trans. Comput.-Hum. Interact", "year": "2020-08", "authors": "C Estelle Smith; Zachary Levonian; Haiwei Ma; Robert Giaquinto; Gemma Lein-Mcdonough; Zixuan Li; O Susan; Svetlana 'conner-Von;  Yarosh"}, {"ref_id": "b104", "title": "Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates", "journal": "", "year": "2018", "authors": "Leslie N Smith; Nicholay Topin"}, {"ref_id": "b105", "title": "Deep Learning for Recommender Systems: A Netflix Case Study", "journal": "AI Magazine", "year": "2021-11", "authors": "Harald Steck; Linas Baltrunas; Ehtsham Elahi; Dawen Liang; Yves Raimond; Justin Basilico"}, {"ref_id": "b106", "title": "Strategies for Fostering a Genuine Feeling of Connection in Technologically Mediated Systems", "journal": "Association for Computing Machinery", "year": "2022", "authors": "Ekaterina R Stepanova; John Desnoyers-Stewart; Kristina H\u00f6\u00f6k; Bernhard E Riecke"}, {"ref_id": "b107", "title": "An Experimental Study of Structural Diversity in Social Networks", "journal": "", "year": "2020-05", "authors": "Jessica Su; Krishna Kamath; Aneesh Sharma; Johan Ugander; Sharad Goel"}, {"ref_id": "b108", "title": "The Effect of Recommendations on Network Structure", "journal": "", "year": "2016", "authors": "Jessica Su; Aneesh Sharma; Sharad Goel"}, {"ref_id": "b109", "title": "CoupleNet: Paying Attention to Couples with Coupled Attention for Relationship Recommendation", "journal": "ICWSM", "year": "2018-06", "authors": "Yi Tay; Luu Tuan; Siu Hui"}, {"ref_id": "b110", "title": "Peer support for CKD patients and carers: overcoming barriers and facilitating access", "journal": "Health Expectations", "year": "2016", "authors": "Francesca Taylor; Robin Gutteridge; Carol Willis"}, {"ref_id": "b111", "title": "Social matching: A framework and research agenda", "journal": "ACM Trans. Comput.-Hum. Interact", "year": "2005-09", "authors": "Loren Terveen; David W Mcdonald"}, {"ref_id": "b112", "title": "Mechanisms Linking Social Ties and Support to Physical and Mental Health", "journal": "J Health Soc Behav", "year": "2011-06", "authors": "Peggy A Thoits"}, {"ref_id": "b113", "title": "Counting on the Group\": Reconciling Online and Offline Social Support among Older Informal Caregivers", "journal": "Association for Computing Machinery", "year": "2016", "authors": "Matthieu Tixier; Myriam Lewkowicz"}, {"ref_id": "b114", "title": "Developing an optimal match within online communities: an exploration of CMC support communities and traditional support", "journal": "Journal of Communication", "year": "2001", "authors": "J W Turner; J A Grube; J Meyers"}, {"ref_id": "b115", "title": "Social Support and Health: A Review of Physiological Processes Potentially Underlying Links to Disease Outcomes", "journal": "J Behav Med", "year": "2006-08", "authors": "Bert N Uchino"}, {"ref_id": "b116", "title": "Social Support and Physical Health: Models, Mechanisms, and Opportunities", "journal": "Springer", "year": "2018", "authors": "Bert N Uchino; Kimberly Bowen; Robert Kent De Grey; Jude Mikel; Edwin B Fisher"}, {"ref_id": "b117", "title": "Social Relationships and Health: A Flashpoint for Health Policy", "journal": "J Health Soc Behav", "year": "2010-03", "authors": "Debra Umberson; Jennifer Karas Montez"}, {"ref_id": "b118", "title": "Good intentions are not enough: how informatics interventions can worsen inequality", "journal": "Journal of the American Medical Informatics Association", "year": "2018-08", "authors": "Tiffany C Veinot; Hannah Mitchell; Jessica S Ancker"}, {"ref_id": "b119", "title": "What is Your Current Mindset?", "journal": "ACM", "year": "2022", "authors": "Sruthi Viswanathan; Behrooz Omidvar-Tehrani; Jean-Michel Renders"}, {"ref_id": "b120", "title": "Interactive query formulation", "journal": "Ann. Rev. Info. Sci. Tech", "year": "2011", "authors": "Nina Wacholder"}, {"ref_id": "b121", "title": "Peer support for people with advanced cancer: a systematically constructed scoping review of quantitative and qualitative evidence", "journal": "Current Opinion in Supportive and Palliative Care", "year": "2018-09", "authors": "Catherine Walshe; Diane Roberts"}, {"ref_id": "b122", "title": "Peer support to maintain psychological wellbeing in people with advanced cancer: findings from a feasibility study for a randomised controlled trial", "journal": "BMC Palliative Care", "year": "2020-08", "authors": "Catherine Walshe; Diane Roberts; Lynn Calman; Lynda Appleton; Robert Croft; Suzanne Skevington; Mari Lloyd-Williams; Gunn Grande; Guillermo Perez; Algorta "}, {"ref_id": "b123", "title": "How much is a \"like\" worth? Engagement and Retention in an Online Health Community", "journal": "Association for Computing Machinery", "year": "2020", "authors": "Ruyuan Wan; Zachary Levonian; Svetlana Yarosh"}, {"ref_id": "b124", "title": "Applied Linear Regression", "journal": "Wiley", "year": "2013", "authors": "Sanford Weisberg"}, {"ref_id": "b125", "title": "Leveraging post-click feedback for content recommendations", "journal": "Association for Computing Machinery", "year": "2019-09", "authors": "Hongyi Wen; Longqi Yang; Deborah Estrin"}, {"ref_id": "b126", "title": "Transformers: State-of-the-Art Natural Language Processing", "journal": "", "year": "2020", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; R\u00e9mi Louf; Morgan Funtowicz; Joe Davison; Sam Shleifer; Clara Patrick Von Platen; Yacine Ma; Julien Jernite; Canwen Plu; Teven Xu; Sylvain Le Scao; Mariama Gugger; Quentin Drame; Alexander M Lhoest;  Rush"}, {"ref_id": "b127", "title": "Deep Learning for Matching in Search and Recommendation", "journal": "INR", "year": "2020-07", "authors": "Jun Xu; Xiangnan He; Hang Li"}, {"ref_id": "b128", "title": "Computational Social Roles", "journal": "", "year": "2019", "authors": "Diyi Yang"}, {"ref_id": "b129", "title": "Seekers, Providers, Welcomers, and Storytellers: Modeling Social Roles in Online Health Communities", "journal": "", "year": "2019", "authors": "Diyi Yang; Robert Kraut; Tenbroeck Smith; Elijah Mayfield; Dan Jurafsky"}, {"ref_id": "b130", "title": "Mixed Negative Sampling for Learning Two-tower Neural Networks in Recommendations", "journal": "ACM", "year": "2020", "authors": "Ji Yang; Xinyang Yi; Derek Zhiyuan Cheng; Lichan Hong; Yang Li; Simon Xiaoming Wang; Taibai Xu; Ed H Chi"}, {"ref_id": "b131", "title": "Online Social Support Received by Patients With Cancer", "journal": "CIN: Computers, Informatics, Nursing", "year": "2014-03", "authors": "Tiina Yli-Uotila; Anja Rantanen; Tarja Suominen"}, {"ref_id": "b132", "title": "This Girl is on Fire\": Sensemaking in an Online Health Community for Vulvodynia", "journal": "ACM", "year": "2019", "authors": "Alyson L Young; Andrew D Miller"}, {"ref_id": "b133", "title": "Explicit or implicit feedback? engagement or satisfaction?: a field experiment on machine-learning-based recommender systems", "journal": "ACM", "year": "2018-09", "authors": "Qian Zhao; F Maxwell Harper; Gediminas Adomavicius; Joseph A Konstan"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "(a) Site home page, as viewed by a logged-in author of that site. The study recruitment banner is visible at the top of the page. (b) Journal page and entry, as viewed by a logged-out visitor. The six logged-in user actions, above a guestbook on the Well Wishes page.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Fig. 1 .1Fig.1. The CaringBridge interface. We record six logged-in user actions: visits (to any of these site pages), follows (clicks on \"Follow Site\" and others, see sec. 3.1), Journal updates, reactions (on Journal updates, comments, or guestbooks), comments (on Journal updates or guestbooks), and guestbooks.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Fig. 2 .2Fig. 2. Existing design of email notifications on CaringBridge and the Site Suggestion email interface designed for this study. Data shown is a representative fabrication.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Fig. 3 .3Fig. 3. CaringBridge peer recommendation system overview", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Fig. 5 .5Fig. 5. Dataset splits and associated author initiation totals. All reported metrics are from the test period.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Fig. 6 .6Fig.6. Site Suggestion email generation during the field study. Recommendations were typically sent around noon, 36 hours after the database snapshot was taken.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Fig. 7 .7Fig. 7. Study timeline", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Fig. 9 .9Fig. 9. Recommendation click counts and percentages by email batch and participant. Participants are shown grouped by decile; 49 participants never clicked a recommendation, while 2 clicked more than half.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Fig.Fig. 11. Visit effect: Estimated impacts of receiving a stranger visit on recommended site behavior in the 90 days after the visit when compared to non-clicked recommendations (square, dark-gray) and pseudo-control recommendations (circle, light-gray). Peer visits and interactions count non-participant author behavior directed at the site. Recommended site author interactions count behavior of authors of the site. The OLS estimate adjusts for pre-study activity on CaringBridge while the doubly robust (DR) estimate adds additional causal identification assumptions. 95% confidence intervals are computed via bootstrapping (1000 iterations).", "figure_data": ""}, {"figure_label": "12", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Fig. 12 .12Fig. 12. Sample sizes needed to detect effects of the magnitude we observed during the feasibility study. Field study effect sizes are shown parenthetically as Cohen's \ud835\udc51.", "figure_data": ""}, {"figure_label": "13", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "Fig. 13 .13Fig. 13. MLP Study offline performance throughout the validation and test period. No evidence of a non-zero relationship between time since model training and offline performance metrics. (All significance tests have \ud835\udc5d>0.05.) This flat relationship suggests that (a) the offline analysis is minimally impacted by training set leakage and (b) frequent model retrains may not be necessary in practice, although we retrained the model weekly during the field study.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Peer characteristics identified in prior work as important for effective peer matching. We included the starred characteristics in our preference survey.", "figure_data": "Peer CharacteristicsProposedUsedExpressed PreferenceHealthDiagnosis*[2, 4, 13, 25, 63, 71, 114] [29, 74-76, 123][19, 24, 41, 111]Treatment*[13, 25][29, 50][19, 41, 111]Symptoms*[82][19]Severity[25, 114][97]Timeline[64][19, 41]Health role[63][41]Relevant knowledge[71][47][41, 44]DemographicsAge[4, 13, 25, 114][2, 41, 47, 50, 74-76] [19, 41]Gender[3, 4, 13, 25][41, 47, 50, 59, 123][41, 97]Ethnicity[22, 47][14]Sexual orientation[97]Nationality[41]LifeGeography/location*[4, 63, 114][22, 29, 59, 123][41]Cultural values/background* [13, 91][75, 76][14, 19, 24]Employment[47][19]Religion[76]Politics[97]Socio-economic status[111]Education level[19]Social role*[100, 114, 130][75][41]Marital status[25][47, 50][41]Has children?[50, 74, 76][41]Language[75]OtherCommunication style*[81][41][111]Lifestyle[59][19, 24]Interests[2, 29][19]Personality[59][44, 111]Commitment to supp-ort & recovery[82][24, 44]create blogs called sites, on which they can publish blog posts called Journal updates. An authoris a user who has published at least one Journal update. Much usage of CaringBridge is basedaround notification emails (an example is shown in Figure"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Associational differences between sites that receive interactions (ints) from at least one peer author and sites that only receive interactions from visitors. Site tenure is the number of months between the first and last Journal update on a site. # updates is the number of Journal updates published more than 30 days after the first update. The common language effect size (CLES) is reported where appropriate. All comparisons are significant at the 99.5% significance level.", "figure_data": "1+ peer int within 30 days Non-peer ints only DifferenceCLESSite Count92,352 sites100,424 sites-8,072 sites-Site Tenure (Median)3.9 months0.8 months+3.2 months 62.4%# updates (M; SD)20.5 (47.6)12.4 (35.7)+8.1 updates 62.1%# updates (Median)8 updates2 updates+6 updates-% sites with 2+ updates 78.1%59.5%+18.7pp-Subscriber name"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Offline performance of an MLP model trained with combinations of the activity (A), network (N) and text (T) feature sets. The model using all feature sets (A+N+T) is MLP Tuned from Table4.", "figure_data": "Feature sets MRR HR@1 HR@5 |R| % UniqueSite Age|S R |/|R| |S U |/|U |A+N0.204 16.12% 23.62% 59812.0% 0.9 weeks 12.0% / 24.4% = 0.49A+N+T0.173 13.37% 20.27% 66513.3% 6.2 weeks 10.8% / 24.6% = 0.44N+T0.144 11.75% 16.58% 80316.1% 5.4 weeks 12.5% / 24.6% = 0.51N (Network) 0.136 11.70% 15.42% 84216.8% 56.4 weeks 11.6% / 24.7% = 0.47A (Activity) 0.058 3.23% 6.99% 110.2% 0.8 weeks 9.1% / 23.9% = 0.38A+T0.043 1.73% 5.25% 1813.6% 0.8 weeks 16.0% / 24.0% = 0.67T (Text)0.017 0.58% 1.85% 982.0% 0.6 weeks 13.3% / 23.9% = 0.55"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "To generate a batch of Site Suggestion emails:", "figure_data": "Create database snapshotIdentify new initiationsExtract features & retrain modelIdentify & score candidatesDraft five sites per participantReview for harmful recsGenerate & send emailsMonitor for feedback+ 8 hrs transfer \u2248 2 hrs snapshot\u2248 1 hr\u2248 2 hrsper participant \u2248 1.5 secs\u2248 0 ms\u2248 1 hrweekly Repeat"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Study phases and corresponding outputs. Each output provides evidence toward one or more aspects of feasibility.", "figure_data": "Phase Component Analysis outputFeasibility AspectSectionSystemDesignInterface ModelObserved prior use CB-adapted rec intervention Implementation Demand Offline evaluation Practicality + Implementation 3.3.4 3.1 3.2 Feature ablations Practicality 3.3.5Self-reported prior useDemand5.1.1SurveyInterest & motivationsDemand5.1.2Peer characteristicsAcceptability 12 weeks of site suggestion 5.1.3FieldStudyRec EmailRec characteristics Click rate Explicit feedbackPracticality Demand Acceptability emails to 90 participants 5.2.3 5.2.1 5.2.2Reading behaviorEfficacy + Acceptability5.3.1WebsiteInteraction behavior Second-order effect estimates Efficacy Efficacy + Acceptability5.3.2 5.3.3Effect size estimatesEfficacy5.3.4SurveySurveySite Suggestion\"Thank you\"End of dataOpenedClosedEmailsEmailcollectionProfileAug 2RecruitmentAug 23MatchingSep 211 weekly Site Suggestion EmailsDec 3MonitoringApr 120213 weeks20212 weeks202112 weeks20215 months2022"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Pre-study CaringBridge usage by participants enrolled in the field study and by a pseudo-control group of eligible non-enrolled authors. Author tenure is the number of days between a user's first published Journal update and Sept. 1, 2021. Density histograms indicate distribution shape for the participants (black) and the pseudo-control group (gray). *Indicates a significant difference at the 99.5% threshold, for a Welch's \ud835\udc61-test on the mean difference and for a Mann-Whitney \ud835\udc48 test (reported as the common language effect size aka ROC AUC). \ud835\udc48 P /(\ud835\udc5b P \ud835\udc5b C ) Journal updates by September 1, 2021 (see sec. 3.3 for discussion of the minimum update requirement). Ultimately, 79 participants were sent Site Suggestion emails. 4.1.2 Observed prior use. The median participant had been writing Journal updates on CaringBridge for fewer than 6 months at the time of enrollment. But, consistent with observations by Levonian et al.", "figure_data": "123100969079banner clicksearly survey exitsurvey completionsineligible or opted out or invalid emailopt-ins to studyno matching profile and no response to follow-up emailprofile matches< 3 published Journal updates by Sept. 1, 2021eligible authorsFig. 8. Recruitment pipeline.Participants (\ud835\udc5b P =79) Pseudo-Control (\ud835\udc5b C =1759)Med. M (SD) M P -M C Author tenure (days) Med. M (SD) 179 709.4 (1146.6) 3894 4078.5 (983.1) -3369.1* 3.2%*Journal updates28 98.6 (262.4)77 167.1 (307.7)-68.531.1%*Peer site visits3 6.5 (12.3)10 32.0 (84.0)-25.5*23.9%*Peer site initiations1 2.5 (4.4)5 11.5 (35.2)-9.0*23.8%*Peer site interactions2 39.7 (94.4)30 201.2 (1384.5)-161.5*29.0%*for profile information, matching 2 additional profiles. Finally, we excluded 11 participants whohad published less than 3"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Recruitment survey responses.", "figure_data": "# checked%"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Explicit quantitative feedback to 11 Site Suggestion emails from 8 total participants. We would like to estimate click rate conditional on opening the email (i.e. the click-through rate) independently from the base click rate, but for logistical and ethical reasons we did not use email trackers in the email HTML so we assume every email was received and opened. 5.2.2 Explicit feedback. 8 participants provided explicit feedback on the recommendations in 13 responses. No participants provided rec feedback after batch 7, so responses reflect initial impressions. Table", "figure_data": "Recommendations interesting in general? Specific recommendations relevant?Very Relevant 20Yes 4Somewhat Relevant 6Unsure/Neutral 3Unsure/Neutral 8No 3Somewhat Irrelevant 17Very Irrelevant/Offensive 55.3."}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Prevalence of identified content categories in the Journal update previews. Only the presence of expressive writing was significantly associated with clicks (47.1% of first-batch recommendations with expressive writing were clicked vs 28.7% without, \ud835\udc5d = 0.017).", "figure_data": "Preview CategoryFirst batch prevalenceReporting health status85.2%Neutral disclosures31.5%Positive disclosures only25.8%Negative disclosures only22.2%Positive & negative disclosures5.8%Expressive Writing31.2%Managing Author/Audience Relationship17.5%Expressions of Appreciation5.8%kind of depressing. Since we're already going through cancer treatments, it's hard to look at what otherpeople are going through. \""}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "Observed participant behavior in response to Site Suggestion emails, from the start of the study (September 2021) to the end of data collection (April 2022).", "figure_data": "RecommendationsParticipantsRecced sitesBehavior\ud835\udc5b % (of 4190 total) \ud835\udc5b % (of 79 total)\ud835\udc5b % (of 526 total)First Visits/Clicks220 5.5%30 38.0%158 30.0%Second Visits86 2.1%17 21.5%76 14.4%Repeat Visits589 -17 21.5%76 14.4%Follows24 0.1%5 6.3%23 4.4%Initiations36 0.9%9 11.4%33 6.3%Interactions948 -9 11.4%33 6.3%Text Interactions 268 -4 5.1%20 3.8%Relationships1 0.0%1 1.3%1 0.2%"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "Fig.10. Participation effect: Estimated impacts of receiving Site Suggestion emails on author behavior in the 90 days after the study. Raw estimate is the mean difference between the participants and the pseudo-control group of unenrolled but eligible authors. The OLS estimate adjusts for pre-study activity on CaringBridge while the doubly robust (DR) estimate adds additional causal identification assumptions. 95% confidence intervals are computed via bootstrapping (1000 iterations).", "figure_data": "6Participation Effect2 0 2 44.30.9 0.70.4 0.0 0.1-0.7 -1.1 -1.0-0.41.9 1.64OLS Journal updates DRRaw OLS Peer site visits DRRaw OLS Peer site interactions DRRaw OLS # days visiting peers DRExcess weekly actions0.10 0.05 0.00 0.05 0.10Raw OLS Peer visits DR Raw OLS Recommended site author outward interactions DRRaw OLS Journal updates DR Raw OLS Peer interactions DR Raw OLS self interactions site author Recommended DR Non-Clicked Pseudo-Control"}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "Offline test performance for various baselines. This table shows additional baselines beyond those presented in Table4.", "figure_data": "NetworkMRR HR@1 HR@5 |R|%Unique MMST|S R |/|R| |S U |/|U |PeopleYouKnow 0.102 7.86% 13.07% 3972 79.44%29.2 weeks 14.9% / 28.1% = 0.53CosSim0.002 0.05% 0.25% 3403 68.1%27.1 weeks 25.2% / 23.3% = 1.08MF0.002 3.05% 15.23% 130.26%40.3 weeks 0.0% / 23.9% = 0.00RecentInits0.036 1.27% 4.81% 60.12%0.4 weeks0.0% / 23.9% = 0.00MostInits0.035 1.32% 4.90% 120.24%112.9 weeks 0.0% / 23.9% = 0.00NewestAuthor0.025 0.97% 3.19% 60.12%0.1 weeks83.3% / 23.8% = 3.50RecentJournals 0.016 0.19% 1.58% 60.12%3.1 weeks16.7% / 23.9% = 0.70MostInteractive 0.007 0.17% 0.68% 50.10%0.9 weeks0.0% / 23.9% = 0.00MostJournals0.004 0.13% 0.38% 50.10%0.4 weeks60.0% / 23.8% = 2.52Random0.001 <0.0% 0.04% 4159 83.18%17.6 weeks 23.7% / 23.9% = 0.99"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_17", "figure_caption": "C.1.10 Free Response -General. (Optional) Anything else you want to share with us about visiting the CaringBridge site of a fellow author? [Free Response] C.2 Feedback Survey Thank you for providing feedback. All questions on this page are optional: submit your feedback by clicking the right arrow at the very bottom of this survey form. Not sure why you received this email & survey? You agreed to receive site suggestion emails in a survey you took in August. See the FAQ [link]. If you don't want to receive these emails anymore, you can unsubscribe [link]. C.2.1 Overall Interest. Overall, did the suggested sites seem interesting to you? \u2022 Yes, the sites were generally interesting to me. \u2022 Unsure or neutral. \u2022 No, the sites were generally uninteresting to me. C.2.2 Free Response -Interesting. Briefly, what seemed interesting to you about the suggested sites? [Free Response] C.2.3 Free Response -Uninteresting. Briefly, what seemed uninteresting to you about the suggested sites? [Free Response] C.2.4 Specific Recommendation Relevance. Specifically, how relevant did you find each of the suggested sites? [5-item response matrix with levels: Very Relevant; Somewhat Relevant; Unsure or Neutral; Somewhat Irrelevant; Very Irrelevant, Offensive, or Spam] Free Response -General. Any other thoughts you want to share about these site suggestions or your experience so far in this study? [Free Response] C.3 Unsubscribe Survey C.3.1 Page 1. To unsubscribe: just enter your email address on the line below and click the right arrow. Or, maybe you're looking for a feedback form [link] or the study FAQ [link] (including contact details for study coordinators) instead.", "figure_data": "\u2022 1st suggested site\u2022 2nd suggested site\u2022 3rd suggested site\u2022 4th suggested site\u2022 5th suggested siteC.2.5 C.3.2 Email.\u2022 Unsubscribe me from additional emails from cb-suggestions@umn.edu. My email address is:[Free response]-Page Break-C.3.3 Free Response -Unsubscribe."}, {"figure_label": "14", "figure_type": "table", "figure_id": "tab_18", "figure_caption": "Observed clicked and non-clicked recommended site behavior from the 35 day window before the point they were clicked (or could have been clicked). Site tenure is in days, Total # of authors is since the site's creation, # of authors and # days visiting peers is for the entire window period, while all other rows are the number of weekly actions.", "figure_data": "-Control(\ud835\udc5b P =4190)(\ud835\udc5b C =4190)Med. M (SD)Med. M (SD)M P -M C \ud835\udc48 P /(\ud835\udc5b P \ud835\udc5b C )Site tenure (days)113 227.4 (519.2)152 262.7 (425.2) -35.3*42.4%*Journal updates1 1.9 (2.0)1 1.6 (2.3)0.3*40.4%*# of authors1 1.3 (0.7)1 1.2 (0.5)0.2*44.9%*Total # of authors2 1.8 (1.0)2 1.7 (0.8)0.2*46.1%*Peer visits0 0.3 (1.2)0 0.1 (0.2)0.2*26.6%*Repeat user visits0 2.1 (6.3)0 0.9 (1.5)1.2*49.5%Peer initiations0 0.8 (1.7)0 0.4 (0.5)0.4*47.2%*Peer interactions0 3.6 (8.0)0 1.8 (3.9)1.8*47.3%*# days visiting peers7 11.6 (11.7)7 9.3 (9.0)2.3*47.4%*Site author interactions0 0.0 (0.2)0 0.0 (0.2)0.050.0%Site author initiations0 0.0 (0.0)0 0.0 (0.0)0.050.0%Site author self interactions6 15.1 (24.9)3 8.4 (15.4)6.8*40.8%*ClickedNon-Clicked(\ud835\udc5b P =220)(\ud835\udc5b C =3970)Med. M (SD) M P -M Site tenure (days) Med. M (SD) 107 267.4 (609.1) 113 225.2 (513.8) 42.249.5%Journal updates1 2.1 (2.2)1 1.9 (2.0)0.246.6%# of authors1 1.4 (0.7)1 1.3 (0.7)0.145.7%*Total # of authors2 1.9 (1.0)2 1.8 (1.0)0.147.4%Peer visits0 0.5 (1.7)0 0.3 (1.1)0.243.9%*Repeat user visits1 3.1 (8.1)0 2.0 (6.2)1.143.6%*Peer initiations0 1.1 (2.1)0 0.8 (1.7)0.345.2%Peer interactions0 4.6 (9.5)0 3.6 (7.9)1.045.7%# days visiting peers11 13.0 (11.7)7 11.5 (11.7)1.545.0%Site author interactions0 0.1 (0.4)0 0.0 (0.2)0.049.2%Site author initiations0 0.0 (0.1)0 0.0 (0.0)0.049.2%Site author self interactions5 14.0 (25.8)6 15.2 (24.9)-1.248.0%"}, {"figure_label": "15", "figure_type": "table", "figure_id": "tab_19", "figure_caption": "Observed clicked and pseudo-control site behavior from the 35 day window before the point they were clicked (or could have been clicked). Site tenure is in days, Total # of authors is since the site's creation, # of authors and # days visiting peers is for the entire window period, while all other rows are the number of weekly actions.", "figure_data": "ClickedPseudo-Control(\ud835\udc5b P =220)(\ud835\udc5b C =4190)Med. M (SD)Med. M (SD)M P -M C \ud835\udc48 P /(\ud835\udc5b P \ud835\udc5b C )Site tenure (days)107 267.4 (609.1)152 262.7 (425.2) 4.743.0%*Journal updates1 2.1 (2.2)1 1.6 (2.3)0.5*37.3%*# of authors1 1.4 (0.7)1 1.2 (0.5)0.2*40.7%*Total # of authors2 1.9 (1.0)2 1.7 (0.8)0.2*43.4%*Peer visits0 0.5 (1.7)0 0.1 (0.2)0.4*23.1%*Repeat user visits1 3.1 (8.1)0 0.9 (1.5)2.3*43.4%*Peer initiations0 1.1 (2.1)0 0.4 (0.5)0.6*47.2%Peer interactions0 4.6 (9.5)0 1.8 (3.9)2.8*47.5%# days visiting peers11 13.0 (11.7)7 9.3 (9.0)3.7*42.3%*Site author interactions0 0.0 (0.2)0 0.0 (0.1)0.049.8%Site author initiations0 0.0 (0.0)0 0.0 (0.0)0.049.8%Site author self interactions5 14.0 (25.8)3 8.4 (15.4)5.6*42.5%*"}, {"figure_label": "16", "figure_type": "table", "figure_id": "tab_20", "figure_caption": "Thematic analysis: summary of themes in recommended Journal update previews", "figure_data": "ThemeDescriptionExample(s)"}, {"figure_label": "17", "figure_type": "table", "figure_id": "tab_21", "figure_caption": "Inter-rater reliability as Cohen's \ud835\udf05 and percent agreement (%A) between the two annotators in the final two coding rounds. Post-discussion codebook updates resulted in greater agreement during round 3.", "figure_data": "Round 2 (\ud835\udc5b=143) Round 3 (\ud835\udc5b=376)Preview Category\ud835\udf05%A\ud835\udf05%AReporting Health0.56 83.9%0.62 87.8%Positive Disclosures0.72 87.4%0.78 89.4%Negative Disclosures0.61 88.1%92.8%Managing Audience Relationship 0.66 90.2%0.74 91.0%Expression of Appreciation0.78 96.5%0.90 98.1%Expressive Writing0.38 69.2%0.48 73.7%All0.33 37.8%0.45 48.9%"}, {"figure_label": "18", "figure_type": "table", "figure_id": "tab_22", "figure_caption": "Logistic regression models predicting recommendation clicks from preview content for the rec subsets defined in sec. 4.2. Preview contents are poor predictors of clicks, as evidenced by near-chance ROC AUC scores (estimated using leave-one-out cross-validation). Note: * p<0.05; * * p<0.01; * * * p<0.001", "figure_data": "Clicked Batches Only B1 Only Clickers OnlyIntercept-0.467-1.943  *  *  *-1.714  *  *  *(0.325)(0.488)(0.206)Rank within email0.096-0.0230.034(0.085)(0.115)(0.052)EOA-0.5810.156-0.546  *5.8% of B1 recs(0.396)(0.621)(0.277)Expressive Writing0.638  *0.762  *0.05031.2% of B1 recs(0.250)(0.353)(0.152)Managing Audience Relationship-0.2930.239-0.35817.5% of B1 recs(0.328)(0.409)(0.192)Reporting health status (categorical)Base level: None (14.8% of B1 recs)Neutral disclosure-0.171-0.235-0.01331.5% of B1 recs(0.338)(0.498)(0.211)Positive disclosure only-0.6150.125-0.07825.8% of B1 recs(0.340)(0.494)(0.205)Negative disclosure only-0.395-0.540-0.09122.2% of B1 recs(0.396)(0.546)(0.247)Pos & neg disclosures-1.070-0.541-0.7065.8% of B1 recs(0.551)(0.768)(0.368)Observations3103651,590Clicks12051220Log-Likelihood-199.69-142.94-632.92ROC AUC0.5540.4690.488"}, {"figure_label": "19", "figure_type": "table", "figure_id": "tab_23", "figure_caption": "Activity variables included in the participant author model and the visited site model. We removed some variables to avoid collinearity, but otherwise opted to incorporate as many potentially relevant behaviors as possible[125].", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "[\ud835\udc4c |\ud835\udc47 = 1, \ud835\udc34] -E[\ud835\udc4c |\ud835\udc47 = 0, \ud835\udc34].", "formula_coordinates": [21.0, 103.25, 195.2, 121.96, 9.36]}, {"formula_id": "formula_1", "formula_text": "B1 Sep02 B2 Sep17 B3 Sep24 B4 Oct01 B5 Oct08 B6 Oct20 B7 Oct27 B8 Nov02 B9 Nov10 B10 Nov17 B11Nov24", "formula_coordinates": [23.0, 93.67, 153.2, 260.34, 19.23]}], "doi": "10.1145/3449169"}
