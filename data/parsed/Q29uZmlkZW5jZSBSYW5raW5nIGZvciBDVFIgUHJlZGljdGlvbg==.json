{"Confidence Ranking for CTR Prediction": "Jian Zhu, Congcong Liu, Pei Wang, Xiwei Zhao, Zhangang Lin, Jingping Shao JD.com Beijing, China {zhujian146,liucongcong25,wangpei959,zhaoxiwei,linzhangang,shaojingping}@jd.com", "ABSTRACT": "Model evolution and constant availability of data are two common phenomenon in large-scale real-world machine learning application, e.g. ads and recommendation system. To adapt, real-world system typically retrain with all available data and online learn with recent available data to update the models periodically with the goal of better serving performance. However, if model and data evolution results in a vastly different training manner, it may induce negative impact on online A/B platform. In this paper, we propose a novel framework, named Confidence Ranking , which designs the optimization objective as a ranking function with two different models. Our confidence ranking loss allows direct optimization of the logits output for different convex surrogate function of metrics, e.g. AUC and Accuracy depending on the target task and dataset. Armed with our proposed methods, our experiments show that the confidence ranking loss can outperform all baselines on CTR prediction of public and industrial datasets. This framework has been deployed in the ad system of JD.com to serve the main traffic in the fine-rank stage. and constant model evolution, the periodic retraining methodology motivate fast adaption and better generalization for approximating the recent decision boundary. While retraining with new data and all data does make the online model more current and confident, the improvement only originates from the alleviation of train-test discrepancy or promotion of model capability. We investigate if modeling previous prediction(confidence) distribution can improve the performance even further. For short, as we train the model at a time \ud835\udc61 with data D \ud835\udc61 , we only know that the model will be optimized in this period while ignoring how the data are produced and influenced by the online deployed model. Our goal in this paper is to improve test-set performance of retrained model compared to the online deployed model for CTR prediction.", "CCS CONCEPTS": "\u00b7 Information systems \u2192 Retrieval models and ranking ; Online advertising ; Recommender systems .", "KEYWORDS": "Click-Through Rate Prediction; Loss function; Deep learning", "ACMReference Format:": "Jian Zhu, Congcong Liu, Pei Wang, Xiwei Zhao, Zhangang Lin, Jingping Shao . 2023. Confidence Ranking for CTR Prediction. In Proceedings of ACM Conference (Conference'17). ACM, New York, NY, USA, 5 pages. https: //doi.org/10.1145/nnnnnnn.nnnnnnn", "1 Introduction": "To alleviate the discrepancy between delayed training and test distribution, typically, the real-world ads and recommendation system widely operate machine learning pipeline as follows: (1) collect user-clicks periodical data (every \u0394 = 24 hours for daily data and \u0394 < 10 minutes for online data); (2) train the model on collected data then deploy the model for serving until next new retrained model is produced. Due to the non-stationary data distribution Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference'17, July 2017, Washington, DC, USA \u00a9 2023 Association for Computing Machinery. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn To improve model performance, a common solution knowledge distillation [6], which involves having a teacher model and mixing its predictions with original labels has proved to be a useful tool in deep learning. Recently, distillation-based works[7, 8, 12] have been widely used in recommendation system for training efficient and effective student models. Despite its success, this methodology requires the teacher model performs better than students while the divergence of model capacity is not large [2]. In this way, knowledge distillation may constrain its power when applied in CTR prediction pipeline because the capacity of retrained model maybe not weaker than the online model. Thus, an important question remains open regarding machine learning pipeline of real-world system: How can we train a model better than the online deployed model? In this paper, we provide an affirmative answer to this question. Our solution is to optimize a novel framework of loss function for machine learning application (MLA) instead of cross-entropy optimization. In particular, we choose to maximize the ranking score between base model and retrained model for CTR prediction. There are several benefits of maximizing the ranking score over minimizing the cross-entropy loss. First, maximizing the ranking score is naturally suitable for classification task in real-world CTR prediction where train-test distribution is nearly identically independent distributed. Directly maximizing the ranking score of different models can approximate improving the model performance relatively. Second, this framework is more suitable for handling various model complexity since maximizing ranking score aims to learn better decision boundary compared to baseline. The foremost challenge in this paper is to determine the surrogate loss for this setting. In our study, a naive approach of exploiting ranking-based pair-wise surrogate loss can be efficiently optimized for various metrics(e.g. AUC and Accuracy). Our contribution can be summarized as: (1) Our proposed loss framework achieves state-of-the-art performance for CTR prediction and we have deployed it to real-world ads system on CTR in the fine-rank stage until now. (2) We give theoretical Conference'17, July 2017, Washington, DC, USA Zhu and Liu, et al. Figure 1: brief system overview of CTR prediction pipeline. Daily Batch Data Offline Mini-Batch Training Offline retraining: daily Minutelevel Batch Data Online learning: Within 10 minutes Online serving predictor Online ad serving platform Mini-Batch Training Online Serving Scored deep model Update New Models click/unclick User Requests Update New Model and experimental analysis on our proposed confidence ranking loss showing the superiority over distillation.", "2 Methods": "", "2.1 Preliminaries": "Formally, as illustrated in Figure 1, a CTR prediction pipeline in real-world system can be split to three parts: (1) Offline training: given old dataset D \ud835\udc5c\ud835\udc59\ud835\udc51 that consists of continuous T days data with corresponding input sample \ud835\udc65 ad ground truth labels \ud835\udc66 , we build a machine learning model \ud835\udc53 with parameters \ud835\udf03 for the aim to optimize in a sequential manner. We note the output logits with \ud835\udc67 \u225c \u210e ( \ud835\udc65 ; \ud835\udf03 ) , ground truth \ud835\udc66 and the corresponding probability with \ud835\udc53 ( \ud835\udc65 ; \ud835\udf03 ) \u225c sigmoid ( \u210e ( \ud835\udc65 ; \ud835\udf03 )) . The goal of first part is to optimize \ud835\udc53 on the D \ud835\udc5c\ud835\udc59\ud835\udc51 with cross-entropy optimization:  (2) Online serving: after the loss L \ud835\udc5c\ud835\udc59\ud835\udc51 converge smaller than \ud835\udeff , where \ud835\udeff is specified by cross-validation on D \ud835\udc5c\ud835\udc59\ud835\udc51 , we deploy the machine learning model \ud835\udc53 \ud835\udf03 on the real-world system for the aim to serve the new arriving dataset D \ud835\udc5b\ud835\udc52\ud835\udc64 where the ground truth \ud835\udc66 \ud835\udc5b\ud835\udc52\ud835\udc64 is unknown until the user clicks the item or leave the browser. Until now, we have demonstrate the pipeline of real world deployment applications. (3) Online learning: As we get new arriving data, we retrain previous deployed model \ud835\udc53 \ud835\udc5c\ud835\udc59\ud835\udc51 in order to overcome the challenge of inconsistency between D \ud835\udc5c\ud835\udc59\ud835\udc51 and D \ud835\udc5b\ud835\udc52\ud835\udc64 before online serving with new model \ud835\udc53 \ud835\udc5b\ud835\udc52\ud835\udc64 . Benefited from mitigation on distributional drift, the strategy of online learning with recent data can efficiently improve the generalization on the next serving stage under the assumption that recent activities reflect the users' evolving interest. Despite effectiveness of the pipeline, this strategy does not take previous model's outputs as auxiliary information which covers the underlying relationship between prediction and ground truth. It keeps unknown that how a vastly different training manner induced by model and data evolution impact online performance. As we open an important question of how to learn better in both stages in our system, we would ideally like to define the concept \"better\" as difference of the metric score generated by \ud835\udc53 and \ud835\udc53 \ud835\udc5c\ud835\udc5b\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52 where \ud835\udc53 and \ud835\udc53 \ud835\udc5c\ud835\udc5b\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52 is the models that we currently train and deploy on the online serving platform respectively. Given a metric function M( \ud835\udc66, \u02c6 \ud835\udc66 ) , where \ud835\udc66 and \u02c6 \ud835\udc66 is the ground truth labels and predicted values, we then seek to minimize the classification risk for \ud835\udc53 , subject to the metric score generated by \ud835\udc53 should be better than \ud835\udc53 \ud835\udc5c\ud835\udc5b\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52 :  where \ud835\udc36 ( \ud835\udc53 ) \u225c M( \ud835\udc66, \ud835\udc53 ( \ud835\udc65 )) - M( \ud835\udc66, \ud835\udc53 \ud835\udc5c\ud835\udc5b\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52 ( \ud835\udc65 )) . In this way, we can design algorithm in the mini-batch training strategy in both stages as shown in Figure 1 with extra online model predictions. However, directly optimizing the metric score is not differentiate for deep models. For the purpose of designing a tractable algorithm, we will instead work with softer notion of \"better\", which evaluates the divergence of their metric scores.", "2.2 Confidence Ranking": "Metric loss typically are calculated by 0-1 loss. For example, accuracy and AUC of a mini-batch samples can be defined by \ud835\udc4e\ud835\udc50\ud835\udc50 = 1 \ud835\udc5b \u02dd \ud835\udc5b \ud835\udc56 = 1 I [ \ud835\udc66 \ud835\udc56 == \u02c6 \ud835\udc66 \ud835\udc56 ] and \ud835\udc4e\ud835\udc62\ud835\udc50 = 1 \ud835\udc5b\ud835\udc5a \u02dd \ud835\udc5b \ud835\udc56 = 1 \u02dd \ud835\udc5a \ud835\udc57 = 1 I [ \u02c6 \ud835\udc66 + \ud835\udc56 > \u02c6 \ud835\udc66 -\ud835\udc57 ] respectively, where I is indicator function. To ease optimization, researchers resort to surrogate score function when maximizing accuracy and auc [9]. Thus, we can devise confidence-ranking loss to directly employ previous learned knowledge. Confidence Ranking (CR) for Accuracy. Aswewanttoachieve better accuracy, the expected metric objective can be defined as \ud835\udc36 \ud835\udc4e\ud835\udc50\ud835\udc50 ( \ud835\udc53 ) = 1 \ud835\udc5b \u02dd \ud835\udc5b \ud835\udc56 = 1 I [ \ud835\udc66 \ud835\udc56 \ud835\udc53 ( \ud835\udc65 \ud835\udc56 ) > \ud835\udc66 \ud835\udc56 \ud835\udc53 \ud835\udc5c\ud835\udc5b\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52 ( \ud835\udc65 \ud835\udc56 )] . As we want to maximize this objective, we only need to induce a surrogate loss function to rank the point-wise model outputs which can be defined as:  where we only consider scoring functions \ud835\udf19 \ud835\udc66 that are strictly proper [4] (e.g. logistic rank loss \ud835\udf19 \ud835\udc66 ( \ud835\udc62, \ud835\udc63 ) = \ud835\udc59\ud835\udc5c\ud835\udc54 ( 1 + \ud835\udc52\ud835\udc65\ud835\udc5d -( \ud835\udc62 -\ud835\udc63 ) ) and square loss \ud835\udf19 \ud835\udc66 ( \ud835\udc62, \ud835\udc63 ) = ( 1 - ( \ud835\udc62 -\ud835\udc63 )) 2 ). For simplicity, in this work we only consider the logistic loss function which can be defined as:  where \ud835\udc62 and \ud835\udc63 are \u210e ( \ud835\udc65 \ud835\udc56 ) and \u210e \ud835\udc5c\ud835\udc5b\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52 ( \ud835\udc65 \ud835\udc56 ) respectively. Relational Confidence Ranking (RCR) for AUC: The pointwise loss that rank the output of current model with online deployed one ensures the network gradually perform better. To further improve the bipartite ranking performance of binary classification, we follow [3] in optimizing bipartite ranking performance. As we want to achieve better bipartite ranking performance, the expected metric objective can be defined as \ud835\udc36 \ud835\udc4e\ud835\udc62\ud835\udc50 ( \ud835\udc53 ) = 1 \ud835\udc5b\ud835\udc5a \u02dd \ud835\udc5b \ud835\udc56 = 1 \u02dd \ud835\udc5a \ud835\udc57 = 1 I [( \ud835\udc53 ( \ud835\udc65 + \ud835\udc56 ) -\ud835\udc53 ( \ud835\udc65 -\ud835\udc57 )) > ( \ud835\udc53 \ud835\udc5c\ud835\udc5b\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52 ( \ud835\udc65 + \ud835\udc56 ) -\ud835\udc53 \ud835\udc5c\ud835\udc5b\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52 ( \ud835\udc65 -\ud835\udc57 ))] where \ud835\udc65 + \ud835\udc56 and \ud835\udc65 -\ud835\udc57 are \ud835\udc56 -th positive sample and \ud835\udc57 -th negative sample. Thus, we define relational confidence ranking risk as:  where function \ud835\udc51 \ud835\udc53 ( \ud835\udc65, \ud835\udc67 ) = \ud835\udc53 ( \ud835\udc65 ) -\ud835\udc53 ( \ud835\udc67 ) performs calculating distance of different samples \ud835\udc65 and \ud835\udc67 , and P + and P are the positive and negative classes respectively. Similar to point-wise confidenceranking loss, we select logistic loss function as our scoring function:  where \ud835\udc62 and \ud835\udc63 are \ud835\udc51 \u210e ( \ud835\udc65 + \ud835\udc56 , \ud835\udc65 -\ud835\udc57 ) and \ud835\udc51 \u210e \ud835\udc5c\ud835\udc5b\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52 ( \ud835\udc65 + \ud835\udc56 , \ud835\udc65 -\ud835\udc57 ) respectively. Confidence Ranking for CTR Prediction Conference'17, July 2017, Washington, DC, USA Table 1: The statistic of CTR prediction datasets Table 2: AUC(%) of test-set performance on Avito, Avazu and Industrial datasets with various backbone and training strategy. * denotes one-pass learning. The results are averaged over 3 runs. Std \u2264 0 . 1% . Training with Confidence Ranking. During training, multiple confidence ranking loss function, including the proposed point-wise CR loss and relational CR loss can be either alone or together with task-specific loss functions, e.g. cross-entropy for classification. Therefore, the final objective is defined as:  where \u2113 \ud835\udc50\ud835\udc52 is a cross-entropy loss in CTR prediction, \u2113 \ud835\udc36\ud835\udc45 and \u2113 \ud835\udc45\ud835\udc36\ud835\udc45 are the point-wise and relational confidence ranking loss respectively, and \ud835\udf06 \ud835\udc36\ud835\udc45 \ud835\udc4e\ud835\udc50\ud835\udc50 and \ud835\udf06 \ud835\udc45\ud835\udc36\ud835\udc45 \ud835\udc4e\ud835\udc62\ud835\udc50 are tunable hyperparameters to control the loss terms. For sampling tuples of pos/neg samples in the proposed relational confidence ranking loss, we simply use all possible pairs in a given mini-batch.", "Theorem 1. ( Bias-Variance bound for confidence ranking )": "Pick any convex loss \u2113 . Suppose we have a teacher model \ud835\udc5d \ud835\udc61 with corresponding empirical confidence ranking risk b \ud835\udc45 ( \ud835\udc53 ) = 1 \ud835\udc41 \u02dd \ud835\udc5b \u2208 \ud835\udc41 \ud835\udc66 ( \ud835\udc65 \ud835\udc5b ) \u2113 ( f ( \ud835\udc65 \ud835\udc5b ) -\ud835\udc53 \ud835\udc61 ( \ud835\udc65 \ud835\udc5b ))) and population risk \ud835\udc45 ( \ud835\udc53 ) = E \ud835\udc65 [ \ud835\udc5d \u2217 ( \ud835\udc65 ) \u2113 ( \ud835\udc53 ( \ud835\udc65 ))] where \ud835\udc53 \ud835\udc61 ( \ud835\udc65 \ud835\udc5b ) is the teacher output. For any predictor \ud835\udc53 : X \u2192 R \ud835\udc3f ,  We have stated a statistical perspective on confidence ranking, resting on the observation that confidence ranking offers a bound which always approximating Bayes probabilities based on the performance of teacher model. However, this bound is not well qualified on deep learning architecture and may be loose and unstable for real-world application especially for the logistic confidence ranking loss. We note the comprehensive bound of confidence ranking requires specifying necessary conditions. Nonetheless, this qualitative bound can still hold majority conditions in practice.", "3 Experiments on CTR prediction": "We evaluate our methods on Industrial, Avazu and Avito datasets with various controllable setting on CTR prediction. Our core algorithm is easy to implement with various machine learning platform. For industrial datasets, we develop it with TensorFlow while we conduct experiments with PyTorch implementation for public datasets. All of our experiments are conducted on one P40 GPU for public datasets and 8 A100 GPUs for industrial dataset respectively. Datasets. We perform our experiments on three datasets with two training setting. Industrial search ads dataset contains 59 numerical and categorical feature fields. All of the fields data are discretized and transformed into sparse anonymous features. This dataset has more than ten billion instances range over one month with hundreds millions of active users and items. Avazu is display recommendation dataset released on Kaggle that contain 40428967 samples with 22 feature fields. Avito is also released as ads click datasets on Kaggle containing 190107687 samples but only 16 feature fields. We construct public datasets by split into training/validation/test set by timestamp where the samples of last day is set for testing and penultimate day's data is set for validation and others are set for training. To split industrial dataset, we use traffic samples of previous 15 days as training set and the last day as test set. We summarize statistic of datasets in Table 1 . Experiments setup . In real-world application, the prediction naturally influence the impression of items (i.e. items that have high confidence are more prone to be exposed to users.) and multiples times for training will cause severe over-fitting issues and much more computation cost. Thus we adopt two various settings for evaluate our methods. The details of configuration are summarized as follows: (1) One-Pass Setting: we adopt one-pass training strategy to imitate online-learning in a cycle serving-and-training process with constant daily and minute-level data. For industrial dataset, it's common setting for evaluating performance of our methods. However, for public datasets, all of them only contain item and user features without any information of the deployed model. To overcome, we first train a one-pass model on \ud835\udc47 -days training set and then launch the cycling serving-and-training process with followed \u02c6 \ud835\udc47 -days data (e.g. serve on \ud835\udc47 + 1-th day data to get predictions as \ud835\udc53 \ud835\udc5c\ud835\udc5b\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52 ( \ud835\udc65 ) then train with our method). We denote our experiments on industrial dataset only adopt one-pass training strategy. (2) Standard Setting: standard supervised learning. In this sense, all of our experiments train on training set with multiple epochs until the validation loss convergences. We use the outputs of previous epoch as the supervised signal for our proposed method. Baselines. Though our main motivation of our work is to utilize the confidence of online deployed model on target items, we still include several baselines under standard supervised learning and one-pass learning in order to benchmark state-of-the-art results. The simplest approach is (1) ERM: we train our networks with binary cross-entropy loss under two various settings. For majority of real-world recommendation and ads system, ctr prediction models are preferred to be trained with ERM; (2) various commonly adopted CTR prediction network architectures designed for recommendation and ads system, DNN, PNN[11], DCN[13], DeepFM[5]; (3) SC[1] integrates self-correction module into CTR prediction networks. Together training with ERM, it achieves state-of-the-art Conference'17, July 2017, Washington, DC, USA Zhu and Liu, et al. Figure 2: Sample margin, prediction mean of negative and positive samples on Avazu in one-pass setting. (a) Sample Margin 0.21 ERM 0.19 RCR 5 CR+RCR 2 0.18 RKD 0.17 0.16 0.15 0.14 Days (b) Negative Mean 0.16 0.15 0.14 0.13 ERM 0.11 KD RCR 0.10 CR+RCR 0.09 RKD Days (c) Positive Mean ERM 34 RCR 0.32 CR+RCR RKD 0.30 0.28 0.26 Days results on multiple CTR prediction datasets with minimal computation cost. (4) Knowledge distillation methods: since dark knowledge can induce useful gradients for model compression, we also adapt KD[6] and RKD[10] to our experimental setting. In this paper, we modify the feature-based RKD to logit-based method for aligning inter-sample distance of the logits output of the base model and current model. For all loss function we tune their loss balance term \ud835\udf06 ranging from 0.1 to 2.0. We select the best results in our experiments. The best \ud835\udf06 \ud835\udc36\ud835\udc45 \ud835\udc4e\ud835\udc50\ud835\udc50 and \ud835\udf06 \ud835\udc45\ud835\udc36\ud835\udc45 \ud835\udc4e\ud835\udc62\ud835\udc50 is [0.4, 0.5] for public datasets and [0.5, 1.0] for industrial dataset. Main Results. Table 2 compare the test-set AUC of our method on Click-Through-Rate prediction task. On Table 2, we first investigate the improvement brought by different feature interaction methods. We observe that PNN achieve best performance with marginal improvement on standard supervised learning setting but fails compared to DeepFM and DCN on one-pass setting. For convenience, we adopt DeepFM as backbone for our experiments. We can observe that the propose method CR outperforms all baselines no matter which setting is adopted. For standard supervised learning, it is also striking to see that on Avazu and Avito, our proposed CR and RCR both can outperform baselines by a large margin after trained with multiple epochs. We denote 0.1% improvement of AUC on Avazu and Avito is significant. For one-pass learning, we still observe that our proposed methods outperforms the backbone model but the margin is smaller than standard setting. It's because one-pass learning may not completely fit on the two public datasets. For industrial dataset, we carefully tune our proposed method with DeepFM due to its succinct implementation. Not surprisingly, it works as well. Compared to vanilla distillation, our methods improve 0.25/0.61/0.31/0.16/0.14% of the AUC respectively. Inter-class margin Visualization. In Figure 2, we show how sample margin, prediction mean value of positive and negative samples vary along time. The relational confidence ranking loss outperforms all the other method by a large margin. We can observe RCR both decrease the negative mean and increase the positive mean in Figure 2b and 2c leading to best bipartite ranking performance among all baseline methods in Table 2. We find CR both decreases negative and positive mean resulting in marginal improvement on sample margin. We demonstrate it's because CTR prediction dataset is usually dominated by negative samples and our loss function tends to depress the negative prediction. In Figure 2, we find KD and RKD \ud835\udc59 give more smooth curve compared to our methods which may constrain the model's learning ability. Table 3: Results of click-through rate improvement on a 5day online A/B experiment.", "4 Online A/B Experiments": "Our architecture comprises of two parts: (1) In online ad serving platform, we additionally collect the outputs \ud835\udc66 \ud835\udc5c\ud835\udc5b\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52 of \ud835\udc53 ( \ud835\udc65 ; \ud835\udf03 \ud835\udc5c\ud835\udc5b\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52 ) as a online deployed prediction into our training data which directly decides which item will be exposed; (2) impose our proposed ranking-based loss to encourage the network to learn better than the online deployed one in both retraining and online-learning stage. Online A/B results. Additional to offline experiments, we conduct online experiments on A/B platform from 2022-8-15 to 20228-19. Our online A/B test experiments split active users into two groups. The first group is served by the recommendation results generated by current main model while the second is served by Confidence Ranking results. As shown in table 3, We observe averaged 1.75% improvement on CTR and apply it to serve main traffic in our system.", "5 Conclusion": "From the perspective of real-world application, we identify the problem of learning model for better generalization on retraining and online-learning stage compared to online deployed model. To address this problem, we propose a loss framework, named as Confidence Ranking, which compares various models' output predictions for maximizing the surrogate metric score. We extend this method to rank accuracy and Auc in CTR prediction. Our theoretical and experimental analysis shows that our method can effectively improve the results compared to cross-entropy optimization and distillation. Confidence Ranking for CTR Prediction", "REFERENCES": "[1] Guohao Cai, Jieming Zhu, Quanyu Dai, Zhenhua Dong, Xiuqiang He, Ruiming Tang, and Rui Zhang. 2022. ReLoop: A Self-Correction Continual Learning Loop for Recommender Systems. arXiv preprint arXiv:2204.11165 (2022). [2] Tri Dao, Govinda M Kamath, Vasilis Syrgkanis, and Lester Mackey. 2021. Knowledge Distillation As Semiparametric Inference. arXiv preprint arXiv:2104.09732 (2021). [3] Yoav Freund, Raj Iyer, Robert E Schapire, and Yoram Singer. 2003. An efficient boosting algorithm for combining preferences. Journal of machine learning research 4, Nov (2003), 933-969. [4] Tilmann Gneiting and Adrian E Raftery. 2007. Strictly proper scoring rules, prediction, and estimation. Journal of the American statistical Association 102, 477 (2007), 359-378. [5] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. arXiv preprint arXiv:1703.04247 (2017). [6] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 2, 7 (2015). [7] Sebastian Hofst\u00e4tter, Sophia Althammer, Michael Schr\u00f6der, Mete Sertkan, and Allan Hanbury. 2020. Improving efficient neural ranking models with crossarchitecture knowledge distillation. arXiv preprint arXiv:2010.02666 (2020). Conference'17, July 2017, Washington, DC, USA [8] SeongKu Kang, Junyoung Hwang, Wonbin Kweon, and Hwanjo Yu. 2021. Topology distillation for recommender system. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining . 829-839. [9] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. 2018. Foundations of machine learning . MIT press. [10] Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho. 2019. Relational knowledge distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 3967-3976. [11] Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, and Jun Wang. 2016. Product-Based Neural Networks for User Response Prediction. In 2016 IEEE 16th International Conference on Data Mining (ICDM) . 1149-1154. [12] Jiaxi Tang and Ke Wang. 2018. Ranking distillation: Learning compact ranking models with high performance for recommender system. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 2289-2298. [13] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & Cross Network for Ad Click Predictions. In Proceedings of the ADKDD'17 . 12."}
