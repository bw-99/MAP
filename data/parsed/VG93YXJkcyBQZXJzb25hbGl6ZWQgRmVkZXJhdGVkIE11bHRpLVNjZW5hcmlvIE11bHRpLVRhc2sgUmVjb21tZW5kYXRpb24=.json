{
  "Towards Personalized Federated Multi-Scenario Multi-Task Recommendation": "Yue Ding dingyue@sjtu.edu.cn Shanghai Jiao Tong University China Xun Cai caixun@sjtu.edu.cn Shanghai Jiao Tong University China",
  "Xin Xin": "xinxin@sdu.edu.cn Shan Dong University China Chang Liu isonomialiu@sjtu.edu.cn Shanghai Jiao Tong University China Yanbiao Ji jiyanbiao@sjtu.edu.cn Shanghai Jiao Tong University China Yuxiang Lu luyuxiang_2018@sjtu.edu.cn Shanghai Jiao Tong University China Xiaofeng Gao gao-xf@sjtu.edu.cn Shanghai Jiao Tong University China",
  "Hongtao Lu": "htlu@sjtu.edu.cn Shanghai Jiao Tong University China",
  "ABSTRACT": "In modern recommender systems, especially in e-commerce, predicting multiple targets such as click-through rate (CTR) and postview conversion rate (CTCVR) is common. Multi-task recommender systems are increasingly popular in both research and practice, as they leverage shared knowledge across diverse business scenarios to enhance performance. However, emerging real-world scenarios and data privacy concerns complicate the development of a unified multi-task recommendation model. In this paper, we propose PF-MSMTrec, a novel framework for personalized federated multi-scenario multi-task recommendation. In this framework, each scenario is assigned to a dedicated client utilizing the Multi-gate Mixture-of-Experts (MMoE) structure. To address the unique challenges of multiple optimization conflicts, we introduce a bottom-up joint learning mechanism. First, we design a parameter template to decouple the expert network parameters, distinguishing scenario-specific parameters as shared knowledge for federated parameter aggregation. Second, we implement personalized federated learning for each expert network during a federated communication round, using three modules: federated batch normalization, conflict coordination, and personalized aggregation. Finally, we conduct an additional round of personalized federated Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference acronym 'XX, June 03-05, 2018, Woodstock, NY ¬© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/18/06...$15.00 https://doi.org/XXXXXXX.XXXXXXX parameter aggregation on the task tower network to obtain prediction results for multiple tasks. Extensive experiments on two public datasets demonstrate that our proposed method outperforms state-of-the-art approaches. The source code and datasets will be released as open-source for public access.",
  "CCS CONCEPTS": "¬∑ Information systems ‚Üí Recommender systems .",
  "KEYWORDS": "Multi-task Recommendation, Federated Learning, Collaborative Filtering",
  "ACMReference Format:": "Yue Ding, Yanbiao Ji, Xun Cai, Xin Xin, Yuxiang Lu, Suizhi Huang, Chang Liu, Xiaofeng Gao, Tsuyoshi Murata, and Hongtao Lu. 2018. Towards Personalized Federated Multi-Scenario Multi-Task Recommendation. In Proceedings of Make sure to enter the correct conference title from your rights confirmation emai (Conference acronym 'XX). ACM, New York, NY, USA, 11 pages. https://doi.org/XXXXXXX.XXXXXXX",
  "1 INTRODUCTION": "Recommender systems leverage users' past behaviors to predict their interests and preferences, thereby delivering tailored recommendations for personalized content. In modern applications of recommender systems, there are often multiple prediction targets. For example, in e-commerce scenarios, it's necessary to estimate both the click-through rate and conversion rate of products. In short video platforms, predictions may involve estimating clicks, playback time, shares, comments, and likes. Therefore, recommender systems should possess the capability to simultaneously perform multiple recommendation tasks to meet the diverse needs of users [42]. Traditional recommendation models usually build separate prediction",
  "Suizhi Huang": "huangsuizhi@sjtu.edu.cn Shanghai Jiao Tong University China Tsuyoshi Murata murata@c.titech.ac.jp Tokyo Institute of Technology Japan Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Yue Ding, Yanbiao Ji, Xun Cai, Xin Xin, Yuxiang Lu, Suizhi Huang, Chang Liu, Xiaofeng Gao, Tsuyoshi Murata, and Hongtao Lu models for different recommendation tasks and then merge them. Nevertheless, this model fusion approach has two main drawbacks: ( ùëñ ) Most mainstream recommendation models rely on deep neural networks with many parameters. Trying to optimize and merge multiple models simultaneously requires a lot of computational resources, making it difficult for practical online applications. ( ùëñùëñ ) There could be connections between different tasks, and optimizing them individually might overlook these relationships. Hence, there's a growing interest in multi-task recommender systems in both research and practical applications. Conventional multi-task recommender systems primarily focus on business data from a single scenario with the aim of simultaneously improving the prediction performance of multiple tasks. When there are more fine-grained businesses in the system, it is necessary to merge and model different recommendation business scenarios to utilize the commonalities between different scenarios to enhance the overall performance. Taking the example of food recommendation on Meituan [50], its business may involve various scenarios such as limited-time flash sale recommendation, search result sorting, and discounted meal package recommendation. Users engage in clicking, browsing, and other actions across multiple scenarios, eventually leading to a purchase. Such businesses can be implemented using a unified framework for multi-scenario multitask recommendation. As real-world business is evolving rapidly, some new and more intricate recommendation business scenarios have emerged. For example, recommender systems can leverage data from multinational corporations' global branches. This data can be segmented by user location, allowing the system to employ country-specific models that treat users from different nations as distinct scenarios. This approach accounts for potential variations in user preferences across geographical regions. In another real-world application, advertising alliance recommendations involve a mediating platform that aggregates advertising inventory from multiple websites. This platform then connects advertisers with these websites, allowing them to display ads and earn revenue based on factors like ad impressions or clicks. In these two cases, privacy concerns arise because data from each participant is private, and individual prediction models are customized. Consequently, the issue is that training a single, global model becomes very difficult. Federated learning (FL) [16] is a collaborative learning paradigm that allows joint optimization across multiple clients while preserving all clients' data privacy. However, it is difficult to simply extend multi-scenario and multi-task recommendation to the FL framework. The key challenging issue is that multiple optimization conflicts overlap and intertwine in this situation, which can easily lead to a decline in overall performance. More specifically, ( ùëñ ) Datalevel conflict . The data distributions of user-item interactions in multiple scenarios vary. In FL, each client has an independent model to fit the scenario-specific data, and all the clients' models may project data to different feature spaces because each client's data is private and isolated. ( ùëñùëñ ) Model-level conflict . In FL, models from different clients are typically aggregated using parameter averaging. Differences in data distribution among clients can lead to discrepancies in model parameters, resulting in performance decline on all clients during federated parameter aggregation. ( ùëñùëñùëñ ) Task-level conflict . Different tasks have different targets which may influence each other. If the model cannot effectively balance these interdependent targets, it can prioritize one task over another, leading to uneven performance, which is recognized as the \"task seesaw phenomenon\" [38]. Figure 1 illustrates the difference and federated multi-scenario multi-task recommendation and non-federated scenario. To the best of our knowledge, federated learning has not yet been explored for multi-scenario multi-task recommendation. To address the research gap and tackle this challenging problem, we propose a P ersonalized F ederated learning framework for M ultiS cenario M ultiT ask recommendation (PF-MSMTrec). Specifically, we assign each scenario to a distinct client, ensuring that the training data for each client remains independent and private. We utilize a Multi-gate Mixture-of-Experts (MMoE) structure for each client, where each client may have multiple expert networks. We decouple the parameters of each expert network into three categories: global shared parameters, task-specific parameters, and scenario-specific parameters, by designing a parameter template. Federated aggregation is then performed on the scenario-specific parameters, while the other parameters are treated as local personalized parameters. We propose a set of modules for federated parameter aggregation, including federated batch normalization, conflict coordination, and personalized aggregation, all of which are utilized during the federated communication rounds. Finally, after processing through the local gate network, we apply the conflict coordination mechanism to the parameters of the tower networks for personalized aggregation, facilitating multi-task prediction. The main contributions of this paper are summarized as follows: ¬∑ We propose a novel personalized federated recommendation framework for multi-scenario multi-task recommendation. To the best of our knowledge, it is the first work to tackle this challenging problem. The proposed method broadens the applicability of recommender systems by tackling more sophisticated business settings. ¬∑ To address the multiple optimization conflicts inherent in federated multi-scenario multi-task recommendation, we propose a meticulously designed bottom-up joint learning mechanism, which incorporates modules for expert network parameter decoupling, federated batch normalization, conflict coordination, and personalized parameter aggregation. The collaborative work of these modules effectively alleviate optimization conflicts and enable personalized learning for local models. ¬∑ We conduct extensive experiments on two public datasets, thoroughly comparing the performance of the proposed method with state-of-the-art (SOTA) multi-scenario multi-task recommendation methods and federated learning approaches. Notably, by adapting our method to the federated learning setting, we exceed the performance of SOTA methods originally designed for traditional centralized learning.",
  "2 RELATED WORK": "",
  "2.1 Multi-task and Multi-scenario Recommendation": "Multi-task learning has been widely researched and applied in the fields of Computer Vision (CV) [40] and Natural Language Processing (NLP) [49]. In the area of recommender systems, multi-task recommendation is generally based on deep learning and mainly Towards Personalized Federated Multi-Scenario Multi-Task Recommendation Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Scenario 1 Scenario 2 Scenario 3 Model Task 1 Task 2 Federated Parameter Aggregation on Server Scenario 1 Scenario 2 Scenario 3 Model 1 Model 2 Model 3 Task 1 Task 2 Model 1 Model 2 Model 3 Task-level Conflict Private Local Data Private Local Data Private Local Data Unseen Model-level Conflict Data-level  Conflict Unseen Model-level Conflict (a) Multi-scenario multi-task recommendation (b) Fedrated multi-scenario multi-task recommendation Figure 1: Illustration of non-federated (a) vs. federated multi-scenario multi-task recommendation (b). In the federated setting, diverse data distributions across scenarios (clients) cause data-level conflicts. Data and model parameters stay private on each client, while the server aggregates parameters, leading to model-level conflicts. Each client also handles multiple tasks, introducing task-level conflicts. includes three categories of techniques [42]: Optimization methods, Training mechanisms, and Parameter sharing. Optimization methods refer to addressing the problem of performance degradation due to gradient conflicts among multiple tasks during parameter training [10]. Training mechanism techniques are aimed at setting training and learning strategies for different tasks [3]. Parameter sharing methods are the most important category among them. The Multi-gate Mixture-of-Expert (MMoE) [25] model is the mainstream architecture for multi-task recommender systems. MMoE is a parameter sharing approach at the expert network level, which combines multiple expert networks using different gates to make predictions for multiple tasks. PLE [38] designs shared experts and task-exclusive experts, customizing learning for different tasks using a customized gating module. ESMM [26] implicitly trains the target task using auxiliary tasks. AITM [45] introduces sequence dependency based on ESMM, enhancing the modeling of inter-task correlations by introducing a self-attention sequence dependency propagation module. CSRec [3] is a contrastive learning-based method that can adaptively update parameters to alleviate task conflicts. CMoIE [41] improves the mixture policy for multiple expert networks by devising conflict resolution, expert communication, and mixture calibration modules. AdaTT [18] uses an adaptive fusion mechanism to jointly learn task-specific and shared features. Multi-scenario recommendation systems enhance understanding of user behaviors and preferences across different domains to provide more personalized recommendations. STAR [34] decouples domain parameters and uses a star topology for multidomain recommendation. AFT [8] employs generative adversarial networks for feature translation between domains. HAMUR [21] uses adapter layers for multi-domain recommendation. EDDA [32] disentangles embeddings and aligns domains to improve knowledge transfer. MetaDomain [48] uses a domain intent extractor and meta-generator to fuse domain intent representations for predictions. ADIN [14] models commonalities and differences across scenarios with an adaptive domain interest network. Maria [39] injects scene semantics at the network's bottom for adaptive feature learning. PLATE [43] proposes a pre-train and prompt-tuning paradigm to efficiently enhance performance for multiple scenarios. SAMD [11] addresses the multi-scenario heterogeneity problem through knowledge distillation. Modeling multiple scenarios and tasks simultaneously is currently a topic of growing interest in the field of recommender systems. AESM 2 [52] proposes an automatic expert search framework for multi-task learning, integrating hierarchical multiple expert layers with different recommendation scenarios. PEPNet [5] is a plug-and-play parameter and embedding personalized network suitable for multi-scenario and multi-task recommendations. HiNet [50] is a multi-scenario multi-task recommendation model based on a hierarchical information extraction network. It achieves information extraction through a knowledge transfer scheme from coarsegrained to fine-grained levels. M3REC [17] is a meta-learning-based framework that realizes unified representations and optimization in multiple scenarios and tasks.",
  "2.2 Federated Learning for Recommendation": "Federated learning is a distributed machine learning framework for preserving data privacy, mainly using the method of passing model parameters to implicitly coordinate the training of models among various participants. According to the differences in data and feature dimensions among different participants, federated learning can be roughly divided into three categories: horizontal federated learning, vertical federated learning, and federated transfer learning [46]. Typical federated learning methods include: FedAvg [29]: It calculates the average of model parameters from all participants as the global model parameters. Personalized Federated Learning (PFL) aims to alleviate the slow convergence and poor performance problems under non-i.i.d. (non-independent and identically distributed) data, making the model personalized for local tasks and datasets [36]. Federated recommender systems are one of the important applications of federated learning. FedRecSys [37] is an open-source federated recommendation systems capable of providing online services. Many collaborative filtering algorithms also have corresponding federated learning versions, such as Federated Collaborative Filtering (FCF) [2] and Federated Matrix Factorization [22]. FedFast [31] and FL-MV-DSSM [12] are representative federated recommender systems based on deep learning techniques. DeepRec [7] proposes federated sequence recommendation. FedGNN [44], PerFedRec [24], and SemiDFEGL [33] are graph neural network-based federated recommendation models. PFedRec [47] is a cross-device personalized federated recommendation framework that learns Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Yue Ding, Yanbiao Ji, Xun Cai, Xin Xin, Yuxiang Lu, Suizhi Huang, Chang Liu, Xiaofeng Gao, Tsuyoshi Murata, and Hongtao Lu lightweight models to capture fine-grained user and item features. RF 2 [27] investigates the fairness problem in federated recommendation, and F 2 PGNN [1] further addresses the group bias issue in graph neural networks, proposing a fair and personalized federated recommendation framework.",
  "2.3 Federated Multi-task Learning": "Federated multi-task learning has emerged as a research problem in recent years [9, 30]. Approaches such as MOCHA [35] and FedEM [28] have proposed methods for jointly training multiple tasks across multiple participants with diverse data distributions. Fedbone [6] enhances feature extraction capability by aggregating encoders from gradients uploaded by each client. Addressing the issue of task heterogeneity, MAS [51] allocates different multi-task models to different clients and aggregates models within clients with the same task set. MaT-FL [4] uses dynamic grouping to combine different client models.",
  "3 METHOD": "Our proposed PF-MSMTrec method is illustrated in Figure 2. We detail our framework components below.",
  "3.1 Problem Definition": "Suppose we have a total of ùëÜ application scenarios, each separated from the others. Correspondingly, we have ùëÜ clients, with each client restricted to accessing only local data. We employ the mixture-ofexperts structure, allowing each client to utilize multiple expert networks to handle various prediction tasks. Assuming there are a total of ùëá tasks and ùëÜ clients, where each client comprises ùëÅ experts. We define the problem within the ùëó -th client for predicting the ùëñ -th task as follows:  where ùëñ ‚àà { 1 , ..., ùëá } and ùëó ‚àà { 1 , ..., ùëÜ } . Fed [‚à• ùëÅ ùëõ = 1 ùëì ùëõ ùëñ (¬∑)] represents the prediction of the ùëñ -th task, which is made by a combination of ùëÅ expert networks within the context of federated learning. Here, x denotes the dense feature, ùê∑ ùëó represents local data in the ùëó -th scenario, ùë° ùëñ and ùë† ùëó denote the task and scenario embeddings, respectively.",
  "3.2 Decoupling Expert Parameters": "For the ùëó -th client that has ùëÅ experts, we input three types of data into each expert network: dense feature vectors, task feature vectors, and scenario feature vectors. For dense feature vectors, we apply Batch Normalization (BN):  where ùêæ represents the number of data samples in one batch, x ùëò denotes the ùëò -th input dense feature vector, ùõæ and ùõΩ are learnable parameters, and ùúñ is a small constant. BN normalizes local input data and mitigates discrepancies in data distribution at the input layer, thereby alleviating conflicts arising from data distribution. To generate task-specific parameters, we employ a parameter template comprising neural networks dedicated to each expert. This design facilitates parameter separation within expert networks, enabling them to extract not only input features but also taskrelated and scenario-specific features:  where ùëõ ‚àà { 1 , 2 , ..., ùëÅ } indicates the number of the expert network, MLP ùë° , and MLP ùë† represent multi-layer perceptrons capable of generating parameters based on the input. t ùëñ and s ùëó are the task and scenario dense embeddings, respectively. By introducing the expertspecific local parameter W ùëõ ùëôùëúùëê , the local task-specific parameters are composed of the following three components:  where ‚äó denotes element-wise product. Subsequently, task-specific feature outputs can be obtained based on the input embedding:  where ùúé represents the ReLU non-linear activation function, and b ùëõ ùë° ùëñ is the bias term. For all experts within the ùëó -th client, the output set for the ùëñ -th task is described as the following set:  Here we use {¬∑} ùëó to denote the set of outputs for the ùëó -th client to avoid confusion caused by using superscripts and subscripts. We also emphasize that the parameters W ùëõ ùëôùëúùëê , W ùëõ ùë° ùëñ , and W ùëõ ùëó are distinct for each client.",
  "3.3 Federated Parameter Aggregation": "3.3.1 Fedrated Batch Normalization. Under the federated learning paradigm, we jointly train expert and tower networks using parameter aggregation. To implement personalization, we designate W ùëõ ùëôùëúùëê and W ùëõ ùë° ùëñ as local parameters, and W ùëõ ùëó as shared parameter for federated learning. We designed this approach to let each expert understand different tasks in various scenarios. We avoid federated aggregation on task parameters due to task commonalities and instead share the scenario-specific parameter W ùëõ ùëó among clients to facilitate mutual learning. Additionally, as shown in Eq. (4), W ùëõ ùëó is integrated into the local parameters, enabling updates to W ùëõ ùëó to influence the learning of personalized local parameters. We employ a simple federated batch normalization strategy, treating all shared expert network parameters as a batch on the server during each communication round.   where the subscript ùëî represents global. It is important to note that we calculate ùõæ ùëî and ùõΩ ùëî by averaging the local ùõæ and ùõΩ values from all clients, as there are no learnable parameters on the server.  Towards Personalized Federated Multi-Scenario Multi-Task Recommendation Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Figure 2: Framework of PF-MSMTrec for personalized federated multi-scenario multi-task recommendation. Each client handles a unique scenario with private data, using the MMoE structure. Parameter decoupling in expert networks enables federated aggregation of scenario-specific features. Federated batch normalization, conflict coordination, and personalized aggregation are applied in each communication round to address optimization conflicts. For clarity, Expert 3 and Expert 4 are used for Client 2, with ÀÜ x ùëò and ÀÜ x ùëò + 1 representing different inputs. Embedding Layer Server Embedding Layer Client 3, ...... , Client Gate 1 Communication Round Tower  1 Tower  2 Gate 2 Tower  1 Tower  2 Task 1 Server Communication Round Parameter Aggregation Conflict Coordination Task 2 Client 1 (Scenario 1) Federated BatchNorm Parameter Aggregation Conflict Coordination Task 1 Embedding Scenario Embedding Task 2 Embedding Parameter Template Expert 2 Local BatchNorm ... Parameter Template Expert 1 Local BatchNorm ... Embedding Layer Embedding Layer Gate 1 Gate 2 Task 1 Task 2 Client 2 (Scenario 2) Parameter Template Expert 4 Local BatchNorm ... Parameter Template Expert 3 Local BatchNorm ... Parameter Template for Expert 2 in Scenario 1 Non-linear activation Element-wise Product Non-linear activation Non-linear activation Non-linear activation Send parameters to server Receive aggregated parameters from server Expert-specific local parameters Input data features ... Task Indicator Scenario Indicator Input Data Task Indicator Scenario Indicator ... The typical federated aggregation approach is parameter averaging: of aggregated parameters:  Interestingly, we found that ¬Ø W ùëõ ùëó is only related to normalization parameter ùõΩ ùëî if federated communication occurs after the completion of each local batch. We provide a proof in Appendix A. It's important to note that local normalization operates on data, while federated normalization operates on parameters uploaded by clients. Their unification shows that parameter normalization during federated aggregation effectively leverages client data characteristics, addressing optimization conflicts due to domain distribution differences. 3.3.2 Conflict Coordination. Large parameter differences between clients during federated parameter aggregation are the cause of performance degradation in federated learning. Since parameters are determined by the gradients from local training, alleviating gradient conflict would be a direct and effective approach. Considering the local shared parameter W ùëõ ùëó in the expert network, we update:  where ùúÇ is the learning rate, g ùëñ denotes the gradient on the ùëñ -th task loss. We define the increment of the parameter update as Œî W ùëõ ùëó = -ùúÇ Àù ùëá ùëñ = 1 g ùëñ . Similarly, we have the global average increment  Drawing inspiration from CAGrad [23], we identify a set of parameters, denoted as U , to mitigate gradient conflicts between Œî W ùëõ ùëó and Œî ¬Ø W ùëõ ùëó through a simple gradient inner product and gradient constraint. This is achieved by optimizing the following objective:  where ‚ü®¬∑ , ¬∑‚ü© denote inner product, ùëê ‚àà [ 0 , 1 ) is the hyper-parameter. Note that federated learning generally restricts clients from uploading gradients due to privacy and security concerns. Therefore, to approximate the gradient, we calculate the difference between the parameters from two consecutive communication rounds:  where ùëü denotes the communication round. Substitute Œî ÀÜ W ùëõ,ùëó for Œî W ùëõ,ùëó in Eq. (13), we can solve the optimization problem by using Lagrangian and adding the constraint of Àù ùëÅ ùëõ = 1 Àù ùëÜ ùëó = 1 ùë§ ùëõ,ùëó = 1 , ùë§ ùëõ,ùëó ‚â• 0, we turn to the following optimization problem for ùë§ :   Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Yue Ding, Yanbiao Ji, Xun Cai, Xin Xin, Yuxiang Lu, Suizhi Huang, Chang Liu, Xiaofeng Gao, Tsuyoshi Murata, and Hongtao Lu We can derive the solution of U ‚àó after obtaining the optimal ùë§ : Table 1: Statistics of test datasets.  3.3.3 Personalized Parameter Aggregation. To maintain the personalization of local parameters while effectively aggregating federated parameters, we introduce a learnable weight parameter, ùúì ùëõ for each expert network. The updating process of the shared parameter W ùëõ ùëó is represented as:  For the gate network that is responsible for aggregating outputs from various experts for the ùëñ -th task in the ùëó -th scenario, we have:  where ùëé 1 , ..., ùëé ùëÅ are weight parameters learned by a neural network, subject to ùëé ùëõ ‚â• 0 and Àù ùëÅ ùëõ = 1 ùëé ùëõ = 1. The task tower, responsible for generating predictions for the ùëñ -th task, is defined as follows:  We conduct a personalized federated aggregation of all parameters within the Tower network.  where Œò Tow ùëñ,ùëó denotes the parameters of the ùëñ -th task tower in the ùëó -th scenario, ùúì ‚Ä≤ ùëõ represents another learnable weight parameter.",
  "3.4 Optimization": "We utilize the binary cross-entropy loss as the loss function:  where |X| ùëó represents the number of data samples in ùê∑ ùëó . To align local shared parameters with global aggregated parameters and prevent performance degradation, we introduce an additional regularization term:  The total loss for the ùëó -th scenario is:  where ùúÜ is a manually set coefficient.",
  "4 EXPERIMENTS": "",
  "4.1 Experimental Settings": "4.1.1 Datasets. Weconduct experiments on two public datasets: (1) AliExpress Dataset 1 . It is collected from a real-world search system in AliExpress, we use four scenarios: Netherlands (NL), Spain (ES), France (FR), and the United States (US). (2) Tenrec Benchmark 2 . Tenrec is a dataset suite for multiple recommendation tasks, collected from two different recommendation platforms of Tencent, QQ BOW (QB) and QQ KAN (QK). Items in QK/QB can be news articles or videos. We use data from two scenarios, QK-video and QB-video. Table 1 describes the statistics of the datasets. 1 https://tianchi.aliyun.com/dataset/74690 2 https://github.com/yuangh-x/2022-NIPS-Tenrec 4.1.2 Baseline Methods. We use two different groups of baseline methods, the first group being SOTA multi-scenario and multi-task recommendation methods: Single-task Model . It employs an MLP to predict the output for a single task. Different tasks are optimized separately in the single-task model. MMoE [25], PLE [38], and ESMM [38] are representative multi-task recommendation baselines. AITM [45] models task dependencies through an attention mechanism. Note that the two prediction tasks in Tenrec (click and like) are unrelated, we did not implement AITM on the Tenrec dataset. STAR [34] is a multi-domain model that includes both center parameters and domain-specific parameters. AESM 2 [52] is an advanced multi-scenario and multi-task recommendation model with automatic expert selection. PEPNet [5] is the SOTA method for multi-scenario multi-task recommendation. The second group consists of SOTA federated learning methods. FedAvg [29] averages the parameters of all clients in federated learning and then shares them back to each client. FedProx [20] tackles heterogeneity in federated learning, and can be seen as an extension of FedAvg. Ditto [19] emphasizes fairness and robustness in personalized federated learning. FedAMP [13] improves collaboration among clients with non-i.i.d. data distributions through attentive message passing. 4.1.3 Evaluation Metric. For the AliExpress dataset, all methods perform two tasks: predicting click-through rate (CTR) and postview click-through and conversion rate (CTCVR). For the Tenrec dataset, the tasks involve predicting user clicks and likes on exposed videos or articles. We adopt the widely used Area Under Curve (AUC) as the evaluation metric in our experiments:  where ùê∑ + ùë°ùëíùë†ùë° and ùê∑ -ùë°ùëíùë†ùë° represent the collections of positive and negative samples in the test set, respectively. ùëì (¬∑) denotes the prediction function, and ùêº (¬∑) denotes the indicator function. 4.1.4 Implementation Details. Weuse the same experimental setup for all methods, including the same embedding layers, input features, and training hyper-parameters. We employ a three-layer MLP with ReLU activation as the expert network, with hidden layer sizes of {512, 256, 128}, and we use a three-layer MLP with sigmoid activation as the tower network, with hidden layer sizes of {128, 64, 32}. We train all the methods with the binary cross-entropy loss. All methods are optimized using the Adam Optimizer [15]. The learning rate is set to 0 . 001 and the dropout rate is set to 0 . 2. For our proposed PF-MSMTrec, the conflict-coordinate hyper-parameter ùëê is set to 0.4 and the coefficient ùúÜ in the loss function is set to 0.5. Towards Personalized Federated Multi-Scenario Multi-Task Recommendation Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Table 2: Performance comparison of multi-scenario and multi-task methods on the AliExpress dataset. Bold text indicates the overall best performance, while underlined text denotes the best performance among baseline methods. * indicates the improvements over the best baseline are statistically significant (i.e., one-sample t-test with p < 0.05). Table 3: Performance comparison with federated learning methods on the Tenrec dataset. Bold text highlights the overall best performance, while underlined text indicates the best among baseline methods. * indicates the improvements over the best baseline are statistically significant (i.e., one-sample t-test with p < 0.05). Table 4: Performance comparison of multi-scenario and multi-task methods on the Tenrec dataset. Bold text highlights the overall best performance, while underlined text denotes the best performance among baseline methods. * indicates the improvements over the best baseline are statistically significant (i.e., one-sample t-test with p < 0.05). Table 5: Performance comparison of federated learning methods on the Tenrec dataset. Bold text indicates the overall best performance, while underlined text denotes the best performance among baseline methods. * indicates the improvements over the best baseline are statistically significant (i.e., one-sample t-test with p < 0.05). In the first group of multi-scenario and multi-task recommendation baseline models, our implementation is as follows: The singletask model is implemented with two separate expert networks and two separate tower networks. For MMoE and PLE, we implement them with four experts, two gate networks, and two towers. MMoE shares all four experts, while PLE shares two of them. For ESMM, we multiply the outputs of the two towers to obtain the CTCVR result. For AITM, we implement the AIT module with three MLPs serving as the query, key, and value of the attention mechanism. Task dependencies are passed through these AIT modules. For STAR, the basic FCN is implemented with center parameters and task-specific parameters. During prediction, the center weight is multiplied by the task-specific weight to generate the output. Additionally, we Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Yue Ding, Yanbiao Ji, Xun Cai, Xin Xin, Yuxiang Lu, Suizhi Huang, Chang Liu, Xiaofeng Gao, Tsuyoshi Murata, and Hongtao Lu Table 6: Ablation study on the AliExpress dataset. Bold text indicates the best performance. Table 7: Impact of the number of experts on the AliExpress dataset. Bold text indicates the best performance. Table 8: Ablation study on the Tenrec dataset. Bold text indicates the best performance. Table 9: Impact of the number of experts on the Tenrec dataset. Bold text indicates the best performance. apply a three-layer MLP as an auxiliary network after shared embedding layers to provide auxiliary information. For AESM 2 , we employ four experts and select experts using the Kullback-Leibler (KL) divergence. For PEPNet, we replace the gate network with one linear layer and softmax activation by one scale factor and two neural layers with sigmoid and ReLU activation. Additionally, we imitate EPNet by applying an extra element-wise attention network to learn the importance of dimensions in the input embedding. In the second group of federated learning baseline models, to ensure a fair comparison, we varied only the federated parameter aggregation method when comparing our approach with the baseline methods, while maintaining the local expert network model structure unchanged. For FedProx, the proximal term constant ùúá is set to 0.01. For Ditto, the coefficient ùúÜ that controls the interpolation between the local and global model is set to 0.1. For FedAMP, the hyper-parameter ùõº ùëò is set to 1.0. 4.1.5 Overall Performance. Table 2 and Table 3 compare the performance of our method with two groups of baseline models on the AliExpress dataset. Table 4 and Table 5 compare their performance on the Tenrec dataset. We have the following observations: (1) Our proposed PF-MSMTrec is evaluated in both federated and non-federated (local) settings. In the non-federated setting, the federated learning module is not required, and all expert networks and tower networks jointly perform predictions. Notably, our proposed method under federated settings even outperforms SOTA multiscenario multi-task methods under non-federated settings. This indicates that our approach effectively mitigates multiple optimization conflicts and supports joint learning across multiple clients while preserving data privacy. (2) Our method also surpasses SOTA federated learning approaches, demonstrating that our designed federated learning paradigm excels in personalized federated parameter aggregation.",
  "4.2 In-depth Analysis": "4.2.1 Ablation study. We conduct four sets of experiments to investigate the effects of different modules in our model: ( A1 ) Change the aggregation method for both expert and tower networks to FedAvg. ( A2 ) Apply federated averaging to all parameters of the expert network, while keeping the tower network unchanged. ( A3 ) Apply federated averaging only to the scenario-specific parameters of the expert network, while keeping the tower network unchanged. ( A4 ) Apply federated averaging to the tower network, while keeping the expert network unchanged. Table 6 and Table 8 describe the results. It can be concluded that expert parameter decoupling and conflict coordination in personalized aggregation are crucial for achieving optimal performance. In contrast, the aggregation method for the tower network has a relatively minor impact. Towards Personalized Federated Multi-Scenario Multi-Task Recommendation Conference acronym 'XX, June 03-05, 2018, Woodstock, NY (a) Ours on AliExpress. 0.10 0.15 0.20 NL FR US ES 2 4 6 8 10 12 14 Communication Round 0.004 0.006 0.008 Loss CTCVR CTR 0.20 0.25 0.30 0.35 QK-video QB-article 2 4 6 8 10 12 Communication Round 0.004 0.006 0.008 0.010 Loss Like Click (b) FedAVG on AliExpress. 0.15 0.20 NL FR US ES 2 4 6 8 10 12 14 Communication Round 0.004 0.006 0.008 Loss &7&95 &75 Loss (c) Ours on Tenrec. 0.20 0.25 0.30 QK-video QB-article 2 4 6 8 10 12 Communication Round 0.006 0.008 0.010 Like Click (d) FedAVG on Tenrec. Figure 3: Convergence on two datasets. 4.2.2 The impact of the number of experts. Wetest the performance with varying numbers of experts per client, ranging from 2 to 6. Table 7 and Table 9 describe the results. The optimal number of experts is 4, it shows that having too few experts hinders feature extraction, while an excessive number can result in conflicts. 4.2.3 Convergence study. We compare our method with FedAvg as a baseline and analyze convergence trends on the test dataset. As shown in Figure 3, our model demonstrates similar convergence behavior to FedAvg and effectively learns multiple tasks across various scenarios on the setting of federated learning.",
  "5 CONCLUSION": "In this paper, we explore a new and challenging problem: federated multi-scenario multi-task recommendation. We propose a novel framework called PF-MSMTrec. Our model incorporates parameter decoupling, federated batch normalization, conflict coordination, and personalized aggregation modules. Our proposed method effectively mitigates the multiple optimization conflict issues that arise in such complex application settings. Extensive experiments demonstrate that our proposed model outperforms SOTA methods. We believe that our proposed method broadens the applicability of recommender systems by tackling sophisticated business settings with a federated learning approach.",
  "REFERENCES": "[1] Nimesh Agrawal, Anuj Kumar Sirohi, Sandeep Kumar, et al. 2024. No Prejudice! Fair Federated Graph Neural Networks for Personalized Recommendation. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 38. 10775-10783. [2] MuhammadAmmad-Ud-Din,ElenaIvannikova, Suleiman A Khan, Were Oyomno, Qiang Fu, Kuan Eeik Tan, and Adrian Flanagan. 2019. Federated collaborative filtering for privacy-preserving personalized recommendation system. arXiv preprint arXiv:1901.09888 (2019). [3] Ting Bai, Yudong Xiao, Bin Wu, Guojun Yang, Hongyong Yu, and Jian-Yun Nie. 2022. A Contrastive Sharing Model for Multi-Task Recommendation. In Proceedings of the ACM Web Conference 2022 . 3239-3247. [4] Ruisi Cai, Xiaohan Chen, Shiwei Liu, Jayanth Srinivasa, Myungjin Lee, Ramana Kompella, and Zhangyang Wang. 2023. Many-Task Federated Learning: A New Problem Setting and a Simple Baseline. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 5036-5044. [5] Jianxin Chang, Chenbin Zhang, Yiqun Hui, Dewei Leng, Yanan Niu, Yang Song, and Kun Gai. 2023. Pepnet: Parameter and embedding personalized network for infusing with personalized prior information. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 3795-3804. [6] Yiqiang Chen, Teng Zhang, Xinlong Jiang, Qian Chen, Chenlong Gao, and Wuliang Huang. 2023. Fedbone: Towards large-scale federated multi-task learning. arXiv preprint arXiv:2306.17465 (2023). [7] Jialiang Han, Yun Ma, Qiaozhu Mei, and Xuanzhe Liu. 2021. Deeprec: On-device deep learning for privacy-preserving sequential recommendation in mobile commerce. In Proceedings of the Web Conference 2021 . 900-911. [8] Xiaobo Hao, Yudan Liu, Ruobing Xie, Kaikai Ge, Linyao Tang, Xu Zhang, and Leyu Lin. 2021. Adversarial feature translation for multi-domain recommendation. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining . 2964-2973. [9] Chaoyang He, Emir Ceyani, Keshav Balasubramanian, Murali Annavaram, and Salman Avestimehr. 2022. Spreadgnn: Decentralized multi-task federated learning for graph neural networks on molecular data. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 36. 6865-6873. [10] Yun He, Xue Feng, Cheng Cheng, Geng Ji, Yunsong Guo, and James Caverlee. 2022. Metabalance: improving multi-task recommendations via adapting gradient magnitudes of auxiliary tasks. In Proceedings of the ACM Web Conference 2022 . 2205-2215. [11] Zhaoxin Huan, Ang Li, Xiaolu Zhang, Xu Min, Jieyu Yang, Yong He, and Jun Zhou. 2023. SAMD: An Industrial Framework for Heterogeneous Multi-Scenario Recommendation. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 4175-4184. [12] Mingkai Huang, Hao Li, Bing Bai, Chang Wang, Kun Bai, and Fei Wang. 2020. A federated multi-view deep learning framework for privacy-preserving recommendations. arXiv preprint arXiv:2008.10808 (2020). [13] Yutao Huang, Lingyang Chu, Zirui Zhou, Lanjun Wang, Jiangchuan Liu, Jian Pei, and Yong Zhang. 2021. Personalized cross-silo federated learning on non-iid data, Vol. 35. 7865-7873. [14] Yuchen Jiang, Qi Li, Han Zhu, Jinbei Yu, Jin Li, Ziru Xu, Huihui Dong, and Bo Zheng. 2022. Adaptive domain interest network for multi-domain recommendation. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management . 3212-3221. [15] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In ICLR 2015 . [16] Jakub Koneƒçn` y, H Brendan McMahan, Felix X Yu, Peter Richt√°rik, Ananda Theertha Suresh, and Dave Bacon. 2016. Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492 (2016). [17] Zerong Lan, Yingyi Zhang, and Xianneng Li. 2023. M3REC: A Meta-based Multiscenario Multi-task Recommendation Framework. In Proceedings of the 17th ACM Conference on Recommender Systems . 771-776. [18] Danwei Li, Zhengyu Zhang, Siyang Yuan, Mingze Gao, Weilin Zhang, Chaofei Yang, Xi Liu, and Jiyan Yang. 2023. AdaTT: Adaptive Task-to-Task Fusion Network for Multitask Learning in Recommendations. arXiv preprint arXiv:2304.04959 (2023). [19] Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. 2021. Ditto: Fair and robust federated learning through personalization. In ICML . 6357-6368. [20] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. 2020. Federated Optimization in Heterogeneous Networks. In MLSys . [21] Xiaopeng Li, Fan Yan, Xiangyu Zhao, Yichao Wang, Bo Chen, Huifeng Guo, and Ruiming Tang. 2023. Hamur: Hyper adapter for multi-domain recommendation. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management . 1268-1277. [22] Guanyu Lin, Feng Liang, Weike Pan, and Zhong Ming. 2020. Fedrec: Federated recommendation with explicit feedback. IEEE Intelligent Systems 36, 5 (2020), 21-30. [23] Bo Liu, Xingchao Liu, Xiaojie Jin, Peter Stone, and Qiang Liu. 2021. Conflictaverse gradient descent for multi-task learning. Advances in Neural Information Processing Systems 34 (2021), 18878-18890. [24] Sichun Luo, Yuanzhang Xiao, and Linqi Song. 2022. Personalized federated recommendation via joint representation learning, user clustering, and model adaptation. In Proceedings of the 31st ACM international conference on information & knowledge management . 4289-4293. [25] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. 2018. Modeling task relationships in multi-task learning with multi-gate mixture-ofexperts. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1930-1939. [26] Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu, and Kun Gai. 2018. Entire space multi-task model: An effective approach for estimating post-click conversion rate. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval . 1137-1140. [27] Kiwan Maeng, Haiyu Lu, Luca Melis, John Nguyen, Mike Rabbat, and Carole-Jean Wu. 2022. Towards fair federated recommendation learning: Characterizing the inter-dependence of system and data heterogeneity. In Proceedings of the 16th ACM Conference on Recommender Systems . 156-167. [28] Othmane Marfoq, Giovanni Neglia, Aur√©lien Bellet, Laetitia Kameni, and Richard Vidal. 2021. Federated multi-task learning under a mixture of distributions. Advances in Neural Information Processing Systems 34 (2021), 15434-15447. Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Yue Ding, Yanbiao Ji, Xun Cai, Xin Xin, Yuxiang Lu, Suizhi Huang, Chang Liu, Xiaofeng Gao, Tsuyoshi Murata, and Hongtao Lu [29] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. 2017. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics . PMLR, 1273-1282. intelligence 44, 7 (2021), 3614-3633. [30] Jed Mills, Jia Hu, and Geyong Min. 2021. Multi-task federated learning for personalised deep neural networks in edge computing. IEEE Transactions on Parallel and Distributed Systems 33, 3 (2021), 630-641. [31] Khalil Muhammad, Qinqin Wang, Diarmuid O'Reilly-Morgan, Elias Tragos, Barry Smyth, Neil Hurley, James Geraci, and Aonghus Lawlor. 2020. Fedfast: Going beyond average for faster training of federated recommender systems. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 1234-1242. [32] Wentao Ning, Xiao Yan, Weiwen Liu, Reynold Cheng, Rui Zhang, and Bo Tang. 2023. Multi-domain Recommendation with Embedding Disentangling and Domain Alignment. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management . 1917-1927. [33] Liang Qu, Ningzhi Tang, Ruiqi Zheng, Quoc Viet Hung Nguyen, Zi Huang, Yuhui Shi, and Hongzhi Yin. 2023. Semi-decentralized federated ego graph learning for recommendation. In Proceedings of the ACM Web Conference 2023 . 339-348. [34] Xiang-Rong Sheng, Liqin Zhao, Guorui Zhou, Xinyao Ding, Binding Dai, Qiang Luo, Siran Yang, Jingshan Lv, Chi Zhang, Hongbo Deng, et al. 2021. One model to serve all: Star topology adaptive recommender for multi-domain ctr prediction. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management . 4104-4113. [35] Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S Talwalkar. 2017. Federated multi-task learning. Advances in neural information processing systems 30 (2017). [36] Alysa Ziying Tan, Han Yu, Lizhen Cui, and Qiang Yang. 2022. Towards personalized federated learning. IEEE Transactions on Neural Networks and Learning Systems (2022). [37] Ben Tan, Bo Liu, Vincent Zheng, and Qiang Yang. 2020. A federated recommender system for online services. In Proceedings of the 14th ACM Conference on Recommender Systems . 579-581. [38] Hongyan Tang, Junning Liu, Ming Zhao, and Xudong Gong. 2020. Progressive layered extraction (ple): A novel multi-task learning (mtl) model for personalized recommendations. In Proceedings of the 14th ACM Conference on Recommender Systems . 269-278. [39] Yu Tian, Bofang Li, Si Chen, Xubin Li, Hongbo Deng, Jian Xu, Bo Zheng, Qian Wang, and Chenliang Li. 2023. Multi-Scenario Ranking with Adaptive Feature Learning. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval . 517-526. [40] Simon Vandenhende, Stamatios Georgoulis, Wouter Van Gansbeke, Marc Proesmans, Dengxin Dai, and Luc Van Gool. 2021. Multi-task learning for dense prediction tasks: A survey. IEEE transactions on pattern analysis and machine Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Towards Personalized Federated Multi-Scenario Multi-Task Recommendation",
  "A THE PROOF IN SECTION 3.3.1": "To proof that the aggregated parameter ¬Ø W ùëõ ùëó is only related to the normalization parameter ùõΩ ùëî if federated communication occurs after the completion of each local batch, we substitute Eq. (8) into Eq. (10), we can derive:  Acommunication round typically occurs after one or more local training epochs, with each epoch comprising multiple batches. Consequently, the ùõΩ value at the communication round differs from that at an individual batch. Received 20 February 2007; revised 12 March 2009; accepted 5 June 2009",
  "keywords_parsed": [
    "Multi-task Recommendation",
    " Federated Learning",
    " Collaborative Filtering"
  ]
}