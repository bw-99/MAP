{"Multi-Modality Co-Learning for Efficient Skeleton-based Action Recognition": "", "Jinfu Liu": "", "Chen Chen": "", "Mengyuan Liu \u2217": "liujf69@gmail.com State Key Laboratory of General Artificial Intelligence, Peking University, Shenzhen Graduate School Shenzhen, China chen.chen@crcv.ucf.edu Center for Research in Computer Vision, University of Central Florida Orlando, USA", "ABSTRACT": "nkliuyifang@gmail.com State Key Laboratory of General Artificial Intelligence, Peking University, Shenzhen Graduate School Shenzhen, China", "KEYWORDS": "Skeleton-based action recognition has garnered significant attention due to the utilization of concise and resilient skeletons. Nevertheless, the absence of detailed body information in skeletons restricts performance, while other multimodal methods require substantial inference resources and are inefficient when using multimodal data during both training and inference stages. To address this and fully harness the complementary multimodal features, we propose a novel multi-modality co-learning (MMCL) framework by leveraging the multimodal large language models (LLMs) as auxiliary networks for efficient skeleton-based action recognition, which engages in multi-modality co-learning during the training stage and keeps efficiency by employing only concise skeletons in inference. Our MMCL framework primarily consists of two modules. First, the Feature Alignment Module (FAM) extracts rich RGB features from video frames and aligns them with global skeleton features via contrastive learning. Second, the Feature Refinement Module (FRM) uses RGB images with temporal information and text instruction to generate instructive features based on the powerful generalization of multimodal LLMs. These instructive text features will further refine the classification scores and the refined scores will enhance the model's robustness and generalization in a manner similar to soft labels. Extensive experiments on NTU RGB+D, NTU RGB+D 120 and Northwestern-UCLA benchmarks consistently verify the effectiveness of our MMCL, which outperforms the existing skeleton-based action recognition methods. Meanwhile, experiments on UTD-MHAD and SYSU-Action datasets demonstrate the commendable generalization of our MMCL in zero-shot and domain-adaptive action recognition. Our code is publicly available at: https://github.com/liujf69/MMCL-Action.", "CCS CONCEPTS": "\u00b7 Computing methodologies \u2192 Activity recognition and understanding . \u2217 Corresponding author is Mengyuan Liu (e-mail: nkliuyifang@gmail.com). Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. MM'24, October 28-November 1, 2024, Melbourne, VIC, Australia \u00a9 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0686-8/24/10 https://doi.org/10.1145/3664647.3681015 Action Recognition, Multi-modality, Multimodal LLMs", "ACMReference Format:": "Jinfu Liu, Chen Chen, and Mengyuan Liu. 2024. Multi-Modality Co-Learning for Efficient Skeleton-based Action Recognition. In Proceedings of the 32nd ACMInternational Conference on Multimedia (MM '24), October 28-November 1, 2024, Melbourne, VIC, Australia. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3664647.3681015", "1 INTRODUCTION": "Human action recognition is an important task in video understanding [11, 31, 32, 40, 42, 58]. The human action conveys central information like body tendencies and thus helps to understand the person in videos. In pursuit of precise action recognition, diverse video modalities have been explored, such as skeleton sequences [6, 20], RGB images [14, 67], text descriptions [54, 62], depth images [60] and human parsing [12, 36]. In particular, the graph-structured skeleton can well represent body movements and is highly robust to environmental changes, thereby adopted in many studies using graph convolutional networks (GCNs) [7-9, 13, 33, 39, 55], it also demonstrates strong scalability and real-time capabilities when deployed on edge devices. However, these skeleton-based methods [20, 44, 59, 71] (Fig. 1 (a)) only using skeletons during training and inference stages will restrict recognition performance due to the inherent defects of skeletal modality. For instance, the skeleton modality lacks the ability to depict detailed body information (e.g. appearance and objects) and has difficulty in fine-grained recognition when dealing with similar actions. A good approach to addressing the aforementioned skeletonbased limitations is to introduce complementary multi-modality for action recognition. The typical process [1, 10, 14, 24, 67] involves employing two networks to separately model the skeletons and other modalities (e.g. RGB images and depth images), then some ensemble techniques are applied to merge the predictions from multiple modalities. These multimodal-based methods (Fig. 1 (b)) can make up for the defects of a single modality and provide more comprehensive information for fine-grained action classification by leveraging the complementary nature of multimodal data. Nevertheless, they suffer from the drawbacks of requiring significant inference resources and appear less efficient when deployed on edge devices due to the use of multi-modality in both training and inference stages(Fig. 1 (b)). The aforementioned skeleton-based and multimodal-based methods naturally lead to a question: How to better leverage the complementary nature of multi-modality while retaining the efficient inference with single-skeleton modality? MM'24, October 28-November 1, 2024, Melbourne, VIC, Australia Jinfu Liu, Chen Chen, & Mengyuan Liu. Figure 1: Existing methods suffer from inherent defects in single-modality and issues of inefficient inference. (a) Most skeleton-based methods only use skeleton/pose modality during training and inference stages, encountering issues associated with skeletal inherent defects. Note that the human pose can be divided into different skeleton modalities (e.g. joint and bone). (b) Most multimodal-based methods use multi-modality during the training and inference stages, which require significant inference resources and are inefficient. (c) Our multi-modality co-learning (MMCL) framework incorporates multimodal features to enhance the modeling of skeletons in the training stage and maintains efficiency in the inference stage by only using concise skeletons. \u2026 Joint Bone GCN1 Ensemble Label \u2026 Pose GCN1 CNN Ensemble Label RGB (a) (b) RGB Pose Auxiliary Modules Text Train / Inference Train / Inference Multi-Modality Co-Learning Efficient Inference GCN1 Prediction Prediction (c) Train with Multi-Modality Only Pose Multi-Modality Only Pose Label Robust Features Joint Bone Pose Joint Bone \u2026 Pose \u2026 Joint Bone \u2026 GCN2 GCN2 GCN1 GCN2 GCN2 Motivated by the above question, we propose a multi-modality co-learning (MMCL) framework for efficient skeleton-based action recognition, which enhances the model's performance and generalization via multi-modality co-learning, while only using the concise skeleton in the inference stage to preserve the efficiency (Fig. 1 (c)). Due to the effectiveness of multi-modality co-learning during the training stage, our proposed MMCL acquires enhanced robustness and generalization when applied to optimize mainstream GCN backbones, exhibiting improved performance in overall accuracy and specific actions. In our MMCL framework, we incorporate the multi-modality co-learning into action recognition and provide instructive multimodal features based on the multimodal LLMs [15, 28, 29, 72] for skeletons. The proposed MMCL is shown in Fig. 2, which will use skeleton, RGB and text modalities during the training stage, while only using the skeleton in inference to achieve efficiency. Given that RGB features [10, 67] effectively compensate for the skeletons, while existing multimodal LLMs exhibit strong generalization on real RGB images and achieve success in various visual tasks [16, 69, 73], we adopt the RGB images as the input for the multimodal LLMs to help better model the skeletons. To mitigate the limitation of some existing multimodal LLMs (e.g. BLIP [29], BLIP2 [28], MiniGPT-4 [72] and LLaMA AdapterV2 [15]) that cannot directly process video streams, we combine frames from the video stream into RGB images that include temporal information. Besides, to better harness the potent modeling capabilities of these multimodal LLMs for intrinsic features in images and text, we input the RGB images with text instructions into the Feature Refinement Module (FRM) for generating instructive text features conducive to action recognition. These instructive text features from multimodal LLMs will further refine the classification scores output by the fully connected layer and enhance the model's robustness and generalization in a manner similar to soft labels. Meanwhile, to better utilize the introduced RGB images with temporal information in LLMs, our MMCL uses a deep neural network to obtain RGB features in the Feature Alignment Module (FAM). Likewise, inspired by CLIP [47], our MMCL uses the contrastive learning [62, 70] to better assist in modeling skeletons. Our contributions are summarized as follows: \u00b7 We propose a novel multi-modality co-learning (MMCL) framework for efficient skeleton-based action recognition, which empowers mainstream GCN models to produce more robust and generalized feature representations by introducing multi-modality co-learning during the training stage, while maintain efficiency by only using concise skeletons in inference. \u00b7 Our proposed MMCL framework is the first to introduce multimodal LLMs for multi-modality co-learning in skeleton-based action recognition. Meanwhile, our MMCL is orthogonal to the backbones and thus can be applied to optimize mainstream GCN models by using different multimodal LLMs. Due to the generalization of multimodal LLMs, our MMCL can be transferred to domain-adaptive and zero-shot action recognition. \u00b7 Extensive experiments on three popular benchmarks namely NTURGB+D,NTURGB+D120andNorthwestern-UCLAdatasets verify the effect of our MMCL framework by outperforming existing skeleton-based methods. Meanwhile, experiments on SYSU-ACTION and UTD-MHAD datasets from different domains indicate that our MMCL exhibits commendable generalization in both domain adaptive and zero-shot action recognition.", "2 RELATED WORK": "", "2.1 Skeleton-based Action Recognition": "Deep learning-based methods[41, 63, 65] have achieved high success using CNNs [22, 23, 27, 35, 61] and RNNs [2, 26, 30, 38, 45, 49, 52, 68] in skeleton-based action recognition. To address the adverse effects due to view variations and noisy data, Liu et al. [41] visualized skeleton sequences as color images and fed them into a multi-stream CNN for deep feature extraction. Wang et al. [52] proposed a two-stream RNN to model the temporal dynamics and spatial structure of skeleton sequences, which breaks through the limitations of RNN in processing raw skeleton data. GCNs can effectively process structured data, thereby being adopted by many researchers in skeleton-based action recognition. For instance, Yan et al. [65] is the first to use GCNs for skeleton-based action recognition by proposing the ST-GCN to learn spatiotemporal features from skeleton data. Chen et al. [6] improve the design of GCNs by proposing a channel-wise topology refinement graph convolutional network (CTR-GCN), which effectively aggregates joint features in each channel. The methods mentioned above are limited by the drawbacks of only using skeleton modality.", "2.2 Multimodal-based Action Recognition": "Benefiting from the emergence of various multimodal datasets and improvements in computing resources, research about multimodalbased action recognition [1, 10, 14, 24, 67] has become popular. To Multi-Modality Co-Learning for Efficient Skeleton-based Action Recognition MM'24, October 28-November 1, 2024, Melbourne, VIC, Australia Figure 2: Framework of our proposed Multi-Modality Co-Learning (MMCL), which integrates multimodal features during the training stage and keeps efficiency in inference by only using concise skeletons. The Feature Alignment Module (FAM) extracts and aligns high-level RGB features to facilitate contrastive learning with global skeleton features. Here, we only align the RGB features with the skeleton features as the text features generated by LLMs have relatively limited information compared to the RGB features. The Feature Refinement Module (FRM) provides instructive text features to refine the classification scores based on the multimodal LLMs. Here we guide the LLMs to generate instructive features based on the defects that the skeleton cannot recognize objects. The text instructions can be modified based on the skeletal defects. Action Label FC Contrastive Loss Classification Loss Inference Stage Train Stage Kinect Sensor Person Detection Skeleton Text Prompt RGB FAM FRM Refinement Loss Multi-Modality Co-Learning \uf053\uf04d \uf053\uf052 Pooling Cosine Similarity Contrastive Loss CrossEntropy Refinement Loss \uf053\uf052 \uf059 \uf059 \uf046\uf047 \uf046\uf047 \uf046\uf043 \uf046\uf043 GCN GCN Backbone \uf053\uf04d Symbols \uf046\uf047\uf020 \u2192 Skeleton Feature \uf046\uf043\uf020 \u2192 RGB Feature \uf053\uf04d\uf020 \u2192 Classification Score \uf053\uf052\uf020 \u2192 Refinement Score \uf059\uf020 \u2192 Action Label I \uf020 \u2192 RGB Image Q \uf020 \u2192 Text Instruction \uf046 text \uf020 \u2192 Text Feature \uf04d\uf020 \u2192 Learnable Matrix I Q \uf04d \uf046 text address GCNs are subject to limitations in robustness, interoperability and scalability, Duan et al. [14] proposed a novel PoseConv3D to use both RGB heatmap and skeleton modalities for robust human action recognition. Das et al. [10] proposed a video-pose network (VPN) to project the 3D poses and RGB cues in a common semantic space, which enabled the action recognition framework to learn better spatiotemporal features exploiting both modalities. Commonly used modalities for multimodal-based action recognition include skeletons, color images, depth maps, text and point clouds. Methods [10, 12, 14, 21, 46, 60, 67] of integrating these multiple modalities have been shown to achieve better performance by leveraging multimodal features. Unlike the above methods that use multimodal data in both training and inference stages, our MMCL introduces multimodal data for multi-modality co-learning in the training stage and only uses the concise skeleton to keep efficient in inference. LLMs for multi-modality co-learning and sets the text instructions according to the skeletal defects. Besides, these methods [54, 62, 64] that explicitly generate text features based on action labels are not advantageous for unknown actions. With the emergence of multimodal LLMs such as BLIP [29], BLIP2 [28], MiniGPT-4 [72] and LLaMA AdapterV2 [15], receiving multimodal input can often generate more robust and richer text information. Inspired by this, our MMCL framework inputs RGB and text modality into multimodal LLMs and obtains the output text features for refining the classification score of the skeleton. These refined scores will enhance the model's robustness and generalization in a manner similar to soft labels. Benefiting from the powerful generalization capabilities of multimodal LLMs, the soft labels obtained by MMCL can be transferred to domain-adaptive action recognition.", "3 METHODOLOGY": "In this section, we present our proposed Multi-Modality Co-Learning (MMCL) framework in detail. MMCL aims to enhance skeleton representation learning with complementary and instructive multimodal features based on the powerful generalization capabilities of multimodal LLMs. Meanwhile, MMCL is orthogonal to the backbone networks and thus can be coupled with various GCN backbones and multimodal large language models. Below we will introduce the proposed MMCL in five parts.", "3.1 GCN Backbone": "Duetotheunique advantages of GCNs in modeling graph-structured data, the GCN is prevailing for skeleton-based action recognition.", "2.3 Large Language Model Auxiliary Learning": "The powerful generation ability of large language models (LLMs) enables them to be transferred to different tasks effectively. Prompt learning and instruction learning are capable techniques of adapting different pre-trained LLMs to different tasks, thereby being applied in action recognition as an auxiliary strategy by many researchers. Wang et al. [54] proposed an ActionCLIP to directly uses class labels as input text to construct cues for video action recognition. Xiang et al. [62] used LLMs based on action labels to provide prior knowledge for skeleton-based action recognition. Different from the above methods that use simple text modalities based on action labels and unimodal LLMs, our MMCL is based on advanced multimodal MM'24, October 28-November 1, 2024, Melbourne, VIC, Australia Jinfu Liu, Chen Chen, & Mengyuan Liu. Figure 3: (a) Extract RGB images from video and use CNN to model RGB features. (b) Display of content generated by different multimodal LLMs. The text instructions used by MMCL are set based on skeletal defects (e.g. lack of object information and appearance details), guiding the LLMs to generate features complementary to the skeleton from RGB images. In implementation, we just use the BLIP [29] to generate text features for training. Detection Concatenation Feature Extraction Video Person Frame RGB Image CNN RGB Feature Whether the {target} are {action}? RGB Image Text Instruction BLIP/BLIP2 Multimodal LLMs MiniGPT4 LLaMA Adapter V2 Yes . The person is holding a cup in one hand and a fork in the other. They are wearing black pants and a black shirt\u2026 The movement in the image shows the woman holding a cup of coffee in her left hand and drinking from it\u2026 Please describe the {target} . A woman standing in front of a white wall. The image shows a room with a woman standing in the middle of the room . The woman is wearing a green shirt and black pants. She is standing in front of a white wall ... The person is a woman who is wearing a black shirt and pants. She is holding a white coffee mug in her hand. (a) (b) 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 {person, holding object} {hand, touching head} {hand, touching foot} {\u2026 \u2026} {person} {hand} {foot} {\u2026} or or In our MMCL framework, we also adopt GCN as the backbone network to model the skeleton modality and the GCN blocks are composed of a graph convolution layer and a temporal convolution layer. The normal graph convolution can be formulated as:  where \ud835\udc3b \ud835\udc59 is the joint features at layer l and \ud835\udf0e is the activation function. D \u2208 R \ud835\udc41 \u00d7 \ud835\udc41 is the degree matrix of N joints and W l is the learnable parameter of the l -th layer. A is the adjacency matrix representing joint connections. Generally, the A can be generated by using static and dynamic ways. The A is generated by using datadriven strategies in dynamic ways while it is defined manually in static ways. [6] introduces A into graph convolution in the form of S = ( s id , scp , s cf ) and s id , scp , s cf indicate the identity, centripetal and centrifugal edge subsets respectively. [25] proposes an HD-Graph to introduce A into graph convolution and uses a six-way ensemble strategy to train the GCN.", "3.2 Multimodal Data Processing": "In our MMCL framework, we use skeleton modality, RGB modality and text modality during the training stage, while only using the concise skeleton modality during the inference stage. The skeleton data is collected through Kinect sensors and it is a natural topological graph, which is denoted as G = { V , E } . V = { v 1 , v 2 , \u00b7 \u00b7 \u00b7 , v \ud835\udc41 } and E = { e 1 , e 2 , \u00b7 \u00b7 \u00b7 , e \ud835\udc3f } represent N joints and L bones. Our MMCL uses four different skeleton modalities, namely joint, bone, joint motion and bone motion defined in [39]. Since the RGB modality has rich human body information, we extract three-channel RGB images from videos to assist the learning of skeletons based on multimodal LLMs. In Fig. 3(a), considering that most existing LLMs are more proficient in handling individual image-text pairs, we combine video frames into an RGB image with temporal information. For example, we select m frames from a action video of drinking water and concatenate them from left to Figure 4: Our MMCL can effectively perform action recognition when faced with skeleton/pose inputs from different domains. Our MMCL employs skeleton interpolation to ensure that the number of skeleton points input to the model is consistent. LLMs FRM Interpolation Text Instruction MMCL Feature Zero-shot Pose w/ LLM w/o LLM Soft Label Prediction Classification Scores 20 Joints\u219225 Joints right in chronological order, thus forming an RGB image containing the temporal information of drinking water, which is show in figure 4(a). In detail, given an RGB video stream K = { k 1 , k 2 , \u00b7 \u00b7 \u00b7 , k \ud835\udc5b } with n frames, we extract the three-channel RGB image I by:  where \ud835\udf00 denotes a detector. Inspired by the top-down pose estimation, we use a person detector \ud835\udf00 to filter out most ambient noise and focus on the human body. Meanwhile, we resize the extracted person frame to save computing and storage resources by \u0393 . This detection and cropping method significantly filters environmental noise, while minimizing the loss of spatial information and has been widely applied in [36, 67]. Inspired by the uniform sampling strategy in [14], we uniformly sample m samples from n person frames to reduce the temporal dimension by U \ud835\udc5a \ud835\udc5b . \u2295 \ud835\udc5a = [ \ud835\udc53 1 \u2295 \ud835\udc53 2 \u2295\u00b7 \u00b7 \u00b7\u2295 \ud835\udc53 \ud835\udc5a -1 \u2295 \ud835\udc53 \ud835\udc5a ] means to concat m person frames along the temporal dimension to form the RGB image with temporal information.", "3.3 Multi-Modality Co-Learning": "3.3.1 Feature Alignment Module. In our MMCL framework, the Feature Alignment Module (FAM) extracts rich RGB features and aligns them with global skeleton features from GCN layers by:  Due to the CNNs have unique advantages in processing Euclidean data (e.g. RGB images), the FAM utilizes a deep convolutional neural network \u0398 to model high-level features from RGB image I . Before extracting RGB features, the FAM will resize and normalize the image to speed up the convergence of the model. In order to achieve alignment with the global skeleton features, the RGB features extracted by CNN will go through the MLP operation M \ud835\udc4e\ud835\udc59\ud835\udc56\ud835\udc54\ud835\udc5b . This alignment operation facilitates subsequent contrastive learning between two features. Contrastive learning is popular and widely used in deep learning, especially playing a key role in multimodal learning, which aims to learn the common features between similar instances and helps to distinguish similar human actions. Our MMCL conducts contrastive learning based on the framework [74] between the RGB features F \ud835\udc56 \ud835\udc50 output by FAM and the skeleton features F \ud835\udc56 \ud835\udc54 extracted by GCN Multi-Modality Co-Learning for Efficient Skeleton-based Action Recognition MM'24, October 28-November 1, 2024, Melbourne, VIC, Australia Table 1: Accuracy improvement of some action categories when MMCL assists GCN backbones. Table 2: Accuracy comparison with state-of-the-art methods. \u2713 means use multi-stream ensemble (e.g. joint+bone). backbone, while calculating the contrastive loss by:    where \ud835\udf03 is the cosine similarity and \ud835\udf0f is a temperature parameter. Note that the skeleton features extracted by the GCN backbone will first go through a MeanPooling layer to obtain the global features. Since the purpose of contrastive learning is to learn common features between similar instances, and the common features between RGB videos and skeletons of the same action are the motion dynamics of this action, so the FAM can promote the learning of motion dynamics from skeleton. 3.3.2 Feature Refinement Module. Since LLMs have powerful text understanding and generation capabilities, they have been used in auxiliary training for many vision tasks successfully. Multimodal LLMs that receive multimodal input are able to understand the content of multiple modalities, thereby generating more robust and comprehensive text features. In Fig. 3(b), we demonstrate the generative capabilities of different multimodal LLMs by using different RGB images and text instructions. In fact, the text instructions of our MMCL are established based on skeletal defects (e.g. the skeletal modality lacks object information) and can be readily modified as per actual requirements. The FRM of our MMCL is based on the multimodal LLMs to obtain instructive text features and refine the classification scores output by a fully connected layer. In detail, the FRM inputs text instruction and RGB image into the LLMs in the form of Visual Question Answering(VQA) in Eq. 7. Then the token features of the multimodal LLM decoder will be retained. It is worth emphasizing our MMCL is orthogonal to LLMs and can be transferred to different multimodal LLMs to obtain text features.  MM'24, October 28-November 1, 2024, Melbourne, VIC, Australia Jinfu Liu, Chen Chen, & Mengyuan Liu. Figure 5: Visualization of improved accuracy about difficult action samples when CTR-GCN used MMCL. The second column represents the prediction of models for the currently visualized sample and the third column represents the accuracy for all samples within the currently visualized category. We selected four difficult action samples that are prone to prediction errors in CTR-GCN, which all belong to categories involving objects or are highly relevant to hands and is difficult to distinguish objects from skeleton diagrams. Our MMCL set text instructions based on skeletal defects and generated instructive features through LLMs to guide the model to focus on the modeling of human hands and objects, thus leading to significant accuracy improvements in these difficult action samples. Method Prediction Acc. (%) w/o MMCL Play with phone 53.68 w/ MMCL Writing 61.40 Writing Cutting Paper Wield Knife Method Prediction Acc. (%) w/o MMCL Staple book 63.18 w/ MMCL Cutting paper 67.36 Method Prediction Acc. (%) w/o MMCL Shoot with gun 71.35 w/ MMCL Wield knife 75.52 Method Prediction Acc. (%) w/o MMCL Take a photo 70.78 w/ MMCL Shoot with gun 79.48 Shoot With Gun  Table 3: Comparison in the case of whether to use the MMCL. In terms of implementation, the FRM unifies text features obtained from different input samples into the same dimension n through \ud835\udf19 . Meanwhile, n learnable matrices M are multiplied with text features F \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 \u2208 R \ud835\udc5b in each feature channel i and optionally refine the classification scores S M . These refined scores S R in Eq. 8 will enhance the model's robustness and generalization in a manner similar to soft labels. Intuition suggests that introducing more detailed text instructions would result in broader and more comprehensive action descriptions, which may be more helpful to the action recognition task. However, inputting complex text instructions into the MiniGPT-4 and BLIP models tends to generate more irrelevant content, which can introduce noise features to some extent. Therefore, our MMCL adopts a brief and direct text instruction based on skeletal defects, aiming to demonstrate the effectiveness of this text prompt and our MMCL framework. In our FRM, we focus on modeling the objects that human holds since the skeleton cannot recognize objects and selectively enhance and refine the classification scores for samples with or without objects. In fact, this focus can be easily transferred to different situations and adapted to different LLMs and text instructions by adjusting the learnable matrices during the training stage. Our FRM effectively demonstrates how to transfer multimodal LLMs to action recognition in a novel and general way. In Eq. 9, the refined classification score S \ud835\udc45 and the true label Y of action samples will be used to calculate the refinement loss by:  where N is the number of samples in a batch and Y \ud835\udc56 is the one-hot presentation of the true label about action sample i . Actually, the FRM of our MMCL trains the network by refining the classification scores to generate pseudo-labels and use text features to supervise the model, which was also shown to be effective in [34].", "3.4 Loss Function": "In our proposed MMCL, we use Cross-Entropy (CE) loss as the classification loss. Our MMCL uses contrastive loss L \ud835\udc36 and refinement loss L \ud835\udc45 as the auxiliary loss to constrain the training of the whole network, thereby introducing multimodal features into skeletonbased action recognition. The complete training loss function is defined as:  where \ud835\udf06 1 and \ud835\udf06 2 represent two hyper-parameters and L \ud835\udc50\ud835\udc59\ud835\udc60 is the classification loss.", "3.5 Domain-Adaptive Action Recognition": "It is easily observed that the distribution of training data derived from public datasets differs from that of testing data collected in real world or other datasets, leading to a domain gap between the training (source) and testing (target) inputs. Considering this situation, our MMCL introduces multi-modality co-learning based on multimodal LLMs to enhance the robustness and generalization of the model. When dealing with skeletons from other domain datasets or real world, our MMCL conducts data interpolation to align with the model's input as shown in Fig. 4, enabling seamless action recognition. Due to the effectiveness of multi-modality co-learning during the training stage, our MMCL exhibits the ability to acquire more robust and deep skeletal feature representations. As a result, it maintains stable recognition performance when confronted with skeletal inputs from other domains. Meanwhile, owing to the strong generalization of multimodal LLMs, they can still produce high-quality textual features when confronted with RGB images Multi-Modality Co-Learning for Efficient Skeleton-based Action Recognition MM'24, October 28-November 1, 2024, Melbourne, VIC, Australia Table 4: Comparison of different CNNs in FAM. Table 5: Comparison of different LLMs in FRM. Table 6: Comparison of accuracy under different hyperparameter settings. Table 7: Comparison in parameters and computation cost when inferring a single action sample. from diverse domains and these textual features can also refine the classification scores through the fixed FRM.", "4 EXPERIMENTS": "Datasets : We conduct experiments on three well-established largescale human action recognition datasets: NTU-RGB+D (NTU 60), NTU-RGB+D 120 (NTU 120) and Northwestern-UCLA (NWUCLA). NTU-RGB+D [48] is a widely used 3D action recognition dataset containing 56,880 skeleton sequences and human action videos, which are categorized into 60 action classes and performed by 40 different performers. We follow the evaluation using two benchmarks provided in the original paper [48]. NTU-RGB+D 120 [37] is derived from the NTU-RGB+D dataset. A total of 114,480 video samples across 120 classes are performed by 106 volunteers and captured using three Kinect V2 cameras. We also follow the evaluation using two benchmarks as outlined in the original research [37]. Northwestern-UCLA [53] comprises 1494 video clips spanning across 10 distinct categories. Our evaluation process aligns Table 8: Comparison of using different adjacency matrices. with the protocol in [53]. To demonstrate the generalization of our MMCL in zero-shot and domain-adaptive action recognition, we also conduct experiments on the UTD-MHAD and SYSU-Action datasets. UTD-MHAD [4] consists of 27 different actions performed by 8 subjects. Each subject repeated the action for 4 times, resulting in 861 action sequences in total. SYSU-Action [19] comprises 12 distinct actions performed by 40 participants, culminating in a collection of 480 video clips. Due to the different collection methods and human joints, the UTD-MHAD and SYSU-Action datasets have domain gaps compared to the NTU-120 dataset. Implementation details : All experiments are conducted on two Tesla V100-PCIE-32GB GPUs. We use SGD to train our model for a total number of 110 epochs with batch size 200 and we use a warm-up strategy [17] to make the training procedure more stable. The initial learning rate is set to 0.1 and reduced by a factor of 10 at 90 and 100 epochs, the weight decay is set to 4e-4. We use CTR-GCN [6] as the GCN backbone and use the InceptionV3 [50] to extract deep RGB features in FAM. We use YoloV5 [51] as the person detector for video frames and uniformly sample five frames from person frames to form the RGB image with temporal information. We extract instructive text features from RGB images and text instruction based on the BLIP [29] in FRM. We introduce four skeleton modalities in [6] and the six-way ensemble in [25] to achieve multi-stream fusion. In concrete implementation, we utilize the HD-Graph [25] to replace the natural connectivity of human joints, thereby altering the initialization of symbol A in Eq. 1.", "4.1 Comparison with State-of-the-Art Methods": "Notably, our MMCL is the first to introduce multimodel features based on multimodel LLMs for multi-modality co-learning into skeleton-based action recognition, which achieves better performance by assisting previous GCN backbones and can be easily migrated to more advanced backbone networks. In Table 1, we demonstrate the accuracy improvement for some action categories when applying the proposed MMCL in different GCN backbones on the NTU120 X-Sub benchmark. We also compare our MMCL with the state-of-the-art methods on the NTU 60, NTU 120 and NWUCLAdatasets in Table 2. On three datasets, our model outperforms all existing methods under nearly all evaluation benchmarks. In Table 2, on the X-Sub benchmark of the NTU 120 dataset, our MMCL using the same ensemble outperforms the baseline CTRGCN [6], which shows that introducing multimodal features based on MMCL into the backbone will achieve better performance. On the X-Set benchmark of the NTU 120 dataset, compared with VPN [10] and TSMF [3] that use both the skeleton and RGB modalities at the inference stage, our MMCL performs better in both recognition accuracy and inference cost by only using the skeleton at the inference stage. Compared with the method [62] that use simple text modalities based on action labels and unimodal LLM, our MMCL achieved better recognition accuracy on three datasets. MM'24, October 28-November 1, 2024, Melbourne, VIC, Australia Jinfu Liu, Chen Chen, & Mengyuan Liu. Table 9: Comparison of different text instructions in the Feature Refinement Module on NTU 120 X-Sub benchmark.", "4.2 Ablation Studies": "4.2.1 Effectiveness in different GCN backbones. In this section, we conduct ablation experiments on the X-Sub benchmark of NTU 120 dataset by applying MMCL to the strong CTR-baseline (CTRS-GCN) and the CTR-GCN. In Table 3, we investigate the effectiveness of MMCLacross different skeleton modalities and GCN backbones. By introducing the MMCL into CTR-GCN, an accuracy improvement of 0.78% and 0.98% is achieved on the joint and bone modalities respectively. The results in Table 3 show that our MMCL leads to improvements across different skeleton modalities and backbones. Besides, our MMCL can help the backbones improve the recognition accuracy of hard actions as shown in Fig. 5. 4.2.2 Effectiveness of FAM and FRM. In Table 4, we demonstrate the effectiveness of the FAM and show its robustness by using different CNNs. When the FAM is applied to CTRS-GCN and CTR-GCN, the recognition performance is improved. We argue that the MMCL enhances the modeling of skeleton features by performing contrastive learning between the RGB features extracted by the FAM and the global skeleton features output by the GCNs. In Table 5, we also show the effectiveness of the FRM and its robustness by using different multimodel LLMs. When the FRM based on different LLMs is applied to the baseline network, the recognition performance is improved. An interesting observation here is that the performance of FRM based on MiniGPT-4 is worse than those based on Blip. We speculate that when faced with the same text instructions, MiniGPT4 tends to focus on describing a lot of clothing-related information and the background (e.g. the color of clothes), which inadvertently introduces unnecessary textual noise. 4.2.3 Efficiency and robustness of MMCL. The Table 6 present the accuracy of our MMCL under different hyperparameter settings. In Table 7, we also show the parameters and computation cost required by MMCL for inference when using a single action sample. In Table 8, we compared the recognition accuracy using different adjacency matrix on the NTU120 X-Sub benchmark. Here, replacing the naturally connected adjacency matrix with HD-Graph resulted in higher accuracy. In Table 9, we explore the recognition accuracy when different text instructions are employed based on skeletal modality defects. The results in Table 9 suggest that the FRM based on different text instructions improves the recognition performance of the baseline model. Simultaneously, they also demonstrate that our FRM can be transferred to different reasonable text instructions as needed and can generate instructive textual features through multimodal LLMs to aid in the modeling of the baseline model.", "4.3 Model Generalization": "In Table 10, we utilized a subset of UTD-MHAD dataset [4] and SYSU-ACTION dataset [19] to investigate the generalization of Table 10: Exploration about the generalization of MMCL and LLMs in different domains. The J represents the use of model weights that are trained on the joint modality. MMCL and multimodal LLMs. We utilize model weights trained on the joint modality of NTU 120 X-Sub benchmark for zero-shot action recognition and use interpolation to extend the joints to fit the models' input as shown in Fig. 4. In zero-shot recognition, all samples from two datasets have never been involved in training and they directly use a previously well-trained model for action recognition. Therefore, the accuracy of zero-shot recognition can effectively reflect the model's generalization and transferability. In Table 10, when the backbone CTR-GCN is trained with MMCL, the top-1 and top-5 accuracy for actions from different domains are 39.17% and 76.67%, while the baseline CTR-GCN trained without MMCL are 27.50% and 51.67% in SYSU dataset. Compared to the baseline CTR-GCN and HD-GCN, our MMCL shows a positive improvement in both Top-1 and Top-5 accuracy. We believe that MMCL benefits from the RGB modality and multimodal LLMs, thus demonstrating commendable generalization and robustness when facing other datasets in different domains. Besides, when introducing text features generated by multimodal LLMs to refine the zero-shot scores based on the Eq. 8, we are surprised to find an improvement in both recognition accuracy. This indicates that soft labels based on multimodal LLMs can be transferred to the domain adaptive action recognition. We argue that multimodal LLMs exhibit strong generalization capabilities, as they also can generate robust and effective text features when presented with RGB images in different domains.", "5 CONCLUSION": "We present a novel Multi-Modality Co-Learning (MMCL) framework for efficient skeleton-based action recognition, which is the first to introduce multimodal features based on multimodal LLMs for multi-modality co-learning into action recognition. The MMCL guides the modeling of skeleton features through complementary multimodal features and maintains the network's simplicity by using only skeleton in inference. Meanwhile, our MMCL is orthogonal to the backbone networks and thus can be coupled with various GCN backbones and multimodal LLMs. The effectiveness of our proposed MMCL is verified on three benchmark datasets, where it outperforms state-of-the-art methods. Experiments on two benchmark datasets also demonstrate the commendable generalization of our MMCL in zero-shot and domain-adaptive action recognition.", "ACKNOWLEDGMENTS": "This work was supported by Natural Science Foundation of Shenzhen (No. JCYJ20230807120801002), National Natural Science Foundation of China (No. 62203476). Multi-Modality Co-Learning for Efficient Skeleton-based Action Recognition MM'24, October 28-November 1, 2024, Melbourne, VIC, Australia", "REFERENCES": "[1] Dasom Ahn, Sangwon Kim, Hyunsu Hong, and Byoung Chul Ko. 2023. STARTransformer: a spatio-temporal cross attention transformer for human action recognition. In Proceedings of the IEEE Winter Conference on Applications of Computer Vision (WACV) . IEEE, Hawaii, USA, 3330-3339. [2] Danilo Avola, Marco Bernardi, Luigi Cinque, Gian Luca Foresti, and Cristiano Massaroni. 2019. Exploiting Recurrent Neural Networks and Leap Motion Controller for the Recognition of Sign Language and Semaphoric Hand Gestures. IEEE Transactions on Multimedia 21, 1 (2019), 234-245. [3] XB Bruce, Yan Liu, and Keith CC Chan. 2021. Multimodal fusion via teacherstudent network for indoor action recognition. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI) . AAAI, Canada, 3199-3207. [4] Chen Chen, Roozbeh Jafari, and Nasser Kehtarnavaz. 2015. UTD-MHAD: A multimodal dataset for human action recognition utilizing a depth camera and a wearable inertial sensor. In Proceedings of the IEEE International Conference on Image Processing (ICIP) . IEEE, Canada, 168-172. [5] Tailin Chen, Desen Zhou, Jian Wang, Shidong Wang, Yu Guan, Xuming He, and Errui Ding. 2021. Learning multi-granular spatio-temporal graph network for skeleton-based action recognition. In Proceedings of the ACM International Conference on Multimedia (ACM MM) . ACM, Chengdu, China, 4334-4342. [6] Yuxin Chen, Ziqi Zhang, Chunfeng Yuan, Bing Li, Ying Deng, and Weiming Hu. 2021. Channel-Wise Topology Refinement Graph Convolution for SkeletonBased Action Recognition. In Proceedings of the IEEE International Conference on Computer Vision (ICCV) . IEEE, Montreal, Canada, 13359-13368. [7] Zhan Chen, Sicheng Li, Bing Yang, Qinghan Li, and Hong Liu. 2021. MultiScale Spatial Temporal Graph Convolutional Network for Skeleton-Based Action Recognition. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI) . AAAI, Canada, 1113-1122. [8] Ke Cheng, Yifan Zhang, Xiangyu He, Weihan Chen, Cheng Jian, and Hanqing Lu. 2020. Skeleton-Based Action Recognition with Shift Graph Convolutional Network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) . IEEE, Seattle, USA, 183-192. [9] Hyung-gun Chi, Myoung Hoon Ha, Seunggeun Chi, Sang Wan Lee, Qixing Huang, and Karthik Ramani. 2022. InfoGCN: Representation Learning for Human Skeleton-Based Action Recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) . IEEE, New Orleans, USA, 2018620196. [10] Srijan Das, Saurav Sharma, Rui Dai, Francois Bremond, and Monique Thonnat. 2020. Vpn: Learning video-pose embedding for activities of daily living. In Proceedings of the European Conference on Computer Vision (ECCV) . Springer, Glasgow, USA, 72-90. [11] Zhichao Deng, Xiangtai Li, Xia Li, Yunhai Tong, Shen Zhao, and Mengyuan Liu. 2024. VG4D: Vision-Language Model Goes 4D Video Recognition. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA) . [12] Runwei Ding, Yuhang Wen, Jinfu Liu, Nan Dai, Fanyang Meng, and Mengyuan Liu. 2023. Integrating Human Parsing and Pose Network for Human Action Recognition. In Proceedings of the CAAI International Conference on Artificial Intelligence (CICAI) . CAAI, Fuzhou, China, 182-194. [13] Haodong Duan, Jiaqi Wang, Kai Chen, and Dahua Lin. 2022. DG-STGCN: Dynamic Spatial-Temporal Modeling for Skeleton-based Action Recognition. [14] Haodong Duan, Yue Zhao, Kai Chen, Dahua Lin, and Bo Dai. 2022. Revisiting skeleton-based action recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) . IEEE, New Orleans, USA, 2969-2978. [15] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, and Yu Qiao. 2023. LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model. [16] Tanmay Gupta and Aniruddha Kembhavi. 2023. Visual Programming: Compositional visual reasoning without training. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) . IEEE, Vancouver, Canada, 14953-14962. [17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) . IEEE, Las Vegas, USA, 770-778. [18] Ruijie Hou, Yanran Li, Ningyu Zhang, Yulin Zhou, Xiaosong Yang, and Zhao Wang. 2022. Shifting perspective to see difference: A novel multi-view method for skeleton based action recognition. In Proceedings of the ACM International Conference on Multimedia (ACM MM) . ACM, Lisbon, Portugal, 4987-4995. [19] Jianfang Hu, Weishi Zheng, Jianhuang Lai, and Jianguo Zhang. 2017. Jointly Learning Heterogeneous Features for RGB-D Activity Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence 39, 11 (2017), 2186-2200. [20] Xiaohu Huang, Hao Zhou, Bin Feng, Xinggang Wang, Wenyu Liu, Jian Wang, Haocheng Feng, Junyu Han, Errui Ding, and Jingdong Wang. 2023. Graph contrastive learning for skeleton-based action recognition. In Proceedings of the International Conference on Learning Representations (ICLR) . OpenReview.net, Kigali, Rwanda. [21] Hamid Reza Vaezi Joze, Amirreza Shaban, Michael L Iuzzolino, and Kazuhito Koishida. 2020. MMTM: Multimodal transfer module for CNN fusion. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) . IEEE, Seattle, USA, 13289-13299. [22] Qiuhong Ke, Mohammed Bennamoun, Senjian An, Ferdous Sohel, and Farid Boussaid. 2017. A New Representation of Skeleton Sequences for 3D Action Recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) . IEEE, Honolulu, USA, 3288-3297. [23] Qiuhong Ke, Mohammed Bennamoun, Senjian An, Ferdous Sohel, and Farid Boussaid. 2018. Learning Clip Representations for Skeleton-Based 3D Action Recognition. IEEE Transactions on Image Processing 27, 6 (2018), 2842-2855. [24] Sangwon Kim, Dasom Ahn, and Byoung Chul Ko. 2023. Cross-Modal Learning with 3D Deformable Attention for Action Recognition. In Proceedings of the IEEE International Conference on Computer Vision (ICCV) . IEEE, Paris, France, 10265-10275. [25] Jungho Lee, Minhyeok Lee, Dogyoon Lee, and Sangyoon Lee. 2023. Hierarchically Decomposed Graph Convolutional Networks for Skeleton-Based Action Recognition. In Proceedings of the IEEE International Conference on Computer Vision (ICCV) . IEEE, Paris, France, 10444-10453. [26] Ce Li, Chunyu Xie, Baochang Zhang, Jungong Han, Xiantong Zhen, and Jie Chen. 2022. Memory Attention Networks for Skeleton-Based Action Recognition. IEEE Transactions on Neural Networks and Learning Systems 33, 9 (2022), 4800-4814. [27] Chao Li, Qiaoyong Zhong, Di Xie, and Shiliang Pu. 2018. Co-occurrence Feature Learning from Skeleton Data for Action Recognition and Detection with Hierarchical Aggregation. [28] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. In Proceedings of the International Conference on Machine Learning (ICML) . ACM, Hawaii, USA, 19730-19742. [29] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. In Proceedings of the International Conference on Machine Learning (ICML) . ACM, Seoul, Korea, 12888-12900. [30] Shuai Li, Wanqing Li, Chris Cook, Ce Zhu, and Yanbo Gao. 2018. Independently Recurrent Neural Network (IndRNN): Building a Longer and Deeper RNN. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) . IEEE, Salt Lake City, USA, 5457-5466. [31] Yue Li, Baiqiao Yin, Jinfu Liu, Jiajun Wen, Jiaying Lin, and Mengyuan Liu. 2024. SemiPL: A Semi-supervised Method for Event Sound Source Localization. In Proceedings of the IEEE International Conference on Multimedia and Expo Workshop (ICMEW) . IEEE, Canada. [32] Jiaying Lin, Jiajun Wen, Mengyuan Liu, Jinfu Liu, Baiqiao Yin, and Yue Li. 2024. SFMViT: SlowFast Meet ViT in Chaotic World. In Proceedings of the IEEE International Conference on Multimedia and Expo Workshop (ICMEW) . IEEE, Canada. [33] Dongjingdin Liu, Pengpeng Chen, Miao Yao, Yijing Lu, Zijie Cai, and Yuxin Tian. 2023. TSGCNeXt: Dynamic-Static Multi-Graph Convolution for Efficient Skeleton-Based Action Recognition with Long-term Learning Potential. [34] Fang Liu, Yuhao Liu, Yuqiu Kong, Ke Xu, Lihe Zhang, Baocai Yin, Gerhard Hancke, and Rynson Lau. 2023. Referring Image Segmentation Using Text Supervision. In Proceedings of the IEEE International Conference on Computer Vision (ICCV) . IEEE, Paris, France, 22124-22134. [35] Hong Liu, Juanhui Tu, and Mengyuan Liu. 2017. Two-Stream 3D Convolutional Neural Network for Skeleton-Based Action Recognition. [36] Jinfu Liu, Runwei Ding, Yuhang Wen, Nan Dai, Fanyang Meng, Shen Zhao, and Mengyuan Liu. 2024. Explore Human Parsing Modality for Action Recognition. CAAI Transactions on Intelligence Technology (2024). [37] Jun Liu, Amir Shahroudy, Mauricio Perez, Gang Wang, Ling-Yu Duan, and Alex C. Kot. 2020. NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding. IEEE Transactions on Pattern Analysis and Machine Intelligence 42, 10 (2020), 2684-2701. [38] Jun Liu, Gang Wang, Ling-Yu Duan, Kamila Abdiyeva, and Alex C. Kot. 2018. Skeleton-Based Human Action Recognition With Global Context-Aware Attention LSTM Networks. IEEE Transactions on Image Processing 27, 4 (2018), 1586-1599. [39] Jinfu Liu, Xinshun Wang, Can Wang, Yuan Gao, and Mengyuan Liu. 2023. Temporal Decoupling Graph Convolutional Network for Skeleton-based Gesture Recognition. IEEE Transactions on Multimedia 26 (2023), 811-823. [40] Jinfu Liu, Baiqiao Yin, Jiaying Lin, Jiajun Wen, Yue Li, and Mengyuan Liu. 2024. HDBN: A Novel Hybrid Dual-branch Network for Robust Skeleton-based Action Recognition. In Proceedings of the IEEE International Conference on Multimedia and Expo Workshop (ICMEW) . IEEE, Canada. [41] Mengyuan Liu, Hong Liu, and Chen Chen. 2017. Enhanced skeleton visualization for view invariant human action recognition. Pattern Recognition 68 (2017), 346-362. [42] Mengyuan Liu, Fanyang Meng, Chen Chen, and Songtao Wu. 2023. Novel motion patterns matter for practical skeleton-based action recognition. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI) , Vol. 37. 1701-1709. [43] Ziyu Liu, Hongwen Zhang, Zhenghao Chen, Zhiyong Wang, and Wanli Ouyang. 2020. Disentangling and unifying graph convolutions for skeleton-based action recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) . IEEE, Seattle, USA, 143-152. MM'24, October 28-November 1, 2024, Melbourne, VIC, Australia Jinfu Liu, Chen Chen, & Mengyuan Liu."}
