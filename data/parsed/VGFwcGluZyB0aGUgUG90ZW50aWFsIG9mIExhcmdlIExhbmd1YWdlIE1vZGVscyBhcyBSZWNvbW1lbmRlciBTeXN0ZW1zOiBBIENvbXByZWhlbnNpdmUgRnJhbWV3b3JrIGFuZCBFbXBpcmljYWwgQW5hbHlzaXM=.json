{
  "Tapping the Potential of Large Language Models as Recommender Systems: A Comprehensive Framework and Empirical Analysis": "LANLING XU and JUNJIE ZHANG, Gaoling School of Artificial Intelligence, Renmin University of China, China BINGQIAN LI, Gaoling School of Artificial Intelligence, Renmin University of China, China JINPENG WANG and SHENG CHEN, Meituan Group, China WAYNE XIN ZHAO ‚àó and JI-RONG WEN, Gaoling School of Artificial Intelligence, Renmin University of China, China Recently, Large Language Models (LLMs) such as ChatGPT have showcased remarkable abilities in solving general tasks, demonstrating the potential for applications in recommender systems. To assess how effectively LLMs can be used in recommendation tasks, our study primarily focuses on employing LLMs as recommender systems through prompting engineering. We propose a general framework for utilizing LLMs in recommendation tasks, focusing on the capabilities of LLMs as recommenders. To conduct our analysis, we formalize the input of LLMs for recommendation into natural language prompts with two key aspects, and explain how our framework can be generalized to various recommendation scenarios. As for the use of LLMs as recommenders, we analyze the impact of public availability, tuning strategies, model architecture, parameter scale, and context length on recommendation results based on the classification of LLMs. As for prompt engineering, we further analyze the impact of four important components of prompts, i.e., task descriptions, user interest modeling, candidate items construction and prompting strategies. In each section, we first define and categorize concepts in line with the existing literature. Then, we propose inspiring research questions followed by detailed experiments on two public datasets, in order to systematically analyze the impact of different factors on performance. Based on our empirical analysis, we finally summarize promising directions to shed lights on future research. CCS Concepts: ¬∑ Information systems ‚Üí Recommender systems ; Language models . Additional Key Words and Phrases: Large Language Models, Recommender Systems, Empirical Study",
  "ACMReference Format:": "Lanling Xu, Junjie Zhang, Bingqian Li, Jinpeng Wang, Sheng Chen, Wayne Xin Zhao, and Ji-Rong Wen. 2025. Tapping the Potential of Large Language Models as Recommender Systems: A Comprehensive Framework and Empirical Analysis. 1, 1 (January 2025), 53 pages. https://doi.org/XXX.XXX",
  "1 INTRODUCTION": "In order to alleviate the problem of information overload [43, 97], recommender systems explore user needs and provide them with recommendations based on their historical interactions, which are widely studied both in industry and academia [35, 40, 41, 113]. During the past decade, various recommendation algorithms have been proposed to solve recommendation tasks by capturing personalized interaction patterns from user behaviors [51, 188]. Despite the progress of conventional recommenders, the performance is highly dependent on the limited training data from a few datasets and domains, and there are two major drawbacks. On the one hand, traditional models lack the general knowledge of the world beyond interaction sequences. For complex scenarios ‚àó Wayne Xin Zhao (batmanfly@gmail.com) is the corresponding author. Authors' addresses: Lanling Xu, xulanling@ruc.edu.cn; Junjie Zhang, junjie.zhang@ruc.edu.cn, Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China; Bingqian Li, Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China; Jinpeng Wang; Sheng Chen, Meituan Group, Beijing, China; Wayne Xin Zhao; Ji-Rong Wen, Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China. , Vol. 1, No. 1, Article . Publication date: January 2025. 2 Lanling Xu et al. that need to think or plan, existing methods do not have commonsense knowledge to solve such tasks [39, 89, 151, 159]. On the other hand, traditional models cannot truly understand intentions and preferences of users. The results of the recommendation do not have explainability, and the requirements expressed by users in explicit forms such as natural languages are difficult to consider [59, 64, 169]. Recently, Large Language Models (LLMs) such as ChatGPT have demonstrated impressive abilities in solving general tasks [36, 158], showing their potential in the development of next-generation recommender systems. The advantages of incorporating LLMs into recommendation tasks are two-fold. Firstly, the excellent performance of LLMs in complex reasoning tasks indicates the rich world knowledge and superior inference ability, which can effectively compensate for the local knowledge of traditional recommenders [1, 96, 115]. Secondly, the language modeling abilities of LLMs can seamlessly integrate massive textual data, enabling them to extract features beyond IDs and even explicitly understand user preferences [42, 62]. Therefore, researchers have attempted to leverage LLMs for recommendation tasks. Typically, there are three ways to employ LLMs to make recommendations: (1) LLMs can serve as the recommender to make recommendation decisions, encompassing both discriminative and generative recommendations [4, 17, 31, 44, 176]. (2) LLMs can be used to improve traditional recommendation models by extracting semantic representations of users and items from text corpora. The extensive semantic information and robust planning capabilities of LLMs are integrated into traditional models [1, 25, 39, 43, 89, 145, 151, 159]. (3) LLMs are used as the recommendation simulator to execute external generative agents in the recommendation process, where LLMs can empower users and items to stimulate the virtual environment [28, 136, 137, 173, 175]. We mainly focus on the first scenario in this paper. Considering the gap between general knowledge from LLMs and domain knowledge from recommendation models [4, 179], there are two key factors to leverage LLMs as recommenders, i.e., how to select an LLM as the foundation model and how to construct task-specific prompts. As for LLMs, a growing number of open-source and closed-source models have emerged, and the same model also has different variants due to settings such as parameter scales and context lengths [184]. It is worth discussing how to select corresponding LLMs for specific scenarios and develop corresponding training strategies. As for prompts, it is an important medium for interactions between humans and language models, and a well-designed prompt can better stimulate the powerful capabilities of LLMs [88, 118]. To stimulate the recommendation ability of LLMs, prompt engineering should involve not only task description and prompting strategies for general tasks, but also the incorporation of user interest and candidate items in recommender systems [88, 167, 185]. Although existing studies have made initial attempts to explore the recommendation capabilities of LLMs like ChatGPT [17, 31, 44, 86, 122], and some studies have used paradigms such as finetuning and instruction tuning to train LLMs in the field of recommender systems [3, 4, 176, 186], they focus on exploring the performance of a certain task instead of constructing a comprehensive framework to formalize the potential applications of LLM-based recommender systems. There are also systematic reviews concentrating on the progress of LLMs [184] and surveys of recommender systems empowered by LLMs [63, 77, 156]. However, previous surveys generally use specific criteria to classify existing work and introduce them separately. They mainly focus on showcasing related work and summarizing advantages and limitations, rather than conducting additional experiments to validate existing results and explore new discoveries. Our work focuses on the capabilities of LLMs to serve as recommender systems, aiming to establish a general framework of leveraging L arge L anguage M odels as R ecommender S ystems, namely LLM-RS. In order to conduct our analysis for LLM-RS, we formalize the input of LLMs for recommendation into natural language prompts with two key aspects: LLMs and prompts, and explain how our framework can be generalized to various recommendation scenarios and tasks. As for the use , Vol. 1, No. 1, Article . Publication date: January 2025. Leveraging Large Language Models as Recommender Systems 3 Table 1. An overview of the primary discoveries presented in our work. We summarize our findings in the second column as 'our findings', and verify findings in existing literature as 're-validated findings'. of LLMs as recommenders, we analyze the impact of the public availability, tuning strategies, model architecture, parameter scale, and context length on recommendation results based on the classification of LLMs. As for prompt engineering, we further analyze the impact of four important components of prompts, i.e., task description, user interest modeling, candidate items construction, and prompting strategies. Given personalized prompts that include task description and user , Vol. 1, No. 1, Article . Publication date: January 2025. 4 Lanling Xu et al. interest, the LLM selects, generates, or explains candidate items based on general world knowledge and personalized user profiles. For each module, we first define and categorize concepts in line with the existing literature. Then, we propose inspiring research questions, followed by detailed experiments to analyze the impact of different conditions on the recommendation performance. Based on the empirical analysis, we finally summarize empirical findings for future research. In general, the contributions of our work can be summarized as follows: ¬∑ We derive a general framework LLM-RS to sum up existing work of utilizing LLMs as foundation models for recommendation, which can be generalized to multiple scenarios and tasks by different LLMs and prompts. ¬∑ We provide a systematic analysis on leveraging LLMs as recommender systems, focusing on two aspects: LLMs and prompt engineering. The use of LLMs includes analysis of public availability, tuning strategies, model architecture, parameter scale, and context length. Moreover, prompt engineering consists of discussions on task description, user interest modeling, candidate items construction and prompting strategies. For each aspect, we define and describe with concepts first, and then provide reference solutions with experiments. ¬∑ Extensive experiments on two public datasets conclude key findings for recommendation with LLMs. As listed in Table 1, our findings include experimental settings on each aspect of our proposed framework, and obtain empirical experience on evaluating the performance of LLMs on recommendation tasks for future research. In what follows, we first review the related work in Section 2. In Section 3, we present our proposed general framework and its instantiation, and introduce overall settings of the following experiments. As the core components of this paper, we discuss two main aspects of LLM-RS, i.e., LLMs and prompts in Section 4 and Section 5, respectively. For each aspect, we generalize key factors that affect recommendation results, and conduct corresponding experiments to summarize empirical findings. At last, Section 6 concludes this paper and sheds lights on future directions.",
  "2 RELATED WORK": "",
  "2.1 Recommender Systems": "For tackling the challenge of information overload [97, 170], recommender systems have become pivotal tools for delivering personalized contents for users across various domains. Based on previous studies, research has delineated two primary categories for common recommendation methods: interaction-based recommendation and content-based recommendation [129]. In line with previous studies, recommendation algorithms aim to derive user preferences and behavioral patterns from their historical interactions. The most common technique for the interaction-based recommendation is Collaborative Filtering (CF) [117, 124], which recommends items based on preferences of similar users. Matrix Factorization (MF) [55] is a prevalent approach in collaborative filtering, and it constructs embedding representations for users and items from the interaction matrix, facilitating the algorithm to calculate similarity scores efficiently. Furthermore, Neural Collaborative Filtering (NCF) [41], integrating deep neural networks, replaces the inner product used in MF with a neural architecture, thereby demonstrating better performance than previous methods. Contemporary advancements in deep neural network architectures have enhanced the integration of user and item embeddings [84]. For example, since recommendation data can be represented as graph-structured data, Graph Neural Network (GNN) [157] can be utilized to encode the information of the interaction graph (nodes consist of users and items), and generate meaningful representations via message propagation and contrastive learning strategies [40, 81, 142, 154]. As Pre-trained Language Models (PLM) gain prominence, there is a growing interest in pre-trained large-scale recommendation models powered by PLMs [43, 178, 188]. Besides user-item pairs and IDs, , Vol. 1, No. 1, Article . Publication date: January 2025. Leveraging Large Language Models as Recommender Systems 5 content-based recommendation algorithms leverage auxiliary modalities such as textual and visual information to augment user and item representations in recommendation tasks [106, 148, 170].",
  "2.2 Large Language Models for Recommender Systems": "Large Language Models (LLMs) are a cutting-edge advancement in artificial intelligence that excel in understanding and generating human-like texts [108, 131, 172]. LLMs are usually transformer-based models and trained on vast amounts of textual data with billions of parameters, allowing them to comprehend contexts, generate coherent sentences, and even mimic human conversations [36, 158]. Through this process, LLMs have shown prominent potentials in the field of Natural Language Processing (NLP), and have demonstrated various incredible capabilities in dealing with complex NLP tasks, including but not limited to In-Context Learning (ICL) [5], instruction following [132] and step-by-step reasoning abilities [184]. Recently, LLMs have been increasingly integrated into recommender systems to provide personalized recommendation [63, 156, 185]. Recent studies have explored the fusion of LLMs with recommender systems, which can be divided into the three paradigms, i.e., LLMs as recommender systems (Section 2.2.1), LLMs improve recommender systems (Section 2.2.2) and LLMs as recommendation simulator (Section 2.2.3) as follows. 2.2.1 LLMs as Recommender Systems. This paradigm takes LLMs as recommender systems. Employing diverse strategies like pre-training, fine-tuning, or prompting, LLMs can combine general knowledge with input data to yield personalized recommendations for users [17, 44, 49]. Due to the variety of recommendation tasks, LLMs as recommender systems can be categorized into two types: discriminative recommendation and generative recommendation . ¬∑ Discriminative recommendation instructs LLMs to make recommendation decisions on the given candidate items, usually focusing on item scoring [29] and re-ranking tasks [17]. For Click-Through Rate (CTR) prediction tasks, TALLRec [4] used the recommendation data to finetune LLaMA [131] as the CTR prediction model, showcasing the powerful capabilities of LLMs in few-shot learning. Liu et al. [86] designed specific zero-shot and few-shot prompts to evaluate abilities of LLMs on rating predictions. LLMs were required to assign a score for the item according to the previous rating history of users and the score range given in prompts, while the result indicated that LLMs can outperform classical rating methods in few-shot conditions [86]. Kang et al. [52] further formulated the rating prediction task as multi-class classification and regression task, investigating the influence of model size on recommendation performance. Different from these methods, Hou et al. [44] structured a re-ranking task, employing ICL approaches for LLMs to rank items in the candidate pool. Previous studies highlighted the sensitivity of LLMs to the sequence of interaction histories provided in prompts [94], which can be alleviated by strategies such as recency-focused prompting [44]. ¬∑ Generative recommendation requires LLMs to generate items recommended to users, either from candidate item lists within prompts or from LLMs with general knowledge [63]. GenRec [49] leveraged the contextual comprehension ability of LLMs to transform interaction histories into formulated prompts for next-item predictions. To address instances where GenRec might propose items absent in candidate lists, GPT4Rec [60] came up with the method that used Best Matching (BM25) [114] algorithm to retrieve the most similar item in candidate item list with the item generated by LLMs. Faced with the challenge of item identifiers for compatibility with LLMs in recommender systems, LC-Rec [186] employed vector quantization to convert natural languages into trainable tokens, enabling the alignment with input formats of LLMs. To further address the limitations of previous methods reliant on historical data, To make the recommendation results generated by LLMs break through the limitations of existing items, SpecGR [24] introduced a draft-then-verify framework to recommend new items in inductive settings. In addition to top-n , Vol. 1, No. 1, Article . Publication date: January 2025. 6 Lanling Xu et al. recommendations, LLMs can be leveraged for generative tasks such as explainable recommendations [16, 56, 61, 93, 166] and review summarization [32, 87, 146]. Moreover, with the incredible abilities in dialogue comprehension and communication, LLMs are naturally considered as the backbone of conversational and interactive recommender systems. ChatRec [31] designed an interactive recommendation framework based on ChatGPT, which can comprehend requirements of users through multi-turn dialogues and traditional recommendation models. Moreover, RecLLM [28] combined the dialogue management module with a ranker module and a controllable LLM-based user simulator to generate synthetic conversations for tuning system modules. Apart from these methods, InteRecAgent [47] employed LLMs as the brain and recommender models as tools, combining their respective strengths to create an interactive recommender system [47]. As a Conversational Recommender System (CRS), InteRecAgent enabled traditional recommender systems to become interactive systems with a natural language interface through the integration of LLMs. The methods for adapting LLMs as recommender systems mainly have two paradigms, i.e., nontuning paradigm and tuning paradigm as follows. ¬∑ Non-tuning paradigm keeps parameters of LLMs fixed and extracts the general knowledge of LLMs with prompting strategies. Existing work of non-tuning paradigm focuses on designing appropriate prompts to stimulate recommendation abilities of LLMs [69, 146, 167]. Liu et al. [86] proposed a prompt construction framework to evaluate abilities of ChatGPT on five common recommendation tasks, each type of prompts contained zero-shot and few-shot versions. Hou et al. [44] not only used prompts to evaluate abilities of LLMs on sequential recommendation, but also introduced recency-focused prompting and ICL strategies to alleviate order perception and position bias issues of LLMs. ChatRec [31] and InteRecAgent [47] mentioned above are also within the classic non-tuning paradigm. ¬∑ Tuning paradigm aims to update parameters of LLMs to inject recommendation capabilities into LLM itself. The tuning strategies include fine-tuning [3, 45, 180, 186] and instruction tuning [92, 105]. P5 [32] proposed five types of instructions targeting at different recommendation tasks to fine-tune a T5 [108] model. The instructions were formulated based on conventional recommendation datasets with designed templates, which equipped LLMs with generation abilities for unseen prompts or items [32]. InstructRec [176] further designed abundant instructions for tuning, including 39 manually designed templates with preference, intention, task form and context of a user. Compared with these methods, TallRec [4] used Low-Rank Adaptation of LLMs (LoRA) [45], a parameter-efficient tuning method, to handle the two-stage tuning for LLMs. To further align LLMs with personalized human preferences, recent research [11, 30, 75] has applied post-training techniques such as Direct Preference Optimization (DPO) [107] to fine-tune LLMs for recommendation, thereby integrating negative samples and user preference information within the tuning process. Although LLMs as recommender systems present a way of utilizing the common knowledge of LLMs, it still encounters problems to be coped with. Due to the high computational cost [76, 132] and slow inference time [66], LLMs are struggled to be efficient enough compared to traditional recommendation methods [31, 44]. Additionally, constraints on the input sequence length will limit the amount of external information ( e.g., candidate item lists) [78], leading to degrading performance of LLMs in scenarios such as sequential recommendation. Furthermore, since information in recommendation tasks is challenging to be expressed in natural language [79, 169], it is hard to formulate appropriate prompts that make LLMs truly understand what they are required to do. 2.2.2 LLMs Improve Recommender Systems. This method mainly utilizes LLMs to generate auxiliary information to enhance the performance of recommendation models [25, 145, 151], based on the reasoning abilities and common knowledge. The research on how to improve recommendation , Vol. 1, No. 1, Article . Publication date: January 2025. Leveraging Large Language Models as Recommender Systems 7 models with LLMs can be divided into three categories, i.e., LLMs as feature encoder , LLMs for data augmentation and LLMs co-optimized with domain-specific models . ¬∑ LLMs as feature encoder . The representation embeddings of users and items are important factors in classical recommender systems [40, 113]. LLMs serving as feature encoders can generate related textual data of users and items, and enrich their representations with semantic information. U-BERT [106] injected user representations with user review texts, item review texts and domain IDs, augmenting the contextual semantic information in user vectors. Wu et al. [153], on the other hand, employed language models to generate item representations for news recommendation. With the development of LLMs and prompting strategies, BDLM [177] constructed the prompt consisting of interaction and contextual information into LLMs, and obtained top-layer feature embeddings as user and item representations, injecting user-item interaction information into embeddings. ¬∑ LLMs for data augmentation . For this paradigm, LLMs are required to generate auxiliary textual information for data augmentation [1, 84, 89, 151]. By using prompting or ICL strategies, the related knowledge will be extracted out in different text forms to facilitate recommendation tasks [25, 145, 159]. One form of auxiliary textual information is summarization or text generation, enabling LLMs to enrich representations of users or items [148]. For example, Du et al. [25] proposed a job recommendation model which utilized the capability of LLMs for summarization to extract user information and job requirements. Mysore et al. [96], on the other hand, used the ICL strategy to instruct LLMs for narrative queries according to the interaction history of users, which was then utilized as the source data for training narrative driver recommendation models. Considering item descriptions and user reviews, KAR [159] extracted the reasoning knowledge on user preferences and the factual knowledge on items through specifically designed prompts, while SAGCN [84] utilized a chain-based prompting strategy to generate semantic information. Another form of using the textual features generated from LLMs is for graph augmentation in the recommendation field. LLMRG [145] leveraged LLMs to extend nodes in recommendation graphs. The resulting reasoning graph was encoded using GNN, which served as additional input to enhance sequential models. LLMRec [151] adopted three types of prompts to generate information for graph augmentation, including implicit feedback, user profile and item attributes. ¬∑ LLMs co-optimized with domain-specific models . The categories mentioned above mainly focus on the impact of common knowledge for domain-specific models [148]. However, LLM itself often struggles to handle domain-specific tasks due to the lack of task-related information [52, 167]. Therefore, some studies conducted experiments to bridge the gap between LLMs and domainspecific models. BDLM [177] proposed an information sharing module serving as an information storage mechanism between LLMs and domain-specific models. The user embeddings and item embeddings stored in the module were updated in turn by the LLM and the domain-specific model, enhancing the performance of both sides. CoLLM [179] combined LLMs with a collaborative model, which formed collaborative embeddings for LLM usage. By tuning LLM and collaborative module, CoLLM showed great improvements in both warm and cold-start scenarios. In CRS, approaches such as ChatRec [31] and InteRecAgent [47] considered LLMs as the backbone, and leveraged traditional recommendation models for candidate item retrieval. In addition to the context limitation and computational cost of LLMs [132], the paradigm that LLM improves recommendation models also encounters other problems. (1) Although LLMs can enhance offline recommender systems to avoid online latency, this paradigm also limits the ability of LLMs to model real-time collaborative filtering information, neglecting the key factor for recommendation [148, 151, 159]. (2) Feature encoding, data augmentation, and collaborative training inevitably expose the user data to LLMs, which may bring privacy, security and ethical issues [7, 120, 152]. , Vol. 1, No. 1, Article . Publication date: January 2025. 8 Lanling Xu et al. 2.2.3 LLM as Recommendation Simulator. Due to the gap between offline metrics and online performance of recommendation methods [41, 101], it is necessary for the designed approach to get intents of users by simulating real-world elements. In this way, LLM as the recommendation simulator is introduced by taking LLMs as the foundational architecture of generative agents, and agents simulate the virtual users in the recommendation environment [137, 173, 175]. Recently, there emerged a lot of work studying the performance of LLMs as the recommendation simulator. Agent4rec [173] was a movie simulator consisting of two core fractions: LLM-empowered generative agents and recommendation environment. The work equipped each agent with user profile, memory and actions modules, mapping basic behaviors of real-world users. AgentCF [175], on the other hand, considered not only users but also items as agents. It captured the two-sided relations between users and items, and optimized these agents by prompting them to reflect on and adjust the misleading simulations collaboratively [175]. Moreover, in addition to behaviors within the recommender system, RecAgent [136, 137] took external influential factors of user agent simulation into account, such as friend chatting and social advertisement. In order to describe users accurately, RecAgent applied five features for users, and implemented two global functions including real-human playing and system intervention to operate agents flexibly. Although LLM as recommendation simulator aims to imitate real-world recommendation behaviors to enhance the recommendation performance, it still has deficiencies in some aspects. Firstly, since current work is mainly demo systems that operate a few agents [136, 137], there still exists a gap between virtual agent environment and real-world practical recommendation applications, which requires further research and development. Additionally, LLMs may arise privacy and safety concerns. Many studies take ChatGPT as the architecture of agents, presenting security risks to the recommended information for users [175]. Moreover, Zhang et al. [173] have explored that hallucination in LLMs can exert huge impact on recommendation simulations. The LLM sometimes fails to accurately simulate human users, such as providing inconsistent score for an item and fabricating non-existent items for rating.",
  "2.3 Differences with Existing Work": "Previous surveys of LLMs for recommender system usually categorized existing work with a classification standard and introduced studies. Lin et al. [77] categorized existing work into the targets and the methods of adapting LLMs to recommendation tasks. Wu et al. [156] mainly focused on the form of information between LLMs and recommender systems. Zhao et al. [185] summarized the framework into four sections, involving deep representation learning, pre-training, fine-tuning and prompting of LLMs. Li et al. [63] concentrated on LLMs for generative recommendation, and summarized the different recommendation tasks that LLMs can serve as in detail. Chen et al. [9] analyzed challenges and opportunities of LLMs in the field of personalization, and Li et al. [70] systematically classified existing work on generative retrieval and generative recommendation. These surveys mainly concentrated on demonstrating related work and summarizing the advantages and limitations, while they lack performance analysis supported by experimental results. Although existing work like LLMRec [87] and LLM-REC [126] has evaluated the recommendation performance of LLMs on multiple tasks, their analysis of LLMs selection and prompt design is not deep enough. Compared to previous work, we concentrate on the performance of LLMs leveraged as recommender systems, and provide a systematic empirical analysis on LLM-based recommendations by devising a general framework LLM-RS. We mainly focus on two aspects, i.e., LLMs and task-specific prompts, providing definitions and solutions from both conceptual and methodological perspectives. Furthermore, we conduct experiments to discover new findings and validate results previously discussed in existing research, serving as an inspiration for future research efforts. , Vol. 1, No. 1, Article . Publication date: January 2025. Leveraging Large Language Models as Recommender Systems 9 Fig. 1. The overall framework of our proposed LLM-RS. In our framework, LLMs are leveraged as recommender systems in four ways: prompting without tuning, full-model fine-tuning, parameter-efficient fine-tuning and instruction tuning; Prompt engineering consists of four components: task description, user interest modeling, candidate items construction and prompting strategies. LLMs act as recommender systems through taskspecific prompts, while LLMs provide response and feedback to optimize prompts. LLMs Leveraged as Recommender Systems Prompting without Tuning LLMs üßä Input Output LLMs üî• LLMs üßä Input Full-model Fine-tuning Output Update Input Loss Parameter-efficient Fine-tuning Output Update Loss üî• Adapter Instruction Tuning LLMs üî• Output Update Task 1 Task ùëõ ‚Ä¶ Loss 1 Loss ùëõ ‚Ä¶ Loss 2 Prompt Engineering üßä : fixed üî• : trainable Prompting / Tuning LLMs Response, Reflection and Recommendation Candidate Items Construction Task Description User Interest Modeling Prompting Strategies Task Definition CTR Prediction Sequential Recommender CRS Task Type Recall Re-rank Generate Rate Task Objective Fairness Diversity Accuracy Interest Type Short-term Interest Long-term Interest Global / Personalized Memory Interest Format Natural Language Vector Embedding Codebook Token Item List Interest Operation Reading & Writing Query & Retrieval Reflection Summarization Item Selection Item Grounding Item Generation LLMs Item Pool RS Candidate Items Recall Recommend LLMs Recommend Prompts Logits Item Pool Calculation & Grounding Match LLMs Recommend Prompts Generate Generated Item Output Zero-shot Prompting Basic prompts only. Few-shot Prompting Demonstrations: [examples] Chain-of-Thought Prompting Let's think step by step. Re-prompting with Feedback The user would not enjoy item ‚Ä¶ Role Prompting You are a recommender system ... Task 2 @",
  "3 GENERAL FRAMEWORK AND OVERALL SETTINGS": "In this section, we present our proposed LLM-RS with two important components, namely LLMs and prompts . Generally speaking, the rich world knowledge and general capabilities of LLMs demonstrate the potential to develop LLM-based recommender systems. Nevertheless, it is essential to introduce appropriate prompting engineering to provide domain-specific knowledge on recommendation tasks for LLMs to serve as personalized recommenders. In the following, we first describe our general framework by defining several key elements (Section 3.1). Then, we explain how our framework can be generalized to various tasks by framework instantiation (Section 3.2). Finally, we introduce our overall experimental settings for further analysis (Section 3.3).",
  "3.1 Overview of the Approach": "Whenit comes to leveraging LLMs as recommender systems, the first step involves choosing suitable LLMs tailored to specific scenarios, with an emphasis on their distinct capabilities. In our framework, LLMs are leveraged as recommender systems in four ways: prompting without tuning , full-model fine-tuning , parameter-efficient fine-tuning and instruction tuning . Subsequently, it is important to conduct prompt engineering to instruct LLMs to perform effective recommendations. Typically, in the realm of recommender systems using LLMs, despite the task settings in existing studies vary a lot, the overall prompting format remains relatively consistent with minor variations [17, 31, 44, 146]. Therefore, to unify existing prompting approaches for different recommendation purposes, as illustrated in Fig. 1, we establish a general framework for prompt engineering in LLM-RS, consisting of four key factors in the prompts. Firstly, task description is required to clearly express the specific goal of recommendation tasks. Secondly, prompts need to be carefully designed, to express user interest and meanwhile enable LLMs to provide personalized recommendations with both world and , Vol. 1, No. 1, Article . Publication date: January 2025. 10 Lanling Xu et al. domain knowledge. Thirdly, the purpose of a recommender system is to provide users with candidate items, and potential candidate items should be constructed for LLMs to facilitate the recommendation with the understanding of domain-specific item information. Furthermore, special prompting strategies can be further employed to enhance the specialized recommendation capabilities of LLMs. In what follows, we will thoroughly examine the impact of each factor on the recommendation performance. 3.1.1 Key Elements in Our Framework. To carry out the experiments, we first describe the key elements in LLM-RS, to clarify their definitions and scope in this work. Specially, we introduce the following five elements for our framework: ¬∑ Large language models (LLMs) . As proposed in previous research [3, 44, 65, 179], there exists a large gap between general language modeling and personalized user behavioral modeling, making it non-trivial to utilize LLMs in recommender systems. In this work, we investigate the efficacy of LLMs from perspectives of public availability , tuning strategies , model architecture , parameter scale , and context length , aiming to gain insights into the selection of appropriate LLMs for performing recommendation tasks. Observations and discussions on the use of LLMs are presented in Section 4. ¬∑ Task description . To adapt LLMs to the scenario of recommendation, it is necessary to clearly express the context and target of recommender systems for LLMs in the prompt, i.e., task description. With different prompt descriptions of tasks, LLMs can be leveraged to various recommendation scenarios and tasks such as click-through rate predictions [3, 4, 52], sequential recommendation [17, 44, 186] and conversational recommender systems [31, 42, 143]. In addition, the well-designed task description also helps to improve the fairness [138, 174] and diversity [8, 20] of recommendation results. ¬∑ User interest modeling . The modeling of user interest is the key to recommendation tasks [41, 113]. From a temporal perspective, user interest can be categorized into short-term instant interest and long-term stable preferences, which collectively constitute the user profile [87, 126]. When leveraging LLMs as recommender systems, users are generally expressed in natural language text [4, 44], which is different from traditional approaches capturing user preference from IDbased behavior sequences [51, 154]. Moreover, learnable tokens [53, 119, 139] and distilled soft prompts [62, 110] for user interest modeling in LLMs have attracted attention of researchers. In this paper, we mainly consider the reflected user interest based on his or her interaction behaviors with interacted items. Especially, as detailed in Section 5.2, we employ item description texts, user profiles, and historical interactions between users and items to reveal the underlying user interest using natural languages [78, 122, 146, 167]. ¬∑ Candidate items construction . The purpose of recommender systems is to provide users with items to choose from, so candidate items construction is a crucial step in our framework [17, 44, 176]. A simple approach is to provide several candidate items in prompts, e.g., the items recalled by traditional recommendation models [78, 171]. Due to the input length limitation of LLMs, it is not possible to include all items in the prompts. Besides selecting candidate sets, there are also methods that directly generate candidate items by LLMs, utilizing strategies such as output probability distribution [171] and vector quantization [186] for item indexing and grounding. Section 5.3 will focus on the construction strategies of candidate items, including selection, grounding and generation. ¬∑ Prompting strategies . Despite the impressive capabilities of LLMs, they tend to exhibit unsatisfactory performance in providing personalized recommendations [17, 44, 52, 86]. The reason may stem from the significant semantic gap between the general knowledge encoded in LLMs and the domain-specific behavioral pattern and item catalogs of recommender systems [65, 179]. To , Vol. 1, No. 1, Article . Publication date: January 2025. Leveraging Large Language Models as Recommender Systems 11 Table 2. Instantiation of existing work for LLM-RS. We summarize three paradigms of leveraging LLMs as recommender systems within the framework of our proposed LLM-RS: (1) prompting without tuning, (2) (parameter-efficient) fine-tuning, and (3) instruction tuning. For each paradigm, we provide a comprehensive overview of related work and its instantiation in our framework. specialize LLMs to recommender systems, we summarize and propose several prompting strategies specialized for recommendation tasks. Details will be discussed in Section 5.4.",
  "3.2 Instantiation of LLM-RS": "By combining the key elements mentioned above, we can instantiate various types of recommender systems in our framework with the following five steps. Specifically, (1) we can employ LLMs with varying levels of public availability, different tuning strategies, model architectures, parameter scales, and context lengths. (2) We can define a range of task description, such as retrieving, rating, recalling, and ranking. (3) Regarding user interest modeling, we can employ different types of interest, representation forms, and modeling methods. (4) When collecting candidate items, , Vol. 1, No. 1, Article . Publication date: January 2025. 12 Lanling Xu et al. Table 3. Statistics of two public datasets for LLM-RS. Since the original MovieLens-1M dataset does not provide description information, we used ChatGPT to generate text descriptions based on item attributes. we take into account their different representation types, sources, and grounding methods. (5) Additionally, we can introduce several well-designed prompting strategies to effectively guide the recommendation capabilities of LLMs. To demonstrate the compatibility and versatility of our framework, we summarize previous work on LLM-based recommender systems in Table 2 based on various settings in our framework LLM-RS.",
  "3.3 Experimental Settings": "In this section, we introduce the overall settings of the following experiments. We first describe the basic information of datasets, and then present the configurations and implementations. 3.3.1 Datasets. The domain characteristics of movies and books are closer to the general knowledge of LLMs, which facilitates the further analysis. Considering the scale, popularity and side information of public datasets, we select two representative datasets to conduct our study, i.e., MovieLens1M [37] and Amazon Books (2018) [97]. Next, we present the details of the two selected datasets: ¬∑ MovieLens-1M [37] is one of the most widely used benchmark datasets in the field of recommender systems, covering movie ratings and attributes on the website movielens.org. We use the one million version from the MovieLens datasets, and it contains 1,000,209 ratings from 6,040 users on 3,706 movies. It is collected by the GroupLens research group. ¬∑ Amazon Books (2018) [97] is an updated version of the Amazon review dataset. Amazon review datasets include reviews and product metadata collected from Amazon, one of the largest online e-commerce company in the United States. At first, Amazon only operated online book sales business, so the data in the book field is the most abundant. To improve the data quality, we filter out inactive users and unpopular products, and remove data without necessary attributes. In our research, we are concerned about how LLMs can fully utilize the domain knowledge to make recommendations, and use the title of items as the input for prompts. However, titles are not enough to describe items, and there are deviations between the text of the title and the content of the item itself ( e.g., the movie Twelve Monkeys). Therefore, we further investigate the benefits of detailed item descriptions on the recommendation effect. As shown in Table 3, there are no item descriptions in the original dataset of MovieLens, only the release year, title, and genre. If LLMs do not include knowledge about a movie, only the release year, title and genre may lead to ambiguity and hallucination. To enrich the movie dataset, we use the general knowledge of ChatGPT 1 to generate text descriptions for movies. We mainly use the interaction information between users and items in the recommendation dataset, as well as the title and description of items. 3.3.2 Configuration and Implementation. As for LLM-RS, we can evaluate the cold-start recommendation ability of LLMs in the zero-shot setting, as well as evaluate the fine-tuned performance with a few or full recommendation samples in the fine-tuning setting. Considering the typical scenarios of LLMs as recommender systems, we conduct experiments on two representative task settings, i.e., (1) the zero-shot ranking task without modifying parameters of LLMs (generative 1 The URL of ChatGPT Application Programming Interface (API): https://chat.openai.com/. Note that there are multiple versions of the ChatGPT API. In the absence of clear annotations, the ChatGPT used in this article is 'gpt3.5-turbo-4k-0613'. , Vol. 1, No. 1, Article . Publication date: January 2025. Leveraging Large Language Models as Recommender Systems 13 Fig. 2. Basic prompts for experiments on the MovieLens-1M dataset in LLM-RS. We explore two application scenarios: (1) prompting LLMs without tuning, and (2) fine-tuning LLMs as recommender systems. The experiments focus on sequential re-ranking and Click-Through Rate (CTR) prediction tasks, respectively. LLMs üßä Output : {Candidate items after re-ranking}. Output : {Candidate items after  e-ranking}. Input : I've watched the following movies in the past in order: { historical interactions of users }.[ User Interest ] Note that my most recently watched movie is {recent interacted item}.[ Recency Prompting ] Now there are 20 candidate movies that I can watch next: { candidate items }.[ Candidate Items ] Please rank these 20 movies by measuring the possibilities that I would like to watch next most, according to my watching history. [ Task Description ] Please think step by step. [ Chain-of-Thought Prompting ] Please show me your ranking results with order numbers. Split your output with line break. You MUST rank the given candidate movies. You can not generate movies that are not in the given candidate list. [ Output Format ] Input : I've watched the following movies in the past in order: historical interactions of users }.[ User Interest ] Note that my most recently watched movie is {recent interacted item}.[ Recency Prompting ] Now there are 20 candidate movies that I can watch next: candidate items }.[ Candidate Items ] Please rank these 20 movies by measuring the possibilit es that I would like to watch next most, according to my watching history. [ Task Description ] Please think step by step. [ Chain-of-Thought Prompting ] Please show me your ranking results with order numbers. Split your output with line break. You MUST rank the given candidate movies. You can not generate movies that are not in the given candidate list. [ Output Format ] Output : Yes. or No.",
  "Input Input :": ": [ Task Description ]: Given the user's preference and unpreference, identify whether the user will like the target movie by answering \"Yes.\" or \"No.\". Given the user's preference and unpreference, identify whether the user will like the target movie by answering \"Yes.\" or \"No.\" [ User Interest ]: User Preference: {historical items with a rating higher than or equal to the rating threshold} User Unpreference: {historical items with a rating lower than the rating threshold} [ Candidate Items ]: Whether the user will like the target movie {target item}? Whether the user will like the target movie {target item}? LLMs üî• (a) Zero-shot ranking task [44]. LLMs üßä { { Output : Yes. or No. [ Task Description ]: [ User Interest ]: User Preference: {historical items with a rating higher than or equal to the rating threshold} User Unpreference: {historical items with a rating lower than the rating threshold} [ Candidate Items ]: LLMs üî• (b) Fine-tuned CTR prediction task [4]. recommendation) [17, 31, 44, 94]; and (2) the Click-Through Rate (CTR) prediction task with LLMs tuned (discriminative recommendation) [4, 29, 52, 121]. ¬∑ Zero-shot ranking task . On the one hand, we evaluate the zero-shot recommendation performance of LLMs for cold-start scenarios to study the effect of LLMs and the design of prompts. In this paper, our approach mainly concentrates on ranking tasks that better reflect the capabilities of LLMs [17, 31, 44, 94]. As shown in Fig. 1, information of users and items is encoded into the prompt as inputs for LLMs. In this setting, we do not modify the parameters of LLMs, so the evaluated models are closed-source LLMs or open-source models without fine-tuning. To conduct experiments on the impact of each factor on recommendation results with LLMs, we implement the overall architecture based on the open-source recommendation library RecBole [161, 182, 183] and the zero-shot re-ranker LLMRank [44]. Our basic prompt used in the MovieLens-1M dataset for the zero-shot ranking task is shown in Fig. 2(a). ¬∑ CTR prediction task with LLMs tuned . On the other hand, we evaluate the fine-tuned recommendation performance of LLMs to explore how LLMs adapt to recommendation scenarios with data provided [4, 29, 52, 121]. Although our framework can be generalized to various recommendation tasks, we concentrate on exploring the fine-tuning performance of LLMs with point-wise CTR prediction tasks to reduce selection bias. In this setting, we not only consider fine-tuning LLMs using recommendation data from scratch, but also devise a two-stage approach of using instruction data to fine-tune LLMs first, and then implement recommendation fine-tuning for further adaptation. Specifically, we compare the fine-tuned recommendation performance of the original LLM and the LLM after instruction tuning, respectively. As for the selection of LLMs, LLaMA-7B [131], LLaMA2-7B [132] and LLaMA3-8B [26] are the original models, while Alpaca-lora-7B [127] and LLaMA2-chat-7B [132] are LLMs after instruction tuning. In terms of the tuning strategies of LLMs, we report results with both Parameter-Efficient Fine-Tuning (PEFT) and full-model fine-tuning. We implement the fine-tuning framework based on the open-source library transformers and the instruction tuning code of LLaMA with Stanford Alpaca data 2 . Our basic prompts to fine-tune LLMs for the CTR prediction task on the MovieLens-1M dataset is illustrated in Fig. 2(b). 2 The repository of Alpaca-LoRA: https://github.com/tloen/alpaca-lora. , Vol. 1, No. 1, Article . Publication date: January 2025. 14 Lanling Xu et al. 3.3.3 Evaluation Metrics. As for the zero-shot ranking task, considering economic and efficiency factors, we refer to existing literature [44, 122] to randomly sample 200 users instead of evaluating results on the whole dataset. The average result of 200 users is the final indicator for performance comparison. For each user from the sample set, we sort all items that the user interacts with in the chronological order. Then, we evaluate the results based on the leave-one-out strategy and treat the last interacted item as the ground truth. For performance comparison, we fix the length of candidate items to 20 as in [44], and mix the other 19 items from the ground truth in random positions by default. As for evaluation, we utilize two widely used ranking metrics in recommender systems, i.e., Recall [6] and Normalized Discounted Cumulated Gain (NDCG) [48]. Since 20 items are selected for candidate generation, we set ùëò = 20 for Recall@ ùëò to measure whether all ground-truth items have been recalled, and the metric Recall@20 can also be expressed as Coverage. In existing literature, the re-generation method of multiple results until the format requirements are met can be employed to obtain the final response [67], while we consider the recall capability within one inference for fair evaluation. Furthermore, we set ùëò = 1 , 10 , 20 for NDCG to explore the detailed recommendation performance in terms of ranking abilities. For a scientific research, we repeat each experiment three times and take the average value as the results. As for the CTR prediction task, we first sort the original dataset by timestamp, use the latest 10,000 records for training and evaluation, and regard other previous data as the interaction history of users. Then, we split the interactions into the training, validation and test sets in ratio of 8:1:1. For each interaction, we retain 10 historical interacted items as user representations, and set thresholds based on rating data to obtain user preferences [4]. Interactions with a rating higher than or equal to the threshold will be considered items that the user likes, while the opposite indicates dislikes. For the MovieLens-1M dataset, the rating threshold is 4, and we set 5 for the Amazon Books dataset. We employ the training data to fine-tune LLMs and evaluate the recommendation performance on the test set. The validation set is used for selecting best checkpoints during the training process. As for evaluation of prediction, we utilize the widely used metric for CTR predictions, i.e., accuracy. In a random state, the accuracy is around 0.5. 3.3.4 Discussion on Variable Factors. To conduct our analysis for LLM-RS, we mainly focus on using LLMs as recommender systems with two key aspects, i.e., LLMs and prompts. As for the effects of LLMs, we analyze the impact of public availability, tuning strategies, model architecture, parameter scale, and context length on recommendation results based on the classification of LLMs. As for prompt engineering, we further analyze the impact of four important components of prompts, i.e., task description, user interest modeling, candidate items construction and prompting strategies. Due to the fact that all factors have an impact on the final result, it is also crucial to select the other aspects when focusing on one aspect. Limited by resources and efficiency, it is neither necessary nor feasible to exhaust all possibilities. When not explicitly specified, the LLM uses ChatGPT released in June 2023 to ensure the consistent recommendation quality. For the design of prompts, we refer to the template in [44] and emphasize the most recently interacted items to re-rank 20 candidate items. The default method for modeling user interest is to concatenate the title sequence of recently interacted items using natural languages.",
  "4 THE IMPACT OF LARGE LANGUAGE MODELS AS RECOMMENDER SYSTEMS": "LLMs are the core of our framework LLM-RS, and largely determine the performance of LLMs as recommender systems [44, 52, 86, 87]. Therefore, it is worth exploring how to choose a suitable LLM as the foundation for recommendation. In this section, we compare the differences between LLMs and traditional models on the recommendation performance, discuss how the different properties , Vol. 1, No. 1, Article . Publication date: January 2025. Leveraging Large Language Models as Recommender Systems 15 and tuning strategies of LLMs affect the recommendation results, present the limitations of LLMs as recommenders, and draw empirical conclusions through systematic experiments.",
  "4.1 Classification of LLMs": "In this paper, we consider language models that have a size larger that one billion as LLMs. In line with existing researches [63, 77, 156, 184, 185], LLMs can be categorized into different classes from several perspectives. Most typically, LLMs can be divided into open-source and closed-source models in terms of the public availability. When it comes to leveraging LLMs as the foundation model in recommender systems, tuning strategies can adjust LLMs towards specific recommendation tasks. From the perspective of the model architecture, various LLMs can also be categorized into types of encoder-decoder, causal decoder, and prefix decoder [184]. For the same framework of a LLM, it is widely acknowledged that the parameter scale and context length are two key factors that jointly determine capabilities of LLMs [44, 78]. To explore the recommendation performance w.r.t. different variants of LLMs, we focus on five aspects, i.e., public availability , tuning strategies , model architecture , parameter scale and context length as follows. 4.1.1 Public Availability. According to whether the model checkpoints can be publicly obtained, existing LLMs can be divided into open-source models and closed-source models. In what follows, we will explain that both the two categories of LLMs can be leveraged as recommender systems. ¬∑ Open-source models refer to LLMs whose model checkpoints can be publicly accessible. As shown in Table 2, researchers often use recommendation data to fine-tune open-source models for performance improvement. As the representative of open-source models, LLaMA [131] and its variants like Vicuna [13] are widely used when leveraging LLMs for recommender systems [66, 186]. Parameter-Efficient Fine-Tuning (PEFT) strategies such as Low-Rank Adaptation (LoRA) [45] are frequently adopted for recommendation data considering the trade-off between effect and efficiency [4, 76]. Other models such as the Flan-T5 [15] series from Google Inc., ChatGLM [2] from Zhipu AI and Qwen [128] from Alibaba are also popular in the field of recommender systems. The publicly available checkpoints of open-source models provide flexibility for LLMs to modify parameters tailored for recommendation tasks. ¬∑ Closed-source models refer to LLMs whose model checkpoints can not be publicly accessible. For the closed-source LLMs leveraged as recommender systems, researchers generally study the zero-shot recommendation ability in cold-start scenarios. The most typical closed-source model is the ChatGPT series from OpenAI. The subsequent GPT-4 [99] and GPT-4o [98] have stronger capabilities compared to ChatGPT, but they are still not open-source. In this paper, ChatGPT refers to the API function of 'gpt3.5-turbo-4k-0613' unless specified. Without checkpoints, OpenAI provides several ways for researchers and users to improve the model performance on specific tasks, such as plugins for website browsing [57] and interfaces for fine-tuning ChatGPT [67]. However, the flexibility of closed-source models as recommender systems is still limited due to the expensive price and black-box parameters. Faced with this challenge, existing literature has explored to inject knowledge of recommender systems into closed-source models by means of prompt design [135, 167], retrieval enhancement [47, 122, 146] and combination of traditional recommendation models [31, 143]. 4.1.2 Tuning Strategies. During the deployment of LLMs in recommender systems, we can also classify existing work depending on whether LLMs are fine-tuned, as well as the various fine-tuning strategies employed, i.e., prompting without tuning , fine-tuning and instruction tuning . ¬∑ Prompting without tuning means evaluating the zero-shot recommendation ability of LLMs, which is generally used for closed-source models such as ChatGPT. By designing prompt templates, LLMs without fine-tuning can be used for recommendation tasks such as CTR predictions [22, 52], , Vol. 1, No. 1, Article . Publication date: January 2025. 16 Lanling Xu et al. sequential recommendation [44, 146], and Conversational Recommender Systems (CRS) [42, 50, 143]. In this case, the user interest is expressed explicitly ( e.g., ratings and reviews) or implicitly (interacted items of users), and the limited candidate items can be recalled by traditional models, while prompting strategies such as In-Context Learning (ICL) and Chain-of-Thoughts (CoT) are used. Inspired by Artificial Intelligence Generated Content (AIGC), it is worth noting that the excellent generation ability of LLMs provides opportunities for generative recommendation [89, 140]. Without providing candidate items, generative language models can generate the desired items that users need based on recommendation requirements, which can also be generalized into our framework. ¬∑ Fine-tuning LLMs means that using recommendation data to fine-tune LLMs as recommender systems. Considering cost and efficiency, researchers often use parameter-efficient finetuning ( e.g., LoRA [45]) to adapt to recommendation scenarios [4, 76, 186]. As for LLMs, open-source models based on LLaMA [131] are widely used, including but not limited to LLaMA, LLaMA2 [132], LLaMA3 [26] and Vicuna [13] with different parameter sizes. Based on whether candidate items are provided, existing work on fine-tuning LLMs can be further divided into two kinds of recommendation tasks. On the one hand, researchers have explored fine-tuning models for recommendation that provide candidate items such as rating, re-ranking and CTR predictions. Specifically, the fine-tuning interface of the closed-source model ChatGPT has brought new breakthroughs to the research of LLMs, and there have been attempts to fine-tune ChatGPT for recommendation tasks [67]. On the other hand, LLMs can also be fine-tuned for recall tasks of recommender systems by retrieving candidates from the whole item pool [3, 66, 79, 103]. Through well-designed indexing, alignment, and retrieval strategies, generating recommendation items without providing lengthy candidate sequences is suitable for practical application scenarios [10, 46]. ¬∑ Instruction tuning means providing template instructions of recommenders as prompts to tuning targeted LLMs, generally involving multiple recommendation tasks [14, 32, 105, 126, 176]. However, some fine-tuning methods ( e.g., TALLRec[4]) also involve the form of instructions. In order to avoid ambiguity, we consider the instructions of a single task template as fine-tuning settings, and the instructions of multiple tasks as instruction-tuning settings. As the backbone of recommender systems, researchers generally use T5 [108] or Flan-T5 [15] for various recommendation scenarios such as rating, ranking, retrieving, explanation generation and news recommendation. Besides Flan-T5, RecRanker [92] is also proposed to instruction tuning LLaMA2 [132] as the ranker for topùëò recommendation. By instantiating appropriate instructions for each recommendation task, this setting can also be summarized into our framework. 4.1.3 Model Architecture. For the architecture design of LLMs leveraged as recommender systems, we consider three mainstream architectures as summarized in [184], i.e., encoder-decoder , prefix decoder , and causal decoder . The recommendation scenarios for each framework are introduced. ¬∑ The encoder-decoder architecture adopts two Transformer blocks as the encoder and decoder, which is the basis of BERT [21]. In line with existing literature on LLM-based recommender systems, the bidirectional property of the encoder-decoder architecture allows LLMs to easily customize encoders and decoders towards recommendation ( e.g., dual-encoder considering both ids and text [105]), and conveniently adapt to multiple recommendation tasks [14, 32, 68, 105, 176]. ¬∑ The prefix decoder architecture is also the decoder-only architectures, which is known as non-causal decoder. It can bidirectionally encode the prefix tokens like the encoder-decoder architecture, and perform unidirectional attention on the generated tokens like the causal decoder architecture. One of the representative LLMs based on the prefix decoder architecture is ChatGLM[2], and researchers have attempted to explore the recommendation performance of ChatGLM as one of the benchmarks in related work [87]. Note that the variants of ChatGLM, i.e., ChatGLM2 and ChatGLM3 employ the causal decoder architecture. , Vol. 1, No. 1, Article . Publication date: January 2025. Leveraging Large Language Models as Recommender Systems 17 ¬∑ The causal decoder architecture has been widely adopted in various LLMs, and the series of GPT [5] and LLaMA [131] are the most representative models. It uses the unidirectional attention mask, and only decoders are deployed to process both the input and output tokens. Due to the popularity of the causal decoder architecture, most LLM-based recommender systems employ this framework to adapt to different recommendation tasks such as CTR predictions and sequential recommendation. 4.1.4 Parameter Scale. To meet the diverse needs of different users, the most typical variant of LLMs is the parameter scale. In general, open-source models have multiple parameter sizes to choose from, and larger parameter sizes generally mean better capabilities [131, 184]. But meanwhile, the corresponding computational and spatial complexity will also increase. Considering the memory and efficiency issues for experiments, researchers in the field of LLM-based recommender systems generally use LLMs with parameters no more than 10B (B is short for billion and the same below), while the performance of LLMs with larger parameters remains to be further explored in the field of recommender systems [52]. 4.1.5 Context Length. Another property closely related to user needs is the length of the input context. The inability to handle inputs with longer contexts means that decision-making cannot be accurately developed, thereby limiting the model capabilities [132, 184]. When the input exceeds the limited length, the input will be truncated, so sufficient context length is crucial for the user experience [17, 44, 94]. However, different lengths of context inputs imply different model architectures and parameters. When expanding the context length of a model, it often leads to higher time and memory complexity. To address the length limitation of LLMs, existing methods either selectively discard previous contexts using sliding windows [160], or only sample a portion of the context for retrieval augmentation [57, 78], or employ small models without emergence ability. Despite recent strategies, the limitation of context length has not yet been truly resolved. Considering economic and efficiency issues, existing LLMs including open-source and closed-source models only provide a limited number of context length options. In this paper, we mainly focus on several classic lengths, i.e., 2K, 4K, 8K, 16K, 32K and 128K (K is the abbreviation for one thousand, the same below).",
  "4.2 Research Questions and Experimental Setup": "4.2.1 Research Questions. In this section, we conduct experiments to verify the impact of different factors of LLMs on recommendation results focusing on the following four Research Questions (RQ). ¬∑ RQ1: What are the performance differences between LLMs and traditional recommenders? ¬∑ RQ2: How do different attributes of LLMs, including the public availability, model architectures, parameter scales, and context lengths affect the performance and inference time? ¬∑ RQ3: What are the similarities and differences in recommendation results with different tuning strategies? Is the LLM after instruction tuning more suitable for recommendation? ¬∑ RQ4: What are the limitations of leveraging LLMs as recommender systems? 4.2.2 Experimental Setup. This section considers two prevalent recommendation scenarios based on the tuning status of LLMs. To address RQ1, RQ2, and RQ4, we evaluate the zero-shot ranking efficacy of LLMs by using prompts as illustrated in Fig. 2(a). In the context of RQ3, we fine-tune LLaMA-based LLMs to act as CTR predictors, with the corresponding data sample presented in Fig. 2(b). Details regarding the experimental configurations are available in Section 3.3. 4.2.3 Evaluated Models. For empirical analysis, we assess the following baselines and LLMs. , Vol. 1, No. 1, Article . Publication date: January 2025. 18 Lanling Xu et al. ¬∑ Random : Random baseline recommends the ùëò (k=20 in this section) candidate items in a random order, which is the basic situation to evaluate the metric values of each dataset. ¬∑ Pop : Pop ranks the candidate items based on their interaction times in the training set. We consider it as the fully-trained method since it uses the statistical information of datasets. ¬∑ BPR [113]: BPR is one of the typical traditional models that utilize matrix factorization for recommendation. It is trained in the pair-wise paradigm a.k.a., BPR training loss without considering temporal information. ¬∑ SASRec [51]: SASRec is a sequential recommendation model based on the backbone of the classic self-attention network a.k.a., Transformer [133], and achieves comparable performance among sequential models. It utilizes the unidirectional attention mask for model training and achieves comparable performance among sequential models. ¬∑ ChatGPT : ChatGPT is a closed-source large-scale pre-trained language model developed by OpenAI, and we adopt the version on June 13, 2023, the same as GPT-4. We explore two context lengths of ChatGPT, i.e., 4K and 16K. ¬∑ GPT-4 [99]: GPT-4 is a closed-source LLM with the context length of 8K. Building upon the success of ChatGPT, represents the next evolutionary leap in LLMs by OpenAI, delivering unprecedented capabilities in natural language processing. ¬∑ GPT-4o [98]: GPT-4o is a state-of-the-art closed-source LLM developed by OpenAI, released in 2024. It features a 128K token context window, significantly enhancing its ability to handle extended conversations and complex tasks requiring long-term memory. ¬∑ GPT-4o mini : GPT-4o mini is a cost-efficient version of the GPT-4o model [98], designed to deliver high-performance natural language processing capabilities with reduced computational requirements. ¬∑ OpenAI o1 mini : OpenAI o1 mini is a closed-source lightweight reasoning model designed for applications that prioritize reasoning over broad factual knowledge, offering faster response time than GPT-4o. Both GPT-4o mini and OpenAI o1 mini support a 128K token context window. ¬∑ Flan-T5 [15]: Flan-T5 is an open-source language model based on the encoder-decoder architecture T5 released by Google [108]. Flan-T5 is extended from T5 by a multi-task finetuning paradigm i.e., instruction tuning to enhance the generalization of different tasks. There are multiple variants of Flan-T5 in terms of parameters, including Flan-T5-Small (80M), FlanT5-Base (250M), Flan-T5-Large (780M), Flan-T5-XL (3B) and Flan-T5-XXL (11B). Since the first three models are too small to meet the requirements of LLMs discussed in this paper (1B, B is short for billion and the same below), we consider the Flan-T5-XL and Flan-T5-XXL for comparison. All variants of Flan-T5 have a 512-token context length limit. ¬∑ ChatGLM [172]: ChatGLM is an open-source bilingual dialogue LLM that supports both Chinese and English, based on the General Language Model (GLM) [172] with the prefix decoder architecture. The team released the second version ChatGLM2 and the third version ChatGLM3 in June 2023 and October 2023, respectively. Note that ChatGLM2 and ChatGLM3 have replaced the model architecture with a casual encoder. All three versions of open-source ChatGLM have a parameter scale of 6B. As for the context length, ChatGLM-6B supports 2K tokens while ChatGLM2-6B can cover 32K tokens at one time. Moreover, ChatGLM3-6B has both 2K and 32K versions. ¬∑ LLaMA [131]: LLaMA is an open-source language model introduced by MetaAI from the causal decoder architecture with four sizes (7B, 13B, 33B and 65B). Due to its outstanding performance and low computational cost, LLaMA has received much attention from researchers so far. , Vol. 1, No. 1, Article . Publication date: January 2025. Leveraging Large Language Models as Recommender Systems 19 ¬∑ Vicuna [13]: Vicuna [13] is one of the most popular variants by extending LLaMA , and it has a context length of 4K tokens with two parameter scales (7B and 13B). ¬∑ LLaMA2 [132]: LLaMA2 is the next generation of LLaMA released in July, 2023. We also conduct experiments of the chatting version of LLaMA2 ( i.e., LLaMA2-chat) [132], and it is specifically tuned for the dialogue scenario by Reinforcement Learning with Human Feedback (RLHF). Both LLaMA2 and LLaMA2-chat have three parameter scales (7B, 13B and 70B), and the context length is upgraded from 2K in LLaMA to 4K in LLaMA2. ¬∑ LLaMA3 [26]: LLaMA3 is an evolution in the LLaMA series developed by Meta, released in 2024. It features a dense Transformer [133] architecture with up to 405 billion parameters and supports a context window of 128K tokens, enabling it to handle long and complex text sequences effectively. We mainly use two parameter scales of LLaMA3 (8B and 70B). ¬∑ Qwen2.5 [128]: Qwen2.5 is a highly advanced series of LLMs developed by Alibaba, featuring a context length of up to 128K tokens and supporting outputs of up to 8K tokens, with model sizes ranging from 0.5B to 72B parameters, and we employ three parameter scales (7B, 32B and 72B). ¬∑ DeepSeek-V3 [19]: DeepSeek-V3 is an open-source LLM developed by DeepSeek, featuring a Mixture-of-Experts (MoE) architecture with 671 billion total parameters, of which 37 billion are activated per token. It also supports a 128K token context window. ¬∑ DeepSeek-R1 [19]: DeepSeek-R1 is a reasoning model developed by DeepSeek with longchain reasoning capabilities and transparent slow-thinking process. DeepSeek-R1 is expected to be open-sourced in subsequent iterations, and we treat it as an open-source model in Table 4. The results are derived from calls made through the official platform 3 .",
  "4.3 Observations and Discussion": "4.3.1 LLMs Compared to Traditional Recommenders (RQ1). Compared to traditional models based on collaborative filtering of interacted data in fully-trained settings, we provide the cold-start recommendation performance of LLMs in zero-shot settings. In what follows, we introduce the empirical findings on the recommendation effect of different models from three aspects, i.e., recommendation performance , the impact of historical item sequences and inference time . ¬∑ Recommendation performance of LLMs . As shown in Table 4, we provide the fully-trained results of four traditional methods, as well as the zero-shot recommendation performance of various LLMs. For traditional recommenders, BPR [113] based on collaborative filtering is significantly better than Pop based on popularity, and the sequential recommendation model SASRec [51] combined with attention mechanism and temporal information is significantly better than BPR, which is consistent with the results in existing literature [51, 113, 188]. It is worth noting that for the 20 candidate items, LLMs that rely on natural languages cannot completely recall all items in most cases, and several models with poor abilities can only output a dozen items, greatly limiting the accuracy of recommendation results. Therefore, 'coverage (recall@20)' indicates the ability of LLMs to memorize, re-rank and output candidate items, and several approaches such as the re-generation method [67] and probability distribution outputs [171] have been proposed to improve the recall performance. While for the traditional models, all candidate items can be recalled ( i.e., coverage equals to 1). Because the candidate items are not recalled completely, the recommendation effect of a few LLMs ( e.g., Flan-T5 and ChatGLM) is not even as good as the random baseline. DeepSeek-R1 performs worse than DeepSeek-V3, indicating that the long-chain reasoning capabilities and slowthinking process of LLMs are optimized for Mathematics and coding tasks, which is not necessarily suitable for the zero-shot recommendation scenario. It might be perplexing to observe that the 3 The URL of DeepSeek API: https://chat.deepseek.com/ , Vol. 1, No. 1, Article . Publication date: January 2025. 20 Lanling Xu et al. Table 4. Overall performance of different models on recommendation. We consider fully-trained settings for traditional models and zero-shot settings for LLMs. 'IT' denotes the average inference time for each user measured in seconds (s). The best results are highlighted in bold , while the runner-up results are underlined. Qwen2.5 model [128] performs well on the NDCG@10 metric, yet all three parameter scales yield , Vol. 1, No. 1, Article . Publication date: January 2025. Leveraging Large Language Models as Recommender Systems 21 Fig. 3. The recommendation performance of LLMs w.r.t. the number of historical item sequences. As in 2(a), we use recently interacted items to model user interest in the zero-shot ranking task. To compare the impact of historical item sequences on LLMs, we illustrate the ranking performance of the traditional sequential recommender SASRec [51] and three closed-source LLMs with different numbers of historical items. 1 5 10 20 30 40 50 The number of historical items 0.30 0.45 0.60 0.75 NDCG@10 ChatGPT 4k ChatGPT 16k GPT-4 8k SASRec (fixed) (a) MovieLens-1M 1 5 10 20 30 40 50 The number of historical items 0.30 0.45 0.60 0.75 NDCG@10 ChatGPT 4k ChatGPT 16k GPT-4 8k SASRec (fixed) (b) Amazon Books equally poor results on NDCG@1. This discrepancy arises because Qwen2.5 tends to provide lengthy explanations in its output, which significantly reduces the top-1 accuracy during output parsing. When evaluating different versions of the same model, a general trend is observed where newer versions tend to outperform their predecessors. For instance, LLaMA3 [26] typically surpasses LLaMA2 [132], and ChatGLM3 [2] is generally superior to ChatGLM2. However, anomalies have been noted that ChatGLM outperforms ChatGLM2 on the Amazon Books dataset, and LLaMA-7B achieves a higher NDCG@10 score compared to LLaMA2-7B on the MovieLens-1M dataset. The underlying reason is that LLMs with parameters less than 10 billion often struggle to comprehend the prompts in recommendation tasks adequately, frequently resulting in outputs that do not meet the required format. We will explore this issue in detail through case studies in Section 4.3.4. For most LLMs, the zero-shot recommendation performance is not as good as the baseline method Pop based on popularity of interactions in the dataset [17, 44, 86]. However, the powerful LLMs like ChatGPT and LLaMA-70B (chat) [132] can achieve better results than Pop in zero-shot settings. Furthermore, GPT-4 and GPT-4o can perform better than the fully-trained matrix factorization model BPR on the MovieLens-1M dataset, and DeepSeek-V3 outperforms BPR on two datasets, indicating the potential of LLMs to serve as the backbone of recommender systems. In addition, the significant differences between the results of LLMs demonstrate the importance of selecting appropriate LLMs for downstream recommendation tasks [52, 86, 87]. ¬∑ The impact of historical item sequences . As for ranking tasks in Fig. 2(a), the recently interacted historical items are used as the user representations. However, there is no standard value for the number of items that represent users. To analyze the impact of historical item sequences on the recommendation performance, we conduct experiments to explore the recommendation effect of the sequential recommender SASRec [51] and the closed-source LLM ChatGPT with different numbers of historical interactions. For the fully-trained SASRec, the maximum length of the historical item sequence will affect the model framework and prediction results [43, 51]. To ensure the model uniformity, we fix the model checkpoint with the historical item sequence of 50 as the 'SASRec (fixed)' for comparison, and evaluate the recommendation performance (NDCG@10 [48]) with different numbers of historical items. For ChatGPT and GPT-4, we get zero-shot results with different items to verify whether the powerful LLMs can deal with the long context for recommendation. In Fig. 3, with the increasing number of historical items, the results of SASRec , Vol. 1, No. 1, Article . Publication date: January 2025. 22 Lanling Xu et al. improve steadily, while the performance of LLMs changes little. Consistent conclusions on two datasets can be drawn that even if LLMs can accept more historical items for user representations, increasing the number of historical items does not bring significant gains in the recommendation performance. The performance trends of LLMs show that the increased historical item sequence is not fully utilized by the language model. This phenomenon may be attributed to the fact that the longsequence capabilities of LLMs have not been specifically optimized for zero-shot recommendation tasks. If targeted optimizations were applied to enhance the ability of LLMs to handle task-specific prompts, it is reasonable to expect that improvements in performance could be achieved. To further improve the mining of user interest for LLMs, approaches such as retrieval augmentation [78] and prompting strategies [146, 167] can be used, which will be analyzed in Section 5. ¬∑ Inference time of LLMs . In the actual deployment of recommender algorithms, the inference efficiency is the decisive factor for industrial applications [125, 134]. In general, the inference time of models is closely related to the size of parameters. As shown in the last column of Table 4, for the lightweight traditional recommenders, the inference time of SASRec is about 1 second for each user. However for LLMs, except that closed-source models cannot accurately obtain inference time due to limitations of the API, the inference time of open-source models takes nearly 10 or more seconds for one prediction, leading to an unacceptable time delay in practical applications.",
  "Observations on the Overall Recommendation Performance of LLMs": "¬∑ In zero-shot scenarios, LLMs have cold-start capabilities, and GPT-4o even surpasses collaborative filtering models. However, all LLMs are inferior to fully-trained sequential recommendation models. ¬∑ The long-chain reasoning capabilities and slow-thinking process of LLMs are optimized for Mathematics and coding tasks, which is not necessarily suitable for the zero-shot recommendation scenario. ¬∑ Even if LLMs can accept more historical items for user representations, increasing the number of historical items does not bring significant gains in the recommendation performance, while traditional recommenders can better utilize longer sequences. It also shows that the user interest should be effectively modeled rather than blindly stacking historical items. ¬∑ The inference time of LLMs for recommendation is unacceptable for applications. 4.3.2 LLMs on Recommendations w.r.t. Four Aspects (RQ2). For different LLMs, differences in public availability and model architecture will lead to different recommendation scenarios, results and inference time [52, 87]. For the same LLM, the parameter scale and context length also affect the efficiency and effectiveness of language models [184]. Therefore, we explore the impact of different LLMs on recommendations from four aspects, namely public availability , model architecture , parameter scale and context length as follows. ¬∑ Public availability . As shown in Table 4, closed-source models achieve significantly better results than the open-source models in the cold-start scenario, but they cannot outperform fullytrained sequential models. The results of the closed-source LLMs can be further improved through approaches like retrieval augmentation, which we will discuss in Section 5.2. In terms of LLMs, ChatGPT with zero-shot settings has comparable recommendation performance with the fullytrained Pop especially on the sparse Amazon Books dataset, indicating the fundamental ability of LLMs on recommendation tasks. Furthermore, the upgraded GPT-4 and GPT-4o exceed ChatGPT by a large margin due to its strong zero-shot generalization ability. The superior zero-shot performance of GPT sheds lights on leveraging LLMs for recommendation. Although DeepSeek-V3 has the , Vol. 1, No. 1, Article . Publication date: January 2025. Leveraging Large Language Models as Recommender Systems 23 Fig. 4. The recommendation performance and inference time of LLMs w.r.t. the parameter scale. We compare the different parameter scales of LLMs, and the larger the symbol in the figure, the larger the parameter scale. 0 50 100 150 200 Inference Time (s) 0.1 0.2 0.3 0.4 0.5 0.6 NDCG@10 7B 33B 65B 13B 13B 70B 7B 13B 70B 1B 3B 8B 70B LLaMA Vicuna LLaMA2 LLaMA2 chat LLaMA3 (a) MovieLens-1M 0 50 100 150 200 Inference Time (s) 0.1 0.2 0.3 0.4 NDCG@10 7B 13B 33B 65B 7B 13B 7B 13B 70B 7B 13B 70B 1B 3B 8B 70B LLaMA Vicuna LLaMA2 LLaMA2 chat LLaMA3 (b) Amazon Books comparable recommendation performance with GPT-4o, the open-source models usually get poor results compared to GPT-4o in the zero-shot settings. The reason is that open-source models lack comprehensive cold-start capabilities, and their strength lies in the ability to integrate domain knowledge through strategies such as prompt tuning. In line with previous studies on LLMs [44, 52, 86, 94], employing a closed-source model in cold-start scenarios yields better results, while an open-source model is more flexible and easy to use when tuning is needed [4, 66, 90, 92, 186]. ¬∑ Model architecture . For the model architecture , Flan-T5 [15] based on the encoder-decoder architecture has almost no ability to recommend items in the cold-start setting, as its training corpus does not involve specialized instructions of our task. Trained with prompts of recommendations, the encoder-decoder architecture is suitable for prompt tuning and instruction tuning [14, 32, 176]. Similarly, the first and first version of ChatGLM [172] performs poorly on the zero-shot ranking task, and is not as good as Vicuna and LLaMA2 based on the causal decoder. However, the third version of ChatGLM, i.e., ChatGLM3 has comparable recommendation performance with Vicuna [13] and LLaMA2 [132], which further indicates the importance of selecting an advanced foundation model. In terms of the series of LLaMA models [131], LLaMA3 achieves the best performance, and Vicuna is better than LLaMA, which is related to their training data and release time. Furthermore, the chat version of LLaMA2, i.e., LLaMA2-chat is a series that uses conversational dialog instructions to fine-tune LLaMA2 [132], which is more suitable for our tasks in ranking settings. Therefore, the results of LLaMA2-chat are significantly improved compared with LLaMA2, and LLaMA2-70B-chat and LLaMA3-70B achieve better performance than the closed-source model ChatGPT. Generally speaking, researchers prefer to study recommendation tasks based on the causal decoder framework such as LLaMA, and the latest version of LLaMA has a better generalization ability. ¬∑ Parameter scale . It is widely recognized that the larger the parameter size, the more powerful the LLMs [44, 52, 184], the same applies in the field of recommender systems. To compare the effect and efficiency of LLMs w.r.t. the parameter scale, we compare the recommendation performance and inference time of LLaMA [131], Vicuna [13], LLaMA2 [132], LLaMA2-chat and LLaMA3 [26] at different parameter scales in Fig. 4. As the scale of parameters enlarges, the recommendation performance and inference time of LLMs steadily increases, and both datasets (Fig. 4(a) and Fig. 4(b)) have consistent conclusions. Therefore, it is necessary to consider the trade-off between performance and efficiency when choosing the parameter scale. Moreover, the performance improvement on the , Vol. 1, No. 1, Article . Publication date: January 2025. 24 Lanling Xu et al. Fig. 5. The recommendation performance of LLMs w.r.t. the context length. We compare the different context length of LLMs, and the larger the symbol in the figure, the longer the context length. 0 6 12 18 24 30 36 Context length (k) 0.30 0.40 0.50 0.60 NDCG@10 4k 16k 2k 32k 8k ChatGPT ChatGLM3 GPT 4 (a) MovieLens-1M 0 6 12 18 24 30 36 Context length (k) 0.20 0.30 0.40 0.50 0.60 NDCG@10 4k 16k 2k 32k 8k ChatGPT ChatGLM3 GPT 4 (b) Amazon Books increasing scale of LLaMA2 and LLaMA3 is more significant than that of LLaMA, indicating that the scaling law of LLMs has to do with the capabilities of the base model. ¬∑ Context length . Different LLMs have different maximum input limitations [131, 132, 184], and a longer context input means LLMs can accommodate more historical items for recommendation. However, it remains to be explored whether the maximum input length of LLMs will affect the recommendation results when the length limitation is not exceeded. Therefore, we conduct experiments to investigate the differences in the recommendation performance between the two length versions of ChatGPT (4K and 16K) and ChatGLM3 (2K and 32K). As shown in Fig. 5, expanding the length limitation of LLMs does not necessarily mean the better recommendation performance, while there is a slight decrease in NDCG@10. Furthermore, when the maximum input of LLMs remains unchanged, increasing the historical input of users results in insignificant gains as shown in Fig. 3. Therefore, the key to the recommendation problem is to enable LLMs to effectively utilize the information within the limited context [78, 167], and a suitable context length selection for LLMs as recommender systems is worthy of deep consideration.",
  "Observations on Recommendations w.r.t. Four Aspects of LLMs": "¬∑ As for the public availability, closed-source models generally outperform the opensource models in the recommendation performance, but have poorer flexibility. ¬∑ As for the model architecture, different frameworks are adapted to different recommendation tasks and fine-tuning strategies, while LLMs with the casual decoder architecture are still mainstream. ¬∑ As for the parameter scale, the larger the parameter scale, the better the recommendation performance. ¬∑ As for the context length, a longer maximum context length leads to worse recommendation results. 4.3.3 Comparisons of Tuning Strategies for LLMs (RQ3). Due to the fact that LLMs are not customized to recommender systems, it is insufficient to only consider the zero-shot recommendation performance in cold-start scenarios [36, 52, 87, 92]. In order to explore the impact of different , Vol. 1, No. 1, Article . Publication date: January 2025. Leveraging Large Language Models as Recommender Systems 25 Table 5. Overall performance of LLMs on CTR predictions. There are three settings, i.e., zero-shot setting without fine-tuning, parameter-efficient fine-tuning (PEFT) setting with a few parameters tuned, and finetuning (FT) setting with all parameters tuned. Table 6. Impact of LoRA hyper-parameters ( ùëü and ùõº represents the rank and scaling factor of LoRA, respectively) on the recommendation performance of LLaMA3-8B. Here, ùëü typically ranges from 4 to 64, and ùõº is usually set to twice the value of ùëü . Table 7. Impact of the dropout rate, a hyperparameter in LoRA, on the performance of LLaMA38B fine-tuned as a CTR recommendation model on the MovieLens-1M and Amazon Books datasets. training strategies of LLMs on recommendations, we compare the CTR prediction performance of five LLaMA-based LLMs on two datasets. Specifically, we consider three training settings of LLMs, i.e., the zero-shot setting without fine-tuning, Parameter-Efficient Fine-Tuning (PEFT) setting (we use the LoRA [45] here) with a few parameters tuned, and Fine-Tuning (FT) setting with all parameters tuned, and summarize empirical conclusions from three aspects: overall performance of different settings , the impact of instruction tuning , and the impact of training data . ¬∑ Overall performance of different settings . As shown in Table 5, we can see that the results of fine-tuning LLMs (PEFT and FT) on only 256 samples are significantly better than the zero-shot performance in cold-start scenarios, and empirical findings are consistent across four LLMs on both datasets. Furthermore, considering the two kinds of fine-tuning strategies, the performance of the fine-tuning setting is even better than that of the PEFT setting since more parameters are tuned [45]. In addition to recommendation effects, training efficiency is also a performance that deserves attention. Therefore, we compare the training time of the two fine-tuning strategies on two datasets. As shown in Fig. 6, the time for PEFT is significantly less than that for fine-tuning all parameters, which is in agreement with the existing literature [45, 184]. Therefore, the balance between efficiency and effectiveness needs to be further considered. ¬∑ The impact of instruction tuning . Before fine-tuning LLMs using the recommendation data, we are also concerned about whether the instruction tuning of LLMs using general data can further enhance the subsequent fine-tuning on recommendations. Therefore, we select two pairs of LLMs as controls, namely LLaMA and Alpaca-LoRA, as well as LLaMA2 and LLaMA2-chat. As for the Alpaca-LoRA model, it is the tuned version of LLaMA [131] using the the LoRA strategy [45] on the Alpaca dataset [127]. As for the LLaMA2-chat model, it is also the updated version of LLaMA2 , Vol. 1, No. 1, Article . Publication date: January 2025. 26 Lanling Xu et al. Fig. 6. Comparison of training time w.r.t. two fine-tuning strategies, i.e., PEFT and FT. The training time of LLMs is averaged over all training epochs, measured in seconds (s). Five LLaMA-based models are compared. LLaMA Alpaca-LoRA LLaMA2 LLaMA2-chat LLaMA3 0 50 100 150 200 250 300 Training Time Per Epoch (s) PEFT FT (a) MovieLens-1M LLaMA Alpaca-LoRA LLaMA2 LLaMA2-chat LLaMA3 0 100 200 300 400 500 600 Training Time Per Epoch (s) PEFT FT (b) Amazon Books Fig. 7. The fine-tuned performance of LLMs w.r.t. the number of sampled data, and LoRA is utilized. For LLaMA-based LLMs, we analyze the effect of sampled data on the recommendation performance (measured by NDCG@10) of few-shot learning, and compare with the traditional recommender SASRec [51]. 0 8 16 32 64 128 256 The number of sampled data 0.45 0.50 0.55 0.60 0.65 0.70 NDCG@10 SASRec LLaMA-7B Alpaca-LoRA-7B LLaMA2-7B LLaMA2-chat-7B LLaMA3.1-8B (a) MovieLens-1M 0 8 16 32 64 128 256 The number of sampled data 0.45 0.60 0.75 0.90 NDCG@10 SASRec LLaMA-7B Alpaca-LoRA-7B LLaMA2-7B LLaMA2-chat-7B LLaMA3.1-8B (b) Amazon Books using the RLHF strategy [132] on the conversational dataset. As illustrated in Table 5 and Fig. 7, the recommendation results of Alpaca-LoRA are better than that of LLaMA, and the performance of LLaMA2-chat outforms that of LLaMA2, indicating the effectiveness of performing fine-tuning on general instructions. That is to say, enhancing the general knowledge of language models can also improve the domain capabilities of LLMs on specified recommendation tasks [4]. ¬∑ The impact of hyper-parameters of LoRA . As depicted in Table 6 and Table 7, we analyze the CTR prediction performance of LLaMA3-8B with different hyper-parameters of LoRA. We can find that increasing the rank ( ùëü ) and scaling factor ( ùõº ) generally enhances the recommendation performance, likely due to the increased model capacity with more parameters tuned. Additionally, the impact of the dropout rate varies between datasets. For MovieLens-1M, a dropout rate of 0.2 yields the highest score of 0.6577, indicating that moderate dropout enhances generalization by preventing overfitting. While for Amazon Books, a dropout rate of 0.4 results in the best performance (0.8610), suggesting that the optimal dropout rate is context-dependent. These findings underscore the importance of meticulously tuning LoRA hyper-parameters to optimize performance of LLMs in CTR recommendation tasks. , Vol. 1, No. 1, Article . Publication date: January 2025. Leveraging Large Language Models as Recommender Systems 27 ¬∑ The impact of training data . For fine-tuning LMs with few-shot learning, we conduct experiments to explore the impact of training samples on the recommendation performance. As shown in Fig. 7, we visualize the fine-tuning results corresponding to different numbers of training samples with the LoRA strategy, and also consider the traditional recommender SASRec [51]. Despite a few sampled data, LLMs such as LLaMA-7B can quickly adapt to the task of CTR predictions, achieving noticeable improvements as the number of training samples increases. With only a small number of training samples, we can stimulate the appreciable recommendation ability of LLMs, reflecting their remarkable emergence ability. However, when the training samples increase from 0 to 256, the performance of the traditional recommendation model SASRec still remains 0.5. Experimental results show that LLMs have advantages over conventional recommenders on abilities of few-shot learning and adapting [4]. In addition, the comparison without sampled data in Fig. 7 also highlights the zero-shot cold-start ability of LLMs.",
  "Observations on Tuning Strategies of LLMs": "¬∑ For LLM-based recommendations, few-shot training results are better than the zeroshot performance, and fine-tuning all parameters is more effective but less efficient than parameter-efficient fine-tuning. ¬∑ The instruction tuning using general data can further enhance the fine-tuning results of LLMs on specified recommendation tasks. ¬∑ As for parameter-efficient fine-tuning LLMs as recommenders with LoRA, increasing the rank ( ùëü ) and scaling factor ( ùõº ) of LoRA generally enhances the recommendation performance due to the increased model capacity with more parameters tuned. ¬∑ In few-shot training scenarios, LLMs are more capable of adapting to recommendation tasks compared to traditional recommendation models. 4.3.4 Case Study of Limitations (RQ4). Despite powerful capabilities of LLMs, there are also some limitations using LLMs as recommender systems. To illustrate the limitations of LLMs for recommendation, we choose four recommendation cases of LLMs as zero-shot re-ranker in Fig. 8 to analyze the possible failure scenarios of LLMs. ¬∑ Position bias . It is widely acknowledged that the generation results of LLMs have randomness, leading to instability in recommendation results, and the position bias is a typical manifestation. As shown in Fig. 8(a), the LLM places the ground-truth movie ' The Mask ' at the bottom of ranking results, because ' The Mask ' is at the end of the candidate item sequence. When the order of candidate items is adjusted, the recommendation results will also change, indicating the position bias during recommendations. Existing literature has also found the position bias in LLM-based recommendations [44, 94, 176], and methods such as bootstrapping [44] and Bayesian probability framework [94] have been proposed to calibrate unstable results. ¬∑ Lack of domain knowledge . The lack of domain knowledge in recommendations may also lead to misunderstandings of LLMs. As shown in Fig. 8(b), the LLM places the second movie in the candidate list, i.e., ' Three Colors: White ' at the first position in the re-ranking result, which still indicates the position bias. Furthermore, the LLM places the ground-truth item ' Dick ' at the bottom of the ranking list. The possible reason is that relying solely on the movie title ' Dick ', LLMs cannot infer that the movie is a comedy film about two teenage girls, which has a greater similarity to the recently watched movie ' Showgirls ' by users. This case demonstrates that LLMs may make inappropriate decisions due to a lack of domain knowledge in the field of recommender systems. , Vol. 1, No. 1, Article . Publication date: January 2025. 28 Lanling Xu et al. ctions of users} eal Deal: A Novel purchase next: ries) ', ' 1. The Pain ental Assassin) ', bers ‚Ä¶‚Ä¶ 8 recommended the candidate list) ts) \" (Hallucination) t recommended] . I've watched the following movies {Historical interactions of users} Note that my most recently watched movie is Batman Forever . Now there are 20 candidate movies that I can watch next: [ '0. Two Moon Juction ', ' 1. Puppet Master 5: The Final Chapter ', ' 2 . Creature Comforts ', '3. You've Got Mail' , ' 4. Anatomy (Anatomie) ', ‚Ä¶‚Ä¶,' 18. Child's Play ', '19. The Mask' ] ‚Ä¶‚Ä¶ Please show me your ranking results with order numbers ‚Ä¶‚Ä¶ OpenAI_Logo.svg I've watched the following movies {Historical interactions of users} Note that my most recently watched movie is Batman Forever . Now there are 20 candidate movies that I can watch next: [ '0. Two Moon Juction ', ' 1. Puppet Master 5: The Final Chapter ', ' 2 . Creature Comforts ', '3. You've Got Mail' , ' 4. Anatomy (Anatomie) ', ‚Ä¶‚Ä¶,' 18. Child's Play ', '19. The Mask' ] ‚Ä¶‚Ä¶ Please show me your ranking results with order numbers ‚Ä¶‚Ä¶ OpenAI_Logo.svg 2023/9 8 19:35 file:/ Users/xulanling/Desktop/ ostgraduate/ ÁæéÂõ¢ /2023.9 8- ‰∏ªÂõæ /OpenAI_Logo.svg 1/ 1. \" You've Got Mail \" - This is a romantic comedy, similar to \" Notting Hill \" and \" High Fidelity \" from your watched list. 2. \" A Life Less Ordinary \" - This is a romantic comedy with a unique twist, which might appeal to you based on your history. ‚Ä¶‚Ä¶ 20. \" The Mask \" - This is a comedy, similar to \" Wayne's World \" and \" Mallrats \" from your watched list. 20. \" The Mask \" - This is a comedy, similar to \" Wayne's World \" and \" Mallrats \" from your watched list. LLM: You've Got Mail [rank 1] Ground-truth label: The Mask [rank 20] (Position Bias) (Position Bias) 202 /9 8 19:35 fil : Users xulanling De k op/ os graduate/ ÁæéÂõ¢ /2023.9 8- ‰∏ªÂõæ /OpenAI_Logo.svg 1/ 1. \" You've Got Mail \" - This is a romantic comedy, similar to \" Notting Hill \" and \" High Fidelity \" from your watched list. 2. \" A Life Le s Ordinary \" - This is a romantic comedy with a unique twist, which might appeal to you based on your history. ‚Ä¶‚Ä¶ LM: You've Got Mail [rank 1] Ground-truth label: The Mask [rank 20] (a) Case study of the position bias when leveraging GPT4 [99] as the zero-shot re-ranker on the MovieLens-1M dataset. LLMs tend to select items in front while the groundtruth movie appears at the end of the candidate list. GPT-4 is affected by the position bias of the candidate list. I've purchased the following books {Historical interactions of users} Note that my most recently purchased book is Landline: A Novel . Now there are 20 candidate books that I consider to purchase next: [' 0. Witches Protection Program ', '1. Maze Runner. Correr o Morir' , ' Catch of a Lifetime ', ' 3. The Queen's Accomplice: A Maggie Hope Mystery ', ‚Ä¶‚Ä¶, ' 19. A Love Like Ours (Porter Family Novels) '] ‚Ä¶‚Ä¶ Please show me your ranking results with order numbers ‚Ä¶‚Ä¶ 2. 1. \" Burned by Magic \" ‚Ä¶‚Ä¶ 17. \" Maze Runner \" 18. \" Jinx \" ‚Ä¶‚Ä¶ 27. \" Landline: A Novel \" LLM: Jinx [rank 18] Ground-truth label: Maze Runner. Correr o Morir [rank 17] (Output Format Not Understood) (The first 17 books are from the candidate list and the last 10 books are from the historical interactions.) 9 I've watched the following movies {Historical interactions of users} Note that my most recently watched movie is Showgirls . Now there are 20 candidate movies that I can watch next: [' 0. Love Bewitched, A (El Amor Brujo) ', ' 1. Three Colors: White ', ' 2. A Far Off Place ', '3. Fried Green Tomatoes', ‚Ä¶‚Ä¶, '14. Dick' , ‚Ä¶‚Ä¶, ' 18. Welcome To Sarajevo ' , ' 19. Getting Even with Dad '] ‚Ä¶‚Ä¶ Please show me your ranking results with order numbers ‚Ä¶‚Ä¶ OpenAI_Logo.svg I've watched the following movies {Historical interactions of users} Note that my most recently watched movie is Showgirls . Now there are 20 candidate movies that I can watch next: [' 0. Love Bewitched, A (El Amor Brujo) ', ' 1. Three Colors: White ', ' 2 Far Off Place ', '3. Fried Green Tomatoes', ‚Ä¶‚Ä¶, '14. Dick' , ‚Ä¶‚Ä¶, ' 18 Welcome To Sarajevo ' , ' 19. Getting Even with Dad '] ‚Ä¶‚Ä¶ Please show me your ranking results with order numbers ‚Ä¶ OpenAI_L o.svg 1. \" Three Colors: White \" 2. \" Fried Green Tomatoes \" ‚Ä¶‚Ä¶ 20. ' Dick ' (Dick is a comedy film released in 1999 that tells the story of two teenage girls who accidentally get caught up in the Watergate scandal and end up bringing down the President of the United States.) 1/ 20. ' Dick ' (Dick is a comedy film released in 1999 tha two teenage girls who accidentally get caught up in t scandal and end up bringing down the President of t 1/ LLM: Three Colors: White [rank 1] Ground-truth label: Dick [rank 20] (Lack of Domain Knowledge) (Lack of Dom 2023/9 8 19:35 file:/ Users/xulanling/Desktop/ ostgraduate/ ÁæéÂõ¢ /2023.9 8- ‰∏ªÂõæ /OpenAI_Logo.svg 1. \" Three Colors: White \" 2. \" Fried Green Tomatoes \" ‚Ä¶‚Ä¶ LLM: Three Colors: White [rank 1] Ground-truth label: Dick [rank 20] 2023/ 8 19:35 fil :/ U ers/xu a ling/Deskt p/ ostgr duate ÁæéÂõ¢ / 023.9 8- ‰∏ªÂõæ /OpenAI_L o.svg (b) Case study of the lack of domain knowledge when leveraging GPT-4 [99] as the zero-shot re-ranker on the MovieLens-1M dataset. 'Dick' is a comedy film about two teenager girls, which is relevant to the movie 'Showgirls', but GPT-4 ranks 'Dick' at the end of the list. I've purchased the following books {Historical interactions of users} Note that my most recently purchased book is Murder At The Rocks . Now there are 20 candidate books that I consider to purchase next: [' 0. Fired Up (Trouble in Texas) (Volume 2) ' , ' 1. Jarek (Scifi Alien Weredragon Romance) (Dragons of Preor) (Volume 1) ', ‚Ä¶‚Ä¶, ' 7. The World Beneath ', ‚Ä¶‚Ä¶, '19. Kiss of Death (Morganville Vampires)'] ‚Ä¶‚Ä¶ Please show me your ranking results with order numbers ‚Ä¶‚Ä¶ 1. \" Murder At The Rocks \" 2. \" Fired Up (Trouble in Texas) \" 3. \" Georgia on Her Mind \" ‚Ä¶‚Ä¶ 19. \" Áî∑Â≠©ÁöÑÁîüÊ¥ª \" 20. \" Á•ûÁßò‰ª£Á†ÅÔºö‰∏Ä‰∏™Â∞ëÂπ¥ÁßëÂ≠¶ÂÆ∂ÁöÑÁßòÂØÜ \" LLM: Murder At The Rocks [rank 1] Ground-truth label: The World Beneath [not recommended] (Hallucination) (The last two Chinese books are not on the candidate list) 9 (c) Case study of misunderstanding output formats when leveraging DeepSeek-R1 [19] as the zero-shot re-ranker on the Amazon Books dataset. The candidate list provides 20 books while DeepSeek-R1 recommends 27 books, of which 10 are from historical interactions. (d) Case study of hallucination when leveraging ChatGLM3-6B [2] as the zero-shot re-ranker on the Amazon Books dataset. ChatGLM3-6B ranks the purchased book first, leaves out the ground-truth book, and generates two Chinese books not within the candidate list. Fig. 8. Case study of limitations when leveraging LLMs as zero-shot recommender systems. ¬∑ Misunderstanding output formats . LLMs may exhibit limitations in retaining critical information regarding output formats from prompt instructions, potentially leading to responses that deviate from the intended requirements. As illustrated in Fig 8(c), while the input prompt explicitly requires the model to re-rank only 20 candidate books, the model generates 27 outputs, including 10 books from the historical interaction sequence. Furthermore, three books from the original candidate list are omitted in the output. This phenomenon directly contributes to the observed suboptimal performance in the coverage metric, as demonstrated in Table 4, where the output of LLMs frequently fails to achieve 100% coverage of the expected results. ¬∑ Hallucination . The hallucination problem in LLM-based recommender systems refers to the phenomenon where LLMs generate items that is not grounded in their training data or the provided input, often producing factually incorrect outputs. As depicted in Fig. 8(d), the LLM ranks the recently purchased book at the top of recommended list, and leaves out the ground-truth I've purch Note that Now ther [' 0. Witch Catch of a Mystery ', Please sh 1. \" Burne ‚Ä¶‚Ä¶ 17. \" Maze 18. \" ‚Ä¶‚Ä¶ 27. \" Landl LLM: Jinx Ground-tr Jinx \" , Vol. 1, No. 1, Article . Publication date: January 2025. Leveraging Large Language Models as Recommender Systems 29 book. Moreover, the LLM generates two Chinese books not within the candidate list. In LLMRS, hallucination can manifest as fabricated details of recommendation reasons, incorrect factual statements of items, or irrelevant content that deviates from the intended recommendation task.",
  "Observations on Limitations of LLMs": "¬∑ Despite capabilities, there are limitations when leveraging LLMs as recommenders. ¬∑ Results of LLMs have randomness, leading to instability such as the position bias. ¬∑ The lack of domain knowledge in recommendations can lead to misunderstandings. ¬∑ LLM-based recommender systems may suffer from the hallucination problem.",
  "5 PROMPT ENGINEERING SPECIALIZED FOR RECOMMENDER SYSTEMS": "Prompt is an important medium for interactions between humans and language models, and a well-designed prompt can better stimulate the powerful capabilities of LLMs [88, 118]. Improving the performance of artificial intelligence by designing and improving prompts is known as prompt engineering When leveraging LLMs as recommender systems, although the specific prompts in different studies may not be the same, the format is largely identical with only minor differences [17, 31, 44, 146]. In our work, we concentrate on the general framework of prompt engineering specialized for recommender systems, and summarize four key elements of prompt formats. Firstly, suitable task description is the primary condition for constructing prompts [32, 146]. As illustrated in Table 8, we list task-specific prompts for LLMs as recommender systems in our framework. Secondly, the characteristic of recommendation tasks lies in the mining and utilization of the user personalized interest, and user interest modeling is the essential aspect to reflect the true intentions and preferences of users [159, 167]. Thirdly, the purpose of recommender systems is to provide users with appropriate items, and the selection, matching, and generation of candidate items is a worthwhile issue to consider, i.e., candidate items construction [78]. Fourthly, prompting strategies are important for eliciting the planning and reasoning abilities of LLMs, which are the key to distinguishing LLMs from other language models [184]. In this section, we will discuss the key components of prompts in LLM-RS.",
  "5.1 Task Description": "Task description of recommendation tasks for LLM-RS is a key part of prompts that makes LLM understand the goal of specific recommendation assignments. Typically, based on the different recommendation objectives, existing recommendation tasks can be divided into the following categories: point-wise optimization for a single item ( e.g., binary cross entropy loss [112]), pair-wise optimization for positive and negative items ( e.g., Bayesian personalized ranking loss [113]), and list-wise optimization for multiple items ( e.g., variational auto-encoders [74]). For LLM-RS, we can adapt our work into multiple tasks by changing the description of prompts and the form of candidate items. That is to say, our framework can not only be used for re-ranking [44] and retrieving [66] tasks, but also support task forms such as point-wise prediction [52], pair-wise comparison [17] and list-wise re-ranking formats. As illustrated in Table 8, we list task-specific prompts for LLMs as recommender systems. ¬∑ Point-wise recommendation [12] regards the recommendation task as a binary classification problem, such as the rating scoring task [14, 32] and click-through rate (CTR) predictions [4, 52]. It generates a score or likelihood of preference for a given user-item pair with feature engineering. The recommender system ranks items solely based on the individual characteristics or historical interactions of users without considering the comparison and mutual influence between items in , Vol. 1, No. 1, Article . Publication date: January 2025. 30 Lanling Xu et al. Table 8. Task-specific prompts for LLMs as recommender systems, and we categorize them into two kinds of tasks, i.e., accuracy-focused recommendation task and beyond-accuracy recommendation task. the input ranking list. When utilizing LLMs for point-wise recommendation, the description mainly involves the user-item information with the range of score or answer list to limit output formats. ¬∑ Pair-wise recommendation [113] involves comparing two items in pairs to determine the relative preference for a particular user. Instead of focusing on individual items, it evaluates pairs of items and calculates the semantic distance between them. This method often creates item pairs, calculates relative scores, and then ranks or recommends items based on pair-wise comparisons. The description of pair-wise recommendation in prompts consists of a positive item and a negative item, instructing LLMs to give the answer from a binary choice list [17]. ¬∑ List-wise recommendation [74] involves optimizing an entire list of recommended items for a user. Instead of considering items individually or in pairs, this method treats the entire recommendation list as a single entity and captures the interior correlations within the list. It aims to create a ranked list of items that collectively maximizes user satisfaction. For LLM-based list-wise recommendations, the prompt contains a list of items and corresponding instructions, eliciting the ability of LLMs to explore the potential relationship within the item sequence [44, 94]. ¬∑ Matching [3, 66] in recommendation involves obtaining a small subset of candidates from the entire item pool for users, and candidate items are not provided in prompts at this stage. Due to the lack of domain knowledge in recommender systems, leveraging LLMs for item retrieval requires additional strategies such as model training, item indexing and grounding approaches [186]. The prompt for matching tasks includes the description of a user in different forms, e.g., textual profiles and user-item interaction histories, in order to quickly match suitable items. ¬∑ Ranking [92] in recommendation involves sorting or ordering items based on the predicted relevance or probability of interest to the target user. It aims to present the most relevant items at the top of the recommendation list. By analyzing historical user-item interactions, user preferences, , Vol. 1, No. 1, Article . Publication date: January 2025. Leveraging Large Language Models as Recommender Systems 31 item features, and the given cadidate item list, it requires LLMs to estimate the relevance of items and determine corresponding positions in the recommendation list [31, 69, 92].",
  "5.2 User Interest Modeling": "Compared to general tasks solved by LLMs, the characteristic of recommendation tasks lies in the mining and utilization of the user personalized interest [122, 167]. Users are influenced by multiple factors when making recommendation decisions, including but not limited to their longterm preferences, short-term intentions, market popular tendencies and occasional environmental biases [12, 41]. Among them, the most essential aspect is the interest of users, which is the purpose of recommender systems [40, 43, 188]. In this section, we first classify the user interest type (Section 5.2.1), outline the user representation forms (Section 5.2.2), and summarize existing modeling methods (Section 5.2.3). Then, we discuss approaches to modeling the user interest (Section 5.2.4), and conduct experiments to provide empirical analysis and key findings. 5.2.1 User Interest Type. In recommender systems, there are multiple ways to classify the user interest [35, 113, 188]. For example, based on the type of feedback between users and the platform, the user interest can be divided into the explicit interest ( e.g., ratings) and implicit interest ( e.g., clicks). Although the explicit feedback can reflect the true intentions of users, the sparse and expensive data limits its application scenarios. In general, we mainly explore user interest modeling in LLMs under implicit feedback scenarios. Another classification of the user interest is based on the time duration and stability, i.e., short-term interest , long-term interest , and hybrid interest as follows: ¬∑ Short-term interest refers to the sudden and accidental intentions or tendencies of personalized users in recent interactions, which is prone to change and can be influenced by environmental factors [44, 78]. That is to say, short-term interest is recent, temporary, and variable [176]. ¬∑ Long-term interest refers to the stable preferences of users towards certain content, themes, and elements, which is not easily changed in the short term and will continue to affect recommendation decisions of users [122, 146]. In contrast to short-term intentions, long-term interest is long-term, sustained and stable. ¬∑ Hybridinterest meansthe combination of long-term preferences and short-term intentions [28, 72, 83]. On the one hand, short-term intentions are dominated by long-term preferences [171]. On the other hand, long-term preferences consist of short-term interest across periods [68, 105]. 5.2.2 User Representation Forms. In LLM-RS, user interest modeling should consider both the input form for the prompt template and the specific storage form for the interest memory [146]. Generally, there are three types of input contents for interest modeling in LLMs: historical item lists , interest descriptions and user embeddings . Corresponding to different representation forms of the user interest, there are also various storage forms in the interest memory. ¬∑ Historical item lists refer to representing personalized users based on the sequence of historical interacted items, which is widely used in session-based recommendation and sequential recommendation [43, 51, 188]. For token-based item indexing, the ID sequence of items is used for user modeling in LLMs, similar to traditional sequential recommendation models [32]. However, without fine-tuning, LLMs cannot recognize the meaning of corresponding IDs. For descriptionbased item indexing, existing researches generally concatenate the attributes ( e.g., titles) of items in the temporal order, and then input the item sequence as text descriptions to LLMs as user representations [44, 59, 72]. Only the item IDs that the user has interacted with need to be stored as the interest memory. The list of items can be stored in the form of numpy or tensor arrays. Despite the simplicity and effectiveness, text attributes such as titles cannot fully represent items with ambiguity [59]. At the same time, item sequences inevitably contain noise information [151], and limited sequence lengths make it difficult to accurately express the user interest. , Vol. 1, No. 1, Article . Publication date: January 2025. 32 Lanling Xu et al. ¬∑ Interest descriptions refer to representing users based on textual descriptions, which is more applicable to the text input of LLMs [167]. That is to say, we can use natural languages to model the user interest in the form of text and input them into LLMs [159]. To facilitate the storage and retrieval of textual descriptions, vector stores are commonly used [132]. In LLMs, vector stores in the form of key-value pairs can connect implicit vector embeddings with explicit textual descriptions, improving the retrieval efficiency and comprehension ability [66, 103]. However, the key lies in how to mine and describe the user interest, and this is what we should make efforts to discuss. ¬∑ User embeddings refer to concatenating embeddings of users to the input of LLMs as user representations [179], which is often used for efficient fine-tuning of language models. In this case, we assign each user a unique ID and corresponding embedding, and add the vector representations of users to the input of LLMs for fine-tuning. At the same time, user embeddings can also be trained from small-scale traditional recommendation models [177], thereby incorporating collaborative filtering features from other users. However, it is worth noting that vector embeddings of user representations are only suitable for scenarios where the domain knowledge can be injected into LLMs, and the black-box property of embeddings also poses challenges to the explainability. 5.2.3 Modeling Methods. As illustrated in Fig. 9, we classify methods for modeling interest as i.e., memory-based methods , retrieval-based methods and generation-based methods . ¬∑ Memory-based methods assign external memory to users for storing interest-related historical information [122, 159]. As for user representations, the encoded interest of users can be obtained from the memory [177], and LLMs are instructed to utilize the memorized interest with carefully designed prompts. The iterative updating of memories is the key to memory-based methods. As for personalized memory, there are three important operations [136]: (1) memory reading is to obtain contents of the interest memory based on the user identifier. (2) Memory writing is to augment new interest of users based on the latest interactions between users and items. (3) Memory reflection is the periodic examination and updating of existing issues in the memory. Strategies for memory reflection include but are not limited to self-summarization, self-correction, and reflection based on user feedback [145]. It is memory reflection that makes memory not just a stack of historical records but a summary of user interest [167]. Note that memory-based methods specifically refer to obtaining contents from the memory without further processing procedures. ¬∑ Retrieval-based methods add a module for personalized interest retrieval on top of the memory-based methods, utilizing retrieval strategies to obtain user interest from the personalized memory [47, 78, 146]. As for the personalized query, there are generally three ways to form a query for retrieval: (1) candidate items as search queries [78] for relevant items, (2) recently interacted items as search queries for similar items [175], and (3) the user profile as search queries for personalized items [146]. For the criteria of retrieval, there are multiple trade-offs, including dimensions such as relevance, recency, and diversity with respect to the user interest [136]. ¬∑ Generation-based methods utilize the generation capabilities of LLMs to summarize, infer, and derive comprehensive user interest [92, 135, 167]. Generally, a combination of memory-based and retrieval-based methods is required for generation-based methods. For generative language models, the user interest induced by generative retrieval can further stimulate the prompting ability of LLMs [89, 173]. The generative approach can also be used for data augmentation, leveraging the general knowledge of LLMs to augment textual descriptions of the user interest. In current recommendations, LLMs mainly utilize the retrieved relevant items to generate summarized interest descriptions, which can also serve as a strategy for the memory reflection [122, 146]. 5.2.4 Research Questions and Observations for Modeling User Interest. In this section, we explore methods for user interest modeling in LLM-RS w.r.t. the following two research questions. , Vol. 1, No. 1, Article . Publication date: January 2025. Leveraging Large Language Models as Recommender Systems 33 Fig. 9. Comparison of three modeling methods of user interest, i.e., memory-based methods, retrieval-based methods, and generation-based methods. Interest memory consists of global and personalized memory. Interest Memory Memory-based Modeling Methods Retrieval-based Modeling Methods Generation-based Modeling Methods Personalized Memory Short-term Interest : Global Memory User Profile Age: 24 Gender: female Prefer: story that resonates with me Recently Interacted Item Interested in skiing scenes and slapstick Dataset Information : Item Pool 1. Toy Story, a beloved ‚Ä¶ 2. Jumanji is a thrilling ‚Ä¶ 3. Grumpier Old Men is ‚Ä¶ xxx category is popular Modeling Methods of User Interest LLMs Read Interface Interest Memory User Identifier Read Prompts Retrieval Interface Interest Memory User Query Retrieval Prompts Identifier User Interest Interest LLMs Retrieval Interface Interest Memory User Query Retrieval Prompts/ Feedback Identifier Interest LLMs LLM-Refined Interest Generate Memory Update Long-term Interest : Read & Retrieval Reflection & Update ¬∑ RQ1: For retrieval-based modeling methods, how to design query and memory to enrich the expression of user interest? ¬∑ RQ2: How do different methods of user interest modeling affect the recommendation results? In this section, we conduct experiments for the zero-shot re-ranking task as illustrated in Fig. 2(a), and ChatGPT is employed as the zero-shot re-ranker. For RQ1, to analyze how to select recent and relevant items as the representation of user interest, we adopt the retrieval-based strategy to retrieve items from the long-term interest. Specifically, following the memory-based and retrieval-based strategy, the personalized query, e.g., candidate items, recently interacted items or the user profile, is encoded into vectors in a unified form. The most relevant items are retrieved from memory based on semantic similarity. Meanwhile, we set weights based on the recency of interactions so that the recent items have a higher probability of being retrieved. Finally, the corresponding retrieved text is obtained based on key-value pairs in the memory. Here, we explore the design of the personalized query and memory of users for item retrieval, and study the influence of different variants as follows. ¬∑ Variants of the personalized query : as for the personalized query, we utilize the interest of users for retrieving items, and compare two strategies for constructing the query. - Short-term interest : we utilize the short-term interest for the memory retrieval of the long-term interest. The summarized text based on 10 recently interacted items is employed as the query for retrieving items. - Long- and short-term interest : we first summarize the personalized profile of each user based on all items he or she has interacted, then concatenate the user profile and the summarized text based on 10 recently interacted items for the memory retrieval of the long-term interest. We utilize the long-term user profile and short-term interest for the memory retrieval of the long-term interest. ¬∑ Variants of the memory : as for the personalized memory, we also consider two variants. , Vol. 1, No. 1, Article . Publication date: January 2025. 34 Lanling Xu et al. - Global memory : we utilize descriptions of items in the datasets as the memory. Since all users in a dataset share the same item space, the memory w.r.t. item descriptions is not the personalized memory but global memory. - Personalized memory : for each item that a user interacts with, we instruct LLMs to generate personalized descriptions based on the rating score or comment text as personalized memories. Therefore, the description of the same item varies among personalized users, which is different from the global memory. For RQ2, to examine the impact of modeling forms on the user interest, we design 10 modeling forms according to short-term interest, long-term interest and hybrid interest as follows:",
  "¬∑ Variants of short-term interest modeling forms :": "- Recent items : we use titles of the recent 10 items as the short-term interest. - Personalized interest of recent items : we construct the personalized memory for each user based on the user-item interactions, and use the personalized descriptions of recently interacted items as the short-term interest. - Recent items with personalized interest : we employ both titles of the recent 10 items, and the personalized text of recent items to prompt LLMs as the short-term interest. - Recent items with LLM-generated short-term interest : we utilize the least-to-most prompting strategy as mentioned in Section 5, which summarizes the interest text based on the recently interacted 10 items, then concatenate the summarized text and the 10 items. ¬∑ Variants of long-term interest modeling forms : - Retrieved items from user history, recent items as query : it retrieves 10 relevant items from the long-term interest. Since recent items have a higher probability to be retrieved. - Retrieved personalized interest, recent items as query : it also retrieves 10 relevant items from the long-term history like 'Retrieved items from user history, recent items as query', but this variant uses the personalized descriptions of items from the personalized memory. - LLM-generated summarization of retrieved personalized interest : it employs the retrieved personalized text from long-term history in the last variant by summarizing the retrieved 10 texts into one description. Not providing 10 specific items for LLMs, this variant only summarizes the user interest through personalized text. ¬∑ Variants of hybrid interest modeling forms : - Recent and retrieved relevant items : it uses both the retrieved 10 items from long-term history and the recent 10 items as the representation of user interest, considering both the long-term and short-term interest. It is worth noting that the retrieved items in this variant do not include the latest 10 items. - Retrieved personalized interest, profile and recent items as query : it retrieves recent and relevant items from the long-term interest, and for the query, both the personalized user profile and short-term interest are utilized. - Recent items with LLM-generated long-term interest : it concatenates the user profile summarized by LLMs and the recently interacted 10 items for interest modeling. 5.2.5 Observations and Discussion. As shown in Table 9 and Table 10, we summarize the following findings for modeling the user interest in LLM-RS: (1) The design of query and memory for retrieval-based modeling methods (RQ1) . We conduct an experiment to explore the retrieval strategies for recent and relevant items, and evaluate the selection of query and memory for item retrieval. As shown in Table 10, we compare four combinations with queries and memories. In terms of the query, combining the design of long-term and short-term , Vol. 1, No. 1, Article . Publication date: January 2025. Leveraging Large Language Models as Recommender Systems 35 Table 9. Performance comparison of user interest modeling forms across three modeling methods. The red color indicates short-term interest modeling, blue indicates long-term interest modeling, and purple indicates hybrid interest modeling. Evaluation metrics N@1 and N@10 represent NDCG@1 and NDCG@10, respectively, and the same below. Following the experimental setup outlined in Table 4, we evaluate the zero-shot recommendation performance of ChatGPT on the MovieLens-1M and Amazon Books datasets. Table 10. Performance comparison of query and memory designs for retrieval-based methods. We use retrieved relevant items from memory as user interest in prompts. The impact of query forms (short-term interest vs. long- and short-term interest) and memory forms (global memory vs. personalized memory) is analyzed. interest yields better results than only the short-term interest. When selecting items that represent interest for recommendation, the query needs to consider both recent tendencies and long-term preferences of users. Our results have shown that the personalized long-term interest of users can have a positive effect on retrieval for interest modeling. In terms of the memory, the design of the personalized memory with user interest outperforms the global memory with general descriptions. This result is in line with features of personalized recommender systems [122, 146], indicating the importance of personalization when designing the user interest for LLMs [175]. (2) The impact of different modeling forms of the user interest (RQ2) . In Table 9, we compare the user interest modeling forms across three modeling methods (memory-based, retrieval-based and , Vol. 1, No. 1, Article . Publication date: January 2025. 36 Lanling Xu et al. generation-based), and highlight three interest types (short-term, long-term and hybrid interest) with three colors. From the perspective of modeling methods, the recommendation performance of retrieval-based and generation-based methods is generally better than that of memory-based methods, indicating the importance of Retrieval-Augmented Generation (RAG) from the interest memory. Meanwhile, on the MovieLens-1M dataset with dense interactions, generative summarization based on short-term interest can further improve the effect, while on the Amazon books dataset with sparse interactions, retrieval-based modeling methods achieve the best, indicating that the retrieval results cannot be completely replaced by summative sentences. On the other hand, long-term interest and short-term interest can complement each other, and combining the two may yield better results. Instead of concatenating the long-term and short-term sequences, one possible combination approach is to use the short-term interest as queries to retrieve the long-term interest, serving as the domain knowledge for LLMs to model user preferences.",
  "Observations of Modeling User Interest": "¬∑ While selecting recent and relevant items, it is preferable to combine long-term and short-term user profiles with personalized descriptions for query and memory. ¬∑ The recommendation performance of retrieval-based and generation-based methods is generally better than that of memory-based methods, indicating the importance of Retrieval-Augmented Generation (RAG) from the interest memory. ¬∑ Long-term interest and short-term interest can complement each other, and combining the two may yield better results. Retrieving long-term preferences based on shortterm intentions is a suitable way for forming hybrid interest descriptions, and more approaches can be further explored.",
  "5.3 Candidate Items Construction": "5.3.1 Procedures of Candidate Items Construction. When leveraging LLMs as recommender systems, a crucial problem is that LLMs tend to recommend items not in the dataset [32]. One of the approaches is to provide a limited number of candidate items for model selection. On the one hand, the source of candidate items determines the upper limit of personalized recommendation accuracy [44, 176]. On the other hand, the form of candidate items reflects the specific recommendation task and exerts huge impacts on performance of LLMs for recommendation [17]. As shown in Fig. 10, we will discuss the main procedures of processing candidate items, including the source, representation and grounding of candidate items, and then conduct experiments on the effects of candidate items. Fig. 10. Main procedures of candidate items construction in LLM-RS, including the source, representation and grounding of candidate items. Prompts Output Item Pooll Source of Candidate Items Representation of Candidate Items Grounding of Candidate Items Filtering and Retrieval Token-based / Description-based Generative / Descriminative results , Vol. 1, No. 1, Article . Publication date: January 2025. Leveraging Large Language Models as Recommender Systems 37 ¬∑ Source of candidate items . In practical industrial applications, recommendation is a multistage process, including two kinds of typical stages, i.e., the recall stage and the re-ranking stage [3, 35, 113]. The recall stage in recommender systems is used for preliminary filtering and selection from the entire set of item candidates, while the re-ranking stage is used for readjusting the selected candidates, which is more suitable for LLMs as the re-ranker [44, 94]. Therefore, the source of candidate items to be ranked is crucial for the results of LLMs for recommendation. Popular approaches for selection of candidate items include traditional recommendation models [171] and retrieval algorithms [78], both of which provide a list of potentially recommended items. ¬∑ Representation of candidate items . In the construction of prompts, we need to use historical items to represent users and provide candidate items for LLMs to rank, both of which involve item indexing [32, 79]. However, there is a gap between the item representation in LLMs and recommendation, so the indexing of items between LLMs and recommender systems becomes the key and difficult point of LLM-RS. There are three typical ways to index items: 1) token-based identifiers, 2) description-based identifiers and 3) hybrid identifiers [139, 186]. For token-based identifiers, researchers often use numerical IDs to identify items [32]. Since token-based identifiers are widely utilized in traditional recommendation model, they are naturally treated as an itemindexing way for aligning LLMs with recommendation tasks. For description-based identifiers, researchers generally use the title or other textual attributes of the item as an identifier, and formalize it into natural language as input [3, 17, 86]. Due to the richness of natural language, description-based identifiers have high readability and flexibility. For hybrid identifiers, considering collaborative and textual information of users or items, LLMs can perceive both token-based and description-based identifiers [66, 76]. Current work using hybrid identifiers can be divided into different types. Some of them add item sequence with textual description and ID information respectively into different positions of prompts [179], while others combine token-based and description-based embeddings together for each item in sequences through concatenation [66]. ¬∑ Grounding to candidate items . After constructing candidate items for recommendation, how to ground the output of LLMs into recommendation item lists is a crucial problem. Methods for grounding to candidate items are associated with different types of recommendation tasks. For discriminative recommendation, which provide candidate item lists in the prompts, it is easy for LLMs to follow instructions and generate the output right within the candidate item lists through elaborately designed prompts [72, 76]. For generative recommendation tasks, which do not explicitly provide candidate item list in prompts but require further processing for the output, recent work indicates effective approaches for mapping the output of LLMs into candidate item lists [186]. For example, GPT4Rec [60] first generates hypothetical 'search queries' using titles of recently interacted items, and then retrieves similar items from the item pool by BM25 algorithm. E4SRec [66] directly utilizes the nearest neighbor search between the output of LLMs and the vectors in the item linear projection component for grounding to candidate items. In this case, the recommendation process will not suffer from length limitation problems since they choose not to include candidate item lists into the input of LLMs. As shown in Figure 1, our framework involves two mapping processes between the recommendation and language space: (1) when using LLMs to represent users, items in the recommendation space need to be used as historical sequences. (2) When re-ranking candidate items, it is required to provide candidate items in the recommendation space for LLMs. The key issue of semantic space alignment is the indexing and grounding of items. As for the item indexing, token-based sequences and description-based natural languages are two typical forms. As for the grounding methods, the generative method renders LLMs to output labels of all candidate items, and other mapping strategies such as logits distribution and similarity calculation can also be considered [66, 171]. , Vol. 1, No. 1, Article . Publication date: January 2025. 38 Lanling Xu et al. Table 11. Performance comparison on the grounding forms of LLMs w.r.t. candidate items. Token-based identifiers represent items numerically, while the description-based identifiers denote textual titles. Table 12. Performance comparison of traditional recommendation methods with and without ChatGPT as a re-ranker. '+ LLM' denotes the recommendation results of LLMs by re-ranking the top-20 candidate items of traditional models. 'Impr.' denotes the percentage improvement (or degradation) achieved by LLMs. 5.3.2 Research Questions and Experimental Setup. In this section, we explore the effect of candidate items on LLM-RS, and provide empirical findings on the following research questions: ¬∑ RQ1: Whether representations of items lead to similar recommendation performance? ¬∑ RQ2: Given a list of candidates recalled by traditional recommenders, can LLMs improve the recommendation results by further re-ranking the given candidates? We explore the performance differences between the following two item representation methods. Basic prompts used in this section is illustrated in Fig. 2(a), and ChatGPT is employed as the zero-shot re-ranker. ¬∑ Token-based identifiers : we can assign serial numbers or indicators ( e.g., ABCD) to the candidate items, and instruct LLMs to only output the re-ranked identifiers. ¬∑ Description-based identifiers : we instruct LLMs to directly output the complete text e.g., titles of candidate items, and the re-ranking results are parsed from the textual output. Moreover, researchers tend to adopt a two-stage recommendation paradigm, in which conventional recommenders initially recall multiple candidates, and subsequently, LLMs evaluate and rank these candidates to provide enhanced recommendations. This approach effectively reduces the computational load of LLMs, enabling more efficient deployment. However, there is still no consensus regarding the effectiveness of LLMs in reranking the candidates retrieved by traditional recommendation models [44, 111]. In order to address this, we conduct experiments to compare the impact of ChatGPT in re-ranking the original recommendation results generated by traditional recommenders. We evaluate three classical recommenders, including Pop, BPR [113], and SASRec [51]. , Vol. 1, No. 1, Article . Publication date: January 2025. Leveraging Large Language Models as Recommender Systems 39 5.3.3 Observations and Discussion. Findings of candidate items construction are as follows. (1) Grounding forms of candidate items (RQ1) . Firstly, we focus on the grounding forms of candidate items. As shown in Table 11, we can see that whether in the movie dataset or the book dataset, the complete name of the output item is more convenient for extracting and grounding recommendation results than the specified identifier. In other words, description-based identifiers outperform Tokenbased identifiers when grounding candidate items for LLMs [44, 184]. However, whether it is ID-based or description-based, the inference time of the grounding strategy based on generative output will increase with the increasing number of candidate items. A feasible solution is to use one prediction to obtain the probability score on all candidate items based on the output logits [171]. Language models are more sensitive to the textual output rather than pure identifiers, but we can also use more advanced index strategies and grounding methods to improve the accuracy. (2) The evaluation for re-ranking abilities of LLMs (RQ2) . Secondly, we discuss the performance difference between traditional algorithms and re-ranking results after LLMs. As shown in Table 12, LLMs improve results of the traditional recommendation method BPR on two datasets by a large margin. As for the basic method Pop and SASRec, the re-ranking effect of LLMs varies between the two datasets. For the MovieLens-1M dataset in the movie domain, our initial prompts cannot instruct LLMs to achieve better results than traditional methods such as Pop and SASRec. The possible reason is that this movie dataset is very dense, and fully-trained collaborative filtering signals are more important than general knowledge of movies. While in the Amazon Books dataset, the re-ranking of LLMs can further improve results of Pop and SASRec, possibly because the general knowledge of LLMs for books can effectively adapt to the sparse recommendation data. The overall experimental results indicate that the re-ranking performance of LLMs for recommendation results varies depending on specific recommendation methods and datasets. However, it is acknowledged that the general knowledge of LLMs can complement the collaborative signals of traditional models, tapping the potential to improve traditional recommendation algorithms [65, 179].",
  "Observations of Candidate Items Construction": "¬∑ LLMs may exhibit limitations in retaining critical information regarding the candidate list from prompt instructions, potentially leading to responses that deviate from the intended requirements. ¬∑ Description-based identifiers outperform token-based identifiers when grounding candidate items. ¬∑ Retrieving candidate items by traditional recommendation models first, and then re-ranking items by LLMs can further improve the results.",
  "5.4 Prompting Strategies": "With the construction of prompts for recommendation tasks, prompting strategies are leveraged to further elicit the general abilities of LLMs in text comprehending and problem solving [88, 118, 184]. In this section, we first summarize prompting strategies specialized for recommender systems, and provide empirical findings on two research questions. 5.4.1 Prompting Strategies Specialized for Recommender Systems. The planning ability emerging from LLMs is the key to distinguishing them from other language models [184]. In general tasks with LLMs, researchers use methods such as Chain-of-Thought (CoT) prompting to stimulate the logical reasoning ability of LLMs for solving complex problems [23, 85]. As for LLM-RS, leveraging LLMs for recommender systems can also employ prompting strategies to further improve the performance of recommendations [83, 167]. In addition, due to the user and item settings of the , Vol. 1, No. 1, Article . Publication date: January 2025. 40 Lanling Xu et al. recommendation task, the prompting strategy needs to be customized based on the specific user needs in recommender systems. In this paper, we concentrate on typical prompting strategies for recommendation tasks, i.e., zero-shot prompting , few-shot prompting , recency-focused prompting , role prompting , chain-of-thought prompting and self-prompting strategy , and more advanced planning strategies in LLM-RS are left for future exploration [88]. ¬∑ Zero-shot prompting is the basic scenario and common form among various prompting strategies. Zero-shot prompting directly provides the context information and task description for LLMs without reference examples. The task description and prompt formats vary in line with different recommendation tasks [17, 86, 146]. ¬∑ Few-shot prompting is opposite to zero-shot prompting, and it provides a few demonstrations in prompts to help LLMs better understand the user intention [5]. While for recommendation tasks, the matching of provided examples with the current interest of personalized users determines the effectiveness of few-shot prompting. Therefore, it is worth discussing how to provide reliable context for subsequent recommendations [44]. ¬∑ Recency-focused prompting is first proposed by [44] based on the fact that the next predicted item in recommendations has a greater correlation with the recently interacted item than other historical items. Therefore, explicitly emphasizing the recently interacted items in prompts is also a practical prompting strategy. In our initial prompt, recency-focused prompting is used by default. ¬∑ Role prompting refers to playing a 'role-playing' game with the language model. In the recommendation task, we can specify the role of LLMs as the recommender system to serve users in a targeted manner, adding auxiliary information to describe the role of recommenders in detailed prompts such as conversational recommender systems [42, 50, 143]. ¬∑ Chain-of-thought prompting is also known as the CoT prompting, which is widely applied in reasoning tasks such as question answering and mathematical inference [149]. CoT prompting can elicit the ability of LLMs to solve problems step by step [54], or further explicitly decompose the reasoning and analysis process of the task using least-to-most prompting strategy [187]. In the field of recommender systems, explicit steps can also be provided manually by researchers to assist in solving recommendation tasks [135]. For LLM-RS, we not only focus on the basic CoT prompts, but also explore the role of custom-designed step-by-step prompts. ¬∑ Self-prompting strategy means that the knowledge for answering questions can be obtained by prompting LLMs multiple times. LLMs can be required to generate relevant knowledge, providing necessary information for concepts in the original problem [85]. Meanwhile, the randomness and self consistency [144] generated by LLMs allow them to generate multiple inference chains, and then use the majority voting method on the results obtained from all chains as final predictions. When using LLMs to the re-ranking stage, there are many possible combinations of candidate items, and the recommendation results from the same LLM with the same prompt can be totally different. Therefore, bootstrapping with multiple trials is a scientific guarantee to reduce bias [44, 67, 94]. 5.4.2 Research Questions and Experimental Setup. In this section, we explore the construction and design of prompting strategies for LLM-RSon the following two research questions: ¬∑ RQ1: When prompting LLMs a zero-shot recommender systems, what are the performance differences of different prompting strategies? ¬∑ RQ2: When fine-tuning LLMs as recommender systems, what are the performance differences of different prompting strategies? In the scenario of the sequential re-ranking task based on Fig. 2(a), we conduct experiments with prompting strategies illustrated in Fig. 11, and the LLM is ChatGPT: ¬∑ Original : it is the basic prompt used in our experiments as illustrated in Figure 2(a). , Vol. 1, No. 1, Article . Publication date: January 2025. Leveraging Large Language Models as Recommender Systems 41 Basic Prompts Prompting Strategies for Sequential Re-ranking Task item 1 ‚Ä¶‚Ä¶ Historical Interacted 10 Items Recency-focused Prompting Note that my most recently interacted is item 10 . Predicted Item item 9 item 10 item 11 Role Prompting 20 3/9 8 19:35 OpenAI_Log .svg file:/ Users/xulanli g/Desktop/ ostgraduate/ ÁæéÂõ¢ /20 3.9 8- ‰∏ªÂõæ /OpenAI_Log .svg 1/ You are an excellent recommender system to recommend the next item based on my historical behavior sequence. CoT Prompting (step-by-step) Let's think step by step. item 1 ‚Ä¶‚Ä¶ Predicted Item item 9 item 10 item 11 20 3/9 8 19:35 OpenAI_Log .svg Please summarize my recent preference based on my historical interactions. CoT Prompting (least-to-most) It seems that you enjoy a mix of ‚Ä¶‚Ä¶ 20 3/9 8 19:35 OpenAI_Log .svg file:/ User /xulan i g/Desktop/ ostgraduate/ ÁæéÂõ¢ /20 3.9 8- ‰∏ªÂõæ /OpenAI_Log .svg 1/ Based on my recent preference you summarize, now there are 20 candidate ‚Ä¶‚Ä¶ ICL (self instruction) ICL (demonstration from others) item 1 ‚Ä¶‚Ä¶ item 9 item 11 Historical Interacted 10 Items Demonstration item 10 Prediction Historical 9 Items item 1 ‚Ä¶‚Ä¶ item 10 item 1 ‚Ä¶‚Ä¶ item 10 item 11 ? Target Sequence Sequence from Similar Users file:/ Users/xulanli g/Desktop/ ostgraduate/ ÁæéÂõ¢ /20 3.9 8- ‰∏ªÂõæ /OpenAI_Log .svg 1/ Fig. 11. Examples of prompting strategies for LLMs in the scenario of the sequential re-ranking task based on Fig. 2(a). 'ICL' denotes In-Context Learning and 'CoT' stands for Chain-of-Thoughts. ¬∑ w/o Recency-focused prompting : we remove prompts on the recently interacted item. ¬∑ w/ Role prompting : we add the role-playing description to the original prompt. ¬∑ w/o CoT (step-by-step) : we remove the magical spell 'let's think step by step' in the original prompt to observe the performance differences. ¬∑ w/ CoT (least-to-most) : we obtain recommendation results by prompting LLMs twice. First, LLMs are prompted to summarize the personal preference of users based on the recently interacted item sequence. Then in the second stage, we concatenate the summarized profile to the original prompt for recommendation. ¬∑ w/ ICL (self instruction) : we use the last interaction of the same user as the example for ICL [44]. ¬∑ w/ ICL (demonstration from others) : we retrieve an example from historical sequences of other users for the current user, and add the demonstration to the original prompt. For the selection of examples, we encode recent items of users by textual representations, and search for similar user sequences based on the inner product between vectors. In the scenario of fine-Tuning LLMs as the CTR predictor based on Fig. 2(b), we employ LLaMA38B as the LLM and LoRA is utilized for PEFT. As for prompting strategies of fine-tuning LLaMA3-8B, we devise four prompts as follows: ¬∑ Implicit prompting : it is the basic prompt used in our experiments as illustrated in Fig. 2(b). According to the rating threshold, we divide the items users interact with into likes and dislikes, and only provide the implicit feedback for LLMs as prompts. ¬∑ Explicit prompting : compared to implicit prompting, explicit prompting does not divide items, and provides LLMs with explicit feedback of ratings scores. An example is provided in the third row of Table 14. ¬∑ Hybrid implicit and explicit prompting : based on implicit prompting, hybrid prompting further adds ratings of the target user for each provided item. An example is provided in the fourth row of Table 14. ¬∑ CoT prompting : we add the magical spell 'let's think step by step' to the basic prompt of implicit prompting. 5.4.3 Observations and Discussion. Findings on prompting strategies of LLMs are listed as follows: , Vol. 1, No. 1, Article . Publication date: January 2025. 42 Lanling Xu et al. Table 13. Performance comparison on prompting strategies in Fig. 11 when leveraging ChatGPT as re-ranker. ' w/o ' denotes that we remove related descriptions compared to the original prompt, and ' w/ ' denotes that we add corresponding instructions to the prompt. Table 14. Performance comparison of prompting strategies for fine-Tuning LLaMA3-8B on the CTR prediction task. The model is fine-tuned using LoRA, and the recommendation performance is evaluated using the accuracy metric on the MovieLens-1M (Movie) and Amazon Books (Book) datasets. (1) The impact of prompting strategies on prompting LLMs (RQ1) . As shown in Table 13, we compare different prompting strategies for ChatGPT as the zero-shot re-ranker, and report results based on the classification of zero-shot prompting and few-shot prompting. As for the zero-shot prompting strategies, we can see that removing the recency-focused prompting sentence ( w/o recency-focused) largely decreases the recommendation performance, demonstrating the key role of recently interacted items in recommendation. Although we provide historical items in chronological order based on timestamps, LLMs still needs explicit guidance to understand the importance of recent items, indicating that heuristic knowledge in the recommendation field needs to be supplemented for LLMs. Meanwhile, adding the role prompting descriptions to the original prompt ( w/ role prompting) significantly improves the performance of zero-shot prompting, which shows that role-playing and expert-like prompts can better leverage the capabilities of LLMs in specific fields or tasks [184]. In addition, an important strategy for zero-shot prompting is the CoT, and we compare effects of two kinds of CoT prompting. When we remove the basic prompting , Vol. 1, No. 1, Article . Publication date: January 2025. Leveraging Large Language Models as Recommender Systems 43 sentence 'let's think step by step' in the original prompt, the recommendation effect is improved, possibly due to the fact that following the step-by-step prompts is not conducive to the extraction of results from textual outputs. It also indicates that specific problem decomposition may be required for recommendation tasks rather than general prompts, and the superior results of summarizing recent interest (least-to-most prompting strategy) confirm this finding. In contrast to zero-shot prompting, the typical representative of few-shot prompting is ICL based on contextual examples, and we study the ICL results considering demonstrations from the current user (self) and other users (others), respectively. It can be seen that using examples of the target user is better than demonstrations from other users, indicating the personalized needs of each user. However, the strategy of few-shot prompting has insignificant advantage compared to the zero-shot prompting, and more advanced planning strategies such as automatic prompting [73] and soft prompting [110] shed lights on LLM-based recommender systems. (2) The impact of prompting strategies on fine-tuning LLMs (RQ1) . As shown in Table 14, we compare four prompting strategies for fine-Tuning LLaMA3-8B with LoRA on the CTR prediction task, highlighting the importance of prompt design in optimizing LLMs. Explicit prompting is the most effective strategy since it provides LLMs with quantifiable user preferences, enabling LLMs to capture nuanced user-item interactions. In contrast, Implicit prompting that relies on two-category data of user preferences gets poor results, suggesting that the lack of explicit guidance may hinder the capabilities of LLMs to infer user interest accurately. The hybrid implicit and explicit prompting strategy, while combining elements of both approaches, does not surpass explicit prompting, indicating that it is not scientific to set the threshold of likes and dislikes artificially. CoT prompting with step-by-step reasoning demonstrates competitive performance, suggesting that incorporating reasoning pathways can enhance the decision-making process when fine-tuning LLMs. These results emphasize that the choice of prompting strategy is not merely a technical detail but a critical factor influencing the effectiveness of fine-tuning LLMs. Future work could explore more sophisticated hybrid approaches or adaptive prompting techniques that dynamically adjust based on the complexity of the input or the task at hand. Additionally, investigating the interplay between prompting strategies and model architectures could provide deeper insights into optimizing LLMs for recommendation tasks [126, 168].",
  "Observations of Prompting Strategies": "¬∑ Although provided with historical items in chronological order, LLMs still needs explicit guidance to understand the importance of recent items. ¬∑ Role-playing and expert-like prompts can better leverage the capabilities of LLMs. ¬∑ In chain-of-thought prompting, specific problem decomposition is required for recommendation tasks rather than general prompts. ¬∑ The few-shot prompting strategy has insignificant advantages in recommendations. ¬∑ When fine-tuning LLMs as recommender systems, the explicit feedback ( e.g., ratings) is better than the implicit feedback ( e.g., interactions) to elicit capacities of LLMs.",
  "6 CONCLUSION": "This paper aims to provide a comprehensive exploration of Large Language Models (LLMs) to serve as recommender systems. It presents a systematic review of the advancements made in LLM-based recommendations, generalizing related work into multiple scenarios and tasks in terms of LLMs and prompts. We also conduct extensive experiments on two public datasets to investigate empirical findings for recommendation with LLMs. Our objective is to assist researchers in gaining , Vol. 1, No. 1, Article . Publication date: January 2025. 44 Lanling Xu et al. a deeper understanding of the characteristics, strengths, and limitations of LLMs leveraged as recommender systems. Considering the significant progress in LLMs, the development of LLMbased recommendations holds the potential to better align the powerful capabilities of LLMs with the evolving needs of intended users in the field of recommender systems. By addressing current challenges, we hope that our work will contribute to the advancement of LLM-based recommendations and serve as an inspiration for future research efforts. Last but not least, we outline promising directions for future research in leveraging LLMs for recommendation. ¬∑ Efficiency optimization of LLMs for recommendation . The key limitation of leveraging LLMs in industrial recommender systems is efficiency[63, 156, 185], including considerations of both time and space. On the one hand, the fine-tuning and inference efficiency of LLMs cannot compare to traditional recommendation models [44, 186]. While techniques such as parameterefficient fine-tuning can aid in keeping LLMs updated in a computationally efficient manner, recommender systems need to iterate continuously over time, i.e., incremental learning. Frequent updates of LLMs inevitably bring spatial and temporal burdens to recommender systems [121]. On the other hand, billions of parameters in LLMs also pose challenges for the lightweight deployment of recommendation algorithms [131, 132]. Therefore, efficiency optimization of LLMs utilized as recommender systems is one of the prerequisites for large-scale applications, which has widespread application prospects and scientific research values [47, 121, 134]. ¬∑ Knowledge distillation of LLMs for recommendation . Since LLMs as recommenders are limited by efficiency, another feasible approach is to distill [65, 125] the recommendation capabilities of LLMs into lightweight models, striking a balance between efficiency and effectiveness. Specifically, knowledge distillation is a classic compression method adopted in recommender systems [125, 130], with the core idea of guiding lightweight student models to 'imitate' teacher models with better performance and more complex structures such as LLMs. In recommender systems, the collaborative optimization of LLMs and recommendation models can also be seen as the distillation process to inject knowledge from LLMs to traditional recommenders, enhancing representations of both users and items [65, 105, 159]. Due to the fact that knowledge distillation can improve efficiency while retaining the capabilities of LLMs for recommendation, more applications need to be fully explored. ¬∑ Multimodal recommendations with LLMs . In addition to IDs and text, multimodal recommendations with LLMs hold considerable promise and warrant comprehensive exploration with the evolving landscape of media consumption [164, 181]. The essence of multimodal recommendations resides in the fusion of textual and visual information for enhanced user engagement [38, 101], and the dual functionality of LLMs is pivotal in this context. LLMs possess the capability to function as multimodal LLMs, enabling the incorporation and encoding of visual information extracted from images. Furthermore, images can be transformed into textual representations by multimodal encoders first, and LLMs are mainly used for the subsequent integration of diverse modalities [38]. In addition, the rich multimodal attributes also provide a basis for diversified recommendation results [79]. As the field progresses, an emphasis on the reproducibility, benchmarking, and standardization of evaluation datasets and metrics will be essential to foster a cohesive and informed advancement in multimodal recommender systems leveraging LLMs [33, 150]. ¬∑ Fairness-aware recommendations of LLMs . Future research in the domain of fairness-aware recommendations with LLMs presents a compelling avenue for scholarly inquiry [18, 69, 138, 174]. In line with existing work, researchers have explored that retrievers in information retrieval are biased towards contents generated by LLMs [18], and LLMs utilized as recommender systems also output unfair recommendation results [69, 138, 174], necessitating a profound investigation into methodologies that ensure equitable and unbiased outcomes for LLM-based recommendations. It is imperative to scrutinize existing fairness-aware algorithms and develop novel approaches that cater to the intricacies of language models, particularly in understanding how fairness metrics align , Vol. 1, No. 1, Article . Publication date: January 2025. Leveraging Large Language Models as Recommender Systems 45 with user expectations. As LLMs continue to evolve, the research community must collaborate to establish standardized evaluation metrics and benchmarks for fairness-aware recommendations, ensuring the reproducibility and comparability of findings across diverse studies in the era of LLMs. In essence, fairness-aware recommendations with LLMs are poised to contribute substantially to the development of ethical and equitable recommender systems [138, 174]. ¬∑ General-purpose LLMs in the vertical field of recommender systems . Developing a comprehensive framework for LLMs to address multiple recommendation tasks represents a significant avenue for scholarly exploration [14, 32, 146, 176]. The endeavor involves formulating a unified and versatile structure that accommodates the intricacies of various recommendation tasks [17], encompassing diverse modalities and user preferences [38, 79]. In addition, using agents of LLMs to simulate recommendation scenarios and make dynamic decisions is also a feasible application of general-purpose LLMs [136, 137, 175]. Research efforts should focus on refining the architecture, training methodologies, and adaptability of such a general framework to ensure optimal performance across different recommendation domains. Investigating transfer learning techniques within this framework, enabling the transfer of knowledge between recommendation tasks, is crucial for enhancing efficiency and leveraging shared information [4, 176]. In general, general-purpose LLMs for recommendation have the potential to revolutionize recommender systems by providing a unified and scalable solution capable of addressing the multifaceted challenges. ¬∑ Privacy and ethical concerns . Prior studies [120, 152] have highlighted the potential issue of language models generating unreliable or personal contents based on certain prompts and insecure instructions. Recommendation systems involve massive amounts of the user data [35, 97], and it is crucial to remove private and potentially harmful information stored in LLMs to enhance the privacy and security of LLM-based applications [7, 56]. Notably, researchers have observed that Reinforcement Learning from Human Feedback (RLHF) and model editing techniques [34] have the potential to restrain the generation of poisonous or harmful contents from LLMs, thereby mitigating privacy and ethical concerns associated with privacy-preserving recommender systems [7]. Nevertheless, it is imperative to acknowledge that the alignment technology of LLMs may be vulnerable to misuse. The apprehension exists that LLMs could be manipulated by malicious users to selectively influence agents and amplify specific viewpoints within recommender systems. Consequently, addressing the security and privacy implications of LLM-based recommendations is crucial, and there is a need to formulate public regulations to mitigate potential risks [152].",
  "ACKNOWLEDGMENTS": "The authors would like to thank Chuyuan Wang and Chenrui Zhang for participating in discussions of this paper. Lanling Xu is supported by Meituan Group during her research internship. Xin Zhao is the corresponding author.",
  "REFERENCES": "[1] Saurabh Agrawal, John Trenkle, and Jaya Kawale. 2023. Beyond Labels: Leveraging Deep Learning and LLMs for Content Metadata. In RecSys . ACM, 1. [2] Zhipu AI. 2024. ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools. arXiv e-prints 2406.12793 (2024). [3] Keqin Bao, Jizhi Zhang, Wenjie Wang, Yang Zhang, Zhengyi Yang, Yancheng Luo, Fuli Feng, Xiangnan He, and Qi Tian. 2023. A Bi-Step Grounding Paradigm for Large Language Models in Recommendation Systems. arXiv e-prints 2308.08434 (2023). [4] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation. In RecSys . ACM, 1007-1014. [5] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, and et al. 2020. Language Models are Few-Shot Learners. In NeurIPS . , Vol. 1, No. 1, Article . Publication date: January 2025. 46 Lanling Xu et al. [6] Michael K. Buckland and Fredric C. Gey. 1994. The Relationship between Recall and Precision. J. Am. Soc. Inf. Sci. 45, 1 (1994), 12-19. [7] Aldo Gael Carranza, Rezsa Farahani, Natalia Ponomareva, Alex Kurakin, Matthew Jagielski, and Milad Nasr. 2023. Privacy-Preserving Recommender Systems with Synthetic Query Generation using Differentially Private Large Language Models. arXiv e-prints 2305.05973 (2023). [8] Diego Carraro and Derek Bridge. 2024. Enhancing Recommendation Diversity by Re-ranking with Large Language Models. ACM Transactions on Recommender Systems abs/2401.11506 (2024). [9] Jin Chen, Zheng Liu, Xu Huang, Chenwang Wu, Qi Liu, Gangwei Jiang, Yuanhao Pu, Yuxuan Lei, Xiaolong Chen, Xingmei Wang, and et al. 2024. When large language models meet personalization: perspectives of challenges and opportunities. World Wide Web (WWW) 27, 4 (2024), 42. [10] Runjin Chen, Mingxuan Ju, Ngoc Bui, Dimosthenis Antypas, Stanley Cai, Xiaopeng Wu, Leonardo Neves, Zhangyang Wang, Neil Shah, and Tong Zhao. 2024. Enhancing Item Tokenization for Generative Recommendation through Self-Improvement. arXiv e-prints 2412.17171 (2024). [11] Yuxin Chen, Junfei Tan, An Zhang, Zhengyi Yang, Leheng Sheng, Enzhi Zhang, Xiang Wang, and Tat-Seng Chua. 2024. On Softmax Direct Preference Optimization for Recommendation. arXiv e-prints 2406.09215 (2024). [12] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, and et al. 2016. Wide & Deep Learning for Recommender Systems. In DLRS@RecSys . ACM, 7-10. [13] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality. https://lmsys.org/blog/2023-03-30-vicuna/ [14] Zhixuan Chu, Hongyan Hao, Xin Ouyang, Simeng Wang, Yan Wang, Yue Shen, Jinjie Gu, Qing Cui, Longfei Li, Siqiao Xue, James Y. Zhang, and Sheng Li. 2023. Leveraging Large Language Models for Pre-trained Recommender Systems. arXiv e-prints 2308.10837 (2023). [15] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, and et al. 2024. Scaling Instruction-Finetuned Language Models. J. Mach. Learn. Res. 25 (2024), 70:1-70:53. [16] Anthony M. Colas, Jun Araki, Zhengyu Zhou, Bingqing Wang, and Zhe Feng. 2023. Knowledge-Grounded Natural Language Recommendation Explanation. In BlackboxNLP@EMNLP . Association for Computational Linguistics, 1-15. [17] Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si, Chen Xu, Zhongxiang Sun, Xiao Zhang, and Jun Xu. 2023. Uncovering ChatGPT's Capabilities in Recommender Systems. In RecSys . ACM, 1126-1132. [18] Sunhao Dai, Yuqi Zhou, Liang Pang, Weihao Liu, Xiaolin Hu, Yong Liu, Xiao Zhang, and Jun Xu. 2023. LLMs may Dominate Information Access: Neural Retrievers are Biased Towards LLM-Generated Texts. arXiv e-prints 2310.20501 (2023). [19] DeepSeek-AI. 2024. DeepSeek-V3 Technical Report. arXiv e-prints 2412.19437 (2024). [20] Yashar Deldjoo. 2024. Understanding Biases in ChatGPT-based Recommender Systems: Provider Fairness, Temporal Stability, and Recency. ACM Transactions on Recommender Systems (2024). [21] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL-HLT (1) . Association for Computational Linguistics, 4171-4186. [22] Dario Di Palma, Giovanni Maria Biancofiore, Vito Walter Anelli, Fedelucio Narducci, Tommaso Di Noia, and Eugenio Di Sciascio. 2023. Evaluating ChatGPT as a Recommender System: A Rigorous Approach. arXiv e-prints 2309.03613 (2023). [23] Shizhe Diao, Pengcheng Wang, Yong Lin, Rui Pan, Xiang Liu, and Tong Zhang. 2024. Active Prompting with Chain-of-Thought for Large Language Models. In ACL (1) . Association for Computational Linguistics, 1330-1350. [24] Yijie Ding, Yupeng Hou, Jiacheng Li, and Julian J. McAuley. 2024. Inductive Generative Recommendation via Retrieval-based Speculation. arXiv e-prints 2410.02939 (2024). [25] Yingpeng Du, Di Luo, Rui Yan, Hongzhi Liu, Yang Song, Hengshu Zhu, and Jie Zhang. 2023. Enhancing Job Recommendation through LLM-based Generative Adversarial Networks. arXiv preprint arXiv:2307.10747 (2023). [26] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, and et al. 2024. The Llama 3 Herd of Models. arXiv e-prints 2407.21783 (2024). [27] Yue Feng, Shuchang Liu, Zhenghai Xue, Qingpeng Cai, Lantao Hu, Peng Jiang, Kun Gai, and Fei Sun. 2023. A Large Language Model Enhanced Conversational Recommender System. arXiv e-prints 2308.06212 (2023). [28] Luke Friedman, Sameer Ahuja, David Allen, Terry Tan, Hakim Sidahmed, Changbo Long, Jun Xie, Gabriel Schubiner, Ajay Patel, Harsh Lara, et al. 2023. Leveraging Large Language Models in Conversational Recommender Systems. arXiv preprint arXiv:2305.07961 (2023). , Vol. 1, No. 1, Article . Publication date: January 2025. Leveraging Large Language Models as Recommender Systems 47 , Vol. 1, No. 1, Article . Publication date: January 2025. 48 Lanling Xu et al. , Vol. 1, No. 1, Article . Publication date: January 2025. Leveraging Large Language Models as Recommender Systems 49 , Vol. 1, No. 1, Article . Publication date: January 2025. 50 Lanling Xu et al. [129] Poonam B Thorat, Rajeshwari M Goudar, and Sunita Barve. 2015. Survey on collaborative filtering, content-based filtering and hybrid recommendation system. International Journal of Computer Applications 110, 4 (2015), 31-36. , Vol. 1, No. 1, Article . Publication date: January 2025. Leveraging Large Language Models as Recommender Systems 51 [153] Chuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng Huang. 2021. Empowering news recommendation with pre-trained language models. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in , Vol. 1, No. 1, Article . Publication date: January 2025. 52 Lanling Xu et al. [176] Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne Xin Zhao, Leyu Lin, and Ji-Rong Wen. 2023. Recommendation as Instruction Following: A Large Language Model Empowered Recommendation Approach. ACM Transactions on , Vol. 1, No. 1, Article . Publication date: January 2025. Leveraging Large Language Models as Recommender Systems 53 Information Systems (2023). [177] Wenxuan Zhang, Hongzhi Liu, Yingpeng Du, Chen Zhu, Yang Song, Hengshu Zhu, and Zhonghai Wu. 2023. Bridging the Information Gap Between Domain-Specific Model and General LLM for Personalized Recommendation. arXiv preprint arXiv:2311.03778 (2023). [178] Yuhui Zhang, Hao Ding, Zeren Shui, Yifei Ma, James Zou, Anoop Deoras, and Hao Wang. 2021. Language models as recommender systems: Evaluations and limitations. In I (Still) Can't Believe It's Not Better! NeurIPS 2021 Workshop . [179] Yang Zhang, Fuli Feng, Jizhi Zhang, Keqin Bao, Qifan Wang, and Xiangnan He. 2023. CoLLM: Integrating Collaborative Embeddings into Large Language Models for Recommendation. arXiv e-prints 2310.19488 (2023). [180] Yang Zhang, Juntao You, Yimeng Bai, Jizhi Zhang, Keqin Bao, Wenjie Wang, and Tat-Seng Chua. 2024. CausalityEnhanced Behavior Sequence Modeling in LLMs for Personalized Recommendation. arXiv e-prints 2410.22809 (2024). [181] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. 2024. Multimodal Chain-ofThought Reasoning in Language Models. Trans. Mach. Learn. Res. 2024 (2024). [182] Wayne Xin Zhao, Yupeng Hou, Xingyu Pan, Chen Yang, Zeyu Zhang, Zihan Lin, Jingsen Zhang, Shuqing Bian, Jiakai Tang, Wenqi Sun, and et al. 2022. RecBole 2.0: Towards a More Up-to-Date Recommendation Library. In CIKM . ACM, 4722-4726. [183] Wayne Xin Zhao, Shanlei Mu, Yupeng Hou, Zihan Lin, Yushuo Chen, Xingyu Pan, Kaiyuan Li, Yujie Lu, Hui Wang, Changxin Tian, and et al. 2021. RecBole: Towards a Unified, Comprehensive and Efficient Framework for Recommendation Algorithms. In CIKM . ACM, 4653-4664. [184] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, and et al. 2023. A Survey of Large Language Models. arXiv e-prints 2303.18223 (2023). [185] Zihuai Zhao, Wenqi Fan, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Zhen Wen, Fei Wang, Xiangyu Zhao, Jiliang Tang, and Qing Li. 2024. Recommender Systems in the Era of Large Language Models (LLMs). IEEE Trans. Knowl. Data Eng. 36, 11 (2024), 6889-6907. [186] Bowen Zheng, Yupeng Hou, Hongyu Lu, Yu Chen, Wayne Xin Zhao, Ming Chen, and Ji-Rong Wen. 2024. Adapting Large Language Models by Integrating Collaborative Semantics for Recommendation. In ICDE . IEEE, 1435-1448. [187] Denny Zhou, Nathanael Sch√§rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V. Le, and Ed H. Chi. 2023. Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. In ICLR . OpenReview.net. [188] Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, and Ji-Rong Wen. 2020. S3-Rec: Self-Supervised Learning for Sequential Recommendation with Mutual Information Maximization. In CIKM . ACM, 1893-1902. [189] Yaochen Zhu, Liang Wu, Qi Guo, Liangjie Hong, and Jundong Li. 2024. Collaborative Large Language Model for Recommender Systems. In WWW . ACM, 3162-3172. Received 6 June 2024; revised 16 Jan 2025 , Vol. 1, No. 1, Article . Publication date: January 2025.",
  "keywords_parsed": [
    "Large Language Models",
    "Recommender Systems",
    "Empirical Study"
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "Beyond Labels: Leveraging Deep Learning and LLMs for Content Metadata"
    },
    {
      "ref_id": "b2",
      "title": "ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools"
    },
    {
      "ref_id": "b3",
      "title": "A Bi-Step Grounding Paradigm for Large Language Models in Recommendation Systems"
    },
    {
      "ref_id": "b4",
      "title": "TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation"
    },
    {
      "ref_id": "b5",
      "title": "Language Models are Few-Shot Learners"
    },
    {
      "ref_id": "b6",
      "title": "The Relationship between Recall and Precision"
    },
    {
      "ref_id": "b7",
      "title": "Privacy-Preserving Recommender Systems with Synthetic Query Generation using Differentially Private Large Language Models"
    },
    {
      "ref_id": "b8",
      "title": "Enhancing Recommendation Diversity by Re-ranking with Large Language Models"
    },
    {
      "ref_id": "b9",
      "title": "When large language models meet personalization: perspectives of challenges and opportunities"
    },
    {
      "ref_id": "b10",
      "title": "Enhancing Item Tokenization for Generative Recommendation through Self-Improvement"
    },
    {
      "ref_id": "b11",
      "title": "On Softmax Direct Preference Optimization for Recommendation"
    },
    {
      "ref_id": "b12",
      "title": "Wide & Deep Learning for Recommender Systems"
    },
    {
      "ref_id": "b13",
      "title": "Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality"
    },
    {
      "ref_id": "b14",
      "title": "Leveraging Large Language Models for Pre-trained Recommender Systems"
    },
    {
      "ref_id": "b15",
      "title": "Scaling Instruction-Finetuned Language Models"
    },
    {
      "ref_id": "b16",
      "title": "Knowledge-Grounded Natural Language Recommendation Explanation"
    },
    {
      "ref_id": "b17",
      "title": "Uncovering ChatGPT's Capabilities in Recommender Systems"
    },
    {
      "ref_id": "b18",
      "title": "LLMs may Dominate Information Access: Neural Retrievers are Biased Towards LLM-Generated Texts"
    },
    {
      "ref_id": "b19",
      "title": "DeepSeek-V3 Technical Report"
    },
    {
      "ref_id": "b20",
      "title": "Understanding Biases in ChatGPT-based Recommender Systems: Provider Fairness, Temporal Stability, and Recency"
    },
    {
      "ref_id": "b21",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
    },
    {
      "ref_id": "b22",
      "title": "Evaluating ChatGPT as a Recommender System: A Rigorous Approach"
    },
    {
      "ref_id": "b23",
      "title": "Active Prompting with Chain-of-Thought for Large Language Models"
    },
    {
      "ref_id": "b24",
      "title": "Inductive Generative Recommendation via Retrieval-based Speculation"
    },
    {
      "ref_id": "b25",
      "title": "Enhancing Job Recommendation through LLM-based Generative Adversarial Networks"
    },
    {
      "ref_id": "b26",
      "title": "The Llama 3 Herd of Models"
    },
    {
      "ref_id": "b27",
      "title": "A Large Language Model Enhanced Conversational Recommender System"
    },
    {
      "ref_id": "b28",
      "title": "Leveraging Large Language Models in Conversational Recommender Systems"
    },
    {
      "ref_id": "b129",
      "title": "Survey on collaborative filtering, content-based filtering and hybrid recommendation system"
    },
    {
      "ref_id": "b153",
      "title": "Empowering news recommendation with pre-trained language models"
    },
    {
      "ref_id": "b176",
      "title": "Recommendation as Instruction Following: A Large Language Model Empowered Recommendation Approach"
    },
    {
      "ref_id": "b177",
      "title": "Bridging the Information Gap Between Domain-Specific Model and General LLM for Personalized Recommendation"
    },
    {
      "ref_id": "b178",
      "title": "Language models as recommender systems: Evaluations and limitations"
    },
    {
      "ref_id": "b179",
      "title": "CoLLM: Integrating Collaborative Embeddings into Large Language Models for Recommendation"
    },
    {
      "ref_id": "b180",
      "title": "Causality-Enhanced Behavior Sequence Modeling in LLMs for Personalized Recommendation"
    },
    {
      "ref_id": "b181",
      "title": "Multimodal Chain-of-Thought Reasoning in Language Models"
    },
    {
      "ref_id": "b182",
      "title": "RecBole 2.0: Towards a More Up-to-Date Recommendation Library"
    },
    {
      "ref_id": "b183",
      "title": "RecBole: Towards a Unified, Comprehensive and Efficient Framework for Recommendation Algorithms"
    },
    {
      "ref_id": "b184",
      "title": "A Survey of Large Language Models"
    },
    {
      "ref_id": "b185",
      "title": "Recommender Systems in the Era of Large Language Models (LLMs)"
    },
    {
      "ref_id": "b186",
      "title": "Adapting Large Language Models by Integrating Collaborative Semantics for Recommendation"
    },
    {
      "ref_id": "b187",
      "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"
    },
    {
      "ref_id": "b188",
      "title": "S3-Rec: Self-Supervised Learning for Sequential Recommendation with Mutual Information Maximization"
    },
    {
      "ref_id": "b189",
      "title": "Collaborative Large Language Model for Recommender Systems"
    }
  ]
}