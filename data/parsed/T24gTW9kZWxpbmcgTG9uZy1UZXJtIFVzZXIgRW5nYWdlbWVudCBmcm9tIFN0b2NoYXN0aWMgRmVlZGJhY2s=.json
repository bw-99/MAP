{"On Modeling Long-Term User Engagement from Stochastic Feedback": "", "Guoxi Zhang": "Graduate School of Informatics, Kyoto University Kyoto, Japan guoxi@ml.ist.i.kyoto-u.ac.jp Xing Yao China Central Depository & Clearing Co., Ltd. Beijing, China 854352859@qq.com Xuanji Xiao \u2217 Shopee Inc. Shenzhen, China growj@126.com", "ABSTRACT": "An ultimate goal of recommender systems (RS) is to improve user engagement. Reinforcement learning (RL) is a promising paradigm for this goal, as it directly optimizes overall performance of sequential recommendation. However, many existing RL-based approaches induce huge computational overhead, because they require not only the recommended items but also all other candidate items to be stored. This paper proposes an efficient alternative that does not require the candidate items. The idea is to model the correlation between user engagement and items directly from data. Moreover, the proposed approach consider randomness in user feedback and termination behavior, which are ubiquitous for RS but rarely discussed in RL-based prior work. With online A/B experiments on real-world RS, we confirm the efficacy of the proposed approach and the importance of modeling the two types of randomness.", "CCS CONCEPTS": "\u00b7 Computing methodologies \u2192 Markov decision processes ; \u00b7 Information systems \u2192 Content ranking ; Personalization.", "KEYWORDS": "Recommender Systems, Reinforcement Learning", "ACMReference Format:": "Guoxi Zhang, Xing Yao, and Xuanji Xiao. 2023. On Modeling Long-Term User Engagement from Stochastic Feedback. In Companion Proceedings of the ACM Web Conference 2023 (WWW '23 Companion), April 30-May 4, 2023, rate (pCTR), can be suboptimal. For instance, news can have higher pCTR than dramma bloopers, yet dramma bloopers prompt more consumptionfor videos about the dramma and staffs in the dramma. Reinforcement learning (RL) has emerged as a promising research direction for optimizing long-term user engagement [2, 4, 12, 14]. Using RL, sequential recommendation is modeled as interaction between an agent and an environment. At each step, the agent receives a state and selects an action . Usually, states are user profiles and their historical activities, and actions are items available for recommendation. After selecting each action the agent receives a scalar reward . The goal of RL is to learn a policy for action selection, with which the agent can maximize the sum of rewards received during interacting with the environment. Thus, RL is an anticing paradigm if one uses utility for items (e.g. clicks) as rewards, as it naturally optimizes long-term utility of sequential RS. However, training RL models for industrial RS can be challenging. To understand the issue, consider a minimalist example in which an agent selects one item from a set of candidate items for a user. To train a model for pCTR, one needs to record information about the user, the selected item, and the feedbacks for items. However, to train a RL model such as the well-known deep Q-network (DQN) [6], one needs to additionally record the candidate item set. This induces huge overhead for industrial applications as the candidate set usually contains several hundreds of items. While the overhead can be reduced by learning RL models using item embedding vectors, it is reported that the performance is much worse than learning RL models using items directly [8]. Austin, TX, USA. ACM,NewYork, NY,USA,5pages. https://doi.org/10.1145/3543873.3587626", "1 INTRODUCTION": "Recommender Systems (RS) are software systems that help users discover engaging items. In particular, a sequential RS allows users to consume new items without ceasing, usually by scrolling down the interface to the RS. The main goal of sequential RS is to increase long-term user engagement. In this regard, myopic decisionmaking, such as ranking items solely by predicted click-through \u2217 Corresponding author Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WWW'23 Companion, April 30-May 4, 2023, Austin, TX, USA \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-9419-2/23/04...$15.00 https://doi.org/10.1145/3543873.3587626 This paper proposes an efficient alternative for optimizing longterm user engagement. The key observation behind the proposed approach is that in industrial setting RS are periodically trained on logs generated by some other RS, which is referred to as the behavior policy afterwards. For applications serving millions of users the behavior policy is usually highly optimized, so by mining the relation between user engagement and items selected by this policy we can obtain a model for user engagement. Notably, the proposed approach does not require the candidate item set in learning. So it can be integrated into any existing myopic RS effortlessly without introducing huge overhead. In addition to the efficiency issue, the proposed approach also considers stochasticity in rewards and termination of interactions. RL algorithms such as DQN usually assume that (a) rewards are deterministic regarding to states and actions and (b) interactions last for infinite steps. Both of them are rarely satisfied in RS. Real-world RS need to serve users with diverse interests, but they only have limited information about users. Consider two different users that have the same feature values. As they have different interests, the feedbacks they provide for the same item will be different. From WWW'23 Companion, April 30-May 4, 2023, Austin, TX, USA Zhang et al. the perspective of RS, this means that the rewards for the same state and actions tend to be random. For the same reason, it is unlikely that they will interact with RS for the same number of steps. Unfortunately, both types of stochasticity have not been addressed so far. To model the former, this paper employs the recent distributional RL framework [1]. For the latter, this paper extends the distributional RL framework to random termination setting and learns a model to predict whether users terminate interaction. Weevaluated the proposed approach on an real-world industrial RS for short videos that serves millions of users. During a week of online A/B test, after deploying the proposed approach we were able to improves the average number of videos viewed by a user by 2.72% and the average duration of video views by 1.59%. Moreover, the experiment also confirmed that modeling stochasticity is essential for effectively modeling long-term user engagement. The contribution of this paper can be summarized as follows. \u00b7 This paper proposes a simple and efficient approach for modeling long-term user engagement in RS. \u00b7 This paper proposes to consider stochasticity in user feedback and termination when modeling user engagement. \u00b7 This paper evaluated the proposed method on an industrial RS and confirmed its efficacy. The rest of this paper organizes as follows. Section 2 briefly reviews relevant literature, and section 3 provide technical background. Section 4 describes the proposed approach. Section 5 presents experiments on real-world RS, and section 6 concludes this paper.", "2 RELATED WORK": "Existing attemts for RL-based recommendation can be classified into policy-gradient based [2] and variants of the DQN algorithm [13, 14]. This paper address the efficiency issue, which can be consider as a complementary approach for resource-constrained scenarios. In the literature of RS, significant effort has been devoted to neural architectures. The proposed approach leverages the DLRM [7] architecture, though it can be combined with other architectures as well. Meanwhile, to model the randomness in rewards, the proposed approach leverages the recent distributional RL framework [1, 3]. As for random termination, in literature it has been considered in a model-based RL [9] and modeled with evolutionary algorithm [11] or meta gradient descend [10]. Our proposed method differs from these approaches as it learns the discount factor from leaving behaviors of users.", "3 PRELIMINARIES": "Modeling Sequential Recommendation with RL. Sequential interaction between a user and a recommender agent is modeled as a discounted infinite-horizon Markov Decision Process (MDP): < S , A , /u1D445, /u1D443 , /u1D6FE > . States S are information about users, which include features such as interest tags and lists of items consumed in past few minutes, hours or days. Actions A are candidate items. In distributional RL, rewards /u1D445 are considered as random variables. This paper utilizes clicking signal as rewards, so a realization /u1D45F = 1 means the corresponding items is clicked, and /u1D45F = 0 otherwise. The transition dynamics /u1D443 governs state transitions, and the discount factor /u1D6FE is used for defining value distribution. Action Value Distribution. Weare interested in modeling the the sum of rewards obtained during agent-environment interaction, which characterizes long-term user engagement. As we assume rewards to be random variables, state-action values /u1D44D /u1D70B ( /u1D460 , /u1D44E ) , the discounted sum of rewards obtained after selecting an action /u1D44E at some state /u1D460 and following /u1D70B afterwards, is also a random variable. Bellemare et al. showed that /u1D44D /u1D70B ( /u1D460 , /u1D44E ) is the fixed point of the distributional Bellman operator T /u1D70B that is defined as [1] :  where /u1D460 \u2032 \u223c /u1D443 ( /u1D460 | /u1D460 , /u1D44E ) . /u1D44B /u1D437 = /u1D44C means that the random variable /u1D44B has the same distribution as random variable /u1D44C . Dabney et al. proposed to parameterize /u1D44D /u1D70B with the so-called quantile distributions [3]. The idea is to learn a parametric function /u1D6FD : S \u00d7 A \u2192 R /u1D440 to estimate /u1D440 evenly spaced quantiles of /u1D44D /u1D70B . Given data from /u1D70B , they proposed to learn /u1D44D /u1D70B by minimizing the quantile huber loss:  where /u1D70F /u1D456 is the i th quantile of value distribution, and \u02c6 /u1D70F /u1D45A = 1 2 ( /u1D70F /u1D45A -1 + /u1D70F /u1D45A ) for /u1D45A = 1 , 2 , . . . , /u1D440 are quantile mid-points. For simplicity, /u1D70F 0 = 0. /u1D70C /u1D705 /u1D70F ( /u1D462 ) = 1 /u1D705 | /u1D70F -I ( /u1D462 < 0 )| /u1D70C /u1D705 ( /u1D462 ) and I (\u00b7) is the indicator function. /u1D705 > 0 is a hyper-parameter, and in practice we use /u1D705 = 1. /u1D6FD \u2032 has the same neural structures as /u1D6FD , and its parameters are periodically copied from /u1D6FD .", "4 PROPOSED METHOD": "", "4.1 The Learning Problem": "Weassumeanagent learns from some offline data D that are generated by behavior policy /u1D44F . D are logs of sequential interactions between users and the agent. We represent the information about the user and as states and recommended items as actions. The reward for a decision equals to one if the corresponding item is clicked, and it equals to zero otherwise. Finally, we organize D as transitions, which can be written as ( /u1D460 , /u1D44E, /u1D45F , /u1D460 \u2032 ) , where /u1D460 and /u1D460 \u2032 are two consecutive states, /u1D44E is the action chosen at /u1D460 and /u1D45F is the reward for this decision. Our goal is to learn a function /u1D454 ( /u1D460 , /u1D44E ) : S \u00d7 A \u2192 R that predicts the the sum of rewards obtained after selecting /u1D44E at step /u1D460 and following /u1D44F afterwards, which characterizes the long-term user engagement of selecting /u1D44E . With /u1D454 ( /u1D460 , /u1D44E ) , we can rank items according to their long-term user engagement.", "4.2 The Proposed Approach": "The proposed method is based on the following observation. Realworld RS are often highly optimized using both learning-based method and rule-based methods, and they are regularly updated to adapt to new users and new items. As a result, users tend to interact with them sequentially, so the data generated by them contain information about items' effect on user engagement. Thus, we propose to mine such information from data. On Modeling Long-Term User Engagement from Stochastic Feedback WWW'23 Companion, April 30-May 4, 2023, Austin, TX, USA Meanwhile, as discussed in Section 1, user feedbacks are stochastic by nature. In consequence, when modeling user engagement as cumulative user feedbacks, we have to appropriately address such randomness. The idea of the proposed approach is to estimate /u1D44D /u1D44F ( /u1D460 , /u1D44E ) , the value distribution of behavior policy /u1D44F from data D . This can be achieved with Equation 2. With /u1D44D /u1D44F ( /u1D460 , /u1D44E ) we can use its expectation as a scoring function for long-term user engagement, i.e. /u1D454 ( /u1D460 , /u1D44E ) = E [ /u1D44D /u1D44F ( /u1D460 , /u1D44E )] = 1 /u1D440 \u2211 /u1D440 /u1D457 /u1D6FD ( /u1D460 , /u1D44E ) /u1D457 . We now discuss how to handle random termination. While the discount factor /u1D6FE in an MDP is used to defined value functions and value distributions, it can also be interpreted as the probability that interaction continues after a state-action pair [5]. Thus, 1 -/u1D6FE is precisely the probability that interaction terminates. Our idea is to estimate such probability using data and learn /u1D44D /u1D44F ( /u1D460 , /u1D44E ) based on such estimate. In consequence, /u1D44D /u1D44F ( /u1D460 , /u1D44E ) will be aware of the correlation between decisions and termination of interactions. This leads to a new distributional Bellman operator T /u1D70B \u2113 :  where /u1D6FE \u2032 ( /u1D460 , /u1D44E ) : = min ( 1 -\u2113 ( /u1D460 , /u1D44E ) , /u1D702 ) . \u2113 ( /u1D460 , /u1D44E ) is the probability that interaction terminates after ( /u1D460 , /u1D44E ) , and /u1D702 \u2208 ( 0 , 1 ) is a hyper-parameter. In this operator, the value distribution of next state /u1D44D /u1D70B ( /u1D460 \u2032 , /u1D70B ( /u1D460 \u2032 )) is weighted by /u1D6FE \u2032 ( /u1D460 , /u1D44E ) , which characterizes the probability that interaction continues after ( /u1D460 \u2032 , /u1D70B ( /u1D460 \u2032 )) . Thus, with this operator the learned /u1D44D /u1D44F ( /u1D460 , /u1D44E ) will be able to model random termination. One potential issue about T /u1D70B \u2113 is its convergence. In what follows we show that T /u1D70B \u2113 is a contraction in tabular case, based on the analysis for T /u1D70B provided by [1]. To begin with, let /u1D439 /u1D44B and /u1D439 /u1D44C be cumulative distribution functions (CDF) of random variable /u1D44B and /u1D44C . The Wasserstein metric /u1D451 /u1D45D between /u1D439 /u1D44B and /u1D439 /u1D44C can be written as /u1D451 /u1D45D ( /u1D439 /u1D44B , /u1D439 /u1D44C ) = \u2016 /u1D439 /u1D44B ( /u1D462 ) -/u1D439 /u1D44C ( /u1D462 )\u2016 /u1D45D 1 . For two action value distributions /u1D44D 1 and /u1D44D 2, defined \u00af /u1D451 /u1D45D ( /u1D44D 1 , /u1D44D 2 ) : = sup /u1D460 ,/u1D44E /u1D451 /u1D45D ( /u1D44D 1 ( /u1D460 , /u1D44E ) , /u1D44D 2 ( /u1D460 , /u1D44E )) . Our analysis is based on following two properties of /u1D451 /u1D45D [1]. Suppose /u1D434 is a random variable that is independent with /u1D44B and /u1D44C and /u1D44E is a constant, then:  Suppose we have samples from policy /u1D70B . Then for two distributions over S \u00d7 A , /u1D44D 1 and /u1D44D 2, and for any state /u1D460 \u2208 S and /u1D44E \u2208 A , wecan characterize the Wasserstein metric between after applying T /u1D70B \u2113 as:  (5) Hence \u00af /u1D451 /u1D45D (T /u1D70B \u2113 /u1D44D 1 , T /u1D70B \u2113 /u1D44D 2 ) = sup /u1D460 ,/u1D44E /u1D451 /u1D45D (T /u1D70B \u2113 /u1D44D 1 , T /u1D70B \u2113 /u1D44D 2 ) \u2264 /u1D702 \u00af /u1D451 /u1D45D ( /u1D44D 1 , /u1D44D 2 ) , which means T /u1D70B /u1D451 ,\u2113 is a /u1D702 -contraction in \u00af /u1D451 /u1D45D . 1 To simplify notation, random variables and their CDF are conflated whenever possible. Algorithm 1: A practical algorithm for the proposed framework) Input: /u1D447 /u1D450 , frequency for copying /u1D6FD to /u1D6FD \u2032 /u1D435 , the size of mini-batches Initialize /u1D6FD randomly and copy its values to /u1D6FD \u2032 for mini-batch /u1D456 = 1 , 2 , . . . , /u1D45B do Update /u1D6FD and if /u1D456 mod \u2113 /u1D447 /u1D6FD \u2032 = /u1D450 = /u1D6FD end end", "4.3 Practical Algorithm": "As for a practical algorithm, we estimate /u1D44D /u1D44F ( /u1D460 , /u1D44E ) and \u2113 ( /u1D460 , /u1D44E ) simultaneously using data D . The objective function to be minimized is as follows. /u1D43F \u2113 ( /u1D6FD , \u2113 ) =  /u1D452 = 1 if interaction terminates at ( /u1D460 , /u1D44E ) and /u1D452 = 0 otherwise. /u1D6FD and \u2113 can be parameterized as a multi-objective network, in which they share feature embeddings but have different feedforward networks. Algorithm 1 provides pseudo code for this algorithm.", "5 EXPERIMENTS": "This section evaluates the proposed method on an industrial RS that generates a feed for 'videos you might also like'. In particular, we investigates the following two questions. (1) Can we improve user engagement of real-world RS via modeling /u1D44D /u1D44F ( /u1D460 , /u1D44E ) ? (2) Does modeling randomness in rewards and termination beneficial in practice?", "5.1 Methodology": "We followed standard protocol for A/B testing. Users were randomly hashed into the control group and test groups. Before the evaluation period, we confirmed that there was no significant difference between the control group and test groups using statistical testing. Every group has at least 630,000 users. As the our goal is to improve RS via modeling long-term user engagement, we evaluated the performance of the RS after deploying algorithms to the system. Let /u1D454 /u1D44F ( /u1D460 , /u1D44E ) be the current ranking function of the RS. Then after deploying estimator for user engagement /u1D454 ( /u1D460 , /u1D44E ) , the new ranking function becomes /u1D454 /u1D44F new ( /u1D460 , /u1D44E ) = /u1D454 /u1D44F ( /u1D460 , /u1D44E ) + /u1D464 \u2217 /u1D454 ( /u1D460 , /u1D44E ) , where /u1D464 is a parameter tuned in preliminaries experiments. We used the metrics for video consumption as evaluation metric for user engagement, which are listed in Table 1. We report the change in performance after deploying algorithms to the RS. A positive change indicates that an algorithm can improve the performance of the current online policy. using Equation 6. /u1D435 0 then WWW'23 Companion, April 30-May 4, 2023, Austin, TX, USA Zhang et al. Table 1: Evaluation metrics used in online experiments. VV is the number of videos clicked by a user, and IMP is the number of videos shown to a user. DUR is the sum of time a user spent on watching videos. These three metrics measure the overall performance during the A/B test, so we consider themasmetricsfor userengagement.CTRand CVRaremetrics for individual videos. They reflect the extent to which a user likes individual items, so they are metrics for shortterm utility.", "5.2 Algorithms": "control group. A highly-optimized RS for short-videos used in some industrial application. Proposed. The proposed approach, which considers randomness in rewards and termination for modeling user engagement. Random-Reward. Avariant of the proposed approach considers randomness in rewards but not randomness in termination. Refer to this method as RR. Full-Deterministic. A variant of the proposed approach that directly estimates /u1D454 ( /u1D460 , /u1D44E ) without considering randomness in rewards and termination. Refer to this method as FD.", "5.3 Implementation Details": "We used the DLRM structure in experiments. There were 40 features in total, including user ID, item ID, and item keywords. These discrete features were mapped to embedding vectors in R 32 . Parameters of the target module are copied from the policy module every 100 gradient steps. Algorithms were trained daily using a single P40 GPU. We used Adam as the optimizer for all three algorithms with a learning rate set to 0 . 00015. Whem modeling randomness in rewards, the number of quantiles was set to 200.", "5.4 Results": "Table 2 shows results for our online evaluations. FD failed to provide any significant improvement over the control group. RR was able to improve VV, CTR and CVR significantly, though its improvement for DUR was not significant. These results imply that users in this group were likely to watch slightly more videos than the control group. Moreover, they show that modeling randomness in rewards is indeed helpful for characterizing items' effect on user engagement. Meanwhile, the proposed method was able to significantly improve VV, DUR, CTR and CVR, showing that randomness in termination is yet another key factor for modeling user engagement. Together, Table 2 confirms the efficacy of the proposed method as an efficient alternative for modeling long-term user engagement for RS. Table 2: Results of our online A/B test. The numbers shown in parentheses are p values. Without considering any randomness, FD failed to improve any metric significantly. After considering randomness in rewards, RR was able to improve VV, CTR and CVR significantly. The proposed apporach improved VV, DUR, CTR and CVR significantly. The results confirm that modeling randomness in rewards and termination is crucial for modeling user engagement.", "6 CONCLUSION": "This paper investigates how to model long-term user engagement for RS efficiently. The proposed approach relies on an observation that the behavior policy (i.e. the RS in production environment) in industrial applications are often highly optimized. So instead of learning the optimal policy from scratch, the proposed model tries to capture correlation between user engagement and recommended item in offline data. Although not guaranteed to be optimal, it enjoys computational efficiency and is confirmed to be effective for real-world systems. Moreover, this paper proposes to model randomness in rewards and termination and confirms the importance of the two factors for modeling user engagement.", "REFERENCES": "[1] Marc G. Bellemare, Will Dabney, and R\u00e9mi Munos. 2017. A distributional perspective on reinforcement learning. In Proceedings of the Thirty-fourth International Conference on Machine Learning . PMLR, Sydney, Australia, 449-458. [2] Minmin Chen, Alex Beutel, Paul Covington, Sagar Jain, Francois Belletti, and Ed H. Chi. 2019. Top-k off-policy correction for a reinforce recommender system. In Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining . Association for Computing Machinery, Melbourne, VIC, Australia, 456-464. [3] Will Dabney, Mark Rowland, Marc G. Bellemare, and R\u00e9mi Munos. 2018. Distributional reinforcement learning with quantile regression. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence . New Orleans, LA, USA, 2892-2901. [4] Eugene Ie, Vihan Jain, Jing Wang, Sanmit Narvekar, Ritesh Agarwal, Rui Wu, Heng-Tze Cheng, Tushar Chandra, and CraigBoutilier. 2019. Slateq: a tractable decomposition for reinforcement learning with recommendation sets. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence . International Joint Conferences on Artificial Intelligence Organization, 2592-2599. [5] Michael L. Littman. 1994. Markov games as a framework for multi-agent reinforcement learning. In Proceedings of the Eleventh International Conference on International Conference on Machine Learning . New Brunswick, NJ, USA, 157163. [6] Volodymyr Mnih et al. 2015. Human-level control through deep reinforcement learning. Nature , 518, 7540, 529-533. [7] Maxim Naumov et al. 2019. Deep learning recommendation model for personalization and recommendation systems. (2019). arXiv: 1906.00091 [cs.IR] . [8] Kai Wang, Zhene Zou, Yue Shang, Qilin Deng, Minghao Zhao, Runze Wu, Xudong Shen, Tangjie Lyu, and Changjie Fan. 2021. Rl4rs: a real-world benchmark for reinforcement learning based recommender system. (2021). arXiv: 2110.11073 [cs.IR] . [9] Martha White. 2017. Unifying task specification in reinforcement learning. In Proceedings of the Thirty-Fourth International Conference on Machine Learning . PMLR, Sydney, Australia, 3742-3750. WWW'23 Companion, April 30-May 4, 2023, Austin, TX, USA On Modeling Long-Term User Engagement from Stochastic Feedback [10] Zhongwen Xu, Hado P van Hasselt, and David Silver. 2018. Meta-gradient reinforcement learning. In Advances in Neural Information Processing Systems . Curran Associates, Inc. [11] Naoto Yoshida, Eiji Uchibe, and Kenji Doya. 2013. Reinforcement learning with state-dependent discount factor. In 2013 IEEE third joint international conference on development and learning and epigenetic robotics . IEEE, 1-6. [12] Xiangyu Zhao, Changsheng Gu, Haoshenglun Zhang, Xiaobing Liu, Xiwang Yang, and Jiliang Tang. 2019. Deep reinforcement learning for online advertising in recommender systems. arXiv preprint arXiv:1909.03602 . [13] Xiangyu Zhao, Long Xia, Liang Zhang, Zhuoye Ding, Dawei Yin, and Jiliang Tang. 2018. Deep reinforcement learning for page-wise recommendations. In Proceedings of the 12th ACM Conference on Recommender Systems . Association for Computing Machinery, Vancouver, BC, Canada, 95-103. [14] Guanjie Zheng, Fuzheng Zhang, Zihan Zheng, Yang Xiang, Nicholas Jing Yuan, Xing Xie, and Zhenhui Li. 2018. Drn: a deep reinforcement learning framework for news recommendation. In Proceedings of the 2018 World Wide Web Conference . International World Wide Web Conferences Steering Committee, Lyon, France, 167-176."}
