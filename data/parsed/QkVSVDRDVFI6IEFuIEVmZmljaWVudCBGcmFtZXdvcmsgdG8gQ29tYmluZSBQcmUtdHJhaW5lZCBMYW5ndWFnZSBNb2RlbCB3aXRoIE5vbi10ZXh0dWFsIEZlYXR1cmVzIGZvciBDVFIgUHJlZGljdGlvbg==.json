{"BERT4CTR: An Efficient Framework to Combine Pre-trained Language Model with Non-textual Features for CTR Prediction": "Dong Wang Microsoft STCA Beijing, China donwa@microsoft.com Kav\u00e9 Salamatian University of Savoie Annecy, France Kave.salamatian@univ-smb.fr Yunqing Xia Microsoft STCA Beijing, China yxia@microsoft.com Weiwei Deng Microsoft STCA Beijing, China dedeng@microsoft.com Qi Zhang Microsoft STCA Beijing, China qizhang@microsoft.com", "ABSTRACT": "Although deep pre-trained language models have shown promising benefit in a large set of industrial scenarios, including ClickThrough-Rate (CTR) prediction, how to integrate pre-trained language models that handle only textual signals into a prediction pipeline with non-textual features is challenging. Up to now two directions have been explored to integrate multimodal inputs in fine-tuning of pre-trained language models. One consists of fusing the outcome of language models and non-textual features through an aggregation layer, resulting into ensemble framework, where the cross-information between textual and nontextual inputs are only learned in the aggregation layer. The second one consists of splitting non-textual features into fine-grained fragments and transforming the fragments to new tokens combined with textual ones, so that they can be fed directly to transformer layers in language models. However, this approach increases the complexity of the learning and inference because of the numerous additional tokens. To address these limitations, we propose in this work a novel framework BERT4CTR, with the Uni-Attention mechanism that can benefit from the interactions between non-textual and textual features while maintaining low time-costs in training and inference through a dimensionality reduction. Comprehensive experiments on both public and commercial data demonstrate that BERT4CTR can outperform significantly the state-of-the-art frameworks to handle multi-modal inputs and be applicable to CTR prediction.", "CCS CONCEPTS": "\u00b7 Information systems \u2192 Online advertising ; Recommender systems . Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference'17, July 2017, Washington, DC, USA \u00a9 2023 Association for Computing Machinery. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00 https://doi.org/10.1145/XXXXXXX.XXXXXXX", "KEYWORDS": "Non-textual features, Multi-modal inputs, Pre-trained language model, CTR prediction, Uni-Attention", "ACMReference Format:": "Dong Wang, Kav\u00e9 Salamatian, Yunqing Xia, Weiwei Deng, and Qi Zhang. 2023. BERT4CTR: An Efficient Framework to Combine Pre-trained Language Model with Non-textual Features for CTR Prediction. In Proceedings of ACM Conference (Conference'17). ACM, New York, NY, USA, 11 pages. https: //doi.org/10.1145/XXXXXXX.XXXXXXX", "1 INTRODUCTION": "Machine learning has frequently to deal with multi-modal inputs that are mixing numerical, ordinal, categorical, and textual data. This is especially the case for the Click-Through-Rate (CTR) prediction, where one tries to predict the likelihood that a recommended candidate shown after a query entered on a search engine, will be clicked based on not only the semantic relevance between the query and the description of candidate, but also the user's attributes such as user's ID, user's gender, user's category, etc. , which are nontextual. In recent work, the pre-trained language models like BERT [4] and RoBERTa [16], which can dig deep for semantic relationship between words in natural language texts, have shown to be beneficial in improving the accuracy of CTR prediction. However, how to integrate pre-trained language models that handle only textual signals into the CTR prediction pipeline with numerous non-textual features is still challenging. Currently these pre-trained language models have been widely used to improve classical CTR prediction, by adding the final score [32] or intermediate embedding [18] after fine-tuning with textual signals, e.g. , < \ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5f\ud835\udc66, \ud835\udc4e\ud835\udc51 > textual pairs, as the independent NLP feature into the existing CTR prediction pipelines with non-textual features, leading to cascading workflow. Nonetheless, such frameworks cannot leverage the cross-information between textual and non-textual signals in the fine-tuning of language models. In this paper we confirm that learning such cross-information in fine-tuning phase is beneficial for the improvement on accuracy of CTR prediction and the goal of our work is to design an efficient framework making the information fusion happen at the beginning stage of the fine-tuning process. It is also noteworthy that although CTR prediction is the main application in this paper, the approach developed here can be extended to a large set of applications that have to deal with multi-modal inputs in pre-trained language models. Conference'17, July 2017, Washington, DC, USA Dong Wang, Kav\u00e9 Salamatian, Yunqing Xia, Weiwei Deng, and Qi Zhang Up to now two directions have been explored to integrate multimodal inputs in fine-tuning of pre-trained language models. In the first approach, called here as \"Shallow Interaction\", the language model with textual input is treated as a separated and specific network, and the outcome of this network (final output score or [CLS] pooling layer) is fused into the other network dealing with nontextual inputs through an aggregation layer. This approach has been adopted in [2][21], resulting into ensemble learning framework, and [30] has done an in-depth analysis on this approach. In this approach, interaction between textual and non-textual features only happens in the last aggregation layer. As a consequence crossinformation between textual and non-textual inputs are not dug enough to fine tune the model. In the second class of approach, nontextual features are directly fed as the inputs of transformer layers in the language model, which enables to leverage the non-textual inputs at the beginning stage of model learning. Such an approach is at the core of VideoBERT [27], VL-BERT [26] and NumBERT [34], where non-textual signals, such as images or numbers, are split into fine-grained fragments ( e.g. , regions-of-interest in images or digits) each of which is transformed as a new token and combined with textual tokens. However, there are always several hundreds of non-textual features in the task of CTR prediction, where the overlong additional inputs complicate the computations and make the time-costs in learning and inference of model intractable. Given the limitations of the two approaches in literature, we introduce a simple and light framework, named BERT4CTR , to handle multi-modal inputs mixing textual and non-textual features in pretrained language models. Our approach is based on a Uni-Attention mechanism integrating the semantic extraction from textual features, with cross-information between textual and non-textual features. We apply a dimensionality reduction operation in order to decrease the time-costs both in learning and inference. Besides, a two-steps joint-training is introduced to fine-tune the model and further improve the accuracy of prediction. The proposed approach scales well with the growing of non-textual features, that can be expected to improve the industrial CTR prediction tasks with large number of features. Through empirical evaluation on both commercial and public data, we show that, compared with these state-of-the-art approaches combining textual and non-textual features in CTR prediction mentioned above, BERT4CTR significantly improves the accuracy of predictions while keeping low latency in training and inference. In particular, our results indicate that by increasing the number of non-textual inputs, the advantages of BERT4CTR are enhanced, e.g. , on the public data set with 57 non-textual features, BERT4CTR compared with NumBERT shows a significant gain of 0.7% for the Area Under the ROC Curve (AUC) along with a decrease in training cost of 30%, and a decrease in inference cost of 29%. In the meanwhile, on the commercial data set with 90 nontextual features, BERT4CTR provides an AUC gain of 0.6%, along with a decrease in training cost of 64%, and in inference cost of 52%. In section 2, we present the related work. The section 3 introduces the design of BERT4CTR. The evaluation is presented in section 4. Finally concluding remarks are provided.", "2 RELATED WORK": "This section presents the related works on multi-modal inputs handling that combine non-textual features with pre-trained language models, and its application to CTR prediction.", "2.1 Multi-modal Inputs Handling": "The issue of handling inputs that are mixing textual and non-textual input, and integrating semantic insights coming from pre-trained language models like BERT [4] has been already investigated in the literature VideoBERT [27], VL-BERT [26], NumBERT [34] and CTR-BERT [21]. The approach followed in these works consists of splitting the non-textual signals into fine-grained fragments each of which is transformed as a new token and combined with textual tokens as the inputs of transformer layers. However, the addition of tokens representing the non-textual features complicates the language model structure and can make the learning and inference phases too costly for model updating and online serving.", "2.2 Models for CTR Prediction": "CTR prediction is one of the major practical applications of deep learning. Clicks made on advertisements or candidates shown along with search results, or web content presentation, are the main source of revenue for a large set of web actors. In this context, models are always used to select quality advertisements or candidates to present according to the web contents, e.g. , in sponsored search engines [11] or personal recommendation systems [24], which should achieve both low-latency and high-accuracy. For example, the CTR prediction model in Baidu.com uses a deep neural network, called Phoenix Nest , fed with a handcrafted set of features extracted from the user, the query, and advertisement properties [5]. Google Ads is using the \"Follow The Regularized Leader' (FTRL) model to predict CTR [19], while Google play is using a Wide & Deep model described in [3]. The work [23] introduces \"Product based Neural Network\" (PNN) model to capture interactive patterns between features. This PNN model is extended in [7] to DeepFM model that emphasizes the interactions between low- and high-order feature. Microsoft Bing.com has adopted a Neural Network boosted with GBDT ensemble model [14] for ads CTR prediction. This is the commercial scenario we are considering through this paper. The features used in these CTR prediction models can be grouped into two categories: one is the raw texts from user, query and ad, and the other is the non-textual features including the attributes of users and items such as gender, age, UserId, AdId, etc. and the outputs generated from sub-models, such as LR model [14][19], pre-trained language model [18][32], etc. .", "2.3 Application of Pre-trained Language Models in CTR Prediction": "Recent work has shown the abilities of pre-trained language models to extract deep relationship in a sentence pair [4][16][13][28][15], that are useful for augmenting the semantic features of query and recommendation pair in CTR prediction [18][32][12][8][33]. Generally, the pre-trained language models are trained against the real click data, targeting directly the prediction of click/non-click labels. Thereafter, the score from the final layer [32][12], or the embedding from the intermediate layer [18][8] of these fine-tuned BERT4CTR: An Efficient Framework to Combine Pre-trained Language Model with Non-textual Features for CTR Prediction Conference'17, July 2017, Washington, DC, USA language models is used as an additional NLP input feature in the CTR prediction model. For example, Microsoft Bing Ads uses the embedding from the hidden layer of TwinBERT model as a semantic feature [18] while Meituan.com and JD.com use the output score of BERT [32][12]. Besides that cascading framework, some works consider the fusion between the outputs of language models and non-textual features through an aggregation layer, resulting into ensemble learning frameworks (called \"Shallow Interaction\" in this paper), such as BST [2] and CTR-BERT [21], and [30] has done an in-depth analysis on the Shallow Interaction frameworks, where the cross-information between textual and non-textual inputs are not dug enough to fine tune the language models.", "3 DESCRIPTION OF BERT4CTR": "", "3.1 Problem Statement": "CTRprediction models are always using multi-modal inputs, mixing \ud835\udc41 textual features like searching query, titles and URLs of potential ads to show, etc. , that are denoted as T = { \ud835\udc61 1 , \ud835\udc61 2 , ..., \ud835\udc61 \ud835\udc41 } , and \ud835\udc40 non-textual features of different type, e.g. , dense features such as historical CTR of the query, last click time of the user, etc. , sparse features, like ID, category of user, etc. , denoted as C = { \ud835\udc50 1 , \ud835\udc50 2 , ..., \ud835\udc50 \ud835\udc40 } . In traditional usage of pre-trained language models in CTR prediction, where only textual features are used in fine-tuning on click data, the learning process can be formalized as calibrating a network of which the predicted score can approximate the conditional probability of all outcome alternatives, click or non-click for CTR application, given the textual contexts of query and candidates:  As stated above, the non-textual features are also crucial in CTR prediction and should not be ignored. When non-textual features are added the conditional probability becomes:  The goal in this paper is to design an efficient network structure that can generate scores approximating the distribution \ud835\udc43 \ud835\udc50\ud835\udc59\ud835\udc56\ud835\udc50\ud835\udc58 in Equation 2, while maintaining acceptable training and inference time-costs for industrial application.", "3.2 Model Design": "We then describe the evolution of our proposed network structure, BERT4CTR, by beginning with the NumBERT framework and gradually adding new components to it. 3.2.1 NumBERT Description. NumBERT [34], is the widely-used systematic approach to integrate textual and numerical features in pre-trained language models. Pre-trained language models like BERT, along with a large class of neural networks, are using attention layers, that enhance over time some part of the input to enable the training process to concentrate on learning them. In each attention layer, a feed-forward network and a residual network are used to control the Vanishing/Exploding gradient issue [9]. NumBERT uses a similar structure with several layers of bidirectional self-attention. The core idea in NumBERT is to replace all numerical instances with their scientific notation representations, i.e. , the number 35 is replaced by \"35 [EXP] 1\", where [EXP] is a new token that is added to the vocabulary. These transformed nontextual inputs are thereafter considered as normal texts and are fed to the language model. For the CTR prediction application, several transformed non-textual inputs might be concatenated using separator token [SEP], to distinguish one numerical feature from other, generating a long string of text which is appended to the end of < \ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5f\ud835\udc66, \ud835\udc4e\ud835\udc51 > textual input, and is used for the fine-tuning of language model on click data. Figure 1 depicts an example of transformation from original non-textual features to transformed inputs in NumBERT. Figure 1: Example of transformed and concatenated nontextual and textual inputs for NumBERT While NumBERT approach enables the language model to understand the numbers in the non-textual signals, the model still misses two crucial elements. First, the contextual relationship between textual features and non-textual ones in CTR prediction are always not obvious. For example, the numerical features such as the historical CTR of user, the ID of user etc. , are less correlated with < \ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5f\ud835\udc66, \ud835\udc4e\ud835\udc51 > texts in semantics. Second issue is related to the fact that the positions of these transformed tokens from non-textual features do not bear semantic meanings as normal texts have, i.e. , the numerical features in CTR prediction models are always independent of each other. These two limitations indicate that sharing the same attention weights and mechanisms for both textual features and the transformed non-textual ones appended are not optimal in fine-tuning, and using simply NumBERT approach to integrate non-textual inputs cannot improve the performance of learning objectives well, as will be shown later in Section 4. 3.2.2 Uni-Attention. To address these two issues, we have improved the architecture of NumBERT. We are using the same bidirectional self-attention mechanism as in NumBERT with inputs only from textual tokens. However, for non-textual part, a new type of attention mechanism is introduced, called Uni-Attention . It is still a Query-Key-Value (QKV) attention function [1], where the Query is coming only from non-textual tokens, while the Key and Value are coming from textual tokens in the same layer, i.e. , in the calculation of uni-attention on each token in non-textual part, one input is the matrix projected from the value of that token itself, and the other input is the matrix projected from values of all tokens in textual part. In the uni-attention mechanism, the nontextual components have no positional-embedding, which avoids the issue on positional semantics described above. Moreover, this hybrid framework allows the tokens in textual part to dig deep for semantic relationship between each other by the aid of pre-trained attention weights, while grasping the cross-information between textual and non-textual ones in parallel. Feed-forward and residual networks are also used on each uniattention output to control the Vanishing/Exploding gradient issue. Weshowin Figure 2 the \"Uni-Attention\" design. In the last attention layer, all uni-attention outputs from transformed non-textual inputs are gathered as a single hidden layer, which is concatenated with the Conference'17, July 2017, Washington, DC, USA Dong Wang, Kav\u00e9 Salamatian, Yunqing Xia, Weiwei Deng, and Qi Zhang [CLS] pooling layer from textual part. Thereafter the concatenated layer is fed to a MultiLayer Perception (MLP) that will finally predict the probability of click/non-click. We will show in Section 4 that the proposed design improves strongly the final AUC for both commercial and public data, compared with simple NumBERT. Figure 2: Framework of Uni-Attention Cllck/No-Cllck Label MLP Poolirg Poollne [cLS] Tok A Tok B [SEP) Tok D Tok E Tok F Tok H Tenual Part Non-tertualPar Token 3.2.3 Dimensionality Reduction. The number of non-textual features used in industry for CTR prediction models can be very large, e.g. , Microsoft Bing Ads uses 90 numerical features transformed each into 4 tokens (accounting for the [EXP] and [SEP] tokens). This large size of inputs impacts negatively the learning cost and the prediction latency in CTR prediction. One way to solve this issue is to apply dimensionality reduction on the non-textual features. Such approaches have already been explored in several previous works like [3][7]. Followed by these works, our approach consists of representing each non-textual feature in C as a \ud835\udc41 -dimensional point in space. The resulting \ud835\udc41 \u00d7|C| space is then mapped to a \ud835\udc3e -dimensional embedding ( \ud835\udc3e \u226a \ud835\udc41 \u00d7 |C| ) through a fully connected network that is fed, along with the embedding from textual tokens, to the calculation of uni-attentions. The mapping to the initial \ud835\udc41 -dimensional space is done differently depending if the non-textual features, are dense, e.g. , the length of the query, the historical value of CTR etc. , or sparse, e.g. , user's gender, query's category etc. . For sparse features, we use an embedding table that lists the \ud835\udc41 -dimensional embedding corresponding to each given value. Dense features are first normalized using a max-min normalization and thereafter expanded into a 101-dimensional one-hot vectors with 0.01 buckets, that are used as index in an embedding table to find the \ud835\udc41 -dimensional embedding. We show in Figure 3 the embedding of non-textual features used in BERT4CTR. Similar with NumBERT, the attention alignment score in textual part is calculated as a dot-product form where the dimensions of Query and Key should be the same. However, in the non-textual part after dimensionality reduction, there is not anymore guarantee that the dimension of embedding in non-textual part is equivalent to the one in textual part. Considering the flexibility of our model, we introduce additive attention [1] instead of dot-product attention in non-textual part. Additive attention, also known as Bahdanau attention, uses a one-hidden layer feed-forward network to calculate the attention alignment score and the formula for attention Figure 3: Dimensionality Reduction on embedding of nontexual features in BERT4CTR K-Dim Feature Embedding NDim NDim Concat N-Dim Extended Embeddine 0 | 0 One-Hot Vector Max-Min Normalization Raw Feature Values alignment score between Query and Key is as follows:  where [ \ud835\udc44 ; \ud835\udc3e ] is the concatenation of Query and Key, and \ud835\udc63 \ud835\udc4e and \ud835\udc4a \ud835\udc4e are learned attention parameters. In [29] it is shown that additive attention and dot-product attention are equivalent in computing the attention alignment score between Query and Key, while additive one does not require Query and Key with same embedding dimensions. In Section 4, we will show in Table 3 and Table 4 that the dimensionality reduction operation proposed here can hold more than 90% of the best AUC achieved with uni-attention while substantially reducing the time-costs of training and inference. 3.2.4 Two-steps Joint-training. The calibration of BERT4CTR consists of joint-training with both textual and non-textual features. In [30] it is shown that a two-steps training can significantly improve the accuracy of prediction on such joint-training framework and inspired with this, BERT4CTR is also trained in two-steps. In the first step, called warm-up step , we pre-train the standard language model with only textual features using a Mask Language Model (MLM) task and then fine-tune this model on the same textual data with click label. The non-textual part with dimensionality reduction, is also pre-trained using a MultiLayer Perceptron (MLP) that predicts the click probability using non-textual features alone. This pre-training phase will calibrate the dimensionality reduction parameters. A cross entropy loss function is used for this learning. The second step of training, called joint-training step , is initialized with the pre-trained textual model, as well as the pre-trained non-textual one, and continues training the whole network of BERT4CTR, mixing textual and non-textual inputs, with a small learning rate. We demonstrate this two-steps joint-training in Figure 4. The results in Section 4 will show that the two-steps joint-training provides significant AUC gain on both commercial and public data sets.", "4 EXPERIMENTS AND EVALUATIONS": "In this section, we evaluate BERT4CTR over two data sets: one is from Microsoft Bing Ads, called commercial data set, and the other one is from KDD CUP 2012, called public data set. At first, we will describe the experimental settings on the data sets, the pre-trained language models, the baselines, the evaluation metrics and the environments used. We then compare four incrementally BERT4CTR: An Efficient Framework to Combine Pre-trained Language Model with Non-textual Features for CTR Prediction Conference'17, July 2017, Washington, DC, USA Figure 4: Two-steps Joint-training in BERT4CTR Pre-trained and Fine-tuned Language Model Pooline Tias] Initializatidn Well-trained Neural Network with Non-textual Features MLP [cLs] (SEP] okc TokD Ioka [seP] Tokc N-Dim N-Dim N-Oim N-Dim N-Dim Textual Part Initialization Dens Densc Non-tertuaip MLP complete versions of the proposed framework, followed by the introductions in Section 3, i.e. , NumBERT alone, with uni-attention added, with dimensionality reduction for non-textual feature embedding added, and with the two-steps joint-training. This will show the improvements coming from each individual component gradually. KDD CUP 2012 Data Set 1 : We also use in our experiments a public data set coming from KDD CUP 2012 Track 2 [22][31]. This data set contains 235 million < \ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5f\ud835\udc66, \ud835\udc4e\ud835\udc51 > pairs with click label, sampled from the logs of Tencent search engine Soso.com. This data set contains 57 non-textual features which can also be classified into dense and sparse features, along with the position of ads. We will also compare BERT4CTR with three current state-ofthe-art frameworks that can handle multi-modal inputs in CTR prediction. These comparisons will provide evidences that BERT4CTR is an efficient framework to combine pre-trained language model with non-textual features for CTR prediction.", "4.1 Experimental Settings": "4.1.1 Data Sets. We use the following two data sets for evaluation. Moreover, to evaluate the robustness of our work, the experiments on different data sets are also based on different pre-trained language models. Microsoft Bing Ads Data Set : Microsoft Bing Ads is a commercial system used by Microsoft to select the ads presented to users after a requested search. This data set consists of 190 million < \ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5f\ud835\udc66, \ud835\udc4e\ud835\udc51 > pairs with click labels, which are randomly sampled from Bing Ads logs obtained in April, 2022. The samples in the first three weeks of April are used as training set, and the remaining as validation set. Similarly to [30], we use the text of query, and ad title concatenated with ad display URL as two textual features. In addition, a set of 90 non-textual features are also available in this data set, which can be grouped as: (1) dense features representing continuous numerical values, such as historical value of CTR per user, number of historical impressions per ad etc. ; (2) sparse features representing discrete values, such as the user's gender, searching query's category etc. ; (3) ads position, a special feature in CTR prediction. As in [14][30], the displayed position of an ad is assumed to be independent of other features, i.e. , we consider that the displayed position and the quality of an ad are independent on the likelihood of a click. However, in this data set, there is no time information as in Bing Ads, meaning that it is not possible to split the training and validation data based on time. Thus, we have generated the training and validation set by randomly selecting 1/11 of samples as validation data and the remaining as training data. 4.1.2 Pre-trained Language Model Settings. The textual part of Bing Ads data set is initialized over the RoBERTa-Large model with 24 layers (abbreviated as RoBERTa-24) created by Facebook [16]. The pre-training of the RoBERTa-24 model is done using the popular Mask Language Model (MLM) task similarly to [4][30]. For the textual part in KDD CUP 2012 data set, a BERT-Base model with 12 layers (abbreviated as BERT-12) [4] is pre-trained with MLM task and then used as initial model for further experiments. To enable reproducibility, we present all the details of the experiments run on the KDD CUP 2012 data, including the data preprocessing steps, hyper-parameter settings and pseudo codes, in the appendix. 4.1.3 Baseline Setups. We compare here BERT4CTR with three state-of -the-art frameworks handling pre-trained language model and non-textual features for CTR prediction. The first baseline framework, called Cascading Framework , is a traditional way to introduce pre-trained language models in CTR prediction. It consists of injecting the outcome (final score or intermediate embedding) of language model fine-tuned on the textual inputs alone as a new input feature, along with the nontextual features, for the CTR prediction. Here we first fine-tune one language model (RoBERTa-24 or BERT-12) sufficiently with 1 https://www.kaggle.com/c/kddcup2012-track2 Conference'17, July 2017, Washington, DC, USA Dong Wang, Kav\u00e9 Salamatian, Yunqing Xia, Weiwei Deng, and Qi Zhang only < \ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5f\ud835\udc66, \ud835\udc4e\ud835\udc51 > textual pairs, and then feed the predicted score of such fine-tuned language model as a new feature into a downstream CTR prediction model. To show the generality of our work, we choose three different CTR prediction models: (1) Wide & Deep [3], introduced by Google, which combines a shallow linear model with a deep neural network; (2) DeepFM [7], an improved version of Wide & Deep, which replaces the linear model with a FactorizationMachine (FM); (3) NN boosted GBDT [14], used for Microsoft Bing Ads, that consists of a Neural Network (NN) boosted with a Gradient Boosting Decision Tree (GBDT) ensemble model. The second baseline is called Shallow Interaction Framework , which is also widely used in practice [2][32][21]. It consists of fusing the non-textual embedding layer and the last layer of pretrained language model, e.g. , the [CLS] pooling layer, through an aggregation layer. We use two variants of this approach: the first one, called Shallow Interaction-1 Layer , connects the language model and the non-textual embedding layer directly through a MultiLayer Perception (MLP). The second one, called Shallow Interaction-N Layers , uses the same number of feed-forward network (FFN) and residual network layers stacked above the non-textual embedding layer, as the ones used in the language model, followed by a MLP. The second variant provides a more fair comparison as the depths of network in textual and non-textual part are the same as BERT4CTR. The third baseline is the NumBERT framework [34] described in Section 3.2.1. 4.1.4 Evaluation Metrics. The Area Under the ROC Curve (AUC) [6] and Relative Information Gain (RIG) [20] are two crucial metrics to evaluate the performance of predictive models, which are also used in our evaluations. Besides the measurements on the whole validation data (called as ALL Slice ), we also focus on the infrequent < \ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5f\ud835\udc66, \ud835\udc4e\ud835\udc51 > pairs (called Tail Slice ), which could lead to cold starting problem in CTR prediction. As reported in [7], 0.1% improvement on AUC or RIG can be seen as significant gain for industrial use. We also use t-test results with \ud835\udefc = 0 . 05 to compare the performances of different models, i.e. , a difference between two AUCs (or RIGs) with t-value larger than 3 can be considered as significant and confident [17]. Besides the AUC and RIG, we also use the average, median, 90th percentile and 95th percentile of the time-costs (milliseconds per sample), both for training and inference, as two additional performance metrics. These two metrics are important for CTR prediction in practice. First, the CTR prediction models have to update frequently to adapt to user's interest drift, e.g. , the CTR model is refreshed weekly in Microsoft Bing Ads. Therefore, the training time should be less than the refreshing interval. Second, the timecost in inference is directly related to the online serving latency and should be set as low as possible. It is noteworthy that for different frameworks, the calculations of time-cost are also different. In terms of cascading framework, the time-cost in training/inference is the summation of time-cost in training/inference of language model and the one in training/inference of downstream CTR prediction model. For both Shallow Interaction and BERT4CTR which need two-steps joint-training, the time-costs in training are calculated as the summation of time taken in warm-up step and the one in jointtraining step, while the time-cost in training of NumBERT is only considered as the time taken in pre-training and fine-tuning. The time-costs in inference of all these three no-cascading frameworks are measured as the time taken in single prediction by language models. 4.1.5 Environments. All model evaluations are implemented with TensorFlow running on NVIDIA V100 GPUs with 32GB memory. The maximum lengths of sequence for < \ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5f\ud835\udc66, \ud835\udc4e\ud835\udc51 > are set as 64, and the batch sizes are set as 10 on both RoBERTa-24 and BERT-12 model. To avoid the random deviation, each experiment for timecost is repeated for twenty times to obtain metrics. Without explicit statement, all AUCs and RIGs shown in this section are obtained at the best step during training.", "4.2 Performance of Components in BERT4CTR": "In this section, we evaluate the improvement on model performance coming from each individual component of BERT4CTR described in Section 3. 4.2.1 NumBERT's Performance. We present in Table 1 the performance of NumBERT for CTR prediction over the two data sets used in this paper. In this case two baseline models can be used for comparison. The first one, called TextOnly , uses the pre-trained language model fine-tuned with only < \ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5f\ud835\udc66, \ud835\udc4e\ud835\udc51 > textual input and without any non-textual features. The second one is the Shallow Interaction-1 Layer described above. We show in the table along with absolute value of AUC and RIG for each model, the difference of metrics between two models with t-values, e.g. , \u0394 \ud835\udc34\ud835\udc48\ud835\udc36 \ud835\udc40 3 -\ud835\udc40 1 , the AUC difference between Model 3 (NumBERT) and Model 1 (TextOnly). One can observe, from Table 1, that NumBERT has been able to benefit from non-textual features. It brings, when compared with the model without non-textual features, 2.7% AUC improvement over Bing Ads data and 6.8% AUC improvement on KDD CUP 2012 data. However, compared with the Shallow Interaction model, NumBERT does not provide benefits on both AUC and RIG, and shows worse performance. This means that even if NumBERT allows textual and non-textual features to interact through complex bidirectional self-attention with multi-layers, it is not efficient in learning the cross-information between multi-modal signals. 4.2.2 Uni-Attention's Performance. We follow up with evaluating the improvements coming from uni-attention architecture. We show in Table 2 the performance achieved by NumBERT compared with NumBERT + Uni-Attention , i.e. , transformed non-textual features (as depicted in Figure 1) are fed to uni-attention architecture, as shown in Figure 2. Table 2 shows that the uni-attention architecture can bring significant AUC and RIG gains, compared with the NumBERT model without uni-attention, over both data sets. For example, the uniattention architecture can bring additional 0.3% AUC gain and 0.5% RIG gain on Tail Slice of Bing Ads data. These gains are even more obvious for KDD CUP 2012 data, where the AUC gain is 0.5% and the RIG improves by 0.6% over Tail Slice. All these changes are statistically significant with t-values larger than 70. 4.2.3 Dimensionality Reduction's Performance. Here, we evaluate the impact of dimensionality reduction of non-textual features, BERT4CTR: An Efficient Framework to Combine Pre-trained Language Model with Non-textual Features for CTR Prediction Conference'17, July 2017, Washington, DC, USA Table 1: AUC and RIG performance of NumBERT on two data sets non-textual part; (3) Two-steps joint-training where both weights in textual part and non-textual part are initialized with the weights trained in advance as described in Section 3.2.4. This last setting is the one used for the BERT4CTR model introduced in this paper. Table 2: AUC and RIG performance of Uni-Attention on two data sets shown in Figure 3, that is made mandatory because of the large number of non-textual inputs in industrial CTR prediction models. The performances of dimensionality reduction on the two data sets are shown in Table 3, where NumBERT + Uni-Attention + Dimensionality Reduction is the NumBERT model with uni-attention framework, as in Figure 2, that is completed with a dimensionality reduction operation in non-textual part, as shown in Figure 3. Table 3 reports that AUC and RIG for both alternative models are close on the two data sets. Besides, no one of the performance differences is statistically significant, i.e. , the performance equality hypothesis cannot be refuted. Besides the accuracy of prediction, the time-costs in training and inference of these two models are also evaluated in Table 4. One can observe, that dimensionality reduction reduces strongly the time-cost, up to 45% of training cost and 24% of inference cost on KDD CUP 2012 data, with 57 non-textual features, and up to 68% in training and 43% in inference on Bing Ads data with 90 non-textual features. This means that dimensionality reduction does not entail a significant performance reduction while reducing obviously the time-costs. Table 3: AUC and RIG performance of Dimensionality Reduction on two data sets 4.2.4 Two-steps Joint-training's Performance. The last component to be evaluated is the two-steps joint-training described in Section 3.2.4. For this purpose, we compare three initialization approaches for the textual and non-textual parts: (1) Pre-trained but not finetuned language model for textual part + Random weights in nontextual part (abbreviated as No Fine-tuned + Randomly Initialized in Table 5); (2) Fine-tuned weights in textual part + Random weights in non-textual part (abbreviated as Fine-tuned + Randomly initialized in Table 5), where the weights in the textual part are initialized using the language model pre-trained and fine-tuned on our < \ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5f\ud835\udc66, \ud835\udc4e\ud835\udc51\ud835\udc60 > textual pairs, and random initial weights are used in Table 5 shows the AUC/RIG performance of these three settings on both data sets. From this table, one can find that two-steps jointtraining brings significant gain for both data sets. On Bing Ads data, the AUC gain is more than 0.3% , and more than 0.4% over KDD CUP 2012 data. All these gains are shown by the t-tests to be significant. 4.2.5 Aggregated Training Loss. WeshowinFigure 5 the evaluation of training loss for all the alternative models. The aggregated logloss is derived after training per million samples, and the trends are reported in Figure 5 for the first training epoch over Bing Ads data set. Figure 5: Curves of aggregated training loss on Bing Ads data Aggregated Training Loss 0.47 0.44 0.43 0.41 100 120 140 Number Training Samples TextOnly NumBERT NumBERT with Uni-Attention Dimensionality Reduction with Uni-Attention BERT4CTR The figure leads to four observations. First, the training loss of the model without non-textual features ( i.e. , TextOnly model) is higher than the ones of the other alternative models, indicating that non-textual features are important for CTR prediction. Second, training loss for NumBERT with uni-attention is below the one of NumBERT, which provides another evidence that uni-attention architecture improves CTR prediction. Third, the training loss curves for NumBERT with uni-attention are close, with and without dimensionality reduction. This means that dimensionality reduction does not compromise the accuracy of prediction much while reducing the time-costs of training and inference. Finally, training loss for BERT4CTR is the lowest one , showing clearly that the two-steps joint-training improves the performance of CTR prediction. These observations are consistent with the ones obtained based on AUC and RIG metrics. Conference'17, July 2017, Washington, DC, USA Dong Wang, Kav\u00e9 Salamatian, Yunqing Xia, Weiwei Deng, and Qi Zhang (a) Training Cost (b) Inference Cost", "Table 4: Time-cost performance (ms/sample) of Dimensionality Reduction on two data sets": "Table 5: AUC and RIG performance of Two-steps Joint-training on two data sets", "4.3 Comparison of BERT4CTR with Other Multi-modal Frameworks": "RoBERTa-24 model. The training takes 5 days in one cycle. According to Table 7, BERT4CTR will take 5.8 days on the same settings, that is still less than the weekly re-calibration deadline. In this part, we compare BERT4CTR performances with the three alternative frameworks, cascading framework, Shallow Interaction framework and NumBERT, that can handle multi-modal inputs for CTR prediction. In Table 6, the AUC and RIG performance are shown for all possible alternatives. Three major observations can be extracted from this table. First, cross-information learning between textual and non-textual features during fine-tuning phase can improve the accuracy of prediction significantly, e.g. , BERT4CTR brings more than 0.5% AUC gain on both Bing Ads data and KDD CUP2012 data, compared with all cascading methods (Wide & Deep, DeepFM and NN+GBDT). Second, although increasing the depth of network in non-textual part improves the accuracy of CTR prediction, deep uni-attentions between textual features and non-textual features still bring considerable improvement for CTR prediction, e.g. , BERT4CTR can bring 0.4% AUC gain on both Bing Ads data and KDD CUP 2012 data, compared with Shallow Interaction-N Layers model, showing the pure benefits brought by the uni-attention architecture. Finally, among all seven alternative models in Table 6, BERT4CTR shows the highest AUC on both data sets, which gives evidence that the design presented in Section 3 is an effective way to learn the cross-information between multi-modal inputs for CTR prediction. The time-costs of training and inference for the alternative models are shown in Table 7. At first, we observe that BERT4CTR does not bring significant increases in training time compared with Shallow Interaction. In detail, the training time of BERT4CTR only increases by 7% compared with Shallow Interaction-N Layers and by 14% compared with Shallow Interaction-1 Layer that have been widely used in industry [30][2]. For example, in Microsoft Bing Ads, Shallow Interaction-1 Layer framework is used to refresh a The inference delay of BERT4CTR is close to cascading, and Shallow Interaction framework, and much less than NumBERT, e.g. , BERT4CTR can reduce inference delay by 52% (resp., 29%) on Bing Ads data (resp., KDD CUP 2012 data), compared with NumBERT. The results from Table 6 and Table 7 give strong evidences that BERT4CTR can achieve both high accuracy and low training and inference delay for CTR prediction.", "5 DISCUSSION": "Although, we used in this paper the CTR prediction with numerical features as our main applicative scenario, the framework of BERT4CTR proposed is applicable to other scenarios mixing textual and non-textual features. For example, one can extract the representative embedding from images through VGGNet [25], ResNet [9] etc. to replace the token \ud835\udc38 \ud835\udc38 in Figure 4 and calculate the uniattentions. Besides, the Knowledge-Distillation technology [10] can be adopted on BERT4CTR, where a light model handling textual and non-textual inputs, with uni-attention and dimensionality reduction, can be learned under the supervision of predicted scores from a well-trained BERT4CTR model with deep layers.", "6 CONCLUSION": "In this paper, we focused on the design of an efficient framework to combine pre-trained language model with non-textual features for CTR prediction. We started from NumBERT which is the traditional usage of pre-trained language model to integrate textual features and numerical ones, and introduced three improvements, uniattention, dimensionality reduction and two-steps joint-training, BERT4CTR: An Efficient Framework to Combine Pre-trained Language Model with Non-textual Features for CTR Prediction Conference'17, July 2017, Washington, DC, USA (a) AUC Performance", "(b) RIG Performance": "Table 6: AUC and RIG performance of BERT4CTR compared with representative models in use on two data sets (a) Training Cost", "(b) Inference Cost": "Table 7: Time-cost performance (ms/sample) of BERT4CTR compared with representative models in use on two data sets to compose a novel framework BERT4CTR. Comprehensive experiments on both commercial data and public data showed that BERT4CTR can achieve significant gains in accuracy of prediction while keeping low time-costs of training and inference, and therefore provide a promising solution of CTR prediction in the real world.", "REFERENCES": "[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473 (2014). [2] Qiwei Chen, Huan Zhao, Wei Li, Pipei Huang, and Wenwu Ou. 2019. Behavior sequence transformer for e-commerce recommendation in alibaba. In Proceedings of the 1st International Workshop on Deep Learning Practice for High-Dimensional Sparse Data . 1-4. [3] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems . 7-10. [4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018). [5] Miao Fan, Jiacheng Guo, Shuai Zhu, Shuo Miao, Mingming Sun, and Ping Li. 2019. MOBIUS: towards the next generation of query-ad matching in baidu's sponsored search. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 2509-2517. [6] Tom Fawcett. 2006. An introduction to ROC analysis. Pattern recognition letters 27, 8 (2006), 861-874. [7] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. arXiv preprint arXiv:1703.04247 (2017). [8] Weiwei Guo, Xiaowei Liu, Sida Wang, Huiji Gao, Ananth Sankar, Zimeng Yang, Qi Guo, Liang Zhang, Bo Long, Bee-Chung Chen, et al. 2020. Detext: A deep text ranking framework with bert. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management . 2509-2516. [9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition . 770-778. [10] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 (2015). [11] Bernard J Jansen and Tracy Mullen. 2008. Sponsored search: an overview of the concept, history, and technology. International Journal of Electronic Business 6, 2 (2008), 114-131. [12] Yunjiang Jiang, Yue Shang, Ziyang Liu, Hongwei Shen, Yun Xiao, Wei Xiong, Sulong Xu, et al. 2020. BERT2DNN: BERT Distillation with Massive Unlabeled Data for Online E-Commerce Search. arXiv preprint arXiv:2010.10442 (2020). Conference'17, July 2017, Washington, DC, USA Dong Wang, Kav\u00e9 Salamatian, Yunqing Xia, Weiwei Deng, and Qi Zhang [13] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942 (2019). [14] Xiaoliang Ling, Weiwei Deng, Chen Gu, Hucheng Zhou, Cui Li, and Feng Sun. 2017. Model ensemble for click prediction in bing search ads. In Proceedings of the 26th International Conference on World Wide Web Companion . 689-698. [15] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. 2019. Multitask deep neural networks for natural language understanding. arXiv preprint arXiv:1901.11504 (2019). [16] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019). [17] Edward H Livingston. 2004. Who was student and why do we care so much about his t-test? 1. Journal of Surgical Research 118, 1 (2004), 58-65. [18] Wenhao Lu, Jian Jiao, and Ruofei Zhang. 2020. TwinBERT: Distilling Knowledge to Twin-Structured Compressed BERT Models for Large-Scale Retrieval. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management . 2645-2652. [19] H Brendan McMahan, Gary Holt, David Sculley, Michael Young, Dietmar Ebner, Julian Grady, Lan Nie, Todd Phillips, Eugene Davydov, et al. 2013. Ad click prediction: a view from the trenches. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining . 1222-1230. [20] Andrew W Moore. 2001. Information gain. School of Computer Science, Carnegie Mellon University, http://www. cs. cmu. edu/\u02dc awm/tutorials (2001). [21] Aashiq Muhamed, Iman Keivanloo, Sujan Perera, James Mracek, Yi Xu, Qingjun Cui, Santosh Rajagopalan, Belinda Zeng, and Trishul Chilimbi. 2021. CTR-BERT: Cost-effective knowledge distillation for billion-parameter teacher models. In Proceedings of the 35th Conference on Neural Information Processing Systems (NeurIPS) . 1-7. [22] Feiyang Pan, Shuokai Li, Xiang Ao, Pingzhong Tang, and Qing He. 2019. Warm up cold-start advertisements: Improving ctr predictions via learning to learn id embeddings. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval . 695-704. [23] Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, and Jun Wang. 2016. Product-based neural networks for user response prediction. In 2016 IEEE 16th International Conference on Data Mining (ICDM) . IEEE, 1149-1154. [24] Lalita Sharma and Anju Gera. 2013. A survey of recommendation system: Research challenges. International Journal of Engineering Trends and Technology (IJETT) 4, 5 (2013), 1989-1992. [25] Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014). [26] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. 2019. Vl-bert: Pre-training of generic visual-linguistic representations. arXiv preprint arXiv:1908.08530 (2019). [27] Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. 2019. Videobert: A joint model for video and language representation learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 74647473. [28] Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, and Hua Wu. 2019. Ernie: Enhanced representation through knowledge integration. arXiv preprint arXiv:1904.09223 (2019). [29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017). [30] Dong Wang, Shaoguang Yan, Yunqing Xia, Kav\u00e9 Salamatian, Weiwei Deng, and Qi Zhang. 2022. Learning Supplementary NLP Features for CTR Prediction in Sponsored Search. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 4010-4020. [31] Fang Wang, Warawut Suphamitmongkol, and Bo Wang. 2013. Advertisement click-through rate prediction using multiple criteria linear programming regression model. Procedia Computer Science 17 (2013), 803-811. [32] Zhe Wang, Rundong Shi, Shijie Li, and Peng Yan. 2020. GBDT and BERT: a Hybrid Solution for Recognizing Citation Intent. Studies 55 (2020), 12c2a39230188. [33] Puxuan Yu, Hongliang Fei, and Ping Li. 2021. Cross-lingual Language Model Pretraining for Retrieval. In Proceedings of the Web Conference 2021 . 1029-1039. [34] Xikun Zhang, Deepak Ramachandran, Ian Tenney, Yanai Elazar, and Dan Roth. 2020. Do language embeddings capture scales? arXiv preprint arXiv:2010.05345 (2020).", "A APPENDIX": "For the reproducibility, we provide the important details about experiments on the public KDD CUP 2012 data set, including the data description, the settings on textual and non-textual part, and the pseudo code of uni-attention.", "A.1 Data Details": "The data set contains 235 million search ads impressions sampled from session logs of Tencent search engine Soso.com. Each sample in this data set is likewise Bing Ads data and contains five components: \u00b7 Query text: which is a list of anonymous tokens hashed from natural language; \u00b7 Ad text: which includes ad title and ad display URL and are also lists of anonymous tokens; \u00b7 CTRprediction features: there are 56 CTR prediction features including sparse features such as UserID, AdID, user's gender etc. , and dense features such as historical CTRs, number of impressions per Ad/Query/User etc. ; \u00b7 Position feature: a special feature which indicates the impressed position of this ad; \u00b7 Click label: where 1 means this ad has been clicked and 0 is not clicked. The 56 non-textual CTR features used in BERT4CTR are generated from the following ways: \u00b7 ID features: we mapped each raw ID attribute (such as AdID, QueryID, UserID, Gender, Age etc. ) appeared in training data to a consecutive number. To consider the applicability of model on validation data where some new IDs are not appeared in training data, we used the robust training, i.e. , we randomly removed 5% IDs in training data and set them as \"Missing\", which is a special mapped number. In inference, we also set the new IDs in validation data as \"Missing\" to make the model work well; \u00b7 Historical features: we calculated the historical CTRs and number of impressions from different perspectives, including the Ad-level, User-level, Query-level, Gender-level and Agelevel etc. ; \u00b7 Length features: these length features include Query Length, AdTitle Length, AdDesription Length and Keyword Length etc. ; \u00b7 Semantic features: we calculated TF \u00d7 IDF value for each token (such as Query tokens, AdTitle tokens, AdDescription tokens etc. ) provided in training data, as a type of semantic feature. As there is no time information in this data set, it is impossible to split the training data and validation data according to impression time. Alternatively, we take a simple way to generate the training and validation set, where we randomly choose 1/11 of samples as validation data and the remaining as training data. Table 8 summaries the statistics of training data and validation data. Table 8: Statistics for KDD CUP 2012 data set", "A.2 Settings on Textual Part": "We choose BERT-12 model as initial model where query and ad title concatenated with ad display URL are used as two input sentences. BERT4CTR: An Efficient Framework to Combine Pre-trained Language Model with Non-textual Features for CTR Prediction Conference'17, July 2017, Washington, DC, USA As the privacy, each textual token in KDD CUP 2012 data set is anonymous to one hash ID and therefore, it is difficult to map the hash ID from data to vocabulary ID of pre-trained language models. To solve this issue, we treat these anonymous tokens as new words, where we calculate the TF \u00d7 IDF for each anonymous token and choose the top 300 thousands of these to build a new vocabulary. The masking rate is 15% for MLM task and Adam optimizer with learning rate of 1 \u00d7 10 -4 is used in both pre-training and fine-tuning phase. After two epochs of pre-training and four epochs of finetuning with this new vocabulary, the AUC of BERT model can achieve 75.91% on ALL slice, meaning that our solution is effective.", "A.3 Settings on Non-textual Part": "Asdescribed in Section 3, the processing methods for sparse features and dense ones are different. For sparse features, we extent each input to a 32-dimensional embedding respectively. While for dense features, we normalize them with max-min normalization at first and then expand each normalized value into 101-dimensional onehot vectors based on 0.01 buckets. Afterwards the 32-dimensional embedding is generated by looking up embedding table. Totally a 1792-dimensional embedding is generated by concatenating all of 56 32-dimensional sub-embeddings (excluding the embedding from position feature) and then a 512-dimensional hidden layer is followed as the final embedding layer to calculate uni-attention. For uni-attention, we set the dimension of hidden layer in Equation 3, denoted as \ud835\udc51 \ud835\udc4e , as 64, and therefore the network to calculate the attention alignment score between Query and Key in uniattention (based on BERT-12) is depicted in Figure 6. Figure 6: The network to calculate the attention alignment score between Query and Key in uni-attention Attention Alignment Score fatt(0, K) 64 Hidden Layer 768 512 Concatenated Layer K", "A.4 Implementation of Uni-Attention": "We also provide the details on the implementation of uni-attention here, which is the core of BERT4CTR. In one layer of BERT4CTR, the output from the non-textual token after dimensionality reduction can be denoted as \ud835\udc4b \u2208 R \ud835\udc51 \ud835\udc4b \u00d7 1 , where \ud835\udc51 \ud835\udc4b is the dimension of output for this non-textual token (512 in our experiment). Beside, the outputs from textual tokens can be denoted as \ud835\udc4c \u2208 R \ud835\udc51 \ud835\udc4c \u00d7 \ud835\udc59 \ud835\udc4c , where \ud835\udc51 \ud835\udc4c is the dimension of output for each textual token (768 in our experiment) and \ud835\udc59 \ud835\udc4c is the maximum sequence length of textual input (64 in our experiment). We also use the same Attention-Mask mechanism as the one used in textual part, for the calculation of uni-attention, where \ud835\udc40\ud835\udc4e\ud835\udc60\ud835\udc58 \u2208 [ 0 , 1 ] \ud835\udc59 \ud835\udc4c . The pseudo code of uni-attention based on additive attention is shown in Algorithm 1, in which \ud835\udc4a \ud835\udc5e , \ud835\udc4f \ud835\udc5e , \ud835\udc4a \ud835\udc58 , \ud835\udc4f \ud835\udc58 , \ud835\udc4a \ud835\udc63 , \ud835\udc4f \ud835\udc63 , \ud835\udc4a \ud835\udc4e , \ud835\udc4f \ud835\udc4e , \ud835\udc4a \ud835\udc56 and \ud835\udc4f \ud835\udc56 are trainable parameters.", "Algorithm 1 \ud835\udc48 \u2190 \ud835\udc48\ud835\udc5b\ud835\udc56\ud835\udc34\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b ( \ud835\udc4b,\ud835\udc4c, \ud835\udc40\ud835\udc4e\ud835\udc60\ud835\udc58 )": ""}
