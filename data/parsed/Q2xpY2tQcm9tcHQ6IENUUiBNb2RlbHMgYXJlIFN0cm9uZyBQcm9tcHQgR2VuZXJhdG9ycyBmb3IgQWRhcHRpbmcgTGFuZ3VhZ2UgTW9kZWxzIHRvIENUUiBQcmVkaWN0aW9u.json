{"ClickPrompt: CTR Models are Strong Prompt Generators for Adapting Language Models to CTR Prediction": "Jianghao Lin chiangel@sjtu.edu.cn Shanghai Jiao Tong University Shanghai, China Hangyu Wang Shanghai Jiao Tong University", "Yunjia Xi": "xiyunjia@sjtu.edu.cn Shanghai Jiao Tong University Shanghai, China", "Kangning Zhang": "zhangkangning@sjtu.edu.cn Shanghai Jiao Tong University Shanghai, China Bo Chen chenbo116@huawei.com Huawei Noah's Ark Lab Shenzhen, China", "Yanru Qu": "kevinqu16@gmail.com Shanghai Jiao Tong University Shanghai, China", "Ruiming Tang": "tangruiming@huawei.com Huawei Noah's Ark Lab Shenzhen, China", "Weinan Zhang \u2217": "wnzhang@sjtu.edu.cn Shanghai Jiao Tong University Shanghai, China", "ABSTRACT": "Click-through rate (CTR) prediction has become increasingly indispensable for various Internet applications. Traditional CTR models convert the multi-field categorical data into ID features via one-hot encoding, and extract the collaborative signals among features. Such a paradigm suffers from the problem of semantic information loss . Another line of research explores the potential of pretrained language models (PLMs) for CTR prediction by converting input data into textual sentences through hard prompt templates. Although semantic signals are preserved, they generally fail to capture the collaborative information ( e.g. , feature interactions, pure ID features), not to mention the unacceptable inference overhead brought by the huge model size. In this paper, we aim to model both the semantic knowledge and collaborative knowledge for accurate CTR estimation, and meanwhile address the inference inefficiency issue. To benefit from both worlds and close their gaps, we propose a novel model-agnostic framework ( i.e. , ClickPrompt), where we incorporate CTR models to generate interaction-aware soft prompts for PLMs. We design a prompt-augmented masked language modeling (PA-MLM) pretraining task, where PLM has to recover the masked tokens based on the language context, as well as the soft prompts generated by CTR model. The collaborative and semantic knowledge from ID and textual features would be explicitly aligned and interacted via the prompt interface. Then, we can either tune the CTR model with PLM for superior performance, or solely tune the CTR model without PLM for inference efficiency. Experiments on four real-world datasets validate the effectiveness of ClickPrompt compared with existing baselines. The source code 12 is available.", "CCS CONCEPTS": "\u00b7 Information systems \u2192 Recommender systems .", "KEYWORDS": "Pretrained Language Models, CTR Prediction", "ACMReference Format:": "Jianghao Lin, Bo Chen, Hangyu Wang, Yunjia Xi, Yanru Qu, Xinyi Dai, Kangning Zhang, Ruiming Tang, Yong Yu, and Weinan Zhang. 2024. ClickPrompt: CTR Models are Strong Prompt Generators for Adapting Language Models to CTR Prediction . In Proceedings of the ACM Web Conference 2024 (WWW'24), May 13-17, 2024, Singapore, Singapore. ACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/3589334.3645396", "1 INTRODUCTION": "Click-through rate (CTR) prediction serves as a key component in various online applications [8, 10, 21, 35, 59]. It aims to estimate the probability of a user's click given a specific context [36], which hangyuwang@sjtu.edu.cn Shanghai, China", "Xinyi Dai": "daixinyi5@huawei.com Huawei Noah's Ark Lab Shenzhen, China Yong Yu yyu@sjtu.edu.cn Shanghai Jiao Tong University Shanghai, China Soft Prompt Vectors Gender is female. Age is 18-25. Title is The Flash. Movie genre is action. Language Model CTR Model CTR Estimation female 18-25 The Flash action Forward Collaborative Knowledge Forward Semantic Knowledge Backward Semantic Knowledge could be formulated as multi-field categorical data format: Over the past decade, various neural CTR models have been proposed to extract collaborative knowledge and capture high-order feature interaction patterns. However, they generally suffer from the problem of semantic information loss . That is, the multi-field categorical data will be converted into ID features with one-hot encoding as shown in Eq. 1 ( e.g. , ' female ' to ' 01 ', and ' male ' to ' 10 '). Therefore, the input data of CTR models is only a collection of ID codes without any semantic information which inherently contains implicit yet beneficial correlations among features. For example, the movie ' The Avengers 4: Endgame ' is not only related to its preceding series ' The Avengers 1-3 ' based on simple text similarity, but is also associated with other superhero movies ( e.g. , ' Iron Man ' and ' Captain America ') based on latent semantic knowledge. However, once converted into one-hot ID codes, it loses the valuable semantic information, which could lead to inferior predictive performance, especially for scenarios with cold-start users/items, low-frequency long-tail features or inadequate click signals. To this end, recent works [12, 18, 34] start to introduce pretrained language models (PLMs) to address the aforementioned semantic information loss problem. They convert multi-field categorical data into textual features with hard prompt templates instead of ID features with one-hot encoding, resulting in another textual modality for the same input sample \ud835\udc65 \ud835\udc56 : In Template A shown above, the underlined words or phrases need to be dynamically filled in according to the input sample \ud835\udc65 \ud835\udc56 . In this way, these works preserve the semantic information by formulating CTR prediction as either a sequence-to-sequence task [7, 11, 63] or a binary classification task [24, 39]. PLMs possess a vast volume of open-world knowledge from the pretraining corpora and even show impressive emergent abilities ( e.g. , logical reasoning) if the parameter size scales up, which helps to capture the semantic information. Nevertheless, simply adapting PLMs for CTR estimation generally suffers from two limitations, i.e. , predictive inaccuracy and inference inefficiency . The predictive inaccuracy is mainly caused by the disability of PLMs in modeling collaborative knowledge [34, 58]. First , there exists a kind of pure ID features that inherently contains no semantic information ( e.g. , item ID, user ID). The tokenization results of these pure ID features are actually meaningless to PLMs ( e.g. , user ID AX529 might be tokenized as [AX, 52, 9]). Second , PLMs struggle to explicitly capture feature interactions, since all the field-wise features are assembled linearly as textual sentences via templates and then broken down into word tokens [28]. PLMs can model the contextual semantic information among word tokens, but it loses the field-level views of feature interactions which are essential for CTR prediction. Preliminary works address such challenges by introducing additional embedding tables [11], maintaining a set of adapter modules [12], and seeking for better ID indexing strategies [19]. However, the collaborative knowledge embedded among ID features is still under-exploited, e.g. , the field-aware feature interactions are not explicitly maintained. The inference inefficiency issue stems from the intrinsic characteristics of pretrained language models, where bigger model size is required for better language understanding ability [34, 62]. Adapting PLM will greatly increase the computational cost and inference time due to its large-scale stacked attention-based transformer layers. This is unacceptable for real-world time-sensitive online services, where a request should be responded within tens of milliseconds. Many works [17, 47, 60] tend to adopt the whole PLM for training, and pre-cache the output representations of PLM for inference acceleration, which heavily requires storage and computational resources, as well as engineering efforts. Moreover, the pre-caching operation might impair the real-time property of recommender systems and thus hurt the predictive performance. In this paper, we aim to capture both the semantic knowledge and collaborative knowledge for accurate CTR prediction, while tackling the inference inefficiency problem in the meantime. To this end, we propose a novel framework named ClickPrompt , where we regard CTR models 3 as soft prompt generators for PLMs. Specifically, we maintain a CTR model and a pretrained language model, which take as inputs the ID features \ud835\udc65 \ud835\udc3c \ud835\udc37 \ud835\udc56 and textual features \ud835\udc65 \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 \ud835\udc56 , respectively. Aprompt generation layer is placed upon the CTR model to produce learnable soft prompt vectors, which will be fed as prefix states into each layer of PLM. ClickPrompt follows the pretrain-finetune learning schemes [9, 36]. We design a prompt-augmented masked language modeling (PA-MLM) pretraining task. To be specific, we first adopt the token-masking strategy from BERT [9] to obtain the masked textual features \u02c6 \ud835\udc65 \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 \ud835\udc56 . Then, PLM is required to recover the corrupted textual features \u02c6 \ud835\udc65 \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 \ud835\udc56 based on the text context, as well as the soft prompts generated from ID features \ud835\udc65 \ud835\udc3c \ud835\udc37 \ud835\udc56 . As shown in Figure 1, with the soft prompts as the bridges, the ID-based collaborative knowledge will be passed to PLM through forward propagation, and the text-based semantic knowledge would flow back into the CTR model via backpropagation. After pretraining, we propose two different finetuning strategies for CTR prediction: \u00b7 Finetune with PLM . We could tune the CTR model and PLM as a whole, where they are connected by the prompt generation layer. The collaborative knowledge from CTR model and semantic knowledge from PLM would explicitly align and interact with each other through the soft prompt interface, resulting in superior CTR performance. \u00b7 Finetune w/o PLM . To further tackle the inference inefficiency issue, we can solely finetune the CTR model without PLM. PAMLM pretraining has provided semantic-aware parameter initialization for downstream CTR finetuning, which promotes the final performance without altering the CTR model structure or adding extra inference costs. ClickPrompt serves as a model-agnostic framework that is compatible with various CTR models and pretrained language models. The main contributions of this paper are concluded as follows: \u00b7 We propose a novel framework ( i.e. , ClickPrompt), where CTR models serve as the soft prompt generators for PLMs. A promptaugmented masked language modeling pretraining (PA-MLM) task is designed to model the mutual interaction and explicit alignment between the collaborative and semantic knowledge via the soft prompt interface, which significantly improves the downstream CTR performance. \u00b7 ClickPrompt is model-agnostic and compatible with various CTR models and PLMs. Moreover, by solely finetuning the CTR model, ClickPrompt can enhance the predictive accuracy without altering the CTR model structure or adding extra inference costs. \u00b7 Extensive experiments on four real-world public datasets demonstrate the superiority of our proposed ClickPrompt, compared with existing baseline models.", "2 PRELIMINARIES": "", "2.1 Traditional CTR Prediction": "Without loss of generality, the basic form of CTR prediction casts a binary classification problem over multi-field categorical data. Each data sample contains \ud835\udc39 fields with each field taking one single value from multiple categories, and can be represented by ( \ud835\udc65 \ud835\udc56 , \ud835\udc66 \ud835\udc56 ) . In traditional CTR prediction, we apply one-hot encoding to convert \ud835\udc65 \ud835\udc56 into a sparse vector \ud835\udc65 \ud835\udc3c \ud835\udc37 \ud835\udc56 as shown in Eq. 1, and maintain \ud835\udc66 \ud835\udc56 \u2208 { 1 , 0 } as the ground-true label (click or not). CTR models estimate the click probability \ud835\udc43 ( \ud835\udc66 \ud835\udc56 = 1 | \ud835\udc65 \ud835\udc56 ) for each instance. According to [36, 55, 64], the structure of most traditional neural CTR models can be abstracted into three layers: (1) embedding layer, (2) feature interaction layer, and (3) prediction layer. Embedding layer transforms the sparse one-hot input \ud835\udc65 \ud835\udc3c \ud835\udc37 \ud835\udc56 into low-dimensional dense embedding vectors E \ud835\udc56 = [ \ud835\udc63 \ud835\udc56 1; \ud835\udc63 \ud835\udc56 2; . . . ; \ud835\udc63 \ud835\udc56\ud835\udc39 ] \u2208 R \ud835\udc39 \u00d7 \ud835\udc51 , where \ud835\udc51 is the embedding size, and \ud835\udc39 is the number of fields. Feature interaction layer , as the key functional module of CTR models, is intended to capture the second- or higher-order feature interactions with various operations ( e.g. , attention, product). This layer would generate a compact representation \ud835\udc5e \ud835\udc56 based on the dense embedding vectors E \ud835\udc56 for the data instance \ud835\udc65 \ud835\udc56 . Prediction layer calculates the click probability \u02c6 \ud835\udc66 \ud835\udc56 = \ud835\udc43 ( \ud835\udc66 \ud835\udc56 = 1 | \ud835\udc65 \ud835\udc56 ) based on the representation \ud835\udc5e \ud835\udc56 produced by the feature interaction layer. It is usually a linear layer or an MLP module followed by a sigmoid function \ud835\udf0e ( \ud835\udc65 ) = 1 /( 1 + \ud835\udc52 -\ud835\udc65 ) . After the prediction layer, the CTR model is trained in an endto-end manner with the binary cross-entropy (BCE) loss: where \ud835\udc41 is the number of training samples.", "2.2 PLM-based CTR Prediction": "With the rising of pretrained language models (PLMs), researchers exploit the semantic-related abilities of PLMs to solve the CTR estimation problem. Different from traditional CTR prediction, the input \ud835\udc65 \ud835\udc56 is transformed into textual sentences \ud835\udc65 \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 \ud835\udc56 via hard prompt templates as illustrated in Template A. According to the task genre and ground-truth label formulation, PLM-based CTR prediction can be roughly divided into two categories [34, 58]. The first one [2, 39, 47] regards CTR prediction as a binary text classification task, where the ground-truth labels are still the same as traditional settings ( i.e. , \ud835\udc66 \ud835\udc56 \u2208 { 0 , 1 } ). They leverage PLMs to extract the dense representation \ud835\udc5e \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 \ud835\udc56 of textual input \ud835\udc65 \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 \ud835\udc56 , which is followed by the prediction layer for click estimation. BCE loss is adopted for optimization. Thesecond category [7, 11, 18] views CTR prediction as a sequenceto-sequence task, where the ground-truth labels are transformed into binary key words ( e.g. , yes/no, good/bad). They utilize encoderdecoder or decoder-only PLMs to follow instructions and answer a binary question ( e.g. , Will the user favor the item? ) appended behind the textual input \ud835\udc65 \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 \ud835\udc56 . PLMs could be either frozen for zero-shot settings, or finetuned via causal language modeling. In this paper, we generally focus on the first category. That is, we place an MLP module upon the textual representation \ud835\udc5e \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 \ud835\udc56 produced by the PLM.", "3 METHODOLOGY": "In this section, we introduce the details of model architecture and learning strategy of our proposed ClickPrompt framework.", "3.1 Overview of ClickPrompt": "As depicted in Figure 2, the model architecture design of ClickPrompt can be mainly divided into three stages: (1) modality transformation, (2) prompt generation, and (3) prompt fusion. Firstly , the modality transformation layer converts the input data \ud835\udc65 \ud835\udc56 into one-hot ID features \ud835\udc65 \ud835\udc3c \ud835\udc37 \ud835\udc56 and textual features \ud835\udc65 \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 \ud835\udc56 , respectively. Secondly , ID features \ud835\udc65 \ud835\udc3c \ud835\udc37 \ud835\udc56 are fed into the CTR model followed by a prompt generation layer to produce independent soft prompt vectors. Finally , during the prompt fusion stage, the soft prompts serve as prefix hidden states at each transformer layer of PLM, which allows for the explicit alignment between collaborative and semantic knowledge. As for the learning strategy, ClickPrompt adopts the common pretrain-finetune scheme. We first design a prompt-augmented masked language modeling (PA-MLM) task for pretraining, where PLMisrequired to recover the masked tokens based on text contexts as well as soft prompts generated by the CTR model. After pretraining, we can conduct supervised finetuning either with or without PLM. The former enables explicit interaction between collaborative CTR Model CTR Model Soft Prompts CTR Model Pooling &  Prediction Layer Tokenization &  Work Embedding Layer ... ... ... ... ... ... Prompt Generation Layer User ID is 523. Gender is male. Age is above 18. Location is New York. Item ID is 9635. Brand is Nike. Title is Air Shoes. User_523 New York Air Shoes User ID Gender Age Location Item ID Brand Title Click 523 male above 18 New York 9635 Nike Air Max Shoes 1 Dataset Modality Transformation Layer 3 ... Prompt Fusion Modality Transformation 1 Prompt Generation 2 Model  Architecture Language Model CTR Model Original ID Features Masked Textual Features Soft Prompts Recovered Tokens PA-MLM  Pretraining I D Features Textual Features Finetune with PLM I D Features Finetune w/o PLM Soft Prompt Vector Hidden State for Word Token Semantic-aware Initialization Collaborative-aware Initialization Language Model and semantic information for superior performance, while the latter addresses the inference inefficiency issue. Hereinafter, we omit the detailed structures of the CTR model and PLM, since ClickPrompt acts as a model-agnostic framework for both of them. where \ud835\udc5d \ud835\udc56,\ud835\udc59,\ud835\udc58 denotes the \ud835\udc58 -th prompt vector at the \ud835\udc59 -th layer of PLM. \ud835\udc3f is the number of transformer layer of PLM, and \ud835\udc3e is the number of soft prompts per layer. Each projection network \ud835\udc54 \ud835\udc59,\ud835\udc58 (\u00b7) is designed as a multi-layer perceptron (MLP), facilitating dimensionality consistency and space transformation.", "3.2 Modality Transformation": "The modality transformation layer converts the input \ud835\udc65 \ud835\udc56 into two different modalities ( i.e. , ID features \ud835\udc65 \ud835\udc3c \ud835\udc37 \ud835\udc56 and textual features \ud835\udc65 \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 \ud835\udc56 respectively). The ID features \ud835\udc65 \ud835\udc3c \ud835\udc37 \ud835\udc56 are obtained via one-hot encoding as shown in Eq. 1. As for the textual features \ud835\udc65 \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 \ud835\udc56 , previous works [4, 16] suggest that sophisticated templates for tabular data might mislead the model and make it fail to grasp the key information among the texts. Therefore, we employ the following simple ' what is what ' hard prompt template: where \ud835\udc53 \ud835\udc5b\ud835\udc4e\ud835\udc5a\ud835\udc52 \ud835\udc57 is the field name of \ud835\udc57 -th field, \ud835\udc53 \ud835\udc56,\ud835\udc57 is the feature value of the \ud835\udc57 -th field in \ud835\udc56 -th data instance, and [\u00b7] denotes the conjunct operator to concatenate elements in the list with white spaces ' '.", "3.3 Prompt Generation": "The prompt generation stage aims to encode the ID features \ud835\udc65 \ud835\udc3c \ud835\udc37 \ud835\udc56 into independent soft prompt vectors that contain rich collaborative knowledge for later fusion. As described in Section 2.1, we feed the ID input \ud835\udc65 \ud835\udc3c \ud835\udc37 \ud835\udc56 through the embedding and feature interaction (FI) layer of the CTR model to obtain the compact representation \ud835\udc5e \ud835\udc56 : Then, we maintain a set of parallel projection networks { \ud835\udc54 \ud835\udc59,\ud835\udc58 (\u00b7)} for soft prompt generation:", "3.4 Prompt Fusion": "As shown in Figure 2, the obtained soft prompts would serve as prefix hidden states at each transformer layer of PLM. To be specific, the textual features \ud835\udc65 \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 \ud835\udc56 are tokenized into \ud835\udc4d word tokens, and the \ud835\udc59 -th layer of PLM can be formulated as: where [ \u210e \ud835\udc56,\ud835\udc59,\ud835\udc67 ] \ud835\udc4d \ud835\udc67 = 1 are the hidden states of tokens at each layer \ud835\udc59 . In this way, through the self-attention mechanism of each transformer layer, the collaborative signals from the CTR model can be explicitly aligned and fused with the semantic knowledge from the text side via the prompt interface. Finally, after the \ud835\udc3f layer propagation, we apply the pooling & prediction layer upon the output states of PLM: The output dimensionality, as well as the following activation and loss function, depends on the task and learning strategy we adopt, which will be further discussed in Section 3.5.", "3.5 Learning Strategy": "As shown in Figure 2, ClickPrompt employs the common pretrainfinetune scheme for learning strategy. Specifically, we first propose prompt-augmented masked language modeling (PA-MLM) as the pretraining task to intermingle the collaborative and semantic knowledge via the linkage of soft prompts, resulting in improved parameter initialization. Next, we could either perform supervised finetuning with PLM for superior CTR performance, or solely finetune the CTR model without PLM to preserve both the improved predictive accuracy and inference efficiency. 3.5.1 Prompt-augmented Masked Language Modeling. As shown in Figure 2, we propose to apply token masking on the textual features \ud835\udc65 \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 \ud835\udc56 to obtain corrupted textual inputs \u02c6 \ud835\udc65 \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 \ud835\udc56 , while preserving the original ID features \ud835\udc65 \ud835\udc3c \ud835\udc37 \ud835\udc56 . Then, PLM is required to recover the masked tokens based on the language context, together with the soft prompts generated from the intact ID features. Therefore, the pooling & prediction layer in Eq. 8 is designed as the classical decoder module of language models, which is followed by a softmax function and cross-entropy loss. Following [9, 44], we uniformly sample 15% tokens for each input \ud835\udc65 \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 \ud835\udc56 , and perform three different operations with ratio 8:1:1, i.e. , (1) [MASK] replacement, (2) random word replacement, and (3) keeping unchanged. To complete such a cloze task over masked tokens, PLM has to extract and incorporate the corresponding 'right answer' embedded in the soft prompts, resulting in fine-grained alignments between the CTR model and PLM towards the same input \ud835\udc65 \ud835\udc56 . 3.5.2 Finetuning with PLM. Obviously, we can retain the whole model structure, and continue supervised finetuning for downstream CTR prediction task. As illustrated in Figure 2, we integrate the predictions from both CTR model and PLM, while they are explicitly interacted with soft prompt vectors: where \ud835\udefc is a learnable parameter to balance the weights of predictions, and \ud835\udf0e (\u00b7) is the sigmoid function. In this way, the collaborative and semantic knowledge from two modalities are thoroughly connected and intertwined during finetuning, thus leading to superior CTR performance. 3.5.3 Finetuning without PLM. To further address the inference inefficiency issue, as depicted in Figure 2, we could solely finetune the CTRmodel without PLM. We have injected the semantic knowledge from PLM into the CTR model through backpropagation during PA-MLM pretraining. Hence, such a semantic-aware parameter initialization would enable implicit interactions between collaborative and semantic knowledge, enhancing CTR performance without altering the CTR model structure or adding extra inference cost: For both two finetuning strategies, we apply the binary cross entropy loss over the estimated click probability as shown in Eq. 2.", "4 EXPERIMENT": "In this section, we conduct extensive experiments to answer the following research questions: RQ1 How does ClickPrompt perform compared with existing baseline models? RQ2 Is ClickPrompt compatible with various CTR models and pretrained language models? RQ3 What are the influences of different model configurations for ClickPrompt? RQ4 How does ClickPrompt perform in scenarios with long-tail low-frequency users or items?", "4.1 Experiment Setups": "4.1.1 Datasets. Since PLM-based CTR prediction requires datasets to maintain the original semantic/textual features, instead of anonymous feature IDs, we select four real-world public datasets from different recommendation scenarios ( i.e. , MovieLens-1M 4 , BookCrossing 5 , Amazon-Toys 6 , and GoodReads 7 ). All datasets are divided into training, validation, and testing sets with proportion 8:1:1 according to the global timestamps. The basic statistics of these four datasets are summarized in Table 1. More detailed information about the datasets and data preprocessing is given in Appendix A 4.1.2 Evaluation Metrics. To evaluate the performance of CTR prediction methods, we adopt AUC (Area under the ROC curve) and Log Loss (binary cross-entropy loss) as the evaluation metrics. Slightly higher AUC or lower Log Loss ( e.g. , 0.001) can be regarded as significant improvement in CTR prediction [33, 55, 57] 4.1.3 Baselines. For traditional CTR models, we select baselines with different feature interaction operators, including FM [53], DNN,DeepFM[13],xDeepFM[33],PNN[49],DCN[56],AutoInt [54], FiGNN [31], FGCNN[38], and DCNv2 [57]. For PLM-based CTR models, we choose CTR-BERT [47], P5 [11], PTab [39], CTRL [28] as the representative baselines. 4.1.4 Implementation Details. We adopt AdamW as the optimizer. For prompt-augmented masked language modeling pretraining, we set batch size to 1024 and learning rate to 5 \u00d7 10 -5 . The warmup ratio is selected from { 0 , 0 . 05 , 0 . 1 } . The number of pretraining epoch is 20. For the finetuning phase, the batch size is set to 256 for Movielens-1M, 256 for BookCrossing, 1024 for AZ-Toys, and 4096 for GoodReads. The learning rate for CTR model part is 1 \u00d7 10 -3 , while the learning rate for PLM part is selected from { 0 , 3 \u00d7 10 -5 , 5 \u00d7 10 -5 } . Setting the learning rate for PLM as zero means that we freeze the language model and only update the CTR model. The projection network \ud835\udc54 \ud835\udc59,\ud835\udc58 for prompt generation is a tanhactivated two-layer MLP with hidden size equal to the embedding size of PLM. The number of prompts per layer \ud835\udc3e is selected from { 1 , 3 , 5 , 7 } . Since ClickPrompt is a model-agnostic framework, we choose DCNv2 [57] as the CTR model and RoBERTa-base [44] as the pretrained language model, unless otherwise specified. Finally, we adopt the model at the iteration with the highest validation AUC for evaluation in the testing set. We also provide the detailed hyperparameter settings for each baseline model in Appendix B ClickPrompt \ud835\udc64 / \ud835\udc5c \ud835\udc43\ud835\udc3f\ud835\udc40 successfully promotes the predictive accuracy without increasing the inference latency.", "4.2 Overall Performance (RQ1)": "We compare the overall performance of our proposed ClickPrompt with the selected baseline models. Note that we choose DCNv2 as the CTR model and RoBERTa-base as the pretrained language model. The results are reported in Table 2, from which we can obtain the following observations: \u00b7 Traditional CTR models show significantly better performance over PLM-based CTR models, except for CTRL. This indicates that the collaborative information embeded among feature crossing patterns is crucial for CTR prediction, and solely relying on semantic knowledge from textual inputs might lead to inferior performance, which is consistent with the results in [28]. \u00b7 CTRL generally achieves the best performance among all the baseline models. CTRL adopts the CLIP-based framework [51], and distills the semantic knowledge from PLM into the CTR model via contrastive pretraining. However, the contrastive objective could only provide coarse-grained instance-level supervisions for implicit alignment and late interaction upon the final representations of PLM and CTR model, resulting in relatively inferior performance compared with our proposed ClickPrompt. \u00b7 ClickPrompt \ud835\udc64\ud835\udc56\ud835\udc61\u210e \ud835\udc43\ud835\udc3f\ud835\udc40 achieves significant improvements over all the baseline models, which validates the effectiveness of explicit alignment and early interaction between collaborative and semantic knowledge through the soft prompt interface. \u00b7 ClickPrompt \ud835\udc64 / \ud835\udc5c \ud835\udc43\ud835\udc3f\ud835\udc40 generally wins the second place, significantly outperforming baseline methods without altering the model structure of DCNv2. This demonstrates the importance of semantic-aware parameter initialization brought by PA-MLM pretraining. By sacrificing the opportunity for explicit interaction with semantic signals during downstream finetuning,", "4.3 Model Compatibility (RQ2)": "To investigate the model compatibility, we apply the ClickPrompt framework over different backbones in terms of both CTR models and PLMs. For CTR models, we select DCNv2 [57], AutoInt [54] and DNN, which represent different type of feature interaction operators. For PLMs, we choose the following three backbones with different model sizes: TinyBERT (14.5M) [22], RoBERTa-base(125M) [44], and RoBERTa-large(335M) [44]. We conduct the model compatibility experiments on Movielens-1M, BookCrossing, and Amazon-Toys datasets. The results are reported in Table 3, from which we can obtain the following observations: \u00b7 ClickPrompt is able to achieve significant improvement over the raw CTR model ( i.e. , N/A) for all the backbones, which demonstrates its superior model compatibility in terms of both CTR models and PLMs. \u00b7 As the model size of PLM continues to grow, the performance improvement over the raw CTR model brought by ClickPrompt also gradually increases, except for few cases. Larger pretrained language models possess a broader range of open-world knowledge, which can benefit the fusion and alignment between semantic and collaborative signals. \u00b7 Although we observe the phenomenon that performance continues to increase with the language model size, a larger volume of PLM does not necessarily lead to a proportional improvement in CTR predictive performance. Therefore, considering the training overhead, we suggest RoBERTa-base to be a more proper and economic choice for ClickPrompt to balance the performance gain and training cost when involving PLMs. ... ... ... ... ... Layerwise Prompt ... ... ... ... ... w/o Layerwise Prompt Soft Prompt Vector Hidden State for Word Token", "4.4 Ablation Study (RQ3)": "We analyze the impact of hyperparameters and different configurations in ClickPrompt, including the prompt strategy, and the collaborative & semantic knowledge fusion strategy. In this section, we select DCNv2, AutoInt and DNN as the backbone CTR models, and choose RoBERTa-base as the PLM backbone. Experiments are conducted on Movielens-1M, BookCrossing, and Amazon-Toys datasets under the finetuning-with-PLM strategy. 4.4.1 Prompt Strategy. We compare two different prompt strategies shown in Figure 3, and report the results in Table 4. We observe that the layerwise prompt strategy consistently outperforms that without layerwise prompting. If the prompt vectors are only placed at the shallow input layer, the collaborative knowledge from CTR model might be overwhelmed during the PLM forwarding, thus leading to unbalanced interactions with semantic knowledge and consequently inferior performance. 4.4.2 Collaborative & Semantic Knowledge Fusion Strategy. In ClickPrompt, there are two key technical points for the interaction and alignment between the collaborative and semantic knowledge. (1) From the model architecture perspective, the layerwise soft prompts serve as the bridge for explicit interactions between CTR models and PLMs. (2) From the learning strategy perspective, PA-MLM pretraining task forces PLM to extract and incorporate the useful collaborative information embedded in prompt vectors, resulting in fine-grained alignments. Hence, we compare ClickPrompt with the following three variants: \u00b7 w/o Prompt . We retain the PA-MLM pretraining phase, but remove the prompt interface between the CTR model and PLM during the finetuning phase. That is, the model architecture for finetuning degenerates into a two-tower version that simply add up the outputs from the CTR model and PLM. \u00b7 w/o Pretrain . We remove the PA-MLM pretraining phase, while preserving the model architecture with soft prompt interface for downstream CTR prediction. \u00b7 w/o Both . We remove both the prompt interface and PA-MLM pretraining, which eliminates the interaction and alignment between collaborative and semantic knowledge during the training. The results are reported in Table 5. When we remove either the prompt interface or PA-MLM pretraining, the performance degrades on three datasets for all backbone CTR models. This suggests that the explicit interaction and fine-grained alignment between collaborative and semantic knowledge can lead to better information extraction and fusion from both input modalities, and thus boost the CTR predictive performance.", "4.5 Long-tail User/Item Analysis (RQ4)": "The semantic information brought by PLM is especially valuable for scenarios with cold-start or long-tail users/items. Hence, in this section, we conduct in-depth analysis to further investigate the reasons for the performance improvement of ClickPrompt over the backbone CTR model from the long-tail user/item perspective. We conduct the experiment on MovieLens-1M dataset with DCNv2 as the backbone CTR model and RoBERTa-base as the backbone PLM. We adopt the finetuning-with-PLM strategy. Specifically, we sort users/items based on their frequency of occurrence in the training set. The bottom 10% in terms of frequency are classified as long-tail low-frequency users/items, while the rest 90% are considered as non-long-tail ones. According to whether users and items are long-tail or not, we divide the entire testing set into four mutually exclusive subsets. We evaluate DCNv2 and ClickPrompt on each subset and report the results in Table 6, from which the following observations are obtained: \u00b7 Long-tail low-frequency users or items can lead to significant performance degradation for the traditional ID-based CTR model ( i.e. , DCNv2), while ClickPrompt can consistently improves the predictive performance across all four subsets. \u00b7 In cases where the long-tail problem is more severe ( e.g. , the subset where users and items are both long-tail), ClickPrompt can bring significantly larger improvements over the backbone CTR model. This confirms that ClickPrompt is effective in addressing cold-start or long-tail problems for recommendation, which mainly contribute to the final performance enhancement.", "5 RELATED WORK": "", "5.1 Traditional CTR Prediction": "To estimate the user click probability, traditional CTR models usually convert the input data into ID features via one-hot encoding. The key idea is to capture the feature crossing patterns, which indicates the combination relationships of multiple features. While the implicit feature interactions are modeled by a deep neural network (DNN), the explicit feature interactions are captured by a specially designed learning function operator: (1) product operator, (2) convolutional operator, and (3) attention operator. Product operators [13, 15, 20, 23, 49, 50] originate from classical shallow models such as FM [53] and POLY2 [5]. For example, DCN [56], xDeepFM [33], DCNv2 [57] are proposed to capture high-order feature interactions by applying product-based feature interactions at each layer explicitly. Convolutional operators [31, 38, 41] ( e.g. , Convolutional Neural Networks (CNN) and Graph Convolutional Networks (GCN)) are also explored to capture the local and global views of feature patterns [38], and promote the interaction modeling through message propagation [31]. Attention operators [6, 30, 54, 61] suggest adopting the attention mechanism to allow feature fields or feature interactions to contribute differently to the final CTR prediction. Although such an ID-based CTR modeling paradigm has achieved remarkable progress in the past decades, they generally suffer from the semantic information loss issue brought by one-hot encoding. This thereby leads to their disabilities to handle scenarios with cold-start users/items or low-frequency long-tail features.", "5.2 PLM-based CTR Prediction": "With the rapid development of pretrained language models (PLMs) in natural language processing (NLP) domains, researchers begin to explore the potential of PLMs for CTR prediction [34, 60]. Different from the ID-based one-hot encoding in traditional CTR prediction, the input data is converted into textual sentences through hard prompt templates as shown in Template A. According to the groundtruth label formulation and task genre, PLM-based CTR prediction can be roughly divided into two categories. The first one [27, 46, 47] retain the ground-truth labels as binary codes { 0 , 1 } similar to traditional settings, and model the CTR prediction task as a binary text classification problem. For instance, PTab [39] first further pretrains a BERT model [9] for the masked language modeling objective based on the textualized CTR data, and then finetune it for downstream CTR estimation with a randomly initialized prediction head. The second category [37, 65] transforms the binary labels into a pair of key answer words ( e.g. , Yes/No, Good/Bad), and thus models the CTR prediction as a sequence-to-sequence task. For example, P5 [11], as well as its variants [12, 18, 19], propose to tune T5 [52] as a unified recommendation model for various downstream tasks in a textual generative manner. Other works [2, 40] also intends to incorporate decoder-only large language models (LLMs) to follow instructions and answer the user preference question appended after the textual input sentences. Although the semantic information loss issue is well addressed, these PLM-based CTR models cannot capture the field-wise collaborative signals, leading to inferior CTR predictive performance. Moreover, the heavy inference overhead brought by the large model size makes it impractical for real-world industrial applications. Our proposed ClickPrompt could not only retain and fuse both the semantic and collaborative knowledge to achieve SOTA CTR predictive performance, but also deal with the inference inefficiency problem by providing a better semantic-aware parameter initialization for solely finetuning CTR model.", "5.3 Prompt Tuning": "Prompt tuning introduces a set of trainable continuous prompts to the pretrained language models for specific NLP tasks ( e.g. , knowledge probing, text classification) [25, 42]. Generally, prompt tuning [14, 29] acts as a parameter-efficient finetuning (PEFT) solution, where we only update the soft prompts' parameters with supervision from downstream tasks, while keeping the entire parameters of the original PLM unchanged [26]. In this way, we can substantially reduce per-task storage and memory usage when finetuning PLMs on different downstream tasks. Moreover, some works [3, 43] propose to tune the soft prompts together with all of or part of the parameters of PLM. Notably, this setting no more belongs to the PEFT methods. It is very similar to the standard pretrain-finetune paradigm, but the addition of the learnable soft prompts can provide additional bootstrapping for model training [42]. Although this line of methods can significantly enhance the model capability, it requires heavy computational and storage resources and may overfit on small datasets. In this paper, ClickPrompt adopts the basic idea of prompt tuning [42, 48] to connect the CTR model and PLM with the layerwise trainable soft prompts. In general, the structure of the CTR model is specially designed to capture essential feature interaction patterns, and is therefore a naturally strong soft prompt generator to adapt PLMs to the downstream CTR prediction task.", "6 CONCLUSION": "In this paper, we propose a novel model-agnostic framework ( i.e. , ClickPrompt), where CTR models serve as the soft prompt generators for PLMs. A pretrain-finetune scheme is designed to enable explicit interaction and alignment between the collaborative knowledge from one-hot ID modality and the semantic knowledge from textual modality, which significantly improves the CTR predictive performance. Furthermore, we provide another lightweight finetuning strategy to solely train the CTR model for downstream tasks without PLMs, thus properly tackling the inference inefficiency issue. Extensive experiments on four real-world datasets validate the superior predictive performance and model compatibility of ClickPrompt compared with baseline models. As for future works, a promising direction is to further improve pretraining efficiency. Moreover, we will explore the application of ClickPrompt on other recommendation tasks ( e.g. , learning to rank).", "ACKNOWLEDGMENTS": "The Shanghai Jiao Tong University team is partially supported by National Key R&D Program of China (2022ZD0114804), Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102) and NSFC (62177033, 62322603). The work is sponsored by Huawei Innovation Research Program. We thank MindSpore [1] for the partial support of this work, which is a new deep learning computing framework.", "REFERENCES": "[1] 2020. MindSpore. https://www.mindspore.cn/ [2] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. Tallrec: An effective and efficient tuning framework to align large language model with recommendation. arXiv preprint arXiv:2305.00447 (2023). [3] Eyal Ben-David, Nadav Oved, and Roi Reichart. 2022. PADA: Example-based Prompt Learning for on-the-fly Adaptation to Unseen Domains. Transactions of the Association for Computational Linguistics 10 (2022), 414-433. [4] Vadim Borisov, Kathrin Se\u00dfler, Tobias Leemann, Martin Pawelczyk, and Gjergji Kasneci. 2022. Language models are realistic tabular data generators. arXiv preprint arXiv:2210.06280 (2022). [5] Yin-Wen Chang, Cho-Jui Hsieh, Kai-Wei Chang, Michael Ringgaard, and Chih-Jen Lin. 2010. Training and testing low-degree polynomial data mappings via linear SVM. Journal of Machine Learning Research 11, 4 (2010). [6] Zekai Chen, Fangtian Zhong, Zhumin Chen, Xiao Zhang, Robert Pless, and Xiuzhen Cheng. 2021. DCAP: Deep Cross Attentional Product Network for User Response Prediction. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management . 221-230. [7] Zeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022. M6Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems. arXiv preprint arXiv:2205.08084 (2022). [8] Xinyi Dai, Jianghao Lin, Weinan Zhang, Shuai Li, Weiwen Liu, Ruiming Tang, Xiuqiang He, Jianye Hao, Jun Wang, and Yong Yu. 2021. An adversarial imitation click model for information retrieval. In Proceedings of the Web Conference 2021 . 1809-1820. [9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018). [10] Lingyue Fu, Jianghao Lin, Weiwen Liu, Ruiming Tang, Weinan Zhang, Rui Zhang, and Yong Yu. 2023. An F-shape Click Model for Information Retrieval on Multiblock Mobile Pages. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining . 1057-1065. [11] Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022. Recommendation as language processing (rlp): A unified pretrain, personalized [36] Jianghao Lin, Yanru Qu, Wei Guo, Xinyi Dai, Ruiming Tang, Yong Yu, and Weinan Zhang. 2023. MAP: A Model-agnostic Pretraining Framework for Click-through Rate Prediction. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery & Data Mining . 1379-1389. [37] Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang Quan, Ruiming Tang, Yong Yu, and Weinan Zhang. 2023. ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation. arXiv preprint arXiv:2308.11131 (2023). [38] Bin Liu, Ruiming Tang, Yingzhi Chen, Jinkai Yu, Huifeng Guo, and Yuzhou Zhang. 2019. Feature generation by convolutional neural network for click-through rate prediction. In WWW . 1119-1129. [39] Guang Liu, Jie Yang, and Ledell Wu. 2022. PTab: Using the Pre-trained Language Model for Modeling Tabular Data. arXiv preprint arXiv:2209.08060 (2022). [40] Junling Liu, Chao Liu, Renjie Lv, Kang Zhou, and Yan Zhang. 2023. Is chatgpt a good recommender? a preliminary study. arXiv preprint arXiv:2304.10149 (2023). [41] Qiang Liu, Feng Yu, Shu Wu, and Liang Wang. 2015. A Convolutional Click Prediction Model. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management . ACM, 1743-1746. [42] Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2021. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602 (2021). [43] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2023. GPT understands, too. AI Open (2023). [44] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019). [45] Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 (2017). [46] Zhiming Mao, Huimin Wang, Yiming Du, and Kam-fai Wong. 2023. UniTRec: A Unified Text-to-Text Transformer and Joint Contrastive Learning Framework for Text-based Recommendation. arXiv preprint arXiv:2305.15756 (2023). [47] Aashiq Muhamed, Iman Keivanloo, Sujan Perera, James Mracek, Yi Xu, Qingjun Cui, Santosh Rajagopalan, Belinda Zeng, and Trishul Chilimbi. 2021. CTR-BERT: Cost-effective knowledge distillation for billion-parameter teacher models. In NeurIPS Efficient Natural Language and Speech Processing Workshop . [48] Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying LMs with mixtures of soft prompts. arXiv preprint arXiv:2104.06599 (2021). [49] Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, and Jun Wang. 2016. Product-based neural networks for user response prediction. In ICDM . [50] Yanru Qu, Bohui Fang, Weinan Zhang, Ruiming Tang, Minzhe Niu, Huifeng Guo, Yong Yu, and Xiuqiang He. 2018. Product-based neural networks for user response prediction over multi-field categorical data. TOIS 37, 1 (2018), 1-35. [51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning . PMLR, 8748-8763. [52] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research 21, 1 (2020), 5485-5551. [53] Steffen Rendle. 2010. Factorization machines. In ICDM . [54] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, and Jian Tang. 2019. Autoint: Automatic feature interaction learning via selfattentive neural networks. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management . 1161-1170. [55] Fangye Wang, Yingxu Wang, Dongsheng Li, Hansu Gu, Tun Lu, Peng Zhang, and Ning Gu. 2022. Enhancing CTR Prediction with Context-Aware Feature Representation Learning. arXiv preprint arXiv:2204.08758 (2022). [56] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network for ad click predictions. In Proceedings of the ADKDD'17 . 1-7. [57] Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong, and Ed Chi. 2021. Dcn v2: Improved deep & cross network and practical lessons for web-scale learning to rank systems. In Proceedings of the Web Conference 2021 . 1785-1797. [58] Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, et al. 2023. A Survey on Large Language Models for Recommendation. arXiv preprint arXiv:2305.19860 (2023). [59] Yunjia Xi, Jianghao Lin, Weiwen Liu, Xinyi Dai, Weinan Zhang, Rui Zhang, Ruiming Tang, and Yong Yu. 2023. A Bird's-eye View of Reranking: from List Level to Page Level. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining . 1075-1083. [60] Yunjia Xi, Weiwen Liu, Jianghao Lin, Jieming Zhu, Bo Chen, Ruiming Tang, Weinan Zhang, Rui Zhang, and Yong Yu. 2023. Towards Open-World Recommendation with Knowledge Augmentation from Large Language Models. arXiv preprint arXiv:2306.10933 (2023). [61] Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua. 2017. Attentional factorization machines: Learning the weight of feature interactions via attention networks. IJCAI (2017). [62] Canwen Xu and Julian McAuley. 2023. A survey on model compression and acceleration for pretrained language models. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 37. 10566-10575. [63] Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne Xin Zhao, Leyu Lin, and Ji-Rong Wen. 2023. Recommendation as instruction following: A large language model empowered recommendation approach. arXiv preprint arXiv:2305.07001 (2023). [64] Weinan Zhang, Jiarui Qin, Wei Guo, Ruiming Tang, and Xiuqiang He. 2021. Deep learning for click-through rate estimation. IJCAI (2021). [65] Zizhuo Zhang and Bang Wang. 2023. Prompt learning for news recommendation. arXiv preprint arXiv:2304.05263 (2023).", "A DATA PREPROCESSING": "We conduct experiments on four real-world datasets from different recommendation scenarios. The information about data preprocessing is given below: \u00b7 Movielens-1M is a movie recommendation dataset from Movielens website with ratings ranging from 1 to 5. We binarize the ratings with a threshold of 4, while removing neutral samples with ratings equal to 3 [32, 54]. \u00b7 BookCrossing is a book recommendation dataset from BookCrossing website with ratings ranging from 0 to 10. We convert the ratings into binary labels with a threshold of 5. \u00b7 Amazon-Toys is a e-commercial dataset of toys category from Amazon with ratings ranging from 1 to 5. We binarize the ratings with a threshold of 4. We apply 5-core filtering to ensure each user or item has at least five interaction records [11, 32]. \u00b7 GoodReads is a book recommendation dataset from GoodReads website with ratings ranging from 1 to 5. We transform the ratings into binary labels with a threshold of 4. Similar to AmazonToys, we apply 10-core filtering to ensure each user or item has at least ten interaction records.", "B BASELINE IMPLEMENTATION": "In this section, we give the hyperparameter configuration for each baseline model from two different categories: (1) traditional CTR models, and (2) PLM-based CTR models.", "B.1 Traditional CTR Models": "We train each traditional CTR model from scratch based on click signals without pretraining. Similar to the finetuning stage of ClickPrompt, we adopt AdamW [45] as the optimizer. The batch size is set to 256 for Movielens-1M, 256 for BookCrossing, 1024 for AZToys, and 4096 for GoodReads. The learning rate is set to 1 \u00d7 10 -3 . We set the embedding size to 32 for MovieLens-1M and BookCrossing, and 16 for Amazon-Toys and GoodReads. The dropout rate is selected from { 0 . 0 , 0 . 1 , 0 . 2 } . We utilize one linear layer after the feature interaction layer to make the final CTR prediction. Unless stated otherwise, we adopt ReLU as the activation function. The model-specific hyperparameter settings for base models are as follows: \u00b7 DNN . We select the size of DNN layer from { 128 , 256 , 512 } , and the number of DNN layers from { 3 , 4 , 5 , 6 } . \u00b7 DeepFM [13]. We select the size of DNN layer from { 128 , 256 , 512 } , and the number of DNN layers from { 3 , 4 , 5 , 6 } . \u00b7 xDeepFM [33]. We choose the number of CIN layers from { 2 , 3 , 4 , 5 } , and the number of units per CIN layer is set to 25. We select the size of DNN layer from { 128 , 256 , 512 } , and the number of DNN layers from { 3 , 4 , 5 , 6 } . \u00b7 IPNN [49]. We select the size of DNN layer from { 128 , 256 , 512 } , and the number of DNN layers from { 3 , 4 , 5 , 6 } . \u00b7 DCN [56]. We select the size of DNN layer from { 128 , 256 , 512 } , and the number of DNN layers from { 3 , 4 , 5 , 6 } . We force the CrossNet module to have the same number of layer as the DNN network. \u00b7 AutoInt [54]. We select the number of attention layers from { 3 , 4 , 5 , 6 } . The number of attention heads per layer and the attention size are set to 1 and 32, respectively. \u00b7 FiGNN [31]. We select the number of layers from { 3 , 4 , 5 , 6 } , and apply residual connection for the graph layers. \u00b7 FGCNN . We maintain 4 tanh-activated convolutional layers with a kernel size of 7 and pooling size of 2 for each layer. The number of channels for each layer is set to 6 , 8 , 10 , 12, respectively. The numbers of channels for recombination layers are all set to 3. \u00b7 DCNv2 [57]. We select the size of DNN layer from { 128 , 256 , 512 } , and the number of DNN layers from { 3 , 4 , 5 , 6 } . We force the CrossNet module to have the same number of layer as the DNN network.", "B.2 PLM-based CTR Models": "This line of methods generally incorporate the pretrained language models for CTR prediction. We keep the structure of PLMs unchanged, and describe the training setting for each model as follows. Note that we utilize AdamW [45] as the optimizer for all of the PLM-based baseline models. \u00b7 CTR-BERT [47]. We maintain a two-tower model structure based on the BERT [9] model to encode the user and item information, respectively. We set the total number of tuning epochs to 10 with batch size of 1024. The learning rate is set to 5 \u00d7 10 -5 with linear decay. The warmup ratio is set to 0.05. \u00b7 P5 [11] is a unified sequence-to-sequence framework with T5 [52] as the backbone pretrained language model for multiple recommendation tasks. In this paper, we leverage P5 for a single task only ( i.e. , CTR prediction). The total number of epochs is set to 10 with batch size of 32. The learning rate is selected from { 5 \u00d7 10 -4 , 1 \u00d7 10 -3 } with linear decay. The warmup ratio is 0.05. Following P5's official implementation, we also perform gradient clip with threshold equal to 1.0. \u00b7 PTab [39] adopts the common pretrain-finetune scheme based on the BERT [9] model. PTab first further pretrains the BERT model with the classical masked language modeling objective based on the textualized CTR data, and then finetunes BERT for downstream CTR prediction as a text classification problem. Following the original paper, we pretrain BERT for 10 epochs with batch size equal to 1024. The learning rate for pretraining is set to 5 \u00d7 10 -5 with linear decay. The warmup ratio is 0.05. As for finetuning, the total number of tuning epoch is set to 10 with batch size of 1024. The learning rate for finetuning is initialized at 5 \u00d7 10 -5 with linear decay. The warmup ratio is 0.01. \u00b7 CTRL [28] designs a contrastive pretraining framework to implicitly align the collaborative knowledge from CTR models and semantic knowledge from PLMs. Following the original paper, we choose AutoInt as the backbone CTR model and TinyBERT [22] as the backbone PLM. We first perform contrastive pretraining for 20 epochs, and then finetune AutoInt for downstream CTR prediction tasks. The model structure of AutoInt is set as the best configuration based on the grid-search result stated in Appendix B.1. Other training configurations are the same as reported in CTRL's original paper [28].", "C ADDITIONAL EXPERIMENT": "", "C.1 Case Study": "We conduct a case study to further illustrate that PLMs extract the corresponding \"right answer\" from soft prompts by adaptively assigning attention weight patterns over them. We conduct the experiment on MovieLens-1M dataset. We set PLM backbone as RoBERTa-base and set CTR backbone model as DCNv2. The number of prompts \ud835\udc3e per layer is set to 5. We show the normalized attention scores over the five soft prompts at the last layer by masking textual features from different fields of the same data sample. The results are given in Table 7. To reconstruct text of different fields for one data sample, PLMs learn to assign different attention weight patterns over the same five soft prompts for collaborative information extraction. Moreover, in most cases, PLMs even learn to adaptively set some attention scores to nearly zero ( i.e. , 0.0000) to filter out information from certain soft prompts. In this way, PLMs extract useful information from the soft prompts by adaptively assigning attention weight patterns over them.", "C.2 Inference Time": "We report the inference time per batch of ClickPrompt (with two different finetuning strategies) and four representative baselines ( i.e. , AutoInt, DCNv2, PTab, P5) on the four datasets. The averaged AUC relative improvement (Avg. Rel. Imrpv.) of each model against AutoInt is also provided. The evaluation batch size is set to 128. For ClickPrompt, we select DCNv2 as the backbone CTR model, and choose RoBERTa-base as the backbone PLM. This experiment is exclusively conducted on the same server with one GeForce RTX 4090 GPU. We report the results in Table 8. we can observe that ClickPrompt \ud835\udc64\ud835\udc56\ud835\udc61\u210e\ud835\udc43\ud835\udc3f\ud835\udc40 achieves the best performance with a relatively higher inference cost. However, we offer another choice to finetune without PLM by exclusively tuning the CTR model with semantic-aware initialization. This approach allows us to achieve efficient finetuning and enhance CTR prediction performance without modifying the model structure or increasing the inference overhead.", "C.3 Generalization": "We conduct the following additional compatibility experiments to further validate the model compatibility of our proposed ClickPrompt in terms of PLM architecture. We select DCNv2 as the backbone CTR model, and choose GPT2 (decoder-only) and BART (encoder-decoder) as the backbone PLMs. The compatibility experiment is conducted on MovieLens-1M, BookCrossing, and AmazonToys datasets. We also apply the layer-wise prompting strategy to GPT2 and BART. For GPT2, the pretraining objective switches from prompt-augmented masked language modeling (PA-MLM) to prompt-augmented causal language modeling (PA-CLM). The results are given in Table ?? . ClickPrompt: CTR Models are Strong Prompt Generators for Adapting Language Models to CTR Prediction WWW'24, May 13-17, 2024, Singapore, Singapore WWW'24, May 13-17, 2024, Singapore, Singapore Jianghao Lin et al."}
