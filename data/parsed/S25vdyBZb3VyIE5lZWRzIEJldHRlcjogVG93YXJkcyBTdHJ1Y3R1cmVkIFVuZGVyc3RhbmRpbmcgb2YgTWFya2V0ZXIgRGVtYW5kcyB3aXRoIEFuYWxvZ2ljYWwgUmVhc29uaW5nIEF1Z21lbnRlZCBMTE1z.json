{"Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs": "Junjie Wang \u2217 Zhejiang University Ant Group Hangzhou, China wangjj2018@zju.edu.cn Dan Yang \u2217 Ant Group Hangzhou, China luoyin.yd@antgroup.com Binbin Hu \u2217 Ant Group Hangzhou, China bin.hbb@antfin.com Yue Shen Ant Group Hangzhou, China zhanying@antfin.com Wen Zhang \u2020 Zhejiang University Hangzhou, China zhang.wen@zju.edu.cn", "ABSTRACT": "In this paper, we explore a new way for user targeting, where non-expert marketers could select their target users solely given demands in natural language form. The key to this issue is how to transform natural languages into practical structured logical languages, i.e., the structured understanding of marketer demands. In practical scenarios, the demands of non-expert marketers are often abstract and diverse. Considering the impressive natural language processing ability of large language models (LLMs), we try to leverage LLMs to solve this issue. To stimulate the LLMs' reasoning ability, the chain-of-thought (CoT) prompting method is widely used, but existing methods still have some limitations in our scenario: (1) Previous methods either use simple 'Let's think step by step' spells or provide fixed examples in demonstrations without considering compatibility between prompts and concrete questions, making LLMs ineffective when the marketers' demands are abstract and diverse. (2) Previous methods are often implemented in closed-source models or excessively large models, which is not suitable in industrial practical scenarios. Based on these, we propose ARALLM ( i.e., Analogical Reasoning Augmented Large Language Models) consisting of two modules: Analogical Reasoning based Prompting and Reasoning-Augmented Multi-Task Model Distillation. To be specific, we first construct a reasoning library consisting of several high-quality and topic-rich reasoning examples. Then, we adopt a retrieval-based method to conduct analogical reasoning with the help of the reasoning library. The experimental results show that this prompting strategy achieves better performance than the ordinary prompting method. Beyond that, we distill knowledge from super LLMs (GPT-3.5) to fine-tune smaller Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. KDD '24, August 25-29, 2024, Barcelona, Spain \u00a9 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0490-1/24/08...$15.00 https://doi.org/10.1145/3637528.3671583 Jinjie Gu \u2020 Ant Group Hangzhou, China jinjie.gujj@antfin.com student LLMs in a multi-task training paradigm, enabling the models to be easily deployed in practical environments. Part of our data and code can be found at https://github.com/alipay/AnalogicReasoning-Augmented-Large-Language-Model.", "CCS CONCEPTS": "\u00b7 Computing methodologies \u2192 Natural language processing .", "KEYWORDS": "User Targeting, Large Language Models, Analogical Reasoning, Knowledge Distillation, Chain of Thought", "ACMReference Format:": "Junjie Wang, Dan Yang, Binbin Hu, Yue Shen, Wen Zhang, and Jinjie Gu. 2024. Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '24), August 25-29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671583", "1 INTRODUCTION": "Recently, the practice of user targeting has gained significant attention in real-world applications ( e.g., Alipay and WeChat), as highlighted by a variety of studies [19, 23, 28, 32, 33]. This approach has the excellent potential to attract high-quality users for specific campaigns, aligning with the goals of marketers to enhance conversions and minimize operational expenses. Roughly speaking, current methods devoted to user targeting mainly fall into two lines: model-based methods (Figure 1 (a)) that perform expansion of seed users/entities with well-designed neural networks [28, 32, 33] and rule-based methods (Figure 1 (b)) that manually group users with different tags based on domain knowledge [19, 23]. Unfortunately, both of the above prior studies have primarily emphasized the intricate architectures for effectively and efficiently gathering targeted users, while commonly ignoring the natural and significant gap between marketers' demand and the capability of current models. In particular, current user targeting pipelines mainly force the marketers to decompose their demands into multiple tags/entities, e.g., 'Preference', 'Resident City' and 'User Age Group' which are characterized in Figure 1. The process incurs several weaknesses: i) unfriendly to marketers; ii) time-consuming Young people in City A who enjoy milk tea or white-collar workers in first-tier cities who enjoy listening to audiobooks. A \u5e02\u559c\u6b22\u5976\u8336\u7684\u5e74\u8f7b\u4eba\u6216\u8005\u4e00\u7ebf\u57ce\u5e02\u7ecf\u5e38\u542c\u4e66 \u7684\u767d\u9886\u3002 Marketer Demand Expert Knowledge \u2022 Preference \uff1a milk tea \u2022 Resident City: City A \u2022 Age: 18-35 \u2022 Days of listening to audiobooks: >3 \u2022 \u2026 Seed users/entities Request Targeting model Intersection/ Union (b) (a) OR AND AND User Age Group Between 18,35 Resident City Belongs To City A Days of listening to audiobooks Greater Than 3 Career Belongs To White-collar City Level Belongs To First-tier Preference Belongs To Milk Tea AND AND OR Structured understanding Targeted User aggregation (c) for manual deconstruction of demands; and iii) unpromising performance due to partial tags/entities. Therefore, in this paper, we take the initial stride towards enhancing current user targeting systems by probing into the structured understanding of marketers' demands in an automatic way, enabling the system to know your needs better solely based on natural language inputs . As shown in Figure 1 (c), the marketers only need to input their native demands like 'Young people in City A who enjoy milk tea or white-collar workers in first-tier cities who enjoy listening to audiobooks' and our new approach can instantly conduct structured decomposition of marketers' demands and thus provide the user targeting cards. After verifying the cards, they can export the target users with just one click and the system will automatically run the existing targeted user aggregation modules. can be translated into SELL as '(( Resident City#Belongs To# City A ) AND ( User Age Group#Between#18,35 ) AND ( Preference#Belongs To#Milk Tea )) OR (( City Level#Belongs To#First-tier ) AND ( Days of listening to audiobooks#Greater Than#3 ) AND ( Career#Belongs To#White-collar ))'. The core of this new approach lies in how to transform natural languages into structured forms , or so-called structured understanding of marketer demands . To address this issue, we first draw upon existing logical and programming languages to design a logical expression that offers both readability and practical applicability for the structured representation of marketers' demands. We name it as SELL, i.e., Structured and Editable Logical Language, which mainly consists of Keys ( e.g., Resident City), Values ( e.g., City A), Operators ( e.g., Belongs To) and Intersection/Union symbols ( i.e., AND, OR). It forms the basis of the user targeting cards shown in Figure 1(c). The main concern of our work is transforming the Natural Language into SELL ( NL2SELL ). For example, the natural language 'Young people in City A who enjoy milk tea or whitecollar workers in first-tier cities who enjoy listening to audiobook' Previous research [7, 15, 17, 29] has highlighted the remarkable abilities of Large Language Models (LLMs) in language translation tasks. LLMs are renowned for their robust natural language processing and zero-shot capabilities, which render them possibly effective in NL2SELL. Consequently, we opt to employ LLMs to tackle this task. However, there are still some challenges in using LLMs in our scenario: (1) The challenge of reasoning accuracy. In practical user targeting scenarios, understanding of marketer's demands could be very challenging. For example, a marketer might want to market products related to education, targeting parents of middle school students who are focused on education. Therefore, the demand she inputs into the system is a simple sentence 'Parents of middle school students', and we need to use existing tags to convert this demand into a structured expression, such as ' (Marital Status#Belongs#True) AND (User Child Age#Between#12,15) AND (Preference#Belongs To#Education) '. However, the key 'Marital Status, User Child Age, and Preferences' do not directly appear in the marketer's demand, which requires the LLMs to possess a high level of language comprehension and reasoning abilities. Recently, it [13, 26] has been pointed out that prompting LLMs with chain-ofthought (CoT) could enhance the reasoning ability of LLMs. These methods either provide fixed reasoning examples as demonstrations through few-shot learning or tell the model 'Let's think step by step' for zero-shot learning. Though effective, they ignore the compatibility between prompts and specific questions. Considering the diversity of marketers' demands themselves, zero-shot learning or having fixed reasoning examples in few-shot learning may not effectively stimulate the LLMs' reasoning ability [22]. Ideally, we can customize a reasoning process for each question to induce the model to reason better, but it is time-consuming and laborious. (2) The challenge of reasoning speed and resource consumption. Although the reasoning ability of super-large language models can be developed through prompt engineering, it is unacceptable to deploy excessively huge LLMs or API-based LLMs in practical situations. For extremely huge LLMs, excessive parameters will consume a lot of resources, and the reasoning speed is also slower. For API-based LLMs such as GPT-3.5, both the security of private data and the cost of API calls are thorny issues. Our model needs to be deployed in real ToC (To Customer) scenarios, where it will generate at least hundreds of tasks daily. Therefore, there is an urgent requirement to increase the inference speed, reduce the cost of inference, and ensure data security. Thus, smaller, free, and white-box LLMs are preferred. To address these challenges, we propose a framework called Analogic Reasoning Augmented Large Language Model (ARALLM) to tackle the NL2SELL task, which consists of two modules: Analogical Reasoning based Prompting and Reasoning-Augmented MultiTask Model Distillation. (1) Analogical Reasoning based Prompting. Creating a CoT for each new targeting demand is impractical and time-consuming (considering the need for deployment). Thus, we propose Analogical Reasoning based Prompting. The fundamental concept of it is that when two targeting demands are similar, they might follow similar reasoning steps and have similar conditional expression structures. Therefore, we construct a compact yet reliable reasoning library to serve as a reference for numerous demands that have unknown reasoning steps. This approach helps reduce the effort of manually writing reasoning steps for every demand. Additionally, we can offer assistance for any unfamiliar demands by employing analogical reasoning with retrieval. (2) Reasoning-Augmented Multi-Task Model Distillation. Since the white-box and smaller model is preferred, we distill knowledge from teacher LLMs (GPT-3.5) to construct an NL2SELL dataset with over 10,000 pieces of training samples and then propose a novel multi-task training method with reasoning augmentation to improve the reasoning ability of student LLMs. We summarize our contributions as follows: (1) We explore a new way for marketer-friendly user targeting, which focuses on the structured understanding of marketers' raw demands. (2) We propose an innovative framework called ARALLM, which is composed of the Analogical Reasoning based Prompting module and the Reasoning-Augmented Multi-Task Distillation module, to address the above task. (3) We demonstrate the superiority of the proposed ARALLM framework through extensive experiments on real-world datasets and make this LLMs-based framework operate in real industry scenarios.", "2 METHOD": "In this work, we focus on how to convert a sentence of marketer's demand \ud835\udc51 into a standard structured expression SELL , while how to parse and execute the SELL and return the target users from the database will be handled by the internal system, which does not constitute the core of this article. Inspired by the analogical reasoning [6, 24, 30] in the classic artificial intelligence field and CoT prompting methods in LLMs, we first propose a novel prompting method enhanced by analogical reasoning and then adopt a reasoning-augmented multi-task training strategy to fine-tune smaller LLMs.", "2.1 Design of SELL": "We first explain the design of SELL to help readers understand our task better. The SELL is mainly composed of four elements: Keys , Values , Operators , and Intersection and Union symbols : (1) Keys are a series of tags that describe the features of the user, such as Gender , Monthly Income , Pet Owning , and so on. (2) Values are the fillings of the corresponding keys. They can be generally divided into three types: numerical type, string type, and boolean type. (3) Operators represent the relationship between keys and values. For numerical values, seven types of operators can be used to connect them with corresponding keys: Equal To, Greater Than, Less Than, Not Equal To, Not Greater Than, Not Less Than, and Between . For values of string type and boolean type, there are two types of operators: Belongs To and Not Belongs To . Keys, operators, and values can form a basic conditional expression in SELL, formatted as '( key#operator#value )', such as '( User Marital Status#Belongs#True )'. A conditional expression represents a cluster of target users. (4) Intersection and Union symbols are used to take the intersection or union of multiple conditional expressions (targeted users). We use the symbols AND and OR to denote the intersection and union operations, respectively. Compared to the popular database languages SQL and logic language FOL, the advantages of SELL are: (1) SELL is simpler in expression and has much fewer syntax symbols, making it easier for non-programming experts to understand. (2) Since SELL can be transformed into a clear tree structure, marketers can easily modify them even if the generated results have flaws. Due to space limitations, we recommend readers to read more details about SELL in the supplementary materials.", "2.2 Prompting With Analogical Reasoning": "The core idea of analogical reasoning in the NL2SELL task is that if two demands are similar, they may share similar reasoning steps, as well as similar logical structures. Thus, we construct a small but high-quality reasoning library to provide references for numerous demands with unknown reasoning steps, minimizing the cost of manually writing reasoning steps for each of them. 2.2.1 Construction of Reasoning Library. To construct a reasoning library, we first randomly collect N \ud835\udc45\ud835\udc3f marketer demands from real-world scenarios and write their corresponding answers based on our expert knowledge to form Q&A pairs R = {( \ud835\udc51 \ud835\udc52 , \ud835\udc60 \ud835\udc52 )} N \ud835\udc45\ud835\udc3f \ud835\udc52 = 1 , where \ud835\udc51 \ud835\udc52 is the demand, \ud835\udc60 \ud835\udc52 is the corresponding SELL expressions. The collected marketer's demands encompass services that are popular on Alipay, including education, technology, reading, travel, financial management, insurance, government affairs, and public welfare, thus ensuring that the reasoning library constructed based ' I want to sell tickets. Please help me select the Young people interested in the Asian Games tickets .' Demand: Students in City A who enjoy football matches. Tag list: [ Resident City,  Age, Career, Preference, \u2026\u2026 ] Reasoning steps: 1. Extract keywords: Students, City A, football matches 2. Select tags: Location, Career, Preference 3. Form conditional expressions: (Location#Belong to#City A), (Career#Belong To#Students), (Preference#Belongs To#Football Matches) 4. Combine: (Resident City #Belongs to#City A) AND (Career#Belongs To#Students) AND (Preference#Belongs To#Football Matches) Answer: (Resident City#Belongs To#City A) AND (Career#Belongs To#Students) AND (Preference#Belongs To#Football Matches) 'Students in City B who enjoy watching football matches.' \u2026\u2026 'Office workers who enjoy Thai cuisine.' 'Photography enthusiasts living in City A.' Reasoning Library Marketer Demands Stage1: Retrieve ex.1 ex.k \u2026 \u2026 ( Instruction ) Given a user profile in natural language form, your task is to transform the demand into a logical expression. The logical expression is defined as \u2026\u2026 Here are some examples: ( Demonstrations ) ( Input ): Demand: Young people interested in the Asian Games tickets. Tag list: [ User Age Group, Exercise Steps, Preference,\u2026\u2026 ] Answer: Stage2: Prompting ex.1 ex.k \u2026 \u2026 Prompt Template LLMs on this can cover as many demand topics as possible. Then we use a small amount of manual effort to write corresponding reasoning details for 10% random selected Q&A pairs \u2208 R to construct the seed examples in the format ( \ud835\udc51 \ud835\udc52 , \ud835\udc5f \ud835\udc52 , \ud835\udc60 \ud835\udc52 ) , where \ud835\udc5f \ud835\udc52 represent the reasoning details. We generally summarize the procedure of solving the NL2SELL problem as four steps: (1) Extract keywords: Extract the keywords from the demand. (2) Select tags: Select the most relevant tags from the tag list based on the keyword as the key in SELL. In addition, to help the LLMs better organize answers, we should also inform the LLMs of the tags in the marketing database as explicit knowledge. To address the issue of an excessive number of tags, we still use BGE as a retriever to retrieve the most relevant n tags of every demand from the marketing database to construct a part of the prompt. The top n tags that get the highest similarity scores when compared with demand will form a small-scale tag list \ud835\udc47 = { \ud835\udc61 \ud835\udc65 } \ud835\udc5b \ud835\udc65 = 1 to provide a reference for selecting keys in SELL. (3) Form conditional expressions: Based on the selected keywords and contextual information, fill in the corresponding operators and values to form conditional expressions. (4) Combine: Combine conditional expressions using intersection or union symbols. What we need to do is to fill in the details of these four steps. For the remaining data in R , we utilize the OpenAI gpt-3.5-turbo1106 (GPT-3.5) API to help us fill the corresponding reasoning steps, and our seed examples serve as demonstrations in the prompt. Since we have standardized the overall framework of reasoning, it is not difficult for GPT-3.5 to complete this part of the reasoning steps, cases are shown in supplemental materials. After being verified by experts. we harvest a batch of solid reasoning details to form a reasoning library R = {( \ud835\udc51 \ud835\udc52 , \ud835\udc5f \ud835\udc52 , \ud835\udc60 \ud835\udc52 )} N \ud835\udc45\ud835\udc3f \ud835\udc52 = 1 . 2.2.2 Retrieval-Based Analogical Prompt Construction. After the construction of the reasoning library, we can provide references for unknown demands through analogical reasoning. As shown in Figure 2, given a marketer demand \ud835\udc51 \ud835\udc5d that needs to predict the corresponding SELL expression, we first retrieve similar demands \ud835\udc51 \ud835\udc52 in R based on their text embedding similarity: where E is the embedding model in which we use BGE-large-zh [27, 31] off the shelf as it is well-performed in the Chinese text retrieval field. The top \ud835\udc58 demands in R that get the highest similarity score when compared with \ud835\udc51 \ud835\udc5d will be fetched as analogical examples in this stage, along with their corresponding \ud835\udc5f and \ud835\udc60 . Therefore, the basic organizational form of the final prompt \ud835\udc65 \ud835\udc5d for predicting is : where I is the instruction which simply describes the task, k is the number of analogical examples, \ud835\udc47 \ud835\udc52 and \ud835\udc47 \ud835\udc5d are the most relevant tag lists of the \ud835\udc51 \ud835\udc52 and \ud835\udc51 \ud835\udc5d respectively. Compared to the ordinary CoT method, our approach exhibits several main differences: (1) The NL2SELL task is more challenging. Unlike solving mathematical problems or common sense reasoning tasks, the training data for LLMs don't inherently include the syntax of SELL. This highlights the importance of appropriate reasoning steps. (2) We place a greater emphasis on the compatibility between prompts and questions. We believe that using an analogical reasoning based prompting method can make LLMs more adaptable to specific problems, rather than relying on fixed few-shot examples, which could potentially limit the model's performance.", "2.3 Reasoning-Augmented Multi-Task Model Distillation": "To train a private small model for deployment, we first distill knowledge from teacher LLM (GPT-3.5) to construct an NL2SELL dataset with over 10,000 pieces of training samples and then propose a multi-task training method with reasoning augmentation. 2.3.1 Knowledge Distillation. We collect nearly 10,000 demands from our daily marketing environment. Following the analogical reasoning method mentioned above, we construct input prompts for each demand in the form of Equation (2), where analogical examples come from the previously constructed reasoning library. Subsequently, we call the GPT-3.5 API to generate SELL answers based on the above prompts, which will then be further verified and corrected through manual crowdsourcing. We denote this part of data comes from demand to answer paradigm. It can be seen that the above distillation method relies on a large number of native demands and tedious manual calibration . At the same time, due to the different frequencies of user tags used in actual marketing activities, the distilled data has a long tail distribution . This means that some demands based on unpopular user tags only exist in a small amount of data, resulting in poor response to such demands. To address the above two issues, we propose a distillation approach based on answer to demand , which can be further divided into two stages: answer generation and demand generation . \u00b7 Answer generation: We first construct a series of logical templates of SELL. The content of logical templates is obtained from the marketing database through random sampling. Specifically, we first randomly sample a tag from the database as a key. Then we sample its operator based on the type of keys. Finally, we randomly select a value from all possible values of that key, thus constructing a unit conditional expression in the form of ( key # operator # value ). Different sampled conditional expressions can be combined by union and intersection to form different types of targeted users. To ensure that the synthesized answers are logically reasonable, we will limit the number of conditional expressions in the logical template, and abandon overly complex intersection and union operations. \u00b7 Demand generation: Considering the strong ability of GPT-3.5, we provide some ( \ud835\udc51 \ud835\udc52 , \ud835\udc60 \ud835\udc52 ) examples in R as demonstrations to guide GPT-3.5 to write demands given above synthesized answers: where \ud835\udc60 \ud835\udc56 is the synthesized SELL answers. The language fluency and logical rationality of the generated demand will be checked. We denote this part of data distilled through the answer to demand pipeline. We obtain 1,200 pieces of data in this way and combine them with data coming from the demand to answer pipeline to form the final training data D \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b = {( \ud835\udc51 \ud835\udc56 , \ud835\udc60 \ud835\udc56 )} \ud835\udc41 \ud835\udc56 = 1 for small model, where N is the number of training data. 2.3.2 Reasoning-Augmented Multi-Task Fine-tune. The training objective of the normal distillation is to minimize the loss L : where f represents the model, \ud835\udc36\ud835\udc38 is the cross-entropy loss function. It [9, 12] has been pointed out that fine-tuning LLMs using data combined with reasoning steps can enhance the reasoning capabilities of the LLMs. Inspired by [9], we first prompt GPT-3.5 to generate intermediate reasoning steps of D \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b based on R : where ( \ud835\udc51 \ud835\udc52 , \ud835\udc5f \ud835\udc52 , \ud835\udc60 \ud835\udc52 ) \u2208 R , ( \ud835\udc51 \ud835\udc56 , \ud835\udc60 \ud835\udc56 ) \u2208 D \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b , \ud835\udc5f \ud835\udc56 is the corresponding generated reasoning steps of ( \ud835\udc51 \ud835\udc56 , \ud835\udc60 \ud835\udc56 ) . Subsequently, we define a multi-task training strategy in which not only \ud835\udc60 \ud835\udc56 but also \ud835\udc5f \ud835\udc56 are used as supervised labels to train the model. To be specific, we denote the input of these two tasks as: where the only difference between them is the content of instruction I \ud835\udc60 and I \ud835\udc5f which guide the model to generate the SELL answer or reasoning steps respectively. The corresponding training objectives of these two tasks are: The final loss function of multi-task fine-tuning is defined as:", "3 EXPERIMENTS": "", "3.1 Experimental Settings": "3.1.1 Benchmarks. We manually construct an NL2SELL testing benchmark for predicting, which contains 170 pieces of data based on expert knowledge in the format D \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 = {( \ud835\udc51 \ud835\udc5d , \ud835\udc60 \ud835\udc5d )} 170 \ud835\udc5d = 1 . The demands in D \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 do not exist in the training set D \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b or reasoning library R , eliminating the issue of label leakage. Table 1 shows the statistics of these 3 parts of data 1 . It should be noted that all training data generated by GPT-3.5 are strictly verified by well-trained volunteers, and we provide some examples in the supplementary materials. We will open source all these datasets on request. 3.1.2 Baseline. We first use GPT-3.5 to test the effectiveness of the analogical reasoning based prompting method. For comparison, we propose four additional variants of prompts as baselines: \u00b7 Zero-shot: Only given the instruction I , tag list \ud835\udc47 \ud835\udc5d and a test demand \ud835\udc51 \ud835\udc5d . \u00b7 Fixed few-shot: Given the instruction I , k fixed examples {( \ud835\udc51 \ud835\udc52 , \ud835\udc60 \ud835\udc52 )} \ud835\udc58 \ud835\udc52 = 1 \u2208 R for all \ud835\udc51 \ud835\udc5d \u2208 D \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 , tag list \ud835\udc47 \ud835\udc5d and a \ud835\udc51 \ud835\udc5d . \u00b7 Fixed few-shot + RS (Reasoning Steps): Given the instruction I , k fixed examples {( \ud835\udc51 \ud835\udc52 , \ud835\udc5f \ud835\udc52 , \ud835\udc60 \ud835\udc52 )} \ud835\udc58 \ud835\udc52 = 1 \u2208 R for all \ud835\udc51 \ud835\udc5d \u2208 D \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 , tag list \ud835\udc47 \ud835\udc5d and a \ud835\udc51 \ud835\udc5d . \u00b7 Random few-shot + RS: Given the instruction I , k randomly sampled examples {( \ud835\udc51 \ud835\udc52 , \ud835\udc5f \ud835\udc52 , \ud835\udc60 \ud835\udc52 )} \ud835\udc58 \ud835\udc52 = 1 \u2208 R for every \ud835\udc51 \ud835\udc5d \u2208 D \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 ,tag list \ud835\udc47 \ud835\udc5d and a \ud835\udc51 \ud835\udc5d . Different \ud835\udc51 \ud835\udc5d has different randomly sampled examples. In the knowledge distillation stage, we choose ChatGLM2-6B32K [5] and Baichuan2-13B-Chat [1] as student LLMs, considering their better comprehension ability in dealing with long Chinese texts compared to other models. In addition to fine-tuning methods mentioned in Section 2.3.1, we also propose three other training strategies as baselines: \u00b7 - MT: Training LLMs without multi-task (MT) strategy. The model input is (6) and the training objective is (8). \u00b7 - RS: Training LLMs with multi-task strategy but the reasoning steps \ud835\udc5f \ud835\udc52 of examples in (6) and (7) are not given in the inputs, in which way we can explore the impact of analogical reasoning. \u00b7 Normal: Training LLMs in a most normal way, in which the input of the model is the simple combination of instruction I and a demand \ud835\udc51 \ud835\udc56 , and the supervised label is \ud835\udc60 \ud835\udc56 only. 3.1.3 Evaluation metrics. To conduct a more reasonable and comprehensive evaluation, we evaluate the output of the LLMs on the NL2SELL task from 4 perspectives: \u00b7 Structure accuracy (Structure Acc.): The logic structure's accuracy of the predicted SELL expression. We remove the keys, operators, and values in the SELL, leaving only intersection and union symbols and separators as logic structures. For example, the structure of '(( Location # Belongs To # Hangzhou ) OR ( Location#Belongs To#Shanghai )) AND ( Pet Owning#Belongs To#True )' is '((##) OR (##)) AND (##)'. We use the mean value of the Levenshtein distance (L) [14] and the Ratcliff/Obershelp similarity (R/O) [2] to evaluate the structure accuracy compared to label SELL. \u00b7 Overall accuracy (Overall Acc.): We use the ScareBLEU [20] score to evaluate the overall similarity of the output and label texts as the overall accuracy. We set this as the main metric to evaluate the quality of predicted SELL answers. \u00b7 Human evaluation (Human Eval.): We conduct more finegrained evaluations through human evaluation. Annotators who receive good training will score the accuracy of the key, value, and logic between 0 and 10, and finally provide an overall score. \u00b7 GPT4 evaluation (GPT4 Eval.): We use the most advanced LLM gpt-4-turbo-preview (GPT-4) as the judges. To be specific, we prompt the GPT-4 to score the predicted answers from 0 to 10 and provide different scoring examples as a reference for GPT-4. 3.1.4 Implementation details. We set the number of analogical examples k as 3. The size of the reasoning library N \ud835\udc45\ud835\udc3f is 100. In the fine-tuning stage, ChatGLM2-6B-32K and Baichuan2-13B-Chat are tuned with LoRA [10] on 1 and 4 A100 (80G). We set the optimizer, maximum context size, batch size, and learning rate to Adam, 4096, 8, and 5e-5 respectively. For testing, a single 80G A100 is used. We use the code framework from LLaMA-Factory 2 [8].", "3.2 Analogical Prompting result on GPT-3.5": "3.2.1 Main Results. Table 2 displays the results of different prompting methods on GPT-3.5, where ARAP represents our Analogical Reasoning Augmented Prompting. The experimental results demonstrate that: (1) ARAP significantly outperforms other prompting \u56fe\u8868\u6807\u9898 \u56fe\u8868\u6807\u9898 52 54 56 58 60 62 1 2 3 4 5 S-BLEU Number of Examples (b) 54 55 56 57 58 59 25 50 75 100 125 150 S-BLEU Size of Reasoning Library (a) methods in various evaluation metrics, especially in terms of structural accuracy which improves by over 7%, proving that analogical examples provide a good logical structure for reasoning. (2) Even randomly selected few-shot reasoning examples are better than fixed ones. This indicates that fixed reasoning examples for all inputs are often suboptimal, demonstrating the necessity of establishing a reasoning library to provide diverse reasoning samples. (3) Reasoning steps are essential in prompting. When comparing results between 'Fixed few-shot' and 'Fixed few-shot + RS', we found that reasoning steps have a significant active impact on reasoning accuracy, which is similar to the trend in other previous research works [26]. Further, we explore the impact of parameter settings, such as the size of reasoning library N \ud835\udc45\ud835\udc3f and the number of analogical examples \ud835\udc58 . As shown in Figure 3 (a), as the size of the reasoning library increases, the overall accuracy gradually increases, but the rate slows down, indicating that when the inference library reaches a certain scale, it can already provide references for most marketer's demands. In Figure 3 (b), it can be observed that the overall accuracy first increases and then decreases when the number of examples in prompts increases. This reflects that when engaging in contextbased learning, providing more examples to LLMs is not always better. When an excessive number of examples are input, irrelevant examples may introduce noise, and an overly lengthy context can also impose a greater burden on the model's understanding. 3.2.2 Case Study. Table 3 is a case we obtained from GPT-3.5 with different versions of prompts. Although we provide the same tag list for a specific demand across different prompt versions, there are significant variations in the selection of key-value pairs. For the first demand 'Company white-collar workers who enjoy drinking Starbucks', all baseline versions ignore the tag of 'Career', indicating that the model cannot smoothly map the description 'white-collar workers' to 'Career', although this is common sense. Meanwhile, in both fixed few-shot versions, the model treats 'company whitecollar workers' as the value of the 'Resident City', which is due to the answers of fixed examples in prompts containing similar expressions such as ( Resident city # Belongs To # Hangzhou ), resulting in the model being inappropriately imitated. While in ARAP, we retrieve similar demands like 'Young people who enjoy swimming or female white-collar workers who enjoy reading in Shanghai' and its reasoning steps from the reasoning library, thus helping the LLM analogically transform the 'white-collar workers' into conditional expression '( Career # Belongs To # White-collar )' as they share the similar description 'white-collar workers'. 3.3.2 Ablation Study. As mentioned in section 2.3.1, there are two sources of distilled knowledge we use when fine-tuning, i.e., knowledge distilled from demand to answer and answer to demand approach. The knowledge distilled from demand to answer is the most commonly used knowledge distillation approach, and what we mainly want to explore is the effectiveness of the knowledge distilled from answer to demand . To verify this, we conduct ablation experiments on the best-performing models 'Baichuan2-13B-Chat' with two training settings ( i.e., ARAFT and ARAFT - RS). Table 5 shows the effect of the knowledge distilled from answer to demand ( a2d ), it can be found that when removing this part of data during the fine-tuning, the performance on both ARAFT and ARAFT - RS significantly decreases, although this part of data only accounts for about one-tenth of the training data. This indicates that adding samples with a more uniform distribution of logic and tags to the training corpus can improve the robustness of the LLMs, and our distillation method from answers to demand achieves this.", "3.3 Knowledge distillation results on smaller LLMs": "3.3.1 Main Results. Table 4 shows the testing results on fine-tuned LLMs with the knowledge distilled from GPT-3.5, where ARAFT represents fine-tuning (FT) in the reasoning-augmented multi-task paradigm mentioned in section 2.3.2. All testing prompts are the same as ARAP prompts in GPT-3.5. As is shown: (1) Both two models show comparable capability to GPT-3.5, especially on Baichuan213B-Chat, where the score of S-BLEU is improved by over 5% compared to GPT-3.5 after fine-tuning. This demonstrates the superiority of our distillation and training methods. (2) Compared with the results of the single-task training strategy (-MT), the multi-task training strategy brings improvement to the reasoning performance of the LLMs. Although during testing, we limit the model to only output the final answers without reasoning steps, adding training tasks that predict reasoning steps can enhance the models' reasoning abilities and robustness. (3) Compared with the results without reasoning steps in the input corpus (-RS), we can conclude that providing explicit reasoning steps in training input is beneficial. If there are no reasoning steps in the input corpus but a multi-task training strategy is used, it will increase the difficulty of the training task and lead to poor performance, which is particularly evident in Baichuan2-13B-Chat. (4) The result of fine-tuning using only the normal demand and answer pairs is not satisfactory, which indicates that it is challenging for LLMs to learn the data patterns solely based on the Q&A pairs in situations where tasks are difficult. Therefore, it is necessary to design appropriate demonstrations and training tasks to fine-tune the LLMs.", "4 APPLICATION": "Model Deployment. The LLM tuned in section 3.3 has been deployed online for application using an A10 with a memory capacity of 24G. To achieve optimal online inference performance, the deployment involves a total of 8 cards, including both the preproduction and online environments. Meanwhile, the retriever service based on BGE has also been deployed on an A10 GPU. Application Case. Figure 4 shows an application case of ARALLM in online user targeting. A marketer wants to do marketing for a stage drama performance, so he can input a raw demand and click the 'Search' button. Our system will invoke the retriever service to retrieve similar top-k demonstrations and top-n tags from the reasoning library embeddings (RL Embs) and tag embeddings (Tag Embs), respectively. After filling the prompt based on retrieved demonstrations and tags, our system requests the deployed ARAFT model to generate the answer expressed in SELL. The SELL expression will be parsed by our parser and results will be visualized to the marketers on the panel as shown in Figure 4. The marketers can verify and edit the card on the panel after clicking the edit button. Finally, they can click the 'Export' button to get the target users. Practical Effects. Our system has been already running for months. We have set two metrics to evaluate the new approach online, including operation time , and number of likes from marketers. By collecting the operation log, the entire system takes no more than 10 seconds to complete a request and the average marketers' operation time of the new system is about 3 minutes, which is 4-10 times shorter than the former on average. Beyond that, we receive likes 1.3 times larger than the former, revealing the operation friendliness of this new way of user targeting. Online A/B testing results. We conduct online A/B experiments to compare our methods with the traditional rule-based methods and use the direct metrics CTR (Click-Through-Rate) and Exposure (the number of users who have been exposed by the campaign) to evaluate them. Results are shown in Table 6. From an end-to-end perspective, our solution has achieved better marketing results than rule-based methods in multiple marketing campaigns.", "5 RELATED WORK": "", "5.1 User targeting": "Methods of user targeting mainly can be categorized into two lines: rule-based and model-based methods. The rule-based methods [19, 23] match potential users with specific demographic tags (age, gender, geography) or interests that are targeted by marketers which need marketers to do user profile association or mining. The model-based methods expand a given seed set by calculating the similarity of all pairs between seed users and candidate users [16, 18] or training customized prediction models for each service or Search Career Value Op Key Belongs To College Students User Number \uff1a xx,xxx,xxx Preference Value Op Key Belongs To Stage Drama User Number \uff1a x,xxx,xxx Age Value Op Key Between 18,35 User Number \uff1a xxx,xx,xxx I want to market theater tickets. Please select young intellectuals who enjoy stage drama performances. OR AND Export Drama-Lover Youngster College Students First tier city Art-lover Intellectual Embedding Recall RL Embs Tag Embs Prompt Filling Top k Top n (Instruction): (Demonstrations): (Input): Demand: Tag list: LLM Inference Parse (Age#Between#18,35)\u2026 Key: xx | Op: xx | Value: xx \u2026\u2026 campaign [32, 33]. These methods show good performance in user targeting but need seed sets. Both methods have primarily focused on intricate architectures for effectively and efficiently targeting users, while commonly disregarding the natural and significant gap between marketers' demands and the capabilities of current models, which can be time-consuming and unfriendly.", "5.2 NL2SQL": "Our NL2SELL task is somewhat similar to the task of translating natural language into SQL (NL2SQL). NL2SQL is an important and challenging task in helping nonexpert users to better manipulate databases. Graph neural networks (GNNs) are popular in NL2SQL research [4, 11, 25], which focus on modeling the relationship between the question and the schema in database system. Recently, large pre-trained language models (PLMs), such as T5 [21] and GPT-3 [3], have shown strong translating ability in NL2SQL tasks. Due to the extensive knowledge injection during the pretrain phase, PLM-based methods achieve better results compared to GNN-based methods by fine-tuning them with a small amount of data. Some works also explore solving NL2SQL tasks with in-context learning [7, 15], it has been pointed out that LLMs show strong few-shot or zero-shot abilities even without any training data, which start a new direction for future research on NL2SQL field. The LLM-based methods in NL2SQL bring inspiration to our works.", "6 CONCLUSION AND FUTURE WORK": "In this paper, we provide a novel user targeting approach by leveraging LLMs to gain a structured understanding of marketers' demands. We first define a new language SELL to express the needs of marketers more clearly. Subsequently, we propose an analogical reasoning augmented framework, ARALLM, which consists of analogical reasoning based prompting and reasoning-augmented multi-task model distillation. The experimental results on GPT-3.5 show that our analogical reasoning based prompting significantly outperforms other baseline prompting methods in the NL2SELL task. In addition, we distill a large-scale dataset using GPT-3.5 and train the student LLMs using a multi-task training approach, which is successfully used for online deployment. Since we primarily focus on the NL2SELL task in practical application scenarios, we have not yet applied the ARALLM framework to other reasoning tasks, such as coding or other structured language translation( e.g., HTML, JSON), which will be explored as future work. At the same time, there are still labor costs in the construction of reasoning libraries and SELL datasets, we will explore more automated construction methods in the future.", "ACKNOWLEDGMENTS": "This work is funded by NSFC62306276, NSFCU23B2055 and NSFCU19B2027. This work is supported by the Fundamental Research Funds for the Central Universities (226-2023-00138). This work was supported by Ant Group.", "REFERENCES": "[1] Baichuan. 2023. Baichuan 2: Open Large-scale Language Models. arXiv preprint arXiv:2309.10305 (2023). https://arxiv.org/abs/2309.10305 [2] Paul E Black. 2004. Ratcliff/Obershelp pattern recognition. Dictionary of algorithms and data structures 17 (2004). [3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877-1901. [4] Ruisheng Cao, Lu Chen, Zhi Chen, Yanbin Zhao, Su Zhu, and Kai Yu. 2021. LGESQL: Line Graph Enhanced Text-to-SQL Model with Mixed Local and NonLocal Relations. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) . https://doi.org/10.18653/v1/2021. acl-long.198 [5] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM: General Language Model Pretraining with Autoregressive Blank Infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . 320-335. [6] Dedre Gentner. 1983. Structure-mapping: A theoretical framework for analogy. Cognitive science 7, 2 (1983), 155-170. [7] Zihui Gu, Ju Fan, Nan Tang, Lei Cao, Bowen Jia, Sam Madden, and Xiaoyong Du. 2023. Few-shot Text-to-SQL Translation using Structure and Content Prompt Learning. Proceedings of the ACM on Management of Data 1, 2 (2023), 1-28. [8] hiyouga. 2023. LLaMA Factory. https://github.com/hiyouga/LLaMA-Factory. [9] Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. 2023. Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. arXiv preprint arXiv:2305.02301 (2023). [10] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021). [11] Binyuan Hui, Ruiying Geng, Qiyu Ren, Binhua Li, Yongbin Li, Jian Sun, Fei Huang, Luo Si, Pengfei Zhu, and Xiaodan Zhu. 2021. Dynamic Hybrid Relation Exploration Network for Cross-Domain Context-Dependent Semantic Parsing. Proceedings of the ... AAAI Conference on Artificial Intelligence,Proceedings of the ... AAAI Conference on Artificial Intelligence (May 2021). [12] Seungone Kim, Se June Joo, Doyoung Kim, Joel Jang, Seonghyeon Ye, Jamin Shin, and Minjoon Seo. 2023. The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning. arXiv preprint arXiv:2305.14045 (2023). [13] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems 35 (2022), 22199-22213. [14] Vladimir I Levenshtein et al. 1966. Binary codes capable of correcting deletions, insertions, and reversals. In Soviet physics doklady , Vol. 10. Soviet Union, 707-710. [15] Aiwei Liu, Xuming Hu, Lijie Wen, and PhilipS. Yu. 2023. A comprehensive evaluation of ChatGPT's zero-shot Text-to-SQL capability. (Mar 2023). [16] Haishan Liu, David Pardoe, Kun Liu, Manoj Thakur, Frank Cao, and Chongzhe Li. 2016. Audience expansion for online social network advertising. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . 165-174. [17] Xuantao Lu, Jingping Liu, Zhouhong Gu, Hanwen Tong, Chenhao Xie, Junyang Huang, Yanghua Xiao, and Wenguang Wang. 2022. Parsing Natural Language into Propositional and First-Order Logic with Dual Reinforcement Learning. In Proceedings of the 29th International Conference on Computational Linguistics . International Committee on Computational Linguistics, Gyeongju, Republic of Korea, 5419-5431. https://aclanthology.org/2022.coling-1.481 [18] Qiang Ma, Eeshan Wagh, Jiayi Wen, Zhen Xia, Robert Ormandi, and Datong Chen. 2016. Score look-alike audiences. In 2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW) . IEEE, 647-654. [19] Ashish Mangalampalli, Adwait Ratnaparkhi, Andrew O Hatch, Abraham Bagherjeiran, Rajesh Parekh, and Vikram Pudi. 2011. A feature-pair-based associative classification approach to look-alike modeling for conversion-oriented usertargeting in tail campaigns. In Proceedings of the 20th international conference companion on World wide web . 85-86. [20] Matt Post. 2018. A Call for Clarity in Reporting BLEU Scores. In Proceedings of the Third Conference on Machine Translation: Research Papers . Association for Computational Linguistics, Belgium, Brussels, 186-191. https://www.aclweb. org/anthology/W18-6319 [21] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and PeterJ. Liu. 2019. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. arXiv: Learning,arXiv: Learning (Oct 2019). [22] Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2021. Learning to retrieve prompts for in-context learning. arXiv preprint arXiv:2112.08633 (2021). [23] Jianqiang Shen, Sahin Cem Geyik, and Ali Dasdan. 2015. Effective audience extension in online advertising. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . 2099-2108. [24] Peter D Turney. 2008. The latent relation mapping engine: Algorithm and experiments. Journal of Artificial Intelligence Research 33 (2008), 615-655. [25] Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov, and Matthew Richardson. 2020. RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . https://doi.org/10.18653/v1/2020.acl-main.677 [26] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems 35 (2022), 24824-24837. [27] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023. C-Pack: Packaged Resources To Advance General Chinese Embedding. arXiv:2309.07597 [cs.CL] [28] Dan Yang, Binbin Hu, Xiaoyan Yang, Yue Shen, Zhiqiang Zhang, Jinjie Gu, and Guannan Zhang. 2023. Who Would be Interested in Services? An Entity Graph Learning System for User Targeting. arXiv preprint arXiv:2305.18780 (2023). [29] Yuan Yang, Siheng Xiong, Ali Payani, Ehsan Shareghi, and Faramarz Fekri. 2023. Harnessing the Power of Large Language Models for Natural Language to FirstOrder Logic Translation. arXiv:2305.15541 [cs.CL] [30] Zhen Yao, Wen Zhang, Mingyang Chen, Yufeng Huang, Yi Yang, and Huajun Chen. 2023. Analogical inference enhanced knowledge graph embedding. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 37. 4801-4808. [31] Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun Nie. 2023. Retrieve Anything To Augment Large Language Models. arXiv:2310.07554 [cs.IR] [32] Yongchun Zhu, Yudan Liu, Ruobing Xie, Fuzhen Zhuang, Xiaobo Hao, Kaikai Ge, Xu Zhang, Leyu Lin, and Juan Cao. 2021. Learning to Expand Audience via Meta Hybrid Experts and Critics for Recommendation and Advertising. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining . 4005-4013. [33] Chenyi Zhuang, Ziqi Liu, Zhiqiang Zhang, Yize Tan, Zhengwei Wu, Zhining Liu, Jianping Wei, Jinjie Gu, Guannan Zhang, Jun Zhou, et al. 2020. Hubble: An industrial system for audience expansion in mobile marketing. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 2455-2463.", "A DESIGN OF SELL": "We provide a detailed explanation of the design and the usage of the SELL. The SELL is mainly composed of four elements: Keys are a series of tags that describe the features of the user, such as Gender , Marital Status , Resident City , and so on. In practical marketing scenarios, the database contains tens of thousands of tags that describe user profiles. Selecting the appropriate keys from this vast array of tags is a challenging task. Values are the fillings of the corresponding keys. They can be generally divided into three types: numerical type, string type, and boolean type. For instance, the value corresponding to the key Monthly Income is of numerical type, because they are continuous and non-enumerable. The value of Gender is of string type, with limited options: Male or Female . Boolean type values have only two states, i.e., True or False , such as Marital Status . Operators represent the relationship between keys and values. For numerical values, seven types of operators can be used to connect them with corresponding keys: Equal To, Greater Than, Less Than, Not Equal To, Not Greater Than, Not Less Than, and Between . For values of string type and boolean type, there are two types of operators: Belongs To and Not Belongs To . Keys, operators, and values can form a basic conditional expression in SELL, formatted as '( key#operator#value )', such as '( Marital Status#Belongs To#True )'. A conditional expression represents a cluster of target users, for example, in the above example, the target users are married. Intersection and Union symbols are used to take the intersection or union of multiple conditional expressions. In practical marketing scenarios, the features of the target users are often complex and difficult to describe using a single conditional expression. Therefore, it is necessary to combine the target users through intersection or union operations. In SELL, we use the symbols AND and OR to denote the intersection and union operations, respectively. For example, '(( Resident City # Belongs To # City A ) OR ( Resident City#Belongs To#City B )) AND ( Pet Owning#Belongs To#True )' describes the pet owners who live in City A or City B. Some demands and their corresponding SELL expressions are shown in Table 7. More data can be found at https://github.com/ alipay/Analogic-Reasoning-Augmented-Large-Language-Model.", "B HUMAN-WRITTEN REASONING STEPS": "As mentioned in the main paper, we write reasoning steps for 10% of data in the reasoning library. Table 8 shows an example. You are a robot skilled in reasoning. Please learn the following examples and complete the final reasoning steps: ### Demand: Mothers who pay attention to baby education Tag list: [User Has Child/Days of Listening to Audiobooks/Homepage Visits/Red Packet Cover Collection/Current Insured Products/Balance of Points/Alipay User in Gourmet Scene/User Age Group/Number of Days Since User's First Account Opening/User Child Age/User Gender/User Marital Status/Preferences/\u2026\u2026] Answer: (Preference#Belongs To# Baby Education) AND (User Gender#Belongs To# Female) AND ((User Has Child#Belongs To#True) OR (User Child Age#Between#0,4)) Reasoning: (1)Extract keywords: Mothers, baby education (2) Select tags: User Has Child, User Child Age, User Gender, User Marital Status, Preferences (3) Form conditional expressions: (User Has Child#Belongs To#True) (User Gender#Belongs To#Female) (User Child Age#Between#0,4) (Preference#Belongs To#Baby Education) (4) Combine:(Preference# Belongs To# Baby Education) AND (User Gender#Belongs To#Female) AND ((User Has Child#Belongs To#True) OR (User Child Age#Between#0,4)) ### ...... ### Demand:{Input Demand} Tag list: {Input Tag list} Answer: {Input Answer} Reasoning: You are a robot skilled in reasoning and story writing, giving you a logical expression that describes a certain group of people, and your task is to transform them into smooth natural language for expression. When generating natural language, please do not copy the expressions in logical expressions, but rather convert them into some synonymous expressions. The following are some examples. Please understand and complete the conversion of the final logical expression. ### Answer: (Reference#Belongs To#Game) AND (User Age Group#Between#15,25) AND ((Career#Equals To#Student) OR (Is College student#Belongs to#True)) Demand: A group of students aged 15-25 who love games. ### ...... ### Answer: {Input Answer} Demand:", "C REASONING STEPS GENERATION PROMPT": "As mentioned in the main paper, we use GPT-3.5 to help us complete the reasoning steps of remained data. Figure 8 shows the prompt we use in this step.", "D DEMAND GENERATION PROMPT": "The prompt framework for demand generation given SELL expressions is shown in Figure 6.", "E LOGIC TEMPLATE": "As mentioned in the main paper, we first construct a series of logical templates of SELL to artificially synthesize answers. Table 9 shows all the logical templates we used. The c in the template", "Table 7: Some examples of marketers' demands and corresponding SELL.": "", "Table 8: Examples of human-written reasoning steps.": "You are a professional grading expert. Given a predicted logic expression and a ground truth logical expression, your task is to assess the accuracy of the predicted logic expression compared to the standard logical expression. Score the accuracy of the predicted logic expression between 0 and 10. 10 if the predicted answer and the ground truth answer are identical, and 0 if they are entirely different. If only part of them is identical, please rate it between 0 and 10 based on your judgment. The closer the score is to 10, the predicted answer is more accurate. Here are some examples: ### Predicted answer: (Game#Belongs To#Genshin Impact) AND (Age#Greater Than#High) AND (Enthusiast#Equal To#Game) AND (Gender#Equal To#Male) Ground Truth Answer: (Preference#Belongs To#Genshin Impact) AND (User Age Group#Between# 18,35) AND (Gender#Belongs To#Male) Accuracy: 3 ### Predicted answer: (User Age Group#Between#16,45) AND (Gender#Belongs To#Female) AND (Pet Owning#Belongs To#True) Ground Truth answer: (User Age Group #Between#16,45) AND (Gender#Belongs To#Female) AND (Pet Owning#Belongs To#Have Pets) Accuracy: 8 ...... Now given a predicted answer and a ground truth answer, please think carefully and output the accuracy score (don't output other contents): Predicted answer: {Input Predicted answer} Ground truth answer:{Input Ground truth answer} Accuracy: You are a robot skilled in reasoning. Please learn the following examples and complete the final reasoning steps: ### Demand: Mothers who pay attention to baby education Tag list: [User Has Child/Days of Listening to Audiobooks/Homepage Visits/Red Packet Cover Collection/Current Insured Products/Balance of Points/Alipay User in Gourmet Scene/User Age Group/Number of Days Since User's First Account Opening/User Child Age/User Gender/User Marital Status/Preferences/\u2026\u2026] Answer: (Preference#Belongs To# Baby Education) AND (User Gender#Belongs To# Female) AND ((User Has Child#Belongs To#True) OR (User Child Age#Between#0,4)) Reasoning: (1)Extract keywords: Mothers, baby education (2) Select tags: User Has Child, User Child Age, User Gender, User Marital Status, Preferences (3) Form conditional expressions: (User Has Child#Belongs To#True) (User Gender#Belongs To#Female) (User Child Age#Between#0,4) (Preference#Belongs To#Baby Education) (4) Combine:(Preference# Belongs To# Baby Education) AND (User Gender#Belongs To#Female) AND ((User Has Child#Belongs To#True) OR (User Child Age#Between#0,4)) ### ...... ### Demand:{Input Demand} Tag list: {Input Tag list} Answer: {Input Answer} Reasoning: represents the basic conditional expression in SELL. To ensure that the synthesized answers are logically reasonable, we limit the number of conditional expressions in the logical template, and abandon overly complex intersection and union operations.", "F GPT4 EVALUATION PROMPT": "The prompt framework for guiding GPT4 to evaluate the quality of answer SELL is shown in Figure 7. Table 9: Logic template for answer generation."}
