{
  "BBE-LSWCM: A Bootstrapped Ensemble of Long and Short Window Clickstream Models": "Arnab Chakraborty A2D-AI, Intuit Bangalore, India Vikas Raturi A2D-AI, Intuit Bangalore, India Shrutendra Harsola A2D-AI, Intuit Bangalore, India",
  "ABSTRACT": "We consider the problem of developing a clickstream modeling framework for real-time customer event prediction problems in SaaS products like QBO. We develop a low-latency, cost-effective, and robust ensemble architecture (BBE-LSWCM), which combines both aggregated user behavior data from a longer historical window (e.g., over the last few weeks) as well as user activities over a short window in recent-past (e.g., in the current session). As compared to other baseline approaches, we demonstrate the superior performance of the proposed method for two important real-time event prediction problems: subscription cancellation and intended task detection for QBO subscribers. Finally, we present details of the live deployment and results from online experiments in QBO. QBO: We will use QuickBooks Online (QBO) product to demonstrate the application of this framework. QBO is an Intuit cloudbased software product that provides various bookkeeping and accounting-related capabilities to small/medium-size businesses. QBO provides a host of features like issuing invoices, accepting payments, tracking business performance, managing inventory, etc., which makes it challenging to model user behavior.",
  "ACMReference Format:": "Arnab Chakraborty, Vikas Raturi, and Shrutendra Harsola. 2024. BBELSWCM: A Bootstrapped Ensemble of Long and Short Window Clickstream Models. In 7th Joint International Conference on Data Science & Management of Data (11th ACM IKDD CODS and 29th COMAD) (CODS-COMAD 2024), January 4-7, 2024, Bangalore, India. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3632410.3632452",
  "1 INTRODUCTION": "Objective: Ability to model clickstream in SaaS products (like QBO) can help improve user experience by making intelligent real-time in-session decisions. Some examples are: · Subscription cancellation (churn) prediction: Predict in realtime if a user is likely to cancel the product subscription within the next few days. These predictions can be used by CRM teams to design in-session proactive and contextual interventions to retain the user. · Intended task detection: Predict user intent when they land on the help section or chat or dial into support. It can be used to personalize the experience by directly presenting users with relevant knowledge articles / FAQs or routing them to the right agent. · Task abandonment prediction: Forecasting in real-time if a user is likely to abandon the task that they started (like setting up payroll). These predictions can be used to offer intervention and guide them toward completing the task. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CODS-COMAD 2024, January 4-7, 2024, Bangalore, India © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-1634-8/24/01...$15.00 https://doi.org/10.1145/3632410.3632452 Motivation: Most studies in clickstream modeling is focused on the eCommerce domain, where the goal is to predict user exits with or without purchase in a session and offer marketing interventions to drive users towards purchase. These models mostly rely on pages visited in the current session to make the predictions. On the other hand, the studies related to the prediction of customer events such as churn do not take the user behavior from the current session into account. There is also a handful of literature on the automated encoding of high-dimensional user-behavior feature interactions for CTR(click-through-rate) prediction, but, how these models perform for real-time customer event predictions using clickstream data as the primary predictor, has not been explored. These approaches are not the most optimal choice for the problems considered here due to the following reasons. · Depending on the event to predict (e.g., churn vs intended task), the importance of current session user behavior vs user activities from beyond the current session changes. A methodology is required that can efficiently incorporate both the current session and beyond-current-session historical user behavior and assign importance automatically based on the underlying data. · In many SaaS products only the pages are not enough to understand the user behavior efficiently and hence, a more generic framework that can incorporate further granular properties of clickstream such as events performed within a page, is required. · To avoid label leakage and to closely replicate the real-time deployment scenario, model training, and evaluation have to be based on reference timestamps (referred to as ref-ts) at which the model inference is requested. There can be multiple ref-ts within a session accumulating a few hundred ref-ts per user making the problem challenging in terms of scale and class imbalance. Proposed Approach: In this paper, we present BBE-LSWCM (Block-Bootstrapped Ensemble of Long and Short Window Clickstream Models) - an ensemble approach for real-time clickstream modeling using both aggregated user behavioral data over a longer historical window (e.g., over the last few weeks) as well as user activities over a short window from recent past (e.g., current session or in last hour). In most of the data systems in the industry, only a few datasets are available in real-time through streaming systems (like CODS-COMAD 2024, January 4-7, 2024, Bangalore, India A. Chakraborty, V. Raturi and S. Harsola Kafka) while most of the other datasets reside in datalake tables and get updated with a frequency of 24 hours or more. Models built on only real-time data are often not robust due to the limited coverage and scope of real-time datasets. On the other hand, predictions coming from models trained only on the daily updated datasets are not sensitive to the dynamic nature of the user behavior. BBE-LSWCM, by its design, can incorporate both the real-time updated and daily updated datasets and model user behavior in real-time robustly. In addition, BBE-LSWCM allows the featurization of the models to be completely automated and data-driven, making this approach reusable for multiple clickstream-based event prediction problems. Results: On the QBO dataset, BBE-LSWCM has achieved at least a 30% better lift score over the next best model for real-time churn detection. In the online A/B test in QBO, we have observed a 20% reduction in the first 31 days churn for high-risk customers detected by BBE-LSWCM deployed in a real production setting. Moreover, by rigorous ablation study, the generalisibility of BBE-LSWCM has been proved through application in both churn and intended-task detection problems. Contributions: The main contributions of this paper are: (1) a novel robust ensemble architecture combining long window and short window user behavior for real-time clickstream modeling; (2) block-bootstrap sampling of ref-ts from clickstream for train data generation; (3) generic modeling framework with automated featurization and ability to incorporate multiple granular clickstream properties like page, event, time; (4) a detailed experimental study on QBO dataset; (5) live deployment of BBE-LSWCM to power scalable yet costeffective in-session proactive interventions.",
  "2 RELATED WORK": "Clickstream modeling has been extensively used in eCommerce domain for multiple problems like next page prediction [3, 5, 14, 17, 20], recommending item to the user [1, 13, 26, 34, 39, 47, 49, 50] and purchase prediction [32, 33, 43, 46]. But as described in Sec 1, these approaches do not take the historical user behavior beyond the current session into account while predicting the target. Moreover, most of these studies are restricted to predicting session-related customer events e.g., next page visit or exit without purchase. The applicability of these methods has not been studied for the types of customer events mentioned in Sec. 1. On the contrary, most studies in churn detection [2, 10, 12, 18, 25, 28, 30] use only the aggregated user-behavior data over a long period of time as input to the model and evaluation is based on a single inference of the model in time. Hence, the methods and results of these papers can not be generalized for real-time customer event prediction problems. In [45], though the authors have proposed a churn modeling framework that considers a sequence of daily activity data as a multivariate time-series input, this methodology does not take the in-session user behavior or real-time deployment scenario into account. There are few recent studies [4, 9] which propose a framework for real-time churn identification, but these studies do not discuss automated use of large-scale clickstream data and application of these methods for other types of customer events such as intended task detection is not straightforward. For problems such as intended task detection or task abandonment prediction using user behavior data, models introduced for CTR prediction in Ads recommendation [7, 8, 15, 27, 48] can also be used. Though these neural network-based architectures provide an automated way to efficiently capture the high-dimensional crossinteractions between the numerical and categorical user behavior features, the efficacy of these models to capture the intrinsic sequential nature of the high-dimensional clickstream data, for the customer event prediction problems considered in this paper, have hardly been explored in literature. To the best of our knowledge, there is no prior work that has addressed all the practical industry challenges mentioned so far in real-time modeling of customer events using clickstream data in the SaaS domain.",
  "3 BBE-LSWCM: BLOCK-BOOTSTRAPPED ENSEMBLE OF LONG AND SHORT WINDOW CLICKSTREAM MODELS": "",
  "3.1 Inference Mechanism": "Weconsider the following inference task: given a ref-ts 𝑇 , on request from the model consumer (product UI, customer success tool, etc.), the concerned task is to predict the target with real-time latency. Target could be the risk of subscription cancellation within a few days from 𝑇 , or abandonment of the current task within a short time window from 𝑇 , or the intended task of the user at time 𝑇 . Output: For a ref-ts 𝑇 , for 𝑖 th user, the output of this prediction task is a time-dependent variable 𝑦 𝑖 ( 𝑇 ) referred to as target . We denote the set of eligible ref-ts's for the 𝑖 th user, as T 𝑖 . For example, in the real-time churn detection problem, 𝑦 𝑖 ( 𝑇 ) = 1 if the 𝑖 th user will cancel their subscription during the time-period ( 𝑇,𝑇 + 𝛿 ] , where typically 𝛿 > 0 is 24 or 48 hours. Here, T 𝑖 can consist of all the time-points 𝑇 at which the 𝑖 th user has an active subscription, and the model consumer can request an inference. On the other hand, for the problem of intended task detection of the user, 𝑦 𝑖 ( 𝑇 ) is a multi-class random variable with T 𝑖 containing all the timestamps whenthe user engages with a help channel for the first time, e.g., the timestamp corresponding to the click to open the digital assistant, or starting the search or, the timestamp when the user calls into the customer care. Note that, unlike the previous works in the eCommerce domain on clickstream modeling [11, 16, 33] where the target is always defined at the end of the clickstream session and only single inference within a session is considered, the formulation considered in this paper supports multiple inferences of the model even within a session and the target can occur further away from the end of a session as well. Input: Input to this prediction task is the historical clickstream data and user profile information up to time-point 𝑇 . Often in large SaaS products like QBO, the user can perform multiple types of activities such as invoicing, banking, payments, payroll, etc. and so, the start and end of a session are not always well-defined. Hence, we define the clickstream data based on a time-window [ 𝑇 -𝑤,𝑇 ) for a given ref-ts 𝑇 ∈ T 𝑖 and a time duration 𝑤 > 0. The clickstream data for 𝑖 th user is C 𝑖 ( 𝑇 ; 𝑤 ) = ⟨( 𝑡 𝑚 𝑖 , 𝑝 𝑚 𝑖 , 𝑒 𝑚 𝑖 )⟩ 𝑀 𝑖 ( 𝑇 ; 𝑤 ) 𝑚 = 1 , BBE-LSWCM: A Bootstrapped Ensemble of Long and Short Window Clickstream Models CODS-COMAD 2024, January 4-7, 2024, Bangalore, India with 𝑝 𝑚 𝑖 ∈ P = { 𝑃 1 , 𝑃 2 , . . . } being the page visit and 𝑒 𝑚 𝑖 ∈ E = { 𝐸 1 , 𝐸 2 , . . . } is the event (action) performed at timestamp 𝑡 𝑚 𝑖 and ⟨·⟩ denotes an ordered sequence of objects. To give an example, on 'banking' page, the user can perform events like 'bankconnect: started', 'account: viewed', 'transaction: viewed' etc. The total number of clicks performed by the 𝑖 th user in [ 𝑇 -𝑤,𝑇 ) is 𝑀 𝑖 ( 𝑇 ; 𝑤 ) and 𝑇 -𝑤 < 𝑡 1 𝑖 < ... < 𝑡 𝑀 𝑖 ( 𝑇 ; 𝑤 ) 𝑖 < = 𝑇 are the ordered timestamps at which the clicks are performed by the user. For simplicity of notation, we drop subscript 𝑖 from C 𝑖 ( 𝑇 ; 𝑤 ) and other variables, when referring to a generic user. User profile information is also another input to the model. For 𝑖 th user the user profile information at time 𝑇 is denoted by 𝑧 𝑖 ( 𝑇 ) which is a multivariate variable consisting of information such as the subscription type, tenure duration, location, and any other demographic information that is available.",
  "3.2 Model Architecture for BBE-LSWCM": "The proposed BBE-LSWCM approach is a hybrid ML framework tailored to solve real-time event prediction tasks. It allows the prediction to incorporate not only the 'recent' in-session user activities but also, the historical user behavior from a longer window. As Figure 1: BBE-LSWCM inference mechanism (black rectangles denote ML modules). User time progression Auto FE EMM LWM SWM mentioned earlier in Sec. 1, in SaaS subscription products such as QBO, often the user behaviors beyond the current session influence the distribution of the target. For example, to predict whether a customer will cancel their subscription within the next few hours or days, in addition to the in-session clickstream, historical user behavior such as activities over the last few weeks in some of the important pages is highly important. On the other hand, for detecting the user's intended task when they are engaging with help channels for the first time, often the in-session user behavior is the key predictor. Hence, depending on the data and problem in hand the modeling framework needs to automatically calibrate the importance of in-session vs longer history of user behavior. Moreover, for a given user, C ( 𝑇 ; 𝑤 ) can contain more than 10 5 clicks if 𝑤 is a few weeks. Processing such a huge volume of clicks in real-time for model inference would be computationally challenging resulting in high inference latency and would also be highly expensive in terms of deployment cost. As shown in Fig. 1, in BBE-LSWCM, we propose to break the historical clickstream C ( 𝑇 ; 𝑤 ) into two parts that are mutually exclusive in time. One part contains the historical clicks that happened in the same session as of the ref-ts 𝑇 or in the very recent past (30 minutes to one hour back) from 𝑇 . We call this clickstream as Short Window Clickstream (SWC) and denote it by C ( 𝑇,𝑤 𝑠 ) (for simplicity denoted as C 𝑠 ( 𝑇 ) ), where 𝑤 𝑠 > 0 can usually vary up to a few hours. The second part of the clickstream, named as Long Window Clickstream (LWC) , consists of the clicks that happened in the longer historical time window ( 𝑇 -𝑤,𝑇 𝑙 ] for a time point 𝑇 𝑙 such that 𝑇 -𝑤 << 𝑇 𝑙 < 𝑇 -𝑤 𝑠 . So, this clickstream data does not contain the clicks from recent history, rather it contains clicks from a further past. We denote LWC as C ( 𝑇 𝑙 ; 𝑤 𝑙 ) (for simplicity denoted as C 𝑙 ( 𝑇 ) ), where 𝑤 𝑙 = 𝑤 - ( 𝑇 -𝑇 𝑙 ) is the length of the long historical window. Instead of building a single model with C ( 𝑇 ; 𝑤 ) as an input - which requires processing thousands of clicks at the time of every real-time inference - we propose a step-wise modeling approach by focusing separately on the recent and further past user activities. We showcase in the later sections how this approach can achieve not only lower deployment latency and cost but also better predictive accuracy. BBE-LSWCM has 3 major components. The (1) Long Window Model (LWM) estimates the conditional probability of the target given the user's clickstream activities from the long historical window, i.e., 𝑃 ( 𝑦 ( 𝑇 )| C 𝑙 ( 𝑇 )) . The output of LWM is denoted by 𝑝 𝑙 ( 𝑇 ) . We make the inference of LWM as a (daily) batch job and make the prediction of LWM available as an input to the real-time inference. By doing this, we reduce the latency and cost of real-time inference significantly while not losing the user behavior context coming from the long historical window. The (2) Short Window Model (SWM) estimates the conditional distribution of the target 𝑃 ( 𝑦 ( 𝑇 )| C 𝑠 ( 𝑇 )) given the user's recent in-session clickstream. The output of the SWM is denoted by 𝑝 𝑠 ( 𝑇 ) . This model is evaluated in real-time by allowing the prediction to be sensitive to the user's actions in the recent past (in-session). Finally, (3) Ensemble Meta Model (EMM) takes 𝑝 𝑠 ( 𝑇 ) , 𝑝 𝑙 ( 𝑇 ) and the user profile information 𝑧 ( 𝑇 ) as inputs to finally estimate the conditional distribution of the target 𝑃 ( 𝑦 ( 𝑇 )| C ( 𝑇 ; 𝑤 ) , 𝑧 ( 𝑇 )) . The output of EMM is denoted by 𝑝 𝑒 ( 𝑇 ) and appropriate thresholding is applied on 𝑝 𝑒 ( 𝑇 ) to finally get the prediction ˆ 𝑦 ( 𝑇 ) . Like SWM, the EMM is also evaluated at the time of real-time inference. In the next sections, we showcase the advantages of this approach: (a) because of the proposed blockbootstrapped ensemble and profile-likelihood-based estimation, BBE-LSWCM has better performance both in terms of prediction accuracy and early warning generation, and (b) the latency and implementation cost of BBE-LSWCM is significantly lower as compared to a joint model which uses a long window of historical clickstream (more than few weeks) to predict the target at the time of real-time inference.",
  "3.3 Model Components": "LWM: The length of the LWC, C 𝑙 ( 𝑇 ) = ⟨( 𝑡 𝑚 , 𝑝 𝑚 , 𝑒 𝑚 )⟩ 𝑀 ( 𝑇 𝑙 ; 𝑤 𝑙 ) 𝑚 = 1 , i.e. 𝑀 ( 𝑇 𝑙 ; 𝑤 𝑙 ) (for simplicity denoted as 𝑀 𝑙 ( 𝑇 ) ), can be as large as ∼ 10 5 . To process such a large volume of varying length clickstream we propose an automated feature engineering block ( Auto FE in Fig. 1) in BBE-LSWCM. For inference at given ref-ts 𝑇 , we define CODS-COMAD 2024, January 4-7, 2024, Bangalore, India A. Chakraborty, V. Raturi and S. Harsola the input feature vector of LWM, 𝑥 𝑙 ( 𝑇 ) , as:   where 𝜏 ( 𝑇 ) is the duration the user is using the product for at 𝑇 , [ ] denotes the concatenation of the vectors, |·| denotes the size of a set, and, 𝐶 ( 𝐸 𝑘 ) , 𝐶 𝐸 is similarly defined as of 𝐶 ( 𝑃 𝑗 ) , 𝐶 𝑃 . The above featurization creates an additive-count-vectorized feature vector 𝑥 𝑙 ( 𝑇 ) of size (| P | + | E | + 2 ) to measure the user activities over the long historical window. Note that, this featurization can be even further extended to include more properties of clickstream beyond page and event, e.g. type of the action (scroll, swipe, click), etc. We model the conditional distribution of the target 𝑦 ( 𝑇 ) nonparametrically  GLYPH<16> GLYPH<17> with tree-based weak learners 𝑓 𝑎 · ; 𝜃 𝑙 𝑎 with unknown parameters Θ 𝑙 = GLYPH<16> 𝜃 𝑙 𝑎 GLYPH<17> 𝐴 𝑎 = 1 . The details of the estimation are mentioned in Sec. 3.4. The output of LWM is denoted by 𝑝 𝑙 ( 𝑇 ) = 𝜙 𝑙 GLYPH<16> 𝑥 𝑙 ( 𝑇 ) ; ˆ Θ 𝑙 GLYPH<17> , where ˆ Θ 𝑙 is the estimated parameter values. SWM: In SWM, we separate out the input clickstream C 𝑠 ( 𝑇 ) of length 𝑀 ( 𝑇 ; 𝑤 𝑠 ) ( = 𝑀 𝑠 ( 𝑇 ) ) to three sequences of timestamps, pages, and events. We convert the page and event sequences into a sequence of tokens coming from vocabularies P , E respectively. The input to SWM is 𝑥 𝑠 ( 𝑇 ) = [⟨ 𝑜 ( 𝑝 𝑚 )⟩ 𝑚 , ⟨ 𝑜 ( 𝑒 𝑚 )⟩ 𝑚 , ⟨ 𝑡 𝑚 ⟩ 𝑚 ] with 𝑜 ( 𝑝 ) is the one-hot-encoded representation of 𝑝 ∈ P and same for the events. For large online subscription products like QBO, | P | can be more than 10 2 and | E | can be as large as 10 4 . Hence, to reduce the dimension, we convert the one-hot-encoded representation to a fixed-length embedding vector as 𝑟 𝑝 ( 𝑇 ) = ⟨ 𝑜 ( 𝑝 𝑚 )⟩ 𝑚 · 𝑅 𝑝 where 𝑅 𝑝 is the 𝑀 𝑠 ( 𝑇 ) × 𝐵 embedding matrix such that 𝐵 << | P | . Similarly, we have an embedding matrix 𝑅 𝑒 for the events. To learn the sequential context information from page-event sequences, instead of random initialization, we propose to initialize 𝑅 𝑒 and 𝑅 𝑝 by training an unsupervised Skip-Gram model and then allow the elements of these matrices to be updated through back-propagation while training for downstream tasks. To model the intrinsic sequential nature of the in-session clicks, we use parallel Bidirectional LSTM (BiLSTM) Layers for pages and events as follows,  where 𝑟 ( 𝑝 𝑚 ) is the embedding representation of the 𝑚 -th visited page and ⊕ denotes the element-wise addition. A similar computation is done in the parallel events network (to get ℎ 𝑚 𝑒 ). As user clicks are often not uni-directional in nature, BiLSTM helps to extract additional context information from the user clicks. We stack multiple BiLSTM layers to extract important sequential information from page and event sequences and, then to summarize the most important features over time, Global Max Pooling (GMP) and Global Average Pooling (GAP) are performed on the final sequential hidden layers h 𝑝 = [ ℎ 1 𝑝 , . . . , ℎ 𝑀 𝑝 ] and h 𝑒 = [ ℎ 1 𝑒 , . . . , ℎ 𝑀 𝑒 ] . For the timestamp sequence ⟨ 𝑡 𝑚 ⟩ 𝑚 , we pass it through a feed-forward-neural network (FNN) and concatenate the output of FNN ( ˜ 𝑡 ) with the global pool layers of page and event networks as shown below.   Finally, as shown above, a two-layer FNN in conjunction with a softmax output layer is used to get the output of SWM, i.e., 𝑝 ( 𝑠 ) ( 𝑇 ) = softmax ( 𝑊 3 ℎ 2 + 𝑏 3 ) (see Eq. (4)). Let us summarize the SWM network in Eq. (3)-(4) as 𝑝 𝑠 𝑖 ( 𝑇 ) = 𝜙 𝑠 GLYPH<16> 𝑥 𝑠 𝑖 ( 𝑇 ) ; Θ 𝑠 GLYPH<17> , with Θ 𝑠 denoting all the parameters in SWM network. EMM: For EMM, we construct the input feature vector 𝑥 𝑒 ( 𝑇 ) as,  where, 𝑧 ( 𝑇 ) is the dynamic user-profile information and 𝑝 𝑙 ( 𝑇 ) , 𝑝 𝑠 ( 𝑇 ) are the output vectors from LWM and SWM respectively with size 𝐶 -1 where 𝐶 is the number of possible classes for the target 𝑦 ( 𝑇 ) . Finally, the conditional distribution of the target given the entire history of clickstream data, starting from a long window (few weeks in history) to the in-session clicks, and the user profile information is modeled as,  In BBE-LSWCM we allow the 𝜙 ( 𝑒 ) (·) to be modeled as a regularized logistic regression or tree-based ensemble model depending on the prediction task.",
  "3.4 Parameter Estimation": "The joint formulation of BBE-LSWCM can be written as (Eq. (6)),   To estimate all the model parameters Θ = [ Θ 𝑙 , Θ 𝑠 , Θ 𝑒 ] we have historically observed training data  with 𝑁 being the number of unique users and T 𝑖 is the set of 'eligible' reference time-points in the training data for the 𝑖 th user. Note that, churn and task abandonment type customer events are usually rare events making the training data imbalanced in nature. Moreover, for problems like real-time churn detection, only the last few observations for churned users are from the positive class amplifying the class imbalance even further. Also, including one training record corresponding to every eligible ref-ts T 𝑖 will result in a data explosion. To handle these challenges, we propose a BlockBootstrap-based sample data selection and training of different components of BBE-LSWCM on independent bootstrap samples. BBE-LSWCM: A Bootstrapped Ensemble of Long and Short Window Clickstream Models CODS-COMAD 2024, January 4-7, 2024, Bangalore, India 1. LWMParameter Estimation: I. Use simple random sample without replacement to obtain a sample set of users N 𝑙 = GLYPH<8> 𝑖 1 , . . . , 𝑖 𝑁 𝑙 GLYPH<9> ⊂ { 1 , . . . , 𝑁 } = N . For each user 𝑖 ∈ N 𝑙 , use BBS to select a random sample of reference time points T 𝑙 𝑖 ⊂ T 𝑖 . II. Use LightGBM [22] to estimate the LWM parameters by minimizing the cross-entropy loss over the bootstrap sample for LWM training D 𝑙 = nGLYPH<16> 𝑦 𝑖 ( 𝑇 ) , 𝑥 𝑙 𝑖 ( 𝑇 ) GLYPH<17> : 𝑇 ∈ T 𝑙 𝑖 , 𝑖 ∈ N 𝑙 o  where ˝ D 𝑙 denotes the summation over the observations in D 𝑙 . 2. SWMParameter Estimation: I. Repeat step 1.I to generate another independent bootstrap training sample for SWM D 𝑠 = nGLYPH<16> 𝑦 𝑖 ( 𝑇 ) , 𝑥 𝑠 𝑖 ( 𝑇 ) GLYPH<17> : 𝑇 ∈ T 𝑠 𝑖 , 𝑖 ∈ N 𝑠 o . II. Use ADAM-based gradient descent algorithm to estimate the SWM network parameters by minimizing the crossentropy loss over ( D 𝑠 ) similar to Eq. (8). 3. EMM Parameter Estimation: I. Repeat step 1.I to generate a bootstrap training sample D 𝑒 = GLYPH<8> ( 𝑦 𝑖 ( 𝑇 ) , C 𝑖 ( 𝑇 ) , 𝑧 𝑖 ( 𝑇 )) : 𝑇 ∈ T 𝑒 𝑖 , 𝑖 ∈ N 𝑒 GLYPH<9> . II. Compute the EMM input features 𝑥 𝑒 𝑖 ( 𝑇 ) using Eq. (5) where 𝑝 𝑙 𝑖 ( 𝑇 ) = 𝜙 𝑙 GLYPH<16> 𝑥 𝑙 𝑖 ( 𝑇 ) , ˆ Θ 𝑙 GLYPH<17> , 𝑝 𝑠 𝑖 ( 𝑇 ) = 𝜙 𝑠 GLYPH<16> 𝑥 𝑠 𝑖 ( 𝑇 ) , ˆ Θ 𝑠 GLYPH<17> . Estimate the EMM parameter Θ 𝑒 by minimizing the crossentropy loss over the EMM training data D 𝑒 .",
  "algorithm 1: Estimation of BBE-LSWCM Parameters": "Block Bootstrap Sampling (BBS): In BBE-LSWCM parameter estimation, we use BBS to generate a bootstrap sub-sample of reference time points denoted by T 𝑏 ⊂ T = { 𝑇 1 < ... < 𝑇 𝑛 } for each user. To do so, first the reference time points in T are grouped into blocks as ℬ 𝑘 ( 𝐿 ) = GLYPH<8> 𝑇 𝑗 : 𝑗 = ( 𝑘 -1 ) 𝐿 + 𝑙, 1 ≤ 𝑙 ≤ 𝐿 GLYPH<9> with 𝐿 > 0 being the block length (or, block size). Next, let us consider the collection of such non-overlapping blocks B = { ℬ 𝑘 ( 𝐿 ) : 1 ≤ 𝑘 ≤ ⌊ 𝑛 / 𝐿 ⌋} . We randomly sample 𝐾 < ⌊ 𝑛 / 𝐿 ⌋ -many blocks from B without replacement and collate the ref-ts's in the sampled blocks to generate T 𝑏 . This block-wise bootstrap technique is used in time series and other dependant data applications to simulate bootstrapped samples for estimation of higher order statistic [23, 24]. By allowing consecutive reference time points to be considered in the sampled training date, BBS brings certain robustification which is demonstrated in the additional experimental results. For problems with high class imbalance, we only down-sample from the majority class through BBS to increase the minority-to-majority class ratio in training data. BBE-LSWCM parameter estimation: For the estimation of BBE-LSWCM parameters Θ , instead of joint estimation using the full training data D , we propose step-wise estimation of LWM, SWM, and EMM parameters as described in Algorithm 1. By taking independent bootstrapped samples from D for separately estimating LWM, SWM, and EMM parameters, we guard against the overfitting problem in stacked-ensemble approaches. Moreover, the estimation procedure in Algorithm 1 uses profile-likelihoodbased estimation [31, 37] of BBE-LSWCM parameters. To estimate the EMM parameters Θ 𝑒 in Eq. (7), the LWM and SWM parameters ( Θ 𝑙 , Θ 𝑠 ) behave like 'nuisance' parameters and hence, these are first estimated separately from bootstrapped samples and then Θ 𝑒 is estimated by profiling out Θ 𝑙 and Θ 𝑠 . Asymptotically the estimated parameters through profile-likelihood methods converge to the true values [31].",
  "4 EXPERIMENTATION RESULTS": "We have chosen two important problems related to QBO to demonstrate the merits of the BBE-LSWCM for real-time customer event predictions using clickstream. QBO is used by millions of small businesses to manage their financial accounts and hence, our experimental data set is highly dynamic in nature and includes a large variety of clickstream behavior and user personas. In the first problem, the prediction task is to infer whether a user is at high risk of canceling their subscription within the next 48 hours (look-ahead window chosen based on CRM team input). This is a binary classification problem and we will refer to this problem as real-time churn prediction . The other problem referred to as intended task detection , requires predicting, in real-time, the task the user is concerned about as they engage with the help channels, e.g., banking, payroll, tax, etc. This is a multi-class classification problem.",
  "4.1 Test Data": "Real-time churn prediction: In this problem we focus on QBO subscribers who are First Time Users (FTU) i.e., the customers who are in their first billing cycle and hence, are more inclined to cancel the subscription because of difficulty and ambiguity in using the product at its full potential and lack of support [45]. Based on marketing research, the efficacy of static offline retention campaigns such as email, message, or phone is lower for these customers [35, 42]. Hence, an in-session intervention powered by a real-time churn prediction system is necessary to reduce the churn rate for FTU. For the results reported in this paper, we have curated a sample dataset of 50,000 FTU subscribers over a period of 8 months. Sample data was curated to keep the proportion of churned customers similar to other studies in literature [6, 12, 30], at 8%. We take the first 6 months as 'train period' for training and the last two months are used as test duration for reporting the evaluation metrics. To replicate the real-time deployment scenario (see Sec 4.2), we evaluate the model multiple times within a day for each user when they are online in the product. Thus, the number of combinations of user and ref-ts in the training is around 2 million, and for the test is around 700 thousand. Task-intent detection: For this problem, the target is a multiclass random variable with 20 possible classes, e.g., [banking, chart, estimates,...]. The first timestamp where the user has interacted with any of the help channels (digital assistant, search, or call to customer care) is considered as the ref-ts ( 𝑇 ), and the corresponding task-intent recorded by the channels post interaction is used to train and evaluate the model. The data considered for training has around 500,000 records of user and ref-ts combinations with their intended CODS-COMAD 2024, January 4-7, 2024, Bangalore, India A. Chakraborty, V. Raturi and S. Harsola tasks and the same for testing is approximately 200,000. The class distribution is highly skewed with the top three most frequent categories contributing more than 40% of all the records. Clickstream and user-profile data: We consider all the timestamped clicks (timestamp - page - event) of the users for the abovementioned time frame of 8 months as input to the model. Based on the dataset considered here, on average in a day the number of clicks per active user can vary between 50 to 5000, and, the entire clickstream data considered in this paper contains over 1 billion clicks with around 350 unique pages and 2500 unique events, making this data-set very diverse. In addition to the clickstream data, we also consider several dynamic user-profile information such as the duration of the user in the product from the start of the subscription in days, the type of the subscription, type of onboarding, type of industry, etc. as input to the models.",
  "4.2 Evaluation Criterion": "In the previous studies on churn detection, mostly the evaluation is done based on a single inference for each user and using usual binary classification metrics such as AUROC, accuracy, and lift scores at certain deciles. In some of the recent research in the eCommerce domain to predict 'customer exits without purchase' using clickstream analytics [16, 33], to prove the efficacy of the models for early warning generations, the model is inferred at pages which are few steps earlier to the user exit, but still a single inference is used to measure the accuracy of the model. Moreover, this approach of evaluation is not reflective of the real-time deployment setup, since it requires the model trigger mechanism to know about the number of steps to user exit beforehand, which is not possible. In this paper, to make the evaluation set-up representative of the production use case, we generate the model inference for each eligible user (FTU and not churned yet) multiple times within a day when they were online in the product. Let us denote all the predictions (churn risk or propensity) for 𝑖 th user during the test period as GLYPH<8> 𝑝 𝑖 ( 𝑇 𝑗 ) : 𝑇 𝑗 ∈ T 𝑖 GLYPH<9> . To make our results comparable to previous studies related to customer churn and, to meet marketing requirements of intervening a certain percentage of users based on predicted churn risk, we propose aggregating the predictions as ˆ 𝑦 𝑖 = max 𝑇 𝑗 ∈ T 𝑖 I GLYPH<8> 𝑝 𝑖 ( 𝑇 𝑗 ) > 𝜆 𝑑 GLYPH<9> . Here, 𝜆 𝑑 is a threshold chosen from the validation data in such a way that ˝ 𝑖 ˆ 𝑦 𝑖 is approximately equal to ( 10 × 𝑑 ) % ( 𝑑 -th decile) of the total number of unique users. Note that, ˆ 𝑦 𝑖 = 1 implies that the model has at least once identified the 𝑖 th user as at high risk of churn during the considered period. Finally, we compare the aggregated predictions ˆ 𝑦 𝑖 with user-level churner vs non-churner, i.e. 𝑦 𝑖 = max 𝑇 𝑦 𝑖 ( 𝑇 ) , to compute the following two evaluation metrics.  with ˜ 𝑇 𝑖 = max { T 𝑖 } is the time the 𝑖 th user has churned (if churner) and ˆ 𝑇 𝑖 denotes the first time the model has predicted high risk of churn of this user, i.e., ˆ 𝑇 𝑖 = min GLYPH<8> 𝑇 𝑗 : 𝑝 𝑖 ( 𝑇 𝑗 ) > 𝜆 𝑑 , 𝑇 𝑗 ∈ T 𝑖 GLYPH<9> . DL 𝑑 is the decile lift at decile 𝑑 and ATWC 𝑑 is the average time from warning to churn. From the definitions in Eq. (9), for a given decile 𝑑 , DL 𝑑 quantifies the lift in the churn identification as compared to a non-predictive random model and ATWC 𝑑 measures the average time from the first detection to churn for the true-positive cases. Additionally, we also evaluate the models w.r.t. the AUROC score at a user level to evaluate the classification ability of the models. For the intended task detection, we compare the models in terms of one-vs-rest AUROC score (ovr-AUROC) [40], as this is a multiclass classification problem. As the dataset is highly imbalanced in terms of class occurrences and there is no proper business insight on which task to give more importance to, we have used macroaveraging of the class-specific OvR-AUROC scores.",
  "4.3 Baseline Approaches": "Baselines: For the real-time churn detection problem, we have compared BBE-LSWCM with a set of baselines which can be categorized into the following four buckets. (1) Batch models: The most commonly used churn detection frameworks use batch predictions, where the propensity of churn is generated for eligible customers on a weekly or monthly basis and the marketers use those for driving static retention campaigns. This model uses domain information to hand-code the features before passing them to the model. Inspired by earlier studies [4, 45], we have considered Logistic Regression (LR), Gradient Boosting Trees (GBDT), and Random Forest (RF). In addition to this, we have also considered the PLSTM architecture [45] using activity embedding (PLSTM) where we provide the daily activity sequence over the last few days as a multivariate time series input to the model. Similar to previous works [12, 41, 45], we have used a fixed length (size 30) hand-coded feature vector from domain knowledge which includes user profile information such as type of subscription, onboarding channels, age of the subscription etc. and user behavior features such as number of clicks, number of page visits, time spent etc. in some of the most important product features in QBO. (2) In-session models: This group of models takes the in-session clickstream and user profile information as input to predict the target. Within this group, we have considered a Gradient Boosted Tree model with count-vectorized clicks and user-profile features (GBDT); a multi-layer LSTM model with one-hot-encoded pages as input sequence combined with userprofile features at the flattened layer (LSTM); multi-layer parallel LSTMs with an embedding layer for page sequences and timestamp sequences (emb-LSTM). (3) Survival models: Survival models are also used for some of the user exit detection problems by modeling the time to exit [29, 44]. We have included a batch model with Random Survival Forest (batch-RSF) [19] and an in-session model DeepSurv (in-ses-DeepSurv) [21] in our set of baselines. (4) Joint models: Like BBE-lSWCM, this set of modeling architectures supports both historical user behavior features and in-session clickstream as inputs along with user profile information. We have considered xDeepFM [27] and DIN [48] from CTR prediction literature where the sequence of page visits is passed as the variable length user-behavior feature and user-profile data is passed as fixed length feature vectors. We also consider CDMM [33] from the user exit prediction paradigm using clickstream data. As input, this model takes the sequence of pages visited, and the delta sequence of timestamps from the current session passed as dynamic features, and, user behavior data from historical time period and user profile information as static features. BBE-LSWCM: A Bootstrapped Ensemble of Long and Short Window Clickstream Models CODS-COMAD 2024, January 4-7, 2024, Bangalore, India Ablation study: We have conducted an ablation study for both real-time churn and intended task detection. For ablation we have considered the following models: (a) LWM: only the long window model with automated clickstream features computed over the last few weeks; (b) SWM: only the short window model with clickstream in the last one hour from the ref-ts; (c) LSWCM: a joint neural network model with recent clicks over the short one hour window passing through the network defined in Eq. (3)-(4) and, the LWM features being concatenated at the flattened layer and then passed through an FNN to get final prediction. We also compare the latency of these models to evaluate real-time performance.",
  "4.4 Results": "All the experiments have been conducted on one ml.m5.12xlarge AWS Sagemaker instance with 32 cores and the CPU is used for both training and inferences for the neural network models. Comparison with baselines in churn identification: With 3.197 lift at the top decile (DL1), BBE-LSWCM is 60% better than the batch baseline models (Table 1), 35% better than any of the in-session models, 50% better than survival models and finally, 30% better than the next best performing joint model DIN [48]. A larger gap between BBE-LSWCM and other models can be observed at the second decile lift (DL2), which justifies the robustness of BBELSWCM as compared to the baselines considered here. Moreover, BBE-LSWCM can early detect the churners with an ATWC1 of 71 hours which is 8% higher compared to the next best joint model and at least 40% higher than any of the batch or in-session models. In terms of AUROC, BBE-LSWCM has outperformed all the baselines. Table 1: Performance in real-time churn detection w.r.t. baselines. Results from ablation study: Table 2 showcases the importance of each BBE-LSWCM component through ablation study. In terms of all three metrics (DL1, DL2, and AUROC), the LWM (LightGBMwith Count-Vec clickstream features) and SWM (BiLSTM with embedded clickstream) have better performance as compared to any of the batch and in-session models reported in Table 1. This justifies the modeling choices for the BBE-LSWCM framework. For the real-time churn detection problem, LWM has a higher AUROC of 0.69 as compared to 0.591 of SWM, indicating the importance of user behavior information beyond the current session. Table 2: Ablation study for churn and intended-task detection. Though SWM is marginally better than LWM in DL1, the lift score of SWM drops sharply at the 2nd decile, proving the limitations of the in-session-only methods for real-time churn detection. By combining LWM and SWM, BBE-LSWCM retains the good properties of both models and achieves a 17% relative improvement in AUROC. Recall that the LSWCM model is a single joint neural network trained on both short-window and long-window features. Though this model has the best performance among all the considered baselines (20% lower DL1 as of BBE-LSWCM), the average inference latency 1 of this model increases by 102% from the in-session SWM model. As BBE-LSWCM only needs to process in-session clickstream data and LWM output to generate the prediction, the latency of BBE-LSWCM has increased by only 18% as compared to SWM, while it has achieved 25% relative improvement in AUROC. For the intended-task detection, unlike the churn problem, SWM (ovr-AUROC 0.701) has better predictive accuracy as compared to LWM(ovr-AUROC 0.683), as in-session user clickstream has more context regarding the task the user is interested in. In this case, BBE-LSWCM has achieved a 12% better AUROC while maintaining a low real-time latency (only a 33% increase relative to SWM as opposed to 112% for the joint model LSWCM). Hyper-parameter analysis: In this section, we study the impact of different hyper-parameter settings of BBE-LSWCM for the intended task detection problem. We consider the following set of hyper-parameters: (1) LWM: n_estimator - number of estimators (or number of trees, or maximum number of iterations), varying within [50, 100, 500, 1000], max_bin - maximum number of bins to group non-missing features, taking values in [20, 50, 100, 255]; (2) SWM: number of units in BiLSTM layer and dropout proportion varying within [0.05, 0.1, 0.25, 0.5] and [8, 16, 32, 64] respectively; (3) BBS: number of blocks in [1, 3, 6] and block size from [2, 4, 10]. Figure 2: ovr-AUROC of BBE-LSWCM and LWM for varying LWM hyper-parameters As we can see in Fig. 2, the ovr-AUROC of LWM only model increases as the number of estimators increases till 500 and then it drops at 1000, whereas BBE-LSWCM is robust against the drop in 1 Latency has been reported as a relative increase from the in-session SWM model. Note that, latency computation excludes the time required to transform the raw clickstream to LWM and SWM input vectors, where also BBE-LSWCM has an advantage as LWM featurization is not required for real-time inference. CODS-COMAD 2024, January 4-7, 2024, Bangalore, India A. Chakraborty, V. Raturi and S. Harsola performance of a single component (LWM in this case). In terms of the max_bins, the performance of both LWM and BBE-LSWCM increases as we allow better distributional approximation through a higher number of bins to group the feature values. Figure 3: ovr-AUROC of BBE-LSWCM and SWM for varying SWM hyper-parameters 0.83 BBE-LSWCM 0.80 0.75 0.70 0.65 055 n_units 0.80 0.75 0.70 0.65 0.60 0.55 0.05 0.25 dropout For varying SWM parameters (see Fig. 3), the best accuracy has been achieved with number of units in BiLSTM as 16 and dropout proportion at 0.1. Note that, as SWM has more contribution to BBE-LSWCM for intended-task detection problem, the SWM hyperparameters have a higher impact on BBE-LSWCM as compared to the LWM ones. Table 3: Accuracy and training time comparison of BBE-LSWCM w.r.t. LSWCM for varying BBS hyper-parameters As the BBS hyper-parameters decide the volume of the training data, we also report the impact of these hyper-parameters in the training time and accuracy as compared to fitting the LSWCM (the joint model) using a random sample from the whole data in Table 3. As the number of blocks (n_blocks) and size of the blocks (block_size) increases, the training time increases. When the number of blocks is 3 and block size is 4, i.e., per user around a maximum of 12 ref-ts are chosen through BBS, BBE-LSWCM has similar training time as of LSWCM (as the sample size is similar) while 7% better ovr-AUROC. All the other hyper-parameters have been selected through 3-fold cross-validation on the training data.",
  "5 IN-SESSION PROACTIVE INTERVENTIONS": "BBE-LSWCM has been used to build a real-time prediction model for early detection of subscription cancellation in QBO. We have set up the data processing, model training and model inference as an end-to-end (e2e) automated pipeline (see Fig. 4) with following components: (1) batch featurization using daily PySpark jobs scheduled on Kubernetes; (2) Real-time featurization using Spark's Structured Streaming with executors running on Kubernetes; (3) AWS DynamoDB based low latency feature store; (4) AWS SageMaker jobs for LWM, SWM and EMM model training; (5) LWM parallel batch inference on Kubernetes pods; (6) AWS sagemaker Figure 4: BBE-LSWCM production deployment architecture. Spark Structured Streaming Sagemaker Inferece Redlime SVM Fealurizabon Ensemble In'crcace ClickStream Events Featune Store AWS DynamoDB Realime Batch LWM Batch Inlerence Batch Event Store Dalalake / 53 PySpark Jobs AWS Sagemeker Mocel Tra ning ML Framework using Kubeflow AutoScaling inference for real-time SWM and EMM inference; (7) Kubeflow for orchestrating the entire pipeline. This model has been integrated with the in-product intervention platform of QBO to run large-scale retention campaigns with realtime, contextual, and proactive interventions. An evaluation of the model (via an HTTP endpoint) at any given time for a customer provides the churn propensity of the user (High or Low with a probability score) based on the recent and long-window product usage in conjunction with user-profile information. When a model inference returns 'High' risk of churn, a proactive chat window pops up in the product with relevant help and the option to directly chat or call with a CRM agent. If the user accepts the proactive help, the intervention platform in that case evaluates the BBE-LSWCM model for intended task detection and passes the predicted intent of the user also to the CRM agent panel (Salesforce) to facilitate more contextual and personalized assistance. Online Results: We have conducted an A/B test on the QBO first-time users for a complete month to track the effect of BBELSWCM-based real-time proactive interventions. We have observed a ∼ 30% reduction in the FTU churn rate (p-value = 0.0057, power = 0.97) for the treatment group (intervention provided if detected as high risk) as compared to the control group (no intervention provided though detected as high risk). Moreover, the acceptance rate of the interventions at an aggregated user level is 4x as compared to other intervention campaigns which are powered by business rules as opposed to a machine learning system like BBE-LSWCM.",
  "6 CONCLUSION": "We have proposed an efficient (cost-effective and low-latency) and robust (higher predictive accuracy compared to the baselines) clickstream modeling framework for real-time event prediction problems. By using a bootstrapped ensemble of short-window and longwindow user behavior models, BBE-LSWCM has outperformed not only batch models but also other state-of-the-art in-session and survival models. We have also described how BBE-LSWCM has been used in a real industry setting for driving real-time proactive interventions for a large SaaS subscription commerce. One of the immediate future directions is to implement BBE-LSWCM for other real-time event prediction problems. In addition, though the local explainability of LWM can easily be generated through modules such as SHAP, LIME [36, 38], generating explainability based on the embedded in-session clicks through the SWM and the EMM layer is another area to be explored. BBE-LSWCM: A Bootstrapped Ensemble of Long and Short Window Clickstream Models CODS-COMAD 2024, January 4-7, 2024, Bangalore, India",
  "REFERENCES": "[1] David Adedayo Adeniyi, Zhaoqiang Wei, and Yang Yongquan. 2016. Automated web usage data mining and recommendation system using K-Nearest Neighbor (KNN) classification method. Applied Computing and Informatics (2016). [2] Abdelrahim Kasem Ahmad, Assef Jafar, and Kadan Aljoumaa. 2019. Customer churn prediction in telecom using machine learning in big data platform. Journal of Big Data (2019). [3] Shelby D Bernhard, Carson K Leung, Vanessa J Reimer, and Joshua Westlake. 2016. Clickstream prediction using sequential stream mining techniques with Markov chains. In IDEAS . [4] Paul Bertens, Anna Guitart, and África Periáñez. 2017. Games and big data: A scalable multi-dimensional churn prediction model. In 2017 IEEE conference on computational intelligence and games (CIG) . [5] Veronika Bogina and Tsvi Kuflik. 2017. Incorporating Dwell Time in SessionBased Recommendations with Recurrent Neural Networks.. In RecTemp@ RecSys . 57-59. [6] Jonathan Burez and Dirk Van den Poel. 2009. Handling class imbalance in customer churn prediction. Expert Systems with Applications (2009). [7] Bo Chen, Yichao Wang, Zhirong Liu, Ruiming Tang, Wei Guo, Hongkun Zheng, Weiwei Yao, Muyu Zhang, and Xiuqiang He. 2021. Enhancing explicit and implicit feature interactions via information sharing for parallel deep ctr models. In Proceedings of the 30th ACM international conference on information & knowledge management . 3757-3766. [8] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems . 7-10. [9] Alexandros Deligiannis and Charalampos Argyriou. 2020. Designing a Real-Time Data-Driven Customer Churn Risk Indicator for Subscription Commerce. IJIEEB (2020). [10] Joana Dias, Pedro Godinho, and Pedro Torres. 2020. Machine Learning for Customer Churn Prediction in Retail Banking. In ICCSA . [11] Amy Wenxuan Ding, Shibo Li, and Patrali Chatterjee. 2015. Learning user realtime intent for optimal dynamic web page transformation. Information Systems Research (2015). [12] Amineh Ghorbani, Fattaneh Taghiyareh, and Caro Lucas. 2009. The application of the locally linear model tree on customer churn prediction. In SoCPaR 2009 . [13] Yulong Gu, Zhuoye Ding, Shuaiqiang Wang, and Dawei Yin. 2020. Hierarchical user profiling for e-commerce recommender systems. In WSDM . [14] Şule Gündüz and M Tamer Özsu. 2003. A web page prediction model based on click-stream tree representation of user behavior. In KDD . 535-540. [15] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. arXiv preprint arXiv:1703.04247 (2017). [16] Tobias Hatt and Stefan Feuerriegel. 2020. Early detection of user exits from clickstream data: A Markov modulated marked point process model. In The Web Conference . [17] Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2015. Session-based recommendations with recurrent neural networks. arXiv preprint arXiv:1511.06939 (2015). [18] Yiqing Huang, Fangzhou Zhu, Mingxuan Yuan, Ke Deng, Yanhua Li, Bing Ni, Wenyuan Dai, Qiang Yang, and Jia Zeng. 2015. Telco churn prediction with big data. In SIGMOD . [19] Hemant Ishwaran, Udaya B Kogalur, Eugene H Blackstone, and Michael S Lauer. 2008. Random survival forests. (2008). [20] Porter Jenkins. 2019. Clickgraph: Web page embedding using clickstream data for multitask learning. In WWW . [21] Jared L Katzman, Uri Shaham, Alexander Cloninger, Jonathan Bates, Tingting Jiang, and Yuval Kluger. 2018. DeepSurv: personalized treatment recommender system using a Cox proportional hazards deep neural network. BMC medical research methodology 18, 1 (2018), 1-12. [22] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. 2017. Lightgbm: A highly efficient gradient boosting decision tree. NeurIPS (2017). [23] SN Lahiri. 2003. Resampling methods for dependent data . Springer Science & Business Media. [24] Soumendra N Lahiri. 1999. Theoretical comparisons of block bootstrap methods. Annals of Statistics (1999). [25] Jiayu Li, Hongyu Lu, Chenyang Wang, Weizhi Ma, Min Zhang, Xiangyu Zhao, Wei Qi, Yiqun Liu, and Shaoping Ma. 2021. A Difficulty-Aware Framework for Churn Prediction and Intervention in Games. In KDD 2021 . [26] Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Tao Lian, and Jun Ma. 2017. Neural attentive session-based recommendation. In CIKM . [27] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong Sun. 2018. xdeepfm: Combining explicit and implicit feature interactions for recommender systems. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1754-1763. [50] Yu Zhu, Hao Li, Yikang Liao, Beidou Wang, Ziyu Guan, Haifeng Liu, and Deng Cai. 2017. What to Do Next: Modeling User Behaviors by Time-LSTM.. In IJCAI .",
  "keywords_parsed": [
    "None"
  ]
}