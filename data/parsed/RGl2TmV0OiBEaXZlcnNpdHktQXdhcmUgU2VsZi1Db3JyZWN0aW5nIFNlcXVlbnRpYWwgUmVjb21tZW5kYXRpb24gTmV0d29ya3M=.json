{"DivNet: Diversity-Aware Self-Correcting Sequential Recommendation Networks": "Shuai Xiao Alibaba Group Shanghai, China shuai.xsh@alibaba-inc.com", "Abstract": "As the last stage of a typical recommendation system , collective recommendation aims to give the final touches to the recommended items and their layout so as to optimize overall objectives such as diversity and whole-page relevance. In practice, however, the interaction dynamics among the recommended items, their visual appearances and meta-data such as specifications are often too complex to be captured by experts' heuristics or simple models. To address this issue, we propose a diversity-aware self-correcting sequential recommendation networks ( DivNet ) that is able to estimate utility by capturing the complex interactions among sequential items and diversify recommendations simultaneously. Experiments on both offline and online settings demonstrate that DivNet can achieve better results compared to baselines with or without collective recommendations.", "CCS Concepts": "", "\u00b7 Information systems \u2192 Learning to rank .": "", "Keywords": "Sequential Recommendation, Diversity", "ACMReference Format:": "Shuai Xiao and Zaifan Jiang. 2024. DivNet: Diversity-Aware Self-Correcting Sequential Recommendation Networks. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management (CIKM '24), October 21-25, 2024, Boise, ID, USA. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/3627673.3680059", "1 Introduction": "Modern recommendation systems are complex, often consisting of multiple stages including matching (a.k.a. retrieval), coarse-to-finegrain tiered ranking, and collective recommendation systems. Given user queries (e.g., a search query or a user session), the matching module quickly identifies a moderate-sized set of relevant candidates from a vast amount (millions or billions) of items in the inventory. Tiered ranking modules then estimate item-wise scores, such as CTR and CVR, based on which items will be ranked w.r.t. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. https://doi.org/10.1145/3627673.3680059", "Zaifan Jiang": "Alibaba Group Beijing, China zaifan.jzf@alibaba-inc.com their relevancy to the target user. Before surfacing the recommendations to the user, a collective recommendation module is usually employed to optimize the value delivery. Without this module, the recommendation systems may suffer from severe problems, such as the discrepancy between training and inference, lack of diversity and so on. The discrepancy between training and inference means the model loss considers interactions between ranked items when training while the item is scored point-wisely or the context for this item has changed during inference. This discrepancy can lead to unreliable prediction performance. Given certain queries, similar items usually tend to have high scores, resulting in over-concentration of recommended items. Recently, collective recommendation module [11, 16, 17, 21] is introduced into recommendation systems to further optimize the item layout, which can boost the performances of recommendation system, such as improving user satisfactions and click conversion rates. Collective recommendations try to improve the whole-page attraction by modeling the item interaction dynamics. Collective recommendation poses unique technical challenges. (I) First, complex interactions exist between items in different positions. Similar items shown in top positions may decrease or increase the click probability of subsequent items (which we call a self-exciting process between items). These depend on subtle causalities that it's hard to define explicitly using rules beforehand. For instance, some users may prefer a diverse recommendation set while a concentrated one is preferred for other users. Besides, distinct combinations of items result in different user experiences. Items combinations with compatible colors or text descriptions can grasp users' attention which in turn improves the click-through rate. Items with a short distance have stronger interactions than those with a long one. (II) Secondly, sequence exposure bias also exists as only a small fraction of possible item combinations have been shown to users, which leads to the estimating of out-of-distribution item combinations unreliable. Recent literature [16] addresses this issue by introducing Bayesian prior distribution of users' preferences into the optimizing objective. Theresulting recommendation sets are close to prior distributions of users' preferences. However, it requires enough data accumulation to estimate an accurate prior distribution which is unsuitable for scenarios, like cold-start recommendations and short-term advertising or marketing activities. Nassif et al. [11, 17] try to balance the click-through rate and item diversity by designing a sub-modular function. Similarly, Wilhelm et al. [21] employ determinantal point processes to diversify YouTube recommendation by computing pairwise item similarity, which is inefficient to capture high-order item interactions and is computationally expensive. All the above methods assume the element-wise utility of items remain unchanged regardless of the contextual items. These methods are inflexible to model complex interactions within items, between items and users, items and positions. The PRM [13] employs a transformer structure to learn the global interactions from all the combinations of item pairs on the list. Seq2Slate [3] employs pointer-net [18] (a sequence-to-sequence network) to capture item interactions where a RNN encoder embeds all items and a RNN decoder iteratively select items using attention mechanism. The PRM and Seq2Slate is not diversity-aware and doesn't model diversity explicitly. To address the above concerns, we propose a self-correcting sequential recommendation networks ( DivNet ) which can estimate the item utility sequentially and diversify recommendation set simultaneously. The DivNet firstly projects items and user features with contextual items into a hidden space using self-attention networks. Then DivNet sequentially selects items by considering influences of previously-selected items and passes its influence to subsequent items. During this selecting process, the DivNet estimates the item utility and computes the similarity between candidates and existing items. The self-correcting module aims at maximizing total utility and improve diversity among selected items. To reduce the effect of exposure bias, we let the DivNet search in the nearby region of original exposed sequences by adding a supervised regularization. We conduct enormous ablation tests in industrial applications. Results of offline evaluation and deployment on commercial platforms demonstrate its superior performances compared to baselines with/without collective recommendations. What's more, the proposed method is practical as it can support large-scale queries in real-world recommendation systems.", "2 Related Works": "", "2.1 Recommendation System": "The architecture of industrial-level recommendation systems usually have matching, ranking (coarse and fine ranking) modules as shown in Figure 1. The difference between matching [6] and rank- ing is that matching mainly emphasizes on inference speed with proper precision while ranking lays stress on ranking precision. Traditional approaches of learning-to-rank estimate the utility of each item, either individually [2] or as a pair [15] or \ud835\udc58 -tuple [23], by learning a ranking function. Items are then ranked in a descending order according to the estimated utility, and the top\ud835\udc58 list is presented to the user. These methods solve the recommendation approximately, and they often work but only to the extent where (1) items can be considered independent such that the utility of the bundle as a whole are decomposable as the sum (or some linear function) of the individual utilities; and (2) the attraction of the whole page depends entirely on the content of these \ud835\udc58 items, regardless of their layout. In any realistic scenario, these two assumptions are regarded too restrictive and cannot reflect the truthful reality [7, 19, 20, 24]. For example, numerous user studies have found that there exist complex interaction patterns not only between items but also among the content of the items, their visual appearances and user's attention distribution over the screen [10].", "2.2 Collective Recommendation": "Collective recommendation is used to address drawbacks discussed above and output optimal item layout. the LambdaMART [4] incorporate the whole list of items and try to find an optimal list through the listwise ranking loss. Nassif et al. [11, 17] try to balance the click-through rate and item diversity by designing a sub-modular function. The sub-modular function guarantees that the function gain decreases as the number of selected items in the same category increases gradually The items can be selected greedily by maximizing the sub-modular objective. Similarly, Wilhelm et al. [21] employ determinantal point processes to diversify YouTube recommendation by computing pair-wise item similarity. This approach is inefficient in capturing high-order item interactions and is computationally expensive. All the above methods assume the utility of single items remains constant regardless of surrounding items. The estimation of item utility and collective ranking is divided into two stages. Actually, the utility changes when similar items have appeared previously. Our work estimates item utility considering the contextual items and diversify recommendations at the same time. The PRM [13] employs a transformer structure to learn the global interactions from all the combinations of item pairs on the list. Seq2Slate [3] employs pointer-net [18] (a sequence-to-sequence network)to capture item interactions where a RNN encoder embeds all items and a RNN decoder iteratively select items using attention mechanism. Seq2Slate is not diversity-aware and doesn't model diversity explicitly.", "2.3 Self-correcting Process": "Self-correcting process [1, 8, 12] is a statistical framework which describes the probability of events happening in the temporal and/or spatial fields. For recommendation system, the propensity that users are satisfied with recommended items constitutes the intensity in self-correcting process. The item interactions when shown sequentially in positions of the display page gives a well-defined spatial self-correcting process. The influence can be positive or negative, depending on item interactions and user preferences.", "3 Collective Recommendation via DivNet": "In this section, we'll give details of network structures of DivNet whose inputs are ranked items A = { \ud835\udc4e \ud835\udc56 } \ud835\udc41 \ud835\udc56 = 1 from upstream ranking modules and sequentially outputs item layout \ud835\udf0b A , a permutation of A and the motivations behind these network structures.", "3.1 Model Architecture": "The DivNet follows a sequence-to-sequence paradigm as shown in Figure 2. The ranked items from upstream are embedded in a hidden low-dimensional space with information from all other items collected in a message-passing way. Therefore, individual item embedding contains information about the accompanying items with whom they cooperate or compete. Input Transformation In order to let the model be aware of the spatial information of page layout, the positions can be encoded in a deterministic way by the following equations where Z \ud835\udc56 \ud835\udc57 is the \ud835\udc57 -th value of position \ud835\udc56 embedding. Z \u2208 R | \ud835\udc34 | \u00d7 \ud835\udc3f is the position embedding matrix where | A | is the total number of positions and \ud835\udc3f is the dimension of position embedding. Besides spatial embedding, the item features of set A are represented as feature matrix I \u2208 R | A | \u00d7 \ud835\udc40 where \ud835\udc40 is the dimension of item embedding. The \ud835\udc56 -th row of I contains \ud835\udc56 -th item's continuous features and trainable embedding of discrete features. To encode query (user context in recommendation) for personalized recommendation, features of user context denoted by U \u2208 R 1 \u00d7 \ud835\udc3e , are concatenated to item features. Then the concatenation is added with spatial embedding. Therefore, the features of the input can be expressed as: where [\u00b7] is the concatenation operator along the 2ed dimension with broadcasting which means that query embedding U is repeated | A | times along the 1st dimension. X \u2208 R | A | \u00d7 ( \ud835\udc40 + \ud835\udc3e ) is an intermediate variable where each row corresponds to the embedding of one item in the item sequence A . After initial feature concatenation and addition of spatial feature, the items A are projected into a hidden feature space with awareness of contextual items. The calculation of projection follows a selfattention mechanism governed by the following equation: where Qe , Ke , Ve are query, key, and value matrix after affine transformation by separately multiplying X with trainable parameters W Q e \u2208 R \ud835\udc42 \u00d7 \ud835\udc37 \ud835\udc3e , W K e \u2208 R \ud835\udc42 \u00d7 \ud835\udc37 \ud835\udc3e , W V e \u2208 R \ud835\udc42 \u00d7 \ud835\udc37 \ud835\udc49 . where W O e is the linear projection parameter. He \u2208 R | A | \u00d7 \ud835\udc37 \ud835\udc3e is the encoding embedding matrix of input items A = { \ud835\udc4e \ud835\udc56 } \ud835\udc41 \ud835\udc56 = 1 . Slate Proposer After the transformation and projection of the initial item list and context features, DivNet will propose slate output by sequentially selecting items and arranging them in an optimal way which maximizes the utility and maintain diversity. The DivNet achieves this goal by considering influences of previouslyselected items on current selection and scoring of items as selfcorrecting process and enforce diversity during the learning and inference process. Assuming selected item list GLYPH<8> \ud835\udc3c \ud835\udf0b 1 , \ud835\udc3c \ud835\udf0b 2 , . . . , \ud835\udc3c \ud835\udf0b \ud835\udc61 -1 } up to time \ud835\udc61 , then the logit of the \ud835\udc61 th item candidate chosen from set \ud835\udc45 \ud835\udc61 : { \ud835\udc3c \ud835\udf0b \ud835\udc61 \u2208 A : { \ud835\udc4e \ud835\udc56 } \ud835\udc41 \ud835\udc56 = 1 , \ud835\udc3c \ud835\udf0b \ud835\udc61 \u2209 { \ud835\udc3c \ud835\udf0b 1 , \ud835\udc3c \ud835\udf0b 2 , . . . , \ud835\udc3c \ud835\udf0b \ud835\udc61 -1 } GLYPH<9> can be computed as follows when considering the influence of selected items: where U is upper triangular matrix which has all the elements below the main diagonal as zero and other elements as one. This ensure the causal influence where only current item depend on previously-selected items. \ud835\udf0e is sigmoid activation function. \ud835\udc66 \ud835\udf0b \ud835\udc61 is the estimated utility logit for item \ud835\udc3c \ud835\udf0b \ud835\udc61 . To induce diversity in the recommendation list, self-correcting process is introduced in the following way. First, item similarity between item \ud835\udc56 and \ud835\udc57 is defined as: where \u00af Hi , \u00af Hj is the embedding of item \ud835\udc56 and \ud835\udc57 in Equation 8 and cosine is the cosine distance between two vectors. Then the relevance of item \ud835\udc3c \ud835\udf0b \ud835\udc61 to previously-selected items can be calculated using the following formula: \ud835\udc51\ud835\udc52\ud835\udc61 ( \ud835\udc3c \ud835\udf0b \ud835\udc61 ) is the determinant of matrice of L where element \ud835\udc3f \ud835\udc56 \ud835\udc57 = \ud835\udc50\ud835\udc5c\ud835\udc60\ud835\udc56\ud835\udc5b\ud835\udc52 ( \u00af Hi , \u00af Hj ) is a scaled measurement of the similarity between items i and j. \ud835\udc51\ud835\udc52\ud835\udc61 ( \ud835\udc3c \ud835\udf0b \ud835\udc61 ) increases when the item \ud835\udc3c \ud835\udf0b \ud835\udc61 is less similar to previous items. As the relevance of current item selection to previous items can be measured. The probability of selecting item \ud835\udc3c \ud835\udf0b \ud835\udc61 at step \ud835\udc61 can be computed: The bigger utility of item \ud835\udc3c \ud835\udf0b \ud835\udc61 and less similar to previous items, the higher probability the item will be selected. \ud835\udefc is a parameter, governing the degree of diversity. For larger values, the DivNet tends to generate more diverse item list.", "3.2 Learning Algorithms": "To estimate the parameters of DivNet, one viable optimization is the widely-used reinforce algorithm [22]. The objective is to maximize R( \ud835\udf0b,\ud835\udc66 ) where \ud835\udf0b is sampled from our DivNet model and \ud835\udc66 comes Items Embeddings \ud835\udc6f\ud835\udfd4 \ud835\udc66\ud835\udc3c6 \ud835\udc6f\ud835\udfcf \ud835\udc6f\ud835\udfd0 \ud835\udc6f\ud835\udfd1 \ud835\udc6f\ud835\udfd2 \ud835\udc6f\ud835\udfd3 \ud835\udc6f\ud835\udfd4 \ud835\udc6f\ud835\udfd5 \ud835\udc6f\ud835\udfd5 \ud835\udc66\ud835\udc3c7 \ud835\udc6f\ud835\udfd0 \ud835\udc66\ud835\udc3c2 \ud835\udc6f\ud835\udfd2 \ud835\udc66\ud835\udc3c4 \ud835\udc6f? \ud835\udc66\ud835\udc3c? \ud835\udc70 \ud835\udfcf \ud835\udc7c\ud835\udfcf \ud835\udc70 \ud835\udfd0 \ud835\udc7c\ud835\udfcf \ud835\udc70 \ud835\udfd1 \ud835\udc7c\ud835\udfcf \ud835\udc70 \ud835\udfd2 \ud835\udc7c\ud835\udfcf \ud835\udc70 \ud835\udfd3 \ud835\udc7c\ud835\udfcf \ud835\udc70 \ud835\udfd4 \ud835\udc7c\ud835\udfcf \ud835\udc70 \ud835\udfd5 \ud835\udc7c\ud835\udfcf \ud835\udc4d1 \ud835\udc4d2 \ud835\udc4d3 \ud835\udc4d4 \ud835\udc4d5 \ud835\udc4d6 \ud835\udc4d7 \ud835\udc7f\ud835\udfcf \ud835\udc7f\ud835\udfd0 \ud835\udc7f\ud835\udfd1 \ud835\udc7f\ud835\udfd2 \ud835\udc7f\ud835\udfd3 \ud835\udc7f\ud835\udfd4 \ud835\udc7f\ud835\udfd5 \ud835\udc6f\ud835\udfce \ud835\udc6f\ud835\udfd4 \ud835\udc6f\ud835\udfd5 \ud835\udc6f\ud835\udfd0 \ud835\udc6f\ud835\udfd2 \ud835\udc6f? \ud835\udc6f\ud835\udfcf\ud835\udc6f\ud835\udfcf \u22ef \ud835\udc6f\ud835\udfcf\ud835\udc6f? \u22ee \u22f1 \u22ee \ud835\udc6f?\ud835\udc6f\ud835\udfcf \u22ef \ud835\udc6f?\ud835\udc6f? \ud835\udc70\ud835\udfd4 Item Order \ud835\udc70\ud835\udfd5 \ud835\udc70\ud835\udfd0 \ud835\udc70\ud835\udfd2 \ud835\udc70? from logged click-through data:", "Algorithm 1: Learning Algorithm of DivNet": "where \ud835\udf03 denotes the parameters of DivNet . The R( \ud835\udf0b,\ud835\udc66 ) can be the evaluation metric measuring the users' satisfaction of item layout. In this paper, we use NDCG as R( \ud835\udf0b,\ud835\udc66 ) . Policy gradient is used to optimize the Equation 13 where the gradient is given by: The Equation 14 can be approximated using Monte-Carlo method by sampling \ud835\udf0b from \ud835\udc43 \ud835\udf03 . As we already know that the sampling space is very large, optimizing from scratch using reinforce suffers from high variance and non-convergence. We indeed observe that the model occasionally converges to an abnormal point during training. To solve the out-of-distribution exposure bias problem, we turn to supervised regularization with discounted objective which is robust and efficient. we can combine the loss of reinforce with this regularization. Considering \ud835\udc61 st step of decoding process, DivNet assigns each item a score using Equation 12. We can define a per-step loss which uses multi-label cross-entropy loss. where \ud835\udc3c \ud835\udf0b \ud835\udc61 \u2032 \u2208 \ud835\udc45 \ud835\udc61 means that at step t we only compute the multilabel cross-entropy loss on items which are not selected up to step t. 1: Input : training dataset D 2: Training Procedure 3: while Not Convergence do 4: Sample batch data A,Y from D . 5: Feed samples into DivNet , and the item list are sampled from DivNet using equation 12. 6: Update DivNet using gradient descent by minimizing Equation 17. 7: end while", "8: Inference Procedure": "9: Given ranked items from upstream modules, pass them into DivNet and output \ud835\udf0b are sampled using Equation 12. The final loss is the sum of per-step cross-entropy loss: where we can optionally set weight \ud835\udc64 \ud835\udc56 = 1 / log ( \ud835\udc61 + 1 ) for \ud835\udc61 -th step, like position discounting in NDCG. In summary, the final loss after addition of supervised regularization is as follows: where \ud835\udf06 is the coefficient of the regularization term. The overall learning procedure is summarized in Algorithm 1.", "4 Experiments": "In the section, we introduce the datasets used and experimental settings. Offline and online experimental results and discussions are also given.", "4.1 Datasets": "Public datasets including Yahoo Webscope 1 and Microsoft 10K [14] and real-world E-commerce dataset are used to testify models' performance. The summary of dataset statistics are given in Table 1. Yahoo Webscope dataset The dataset are collected from Yahoo search engine and made public to researchers by Yahoo for learning-to-rank method development. The total queries have the size of 29 thousands and the number of documents is 700 thousands. The document relevance scores given a query range from 0 (most irrelevant) to 4 (most relevant). The ratings (0-4) are converted into binary labels (0-2 negative and 3-4 positive) The documents contains textual features and statistics features. The queries are evenly sampled from the search engine to ensure the dataset is unbiased. Microsoft 10K dataset The Microsoft 10K dataset is also collected from Bing search engine in a similar way to Yahoo dataset. It contains 10 thousands of queries and 1.2 millions of documents. The document also contain statistic features and relevance scores ranging from 0 to 4, denoting the matching score between queries and documents. The ratings (0-4) are also converted into binary labels (0-2 negative and 3-4 positive). E-commerce dataset The large-scale dataset collected from a real-world E-commerce recommendation system contains records which have users, and recommended item list and user's response (click and non-click). Since different items have different color/icons and text descriptions, items would interact with surrounding items and influence users' responses.", "4.2 Experimental Setup": "Experimental Settings For Yahoo and Microsoft dataset, the initial ranking lists are generated by LambdaMART. The dimensions \ud835\udc3f of item embedding I \u2208 R | A | \u00d7 \ud835\udc40 and user embedding U \u2208 R | A | \u00d7 \ud835\udc3e used are both 64. The dimension \ud835\udc37 \ud835\udc3e , \ud835\udc37 \ud835\udc49 of parameter matrix of key and value W K e , W V e , W K d , W V d are 64. Network parameters are initialized uniformly at random in [-0.1, 0.1]. The stochastic gradient descent optimizer we used is ADAM [9] with an initial learning rate of 0.01. All experiments use batches of 256 training examples. The dataset are split into training, validation and testing by 0.8, 0.1 and 0.1. We test the parameter \ud835\udf06 from 0.01 to 1. Baselines For off-line evaluation, we compare the proposed method with ranking methods without collective recommendation and ranking methods with diverse collective recommendation. \u00b7 Wide&Deep This model represents recommendation system without collective recommendation module. The method used is a widely-used Wide&Deep ranking model [5]. \u00b7 LambdaMART This method [4] is the SOTA learning-torank algorithm which uses listwise loss to model the contextual items when scoring items. \u00b7 Submodular This method [11] tries to balance the utility, such as click-through rate and item diversity by designing a sub-modular function. The items can be selected greedily by maximizing the sub-modular objective. \u00b7 DPP This method [21] employs determinantal point processes to diversify recommendation set by computing pairwise item similarity and iteratively select items from the learned determinantal point processes probabilistic model. \u00b7 PRM [13] uses self-attention networks to encode contextual items and estimate their utilities based on contextual features. The items are reranked in a descending order based on the estimation of utility. \u00b7 Seq2Slate [3] uses recurrent neural networks to encode item interaction and sequentially output item ranking. Seq2Slate uses pointer network to sequentially generate the final list and the current item selection depends on previously-selected items. Metrics To evaluate the performance of different methods, We use the user engagement metrics as the proxy for user overall satisfaction with the whole-page recommendation, which includes relevance and diversity. Some of them are used in off-line evaluation and some of them are calculated by online A/B testing platform. The definitions of those metrics are as follows: \u00b7 NDCG is normalized discounted cumulative gain which measures the ranking quality. NDCG = DCG/IDCG, where DCG is calculated as the following equation: where \ud835\udc5f\ud835\udc52\ud835\udc59 \ud835\udc56 is the click label of \ud835\udc56 -th item. IDCG is ideal discounted cumulative gain and is equal to DCG after transforming \ud835\udc5f\ud835\udc52\ud835\udc59 \ud835\udc56 by sorting \ud835\udc5f\ud835\udc52\ud835\udc59 \ud835\udc56 decreasingly. \u00b7 Pre@K is defined as the percentage of clicked items in the top-k following the user scan order. where \ud835\udc3c ( \ud835\udc56 ) is the indicator function about whether the \ud835\udc56 -th item is clicked. R is the number of all user requests in the test dataset. \ud835\udc46 \ud835\udc5f is the ordered items generated by a ranking model.", "4.3 Offline Experiments": "In this section, we first conduct experiments on Yahoo, Microsoft and E-commerce datasets. Then ablation study is carried out to show the contribution of each part in DivNet . The results of different methods on three datasets are shown in Table 2. LambdaMART compares the utility scores of the list of items and performs better than Wide&Deep which neglects the influence of contextual items. Similarly, PRM outperforms Wide&Deep by using the list of items as input features to model the impact of contextual items. The performances of Submodular and DPP vary in different datasets. DPP works better than Submodular in Yahoo and E-commerce datasets and worse than Submodular in Microsoft datasets, which shows the complexity of real-world dataset and the expertise knowledge in the model limits its expressive power. Seq2Slate and DivNet outperforms alternatives with a large margin, which demonstrates that modeling user's sequential viewing behaviors is vital in deciding the item list. DivNet improves NDCG by 3.6% and MAP by 3.3% over SOTA Seq2Slate in E-commerce dataset and also improves in Yahoo and Microsoft datasets. To understand the effectiveness of the sub-modules of DivNet , ablation test is carried out. The results are given in Table 3. The performance of DivNet without Supervised learning (SL) where \ud835\udf06 equals zero is slightly worse than DivNet, which means that SL provides additional signal and guide the DivNet converge to superior region than DivNet without SL. Removing sampling by greedy decoding item when generating item lists deteriorates the model performance. The diversity of generated samples enlarges the data distribution of training samples and reduces the distribution gap between training and testing samples.", "4.4 Online Deployment Results": "We also deploy the proposed DivNet for item recommendation in Ecommerce recomendation platform. The service is a marketing page that recommends items of various categories such as coupons and popular products to users. Our goal is to maximize the one-hop clickthrough rate by rearranging the whole-page item layout. We have a base ranker, a sophisticated and heavy DNN model who emphasizes on fine-ranking. After the base ranker, a simple rule checker is used to control duplicated items from the same category. We want to model mutual influence between items instead of simply taking category duplication into consideration via rules. Since different services have different color/icons and text descriptions, we believe different combinations will lead different results. In this way, we can provide users with items that they are interested in and improve the adhesiveness of our application. Due to the huge number of users, daily logged data is enormous. We use the business data collected over two months as our training and testing data. Some data statistics are given in Table 4 Table 4: Statistics of sampled logged data from E-commerce platform for training and testing. UVs means the number of user-views and PVs represents the number of page-views. Results of online performances are shown in Table 5. Compared to the baseline Wide&Deep, DivNet has the highest page clickthrough rate improvement (+8.7%) and user click-through rate (+4.26%). We also observe other business indicators increase, such as user CVR. Now the DivNet have been deployed in the E-commerce production task and is responsible for 100% of user requests. Self-Correcting Effect One example of pair-wise influence in Equation Softmax GLYPH<0> Qd \u2217 Kd \ud835\udc47 \u2217 U ) \u221a DK between items in E-commerce datset is visualized in Figure 3. In this case, item 10 receive the most influence from previous items 2, 4, 8 where they share the same clothes category, which demonstrates items from the same category tend to have a larger self-correcting effect.", "5 Conclusion": "In this paper, we propose a computationally-efficient and effective item layout optimization model called DivNet , which has been easily integrated into existing commercial recommendation systems.", "Table 5: Online Performances of different approaches on Ecommerce recommendation task.": "0 1 2 3 4 5 6 7 8 9 10 11 0 1 2 3 4 5 6 7 8 9 10 11 0.000 0.025 0.050 0.075 0.100 0.125 This model follows a sequence-to-sequence framework and explicitly consider the self-correcting effect when recommendation sets are sequentially viewed by users. The model can be learned end-toend and is data-driven. Experimental results of off-line and online analyses show its superior performance. For the future work, incorporation of structure knowledge in DivNet is a promising direction, e.g, making the objective function sub-modular. Multi-task objective optimization is also needed as the business operators usually care about multiple business indicators.", "References": "[1] Odd Aalen, Ornulf Borgan, and Hakon Gjessing. 2008. Survival and event history analysis: a process point of view . Springer Science & Business Media. [2] Aman Agarwal, Kenta Takatsu, Ivan Zaitsev, and Thorsten Joachims. 2019. A General Framework for Counterfactual Learning-to-Rank. In ACM Conference on Research and Development in Information Retrieval (SIGIR) . [3] Irwan Bello, Sayali Kulkarni, Sagar Jain, Craig Boutilier, Ed Chi, Elad Eban, Xiyang Luo, Alan Mackey, and Ofer Meshi. 2018. Seq2slate: Re-ranking and slate optimization with rnns. arXiv preprint arXiv:1810.02019 (2018). [4] Christopher JC Burges. 2010. From ranknet to lambdarank to lambdamart: An overview. Learning 11, 23-581 (2010), 81. [5] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems . 7-10. [6] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks for youtube recommendations. In Proceedings of the 10th ACM conference on recommender systems . 191-198. [7] Weicong Ding, Dinesh Govindaraj, and SVN Vishwanathan. 2019. Whole Page Optimization with Global Constraints. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . ACM, 31533161. [8] Alan G Hawkes. 1971. Spectra of some self-exciting and mutually exciting point processes. Biometrika 58, 1 (1971), 83-90. [9] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014). [10] JooWon Lee and Jae-Hyeon Ahn. 2012. Attention to Banner Ads and Their Effectiveness: An Eye-Tracking Approach. International Journal of Electronic Commerce 17, 1 (2012), 119-137. https://doi.org/10.2753/JEC1086-4415170105 [11] Houssam Nassif, Kemal Oral Cansizlar, Mitchell Goodman, and SVN Vishwanathan. 2018. Diversifying music recommendations. arXiv preprint arXiv:1810.01482 (2018). [12] Yosihiko Ogata. 1998. Space-time point-process models for earthquake occurrences. Annals of the Institute of Statistical Mathematics 50, 2 (1998), 379-402. [13] Changhua Pei, Yi Zhang, Yongfeng Zhang, Fei Sun, Xiao Lin, Hanxiao Sun, Jian Wu, Peng Jiang, Junfeng Ge, Wenwu Ou, et al. 2019. Personalized re-ranking for recommendation. In Proceedings of the 13th ACM Conference on Recommender Systems . ACM, 3-11. [14] Tao Qin and Tie-Yan Liu. 2013. Introducing LETOR 4.0 datasets. arXiv preprint arXiv:1306.2597 (2013). [15] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2009. BPR: Bayesian Personalized Ranking from Implicit Feedback. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (Montreal, Quebec, Canada) (UAI '09) . AUAI Press, Arlington, Virginia, USA, 452-461. [16] Harald Steck. 2018. Calibrated recommendations. In Proceedings of the 12th ACM conference on recommender systems . 154-162. [17] Choon Hui Teo, Houssam Nassif, Daniel Hill, Sriram Srinivasan, Mitchell Goodman, Vijai Mohan, and SVN Vishwanathan. 2016. Adaptive, personalized diversity for visual discovery. In Proceedings of the 10th ACM conference on recommender systems . 35-38. [18] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer networks. In Advances in neural information processing systems . 2692-2700. [19] Yingfei Wang, Hua Ouyang, Chu Wang, Jianhui Chen, Tsvetan Asamov, and Yi Chang. 2017. Efficient ordered combinatorial semi-bandits for whole-page recommendation. In Thirty-First AAAI Conference on Artificial Intelligence . [20] Yue Wang, Dawei Yin, Luo Jie, Pengyuan Wang, Makoto Yamada, Yi Chang, and Qiaozhu Mei. 2016. Beyond ranking: Optimizing whole-page presentation. In Proceedings of the Ninth ACM International Conference on Web Search and Data Mining . ACM, 103-112. [21] Mark Wilhelm, Ajith Ramanathan, Alexander Bonomo, Sagar Jain, Ed H Chi, and Jennifer Gillenwater. 2018. Practical diversified recommendations on youtube with determinantal point processes. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management . 2165-2173. [22] Ronald J Williams. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning 8, 3-4 (1992), 229-256. [23] Shuang-Hong Yang, Bo Long, Alexander J. Smola, Hongyuan Zha, and Zhaohui Zheng. 2011. Collaborative Competitive Filtering: Learning Recommender Using Context of User Choice. In Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval (Beijing, China) (SIGIR '11) . Association for Computing Machinery, New York, NY, USA, 295-304. https://doi.org/10.1145/2009916.2009959 [24] Xiangyu Zhao, Long Xia, Liang Zhang, Zhuoye Ding, Dawei Yin, and Jiliang Tang. 2018. Deep reinforcement learning for page-wise recommendations. In Proceedings of the 12th ACM Conference on Recommender Systems . ACM, 95-103."}
