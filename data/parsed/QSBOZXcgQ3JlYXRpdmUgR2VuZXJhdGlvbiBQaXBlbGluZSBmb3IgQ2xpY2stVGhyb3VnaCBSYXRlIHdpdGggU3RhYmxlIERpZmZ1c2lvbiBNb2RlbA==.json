{
  "A New Creative Generation Pipeline for Click-Through Rate with Stable Diffusion Model": "Hao Yang Shopee Discovery Ads Beijing, China Jianxin Yuan Shopee Discovery Ads Beijing, China Shuai Yang Shopee Discovery Ads Beijing, China Linhe Xu Shopee Discovery Ads Beijing, China Shuo Yuan Shopee Discovery Ads Beijing, China",
  "ABSTRACT": "In online advertising scenario, sellers often create multiple creatives to provide comprehensive demonstrations, making it essential to present the most appealing design to maximize the Click-Through Rate (CTR). However, sellers generally struggle to consider users' preferences for creative design, leading to the relatively lower aesthetics and quantities compared to Artificial Intelligence (AI)-based approaches. Traditional AI-based approaches still face the same problem of not considering user information while having limited aesthetic knowledge from designers. In fact that fusing the user information, the generated creatives can be more attractive because different users may have different preferences. To optimize the results, the generated creatives in traditional methods are then ranked by another module named creative ranking model. The ranking model can predict the CTR score for each creative considering user features. However, the two above stages (generating creatives and ranking creatives) are regarded as two different tasks and are optimized separately. Specifically, generating creatives in the first stage without considering the target of improving CTR task may generate several creatives with poor quality, leading to dilute online impressions and directly making bad effectiveness on online results. In this paper, we proposed a new automated C reative G eneration pipeline for C lickT hrough R ate ( CG4CTR ) 1 with the goal of improving CTR during the creative generation stage. In this pipeline, a new creative is automatically generated and selected by stable diffusion method with the LoRA model and two novel models: prompt model and reward model. Our contributions have four parts: 1) The inpainting mode in stable diffusion method is firstly applied to creative image generation task in online advertising scene. A self-cyclic generation pipeline is proposed to ensure the convergence of training. 2) Prompt model is designed to generate individualized creative images for different user groups, which can further improve the diversity and quality of the generated 1 The code is at https://github.com/HaoYang0123/Creative_Generation_Pipeline. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference acronym 'XX, June 03-05, 2018, Woodstock, NY ¬© 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-XXXX-X/18/06...$15.00 https://doi.org/XXXXXXX.XXXXXXX",
  "Yifan Zeng": "Shopee Discovery Ads Beijing, China creatives. 3) Reward model comprehensively considers the multimodal features of image and text to improve the effectiveness of creative ranking task, and it is also critical in self-cyclic generation pipeline. 4) The significant benefits obtained in online and offline experiments verify the significance of our proposed method.",
  "KEYWORDS": "Stable Diffusion, Creative Generation, Prompt and Reward Models",
  "ACMReference Format:": "Hao Yang, Jianxin Yuan, Shuai Yang, Linhe Xu, Shuo Yuan, and Yifan Zeng. 2018. A New Creative Generation Pipeline for Click-Through Rate with Stable Diffusion Model. In Proceedings of Make sure to enter the correct conference title from your rights confirmation emai (Conference acronym 'XX). ACM, New York, NY, USA, 10 pages. https://doi.org/XXXXXXX.XXXXXXX",
  "1 INTRODUCTION": "Recently, the topic of AI-Generated Content (AIGC) has developed rapidly, such as GLIDE [24], DALL-E 1-3 [31, 36, 45], Imagen [40], Stable Diffusion (SD) [37] and ChatGPT [43]. One of the most representative tasks is text-to-image (T2I) generation, which has received much attention in both academia and industry. The target of T2I task is to generate images with similar semantics given input language prompt. Currently, the diffusion-based methods [18, 23, 37] have become the state-of-the-art (SoTA) method in T2I task due to the stationary training objective and easy scalability. Although these diffusion methods have a satisfactory performance on the T2I task, directly using these methods in creative image generation is infeasible in the advertising scene because they will modify the main product image. Traditional creative generation methods in the advertising scene use deep learning to generate some objects/tags [19, 20], dense captions [32] or layout information [33, 46, 47] on image. Different from these methods, we first use diffusion model to generate background images while keeping the main product information unchanged in creative generation task for the advertising scene. In the experimental analysis, we found that modifying the background while keeping the visual pixels of the main product unchanged can also significantly improve the CTR. In addition, when using the diffusion model, there are two very important factors, prompt and diffusion models, that will directly affect the effect of the generated image. As shown in Fig. 1, for the original product image uploaded by the seller, the background image is simple with solid white color. To make the background more attractive, we can use the inpainting mode in SD method to generate more individualized background images with richer colors. However, if a proper prompt (a prompt consists of several Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Hao Yang, Jianxin Yuan, Shuai Yang, Linhe Xu, Shuo Yuan, and Yifan Zeng tokens) is not added into the generation process, the result is often poor with the cluttered background. Then we need to use a Prompt Model (PM) to select a good prompt, which is then added to guide the generation process. Despite that a good background can be generated given an acceptable prompt, the diffusion model is not fine-tuned in our industrial data, and the effect is not optimal. It is necessary to fine-tune diffusion model in our data. However, the parameters of diffusion model are very huge, and directly training whole model is not realistic because of high training cost. Therefore, Low-Rank Adaptation (LoRA) mechanism [27, 48] is used to speed up the training process. Figure 1: Generating creatives only by modifying background using SD method in inpainting mode. without prompt without LoRA model without LoRA mode Stable Dilfusion Saliency Detection (withwithout Lora) with LoRA model As described earlier, Prompt Model (PM) is important to guide image generation. PM considers user feature ( e.g. , user age, user gender), item feature ( e.g. , item ID and category ID) and candidate tokens as input, then outputs score for each input token. We select topùëù tokens to assemble into one prompt. Considering that different users have different preferences for creative images, for example, young users are more inclined to cool color and electronic style while old users are more inclined to warm color and minimalistic style. Then user feature is considered as a vital feature to generate individualized prompts for different user groups. The traditional creative generation methods [19, 20, 32, 33, 46, 47] do not consider the user information in generating creative image process. Some works [16, 25, 39, 51] use another model to rank all generated creatives considering user information and select the best one to show online. These traditional pipelines do not maximize the online performance because in first stage (generating creative stage), they do not regard the improving CTR as the target, resulting in many \"bad\" creatives diluting the online impressions. So it is necessary and critical to directly generate more attractive creatives by considering user information and regarding the CTR as an optimization target in generating stage. Currently, several works [50, 52] focus on enhancing the generation quality of diffusion models by reinforcement learning with human feedback. Although these methods can generate creatives using diffusion models for a specific target, such as CTR, they still face the problems of modifying the original product with relatively low efficiency and diversity. To guide the prompt model and LoRA model to generate more attractive creatives, another Reward Model (RM) is used to predict the Click-Through Rate (CTR) score for each creative image given one product. For each product, if too many creatives are uploaded online, it will lead to sub-optimal performance due to the inferior performance of the generated creatives [25]. Then we need RM to select the proper creatives for displaying online. At the same time, RM can be used to generate samples (good creatives with high predicted scores and bad creatives with low scores) to train LoRA and prompt models. To summarize, there are four contributions in this work: 1) The inpainting mode in stable diffusion method is firstly applied to creative image generation task in online advertising scene. A selfcyclic generation pipeline is proposed to ensure the convergence of training. 2) Prompt model is designed to generate individualized creative images for different user groups, which can further improve the diversity and quality of the generated creatives. 3) Reward model comprehensively considers the multi-modal features of image and text to improve the effectiveness of creative ranking task, and it is also critical in self-cyclic generation pipeline. 4) The significant benefits obtained in online and offline experiments verify the significance of our proposed method.",
  "2 RELATED WORKS": "",
  "2.1 Text-to-image Generation": "Recently, in text-to-image generation (T2I) task, more and more deep learning models to generate image given text are emerging. For example, the generative adversarial networks (GANs) [4, 6] use a generator and a discriminator to generate image, where generator network is responsible for generating image and discriminator network is to distinguish the generated image and the real one. There are some related GANs works, such as ProGAN [10], StyleGAN [15, 17, 21], Projected GAN [28], VQGAN [30]. Different from GANs, some autoencoder (AE) related works appear, such as deep autoencoder [2], variational autoencoder (VAE) [3], vector quantised-variational autoencoders (VQ-VAE) [9, 14]. The encoder receives the input and encodes it in a latent space of a lower dimension while the decoder decodes this vector to produce the original input. As auto-regressive models developed in text generation recently, a lot of works adopted this to achieve amazing results for T2I generation, such as DALL-E 1-3 [31, 36, 45], CogView [22] and Pariti [42]. Currently, the diffusion models (DM) [24, 37, 40] appeared as SoTA T2I methods due to the natural fit to inductive biases from image data. For example, GLIDE [24] uses an adequate textguidance strategy to generate and edit photorealistic image. Latent DM[37] enables DM training on limited computational resources while retaining the quality and flexibility by applying in latent space of powerful pre-trained autoencoders. Imagen [40] directly diffuses pixels using a pyramid structure without using latent images. Recently, some works [19, 20, 32, 33, 35, 46, 47] study the task of creative generation in the advertising scene. For example, [19] automatically annotates the objects from the image and generates optimal banner layout information. CapOnImage [32] generates dense captions at different locations of the image by pre-training and fine-tuning a multi-modal framework. CreaGAN [35] generalizes to other product materials by utilizing existing design elements ( e.g. , background material). Other works [33, 46, 47] generate layout information of some elements, such as text and logo, given the original image. A New Creative Generation Pipeline for Click-Through Rate with Stable Diffusion Model Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Different from the above methods, we propose a novel pipeline for generating creatives in advertising scene. A stable diffusion method is used in inpainting mode to generate only the background pixels, while keeping the other pixels on the main product unchanged. On the other hand, by introducing user information, we can support individualized generation for various user group.",
  "2.2 Creative CTR Prediction": "Creative CTR prediction task is to rank all creative images provided by sellers or automatically generated by models, given one product or advertisement. AMS [12] combines the behavior images into modeling user preference in CTR prediction. PEAC [16] evaluates and selects advertisement creatives offline before being placed online. VAM [25] uses a visual-aware ranking model to order the creatives according to their visual appearance. CACS [39] considers the different image features in ranking stage to joint optimization of intra-advertisement creative selection and inter-advertisement ranking. CLIK [34] considers the topic of the given product to select representatives from several images with intuitive visual information by contrastive learning. [51] proposes a two-stage pipeline method with content-based recall model and CTR-based ranking model to find an appropriate image. Compared to these methods, our proposed reward model performs better because of the multimodal structure and pre-training techniques.",
  "3 METHODS": "",
  "3.1 Pipeline": "The pipeline is shown in Fig. 2 and the pseudo-code is shown in Alg. 1. In the creative generation pipeline, for each item, we first select one original image with a relatively clean background provided by sellers, which then is inputted into saliency model [38] to get the saliency object image and mask image. We use Stable Diffusion (SD) [37] method to generate the creatives given the saliency image, LoRA model [27, 48] and prompts where LoRA model is fine-tuned in our commercial data and prompts are selected by another prompt model trained on online creative clicked data. The LoRA model and prompt model are critical for the impact on generating creatives, especially since different prompts can cause creatives to vary greatly. To consistently generate good creatives, we need to fine-tune LoRA model and prompt model in this pipeline. The details are described in Section 3.2 and 3.3. Next, we use reward model (details in Section 3.4) to predict the CTR score for each generated creative, and only topk creatives are retained to further train LoRA model while these topk creatives are treated as positive samples and others are treated as negative samples to train prompt model. Last but not least, various users may have distinct preferences on creatives. So we need to generate different types of creatives for various users. Then user information is considered in prompt model, although the input product or image is the same, the prompts may need to be adjusted based on the characteristics of the user group, and the resulting generated creatives can also be different. To improve the quality and stability of the generated creative, the LoRA model and prompt model are updated iteratively. Precisely, in first step, only parameters in LoRA model are updated while parameters in prompt model are fixed; in second step, parameters in LoRA model are fixed while parameters in prompt model are updated, ensuring the convergence of the entire framework.",
  "Algorithm 1 Creative Generation Pipeline for CTR.": "Input: ùëÄ is the number of items ùëÅ is the number of training epochs ùúÉ ùëù is the parameters of prompt model ùúÉ ùëô is the parameters of LoRA model ùúÉ ùëü is the parameters of reward model ùúÉ ùë† is the parameters of saliency model ùúÉ ùëê is the parameters of CLIP-Interrogator model ùëù is the number of tokens select by prompt model ùëû is the number of generated creative images ùëò is the number of images select by reward model 1: Initialization: ùúÉ ùëù and ùúÉ ùëô are trained on clicked data as the model parameters for initialization. Note that ùúÉ ùëü has been trained beforehand while ùúÉ ùë† and ùúÉ ùëê are the public models, these three model parameters are frozen in this pipeline. 2: for ùëñ = 1 ‚Üí ùëÅ do 3: G, B = ‚àÖ , ‚àÖ , which refer to the set of good case and bad case, respectively. 4: for ùëó = 1 ‚Üí ùëÄ do 5: Get the original images of ùëñùë°ùëíùëö ùëó provided by seller 6: Get saliency images and masked images by ùúÉ ùë† 7: Select topùëù tokens as the appropriate prompt ùëùùëüùëúùëöùëùùë° ùëó for ùëñùë°ùëíùëö ùëó by ùúÉ ùëù given user group feature 8: Generate ùëû creative images for ùëñùë°ùëíùëö ùëó by ùúÉ ùëô given ùëùùëüùëúùëöùëùùë° ùëó 9: Sort images by predicted CTR scores from ùúÉ ùëü 10: Select topùëò images as positive samples and other images as negative samples 11: Generate the prompt for each image by ùúÉ ùëê 12: Insert samples of ( ùëñùë°ùëíùëö ùëó , ùëñùëöùëéùëîùëí 1, ùëùùëüùëúùëöùëùùë° 1, ùë†ùëêùëúùëüùëí 1), ... ( ùëñùë°ùëíùëö ùëó , ùëñùëöùëéùëîùëí ùëò , ùëùùëüùëúùëöùëùùë° ùëò , ùë†ùëêùëúùëüùëí ùëò ) into G 13: Insert samples of ( ùëñùë°ùëíùëö ùëó , ùëñùëöùëéùëîùëí ùëò + 1 , ùëùùëüùëúùëöùëùùë° ùëò + 1 , ùë†ùëêùëúùëüùëí ùëò + 1 ), ... ( ùëñùë°ùëíùëö ùëó , ùëñùëöùëéùëîùëí ùëû , ùëùùëüùëúùëöùëùùë° ùëû , ùë†ùëêùëúùëüùëí ùëû ) into B 14: end for 15: 16: 17: 18: if ùëñ % 2 == 0 then Train prompt model ùúÉ ùëù given G and B by Equation 8 else Train LoRA model ùúÉ ùëô given G by Equation 3 19: end if 20: end for Output: ùúÉ ùëù and ùúÉ ùëô",
  "3.2 LoRA Model in Stable Diffusion": "As the original creatives designed by sellers may have quality and quantity problems, we need to modify the images to a certain extent. Recently, a lot of works [18, 24, 36, 44] use diffusion models to generate images, which uses a variational autoencoder (VAE) [3] to operate the diffusion process in a low-dimensional latent space with good computational efficiency. In detail, one image I is given into an encoder ùëâùê¥ùê∏ to get the latent feature ùëç , i.e. , ùëç = ùëâùê¥ùê∏ ( ùêº ) . A diffusion model is to denoise an input latent feature ùëç ùë° at each timestep ùë° conditioned on a given prompt. During the training process, for each timestep ùë° , the diffusion denoising network ùúÄ ùúÉ is optimized to remove the noise ùúÄ from the noised version of latent code ùëç ùë° conditioned on prompt ùë¶ shown in following equation, where ùúè ùúÉ is the CLIP [26] text encoder, ùë¶ is the given text or prompt, ùúÄ is the noise sampled according to a standard normal distribution. The diffusion network ùúÄ ùúÉ often uses UNet [5] consisting of convolution, self-attention and cross-attention layers. ùëç ùë° is the noised embedding encoded by a scheduler in low-dimensional space with VAE model given image ùêº and noise ùúÄ . The scheduler is used to add noise information into low-dimensional embedding from the given image.   Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Hao Yang, Jianxin Yuan, Shuai Yang, Linhe Xu, Shuo Yuan, and Yifan Zeng Figure 2: (a) Creative generation pipeline. (b) The structure of prompt model. (c) The structure of reward model. (a) CGACTR Pipeline Product Information Train by i and by user group feature. Tangenne Vita Prompt Model CLIP-Interrogator (itamin Whitening Essonco Fo Dry Skin) Image (realistic, WI) (water, w2) (petal, w3) CTR: 0.18 Stable Diffusion Reward +Lora Model (ii)X CTR: 0.07 Saliency Detection Creatives Train by (b) Prompt Model (c Reward Model Caption Creative image Query Natural light [GOODAL] GREEN TANGERINE Tangerine Vita Dark VITA-C Dark Spot Age Gender Phone Day/Night ID Category ID Clean Spol Care Serum Care SERUM 4Oml (Vitamin goodal Whitening Essence UserAttribute Context Itemlnfo Tokens For Dry Dull Skin) emb titlej captionj Transformers Concate - > FC (64/32) SWIN mean-pooling emba emb DeepFM Transformers (num_heads-4, num layers=4) 0.65 Natural light Token Scores 0.36 Clean Prompt Concatenate & FC Select top-p CTR Score Frozen Trainable Train flow Inference flow Good creative X Bad creative fusing Title Green imagei However, the standard SD method cannot be used in our pipeline because it will modify the main product in advertising platform. For example, the product image is a pair of shoes uploaded by a seller, after the SD generating process, the generated image might be a pair of socks or a hat, which is not accepted. To generate one image with a higher quality while leaving the main content of original product unchanged, we can get the pixels (referred as ùêº ùë†ùëéùëô ) of the main product image ùêº by using the saliency detection network [38]. The pre-trained saliency detection network can point out the main product locations and then ùêº ùë†ùëéùëô are masked and only background pixels ùêº ùëèùëòùëî are generated by SD method in inpainting mode [41]. The only difference between the inpainting mode and normal mode in SD is that the inpainting mode concatenates the low-dimensional embedding ( ùëâùê¥ùê∏ ( ùêº ùë†ùëéùëô ) ) encoded by VAE given masked pixels (Equation 4). The training loss is similar to the normal mode. model, including UNet [5] and CLIP [26]. Low-Rank Adaptation (LoRA) [27, 48] can help to solve the above problem, which allows us to use low-rank adaptation technology to quickly fine-tune SD model. With fine-tuned LoRA model, new creative image with a generated background can be acquired by SD method given an appropriate prompt.   Because of the difference in image distribution between our commercial data and public data, we should fine-tune the public SD model on our commercial data. However, it is very difficult to fine-tune SD model because of huge number of parameters in SD Wetake Fig. 1 to explain the importance of the prompt and LoRA in generating creative workflow. The original image of the product given by seller is poor due to the simple colored background. To improve the quality of this creative, firstly we get masked pixels of the main product ( e.g. , skin care product) by saliency detection model [38]. Given the original image and masked image, we use SD method in inpainting mode to get better creatives with suitable backgrounds. Of course, to explain the importance of prompt and LoRA model in generating process, firstly we only use the SD method without prompt and LoRA model, the results are not as expected with mussy background same as the original image (three upper images in Fig. 1). To improve the quality of background, we can add a suitable prompt ( e.g. , product photo, petals, water flowers etc. ), and the results can be better (three middle images). Finally, LoRA model is increased in generating process, the quality can be further improved (three lower images). A New Creative Generation Pipeline for Click-Through Rate with Stable Diffusion Model Conference acronym 'XX, June 03-05, 2018, Woodstock, NY On the other hand, to distinguish the good and bad creatives in generating process, a reward model is proposed to predict the CTR, we only retain the good creatives with high CTRs to train LoRA model. At the same time, the good creatives with prompts generated by CLIP-Interrogator model 2 are regarded as positive samples and the bad ones are regarded as negative samples to train prompt model. After multiple rounds of training processes, the LoRA model and prompt model can improve the quality of generated creatives and improve user experience.",
  "3.3 Prompt Model": "The target of the prompt model is to select several appropriate words/tokens combined as one prompt, which can effectively describe the style of the image, and then input it into the stable diffusion model to generate more appealing images. The input of the prompt model consists of four parts: user attributes (user age, user gender, etc. ), item attributes (item ID, category ID, etc. ), contextual attributes (user's device model, access time, etc. ), and tokens used for creative generation. The user attributes, item attributes, and contextual attributes are referred as Query . The output of the prompt model is the estimated CTR score for each token in the whole word vocabulary 3 under the given Query conditions. The topùëù tokens with the highest CTRs are selected, which are assembled as a new prompt and it is then inputted into a stable diffusion model to generate creative images. The network architecture of the prompt model is illustrated in sub-figure (b) of Fig. 2. On the Query side, user attributes, item attributes, and contextual attributes are one-hot encodings and then converted into corresponding embeddings through lookup table. These three embeddings are concatenated into a query embedding named ùëíùëöùëè ùëû . Simultaneously, all tokens in the prompt generated from the creative image are multi-hot encodings and then transformed into multiple embeddings named ùëíùëöùëè ‚àó ùëù in the same way. Then we send ùëíùëöùëè ‚àó ùëù into transformers and a mean-pooling layer to obtain one final embedding ùëíùëöùëè ùëù representing for the prompt. These two embeddings, ùëíùëöùëè ùëû and ùëíùëöùëè ùëù are concatenated and passed through DeepFM [8] to produce a final estimated score. The prompts generated from the clicked creatives are regarded as positive samples, and other prompts from non-clicked creatives are treated as negative samples. This enables the model to select the appropriate prompts that are likely to improve CTR under Query conditions. Wetrain the prompt model under the cross-entropy loss function using the clicked data, which we call this loss function as \"hard loss\" ( ùêø ‚Ñéùëéùëüùëë in Equation 6). Additionally, to reduce the exploration cost in creative generation, we utilize the output values from the reward model as the soft labels for auxiliary training in self-cycling in Fig 2. The final loss is a weighted sum of the hard loss and soft loss, where ùúÜ ùëù is 0.1 in our experiments.     2 CLIP-Interrogator model is at https://github.com/pharmapsychotic/clip-interrogator. 3 We have collected item images provided by sellers and designers and run CLIPInterrogator to get the complete tokens, which are then post-processed, such as deleting stop-words and entity nouns, to form the whole word vocabulary. Furthermore, individualized prompts can be created for different user groups based on user information, as different users may prefer creatives with different styles. The generated creatives can enhance user experience while reducing exploration costs. For example, without considering user information, the prompt model may select and combine some normal tokens, such as \"minimalistic\" and \"translucent\". But for a teenager, better tokens may be \"electronic\" and \"translucent\" while for an old user, \"golden\" and \"minimalistic\" may be more appropriate. The normal prompt combined with \"minimalistic\" and \"translucent\" might not be the favorite for either user group, hence failing to meet the actual needs.",
  "3.4 Reward Model": "The structure of reward model is shown in sub-figure (c) of Fig. 2, which is used to predict the CTRs of the creatives from the same item generated by stable diffusion model. Each creative contains title, image and caption where title of creative is the same as the title of item. As the titles of all creatives are the same, the title feature seems redundant for predicting CTR scores. To explain why title feature is still important for this task, we use a multi-head self-attention sub-module to compute the relationship between the textual feature and visual feature of each creative. In detail, the multi-head self-attention sub-module can learn the relationship ( e.g. , similarity) between textual feature and visual feature, such that the creative with a high similarity score may be predicted with high CTR score. In addition, to make reward model to learn the creative content better, the caption information is also used, which is acquired by CLIP-Interrogator model. CLIP-Interrogator model uses the image as input and outputs the textual caption to describe the image. The title and caption information of j -th creative for i -th item are inputted into pre-trained BERT model [11] to extract textual features, referred as ùë° ùëó and ùëê ùëó , respectively (Equation 9). The pretrained Swin model [29] is used to extract visual feature (referred as ùëñ ùëó ) from image information (Equation 9). Both BERT model and Swin model are pre-trained in our commercial data with token mask loss and patch mask loss, respectively. In particular, BERT model is further fine-tuned in our commercial clicked data [49]. Two fully-connected layers are used to reduce the dimension of feature (Equation 10). To fusion the cross-model features and extract the relationship between textual and visual features, we use transformers [7] to get self-attention features (Equation 11). The fused title, image and caption features ( ùë° ùë° ùëó , ùëñ ùë° ùëó , ùëê ùë° ùëó ) are concatenated into final feature. Other two fully-connected layers are used to predict the CTR score ( ùëù ùëñ ùëó ) of j -th creative for i -th item (Equation 12).     The target of reward model is to predict the CTR score of each creative. The real CTR value is calculated by the following equation.  List-wise and point-wise loss functions are used to train reward model. The list-wise loss function is shown in Equation 14 where ÀÜ ùë¶ ùëñ ùëó Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Hao Yang, Jianxin Yuan, Shuai Yang, Linhe Xu, Shuo Yuan, and Yifan Zeng and ùëù ùëñ ùëó are the real CTR and predicted CTR, respectively. And ùë§ ùëñ ùëó is the sample impression weight of j -th creative for i -th item, shown in Equation 15. Through the list-wise loss function, the model focuses on ranking the creatives within the same item. Although the listwise loss function can lead model to correctly rank the creatives, the absolute difference between the real and predicted scores may be very huge. Then we use point-wise loss function as an auxiliary function to enforce model to produce more accurate scores, as shown in Equation 16.    As shown in Equation 17, the final loss is the sum of list-wise loss and point-wise loss, where ùúÜ ùëü is a hyperparameter and is 0.1 in our experiments.",
  "4 EXPERIMENTS": "In this section, we evaluate our proposed generation pipeline on online experiments. To validate the effectiveness of reward model, we use commercial data and public data compared with two other public methods. On the other hand, we make some ablation experiments for prompt model to explain the importance of considering user information. Finally, we perform some analyses and show some cases to demonstrate the effectiveness of the self-cycling training mode in our generation pipeline.",
  "4.1 Online Results": "We select five categories (including women shoes, women bags, travel, beauty and mobile) in our commercial scene for online experiments. For each item, the prompt model is used to select suitable tokens in the whole vocabulary as one appropriate prompt considering user features. As different users may have varying preferences for creatives, so we need to use prompt model to extract different prompts based on the characteristics of users. Then 10 creatives for each item are automatically generated by stable diffusion with LoRA model given the selected prompts. These creatives are then inputted into reward model to predict CTRs, and the top 5 creatives with highest predicted CTRs are retained. These retained creatives are regarded as training samples to further train LoRA model. To further optimize the quality of generated creatives in the next step, these retained creatives in current step are regarded as positive samples and others are regarded as negative samples, which are used to train prompt model. This can indirectly improve creative quality by facilitating better prompts in subsequent steps. With ten rounds of the above processes, top 5 creatives for each item are generated and uploaded to online platform. We use Epsilon-greedy [1, 13] as online display strategy. As shown in Table 1, without the self-cycling training in our pipeline (referred as \"w/o self-cycling\" in the table), we only use the first versions of LoRA and prompt models to generate the creatives. The CTR and revenue improvements are only 4.2% and 3.8%, respectively. After using LoRA and prompt models with five rounds Table 1: Online results. 1 The first metric is CTR improvement compared with the baseline of using the original image with percentage. 2 The second metric is revenue improvement with percentage. 3 \"Middle\" means the middle versions of prompt model and LoRA model in selfcycling. (referred as \"Middle\" in the table) and ten rounds (referred as \"Ours\" in the table) of training, the results become much better.",
  "4.2 In-depth Analysis": "4.2.1 Ablation Study on Reward Model. As described in METHODS section, reward model plays a crucial role in the self-cycling training process of generating creative pipeline. In reward model, we use topk CTR uplift to evaluate the performance. The topk CTR uplift metric is calculated as follows:    , where ùê∂ùëáùëÖ -ùë†ùëêùëúùëüùëí ùëò is the cumulative CTR scores of topk creatives with the highest scores predicted by reward model for each item. ùëêùëôùëñùëêùëò ùëó ùëñ and ùëñùëöùëùùëüùëíùë†ùë†ùëñùëúùëõ ùëó ùëñ are the numbers of clicks and impressions of j -th creative for i -th item. ùë°ùëúùëùùëò ùëñ ùê∂ is the set of topk creatives for the i -th item. If ùê∂ùëáùëÖ -ùë¢ùëùùëôùëñùëì ùë° ùëò metric is higher, the performance of the reward model is better. The high topùëò CTR uplift metrics only ensure that the order of outputs is good, so we use another MSE metric to measure the difference between the predicted CTRs and real ones as follows:  , where ùëõ is the number of items and ùëö ùëñ is the number of creatives in ùëñ -th item. The commercial data and public data 4 [25] are tested to demonstrate the effectiveness of our proposed reward model structure. The commercial data has 1.0M creatives from 286k items. Each creative has item title, image, caption (generated by CLIP-Interrogator), real click and real impression. Creatives from the 5% of items are randomly sampled as the testing data while the others are training data. The public data contains 1.7M creatives from 500k items. As there is no title information in public data, we average the image features of creatives from the same item as the title feature. The BERT and Swin in reward model are pre-trained on text information (title and caption) and image information with token mask loss and patch mask loss, respectively. Then in training reward model process, the parameters of BERT and Swin are fixed to speed up. We use adam optimizer with mini-batch of 2048 and learning rate of 5e-4. The number of epochs is set 30. 4 The public data is at https://tianchi.aliyun.com/dataset/93585. A New Creative Generation Pipeline for Click-Through Rate with Stable Diffusion Model Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Table 2: Ablation study on reward model. 1 As rank model uses entity textual knowledge and entity image knowledge while this information does not exist in the public data and commercial data, then we delete these input features to compare. 2 As there is no title information in public data, we use the averaged image features to simulate the title feature.",
  "Table 3: Ablation study on prompt model.": "1 This is an offline metric, it is the historical CTR improvement given the clicked data. In detail, we can calculate the cumulative CTR improvements of topùëò tokens. 2 Model structure experiments are not performed online, and so only the offline metric of \"Historical CTR\" is shown. (a) (b) denoised ; Poomptododel wlo user electronie 1 trenpty realistic younger trans grecian 1 intense 5 0.2 octered elder minimall We tested two public methods, including VAM [25] and Rank [51]. On the commercial data, the top-1 to top-5 CTR uplift scores of our proposed method are much higher than VAM and Rank while the MSE metric of the proposed method is smaller than other methods. We have similar results on the public data. In the reward model, we use title, image and caption information to predict the CTR scores. To demonstrate that the title and caption information is also helpful for this task, this information is deleted in the original network. As shown in Table 2, only considering the image feature to predict the CTR, the results are not good. Simply concatenating three multi-modal features without transformers can improve the CTR ranking effect, but the improvement is limited. Fusing transformers can further improve the results, which verifies the effectiveness of combining different types of input information and using transformers to extract the multi-model feature. In our proposed method, we have pre-trained BERT and Swin models with mask loss on corresponding data. As shown in this table, the results without pre-training are slightly inferior to our results, which verifies the importance of pre-training. Lastly, we have tested the effects of the loss functions. The results show that both list-wise and point-wise loss functions are useful for improving the results. 4.2.2 Ablation Study on Prompt Model. This section will show the ablation study of the prompt model. We use online clicked data to train the prompt model, with 20M samples as training data and 2M samples as testing data. Each sample has an average of 8.5 tokens. We employ Adam as the optimizer, set the learning rate to 1e-3, and train the model in 2 epochs to avoid overfitting with a batch size of 2048. The parameter of prompt model trained in this clicked data Epoch Number Figure 3: (a) Word cloud analysis of generated tokens by prompt models without and with considering user information. (b) The creatives generated by different versions of LoRA and prompt models are scored by reward model to show the effectiveness of self-cycling training process. is initialized as the first version of prompt model in self-cycling training process. We perform two experiments to severally explain the advantage of importing the user group information and the improvements of prompt model in self-cycling training process. In the fusing user group information experiment, the user group information is deleted in prompt model in both training (in self-cycling training mode) and testing processes, the CTR and revenue improvements are both limited (Table 3). To further improve the results, we import the user groups information. In detail, for each item, all user groups are enumerated, and one best prompt is selected for each user group by prompt model. Assuming that there are 5 creatives for each item without user information, after introducing user features, there are 5 √ó ùë¢ creatives where ùë¢ refers to the number of user groups. In online display strategy, we select the corresponding 5 creatives with the same user group as the current user for personalized creative recommendation. As shown in this table, the metrics in \"individuation\" have great improvements. In addition, we collected the tokens generated by prompt model to perform the word cloud analysis (Fig. 3(a)). After considering the user group information ( e.g. , younger and elder), the generated tokens are more \"individualized\" where younger users prefer \"electronic\" and \"realistic\" while elder users prefer \"pure\" and \"minimal\". This phenomenon illustrates that prompt model can learn the fact that different user groups have various preferences for diverse tokens. Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Hao Yang, Jianxin Yuan, Shuai Yang, Linhe Xu, Shuo Yuan, and Yifan Zeng Figure 4: Cases with the original image and generated creatives in different steps of self-cycling. Bottom score is the real CTR.",
  "Original image": "Initialization Middle Ours 0.11 0.10 0.14 Lali-h 0.14 0.12 0.17 0.19 TR 0.09 0.08 0.09 0.13 In another experiment of self-cycling training, we compare the results of the first version and last version of prompt model (Table 1). After self-cycling training, the final version of prompt model has higher metrics which further verifies the necessity of self-cycling. Lastly, we find that the transformers and DeepFM sub-modules in prompt model are also helpful with good historical CTR improvements. 4.2.3 Self-cycling Analysis. In previous section, we have already demonstrated the effectiveness of reward model on commercial and public data. Then we can use reward model to offline analyze the performances of LoRA and prompt models in self-cycling training process. As shown in Fig. 3(b), the reward scores of generated creatives increase during self-cycling training, demonstrating the effectiveness of our proposed training process. prompt model and LoRA model are updated alternately and iteratively. As the traditional creative generation methods do not use the CTR as the target, some bad creatives with poor CTR performance may be generated, leading to sub-optimal online results. To solve this problem, we fuse the CTR target in CG4CTR and consider user information in prompt model to select the appropriate prompt and indirectly generate individualized creatives. In self-cycling, we need the reward model to judge which creatives are good and which ones are bad, and then it can help to further improve the quality of generated creatives. Both offline and online experiments show that CG4CTR can generate better creatives with higher CTR compared with the original images provided by sellers.",
  "4.3 Case Study": "Lastly, we will show some amazing cases to validate the effectiveness of our proposed creative generation pipeline (Fig. 4). The aesthetics and harmony of creatives generated by our proposed method are much better than the original image provided by seller.",
  "5 DISCUSSION": "A new automated C reative G eneration pipeline for C lickT hrough R ate (CG4CTR) is proposed to fuse the CTR target into the creative generation task. In CG4CTR, we first use inpainting mode in diffusion method to generate the creative image in advertising scene. The generation process is carried out by self-cycling, where the Our proposed CG4CTR pipeline can be inserted at different stages of the item recommendation. Specifically, if CG4CTR is inserted after the item recommendation (named A-stage), for one user comes, candidate items are ranked by the ranking model, then the top items are used for CG4CTR to generate creatives for each item. In A-stage, the ranking model can only consider the original image rather than creative image, leading to the reduced performance. If CG4CTR is inserted before the item recommendation (named B-stage), the creatives for all candidate items are generated by CG4CTR, then ranking model can simultaneously consider both item information and creative information to sort them, which may have better performance but also need higher online time consuming. In future work, we will explore the difference in performance between these two stages. Furthermore, we can employ various A New Creative Generation Pipeline for Click-Through Rate with Stable Diffusion Model Conference acronym 'XX, June 03-05, 2018, Woodstock, NY styles of LoRA models (rather than one LoRA in CG4CTR) to generate creatives with enhanced styles and superior quality, where these styles can be provided by designers as initial samples to train LoRA model, and then their parameters are updated in self-cycling training mode. Lastly, more ways to generate creatives rather than modifying the background need to be investigated in the same framework.",
  "REFERENCES": "[1] Christopher John Cornish Hellaby Watkins. 1989. Learning from delayed rewards. (1989). [2] Geoffrey E Hinton and Ruslan R Salakhutdinov. 2006. Reducing the dimensionality of data with neural networks. science 313, 5786 (2006), 504-507. [3] Diederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 (2013). [4] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. Advances in neural information processing systems 27 (2014). [5] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18 . Springer, 234-241. [6] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. 2016. Generative adversarial text to image synthesis. In International conference on machine learning . PMLR, 1060-1069. [7] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017). [8] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. arXiv preprint arXiv:1703.04247 (2017). [9] Aaron Van Den Oord, Oriol Vinyals, et al. 2017. Neural discrete representation learning. Advances in neural information processing systems 30 (2017). [10] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. 2017. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196 (2017). [11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018). [12] Tiezheng Ge, Liqin Zhao, Guorui Zhou, Keyu Chen, Shuying Liu, Huimin Yi, Zelin Hu, Bochao Liu, Peng Sun, Haoyu Liu, et al. 2018. Image matters: Visually modeling user behaviors using advanced model server. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management . 2087-2095. [13] Vincent Fran√ßois-Lavet, Peter Henderson, Riashat Islam, Marc G Bellemare, Joelle Pineau, et al. 2018. An introduction to deep reinforcement learning. Foundations and Trends¬Æ in Machine Learning 11, 3-4 (2018), 219-354. [14] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. 2019. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems 32 (2019). [15] Tero Karras, Samuli Laine, and Timo Aila. 2019. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition . 4401-4410. [16] Zhichen Zhao, Lei Li, Bowen Zhang, Meng Wang, Yuning Jiang, Li Xu, Fengkun Wang, and Weiying Ma. 2019. What you look matters? offline evaluation of advertising creatives for cold-start problem. In Proceedings of the 28th ACM international conference on information and knowledge management . 2605-2613. [17] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. 2020. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition . 8110-8119. [18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. Advances in neural information processing systems 33 (2020), 6840-6851. [19] Sreekanth Vempati, Korah T Malayil, V Sruthi, and R Sandeep. 2020. Enabling hyper-personalisation: Automated ad creative generation and ranking for fashion e-commerce. In Fashion Recommender Systems . Springer, 25-48. [20] Shaunak Mishra, Manisha Verma, Yichao Zhou, Kapil Thadani, and Wei Wang. 2020. Learning to create better ads: Generation and ranking approaches for ad creative refinement. In Proceedings of the 29th ACM international conference on information & knowledge management . 2653-2660. [21] Tero Karras, Miika Aittala, Samuli Laine, Erik H√§rk√∂nen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. 2021. Alias-free generative adversarial networks. Advances in Neural Information Processing Systems 34 (2021), 852-863. [22] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. 2021. Cogview: Mastering text-to-image generation via transformers. Advances in Neural Information Processing Systems 34 (2021), 19822-19835. [23] Prafulla Dhariwal and Alexander Nichol. 2021. Diffusion models beat gans on image synthesis. Advances in neural information processing systems 34 (2021), 8780-8794. [24] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. 2021. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741 (2021). [25] Shiyao Wang, Qi Liu, Tiezheng Ge, Defu Lian, and Zhiqiang Zhang. 2021. A hybrid bandit model with visual priors for creative ranking in display advertising. In Proceedings of the web conference 2021 . 2324-2334. [26] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning . PMLR, 8748-8763. [27] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021). [28] Axel Sauer, Kashyap Chitta, Jens M√ºller, and Andreas Geiger. 2021. Projected gans converge faster. Advances in Neural Information Processing Systems 34 (2021), 17480-17492. [29] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. 2021. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision . 10012-10022. [30] Patrick Esser, Robin Rombach, and Bjorn Ommer. 2021. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition . 12873-12883. [31] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image generation. In International Conference on Machine Learning . PMLR, 8821-8831. [32] Yiqi Gao, Xinglin Hou, Yuanmeng Zhang, Tiezheng Ge, Yuning Jiang, and Peng Wang. 2022. Caponimage: Context-driven dense-captioning on image. arXiv preprint arXiv:2204.12974 (2022). [33] Min Zhou, Chenchen Xu, Ye Ma, Tiezheng Ge, Yuning Jiang, and Weiwei Xu. 2022. Composition-aware graphic layout GAN for visual-textual presentation designs. arXiv preprint arXiv:2205.00303 (2022). [34] Jihyeong Ko, Jisu Jeong, and Kyumgmin Kim. 2022. Contrastive Learning for Topic-Dependent Image Ranking. In Workshop on Recommender Systems in Fashion and Retail . Springer, 79-98. [35] Shiyao Wang, Qi Liu, Yicheng Zhong, Zhilong Zhou, Tiezheng Ge, Defu Lian, and Yuning Jiang. 2022. CreaGAN: An Automatic Creative Generation Framework for Display Advertising. In Proceedings of the 30th ACM International Conference on Multimedia . 7261-7269. [36] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125 1, 2 (2022), 3. [37] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition . 10684-10695. [38] Xuebin Qin, Hang Dai, Xiaobin Hu, Deng-Ping Fan, Ling Shao, and Luc Van Gool. 2022. Highly accurate dichotomous image segmentation. In European Conference on Computer Vision . Springer, 38-56. [39] Kaiyi Lin, Xiang Zhang, Feng Li, Pengjie Wang, Qingqing Long, Hongbo Deng, Jian Xu, and Bo Zheng. 2022. Joint Optimization of Ad Ranking and Creative Selection. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval . 2341-2346. [40] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. 2022. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems 35 (2022), 36479-36494. [41] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. 2022. Resolution-robust large mask inpainting with fourier convolutions. In Proceedings of the IEEE/CVF winter conference on applications of computer vision . 2149-2159. [42] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. 2022. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789 2, 3 (2022), 5. [43] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems 35 (2022), 27730-27744. Conference acronym 'XX, June 03-05, 2018, Woodstock, NY [44] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. 2023. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 22511-22521. [45] James Betker, Gabriel Goh, and et.al. Jing. 2023. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf (2023). [46] Naoto Inoue, Kotaro Kikuchi, Edgar Simo-Serra, Mayu Otani, and Kota Yamaguchi. 2023. LayoutDM: Discrete Diffusion Model for Controllable Layout Generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 10167-10176. [47] Leigang Qu, Shengqiong Wu, Hao Fei, Liqiang Nie, and Tat-Seng Chua. 2023. LayoutLLM-T2I: Eliciting Layout Guidance from LLM for Text-to-Image Generation. arXiv preprint arXiv:2308.05095 (2023). [48] Simo Ryu. 2023. Low-rank adaptation for fast text-to-image diffusion fine-tuning. Hao Yang, Jianxin Yuan, Shuai Yang, Linhe Xu, Shuo Yuan, and Yifan Zeng [49] HaoYang, Ziliang Wang, Weijie Bian, and Yifan Zeng. 2023. Practice on Effectively Extracting NLP Features for Click-Through Rate Prediction. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management . 4887-4893. [50] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. 2023. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301 (2023). [51] Sheng You, Chao Wang, Baohua Wu, Jingping Liu, Quan Lu, Guanzhou Han, and Yanghua Xiao. 2023. What Image do You Need? A Two-stage Framework for Image Selection in E-commerce. In Companion Proceedings of the ACM Web Conference 2023 . 452-456. [52] Zhiwei Tang, Dmitry Rybin, and Tsung-Hui Chang. 2023. Zeroth-Order Optimization Meets Human Feedback: Provable Learning via Ranking Oracles. arXiv preprint arXiv:2303.03751 (2023).",
  "keywords_parsed": [
    "Stable Diffusion",
    "Creative Generation",
    "Prompt and Reward Models"
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "Learning from delayed rewards"
    },
    {
      "ref_id": "b2",
      "title": "Reducing the dimensionality of data with neural networks"
    },
    {
      "ref_id": "b3",
      "title": "Auto-encoding variational bayes"
    },
    {
      "ref_id": "b4",
      "title": "Generative adversarial nets"
    },
    {
      "ref_id": "b5",
      "title": "U-net: Convolutional networks for biomedical image segmentation"
    },
    {
      "ref_id": "b6",
      "title": "Generative adversarial text to image synthesis"
    },
    {
      "ref_id": "b7",
      "title": "Attention is all you need"
    },
    {
      "ref_id": "b8",
      "title": "DeepFM: a factorization-machine based neural network for CTR prediction"
    },
    {
      "ref_id": "b9",
      "title": "Neural discrete representation learning"
    },
    {
      "ref_id": "b10",
      "title": "Progressive growing of gans for improved quality, stability, and variation"
    },
    {
      "ref_id": "b11",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding"
    },
    {
      "ref_id": "b12",
      "title": "Image matters: Visually modeling user behaviors using advanced model server"
    },
    {
      "ref_id": "b13",
      "title": "An introduction to deep reinforcement learning"
    },
    {
      "ref_id": "b14",
      "title": "Generating diverse high-fidelity images with vq-vae-2"
    },
    {
      "ref_id": "b15",
      "title": "A style-based generator architecture for generative adversarial networks"
    },
    {
      "ref_id": "b16",
      "title": "What you look matters? offline evaluation of advertising creatives for cold-start problem"
    },
    {
      "ref_id": "b17",
      "title": "Analyzing and improving the image quality of stylegan"
    },
    {
      "ref_id": "b18",
      "title": "Denoising diffusion probabilistic models"
    },
    {
      "ref_id": "b19",
      "title": "Enabling hyper-personalisation: Automated ad creative generation and ranking for fashion e-commerce"
    },
    {
      "ref_id": "b20",
      "title": "Learning to create better ads: Generation and ranking approaches for ad creative refinement"
    },
    {
      "ref_id": "b21",
      "title": "Alias-free generative adversarial networks"
    },
    {
      "ref_id": "b22",
      "title": "Cogview: Mastering text-to-image generation via transformers"
    },
    {
      "ref_id": "b23",
      "title": "Diffusion models beat gans on image synthesis"
    },
    {
      "ref_id": "b24",
      "title": "Glide: Towards photorealistic image generation and editing with text-guided diffusion models"
    },
    {
      "ref_id": "b25",
      "title": "A hybrid bandit model with visual priors for creative ranking in display advertising"
    },
    {
      "ref_id": "b26",
      "title": "Learning transferable visual models from natural language supervision"
    },
    {
      "ref_id": "b27",
      "title": "Lora: Low-rank adaptation of large language models"
    },
    {
      "ref_id": "b28",
      "title": "Projected gans converge faster"
    },
    {
      "ref_id": "b29",
      "title": "Swin transformer: Hierarchical vision transformer using shifted windows"
    },
    {
      "ref_id": "b30",
      "title": "Taming transformers for high-resolution image synthesis"
    },
    {
      "ref_id": "b31",
      "title": "Zero-shot text-to-image generation"
    },
    {
      "ref_id": "b32",
      "title": "Caponimage: Context-driven dense-captioning on image"
    },
    {
      "ref_id": "b33",
      "title": "Composition-aware graphic layout GAN for visual-textual presentation designs"
    },
    {
      "ref_id": "b34",
      "title": "Contrastive Learning for Topic-Dependent Image Ranking"
    },
    {
      "ref_id": "b35",
      "title": "CreaGAN: An Automatic Creative Generation Framework for Display Advertising"
    },
    {
      "ref_id": "b36",
      "title": "Hierarchical text-conditional image generation with clip latents"
    },
    {
      "ref_id": "b37",
      "title": "High-resolution image synthesis with latent diffusion models"
    },
    {
      "ref_id": "b38",
      "title": "Highly accurate dichotomous image segmentation"
    },
    {
      "ref_id": "b39",
      "title": "Joint Optimization of Ad Ranking and Creative Selection"
    },
    {
      "ref_id": "b40",
      "title": "Photorealistic text-to-image diffusion models with deep language understanding"
    },
    {
      "ref_id": "b41",
      "title": "Resolution-robust large mask inpainting with fourier convolutions"
    },
    {
      "ref_id": "b42",
      "title": "Scaling autoregressive models for content-rich text-to-image generation"
    },
    {
      "ref_id": "b43",
      "title": "Training language models to follow instructions with human feedback"
    },
    {
      "ref_id": "b44",
      "title": "Gligen: Open-set grounded text-to-image generation"
    },
    {
      "ref_id": "b45",
      "title": "Improving image generation with better captions"
    },
    {
      "ref_id": "b46",
      "title": "LayoutDM: Discrete Diffusion Model for Controllable Layout Generation"
    },
    {
      "ref_id": "b47",
      "title": "LayoutLLM-T2I: Eliciting Layout Guidance from LLM for Text-to-Image Generation"
    },
    {
      "ref_id": "b48",
      "title": "Low-rank adaptation for fast text-to-image diffusion fine-tuning"
    },
    {
      "ref_id": "b49",
      "title": "Practice on Effectively Extracting NLP Features for Click-Through Rate Prediction"
    },
    {
      "ref_id": "b50",
      "title": "Training diffusion models with reinforcement learning"
    },
    {
      "ref_id": "b51",
      "title": "What Image do You Need? A Two-stage Framework for Image Selection in E-commerce"
    },
    {
      "ref_id": "b52",
      "title": "Zeroth-Order Optimization Meets Human Feedback: Provable Learning via Ranking Oracles"
    }
  ]
}