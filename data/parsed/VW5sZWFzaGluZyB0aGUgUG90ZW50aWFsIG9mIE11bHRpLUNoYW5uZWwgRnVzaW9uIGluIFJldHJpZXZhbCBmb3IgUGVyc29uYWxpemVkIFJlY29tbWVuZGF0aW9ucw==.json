{
  "Unleashing the Potential of Multi-Channel Fusion in Retrieval for Personalized Recommendations": "Junjie Huang Shanghai Jiao Tong University China huangjunjie2019@sjtu.edu.cn Jianghao Lin Shanghai Jiao Tong University China chiangel@sjtu.edu.cn",
  "â€ ": "",
  "ABSTRACT": "",
  "Jiarui Qin": "Shanghai Jiao Tong University China qjr1996@sjtu.edu.cn Yong Yu Shanghai Jiao Tong University China yyu@sjtu.edu.cn Weinan Zhang Shanghai Jiao Tong University China wnzhang@sjtu.edu.cn",
  "KEYWORDS": "Recommender systems (RS) are pivotal in managing information overload in modern digital services. A key challenge in RS is efficiently processing vast item pools to deliver highly personalized recommendations under strict latency constraints. Multi-stage cascade ranking addresses this by employing computationally efficient retrieval methods to cover diverse user interests, followed by more precise ranking models to refine the results. In the retrieval stage, multi-channel retrieval is often used to generate distinct item subsets from different candidate generators, leveraging the complementary strengths of these methods to maximize coverage. However, forwarding all retrieved items overwhelms downstream rankers, necessitating truncation. Despite advancements in individual retrieval methods, multi-channel fusion , the process of efficiently merging multi-channel retrieval results, remains underexplored. We are the first to identify and systematically investigate multi-channel fusion in the retrieval stage. Current industry practices often rely on heuristic approaches and manual designs, which often lead to suboptimal performance. Moreover, traditional gradient-based methods like SGD are unsuitable for this task due to the non-differentiable nature of the selection process. In this paper, we explore advanced channel fusion strategies by assigning systematically optimized weights to each channel. We utilize black-box optimization techniques, including the Cross Entropy Method and Bayesian Optimization for global weight optimization, alongside policy gradient-based approaches for personalized merging. Our methods enhance both personalization and flexibility, achieving significant performance improvements across multiple datasets and yielding substantial gains in real-world deployments, offering a scalable solution for optimizing multi-channel fusion in retrieval.",
  "CCS CONCEPTS": "Â· Information systems â†’ Recommender systems . â€  Corresponding authors. Conference'17, July 2017, Washington, DC, USA Â© 2025 Association for Computing Machinery. This is the author's version of the work. It is posted here for your personal use. Not for redistribution. The definitive Version of Record was published in Proceedings of ACM Conference (Conference'17) , https://doi.org/XXXXXXX.XXXXXXX. Recommender Systems, Retrieval, Multi-Channel Fusion",
  "ACMReference Format:": "Junjie Huang, Jiarui Qin, Jianghao Lin, Ziming Feng â€  , Yong Yu, and Weinan Zhang â€  . 2025. Unleashing the Potential of Multi-Channel Fusion in Retrieval for Personalized Recommendations. In Proceedings of ACM Conference (Conference'17). ACM, New York, NY, USA, 12 pages. https://doi.org/XXXXXXX. XXXXXXX",
  "1 INTRODUCTION": "In the era of information overload, recommender systems (RS) have become indispensable in modern web services, ranging from video streaming platforms to online shopping services. One of the main technical challenges in RS is to efficiently process billions of items to provide personalized experiences to millions of users under strict latency restrictions [25, 34]. As shown in Figure 1, a widely used solution is multi-stage cascade ranking systems [11, 14, 60]. In the first stage of the cascade system, known as the retrieval stage (also called matching or recall stage [43, 70]), a group of computationally efficient candidate generators selects a small set of candidates. These candidates are then further filtered, ranked, and ultimately presented to the user by slower but more accurate rankers. In this process, the retrieval stage acts as both the cornerstone and bottleneck of the RS. Without effective retrieval, even the most advanced ranking algorithms cannot perform optimally. Typically, multi-channel retrieval [27, 37] is essential for efficiently and effectively retrieving items from large-scale item pools, as shown in Figure 1. Topğ¾ items from each channel are merged and passed to the next stage, with ğ¾ varying across channels. The underlying reason is that forwarding all retrieved items from each channel would overwhelm downstream rankers, necessitating truncation. Therefore, the primary challenge in multi-channel retrieval lies in effectively merging the diverse items retrieved by each candidate generator. This process involves determining the appropriate ğ¾ for each channel's topğ¾ selection or assigning optimal weights to each channel during the merging process. For more details on why multi-channel retrieval is favored over single-channel and the rationale behind weight assignment, refer to Sections 2 and 3.2. Despite advancements in individual retrieval methods [11, 23, 36], the task of efficiently merging multi-channel retrieval results, Conference'17, July 2017, Washington, DC, USA Junjie Huang et al. Figure 1: Up: Illustration of multi-stage cascade ranking and multi-channel retrieval in recommender systems. Bottom: Performance variations with different weight combinations on Gowalla (left) and Amazon_Books (right). Item Pool Retrieval Stage Item Pool Collaborative Filtering Popularity Neural Methods Other Channel Top Multi-Channel Retrieval Pre-Ranking Ranking User Candidate Item Set Re-Ranking Subsequent Ranking Stage Top Top Top which we define as multi-channel fusion , has received limited attention. We identify three key challenges in multi-channel fusion: Â· (C1) Current industry practices often rely on heuristic approaches and manual designs, such as snake-merge or simple quota mechanisms [37], guided by business needs. These methods lack systematic analysis, leading to suboptimal performance and a poorer user experience. Additionally, existing simple quota mechanisms are inflexible and fail to accommodate personalization, where different users may benefit from varying weight assignments. Â· (C2) The performance of multi-channel fusion is highly sensitive to weight combinations. Figure 1 demonstrates the performance variations across different weight combinations on two public datasets: Gowalla and Amazon_Books. We implement nine retrieval channels and observe significant fluctuations in precision and recall by adjusting weight combinations, while keeping the retrieved items from each channel constant. On Gowalla and Amazon_Books, random selection of eight weight combinations results in Recall@200 variations of up to 79.7% and 86.7%, respectively. This underscores the critical need for better optimized weights, which we will discuss further in Section 3. Â· (C3) Traditional gradient-based methods are unsuitable for this task due to the non-differentiable selection process in multichannel fusion, complicating weight optimization strategies. In this paper, we formulate the task of multi-channel fusion in retrieval, laying a cornerstone for future research. We conduct comprehensive analysis and validation, introducing methods for effective multi-channel fusion, unlocking its potential in the retrieval stage to enhance personalized recommendations. Our approach consists of two parts. First, we explore assigning globally unified weights, where weight combinations remain consistent for all users, reflecting current industry practices. We model this problem as a black-box optimization task, where the input consists of weight combinations, and the output is the corresponding retrieval performance. We adopt a two-stage exploration method. In the first stage, the Cross Entropy Method [48] iteratively refines the weight distribution to converge on near-optimal solutions. In the second stage, Bayesian Optimization [15] refines this solution by building a probabilistic model to predict retrieval performance, allowing more efficient exploration of the local search space. In the second part, we shift from assigning globally unified weights to personalized weights, as users exhibit diverse preferences and behaviors. To optimize this personalized merging process and tackle the non-differentiable selection process of multi-channel fusion, we utilize a policy gradient approach from Reinforcement Learning [62, 65]. These methods go beyond conventional heuristics, paving the way for more intelligent, scalable, and adaptive RS, advancing the frontier of personalized recommendations. In summary, the contributions of this paper are as follows: Â· We are the first to define the challenge of multi-channel fusion in retrieval and demonstrate that systematically optimized weight assignments greatly improve personalized recommendations. Â· We propose a two-stage optimization strategy using black-box optimization techniques for non-personalized weight assignment, achieving state-of-the-art (SOTA) performance. â€¢ We introduce a policy gradient-based method for personalized merging, enabling more dynamic and tailored recommendations. Â· Extensive experiments on three large-scale, real-world datasets validate the superiority of our approach over current baselines. Moreover, we successfully deploy our method in the recommender system at Company X, resulting in a significant improvement in performance and user experience.",
  "2 BACKGROUND": "Multi-Stage Cascade Ranking System. In modern information retrieval systems, multi-stage cascade ranking is commonly employed [60] to balance efficiency and effectiveness, as illustrated in Figure 1. While complex models [41, 42] often deliver higher accuracy, their inefficiency makes online deployment challenging due to latency constraints [40]. In contrast, simpler models [28, 44] are less powerful but can efficiently process a large number of items because of their low time complexity. Typically, the system consists of a set of candidate generators and various rankers, structured like a funnel that narrows from bottom to top. Each stage selects the topğ¾ items and passes them to the next. On the left side of Figure 1, we show the approximate output size for each stage. Retrieval Strategy. Retrieval strategies operate as high-level frameworks and can be classified into (1) non-personalized and (2) personalized retrieval. A common non-personalized strategy is promoting popular items, following the 'wisdom of the crowd' [57]. Personalized strategies include U2I and I2I, where U2I links the target user with items they might like directly, while I2I finds items similar to those the user has interacted with. Each strategy provides a distinct approach to discovering items of interest for users. Multi-ChannelRetrieval. Multi-channel retrieval [27, 37] is widely adopted in RS, employing independent candidate generators to retrieve distinct item subsets separately [11, 19]. These candidate Unleashing the Potential of Multi-Channel Fusion in Retrieval for Personalized Recommendations Conference'17, July 2017, Washington, DC, USA generators are diverse, utilizing techniques such as associative rules and neural networks, with common methods including matrix factorization [30] and two-tower architectures [64]. As illustrated in Figure 1, the retrieved item subsets are combined to create a comprehensive candidate pool for subsequent ranking stage. The main objective is to expand coverage of users' diverse interests and improve recall rates through various retrieval methods [66], capturing a broad range of user preferences and enhancing performance.",
  "3 PRELIMINARIES": "",
  "3.1 Problem Formulation": "In this section, we formulate the problem and introduce key notations. Given multiple ranked lists generated by different retrieval channels for each user, the goal is to merge these lists into a unified recommendation set. Let U and I denote the sets of users and items, and ğ¾ represent the total number of retrieval channels. Each channel ğ‘˜ provides a ranked list L ğ‘¢ğ‘˜ for user ğ‘¢ âˆˆ U , where L ğ‘¢ğ‘˜ âŠ† I . The objective is to construct the final recommendation set R ğ‘¢ for each user by selecting top-ranked items from these lists based on a set of weights, with |R ğ‘¢ | = ğ¿ , representing a fixed number of items delivered to the subsequent ranking stage. We summarize the notations in Table 4 in Appendix A. We will now detail the merging strategies, constraints, and optimization objectives. Merging Strategies: Merging can be either non-personalized (globally unified) or personalized, depending on whether the weights assigned to each retrieval channel are the same for all users or individualized for each user. In the non-personalized case, each retrieval channel ğ‘˜ is assigned a global weight ğ‘¤ ğ‘˜ . For each channel, we select the top nearest_int ( ğ‘¤ ğ‘˜ Ã— ğ¿ ) items from L ğ‘¢ğ‘˜ , forming subsets L ( ğ‘¤ ğ‘˜ ) ğ‘¢ğ‘˜ . The final recommendation set R ğ‘¢ for user ğ‘¢ is the union of these selected subsets from all ğ¾ channels, ensuring no duplicate items, as shown in Equation (1.1).  In the personalized case, weights ğ‘¤ ğ‘¢ğ‘˜ vary by user, allowing for a more customized retrieval process. The top nearest_int ( ğ‘¤ ğ‘¢ğ‘˜ Ã— ğ¿ ) items from each list L ğ‘¢ğ‘˜ are selected to form L ( ğ‘¤ ğ‘¢ğ‘˜ ) ğ‘¢ğ‘˜ , and the final recommendation set R ğ‘¢ is given by Equation (1.2). Constraints: (1) Weight Normalization: Weights ğ‘¤ ğ‘˜ for each channel must satisfy Equation (2.1) in the non-personalized case; weights ğ‘¤ ğ‘¢ğ‘˜ for each user ğ‘¢ must satisfy Equation (2.2) in the personalized case. (2) Weight Bounds: Weights also have bounds as shown in Equation (3), ensuring no channel is over- or underrepresented, reflecting practical requirements in specific scenarios.   Optimization Objectives: To optimize the weights ğ‘¤ ğ‘˜ or ğ‘¤ ğ‘¢ğ‘˜ , the goal is to maximize the average evaluation metric across all users, where T ğ‘¢ is the ground truth set of relevant items for user ğ‘¢ , and Eval (R ğ‘¢ , T ğ‘¢ ) represents the evaluation metric.",
  "3.2 Rationale Behind Weight Assignment": "Figure 1 illustrates how different weight combinations can significantly impact performance of multi-channel fusion. We now explore the rationale for assigning varying weights to the retrieved subsets from different candidate generators. Figure 2 shows our findings on the diversity of candidate generators from multiple perspectives. We implement nine retrieval channels on the Amazon_Books dataset, including associative rule-based methods such as Pop, ItemKNN [51], UserKNN [46], and neural network-based methods like BPR [45], NeuMF [24], SimpleX [36], and LightGCN [23] (detailed in Appendix C). U2I and I2I retrieval strategies are applied for both SimpleX and LightGCN. Each candidate generator retrieves 200 items per user. For items, we measure the pairwise Jaccard similarity [38] between channels, averaged across users:  where |L ğ‘¢ğ‘˜ 1 âˆ© L ğ‘¢ğ‘˜ 2 | is the number of common items, and |L ğ‘¢ğ‘˜ 1 âˆª L ğ‘¢ğ‘˜ 2 | represents the total unique items. A lower Jaccard score indicates higher diversity across channels. For users, we rank them for each channel based on recall scores, forming a user ranking list U ğ‘˜ . Rank-Biased Overlap (RBO) similarity [61] between the user rankings from two retrieval channels ğ‘˜ 1 and ğ‘˜ 2 is formulated as:  where ğ‘ is the persistence parameter ( ğ‘ =0.9), controlling emphasis on top-ranked users, and |U ( ğ‘‘ ) ğ‘˜ 1 âˆ© U ( ğ‘‘ ) ğ‘˜ 2 | represents the overlap of users at depth ğ‘‘ . RBO ranges from 0 (no overlap) to 1 (identical rankings). By computing RBO for all channel pairs, we can evaluate how similarly each channel ranks users. Figure 2 visualizes Jaccard and RBO similarity matrices, where most channels exhibit low overlap, indicating (1) effective multi-channel fusion is crucial as no single channel covers all user interests, and (2) personalized weight assignment is necessary since different channels perform well for different users, supporting argument in Section 1.",
  "4 METHODOLOGY": "In this section, we explore effective multi-channel fusion strategies in retrieval, starting with globally unified methods, followed by personalized approaches. We present our main idea in Figure 3.",
  "4.1 Globally Unified Weight Assignment": "In the non-personalized case, the challenge lies in determining the optimal weights for each retrieval channel to maximize the overall performance. Since the objective function, such as overall recall, lacks an explicit mathematical form describing how the weights influence the results, this makes it well-suited for black-box optimization, where the objective is evaluated based on sampled weights without requiring gradient information or predefined problem structure. We adopt a two-phase optimization strategy, which Conference'17, July 2017, Washington, DC, USA Junjie Huang et al. Figure 2: Diversity among various candidate generators from both item and user perspectives on Amazon_Books. 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 1.0 0.0 0.0 0.1 0.2 0.0 0.1 0.0 0.0 0.0 1.0 0.3 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.3 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.1 0.0 0.0 1.0 0.2 0.1 0.1 0.1 0.0 0.2 0.0 0.0 0.2 1.0 0.1 0.1 0.0 0.0 0.0 0.0 0.0 0.1 0.1 1.0 0.2 0.2 0.1 0.1 0.0 0.0 0.1 0.1 0.2 1.0 0.1 0.1 0.0 0.0 0.0 0.1 0.0 0.2 0.1 1.0 0.2 0.0 0.0 0.0 0.0 0.0 0.1 0.1 0.2 1.0 (a) Jaccard Similarity Between Items Retrieved by Various Candidate Generators 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.5 0.4 0.3 0.3 0.3 0.0 0.0 0.0 0.5 1.0 0.4 0.2 0.7 0.6 0.0 0.0 0.0 0.4 0.4 1.0 0.3 0.3 0.3 0.0 0.0 0.0 0.3 0.2 0.3 1.0 0.1 0.1 0.0 0.0 0.0 0.3 0.7 0.3 0.1 1.0 0.7 0.0 0.0 0.0 0.3 0.6 0.3 0.1 0.7 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 (b) RBO Similarity Between User Rankings by Various Candidate Generators 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 1: Pop 2: ItemKNN 3: UserKNN 4: BPR 5: NeuMF 6: SimpleX (U2I) 7: LightGCN (U2I) 8: SimpleX (I2I) 9: LightGCN (I2I) ensures both a broad exploration of the solution space and a more targeted fine-tuning of the best-performing weights. We provide a detailed pseudocode of the training process in Appendix B. 4.1.1 Cross Entropy Method. In the first phase, we apply the Cross Entropy Method (CEM), a stochastic optimization technique, to explore the global weight space. Originally introduced by Rubinstein [49] for rare-event probability estimation, CEM uses KullbackLeibler divergence to update the sampling distribution. It was later adapted for optimization [48, 50], with the search for optimal solutions treated as a rare-event estimation task. CEM iteratively refines the distribution to increase the likelihood of generating nearoptimal solutions. Since the weights of various retrieval channels must sum to one, we model the weight vector using the Dirichlet distribution [1]. CEM operates in iterative steps, as outlined below: Initialization and Sampling: We initialize the Dirichlet distribution with parameters ğœ¶ ( 0 ) = [ ğ›¼ 1 , ğ›¼ 2 , . . . , ğ›¼ ğ¾ ] âŠ¤ , where each ğ›¼ ğ‘˜ represents the concentration of weight for retrieval channel ğ‘˜ . The Dirichlet distribution enforces the constraint that weights sum to one. In each iteration, we sample ğ‘„ weight vectors w 1 , w 2 , . . . , w ğ‘„ from the current Dirichlet distribution:  The probability density function (PDF) of the Dirichlet distribution for a vector w = [ ğ‘¤ 1 , ğ‘¤ 2 , . . . , ğ‘¤ ğ¾ ] âŠ¤ , with the concentration parameter ğœ¶ = [ ğ›¼ 1 , ğ›¼ 2 , . . . , ğ›¼ ğ¾ ] âŠ¤ , is defined as:  where Î“ (Â·) is the Gamma function. These samples represent different possible weight combinations across the retrieval channels. Performance Evaluation: For each sampled weight vector w ğ‘– , we compute the retrieval performance ğ‘† ( w ğ‘– ) using a metric like expected recall. This metric acts as a proxy for how well the weights enhance retrieval results. The performance is evaluated for all samples without requiring an explicit objective function. Selecting Elite Samples: After evaluating all ğ‘„ samples, we rank them in descending order and select the top ğ‘ -percentile as the elite set. The performance threshold Ë† ğ›¾ ğ‘¡ is the score of the lowestranked sample in the elite set, where ğ‘„ ğ‘’ = âŒˆ ğ‘ğ‘„ âŒ‰ is the number of Globally Unified Multi-Channel Fusion. (Same Weights for All Users) (a) Popularity Neural Methods Other Channel Iteration T Iteration 1 Sampled Weights AlphaGenerator Performance Evaluation State Representation Feedback Final Weights Personalized for Target User AlphaGenerator Evaluation Training Channel Channel Channel Target User Channel Info Recall Score Update with Feedback Channel Info Target User Recall Score State Representation Merged Candidates Merged Candidates Iteration T-1 Final Weights Shared by Users User-Specific Multi-Channel Fusion. (Personalized Weights for Different Users) (b) Figure 3: An illustration of our non-personalized and personalized multi-channel fusion strategies in the retrieval stage. elite samples. All samples with ğ‘† ( wi ) â‰¥ Ë† ğ›¾ ğ‘¡ are retained.  ParameterUpdate(Cross-EntropyStep): Weiteratively refine the weight distribution to focus on better-performing solutions by updating ğœ¶ at each iteration. In Equation (10), the new parameters ğœ¶ âˆ— maximize the likelihood of generating these elite samples, where I ( ğ‘† ( w ğ‘– ) â‰¥ Ë† ğ›¾ ğ‘¡ ) is an indicator function that selects the elite samples.  Once ğœ¶ âˆ— is found, the parameters are smoothly updated using a learning rate ğœ‚ 1. This weighted average gradually shifts the distribution toward elite samples while maintaining stability.  4.1.2 Bayesian Optimization. After the global exploration with CEM,werefine the solution using Bayesian Optimization (BayesOpt), which fine-tunes the Dirichlet distribution's parameters in a constrained search space. Specifically, the search space for parameter ğœ· is set to the range [ 0 . 5 ğœ¶ ( ğ‘¡ ) , 1 . 5 ğœ¶ ( ğ‘¡ ) ] , where ğœ¶ ( ğ‘¡ ) is the result from the CEM stage, ensuring that the optimization remains focused on promising regions. BayesOpt has two key components [15, 52]: Surrogate Model: AGaussian Process (GP) models the objective function ğ‘† (Â·) , such as the expected recall. The GP provides both predictions and uncertainty estimates for unexplored regions:  where ğœ‡ (Â· is the predicted mean, and ğ‘˜ (Â·) is the covariance function. Acquisition Function: This function selects the next sample by balancing exploration and exploitation. The next Dirichlet parameters are chosen to maximize expected improvement (EI) in retrieval performance, with ğ‘† best representing the best performance so far.  The process iteratively refines ğœ· to converge toward optimal parameters. Once ğœ· is determined, the final step is to derive the optimal Unleashing the Potential of Multi-Channel Fusion in Retrieval for Personalized Recommendations Conference'17, July 2017, Washington, DC, USA weight vector w . Since ğœ· parameterizes a Dirichlet distribution, the optimal weights are the expected value of the distribution.",
  "4.2 Personalized Weight Assignment": "Globally unified weights provide a general solution but overlook individual user preferences. Personalized fusion are essential, as users benefit from different retrieval combinations based on their unique behaviors and preferences. Due to the non-differentiable nature of the selection process in multi-channel fusion, traditional gradient-based methods like SGD are unsuitable. To address this, we employ a policy gradient approach (PG) from Reinforcement Learning [62, 65] to optimize the merging strategy. We model the weight assignment as a policy that generates a probability distribution over possible weights for each user. This policy, parameterized by a neural network, takes as input the user representation ğ’– , the recall scores from each retrieval channel ğ’“ ğ‘¢ = [ ğ‘Ÿ ğ‘¢ 1 , ğ‘Ÿ ğ‘¢ 2 , . . . , ğ‘Ÿ ğ‘¢ğ¾ ] âŠ¤ , and the retrieval channel representations { ğ’„ ğ‘¢ğ‘˜ } ğ¾ ğ‘˜ = 1 . These components together constitute the state ğ‘  ğ‘¢ for user ğ‘¢ :  Our policy outputs the parameters ğœ¶ ğ‘¢ of a Dirichlet distribution for each user ğ‘¢ , which determines the weight distribution w ğ‘¢ . 4.2.1 Model Architecture. During forward propagation, we compute Dirichlet parameters ğœ¶ ğ‘¢ for each user, which are then used to sample weights w ğ‘¢ for merging retrieval results. Let ğ’– âˆˆ R ğ‘‘ , ğ’„ ğ‘¢ğ‘˜ âˆˆ R ğ‘‘ , ğ‘Ÿ ğ‘¢ âˆˆ R ğ¾ , and â„ represent the hidden dimension size. After training the single-channel models, ğ‘Ÿ ğ‘¢ remains a fixed constant. In our method, ğ’– is the user representation generated from one of the pre-trained retrieval models, and ğ’„ ğ‘¢ğ‘˜ is obtained by pooling the topğ‘š item representations from channel ğ‘˜ of the same model. Since the retrieval results vary for each user, the channel representations ğ’„ ğ‘¢ğ‘˜ are user-dependent. First, we apply linear transformations followed by ReLU activations to both the user representations and each channel's representations, where ğ’‰ ğ‘¢ âˆˆ R â„ , ğ’‰ ğ‘ ğ‘¢ğ‘˜ âˆˆ R â„ .  Here, W ğ‘¢ âˆˆ R â„ Ã— ğ‘‘ , ğ’ƒ ğ‘¢ âˆˆ R â„ , W ğ‘ âˆˆ R â„ Ã— ğ‘‘ and ğ’ƒ ğ‘ âˆˆ R â„ , are learnable parameters. Next, we compute the dot product between the transformed user and channel representations to model user preference toward each channel ğ‘£ ğ‘¢ğ‘˜ âˆˆ R :  Wecombine attention scores with user recall scores from each channel to generate combined scores ğ’† ğ‘¢ âˆˆ R ğ¾ . ğœ¶ ğ‘¢ âˆˆ R ğ¾ are computed using a scaled hyperbolic tangent activation, with ğ›¿ max controlling the maximum adjustment. To ensure ğœ¶ ğ‘¢ remains positive, we apply a ReLU activation and add a small constant ğœ– to avoid zero values. This entire process of generating ğœ¶ ğ‘¢ from state ğ‘  ğ‘¢ in Equation (15) is referred to as AlphaGenerator, as shown in Figure 3.  After computing ğœ¶ ğ‘¢ , we sample the weight vector w ğ‘¢ âˆˆ R ğ¾ from the Dirichlet distribution. These weights merge the retrieval results across channels for user ğ‘¢ , and the reward ğ‘… ( ğ‘  ğ‘¢ , w ğ‘¢ ) is calculated based on a performance metric of the merged results. During evaluation, we use Equation (14) to compute the expected value of the distribution, which serves as the final optimal weights w ğ‘¢ . 4.2.2 Objective Function. Our objective is to maximize the expected reward ğ½ ( ğœƒ ) , where ğœƒ represents the parameters of the neural network and ğ‘… ( ğ‘  ğ‘¢ , w ğ‘¢ ) is the reward obtained by applying weights w ğ‘¢ in state ğ‘  ğ‘¢ . The policy ğœ‹ ğœƒ ( w ğ‘¢ | ğ‘  ğ‘¢ ) is defined as a Dirichlet distribution parameterized by ğœ¶ ğ‘¢ , where ğœ¶ ğ‘¢ is computed from the neural network named AlphaGenerator based on the state ğ‘  ğ‘¢ .    To maximize ğ½ ( ğœƒ ) , we compute the gradient with respect to ğœƒ , as in Equation (22), using Monte Carlo sampling. For each user ğ‘¢ , we sample ğ‘† weight vectors { w ğ‘¢,ğ‘– } ğ‘† ğ‘– = 1 from the policy ğœ‹ ğœƒ ( w ğ‘¢ | ğ‘  ğ‘¢ ) and compute the corresponding rewards { ğ‘… ğ‘¢,ğ‘– } ğ‘† ğ‘– = 1 .  We define the loss function as the negative expected reward over all users to perform gradient ascent on ğ½ ( ğœƒ ) :  To prevent overfitting and encourage the learned weights to stay close to the global weights w global from Section 4.1, we add a regularization term that penalizes large deviations, with ğœ† controlling the penalty strength. The total loss function is a combination of the policy loss and the regularization term.",
  "5 EXPERIMENTS": "In this section, we detail our experimental settings and results on three large-scale public datasets. We evaluate our methods, including the Cross Entropy Method (CEM), Bayesian Optimization (BayesOpt), and the policy gradient approach (PG), against strong baseline models, demonstrating state-of-the-art performance. Conference'17, July 2017, Washington, DC, USA Junjie Huang et al. Table 1: Overall performance of various methods on three public datasets. The top two results in each column are highlighted to indicate SOTA performance. The strongest baseline is underlined, and relative improvement (RelImp) is reported. Table 2: Statistics of the experimental datasets.",
  "5.1 Dataset and Experimental Flow": "We use three real-world datasets: Gowalla 1 , Amazon_Books 2 , and Tmall 3 . Dataset statistics are shown in Table 2. Only users with at least 10 recorded behaviors are included [69]. We split the datasets into training, validation, and test sets in a 5:2:3 ratio based on timestamps. For each implicit feedback instance, we randomly select 100 negative samples for Gowalla and Amazon_Books, and 200 negative samples for Tmall. Further details on the datasets and implementation details can be found in Appendix C.1 and C.3.",
  "5.2 Baselines and Evaluation Metrics": "For each dataset, we implement nine retrieval channels, including associative rule-based methods such as Pop, ItemKNN [51], UserKNN [46], and neural network-based methods like BPR [45], NeuMF [24], SimpleX [36], and LightGCN [23]. For SimpleX and 1 https://snap.stanford.edu/data/loc-gowalla.html. 2 https://jmcauley.ucsd.edu/data/amazon/amazonbooks. 3 https://tianchi.aliyun.com/dataset/53. LightGCN, we apply both U2I and I2I retrieval strategies to retrieve distinct item subsets, enhancing diversity (see Appendix C.2 for details). Additionally, we implement two basic merging methods: the first is equal-weight merging, where all retrieval channels are assigned the same weight; the second is statistical merging, where weights are normalized based on the proportion of retrieved items clicked by users. In statistical merging, channels with higher performance are usually assigned greater weights. It simulates heuristic weighting methods commonly used in industry practices. To evaluate the effectiveness of different methods, we use Precision@L (P@L), Recall@L (R@L), and F-Measure@L (F1@L) metrics [32], as our focus is on the number of relevant items returned rather than specific ranking order. Using the notations in Table 4, we present the formulas for these metrics in Appendix C.5.",
  "5.3 Performance Comparison": "Table 1 presents the recommendation performance of different models in terms of precision, recall, and F-measure across three datasets, from which we have the following observations: Â· All three methods we propose significantly outperform existing baselines. Specifically, BayesOpt (non-personalized) and PG (personalized) improve upon the strongest baseline by 6.18% and 9.08% in R@200 on Gowalla and 6.12% and 10.43% on Ama-zon_Books. These results underscore two key contributions: (1) Unleashing the Potential of Multi-Channel Fusion in Retrieval for Personalized Recommendations Conference'17, July 2017, Washington, DC, USA our method offers a more effective merging strategy, and (2) personalized multi-channel fusion further enhances performance over the industry-standard globally unified weighting approach, emphasizing the importance of personalization. Â· Compared to rule-based methods like Pop, neural network-based approaches, particularly state-of-the-art models such as SimpleX [36] and LightGCN [23], demonstrate superior performance. Â· Even simple multi-channel merging methods, such as statistical merging with heuristic-based weight assignments, easily surpass the performance of the best single-channel retrieval models, demonstrating the effectiveness of multi-channel retrieval.",
  "5.4 In Depth Analysis": "(a) Optimal Weights on Amazon Books (b) Optimal Weights on Tmall 5.3% 5.8% 7.8% 4.8% 31.3% 24.1% 11.2% 8.4% 4.9% 7.1% 4.1% 4.9% 5.1% 40.7% 5.8% 5.3% 22.1% Pop ItemKNN UserKNN BPR NeuMF SimpleX (U2I) LightGCN (U2I) SimpleX (I2I) LightGCN (I2I) Figure 4: Optimal weights for various retrieval channels generated by Bayesian Optimization on Amazon Books and Tmall. Proportions below 2% are omitted for clarity. 5.4.1 Context Dependency. Figure 4 presents the global weights w optimized through BayesOpt. As shown, SimpleX (U2I), SimpleX (I2I), LightGCN (U2I), and LightGCN (I2I) account for the largest proportions, while the remaining five models contribute relatively less. Furthermore, we observe that even with the same nine retrieval models, the optimal weights vary across different datasets and scenarios. This highlights that the distribution of weights in multichannel retrieval is highly context-dependent, with no fixed rule dictating how much weight each model should carry. Figure 5: Effect of ğœ‰ on Gowalla and Amazon_Books. 0.00.10.20.30.40.50.60.70.80.91.0 0.60 0.65 0.70 0.75 0.80 0.85 0.90 P@200 (%) (a) Effect of on Gowalla CEM P@200 Bayes P@200 0.00.10.20.30.40.50.60.70.80.91.0 0.20 0.22 0.24 0.26 0.28 0.30 P@200 (%) (b) Effect of   on Amazon_Books CEM P@200 Bayes P@200 11.5 12.0 12.5 13.0 13.5 R@200 (%) CEM R@200 Bayes R@200 7.6 7.8 8.0 8.2 8.4 8.6 8.8 9.0 9.2 R@200 (%) CEM R@200 Bayes R@200 5.4.2 Globally Unified Weight Assignment. To further investigate the distribution parameters ğœ¶ optimized by CEM and BayesOpt, we introduce the following adjustment in Equation (26):  where ğœ¶ ( ğ‘¡ ) represents the optimal distribution parameters obtained from CEM or BayesOpt, and ğœ‰ varies from 0 to 1 with intervals of 0.1. Figure 5 illustrates how retrieval performance changes as ğœ‰ increases. We observe a steady improvement in performance as ğœ‰ grows, indicating that the global weights optimized by our methods are well-founded and not a result of random chance. 5.4.3 Personalized Weight Assignment. We now provide a detailed analysis of our personalized merging strategy PG. Hyperparameter Study. Regularization weight ğœ† in Equation (24) is a key hyperparameter in the PG method. Figure 6 shows the impact of different ğœ† values on PG performance. When ğœ† is too small, the constraint between personalized and global weights is too weak, resulting in suboptimal performance. Increasing ğœ† initially improves results, but when it becomes too large, performance declines as the tight constraint limits the potential of personalization. Figure 6: Effect of regularization weight ğœ† on Amazon_Books and Tmall. 0.1 0.5 1.0 5.0 10.0 Reg Weight 0.26 0.27 0.28 0.29 P@200 (%) (a) Effect of on Amazon_Books P@200 0.1 0.5 1.0 5.0 10.0 Reg Weight 0.52 0.53 0.54 0.55 P@200 (%) (b) Effect of on Tmall P@200 8.6 8.8 9.0 9.2 9.4 R@200 (%) R@200 9.8 9.9 10.0 10.1 R@200 (%) R@200 Visualization. We randomly select 2,000 users on Amazon_Books and Tmall to visualize the personalized weight distributions generated by PG in Figure 7, which reveal several key insights: Â· Weight Distribution Consistency : SimpleX and LightGCN have the largest weights across both datasets, which is consistent with the global weight assignment results. Â· User-Specific Diversity: The parallel coordinate plots highlight diverse weight distributions across users, with multiple peaks indicating varying user preferences for different channels. Â· Performance-Weight Relationship: The weight assigned to a retrieval channel is not strictly tied to its performance. For instance, despite ItemKNN's lower retrieval performance, its weight remains notable, suggesting that factors such as item overlap between channels also play a role in weight optimization. See Appendix D for further discussion of the experiments.",
  "6 REAL WORLD DEPLOYMENT": "To validate the effectiveness of our multi-channel fusion strategies in real-world scenarios, we deploy our method in one main recommendation scenario (called 'Smart Living') at Company X, a main-stream bank company. This application serves millions of Conference'17, July 2017, Washington, DC, USA Junjie Huang et al. Figure 7: Visualization of personalized weights across Ama-zon_Books and Tmall. (a) (b) Channel weight distribution and patterns on Amazon_Books (violin plot and parallel coordinate plot). (c) (d) Channel weight distribution and patterns on Tmall (violin plot and parallel coordinate plot). 0.0 0.2 0.4 0.6 Weight 1: Pop 2: ItemKNN 3: UserKNN 4: BPR 5: NeuMF 6: SimpleX (U2I) 7: LightGCN (U2I) 8: SimpleX (I2I) 9: LightGCN (I2I) Channel (a) 1 2 3 4 5 6 7 8 9 Channel 0.0 0.1 0.2 0.3 0.4 0.5 0.6 Weight (b) 0.0 0.1 0.2 0.3 0.4 0.5 Weight 1: Pop 2: ItemKNN 3: UserKNN 4: BPR 5: NeuMF 6: SimpleX (U2I) 7: LightGCN (U2I) 8: SimpleX (I2I) 9: LightGCN (I2I) Channel (c) 1 2 3 4 5 6 7 8 9 Channel 0.0 0.1 0.2 0.3 0.4 0.5 Weight (d) daily active users, generating billions of user logs through implicit feedback, such as click behavior. For further details on the deployment process, please refer to the discussion in Appendix E.",
  "6.1 Offline Evaluation": "For the offline experiment, we use a daily updated dataset collected from July 2024 to August 2024 in the 'Smart Living' recommendation scenario for training and evaluation. The scenario involves 11 retrieval channels. Under real-world conditions, the number of items retrieved by each channel may vary; for instance, cold-start users with no interaction history may yield insufficient results from the I2I retrieval method. To address this, we pad the shorter retrieval channels to match the longest one, aligning with our problem formulation in Section 3. The dataset includes true exposure data, capturing items where users paused briefly instead of scrolling past. Since users typically engage with only a few to a few dozen items during a recommendation session, we evaluate the top 10 items using P@10, R@10, and F1@10. As shown in Table 3, CEM outperforms the current production strategy significantly, delivering approximately a 28.6% improvement in offline metrics. Table 3: Comparison of different merging strategies on realworld recommendation scenarios.",
  "6.2 Online Evaluation": "Besides offline experiments, we conduct a five-day online A/B test in October 2024, deploying our method in the 'Smart Living' recommendation scenario of Company X. As mentioned earlier, industry recommender systems typically enforce bounds on multi-channel weight assignments to ensure balanced representation across channels, as shown in Equation 3. This makes equal-weight merging infeasible in real-world deployments. Additionally, due to the current pipeline and engineering limitations, we could not implement personalized multi-channel fusion methods like our PG approach. Instead, we deploy our globally unified weight assignment strategy CEM, which aligns with common industry practice. The control group uses the heuristic-based merging strategy from the current production system, while the test group implements our globally unified CEM strategy at the retrieval stage. Both groups use the same ranking strategy to ensure a fair comparison. We evaluate performance using Click-Through Rate (CTR), defined as: CTR = #clicks #impressions where #clicks and #impressions are the number of clicks and impressions. We report the average results in Table 3, and Figure 8 presents the daily and hourly improvements of CEM over the current strategy. It is evident that CEM significantly outperforms the baseline with a CTR increase of 12% to 20% (average 17%), highlighting the critical role of our optimized multi-channel fusion in recommendation performance. Figure 8: Daily and hourly CTR results from online A/B test in the 'Smart Living' scenario. Day1 Day2 Day3 Day4 Day5 1.50 1.60 1.70 1.80 1.90 2.00 2.10 2.20 2.30 CTR (%) (a) CTR by Day Current Strategy CEM 12AM 2AM 4AM 6AM 8AM 10AM12PM 2PM 4PM 6PM 8PM 10PM 1.40 1.60 1.80 2.00 2.20 2.40 2.60 CTR (%) (b) CTR by Hour Current Strategy CEM",
  "7 RELATED WORK": "Retrieval Methods in Recommendation. Retrieval is the process of efficiently selecting relevant item candidates that match user interests, also referred to as candidate generation or matching [11, 26]. Retrieval methods vary widely, which can be broadly categorized into two types: (1) non-personalized and (2) personalized retrieval [27]. Non-personalized retrieval highlights popular items or trending content, which, while not tailored to individual preferences, often attract user clicks due to their widespread appeal. In contrast, personalized retrieval customizes recommendations to align with specific user preferences, significantly boosting engagement and retention. Common examples include user-to-item (U2I) [30] and item-to-item (I2I) [51] retrieval. Diving deeper into model structures, there are shallow structures like neighborhoodbased collaborative filtering (CF) approaches such as ItemKNN [51] and UserKNN [46], as well as matrix factorization (MF)-based CF approaches [30]. With the advancement of deep learning, there has been a shift toward more sophisticated architectures, including Unleashing the Potential of Multi-Channel Fusion in Retrieval for Personalized Recommendations Conference'17, July 2017, Washington, DC, USA two-tower retrieval models [11, 19, 28, 36], autoencoder-based models [33, 35, 53, 63], graph embedding-based models [4, 20, 39, 59], graph neural network-based models [5, 23, 56], tree-based models [68, 69, 71], and multi-interest retrieval models [9, 31, 58]. These diverse retrieval channels improve both relevance and diversity of recommendation results. In our experiments, we select models from different categories to minimize overlap and enhance diversity. Multi-Channel Retrieval. Multi-channel retrieval is widely used in modern industry practices for cascade recommender systems. MIC [37] effectively aligns users and items based on semantic similarity across channels (U2U, I2I, U2I), leveraging rich cross-channel information. Hron et al. [25] empirically and theoretically explore the differences between single- and two-stage recommenders, showing that when each candidate generator specializes in a different subset of the item pool, performance improves significantly. Similar concepts include recommender ensembling [7], such as weighted hybrid, cross-harmonic, and meta-model mixed recommendation algorithms [6]. However, none of these approaches offer a scientific or systematic solution for multi-channel fusion in the retrieval stage, which is a critical aspect in real-world implementations. Combinatorial Optimization. In addition to the Cross Entropy Method [50] and Bayesian Optimization [15] we use, other wellknown approaches for combinatorial optimization include simulated annealing [2, 10, 12, 47], later extended in [22] and [29], as well as tabu search [17] and genetic algorithms [18]. More recent methods include nested partitioning [54], stochastic comparison [3], and ant colony optimization [13, 21].",
  "8 CONCLUSION": "In this paper, we address the challenge of multi-channel fusion in retrieval. Moving beyond the heuristic and manual methods commonly used in industry, we demonstrate that our optimized weight combinations significantly enhance personalized recommendations. By leveraging black-box optimization and a policy gradient-based method, we provide a user-tailored approach that advances beyond simple quota mechanisms. Extensive experiments across multiple datasets show our approach consistently outperforms existing baselines, and its successful deployment in real-world systems results in notable improvements in both performance and user satisfaction, offering a scalable solution for multi-channel fusion.",
  "REFERENCES": "[1] 2000. Estimating a Dirichlet distribution. [2] Emile Aarts and Jan Korst. 1989. Simulated annealing and Boltzmann machines: a stochastic approach to combinatorial optimization and neural computing . John Wiley & Sons, Inc. [3] SigrÃºn AndradÃ³ttir. 1996. A global search method for discrete stochastic optimization. SIAM Journal on Optimization 6, 2 (1996), 513-530. [4] Oren Barkan and Noam Koenigstein. 2016. Item2vec: neural item embedding for collaborative filtering. In 2016 IEEE 26th International Workshop on Machine Learning for Signal Processing (MLSP) . IEEE, 1-6. [5] Rianne van den Berg, Thomas N Kipf, and Max Welling. 2017. Graph convolutional matrix completion. arXiv preprint arXiv:1706.02263 (2017). [6] Svetlin Bostandjiev, John O'Donovan, and Tobias HÃ¶llerer. 2012. TasteWeights: a visual interactive hybrid recommender system. In Proceedings of the sixth ACM conference on Recommender systems . 35-42. [7] Erion Ã‡ano and Maurizio Morisio. 2017. Hybrid recommender systems: A systematic literature review. Intelligent data analysis 21, 6 (2017), 1487-1524. [8] Pablo Castells, SaÃºl Vargas, Jun Wang, et al. 2011. Novelty and diversity metrics for recommender systems: choice, discovery and relevance. In International Workshop on Diversity in Document Retrieval (DDR 2011) at the 33rd European Conference on Information Retrieval (ECIR 2011) . Citeseer, 29-36. [9] Yukuo Cen, Jianwei Zhang, Xu Zou, Chang Zhou, Hongxia Yang, and Jie Tang. 2020. Controllable multi-interest framework for recommendation. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 2942-2951. [10] Harry Cohn and Mark Fielding. 1999. Simulated annealing: searching for an optimal temperature schedule. SIAM Journal on Optimization 9, 3 (1999), 779-802. [11] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks for youtube recommendations. In Proceedings of the 10th ACM conference on recommender systems . 191-198. [12] Yves Crama, Antoon WJ Kolen, and EJ Pesch. 2005. Local search in combinatorial optimization. Artificial Neural Networks: An Introduction to ANN Theory and Practice (2005), 157-174. [13] Marco Dorigo. 1996. The Any System Optimization by a colony of cooperating agents. IEEE Trans. System, Man & Cybernetics-Part B 26, 1 (1996), 1-13. [14] Chantat Eksombatchai, Pranav Jindal, Jerry Zitao Liu, Yuchen Liu, Rahul Sharma, Charles Sugnet, Mark Ulrich, and Jure Leskovec. 2018. Pixie: A system for recommending 3+ billion items to 200+ million users in real-time. In Proceedings of the 2018 world wide web conference . 1775-1784. [15] Peter I Frazier. 2018. A tutorial on Bayesian optimization. arXiv preprint arXiv:1807.02811 (2018). [16] Mouzhi Ge, Carla Delgado-Battenfeld, and Dietmar Jannach. 2010. Beyond accuracy: evaluating recommender systems by coverage and serendipity. In Proceedings of the fourth ACM conference on Recommender systems . 257-260. [17] F Glover and ML Laguna. 1997. Modern heuristic techniques for combinatorial optimization. [18] David E Goldberg. 1989. Optimization, and machine learning. Genetic algorithms in Search (1989). [19] Mihajlo Grbovic and Haibin Cheng. 2018. Real-time personalization using embeddings for search ranking at airbnb. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 311-320. [20] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining . 855-864. [21] Walter J Gutjahr. 2002. ACO algorithms with guaranteed convergence to the optimal solution. Information processing letters 82, 3 (2002), 145-153. [22] WKeith Hastings. 1970. Monte Carlo sampling methods using Markov chains and their applications. (1970). [23] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for recommendation. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval . 639-648. [24] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural collaborative filtering. In Proceedings of the 26th international conference on world wide web . 173-182. [25] Jiri Hron, Karl Krauth, Michael Jordan, and Niki Kilbertus. 2021. On component interactions in two-stage recommender systems. Advances in neural information processing systems 34 (2021), 2744-2757. [26] Junjie Huang, Guohao Cai, Jieming Zhu, Zhenhua Dong, Ruiming Tang, Weinan Zhang, and Yong Yu. 2024. Recall-Augmented Ranking: Enhancing Click-Through Rate Prediction Accuracy with Cross-Stage Data. In Companion Proceedings of the ACM on Web Conference 2024 . 830-833. [27] Junjie Huang, Jizheng Chen, Jianghao Lin, Jiarui Qin, Ziming Feng, Weinan Zhang, and Yong Yu. 2024. A Comprehensive Survey on Retrieval Methods in Recommender Systems. arXiv preprint arXiv:2407.21022 (2024). [28] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. 2013. Learning deep structured semantic models for web search using clickthrough data. In Proceedings of the 22nd ACM international conference on Information & Knowledge Management . 2333-2338. [29] Scott Kirkpatrick, C Daniel Gelatt Jr, and Mario P Vecchi. 1983. Optimization by simulated annealing. science 220, 4598 (1983), 671-680. [30] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization techniques for recommender systems. Computer 42, 8 (2009), 30-37. [31] Chao Li, Zhiyuan Liu, Mengmeng Wu, Yuchi Xu, Huan Zhao, Pipei Huang, Guoliang Kang, Qiwei Chen, Wei Li, and Dik Lun Lee. 2019. Multi-interest network with dynamic routing for recommendation at Tmall. In Proceedings of the 28th ACM international conference on information and knowledge management . 2615-2623. [32] Dawen Liang, Jaan Altosaar, Laurent Charlin, and David M Blei. 2016. Factorization meets the item embedding: Regularizing matrix factorization with item co-occurrence. In Proceedings of the 10th ACM conference on recommender systems . 59-66. [33] Dawen Liang, Rahul G Krishnan, Matthew D Hoffman, and Tony Jebara. 2018. Variational autoencoders for collaborative filtering. In Proceedings of the 2018 world wide web conference . 689-698. [34] Chengkai Liu, Jianghao Lin, Jianling Wang, Hanzhou Liu, and James Caverlee. 2024. Mamba4rec: Towards efficient sequential recommendation with selective state space models. arXiv preprint arXiv:2403.03900 (2024). Conference'17, July 2017, Washington, DC, USA Junjie Huang et al. [35] Jianxin Ma, Chang Zhou, Peng Cui, Hongxia Yang, and Wenwu Zhu. 2019. Learning disentangled representations for recommendation. Advances in neural information processing systems 32 (2019). [36] Kelong Mao, Jieming Zhu, Jinpeng Wang, Quanyu Dai, Zhenhua Dong, Xi Xiao, and Xiuqiang He. 2021. SimpleX: A simple and strong baseline for collaborative filtering. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management . 1243-1252. [37] Ping Nie, Yujie Lu, Shengyu Zhang, Ming Zhao, Ruobing Xie, William Yang Wang, and Yi Ren. 2022. MIC: model-agnostic integrated cross-channel recommender. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management . 3400-3409. [38] Bidyut Kr Patra, Raimo Launonen, Ville Ollikainen, and Sukumar Nandi. 2015. A new similarity measure using Bhattacharyya coefficient for collaborative filtering in sparse data. Knowledge-Based Systems 82 (2015), 163-177. [39] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining . 701-710. [40] Qi Pi, Weijie Bian, Guorui Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Practice on long sequential user behavior modeling for click-through rate prediction. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 2671-2679. [41] Qi Pi, Guorui Zhou, Yujing Zhang, Zhe Wang, Lejian Ren, Ying Fan, Xiaoqiang Zhu, and Kun Gai. 2020. Search-based user interest modeling with lifelong sequential behavior data for click-through rate prediction. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management . 2685-2692. [42] Jiarui Qin, Weinan Zhang, Rong Su, Zhirong Liu, Weiwen Liu, Ruiming Tang, Xiuqiang He, and Yong Yu. 2021. Retrieval & interaction machine for tabular data prediction. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining . 1379-1389. [43] Jiarui Qin, Jiachen Zhu, Bo Chen, Zhirong Liu, Weiwen Liu, Ruiming Tang, Rui Zhang, Yong Yu, and Weinan Zhang. 2022. RankFlow: Joint Optimization of MultiStage Cascade Ranking Systems as Flows. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval . 814-824. [44] Steffen Rendle. 2010. Factorization machines. In 2010 IEEE International conference on data mining . IEEE, 995-1000. [45] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2012. BPR: Bayesian personalized ranking from implicit feedback. arXiv preprint arXiv:1205.2618 (2012). [46] Paul Resnick, Neophytos Iacovou, Mitesh Suchak, Peter Bergstrom, and John Riedl. 1994. Grouplens: An open architecture for collaborative filtering of netnews. In Proceedings of the 1994 ACM conference on Computer supported cooperative work . 175-186. [47] HEdwin Romeijn and Robert L Smith. 1994. Simulated annealing for constrained global optimization. Journal of Global Optimization 5 (1994), 101-126. [48] Reuven Rubinstein. 1999. The cross-entropy method for combinatorial and continuous optimization. Methodology and computing in applied probability 1 (1999), 127-190. [49] Reuven Y Rubinstein. 1997. Optimization of computer simulation models with rare events. European Journal of Operational Research 99, 1 (1997), 89-112. [50] Reuven Y Rubinstein. 2001. Combinatorial optimization, cross-entropy, ants and rare events. Stochastic optimization: algorithms and applications (2001), 303-363. [51] Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. 2001. Item-based collaborative filtering recommendation algorithms. In Proceedings of the 10th international conference on World Wide Web . 285-295. [52] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando De Freitas. 2015. Taking the human out of the loop: A review of Bayesian optimization. Proc. IEEE 104, 1 (2015), 148-175. Unleashing the Potential of Multi-Channel Fusion in Retrieval for Personalized Recommendations Conference'17, July 2017, Washington, DC, USA",
  "A NOTATIONS": "We summarize the key notations and their corresponding descriptions used in this paper in Table 4.",
  "Table 4: Notations and descriptions.": "",
  "B PSEUDOCODE FOR TRAINING PROCEDURE OF GLOBALLY UNIFIED MERGING": "",
  "Algorithm 1 Globally Unified Weight Assignment": "Require: Elite fraction ğ‘ , number of samples per iteration ğ‘„ , performance evaluation function S(Â·) , acquisition function ğ‘ EI (Â·) , number of BayesOpt iterations ğ‘‡ 1: Stage 1: Cross Entropy Method (CEM) 2: Initialize concentration parameter vector ğœ¶ ( 0 ) , ğ‘¡ 3: repeat 4: Sample ğ‘„ weight vectors from Dirichlet( ğœ¶ ( ğ‘¡ ) ) 5: Evaluate retrieval performance for each sampled weight: S( w ğ‘– ) for ğ‘– = 1 , 2 , . . . , ğ‘„ 6: Select the elite samples based on performance 7: Update ğ›¼ ( ğ‘¡ + 1 ) using elite samples 8: Increment ğ‘¡ 9: until convergence 10: Set ğ›½ ( 0 ) = ğ›¼ ( ğ‘¡ ) // Initialize BayesOpt with final CEM parameters 11: Stage 2: Bayesian Optimization (BayesOpt) 12: Constrained Search Space: [ 0 . 5 ğœ· ( 0 ) , 1 . 5 ğœ· ( 0 ) ] 13: Initialize Gaussian Process (GP) model GP with ğœ· ( 0 ) 14: for ğ‘¡ = 1 , 2 , . . . , ğ‘‡ do 15: Fit GP model to observed data 16: Predict objective function ğ‘† ( ğœ· ) for unexplored regions 17: Compute acquisition function ğ‘ EI ( ğœ· ) 18: Find next sample ğœ· ( ğ‘¡ + 1 ) = arg max ğ‘ EI ( ğœ· ) ğœ· 19: Evaluate objective function ğ‘† ( ğœ· ( ğ‘¡ + 1 ) ) 20: Update GP with new data ( ğœ· ( ğ‘¡ + 1 ) , ğ‘† ( ğœ· ( ğ‘¡ + 1 ) )) 21: end for 22: return Optimal weight vector w using Equation (14) = 0",
  "C EXPERIMENTAL CONFIGURATION": "",
  "C.1 Dataset Description": "We conduct experiments on three real-world, large-scale datasets. Gowalla 4 dataset is collected from the Gowalla social network, a location-based platform where users could check in at physical locations and share their activities with friends. Amazon_Books 5 dataset is a subset of the Amazon review dataset, which contains millions of reviews written by Amazon customers for various products on the Amazon e-commerce platform. Tmall 6 dataset is provided by Ant Financial Services, containing users' online and on-site behavior from July to November 2015.",
  "C.2 Baseline Description": "In our experiment, we implement nine retrieval channels on each of the three datasets. Brief descriptions of these channels are provided: Â· Pop is a basic model that consistently recommends the most popular items. Â· ItemKNN [51] is a simple model that calculates item similarity using the interaction matrix. Â· UserKNN [46] is a simple model that calculates user similarity using the interaction matrix. Â· BPR [45] is a basic matrix factorization model trained using a pairwise learning approach. Â· NeuMF [24] enhances matrix factorization with a neural network by replacing the dot product with an MLP, offering a more precise model of user-item interactions. Â· SimpleX [36] is a straightforward two-tower retrieval model that stands out for its loss function. It incorporates a larger pool of negative samples and filters out uninformative ones using a threshold. Additionally, it balances the loss between positive and negative samples by applying relative weights. Â· LightGCN [23] focuses solely on the core aspect of GCN, neighborhood aggregation, for collaborative filtering. It learns user and item embeddings through linear propagation on the user-item interaction graph, and combines the embeddings from all layers using a weighted sum to produce the final embedding. For SimpleX (I2I) and LightGCN (I2I), we take the user's three most recent interactions from the training set and retrieve the 80 most similar items for each. After merging and removing duplicates, if fewer than 200 items remain, we continue adding more until we reach 200 items. If the final set exceeds 200 items, we truncate it to ensure the retrieved item set contains exactly 200 items.",
  "C.3 Implementation Details": "We implement all methods with PyTorch using Recbole [67], a comprehensive framework for recommendation models. The hyperparameters for the nine retrieval channels are provided in Appendix C.4, with each channel retrieving 200 items. For globally unified weight assignment, we initialize the Dirichlet distribution with ğœ¶ ( 0 ) = [ 1 , 1 , . . . , 1 ] âŠ¤ . The learning rate ğœ‚ 1 in Equation (11) is set to 0.1 with a decay factor of 0.95 applied if no performance improvement is observed. In each round, 60 samples are drawn, 4 https://snap.stanford.edu/data/loc-gowalla.html. 5 https://jmcauley.ucsd.edu/data/amazon/amazonbooks. 6 https://tianchi.aliyun.com/dataset/53. Conference'17, July 2017, Washington, DC, USA Junjie Huang et al. and the top 10% are selected as elite samples. Early stopping occurs after five iterations without improvement. Bayesian Optimization (BayesOpt) refines the CEM results by performing 10 calls (T=10 in Algorithm 1) to optimize global weights. For personalized weight assignment, optimal hyperparameters are found via grid search, with learning rates ğœ‚ 2 in {1e-5, 5e-5, 1e-4} and regularization weights ğœ† in {0.5, 1, 5}. The number of sampled weight vectors for each user ğ‘† in Equation (22) is set to 1. In Equation (18), ğ›¿ max = 10 . 0 and ğœ– = 10 -6 are used. Pre-trained user and item representations from SimpleX are used for initialization. The top 10 items retrieved by each channel are pooled to represent the channel, denoted as ğ’„ ğ‘¢ğ‘˜ . The best models are selected based on R@200 on the validation set, and final metrics are reported on the test set.",
  "C.4 Hyperparameters of Baselines": "We now present the hyperparameters used for the baselines across the three datasets. For ItemKNN and UserKNN, we set ğ‘˜ = 10 due to the large number of both users and items. The remaining models are configured as follows: BPR: {learning rate: 5e-4}, NeuMF: {learning rate: 1e-4, MLP hidden sizes: [64, 32, 16]}, SimpleX: {learning rate: 1e-4, margin: 0.3, negative weight: 150}, and LightGCN: {learning rate: 1e-3, regularization weight: 1e-2, n layers: 3}.",
  "C.5 Evaluation Metrics": "The metrics used in the experiments, denoted as P@L, R@L, and F1@L, are presented in the following equations:",
  "D EXTENDED ANALYSIS AND RESULTS": "",
  "D.1 Retrieval Performance and Diversity": "Table 5: Evaluation of the trade-off between retrieval accuracy and diversity on Amazon_Books and Tmall. There is often a trade-off between retrieval performance and diversity, yet diversity in recommendation results is crucial for user experience [37]. Various approaches [8] have been proposed to measure the diversity of the recommended list of items. We use item coverage [16, 55], which calculates the proportion of items recommended across all users, as defined in Equation (30):  Table 5 presents the retrieval performance and diversity of several methods. Our scientifically optimized multi-channel retrieval merging strategies achieve better retrieval performance while maintaining high diversity, effectively striking a balance.",
  "E DEPLOYMENT DISCUSSION": "In this section, we share our hands-on experience implementing our multi-channel fusion strategy in Company X's recommendation scenario. As discussed in Section 6, the number of items retrieved from each channel varies. Given this variability, the current production system sets an upper limit on the number of items retrieved from each channel. This limitation prevents the direct application of our globally optimized weights from experimental results. Instead, we calculate adjustable ratios based on the results from CEM, ensuring they meet business requirements by truncating channels that exceed their limits and adjusting toward the optimized weights. As a result, the deployed version is an approximation of CEM, adapted to fit practical constraints.",
  "keywords_parsed": [
    "Recommender Systems",
    "Retrieval",
    "Multi-Channel Fusion"
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "Estimating a Dirichlet distribution"
    },
    {
      "ref_id": "b2",
      "title": "Simulated annealing and Boltzmann machines: a stochastic approach to combinatorial optimization and neural computing"
    },
    {
      "ref_id": "b3",
      "title": "A global search method for discrete stochastic optimization"
    },
    {
      "ref_id": "b4",
      "title": "Item2vec: neural item embedding for collaborative filtering"
    },
    {
      "ref_id": "b5",
      "title": "Graph convolutional matrix completion"
    },
    {
      "ref_id": "b6",
      "title": "TasteWeights: a visual interactive hybrid recommender system"
    },
    {
      "ref_id": "b7",
      "title": "Hybrid recommender systems: A systematic literature review"
    },
    {
      "ref_id": "b8",
      "title": "Novelty and diversity metrics for recommender systems: choice, discovery and relevance"
    },
    {
      "ref_id": "b9",
      "title": "Controllable multi-interest framework for recommendation"
    },
    {
      "ref_id": "b10",
      "title": "Simulated annealing: searching for an optimal temperature schedule"
    },
    {
      "ref_id": "b11",
      "title": "Deep neural networks for youtube recommendations"
    },
    {
      "ref_id": "b12",
      "title": "Local search in combinatorial optimization"
    },
    {
      "ref_id": "b13",
      "title": "The Any System Optimization by a colony of cooperating agents"
    },
    {
      "ref_id": "b14",
      "title": "Pixie: A system for recommending 3+ billion items to 200+ million users in real-time"
    },
    {
      "ref_id": "b15",
      "title": "A tutorial on Bayesian optimization"
    },
    {
      "ref_id": "b16",
      "title": "Beyond accuracy: evaluating recommender systems by coverage and serendipity"
    },
    {
      "ref_id": "b17",
      "title": "Modern heuristic techniques for combinatorial optimization"
    },
    {
      "ref_id": "b18",
      "title": "Optimization, and machine learning"
    },
    {
      "ref_id": "b19",
      "title": "Real-time personalization using embeddings for search ranking at airbnb"
    },
    {
      "ref_id": "b20",
      "title": "node2vec: Scalable feature learning for networks"
    },
    {
      "ref_id": "b21",
      "title": "ACO algorithms with guaranteed convergence to the optimal solution"
    },
    {
      "ref_id": "b22",
      "title": "Monte Carlo sampling methods using Markov chains and their applications"
    },
    {
      "ref_id": "b23",
      "title": "Lightgcn: Simplifying and powering graph convolution network for recommendation"
    },
    {
      "ref_id": "b24",
      "title": "Neural collaborative filtering"
    },
    {
      "ref_id": "b25",
      "title": "On component interactions in two-stage recommender systems"
    },
    {
      "ref_id": "b26",
      "title": "Recall-Augmented Ranking: Enhancing Click-Through Rate Prediction Accuracy with Cross-Stage Data"
    },
    {
      "ref_id": "b27",
      "title": "A Comprehensive Survey on Retrieval Methods in Recommender Systems"
    },
    {
      "ref_id": "b28",
      "title": "Learning deep structured semantic models for web search using clickthrough data"
    },
    {
      "ref_id": "b29",
      "title": "Optimization by simulated annealing"
    },
    {
      "ref_id": "b30",
      "title": "Matrix factorization techniques for recommender systems"
    },
    {
      "ref_id": "b31",
      "title": "Multi-interest network with dynamic routing for recommendation at Tmall"
    },
    {
      "ref_id": "b32",
      "title": "Factorization meets the item embedding: Regularizing matrix factorization with item co-occurrence"
    },
    {
      "ref_id": "b33",
      "title": "Variational autoencoders for collaborative filtering"
    },
    {
      "ref_id": "b34",
      "title": "Mamba4rec: Towards efficient sequential recommendation with selective state space models"
    },
    {
      "ref_id": "b35",
      "title": "Learning disentangled representations for recommendation"
    },
    {
      "ref_id": "b36",
      "title": "SimpleX: A simple and strong baseline for collaborative filtering"
    },
    {
      "ref_id": "b37",
      "title": "MIC: model-agnostic integrated cross-channel recommender"
    },
    {
      "ref_id": "b38",
      "title": "A new similarity measure using Bhattacharyya coefficient for collaborative filtering in sparse data"
    },
    {
      "ref_id": "b39",
      "title": "Deepwalk: Online learning of social representations"
    },
    {
      "ref_id": "b40",
      "title": "Practice on long sequential user behavior modeling for click-through rate prediction"
    },
    {
      "ref_id": "b41",
      "title": "Search-based user interest modeling with lifelong sequential behavior data for click-through rate prediction"
    },
    {
      "ref_id": "b42",
      "title": "Retrieval & interaction machine for tabular data prediction"
    },
    {
      "ref_id": "b43",
      "title": "RankFlow: Joint Optimization of MultiStage Cascade Ranking Systems as Flows"
    },
    {
      "ref_id": "b44",
      "title": "Factorization machines"
    },
    {
      "ref_id": "b45",
      "title": "BPR: Bayesian personalized ranking from implicit feedback"
    },
    {
      "ref_id": "b46",
      "title": "Grouplens: An open architecture for collaborative filtering of netnews"
    },
    {
      "ref_id": "b47",
      "title": "Simulated annealing for constrained global optimization"
    },
    {
      "ref_id": "b48",
      "title": "The cross-entropy method for combinatorial and continuous optimization"
    },
    {
      "ref_id": "b49",
      "title": "Optimization of computer simulation models with rare events"
    },
    {
      "ref_id": "b50",
      "title": "Combinatorial optimization, cross-entropy, ants and rare events"
    },
    {
      "ref_id": "b51",
      "title": "Item-based collaborative filtering recommendation algorithms"
    },
    {
      "ref_id": "b52",
      "title": "Taking the human out of the loop: A review of Bayesian optimization"
    }
  ]
}