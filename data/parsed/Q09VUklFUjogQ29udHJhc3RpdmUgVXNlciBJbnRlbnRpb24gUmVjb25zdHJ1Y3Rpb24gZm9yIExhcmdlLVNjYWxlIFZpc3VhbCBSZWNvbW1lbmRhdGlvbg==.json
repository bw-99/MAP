{"title": "COURIER: Contrastive User Intention Reconstruction for Large-Scale Visual Recommendation", "authors": "Jia-Qi Yang; Chenglei Dai; Dan Ou; Dongshuai Li; Ju Huang; De-Chuan Zhan; Xiaoyi Zeng; Yang Yang", "pub_date": "2024-06-06", "abstract": "With the advancement of multimedia internet, the impact of visual characteristics on the decision of users to click or not within the online retail industry is increasingly significant. Thus, incorporating visual features is a promising direction for further performance improvements in click-through rate (CTR). However, experiments on our production system revealed that simply injecting the image embeddings trained with established pre-training methods only has marginal improvements. We believe that the main advantage of existing image feature pretraining methods lies in their effectiveness for cross-modal predictions. However, this differs significantly from the task of CTR prediction in recommendation systems. In recommendation systems, other modalities of information (such as text) can be directly used as features in downstream models. Even if the performance of cross-modal prediction tasks is excellent, it is challenging to provide significant information gain for the downstream models. We argue that a visual", "sections": [{"heading": "Introduction", "text": "Predicting the click-through rate (CTR) is an essential task in recommendation systems based on deep learning [1][2][3]. Typically, CTR models measure the probability that a user will click on an item based on the user's profile and behavior history [2,4], such as clicking, purchasing, adding a cart, etc. The behavior histories are represented by sequences of item IDs, titles, and some statistical features, such as monthly sales and favorable rate [1].\nFrom an intuitive perspective, the visual representations of items hold significant importance in online item recommendation, particularly in categories like women's clothing. Recent studies have shown that incorporating modality information through end-to-end training can enhance the effectiveness of recommendations [5]. However, when dealing with large-scale product recommendation systems, where users generate billions of clicks daily, implementing end-to-end training becomes impractical [6]. A feasible approach involves enhancing image features through the utilization of representation learning methods specifically designed for image features. Nevertheless, our experiments reveal that employing embeddings derived from existing image feature learning methods such as SimCLR [7], SimSiam [8], and CLIP [9], only yield marginal effects on the relevant downstream CTR prediction task (Table . (5)). We ascribe this lack of success to two factors. Firstly, in the context of recommendation scenarios, user preferences for visual appearance tend to be vague and imprecise, which may not be captured by existing image feature learning methods that focus on label prediction. Besides, augmentation methods that excel in tasks like image classification, due to their significant alteration of the image's appearance, are not suitable for recommendation systems. Secondly, the label information embedded in the pre-trained embeddings, such as classes or language descriptions, can already be directly utilized in item recommendations, rendering the information gain provided by pre-trained embeddings redundant. For instance, we already possess categories, item titles, and style tags provided by the merchants in Taobao. Consequently, a pre-trained model that performs well in predicting categories or titles contributes little novel information and does not enhance CTR prediction task. We propose a user intention reconstruction method to mine potential visual features that cannot be reflected by cross-modal labels. In this example, the user searched for \"Coat\" and received two recommendations (Page-viewed items). The user clicked on the one on the right. Through our user intention reconstruction, we identified similar items from the user's click history with larger attention, the reconstructed PV item embeddings are denoted as R j pv . Then, we optimize the PV embeddings E j pv and reconstructions R j pv to be closer if the corresponding item is clicked and more far apart otherwise.\nTo boost the performance of downstream CTR prediction tasks, we argue that the representation learning method should be aware of the downstream CTR task and should also be decoupled from the CTR task to reduce computation. To achieve this goal, we propose a COntrastive UseR IntEntion Reconstruction (COURIER) method. Our method is based on an intuitive assumption: An item clicked by a user is likely to have visual characteristics similar to some of the clicking history items. One straightforward approach is to optimize the distance between user embeddings (comprised of item images previously clicked by the user) and clicked item image embeddings. Unlike the typical one-to-one correspondence in common contrastive learning, this establishes a many-to-one correspondence. Consequently, we must aggregate multiple item image embeddings from the user's historical clicks. Common aggregation methods include self-attention or pooling. However, it should be noted that user click histories often contain a significant number of images that have low relevance to the clicked items. Directly minimizing the distance between these aggregated embeddings may result in all images having very similar embeddings. To mine the visual features related to user interests, we propose reconstructing the next clicking item with a cross-attention mechanism on the clicking history items. The reconstruction can be interpreted by a weighted sum of history item embeddings, which effectively selects related images from history in an end-to-end manner, as depicted in Fig. 1. We propose to optimize a contrastive loss that not only encourages lower reconstruction error, but also push embeddings of un-clicked items further apart. We conducted various experiments to verify our motivation and design of the method. Our pre-trained image embedding achieves 12% improvements in NDCG and Recall on several public datasets compared with strong baselines. We conducted experiments in a large-scale Taobao dataset, our method achieves 0.46% absolute offline improvement on AUC. In online A/B tests, we achieve 0.88% improvement on GMV in the women's clothing category, which is significant considering the volume of Taobao online shopping platform.\nOur contribution can be summarized as follows:\n\u2022 To establish a one-to-one correspondence between images clicked by the user in the past and the image currently clicked by the user, we propose a user intention reconstruction method, which can mine latent user intention from history click sequences without any explicit semantic labels. \u2022 The user intention reconstruction objective alone may lead to a collapsed solution.\nTo solve this problem, we propose a contrastive training method that utilizes the un-clicked item efficiently. \u2022 We conduct comprehensive experiments on both private and public datasets to validate the effectiveness of our method. Additionally, we provide insights and share our practical experience regarding the deployment of image feature models in realworld large-scale recommendation systems.", "publication_ref": ["b0", "b1", "b2", "b1", "b3", "b0", "b4", "b5", "b6", "b7", "b8"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Related work", "text": "From collaborative filtering [10,11] to deep learning based recommender systems [12][13][14], IDs and categories (user IDs, item IDs, advertisement IDs, tags, etc.) are the essential features in item recommendation systems, which can straightforwardly represent the identities. However, with the development of the multi-media internet, ID features alone can hardly cover all the important information. Thus, recommendations based on content such as images, videos, and texts have become an active research field in recommender systems. In this section, we briefly review the related work and discuss their differences compared with our method.", "publication_ref": ["b9", "b10", "b11", "b12", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Content-based recommendation", "text": "In general, content is more important when the content itself is the concerned information. Thus, the content-based recommendation has already been applied in news [15], music [16], image [17], and video [18] recommendations.\nResearch on the content-based recommendation in the item recommendation task is much fewer because of the dominating ID features. Early applications of image features typically use image embeddings extracted from a pre-trained image classification model, such as [19][20][21][22][23][24][25]. The image features adopted by these methods are trained on general classification datasets such as ImageNet, which may not fit recommendation tasks. Thus, [26] proposes to train on multi-modal data from recommendation task. However, [26] does not utilize any recommendation labels such as clicks and payments, which will have marginal improvement if we are already using information from other modalities.\nWith the increase in computing power, some recent papers propose to train item recommendation models end-to-end with image feature networks [5,27]. However, the datasets used in these papers are much smaller than our application scenario. For example, [27] uses a dataset consisting of 25 million interactions. While in our online system, average daily interactions in a single category (women's clothing) are about 850 million. We found it infeasible to train image networks in our scenario end-toend, which motivates our decoupled two-stage framework with user-interest-aware embedding learning.", "publication_ref": ["b14", "b15", "b16", "b17", "b18", "b19", "b20", "b21", "b22", "b23", "b24", "b25", "b25", "b4", "b26", "b26"], "figure_ref": [], "table_ref": []}, {"heading": "Image representation learning in recommendation", "text": "Self-supervised pre-training has been a hot topic in recent years. We classify the selfsupervised learning methods into two categories: Augmentation-based and predictionbased. Augmentation-based. The augmentation-based methods generate multiple different views of an image by random transformations, then the model is trained to pull the embeddings of different views closer and push other embeddings (augmented from different images) further. SimCLR [7], SimSiam [8], BYOL [28] are some famous self-supervised methods in this category. These augmentation-based methods do not perform well in the item recommendation task as shown in our experiments in Section. 4.2.4. Since the augmentations are designed for classification (or segmentation, etc.) tasks, they change the visual appearance of the images without changing their semantic class, which contradicts the fact that visual appearance is also important in recommendations (e.g., color, shape, etc.). Prediction-based. If the data can be split into more than one part, we can train a model taking some of the parts as inputs and predict the rest parts, which is the basic idea of prediction-based pre-training. Representative prediction-based methods include BERT [29], CLIP [9], etc. The prediction-based methods can be used to train multimodal recommendation data as proposed by [26]. However, if we are already utilizing multi-modal information, the improvements are limited, as shown in our experiments. To learn user interests information that can not be provided by other modalities, we argue that user behaviors should be utilized, as in our proposed method.", "publication_ref": ["b6", "b7", "b27", "b28", "b8", "b25"], "figure_ref": [], "table_ref": []}, {"heading": "Contrastive learning in recommender systems", "text": "Contrastive learning methods are also adopted in recommendation systems in recent years. The most explored augmentation-based method is augmenting data by dropping, reordering, and masking some features [30], items [31,32], and graph edges [33]. Prediction-based methods are also adopted for recommendation tasks, e.g., BERT4Rec [34] randomly masks some items and makes predictions. However, all these recommender contrastive learning methods concentrate on augmentation and pretraining with ID features, while our method tackles representation learning of image features. Several recent works also considered mining users' intents with contrastive learning [35,36]. Different from our concentration on visual features, they focus on learning with graph structures while image features are not considered.", "publication_ref": ["b29", "b30", "b31", "b32", "b33", "b34", "b35"], "figure_ref": [], "table_ref": []}, {"heading": "Contrastive user intention reconstruction", "text": "We briefly introduce some essential concepts and notations, then introduce our method in detail.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Preliminary", "text": "Notations. A data sample for CTR prediction in item search can be represented with a tuple of (user, item, query, label). Typically, in recommendation tasks, there is no explicit query, and the remaining aspects align with search. Our approach is universally applicable in these scenarios. For simplicity, the subsequent sections will consistently use the term \"recommendation\". A user searches for a query text, and several items are shown to the user. Then the items that the user clicked are labeled as positive and negative otherwise. When a user views a page, we have a list of items that are presented to the user, we call them page-view (PV) items. The length of this PV list is denoted by l pv . Each PV item has a cover image, denoted by I j pv , where 0 \u2264 j < l pv . The corresponding click labels are denoted by y j pv , where y j pv \u2208 {0, 1}. Each user has a list of clicked item history, the image of each item is denoted by I k click . 0 \u2264 k < l click , where l click is the length of click history. The l pv and l click may vary by different users and pages, we trim or pad to the same length practically.\nAttention. An attention layer is defined as follows:\nAttention(Q, K, V ) = softmax( QK T \u221a d K )V (1\n)\nwhere Q is the query matrix, K is the key matrix, V is the value matrix. The mechanism of the attention layer can be interpreted intuitively: For each query, a similarity score is computed with every key, the values corresponding to keys are weighted by their similarity scores and summed up to obtain the outputs. We refer interested readers to [37] for more details of the attention mechanism.", "publication_ref": ["b36"], "figure_ref": [], "table_ref": []}, {"heading": "User intention reconstruction", "text": "In the following discussion, we only consider a single line of data (a PV list and corresponding list of user click history), the batch training method will be discussed later. We use I pv and I click to denote the matrix of all the l pv and l click images. All the images are fed to the image backbone (IB) to get their embeddings. We denote the embeddings as E j pv = IB(I j pv ) and E k click = IB(I k click ) correspondingly. In our user interest reconstruction method, we treat the embeddings of PV images E pv as queries Q, and we input the embeddings of click images E click as values V and keys K. Then the user interest reconstruction layer can be calculated by\nR j pv = Attention(E j pv , E click , E click ) (2) = \u03b1 k E k click (3\n)\nwhere \u03b1 \u223c sof tmax E j pv E T click . \u03b1 k is the attention on kth history click item. The reason for its name (user intention reconstruction) is that the attention layer forces the outputs to be a weighted sum of the embeddings of historical click sequences. Thus, the output space is limited to the combination of E click within a simplex. The user intention reconstruction module cannot prevent the trivial solution that all the embeddings collapse to the same value. Thus, we propose a contrastive method to train the user-interest reconstruction module.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Contrastive training method", "text": "With the PV embeddings E pv , and the corresponding reconstructions R pv . We calculate pairwise similarity score of E j0 pv and R j1 pv , where E j0 pv represents embedding of the j 0 -th PV item and R j1 pv represents the reconstruction of the j 1 -th PV item:\nSim(j 0 , j 1 ) = E j0 pv T R j1 pv ||E j0 pv || ||R j1 pv ||(4)\nThen we calculate the contrastive loss by\nL pv = L contrast (E pv , E click , y)(5)\n= j0,j1\n-log e Sim(j0,j1)/\u03c4 j0 e Sim(j0,j1)/\u03c4 I[j 0 = j 1 and y j0\n= 1](6)\nHere I[j 0 = j 1 and y j0 = 1] is an indicator function that equals 1 when j 0 = j 1 and y j0 = 1, and equals 0 otherwise. The contrastive loss with user interest reconstruction is depicted in Fig. 2. The softmax function is calculated column-wisely, and only the positive PV images are optimized to be reconstructed (the two columns in dashed boxes). The behavior of the contrastive loss aligns with our assumption: Positive PV images are pulled closer to the corresponding reconstructions (j 0 = j 1 and y j0 = 1), while the negative PV images are not encouraged to be reconstructed (j 0 = j 1 and y j0 = 0). All the PV embeddings E j0 pv with j 0 \u0338 = j 1 and y j0 = 1, are pushed further away from R j1 pv , which can prevent the trivial solution that all the embeddings are the same. Some elements in the similarity matrix are not used in our loss, namely the elements with y j1 = 0, since the negative PV images are not supposed to be reconstructible. We left these columns for ease of implementation. Extending to batched contrastive loss. The above contrastive loss is calculated using PV items within a single page (we have at most 10 items on a page), which can only provide a limited number of negative samples. However, a well-known property of contrastive loss is that the performance increases as the number of negative samples increases, which is verified both practically [7] and theoretically [38]. To increase the number of negative samples, we propose to treat all other in-batch PV items as negative samples. Specifically, we have (batch size*l pv ) PV items in this batch. For a positive PV item, all the other (batch size*l pv -1) PV items are used as negatives, which significantly increases the number of negatives.", "publication_ref": ["b6", "b37"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Contrastive learning on click sequences", "text": "In the contrastive loss introduced in Section. 3.3, the negative and positive samples are from the same query. Since all the PV items that are pushed to the users are already ranked by our online recommender systems, they may be visually and conceptually similar and are relatively hard to distinguish by contrastive loss. Although the batch training method introduces many easier negatives, the hard negatives still dominate the contrastive loss at the beginning of training, which makes the model hard to train.\nThus, we propose another contrastive loss on the user click history, which does not have hard PV negatives. Specifically, suppose the user's click history is denoted by\ncorresponding embeddings E 0 click , ..., E l click -1 click\nWe treat the last item as the next item to be clicked (since the items are sorted by their click timestamp), and the rest items are treated as click history. The user click sequence loss is calculated as follows:\nL ucs = L contrast (E l click -1 click , E 0:l click -1 click , 1)\nThe L contrast function is the same as in Section. 3.3. Here the label y = 1 because all the samples in history click sequences are positive. The user sequence loss provides an easier objective at the start of the training, which helps the model to learn with a curriculum style. It also introduces more signals to train the model, which improves data efficiency. The overall loss of COURIER is:\nL COURIER = L pv + L ucs(7)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Differences compared to established contrastive methods", "text": "Although COURIER is also a contrastive method, it is significantly different from classic contrastive methods. First, in contrastive methods such as SimCLR [7] and CLIP [9], every sample has a corresponding positive counterpart. In our method, a negative PV item does not have a corresponding positive reconstruction, but it still serves as a negative sample in the calculation. Secondly, there is a straightforward oneto-one matching in SimCLR and CLIP, e.g., text and corresponding image generated by augmentation. In recommendation systems, image augmentations are not applicable due to the distortion of appearance. Instead, a positive PV item corresponds to a list of history click items, which is transformed into a one-to-one matching with our user interest recommendation module introduced in Section. 3.2. Thirdly, another approach to convert this multi-to-one matching to one-to-one is to apply self-attention on the multi-part, as suggested in [39], which turned out to perform worse in our scenario. We experiment and analyze this method in Section. 4.3.2 (w/o Reconstruction).", "publication_ref": ["b6", "b8", "b38"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "To validate the efficacy of our proposed image representation learning method in enhancing recommendation performance, we conducted experiments on several publicly available product recommendation datasets. Subsequently, we proceeded with systematic experiments on actual data from Taobao and integrated our approach into the online system.foot_0 ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments on public datasets", "text": "Since nearly all recommendation methods incorporate ID features, the addition of image or text features typically characterizes multimodal recommendation methods.\nOur CTR prediction model falls under this category as well. These methods primarily rely on fixed image and text backbone models to extract image features. However, our proposed image feature pre-training method aims to enhance the representational capabilities of image features in recommendation system methods. In this section, we will experimentally investigate whether our pre-trained image features can further enhance the performance of these methods. Datasets. In order to validate the effectiveness of our proposed visual feature pretraining method in learning user-intention related features, we select three categories from the Amazon Review dataset [40], namely Baby, Sports, and Clothing. Each item has corresponding image and text descriptions. We apply our method to the image features while keeping the text features unchanged. The statistics are shown in Table . \n(1). To evaluate the performance of different visual feature extraction methods, we divided the dataset into pre-training, training, validation, and testing sets, with proportions of 50%, 30%, 10%, and 10% respectively. We group the datasets by user IDs to construct a list of positive items, and then we uniformly sample 10 negative items for each user to construct the pre-training dataset.\nBaselines. We compare with several representative ID-based and multi-modal recommendation methods. We select two ID-based recommendation methods, namely BPR [41] and SLMRec [23]. We select five multi-modal recommendation methods, namely DualGNN [21], LATTICE [22], MGCN [24], MMGCN [20] and BM3 [25]. To validate the information gained from our visual feature learning method, we concatenate the pre-trained embeddings with the original embedding provided by Zhou [42]. Then, we apply the augmented embeddings to BM3 and MMGCN, which corresponds to BM3+ours and MMGCN+ours, respectively. Implementation. All the baseline methods are tuned following Zhou [42]. Specifically, we tune the hyper-parameters (learning rate, weight decay, and method-specific hyperparameters such as contrastive loss weight in MGCN) of each method with a validation dataset. Then, we run each method with 5 different random seeds and report their average performance.\nResults. As observed in Table . (2), our method, when combined with existing multimodal recommendation algorithms, can further enhance performance, achieving an average improvement of around 12% in terms of both Recall and NDCG. ", "publication_ref": ["b39", "b40", "b22", "b20", "b21", "b23", "b19", "b24", "b41", "b41"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments on Taobao offline dataset", "text": "To evaluate the information increment of pre-trained image embeddings on the CTR prediction task, we use an architecture that aligns with our online recommender system. In essence, each item is associated with a unique item ID and features. The item ID is transformed into an embedding and concatenated with other features, including the image embedding we have learned, to form the item features. Users consist of user features and their behavioral history, where the behavioral history comprises numerous items. Therefore, the user behavioral history is aggregated using self-attention, which is then concatenated with other user features. Subsequently, the user and item features are concatenated and serve as inputs to an MLP. The MLP's final output is the user's click probability, representing the predicted CTR. We provide a detailed description of this CTR model in the Appendix. Downstream usage of image representations. Practically, we find that how we feed the image features into the downstream CTR model is critical for the final performance. We experimented with three different methods: 1. directly using embedding vectors. 2. using similarity scores to the target item. 3. using the cluster IDs of the embeddings. Cluster-ID is the best-performing method among the three methods, bringing about 0.1%-0.2% improvements on AUC compared to using embedding vectors directly. We attribute the success of Cluster-ID to its better alignment with our pre-training method. We provide a more detailed analysis in the Appendix. Pre-training dataset. The pre-training dataset is collected during 2022.11.18-2022.11.25 on our online search service. To reduce the computational burden, we down-sample to 20% negative samples. So the click-through rate (CTR) is increased to around 13%. To reduce the computational burden, we sort the PV items with their labels to list positive samples in the front, then we select the first 5 PV items to constitute the training targets (l pv = 5). We retain the latest 5 user-click-history items (l click = 5). Thus, there are at most 10 items in one sample. There are three reasons for such data reduction: First, our dataset is still large enough after reduction. Second, the number of positive items in PV sequences is less than 5 most of the time, so trimming PV sequences to 5 will not lose many positive samples, which is generally much more important than negatives. Third, we experimented with l click = 10 and did not observe significant improvements, while the training time is significantly longer. Thus, we keep l click = 5 and l pv = 5 fixed in all the experiments. We remove the samples without clicking history or positive items within the page. Intuitively, women's clothing is one of the most difficult recommendation tasks (the testing AUC is significantly lower than average) which also largely depends on the visual appearance of the items. Thus, we select the women's clothing category to form the dataset of the training dataset of the pre-training task. The statistics of the pre-training dataset are summarized in Table . (3).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Taobao pre-training and CTR dataset", "text": "In the pre-training dataset, we only retain the item images and the click labels. All other information, such as item title, item price, user properties, and even the query by the users are dropped. We report the experimental results of training with text information in Section. 4.3.4, which indicates that additional information is unnecessary. CTR dataset. The average daily statistics of the downstream CTR datasets in all categories and women's clothing are summarized in Table . (4). To avoid any information leakage, we use data collected from 2022.11.27 to 2022.12.04 on our online shopping platform to train the downstream CTR model, and we use data collected on 2022.12.05 to evaluate the performance. In the evaluation stage, the construction of the dataset aligns with our online system. We use all the available information to train the downstream CTR prediction model. The negative samples are also down-sampled to 20%. Different from the pre-training dataset, we do not group the page-view data in the evaluation dataset, so each sample corresponds to an item.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation metrics", "text": "\u2022 Area Under the ROC Curve (AUC): AUC is the most commonly used evaluation metric in evaluating ranking methods, which denotes the probability that a random positive sample is ranked before a random negative sample. \u2022 Grouped AUC (GAUC): The AUC is a global metric that ranks all the predicted probabilities. However, in the online item-searching task, only the relevant items are considered by the ranking stage, so the ranking performance among the recalled items (relevant to the user's query) is more meaningful than global AUC. Thus, we propose a Grouped AUC metric, which is the average AUC within searching sessions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Compared methods", "text": "\u2022 Baseline: Our baseline is a CTR model that serves our online system. It's noteworthy that we adopt a warmup strategy that uses our online model trained with more than one year's data to initialize all the weights (user ID embeddings, item ID embeddings, etc.), which is a fairly strong baseline. \u2022 Supervised: To pre-train image embeddings with user behavior information, a straightforward method is to train a CTR model with click labels and image backbone network end-to-end. We use the trained image network to extract embeddings as other compared methods.\n\u2022 SimCLR [7]: SimCLR is a self-supervised image pre-training method based on augmentations and contrastive learning. \u2022 SimSiam [8]: SimSiam is also an augmentation-based method. Different from Sim-CLR, SimSiam suggests that contrastive loss is unnecessary and proposes directly minimizing the distance between matched embeddings. \u2022 CLIP [9]: CLIP is a multi-modal pre-training method that optimizes a contrastive loss between image embeddings and item embeddings. We treat the item cover image and its title as a matched sample. We use a pre-trained BERT [43] as the feature network of item titles, which is also trained end-to-end. \u2022 MaskCLIP [44]: MaskCLIP is an improved version of CLIP with masked selfdistillation in images and text.\nTable 5 The improvements of AUC (\u2206AUC) in the women's clothing category. And performances of \u2206AUC, \u2206GAUC in all categories. We report the relative improvements compared to the Baseline method, and the raw values of the metrics are in parentheses.\nMethods \u2206AUC (Women's Clothing) \u2206AUC \u2206GAUC Baseline 0.00% (0.7785) 0.00% (0.8033) 0.00% (0.7355) Supervised +0.06% (0.7790) -0.14% (0.8018) -0.06% (0.7349) CLIP [9] +0.26% (0.7810) +0.04% (0.8036) -0.09% (0.7346) SimCLR [7] +0.28% (0.7812) +0.05% (0.8037) -0.08% (0.7347) SimSiam [8] +0.10% (0.7794) -0.10% (0.8022) -0.29% (0.7327) MaskCLIP [44] +0.31% (0.7815) +0.03% (0.8035) -0.03% (0.7352) COURIER(ours) +0.46% (0.7830) +0.16% (0.8048) +0.19% (0.7374)", "publication_ref": ["b6", "b7", "b8", "b42", "b43", "b8", "b6", "b7", "b43"], "figure_ref": [], "table_ref": []}, {"heading": "Performance in downstream CTR task", "text": "The performances of compared methods are summarized in Table . (5). We have the following conclusions: First, since all the methods are pre-trained in the women's clothing category, they all have performance improvement on the AUC of the downstream women's clothing category. SimCLR, SimSiam, CLIP, MaskCLIP and COURIER outperform the Baseline and Supervised pre-training. Among them, our COURIER performs best, outperforms baseline by 0.46% AUC and outperforms second best method MaskCLIP by 0.15% AUC, which verified our analysis that traditional CV pre-training methods provide little information gain to the CTR model. Secondly, we also check the performance in all categories. Our COURIER also performs best with 0.16% improvement in AUC. However, the other methods' performances are significantly different from the women's clothing category. The Vanilla and SimSiam method turned out to have a negative impact in all categories. And the improvements of CLIP, SimCLR and MaskCLIP become marginal. The reason is that the pre-training methods failed to extract general user interest information and overfit the women's clothing category. We analyze the performance in categories other than women's clothing in Section. 4.3.5. Thirdly, the performance on GAUC indicates that the performance gain of CLIP, SimCLR and MaskCLIP vanishes when we consider in-page ranking, which is indeed more important than global AUC, as discussed in Section. 4.2.2. The GAUC performance further validates that COURIER can learn fine-grained user interest features that can distinguish between in-page items.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Further experimental analysis", "text": "We have validated the efficacy of the COURIER approach in enhancing the overall performance of recommendation algorithms on both public datasets and the Taobao dataset. However, we also aim to address the following inquiries:\n1. Existing research has highlighted the significant impact of temperature coefficients in pre-training. Does the temperature coefficient similarly affect our pre-training task? 2. What is the respective contribution of each module in the COURIER approach towards performance improvement? 3. Currently, we only utilize image and user click information in the training of embeddings. Would the inclusion of other modalities, such as text, further enhance the performance? 4. As a pre-training method, can the embeddings acquired in the women's clothing category also yield improvements in other categories? 5. Can our method solely utilize user click information to learn features relevant to user intent?\nWe will address the aforementioned inquiries through experimental investigation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Influence of temperature", "text": "We experiment with different \u03c4 . The results are in Fig. 3. Note that the experiments are run with the Simscore method, which is worse than Cluster-ID but is much faster. We find that \u03c4 = 0.05 performs best for COURIER and keep it fixed. We conduct the following ablation experiments to verify the effect of each component of COURIER.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Ablation study", "text": "\u2022 w/o UCS: Remove the user click sequence loss.\n\u2022 w/o Contrast: Remove the contrastive loss, only minimize the reconstruction loss, similar to SimSiam [8].\n\u2022 w/o Reconstruction: Use self-attention instead of cross-attention in the user interest recommendation module. \u2022 w/o Neg PV: Remove negative PV samples, only use positive samples.\nThe results in Table. (6) indicate that all the proposed components are necessary for the best performance.", "publication_ref": ["b7", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Influence of batch size", "text": "Due to both theoretical [38] and practical evidence [7] indicating that increasing batch size in contrastive learning can enhance model generalization, we conducted experiments with different batch sizes. The results in Table. (7) also demonstrate that in our method, the larger the batch size, the better the performance.\nOn the flip side, augmenting the batch size necessitates a substantial increase in computational power. In our framework, configuring a batch size of 3072 requires 48 Nvidia V100 GPUs, while a batch size of 4096 demands 64 GPUs. Considering the diminishing returns associated with further escalating the batch size, coupled with the consideration of training costs, we ultimately opted for a batch size of 3072 for deployment.  Text information is important for searching and recommendation since it's directly related to the query by the users and the properties of items. Thus, raw text information is already widely used in real-world systems. The co-train of texts and images also shows significant performance gains in computer vision tasks such as classification and segmentation. So we are interested in verifying the influence to COURIER by co-training with text information. Specifically, we add a CLIP [9] besides COURIER, with the loss function becomes L = L COURIER + L CLIP . The CLIP loss is calculated with item cover images and item titles. However, such multi-task training leads to worse downstream CTR performance as shown in Table. (8), which indicates that co-training with the text information may not help generalization when the text information is available in the downstream task. In Fig. 4, we plot the AUC improvements of COURIER in different categories. We have the following conclusions: First, the performance improvement in the women's clothing category is the most, which is intuitive since the embeddings are trained with women's clothing data. Secondly, there are also significant improvements in women's shoe, children's shoe, children's clothing, underwear, etc. These categories are not used in the pre-training task, which indicates the COURIER method can learn general visual characteristics that reflect user interests. Thirdly, the performances in the bedding, cosmetics, knapsack, and handicrafts are also improved by more than 0.1%. These categories are significantly different from women's clothing in visual appearance, and COURIER also learned some features that are transferable to these categories. Fourthly, COURIER does not have a significant impact on some categories, and has a negative impact on the car category. These categories are less influenced by visual looking and can be ignored when using our method to avoid performance drop. Fifthly, the performance is also related to the amount of data. Generally, categories that have more data tend to perform better. Fig. 6 T-SNE visualization of embeddings with different style tags. We also plot some item images with different tags below the corresponding figures.", "publication_ref": ["b37", "b6", "b6", "b8", "b7"], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Generalization in unseen categories", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Visualization of trained embeddings", "text": "Did COURIER really learn features related to user interests? verified the quantitative improvements on CTR in Section. 4.2.4. Here, we also provide some qualitative analysis. During the training of COURIER, we did not use any additional information other than images and user clicks. Thus, if the embeddings contain some semantic information, such information must be extracted from user behaviors. So we plot some randomly selected embeddings with specific categories and style tags in Fig. 5 and Fig. 6. First, embeddings from different categories are clearly separated, which indicates that COURIER can learn categorical semantics from user behaviors. Secondly, some of the style tags can be separated, such as Cool vs. Sexy. The well-separated tags are also intuitively easy to distinguish. Thirdly, some of the tags can not be separated clearly, such as Mature vs. Cuties, and Grace vs. Antique, which is also intuitive since these tags have relatively vague meanings and may overlap. Despite this, COURIER still learned some gradients between the two concepts. To conclude, the proposed COURIER method can learn meaningful user-interest-related features by only using images and click labels.", "publication_ref": [], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "Online experiments and deployment", "text": "During the deployment of our image feature learning method, we devoted substantial efforts to optimizing the model's performance. Below, we summarize some of the impactful optimization measures that were undertaken: Image backbone and performance optimization. We use the Swin-tiny transformer [45] trained on Imagenet as the image backbone model, which is faster than the Swin model and has comparable performance. We train the backbone network end-to-end in the pre-training phase. However, since we have about 3 \u00d7 10 9 images in our pre-training dataset (230 times bigger than the famous Imagenet dataset, which has about 1.3 \u00d7 10 6 images), even the Swin-tiny model is slow. After applying gradient checkpointing [46], mixed-precision computation [47], and optimization on distributed IO of images, we managed to reduce the training time for one epoch on the pre-training dataset from 150 hours to 50 hours (consuming daily data in < 10 hours) with 48 Nvidia V100 GPUs (32GB). Efficient clustering. In our scenario, there are about N = 6 * 10 7 image embeddings to be clustered, and we set the cluster number to C = 10 5 . The computational complexity of vanilla k-means implementation is O(N * C * d) per iteration, which is unaffordable. Practically, we implement a high-speed learning-based clustering problem proposed in [48]. The computing time is reduced significantly from more than 7 days to about 6 hours. We will assign a new image to its closest cluster center ID. The amount of new items is relatively small and has little impact on the performance. We will re-train and replace all the cluster-IDs regularly (e.g., half a year). Online serving performance. To evaluate the performance improvement brought by COURIER on our online system, we conduct online A/B testing in our online shopping platform for 30-days. We report improvements on number of orders (\u2206#Order), click-through rate (\u2206 CTR), and cross merchandise volume (\u2206 GMV). Compared with the strongest deployed online baseline, COURIER significantly (p-value < 0.01) improves the CTR and GMV by +0.34% and +0.88% in women's clothing, respectively (the noise level is less than 0.1% according to the online A/A test). Such improvements are considered significant with the large volume of our online shopping platform. The model has also been successfully deployed into production, serving the main traffic.", "publication_ref": ["b44", "b45", "b46", "b47"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "Visually, the image information of a product has a significant impact on whether a user clicks. However, we have observed that the features extracted by existing image pre-training methods have limited utility for improving downstream CTR models. We attribute this phenomenon to the fact that the labels typically used in existing methods, such as those for classification or image-text retrieval, have already been applied as features in CTR models. Existing image pre-training methods are insufficient for effectively extracting features relevant to user interests. To address this issue, we propose a method for user interest reconstruction to extract image features relevant to user click behavior. Specifically, to mitigate the problem of images in a user's click history that may not be relevant to the current image, we employ an attention-based approach to identify images in the click sequence that are similar to the current image. We then attempt to reconstruct the user's next-click image through a weighted sum of the embeddings of these related images. Furthermore, to prevent trivial solutions, we optimize using a contrastive learning loss, reducing the reconstruction error for clicked images while increasing it for non-clicked images. Consequently, our method learns visual embeddings that influence whether a user clicks without relying on any downstream features directly. Instead, it enhances the information available for downstream utilization from the perspective of interest reconstruction. Experiments conducted on various datasets and large-scale online evaluations confirm that the embeddings learned by our method significantly enhance the performance of downstream recommendation models. statistical features are treated similarly to item features. The most important feature for personalized recommendation is the history item sequence. In our CTR model, we use three different item sequences: 1. Long-term click history consists of the latest up to 1000 clicks on the concerned category (the one the user is searching for) within 6 months. 2. Long-term payment history consists of up to 1000 paid items within 2 years. 3. The recent up to 200 item sequences in the current shopping cart. All the items are embedded by the item embedding network. The embedded item sequences are fed to multi-head attention and layer-norm layers. Then, the item embeddings are mean-pooled and projected to a proper dimension, which is concatenated with other user features to form the final user embeddings. Query embedding and CTR prediction network. The user queries are treated in the same way as item titles. The user embeddings, item embeddings, and query embeddings are flattened and concatenated into a single vector. Then, we use an MLP model with 5 layers to produce the logits for CTR prediction. The CTR model is trained with the cross entropy loss on the downstream user click data.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Appendix B Downstream usage of image embeddings", "text": "How the features are fed to the model is a critical factor that affects the performance of machine learning algorithms. For example, normalizing the inputs before input to neural networks is a well-known preprocessing that matters a lot. In our practice of pre-training and utilizing pre-trained embeddings in downstream tasks, we also find that the way we insert the pre-trained embeddings is critical to the downstream performance. We explore three different approaches: Vector, Similarity score, and Cluster ID.\nVector. The most straightforward and common practices of utilizing pre-trained embeddings are to use the embeddings as input to the downstream tasks directly. In such cases, the features of the item images are represented as embedding vectors, which is also the first approach we have tried. However, our experiments show that no matter how we train the image embeddings, the improvements in the CTR task are only marginal. We think the reason is that the embedding vectors are relatively hard to be used by the downstream model, so the downstream model ignores the embedding vectors. The existing recommender systems already use item IDs as features, and the embeddings of the IDs can be trained directly. So the IDs are much easier to identify an item than image embeddings, which require multiple layers to learn to extract related information. Thus, the model will achieve high performance by updating ID embeddings before the image-related weights can learn useful representations for CTR. Then the gradient vanishes because the loss is nearly converged, and the image-related weights won't update significantly anymore, which is also observed in our experiments.\nSimilarity Score. With the hypothesis about the embedding vectors, We experiment with a much-simplified representation of the embedding vectors. Specifically, assuming we want to estimate the CTR of an item with Img pv and a user of click history Img k click . The vector approach uses Emb k click and Emb pv as inputs directly. and the scores (each image corresponds to a real-valued score) are used as image features. The results are in Table . (B1). Experimental results indicate that the simple similarity score features perform significantly better than inserting embedding vectors directly, although the embedding vectors contain much more information. The performance of the similarity score method verified our hypothesis that embedding vectors may be too complex to be used in the CTR task. Cluster IDs. Our experiments on embedding vectors and similarity scores indicate that embedding vectors are hard to use and simply similarity scores contain information that can improve downstream performance. Can we insert more information than similarity scores but not too much? ID features are the most important features used in recommender systems, and it has been observed that ID features are easier to train than non-ID features, and perform better [49]. Thus, we propose to transform embedding vectors into ID features. A straightforward and efficient method is to hash the embedding vectors and use the hash values as IDs. However, hashing cannot retain semantic relationships between trained embeddings, such as distances learned in contrastive learning. Thus, we propose to use the cluster IDs to represent the embedding Where Sim(j 0 , j 0 ) measures the cosine similarity between E j0 pv and R j0 pv , and Sim(j 0 , j 1 ) measures the cosine similarity between E j0 pv and R j1 pv with j 1 \u0338 = j 0 , m > 0 is a tunable hyper-parameter corresponding to margin. By definition, we want to make Sim(j 0 , j 0 ) > Sim(j 0 , j 1 ). The hinge loss in Eq. (F5) will equal to 0 of Sim(j 0 , j 0 ) > Sim(j 0 , j 1 ) + m, and equal to Sim(j 0 , j 1 ) -Sim(j 0 , j 0 ) + m otherwise.\nIn triplet-based metric learning, sampling of positive and negative instances is required. In our scenario, positive instances have been constructed through user intention reconstruction, yet negative instances still need to be sampled. We randomly sample a mismatched reconstruction embedding from the batch as a negative instance to calculate the loss. We have tuned the implementation of this hinge loss, including parameters such as learning rate and margin parameter m. We keep other configurations the same as our method. Table . (F3) presents the best results we have achieved. The results obtained using triplet hinge loss are significantly worse than those obtained using contrastive loss, mainly for two reasons. Firstly, the performance of metric learning based on triplet hinge loss is highly sensitive to the sampling method [52], which may require complex sampling techniques to achieve better results. However, implementing complex sampling methods can increase training overhead, which is beyond the scope of our work. Secondly, contrastive loss allows adjusting the weight of negative examples by tuning the temperature \u03c4 parameter, effectively weighting hard negatives. This parameter tuning is simpler than implementing complex sampling methods and may lead to better results. Moreover, contrastive loss is more suitable for large-scale training.", "publication_ref": ["b48", "b51"], "figure_ref": [], "table_ref": []}, {"heading": "Appendix A CTR model", "text": "The CTR model consists of a user embedding network, an item embedding network, and a query embedding network. Item embedding network. The item embedding network takes the cover image of the item, the item ID, the item title, and various statistical features as input, as depicted in Fig. A1. The image features are fed by one of the three methods (Vector, Similarity score, Cluster ID). ID features are transformed into corresponding embeddings. The item titles are tokenized, then the tokens are converted to embeddings. All the features are then concatenated to form the item embeddings. User embedding network. The user embeddings consist of the embeddings of history item sequences, the user IDs, and other statistical features. The user IDs and vectors instead. Specifically, we run a clustering algorithm [48] on all the embedding vectors, then we use the ID of the nearest cluster center as the ID feature for each embedding vector. There are some benefits of using cluster IDs: First, the clustering algorithm is an efficient approximation method that are scalable to large data. Second, the cluster centers learned by the clustering algorithm have clear interpretations and retain the global and local distance structures. Third, since we are using Euclidean distance in the clustering algorithm, the learned cluster IDs can retain most of the distance information learned in the contrastive pre-training stage. From Table . (B1) we can conclude that the Cluster ID method consistently outperforms the Vector and SimScore method by a significant gap. The overall trend of SimScore and Cluster ID are similar, but evaluating with SimScore is much faster. Thus, practically we use the SimScore method to roughly compare different hyperparameters, and the most promising hyper-parameters are further evaluated with the cluster ID method.\nWhy Cluster-ID is suitable for contrastive pre-training methods. The cosine similarity is related with the Euclidean distance as follows:\nIf we constrain the embedding vectors to be \u21132 normalized, i.e., ||x|| = ||y|| = 1, which is the same as in contrastive learning. Then we have\nThus, maximizing the cosine similarity is equivalent to minimizing the Euclidean distance between \u21132 normalized vectors. Since we are optimizing the cosine similarity of the embedding vectors, we are indeed optimizing their Euclidean distances. And such distance information is retained by the clustering algorithm using Euclidean distances. By adjusting the number of clusters, we can also change the information to be retained. To conclude, the Cluster-ID method aligns with both the pre-training and downstream stages, resulting in better performance.", "publication_ref": ["b47"], "figure_ref": [], "table_ref": []}, {"heading": "Appendix C Why not use random masked prediction and self-attention?", "text": "Random masked prediction is a popular pre-training style, which may also be applied to our framework. However, considering the data-generating process, random masking is indeed unnecessary and may cause some redundant computation and information leakage. Because of the time series nature of user click history, we will train on all the possible reconstruction targets in click history. That is, we will train with (B; A) at first, where (A) is the history and B is the target. After the user has clicked on item C, we will train on (C; B, A), and so on. Thus, every item in the click history will be treated as a target exactly once.\nSuppose we adopted the random masking method, there is a chance that we train on (A; C, B), then we train on (C; A, B). Since the model has already observed the co-occurrence of A, B, and C, the next (C; A, B) prediction will be much easier. The random masking method also violates the potential causation that observing A and B causes the user's interest in C.\nMasked predictions typically use self-attention instead of cross attention with knowledge about the target as our proposed user intention reconstruction method. We also try with the self-attention method in the experiment section, which performs worse than our method. The reason is that learning to predict the next item with only a few click history is tough, but training to figure out how the next item can be reconstructed with the clicking history is much easier and can provide more meaningful signals during training. Most of the contrastive pre-training methods are equipped with projection heads. A projection head refers to one or multiple layers (typically an MLP) that is appended after the last embedding layer. During training, the contrastive loss (e.g., InfoNCE) is calculated with the output of the projection head instead of using the embeddings directly. After training, the projection heads are dropped, and the embeddings are used in downstream tasks, e.g, classification. It is widely reported that training with projection heads can improve downstream performance [7,8,50]. However, the reason for the success of the projection head is still unclear. We experiment with projection heads as an extension of COURIER, and the results are in Table . (D2). We find that projection heads have a negative impact on COURIER. There are two possible reasons: 1. In those contrastive methods, the downstream usage and the pre-training stage are inconsistent. In pre-training, the contrastive loss pushes embeddings and their augmented embeddings to be closer, which wipes away detailed information other than distance, while the distance itself cannot be used in classification. By adding projection heads, the projection head can learn the distance information without wiping away much detailed information. 2. In our COURIER method, we use the distance information learned in the pre-training stage directly by calculating the similarity score or cluster ID. Thus, the contrastive loss is consistent with the downstream usage. If we train with a projection head, the embeddings are not well suited for calculating the similarity scores since the embeddings are not trained to learn cosine similarity directly.", "publication_ref": ["b6", "b7", "b49"], "figure_ref": [], "table_ref": []}, {"heading": "Appendix D Does projection head help?", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Appendix E Implementation and Reproduction", "text": "Image Backbone. We adopt the swin-tiny implementation in Torchvision. The output layer of the backbone model is replaced with a randomly initialized linear layer, and we use weights pre-trained on the ImageNet dataset to initialize other layers. We apply gradient checkpoint [46] on all the attention layers in swin-tiny to reduce memory usage and enlarge batch size. We apply half-precision (float 16) computation [47] in all our models to accelerate computing and reduce memory. We train all the methods on a cluster with 48 Nvidia V100 GPUs (32GB), which enables single GPU batch size = 64, overall batch size = 64*48 = 3072. Note that each line of data contains 10 images, so the number of images within a batch is 30720 (some are padded zeros). The images are resized, cropped, and normalized before feeding to the backbone model. The input image size is 224.\nWe use the following hyperparameters for all the methods. Learning rate=1e-4, embedding size=256, weight decay=1e-6. We use the Adam optimizer. We have tuned these hyperparameters roughly but did not find significantly better choices on specific methods.\nHyperparameters of COURIER: We use \u03c4 = 0.05 as reported in the paper. Implementation of CLIP: We tune the \u03c4 within [0.1, 0.2, 0.5], and find that \u03c4 = 0.1 performs best, corresponding to the reported results. The batch size of CLIP is 32*48 since we have to reduce the batch size to load the language model. The effective batch size of CLIP is 32*48*5, since we only have titles of the PV items in our dataset. The text model is adapted from Chinese-BERT [43] and is initialized with the pre-trained weights provided by the authors.\nImplementation of SimCLR: We tune the \u03c4 with in [0.1, 0.2, 0.5], and find that \u03c4 = 0.2 performs best, corresponding to the reported results. The effective batch size of SimCLR is 64*48*10. We implemented the same augmentation strategy as suggested by the SimCLR paper [7].\nImplementation of SimSiam: The effective batch size of SimSiam is 64*48*10. The augmentation method is the same as SimCLR.", "publication_ref": ["b45", "b46", "b42", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Appendix F Experiment on hinge loss", "text": "From the perspective of metric learning [51], the current popular contrastive loss and the traditional hinge loss in metric learning serve similar purposes: they both aim to minimize the distance between similar embeddings. We agree that this is also a direction worth exploring.\nFollowing our definition of similarity between PV item embeddings and reconstruction in Eq. (F4)\nWhere j 0 enumerates over PV image embeddings, and j 1 enumerates over reconstructions. We can define a hinge loss as L hinge = max(0, Sim(j 0 , j 1 ) -Sim(j 0 , j 0 ) + m), j 1 \u0338 = j 0 (F5)", "publication_ref": ["b50"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Deep learning for click-through rate estimation", "journal": "", "year": "2021", "authors": "W Zhang; J Qin; W Guo; R Tang; X He"}, {"ref_id": "b1", "title": "Deep interest evolution network for click-through rate prediction", "journal": "", "year": "2019", "authors": "G Zhou; N Mou; Y Fan; Q Pi; W Bian; C Zhou; X Zhu; K Gai"}, {"ref_id": "b2", "title": "Beyond probability partitions: Calibrating neural networks with semantic aware grouping", "journal": "NeurIPS", "year": "2023", "authors": "J Yang; D Zhan; L Gan"}, {"ref_id": "b3", "title": "Generalized delayed feedback model with post-click information in recommender systems", "journal": "NeurIPS", "year": "2022", "authors": "J Yang; D Zhan"}, {"ref_id": "b4", "title": "Where to go next for recommender systems? ID-vs. modality-based recommender models revisited", "journal": "", "year": "2023", "authors": "Z Yuan; F Yuan; Y Song; Y Li; J Fu; F Yang; Y Pan; Y Ni"}, {"ref_id": "b5", "title": "Learnability with time-sharing computational resource concerns", "journal": "", "year": "2023", "authors": "Z.-H Zhou"}, {"ref_id": "b6", "title": "A simple framework for contrastive learning of visual representations", "journal": "", "year": "2020", "authors": "T Chen; S Kornblith; M Norouzi; G E Hinton"}, {"ref_id": "b7", "title": "Exploring simple siamese representation learning", "journal": "", "year": "2021", "authors": "X Chen; K He"}, {"ref_id": "b8", "title": "Learning transferable visual models from natural language supervision", "journal": "", "year": "2021", "authors": "A Radford; J W Kim; C Hallacy; A Ramesh; G Goh; S Agarwal; G Sastry; A Askell; P Mishkin; J Clark; G Krueger; I Sutskever"}, {"ref_id": "b9", "title": "The Adaptive Web, Methods and Strategies of Web Personalization", "journal": "", "year": "2007", "authors": "J B Schafer; D Frankowski; J L Herlocker; S Sen"}, {"ref_id": "b10", "title": "Amazon.com recommendations: Item-to-item collaborative filtering", "journal": "IEEE Internet Comput", "year": "2003", "authors": "G Linden; B Smith; J York"}, {"ref_id": "b11", "title": "Deep learning based recommender system: A survey and new perspectives", "journal": "ACM Comput. Surv", "year": "2019", "authors": "S Zhang; L Yao; A Sun; Y Tay"}, {"ref_id": "b12", "title": "Learning deep structured semantic models for web search using clickthrough data", "journal": "", "year": "2013", "authors": "P Huang; X He; J Gao; L Deng; A Acero; L P Heck"}, {"ref_id": "b13", "title": "Capturing delayed feedback in conversion rate prediction via elapsed-time sampling", "journal": "", "year": "2021", "authors": "J Yang; X Li; S Han; T Zhuang; D Zhan; X Zeng; B Tong"}, {"ref_id": "b14", "title": "Empowering news recommendation with pre-trained language models", "journal": "", "year": "2021", "authors": "C Wu; F Wu; T Qi; Y Huang"}, {"ref_id": "b15", "title": "Deep content-based music recommendation", "journal": "", "year": "2013", "authors": "A Oord; S Dieleman; B Schrauwen"}, {"ref_id": "b16", "title": "A hierarchical attention model for social contextual image recommendation", "journal": "IEEE Trans. Knowl. Data Eng", "year": "2020", "authors": "L Wu; L Chen; R Hong; Y Fu; X Xie; M Wang"}, {"ref_id": "b17", "title": "Deep neural networks for youtube recommendations", "journal": "", "year": "2016", "authors": "P Covington; J Adams; E Sargin"}, {"ref_id": "b18", "title": "Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering", "journal": "", "year": "2016", "authors": "R He; J J Mcauley"}, {"ref_id": "b19", "title": "MMGCN: multi-modal graph convolution network for personalized recommendation of micro-video", "journal": "", "year": "2019", "authors": "Y Wei; X Wang; L Nie; X He; R Hong; T Chua"}, {"ref_id": "b20", "title": "Dualgnn: Dual graph neural network for multimedia recommendation", "journal": "IEEE Trans. Multim", "year": "2023", "authors": "Q Wang; Y Wei; J Yin; J Wu; X Song; L Nie"}, {"ref_id": "b21", "title": "Mining latent structures for multimedia recommendation", "journal": "", "year": "2021", "authors": "J Zhang; Y Zhu; Q Liu; S Wu; S Wang; L Wang"}, {"ref_id": "b22", "title": "Self-supervised learning for multimedia recommendation", "journal": "IEEE Trans. Multim", "year": "2023", "authors": "Z Tao; X Liu; Y Xia; X Wang; L Yang; X Huang; T Chua"}, {"ref_id": "b23", "title": "Multi-view graph convolutional network for multimedia recommendation", "journal": "", "year": "2023", "authors": "P Yu; Z Tan; G Lu; B Bao"}, {"ref_id": "b24", "title": "Bootstrap latent representations for multi-modal recommendation", "journal": "", "year": "2023", "authors": "X Zhou; H Zhou; Y Liu; Z Zeng; C Miao; P Wang; Y You; F Jiang"}, {"ref_id": "b25", "title": "M5product: Self-harmonized contrastive learning for e-commercial multi-modal pretraining", "journal": "", "year": "2022", "authors": "X Dong; X Zhan; Y Wu; Y Wei; M C Kampffmeyer; X Wei; M Lu; Y Wang; X Liang"}, {"ref_id": "b26", "title": "Transrec: Learning transferable recommendation from mixture-of-modality feedback", "journal": "", "year": "2022", "authors": "J Wang; F Yuan; M Cheng; J M Jose; C Yu; B Kong; Z Wang; B Hu; Z Li"}, {"ref_id": "b27", "title": "Bootstrap your own latent -A new approach to self-supervised learning", "journal": "NeurIPS", "year": "2020", "authors": "J Grill; F Strub; F Altch\u00e9; C Tallec; P H Richemond; E Buchatskaya; C Doersch; B \u00c1 Pires; Z Guo; M G Azar; B Piot; K Kavukcuoglu; R Munos; M Valko"}, {"ref_id": "b28", "title": "BERT: pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2019", "authors": "J Devlin; M Chang; K Lee; K Toutanova"}, {"ref_id": "b29", "title": "Self-supervised learning for large-scale item recommendations", "journal": "", "year": "2021", "authors": "T Yao; X Yi; D Z Cheng; F X Yu; T Chen; A K Menon; L Hong; E H Chi; S Tjoa; J J Kang; E Ettinger"}, {"ref_id": "b30", "title": "Contrastive learning for representation degeneration problem in sequential recommendation", "journal": "", "year": "2022", "authors": "R Qiu; Z Huang; H Yin; Z Wang"}, {"ref_id": "b31", "title": "Towards universal sequence representation learning for recommender systems", "journal": "", "year": "2022", "authors": "Y Hou; S Mu; W X Zhao; Y Li; B Ding; J Wen"}, {"ref_id": "b32", "title": "Self-supervised graph learning for recommendation", "journal": "", "year": "2021", "authors": "J Wu; X Wang; F Feng; X He; L Chen; J Lian; X Xie"}, {"ref_id": "b33", "title": "Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer", "journal": "", "year": "2019", "authors": "F Sun; J Liu; J Wu; C Pei; X Lin; W Ou; P Jiang"}, {"ref_id": "b34", "title": "Intent-aware recommendation via disentangled graph contrastive learning", "journal": "", "year": "2023", "authors": "Y Wang; X Wang; X Huang; Y Yu; H Li; M Zhang; Z Guo; W Wu"}, {"ref_id": "b35", "title": "Disentangled contrastive collaborative filtering", "journal": "", "year": "2023", "authors": "X Ren; L Xia; J Zhao; D Yin; C Huang"}, {"ref_id": "b36", "title": "Attention is all you need", "journal": "", "year": "2017", "authors": "A Vaswani; N Shazeer; N Parmar; J Uszkoreit; L Jones; A N Gomez; L Kaiser; I Polosukhin"}, {"ref_id": "b37", "title": "On variational bounds of mutual information", "journal": "", "year": "2019", "authors": "B Poole; S Ozair; A Oord; A A Alemi; G Tucker"}, {"ref_id": "b38", "title": "Multimodal contrastive learning with LIMoe: the language-image mixture of experts", "journal": "", "year": "2022", "authors": "B Mustafa; C R Ruiz; J Puigcerver; R Jenatton; N Houlsby"}, {"ref_id": "b39", "title": "Image-based recommendations on styles and substitutes", "journal": "", "year": "2015", "authors": "J J Mcauley; C Targett; Q Shi; A Hengel"}, {"ref_id": "b40", "title": "VBPR: visual bayesian personalized ranking from implicit feedback", "journal": "", "year": "2016", "authors": "R He; J J Mcauley"}, {"ref_id": "b41", "title": "Mmrec: Simplifying multimodal recommendation", "journal": "", "year": "2023", "authors": "X Zhou"}, {"ref_id": "b42", "title": "Chinesebert: Chinese pretraining enhanced by glyph and pinyin information", "journal": "", "year": "2021", "authors": "Z Sun; X Li; X Sun; Y Meng; X Ao; Q He; F Wu; J Li"}, {"ref_id": "b43", "title": "Maskclip: Masked self-distillation advances contrastive language-image pretraining", "journal": "", "year": "2023", "authors": "X Dong; J Bao; Y Zheng; T Zhang; D Chen; H Yang; M Zeng; W Zhang; L Yuan; D Chen; F Wen; N Yu"}, {"ref_id": "b44", "title": "Swin transformer: Hierarchical vision transformer using shifted windows", "journal": "", "year": "2021", "authors": "Z Liu; Y Lin; Y Cao; H Hu; Y Wei; Z Zhang; S Lin; B Guo"}, {"ref_id": "b45", "title": "Training deep nets with sublinear memory cost", "journal": "", "year": "2016", "authors": "T Chen; B Xu; C Zhang; C Guestrin"}, {"ref_id": "b46", "title": "Mixed precision training", "journal": "ICLR", "year": "2018", "authors": "P Micikevicius; S Narang; J Alben; G F Diamos; E Elsen; D Garc\u00eda; B Ginsburg; M Houston; O Kuchaiev; G Venkatesh; H Wu"}, {"ref_id": "b47", "title": "A single-pass algorithm for efficiently recovering sparse cluster centers of high-dimensional data", "journal": "", "year": "2014", "authors": "J Yi; L Zhang; J Wang; R Jin; A K Jain"}, {"ref_id": "b48", "title": "Towards understanding the overfitting phenomenon of deep click-through rate models", "journal": "", "year": "2022", "authors": "Z Zhang; X Sheng; Y Zhang; B Jiang; S Han; H Deng; B Zheng"}, {"ref_id": "b49", "title": "Big selfsupervised models are strong semi-supervised learners", "journal": "NeurIPS", "year": "2020", "authors": "T Chen; S Kornblith; K Swersky; M Norouzi; G E Hinton"}, {"ref_id": "b50", "title": "Improved deep metric learning with multi-class n-pair loss objective", "journal": "", "year": "2016", "authors": "K Sohn"}, {"ref_id": "b51", "title": "Hard negative examples are hard, but useful", "journal": "", "year": "2020", "authors": "H Xuan; A Stylianou; X Liu; R Pless"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Fig. 11Fig.1(a) Existing image feature learning methods are tailored for cross-modal prediction tasks. (b) We propose a user intention reconstruction method to mine potential visual features that cannot be reflected by cross-modal labels. In this example, the user searched for \"Coat\" and received two recommendations (Page-viewed items). The user clicked on the one on the right. Through our user intention reconstruction, we identified similar items from the user's click history with larger attention, the reconstructed PV item embeddings are denoted as R j pv . Then, we optimize the PV embeddings E j pv and reconstructions R j pv to be closer if the corresponding item is clicked and more far apart otherwise.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Fig. 22Fig. 2 The contrastive user intention reconstruction method. The images are fed into the image backbone model to obtain the corresponding embeddings. The embeddings of PV (Page-View) sequences are blue-colored, and the embeddings of click sequences are yellow-colored. The reconstructions are in green. Red boxes denote positive PV items.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Fig. 33Fig.3The impact of different values of temperature \u03c4 on the performance of downstream CTR tasks. The horizontal axis represents the values of \u03c4 , while the vertical axis denotes the change (%) in the metrics.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Fig. 44Fig.4The AUC improvements of COURIER compared to the Baseline on different categories. The x-axis is sorted by the improvements.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Fig. 5 T5Fig. 5 T-SNE visualization of embeddings in different categories.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Statistics of public datasets.", "figure_data": "# User # Items # InteractionsBaby194457050160792Sports3559818357296337Clothing3938723033278677"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Averge Recall and NDCG performance comparison on public datasets.", "figure_data": "DatasetSportsBabyClothingMethodR@20R@50N@50R@20R@50N@50R@20R@50N@50BPR0.0040.0080.0030.0070.0140.0050.0040.0070.002SlmRec0.0060.0160.0060.0140.0280.0110.0040.0100.003DualGNN0.0120.0230.0090.0180.0370.0140.0070.0140.005LATTICE0.0120.0210.0080.0100.0220.008---MGCN0.0150.0270.0110.0170.0320.0130.0110.0190.008MMGCN0.0150.0280.0120.0240.0610.0240.0080.0170.006BM30.0180.0330.0140.0310.0650.0260.0090.0160.006MMGCN+ours0.0170.0310.0130.0300.0610.0240.0100.0190.007BM3+ours0.0190.0340.0150.0350.0680.0270.0120.0190.007"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Pre-training dataset collected from the women's clothing category.", "figure_data": "# User# Item# SamplesCTR # Hist # PV Items71.7 million 35.5 million 311.6 million 0.1355"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Daily average statistics of the downstream dataset.", "figure_data": "# User# Item# SamplesCTR# HistAll0.118 billion0.117 billion4.64 billion0.13998Women's 26.39 million 12.29 million 874.39 million 0.145111.3"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Ablation studies of COURIER.", "figure_data": "\u2206AUC (women's clothing) \u2206AUC \u2206GAUCw/o UCS0.06%-0.13%0.11%w/o Contrast0.23%0.03%-0.11%w/o Reconstruction0.25%0.02%-0.11%w/o Neg PV0.30%0.07%-0.06%COURIER0.46%0.16%0.19%"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Influence of different batch on performance", "figure_data": "Batch Size Femal AUC AUCGAUC640.15% -0.06%-0.08%2560.23%0.04%0.02%5120.36%0.09%0.10%20480.43%0.15%0.17%30720.46%0.16%0.19%40960.47%0.16%0.21%4.3.4 Train with text information"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Train with text information.", "figure_data": "\u2206AUC (women's clothing) \u2206AUC \u2206GAUCw CLIP0.26%0.04%-0.09%COURIER0.46%0.16%0.19%"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "The A/B testing improvements of COURIER.", "figure_data": "\u2206 # Order \u2206 CTR\u2206 GMVAll categories+0.1%+0.18% +0.66%Women's clothing +0.31%+0.34% +0.88%"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Instead, we calculate a cosine similarity score for each click history items, sim(Img pv , Img k click ) =", "figure_data": "Img T pv Img k click ||Img pv || ||Img k click ||(B1)"}, {"figure_label": "B1", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Performance of inserting image information with Vector, SimScore, and Cluster ID. Since we performed this comparison in the early stage of our development, the exact configurations of each version are hard to describe in detail. And the different versions may not be comparable to each other (different training data sizes, learning rates, training methods, etc.). We only list the version number for clarity. Results within each row are comparable since they are generated from the same version of embeddings. The Baseline does not use images. \"-\" denotes that we did not evaluate Cluster-ID of these versions.", "figure_data": "VectorSimScore Cluster IDBaseline0.00%0.00%0.00%V10.07%0.14%0.23%V20.08%0.16%0.23%V30.09%0.18%0.28%V40.06%0.16%0.22%V50.00%0.11%-V6-0.02%0.07%-V7-0.04%-0.01%-V80.04%0.13%-V90.05%0.09%-"}, {"figure_label": "F3", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Train with hinge loss.", "figure_data": "\u2206AUC (women's clothing) \u2206AUC \u2206GAUCHinge loss0.15%0.02%0.02%COURIER0.46%0.16%0.19%"}], "formulas": [{"formula_id": "formula_0", "formula_text": "Attention(Q, K, V ) = softmax( QK T \u221a d K )V (1", "formula_coordinates": [6.0, 223.87, 333.77, 267.09, 25.42]}, {"formula_id": "formula_1", "formula_text": ")", "formula_coordinates": [6.0, 490.96, 342.09, 4.24, 8.74]}, {"formula_id": "formula_2", "formula_text": "R j pv = Attention(E j pv , E click , E click ) (2) = \u03b1 k E k click (3", "formula_coordinates": [6.0, 233.01, 569.04, 262.2, 30.99]}, {"formula_id": "formula_3", "formula_text": ")", "formula_coordinates": [6.0, 490.96, 589.41, 4.24, 8.74]}, {"formula_id": "formula_4", "formula_text": "Sim(j 0 , j 1 ) = E j0 pv T R j1 pv ||E j0 pv || ||R j1 pv ||(4)", "formula_coordinates": [7.0, 224.69, 427.86, 245.99, 29.07]}, {"formula_id": "formula_5", "formula_text": "L pv = L contrast (E pv , E click , y)(5)", "formula_coordinates": [7.0, 157.63, 483.42, 313.05, 9.65]}, {"formula_id": "formula_6", "formula_text": "= 1](6)", "formula_coordinates": [7.0, 394.86, 508.5, 75.82, 8.74]}, {"formula_id": "formula_7", "formula_text": "corresponding embeddings E 0 click , ..., E l click -1 click", "formula_coordinates": [8.0, 124.59, 382.34, 196.51, 13.62]}, {"formula_id": "formula_8", "formula_text": "L ucs = L contrast (E l click -1 click , E 0:l click -1 click , 1)", "formula_coordinates": [8.0, 223.4, 430.3, 172.99, 13.62]}, {"formula_id": "formula_9", "formula_text": "L COURIER = L pv + L ucs(7)", "formula_coordinates": [8.0, 257.96, 528.66, 237.24, 10.04]}], "doi": ""}
