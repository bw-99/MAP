{"RAT: Retrieval-Augmented Transformer for Click-Through Rate Prediction": "", "Yushen Li \u2217 Jinpeng Wang \u2217": "", "Tao Dai": "Tsinghua Shenzhen International Graduate School, Tsinghua University Shenzhen, China liyushen22@mails.tsinghua.edu.cn wjp20@mails.tsinghua.edu.cn Jieming Zhu \u2020 Huawei Noah's Ark Lab Shenzhen, China jiemingzhu@ieee.org Jun Yuan Huawei Noah's Ark Lab Shenzhen, China yuanjunfy@163.com College of Computer Science and Software Engineering, Shenzhen University Shenzhen, China daitao.edu@gmail.com", "Rui Zhang": "www.ruizhang.info Shenzhen, China rayteam@yeah.net", "ABSTRACT": "", "Shu-Tao Xia \u2020": "Tsinghua Shenzhen International Graduate School, Tsinghua University Shenzhen, China Research Center of Artificial Intelligence, Peng Cheng Laboratory Shenzhen, China xiast@sz.tsinghua.edu.cn", "KEYWORDS": "Predicting click-through rates (CTR) is a fundamental task for Web applications, where a key issue is to devise effective models for feature interactions. Current methodologies predominantly concentrate on modeling feature interactions within an individual sample, while overlooking the potential cross-sample relationships that can serve as a reference context to enhance the prediction. To make up for such deficiency, this paper develops a R etrievalA ugmented T ransformer (RAT), aiming to acquire fine-grained feature interactions within and across samples. By retrieving similar samples, we construct augmented input for each target sample. We then build Transformer layers with cascaded attention to capture both intra- and cross-sample feature interactions, facilitating comprehensive reasoning for improved CTR prediction while retaining efficiency. Extensive experiments on real-world datasets substantiate the effectiveness of RAT and suggest its advantage in long-tail scenarios. The code has been open-sourced at https://github.com/YushenLi807/WWW24-RAT.", "CCS CONCEPTS": "", "\u00b7 Information systems \u2192 Recommender systems .": "This work is licensed under a Creative Commons Attribution International 4.0 License. WWW'24 Companion, May 13-17, 2024, Singapore, Singapore \u00a9 2024 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-0172-6/24/05 https://doi.org/10.1145/3589335.3651550 Retrieval-augmented learning, CTR prediction, Transformer, crosssample interaction", "ACMReference Format:": "Yushen Li, Jinpeng Wang, Tao Dai, Jieming Zhu, Jun Yuan, Rui Zhang, and Shu-Tao Xia. 2024. RAT: Retrieval-Augmented Transformer for ClickThrough Rate Prediction. In Companion Proceedings of the ACM Web Conference 2024 (WWW '24 Companion), May 13-17, 2024, Singapore, Singapore. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3589335.3651550", "1 INTRODUCTION": "Click-through rate (CTR) prediction is a binary classification task that aims to forecast whether a user will click on a given item. It has been broadly applicable in commercial fields such as advertising placement and recommender systems [4, 9, 11, 17]. Feature interaction modeling plays an essential role in CTR prediction. As shown in Figure 1, traditional methods [2, 10, 13, 15] primarily focus on feature interactions within each sample, but seldom consider cross-sample information that can serve as a reference context to enhance the prediction. Since features and their interactions are usually sparse, it necessitates CTR models to capture and memorize all interaction patterns, posing challenges in robustness and scalability. Recently, retrieval-augmented (RA) learning has shown effective in natural language processing [5] and computer vision [1], whose typical idea is to retrieve similar samples and enhance model prediction with these external demonstrations. Inspired by its success in relieving long-tail problems [12], we believe it is a promising paradigm to relieve the aforementioned issue in CTR prediction. In this direction, RIM [14], DERT [19] and PET [3] are three preliminary works on RA CTR prediction. However, they either compromise intra- or cross-sample feature interaction, which are still sub-optimal practices. Specifically, RIM simply aggregates retrieved samples on each feature field, which sacrifices fine-grained label embedding is unknown and padded USER ID ITEM ID CITY CATEGORY BRAND CLICK U1 I1 Rome shoes B1 0 U2 I2 LA gloves B2 1 U2 I3 NYC shoes B3 0 U3 I1 LA dress B3 ? Target Row Cross Row (Sample) Interaction Cross Column Interaction (Intra Sample Interaction) cross-sample knowledge. Although DERT improves RIM on sample retrieval and supplies better reference samples to the target sample, it still follows the same retrieval-augmented modeling as RIM, thereby sharing common limitations. PET constructs a hypergraph to represent the relations between rows and columns of tabular data. Via the star expansion, it converts the hypergraph into a heterogeneous graph that includes label nodes and feature nodes. PET can effectively capture the cross-sample information by designing a message-passing mechanism, but all feature nodes need to be transitioned through label nodes on the graph, showing inflexibility in modeling intra-sample feature interactions. To remedy the shortcomings in previous works, we propose a unified framework termed R etrievalA ugmented T ransformer (RAT) to enhance fine-grained intra- and cross-sample feature interactions for CTR prediction. Given a target sample, we retrieve similar samples from a reference pool ( e.g. , historical logs) using the sparse retrieval algorithm. Then we develop a Transformer-based model to acquire fine-grained feature interactions within and across samples. In particular, we find that intra-cross cascade attention not only improves the efficiency beyond joint modeling but also enhances the robustness of RAT. Without bells and whistles, we condense the semantic information to one token representation, which is fed to the binary classifier to make the final prediction. We conduct extensive experiments on three real-world datasets: ML-Tag, KKBox, and Tmall, demonstrating the promise of retrievalaugmented approaches and the further improvement of RAT. We also show that RAT can enhance long-tail sample prediction, which suggests its capacity to tackle feature sparsity and cold start issues. To summarize, we make the following contributions: \u2663 We propose a Retrieval-augmented Transformer (RAT) for CTR prediction, which enhances fine-grained intra- and cross-sample feature interaction in a unified model. \u2663 We find that intra-cross cascade attention not only improves the efficiency but also enhances the robustness of RAT (\u00a73.3.1). \u2663 Extensive experiments on real-world datasets validate RAT's efficacy and suggest its advantage against the feature sparsity and cold start issues.", "2 THE PROPOSED METHOD": "We design RE trieval-augmented T ransformer (RAT), considering both intra- and cross-sample interactions for CTR prediction. Figure 2 briefly illustrates the framework of RAT.", "2.1 Retrieve Similar Samples as Context": "Given the \ud835\udc39 -field record \ud835\udc65 \ud835\udc56 = [ \ud835\udc65 1 \ud835\udc56 ; ... ; \ud835\udc65 \ud835\udc39 \ud835\udc56 ] of a target sample, we search for similar samples as the reference context from a reserved sample pool P . We use BM25 [16] for retrieval because of its training-free nature, which also aligns with previous works [3, 14]. Specifically, the relevant score of query \ud835\udc65 \ud835\udc56 and the key \ud835\udc65 \ud835\udc50 of a candidate sample ( \ud835\udc65 \ud835\udc50 , \ud835\udc66 \ud835\udc50 ) \u2208 P is defined as where I {\u00b7} is an indicator function. \ud835\udc41 P is the number of samples in P , while \ud835\udc41 P ( \ud835\udc65 \ud835\udc53 \ud835\udc61 ) denotes the number of samples containing the feature \ud835\udc65 \ud835\udc53 \ud835\udc61 in P . Different from Du et al. [3], Qin et al. [14] that implemented the retrieval with Elasticsearch on CPUs, we provide an efficient GPU-based implementation to enable faster speed. Finally, we retrieve \ud835\udc3e samples from P with the highest scores: Notes on Avoiding Information Leakage. We sort the samples in chronological order if there is timestamp information and restrict a query only to retrieve samples that occur earlier than it. For validation and testing, we take the whole training set as the reference pool. This strategy still satisfies the restriction and is safe, because the testing set and the validation set are the latest and the next latest parts of the whole dataset in our experiments.", "2.2 Construct Retrieval-augmented Input": "We build an embedding layer to transform discrete features into \ud835\udc37 -dimensional embedding vectors. In particular, we treat the labels of retrieved samples as special features and also build an embedding table for that field. Let us denote the set of embedding tables as E = { \ud835\udc6c 1 , \ud835\udc6c 2 , \u00b7 \u00b7 \u00b7 , \ud835\udc6c \ud835\udc6d , \ud835\udc73 } , where \ud835\udc6c \ud835\udc87 is the embedding table of the \ud835\udc53 -th feature field. \ud835\udc73 \u2208 R 3 \u00d7 \ud835\udc37 is the label embedding table, where the 0-th and the 1-st slots are for unclick and click labels of retrieved samples, respectively, and the 2-nd slot is reserved for the target sample to represent the unknown click label ( i.e. , [UNK] ). For a retrieved sample ( \ud835\udc65 \ud835\udc50 \ud835\udc58 , \ud835\udc66 \ud835\udc50 \ud835\udc58 ) , we lookup feature embeddings and the label embedding from the embedding tables and obtain \ud835\udc38 \ud835\udc50 \ud835\udc58 = [ \ud835\udc59 \ud835\udc50 \ud835\udc58 ; \ud835\udc52 1 \ud835\udc50 \ud835\udc58 ; \ud835\udc52 2 \ud835\udc50 \ud835\udc58 ; \u00b7 \u00b7 \u00b7 ; \ud835\udc52 \ud835\udc39 \ud835\udc50 \ud835\udc58 ] \u2208 R ( \ud835\udc39 + 1 ) \u00d7 \ud835\udc37 . Analogously, for the target record \ud835\udc65 \ud835\udc56 , we obtain \ud835\udc38 \ud835\udc56 = [ \ud835\udc59 [UNK] \ud835\udc56 ; \ud835\udc52 1 \ud835\udc56 ; \ud835\udc52 2 \ud835\udc56 ; \u00b7 \u00b7 \u00b7 ; \ud835\udc52 \ud835\udc39 \ud835\udc56 ] \u2208 R ( \ud835\udc39 + 1 ) \u00d7 \ud835\udc37 . Finally, the retrieval-augmented input for the target record \ud835\udc65 \ud835\udc56 is obtained by stacking \ud835\udc38 \ud835\udc56 and \ud835\udc38 \ud835\udc50 1 , \ud835\udc38 \ud835\udc50 2 , \u00b7 \u00b7 \u00b7 , \ud835\udc38 \ud835\udc50 \ud835\udc3e together:", "2.3 Intra- and Cross-sample Feature Interaction": "To integrate intra- and cross-sample feature interactions to enhance CTR prediction, a na\u00efve idea is to unstack all retrieved samples, append them to the target record as extra feature fields, and use joint attention to model full feature interactions. However, it exhibits an efficiency issue: the complexity of each joint self-attention is O(( \ud835\udc3e + 1 ) 2 \u00b7 ( \ud835\udc39 + 1 ) 2 ) . We also find it inferior in performance (Table 4), possibly due to the influence of noisy feature interactions. To address the above issues, we decompose the joint attention. As shown in Figure 2, each RAT block comprises the cascade of an Prediticon Head GLYPH(cmap:d835) GLYPH(cmap:d835) GLYPH(cmap:d835) GLYPH(cmap:d835) [GLYPH(cmap:d835)GLYPH(cmap:d835)GLYPH(cmap:d835)] \u2026 GLYPH(cmap:d835) GLYPH(cmap:d835) Embedding Layer target sample query Search Engine Reference 11 Pool retrieve reference samples \u211bGLYPH(cmap:d835) \u2026 GLYPH(cmap:d835)1 GLYPH(cmap:d835) GLYPH(cmap:d835) GLYPH(cmap:d835)2 GLYPH(cmap:d835) GLYPH(cmap:d835) GLYPH(cmap:d835) GLYPH(cmap:d835)1 GLYPH(cmap:d835) GLYPH(cmap:d835)1 GLYPH(cmap:d835) GLYPH(cmap:d835)2 GLYPH(cmap:d835) GLYPH(cmap:d835)2 GLYPH(cmap:d835) GLYPH(cmap:d835) GLYPH(cmap:d835) GLYPH(cmap:d835) GLYPH(cmap:d835) GLYPH(cmap:d835) \u2026 \u2026 \u2026 RAT Block \u00d7 L MLP GLYPH(cmap:d835) GLYPH(cmap:d835) GLYPH(cmap:d835) GLYPH(cmap:d835) GLYPH(cmap:d835) GLYPH(cmap:d835) Cross Block Layer Norm Layer Norm Cross-sample Attention GLYPH(cmap:d835) GLYPH(cmap:d835) GLYPH(cmap:d835) GLYPH(cmap:d835) GLYPH(cmap:d835) GLYPH(cmap:d835) Intra Block Layer Norm Intra-sample Attention intra-block, a cross-block, and a multi-layer perception (MLP). The forward process of the \u2113 -th RAT block is formulated as where \ud835\udc4b \u2113 \ud835\udc56 denote the input of the \u2113 -th RAT block. \ud835\udc4b 0 \ud835\udc56 = \u02dc \ud835\udc38 \ud835\udc56 . \ud835\udc3b \u2113 \ud835\udc56 and \ud835\udc3b \u2032 \u2113 \ud835\udc56 denote the hidden states. LN (\u00b7) is the layer norm operation. ISA (\u00b7) and CSA (\u00b7) represent the intra-sample and cross-sample attention modules, respectively, which perform multi-head selfattentions along the field-axis and sample-axis, respectively. Compared with the vanilla joint attention, our design of cascaded attention favorably reduces the complexity to O(( \ud835\udc3e + 1 ) 2 +( \ud835\udc39 + 1 ) 2 ) , which helps to keep efficient. We will also investigate more block designs in experiments (\u00a73.3.1) and demonstrate the advantages of cascaded attention for RAT.", "3 EXPERIMENTS": "", "3.1 Experimental Setup": "3.1.1 Datasets. . We evaluate RAT on three popular datasets. MLtag [7], KKBox [20] and Tmall [3]. We follow the standard CTR prediction benchmark, BARS [20], to split ML-tag to train, validation and test sets by 7:2:1, and split KKBox by 8:1:1. For Tmall, we follow [14] by dividing the data based on global timestamps to avoid information leakage. Dataset statistics is shown in Table 1. 3.1.2 Metrics. We use two commonly used metrics, AUC (Area Under ROC) and Logloss (cross entropy) as the evaluation metrics. 3.1.3 Baselines. We select two types of baselines. ( i ) Traditional (and not retrieval-augmented) models: DeepFM [4], xDeepFM [11], DCNv2 [18], AOANet [8]; ( ii ) Retrieval-augmented models: RIM [14], PET [3]. For fair comparison, we use BM25 for aligning settings. DERT [19] is not included in our benchmark because it is a special case of RIM with a new dense retrieval setting. Besides, there is no open-sourced implementation for it, making fair comparison hard. 3.1.4 Implementation Details. For fair comparisons, we implement all methods with FuxiCTR [21] and follow the BARS [20] benchmark settings. For retrieval-augmented models, we set \ud835\udc3e to 5 by default.", "3.2 Comparison with State-of-the-arts": "3.2.1 Overall Performance. We report the model performances in Table 2. Note that for CTR prediction, a \u2030-level AUC increase is considered acceptable, as even such a minor improvement, if statistically significant, can lead to substantial gains in revenue [6, 11]. According to the results in Table 2, retrieval-augmented (RA) models generally outperform traditional models, indicating that incorporating cross-sample information is effective in enhancing CTR prediction. Although PET does not surpass xDeepFM in overall performance, mainly due to its insufficient capability to capture intra-sample feature interactions, it exhibits superiority over other traditional models. Moreover, RAT outperforms the SoTA RA model, RIM, demonstrating the strong capability of Transformer in modeling fine-grained intra- and cross-sample interactions. 3.2.2 Performance on Long-tail Data. Inspired by the success of RA mechanisms on long-tail recognition [12], here we pose a question: Can RAT enhance CTR prediction on long-tail data? Taking ML-tag for a case study, we investigate the model efficacy on sample subsets associated with tailed 10% and 20% users, respectively, ranked by user's sample amount. As shown in Table 3, RA models perform better than traditional ones, indicating RA context indeed assists reasoning over sparse features and their interaction. Furthermore, RAT outperforms RIM and PET considerably, validating that Transformer-based fine-grained modeling improves the utilization of retrieved samples. The evidence suggests RAT's capability in addressing important issues like feature sparsity and cold start.", "3.3 Model Analysis": "3.3.1 Designs of RAT Block. In addition to the proposed design, we further explore more variants: ( i ) Intra-Cross J oint M odeling ( JM ) : The vanilla joint attention modeling for all intra- and crosssample feature interactions. ( ii ) Intra-Cross C ascaded E ncoder ( CE ) : Rather than the cascaded attentions in a single block, it separates the intra- and cross-sample attention into two cascaded Transformer blocks, yielding a double block number compared to other variants. ( iii ) Intra-Cross P arallel A ttention ( PA ) : It designs intra- and cross-sample attentions as two parallel branches. The hidden dimension of each branch is halved, and the outputs of the two branches are concatenated. We report the experimental results on model performance and efficiency in Tables 4 and 5, respectively. From Table 5 we can see that decomposed modeling designs are indeed more effective than joint modeling ( i.e. , JM ). From Table 4 we can learn that joint modeling is not the best practice, possibly due to the influence of noisy feature interactions. In contrast, attention decomposition provides an inductive bias for model robustness and leads to better performance in general. Comparing RAT and RAT CE , block-level decomposition does not bring extra gain upon attention-level decomposition. Comparing RAT and RAT PA , cascaded attention performs slightly better, suggesting that the successive order benefits the feature interaction modeling.", "4 CONCLUSION": "In this paper, we present R etrievalA ugmented T ransformer (RAT) for CTR prediction. It highlights the importance of cross-sample information and explores Transformer-based architectures to capture fine-grained feature interactions within and between samples, effectively remedying the shortcomings of existing works. By decomposing the intra- and cross-sample interaction modeling, RAT enjoys better efficiency while further enhancing the robustness. We conduct comprehensive experiments to validate the effectiveness of RAT, and show its advantage in tackling long-tail data.", "ACKNOWLEDGMENTS": "This work is supported in part by the National Natural Science Foundation of China under Grant (62302309, 62171248), Shenzhen Science and Technology Program (JCYJ20220818101012025), and the PCNL KEY project (PCL2023AS6-1). We gratefully acknowledge the support of MindSpore 1 , which is a new deep learning framework for this research.", "REFERENCES": "[1] Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W Cohen. 2023. ReImagen: Retrieval-Augmented Text-to-Image Generator. In ICLR . [2] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & Deep Learning for Recommender Systems. In RecSys ."}
