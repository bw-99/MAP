{
  "RAT: Retrieval-Augmented Transformer for Click-Through Rate Prediction": "",
  "Yushen Li ∗ Jinpeng Wang ∗": "",
  "Tao Dai": "Tsinghua Shenzhen International Graduate School, Tsinghua University Shenzhen, China liyushen22@mails.tsinghua.edu.cn wjp20@mails.tsinghua.edu.cn Jieming Zhu † Huawei Noah's Ark Lab Shenzhen, China jiemingzhu@ieee.org Jun Yuan Huawei Noah's Ark Lab Shenzhen, China yuanjunfy@163.com College of Computer Science and Software Engineering, Shenzhen University Shenzhen, China daitao.edu@gmail.com",
  "Rui Zhang": "www.ruizhang.info Shenzhen, China rayteam@yeah.net",
  "ABSTRACT": "",
  "Shu-Tao Xia †": "Tsinghua Shenzhen International Graduate School, Tsinghua University Shenzhen, China Research Center of Artificial Intelligence, Peng Cheng Laboratory Shenzhen, China xiast@sz.tsinghua.edu.cn",
  "KEYWORDS": "Predicting click-through rates (CTR) is a fundamental task for Web applications, where a key issue is to devise effective models for feature interactions. Current methodologies predominantly concentrate on modeling feature interactions within an individual sample, while overlooking the potential cross-sample relationships that can serve as a reference context to enhance the prediction. To make up for such deficiency, this paper develops a R etrievalA ugmented T ransformer (RAT), aiming to acquire fine-grained feature interactions within and across samples. By retrieving similar samples, we construct augmented input for each target sample. We then build Transformer layers with cascaded attention to capture both intra- and cross-sample feature interactions, facilitating comprehensive reasoning for improved CTR prediction while retaining efficiency. Extensive experiments on real-world datasets substantiate the effectiveness of RAT and suggest its advantage in long-tail scenarios. The code has been open-sourced at https://github.com/YushenLi807/WWW24-RAT.",
  "CCS CONCEPTS": "",
  "· Information systems → Recommender systems .": "∗ Both authors contributed equally to this work. † Corresponding authors. This work is licensed under a Creative Commons Attribution International 4.0 License. WWW'24 Companion, May 13-17, 2024, Singapore, Singapore © 2024 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-0172-6/24/05 https://doi.org/10.1145/3589335.3651550 Retrieval-augmented learning, CTR prediction, Transformer, crosssample interaction",
  "ACMReference Format:": "Yushen Li, Jinpeng Wang, Tao Dai, Jieming Zhu, Jun Yuan, Rui Zhang, and Shu-Tao Xia. 2024. RAT: Retrieval-Augmented Transformer for ClickThrough Rate Prediction. In Companion Proceedings of the ACM Web Conference 2024 (WWW '24 Companion), May 13-17, 2024, Singapore, Singapore. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3589335.3651550",
  "1 INTRODUCTION": "Click-through rate (CTR) prediction is a binary classification task that aims to forecast whether a user will click on a given item. It has been broadly applicable in commercial fields such as advertising placement and recommender systems [4, 9, 11, 17]. Feature interaction modeling plays an essential role in CTR prediction. As shown in Figure 1, traditional methods [2, 10, 13, 15] primarily focus on feature interactions within each sample, but seldom consider cross-sample information that can serve as a reference context to enhance the prediction. Since features and their interactions are usually sparse, it necessitates CTR models to capture and memorize all interaction patterns, posing challenges in robustness and scalability. Recently, retrieval-augmented (RA) learning has shown effective in natural language processing [5] and computer vision [1], whose typical idea is to retrieve similar samples and enhance model prediction with these external demonstrations. Inspired by its success in relieving long-tail problems [12], we believe it is a promising paradigm to relieve the aforementioned issue in CTR prediction. In this direction, RIM [14], DERT [19] and PET [3] are three preliminary works on RA CTR prediction. However, they either compromise intra- or cross-sample feature interaction, which are still sub-optimal practices. Specifically, RIM simply aggregates retrieved samples on each feature field, which sacrifices fine-grained WWW'24 Companion, May 13-17, 2024, Singapore, Singapore Li and Wang, et al. Figure 1: Traditional CTR prediction methods only focus on cross-column interaction. In this paper, we focus on a new paradigm of retrieval-augmented (RA) CTR, further incorporating cross-row interaction for more effective prediction. label embedding is unknown and padded USER ID ITEM ID CITY CATEGORY BRAND CLICK U1 I1 Rome shoes B1 0 U2 I2 LA gloves B2 1 U2 I3 NYC shoes B3 0 U3 I1 LA dress B3 ? Target Row Cross Row (Sample) Interaction Cross Column Interaction (Intra Sample Interaction) cross-sample knowledge. Although DERT improves RIM on sample retrieval and supplies better reference samples to the target sample, it still follows the same retrieval-augmented modeling as RIM, thereby sharing common limitations. PET constructs a hypergraph to represent the relations between rows and columns of tabular data. Via the star expansion, it converts the hypergraph into a heterogeneous graph that includes label nodes and feature nodes. PET can effectively capture the cross-sample information by designing a message-passing mechanism, but all feature nodes need to be transitioned through label nodes on the graph, showing inflexibility in modeling intra-sample feature interactions. To remedy the shortcomings in previous works, we propose a unified framework termed R etrievalA ugmented T ransformer (RAT) to enhance fine-grained intra- and cross-sample feature interactions for CTR prediction. Given a target sample, we retrieve similar samples from a reference pool ( e.g. , historical logs) using the sparse retrieval algorithm. Then we develop a Transformer-based model to acquire fine-grained feature interactions within and across samples. In particular, we find that intra-cross cascade attention not only improves the efficiency beyond joint modeling but also enhances the robustness of RAT. Without bells and whistles, we condense the semantic information to one token representation, which is fed to the binary classifier to make the final prediction. We conduct extensive experiments on three real-world datasets: ML-Tag, KKBox, and Tmall, demonstrating the promise of retrievalaugmented approaches and the further improvement of RAT. We also show that RAT can enhance long-tail sample prediction, which suggests its capacity to tackle feature sparsity and cold start issues. To summarize, we make the following contributions: ♣ We propose a Retrieval-augmented Transformer (RAT) for CTR prediction, which enhances fine-grained intra- and cross-sample feature interaction in a unified model. ♣ We find that intra-cross cascade attention not only improves the efficiency but also enhances the robustness of RAT (§3.3.1). ♣ Extensive experiments on real-world datasets validate RAT's efficacy and suggest its advantage against the feature sparsity and cold start issues.",
  "2 THE PROPOSED METHOD": "We design RE trieval-augmented T ransformer (RAT), considering both intra- and cross-sample interactions for CTR prediction. Figure 2 briefly illustrates the framework of RAT.",
  "2.1 Retrieve Similar Samples as Context": "Given the 𝐹 -field record 𝑥 𝑖 = [ 𝑥 1 𝑖 ; ... ; 𝑥 𝐹 𝑖 ] of a target sample, we search for similar samples as the reference context from a reserved sample pool P . We use BM25 [16] for retrieval because of its training-free nature, which also aligns with previous works [3, 14]. Specifically, the relevant score of query 𝑥 𝑖 and the key 𝑥 𝑐 of a candidate sample ( 𝑥 𝑐 , 𝑦 𝑐 ) ∈ P is defined as  where I {·} is an indicator function. 𝑁 P is the number of samples in P , while 𝑁 P ( 𝑥 𝑓 𝑡 ) denotes the number of samples containing the feature 𝑥 𝑓 𝑡 in P . Different from Du et al. [3], Qin et al. [14] that implemented the retrieval with Elasticsearch on CPUs, we provide an efficient GPU-based implementation to enable faster speed. Finally, we retrieve 𝐾 samples from P with the highest scores:  Notes on Avoiding Information Leakage. We sort the samples in chronological order if there is timestamp information and restrict a query only to retrieve samples that occur earlier than it. For validation and testing, we take the whole training set as the reference pool. This strategy still satisfies the restriction and is safe, because the testing set and the validation set are the latest and the next latest parts of the whole dataset in our experiments.",
  "2.2 Construct Retrieval-augmented Input": "We build an embedding layer to transform discrete features into 𝐷 -dimensional embedding vectors. In particular, we treat the labels of retrieved samples as special features and also build an embedding table for that field. Let us denote the set of embedding tables as E = { 𝑬 1 , 𝑬 2 , · · · , 𝑬 𝑭 , 𝑳 } , where 𝑬 𝒇 is the embedding table of the 𝑓 -th feature field. 𝑳 ∈ R 3 × 𝐷 is the label embedding table, where the 0-th and the 1-st slots are for unclick and click labels of retrieved samples, respectively, and the 2-nd slot is reserved for the target sample to represent the unknown click label ( i.e. , [UNK] ). For a retrieved sample ( 𝑥 𝑐 𝑘 , 𝑦 𝑐 𝑘 ) , we lookup feature embeddings and the label embedding from the embedding tables and obtain 𝐸 𝑐 𝑘 = [ 𝑙 𝑐 𝑘 ; 𝑒 1 𝑐 𝑘 ; 𝑒 2 𝑐 𝑘 ; · · · ; 𝑒 𝐹 𝑐 𝑘 ] ∈ R ( 𝐹 + 1 ) × 𝐷 . Analogously, for the target record 𝑥 𝑖 , we obtain 𝐸 𝑖 = [ 𝑙 [UNK] 𝑖 ; 𝑒 1 𝑖 ; 𝑒 2 𝑖 ; · · · ; 𝑒 𝐹 𝑖 ] ∈ R ( 𝐹 + 1 ) × 𝐷 . Finally, the retrieval-augmented input for the target record 𝑥 𝑖 is obtained by stacking 𝐸 𝑖 and 𝐸 𝑐 1 , 𝐸 𝑐 2 , · · · , 𝐸 𝑐 𝐾 together:",
  "2.3 Intra- and Cross-sample Feature Interaction": "To integrate intra- and cross-sample feature interactions to enhance CTR prediction, a naïve idea is to unstack all retrieved samples, append them to the target record as extra feature fields, and use joint attention to model full feature interactions. However, it exhibits an efficiency issue: the complexity of each joint self-attention is O(( 𝐾 + 1 ) 2 · ( 𝐹 + 1 ) 2 ) . We also find it inferior in performance (Table 4), possibly due to the influence of noisy feature interactions. To address the above issues, we decompose the joint attention. As shown in Figure 2, each RAT block comprises the cascade of an RAT: Retrieval-Augmented Transformer for Click-Through Rate Prediction WWW'24 Companion, May 13-17, 2024, Singapore, Singapore Figure 2: The overview framework of RAT. Prediticon Head GLYPH(cmap:d835) GLYPH(cmap:d835) GLYPH(cmap:d835) GLYPH(cmap:d835) [GLYPH(cmap:d835)GLYPH(cmap:d835)GLYPH(cmap:d835)] … GLYPH(cmap:d835) GLYPH(cmap:d835) Embedding Layer target sample query Search Engine Reference 11 Pool retrieve reference samples ℛGLYPH(cmap:d835) … GLYPH(cmap:d835)1 GLYPH(cmap:d835) GLYPH(cmap:d835) GLYPH(cmap:d835)2 GLYPH(cmap:d835) GLYPH(cmap:d835) GLYPH(cmap:d835) GLYPH(cmap:d835)1 GLYPH(cmap:d835) GLYPH(cmap:d835)1 GLYPH(cmap:d835) GLYPH(cmap:d835)2 GLYPH(cmap:d835) GLYPH(cmap:d835)2 GLYPH(cmap:d835) GLYPH(cmap:d835) GLYPH(cmap:d835) GLYPH(cmap:d835) GLYPH(cmap:d835) GLYPH(cmap:d835) … … … RAT Block × L MLP GLYPH(cmap:d835) GLYPH(cmap:d835) GLYPH(cmap:d835) GLYPH(cmap:d835) GLYPH(cmap:d835) GLYPH(cmap:d835) Cross Block Layer Norm Layer Norm Cross-sample Attention GLYPH(cmap:d835) GLYPH(cmap:d835) GLYPH(cmap:d835) GLYPH(cmap:d835) GLYPH(cmap:d835) GLYPH(cmap:d835) Intra Block Layer Norm Intra-sample Attention Table 1: Statistics of datasets intra-block, a cross-block, and a multi-layer perception (MLP). The forward process of the ℓ -th RAT block is formulated as    where 𝑋 ℓ 𝑖 denote the input of the ℓ -th RAT block. 𝑋 0 𝑖 = ˜ 𝐸 𝑖 . 𝐻 ℓ 𝑖 and 𝐻 ′ ℓ 𝑖 denote the hidden states. LN (·) is the layer norm operation. ISA (·) and CSA (·) represent the intra-sample and cross-sample attention modules, respectively, which perform multi-head selfattentions along the field-axis and sample-axis, respectively. Compared with the vanilla joint attention, our design of cascaded attention favorably reduces the complexity to O(( 𝐾 + 1 ) 2 +( 𝐹 + 1 ) 2 ) , which helps to keep efficient. We will also investigate more block designs in experiments (§3.3.1) and demonstrate the advantages of cascaded attention for RAT.",
  "3 EXPERIMENTS": "",
  "3.1 Experimental Setup": "3.1.1 Datasets. . We evaluate RAT on three popular datasets. MLtag [7], KKBox [20] and Tmall [3]. We follow the standard CTR prediction benchmark, BARS [20], to split ML-tag to train, validation and test sets by 7:2:1, and split KKBox by 8:1:1. For Tmall, we follow [14] by dividing the data based on global timestamps to avoid information leakage. Dataset statistics is shown in Table 1. Table 2: CTR prediction performance comparison. Δ 𝐴𝑈𝐶 indicates averaged AUC improvement compared to xDeepFM. Table 3: Performance w.r.t. long-tail users on ML-tag subset. 3.1.2 Metrics. We use two commonly used metrics, AUC (Area Under ROC) and Logloss (cross entropy) as the evaluation metrics. 3.1.3 Baselines. We select two types of baselines. ( i ) Traditional (and not retrieval-augmented) models: DeepFM [4], xDeepFM [11], DCNv2 [18], AOANet [8]; ( ii ) Retrieval-augmented models: RIM [14], PET [3]. For fair comparison, we use BM25 for aligning settings. DERT [19] is not included in our benchmark because it is a special case of RIM with a new dense retrieval setting. Besides, there is no open-sourced implementation for it, making fair comparison hard. 3.1.4 Implementation Details. For fair comparisons, we implement all methods with FuxiCTR [21] and follow the BARS [20] benchmark settings. For retrieval-augmented models, we set 𝐾 to 5 by default.",
  "3.2 Comparison with State-of-the-arts": "3.2.1 Overall Performance. We report the model performances in Table 2. Note that for CTR prediction, a ‰-level AUC increase is considered acceptable, as even such a minor improvement, if statistically significant, can lead to substantial gains in revenue [6, 11]. According to the results in Table 2, retrieval-augmented (RA) models generally outperform traditional models, indicating that incorporating cross-sample information is effective in enhancing CTR prediction. Although PET does not surpass xDeepFM in overall performance, mainly due to its insufficient capability to capture intra-sample feature interactions, it exhibits superiority over other traditional models. Moreover, RAT outperforms the SoTA RA model, RIM, demonstrating the strong capability of Transformer in modeling fine-grained intra- and cross-sample interactions. 3.2.2 Performance on Long-tail Data. Inspired by the success of RA mechanisms on long-tail recognition [12], here we pose a question: Can RAT enhance CTR prediction on long-tail data? Taking ML-tag for a case study, we investigate the model efficacy on sample subsets associated with tailed 10% and 20% users, respectively, ranked by user's sample amount. As shown in Table 3, RA models WWW'24 Companion, May 13-17, 2024, Singapore, Singapore Li and Wang, et al. Table 4: Performance comparison of different RAT designs. Table 5: Efficiency comparison of different RAT designs. perform better than traditional ones, indicating RA context indeed assists reasoning over sparse features and their interaction. Furthermore, RAT outperforms RIM and PET considerably, validating that Transformer-based fine-grained modeling improves the utilization of retrieved samples. The evidence suggests RAT's capability in addressing important issues like feature sparsity and cold start.",
  "3.3 Model Analysis": "3.3.1 Designs of RAT Block. In addition to the proposed design, we further explore more variants: ( i ) Intra-Cross J oint M odeling ( JM ) : The vanilla joint attention modeling for all intra- and crosssample feature interactions. ( ii ) Intra-Cross C ascaded E ncoder ( CE ) : Rather than the cascaded attentions in a single block, it separates the intra- and cross-sample attention into two cascaded Transformer blocks, yielding a double block number compared to other variants. ( iii ) Intra-Cross P arallel A ttention ( PA ) : It designs intra- and cross-sample attentions as two parallel branches. The hidden dimension of each branch is halved, and the outputs of the two branches are concatenated. We report the experimental results on model performance and efficiency in Tables 4 and 5, respectively. From Table 5 we can see that decomposed modeling designs are indeed more effective than joint modeling ( i.e. , JM ). From Table 4 we can learn that joint modeling is not the best practice, possibly due to the influence of noisy feature interactions. In contrast, attention decomposition provides an inductive bias for model robustness and leads to better performance in general. Comparing RAT and RAT CE , block-level decomposition does not bring extra gain upon attention-level decomposition. Comparing RAT and RAT PA , cascaded attention performs slightly better, suggesting that the successive order benefits the feature interaction modeling.",
  "4 CONCLUSION": "In this paper, we present R etrievalA ugmented T ransformer (RAT) for CTR prediction. It highlights the importance of cross-sample information and explores Transformer-based architectures to capture fine-grained feature interactions within and between samples, effectively remedying the shortcomings of existing works. By decomposing the intra- and cross-sample interaction modeling, RAT enjoys better efficiency while further enhancing the robustness. We conduct comprehensive experiments to validate the effectiveness of RAT, and show its advantage in tackling long-tail data.",
  "ACKNOWLEDGMENTS": "This work is supported in part by the National Natural Science Foundation of China under Grant (62302309, 62171248), Shenzhen Science and Technology Program (JCYJ20220818101012025), and the PCNL KEY project (PCL2023AS6-1). We gratefully acknowledge the support of MindSpore 1 , which is a new deep learning framework for this research.",
  "REFERENCES": "[1] Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W Cohen. 2023. ReImagen: Retrieval-Augmented Text-to-Image Generator. In ICLR . [2] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & Deep Learning for Recommender Systems. In RecSys . [3] Kounianhua Du, Weinan Zhang, Ruiwen Zhou, Yangkun Wang, Xilong Zhao, Jiarui Jin, Quan Gan, Zheng Zhang, and David Wipf. 2022. Learning Enhanced Representations for Tabular Data via Neighborhood Propagation. In NeurIPS . [4] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: A Factorization-Machine Based Neural Network for CTR Prediction. In IJCAI . [5] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-wei Chang. 2020. REALM: Retrieval-Augmented Language Model Pre-Training. In ICML . [6] Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. 2019. FiBiNET: Combining Feature Importance and Bilinear Feature Interaction for Click-Through Rate Prediction. In RecSys . [7] Denis Kotkov, Alexandr Maslov, and Mats Neovius. 2021. Revisiting the Tag Relevance Prediction Problem. In SIGIR . [8] Lang Lang, Zhenlong Zhu, Xuanye Liu, Jianxin Zhao, Jixing Xu, and Minghui Shan. 2021. Architecture and Operation Adaptive Network for Online Recommendations. In KDD . [9] Wenjie Li, Zhongren Wang, Jingpeng Wang, Shu-Tao Xia, Jile Zhu, Mingjian Chen, Jiangke Fan, Jia Cheng, and Jun Lei. 2024. ReFer: Retrieval-Enhanced Vertical Federated Recommendation for Full Set User Benefit. In SIGIR . [10] Zeyu Li, Wei Cheng, Yang Chen, Haifeng Chen, and Wei Wang. 2020. Interpretable Click-Through Rate Prediction Through Hierarchical Attention. In WSDM . [11] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong Sun. 2018. xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems. In KDD . [12] Alexander Long, Wei Yin, Thalaiyasingam Ajanthan, Vu Nguyen, Pulak Purkait, Ravi Garg, Alan Blair, Chunhua Shen, and Anton van den Hengel. 2022. Retrieval Augmented Classification for Long-Tail Visual Recognition. In CVPR . [13] Kelong Mao, Jieming Zhu, Liangcai Su, Guohao Cai, Yuru Li, and Zhenhua Dong. 2023. FinalMLP: An Enhanced Two-Stream MLP Model for CTR Prediction. In AAAI . [14] Jiarui Qin, Weinan Zhang, Rong Su, Zhirong Liu, Weiwen Liu, Ruiming Tang, Xiuqiang He, and Yong Yu. 2021. Retrieval & Interaction Machine for Tabular Data Prediction. In KDD . [15] Steffen Rendle, Zeno Gantner, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2011. Fast Context-Aware Recommendations with Factorization Machines. In SIGIR . [16] Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford, et al. 1995. Okapi at TREC-3. NIST SP (1995). [17] Jinpeng Wang, Ziyun Zeng, Yunxiao Wang, Yuting Wang, Xingyu Lu, Tianxiang Li, Jun Yuan, Rui Zhang, Hai-Tao Zheng, and Shu-Tao Xia. 2023. MISSRec: PreTraining and Transferring Multi-Modal Interest-Aware Sequence Representation for Recommendation. In MM . [18] Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong, and Ed Chi. 2021. DCN v2: Improved Deep & Cross Network and Practical Lessons for Web-Scale Learning to Rank Systems. In WWW . [19] Lei Zheng, Ning Li, Xianyu Chen, Quan Gan, and Weinan Zhang. 2023. Dense Representation Learning and Retrieval for Tabular Data Prediction. In KDD . [20] Jieming Zhu, Quanyu Dai, Liangcai Su, Rong Ma, Jinyang Liu, Guohao Cai, Xi Xiao, and Rui Zhang. 2022. BARS: Towards Open Benchmarking for Recommender Systems. In SIGIR . [21] Jieming Zhu, Jinyang Liu, Shuai Yang, Qi Zhang, and Xiuqiang He. 2021. Open Benchmarking for Click-Through Rate Prediction. In CIKM . 1 https://www.mindspore.cn",
  "keywords_parsed": [
    "Predicting click-through rates (CTR) is a fundamental task for Web applications",
    " where a key issue is to devise effective models for feature interactions. Current methodologies predominantly concentrate on modeling feature interactions within an individual sample",
    " while overlooking the potential cross-sample relationships that can serve as a reference context to enhance the prediction. To make up for such deficiency",
    " this paper develops a R etrievalA ugmented T ransformer (RAT)",
    " aiming to acquire fine-grained feature interactions within and across samples. By retrieving similar samples",
    " we construct augmented input for each target sample. We then build Transformer layers with cascaded attention to capture both intra- and cross-sample feature interactions",
    " facilitating comprehensive reasoning for improved CTR prediction while retaining efficiency. Extensive experiments on real-world datasets substantiate the effectiveness of RAT and suggest its advantage in long-tail scenarios. The code has been open-sourced at https://github.com/YushenLi807/WWW24-RAT."
  ]
}