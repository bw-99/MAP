{
  "Triple Sequence Learning for Cross-domain Recommendation": "HAOKAI MA , School of Software, Shandong University, China † , • RUOBING XIE † , WeChat, Tencent, China LEI MENG ∗ , School of Software, Shandong University; Shandong Research Institute of Industrial Technology, China XIN CHEN, WeChat, Tencent, China XU ZHANG, WeChat, Tencent, China LEYU LIN, WeChat, Tencent, China JIE ZHOU, WeChat, Tencent, China Cross-domain recommendation (CDR) aims to leverage the correlation of users' behaviors in both the source and target domains to improve the user preference modeling in the target domain. Conventional CDR methods typically explore the dual-relations between the source and target domains' behaviors. However, this may ignore the informative mixed behaviors that naturally reflect the user's global preference. To address this issue, we present a novel framework, termed triple sequence learning for cross-domain recommendation (Tri-CDR), which jointly models the source, target, and mixed behavior sequences to highlight the global and target preference and precisely model the triple correlation in CDR. Specifically, Tri-CDR independently models the hidden representations for the triple behavior sequences and proposes a triple cross-domain attention (TCA) method to emphasize the informative knowledge related to both user's global and target-domain preference. To comprehensively explore the cross-domain correlations, we design a triple contrastive learning (TCL) strategy that simultaneously considers the coarse-grained similarities and fine-grained distinctions among the triple sequences, ensuring the alignment while preserving information diversity in multi-domain. We conduct extensive experiments and analyses on six cross-domain settings. The significant improvements of Tri-CDR with different sequential encoders verify its effectiveness and universality. The code will be released upon acceptance. CCS Concepts: · Information systems → Recommender systems . Additional Key Words and Phrases: cross-domain recommendation, contrastive learning, triple learning",
  "ACMReference Format:": "Haokai Ma † , · , Ruobing Xie † , Lei Meng ∗ , Xin Chen, Xu Zhang, Leyu Lin, and Jie Zhou. 2018. Triple Sequence Learning for Cross-domain Recommendation. In . ACM, New York, NY, USA, 24 pages. https://doi.org/XXXXXXX.XXXXXXX",
  "1 INTRODUCTION": "Personalized recommendation aims to capture user interests and provide appropriate items [20, 40]. Sequential recommendation (SR), which focuses on discovering user preferences from the essential information of users' historical behaviors, has attracted significant attention [14, 59]. However, real-world SR models usually face the data sparsity † indicates equal contributions. ∗ indicates corresponding author. · This work has been done when the author was at Tencent for internship. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. © 2018 Association for Computing Machinery. Manuscript submitted to ACM 1 Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Haokai Ma et al. Fig. 1. Triple sequence learning on the source, target, and mixed behavior sequences in CDR. Transformers AI AlphaGo AI Robot Source sequence Target sequence Mixed sequence Triple Cross-domain Attention (TCA) Capture positive transfer Triple Contrastive Learning (TCL) Model correlation among  three domains Car Target item Triple Sequence Learning problem, since users usually have few behaviors [43, 47]. In practice, making full use of user behaviors in other domains under the user's approval is a straightforward and effective solution to the data sparsity issue in a single domain. Cross-domain recommendation (CDR) concentrates on transferring useful information from the source domain to the target domain for performance gains in the target domain [12, 18, 52]. Existing CDR methods mainly focus on modeling the relations between the source and target domains. EMCDR [25] and SSCDR [13] attempt to learn a mapping function across the source/target domains via aligned objects. CoNet [12] and MiNet [27] adopt explicit cross-domain information paths or attention mechanisms for knowledge transfer. Some CDR methods further build the cross-domain connections via (multi-domain) global graphs [45, 54] or feature correlations [17, 46]. However, most existing CDR models simply focus on the dual relations between the source and target behavior sequences, ignoring the rich information of the natural mixed (i.e., source+target) behavior sequence. We define the mixed behavior sequence in CDR as a complete user behavior sequence containing behaviors in both the source and target domains which are ordered chronologically. The left part of Figure 1 shows an example of the source, target, and mixed behavior sequences in a cross-domain sequential recommendation (CDSR) scenario. The mixed sequence can reflect a user's complete behavioral pattern and preference evolution more comprehensively and thus helps to better extract users' global interests. For example, the sequential behaviors in domain book and movie are not consistent and reasonable. Only through the complete mixed sequence containing sequential behaviors of [ book: AlphaGo ] → [ movie: AI ] → [ movie: AI Robot ] → [ book: Transformers ] → [ movie: Car ] can we fully understand the user's sequential action logic. We firmly believe that jointly modeling the mixed behavior sequence with the original two source/target sequences is beneficial to capture both inter-domain and intra-domain information in CDR. In this work, we propose a new paradigm that jointly models source, target, and mixed behavior sequences in CDR. The challenges are three-fold: (1) How to extract more informative knowledge from source and mixed sequences? User behaviors in other domains may be good supplements, while it is also common that users have different preferences in these domains. We should maximize the cross-domain information gain while alleviating the negative transfer from possible noises. (2) How to model the triple correlations among source, target, and mixed sequences? The mixed sequence is built by source and target behaviors. Both the coarse-grained similarities and fine-grained distinctions among the three sequences should be carefully considered. The dual relation learning of conventional CDR models cannot be directly transferred to the triple learning task with the additional mixed sequence. (3) How to construct a universal CDR framework that could smoothly cooperate with different types of single-domain SR models? Currently, lots of CDR models rely on complicated and customized networks for inter-domain interactions, which 2 Triple Sequence Learning for Cross-domain Recommendation Conference acronym 'XX, June 03-05, 2018, Woodstock, NY are hard to be directly adopted with other single-domain models. We aim to build a universal model-agnostic CDR framework that could be beneficial with frequently updated single-domain models. To address these issues, we propose a novel Triple sequence learning for cross-domain recommendation (Tri-CDR) , serving as a model-agnostic framework to jointly model the source, target, and mixed sequences in CDSR. Specifically, we first build three sequence encoders for the source, mixed, and target domains, respectively, which model the intra-domain behavior interactions to get three hidden sequence representations. Next, to alleviate the irrelevant negative transfer, we design a triple cross-domain attention (TCA) method on three sequence representations to capture the informative knowledge related to users' target-domain preferences and global interests. These attentionenhanced sequence representations are then combined and fed into a Multi-Layer Perceptron (MLP) to get the final user representation. We further propose a triple contrastive learning (TCL) strategy to comprehensively model the correlations among three sequences. TCL adopts three CL losses to capture the coarse-grained similarities between any two sequence representations of the same user compared to other users'. More importantly, it further employs a margin-based triple loss among three sequence representations to model their fine-grained distinctions, keeping the information diversity in three domains. The advantages of Tri-CDR are: (1) the TCA enables an informative knowledge transfer related to users' target-domain preferences and global interests. (2) The TCL helps to better capture the correlations among three domains in representation learning. (3) Tri-CDR is effective, universal, and easy-to-deploy, which could be conveniently applied with different sequence encoders and additional objectives. In experiments, we have conducted an extensive evaluation on six cross-domain settings with various base sequence encoders. As a result, Tri-CDR achieves significant improvements on all settings. We also conduct various ablation studies, universality analyses, parameter analyses, and visualization to verify the effectiveness of the proposed TCA and TCL. The contributions are summarized as follows: · We have verified the significance of the triple sequence modeling for comprehensive user interest understanding. To the best of our knowledge, we are the first to present the triple sequence learning among source, target, and mixed behavior sequences in CDR. · We propose a triple cross-domain attention (TCA) method to enable more positive transfer of knowledge from the source domain to the target one, which considers both the user's target-domain preferences and global interest from the source and mixed behavior sequences. · We creatively design the triple contrastive learning (TCL) strategy, which not only models the coarse-grained similarities among multi-domain sequence representations of the same user but also detects the fine-grained distinctions via a margin-based triple loss. · We conduct an extensive evaluation to verify the effectiveness of our Tri-CDR on multiple datasets with different base models. The proposed model is effective, universal, and easy-to-deploy.",
  "2 RELATED WORK": "Cross-domain Recommendation. Cross-domain recommendation (CDR) is a representative method to alleviate the data sparsity problem in a single domain with auxiliary information from other domains [18, 25]. The basic assumption is that users' behaviors in different domains reflect the user's personal preferences to a certain extent. Classical CDR methods aim to model cross-domain knowledge transfer through directly mapping [13, 25], multi-domain interaction modeling [1, 27], meta-learning [63, 64], and transformation matrix [12, 18]. Recently, some CDR methods also leverage alignment constraint [22, 38] and adversarial learning [15] for cross-domain knowledge representation and fusion. CDSR concentrates more on users' multi-domain sequential behavior modeling in CDR [17, 21, 24, 58]. BiTGCF [21] designs a 3 Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Haokai Ma et al. bi-directional transfer learning method to transfer users' single-domain preferences across two domains. Lots of CDSR methods focus on the shared-account CDSR scenario [24, 39]. 𝜋 -Net [24] and PSJNet [34] utilize a shared account filter unit and a cross-domain transfer unit to share information across both domains synchronously. MIFN [23] further enriches the sequence representation with knowledge graphs based on 𝜋 -Net. DDGHM [58] builds a global dynamic graph to model source-target interactions directly, and jointly predicts via local and global information. CDHRM [39] jointly captures users' inter-session and intra-session behavioral dynamics from different domains. DAGCN [7] designs a domain-aware graph convolution network to learn user-specific node representations on the global static graph. DASL [17] proposes the dual embedding and dual attention strategies to model the correlations between source and target domains' sequences. DR-MTCDR [8] designs a unified disentanglement module to capture the domain-shared and domain-specific information, with the aim to transfer the trustworthy information across domains. COAST [55] introduces a unified cross-domain heterogeneous graph to capture user-item similarity and achieve user interest alignment by exploring the user-user and user-item interest invariance. However, existing CDSR methods merely focus on dual relations of source → target or global → local, ignoring directly modeling the natural mixed behavior sequence with the correlations among source, target, and mixed domains. Our Tri-CDR is different from these works: (a) we directly model three mixed, source, and target behavior sequences and use them for recommendations simultaneously. (b) We emphasize the ternary relationship among three sequences for positive knowledge transfer. (c) Tri-CDR is model-agnostic and easy-to-deploy that can be applied to different base sequence encoders and even intra-domain CL tasks. By doing this, our work can not only learn both complete and independent preferences from mixed, source, and target behavior sequences but also explain users' sequential behavior comprehensively through their interactions. Sequential Recommendation. Sequential Recommendation (SR) attempts to capture the user's time-aware preferences by modeling the sequential dependencies of the user's historical behavior to recommend the next item that the user may be interested in. Early works reason users' short-term preference through the Markov Chains (MCs) [10, 32]. In recent years, researchers have leveraged the Convolution Neural Network (CNN) [35, 49], Recurrent Neural Network (RNN) [11, 16] and Transformer [14, 33] to capture users' preference patterns from users' historical behaviors. Among them, GRU4Rec [11] leverages the Gate Recurrent Unit (GRU) as the sequential encoder to learn users' long-term dependencies. SASRec [14] introduces Transformer for behavior interaction modeling and is widely used in practice. S 3 Rec [60] leverage the mutual information maximization principle to employ four self-supervised objectives among item, attribute, sub-sequence, and sequence. DUVRec [50] encodes the sequential information with dual-view user representation (item-view and factor-view) to achieve enhanced SR performance. CL4SRec [47] is one of the state-of-the-art SR models that further enhance the sequential modeling with various intra-domain CL tasks. In this work, we have successfully adopted Tri-CDR with different sequence encoders, including GRU4Rec, SASRec, and CL4SRec. Contrastive Learning in Recommendation. As a common self-supervised learning (SSL) method, contrastive learning (CL) has been widely used in fields of Computer Vision (CV) [4, 9], Natural Language Processing (NLP) [6, 44] and Recommendation System (RS) [41, 45, 47]. In recommendation, CL is widely applied to session-based recommendation [61], multi-behavior recommendation [43], sequential recommendation [30, 47, 60], and cross-domain recommendation [2, 45]. C 2 -CRS [61] proposes a coarse-to-fine contrastive learning method to model user preference with multi-level semantic fusion. MMCLR [43] designs three CL tasks to learn the correlations among different behavior types and modeling views. DUORec [30] proposes a contrastive regularization with the model-level augmentation to reshape and improve the embedding distribution and sequence representations. CL4SRec [47] proposes three sequence-based augmentations to build positive pairs in SSL. CCDR [45] designs an intra-domain CL task and three inter-domain CL tasks for cross-domain knowledge transfer in graph-based matching. C 2 DSR [2] conducts a cross-domain infomax 4 Triple Sequence Learning for Cross-domain Recommendation Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Fig. 2. The overall structure of Tri-CDR. TCA highlights informative knowledge related to users' target-domain preferences and global interests, while TCL captures the triple correlations among three domains for better cross-domain knowledge transfer. Source sequence 𝑆 ! : 𝒅 \" ! , 𝒅 # ! , … , 𝒅 $ ! Target sequence 𝑆 % : 𝒅 \" % , 𝒅 # % , … , 𝒅 & % Mixed sequence 𝑆 ' : 𝒅 \" ' , 𝒅 # ' , … , 𝒅 $(& ' Sequence Encoder Sequence Encoder MLP Final user representation 𝒖 % Forward Knowledge Propagation Triple Margin Pushing Constraint 𝐿 )*' Triple Margin Pulling Constraint 𝐿 )*' Contrastive Constraint 𝐿 +, Fine-grained Distinction Modeling Source hidden matrix 𝑯 ! Mixed hidden matrix 𝑯 ' Target hidden matrix 𝑯 % 𝐿 +, 𝐿 +, 𝐿 +, 𝐿 )*' Pull closer 𝐿 )*' Push away Mixed sequence representation 𝒔 ' Source sequence representation 𝒔 ! Target sequence representation 𝒔 % Source projected representation * 𝒔 ! Target projected representation * 𝒔 % Mixed projected representation * 𝒔 ' Source projected representation * 𝒔 ! Triple Contrastive Learning Coarse-grained Similarity Modeling Triple Sequence Encoding Triple Cross-domain Attention Multi-sequence Aggregation Triple Contrastive Learning Mixed projected representation * 𝒔 ' Target projected representation * 𝒔 % Representation of user 1 Representation of user 3 Representation of user 2 ... ... ... Activation Unit Activation Unit Activation Unit Global interests Target domain preference Sequence Encoder ... ... ... objective to enhance the correlation between global and local representations with domain-specific global augmentations. Some recent works [29, 42, 43, 53] have conducted certain triplet losses for more precise representation learning in recommendation. However, existing CL-based CDR models simply maximize the mutual information of representations in different domains, ignoring their conflicts that may lead to negative transfer and model collapse. To the best of our knowledge, we are the first to jointly model coarse-grained similarity and fine-grained distinction via CL in CDR.",
  "3 METHOD": "",
  "3.1 Problem Formulation": "We first define the source behavior sequence 𝑆 𝑆 = { 𝒅 𝑆 1 , 𝒅 𝑆 2 , · · · , 𝒅 𝑆 𝑝 } in the source domain 𝑆 and the target behavior sequence 𝑆 𝑇 = { 𝒅 𝑇 1 , 𝒅 𝑇 2 , · · · , 𝒅 𝑇 𝑞 } in the target domain 𝑇 for each user, where 𝑝 , 𝑞 are the source/target historical behavior lengths, and 𝒅 𝑆 𝑖 and 𝒅 𝑇 𝑗 are behavior embeddings. In Tri-CDR, we propose a third mixed behavior sequence 𝑆 𝑀 = { 𝒅 𝑀 1 , 𝒅 𝑀 2 , · · · , 𝒅 𝑀 𝑝 + 𝑞 } as a supplement to source/target sequences, which is the complete user behavior sequence containing both source and target behaviors in chronological order. Given three behavior sequences 𝑆 𝑆 , 𝑆 𝑇 and 𝑆 𝑀 , Tri-CDR tries to recommend the target item 𝑑 𝑇 𝑞 + 1 that will be interacted by this user in the target domain.",
  "3.2 Overall Framework": "In this section, we describe the proposed model-agnostic Triple sequence learning for cross-domain recommendation (Tri-CDR) framework, which jointly models source, target, and mixed behavior sequences to improve CDR. Specifically, we first model the source, target, and mixed behavior sequences through three base sequence encoders separately to generate their corresponding hidden sequence representations in three domains. Then we propose a triple cross-domain attention (TCA) method to highlight informative knowledge related to the user's target-domain preferences and global interests in building three domains' sequence representations, mitigating negative knowledge transfer. To better model the correlations among multi-domain sequence representations, we design a novel triple contrastive learning (TCL) strategy with two contrastive constraints: (a) Coarse-grained similarity modeling, which enables source/target/mixed sequence representations from the same user to be more similar than other users'. (b) Fine-grained distinction modeling, which recognizes users' diversified preferences in different domains to keep the information gains brought by the source 5 Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Haokai Ma et al. and mixed sequences. TCL helps to learn more informative and accurate multi-domain representations to capture user preferences comprehensively. The overall structure of Tri-CDR is illustrated in Fig. 2. In the following subsections, we first present the base sequence encoder of the proposed Tri-CDR which is implemented with the self-attention module. Subsequently, we introduce TCA, which emphasizes the user's target-domain preferences and global interest in triple domains. And then, we describe TCL to precisely model the user's coarse-grained similarities and fine-grained distinctions among triple sequences. Finally, we present the discussions on the proposed Tri-CDR.",
  "3.3 Base Sequence Encoder": "Inspired by the success of the self-attention mechanism in sequential recommendation, we apply SASRec [14] as our sequence encoder for all domains. Without losing generality, for the target domain sequence 𝑆 𝑇 , we build the input matrix 𝑫 𝑇 ∈ R 𝑞 × 𝑑 , where each behavior embedding 𝒅 𝑇 𝑖 consists of a learnable item ID embedding and a position embedding, and 𝑑 is the embedding size. Then the sequence encoder transposes 𝑫 𝑇 into three matrices by linear projections, and feeds them into the attention method as query, key, and value, which can be defined as:  where 𝑸 = 𝑫 𝑇 𝑾 𝑄 , 𝑲 = 𝑫 𝑇 𝑾 𝐾 , 𝑽 = 𝑫 𝑇 𝑾 𝑉 , and 𝑾 𝑄 , 𝑾 𝐾 , 𝑾 𝑉 denote the linear projections respectively. Meanwhile, we conduct a point-wise feed-forward network to get the hidden behavior matrix 𝑯 𝑇 of the target domain, which is measured as follows:  where 𝒘 1, 𝒘 2, 𝒃 1, 𝒃 2 denote the weight matrices and bias vectors respectively. The hidden matrices of source/mixed sequences 𝑯 𝑆 , 𝑯 𝑀 are similarly constructed with the intra-domain behavior interactions. Note that the same items in different domains are allocated with different behavior embeddings for better representation capacity, avoiding too homogeneous representations across different domains. It is also worth noting that we can conveniently adopt other sequential models as our sequence encoder or even with other intra-domain CL tasks in Tri-CDR (we have tested GRU4Rec [11] and CL4SRec [47], see the universality analysis in Sec. 4.6).",
  "3.4 Triple Cross-domain Attention": "We assume that different historical behaviors in three domains should have different importance for the target-domain prediction. Precisely, We expect that the information emphasized in three sequences should be (a) relevant to the user's target-domain preferences, so as to fit the target-domain prediction task, and (b) relevant to the user's global interests, so as to understand the user's comprehensive preferences and bring in more information gain. Hence, we propose a Triple Cross-domain Attention (TCA) on three hidden behavior matrices to enable more positive transfer. TCAfunctions when we aggregate the hidden behavior embeddings in 𝑯 𝑆 , 𝑯 𝑇 , 𝑯 𝑀 to get three sequence representations 𝒔 𝑆 , 𝒔 𝑇 , 𝒔 𝑀 of the source, target, and mixed domains. Specifically, for the source domain, given 𝑯 𝑆 = { 𝒉 𝑆 1 , · · · , 𝒉 𝑆 𝑝 } and the target item 𝑑 𝑇 𝑞 + 1 , TCA calculates the attention weight 𝛼 𝑆 𝑖 of the 𝑖 -th behavior's hidden embedding 𝒉 𝑆 𝑖 in the source sequence as follows:  6 Triple Sequence Learning for Cross-domain Recommendation Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Here, ⊙ performs element-wise vector product, ∥ denotes the concatenation operation, and MLP 𝑎 (·) is a two-layer fully-connected network where each layer is followed by a PReLU activation function. 𝒉 𝑇 𝑞 and 𝒉 𝑀 𝑝 + 𝑞 denote the last hidden behavior embeddings in target and mixed domains. This attention setting is inspired by [59], while we adopt 𝒉 𝑇 𝑞 to indicate the user's current target-domain preference instead of using different candidate target item's embedding 𝒅 𝑇 𝑞 + 1 , since it is much more efficient in online serving. Moreover, we also highlight 𝒉 𝑀 𝑝 + 𝑞 in this attention, since we assume the hidden embedding of the last mixed behavior implies the user's global interests, which are good supplements to the target-domain preference. Based on this, we aggregate different behavior hidden embeddings 𝒉 𝑆 𝑖 to get the source sequence representation 𝒔 𝑆 as:  Similarly, we also have 𝒔 𝑀 = ˝ 𝑝 + 𝑞 𝑖 = 1 Softmax GLYPH<16> f ( 𝒉 𝑇 𝑞 , 𝒉 𝑀 𝑝 + 𝑞 , 𝒉 𝑀 𝑖 ) GLYPH<17> 𝒉 𝑀 𝑖 and 𝒔 𝑇 = ˝ 𝑞 𝑖 = 1 Softmax GLYPH<16> f ( 𝒉 𝑇 𝑞 , 𝒉 𝑀 𝑝 + 𝑞 , 𝒉 𝑇 𝑖 ) GLYPH<17> 𝒉 𝑇 𝑖 for the mixed and target sequence representations. With TCA, we can not only directly focus on the information related to the user's target-domain preferences, but also keep aware of the user's global interests for a more comprehensive positive transfer from the three domains.",
  "3.5 Triple Contrastive Learning": "Compared to classical CDR models that learn dual relations, Tri-CDR faces a more challenging task to comprehensively understand triple correlations among source, target, and mixed sequences. In this work, we propose a novel Triple Contrastive Learning (TCL) to smartly model the correlations among three sequence representations. Precisely, we design two CL tasks in TCL, including the coarse-grained similarity modeling and the fine-grained distinction modeling. The former CL task is adopted to capture the coarse-grained similarities between any two sequence representations of the same user compared to others. In contrast, the latter CL task is conducted to model the fine-grained distinctions among a user's multi-domain sequence representations, keeping the diversity across different domains to enhance information gains. 3.5.1 Coarse-grained Similarity Modeling (CSM). It is natural that a user's behavior sequences in different domains should share common general preferences. Hence, we design a coarse-grained similarity modeling (CSM) to model the coarse-grained similarities among three domains' sequence representations of the same user. Specifically, we project the sequence representations 𝒔 𝑆 , 𝒔 𝑇 , 𝒔 𝑀 into their spaces via domain-specific projectors P 𝑆 (·) , P 𝑇 (·) , P 𝑀 (·) (we build these projectors via one-layer MLPs). After obtaining the projected sequence representations ¯ 𝒔 𝑆 = P 𝑆 ( 𝒔 𝑆 ) , ¯ 𝒔 𝑇 = P 𝑇 ( 𝒔 𝑇 ) , ¯ 𝒔 𝑀 = P 𝑀 ( 𝒔 𝑀 ) of three domains, we calculate the contrastive loss L 𝐶𝐿 with any two of them as positive instances in CL. Formally, for ¯ 𝒔 𝑆 and ¯ 𝒔 𝑇 , we follow the classical InfoNCE [4] as:  𝐵 denotes the sampled batch, ¯ 𝒔 𝑇 𝑗 denotes the randomly selected negative sample for 𝑖 in 𝐵 , 𝜏 denotes the temperature coefficient, and sim GLYPH<16> ¯ 𝒔 𝑆 𝑖 , ¯ 𝒔 𝑇 𝑖 GLYPH<17> = ( ¯ 𝒔 𝑆 𝑖 ) 𝑇 ( ¯ 𝒔 𝑇 𝑖 ) ∥ ¯ 𝒔 𝑆 𝑖 ∥· ∥ ¯ 𝒔 𝑇 𝑖 ∥ denotes the cosine similarity. Finally, the CSM loss L 𝐶𝑆𝑀 is formulated with three positive pairs as:  7 Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Haokai Ma et al. where 𝜆 1, 𝜆 2, and 𝜆 3 denote the loss weights respectively. 3.5.2 Fine-grained Distinction Modeling (FDM). CSM assumes that a user's domain-specific preferences should be more similar, and monotonously pulls the multi-domain representations of the same user closer. It functions well in general, while there does exist fine-grained distinctions across the user's preferences in triple domains. Over-optimizing L 𝐶𝑆𝑀 may inevitably cause the model to collapse to the proximal point, where the source and mixed sequences cannot provide additional information gain for their too-similar target sequence, putting the cart before the horse. To address this issue, we look back to the composition of the proposed mixed sequence, whose subsequences contain both source and target sequences. Hence, it is intuitive that the source-mixed and target-mixed distances should be smaller than the source-target distance. Under this intuition, we propose a new CL task named fine-grained distinction modeling (FDM) based on a margin-based triplet loss. We intuitively assume the distance between ¯ 𝒔 𝑆 and ¯ 𝒔 𝑇 should be larger than the distance between ¯ 𝒔 𝑆 and ¯ 𝒔 𝑀 by at least the margin 𝛾 . Formally, we define the FDM loss L 𝐹𝐷𝑀 as:  where 𝑑 GLYPH<16> ¯ 𝒔 𝑆 , ¯ 𝒔 𝑇 GLYPH<17> measures the similarity between ¯ 𝒔 𝑆 and ¯ 𝒔 𝑇 , which is calculated as the 𝐿 2 distance. 𝛾 denotes the margin to be controlled. Through jointly optimizing L 𝐶𝑆𝑀 and L 𝐹𝐷𝑀 , Tri-CDR could intelligently find a good balance in ensuring the alignment among a user's multi-domain preferences while keeping the fine-grained distinctions to bring in more information gain from other domains.",
  "3.6 Optimization Objectives": "After TCA and TCL, we concatenate the triple sequence representations 𝒔 𝑀 , 𝒔 𝑆 , 𝒔 𝑇 and feed them into the following multi-sequence aggregation layer to generate the user final representation 𝒖 𝑇 , which is formulated as follows:  MLP 𝑓 (·) denotes a two-layer fully-connected network with the LeakyReLU activation. Finally, we calculate the predicted probability ˆ 𝑦 𝑇 = ( 𝒖 𝑇 ) ⊤ 𝒅 𝑇 𝑞 + 1 of the user 𝑢 on the target item 𝑑 𝑇 𝑞 + 1 with the final user representation 𝒖 𝑇 and the item embedding 𝒅 𝑇 𝑞 + 1 . We formulate the binary cross-entropy loss L 𝐶𝑇𝑅 as follows:  where 𝑅 𝑇 is the target-domain training set, 𝑦 𝑇 𝑢,𝑑 = 1 and 𝑦 𝑇 𝑢,𝑑 = 0 denote the positive and negative samples respectively, and ˆ 𝑦 𝑇 𝑢,𝑑 denotes the predicted probability of ( 𝑢,𝑑 ) . To optimize across triple sequences in conjunction with CL tasks, the overall objective function 𝐿 is a linear combination of L 𝐶𝑇𝑅 , L 𝐶𝑆𝑀 and L 𝐹𝐷𝑀 as:  where 𝜆 𝐶𝑆𝑀 and 𝜆 𝐹𝐷𝑀 denote the loss weights of L 𝐶𝑆𝑀 and L 𝐹𝐷𝑀 .",
  "3.7 Training Strategy of CDSR": "Training Tri-CDR from scratch sometimes may incur the difficulty in model convergence in the early training stage, leading to unstable and unsatisfactory performance. This is mainly caused by the feature space conflicts between the source and target domains in CDR. We also observe the same phenomenon in other CDSR models. To address this 8 Triple Sequence Learning for Cross-domain Recommendation Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Table 1. Comparison among representative cross-domain sequential recommendation algorithms. issue, we conduct a two-step training strategy (i.e., single-domain pre-training + cross-domain fine-tuning) to achieve more stable and satisfactory performances. Specifically, we first pre-train two source/target SASRec models with their corresponding single-domain losses. Next, the pre-trained SASRec parameters are used as the initialization of Tri-CDR (and other CDR baselines). All parameters are then tuned via 𝐿 in Eq. (10). Through this, CDR models could be trained more effectively and stably.",
  "3.8 In-depth Model Discussions": "In this section, we undertake a comparison between the proposed Tri-CDR and the existing CDSR methods specifically tailored for the domain utilization, CL implementation, sequence modeling universality and complexity analyses, intending to analyze its novelty and effectiveness. 3.8.1 Comparison with Existing CDR Methods in Domain Utilization. CDSR aims to predict the next item that the user will be consumed in the target domain by leveraging the historical behavior sequence in the source domain. Therefore, the pioneer CDSR methods primarily focus on exploring how to achieve meaningful information transfer with the leverage of user's behavior in the source domain [17, 18, 62]. With the profound advancement of CDSR, some studies attempt to incorporate the mixed behavior sequence containing both source and target behaviors in chronological order to model the user's global interests. However, these studies are specifically designed for Cross-domain Session-based Recommendation [39] and Cross-domain Share-account recommendation [24], and may not be directly applicable to CDSR scenarios. Recently, DDGHM [58] and C 2 DSR [2] introduce the mixed behavior sequence into CDSR through the utilization of the global graph, yielding promising performance. Nevertheless, the aforementioned approaches typically model users' global and local interests separately with the mixed and target behavior sequences to capture their dynamic correlations. Despite the commendable performance of these methods, it overlooks the modeling of the user's preference in the source domain, which is the coherent sequence with its internal logic and equally critical for CDSR. Consequently, it may result in sub-optimal performance. In contrast, the proposed Tri-CDR jointly models the user's dynamic preference from the source, mixed and target domain, which facilitates the flexible and convenient integration of all available information into CDSR scenarios. Meanwhile, Tri-CDR proposes the Triple Cross-domain Attention mechanisms and Triple Contrastive Learning strategies for negative filtering, enabling fine-grained inter-domain relationship modeling and precise cross-domain positive knowledge transfer. The source, mixed and target domains are beneficial in CDR, while it is non-trivial to make full use of them. More detailed experimental results and analyses of the domain utilization are described in Sec.4.7. 3.8.2 Comparison with Existing CDR Methods in CL Implementation. As a self-supervised learning strategy, CL has been widely applied to collaborative filtering [19], multi-media recommendation [41], and sequential recommendation [30, 47]. Recently, some studies [2, 45, 58] have also introduced CL into CDR. DDGHM [58] designs an intra-domain 9 Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Haokai Ma et al. contrastive metric with the random item augmentation operator to enhance representation learning and alleviate data sparsity issues. However, despite employing random sequence-based CL augmentation for inferring informative representation, DDGHM's intra-CL method does not explicitly consider the knowledge transfer and inter-domain correlation necessary for cross-domain modeling. C 2 DSR [2] is the most related CDR model, which develops an inter-domain contrastive infomax objective to improve the correlation between the domain-aware prototype representations and corresponding corrupted representations. However, its inter-CL objective over-maximizes the mutual information of the individual representations between the mixed and target domain, disregarding the potential similarity conflicts that may result in optimization collapse. Both coarse-grained similarity (refer to positive transfer) and fine-grained distinction (refer to negative transfer) should be considered in inter-domain CL. Rather than solely relying on inter-CL between two domains, Tri-CDR incorporates all three domains and carefully model their triple relationships in contrastive tasks, ensuring that the optimization does not excessively collapse and get sufficient and accurate training. Precisely, Tri-CDR creatively proposes the coarse-gained similarity modeling and the fine-gained distinction modeling to comprehensively understand the triple correlations. The former captures the coarse-gained similarities between any two single-domain sequences of the same user to enable effective representation learning, while the latter maintains the robust cross-domain positive transfer through modeling the fine-gained distinction among triple sequences (see Sec. 4.5). Moreover, as an effective and model-agnostic framework, Tri-CDR is able to bring the SOTA CL-based SR models (intra-domain CL) into the sequence modeling and achieves significant improvements in CDR by leveraging the combined interplay of both inter-CL and intra-CL strategies (see Sec. 4.6). 3.8.3 Comparison with Existing CDR Methods in Sequence Modeling Universality. Behavior sequence modeling is essential in SR. Lots of novel techniques such as attention mechanisms [5, 31], side information [31, 48], and contrastive learning [47, 60] have been continuously proposed and verified to improve the performance of single-domain SR models. Amounts of CDR experiments have revealed that more recent and powerful single-domain SR models are able to outperform conventional CDSR algorithms [3, 8, 15, 58], which is also observed in our experiments (see Sec. 4.4). However, most existing CDSR models commonly incorporate simple SR models as their sequence encoders to verify their CDR strategies, or design complicated and customized networks to learn multi-domain interactions tailored to their specific cross-domain settings, overlooking whether the proposed algorithm possess the flexibility to adapt to different (current or future-updated) strong sequence encoders. Different from them, we argue that the universality of a CDSR algorithm with different sequence encoders is a crucial factor in guaranteeing the sustained effectiveness of the proposed framework, since real-world systems always prefer simple and universal methods. In this work, we try our best to enhance the universality and maintain the simplification of TCA and TCL without customized designs in the single-domain sequence modeling (see 4.6 for detailed results). Therefore, Tri-CDR could leverage possible advancements in single-domain sequence modeling (including model evolution in the future), thereby extending the lifecycle of our proposed CDSR algorithms. 3.8.4 Complexity Analyses . In this section, we analyze the complexity of Tri-CDR and compare it with classical SR models (SASRec [14], CL4SRec [47]) and CDSR algorithms (DASL [17], C 2 DSR [2]). The space complexity of the proposed Tri-CDR is largely determined by its sequence encoder. For example, apart from the mandatory space allocation for training three SASRec models (for source, mixed and target domains respectively), Tri-CDR (SASRec) only requires a limited number of MLPs (mentioned in Sec. 3.4 and 3.5) to be additionally trained. That is, Tri-CDR does not introduce excessive trainable parameters, which renders the space complexity of Tri-CDR akin to its sequence encoder. 10 Triple Sequence Learning for Cross-domain Recommendation Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Table 2. Statistics of three CDR datasets with six domains. As for the time complexity in model training, the base sequence encoders serve as the pivotal component of the CDSR tasks, and dominate its time complexity. Since different sequence encoders have different time complexity, we provide the time complexity of the used sequence encoders as follows: SASRec [14]: O(( 𝐿 2 + 𝐿 ) 𝑑 | 𝑈 |) , CL4SRec [47]: O(( 𝐿 2 + 𝐿 + 𝐵 ) 𝑑 | 𝑈 |) , where 𝐿 denotes the length of the behavior sequence, | 𝑈 | denotes the number of users, 𝐵 is the size of mini-batch. Meanwhile, we also analyze the time complexity of the state-of-the-art CDSR algorithms which are armed with dual-attention mechanism (DASL [17]), graph representation learning, and contrastive learning paradigm (C 2 DSR [2]). That is, the time complexity of DASL and C 2 DSR is O(( 𝐿 2 + 𝐿 ) 𝑑 | 𝑈 |) and O(( 𝐿 2 + 𝐿 + 𝐵 +|R 𝑀 | + |R 𝑇 |) 𝑑 | 𝑈 |) respectively, where |R 𝑀 | and |R 𝑇 | denotes the number of nonzero entries in the Laplacian matrix in the mixed and target domain respectively. In contrast, we performed a comprehensive analysis of the time complexity of Tri-CDR on different sequence encoders, revealing that both Tri-CDR (SASRec) and Tri-CDR (CL4SRec) share the identical time complexity as O(( 𝐿 2 + 𝐿 + 𝐵 ) 𝑑 | 𝑈 |) . The aforementioned analyses indicate that Tri-CDR has asymptotic similar time complexity with CL4SRec in training. It is worth noting that Tri-CDR does not conduct TCL in the inference phase. Therefore, its online inference item complexity (which is the central metric considered in practical usages of recommendation models rather than the training time complexity) is comparable with DASL in magnitude. These computational complexities make Tri-CDR scalable on large cross-domain datasets. Therefore, the proposed Tri-CDR is able to employ cross-domain positive transfer and obtain high-quality sequence representations with only tolerant additional time cost and space cost.",
  "4 EXPERIMENTS": "In this section, we conduct extensive experiments to answer the following research questions: (RQ1): How does Tri-CDR perform against the state-of-the-art SR and CDSR baselines (see Sec. 4.4)? (RQ2): How do different components of Tri-CDR benefit its performance (see Sec. 4.5)? (RQ3): Is Tri-CDR still effective with other base sequence encoders (see Sec. 4.6)? (RQ4): Does the introduction of additional domains lead to reasonable performance improvements? (see Sec. 4.7)?(RQ5): How do some important hyper-parameters affect the performance of Tri-CDR (see Sec. 4.8)? (RQ6): How do the the coarse-grained similarity modeling and fine-grained distinction modeling contribute to the multi-domain representation learning in CDR(see Sec. 4.9)?",
  "4.1 Datasets": "To verify the effectiveness and universality of Tri-CDR, we conduct extensive experiments on six real-world CDR settings with four domains from Amazon [26] and two domains from Douban [57]. Following [17, 58], we select Amazon Book & Movie , Amazon Book & Movie and Douban Book & Music to form six CDR tasks. Amazon Toy & Game, Amazon Book & Movie and Douban Book & Music include the review records from October 2000 to October 2018, from December 1996 to September 2018 and from July 2005 to December 2011 respectively. These three cross-domain datasets are pre-processed via the same method following classical CDR studies [15, 17]. Specifically, we build three user behavior sequences on the source, target, and mixed domains in chronological order. We set the last interacted item of each user 11 Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Haokai Ma et al. as the test set and the penultimate interacted item as the valid set based on the Leave-one-out splitting method [33, 56]. We randomly select the users that have behaviors in both domains, filter out users having less than 3 behaviors, and treat all rating records as interacted behaviors [13, 28]. The detailed statistics are shown in Table 2.",
  "4.2 Baselines": "In this section, we compare Tri-CDR with three representative SR models and five cross-domain SR models as follows: · GRU4Rec. GRU4Rec [11] is a classical session-based recommendation method encoding the item sequence via GRU. · SASRec. SASRec [14] is a widely-used sequential recommendation model. It applies the self-attention mechanism to model behavior interactions in sequential recommendation. · CL4SRec. CL4SRec [47] is the SOTA CL-based SR model, which adopts three sequence augmentation approaches to generate self-supervised signals via three intra-domain CL tasks. Benefiting from the implementation of the self-attention mechanism, SASRec achieves a significant improvement relative to RNN-based algorithms in SR. For fair comparisons, we use the same sequence encoder of Tri-CDR (i.e., SASRec) in all cross-domain SR baselines (noted as Model+ ). They also share the same features and historical behaviors: · SASRec(S+T). SASRec(S+T) is a straightforward CDSR method based on SASRec. It first generates the source and target sequence representations respectively, then concatenates and feeds them into an MLP for the final prediction. · DTCDR+. DTCDR [62] is a pioneer dual-target CDR method, which proposes an adaptable embedding sharing strategy to combine and share the user embeddings across domains based on the multi-task learning paradigm. · DDTCDR+. DDTCDR [18] designs a latent orthogonal mapping function for extracting and transferring user preferences between two related domains while preserving cross-domain relations across different latent spaces in an iterative manner through a dual-transfer method. · DASL+. DASL [17] constructs dual embeddings to extract the user's independent preferences and captures the user's cross-domain preference through a dual-attention learning mechanism. With its inherent structural superiority, DASL facilitates smooth incorporation of different sequence encoders. After replacing the sequence encoder from GRU to SASRec, DASL further achieves the promising performance, establishing itself as a strong baseline in CDSR. · C 2 DSR+. C 2 DSR [2] is the SOTA model in CDSR, which leverages a effective graph neural network to exploit the inter-domain co-occurrence collaborative relationship and proposes an contrastive infomax objective to capture and transfer the user's cross- domain preferences via the mutual information maximization mechanism.",
  "4.3 Experimental Settings": "We implement the above methods using PyTorch with python 3.8.10. For fair comparisons, we take Adam as the optimizing method and the learning rate is set as 0 . 0005. We initialize model parameters randomly using the Xavier method. The batch size and the dimension of embedding size are set as 120 and 64. We adopt the same maximum length of sequence for each model, which is 200 on all datasets. We conduct a grid search to select hyper-parameters. Specifically, we select the 𝜆 𝐶𝑆𝑀 and 𝜆 𝐹𝐷𝑀 in {0.1, 0.5, 1, 4, 10}. For the Amazon dataset, which exhibits significant disparities in sparsity between the two domains, we define the ratio between 𝜆 1, 𝜆 2 and 𝜆 3 as 1:1:1 for the sparser Amazon Toy and Amazon Book and 100:1:1 and 1000:1:1 for the denser Amazon Movie and Amazon Game respectively. In contrast, we define the ratio between 𝜆 1, 𝜆 2 and 𝜆 3 as 1:1:1 for the Douban dataset with similar sparsity of two domains. The temperature coefficient 𝜏 is set to be 0 . 1. According to the average natural distribution of the behavior data in Amazon dataset, we set 𝛾 to 4 . 0 in the sparser Amazon Toy and Amazon Book, 0 . 5 in the denser Amazon Game 12 Triple Sequence Learning for Cross-domain Recommendation Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Table 3. Results on cross-domain recommendation on Amazon platform. All improvements are significant (p<0.05 with paired t-tests). and Amazon Movie. Regarding for the Douban dataset, we define the 𝛾 as 0 . 5 and 0 . 05 for Douban Book and Douban Music respectively. We conduct five runs with different random seeds and report the average results of all models.",
  "4.4 Performance Comparison on Cross-domain Sequential Recommendation (RQ1)": "We conduct our experiments on six CDR tasks, adopting three typical evaluation metrics including NDCG@k (N@k), Hit Rate@k (HR@k), and AUC with different 𝑘 = 5 , 10 , 20 , 50. Following [14], we randomly sample 99 negative items for each positive instance in testing phase. Table 3 and Table 4 shows the results on the Amazon platform and Douban platform respectively, and we can observe that: (1) Tri-CDR achieves the best performances on all metrics and datasets compared to all baselines, with the significance level 𝑝 < 0 . 05. The NDCG@10 improvements of Tri-CDR over the best baseline are 4 . 52%/2 . 95%/5 . 18%/1 . 42% on 13 Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Haokai Ma et al. Table 4. Results on cross-domain recommendation on Douban platform. All improvements are significant (p<0.05 with paired t-tests). Amazon Toy/Game/Book/Movie, and the HR@10 improvements are 3 . 89%/2 . 63%/4 . 25%/0 . 37% on four Amazon datasets consistently. Similarly, the NDCG@10 improvements of Tri-CDR over the best baseline are 1 . 73%/1 . 25% on Douban Book/Music, and the HR@10 improvements are 1 . 26%/0 . 88%. It indicates that our triple sequence learning is beneficial in CDR. Moreover, it also demonstrates that Tri-CDR can well model the correlations among users' triple behavior sequences, and successfully capture useful information related to the target-domain prediction from all domains. (2) Tri-CDR outperforms all CDR baselines that also consider multi-domain behaviors. It confirms the significance of (a) explicit triple sequence learning with the mixed behavior sequence that contains the user's global interests, and (b) our TCL and TCA that could better model triple correlations among three domains and combine them into user representations. Comparing with SOTA CDSR models we can know that, DDTCDR and DASL focus on the dual transfer between source and target domains. They perform worse than Tri-CDR due to the lack of triple sequence learning. C 2 DSR also conducts contrastive infomax objectives to improve global-local dual correlations. However, it does not consider the fine-grained distinctions in domain correlation modeling and triple cross-domain attention in multi-domain combination. Sometimes the infomax loss is not well functioned, which may be caused by its unreal negative sequences. Tri-CDR also outperforms different ablation versions besides baselines as shown in Sec. 4.5. (3) Comparing the improvements among different domains, we find that Tri-CDR is more beneficial on Game → Toy(sparse) and Movie → Book(sparse) settings. It not only confirms the effectiveness of Tri-CDR on different data distributions, but also reflects that Tri-CDR functions well on relatively sparser target domains, where the positive knowledge transfer from the source domain should bring in more essential information as supplements, implying the practical usage of Tri-CDR. (4) To confirm its universality with different application platforms, we have also evaluated Tri-CDR on the Douban dataset (Book & Music) besides Amazon datasets. In combination with the statistics of CDR datasets in Table 4, we discover that: (a) consistent with the conventional CDSR algorithms, Tri-CDR functions well on relatively sparser target domains by transferring the positive knowledge from the denser source domain. (b) In contrast to the cross-domain settings on the Amazon platform, Tri-CDR shows relatively similar improvements in the two cross-domain settings on 14 Triple Sequence Learning for Cross-domain Recommendation Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Table 5. Results on ablation study of Tri-CDR(SASRec). Douban. This may be attributed to the fact that users have longer average interactions on Douban dataset (48.93 and 60.89 for Book and Music respectively). (c) The experimental results also demonstrate that some dual-modeling CDSR methods can obtain promising performance on the Douban dataset. However, Tri-CDR outperforms these algorithms on most metrics, which provides further evidence of Tri-CDR's significant role in alleviating cross-domain negative transfer and accurately modeling the correlations among the triple domains. We further conduct a universality analysis on Tri-CDR adopted with different base sequence encoders in Sec. 4.6.",
  "4.5 Ablation Study (RQ2)": "In this section, we aim to find whether different components are effective in Tri-CDR. Thus we compare Tri-CDR with Tri-CDR w/o TCA&TCL, Tri-CDR w/o TCA and Tri-CDR w/o FDM to verify the benefits of TCA, TCL and FDM. In general, most components' improvements are significant (the average error range ≤ 0 . 003). In Table 5 we have: (1) Tri-CDR w/o TCA performs significantly better than Tri-CDR w/o TCA&TCL, verifying the effectiveness of TCL. TCL takes full advantage of CL's alignment and uniformity [37, 51] and extends it to triple domains, maximizing the multi-domain mutual information while remaining necessary preference diversity in knowledge transfer. (2) Tri-CDR further improves the results of Tri-CDR w/o TCA. Our TCA highlights the information related to the target-domain preference learned from the current target-domain historical behaviors as well as the user's global interests learned from mixed behaviors via cross-domain attention, which enables more positive knowledge transfer and is beneficial for CDR. (3) Comparing Tri-CDR with and without FDM, we further demonstrate that the fine-grained distinction modeling in TCL is indispensable. FDM keeps the domain diversity and significantly brings in additional 1 . 60%/0 . 78% average 15 Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Haokai Ma et al. Fig. 3. Universality analysis on Tri-CDR. We show the results of different versions of Tri-CDR on GRU4Rec and CL4SRec. 0.13 0.15 0.17 0.19 NDCG@10 0.43 0.44 0.45 0.46 NDCG@10 0.23 0.25 0.27 0.29 0.31 HR@10 0.63 0.64 0.65 0.66 0.67 HR@10 (a) Amazon Toy (d) Amazon Movie (e) Amazon Toy (h) Amazon Movie GRU4Rec (T) GRU4Rec (M) GRU4Rec (T+S+M) Tri-CDR (GRU4Rec) w/o FDM Tri-CDR (GRU4Rec) 0.2 0.21 0.22 0.23 0.24 NDCG@10 0.45 0.47 0.49 0.51 NDCG@10 0.32 0.33 0.34 0.35 0.36 HR@10 0.66 0.67 0.68 0.69 0.7 HR@10 (i) Amazon Toy (l) Amazon Movie (m) Amazon Toy (p) Amazon Movie CL4SRec (T) CL4SRec (M) CL4SRec (T+S+M) Tri-CDR (CL4SRec) w/o FDM Tri-CDR (CL4SRec) 0.29 0.3 0.31 0.32 0.33 0.34 NDCG@10 0.45 0.47 0.49 0.51 0.53 HR@10 (b) Amazon Game (f) Amazon Game 0.34 0.36 0.38 0.4 NDCG@10 0.53 0.55 0.57 0.59 HR@10 (j) Amazon Game (n) Amazon Game 0.24 0.26 0.28 0.3 0.32 NDCG@10 0.39 0.41 0.43 0.45 0.47 HR@10 (c) Amazon Book (g) Amazon Book 0.3 0.32 0.34 0.36 NDCG@10 0.46 0.48 0.5 0.52 HR@10 (k) Amazon Book (o) Amazon Book NDCG@10/HR@10 improvements on six datasets. Besides, it not only helps Tri-CDR to learn better multi-domain sequence representations, but also makes the model training more stable with different parameters.",
  "4.6 Universality of Tri-CDR (RQ3)": "Tri-CDR is a model-agnostic framework. We further evaluate its universality on Amazon datasets based on GRU4Rec and CL4SRec. Fig. 3 illustrates Tri-CDR models on GRU4Rec and CL4SRec, and we can find that: (1) In general, Tri-CDR still achieves consistent and significant improvements over different ablation versions on all datasets with both GRU4Rec and CL4SRec, which confirms its universality when adopted with different sequential models and even other CL tasks. The improvements are consistent with other metrics. (2) Comparing with different Tri-CDR's ablation versions, we reconfirm that (a) the mixed behavior sequence is informative, while directly combining source, target, and mixed sequences may also bring in noises, and (b) the proposed TCA and TCL with FDM are effective to make full use of information of three domains in CDR. (3) We should highlight that CL4SRec also conducts intra-domain CL tasks based on some sequence augmentations. The improvement brought by Tri-CDR implies that our inter-domain CL could cooperate well with various intra-domain CL. We notice that CL4SRec(M) has comparable or even better results over Tri-CDR on the Book domain after careful 16 Triple Sequence Learning for Cross-domain Recommendation Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Table 6. Results on different domain utilization of Tri-CDR (SASRec). parameter selections. It is because that the intra-domain CL of CL4SRec on the mixed sequence (containing multi-domain behaviors) works as certain inter-domain CL tasks. Nevertheless, Tri-CDR still achieves the best results in general.",
  "4.7 Analyses on the domain utilization of Tri-CDR (RQ4)": "To inspect the effectiveness of different domains on Tri-CDR, we use S, T, and M to represent using source, target, and mixed sequences respectively, and compare Tri-CDR with five combinations. It is worth noting that SASRec (T) and SASRec (M) denote the single-domain SR models with the user's target behavior sequence and mixed behavior sequence. On the other hand, SASRec (S+T), SASRec (M+T), and SASRec (S+M+T) refer to the cross-domain SR models based on the user's source and target domain [17, 18, 24, 62], mixed and target domain [2, 58], and mixed, source and target domain (the proposed CDSR setting), respectively. From Table. 6, we can observe that: (a) Comparing the first two versions, SASRec (M) significantly outperforms SASRec (T) on all metrics and datasets. It is reasonable since the mixed sequence is the complete user chronological behavior sequence from both the source and target domains. The sequential encoder on the mixed behavior sequence can be viewed as the cross-domain sequence encoder to some extent, yielding cross-domain information gain. (b) Dual-modeling CDR methods (SASRec (S+T) and SASRec (M+T)) do not always achieve consistent improvement over the single-domain SR methods, and the simply triple-modeling CDR method (SASRec (S+T+M)) may even lead to further performance deterioration in some cross-domain settings. It further reinforces that both source and mixed sequences encompass cross-domain knowledge and noise information simultaneously and that simple dual- and triplemodeling strategies may be insufficient to accurately distinguish and model the complex correlations and confounding knowledge in multiple sequences. 17 Fig. 4. Parameter analyses on loss weight 𝜆 𝐶𝑆𝑀 of L 𝐶𝑆𝑀 , 𝜆 𝐹𝐷𝑀 of L 𝐹𝐷𝑀 , margin 𝛾 of L 𝐹𝐷𝑀 and temperature coefficient 𝜏 . (a)-(h) shows the results of Game → Toy, and (i)-(p) shows the results of Toy → Game. Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Haokai Ma et al. (a) Loss weight 𝜆 !\"# of 𝐿 !\"# (b) Loss weight 𝜆 $%# of 𝐿 $%# (c) Margin γ of 𝐿 $%# (e) Loss weight 𝜆 !\"# of 𝐿 !\"# (f) Loss weight 𝜆 $%# of 𝐿 $%# (g) Margin γ of 𝐿 $%# (d) Temperature coefficient τ (h) Temperature coefficient τ 0.18 0.2 0.22 0.24 0.01 0.05 0.1 1 10 NDCG@5 NDCG@10 0.18 0.2 0.22 0.24 1 5 10 20 50 NDCG@5 NDCG@10 0.18 0.2 0.22 0.24 0 1 4 10 100 NDCG@5 NDCG@10 0.18 0.2 0.22 0.24 0.05 0.1 0.2 0.5 1 NDCG@5 NDCG@10 0.34 0.35 0.36 0.26 0.27 0.28 0.01 0.05 0.1 1 10 HR@5 HR@10 … 0.34 0.35 0.36 0.26 0.27 0.28 0.01 0.05 0.1 1 10 HR@5 HR@10 0.34 0.35 0.36 0.26 0.27 0.28 0.01 0.05 0.1 1 10 HR@5 HR@10 0.34 0.35 0.36 0.26 0.27 0.28 0.01 0.05 0.1 1 10 HR@5 HR@10 … … … 0.3 0.32 0.34 0.36 0.38 0.4 0.1 1 4 10 50 NDCG@5 NDCG@10 0.3 0.32 0.34 0.36 0.38 0.4 0 1 5 10 50 NDCG@5 NDCG@10 0.3 0.32 0.34 0.36 0.38 0.4 0 0.1 1 5 20 NDCG@5 NDCG@10 0.3 0.32 0.34 0.36 0.38 0.4 0.05 0.1 0.2 0.5 1 NDCG@5 NDCG@10 0.55 0.57 0.59 0.42 0.45 0.48 0.1 1 4 10 50 HR@5 HR@10 … 0.55 0.57 0.59 0.42 0.45 0.48 0 1 5 10 50 HR@5 HR@10 0.55 0.57 0.59 0.42 0.45 0.48 0 0.1 1 5 20 HR@5 HR@10 0.55 0.57 0.59 0.42 0.45 0.48 0.05 0.1 0.2 0.5 1 HR@5 HR@10 … … … (i) Loss weight 𝜆 !\"# of 𝐿 !\"# (j) Loss weight 𝜆 $%# of 𝐿 $%# (k) Margin γ of 𝐿 $%# (l) Temperature coefficient τ (m) Loss weight 𝜆 !\"# of 𝐿 !\"# (n) Loss weight 𝜆 $%# of 𝐿 $%# (o) Margin γ of 𝐿 $%# (p) Temperature coefficient τ (c) Comparing SASRec (S+T+M) and Tri-CDR on four cross-domain settings, we further demonstrate the effectiveness of the proposed TCA and TCL, which is primarily due to the fact that TCA highlights the users' target-domain preferences and comprehensive interests from the cross-domain knowledge transfer, and TCL precisely models the correlation among triple behavior sequences.",
  "4.8 Parameter Sensitivity Analyses (RQ5)": "4.8.1 Analyses on CL loss weights 𝜆 𝐶𝑆𝑀 , 𝜆 𝐹𝐷𝑀 , margin 𝛾 and temperature coefficient 𝜏 . In Fig. 4, we conduct four parameter analyses on the Game → Toy setting (the first two rows) and the Toy → Game setting (the last two rows) to investigate the performance trends with different loss weight 𝜆 𝐶𝑆𝑀 of L 𝐶𝑆𝑀 , loss weights 𝜆 𝐹𝐷𝑀 of L 𝐹𝐷𝑀 , margin 𝛾 of L 𝐹𝐷𝑀 and temperature coefficient 𝜏 in Eq. (6) and Eq. (7). We observe that: (1) Tri-CDR's performance first increases and then decreases as 𝜆 𝐶𝑆𝑀 and 𝜏 gets larger. According to the behavior distribution characteristics, the loss weight 𝜆 𝐹𝐷𝑀 of L 𝐹𝐷𝑀 differs for different cross-domain settings. 𝜆 𝐶𝑆𝑀 = 0 . 1 / 4 . 0 achieves the best performance on Game → Toy and Toy → Game settings, respectively. In contrast, Tri-CDR is insensitive to the temperature coefficient 𝜏 , so we set it as 0 . 1 for all CDSR settings with the purpose of simplifying the hyper-parameter tuning and obtaining the promising performance. (2) Too smaller 𝜆 𝐹𝐷𝑀 may weaken the power of FDM in TCL, while too larger 𝜆 𝐹𝐷𝑀 may also disturb the triple correlation learning in CSM. It is also natural that Tri-CDR is less sensitive to fine-grained 𝜆 𝐹𝐷𝑀 18 Triple Sequence Learning for Cross-domain Recommendation Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Table 7. Parameter analyses on the ratio among loss weights 𝜆 1 , 𝜆 2 and 𝜆 3 compared to coarse-grained 𝜆 𝐶𝑆𝑀 . The optimal performance of Tri-CDR is observed when 𝜆 𝐹𝐷𝑀 is set to 10 . 0 and 5 . 0 on Game → Toy and Toy → Game settings. (3) 𝛾 = 0 indicates that the model only wants the source-mixed distance to be smaller than the source-target distance. When 𝛾 = 100, Eq. (7) is always active to broaden the fine-grained cross-domain distance gap. Tri-CDR performs relatively poor under the extreme values of both sides, indicating the importance of an appropriate margin in FDM (4 . 0 and 1 . 0 for the Game → Toy and Toy → Game settings). 4.8.2 Analyses on loss weights 𝜆 1 , 𝜆 2 , and 𝜆 3 in CSM. We implement a series of experiments to investigate the effect of different ratios among 𝜆 1, 𝜆 2, and 𝜆 3 of L 𝐶𝑆𝑀 in Tri-CDR. Table 7 shows the results of Tri-CDR(SASRec) with or without FDM on the Game → Toy and Toy → Game settings. We observe that: (1) for Tri-CDR w/o FDM, larger loss weights of 𝜆 1 lead to better performance on the Toy → Game setting (the relatively denser target domains). It is natural since the correlations between the source and mixed behavior sequences should be more highlighted to ensure learning informative source sequence representations from the sparser source behavior sequences. For the Game → Toy setting (the sparser target domains), we find that the loss weight ratio of 1:1:1 already achieves the best performance. (2) For Tri-CDR, we also observe the same trend on the Toy → Game setting that the performances increase as 𝜆 1 becomes larger. Moreover, regardless of the ratio among 𝜆 1, 𝜆 2, and 𝜆 3, Tri-CDR shows consistent improvements compared to Tri-CDR w/o FDM. Enhanced by FDM, the results are relatively satisfactory even with imperfect loss weights (average significant improvements of 1 . 17% and 0 . 51% on NDCG@10 and HR@10 respectively). It demonstrates that the proposed FDM helps to improve the effectiveness as well as the robustness of triple contrastive learning, making it less sensitive to different loss weights in CSM and more practical in real-world scenarios. (3) In comparing the different performances of Tri-CDR with different ratios between 𝜆 2, and 𝜆 3 in two cross-domain settings, we notice that Tri-CDR exhibited further improvements after reducing 𝜆 3 (loss weight of L 𝐶 𝑆𝑀 between source and target domains) on the Game → Toy setting. This improvement is interpretable as its underlying principle aligns with certain assumptions of FDM. Specifically, Tri-CDR precisely controls the similarity between the source and target domains by constraining the hyper-parameters, thereby modeling the fine-grained distinction among triple sequences. To mitigate the complexity of 19 Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Haokai Ma et al. Fig. 5. Visualization of different Tri-CDR versions among triple domains, where the blue cross, green clubs, and red circle denote the mixed sequence representation, source sequence representation and target sequence representation respectively. (a) SASRec (S+T+M) (b) Tri-CDR w/o FDM (c) Tri-CDR (d) SASRec (S+T+M) (e) Tri-CDR w/o FDM (f) Tri-CDR Mixed sequence representation Source sequence representation Target sequence representation model training while preserving its scalability, we maintain the balanced ratio of 𝜆 2, and 𝜆 3 as 1:1 when presenting the performance of Tri-CDR. Experimental results also indicate that Tri-CDR achieves significant improvements over the current state-of-the-art baseline, even without fine-grained tuning of the ratio between 𝜆 2, and 𝜆 3.",
  "4.9 Estimation of User Preference Modeling in Tri-CDR (RQ6)": "To intuitively show the impacts of both similarity and distinction modeling in TCL, we show the visualization of randomly selected multi-domain sequence representations in SASRec (S+T+M), Tri-CDR w/o FDM, and Tri-CDR via t-Distributed Stochastic Neighbor Embedding (t-SNE) [36]. Fig. 5 and Fig. 6 illustrate the distributions of three domains and different users via different colors and shapes, respectively. In each figure, the first row refers to the visualization on the Game → Toy setting, while the second row represents the visualization on the Toy → Game setting. 4.9.1 Visualization of the overall distribution across triple domains. We depict the overall distribution of the randomly selected 100 users' sequential representations across different domains via t-SNE in two cross-domain settings, as illustrated in Fig. 5. We observe that: (1) As shown in Fig. 5 (a) and (d), most users' multi-domain sequence representations are naturally clustered via their domains rather than their users in SASRec (S+T+M). (2) Comparing Fig. 5 (b) and (e) with Fig. 5 (a) and (d), the multi-domain sequence representations are converted from domain-based clustering to the user-based, which indicates that the coarse-grained similarity modeling in TCL does make triple sequence representations of a user to be similar. (3) In contrast to Fig. 5 (e), the target domain sequence representations in Fig. 5 (f) do not aggregate in the approximate location but rather distributed around user-specific cross-domain information. This indicates that FDM enhances the discriminability of the target domain sequence representations in coarse-grained comparisons, enabling precise modeling of the users' target interests. 4.9.2 Visualization of the independent distribution among different users. In order to investigate the independent distribution of different domain sequence representations among different users, we randomly select 10 users and visualized the aforementioned representations with t-SNE. The observations are as follows: (1) Similar to Fig. 5(a) and 20 Triple Sequence Learning for Cross-domain Recommendation Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Fig. 6. Visualization of different Tri-CDR versions from the perspective of different users. We employ color differentials to distinguish different users while utilizing shapes to differentiate the same user's sequential representations across triple domains. (a) SASRec (S+T+M) (b) Tri-CDR w/o FDM (c) Tri-CDR (d) SASRec (S+T+M) (e) Tri-CDR w/o FDM (f) Tri-CDR Mixed sequence representation Source sequence representation Target sequence representation (d), the distances among the same user's triple sequence representations are relatively large in 6(a) and (d). This leads to a failure in building adequate cross-domain correlations, and thus cannot make full use of additional source/mixed domains' information. (2) In Fig. 6(b) and (e), irrespective of whether adjust the ratio hyper-parameters or not, the association among triple sequence representations of the same user is consistently improved with CSM. This proves the effectiveness of the information gain derived from coarse-grained similarity modeling. (3) In Fig. 6(b), some users' multi-domain sequence representations are too close to form small acute triangles. Too homogeneous multi-domain representations may weaken the additional information gains from source/mixed sequences, which will harm the positive knowledge transfer. In contrast, armed with FDM, these users in Fig. 6(c) have more distinguishable sequence representations forming obtuse triangles, and thus Tri-CDR could achieve better results. (4) Despite the employment of sufficient tuning ratios among 𝜆 1, 𝜆 2, and 𝜆 3 in CSM, Fig. 6(e) still exhibits unexpected deviations in the triple correlation. In contrast, the visualization of triangles in Fig. 6(f) provides tangible evidence of the practical significance of FDM.",
  "5 CONCLUSION": "In this work, we propose a model-agnostic Triple sequence learning for Cross-Domain Recommendation (Tri-CDR) framework. Conventional CDR methods mainly focus on modeling the dual-relations between the source and target domains or the mixed and target domains, failing to explore the triple correlation among the source, mixed, and target domains. Tri-CDR conducts a triple cross-domain attention method to highlight useful information and accelerate positive knowledge transfer and enables a more accurate multi-domain sequence representation learning strategy via both the coarse-grained similarity modeling and fine-grained distinction modeling. The extensive evaluation and analyses on six benchmark cross-domain settings demonstrate that Tri-CDR is able to precisely model the similarity while preserving the information diversity among triple domains, which reveals the underlying principles of its effectiveness and universality. We believe that the triple sequence learning paradigm will provide a solid foundation for researchers and practitioners to explore new directions in cross-domain recommendation. 21 Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Haokai Ma et al. In the future, we will continue to explore the correlations among the source, target, and natural mixed behavior sequences, as well as more sophisticated modeling on their representation learning and multi-domain aggregation. We will also enhance Tri-CDR's capability by incorporating more modality information of item contents as semantic bridges in multi-domain recommendation.",
  "ACKNOWLEDGMENTS": "To Robert, for the bagels and explaining CMYK and color spaces.",
  "REFERENCES": "[1] Ye Bi, Liqiang Song, Mengqiu Yao, Zhenyu Wu, Jianming Wang, and Jing Xiao. 2020. A heterogeneous information network based cross domain insurance recommendation system for cold start users. In Proceedings of International Conference on Research on Development in Information Retrieval (SIGIR) . [2] Jiangxia Cao, Xin Cong, Jiawei Sheng, Tingwen Liu, and Bin Wang. 2022. Contrastive Cross-Domain Sequential Recommendation. In Proceedings of ACM International Conference on Information and Knowledge Management (CIKM) . [3] Jiangxia Cao, Xixun Lin, Xin Cong, Jing Ya, Tingwen Liu, and Bin Wang. 2022. DisenCDR: Learning Disentangled Representations for Cross-Domain Recommendation. In Proceedings of International Conference on Research on Development in Information Retrieval (SIGIR) . [4] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A simple framework for contrastive learning of visual representations. In Proceedings of International Conference on Machine Learning (ICML) . [5] Ziwei Fan, Zhiwei Liu, Yu Wang, Alice Wang, Zahra Nazari, Lei Zheng, Hao Peng, and Philip S Yu. 2022. Sequential Recommendation via Stochastic Self-Attention. In Proceedings of International World Wide Web Conferences (WWW) . [6] John Giorgi, Osvald Nitski, Bo Wang, and Gary Bader. 2021. DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations. In Proceedings of Annual Meeting of the Association for Computational Linguistics (ACL) . [7] Lei Guo, Li Tang, Tong Chen, Lei Zhu, Quoc Viet Hung Nguyen, and Hongzhi Yin. 2021. DA-GCN: a domain-aware attentive graph convolution network for shared-account cross-domain sequential recommendation. Proceedings of International Joint Conference on Artificial Intelligence (IJCAI) . [8] Xiaobo Guo, Shaoshuai Li, Naicheng Guo, Jiangxia Cao, Xiaolei Liu, Qiongxu Ma, Runsheng Gan, and Yunan Zhao. 2023. Disentangled Representations Learning for Multi-target Cross-domain Recommendation. ACM Transactions on Information Systems (TOIS) (2023). [9] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. Momentum contrast for unsupervised visual representation learning. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR) . [10] Ruining He and Julian McAuley. 2016. Fusing similarity models with markov chains for sparse sequential recommendation. In Proceedings of International Conference on Data Mining (ICDM) . [11] Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2016. Session-based recommendations with recurrent neural networks. In Proceedings of International Conference on Learning Representations (ICLR) . [12] Guangneng Hu, Yu Zhang, and Qiang Yang. 2018. Conet: Collaborative cross networks for cross-domain recommendation. In Proceedings of ACM International Conference on Information and Knowledge Management (CIKM) . [13] SeongKu Kang, Junyoung Hwang, Dongha Lee, and Hwanjo Yu. 2019. Semi-supervised learning for cross-domain recommendation to cold-start users. In Proceedings of ACM International Conference on Information and Knowledge Management (CIKM) . [14] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recommendation. In Proceedings of International Conference on Data Mining (ICDM) . [15] Chenglin Li, Mingjun Zhao, Huanming Zhang, Chenyun Yu, Lei Cheng, Guoqiang Shu, Beibei Kong, and Di Niu. 2022. RecGURU: Adversarial Learning of Generalized User Representations for Cross-Domain Recommendation. In Proceedings of ACM International Conference on Web Search and Data Mining (WSDM) . [16] Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Tao Lian, and Jun Ma. 2017. Neural attentive session-based recommendation. In Proceedings of ACM International Conference on Information and Knowledge Management (CIKM) . [17] Pan Li, Zhichao Jiang, Maofei Que, Yao Hu, and Alexander Tuzhilin. 2021. Dual Attentive Sequential Learning for Cross-Domain Click-Through Rate Prediction. In Proceedings of ACM Knowledge Discovery and Data Mining (SIGKDD) . [18] Pan Li and Alexander Tuzhilin. 2020. Ddtcdr: Deep dual transfer cross domain recommendation. In Proceedings of ACM International Conference on Web Search and Data Mining (WSDM) . [19] Zihan Lin, Changxin Tian, Yupeng Hou, and Wayne Xin Zhao. 2022. Improving Graph Collaborative Filtering with Neighborhood-enriched Contrastive Learning. In Proceedings of International World Wide Web Conferences (WWW) . [20] Kang Liu, Feng Xue, Dan Guo, Le Wu, Shujie Li, and Richang Hong. 2023. Megcf: Multimodal entity graph collaborative filtering for personalized recommendation. Proceedings of ACM Transactions on Information Systems (TOIS) (2023). [21] Meng Liu, Jianjun Li, Guohui Li, and Peng Pan. 2020. Cross Domain Recommendation via Bi-Directional Transfer Graph Collaborative Filtering Networks. In Proceedings of ACM International Conference on Information and Knowledge Management (CIKM) . 22 Triple Sequence Learning for Cross-domain Recommendation Conference acronym 'XX, June 03-05, 2018, Woodstock, NY In Proceedings of the 16th ACM Conference on Recommender Systems (Recsys) . [32] Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010. Factorizing personalized markov chains for next-basket recommendation. In Proceedings of International World Wide Web Conferences (WWW) . 811-820. [33] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer. In Proceedings of ACM International Conference on Information and Knowledge Management (CIKM) . [34] Wenchao Sun, Muyang Ma, Pengjie Ren, Yujie Lin, Zhumin Chen, Zhaochun Ren, Jun Ma, and Maarten De Rijke. 2021. Parallel Split-Join Networks for Shared Account Cross-domain Sequential Recommendations. Proceedings of IEEE Transactions on Knowledge and Data Engineering (TKDE) . [35] Jiaxi Tang and Ke Wang. 2018. Personalized top-n sequential recommendation via convolutional sequence embedding. In Proceedings of ACM International Conference on Web Search and Data Mining (WSDM) . [36] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. Journal of machine learning research (2008). [37] Tongzhou Wang and Phillip Isola. 2020. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In Proceedings of International Conference on Machine Learning (ICML) . [38] Tianxin Wang, Fuzhen Zhuang, Zhiqiang Zhang, Daixin Wang, Jun Zhou, and Qing He. 2021. Low-dimensional Alignment for Cross-Domain Recommendation. In Proceedings of ACM International Conference on Information and Knowledge Management (CIKM) . [39] Yaqing Wang, Caili Guo, Yunfei Chu, Jenq-Neng Hwang, and Chunyan Feng. 2020. A cross-domain hierarchical recurrent model for personalized session-based recommendations. Proceedings of Neurocomputing . [40] Yiqi Wang, Chaozhuo Li, Zheng Liu, Mingzheng Li, Jiliang Tang, Xing Xie, Lei Chen, and Philip S Yu. 2022. An Adaptive Graph Pre-training Framework for Localized Collaborative Filtering. Proceedings of ACM Transactions on Information Systems (TOIS) (2022). [41] Yinwei Wei, Xiang Wang, Qi Li, Liqiang Nie, Yan Li, Xuanping Li, and Tat-Seng Chua. 2021. Contrastive learning for cold-start recommendation. In Proceedings of ACM International Conference on Multimedia (ACM MM) . [42] Haolun Wu, Chen Ma, Yingxue Zhang, Xue Liu, Ruiming Tang, and Mark Coates. 2022. Adapting Triplet Importance of Implicit Feedback for Personalized Recommendation. Proceedings of ACM International Conference on Information and Knowledge Management (CIKM) . [43] Yiqing Wu, Ruobing Xie, Yongchun Zhu, Xiang Ao, Xin Chen, Xu Zhang, Fuzhen Zhuang, Leyu Lin, and Qing He. 2022. Multi-view Multi-behavior Contrastive Learning in Recommendation. In Proceedings of Database Systems for Advanced Applications (DASFAA) . [44] Zhuofeng Wu, Sinong Wang, Jiatao Gu, Madian Khabsa, Fei Sun, and Hao Ma. 2020. Clear: Contrastive learning for sentence representation. arXiv preprint arXiv:2012.15466 . [45] Ruobing Xie, Qi Liu, Liangdong Wang, Shukai Liu, Bo Zhang, and Leyu Lin. 2022. Contrastive cross-domain recommendation in matching. In Proceedings of ACM Knowledge Discovery and Data Mining (SIGKDD) . [46] Ruobing Xie, Zhijie Qiu, Jun Rao, Yi Liu, Bo Zhang, and Leyu Lin. 2020. Internal and Contextual Attention Network for Cold-start Multi-channel Matching in Recommendation.. In Proceedings of International Joint Conference on Artificial Intelligence (IJCAI) . [47] Xu Xie, Fei Sun, Zhaoyang Liu, Shiwen Wu, Jinyang Gao, Jiandong Zhang, Bolin Ding, and Bin Cui. 2022. Contrastive learning for sequential recommendation. In Proceedings of IEEE International Conference on Data Engineering (ICDE) . 23 Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Haokai Ma et al. [48] Yueqi Xie, Peilin Zhou, and Sunghun Kim. 2022. Decoupled side information fusion for sequential recommendation. In Proceedings of International Conference on Research on Development in Information Retrieval (SIGIR) . [49] Chengfeng Xu, Pengpeng Zhao, Yanchi Liu, Jiajie Xu, Victor S Sheng S. Sheng, Zhiming Cui, Xiaofang Zhou, and Hui Xiong. 2019. Recurrent convolutional neural network for sequential recommendation. In Proceedings of International World Wide Web Conferences (WWW) . [50] Lyuxin Xue, Deqing Yang, Shuoyao Zhai, Yuxin Li, and Yanghua Xiao. 2023. Learning Dual-view User Representations for Enhanced Sequential Recommendation. Proceedings of ACM Transactions on Information Systems (TOIS) (2023). [51] Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Lizhen Cui, and Quoc Viet Hung Nguyen. 2022. Are graph augmentations necessary? simple graph contrastive learning for recommendation. In Proceedings of International Conference on Research on Development in Information Retrieval (SIGIR) . [52] Tianzi Zang, Yanmin Zhu, Haobing Liu, Ruohan Zhang, and Jiadi Yu. 2022. A survey on cross-domain recommendation: taxonomies, methods, and future directions. Proceedings of ACM Transactions on Information Systems (TOIS) (2022). [53] Shengyu Zhang, Dong Yao, Zhou Zhao, Tat-Seng Chua, and Fei Wu. 2021. Causerec: Counterfactual user sequence synthesis for sequential recommendation. In Proceedings of International Conference on Research on Development in Information Retrieval (SIGIR) . [54] Cheng Zhao, Chenliang Li, and Cong Fu. 2019. Cross-domain recommendation via preference propagation graphnet. In Proceedings of ACM International Conference on Information and Knowledge Management (CIKM) . [55] Chuang Zhao, Hongke Zhao, Ming HE, Jian Zhang, and Jianping Fan. 2023. Cross-domain recommendation via user interest alignment. In Proceedings of International World Wide Web Conferences (WWW) . [56] Wayne Xin Zhao, Junhua Chen, Pengfei Wang, Qi Gu, and Ji-Rong Wen. 2020. Revisiting Alternative Experimental Settings for Evaluating Top-N Item Recommendation Algorithms. In Proceedings of ACM International Conference on Information and Knowledge Management (CIKM) . [57] Wayne Xin Zhao, Shanlei Mu, Yupeng Hou, Zihan Lin, Kaiyuan Li, Yushuo Chen, Yujie Lu, Hui Wang, Changxin Tian, Xingyu Pan, Yingqian Min, Zhichao Feng, Xinyan Fan, Xu Chen, Pengfei Wang, Wendi Ji, Yaliang Li, Xiaoling Wang, and Ji-Rong Wen. 2021. Recbole: Towards a unified, comprehensive and efficient framework for recommendation algorithms. In Proceedings of ACM International Conference on Information and Knowledge Management (CIKM) . [58] Xiaolin Zheng, Jiajie Su, Weiming Liu, and Chaochao Chen. 2022. DDGHM: Dual Dynamic Graph with Hybrid Metric Training for Cross-Domain Sequential Recommendation. In Proceedings of ACM International Conference on Multimedia (ACM MM) . [59] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through rate prediction. In Proceedings of ACM Knowledge Discovery and Data Mining (SIGKDD) . [60] Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, and Ji-Rong Wen. 2020. S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization. In Proceedings of ACM International Conference on Information and Knowledge Management (CIKM) . [61] Yuanhang Zhou, Kun Zhou, Wayne Xin Zhao, Cheng Wang, Peng Jiang, and He Hu. 2022. C 2 -CRS: Coarse-to-Fine Contrastive Learning for Conversational Recommender System. In Proceedings of ACM International Conference on Web Search and Data Mining (WSDM) . [62] Feng Zhu, Chaochao Chen, Yan Wang, Guanfeng Liu, and Xiaolin Zheng. 2019. Dtcdr: A framework for dual-target cross-domain recommendation. In Proceedings of ACM International Conference on Information and Knowledge Management (CIKM) . [63] Yongchun Zhu, Kaikai Ge, Fuzhen Zhuang, Ruobing Xie, Dongbo Xi, Xu Zhang, Leyu Lin, and Qing He. 2021. Transfer-meta framework for cross-domain recommendation to cold-start users. In Proceedings of International Conference on Research on Development in Information Retrieval (SIGIR) . [64] Yongchun Zhu, Zhenwei Tang, Yudan Liu, Fuzhen Zhuang, Ruobing Xie, Xu Zhang, Leyu Lin, and Qing He. 2022. Personalized transfer of user preferences for cross-domain recommendation. In Proceedings of ACM International Conference on Web Search and Data Mining (WSDM) . Received 30 May 2023; revised 12 March 2009; accepted 5 June 2009 24",
  "keywords_parsed": [
    "cross-domain recommendation",
    "contrastive learning",
    "triple learning"
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "A heterogeneous information network based cross domain insurance recommendation system for cold start users"
    },
    {
      "ref_id": "b2",
      "title": "Contrastive Cross-Domain Sequential Recommendation"
    },
    {
      "ref_id": "b3",
      "title": "DisenCDR: Learning Disentangled Representations for Cross-Domain Recommendation"
    },
    {
      "ref_id": "b4",
      "title": "A simple framework for contrastive learning of visual representations"
    },
    {
      "ref_id": "b5",
      "title": "Sequential Recommendation via Stochastic Self-Attention"
    },
    {
      "ref_id": "b6",
      "title": "DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations"
    },
    {
      "ref_id": "b7",
      "title": "DA-GCN: a domain-aware attentive graph convolution network for shared-account cross-domain sequential recommendation"
    },
    {
      "ref_id": "b8",
      "title": "Disentangled Representations Learning for Multi-target Cross-domain Recommendation"
    },
    {
      "ref_id": "b9",
      "title": "Momentum contrast for unsupervised visual representation learning"
    },
    {
      "ref_id": "b10",
      "title": "Fusing similarity models with markov chains for sparse sequential recommendation"
    },
    {
      "ref_id": "b11",
      "title": "Session-based recommendations with recurrent neural networks"
    },
    {
      "ref_id": "b12",
      "title": "Conet: Collaborative cross networks for cross-domain recommendation"
    },
    {
      "ref_id": "b13",
      "title": "Semi-supervised learning for cross-domain recommendation to cold-start users"
    },
    {
      "ref_id": "b14",
      "title": "Self-attentive sequential recommendation"
    },
    {
      "ref_id": "b15",
      "title": "RecGURU: Adversarial Learning of Generalized User Representations for Cross-Domain Recommendation"
    },
    {
      "ref_id": "b16",
      "title": "Neural attentive session-based recommendation"
    },
    {
      "ref_id": "b17",
      "title": "Dual Attentive Sequential Learning for Cross-Domain Click-Through Rate Prediction"
    },
    {
      "ref_id": "b18",
      "title": "Ddtcdr: Deep dual transfer cross domain recommendation"
    },
    {
      "ref_id": "b19",
      "title": "Improving Graph Collaborative Filtering with Neighborhood-enriched Contrastive Learning"
    },
    {
      "ref_id": "b20",
      "title": "Megcf: Multimodal entity graph collaborative filtering for personalized recommendation"
    },
    {
      "ref_id": "b21",
      "title": "Cross Domain Recommendation via Bi-Directional Transfer Graph Collaborative Filtering Networks"
    },
    {
      "ref_id": "b22",
      "title": "Triple Sequence Learning for Cross-domain Recommendation"
    },
    {
      "ref_id": "b32",
      "title": "Factorizing personalized markov chains for next-basket recommendation"
    },
    {
      "ref_id": "b33",
      "title": "BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer"
    },
    {
      "ref_id": "b34",
      "title": "Parallel Split-Join Networks for Shared Account Cross-domain Sequential Recommendations"
    },
    {
      "ref_id": "b35",
      "title": "Personalized top-n sequential recommendation via convolutional sequence embedding"
    },
    {
      "ref_id": "b36",
      "title": "Visualizing data using t-SNE"
    },
    {
      "ref_id": "b37",
      "title": "Understanding contrastive representation learning through alignment and uniformity on the hypersphere"
    },
    {
      "ref_id": "b38",
      "title": "Low-dimensional Alignment for Cross-Domain Recommendation"
    },
    {
      "ref_id": "b39",
      "title": "A cross-domain hierarchical recurrent model for personalized session-based recommendations"
    },
    {
      "ref_id": "b40",
      "title": "An Adaptive Graph Pre-training Framework for Localized Collaborative Filtering"
    },
    {
      "ref_id": "b41",
      "title": "Contrastive learning for cold-start recommendation"
    },
    {
      "ref_id": "b42",
      "title": "Adapting Triplet Importance of Implicit Feedback for Personalized Recommendation"
    },
    {
      "ref_id": "b43",
      "title": "Multi-view Multi-behavior Contrastive Learning in Recommendation"
    },
    {
      "ref_id": "b44",
      "title": "Clear: Contrastive learning for sentence representation"
    },
    {
      "ref_id": "b45",
      "title": "Contrastive cross-domain recommendation in matching"
    },
    {
      "ref_id": "b46",
      "title": "Internal and Contextual Attention Network for Cold-start Multi-channel Matching in Recommendation"
    },
    {
      "ref_id": "b47",
      "title": "Contrastive learning for sequential recommendation"
    },
    {
      "ref_id": "b48",
      "title": "Decoupled side information fusion for sequential recommendation"
    },
    {
      "ref_id": "b49",
      "title": "Recurrent convolutional neural network for sequential recommendation"
    },
    {
      "ref_id": "b50",
      "title": "Learning Dual-view User Representations for Enhanced Sequential Recommendation"
    },
    {
      "ref_id": "b51",
      "title": "Are graph augmentations necessary? simple graph contrastive learning for recommendation"
    },
    {
      "ref_id": "b52",
      "title": "A survey on cross-domain recommendation: taxonomies, methods, and future directions"
    },
    {
      "ref_id": "b53",
      "title": "Causerec: Counterfactual user sequence synthesis for sequential recommendation"
    },
    {
      "ref_id": "b54",
      "title": "Cross-domain recommendation via preference propagation graphnet"
    },
    {
      "ref_id": "b55",
      "title": "Cross-domain recommendation via user interest alignment"
    },
    {
      "ref_id": "b56",
      "title": "Revisiting Alternative Experimental Settings for Evaluating Top-N Item Recommendation Algorithms"
    },
    {
      "ref_id": "b57",
      "title": "Recbole: Towards a unified, comprehensive and efficient framework for recommendation algorithms"
    },
    {
      "ref_id": "b58",
      "title": "DDGHM: Dual Dynamic Graph with Hybrid Metric Training for Cross-Domain Sequential Recommendation"
    },
    {
      "ref_id": "b59",
      "title": "Deep interest network for click-through rate prediction"
    },
    {
      "ref_id": "b60",
      "title": "S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization"
    },
    {
      "ref_id": "b61",
      "title": "C 2 -CRS: Coarse-to-Fine Contrastive Learning for Conversational Recommender System"
    },
    {
      "ref_id": "b62",
      "title": "Dtcdr: A framework for dual-target cross-domain recommendation"
    },
    {
      "ref_id": "b63",
      "title": "Transfer-meta framework for cross-domain recommendation to cold-start users"
    },
    {
      "ref_id": "b64",
      "title": "Personalized transfer of user preferences for cross-domain recommendation"
    }
  ]
}