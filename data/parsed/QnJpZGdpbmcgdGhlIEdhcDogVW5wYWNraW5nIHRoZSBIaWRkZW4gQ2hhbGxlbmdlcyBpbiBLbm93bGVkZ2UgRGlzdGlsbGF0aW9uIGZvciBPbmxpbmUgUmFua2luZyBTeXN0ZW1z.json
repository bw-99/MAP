{"Bridging the Gap: Unpacking the Hidden Challenges in Knowledge Distillation for Online Ranking Systems": "Nikhil Khani Google LLC USA nkhani@google.com Shuo Yang Google LLC USA yshuo@google.com Aniruddh Nath Google LLC USA aniruddhnath@google.com Yang Liu Google LLC USA ylyangliu@google.com Pendo Abbo Google LLC USA pabbo@google.com Li Wei Google LLC USA liwei@google.com Shawn Andrews Google LLC USA shawnandrews@google.com Maciej Kula Google DeepMind USA maciejkula@google.com Jarrod Kahn Google DeepMind USA jarrodk@google.com Zhe Zhao University of California, Davis USA zao@ucdavis.edu Lichan Hong Google DeepMind USA lichan@google.com Ed Chi Google DeepMind USA edchi@google.com", "ABSTRACT": "", "KEYWORDS": "Knowledge Distillation (KD) is a powerful approach for compressing a large model into a smaller, more efficient model, particularly beneficial for latency-sensitive applications like recommender systems. However, current KD research predominantly focuses on Computer Vision (CV) and NLP tasks, overlooking unique data characteristics and challenges inherent to recommender systems. This paper addresses these overlooked challenges, specifically: (1) mitigating data distribution shifts between teacher and student models, (2) efficiently identifying optimal teacher configurations within time and budgetary constraints, and (3) enabling computationally efficient and rapid sharing of teacher labels to support multiple students. We present a robust KD system developed and rigorously evaluated on multiple large-scale personalized video recommendation systems within Google. Our live experiment results demonstrate significant improvements in student model performance while ensuring consistent and reliable generation of highquality teacher labels from a continuous data stream of data.", "CCS CONCEPTS": "\u00b7 Information systems \u2192 Recommender systems ; Learning to rank ; \u00b7 Computing methodologies \u2192 Multi-task learning . Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). RecSys '24, October 14-18, 2024, Bari, Italy ACM ISBN 979-8-4007-0505-2/24/10 \u00a9 2024 Copyright held by the owner/author(s). https://doi.org/10.1145/3640457.3688055 Recommender Systems, Knowledge Distillation, Learning to Rank, Multitask Learning", "ACMReference Format:": "Nikhil Khani, Shuo Yang, Aniruddh Nath, Yang Liu, Pendo Abbo, Li Wei, Shawn Andrews, Maciej Kula, Jarrod Kahn, Zhe Zhao, Lichan Hong, and Ed Chi. 2024. Bridging the Gap: Unpacking the Hidden Challenges in Knowledge Distillation for Online Ranking Systems. In 18th ACM Conference on Recommender Systems (RecSys '24), October 14-18, 2024, Bari, Italy. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3640457.3688055", "1 INTRODUCTION": "Modern recommendation systems demand minimal latency. Even slight delays can negatively impact user experience, especially for large-scale video platforms serving billions of users 1 . While larger models improve accuracy [1, 3, 4], they also increase serving latency, presenting a critical speed and accuracy trade off. Knowledge Distillation (KD) offers a compelling solution by transferring knowledge from a complex \"teacher\" to a smaller \"student\" model [2, 6]. This process allows the student to achieve comparable performance to the teacher without additional latency. The most prevalent method of KD involves three main steps: (1) training a large teacher model on observed data (hard-labels), (2) using teacher to generate predictions on student's training data (soft-labels), (3) training the student on both soft and hard labels. This paper addresses three key challenges in deploying KD to realworld recommender systems: (1) mitigating data distribution shifts between teacher and student models using an online distillation framework with continuous teacher updates and a novel auxiliary 1 https://www.thinkwithgoogle.com/marketing-strategies/app-and-mobile/pageload-time-statistics/ RecSys '24, October 14-18, 2024, Bari, Italy Khani et al. task based distillation strategy that allows the student to ground its learning in teacher's knowledge without leaking teacher biases in the student; (2) navigating the costly and time-consuming process of identifying optimal teacher configurations, which can take months (Fig 1), by providing empirically-backed heuristics derived from real-world experiments; and (3) highlighting the overlooked infrastructure challenge of efficiently supporting multiple students distilling from a single teacher for cost amortization and practical deployment.", "2 CHALLENGES & DISTILLATION SETUP": "Our study examines multi-objective pointwise models for ranking videos within a large-scale recommendation system. These models predict short-term objectives like CTR (Click-Through Rate) and long term ones like E(LTV) , estimating the overall value a user drives on the platform over an extended horizon. AB experiment (Phase 3) Student starts (weeks to converge) Student continuously trained on teacher's predictions (Phase 2) Teacher convergence period (~months) Teacher is continuously trained (Phase Teacher starts Figure 1: A timeline of stages in KD setup Our teacher/student models share similar architectures (Fig. 2), beginning with input and embedding layers at the bottom, then multiple shared layers stacked together. The output from the last shared layer feeds into individual towers, producing final per-task prediction logits. Teacher Setup typically utilizes deeper and/or wider shared and task layers compared to the students. To address model divergence common in large models, we employ training stabilization techniques, including learning rate warmup[5], activation clipping[7], and Clippy optimizer[10]. Student Setup jointly trains on both hard (observed data) and soft labels (teacher predictions) by incorporating an additional distillation loss term. The most commonly used method of distillation, referred as 'direct distillation' (Fig 2) uses a single logit to minimize both hard and soft label losses. While effective in CV/NLP with static data distribution, this is suboptimal for recommender systems with rapidly changing data. As an example, we have E(LTV), a noisy and under-calibrated objective. While larger models with more learning capacity can predict LTV accurately, they continue to be undercalibrated [9]. Directly using these biased predictions from teachers as guiding examples in students leads to \"noise-compounding\" (Challenge #1) . This observation highlights that during distillation the teacher imparts not only its knowledge, but also its inherent biases to the student . Our auxiliary distillation strategy addresses this by using separate task logits for hard and soft label losses. This reduces the coupling between observed data and teacher labels, improving the student's ability to leverage teacher knowledge while avoiding bias. Table 1 shows a 0.4% reduction in E(LTV) loss using this technique. Auxiliary distillation has shown to be effective in multiple KD implementations within Google. Additionally, we also continuously train the teacher on new data to ensure up-to-date guidance for students. Experimentation Time: As illustrated in Fig. 1, distillation lifecycle involves months of continuously training teachers and additional weeks of student training and live experiments. This extensive process makes teacher development computationally expensive, especially for new teacher setups (Challenge #2) . To reduce this burden, we offer empirically-backed heuristics based on experimental results (Section 3), addressing questions such as: (1) What's an ideal teacher size to train? , (2) Should all objectives be distilled? , (3) If not, which ones to avoid? Figure 2: Direct vs Auxiliary distillation Direct Distillation Auxiliary Distillation Task 1 Task Task N Task N Hard Label Dislillation Label Hard Label Distillation Label Single Output Logit Logit Logit 2 Task Layer(s) Task N Layer(s) Top) Shared Layer Shared Layer(s Input features and Embeddings Amortizing Teacher Cost: A commonly overlooked infrastructure challenge in KD is the high cost of training and maintaining a teacher. Unlike CV/NLP where data stability allows for less frequent retraining, recommendation systems with dynamic user preferences and item catalogs require continuous model updates. To reduce this burden of continuously updating the large model we amortize its cost across multiple student models served in different contexts (Fig 3) allowing a single teacher model to improve a fleet of students. This requires efficient storage and sharing of teacher labels (Challenge #3) . Our proposed solution involves writing inferences from the trained teacher into a columnar database, prioritizing read performance over strict ACID properties. High data consistency is required to ensure students access the same soft-labels and have similar teacher label coverage. Doing all this on new data (teacher training, label writing, label dissemination) with minimal delay, consistently and reliably is a significant infrastructure challenge. While we use an internal version of BigQuery for our implementation, other columnar databases like Apache Cassandra would work equally well.", "3 LIVE EXPERIMENTS": "We evaluate our setup with various teacher configurations and distillation strategies, comparing their offline (AUC for classification, RMSE for regression) and online (engagement, satisfaction) Bridging the Gap: Unpacking the Hidden Challenges in Knowledge Distillation for Online Ranking Systems RecSys '24, October 14-18, 2024, Bari, Italy performance against a control model with similar architecture as the student, but trained on hard labels only (no-distillation). Various students distilling from a single teacher's predictions (soft labels) Teacher Prediction writing job Columnar Soft Storage Labels Student Student Student Student #2 #3 #k Figure 3: Fleet of students sharing a single teacher's labels Limiting Bias Leakage in Distillation: Table 1 shows the benefit of using auxiliary distillation on noisy tasks, or more generally, as a way to reduce teacher bias from leaking into students. Auxiliary distillation shows a 0.4% improvement in E(LTV) compared to direct distillation. In addition, students using direct distillation learn very little from the teacher, as evidenced by the fact that its offline E(LTV) performance is virtually identical to the no-distillation control. Table 1: Effect of distillation strategy on noisy objectives (lower rmse is better). Knowledge Distillation By Teacher/Student Size: Table 2 shows how student performance improves when distilling even from a relatively small teacher (2x the student size). The 4x teacher yields further gains (additional 0.43% engagement, 0.46% satisfaction). However, this scaling effect of teacher is not expected to continue indefinitely. As demonstrated by Mirzadeh et al. [8], excessively growing the teacher size eventually creates a large knowledge gap between teacher-student making it harder for the student to learn from teacher predictions and hindering the overall performance. Identifying the Objectives to Distill: Determining a priori which tasks benefit from KD in a multi-task setup is difficult. We investigate this by categorizing model objectives into (1) Primary Engagement Tasks (PET) (2) Primary Satisfaction Tasks (PST) and (3) Others. We train student models with a combinations of these objectives. Table 3 shows that distilling only PET objectives results in the lowest performance gains, aligning with our existing understanding that overemphasizing engagement can promote clickbaity recommendations and harm user satisfaction. Interestingly, distilling both PET and PST objectives led to the highest improvements, even surpassing distilling all objectives (PET+PST+Others). This suggests that while KD targets individual tasks, its effects can spread to Table 2: Student offline (CTR AUC) and online (LE metrics) performance (higher is better), as teacher size increases. All values compared to no-distillation control. Values in parenthesis indicate % change from the previous row and * denotes statsig with 95% CI shared model layers, potentially causing task conflicts. Thus, a selective approach to task distillation is crucial for optimizing student model performance in multi-task setting. Table 3: Effect of distilling various objectives on student quality.", "4 CONCLUSIONS AND FUTURE WORK": "In this paper, we addressed the unique challenges of implementing Knowledge Distillation (KD) in large-scale recommender systems, a domain often neglected in KD research. We introduced an online distillation framework with continuous teacher updates and a novel auxiliary task-based distillation strategy to mitigate data distribution shifts and bias leakage. Additionally, we presented empiricallyderived heuristics gleaned from real-world experiments. We recommend starting with a teacher model 2x the size of the student, so it can train and converge faster, without being so over-parameterized as to increase the knowledge gap between teacher and student. And, while the ideal set of distilled objectives is context-dependent, we recommend prioritizing primary engagement and satisfaction tasks, to reduce task conflict during distillation. Removing these tasks from teacher training altogether can further enhance the teacher's accuracy on PET and PST. Future work in this area will focus on optimizing the latency of teacher label propagation, efficiently training larger model sizes and increasing the breadth of the teacher by including data and objectives from from multiple surfaces while incorporating domain generalization techniques.", "5 SPEAKER BIO": "Nikhil Khani is a Senior Software Engineer at YouTube (Google), where he works on improving YouTube's Homepage Ranking. Li Wei is a Senior Staff Engineer at YouTube (Google), working on the WatchNext Team. RecSys '24, October 14-18, 2024, Bari, Italy Khani et al.", "REFERENCES": "[1] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems . 7-10. [3] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks for youtube recommendations. In Proceedings of the 10th ACM conference on recommender systems . 191-198. [2] Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. 2017. A survey of model compression and acceleration for deep neural networks. arXiv 2017. arXiv preprint arXiv:1710.09282 (2017). [4] Carlos A Gomez-Uribe and Neil Hunt. 2015. The netflix recommender system: Algorithms, business value, and innovation. ACM Transactions on Management Information Systems (TMIS) 6, 4 (2015), 1-19. [5] Priya Goyal, Piotr Doll\u00e1r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. 2017. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677 (2017). [7] Alex Krizhevsky and Geoff Hinton. 2010. Convolutional deep belief networks on cifar-10. Unpublished manuscript 40, 7 (2010), 1-9. [6] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 (2015). [8] Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, and Hassan Ghasemzadeh. 2020. Improved knowledge distillation via teacher assistant. In Proceedings of the AAAI conference on artificial intelligence , Vol. 34. 5191-5198. [10] Jiaxi Tang, Yoel Drori, Daryl Chang, Maheswaran Sathiamoorthy, Justin Gilmer, Li Wei, Xinyang Yi, Lichan Hong, and Ed H Chi. 2023. Improving Training Stability for Multitask Ranking Models in Recommender Systems. arXiv preprint arXiv:2302.09178 (2023). [9] Harald Steck. 2018. Calibrated recommendations. In Proceedings of the 12th ACM conference on recommender systems . 154-162."}
