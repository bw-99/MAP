{
  "MetaSplit: Meta-Split Network for Limited-Stock Product Recommendation": "Wenhao Wu âˆ—",
  "Jialiang Zhou âˆ—": "wwh137@stu.xjtu.edu.cn Xi'an Jiaotong University Xi'an, China Ailong He along.hal@alibaba-inc.com Alibaba Group Hangzhou, China",
  "B": "Shuguang Han shuguang.sh@alibaba-inc.com Alibaba Group Hangzhou, China zhoujialiang.zjl@alibaba-inc.com Alibaba Group Hangzhou, China",
  "Jufeng Chen": "jufeng.cjf@alibaba-inc.com Alibaba Group Hangzhou, China",
  "ABSTRACT": "",
  "Bo Zheng": "bozheng@alibaba-inc.com Alibaba Group Hangzhou, China",
  "CCS CONCEPTS": "Compared to business-to-consumer (B2C) e-commerce systems, consumer-to-consumer (C2C) e-commerce platforms usually encounter the limited-stock problem, that is, a product can only be sold one time in a C2C system. This poses several unique challenges for click-through rate (CTR) prediction. Due to limited user interactions for each product (i.e. item), the corresponding item embedding in the CTR model may not easily converge. This makes the conventional sequence modeling based approaches cannot effectively utilize user history information since historical user behaviors contain a mixture of items with different volume of stocks. Particularly, the attention mechanism in a sequence model tends to assign higher score to products with more accumulated user interactions, making limited-stock products being ignored and contribute less to the final output. To this end, we propose the Meta-Split Network (MSNet) to split user history sequence regarding to the volume of stock for each product, and adopt differentiated modeling approaches for different sequences. As for the limited-stock products, a meta-learning approach is applied to address the problem of inconvergence, which is achieved by designing meta scaling and shifting networks with ID and side information. In addition, traditional approach can hardly update item embedding once the product is consumed. Thereby, we propose an auxiliary loss that makes the parameters updatable even when the product is no longer in distribution. To the best of our knowledge, this is the first solution addressing the recommendation of limited-stock product. Experimental results on the production dataset and online A/B testing demonstrate the effectiveness of our proposed method. âˆ— Wenhao Wu and Jialiang Zhou contributed equally to this research. This work was done when Wenhao Wu was a research intern at Alibaba Group. Shuguang Han is the corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WWW'24 Companion, May 13-17, 2024, Singapore, Singapore Â© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0172-6/24/05...$15.00 https://doi.org/10.1145/3589335.3648319 Â· Information systems â†’ Information systems applications .",
  "KEYWORDS": "Recommendation System, Click-through Rate Prediction, Meta Learning, Limited-Stock Product Recommendation",
  "ACMReference Format:": "Wenhao Wu, Jialiang Zhou, Ailong He, Shuguang Han B , Jufeng Chen, and Bo Zheng. 2024. MetaSplit: Meta-Split Network for Limited-Stock Product Recommendation. In Companion Proceedings of the ACM Web Conference 2024 (WWW '24 Companion), May 13-17, 2024, Singapore, Singapore. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3589335.3648319",
  "1 INTRODUCTION": "In today's digital era, recommendation systems are crucial in various domains like e-commerce, streaming services, and social media platforms [2, 6, 34, 42]. These systems utilize user data to learn user interests, estimate preferences from collected user interactions, and provide personalized suggestions [1, 27, 37, 38, 40], thereby enhancing user experience and driving sales. An accurate click-through rate (CTR) prediction model is vital in achieving this goal. In recent years [11, 20, 25], industrial click-through rate prediction models have shifted from traditional shallow models [19, 26] to deep learning models [28, 29, 33]. The deep CTR prediction model generally follows an Embedding and MLP architecture [5]. The embedding layer transforms each discrete feature from raw input into a low-dimensional vector. The deep model can take various forms, such as models that learn high-order interactions or take into account historical behavior sequences. By utilizing deep models, these systems can effectively capture complex patterns and relationships in user data, leading to more accurate predictions of user engagement. These methods have demonstrated state-of-the-art performance across a wide range of tasks [8, 43]. The interaction between products and consumers can go beyond the traditional Business-to-Consumer (B2C) model, and the Consumer-to-Consumer (C2C) model has emerged in recent years. While B2C platforms involve transactions between businesses and individual consumers, C2C platforms facilitate transactions among individual consumers themselves. As China's largest online C2C platform, Xianyu allows both individuals and small professional WWW'24 Companion, May 13-17, 2024, Singapore, Singapore Wenhao Wu et al. Figure 1: An illustrative example highlighting the distinctions between B2C platforms and C2C platforms. Recommender at B2C Platform Professional Provide with Multiple stocks Purchase Click Explicit Relations User User Recommender at C2C Platform Seller (Also User) Provide with only one stock Unavailable for future Purchase exposure and click Implicit Relations User User Shop shop owners to sell their products. This paper conducts all its experiments within the Xianyu platform, leveraging its unique C2C dynamics for insightful analysis. The products on B2C platforms are mainly from professional merchants and these products always have a wide range of stock available. These merchants operate as businesses and specialize in selling products directly to consumers. Meanwhile, the products on C2C platforms are mainly posted by individual users, which often have limited stock availability (e.g., a second-hand baby stroller with only one stock). The main characteristic of limited-stock products is that once sold, they are no longer available on the platform. The distinction between B2C and C2C can be simply visualized as the example shown in Fig. 1. However, it is important to note that although C2C platforms mainly consist of limited-stock products, there are also products with multiple-stocks provided by professional merchants. There are two major challenges arising from the recommendation system on C2C platform. Â· The existence of limited-stock products makes the interaction between items and consumers much more sparse, which usually causes slower convergence rates or even failure in learning unique item ID embedding during model training. It has been widely known in the industry that a well-trained ID embedding can largely improve the recommendation performance [39]. Consequently, the problem of limited-stock products poses a challenge in accurately estimating the clickthrough rate (CTR), and subsequently affects the precision of CTR prediction, especially for products with limited stocks availability. Â· The coexistence of these two types of items in the user historical behavior sequence complicates the user interest modeling process. In particular, attention mechanisms, commonly used in sequence modeling, may overlook the limited-stock product due to the behavior sparsity. Compared to limitedstock items, multiple-stock items usually have much more user interactions. This can cause the result recommendation to be biased towards multi-stock items. Thus, greatly affecting the overall modeling effectiveness. A common solution to address the above problems is to treat the limited-stock products as new items using item cold-start methods. However, this cannot fully address this issue, and the different volumes of product is being neglected. To address these limitations, this paper proposes a novel approach called the Meta-Split Network (MSNet), which splits the user history sequence based on the volume of stock for each product and employs different modeling methods for different sequences. For multi-stock products, we apply the traditional Deep Interest Model [42]; and for limited-stock products, we introduce a metalearning approach to handle the problem of model convergence. Specifically, we design meta-scaling and shifting networks with both ID and side information (such as category) to better utilize item information. In addition, the traditional approach can hardly update item embedding once the product is consumed. Thereby, we propose an auxiliary loss to facilitate parameter updates even when the product is no longer in distribution. By incorporating this auxiliary loss, we can continue to refine and adjust the embedding even after the product has been consumed. This approach allows for learning a better item representation and helps a better understanding of products even if they are longer available for recommendation. It enables the model to capture more accurate and up-to-date information about the products, resulting in an improved CTR prediction performance. To evaluate the effectiveness of our proposed method, we conducted experiments on a production dataset and performed online A/B testing. The results demonstrate the superiority of our approach over existing methods in accurately predicting CTR for overall products and limited-stock products items. We believe that this is the first solution to effectively address the recommendation of limited-stock items in e-commerce settings, opening up new possibilities for further improvement. To summarize, the main contributions of our work are as follows: Â· To the best of our knowledge, this is the first solution to deal with the recommendation of limited-stock products in C2C e-commerce platforms. We reveal that there are key components for addressing this issue, non-convergence embedding representation for limited-stock products and the mixed user behavior sequences containing both multiple-stocks products and limited-stock products. We believe this can open up new possibilities for further improvement. Â· WeproposeanovelmethodnamedMeta-Split Network(MSNet) to address the challenges faced by consumer-to-consumer (C2C) e-commerce platforms, specifically in the context of limited-stock products. MSNet adopt differentiated modeling approaches for user history sequences based on the volume MetaSplit: Meta-Split Network for Limited-Stock Product Recommendation WWW'24 Companion, May 13-17, 2024, Singapore, Singapore of product stock. For limited-stock products, a meta-learning approach is employed to overcome issues of in-convergence, utilizing meta scaling and shifting networks with ID and side information. Additionally, the paper introduces an auxiliary loss to update item embedding even after the product is consumed. Â· Experimental results on industrial production datasets and online A/B testing demonstrate the superiority of MSNet over existing methods in accurately predicting CTR for overall products and limited-stock products items.",
  "2 RELATED WORK": "This section briefly introduces the progress in the field of cold-start item recommendation and meta-learning for recommendation.",
  "2.1 Cold-Start Recommendation": "Recommendations for new items, or the cold-start item recommendation problem, can be challenging due to the limited interactions between users and items [33]. Cold-start item recommendation is typically divided into two stages by the amount of user interactions. In the first stage, the new items have just entered into the system and thus received very limited exposure. At this moment, any existing recommendation models may fail because of the minimal user interactions. To avoid direct competition with existing items, industrial recommendation systems may offer a separate channel only for new items. In this case, high-quality new items only compete with each other, and they can warm up and enter the next stage after obtaining a certain level of interaction. After that, one can calculate simple real-time statistical information or incorporate a limited amount of user and item features for recommendation. The common approaches [21, 23, 32, 35, 36] utilize generalization features or explore information across different domains and modalities. The most effective algorithms, of course, are usually the ones that involve introducing additional user-item interaction data from other domains [12, 38]. In the second stage, new items may receive a few interactions from the separate channel, And now, they compete directly with the existing items. ClassBalance [7] and LogQ [22] attempted to enhance the importance of new items during the training process by adjusting the weights of the samples or their corresponding network parameters. DropoutNet [31] and POSO [9] enhanced the expressive ability and robustness of models for new items by designing specialized networks tailored to their characteristics. There are indeed many other research studies on this topic, we do not list them all here due to the content limit.",
  "2.2 Meta Learning for Recommendation": "Meta learning enables new tasks to be learned on a small number of samples by learning the general or auxiliary knowledge. Methods like MetaEmb [24] and MWUF [44] leverage additional information from items and users to generate or adjust item ID embedding. Techniques such as model-agnostic meta learning (MAML) [10] and metric-based meta learning have been applied to learn transferable representations or optimize the learning process across different recommendation scenarios. Figure 2: An illustration of Embedding and MLP structure with sequntial information modeling for the deep CTR prediction model Output: Predicted CTR MLP Layers & Sigmoid Concat & Flatten Attention Mechanism Concat Concat Concat Concat Concat Concat Embedding Layer 8888 8888 666 666 Item Item 2 Item N Context User Item Feature Fields Feature Fields Feature Fields User Behavior Sequence Input: User & Item Pair",
  "3 PRELIMINARIES": "In this section, we formulate the problem and briefly introduce the existing production CTR prediction model on the large-scale recommender system of Xianyu.",
  "3.1 Problem Formulation": "Given a dataset D = { x , ğ‘¦ } ğ‘ , ( x , ğ‘¦ ) marks a sample and ğ‘ is the number of samples, where x denotes the high dimensional feature vector consisting of multi-fields (e.g., user and item field), and ğ‘¦ is the binary label with ğ‘¦ = 1 indicating the sample is clicked. Our task is to accurately predict the probability of CTR ğ‘ ğ‘ğ‘¡ğ‘Ÿ = ğ‘ ( ğ‘¦ = 1 | ğ‘¥ ) for the testing sample ğ‘¥ .",
  "3.2 Production CTR Prediction Model": "We select Deep Interest Network [42] as our base model due to its online efficiency and effectiveness, which follows the conventional Embedding and MLP paradigm and utilizes an attention mechanism to model user behavior sequences, as depicted in Figure 2. Embedding Layer : The inputs are composed of non-sequential features (e.g., user ID) and sequential features (e.g., user's history clicked items). The embedding layer is employed to convert each discrete feature from the raw input into a vector of lower dimensions, by using an embedding look-up table. The embedding of non-sequential features is simply concatenated, whereas for the embedding of sequential features, a sequence information modeling module is used to assemble them into a fixed-size representation. Sequence Information Modeling : Deep Interest Network(DIN) utilizes a local attention mechanism to dynamically capture the user's interests based on the similarity between their historical clicked items and the target item. This mechanism allows for the acquisition of a personalized representation of the user's interests WWW'24 Companion, May 13-17, 2024, Singapore, Singapore Wenhao Wu et al. Table 1: Analysis of average attention scores (before softmax function) in DIN Module, where \"M/s\" is short for \"multiplestocks\" and \"L/s\" is short for \"limited-stock\". since it enables weight pooling of the information in the sequence of varying length. Additionally, DIN could be further optimized by leveraging the target attention mechanism equations [4, 16, 30] to replace the original local attention mechanism. In the existing Multi-head Target-attention (MHTA) implementation, the target item ğ¼ğ‘¡ğ‘’ğ‘š ğ‘¡ is consider as query ( ğ¾ ) and the history click sequence ğ‘º ğ‘¢ is considered both as keys ( ğ¾ ) and values ( ğ‘‰ ) , where ğ‘º ğ‘¢ = { ğ¼ğ‘¡ğ‘’ğ‘š 1 , ğ¼ ğ‘¡ğ‘’ğ‘š 2 , . . . , ğ¼ ğ‘¡ğ‘’ğ‘š ğ» } is the set of embedding vectors of items in the user behaviors with length of ğ» . Specifically, the output of MHTA can be formalized as follows :  where ğ‘„ = ğ‘Š ğ‘„ ğ¼ğ‘¡ğ‘’ğ‘š ğ‘¡ , ğ¾ = ğ‘Š ğ¾ ğ‘‹ seq , and ğ‘‰ = ğ‘Š ğ‘‰ ğ‘‹ seq , the linear projections matrices ğ‘Š ğ‘„ âˆˆ R ğ‘‘ Ã— ğ‘‘ , ğ‘Š ğ¾ âˆˆ R ğ‘‘ Ã— ğ‘‘ , ğ‘Š ğ‘‰ âˆˆ R ğ‘‘ Ã— ğ‘‘ are learn-able parameters and ğ‘‘ stands for the dimension of hidden space. The temperature âˆš ğ‘‘ is introduced to produce a softer attention distribution for avoiding extremely small gradients. Finally, all non-sequential embedding and transformed user sequential embedding is concatenated with other continuous features together to generate the overall embedding and pass through a deep network to get the final prediction. Loss : The objective function used in DIN and our proposed MSNet is both the negative log-likelihood function defined as:  where ğ· is the training set, each sample ğ‘¥ is associated with a ground-truth label ğ‘¦ . The output of the model, denoted as ğ‘“ ( ğ‘¥ ) , represents the predicted probability of sample ğ‘¥ being clicked.",
  "4 METHODOLOGY": "This chapter outlines the comprehensive design of our method, including the architecture overview of our proposed method, MetaSplit Network (MSNet), and the detailed structure.",
  "4.1 Architecture Overview": "Ignoring the fact that limited-stock product have insufficient embedding representation and not distinguish limited-stock product in user sequence will leads to inferior model performance. To this end, we propose the Meta Split Network (MSNet) for CTR prediction to better utilize limited-stock items by adopting differentiated modeling approaches for user history behavior sequences. As shown in Figure 3, MSNet consists of three main components: (1) the sequence split module (2) the sequence meta learning network (3) the auxiliary loss. MSNet first split user behavior history sequence based on the volume of product stock. For multi-stock products, we apply the traditional Deep Interest Model, and for limited-stock products, we introduce a meta-learning approach to handle the problem of model convergence. The meta learning module involves employing meta scaling and shifting networks to enhance ID and side information mutually. Furthermore, MSNet use an auxiliary loss that updates item embedding even after the product has been consumed. This ensures that the model continues to learn and adapt even when products are no longer available. Afterwards, the attention mechanism receives the enhanced user sequential embedding and process them into transformed user behavior interest. The user behavior interest are further concatenate with the non-sequential features and passed to the deep network for CTR prediction.",
  "4.2 Detail Structures": "In this section, we will introduce the components of our proposed method in detail. 4.2.1 Sequence Split Module . The user behavior sequence includes the items they have clicked with, including item ID and side information like category and genre. Concretely, representation of a single item and the whole sequence is given as: The representation of a single item and the whole sequence can be written in the following way:   where ğ‘¬ Item , ğ¸ (F ğ¼ ğ· ) , ğ¸ (F ğ‘†ğ‘–ğ‘‘ğ‘’ ) , ğ‘¬ Seq means the embedding for item, id feature, side feature and sequence feature respectively. We split the user behavior sequence based on item type (deep stock or limited stock) and this will further facilitates the subsequent differential modeling of the two sequences to prevent highfrequency signals from overriding low-frequency signals. where ğ‘¬ B_Seq , ğ‘¬ C_Seq means the embedding for b seq and c seq respectively. During the actual execution, this sequence split process can be achieved in the training samples construction process, or by using a mask mechanism during training. Our threshold for distinguishing between multiple-stocks and limited-stock items is set at one item: products with a volume greater than one are categorized as multiple-stocks, and a product with only one item is identified as limited-stock. 4.2.2 Sequence Meta Learning Module . Inspired by the concept of MWUF [44], which implements a meta scaling and shifting network to generate embedding representation for new items through existing embedding representation of old items, we propose sequence meta learning module. We have made modifications to its usage and applied it in a sequential manner, while maintaining the names of the two models. Webegin with the equation of attention mechanism, the equation is listed below:  where Q is the target to match others, K is the Key to be matched, and V is the value to be multiplied after matching. In the context MetaSplit: Meta-Split Network for Limited-Stock Product Recommendation WWW'24 Companion, May 13-17, 2024, Singapore, Singapore Figure 3: The schematic framework of MSNet can be broadly divided into three components: the sequence split module, the sequence meta-learning module, and the auxiliary loss. Figure (a) provides an overview of the model, with the sequence split module depicted at the bottom. Figure (b) illustrates the sequence meta-learning module, while Figure (c) offers detailed insights into the auxiliary loss. Output: Predicted CTR (a) Model Overview (b) Sequence Meta Learning MLP Layers & Sigmoid Target Item History Click Features Sequence Features Concat & Flatten Linear Single Item New K & V Query Mcta Scalng Query Attention Mechanism Attention Mechanism Sequence Meta Learning Auxiliary Loss Concat Concat Concat Concat Concat Concat Linear Linear Item ID Embedding Concat Concat Concat Side Info Embedding Gradient Inner Product & Softmax Matmul Output Embedding Layer 8888 8888 666 666 666 666 (c) Auxiliary Loss Item ID Item Item 2 Item N Item Item 2 Item N , Item Similarity Embedding Context User Item with multiple stocks Item with limited stocks Feature Fields Feature Fields Feature Fields Side Info Sequence Split Embedding Behavior Sequence Side Info Similarity Input: User & Item Pair Add MSE between the two similarity to final loss Stop User of CTR prediction, Q represents the item that needs to be inferred, while both K and V represent the user behavior sequence. by the following equation As we can see, Q and K are used for similarity matching, while V is utilized for the final output. Our objective is to achieve a more equitable matching between Q and K, where the side information with higher generalization capability plays a more vital role, especially when the ID embedding are not converged and lack sufficient training. Simultaneously, we aim to enhance the information contained in the returned V. To accomplish this, we scale the side information of Q and K based on the sequence of Item IDs and generate a meta ID using the side information from the product sequence. Subsequently, this meta ID is employed to shift the information in V. Meta Scaling Network : We use ID feature to generate weight for each feature field of side information. Specifically the scaling weight is generate by the following equation  where ğ‘¬ (S ğ¼ ğ· ) means the ID embedding and âŠ˜ means stop gradient. we scale the side feature embedding accordingly by a field-wise product manner  Meta Shifting Network : By enhancing the ID information, we are able to better capture the unique characteristics and features of each limited-stock item. Specifically the scaling weight is generate  where ğ‘¬ (S ğ‘†ğ‘–ğ‘‘ğ‘’ ) is the side information embedding of the sequence feature. We want to make more use of meta ID information when the original ID is relatively weak, and less use of meta ID information when the original ID is relatively strong. Thus we add the meta ID embedding with the origin ID embedding w.r.t the norm ratio   where ğ‘£ is the embedding norm ratio for the ğ‘¬ (S ğ¼ ğ· ) . The final representation of a item is:   The scaling network aims to identify the importance of side information using ID information, while the shifting network aims to enhance the ID information using side information. Therefore, both of these networks can be classified as meta-learning networks. 4.2.3 Auxiliary Loss . In the original attention mechanism, the items in the sequence are aggregated into a single vector through weighted pooling and passed into the DNN network. For a single item in the sequence, the gradient returned during back-propagation depend on the values passed during aggregation. As a result, items WWW'24 Companion, May 13-17, 2024, Singapore, Singapore Wenhao Wu et al. that contribute more significantly will have larger gradient updates, while items with lower contributions will have smaller gradient updates. This can lead to items in the sequence that are ignored due to insufficient training of their item id embeddings being continuously ignored and not receiving significant updates. The introduction of an auxiliary loss is based on the assumption that items with similar categories should have similar ID embeddings. To implement this, we first calculate the cosine similarity between the sequence of items and the target item's category, as well as the cosine similarity between the sequence of items and the target item's ID. We then compute the mean squared error (MSE) between these two similarities and incorporate it into the final loss.   To facilitate the updating of item id embedding for limited-stock items in the sequence, we can prevent the already trained side information embedding from being misleading. One approach is to apply the stop gradient operation to the similarity of the side information. This ensures that the gradients do not flow back to the side information embedding during the training process, allowing the focus to be primarily on updating the item id embedding.   Please note that our model mainly focuses on the Embedding module in the input layer of the DNN, which can be applied to various DNN networks and sequence information modeling method using the attention mechanism.",
  "5 EXPERIMENTS": "In this section, we conduct extensive experiments on the offline dataset and online A/B testing to evaluate the performance of the proposed method. The following research questions (RQs) are addressed: Â· RQ1 : Does your proposed method outperform state-of-theart recommender? (Section 5.2) Â· RQ2 : Does your proposed method address the issue of limitedstock product in CTR prediction? (Section 5.2) Â· RQ3 : How do critical components affect the performance of your proposed method? (Section 5.3) Â· RQ4 : Does your proposed method work in real large-scale online recommendation scenarios? (Section 5.4)",
  "5.1 Experiment Setup": "5.1.1 Xianyu Dataset . We collect click traffic logs of 8 days from Alibaba's Xianyu Recommendation Platform 1 to build the production data-set with 10 billion samples (1.5 billion sample per day), with numbers of items and people, 209 features (e.g., user and item features). The samples in the first 7 days and 8th day are employed 1 To the best of our knowledge, there are no suitable public data-set for CTR prediction under the isolate item problem. for training and testing respectively, and we randomly partition the testing set into 10 parts and report average evaluation results. 5.1.2 Baselines . We compare the proposed MSNet with four types of baselines: the first type are common DNN-based methods, including: Â· DNN : a method proposed by Youtube, and has been been implemented in various industrial applications. Â· Wide & Deep [5]: a method proposed by Google that consists of a wide model and a deep model to and has gained significant adoption in various applications. Â· DeepFM [15]: a method proposed by Huawei Noah lab that that combines deep learning and factorization machine techniques. Â· DIN [42]: our base model, as described in Section 4, which uses attention mechanism to aggregate historical behaviors sequence information based on the similarity between historical behaviors item and the target item. The second type are meta learning methods Â· GroupID [13] a meta-learning approach proposed by Airbnb to generate new item embeddings by calculating the average of similar item embeddings. Â· Meta-E [24]: a meta-learning approach to address the coldstart problem by generating ID embeddings for cold items using other available features. Â· MWUF [44]: a meta-learning method to address the coldstard problem, which consists of two parts. The Scaling Network can produce a scaling function to transform the cold ID embedding into warm feature space, while the Shifting Network is able to produce a shifting function that can produce stable embedding from the noisy embedding. The third type are some gate network method, because our method only boosts the embedding part, we only implement the embedding gate for following methods. Â· GateNet [17]: a method proposed by Baidu, using gate layer to generate weight for the embedding layer and the hidden layer. Â· FiBiNet [18]: a method proposed by Sinanet, filtering the information of embedding based on their importance using the SENet module. Â· POSO [9]: a method proposed by KuaiShou, a short-video application. We implement it as described in the paper with personal MLP and personal MHA. As for the fourth type of baselines, we implement two label de-bias approaches Â· Log-Q [22]: it introduces an item frequency-based factor to modify the softmax logit within the loss function. This modification allows for differential weighting of head and tail items during the learning process. Â· ClassRebalance [7]: a method to address class imbalance, which computes the effective number of samples for each imbalanced class and adopts it as a re-weighting factor within the loss function. 5.1.3 Evaluation Metric . Following previous works [14, 28, 41, 42], we adopt some widely used metrics, AUC, GAUC and RelaImpr, MetaSplit: Meta-Split Network for Limited-Stock Product Recommendation WWW'24 Companion, May 13-17, 2024, Singapore, Singapore Table 2: Model comparison on the production dataset. All the lines calculate RelaImpr by comparing with the performance of DIN on different set of products respectively. The best performance for each metric is bold-faced. The improvements are statistically significant (i.e. two-side t-test with ğ‘ < 0 . 05 ) over the original model, where \"RI\" is short for \"RelaImpr\", and \"L/s\" is short for \"limited-stock\". Table 3: Ablation study of different MSNet variants on production datasets, where \"w/o\" is short for \"without\". Table 4: Comparison between production model and MSNet in online A/B tests, where \"L/s\" is short for \"limited-stock\", and \"M/s\" is short for \"multiple-stock\". We followed industry standards and best practices for online A/B testing, ensuring reliability and validity through randomized assignment and control of external factors that could bias the results. to evaluate the performance of our proposed method. The calculation of AUC, GAUC, RelaImpr can be expressed as follow:  where ğ‘ƒ and ğ‘ denote positive sample set and negative sample set, respectively. Î˜ is the estimator function and ğ¼ is the indicator function.  where the AUC is first calculated within samples of each user, and averaged w.r.t sample count, where ğ‘› is the number of users, #impression ğ‘– and AUC ğ‘– are the number of impressions and AUC corresponding to the ğ‘– -th user.  where RelaImpr metric to measure relative improvement over models and 0.5 stands for the AUC of a random guesser. In order to mitigate the interference caused by the distribution of the prediction dataset, we randomly partition the users into ten groups. We calculate the AUC for each group separately and then compute the average and standard deviation of these ten AUC metrics, which are represented as avg(AUC) and std(AUC). Furthermore, to assess the accuracy of the absolute values of the model scores, we utilize the Cal-N metric to represent the calibrated error between the model scores and the predicted probabilities of the label in the prediction dataset, which is calculated as follows:    The PCOC (Predict Click Over Click) metric is calculated over each sub-dataset split by N partitions and then the error of each WWW'24 Companion, May 13-17, 2024, Singapore, Singapore Wenhao Wu et al. sub-dataset is calculated by [2]. Finally, the calibration-N metric can be obtained by calculating the standard deviation of N errors. A smaller Cal-N indicates higher accuracy in the absolute values of the model scores. In general, for AUC and GAUC, the higher the value, the better the performance. For PCOC, the closer to 1, the better the performance. For Calib-N, the smaller the value, the better the performance. 5.1.4 Implementation Details. We implement the DNN part of DeepFM, Wide & Deep, DNN, DIN all the same architecture, i.e., a three-layer MLP Network with 512, 256 and 128 hidden units. For all attention layers in above models, the number of hidden units are all set to 128. AdagradDecay optimizer is adopted in all the methods, the learning rate of 1e-4 is set. Batch size are set to 4096 for all DNNs. And the histoty click sequence are collected with last 30days and max length are 50.",
  "5.2 RQ1 & RQ2: Overall Comparison with Baselines": "Weconduct an evaluation of different approaches on the production dataset for CTR prediction tasks and the experimental results are presented in Table 2. Based on the information provided in the table, we have the following observations: Â· Our method outperformed all other methods in terms of overall item metrics. Specifically, compared to the base model DIN, our approach achieved a 1.05% improvement on AUC and a 1.93% increase on GAUC, which answers our research question RQ1. 2 Â· Our method also achieved the highest AUC metric for certain items, except for some precision-related metrics. It effectively mitigated the issues related to limited-stock items in our scenario. Specifically, compared to the base model DIN, our approach resulted in a 1.47% improvement in AUC for new items, a decrease in pcoc from 1.1037 to 1.0558, and a decrease in calib-n from 0.01057 to 0.00274. For type C items, our method showed a 1.01% improvement in AUC, a decrease in pcoc from 1.1037 to 1.0558, and a decrease in calib-n from 0.01057 to 0.00274. These results address our research question RQ2. Â· Other methods were not able to effectively address our problem, although some methods showed improvements compared to the base model. In the context of meta learning methods, MELU and MWUF outperformed GroupID. This could be because GroupID replaces the item ID of new items with the ID of popular items in the same category, which may not accurately capture the fine-grained similarities between different items. On the other hand, MELU and MWUF generate item ID information based on other characteristics of the items, leading to better performance. However, these methods do not consider factors such as inventory and sequence information, resulting in less significant improvements compared to our method. Similarly, the debias method showed improvements by modifying the loss function.As for the 2 Note that the 0.1% AUC gain is already considerable in large-scale industrial recommendation system [3], especially when we didn't add additional information gate method, simply increasing the parameters of the gate control network can further bias the already biased DNN network, making it unable to bring any improvement. In the future, when we deploy our system, we will also consider integrating algorithms that can provide benefits.",
  "5.3 RQ3: Ablation Study": "To further validate the effectiveness of the sub-modules proposed in the MSNet model, we compare the offline performance of models without the sequence split module, without the seq meta learning module, without auxiliary loss, without all modules, and the complete model, as shown in Table 3. The results clearly demonstrate the effectiveness of our proposed MSNet, highlighting the necessity of sequence splitting, enhancing sequence signals, through sequence meta learning, and addtional auxiliary loss in updating sequence item IDs, respectively.",
  "5.4 RQ4: Online A/B test": "In Xianyu's recommendation system, we compared a new model's effectiveness against a highly-optimized baseline (DIN architecture) through an online A/B test. Users were randomly assigned to control and experimental groups, with device IDs uniformly distributed via MD5 hashing for impartial partitioning. The 10-day experiment period provided statistically validated, reliable results.We used CTR (Click-Through Rate), Clicks, and exposure ratio as metrics to evaluate the online effectiveness. The results are shown in Table 4. The MSNet model yielded a notable improvement across key metrics compared to the base model, with a 3.56% increase in CTR and a 3.31% increase in Clicks. Particularly significant were the enhancements observed in limited-stock products, with a 3.72% increase in CTR, a 5.20% increase in Clicks, and a 1.0% absolute gain in exposure ratio. Regarding time complexity, MSNet architecture allows parallel processing with minimal computational cost. The new introduced meta-DNN, utilizing a small MLP network, requires limited resources. In our online environment, MSNet's implementation increases response time by less than 5%, acceptable for our recommendation system.",
  "6 CONCLUSION": "In this study, we present the Meta-Split Network (MSNet) for enhancing click-through rate (CTR) predictions in C2C e-commerce, especially for limited-stock products. Traditional models falter with limited-stock items due to sparse data affecting item embeddings. MSNet tackles this by segmenting user history based on stock levels and applying a unique meta-learning approach for low-stock items. This method, supported by meta scaling and shifting networks, optimizes performance. Additionally, an auxiliary loss component updates item embeddings beyond consumption. Demonstrated effective through experiments and online A/B testing, MSNet emerges as a pioneering solution for C2C limited-stock recommendations. It offers valuable insights and approaches for similar challenges in e-commerce recommendation systems. WWW'24 Companion, May 13-17, 2024, Singapore, Singapore MetaSplit: Meta-Split Network for Limited-Stock Product Recommendation",
  "REFERENCES": "[1] Yi Cao, Sihao Hu, Yu Gong, Zhao Li, Yazheng Yang, Qingwen Liu, and Shouling Ji. 2022. GIFT: Graph-GuIded Feature Transfer for Cold-Start Video ClickThrough Rate Prediction. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management (Atlanta, GA, USA) (CIKM '22) . Association for Computing Machinery, New York, NY, USA, 2964-2973. https://doi.org/10.1145/3511808.3557120 [2] Zhangming Chan, Yu Zhang, Shuguang Han, Yong Bai, Xiang-Rong Sheng, Siyuan Lou, Jiacen Hu, Baolin Liu, Yuning Jiang, Jian Xu, and Bo Zheng. 2023. Capturing Conversion Rate Fluctuation during Sales Promotions: A Novel Historical Data Reuse Approach. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (Long Beach, CA, USA) (KDD '23) . Association for Computing Machinery, New York, NY, USA, 3774-3784. https://doi.org/10.1145/ 3580305.3599788 [3] Jianxin Chang, Chenbin Zhang, Yiqun Hui, Dewei Leng, Yanan Niu, Yang Song, and Kun Gai. 2023. PEPNet: Parameter and Embedding Personalized Network for Infusing with Personalized Prior Information. arXiv:2302.01115 [cs.IR] [4] Qiwei Chen, Huan Zhao, Wei Li, Pipei Huang, and Wenwu Ou. 2019. Behavior Sequence Transformer for E-Commerce Recommendation in Alibaba. In Proceedings of the 1st International Workshop on Deep Learning Practice for High-Dimensional Sparse Data (Anchorage, Alaska) (DLP-KDD '19) . Association for Computing Machinery, New York, NY, USA, Article 12, 4 pages. https://doi.org/10.1145/3326937.3341261 [5] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and Hemal Shah. 2016. Wide & Deep Learning for Recommender Systems. In Proceedings of the 1st Workshop on Deep Learning for Recommender Systems (Boston, MA, USA) (DLRS 2016) . Association for Computing Machinery, New York, NY, USA, 7-10. https://doi.org/10.1145/2988450.2988454 [6] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep Neural Networks for YouTube Recommendations. In Proceedings of the 10th ACM Conference on Recommender Systems (Boston, Massachusetts, USA) (RecSys '16) . Association for Computing Machinery, New York, NY, USA, 191-198. https://doi.org/10.1145/ 2959100.2959190 [7] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. 2019. ClassBalanced Loss Based on Effective Number of Samples. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . 9260-9269. https://doi.org/ 10.1109/CVPR.2019.00949 [8] Zeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022. M6Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems. arXiv:2205.08084 [cs.IR] [9] Shangfeng Dai, Haobin Lin, Zhichen Zhao, Jianying Lin, Honghuan Wu, Zhe Wang, Sen Yang, and Ji Liu. 2021. POSO: Personalized Cold Start Modules for Large-scale Recommender Systems. arXiv:2108.04690 [cs.IR] [10] Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-Agnostic MetaLearning for Fast Adaptation of Deep Networks. (2017). [11] Jerome H. Friedman. 2001. Greedy function approximation: A gradient boosting machine. Annals of Statistics 29 (2001), 1189-1232. https://api.semanticscholar. org/CorpusID:39450643 [12] Jingyue Gao, Shuguang Han, Han Zhu, Siran Yang, Yuning Jiang, Jian Xu, and Bo Zheng. 2023. Rec4Ad: A Free Lunch to Mitigate Sample Selection Bias for Ads CTR Prediction in Taobao. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management (<conf-loc>, <city>Birmingham</city>, <country>United Kingdom</country>, </conf-loc>) (CIKM '23) . Association for Computing Machinery, New York, NY, USA, 4574-4580. https://doi.org/10.1145/ 3583780.3615496 [13] Mihajlo Grbovic and Haibin Cheng. 2018. Real-Time Personalization Using Embeddings for Search Ranking at Airbnb. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (London, United Kingdom) (KDD '18) . Association for Computing Machinery, New York, NY, USA, 311-320. https://doi.org/10.1145/3219819.3219885 [14] Xiaoqiang Gui, Yueyao Cheng, Xiang-Rong Sheng, Yunfeng Zhao, Guoxian Yu, Shuguang Han, Yuning Jiang, Jian Xu, and Bo Zheng. 2023. Calibrationcompatible Listwise Distillation of Privileged Features for CTR Prediction. arXiv preprint arXiv:2312.08727 (2023). [15] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: A Factorization-Machine Based Neural Network for CTR Prediction. In Proceedings of the 26th International Joint Conference on Artificial Intelligence (Melbourne, Australia) (IJCAI'17) . AAAI Press, 1725-1731. [16] Jiacen Hu, Zhangming Chan, Yu Zhang, Shuguang Han, Siyuan Lou, Baolin Liu, Han Zhu, Yuning Jiang, Jian Xu, and Bo Zheng. 2023. PS-SA: An Efficient SelfAttention via Progressive Sampling for User Behavior Sequence Modeling. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management (<conf-loc>, <city>Birmingham</city>, <country>United Kingdom</country>, </conf-loc>) (CIKM '23) . Association for Computing Machinery, New York, NY, USA, 4639-4645. https://doi.org/10.1145/3583780.3615495 [17] Tongwen Huang, Qingyun She, Zhiqiang Wang, and Junlin Zhang. 2020. GateNet: gating-enhanced deep network for click-through rate prediction. arXiv preprint arXiv:2007.03519 (2020). [18] Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. 2019. FiBiNET: Combining Feature Importance and Bilinear Feature Interaction for Click-through Rate Prediction. In Proceedings of the 13th ACM Conference on Recommender Systems (Copenhagen, Denmark) (RecSys '19) . Association for Computing Machinery, New York, NY, USA, 169-177. https://doi.org/10.1145/3298689.3347043 [19] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix Factorization Techniques for Recommender Systems. Computer 42, 8 (2009), 30-37. https: //doi.org/10.1109/MC.2009.263 [20] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H. Chi. 2018. Modeling Task Relationships in Multi-Task Learning with Multi-Gate Mixtureof-Experts. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (London, United Kingdom) (KDD '18) . Association for Computing Machinery, New York, NY, USA, 1930-1939. https: //doi.org/10.1145/3219819.3220007 [21] Tong Man, Huawei Shen, Xiaolong Jin, and Xueqi Cheng. 2017. Cross-Domain Recommendation: An Embedding and Mapping Approach. In Proceedings of the 26th International Joint Conference on Artificial Intelligence (Melbourne, Australia) (IJCAI'17) . AAAI Press, 2464-2470. [22] Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain, Andreas Veit, and Sanjiv Kumar. 2021. Long-tail learning via logit adjustment. In International Conference on Learning Representations . https://openreview.net/ forum?id=37nvvqkCo5 [23] Kaixiang Mo, Bo Liu, Lei Xiao, Yong Li, and Jie Jiang. 2015. Image Feature Learning for Cold Start Problem in Display Advertising. In Proceedings of the 24th International Conference on Artificial Intelligence (Buenos Aires, Argentina) (IJCAI'15) . AAAI Press, 3728-3734. [24] Feiyang Pan, Shuokai Li, Xiang Ao, Pingzhong Tang, and Qing He. 2019. Warm Up Cold-Start Advertisements: Improving CTR Predictions via Learning to Learn ID Embeddings. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (Paris, France) (SIGIR'19) . Association for Computing Machinery, New York, NY, USA, 695-704. https: //doi.org/10.1145/3331184.3331268 [25] Yanru Qu, Bohui Fang, Weinan Zhang, Ruiming Tang, Minzhe Niu, Huifeng Guo, Yong Yu, and Xiuqiang He. 2018. Product-Based Neural Networks for User Response Prediction over Multi-Field Categorical Data. ACM Trans. Inf. Syst. 37, 1, Article 5 (oct 2018), 35 pages. https://doi.org/10.1145/3233770 [26] Steffen Rendle. 2010. Factorization Machines. 2010 IEEE International Conference on Data Mining (2010), 995-1000. https://api.semanticscholar.org/CorpusID: 17265929 [27] Xiang-Rong Sheng, Jingyue Gao, Yueyao Cheng, Siran Yang, Shuguang Han, Hongbo Deng, Yuning Jiang, Jian Xu, and Bo Zheng. 2023. Joint Optimization of Ranking and Calibration with Contextualized Hybrid Model. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (Long Beach, CA, USA) (KDD '23) . Association for Computing Machinery, New York, NY, USA, 4813-4822. https://doi.org/10.1145/3580305.3599851 [28] Xiang-Rong Sheng, Liqin Zhao, Guorui Zhou, Xinyao Ding, Binding Dai, Qiang Luo, Siran Yang, Jingshan Lv, Chi Zhang, Hongbo Deng, and Xiaoqiang Zhu. 2021. One Model to Serve All: Star Topology Adaptive Recommender for Multi-Domain CTR Prediction. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management (Virtual Event, Queensland, Australia) (CIKM '21) . Association for Computing Machinery, New York, NY, USA, 4104-4113. https://doi.org/10.1145/3459637.3481941 [29] HongyanTang,Junning Liu, Ming Zhao, and Xudong Gong. 2020. Progressive Layered Extraction (PLE): A Novel Multi-Task Learning (MTL) Model for Personalized Recommendations. In Proceedings of the 14th ACM Conference on Recommender Systems (Virtual Event, Brazil) (RecSys '20) . Association for Computing Machinery, New York, NY, USA, 269-278. https://doi.org/10.1145/3383313.3412236 [30] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is All You Need. In Proceedings of the 31st International Conference on Neural Information Processing Systems (Long Beach, California, USA) (NIPS'17) . Curran Associates Inc., Red Hook, NY, USA, 6000-6010. [31] Maksims Volkovs, Guangwei Yu, and Tomi Poutanen. 2017. DropoutNet: Addressing Cold Start in Recommender Systems. In Proceedings of the 31st International Conference on Neural Information Processing Systems (Long Beach, California, USA) (NIPS'17) . Curran Associates Inc., Red Hook, NY, USA, 4964-4973. [32] Hongwei Wang, Fuzheng Zhang, Jialin Wang, Miao Zhao, Wenjie Li, Xing Xie, and Minyi Guo. 2018. RippleNet: Propagating User Preferences on the Knowledge Graph for Recommender Systems. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management (Torino, Italy) (CIKM '18) . Association for Computing Machinery, New York, NY, USA, 417-426. https: //doi.org/10.1145/3269206.3271739 [33] Kailun Wu, Weijie Bian, Zhangming Chan, Lejian Ren, Shiming Xiang, ShuGuang Han, Hongbo Deng, and Bo Zheng. 2022. Adversarial gradient driven exploration for deep click-through rate prediction. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 2050-2058. WWW'24 Companion, May 13-17, 2024, Singapore, Singapore Wenhao Wu et al. [34] Qitian Wu, Hengrui Zhang, Xiaofeng Gao, Peng He, Paul Weng, Han Gao, and Guihai Chen. 2019. Dual Graph Attention Networks for Deep Latent Representation of Multifaceted Social Effects in Recommender Systems. In The World Wide Web Conference (San Francisco, CA, USA) (WWW '19) . Association for Computing Machinery, New York, NY, USA, 2091-2102. https: //doi.org/10.1145/3308558.3313442 [35] Dongo Xi, Fuzhen Zhuang, Yanchi Liu, Jingjing Gu, Hui Xiong, and Qing He. 2019. Modelling of Bi-Directional Spatio-Temporal Dependence and Users' Dynamic Preferences for Missing POI Check-in Identification. In Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence (Honolulu, Hawaii, USA) (AAAI'19/IAAI'19/EAAI'19) . AAAI Press, Article 669, 8 pages. https: //doi.org/10.1609/aaai.v33i01.33015458 [36] Ruobing Xie, Zhijie Qiu, Jun Rao, Yi Liu, Bo Zhang, and Leyu Lin. 2021. Internal and Contextual Attention Network for Cold-Start Multi-Channel Matching in Recommendation. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence (Yokohama, Yokohama, Japan) (IJCAI'20) . Article 379, 7 pages. [37] Jia-Qi Yang, Xiang Li, Shuguang Han, Tao Zhuang, De chuan Zhan, Xiaoyi Zeng, and Bin Tong. 2020. Capturing Delayed Feedback in Conversion Rate Prediction via Elapsed-Time Sampling. In AAAI Conference on Artificial Intelligence . https: //api.semanticscholar.org/CorpusID:227333826 [38] Yujing Zhang, Zhangming Chan, Shuhao Xu, Weijie Bian, Shuguang Han, Hongbo Deng, and Bo Zheng. 2022. KEEP: An industrial pre-training framework for online recommendation via knowledge extraction and plugging. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management . 3684-3693. [39] Zhao-Yu Zhang, Xiang-Rong Sheng, Yujing Zhang, Biye Jiang, Shuguang Han, Hongbo Deng, and Bo Zheng. 2022. Towards Understanding the Overfitting Phenomenon of Deep Click-Through Rate Models. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management (Atlanta, GA, USA) (CIKM '22) . Association for Computing Machinery, New York, NY, USA, 2671-2680. https://doi.org/10.1145/3511808.3557479 [40] Yunfeng Zhao, Xu Yan, Xiaoqiang Gui, Shuguang Han, Xiang-Rong Sheng, Guoxian Yu, Jufeng Chen, Zhao Xu, and Bo Zheng. 2023. Entire Space Cascade Delayed Feedback Modeling for Effective Conversion Rate Prediction. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management . 4981-4987. [41] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Deep Interest Evolution Network for Click-through Rate Prediction. In Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence (Honolulu, Hawaii, USA) (AAAI'19/IAAI'19/EAAI'19) . AAAI Press, Article 729, 8 pages. https://doi.org/10.1609/aaai.v33i01.33015941 [42] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep Interest Network for ClickThrough Rate Prediction. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (London, United Kingdom) (KDD '18) . Association for Computing Machinery, New York, NY, USA, 1059-1068. https://doi.org/10.1145/3219819.3219823 [43] Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, and Ji-Rong Wen. 2020. S3-Rec: Self-Supervised Learning for Sequential Recommendation with Mutual Information Maximization. ACM. https://doi.org/10.1145/3340531.3411954 [44] Yongchun Zhu, Ruobing Xie, Fuzhen Zhuang, Kaikai Ge, Ying Sun, Xu Zhang, Leyu Lin, and Juan Cao. 2021. Learning to Warm Up Cold Item Embeddings for Cold-Start Recommendation with Meta Scaling and Shifting Networks. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, Canada) (SIGIR '21) . Association for Computing Machinery, New York, NY, USA, 1167-1176. https://doi.org/10.1145/3404835.3462843",
  "keywords_parsed": [
    "Recommendation System",
    "Click-through Rate Prediction",
    "Meta Learning",
    "Limited-Stock Product Recommendation"
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "GIFT: Graph-GuIded Feature Transfer for Cold-Start Video ClickThrough Rate Prediction"
    },
    {
      "ref_id": "b2",
      "title": "Capturing Conversion Rate Fluctuation during Sales Promotions: A Novel Historical Data Reuse Approach"
    },
    {
      "ref_id": "b3",
      "title": "PEPNet: Parameter and Embedding Personalized Network for Infusing with Personalized Prior Information"
    },
    {
      "ref_id": "b4",
      "title": "Behavior Sequence Transformer for E-Commerce Recommendation in Alibaba"
    },
    {
      "ref_id": "b5",
      "title": "Wide & Deep Learning for Recommender Systems"
    },
    {
      "ref_id": "b6",
      "title": "Deep Neural Networks for YouTube Recommendations"
    },
    {
      "ref_id": "b7",
      "title": "ClassBalanced Loss Based on Effective Number of Samples"
    },
    {
      "ref_id": "b8",
      "title": "M6Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems"
    },
    {
      "ref_id": "b9",
      "title": "POSO: Personalized Cold Start Modules for Large-scale Recommender Systems"
    },
    {
      "ref_id": "b10",
      "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"
    },
    {
      "ref_id": "b11",
      "title": "Greedy function approximation: A gradient boosting machine"
    },
    {
      "ref_id": "b12",
      "title": "Rec4Ad: A Free Lunch to Mitigate Sample Selection Bias for Ads CTR Prediction in Taobao"
    },
    {
      "ref_id": "b13",
      "title": "Real-Time Personalization Using Embeddings for Search Ranking at Airbnb"
    },
    {
      "ref_id": "b14",
      "title": "Calibrationcompatible Listwise Distillation of Privileged Features for CTR Prediction"
    },
    {
      "ref_id": "b15",
      "title": "DeepFM: A Factorization-Machine Based Neural Network for CTR Prediction"
    },
    {
      "ref_id": "b16",
      "title": "PS-SA: An Efficient Self-Attention via Progressive Sampling for User Behavior Sequence Modeling"
    },
    {
      "ref_id": "b17",
      "title": "GateNet: gating-enhanced deep network for click-through rate prediction"
    },
    {
      "ref_id": "b18",
      "title": "FiBiNET: Combining Feature Importance and Bilinear Feature Interaction for Click-through Rate Prediction"
    },
    {
      "ref_id": "b19",
      "title": "Matrix Factorization Techniques for Recommender Systems"
    },
    {
      "ref_id": "b20",
      "title": "Modeling Task Relationships in Multi-Task Learning with Multi-Gate Mixture-of-Experts"
    },
    {
      "ref_id": "b21",
      "title": "Cross-Domain Recommendation: An Embedding and Mapping Approach"
    },
    {
      "ref_id": "b22",
      "title": "Long-tail learning via logit adjustment"
    },
    {
      "ref_id": "b23",
      "title": "Image Feature Learning for Cold Start Problem in Display Advertising"
    },
    {
      "ref_id": "b24",
      "title": "Warm Up Cold-Start Advertisements: Improving CTR Predictions via Learning to Learn ID Embeddings"
    },
    {
      "ref_id": "b25",
      "title": "Product-Based Neural Networks for User Response Prediction over Multi-Field Categorical Data"
    },
    {
      "ref_id": "b26",
      "title": "Factorization Machines"
    },
    {
      "ref_id": "b27",
      "title": "Joint Optimization of Ranking and Calibration with Contextualized Hybrid Model"
    },
    {
      "ref_id": "b28",
      "title": "One Model to Serve All: Star Topology Adaptive Recommender for Multi-Domain CTR Prediction"
    },
    {
      "ref_id": "b29",
      "title": "Progressive Layered Extraction (PLE): A Novel Multi-Task Learning (MTL) Model for Personalized Recommendations"
    },
    {
      "ref_id": "b30",
      "title": "Attention is All You Need"
    },
    {
      "ref_id": "b31",
      "title": "DropoutNet: Addressing Cold Start in Recommender Systems"
    },
    {
      "ref_id": "b32",
      "title": "RippleNet: Propagating User Preferences on the Knowledge Graph for Recommender Systems"
    },
    {
      "ref_id": "b33",
      "title": "Adversarial gradient driven exploration for deep click-through rate prediction"
    },
    {
      "ref_id": "b34",
      "title": "Dual Graph Attention Networks for Deep Latent Representation of Multifaceted Social Effects in Recommender Systems"
    },
    {
      "ref_id": "b35",
      "title": "Modelling of Bi-Directional Spatio-Temporal Dependence and Users' Dynamic Preferences for Missing POI Check-in Identification"
    },
    {
      "ref_id": "b36",
      "title": "Internal and Contextual Attention Network for Cold-Start Multi-Channel Matching in Recommendation"
    },
    {
      "ref_id": "b37",
      "title": "Capturing Delayed Feedback in Conversion Rate Prediction via Elapsed-Time Sampling"
    },
    {
      "ref_id": "b38",
      "title": "KEEP: An industrial pre-training framework for online recommendation via knowledge extraction and plugging"
    },
    {
      "ref_id": "b39",
      "title": "Towards Understanding the Overfitting Phenomenon of Deep Click-Through Rate Models"
    },
    {
      "ref_id": "b40",
      "title": "Entire Space Cascade Delayed Feedback Modeling for Effective Conversion Rate Prediction"
    },
    {
      "ref_id": "b41",
      "title": "Deep Interest Evolution Network for Click-through Rate Prediction"
    },
    {
      "ref_id": "b42",
      "title": "Deep Interest Network for ClickThrough Rate Prediction"
    },
    {
      "ref_id": "b43",
      "title": "S3-Rec: Self-Supervised Learning for Sequential Recommendation with Mutual Information Maximization"
    },
    {
      "ref_id": "b44",
      "title": "Learning to Warm Up Cold Item Embeddings for Cold-Start Recommendation with Meta Scaling and Shifting Networks"
    }
  ]
}