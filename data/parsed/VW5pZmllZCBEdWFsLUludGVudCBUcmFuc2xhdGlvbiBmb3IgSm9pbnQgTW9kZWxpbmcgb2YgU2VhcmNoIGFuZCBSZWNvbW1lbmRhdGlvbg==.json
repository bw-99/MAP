{
  "Unified Dual-Intent Translation for Joint Modeling of Search and Recommendation": "",
  "Yuting Zhang âˆ—": "",
  "Yiqing Wu âˆ—": "Institute of Computing Technology, Chinese Academy of Sciences Beijing, China zhangyuting21s@ict.ac.cn Ruidong Han Meituan Beijing, China hanruidong@meituan.com",
  "Ying Sun": "Institute of Computing Technology, Chinese Academy of Sciences Beijing, China iwu_yiqing@163.com",
  "Yongchun Zhu": "Thrust of Artificial Intelligence, The Hong Kong University of Science and Technology (Guangzhou) Guangzhou, China yings@hkust-gz.edu.cn Institute of Computing Technology, Chinese Academy of Sciences Beijing, China zhuyc0204@gmail.com",
  "Fuzhen Zhuang â€ ": "",
  "Zhulin An â€ ": "Institute of Artificial Intelligence, Beihang University Beijing, China Zhongguancun Laboratory Beijing, China zhuangfuzhen@buaa.edu.cn",
  "ABSTRACT": "Recommendation systems, which assist users in discovering their preferred items among numerous options, have served billions of users across various online platforms. Intuitively, users' interactions with items are highly driven by their unchanging inherent intents (e.g., always preferring high-quality items) and changing demand intents (e.g., wanting a T-shirt in summer but a down jacket in winter). However, both types of intents are implicitly expressed in recommendation scenario, posing challenges in leveraging them for accurate intent-aware recommendations. Fortunately, in search scenario, often found alongside recommendation on the same online platform, users express their demand intents explicitly through their query words . Intuitively, in both scenarios, a user shares the same inherent intent and his/her interactions may be influenced by the same demand intent. It is therefore feasible to utilize the interaction data from both scenarios to reinforce the dual intents for joint intent-aware modeling. But the joint modeling should deal with two problems: (1) accurately modeling users' implicit demand intents in recommendation ; (2) modeling the relation between the dual intents and the interactive items . To address these problems, we propose a novel âˆ— Yuting Zhang and Yiqing Wu are also at University of Chinese Academy of Sciences â€  Fuzhen Zhuang and Zhulin An are Corresponding authors. This work is licensed under a Creative Commons Attribution International 4.0 License. KDD '24, August 25-29, 2024, Barcelona, Spain Â© 2024 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-0490-1/24/08. https://doi.org/10.1145/3637528.3671519",
  "Yongjun Xu": "Institute of Computing Technology, Chinese Academy of Sciences Beijing, China anzhulin@ict.ac.cn xyj@ict.ac.cn model named Unified Dual-Intents Translation for joint modeling of Search and Recommendation (UDITSR). To accurately simulate users' demand intents in recommendation, we utilize real queries from search data as supervision information to guide its generation. To explicitly model the relation among the triplet <inherent intent, demand intent, interactive item>, we propose a dual-intent translation propagation mechanism to learn the triplet in the same semantic space via embedding translations. Extensive experiments demonstrate that UDITSR outperforms SOTA baselines both in search and recommendation tasks. Moreover, our model has been deployed online on Meituan Waimai platform, leading to an average improvement in GMV (Gross Merchandise Value) of 1.46% and CTR(Click-Through Rate) of 0.77% over one month.",
  "CCS CONCEPTS": "Â· Information systems â†’ Recommender systems .",
  "KEYWORDS": "Joint learning, Search and recommendation, Dual intent modeling, Intent translation",
  "ACMReference Format:": "Yuting Zhang, Yiqing Wu, Ruidong Han, Ying Sun, Yongchun Zhu, Xiang Li, Wei Lin, Fuzhen Zhuang, Zhulin An, and Yongjun Xu. 2024. Unified Dual-Intent Translation for Joint Modeling of Search and Recommendation. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '24), August 25-29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3637528.3671519 Xiang Li Wei Lin Meituan Beijing, China lixiang245@meituan.com linwei31@meituan.com KDD '24, August 25-29, 2024, Barcelona, Spain Yuting Zhang et al.",
  "1 INTRODUCTION": "Aiming to help users discover items of interest from a vast array of options, recommendation systems have become an essential component of various online platforms, such as e-commerce [26, 48, 49] and digital news services [7, 19, 35]. Existing recommendation models [15, 16, 48, 49] typically exploit users' implicit feedback, such as click history, to predict their interests. For instance, traditional Collaborative Filtering (CF) [16] assumes that users will interact with items similar to those with which they've previously interacted. Furthermore, various models [48, 49] have been developed to capture the sequential dynamics of users' implicit feedback to model their evolving interests. In practice, user feedback patterns in recommendation systems are highly driven by their complex intents, which can be broadly categorized into unchanging inherent intents and changing demand intents. For example, Amy and Tom may have the same noodle demand but choose different restaurants due to Amy's inherent intent for spicy flavors and Tom's for sweet. Besides, a single user's interactions can vary due to their changing demands. Yet, these intents are often implicitly expressed in the recommendation, presenting a challenge for accurate intent-aware recommendations. Existing intent-aware recommendation models [5, 23, 50] typically rely on users' implicit feedback to learn their intents. However, these models encounter a significant problem: different users may have different inherent or demand intents despite similar historical feedback. As shown in Figure 1(a), Amy's interaction with Pizza Hut might indicate a demand intent for pasta, while Tom may demand pizza instead. Ideally, recommendation systems should suggest pasta-related options to Amy and pizza-related ones to Tom. However, without any explicit intent information, existing models struggle to distinguish between these intents, resulting in inaccurate recommendations. Fortunately, in search services, which often accompany recommendation services on the same online platform, users explicitly express their demand intents through query words, as shown in Figure 1(b). Such explicit search demand information can serve as additional explicit information to assist in learning implicit demand intents for recommendation. Indeed, both search and recommendation tasks aim to comprehend users' intents to aid them in obtaining desired items [2]. In addition, in search scenario, users' interactions are influenced not only by their explicit demand intents but also by their personalized inherent intents. Yet, search models typically focus on the match between search results and users' demand intents, often overlooking the impact of their personalized inherent intents, which are indeed significant [33]. Intuitively, in both scenarios, a user maintains the same inherent intent and his/her behaviors are likely to be determined by the same demand intent. Therefore, it is feasible to leverage interaction data from both scenarios to reinforce or complement each other's dual intents for joint intent-aware modeling. Nevertheless, this joint modeling is not trivial due to the following challenges: (1) How to accurately model a user's implicit demand intent in recommendation with search data? A user's demand intent is implicit within recommendation but is explicitly indicated by search queries. If the changing demand intents in recommendation can be accurately generated, search and recommendation Figure 1: Examples of interaction behaviors in recommendation and search scenarios. hamburger (b) Search (a) Recommendation Amy Tom recommendation interaction search interaction with query words ** hamburger pasta pizza can be well modeled in a unified manner. The existing method, SRJGraph [47], employs the unchanging padding query in recommendation for unified modeling. This approach assumes an unchanging demand intent across all recommendation interactions, which may hinder recommendation performance. To learn demand intents, an intuitive approach is to simply incorporate users' historical queries as additional demand information into the recommendation model. However, without explicit supervision to verify the accuracy of demand intents, there may be a significant discrepancy between the learned and the actual demand intents. (2) How to couple the dual intents to model the relation among the intents and the interactive items? Both inherent intent and demand intent affect the interactive item. Intuitively, the superimposition of inherent intents (e.g., preferring cheap items) and changing demand intents (needing a T-shirt in summer but a down jacket in winter) leads to changing interactive results (interacting with a cheap T-shirt and cheap down jacket , respectively). In essence, the demand intent can be regarded as the changing deviation from the inherent intent to the changing interactive item. A common approach is to simply feed the two intents as input features, but it cannot fully capture the relation between the dual intents and the interactive item. To tackle these challenges, we propose a novel model named Unified Dual-Intent Translation for joint modeling of Search and Recommendation (UDITSR). Overall, UDITSR comprises a searchsupervised demand intent generator and a dual-intent translation module. Specifically, in the demand intent generator, search queries serve as supervision information, allowing us to learn and understand a user's changing demand intent for recommendations both reliably and accurately. Moreover, we develop a dual-intent translation propagation mechanism. This mechanism explicitly models the interpretable relation among the triplet elements -user's <inherent intent, demand intent, interactive item> -within a shared semantic space by employing embedding translations. Particularly, we design an intent translation contrastive learning to further constrain the translation relation. Extensive offline and online experiments were conducted to demonstrate our model's effectiveness. To gain deeper insights into the effectiveness of our model, we also provide a visual analysis of relevant intents. Unified Dual-Intent Translation for Joint Modeling of Search and Recommendation KDD '24, August 25-29, 2024, Barcelona, Spain",
  "2 RELATED WORK": "",
  "2.1 Recommendation and Search Models": "Recommendation aims to filter items from vast candidate pools to match user interests. Traditional models, such as Collaborative Filtering (CF), assume users with similar behaviors share item preferences [6, 13, 15, 31]. Later studies [11, 34, 49] focus on decoding users' evolving interests from their historical behaviors, using techniques like DIN [49], which employs attention mechanism to connect past behaviors with current targets. Recognizing that users' interactions are driven by their intrinsic intents, recent studies [5, 37, 38, 50] exploit users' historical behavior sequences to understand their changing intents, aiming to better meet user needs. For instance, KA-MemNN [50] uses item categories from user behavior as intent proxies, implementing memory networks for dynamic intent modeling. However, these approaches often deduce intents from interaction behaviors or directly equate behavior with intent, without mining real intrinsic intents. In contrast, our model utilizes the user's actual demand intents in the search scenario as supervision information to imitate the intents in recommendation. Search and recommendation services often coexist on the same platform [27]. Earlier research [2] suggests their goals are essentially equivalent -helping people get the items they want, prompting studies on their joint optimization. For example, JSR [44] introduces a shared-parameter framework, with user and item embeddings shared. USER [43] treats recommendation behavior as a form of search behavior with unchanging padding query, unifying the modeling of search and recommendation sequences. Furthermore, SRJgraph [47] constructs a unified graph from search and recommendation behaviors, incorporating search queries and a padding query for recommendation as attributes of user-item edges. These models assume the query-related intents in recommendation are unchanging while the matching degree between the query and the candidate items significantly affects search performance. This assumption creates a significant gap between the modeling of search and recommendation, greatly hindering the effectiveness of joint modeling approaches. Our model, however, adapts to learn personalized and changing query-related intents for distinct user-item pairs in recommendation, thus enhancing the unification of joint search and recommendation.",
  "2.2 Graph Neural Network": "Graph Neural Networks (GNNs) [32, 41] have gained tremendous attention in recent years due to their remarkable ability to process graph-structured data. For instance, Graph Convolutional Network (GCN) [18] employs a localized filter to aggregate information from neighbors, and Graph Attention Network (GAT) [36] leverages the attention mechanism to weigh the importance of each neighbor node during the aggregation process. Since then, numerous variants of GNNs [8, 42, 46] have been proposed to tackle various types of graphs. Nowadays, Graph Neural Networks have shown great potential in a wide range of applications, such as recommendation [14, 39, 40] and search [10, 22, 25] scenarios. In this work, we propose incorporating demand intents that are generated through search supervision in recommendation scenario, as well as explicitly stated search intents, into the construction of a unified graph. Specifically, these demand intents serve as the attributes of the edges connecting users and items. Moreover, the invariant node representations for a user across different interactions are used to indicate their inherent intents. Based on the graph, we propose a novel dual-intent translation propagation for unified dual intentaware modeling.",
  "3 PRELIMINARY": "Let U and I denote the universal sets of users and items in both search and recommendation scenarios. In order to distinguish these two scenarios, we define the interaction records in each scenario as follows: Definition 1 . search scenario : In the search data X ğ‘  , each interaction record ğ‘¥ ğ‘  âˆˆ X ğ‘  can be formulated as ğ‘¥ ğ‘  = ( ğ‘¢, ğ‘–, ğ‘ ) , which represents that user ğ‘¢ âˆˆ U clicked item ğ‘– âˆˆ I with the explicit query ğ‘ . The query ğ‘ can be segmented into several shorter terms as ğ‘ = [ ğ‘¤ 1 , Â· Â· Â· , ğ‘¤ | ğ‘ | ] , where ğ‘¤ ğ‘– denotes the ğ‘– -th term and | ğ‘ | is the number of terms in query ğ‘ . Definition 2 . recommendation scenario : In the recommendation data X ğ‘Ÿ , each interaction record ğ‘¥ ğ‘Ÿ âˆˆ X ğ‘Ÿ can be formulated as ğ‘¥ ğ‘Ÿ = ( ğ‘¢, ğ‘– ) , which represents user ğ‘¢ âˆˆ U clicked item ğ‘– âˆˆ I without an explicit query. Thereby, the double-scenario graph including all user click behaviors in both scenarios can be constructed as follows: Definition 3 . double-scenario graph : Given the set of all user click behaviors in both scenarios, denoted as X = X ğ‘  âˆª X ğ‘Ÿ , the double-scenario graph can be formulated as G = (U âˆª I , E ğ‘  âˆª E ğ‘Ÿ ) . Each search edge ğœ– âˆˆ E ğ‘  corresponds to a record ( ğ‘¢, ğ‘–, ğ‘ ) in X ğ‘  , while each recommendation edge ğœ– âˆˆ E ğ‘Ÿ corresponds to a record ( ğ‘¢, ğ‘– ) in X ğ‘Ÿ . In Figure 2(a), there is an example of our double-scenario graph . For instance, user ğ‘¢ 1 searches for query ğ‘ 12 and then clicks item ğ‘– 2 in search scenario. Thus, an edge exists between nodes ğ‘¢ 1 and ğ‘– 2, with query ğ‘ 12 assigned as an attribute of this edge. Likewise, in recommendation scenario, when user ğ‘¢ 1 clicks item ğ‘– 1, an edge also exists between user ğ‘¢ 1 and item ğ‘– 1, but without any query attribute. Based on the above definitions, the joint modeling of search and recommendation can be defined as follows: Problem definition . Given search data X ğ‘  , recommendation data X ğ‘Ÿ and double-scenario graph G , this task is to train a joint model of search and recommendation to predict the most appropriate items ğ‘– âˆˆ I that user ğ‘¢ âˆˆ U will interact.",
  "4 METHODOLOGY": "In this section, we introduce UDITSR for dual intent-aware joint modeling of search and recommendation, as depicted in Figure 2. We begin with the model's embedding layer in Section 4.1. Then, in Section 4.2, we detail a search-supervised demand intent generator that leverages search query data to infer recommendation intents, which allows us to convert the double-scenario graph into a unified graph . Utilizing this graph, we describe dual-intent translation propagation to couple inherent intents and demand intents, enhanced by a contrastive loss to constrain the translation relation. Finally, the prediction layer and optimization are illustrated in Section 4.4. KDD '24, August 25-29, 2024, Barcelona, Spain Yuting Zhang et al. Figure 2: Overall framework of our proposed UDITSR. The mean in dual-intent translation represents the mean-pooling operation in Eq. 5. For clarity, only two interaction examples are displayed for each graph aggregation in the dual-intent translation . search edge demand intent generator ğ‘¢ ! â„’ \"# â„’ !\" ğ‘’ $ % = ğ‘šğ‘’ğ‘ğ‘› (ğ‘’ & %'! + Ìƒ ğ‘’ ( ) Ìƒ ğ‘’ ( + ğ‘¦ &,$ ğ‘’ & % = ğ‘šğ‘’ğ‘ğ‘›(ğ‘’ $ %'! - Ìƒ ğ‘’ ( ) MLP * /MLP + â„’ # intent translation contrastive loss BPR loss ğ‘’ $ âˆ— ğ‘’ & âˆ— + ğ‘ !! ğ‘¢ - ğ‘¢ ! ğ‘ -! ğ‘ !- + ğ‘ !! ğ‘– - ğ‘– ! ğ‘¢ ! ğ‘ ğŸ- ğ‘ ğŸ/ ... ğ‘– ! ğ‘ -ğŸ ğ‘ /ğŸ ... search recommendation search-supervised generation loss ğ‘ !- ğ‘ !/ ğ‘ !0 ğ‘– - ğ‘ /! ğ‘¢ - ğ‘¢ ! ğ‘– / ğ‘– 0 ğ‘¢ / + ğ‘ !! + ğ‘ -- ğ‘ -! + ğ‘ /0 ğ‘– ! ğ‘– ! ğ‘™ = 1 ğ‘™ = 2 ğ‘™ = 1 ğ‘™ = 2 ğ‘ !- ğ‘ !/ ğ‘ !0 ğ‘– - ğ‘ /! ğ‘¢ - ğ‘¢ ! ğ‘– / ğ‘– 0 ğ‘¢ / ğ‘ -! ğ‘– ! (a) Search-supervised demand intent generation (b) Dual-intent translation generated demand intent recommendation edge recommendation edge with generated demand Ìƒ ğ‘’ ( = 4 ğ‘’ğ‘šğ‘ ğ‘ for search ğ‘’ğ‘šğ‘ + ğ‘ for recommendation",
  "4.1 Embedding Layer": "Byfeeding the user ID and item ID into the user and item embedding matrices respectively, we can obtain the embeddings of user ğ‘¢ and item ğ‘– as e ğ‘¢ , e ğ‘– . Since each query ğ‘ is a sequence of shorter terms as [ ğ‘¤ 1, ğ‘¤ 2, Â· Â· Â· , ğ‘¤ | ğ‘ | ], we can obtain the representation of query ğ‘ by combining the embeddings of its terms:   where e ğ‘¤ ğ‘˜ represents the embedding of the ğ‘˜ -th query term in ğ‘ and ğ‘“ (Â·) denotes a combination function. In this study, we choose the element-wise sum-pooling operation because it is both efficient and effective for this combination through empirical analysis.",
  "4.2 Demand Intent Generation": "4.2.1 Search-Supervised Demand Intent Generator. The notable difference between search and recommendation is that a user explicitly expresses demand intents in search, whereas recommendation lacks such explicit intents. To bridge this gap, we propose to utilize the abundant query information from search to supervise the generation of users' demand intents in recommendation. Below we describe the generator in detail. Since the user's historical queries ğ‘ ğ‘¢ = [ ğ‘¤ ğ‘¢ 1 , ğ‘¤ ğ‘¢ 2 , Â· Â· Â· , ğ‘¤ ğ‘¢ | ğ‘ ğ‘¢ | ] and the item's historical queries ğ‘ ğ‘– = [ ğ‘¤ ğ‘– 1 , ğ‘¤ ğ‘– 2 , Â· Â· Â· , ğ‘¤ ğ‘– | ğ‘ ğ‘– | ] contain abundant demand intent information, we leverage them as auxiliary information to simulate the user's demand intent for recommendation. Similar to the processing of ğ‘ in Eq. 1, we adopt the elementwise sum-pooling operation to obtain the representation of ğ‘ ğ‘¢ as in ğ‘ ğ‘¢ . Since ğ‘ ğ‘– contains query words from multiple users, we introduce a user-aware gate mechanism to model personalized demand intents. Particularly, the user-aware gating network ğ‘” yields a distribution over the | ğ‘ ğ‘– | query words. The personalized representation of ğ‘ ğ‘– is then formulated as the weighted sum of the embeddings of its query words, as follows:  where Â·âˆ¥Â· denotes the concatenation operation; W g is used to match the dimensions of vector e ğ‘¤ ğ‘– ğ‘˜ and the concatenated vector. Then, with user-related representations e ğ‘¢ , e ğ‘ ğ‘¢ and item-related representations e ğ‘– , e ğ‘ ğ‘– , the user's demand intent about the item can be estimated as follows:  where MLP denotes a multi-layer perceptron. Since the ground truth queries in search data serve as the supervision information for generating demand intent, we design the generation loss as KDD '24, August 25-29, 2024, Barcelona, Spain Unified Dual-Intent Translation for Joint Modeling of Search and Recommendation follows:  4.2.2 Unified Graph. After generating the demand intents, each recommendation record ( ğ‘¢, ğ‘– ) in X ğ‘Ÿ can be converted into a triplet ( ğ‘¢, ğ‘–, Ë† ğ‘ ), where the embedding of Ë† ğ‘ corresponds to the generated intents Ë† e ğ‘ . For simplicity, we directly generate the representation of intent Ë† e ğ‘ instead of indirectly predicting the specific query Ë† ğ‘ . With the generated demand intents, the double-scenario graph can be converted into a unified graph . Specifically, an additional attribute Ë† ğ‘ is attached to each recommendation edge ( ğ‘¢, ğ‘– ) in G . For brevity, we use e ğ‘ / e e ğ‘ to uniformly represent the real ğ‘ / e ğ‘ in search scenario and the generated Ë† ğ‘ / Ë† e ğ‘ in recommendation scenario correspondingly. Based on the unified graph, we implement the unified modeling of recommendation and search below.",
  "4.3 Dual-Intent Translation Propagation": "To explicitly model the relation among the dual intents and the interactive items, we propose a dual-intent translation module inspired by the triplet-based representation learning in knowledge graphs [4]. Specifically, we use the user's embedding representation, which remains inherent for a single user, to represent their inherent intent. The search query representation and the generated demand intent in recommendation represent the user's demand intent. The representation of an interactive item is given by its embedding. We assume that a user's changing interactive item should be close to their inherent intent plus changing demand intent. Consequently, we aggregate the neighbor embeddings as follows:  where N ğ‘¢ and N ğ‘– denote the neighboring nodes of user ğ‘¢ and item ğ‘– respectively, in the unified graph; e 0 ğ‘¢ = e ğ‘¢ and e 0 ğ‘– = e ğ‘– . In particular, the subtraction aggregation operation, as opposed to the addition operation, for aggregating the embeddings of user neighboring nodes to simulate users' inherent intents. Finally, the weighted-pooling operation is applied to generate the aggregated representations by operating on the propagated ğ¿ layers:  where ğ›¼ ğ‘™ indicates the importance of the ğ‘™ -th layer representation in constituting the final embedding. Following LightGCN [14], we set ğ›¼ ğ‘™ as 1 ( ğ‘™ + 1 ) , as the focus of our work is not on its selection. To further constrain the translation relation, we design an intent translation contrastive learning approach that adopts a marginbased ranking criterion. Specifically, we aim to ensure that e âˆ— ğ‘¢ + e e ğ‘ â‰ˆ e âˆ— ğ‘– (i.e., the ground truth interactive item e âˆ— ğ‘– should be near to the translated intent e âˆ— ğ‘¢ + e e ğ‘ ), while the negative e âˆ— ğ‘– â€² should be distant from e âˆ— ğ‘¢ + e e ğ‘ , as follows:  where e e ğ‘ denotes the representation of real query in search or the generated demand intent in recommendation for ( ğ‘¢, ğ‘– ) pair; ğ‘Œ = {( ğ‘¢, ğ‘–, ğ‘– â€² )|( ğ‘¢, ğ‘– ) âˆˆ ğ‘… + , ( ğ‘¢, ğ‘– â€² ) âˆˆ ğ‘… - } denotes the pairwise training data where ğ‘… + indicates the positive observed interaction set, and ğ‘… -represents the randomly-sampled negative set; ğœ (Â·) stands for the sigmoid function.",
  "4.4 Model Prediction and Optimization": "After obtaining the representations e âˆ— ğ‘¢ , e âˆ— ğ‘– , e e ğ‘ , we fuse them to obtain the overall representation for the input sample ğ‘¥ = ( ğ‘¢, ğ‘–, e ğ‘ ) :  Then, two different MLPs are employed to make prediction for search and recommendation tasks, respectively:  We adopt pairwise training to train the model. Specifically, we adopt the Bayesian Personalized Ranking (BPR) [30] loss to emphasize that the observed interaction should be assigned a higher score than the unobserved one as follows:  where the representation of e ğ‘ â€² denotes the demand intent for the negative pair ( ğ‘¢, ğ‘– â€² ). Finally, the overall loss L is defined using hyper-parameters ğœ† 1 and ğœ† 2 as:",
  "5 EXPERIMENTS": "In this section, we present empirical results to demonstrate the effectiveness of our proposed UDITSR. These experiments are designed to answer the following research questions: RQ1 How does UDITSR perform compared with state-of-the-art search and recommendation models? RQ2 What are the effects of the demand intent generator and dual-intent translation mechanism in UDITSR? RQ3 Why could UDITSR perform better? RQ4 How does UDITSR perform in real-world online recommendations with practical metrics? RQ5 How do the hyper-parameters in UDITSR impact the search and recommendation performance?",
  "5.1 Experimental Settings": "5.1.1 Dataset Description. We conducted experiments on two realworld datasets, denoted as MT-Large and MT-Small datasets 1 . These two datasets are obtained from the Meituan platform, one of the largest takeaway platforms in China. Both datasets span eight days across two cities. Each sample in the datasets contains a user and an item, and each search sample additionally contains a query. Specifically, with 111,891 search and 65,035 recommendation interactions collected, our MT-Small dataset comprises 56,887 users and 4,059 items and the average number of split words per query record is 1.6801. With 1,527,869 search and 1,168,491 recommendation interactions collected, the MT-Large dataset contains 433,573 users and 22,967 items and the average number of split words per query is 1.5561. To evaluate model performance, we split the first six days' data for training, the seventh day's data for validation, and the 1 We collected this dataset because there was no public dataset that includes both search and recommendation data. Our code and data will be available at https://github.com/17231087/UDITSR. KDD '24, August 25-29, 2024, Barcelona, Spain Yuting Zhang et al. last day's data for testing. For each ground truth test record, we randomly sampled 99 items that the user did not interact with as negative samples. Table 1: Network Configuration 5.1.2 Implementation Details. We implement all models using PyTorch 2 , a well-known software library for deep learning. In Section 5.6, we report the impact of essential hyper-parameters in our model, including the loss weights ğœ† 1 and ğœ† 2, and we utilize the best settings for these hyper-parameters. The remaining network configurations are presented in Table 1. To ensure a fair comparison, we apply the above-mentioned settings across all models. Moreover, we search for optimal values of the other hyper-parameters of the baseline models as suggested in their respective original papers. Finally, we employ the early stopping strategy based on the models' performance on the validation set to avoid overfitting. 5.1.3 Evaluation Metrics. To evaluate our model's performance, weutilize four widely-used ranking metrics: Hit@K, NDCG@K [17] (we set K as 5 by default), MRR [28] and Average position of the Clicked items (Avg.C) [43]. Additionally, we adopt an accuracy metric, AUC [12] for the recommendation task. 5.1.4 Baselines. In our work, we evaluate the performance of our model with two groups of baselines to examine its effectiveness. (1) Graph-free baselines Â· NeuMF [15] combines traditional matrix decomposition with the MLP to extract low-dimensional and high-dimensional features simultaneously. Â· DNN combines the embedding layer described in Section 4.1 with the prediction layer described in Section 4.4. Â· xDeepFM [21] consists of a compressed interaction network (CIN) and an MLP for prediction, where CIN generates explicit feature interactions at the vector-wise level. Â· DIN [49] utilizes an attention mechanism between the historical behavior sequence and the target item to model the evolving interests. Â· AEM [1] allocates different attention values to the previous behavior sequence based on the current search queries. 2 https://pytorch.org/ Â· TEM [3] feeds the sequence of query and user behavior history into a transformer layer to extract the search intents. Â· JSR [45] integrates neural collaborative filtering and language modeling to reconstruct query text descriptions, enabling the joint model of search and recommendation. Â· SimpleX [24] is a simplified variant of the two-tower model with user behavior modeling. Â· MGDSPR [20] utilizes an attention mechanism to model the relationship between users' query multi-grained semantics and their personalized behaviors for prediction. (2) Graph-based baselines Â· GAT [36] utilizes the attention mechanism to measure the importance of neighbor nodes during the aggregation process. Â· NGCF [39] enhances the Graph Convolutional Networks (GCN) by incorporating user-item interactions. Â· LightGCN [14] streamlines GCN by relying solely on neighborhood aggregation to capture collaborative filtering, omitting feature transformation and non-linear activation components. Â· GraphSRRL [22] exploits three specific structural patterns within a user-query-item graph. Â· SRJGraph [47] incorporates padding queries for recommendation and search queries as attributes into interaction edges, enabling joint modeling of both tasks. Â· DCCF [29] leverages an adaptive self-supervised augmentation to disentangle intents behind user-item interactions. Specifically, NeuMF, xDeepFM, DIN, DCCF, SimpleX, NGCF and LightGCN are proposed for the recommendation task , while AEM, TEM, MGDSPR and GraphSRRL are proposed for the search task . JSR and SRJGraph are designed for joint learning of both tasks . To adapt these baselines for both tasks, real query representations for search and padding query representations for recommendation are incorporated into the prediction layer described in Section 4.4. Previous studies [44, 47] have demonstrated that joint optimization of search and recommendation models can improve performance, so all baselines are directly trained on both search and recommendation data. All baselines use the same settings for the embedding layer and the prediction layer, and the interaction graph is built on both search and recommendation interactions.",
  "5.2 Overall Performance Comparison (RQ1)": "We present the results on the two adopted datasets in Table 2. From the results, we can observe that: Â· UDITSR significantly outperforms all the competitive baselines on both tasks. Specifically, compared to the best-performing baselines, UDITSR gains an average improvement of 6.22% and 3.06% in the search and recommendation tasks, respectively. Â· Most graph-based methods, such as NGCF, LightGCN, and GraphSRRL, perform well in both tasks, potentially due to their ability to effectively capture complex high-order interactive patterns. Â· SRJgraph assumes that query-related intents in recommendation remain unchanging whereas in search, the matching degree between the query and the candidate items is deemed crucial. Unified Dual-Intent Translation for Joint Modeling of Search and Recommendation KDD '24, August 25-29, 2024, Barcelona, Spain Table 2: Overall performance on both datasets. â†“ represents that a smaller Avg.C metric value indicates better performance. Impr.% indicates the relative improvements of the best-performing method (bolded) over the strongest baselines (underlined). * indicates 0.05 significance level from a paired t-test comparing UDITSR with the best baselines. Consequently, such an assumption may limit the model's performance, particularly when compared to our UDITSR, which learns and adapts to changing query-related intents. Â· UDITSR(w/o DeIntGen & IntTrans) removes both the demand intent generator and dual-intent translation propagation, as described in the two ablation studies above. From the results of ablation studies in Table 3, we can find that: Â· UDITSR(w/o DeIntGen & IntTrans) performs the worst on both search and recommendation tasks, suggesting that the significant improvement of our model stems from our proposed demand intent generator and dual-intent translation propagation. Â· UDITSR(w/o IntTrans) performs worse than the original UDITSR, highlighting the effectiveness of our proposed intent translation propagation mechanism. Â· UDITSR(w/o DeIntGen) performs worse than UDITSR, especially for recommendation task, indicating that the search-supervised",
  "5.3 Ablation Study (RQ2)": "As the demand intent generator and dual-intent translation propagation are the core of our model, we conduct the following ablation studies to investigate their effectiveness: Â· UDITSR(w/o DeIntGen) masks all generated demand intents e e ğ‘ by assigning the embedding of the padding query to each recommended record. Â· UDITSR(w/o IntTrans) replaces dual-intent translation with classical mean-pooling propagation between the user and item nodes. KDD '24, August 25-29, 2024, Barcelona, Spain Yuting Zhang et al. Table 3: Ablation study on our proposed search-supervised demand intent generator and dual-intent translation propagation. search (a) Intents in UDITSR(w/o IntTrans) for search data (b) Inherent intents in UDITSR for search data (c) Translated intents in UDITSR for search data (d) Intents in UDITSR(w/o IntTrans) for recommendation data (e) Inherent intents in UDITSR for recommendation data (f) Translated intents in UDITSR for recommendation data 0.0 0.5 1.0 1.5 2.0 0.62 0.65 0.68 0.71 search 0.35 0.40 0.45 0.50 recommendation Hit@5 search recommend 0.0 0.5 1.0 1.5 2.0 0.46 0.50 0.54 0.58 0.24 0.28 0.32 0.36 recommendation NDCG@5 search recommend Figure 3: t-SNE visualization of learned intents and interactive items. Blue dots represent the interactive items(i.e., e âˆ— ğ‘– ) and orange dots represent the learned intents. demand intent generator can help UDITSR learn implicit intents more accurately in recommendation.",
  "5.4 Intent Visualization (RQ3)": "In this section, we visualize the learned intents to further investigate why our model performs better. We compare UDITSR with its ablated version without the dual-intent translation propagation (UDITSR(w/o IntTrans), detailed in Section 5.3). We employ the default setting of the t-SNE [9] provided by Scikit-learn to visualize the distribution of the learned intents and the interactive items. For clarity, we randomly sample 100 positive records from the search and recommendation test datasets respectively for plotting. Specifically, in UDITSR(w/o IntTrans), user embeddings (i.e., e âˆ— ğ‘¢ ) are regarded as the learned intents, as shown in Figures 3(a) and (d), similar to preference/intent captured by models like NGCF and LightGCN. UDITSR, however, couples inherent and demand intents via intent translation to form the final intents (i.e., e âˆ— ğ‘¢ + e e ğ‘ ), as shown in Figure 3(c) and (f). To ensure a fair comparison, we present the inherent intents (i.e., e âˆ— ğ‘¢ ) learned by UDITSR in Figure 3(b) and (e). Figure 4: Performance w.r.t ğœ† 1 of search-supervised demand intent generator for search and recommendation tasks. 0.0 0.2 0.4 0.6 0.8 1.0 0.68 0.69 0.70 0.71 search 0.47 0.48 0.49 0.50 recommendation Hit@5 search recommend Figure 5: Performance w.r.t ğœ† 2 of the intent translation contrastive learning for search and recommendation tasks. 0.0 0.2 0.4 0.6 0.8 1.0 0.545 0.555 0.565 0.575 search 0.34 0.35 0.36 0.37 recommendation NDCG@5 search recommend Ideally, the distribution of learned intents should match that of interactive item representations. Figures 3(a) and (d) reveal that the intents learned by UDITSR(w/o IntTrans) are concentrated while the positive interactive items are scattered, indicating a mismatch. Meanwhile, the inherent intents learned by UDITSR are relatively scattered, indicating that our model can better learn the personalized inherent intents of different users. However, there still exist obvious gaps between the intents and items, highlighting the necessity of learning demand intents. In contrast, the translated intents learned by UDITSR are scattered in the space of the target interactive items, demonstrating its excellent intent modeling capability. The better fit of the distribution of translated intents to the target interactive distribution could be the fundamental reason for the better overall performance of UDITSR.",
  "5.5 Online A/B test (RQ4)": "Owing to the distinct architectural differences between the search and recommender systems on the Meituan Waimai platform, we have initially focused our methodological deployment on the homepage recommender systems. We conducted a month-long online A/B test from December 18, 2023, to January 17, 2024. Specifically, Unified Dual-Intent Translation for Joint Modeling of Search and Recommendation KDD '24, August 25-29, 2024, Barcelona, Spain we utilized the search data with query information to guide the learning of user demand intent representation and leveraged the learned graph embeddings as additional features in the downstream recommendation model. The control bucket was the original online recommendation method of Meituan Waimai platform. The deployment of our method increased the GMV (Gross Merchandise Volume) by 1.46 % and CTR (Click-Through Rate) by 0.77% , which demonstrated the effectiveness of our method. In the future, we will continue to conduct comprehensive online experiments that encompass both search and recommendation scenarios.",
  "5.6 Hyper-Parameter Studies (RQ5)": "In this section, we conduct experiments on the loss weights ( ğœ† 1, ğœ† 2) in Eq. 11 on MT-Small dataset to explore their impact. (1) Loss weight of the demand intent generator ( ğœ† 1). We vary ğœ† 1 within { 0 , 0 . 5 , 1 . 0 , 1 . 5 , 2 . 0 } . The results in Figure 4 indicate that performance improves and then declines with increasing ğœ† 1. With ğœ† 1 = 0, the demand intent generator degenerates to an ordinary generator without any search-supervision information. All models with search supervision (i.e. ğœ† 1 â‰  0) outperform models without it (i.e. ğœ† 1 = 0). This may stem from UDITSR's effective learning of user demand intents through explicit supervision from search . Furthermore, our model excels across most metrics for both search and recommendation tasks at ğœ† 1 = 1 . 5. Thus, we set ğœ† 1 = 1 . 5 for MT-Small dataset. After a similar experiment conducted on MT-Large dataset, we adopt the best-performing setting ( ğœ† 1 = 1). (2) Loss weight of the intent translation contrastive learning ( ğœ† 2). To investigate the impact of our proposed intent translation contrastive learning, we vary ğœ† 2 in { 0 . 0 , 0 . 2 , 0 . 4 , 0 . 6 , 0 . 8 , 1 . 0 } . Overall, the performance initially increases and then decreases with the increase of ğœ† 2. Particularly, our model with ğœ† 2 set in {0.2, 0.4, 0.6} outperforms the version without translation contrastive learning ğœ† 2 = 0 on all metrics, demonstrating a proper loss weight of intent translation contrastive learning can aid in intent relation modeling . The optimal ğœ† 2 for search is 0.2 while for recommendation task, it is 0 . 2 for Hit@5 and 0 . 4 for NDCG@5. Therefore, we set ğœ† 2 = 0 . 2 for MT-Small dataset. Also, after conducting a similar experiment on MT-Large dataset, we adopt the best-performing setting ğœ† 2 = 0 . 4.",
  "6 CONCLUSION": "This paper introduced a novel approach to unified intention-aware modeling for joint optimization of search and recommendation tasks. We recognized that user behaviors were motivated by their inherent intents and changing demand intents. To accurately learn users' implicit demand intents for recommendation, we innovated a demand intent generator that utilized explicit queries from search data for supervised learning. Furthermore, we proposed a dualintent translation propagation mechanism for interpretive modeling of the relation between users' dual intents and their interactive items. In particular, we introduced an intent translation contrastive method to further constrain this relation. Our extensive offline experiments demonstrated that UDITSR outperformed the leading baselines in both search and recommendation tasks. Besides, online A/B tests further confirmed the superiority of our model. Finally, the intent visualization clearly explained the deeper reason for the remarkable improvement of our model.",
  "ACKNOWLEDGMENTS": "This research work is supported by the National Key Research and Development Program of China under Grant No.2021ZD0113602, the National Natural Science Foundation of China under Grant No.62176014 and No.62306255, the Fundamental Research Funds for the Central Universities and the Fundamental Research Project of Guangzhou under Grant No. 2024A04J4233.",
  "REFERENCES": "[1] Qingyao Ai, Daniel N Hill, SVN Vishwanathan, and W Bruce Croft. 2019. A zero attention model for personalized product search. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management . 379-388. [2] Nicholas J Belkin and W Bruce Croft. 1992. Information filtering and information retrieval: Two sides of the same coin? Commun. ACM 35, 12 (1992), 29-38. [3] Keping Bi, Qingyao Ai, and W Bruce Croft. 2020. A transformer-based embedding model for personalized product search. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . 1521-1524. [4] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi-relational data. Advances in neural information processing systems 26 (2013). [5] Tong Chen, Hongzhi Yin, Hongxu Chen, Rui Yan, Quoc Viet Hung Nguyen, and Xue Li. 2019. Air: Attentional intention-aware recommender systems. In 2019 IEEE 35th International Conference on Data Engineering (ICDE) . IEEE, 304-315. [6] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems . 7-10. [7] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks for youtube recommendations. In Proceedings of the 10th ACM conference on recommender systems . 191-198. [8] Tyler Derr, Yao Ma, and Jiliang Tang. 2018. Signed graph convolutional networks. In 2018 IEEE International Conference on Data Mining (ICDM) . IEEE, 929-934. [9] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. 2014. Decaf: A deep convolutional activation feature for generic visual recognition. In International conference on machine learning . PMLR, 647-655. [10] Lu Fan, Qimai Li, Bo Liu, Xiao-Ming Wu, Xiaotong Zhang, Fuyu Lv, Guli Lin, Sen Li, Taiwei Jin, and Keping Yang. 2022. Modeling user behavior with graph convolution for personalized product search. In Proceedings of the ACM Web Conference 2022 . 203-212. [11] Yufei Feng, Fuyu Lv, Weichen Shen, Menghan Wang, Fei Sun, Yu Zhu, and Keping Yang. 2019. Deep session interest network for click-through rate prediction. In Proceedings of the 28th International Joint Conference on Artificial Intelligence . 2301-2307. [12] Cesar Ferri, JosÃ© HernÃ¡ndez-Orallo, and Peter A Flach. 2011. A coherent interpretation of AUC as a measure of aggregated classification performance. In Proceedings of the 28th International Conference on Machine Learning (ICML-11) . 657-664. [13] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. In Proceedings of the 26th International Joint Conference on Artificial Intelligence . 1725-1731. [14] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for recommendation. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval . 639-648. [15] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural collaborative filtering. In Proceedings of the 26th international conference on world wide web . 173-182. [16] Yifan Hu, Yehuda Koren, and Chris Volinsky. 2008. Collaborative filtering for implicit feedback datasets. In 2008 Eighth IEEE international conference on data mining . Ieee, 263-272. [17] Kalervo JÃ¤rvelin and Jaana KekÃ¤lÃ¤inen. 2002. Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems (TOIS) 20, 4 (2002), 422-446. [18] Thomas N Kipf and Max Welling. 2016. Semi-Supervised Classification with Graph Convolutional Networks. In International Conference on Learning Representations . KDD '24, August 25-29, 2024, Barcelona, Spain Yuting Zhang et al. [50] Nengjun Zhu, Jian Cao, Yanchi Liu, Yang Yang, Haochao Ying, and Hui Xiong. 2020. Sequential modeling of hierarchical user intention and preference for next-item recommendation. In Proceedings of the 13th international conference on web search and data mining . 807-815.",
  "keywords_parsed": [
    "Joint learning",
    " Search and recommendation",
    " Dual intent modeling",
    " Intent translation"
  ]
}