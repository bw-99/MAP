{
  "Reciprocating Locks": "Listing 9. Reciprocating Locks - Alternative '2 Lanes' Formulation - Imposes Long-Term Fairness 253 // We just detached ourselves : &E -- both alfa and omega 254 // Thus no new arrivals 255 // Appears to be uncontended fast-path release with no pending waiters 256 // Release Leader Lock 257 assert (Grant.load() == tx) ; 258 Grant.store (tx+1, std::memory_order_release) ; 259 } 260 } 261 } ; 2025-06-12 Â· Copyright Oracle and or its affiliates",
  "Dave Dice": "",
  "Alex Kogan": "dave.dice@oracle.com Oracle Labs USA",
  "Abstract": "We present Reciprocating Locks , a novel mutual exclusion locking algorithm, targeting cache-coherent shared memory (CC), that enjoys a number of desirable properties. The doorway arrival phase and the Release operation both run in constant-time. Waiting threads use local spinning and only a single waiting element is required per thread, regardless of the number of locks a thread might hold at a given time. While our lock does not provide strict FIFO admission, it bounds bypass and has strong anti-starvation properties. The lock is compact, space efficient, and has been intentionally designed to be readily usable in real-world general purpose computing environments such as pthreads, or C++. Weshowthe lock exhibits high throughput under contention and low latency in the uncontended case. Under sustained contention, Reciprocating Locks generate less coherence traffic than MCS and CLH. The performance of Reciprocating Locks is competitive with and often better than the best state-of-the-art scalable queue-based spin locks. CCS Concepts: Â· Software and its engineering â†’ Multithreading ; Mutual exclusion ; Concurrency control ; Process synchronization . Keywords: Synchronization; Locks; Mutual Exclusion; Mutex; Scalability; Cache-coherent Shared Memory",
  "1 Introduction": "Locks often have a crucial impact on the performance of parallel software, hence they remain the focus of intensive research with a steady stream of algorithms proposed over the last several decades. Reciprocating Locks was motivated by the desire for a lock algorithm that scales well under contention, but also avoids the various entanglements of 'queue node' lifecycle concerns that manifest under current state-of-the-art locks such as CLH[16, 42] and MCS[46]. Such concerns can make those locks a challenge to integrate into real-world software. We also wanted constant-time arrival and release paths, but were willing to forego strict FIFO admission order. Finally, we were motivated to design an architecturally informed lock that works well with recent developments in modern NUMA cache-coherent communication fabrics. Common lock algorithms include Ticket Locks, MCS and CLH. Ticket Locks [31, 45, 50] are simple and compact, requiring just two words for each lock instance and no perthread data. They perform well in the absence of contention, alex.kogan@oracle.com Oracle Labs USA exhibiting low latency because of short code paths. Under contention, however, performance suffers [22] because all threads contending for a given lock will busy-wait on a central location, increasing coherence costs. For contended operation, so-called queue based locks, such as CLH[16, 42] and MCS[46] provide relief via local spinning [28]. For both CLH and MCS, arriving threads enqueue an element (sometimes called a 'node') onto the tail of a queue and then busy-wait on a flag in either their own element (MCS) or the predecessor's element (CLH). Critically, at most one thread busy-waits on a given location at any one time, increasing the rate at which ownership can be transferred from thread to thread relative to techniques that use global spinning, such as Ticket Locks.",
  "2 The Reciprocating Lock Algorithm": "Briefly, under contention, Reciprocating Locks 1 partitions the set of waiting threads into two disjoints lists, which we call the arrival and entry segments. Threads arriving to acquire the lock will push (prepend) themselves onto a stack, using an atomic exchange operation, forming the arrival segment . When the owner releases the lock, it first tries to pass ownership to any threads found in the entry segment . Otherwise, if the entry segment is found empty, the thread then uses an atomic exchange to detach the entire current arrival segment (setting it empty), which then becomes the next entry segment, and then passes ownership to the first element of the entry segment 2 . In Reciprocating Locks, a lock instance consists of an arrival word. We deem the lock held if the arrival word is non-zero. Specifically, a value of 0 ( nullptr ) encodes the unlocked state, 1 ( LOCKEDEMPTY ) encodes the state of simple locked - locked with an empty arrival segment - and other values encode being locked, where the remainder of the arrival word points to a stack of threads that have recently arrived at the lock and are waiting for admission, forming the arrival segment. Threads arriving to acquire the lock use an atomic swap (exchange) operator to install the address of a thread-private waiting element into the arrival word. If the return value from the atomic exchange was nullptr then the arriving thread managed to acquire the lock without contention and can immediately enter the critical section. Otherwise, our thread has encountered contention and must wait. By virtue 1 PPoPP 2025 Conference Version: https://doi.org/10.1145/3710848.3710862 2 An implementation can be found at https://github.com/pabuhr/concurrent- locking/blob/master/Reciprocating.c Alternative names : reciprocating; palindrome; Retrograde : ap Segmented; push-swap; pop-stack; LIFO; FCS = First-come so tmetic lock; tomos; tomic; Candi ate names : finalist best candi : retrograde reciprocating ates : Retrograde : ap FLIFO FCS arent motion of stars reciprocating bifurcated cleft or cloven Segmented push-swap = First-come so n served boustrophedonic PD = Push-Detach tmesi switchback alternatives respiratory : inhale- xhale two-stroke backtracking piston deranged schisma split list dichot bi-l st cleaved list discursive dual-segment or dual- ist disjointed LIFO-FI / tmetic lock tomos = cut omic = mized cleft O punctuated punctual schism split; fractured; parti oned pushlock; alternating; tog le; up-down yo-yo regres ive Violin Bow motion; sawing; sawto th; ratchet regres ive levator palindrome; contrapuntal eb -flow; back-forth; to-fro; cancrine; contra ian zig-zag leap/jump/hop forward walk backward then leap forward obverse arent motion of stars ; bifurcated; cleft or cloven; n served; boustrophedonic; push-detach; tmesi or 2025-06-12 Â· Copyright Oracle and or its affiliates Dice and Kogan of the atomic exchange, the thread has managed to push its waiting element onto the arrival segment. The nonnullptr value returned from the atomic exchange identifies the next thread in the stack. Similar to the HemLock[26] and CLH[16, 42] lock algorithms, a thread knows only the identity of its immediate neighbor in the arrival segment, and unlike MCS, no explicit linked list of waiting threads is formed or required 3 . That is, the arrival stack is implicit with no next pointer fields in the waiting elements. Our thread then proceeds to local spinning on a flag field within its waiting element. This flag will eventually be set during normal ownership succession by some thread running in the Release operation, passing ownership to our waiting thread. Our thread, still executing in Acquire and now the owner, arranges to convey the address of the next thread in the entry segment, which was obtained from the atomic exchange, to the subsequent corresponding Release operation. The thread identified by that address will subsequently serve as the successor to our current thread. Our thread finally enters the critical section. In the corresponding Release operator, if a successor was passed from the corresponding Acquire operator, we simply enable that thread to enter the critical section by setting the flag in its waiting element. Otherwise, we attempt to use an atomic compare_and_exchange (CAS) operation to swing the arrival word from simple locked state back to unlocked . If the CAS was successful then no waiting threads exist and the lock reverts to unlocked state. If the CAS failed, however, additional newly arrived threads must be present on the arrival segment. In that case we employ an atomic exchange to detach the entire arrival segment, leaving the arrival word in simple locked state, encoded as 1. We then pass lock ownership to the first thread in the detached segment by setting the flag in its waiting element. Crucially, under contention, threads arrive and join the arrival segment. While the entry segment remains populated, ownership is passed through the entry segment elements in turn. When the current entry segment becomes empty, the Release operator detaches the arrival segment, (via an atomic exchange) which then becomes the next entry segment. Threads migrate, in groups, from the arrival segment to the entry segment. The arrival segment consists of those newly arrived threads currently pushed onto the stack anchored at the arrival word while entry segment reflects a set of threads that have already been detached from the arrival stack. The Release operator consults the entry segment first via the successor reference passed from Acquire to Release - and passes ownership to the successor if possible. The sequence of successor references passed from Acquire to 3 Regarding terminology, say thread ğ´ arrives and pushes itself onto the stack and then ğµ follows. In terms of arrival, ğ´ is ğµ 's predecessor . But in terms of subsequent admission order, in Reciprocating Locks, ğ´ is ğµ 's successor . There is no such situational distinction between arrival and admission for FIFO Release constitutes the entry segment. But if the entry segment is empty - the passed successor argument is nullptr -Release then attempts to replenish the entry segment by detaching the arrival segment, and transferring ownership to the first element. In the event the arrival segment is found empty, the lock reverts to unlocked state. The arrival segment is implemented by means of a concurrent pop-stack [9], where the key primitives are push and detach-all , which makes our technique immune to the A-B-A pathology [54]. By convention, in Reciprocating Locks, only the current lock holder detaches the arrival segment. The waiting element is similar to the CLH or MCS 'queue node'. In our implementation, we opt to place a thread's wait element in thread-local storage (TLS). As a thread can wait on at most one lock at any given time, such a singleton suffices, and tightly bounds memory usage. Given that we form a stack for arriving threads, admission order is LIFO within a segment, but remains FIFO between segments. As such, if thread ğ‘‡ 1 pushes itself onto the arrival segment in Acquire , and then waits, and ğ‘‡ 2 arrives and pushes itself after ğ‘‡ 1, then a given thread ğ‘‡ 2 can bypass or overtake ğ‘‡ 1 at most once before ğ‘‡ 1 is next granted ownership, providing thread-specific bounded bypass and thus avoiding indefinite starvation. Alternatively, we could say Reciprocating Locks provides classic ğ¾ -bounded bypass (worst case) where ğ¾ reflects the cardinality of the population of threads that might compete for the lock, yielding population bounded bypass .",
  "3 Implementation Details": "In Listing-1 we show an implementation of Reciprocating Locks in modern C++. For brevity, this version expresses the critical section as a C++ lambda expression and passes the variables succ and eos from the acquire phase to the corresponding release phase. If desired, it is possible to collapse those into just one variable by a change in encoding. Under a legacy locking interface, with distinct lock() and unlock() operators, context may be passed via extra fields in the lock body or conveyed via thread-local storage. Nonescaping lambdas are efficient and add no particular runtime overhead 4 . The assert statements in the listing express invariants and do not constitute checks against errant usage of the lock. We also assume the existence of a 'polite' Pause() operator for busy-waiting. For encoding the arrival word, we assume that low-order bit of wait element addresses are 0. We further assume the existence of a wait-free atomic exchange operator. To maintain progress properties, the implementation thereof should not be via loops that employ optimistic compare-and-swap or load-locked and store-conditional primitives. In particular we assume that C++ std::atomic 4 Example usage : ReciprocatingLock L {}; int v = 5; L + [&]{ v += 2;} ; Some Confusion and variations for definit on of 'bounde bypas '. * Wik pedia : Bounde means that he number of times a proces wait ng, or bounde bypas , is bypas ed by another proces after it has indicated its desire to enter the crit cal section is bounde by a function of the number of proces es in the sy tem. * Variations @ bypas - by proces es number of incidences of overtaking any any proces by any proces @bypas by any one specific - by a proces of incidences of overtaking by one proces * terminol gy : bounde fre dom * Keywords : bypas ; overtake; starvation; bounde finite wait ng; number bypas vs bounde wait ng * Raynal : starvation bypas ; livenes ; progres ; overtaking lockout-fre dom; 2025-06-12 Â· Copyright Oracle and or its affiliates Reciprocating Locks Listing 1. Reciprocating Lock Algorithm 1 struct ReciprocatingLock { 2 struct WaitElement { 3 std::atomic<WaitElement *> Gate alignas(128) { nullptr } ; 4 } ; 5 static inline const auto LOCKEDEMPTY = 6 (WaitElement *) uintptr_t(1) ; 7 // Encoding Arrivals field : 8 // 0:0 = Unlocked 9 // 0:1 = Locked with empty arrival list : LOCKEDEMPTY 10 // T:1 = Locked with populated arrival list where T is the 11 // most recently arrived thread on the stack 12 std::atomic <WaitElement *> Arrivals { nullptr } ; 13 public : 14 inline auto operator+ (std::invocable auto && csfn) â†’ void { 15 // Acquire Phase ... 16 static thread_local WaitElement E alignas(128) { nullptr } ; 17 E.Gate.store ( nullptr , std::memory_order_release) ; 18 WaitElement * succ = nullptr ; 19 auto eos = &E ; // anticipate fast-path 20 auto tail = Arrivals.exchange (&E) ; 21 assert (tail â‰  &E) ; 22 if (tail â‰  nullptr ) { 23 // Coerce LOCKEDEMPTY to nullptr 24 // succ will be our successor when we subsequently release 25 succ = (WaitElement *) (uintptr_t(tail) & ~1) ; 26 assert (succ â‰  &E) ; 27 28 // Contention -- waiting phase ... 29 for (;;) { 30 eos = E.Gate.load(std::memory_order_acquire) ; 31 if (eos â‰  nullptr ) break ; 32 Pause() ; 33 } 34 assert (eos â‰  &E) ; 35 36 // Detect logical end-of-segment sentinel marker address 37 if (succ == eos) { // terminus ? 38 succ = nullptr ; // quash ! 39 eos = LOCKEDEMPTY ; 40 } 41 } 42 43 // Critical section expressed as lambda 44 // pass \"succ\" and \"eos\" as context 45 assert (eos â‰  nullptr ) ; 46 assert (Arrivals.load() â‰  nullptr ) ; 47 csfn() ; 48 assert (Arrivals.load() â‰  nullptr ) ; 49 50 // Release Phase ... 51 // Case : entry list populated 52 // appoint successor from entry list 53 if (succ â‰  nullptr ) { 54 assert (eos â‰  &E && succ â†’ Gate.load() == nullptr ) ; 55 // Normal contended succession through entry segment 56 // Enable successor _and propagate identity of eos 57 // toward the tail of the segment 58 succ â†’ Gate.store (eos, std::memory_order_release) ; 59 return ; 60 } 61 62 // Case : entry list and arrivals are both empty 63 // try fast-path uncontended unlock 64 assert (eos == LOCKEDEMPTY || eos == &E) ; 65 auto v = eos ; 66 if (Arrivals.compare_exchange_strong (v, nullptr )) return ; 67 68 // Case : entry list is empty and arrivals is populated 69 // New threads have arrived and pushed themselves onto the 70 // arrival stack. 71 // We now detach that segment, shifting those new arrivals 72 // to become the next entry segment 73 auto w = Arrivals.exchange (LOCKEDEMPTY) ; 74 assert (w â‰  nullptr && w â‰  LOCKEDEMPTY && w â‰  &E) ; 75 assert (w â†’ Gate.load() == nullptr ) ; 76 w â†’ Gate.store (eos, std::memory_order_release) ; 77 } 78 } ; exchange and compare_and_exchange primitives are implemented in a wait-free fashion, as is the case on AMD or Intel x86 processors or ARM processors that support the LSE instruction subset.",
  "4 Execution Scenarios": "To further explain the operation of Reciprocating Locks we next annotate key scenarios, showing Reciprocating Locks in action. â–¶ Simple uncontended Acquire and Release : 1 Thread ğ‘‡ 1 arrives at Line-14 to acquire lock ğ¿ . ğ¿ 's arrival word is currently nullptr , indicating that ğ¿ is in unlocked state. 2 At Line-17, ğ‘‡ 1 initializes its thread-specific waiting element, ğ¸ , in anticipation of potential contention. 3 ğ‘‡ 1 then swaps the address of ğ¸ into ğ¿ 1's arrival word in Line-20. The atomic exchange returns nullptr , indicating uncontended acquisition, so control passes into the critical section at Line-47. When the critical section completes execution, control enters the release phase at Line-50. 4 As there are no successors, execution reaches the CAS operation at Line-66. As no additional threads have arrived, the CAS is successful and control returns from Line-66, completing the operation. The CAS reverts the arrival to unlocked state. â–¶ Onset of contention In this scenario we show how to recover from a race in Acquire where a thread pushes its wait element onto the stack but, because of other arriving threads, is not able, in the Release operation, to CAS the arrival word back to 1, and its element becomes 'submerged' on the arrival stack. We tolerate this situation by conveying the address of that submerged element through the segment, during succession, allowing us to treat the buried element as the effective end-of-segment (equivalent to nullptr ) and otherwise ignore it. We convey that address through the wait element Gate field, when passing ownership. In this case we say we have a zombie terminal element. During succession a thread checks the address of its successor to determine if matches the submerged terminal element. 1 Lock ğ¿ is in unlocked state and thread ğ‘‡ 1 arrives and acquires the lock. ğ‘‡ 1 executes the atomic exchange at Line-20 to install the address of its wait element ğ¸ , which we will designate ğ¸ 1, into ğ¿ 's arrival word, the exchange returns 0 ( nullptr ), so ğ‘‡ 1 now holds the lock. ğ‘‡ 1 enters the critical section. 2 Thread ğ‘‡ 2 now arrives to acquire ğ¿ and exchanges the address of its wait element, ğ¸ 2 into the lock's arrival word. ğ‘‡ 2's successor is ğ‘‡ 1 and ğ‘‡ 2 enters the waiting loop at Line-30. 3 Thread ğ‘‡ 3 also arrives and pushes the address of its wait element, ğ¸ 3, onto the arrival stack. ğ‘‡ 3, whose successor is ğ‘‡ 2, enters the waiting loop at Line-30. 4 ğ‘‡ 1 finishes execution of the critical section and then starts the Release operation. At Line-66, ğ‘‡ 1 attempts uses a CAS to try to release ğ¿ replacing the address of its ğ¸ 1 with nullptr encoding. The CAS fails, however, as other threads have Explicate, xplain, expound, il ustrate, demonstrate, show, il uminate Nar ative, an ota ed scenario, flow, example, depict Reciprocating Locks in action remedy, mit gate, recover, tolerate, logical, effective, tantamount 2025-06-12 Â· Copyright Oracle and or its affiliates Dice and Kogan arrived, and the arrival word now points to ğ¸ 3 instead of ğ¸ 1. At this point, ğ‘‡ 1's wait element ğ¸ 1 is 'buried' or 'submerged' in the arrival stack, residing at the distal end. 5 As the CAS failed, control reaches Line-73 where ğ‘‡ 1 detaches the new arrivals. The atomic exchange replaces ğ¸ 3 with a special LOCKEDEMPTY distinguished value, which indicates the lock is held but that the arrival list is empty and has been previously detached. 6 At Line-76, ğ‘‡ 1 stores its eos (end-of-segment) address, which refers to ğ¸ 1, into ğ¸ 3's Gate field. This both enables ğ‘‡ 3 and conveys the address of ğ¸ 1 to ğ‘‡ 3. ğ‘‡ 1 has completed is locking episode. 7 ğ‘‡ 3, at Line-30, observes that its Gate field is now ğ¸ 1. Its local eos variable now refers to ğ¸ 1. ğ‘‡ 3 now has ownership. The comparison at Line-37 does not match, so ğ‘‡ 3 enters the critical section. 8 ğ‘‡ 3 executes the critcal section. Upon return from the critical section, ğ‘‡ 3 at Line-53, recognizes that it has a successor, ğ‘‡ 2 ( ğ¸ 2), on the detached entry segment. At Line-58, ğ‘‡ 3 stores its eos value, ğ¸ 1, into ğ¸ 2's Gate field. The store conveys both the identity of the end-of-segment and ownership. ğ‘‡ 3 has finished its locking episode and returns at line-59. 9 ğ‘‡ 2 observes at Line-30 that is has received ownership, and its eos value is now ğ¸ 1. ğ‘‡ 2's indicated successor is ğ‘‡ 1 ( ğ¸ 1) and the comparsion at Line-37 matches, so we have reached the end of the detached entry segment. ğ‘‡ 2 then clears its local succ successor pointer and sets its local eos value to LOCKEDEMPTY . 10 ğ‘‡ 2 enters and executes the critical section. 11 Control in ğ‘‡ 2 reaches line-66, where the CAS succeeds and swings the arrival word from LOCKEDEMPTY to unlocked . We note that a contended Release operation might need to execute two atomic operations, the CAS at Line-66 and the exchange at Line-76, potentially increasing RMR complexity [4, 7]. In practice, given the narrow window, the underlying cache line tends remain to local modified state. If desired, to mitigate this concern, we could condition the CAS on an immediate prior load, reducing the number of futile CAS operations. We found this optimization provided no observable benefit, and did not use it. In the case where ğ¸ is submerged, we use ğ¸ 's address for addressed-based comparisons (Line-37) as a distinguished marker or sentinel to indicate the logical end-of-segment. ğ¸ itself, however, will not be subsequently accessed by succession within the segment. Elements associated with a given thread can appear on at most one segment at any time, but, when an address is used as an end-of-segment marker, it is possible that it appears on both the arrival segment and entry segment. We observe that the eos value could also reside in a field the lock body, potentially sequestered and isolated on a dedicated cache line, instead of in the waiting elements. While viable, that approach increases the size of the lock body, and increases induced coherence traffic. Instead, we borrow a technique from Compact NUMA-Aware Locks (CNA) [21] and avoid such shared central fields by propagating information - in this case the address of the terminal element of the segment - through the chain of waiting elements. â–¶ Sustained contention 1 Lock ğ¿ is initially in unlocked state. Thread ğ‘‡ 1 arrives to acquire ğ¿ . ğ‘‡ 1's exchange operation at Line-20 installs the address of ğ‘‡ 1's wait element, ğ¸ 1, into ğ¿ 's arrival word. As the exchange returned nullptr , ğ‘‡ 1 has acquired the lock. 2 ğ‘‡ 1 enters and executes the critical section. 3 While ğ‘‡ 1 holds ğ¿ , thread ğ‘‡ 2 arrives and ğ¸ 2 pushes onto the arrival stack. The exchange operation at Line-20 returns ğ¸ 1 into ğ‘‡ 2's local tail variable. As tail is non-0, ğ‘‡ 2 must wait at Line-29. ğ‘‡ 2 local succ variable refers to ğ¸ 1. 4 With ğ‘‡ 1 still holding ğ¿ , ğ‘‡ 3 also arrives and uses the atomic exchange to push its element ğ¸ 3 onto the arrival stack. The exchange returns ğ¸ 2 and ğ‘‡ 3's succ variable points to ğ¸ 2. The arrival stack consists of ğ¸ 3 followed by ğ¸ 2 followed by ğ¸ 1, although ğ¸ 1 is 'buried' and will be excised during subsequent succession. The entry segment is empty. ğ‘‡ 3 waits on ğ¸ 3 at Line-30 5 Similarly, ğ‘‡ 4 arrives and pushes ğ¸ 4 onto the arrival stack and then waits. ğ‘‡ 4's successor variable points to ğ¸ 3. 6 ğ‘‡ 1 eventually completes the critical section and then executes the Release phase at Line-50. As ğ‘‡ 1's succ variable is nullptr , indicating an empty entry segment, ğ‘‡ 1 then attempts the CAS, at Line-66 which fails. ğ‘‡ 1 then executes exchange( LOCKEDEMPTY ) to detach the arrival segment at Line-73. The exchange operator returns ğ¸ 4. The arrival segment is now empty and the entry segment consists of ğ¸ 4 then ğ¸ 3 then ğ¸ 2 then ğ¸ 1. ğ‘‡ 1 passes ownership to ğ‘‡ 4 at Line-76, passing the address of ğ¸ 1 as the end-of-segment marker. 7 ğ‘‡ 4 is now the owner and departs its waiting phase at Line-31, having received ğ¸ 1 as the end-of-segment address. The end-of-segment address check at Line-37 does not match, so ğ‘‡ 4 enters the critical section. 8 while ğ‘‡ 4 holds ğ¿ , ğ‘‡ 5 and then ğ‘‡ 6 arrive to acquire ğ¿ , pushing ğ¸ 5 and then ğ¸ 6, respectively, onto the arrival stack. The arrival segment consists of ğ¸ 6 then ğ¸ 5 and the detached entry segment is just ğ¸ 3 and ğ¸ 2 and ğ¸ 1. When ğ‘‡ 5 arrived, its exchange replaced LOCKEDEMPTY with ğ¸ 5. Crucially, Line-25 converted LOCKEDEMPTY , which is encoded as 1, to a effective successor value of nullptr . 9 ğ‘‡ 4 releases the lock. As ğ¸ 3 is ğ‘‡ 4's the successor, we grant ownership to ğ‘‡ 3 at Line-58, again passing ğ¸ 1 as the end-of-segment address. 10 ğ‘‡ 3 departs its wait phase at Line-31 and enters and executes the critical section. 11 ğ‘‡ 3 releases the lock, observing ğ¸ 2 as its successor and grants ownership to ğ‘‡ 2 at Line-58, again, passing ğ¸ 1 as the end-of-segment, 12 ğ‘‡ 2 departs its wait phase at line-31. In this particular step, the address check at Line-37 matches, and ğ‘‡ 2 annuls its succ variable and sets its eos to LOCKEMPTY . We have reached the end of the entry segment, which is now exhausted. ğ‘‡ 2 enters and executes the critical section. 13 ğ‘‡ 2 executes the release phase, and since succ is nullptr at Line-53, the current entry segment is empty. ğ‘‡ 2 then attempts the CAS at line 66, Terminus, end-of-segment, eos, marker, sentinel, ghost, zombie, alternative end mark, tombstone, section mark; pilcrow, fleuron, hed ra, dinkus, -30- Eos; end-of-segment; erminus; Vesper; Astraeus; fini; Boustrophedonic UB; Nas l Demons; halt-and-catch-fire locks diverge from ac demic locks nd, Vesper, punctuation, virtual, phantom 2025-06-12 Â· Copyright Oracle and or its affiliates Reciprocating Locks practical; pragmatic; de facto; QoI = quality of implementa ion, desiderat multi udes; plural ocking which tries to replace LOCKEDEMPTY with unlocked . The CAS fails as the arrival segment is populated with ğ‘‡ 6 and ğ‘‡ 5 and the arrival word points to ğ¸ 6. ğ‘‡ 2 then detaches the new arrivals at Line-73 and passes ownership to ğ‘‡ 6 (via ğ¸ 6) at Line-76, and also conveying the end-of-segment address, which is now LOCKEDEMPTY , to ğ‘‡ 6. 14 ğ‘‡ 6 exits the waiting loop at Line-30, noting that eos is now LOCKEDEMPTY . ğ‘‡ 6's successor is ğ‘‡ 5. ğ‘‡ 6 enters the critical section. 15 ğ‘‡ 6 completes the critical section and executes the release phase. As ğ‘‡ 6's succ variable refers to ğ‘‡ 5, we pass ownership and the end-of-segment address ( LOCKEDEMPTY at this juncture) to ğ‘‡ 5. 16 ğ‘‡ 5 recognizes ownership at Line-30 and then enters the critical section at Line-47. 17 ğ‘‡ 5 releases the lock. It's succ value is nullptr to it the attempts the CAS at Line-66. In this case the CAS succeeds, replacing LOCKEDEMPTY with the unlocked encoding of nullptr . ğ¿ is restored to unlocked state.",
  "5 Additional Requirements": "We target lock algorithms that are suitable for environments such as the Linux kernel, as a replacement for the user-level pthreads_mutex primitive, or for use in runtime environments such as the HotSpot Java Virtual Machine (JVM), which is written in C++. As such, we identify additional de-facto requirements for general purpose lock algorithms. Safe Against Prompt Lock Destruction Various lock algorithms perform stores in the Release operation that potentially release ownership, but then perform additional accesses to fields in the lock body to ensure succession and progress. This can result in a class of unsafe use-after-free memory reference errors [13, 47] if a lock is used to protect its own existence. A detailed description of such an error can be found in [26]. All algorithms for use in the linux kernel or in the pthreads environment are expected to be prompt lock destruction safe, constituting a de facto requirement. Support for Large Numbers of Extant Threads The algorithms must support the simultaneous participation of an arbitrary number of threads, where threads are created and destroyed dynamically, and the peak number of extant threads is not known in advance. Support for Large Numbers of Extant Locks Similarly, the algorithm needs to support large numbers of lock instances, which can be created and destroyed dynamically. Recent work [40] shows the Linux kernel has more than 6000 statically initialized lock instances. Again, the number of locks is not know in advance, as drivers load, and unload, for instance, or as data structures containing locks dynamically resize. Plural Locking A given thread is expected to be able to lock and hold a large number of locks simultaneously. In the Linux kernel, for instance, situations arise when 40 or more distinct locks are held by a single thread at a given time, as evidenced by the MAX_LOCK_DEPTH tunable, which is used by the kernel's 'lockdep' facility to track the set of locks held in an explicit per-thread list for the purposes of detecting potential deadlock. Furthermore, locks must be able to be released in non-LIFO imbalanced order as it is fairly common to acquire a lock in one routine, return, and then release the lock in the caller. Space Efficient We expect the lock algorithm to be frugal and parsimonious with regard to space usage. Critically, any algorithms that require per-lock and per-thread storage - with ğ‘‡â„ğ‘Ÿğ‘’ğ‘ğ‘‘ğ‘  âˆ— ğ¿ğ‘œğ‘ğ‘˜ğ‘  space consumption - such as Anderson's Array-Based Queue Lock [6], are not suitable. FIFO as a non-goal We also note that many real-world lock implementations, such as the current implementation of Java's 'synchronized' in the HotSpot Java Virtual Machine, the java.util.Concurrent.ReentrantLock , and the default pthread_mutex on Linux, are non-FIFO, and in fact permit unbounded bypass and indefinite starvation. Relatedly, Compact NUMA-Aware Locks (CNA) [21] intentionally imposes non-FIFO admission order to improve throughput on NUMA platforms, trading aggregate throughput against strict FIFO ordering. 5 . In fact, strict FIFO locking is an antipattern for common practical lock designs, as it suffers from reduced throughput compared to more relaxed admission schedules, and has shortcomings when used with waiting techniques that deschedule threads, or when involuntary preemption is in play [18, 20, 23, 24]. Forgoing strict FIFO admission schedules allows more opportunism that in some circumstances may translate into improved throughput. Work Conserving We desire that the lock is work conserving and avoids the use of backoff delays, which can otherwise constitute 'dead time'. Given these particular constraints, we exclude a number of algorithms from consideration. Dvir's algorithms [29], Rhee's algorithm [51], Lee's HL1 and HL2 [38, 39], and Jayanati's 'Ideal Queued Spinlock' [36] [37] algorithms, when simplified for use in cache-coherent (CC) environments, all have extremely simple and elegant paths and low RMR complexity, suggesting they would be competitive and good candidates, but they do not readily tolerate multiple locks being held simultaneously. The above tend to use so-called 'nodetoggling' and 'node-switching' techniques - also called 'two-face' by Lee - that, while they work well when a thread holds at most one lock, are awkward for general purpose use. Our specific concerns are space blow-up due to the need to maintain toggle element pairs and a 'face' index for pairs of locks and threads, in addition to the requirement that we re-associate that metadata with the lock instance at releasetime. Some of Dvir and Lee's algorithms are also not safe against prompt lock destruction. 5 Even if a lock has strict FIFO admission order, otherwise identical threads can still suffer from persistent long-term unfairness related to shared cache residency imbalance, reflecting the 'Matthew Effect'[25]. 2025-06-12 Â· Copyright Oracle and or its affiliates Dice and Kogan",
  "6 Related Work": "While mutual exclusion remains an active research topic [1-3, 16, 19-22, 28-30, 34-37, 46, 49, 52] we focus on locks closely related to our design. Simple test-and-set or polite test-and-test-and-set [52] locks are compact and exhibit excellent latency for uncontended operations, but fail to scale and may allow unfairness and even indefinite starvation. Ticket Locks are compact and FIFO, and also have excellent latency for uncontended operations, but they also fail to scale because of global spinning, although some variations attempt to overcome this obstacle, at the cost of increased space [22, 48]. For instance, Anderson's array-based queueing lock [5, 6] is based on Ticket Locks but provides local spinning. It employs a waiting array for each lock instance, sized to ensure there is at least one array element for each potentially waiting thread, yielding a potentially large footprint. The maximum number of participating threads must be known in advance when initializing the lock. TWA[22] is a variation on ticket locks that reduces the incidence of global spinning. Queue-based locks such as MCS or CLH are FIFO and provide local spinning and are thus more scalable. MCS is used in the linux kernel for the low-level 'qspinlock' construct [14, 15, 41]. Modern extensions of MCS edit the queue order to make the lock NUMA-Aware [21]. MCS readily allows editing and re-ordering of the queue of waiting threads, [19, 21, 43] whereas editing the chain is more difficult under CLH, HemLock and Reciprocating Locks, where there are no explicit linked lists. CLH is extremely simple, has excellent RMR complexity, and requires just an single atomic exchange operation in the Acquire operation and no atomic read-modify-write instructions in Release . Unfortunately the waiting elements migrate between threads, which may be inimical to performance in NUMA environments. CLH locks also require explicit constructors and destructors, which may be inconvenient 6 . Our specific implementation uses a variation on Scott's [52] Figure 4.14, which converts the CLH lock to be context-free [56], adhering to a simple programming interface that passes just the address of the lock, albeit at the cost of adding an extra field to the lock body to convey the address of the head waiting element to Release . The K42 [8, 52] variation of MCS can recover the queue element before returning from Acquire whereas classic MCS 6 Many lock implementations require just trivial constructors to set the lock fields to 0 or some constant, and trivial destructors, which do nothing. The Linux kernel spinlock_t / qspinlock_t interface provides a constructor, but does not even expose a destructor. Similarly, C++ std::mutex is allowed to be trivially destructible , meaning storage occupied by trivially destructible objects may be reused without calling the destructor. Under both GCC g++ version 13 and Clang++ Version 18, is_trivially_destructible reports True for std::mutex , and as such, destructors do not run. This situation could be rectified by recompiling, but we would also need to recompile all libraries transitively depended on by the application, including those deployed in shared libraries in binary-only form on the system. recovers the queue element in Release . That is, under K42, a queue element is needed only while waiting but not while the lock is held, and as such, queue elements can always be allocated on stack, if desired. While appealing, the paths are much more complex and touch more cache lines than the classic version, impacting performance. In addition, neither the doorway nor the Release path operate in constant time. HemLock combines aspects of both CLH and MCS to form a lock that has very simple waiting node lifecycle constraints, is context-free but still scales well in common usage scenarios. HemLock does not provide constant remote memory reference (RMR) complexity [29] in scenarios where a thread holds multiple contended locks. In this situation the single node suffers from multi-waiting [26]. Similar to MCS, HemLock lacks a constant-time unlock operation, whereas the unlock operator for CLH and Tickets is constant-time. Unlike MCS, HemLock requires active synchronous back-and-forth communication in the unlock path between the outgoing thread and its successor to protect the lifecycle of the waiting element. We note, however, that HemLock remains constanttime in the Release operator to the point where ownership is conveyed to the successor. HemLock uses address-based transfer of ownership, writing the address of the lock instead of a boolean, differentiating it from MCS and CLH. Reciprocating Locks, like HemLock, requires just a singleton per-thread waiting element allocated in thread-local storage. The closest related work to Reciprocating Locks that we know of is Chen's mutual exclusion algorithm[11, 12], where arriving threads use an atomic exchange on arrival to either acquire the lock or join a LIFO stack of recently arrived waiting threads. A new stack is detached (or closed in their terminology) when the current stack is exhausted. Their progress and bounded bypass properties are the same as found in Reciprocating Locks. Their approach uses global spinning, however, and requires at least 2 shared global variables - above and beyond context passed from Acquire to Release in the form of the immediate successor. In addition, each Release mutates at least one global shared variable, increasing coherence traffic. We opted to exclude NUMA-aware locks such as Cohort Locks [27, 28] and Compact NUMA-Aware Locks (CNA) [21] from consideration. We excluded Fissile Locks and GCR[20] as they have lax time-based anti-starvation mechanism. Fissile, specifically, depends on the owner of the inner lock to make progress to monitor for starvation. We also excluded MCSH[35] which is a recent variation of MCS that uses on-stack allocation of queue nodes and hence supports a standard locking interface. Like MCS, the lock is not innately context-free, and additional information needs to be passed from Acquire to Release via extra fields in the lock body. In our experiments, the performance of MCSH is typically on par with MCS proper. We further restrict our comparison to locks that use direct succession and hand off ownership directly from the owner 2025-06-12 Â· Copyright Oracle and or its affiliates Reciprocating Locks Exclude locks that use barging We xclude MCSH[35], which is es ential y a simplified Fis ile CNA for the in ock that use er lock, removes the fast-path barging at empt on ar ival, has and use Fis ile's 'partial MCS rel operation. ase' optimization to def MCS instead of MCS-based NUMA-aware patience set o 0, yielding FI O admis ion, r the final step in rel asing the in er lock until the Rel ase Table 1. Comparison of lock algorithm properties to a specific successor and do not admit barging or pouncing where the lock is released even though waiting threads exist, which allow newly arriving threads to sieze the lock. In Table-1 we compare the attributes of various local algorithms. Red -colored cells indicate potential undesirable properties. Note that all the locks provide a constant-time doorway phase. In the following we explain the meaning of each property. Spinning : reflects whether the lock utilizes local , global or semi-local [26] spinning. Constant-time Unlock : indicates if the lock Release operation is bounded. As noted above, HemLock is constant-time up to the point where the lock is either released or transferred to a successor, but the Release operator, for reasons of memory safety, then waits for the successor to acknowledge transfer of ownership before the memory underlying the queue element can be potentially reused. Specifically, in HemLock an uncontended Release operation is constant-time and a contended Release is constant-time up to and including the point where ownership is conveyed to the successor. FIFO : indicates the lock provides strict FIFO admission. Context-free : indicates additional information does not need to be transferred from the lock operator to the corresponding Release operation. Path Complexity Acquire and Path Complexity Release : we measured the size of the Acquire and Release methods in units of platform independent LLVM intermediate representation (IR) instructions, as emitted by clang++-18 , which serves as a simple measure for path complexity. We note that much of the complexity found in TWA manifests through the use of the hash function which maps lock address and ticket value pairs to slots in the waiting array. On-Stack : indicates the queue elements, if any, may be allocated on-stack. This also implies the nodes do not migrate and have a tenure constrained to the duration of the locking episode. Nodes Circulate : queue elements migrate between threads. This often implies the need for an explicit queue element lifecycle management system and precludes convenient on-stack allocation of queue elements. Migration may also be unfriendly to performance in NUMA environments. Explicit CTOR/DTOR Required : indicates the lock requires nontrivial constructors or destructors. CLH, for instance, requires destructors to run to release the wait elements referenced in the lock, to avoid memory leaks. Maximum Remote Misses per episode : is the worst-case maximum number of misses to remote memory incurred, under simple sustained contention, by a matching Acquire -Release pair. Misses to remote memory - memory homed on a different NUMA node - may be more expensive than local misses on various platforms, such as modern Intel systems that use the UPI coherence fabric [17], where miss requests are first adjudicated by the home node of the cache line. Algorithms where nodes circulate appear more vulnerable to accumulating such remote misses. For HemLock , we assume simple contention with no multi-waiting . We derived the 'Maximum Remote Misses per episode' value via static inspection of the code, and determining which of the accesses that might cause coherence misses might also be to locations homed on remote nodes. Invalidations per episode : is the number of coherence misses, under sustained contention, experienced by an Acquire -Release episode in a given thread. We empirically approximate that number as follows. Using an ARMv8 system (described below), we modifed the MutexBench microbenchmark to have a degenerate critical section that advanced only a local random number generator and to pass any context from Acquire to Release via thread-local storage, in order to reduce mutation of shared memory. Absent 2025-06-12 Â· Copyright Oracle and or its affiliates Dice and Kogan semi-local; com saep - only, frequently, often, us al y mostly : fer - ; pen - ; quotide ; semi-; cotid e ; vulgo-; plerumque ; coherence misses, an Acquire -Release episode, including the critical section, manages to remain completely resident in the private L1 data cache. As such, any misses are coherence misses. This technique yields a useful empirical metric on howmuchcoherence traffic various lock algorithms generate. Weused the ARM l2d_cache_inval hardware performance counter, which tallies L2 cache invalidation events, as simple proxy for coherence traffic - both latency and bandwith. We report the number of l2d_cache_inval events per episode. As can be seen, Reciprocating Locks is parsimonious and incurs just 4 invalidations per episode, while CLH requires 5 7 . Ticket locks require ğ‘‡ (10 in our example) invalidations per episode, where ğ‘‡ is the number of participating threads, given the global spinning. The number of misses incurred by CLH, MCS, HemLock and Reciprocating Locks is constant and not a function of the number of threads. These empirically-derived results align closely with a static analysis of the code and the expected number of coherence misses in the Acquire and Release paths. Space Complexity : reflects the space complexity of the lock algorithm where ğ‘‡ is the number of active threads, ğ¿ is the number of extant locks, ğ´ is the number of locks currently held plus the number of threads currently waiting on locks, ğ¸ is the size of the waiting element. ğ‘† is the size of a lock instance. When a lock algorithm requires context to be passed from Acquire to the corresponding Release , we set ğ‘† to 2 to indicate that our implementation allocated an extra word in the lock body to pass such information. Implementations that require context can avoid that space requirement in the lock body if they opt to pass context information by other means, such as via thread-local storage, in which case ğ‘† would be 1. HemLock is context-free, so the per-lock space usage is just 1, while Ticket Locks and TWA require 2 words per lock. TWA requires an additional 4096 words for the global waiting table that is shared over all thread and lock instances. For MCS and CLH we assume that the implementation stores the head of the chain - reflecting the current owner - in an additional field in the lock body, and thus the lock consists of head and tail fields, requiring 2 words in total.",
  "7 Empirical Performance Results": "Unless otherwise noted, all data was collected on an Oracle X5-2 system system having 2 sockets, each populated with an Intel Xeon E5-2699 v3 CPU running at 2.30GHz. Each socket has 18 cores, and each core is 2-way hyperthreaded, yielding 7 We observed similar ratios with performance counters that reported the number of 'snoop' messages, and on Intel by counting OFFCORE requests. We also observed in passing, while examining data from hardware performance counters, that CLH and MCS suffered from more stalls (both event counts and duration) from misses than did Reciprocating locks. CLH in particular executes a dependent load in the critical path in the arrival doorway on the address returned from the atomic exchange . The address to be loaded from is not known until after the exchange returns, denying the processor the ability to speculate or execute out-of-order. 72 logical CPUs in total. The system was running Ubuntu 20.04 with a stock Linux version 5.4 kernel, and all software was compiled using the provided GCC version 9.3 toolchain at optimization level '-O3'. 64-bit C or C++ code was used for all experiments. Factory-provided system defaults were used in all cases, and Turbo mode [55] was left enabled. In all cases default free-range unbound threads were used, with no pinning of threads to processors. At above 18 ready threads, the kernel starts to place additional threads onto the second socket, and NUMA effects come into play. Weimplementedall user-mode locks within LD_PRELOAD interposition libraries that expose the standard POSIX pthread_ mutex_t programming interface using the framework from [28]. This allows us to change lock implementations by varying the LD_PRELOAD environment variable and without modifying the application code that uses locks. The C++ std::mutex construct is implemented directly via pthread_ mutex primitives, so interposition works for both C and C++ code. All lock busy-wait loops on Intel used the PAUSE instruction and YIELD on ARMv8. To reduce false sharing, all waiting elements were aligned and sequestered at 128-byte boundaries, and, for the 'MutexBench' benchmarks, below, the lock instances where similarly sequestered. For the CLH, MCS, and Reciprocating Locks implementations used in the benchmark section of this paper, we elected to pass any additional context information via extra fields in the lock instance.",
  "7.1 MutexBench benchmark": "The MutexBench benchmark spawns ğ‘‡ concurrent threads. Each thread loops as follows: acquire a central lock L; execute a critical section; release L; execute a non-critical section. At the end of a 10 second measurement interval the benchmark reports the total number of aggregate iterations completed by all the threads. We report the median of 7 independent runs in Figure-1a where the critical section advances a shared global std::mt19937 Mersenne Twister pseudo-random number generator (PRNG) one step, and the non-critical section is empty, subjecting the lock to extreme contention. (At just one thread, this configuration also constitutes a useful benchmark for uncontended latency). The ğ‘‹ -axis reflects the number of concurrently executing threads contending for the lock, and the ğ‘Œ -axis reports aggregate throughput - the tally of all loops executed by all the threads in the measurement interval. For clarity and to convey the maximum amount of information to allow a comparison of the algorithms, the ğ‘Œ -axis is offset to the minimum score and the ğ‘‹ -axis is logarithmic. We ran the benchmark under the following lock algorithms: MCS is classic MCS. To avoid memory allocation during the measurement interval, the MCS implementation uses a thread-local stack of free queue elements. CLH is CLH 2025-06-12 Â· Copyright Oracle and or its affiliates Reciprocating Locks Figure 1. MutexBench 1 2 5 10 20 50 Threads 2.0 4.0 8.0 16.0 32.0 64.0 Aggregate Throughput in Mops/sec TKT MCS CLH TWA HemLock Recipro (a) Maximum Contention Intel 1 2 5 10 20 50 100 Threads 0.5 1.0 2.0 4.0 8.0 16.0 32.0 64.0 Aggregate Throughput in Mops/sec TKT MCS CLH TWA HemLock Recipro (c) Maximum Contention ARMv8 based on Scott's CLH variant with a standard interface Figure4.14 of [52]; For the MCS and CLH locks, our implementation stores the current head of the queue - the owner - in a field adjacent to the tail, so the lock body size was 2 words. CLH presents something of a challenge when used under the pthread_mutex interface. First, pthreads allows the programmer to use trivial initializers - setting the mutex body to all 0 - and avoid calling pthread_mutex_init . To compensate, we modified pthread_mutex_lock to populate such an uninitialized lock with the CLH 'dummy node' lazily, ondemand, on the first lock operation. In pthread_mutex_destroy we free the node, if populated, but many applications also do not call pthread_mutex_destroy , which constitutes a memory leak. As such, to allow CLH to be included, we avoided applications that create and then abandon large numbers of locks in a dynamic fashion. Ticket is a classic Ticket Lock; HemLock is the HemLock algorithm, with the CTR (coherence traffic reduction) optimization. As we are implementing a general purpose pthreads locking interface, a thread can hold multiple locks at one time. Using MCS as an example, lets say thread ğ‘‡ 1 currently holds locks ğ¿ 1, ğ¿ 2, ğ¿ 3 and ğ¿ 4. We'll assume no contention. ğ‘‡ 1 will have deposited MCS queue nodes into each of those locks. MCS nodes can not be reclaimed until the corresponding unlock operation. Our implementation could malloc and free nodes as necessary - allocating in the lock operator and freeing in unlock - but to avoid malloc and its locks, we instead use a thread-local stack of free queue nodes. In the lock operator, we first try to allocate from that free list, and then fall back to malloc only as necessary. In unlock, we return nodes to that free list. This approach reduces malloc-free traffic and the incumbent scalability concerns. We currently don't bother to trim the thread-local stack of free elements. So, if thread ğ‘‡ 1 currently holds no locks, the free stack will contain ğ‘ elements where ğ‘ is the maximum number of locks concurrently held by ğ‘‡ 1. We reclaim the elements from the stack when ğ‘‡ 1 exits. A stack is convenient for locality. In Figure-1a we make the following observations regarding operation at maximal contention with an empty critical section: Â· At 1 thread the benchmark measures the latency of uncontended Acquire and Release operations. Ticket Locks are the fastest, followed closely by HemLock, Reciprocating Locks, CLH and MCS. 1 2 5 10 20 50 Threads 2.0 4.0 Aggregate Throughput in Mops/sec TKT MCS CLH TWA HemLock Recipro (b) Moderate Contention Intel 1 2 5 10 20 50 100 Threads 0.50 1.00 2.00 4.00 Aggregate Throughput in Mops/sec TKT MCS CLH TWA HemLock Recipro (d) Moderate Contention ARMv8 2025-06-12 Â· Copyright Oracle and or its affiliates Dice and Kogan Â· As we increase the number of threads, Ticket Locks initially do well but then fade, exhibiting a precipitous drop in performance. TWA is the clear leader in the 'middle' area of the graph, between 4 and 16 threads. Â· Broadly, at higher thread counts, HemLock performs slightly better than or the same as CLH or MCS, while Reciprocating Locks provides the best throughput. We note in passing that care must be taken when negative or retrograde scaling occurs and aggregate performance degrades as we increase threads. As a thought experiment, if a hypothetical lock implementation were to introduce additional synthetic delays outside the critical path, aggregate performance might increase as the delay throttles the arrival rate and concurrency over the contended lock [33]. As such, evaluating just the maximal contention case in isolation is insufficient. In Figure-1b we pass arguments to MutexBench that configure it to implement a delay in the non-critical section. Each thread has a private std::mt19937 pseudo-random number generator. The non-critical section generates, using the private PRNG, a uniform random number in the range [ 0 -250 ) and then advances its private PRNG that many steps. (We take care to make sure this operation can not be optimized away by consuming the PRNG value at the end of the thread's run). As above, the critical section advances the shared PRNG one step. As can be seen, we enjoy positive scalability up to about 5 threads. At higher thread counts, Reciprocating Locks manages to exhibit the least reduction in scalability. In Figure-1c and Figure-1d, we show the results of MutexBench on an ARMv8 (aarch64) system, showing that the relative performance of the various algorithms remains portable over disparate architectures. The ARMv8 system was an Ampere Altra Max NeoVerse-N1 with 128 processors on a single socket and was running Ubuntu 24.04. We compiled all code using the -mno-outline-atomics -march= armv8.2-a+lse flags in order to allow direct use of modern atomic exchange , CAS and fetch-and-add instructions instead of the legacy LL-SC (load-locked store-conditional) forms thereof.",
  "7.2 std::atomic": "In Figure-2a, our C++ atomic exchange benchmark defines a simple structure type S that contains 5 32-bit integers. Each thread declares a local S instance, and we also declare a shared global std::atomic<S> A {} . The C++ compiler and runtime implement std::atomic for such objects by hashing the address of the instance into an array of mutexes, and acquiring those as needed to implement the desired atomic action. Each thread loops for 10 seconds, using std::atomic<S>::exchange to swap its local copy with the shared global, and we then report aggregate throughput rate, in terms of completed exchange operations, at the end of the measurement interval. The figure depicts the median score of 7 runs and the overall ranking of the locks is similar to that observed under MutexBench. Figure-2b is similar, but instead of std::atomic:exchange we use std::atomic:load to fetch the shared global, make a local copy, increment the 1st of the five constituent integer fields in that copy, and then call std::atomic:compare_ exchange_strong (CAS) in an inner loop to try replace the global copy with the updated local version. The load operator acquires and releases all necessary covering locks, as does the compare_exchange_strong operator. If the CAS fails, we update our fresh local copy (returned from CAS) and retry. At the end of the 10 second measurement interval we report total number of successful CAS operations. The graph reports the median score of 7 runs. For both benchmarks, the relative ranks of the various lock algorithms remains similar to what was observed with MutexBench.",
  "7.3 LevelDB": "In Figure-3 we used the 'readrandom' benchmark in LevelDB version 1.20 database 8 varying the number of threads and reporting throughput from the median of 5 runs of 50 seconds each. Each thread loops, generating random keys and then tries to read the associated value from the database. We first populated a database 9 and then collected data 10 . We made a slight modification to the db_bench benchmarking harness to allow runs with a fixed duration that reported aggregate throughput. LevelDB uses coarse-grained locking, protecting the database with a single central mutex: DBImpl::Mutex . Profiling indicates contention on that lock via leveldb::DBImpl::Get() . The results in Figure-3 largely echo those in Figure-1b.",
  "8 Discussion": "â–¶ Constant-time arrival doorway and Release paths, as a consequence of having desirable progress properties, may yield more predictable performance. Furthermore, such designs are more inherently amenable to 'polite' waiting, where a thread voluntarily deschedules itself and explicitly waits to be notified of transfer of ownership, by means of thread identity-based waiting primitives, such as park-unpark[18] or address-based waiting , such as the linux futex [32] primitive, which underlies the pthreads synchronization services. Use of such primitives allows the implementation to avoid toxic waiting policies, such as unbounded busy-waiting; periodic polling via timed sleep operations; or operating-system advisory yield calls. Specifically, having constant-time paths means contending threads wait on only one condition transfer of ownership - and have one waiting phase. In 8 leveldb.org 9 db_bench --threads=1 --benchmarks=fillseq --db=/tmp/db/ 10 db_bench --threads= threads --benchmarks=readrandom --use_existing_db=1 --db=/tmp/db/ --duration=50 , 2025-06-12 Â· Copyright Oracle and or its affiliates Reciprocating Locks 1 2 5 10 20 50 Threads 4 8 16 32 64 Aggregate Throughput in Mops/sec TKT MCS CLH TWA HemLock Recipro (a) exchange() 1 2 5 10 20 50 Threads 0.1 0.2 1.0 4.0 16.0 Aggregate Throughput in Mops/sec TKT MCS CLH TWA HemLock Recipro (b) compare_exchange_strong() Figure 2. C++ std::atomic<struct> 1 2 5 10 20 50 Threads 1.049 2.097 Aggregate Throughput in Mops/sec 1e6 TKT MCS CLH TWA HemLock Recipro the HotSpot Java Virtual Machine, with balanced locking under the 'synchronized' construct, such context information can be kept in the stack frame. In C++, context can be kept in std::lock_guard or std::scoped_lock instance, which is typically allocated on stack. Such constructions are an example of the the Resource Acquisition is Initialization(RAII) [59] idiom. And in general, we observe that more flexible locking interfaces that allow context to be easily passed, outside of the lock body, will allow a broader range of lock algorithms to be deployed. Figure 3. LevelDB : readrandom contrast, locks such as MCS or HemLock may need to wait on additional conditions. â–¶ Lifecycle management : In classic MCS the address of the owner's 'queue node' must be passed from an acquire operation to the corresponding release. Relatedly, that queue node must remain extant during the entire acquire-release period, and can not be use for other locking operations. In Reciprocating Locks, the element needs to remain extant only for the duration of the acquire operation, affording the implementer more latitude as to how and where the element is allocated. A singleton element allocated in thread- local storage suffices. â–¶ Passing of context from acquire to release : Like MCS and CLH, Reciprocating Locks requires context to be passed from an acquire operation to the corresponding release. Broadly, any context is undesirable and to the extent possible, we might prefer context-free algorithms. In practice, it is common to keep required context in extra fields in the lock body (which can induce increased coherence traffic and increase the size of the lock) or in thread-local storage constructs that track which locks are held by a thread and can also be used to convey context. In managed runtime environments such as â–¶ Handoff costs: Under sustained contention, CLH and MCS Release operators can perform handover without any need to access the central shared lock body, via direct threadto-thread communication that does not involve the lock. Reciprocating Locks, however, needs to occasionally consult the central lock arrivals pointer when the entry segment is found to be exhausted and must be reprovisioned. Compared to CLH and MCS, this can increase RMR complexity and generate additional coherence traffic. Interestingly, as we increase the number of contending threads, reciprocating locks will tend to enjoy longer segments and thus need to access the central arrival word less often. In addition, the conditional branch found in the release path of reciprocating locks - which tests whether the successor is nullptr - will tend to be less well-predicted when the segments are shorter. â–¶ As to why Reciprocating Locks sometimes exceeds the performance of CLH, we make the following observations : (A) on NUMA systems, the waiting element in Reciprocating Locks is almost surely homed on same NUMA node as the associated thread, providing a performance benefit if home-based snooping is employed. Under CLH, the elements circulate. We note, however, that Reciprocating Locks still tends to outperform CLH on non-NUMA systems, although the difference is lessened. (B) Reciprocating Locks incurs less indirection and suffers less from stalls arising from address-dependent data loads. Les ons for locking API designers regarding the interplay betwe n algorithms and interfaces 2025-06-12 Â· Copyright Oracle and or its affiliates Dice and Kogan Rel gate; demote; def r; aside epi-cy les, epi-palindromes Under CLH, the address to be waited upon is not known until after the atomic exchange completes - and the coherence miss from the exchange is satisfied - whereas in Reciprocating Locks the address to be waited upon under contention is known and fixed for the thread. Similarly, consider the step in CLH before the atomic exchange where a thread clears the flag in the element about to be installed via an atomic exchange. Assuming the implementation of CLH found in Figure 4.14 of [52], thread-local storage contains a pointer to the element, which in turn holds the flag. We find a load from thread-local storage to fetch the address of the element, followed by a dependent store into the element. Under Reciprocating Locks, threadlocal storage simply contains the flag proper, embedded in the WaitingElement , reducing indirection. We thus require just a store to prepare the element. Broadly, CLH requires more indirection because elements circulate. The address of the element to be installed via exchange, and the address of the element to be waited upon will, for a given thread, vary between iterations. In Reciprocating Locks, those addresses are fixed for the thread. (C) If we tally the minimum number of coherence misses in an idealized contended acquire-release episode, for CLH we have 5 : the store to prepare the element before the exchange; the exchange; the 1st load in the waiting phase after the exchange; the last load in the waiting phase when ownership is passed; and finally, the store in the release operation that conveys ownership. The store to prepare the element, prior to the exchange, causes a coherence miss as the last store to the body of that element was from a different thread, before the element migrated to the thread in question. (This miss occurs outside and before the critical section, but is still undesirable as it causes a stall and consumes interconnect bandwidth). For Reciprocating Locks, in Listing-1, we incur a simple shared â†’ modified coherence upgrade at Line-17 (the line in already in shared state, because of the previous Acquire operation), a miss at Line-20 to exchange the address of the element, a miss at Line-30 when the busy-wait loop condition is ultimately satisfied and ownership is conveyed, and, in the Release operation, a miss at Line-58 when transferring ownership to the successor, for a total of 4 coherence misses. The misses at Line-17 and Line-30 are to memory expected to be homed on the same NUMA node as the thread.",
  "9 Palindromic Admission Schedules": "Reciprocating Locks may allow a palindromic admission schedule which, under the right circumstances, can persist for long periods.",
  "9.1 Example Scenario": "Table-2 illustrates the phenomena with a simple scenario. Threads ğ´ ğµ ğ¶ ğ· and ğ¸ all complete for a given lock ğ¿ . Initially, at time 1, ğ´ is the owner, executing in the critical section, the entry segment is empty and the arrival segment consists of ğµ then ğ¶ then ğ· then ğ¸ ( ğµ + ğ¶ + ğ· + ğ¸ ). The noncritical section is empty, so when a thread releases the lock, it immediately recirculates and tries to reacquire. ğ´ completes the critical section and invokes Release , which, as the entry segment is empty, reverts to and detaches the arrival segment of ğµ + ğ¶ + ğ· + ğ¸ and moves those threads en-masse to entry segment, and then passes ownership to the head of the entry segment, ğµ . ğ´ recirculates, calls Acquire again, and emplaces itself on the arrival segment, reflecting the state at time 2. Next, ğµ releases the lock, and passes ownership to ğ¶ . ğµ then calls Acquire and prepends itself to the arrival segment stack, which now contains ğµ + ğ´ , as shown at time 3. ğ¶ releases the lock and conveys ownership to the head of the entry segment, ğ· . ğ¶ then recirculates and pushes itself onto the arrival segment, which now holds ğ¶ + ğµ + ğ´ at time 4. ğ· releases ğ¿ , cedes ownership to the head of the entry segment, ğ¸ and then recirculates, adding itself to the arrival segment, now containing ğ· + ğ¶ + ğµ + ğ´ , at time 5. ğ¸ calls Release and, as the entry segment is empty, ğ¸ detaches the arrival segment of ğ· + ğ¶ + ğµ + ğ´ , shifting those threads into the arrival segment, and then enables ğ· . ğ¸ recirculates and joins the arrival segment, leaving the configuration as seen at time 6. ğ· releases the lock and passes ownership to ğ¶ and ğ· then joins the arrival segment, as shown at time 7. ğ¶ releases ğ¿ and conveys ownership to ğµ , and then prepends itself to the arrival segment, leaving the state as shown at time 8. ğµ releases ğ¿ , enables ğ´ , and then joins the arrival segment, leaving the state as shown at time 9. The states at times 1 and 9 are identical, so the admission schedule repeats with a period length of 8 steps.",
  "9.2 Long-term Admission Unfairness": "While there is no long-term starvation, within the admission cycle ğ´ğµğ¶ğ·ğ¸ğ·ğ¶ğµ we see that ğ´ and ğ¸ are admitted just once while the others thread are admitted twice, which can manifest as long-term relative unfairness between the participating threads. While not a perfect palindrome, we say such a schedule is palindromic and note that the worst case admission unfairness that might manifest by virtue of the lock admission policy is 2 ğ‘‹ - assuming constant offered load, the most favored thread can be admitted no more than twice as often as the most 'unlucky' thread. In general we find a bimodal distribution of progress over the participating threads.",
  "9.3 Cache residency": "In addition to simple admission, the palindromic schedule introduces an interesting secondary effect related to residual 2025-06-12 Â· Copyright Oracle and or its affiliates Reciprocating Locks cache residency. We assume a simple system model where all threads circulating over a lock access the same shared last-level cache (LLC). While threads are waiting, their residency in the LLC undergoes decay because of the actions of the other threads executing in the critical section or their respective non-critical sections. Arguably, a simple round-robin FIFO FCFS admission schedule results in the worst aggregate miss rate and thus the worst overall throughput. 11 . In contrast, the palindromic admission order can result in a better aggregate miss rate in the LLC. Yuan et al. [60] use the term 'sawtooth' for the pure palindrome schedule. In our case, the 'elements' are threads contending for a lock. Various threads, however, can experience persistent disparate LLC miss rates, yielding a different form of unfairness. Table 2. Example of a palindromic admission schedule",
  "9.4 Mitigation": "If desired, we can mitigate the unfairness from the effects above in a number of ways. A simple and expedient approach is to stochastically disrupt or perturb the repeating cycle, which reestablishes statistical long-term fairness. A viable technique is for incoming owners, having just acquired the lock, to run a thread-local Bernoulli trial, and based on the outcome, occasionally defer and immediately cede ownership to the next element in the entry segment, and propagate a reference to its wait element through the entry segment, where it will percolate to the tail, and eventually be re-granted ownership. This modification does not abrogate or otherwise violate our bypass guarantees or imperil anti-starvation as the reordering is strictly intra-segment. This particular approach, while expedient, surrenders the constant-time arrival property as a thread may need to wait in two phases during one acquisition episode, once to acquire ownership, and a 2nd time to be regranted ownership after it abdicated. More generally, if we simply pick random elements, without replacement, from the entry segment for succession, we 11 A simple analogy is sequential single-threaded code where an application needs to iterate over all elements of an array or linked list. A naive approach is to simply access the elements in ascending order until reaching the end, and then repeat, yielding a robin-robin order. Taking residual cache residency in account, however it is better to alternate ascending then descending orders - akin to a classic elevator seek or boustrophedonic order - which yields a palindrome access pattern. Such cache-friendly alternating direction optimizations are well known and appear in various sorting algorithms [57, 58] still retain the desirable population bounded anti-starvation property, statistically avoid long-term admission unfairness and cache residency fairness, and continue to enjoy aggregate miss rates (and throughput) that on average are the same or better than classic FIFO. Crucially, as all such reordering is intra-segment, we preserve our bounded bypass property.",
  "10 Conclusion": "Reciprocating Locks has a fully constant-time doorway phase and a constant-time Release , is practical and fulfills the criteria for general purpose locking. Reciprocating Locks is also architecturally informed as it works well with recent developments in modern NUMA cache-coherent communication fabrics. In the future, we plan on exploring the 'coherence traffic reduction' optimization (CTR), from HemLock, with Reciprocating Locks. Relatedly, using the Intel CLDEMOTE instruction or non-temporal stores to covey ownership may also act to reduce coherence traffic. Using modern architecturally enhanced waiting mechanisms such as MONITOR-MWAIT (Intel) or WFE (ARM) may also prove useful for situations where a thread waits for a local flag to toggle and then resets the flag, as we can instead wait for invalidation of a cache line and then use an atomic exchange to try to reset the flag, avoiding intermediate upgrades from MESI/MOESI shared to modified states. Finally, we are exploring techniques to impose long-term statistical admission fairness by means of randomizing or perturbing the order of the entry segment in order to break repeating palindromic admission schedules. Hemlock trades off improved space complexity against the cost of higher remote memory reference (RMR) complexity. Hemlock is exceptionally simple with short paths, and avoids the dependent loads and indirection required by CLH or MCS to locate queue nodes. The contended handover critical path is extremely short - the unlock operator conveys ownership to the successor in an expedited fashion. Despite being compact, it provides local spinning in common circumstances and scales better than Ticket Locks. Instead of traditional queue elements, as found in CLH and MCS, we use a per-thread shared singleton element. Finally, Hemlock is practical and readily usable in real-world lock implementations.",
  "Acknowledgments": "Wethank Peter Buhr at the University of Waterloo for access to their ARMv8 system.",
  "References": "[1] Ole Agesen, David Detlefs, Alex Garthwaite, Ross Knippel, Y. S. Ramakrishna, and Derek White. An efficient meta-lock for implementing ubiquitous synchronization. SIGPLAN Notices OOPSLA 1999 , 1999. doi:10.1145/320385.320402 . claim, novel, novelty, contribution 2025-06-12 Â· Copyright Oracle and or its affiliates Dice and Kogan [2] Vitaly Aksenov, Dan Alistarh, and Petr Kuznetsov. Performance prediction for coarse-grained locking. CoRR , abs/1904.11323, 2019. URL: http://arxiv.org/abs/1904.11323 , arXiv:1904.11323 . [3] Vitaly Aksenov, Daniil Bolotov, and Petr Kuznetsov. Peformance prediction for coarse-grained locking: MCS case. CoRR , abs/2110.05545, 2021. URL: https://arxiv.org/abs/2110.05545 , arXiv:2110.05545 . [4] James H. Anderson and Yong-Jik Kim. An improved lower bound for the time complexity of mutual exclusion. Distrib. Comput. , 2002. doi:10.1007/s00446-002-0084-2 . [5] J.H. Anderson, Y.J. Kim, and T. Herman. Shared-memory mutual exclusion: major research trends since 1986. Distributed Computing , 2003. URL: https://doi.org/10.1007/s00446-003-0088-6 . [6] T. E. Anderson. The performance of spin lock alternatives for sharedmoney multiprocessors. IEEE Transactions on Parallel and Distributed Systems , 1990. doi:10.1109/71.80120 . [7] Hagit Attiya, Danny Hendler, and Philipp Woelfel. Tight RMR lower bounds for mutual exclusion and other problems. In Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing , STOC '08, 2008. doi:10.1145/1374376.1374410 . [8] M. Auslander, D. Edelsohn, O. Krieger, B. Rosenburg, and R. Wisniewski. Enhancement to the MCS lock for increased functionality and improved programmability - U.S. patent application number 20030200457, 2003. URL: https://patents.google.com/patent/ US20030200457 . [9] D. Avis and M. Newborn. On pop-stacks in series. Utilitas Math. 19 , pages 129-140, 1981. [10] Stephen Boyd and Lieven Vandenberghe. Convex optimization . Cambridge university press, 2004. [11] Sheng-Hsiung Chen and Ting-Lu Huang. A fair and space-efficient mutual exclusion. In 11th International Conference on Parallel and Distributed Systems (ICPADS'05) , 2005. doi:10.1109/ICPADS.2005. 23 . [12] Sheng-Hsiung Chen and Ting-Lu Huang. Bounded-bypass mutual exclusion with minimum number of registers. IEEE Trans. Parallel Distrib. Syst. , 2009. doi:10.1109/TPDS.2009.28 . [13] Jonathan Corbet. A surprise with mutexes and reference counts. https://lwn.net/Articles/575460 , December 4, 2013. [14] Jonathan Corbet. MCS locks and qspinlocks. https://lwn.net/Articles/ 590243 , March 11, 2014, 2014. Accessed: 2018-09-12. [15] Jonathan Corbet. Mcs locks and qspinlocks, 2014. URL: https://lwn. net/Articles/590243/ . [16] Travis Craig. Building FIFO and priority-queueing spin locks from atomic swap, 1993. [17] Intel Corporation David Mulnix. IntelÂ® XeonÂ® Processor Scalable Family Technical Overview, 2017. Updated 2022. URL: https://www.intel.com/content/www/us/en/developer/articles/ technical/xeon-processor-scalable-family-technical-overview.html . [18] Dave Dice. Malthusian locks. CoRR , abs/1511.06035, 2015. URL: http://arxiv.org/abs/1511.06035 , arXiv:1511.06035 . [19] Dave Dice. Malthusian locks. In Proceedings of the Twelfth European Conference on Computer Systems , EuroSys '17, 2017. URL: http://doi. acm.org/10.1145/3064176.3064203 . [20] Dave Dice and Alex Kogan. Avoiding scalability collapse by restricting concurrency. In Euro-Par 2019: Parallel Processing - 25th International Conference on Parallel and Distributed Computing, GÃ¶ttingen, Germany, August 26-30, 2019, Proceedings , Lecture Notes in Computer Science. Springer, 2019. doi:10.1007/978-3-030-29400-7\\_26 . [21] Dave Dice and Alex Kogan. Compact NUMA-Aware Locks. In Proceedings of the Fourteenth EuroSys Conference 2019 , EuroSys '19. Association for Computing Machinery, 2019. doi:10.1145/3302424.3303984 . [22] Dave Dice and Alex Kogan. TWA - ticket locks augmented with a waiting array. In Euro-Par 2019: Parallel Processing - 25th International Conference on Parallel and Distributed Computing, GÃ¶ttingen, Germany, August 26-30, 2019, Proceedings . Springer, 2019. doi:10.1007/978-3-",
  "030-29400-7\\_24 .": "[23] Dave Dice and Alex Kogan. Fissile locks, 2020. URL: https://arxiv.org/ abs/2003.05025 , arXiv:2003.05025 . [24] Dave Dice and Alex Kogan. Fissile locks. In Networked Systems (NETYS 2020) , 2021. URL: https://doi.org/10.1007/978-3-030-67087-0_13 . [25] Dave Dice, Virendra J. Marathe, and Nir Shavit. Persistent unfairness arising from cache residency imbalance. In Proceedings of the 26th ACM Symposium on Parallelism in Algorithms and Architectures , SPAA '14, 2014. URL: https://doi.org/10.1145/2612669.2612703 . [26] David Dice and Alex Kogan. Hemlock : Compact and scalable mutual exclusion. In Proceedings of the 33rd ACM Symposium on Parallelism in Algorithms and Architectures , SPAA, 2021. URL: https://doi.org/10. 1145/3409964.3461805 . [27] David Dice, Virendra J. Marathe, and Nir Shavit. Lock Cohorting: A General Technique for Designing NUMA Locks. In Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming , PPoPP '12. Association for Computing Machinery, 2012. doi:10.1145/2145816.2145848 . [28] David Dice, Virendra J. Marathe, and Nir Shavit. Lock Cohorting: A General Technique for Designing NUMA Locks. ACM Trans. Parallel Comput. , 2015. URL: http://doi.acm.org/10.1145/2686884 , doi:10.1145/2686884 . [29] Rotem Dvir and Gadi Taubenfeld. Mutual Exclusion Algorithms with Constant RMR Complexity and Wait-Free Exit Code. In James Aspnes, Alysson Bessani, Pascal Felber, and JoÃ£o LeitÃ£o, editors, 21st International Conference on Principles of Distributed Systems (OPODIS 2017) , Leibniz International Proceedings in Informatics (LIPIcs), Dagstuhl, Germany, 2018. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik. URL: http://drops.dagstuhl.de/opus/volltexte/2018/8652 , doi:10.4230/LIPIcs.OPODIS.2017.17 . [30] Stijn Eyerman and Lieven Eeckhout. Modeling Critical Sections in Amdahl's Law and Its Implications for Multicore Design. In Proceedings of the 37th Annual International Symposium on Computer Architecture , ISCA '10. ACM, 2010. URL: http://doi.acm.org/10.1145/1815961. 1816011 , doi:10.1145/1815961.1816011 . [31] M. J. Fischer, N. A. Lynch, J. E. Burns, and A. Borodin. Resource allocation with immunity to limited process failure. In 20th Annual Symposium on Foundations of Computer Science (FOCS 1979) , 1979. URL: http://dx.doi.org/10.1109/SFCS.1979.37 . [32] Hubertus Franke, Rusty Russel, and Matthew Kirkwood. Fuss, futexes and furwocks: Fast user-level locking in linux. https://www.kernel.org/ doc/ols/2002/ols2002-pages-479-495.pdf . Ottawa Linux Symposium. [33] Carlos Gershenson and Dirk Helbing. When Slower is Faster. CoRR , 2011. URL: http://arxiv.org/abs/1506.06796v2 . [34] Rachid Guerraoui, Hugo Guiroux, Renaud Lachaize, Vivien QuÃ©ma, and Vasileios Trigonakis. Lock-unlock: Is that all? a pragmatic analysis of locking in software systems. ACM Trans. Comput. Syst. , 2019. doi: 10.1145/3301501 . [35] WimH.Hesselink and Peter A. Buhr. MCSH, a Lock with the Standard Interface. ACM Trans. Parallel Comput. , 2023. URL: https://doi.org/10. 1145/3584696 . [36] Prasad Jayanti, Siddhartha Jayanti, and Sucharita Jayanti. Towards an ideal queue lock. In Proceedings of the 21st International Conference on Distributed Computing and Networking , ICDCN 2020. Association for Computing Machinery, 2020. URL: https://doi.org/10.1145/3369740. 3369784 . [37] Siddhartha Visveswara Jayanti. Simple, fast, scalable, and reliable multiprocessor algorithms. Massachusetts Institute of Technology, 2023. URL: https://dspace.mit.edu/handle/1721.1/150219 . [38] Hyonho Lee. Local-Spin Mutual Exclusion Algorithms on the DSM Model Using fetch&store Objects. Masters Thesis, University of Toronto. URL: http://www.cs.toronto.edu/pub/hlee/thesis.ps . [39] Hyonho Lee. Transformations of mutual exclusion algorithms from the cache-coherent model to the distributed shared memory model. In 2025-06-12 Â· Copyright Oracle and or its affiliates Reciprocating Locks 25th IEEE International Conference on Distributed Computing Systems (ICDCS'05) , 2005. doi:10.1109/ICDCS.2005.83 . [40] Alexander Lochmann, Horst Schirmeier, Hendrik Borghorst, and Olaf Spinczyk. Lockdoc: Trace-based analysis of locking in the linux kernel. In Proceedings of the Fourteenth EuroSys Conference 2019 , EuroSys '19. Association for Computing Machinery, 2019. doi:10.1145/3302424. 3303948 . [41] Waiman Long. qspinlock: Introducing a 4-byte queue spinlock implementation. https://lwn.net/Articles/561775 , July 31, 2013, 2013. Accessed: 2018-09-19. [42] P. Magnusson, A. Landin, and E. Hagersten. Queue locks on cache coherent multiprocessors. In Proceedings of 8th International Parallel Processing Symposium , 1994. doi:10.1109/IPPS.1994.288305 . [43] Evangelos P. Markatos and Thomas J. LeBlanc. Multiprocessor synchronization primitives with priorities. In 8th IEEE Workshop on Real-Time Operating Systems and Software . IEEE, 1991. doi:10.1016/ S1474-6670(17)51259-8 . [44] George Marsaglia. Xorshift rngs. Journal of Statistical Software, Articles , 2003. URL: https://www.jstatsoft.org/v008/i14 , doi:10.18637/jss. v008.i14 . [45] John M. Mellor-Crummey and Michael L. Scott. Algorithms for scalable synchronization on shared-memory multiprocessors. ACM Trans. Comput. Syst. , 1991. URL: http://doi.acm.org/10.1145/103727.103729 . [46] John M. Mellor-Crummey and Michael L. Scott. Scalable reader-writer synchronization for shared-memory multiprocessors. In Proceedings of the Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming , PPOPP '91. ACM, 1991. URL: http://doi.acm. org/10.1145/109625.109637 . [47] Atsushi Nemoto. Bug 13690 - pthread_mutex_unlock potentially cause invalid access. https://sourceware.org/bugzilla/show_bug.cgi? id=13690 , February 14, 2012. [48] Pedro Ramalhete. Ticket lock -array of waiting nodes (awn), 2015. URL: http://concurrencyfreaks.blogspot.com/2015/01/ticketlock-array-of-waiting-nodes-awn.html . [49] Pedro Ramalhete and Andreia Correia. Tidex: A mutual exclusion lock. In Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming , PPoPP '16, 2016. URL: http: //doi.acm.org/10.1145/2851141.2851171 . [50] David P. Reed and Rajendra K. Kanodia. Synchronization with eventcounts and sequencers. Commun. ACM , 1979. URL: http: //doi.acm.org/10.1145/359060.359076 . [51] I. Rhee. Optimizing a FIFO, scalable spin lock using consistent memory. In 17th IEEE Real-Time Systems Symposium , 1996. doi:10.1109/REAL. 1996.563705 . [52] Michael L. Scott and Trevor Brown. Shared-Memory Synchronization, Second Edition . Springer, 2024. doi:10.1007/978-3-031-38684-8 . [53] Harold S. Stone and Dominique Thibaut. Footprints in the cache. SIGMETRICS Perform. Eval. Rev. , 1986. doi:10.1145/317531.317533 . [54] R.K. Treiber. Technical Report RJ 5118, IBM Almaden Research Center, Systems programming: Coping with parallelism, 1986. [55] U. Verner, A. Mendelson, and A. Schuster. Extending Amdahl's Law for Multicores with Turbo Boost. IEEE Computer Architecture Letters , 2017. URL: https://doi.org/10.1109/LCA.2015.2512982 . [56] Tianzheng Wang, Milind Chabbi, and Hideaki Kimura. Be my guest: Mcs lock now welcomes guests. SIGPLAN PPoPP , 2016. doi:10.1145/ 3016078.2851160 . [57] Wikipedia. Cocktail shaker sort, 2018. URL: https://en.wikipedia.org/ wiki/Cocktail_shaker_sort . [58] Wikipedia. Gnome sort, 2018. URL: https://en.wikipedia.org/wiki/ Gnome_sort . [59] Wikipedia. Resource acquisition is initialization, 2022. URL: https: //en.wikipedia.org/wiki/Resource_acquisition_is_initialization . [60] Liang Yuan, Chen Ding, Wesley Smith, Peter Denning, and Yunquan Zhang. A relational theory of locality. TACO - ACM Trans. Archit. Code Optim. , 2019. doi:10.1145/3341109 . 2025-06-12 Â· Copyright Oracle and or its affiliates Dice and Kogan",
  "A Naming": "The name Reciprocating Locks arises from the following analogy with a piston-based compressor or reciprocating pump. The pump's cylinder has distinct intake and exhaust ports and the piston is initially positioned at the bottom of the cylinder. Threads intending to acquire the lock - say, ğ´ , ğµ and ğ¶ where ğ´ was the first to arrive and ğ¶ last - arrive at the intake port and wait there. On the intake cycle, the piston pulls those threads from the intake port into the cylinder body. On the subsequent exhaust cycle, the piston expels the threads through the exhaust port. Passing through the exhaust port is analogous to passing through the lock's critical section. Like packets of air or gas, the threads are expelled from the cylinder through the exhaust port in an order reversed from that in which they originally arrived : ğ¶ then ğµ then ğ´ . The entry segment corresponds to the current cylinder contents and the arrival segment reflects threads that reside in the intake manifold, before entering the cylinder. The non-critical section is where threads circulate back from the exhaust to the intake. The intake phase is analogous to detaching the current arrival segment and shifting those threads into the entry segment.",
  "B On-stack allocation of Wait Elements": "As described above, our implementation places wait elements in thread-local storage. This simplifies reasoning about memory correctness as the tenure and lifespan of thread-local storage is the same as that as the associated thread. We observe, however, that in many environments, wait elements can also be safely allocated on-stack, in the activation frame of Acquire . This approach reduces memory usage to ğ¾ âˆ— ğ¸ + ğ¿ âˆ— ğµ where ğ¾ is the number of waiting threads, ğ¸ is the size of the waiting element in the stack frame, ğ¿ is the number of currently extant locks, and ğ‘† is the size of the lock body. At a conceptual level, the element on which threads spin requires a short lifespan (tenure) and is only required to exist and remain in scope for the duration of the Acquire operation, and as such can be allocated in the Acquire function's frame. This is more convenient in terms of lifecycle than CLH or MCS. But, in some circumstances - zombies - we also use the address of an element as an end-of-segment marker. In this case the address escapes the frame as it passed through the Gate field. Specifically, the address of the wait element has a longer lifespan than the wait element itself, and the address persists even after the wait element has fallen out of scope. (Note that we use address-based comparisons to detect the end-of-segment, but never actually reference the defunct wait elements). An address escaping its frame or scope, and having a longer tenure or lifespan that its referent, is considered 'undefined behavior' in C++. In practice, though, we believe the technique is viable. If a given virtual address stack address remains associated with a given thread for the lifetime of the thread, as is the common case for pthreads environments, then the approach is safe. We note, however, that such onstack allocation might not be safe under exotic non-standard thread models where multiple lightweight threads can run or be 'mounted' on a given stack at different points in time, and stack addresses do not map to or otherwise convey thread identity in a stable fashion. If this particular aspect is a concern, the implementation could simply opt to adhere to our basic design and place the wait element into thread-local storage (TLS), which completely obviates the issue. Wealso note that modern compilers can detect such behavior, and generate warnings, and in some cases, to help protect against undefined behavior, will actively annul references that can be statically determined to fall out of scope. As a thread can wait on at most on lock at a time, and a wait element is only needed for the duration of an Acquire operation, using a singleton in thread-local storage is entirely sufficient. We are using ğ¸ 's address for addressed-based comparisons as a distinguished marker to indicate the logical end-of-segment. ğ¸ itself will not be subsequently accessed. UB; Nas l Demons; halt-and-catch-fire 2025-06-12 Â· Copyright Oracle and or its affiliates Reciprocating Locks",
  "C Potential Throughput Benefits from Palindromic Schedule Order": "We assume a simple system model where all threads circulating over a lock access the same shared last-level cache (LLC). While threads are waiting, their residency in the LLC undergoes decay - usually modelled as exponential - because of the actions of the other threads executing in the critical section or their respective non-critical sections. We consider a true repeating palindrome admission schedule, ğ´ğµğ¶ğ·ğ¸ -ğ¸ğ·ğ¶ğµğ´ , as compared to the FIFO schedule of ğ´ğµğ¶ğ·ğ¸ -ğ´ğµğ¶ğ·ğ¸ . Applying a simplistic decay model, when a thread ceases waiting and takes ownership of the lock, it incurs a 'cache reload transient'[53] where it suffers a burst of cache misses as it reprovisions the LLC with its own previously displaced private data. The residual residency fraction can be approximated as ğ‘…ğ‘’ğ‘ ğ‘–ğ‘‘ğ‘¢ğ‘ğ‘™ ( ğ‘‡ ) = ğ‘’ğ‘¥ğ‘ (-ğ‘‡ âˆ— ğœ† ) where ğ‘‡ is the sojourn or waiting time - the number of quanta since the thread last ran - and ğœ† parameterizes the decay rate. ğœ† is usually expressed as half-life . As ğ‘…ğ‘’ğ‘ ğ‘–ğ‘‘ğ‘¢ğ‘ğ‘™ is a convex function, we can employ Jensen's inequality [10] as follows. Taking thread ğµ as a specific example, its waiting times under the FIFO schedule is always 4 time units and under the palindrome schedule the waiting time alternate 2-6-2-6 etc. The average waiting time is the same under both schedules, but the average residual LLC residency when the thread resumes is the same or better under the palindrome schedule as ğ‘…ğ‘’ğ‘ ğ‘–ğ‘‘ğ‘¢ğ‘ğ‘™ ( 2 )+ ğ‘…ğ‘’ğ‘ ğ‘–ğ‘‘ğ‘¢ğ‘ğ‘™ ( 6 ) â‰¥ ğ‘…ğ‘’ğ‘ ğ‘–ğ‘‘ğ‘¢ğ‘ğ‘™ ( 4 )+ ğ‘…ğ‘’ğ‘ ğ‘–ğ‘‘ğ‘¢ğ‘ğ‘™ ( 4 ) . In fact, each and every thread will have the same or better residual fraction under the palindrome schedule than under FIFO although there will be disparity of benefits over the set of threads within the palindrome schedule. Intuitively, as the decay process is exponential in nature, the retained residency benefits accrued by the relatively short gap outweigh the decay penalty of the subsequent longer gap found in the palindrome schedule. The overall aggregate miss rate for the palindrome schedule, as computed over all the threads, will be less than that in the round-robin FIFO schedule, yielding better overall throughput. (Higher residency fractions implies reduced miss rates and better performance). Specifically, The palindrome schedule enjoys better overall aggregate LLC miss rates and throughput than a simple repeating round-robin FIFO schedule of ğ´ğµğ¶ğ·ğ¸ -ğ´ğµğ¶ğ·ğ¸ . And in fact the FIFO schedule is pessimal for aggregate miss rate if we require equal fairness as measured over two back-to-back cycles. The same palindrome schedule effects on cache residency also apply by analogy to page working sets. Also by analogy, a simple uniprocessor scheduler would admit better cache residency using a palindrome schedule as compared to a classic FIFO round-robin schedule. Wecall out an analogy in simple sequential single-threaded code where an application needs to iterate over all elements of an array or linked list. A naive approach is to simply access the elements in ascending order until reaching the end, and then repeat, yielding a robin-robin order. Taking residual cache residency in account, however it is better to alternate ascending then descending orders - akin to a classic elevator seek or boustrophedonic order - which yields a palindrome access pattern. Such cache-friendly alternating direction optimizations are well known and appear in various sorting algorithms [57, 58]. Yuan et al. [60] use the term 'sawtooth' for the palindrome schedule. In our case, the 'elements' are threads contending for a lock. Considering a palindrome lock admission order of ğ´ğµğ¶ğ·ğ¸ -ğ¸ğ·ğ¶ğµğ´ -ğ´ğµğ¶ğ·ğ¸... , for instance, we will have fair admission over the long term, but threads ğ´ and ğ¸ will enjoy persistently lower shared cache miss rates than the other threads, imposing a different form of unfairness related to residual cache residency. Crucially, under the palidrome schedule, threads can incur disparate cache hits rates, reflecting a form of long-term cache-based unfairness, even if the the admission is long-term fair. The overall aggregate miss rate for the palindrome schedule, however, as computed over all the threads, will be less than that in the round-robin FIFO schedule. We note in passing that a random admission order is statistically long-term fair for both admission frequency and cache residency, and also will display a lower aggregate cache miss rate than FIFO. The same effects that apply to cache residency also have analogs in page working sets. As a coral ary to the above, if we as ume a set of threads equitably but otherwise distributed in a random fashion over a set of NUMA nodes, a palindrome admis ion order exhib ts, on average, fewer NUMA lock migrations than does a round-robin (FI O) order, making palindrome admis ion in ately NUMA-friendly without being specifical y NUMA-aware. ap ly by an logy to Palindrome-adjacent 2025-06-12 Â· Copyright Oracle and or its affiliates Dice and Kogan Les on, Informs, Instructs, Guides",
  "D Locking APIs": "Modern C++ locking constructs such as std::scoped_lock and std::lock_guard , by means of the Resource Acquisition is Initialization(RAII) [59] idiom, where the constructor acquires the lock and the destructor releases the lock, manage to avoid explicit lock and unlock calls in application code. This same design pattern readily supports underlying lock primitives that require context to be passed. Specifically, we can pass extra context through additional fields in the RAII wrapper classes. Likewise, locking interfaces that specify the critical section as a C++ lambda also allow such latitude. Interfaces that use scoped locking, such as Java's 'synchronized\" construct, permit the implementation to trivially pass information from the underlying lock cite to the corresponding unlock . If an implementation is forced to use a legay interface, such as that exposed via pthread_mutex_lock and pthread_mutex_ unlock , or C++ std::BasicLockable , but the lock algorithm is not context-free , then we can resort to passing context information through extra fields in the lock body, which are protected by the lock itself, and written and read only by the lock holder. This increases the size of the lock body and accesses to those fields can induce extra coherence traffic. Another approach is to keep track of held locks in thread-local storage (TLS) and convey the information in that fashion. Most locking algorithms that are not innately context-free can be transformed to become context-free through such techniques. We note that MCS and CLH also also must pass additional contextual information from Acquire to Release , so this concern is not specific to Reciprocating Locks. More modern interfaces,as noted above, may confer various advantages and obviate some of the concerns associated with passing context. We hope these observations serve to inform API designers, in that more flexible and liberal locking APIs may more easily accomodate a wider array of underlying lock algorithms, and afford more latitude to implementors.",
  "E Simplified Example": "In Listing-2 we show, for purposes of explication, a slightly simplified form of Reciprocating Locks, which is both easy to understand and implement. We recommend that implementors start with this version. This variant passes the identity of the successor ( succ ) as context. The identity of the terminus end-of-segment address is encoding via a global variable, eos , in the lock body. To reduce coherence traffic and false sharing between eos and Arrivals , we sequester and isolate eos as the sole occupant of its cache sector. Crucially, eos is not written during steady-state sustained contention, so does not tend to generate coherence misses in that circumstance. Exemplar; explainer 2025-06-12 Â· Copyright Oracle and or its affiliates Reciprocating Locks 1 struct ReciprocatingLock { 2 struct WaitElement { 3 std::atomic< int > Gate {0} ; 4 } ; 5 6 // CONSIDER: packed or dense layout for Arrivals vs eos 7 // Arrival word : 8 std::atomic <WaitElement *> Arrivals { nullptr } ; 9 // Terminus end-of-segment sentinel : 10 std::atomic <WaitElement *> eos alignas(128) { nullptr } ; 11 12 static inline const auto NEMO = (WaitElement *) uintptr_t(1) ; 13 static inline constinit thread_local WaitElement E alignas(128) {0} ; 14 15 public : 16 inline auto operator+ (std::invocable auto && csfn) â†’ void { 17 // Acquire Phase ... 18 E.Gate.store (0, std::memory_order_release) ; 19 auto succ = Arrivals.exchange (&E) ; 20 assert (succ â‰  &E) ; 21 if (succ == nullptr ) { 22 // fast-path uncontended aquire 23 eos.store (&E, std::memory_order_release) ; 24 } else { 25 // Coerce NEMO to nullptr 26 // succ will be our successor when we subsequently release 27 succ = (WaitElement *) (uintptr_t(succ) & ~1) ; 28 assert (succ â‰  &E) ; 29 30 // slow path : contention --waiting phase 31 while (E.Gate.load (std::memory_order_acquire) == 0) { 32 Pause() ; 33 } 34 35 // Check for EOS-termimated entry segment chain 36 // On match : annul succ and reset global eos variable 37 // CRITICAL : Note that eos does NOT change under sustained 38 // contention after we reach steady state ! 39 // The global \"eos\" field becomes stable 40 auto veos = eos.load (std::memory_order_acquire) ; 41 assert (veos â‰  &E) ; 42 assert (veos â‰  nullptr ) ; 43 if (succ == veos) { 44 succ = nullptr ; 45 eos.store (NEMO, std::memory_order_release) ; 46 } 47 } 48 49 // Critical section expressed as lambda 50 // We pass \"succ\" as context from Acquire to corresponding Release 51 // The Release phase does NOT access eos 52 // Access to global eos is constrained to Acquire 53 // &E is implicit context 54 EnterCS: 55 assert (Arrivals.load() â‰  nullptr ) ; 56 csfn() ; 57 assert (Arrivals.load() â‰  nullptr ) ; 58 59 // Acquire Phase ... 60 // Case : entry list populated --appoint successor from entry list 61 if (succ â‰  nullptr ) { 62 assert (succ â†’ Gate.load(std::memory_order_acquire) == 0) ; 63 succ â†’ Gate.store (1, std::memory_order_release) ; 64 return ; 65 } 66 67 // Case : entry list and arrivals are both empty 68 // try fast-path uncontended unlock 69 auto k = Arrivals.load (std::memory_order_acquire) ; 70 if (k == &E || k == NEMO) { 71 if (Arrivals.compare_exchange_strong (k, nullptr )) return ; 72 } 73 74 // Case : entry list is empty and arrivals is populated 75 // New threads have arrived and pushed themselves onto the 76 // arrival stack. 77 // We now detach that segment, shifting those arrivals to 78 // become the next entry segment 79 Arrivals.exchange(NEMO) â†’ Gate.store (1, std::memory_order_release) ; 80 } 81 } ; Listing 2. Reciprocating Locks - Simplified Starting Point 2025-06-12 Â· Copyright Oracle and or its affiliates Dice and Kogan The variation show in List ng-3 avoids the use of the nd-of-segment, yielding a simplified algorithm. This also reduces context hat ne ds to be pas ed from Acquire to Rel ase to just he identi y, if any, of the suc es or. If ad it onal threads raced and ar ived and pushed onto the ar ival segment in the window betwe n the two exchange operations, the cur ent owner simply abdicates and relays ownership to the first hread in the newly detached entry list, and then proce ds to wait. While simpler, we beli ve this variant has po re ownership ne ds to relay through the race victim thread. progres properties, as, when the race manifest ,",
  "F Variations": "All the implementations in this section employ a double swap arrival for uncontended operations. Arriving threads, as usual, use an atomic exchange (swap) to first install the address of their waiting element, ğ¸ . If the exchange returned nullptr , then the thread has gained ownership. In that case, to try to avoid ğ¸ becoming submerged, they immediately exchange LOCKEDEMPTY into the Arrival word. If that second exchanged returned ğ¸ , then the double swap was successful, and the thread can immediately enter the critical section. If other threads arrived between between two exchange operations, however, then ğ¸ become submerged and the second swap detached a new arrival segment, with ğ¸ residing at the 'far' end. Our thread needs to recover from that condition. In Listing-3, in the event of an arrival race, our thread, which is still currently the owner, simply cedes ownership to the most recently arrived thread on the newly detached segment, identified via ğ‘… (Line-56) and then proceeds to wait. This approach is extremely simple and avoids any need to use or convey end-of-segment information. The only context that needs to be passed to the Release phase is the address of the successor . This approach also preserves the desirable thread-specific bounded bypass property, but, arguably, it has poorer progress properties and is no longer constant-time as ownership needs to be relayed for the victim to ğ‘… . In practice we find the arrival race between the two swaps to be rare, likely as the window of vulnerability is short, and because it takes time for the coherent interconnect to re-arbitrate the cache line between processors. Also, the race can only occur at the onset of contention, when the first arriving thread found the lock not held and then other threads arrived in quick succession. As a trivial and racy counter-measure, to reduce the rate of double swap arrivals, in Acquire , we can first load the arrival word, and, if it is nullptr , conditionally attempt an atomic CAS to opportunistically try to swing the word to LOCKEDEMPTY . If the CAS is successful, the thread can return immediately, otherwise we let control fall through into the swap path. This may act to reduce the incidence of doubleswap operations for mostly uncontended locks. In theory the double swap serves to increase theoretical RMR complexity complexity [4, 7]. In practice, however, as the lock is uncontended, the underlying cache line tends to remain in local modified state in arriving thread's cache, so the second exchange incurs very little additional cost and no additional coherence misses. As noted above, the double swap is executed only absent contention, where the arriving thread found the lock in unlocked state. Under sustained contention, we avoid the double swap . In this particular variation we can safely allocate the waiting elements on stack, if desired, as the address of the element ğ¸ never escapes the Acquire phase, even if Acquire and Release are implemented as different functions. In Listing-4 we show another variation that uses only one atomic fetch-and-add(1) in the Release phase. This form significantly alters the encoding of the Arrival word, using the low-order two bits as a tag. This form avoids the need for an explicit LOCKEDEMPTY distinguished value, and also avoids the need to convey end-of-segment addresses through the chain. The only context that needs to be passed to the Release phase is the address of the successor on the detached entry segment. Listing-5 shows a variation where a thread retains ownership even if other threads arrived in the double swap or exchange -fetch-and-add window. In this case the second atomic in the putative uncontended arrival path - be it exchange or fetch-and-add - detached an arrival segment identified via ğ‘… (Line-40). The owner records ğ‘… as its successor, and also passes the address of ğ¸ through ğ‘… towards the tail of entry segment (Line-59). In this particular implementation, the end-of-segment eos field is used only at the onset of contention and only when the race occured in the arrival window, and eos is always otherwise nullptr . In addition, the use of eos is encapsulated and restricted to the Acquire phase, and eos does not need to communicated as context to the corresponding Release phase, resulting in a more simple and easy to understand algorithm. We note that approaches in Listing-3 and Listing-5 can easily be combined into a form that avoids the use of atomic fetch-and-add , yielding Listing-6. 2025-06-12 Â· Copyright Oracle and or its affiliates Reciprocating Locks 1 struct ReciprocatingLock { 2 struct WaitElement { 3 std::atomic< int > Gate alignas(128) {0} ; 4 } ; 5 6 Atomic <WaitElement *> Arrivals { nullptr } ; 7 static inline const auto LOCKEDEMPTY = 8 (WaitElement *) uintptr_t(1) ; 9 10 public : 11 inline auto operator+ (std::invocable auto && csfn) â†’ void { 12 // Acquire Phase ... 13 // Can place E either on-stack or in TLS 14 // Consider alignas(128) 15 WaitElement E {} ; 16 auto succ = (WaitElement *) nullptr ; 17 auto tail = Arrivals.exchange (&E) ; 18 assert (tail â‰  &E) ; 19 // fast-path uncontended acquire 20 if (tail == nullptr ) { 21 auto R = Arrivals.exchange (LOCKEDEMPTY) ; 22 assert (R â‰  nullptr && R â‰  LOCKEDEMPTY) ; 23 if (R == &E) goto EnterCS ; 24 // Cede --Abdicate and fall thru into waiting phase ... 25 // Other threads arrived and pushed onto L â†’ Arrivals in 26 // the Exchange-Exchange window, above 27 // Our &E is now burried in the arrival stack. 28 // We expect this scenario to be relatively rare. 29 // 30 // We recover and compensate by passing or diverting 31 // ownership directly to the thread associated with R. 32 // Our own element, E, which resides within the now-detached 33 // stack, will eventually gain ownership via natural 34 // succession. 35 // R is the most-recently-arrivated thread (at the time 36 // of the 2nd exchange, above) and is the head of that 37 // detached stack. 38 // 39 // This form arguably does _not provide a classic bounded 40 // wait-free transfer of ownership, as, in this path, 41 // ownership needs to pass transiently through this thread 42 // as an intermediary before reaching R. 43 // We relay ownership immediately to R. 44 // Succession and progress depend on this thread making 45 // progress to enable the actual successor, R. 46 // 47 // And in general, if a lock acquisition episode has 2 48 // distinct waiting phases or modes for a given thread, 49 // then we don't have have a constant-time doorway. 50 // 51 // And while simpler, this approach can generate 52 // more coherent communication traffic as ownership bounces 53 // from this thread and then immediately to the intended 54 // successor, R. 55 assert (R â†’ Gate.load() == 0) ; 56 R â†’ Gate.store (1, std::memory_order_release) ; 57 // fall through into normal waiting ... 58 } 59 60 // Coerce LOCKEDEMPTY to nullptr 61 // succ will be our successor when we subsequently release 62 succ = (WaitElement *) (uintptr_t(tail) & ~1) ; 63 assert (succ â‰  &E) ; 64 65 // slow path : contention --waiting phase 66 while (E.Gate.load(std::memory_order_acquire) == 0) { 67 Pause() ; 68 } 69 70 // Critical section expressed as lambda 71 // We pass \"succ\" as context from Acquire to corresponding Release 72 EnterCS: 73 assert (Arrivals.load() â‰  nullptr ) ; 74 csfn() ; 75 assert (Arrivals.load() â‰  nullptr ) ; 76 77 // Release Phase ... 78 // Case : entry list populated --appoint successor from entry list 79 // If successor exists on implicit Entrylist, 80 // just pass ownership to that waiting thread. 81 if (succ â‰  nullptr ) { 82 assert (succ â†’ Gate.load() == 0) ; 83 succ â†’ Gate.store (1, std::memory_order_release) ; 84 return ; 2025-06-12 Â· Copyright Oracle and or its affiliates Dice and Kogan Listing 3. Reciprocating Locks - Relay 85 } 86 87 // Case : entry list and arrivals are both empty 88 // try fast-path uncontended unlock 89 // Can use either raw CAS or polite LD-CAS 90 auto k = LOCKEDEMPTY ; 91 if (Arrivals.compare_exchange_strong (k, nullptr )) return ; 92 93 // Case : entry list is empty and arrivals is populated 94 // New threads have arrived and pushed themselves onto the 95 // arrival stack. 96 // We now detach that segment, shifting those arrivals to 97 // become the next entry segment 98 // Decapitate the arrival segment, leaving the lock held. 99 // Reprovision the entrylist with those arrivals. 100 auto w = Arrivals.exchange (LOCKEDEMPTY) ; 101 assert (w â‰  nullptr && w â‰  LOCKEDEMPTY) ; 102 assert (w â†’ Gate.load() == 0) ; 103 w â†’ Gate.store (1, std::memory_order_release) ; 104 } 105 } ; 2025-06-12 Â· Copyright Oracle and or its affiliates Reciprocating Locks 1 // @ Avoids LOCKEDEMPTY encoding 2 // @ Arrivals encoding : tagged pointer 3 // E:00 : Locked --Arrival stack populated 4 // E:01 : Locked --Arrival segment logically detached and empty 5 // *:10 : Unlocked -- Equivalent to nullptr : end-of-segment 6 // *:11 : illegal and undefined 7 // 0:00 : illegal and undefined 8 // @ Intentionally designed encoding above to work with atomic fetch_add(1) 9 // fetch_add(1) low-bit values drives state machine 10 // Arrived â†’ detached â†’ unlocked 11 // @ Must initialize Arrivals to 0:10 12 // @ Very tight paths with reduced branch complexity 13 // Retains constant-time wait-free arrival and unlock paths 14 // Retains usual Reciprocating admission order 15 // Retains thread-specific bounded bypass 16 // Release phase has only one atomic ! 17 struct ReciprocatingLock { 18 struct WaitElement { 19 std::atomic< int > Gate alignas(128) {0} ; 20 } ; 21 22 std::atomic <WaitElement *> Arrivals { (WaitElement *) uintptr_t(2) } ; 23 static inline constinit thread_local WaitElement E alignas(128) {0} ; 24 25 static WaitElement * AnnulMarked (WaitElement * v) { 26 auto u = uintptr_t(v) ; 27 return (WaitElement *) (u & ((u & 1) - 1)) ; 28 } 29 30 // Mark and return MRA element but leave it in emplaced on chain 31 WaitElement * FetchAndMark () { 32 return (WaitElement *) 33 (((std::atomic<uintptr_t> *) & this â†’ Arrivals) â†’ fetch_add(1)) ; 34 } 35 36 public : 37 inline auto operator+ (std::invocable auto && csfn) â†’ void { 38 // Acquire Phase ... 39 E.Gate.store (0, std::memory_order_release) ; 40 auto succ = Arrivals.exchange (&E) ; 41 assert (succ â‰  &E) ; 42 assert ((uintptr_t(succ) & 3) â‰  3) ; 43 assert (uintptr_t(succ) â‰  0) ; 44 if (uintptr_t(succ) & 2) { 45 // We are the owner -- uncontended acquisition 46 // exchange() transitioned Arrivals low-order bits from 47 // unlocked to lock state 48 auto R = FetchAndMark () ; 49 assert ((uintptr_t(R) & 3) == 0) ; 50 succ = nullptr ; 51 // fast path uncontended arrival ... 52 if (R == &E) goto EnterCS ; 53 54 // New threads arrived within the narrow 55 // exchange-fetch_add() window. 56 // This should be relatively rare 57 // Occurs only at the onset of contention 58 // Just delegate ownership of lock to the 59 // most-recently-arrived-thread \"R\" 60 // R is now the head of a detached segment 61 // Our E resides buried at the distal end of that 62 // detached segment. 63 // succ is effectively null 64 assert (R â‰  nullptr ) ; 65 assert (R â†’ Gate.load() == 0) ; 66 R â†’ Gate.store (1, std::memory_order_release) ; 67 // fall through into waiting 68 } 69 70 succ = AnnulMarked (succ) ; 71 assert ((uintptr_t(succ) & 3) == 0) ; 72 assert (succ â‰  &E) ; 73 while (E.Gate.load(std::memory_order_acquire) == 0) { 74 Pause() ; 75 } 76 77 // Critical section expressed as lambda 78 // pass \"succ\" as context Acquire to corresponding Release 79 EnterCS: 80 assert (succ â‰  &E) ; 81 assert ((uintptr_t(Arrivals.load()) & 2) == 0) ; 82 assert (uintptr_t(Arrivals.load()) â‰  0) ; 83 csfn() ; 2025-06-12 Â· Copyright Oracle and or its affiliates Dice and Kogan Listing 4. Reciprocating Locks - fetch-add 84 assert ((uintptr_t(Arrivals.load()) & 2) == 0) ; 85 assert (uintptr_t(Arrivals.load()) â‰  0) ; 86 87 // Release Phase ... 88 if (succ == nullptr ) { 89 // Mark element but leave it in emplaced on chain 90 succ = FetchAndMark () ; 91 assert ((uintptr_t(succ) & 2) == 0) ; 92 assert (uintptr_t(succ) â‰  0) ; 93 if (uintptr_t(succ) & 1) return ; 94 95 // We just detached fresh arrivals 96 assert ((uintptr_t(succ) & 3) == 0) ; 97 assert (succ â‰  nullptr ) ; 98 } 99 100 assert (succ â†’ Gate.load() == 0) ; 101 succ â†’ Gate.store (1, std::memory_order_release) ; 102 } 103 } ; 2025-06-12 Â· Copyright Oracle and or its affiliates Reciprocating Locks 1 struct ReciprocatingLock { 2 struct WaitElement { 3 std::atomic < int > Gate alignas(128) {0} ; 4 std::atomic <WaitElement *> eos { nullptr } ; 5 } ; 6 7 std::atomic <WaitElement *> Arrivals { (WaitElement *) uintptr_t(2) } ; 8 9 // Variations for E placement : 10 // @ auto on-stack in operator()+ 11 // @ class scoped : static inline thread_local 12 // @ method local : static thread_local 13 static inline constinit thread_local WaitElement E alignas(128) {0, nullptr } ; 14 15 static WaitElement * AnnulMarked (WaitElement * v) { 16 auto u = uintptr_t(v) ; 17 return (WaitElement *) (u & ((u & 1) - 1)) ; 18 } 19 20 // Mark and return MRA element but leave it in emplaced on chain 21 WaitElement * FetchAndMark () { 22 return (WaitElement *) 23 (((std::atomic<uintptr_t> *) & this â†’ Arrivals) â†’ fetch_add(1)) ; 24 } 25 26 public : 27 inline auto operator+ (std::invocable auto && csfn) â†’ void { 28 // Acquire Phase 29 E.eos.store ( nullptr , std::memory_order_release) ; 30 E.Gate.store (0 , std::memory_order_release) ; 31 auto succ = Arrivals.exchange (&E) ; 32 assert (succ â‰  &E) ; 33 assert ((uintptr_t(succ) & 3) â‰  3) ; 34 assert (uintptr_t(succ) â‰  0) ; 35 if (uintptr_t(succ) & 2) { 36 // We are the owner -- uncontended acquisition 37 // exchange() transitioned Arrivals low-order bits from 38 // \"unlocked\" to \"locked\" state 39 // succ is logically nullptr 40 auto R = FetchAndMark() ; 41 assert ((uintptr_t(R) & 3) == 0) ; 42 if (R == &E) [[likely]] { 43 succ = nullptr ; 44 // fast path uncontended arrival ... 45 goto EnterCS ; 46 } 47 // New threads arrived within the exchange-fetch_add() 48 // window. 49 // As the window is narrow this situation should be 50 // relatively rare. 51 // We have the onset of contention 52 // Those new arrivals become our detached entry segment. 53 // Our &E is now buried at the distal end of the 54 // arrival stack 55 // We need to convey &E thru the chain during succession 56 // so that the penultimate thread in the entry segment is 57 // able to detect and treat &E as logical end-of-segment 58 assert (R â†’ eos.load() == nullptr ) ; 59 R â†’ eos.store (&E, std::memory_order_release) ; 60 succ = R ; 61 } else { 62 63 succ = AnnulMarked (succ) ; 64 assert ((uintptr_t(succ) & 3) == 0) ; 65 assert (succ â‰  &E) ; 66 while (E.Gate.load(std::memory_order_acquire) == 0) { 67 Pause() ; 68 } 69 70 auto eos = E.eos.load (std::memory_order_acquire) ; 71 if (eos == nullptr ) [[likely]] { 72 goto EnterCS ; 73 } 74 75 assert (succ â‰  nullptr ) ; 76 77 // This should be rare 78 // We only reach here at the onset of contention _AND 79 // when the initial arriving thread, which siezed the lock, 80 // raced in the exchance()-cas() window above and then 81 // became submerged on the arrival stack. 82 // Crucially, we usually avoid coherence traffic generated 83 // by storing into succ â†’ eos 84 2025-06-12 Â· Copyright Oracle and or its affiliates Dice and Kogan 85 // Detect end-of-segment sentinel-marker address 86 // Annul or squash/quash succ as needed 87 if (eos == succ) { 88 succ = nullptr ; 89 goto EnterCS ; 90 } 91 92 // propapage eos thru chain toward tail 93 assert (succ â‰  nullptr ) ; 94 assert (succ â†’ Gate.load() == 0) ; 95 assert (succ â†’ eos.load() == nullptr ) ; 96 succ â†’ eos.store (eos, std::memory_order_release) ; 97 } 98 99 // Critical section expressed as lambda 100 // pass \"succ\" as context from Acquire to corresponding Release 101 EnterCS: 102 assert (succ â‰  &E) ; 103 assert ((uintptr_t(Arrivals.load()) & 2) == 0) ; 104 assert (uintptr_t(Arrivals.load()) â‰  0) ; 105 csfn() ; 106 assert ((uintptr_t(Arrivals.load()) & 2) == 0) ; 107 assert (uintptr_t(Arrivals.load()) â‰  0) ; 108 109 // Release Phase ... 110 // Case : entry list populated 111 // appoint successor from entry list 112 if (succ â‰  nullptr ) { 113 assert (succ â†’ Gate.load() == 0) ; 114 succ â†’ Gate.store (1, std::memory_order_release) ; 115 return ; 116 } 117 118 // Mark MRA element but leave it in emplaced on chain 119 // FetchAndMark() 120 // converts locked+arrived to locked+detached 121 // converts locked+detached to unlocked 122 auto k = FetchAndMark () ; 123 assert ((uintptr_t(k) & 2) == 0) ; 124 assert (uintptr_t(k) â‰  0) ; 125 if (uintptr_t(k) & 1) return ; 126 127 // We just detached fresh arrivals 128 assert ((uintptr_t(k) & 3) == 0) ; 129 assert ((uintptr_t(k) & ~3) â‰  uintptr_t(&E)) ; 130 assert (k â‰  nullptr ) ; 131 assert (k â†’ Gate.load() == 0) ; 132 k â†’ Gate.store (1, std::memory_order_release) ; 133 } 134 } ; Listing 5. Reciprocating Locks - Simplified 2025-06-12 Â· Copyright Oracle and or its affiliates Reciprocating Locks 1 struct ReciprocatingLock { 2 struct WaitElement { 3 std::atomic< int > Gate alignas(128) {0} ; 4 std::atomic<WaitElement *> eos { nullptr } ; 5 } ; 6 7 static inline constinit thread_local WaitElement 8 E alignas(128) {0, nullptr } ; 9 Atomic <WaitElement *> Arrivals { nullptr } ; 10 static inline const auto LOCKEDEMPTY = (WaitElement *) uintptr_t(1) ; 11 12 public : 13 inline auto operator+ (std::invocable auto && csfn) â†’ void { 14 // Acquire Phase ... 15 E.eos.store ( nullptr , std::memory_order_release) ; 16 E.Gate.store (0 , std::memory_order_release) ; 17 WaitElement * succ = nullptr ; 18 19 auto tail = Arrivals.exchange (&E) ; 20 assert (tail â‰  &E) ; 21 if (tail == nullptr ) { 22 // fast-path uncontended acquire --We now hold the lock 23 // Try to replace &E with SIMPLELOCKED. 24 auto R = Arrivals.exchange (LOCKEDEMPTY) ; 25 assert (R â‰  nullptr ) ; 26 if (R â‰  &E) { 27 // We couldn't replace E, but we otherwise detached 28 // a new arrival chain, which becomes our entry segment. 29 // This should be rare -Onset of contention 30 // Other threads arrived and pushed onto Arrivals in 31 // the exchange-exchange window, above. 32 // Our &E is now buried at distal end of arrival stack 33 // The exchange() above snapped off a new entry segment, 34 // which is anchored at \"R\". 35 R â†’ eos.store (&E, std::memory_order_release) ; 36 succ = R ; 37 } 38 } else { 39 // Coerce LOCKEDEMPTY to nullptr 40 // succ will be our successor when we subsequently release 41 succ = (WaitElement *) (uintptr_t(tail) & ~1) ; 42 assert (succ â‰  &E) ; 43 44 // slow path : contention --waiting phase 45 while (E.Gate.load (std::memory_order_acquire) == 0) { 46 Pause() ; 47 } 48 49 auto eos = E.eos.load(std::memory_order_acquire) ; 50 assert (eos â‰  &E) ; 51 if (eos â‰  nullptr ) [[unlikely]] { 52 // Arriving at this point should be rare. 53 // We only reach here at onset of contention _AND 54 // when the initial arriving thread, which siezed the 55 // lock, raced in the exchance()-exchange() window above 56 // and then became submerged on the arrival stack. 57 // Once in steady-state contention, control will not 58 // reach here. 59 // Crucially, we can then usually avoid coherence 60 // traffic generated by storing into succ â†’ eos. 61 62 // Detect logical end-of-segment sentinel-marker address 63 // Annul or quash succ as needed 64 if (eos == succ) { 65 succ = nullptr ; 66 goto EnterCS ; 67 } 68 69 // propapage eos thru chain toward tail 70 assert (succ â‰  nullptr ) ; 71 assert (succ â†’ Gate.load() == 0) ; 72 assert (succ â†’ eos.load() == nullptr ) ; 73 succ â†’ eos.store (eos, std::memory_order_release) ; 74 } 75 } 76 77 // Critical section expressed as lambda 78 // We pass \"succ\" as context from Acquire to corresponding Release 79 EnterCS: 80 assert (Arrivals.load() â‰  nullptr ) ; 81 csfn() ; 82 assert (Arrivals.load() â‰  nullptr ) ; 83 84 // Release Phase ... 2025-06-12 Â· Copyright Oracle and or its affiliates Dice and Kogan Listing 6. Reciprocating Locks - Variation 85 if (succ == nullptr ) { 86 // Case : entry list and arrivals are both empty 87 // try fast-path uncontended unlock 88 auto v = Arrivals.cas (LOCKEDEMPTY, nullptr ) ; 89 assert (v â‰  nullptr && v â‰  &E) ; 90 if (v == LOCKEDEMPTY) return ; 91 92 // Detach a new arrival segment 93 succ = Arrivals.exchange (LOCKEDEMPTY) ; 94 assert (succ â‰  nullptr && succ â‰  LOCKEDEMPTY) ; 95 } 96 assert (succ â†’ Gate.load() == 0) ; 97 succ â†’ Gate.store (1, std::memory_order_release) ; 98 } 99 } ; 2025-06-12 Â· Copyright Oracle and or its affiliates Reciprocating Locks",
  "G Retrograde Ticket Lock": "In Listing-7 we describe the retrograde ticket lock algorithm, which mimics the admission order policy of Reciprocating Locks but is implemented as a version of the classic ticket lock algorithm. Figure 4. Retrograde Ticket Locks in Action 0 ğµğ‘ğ‘ ğ‘’ â† ğºğ‘Ÿğ‘ğ‘›ğ‘¡ ğ‘‡ğ‘œğ‘ ğ‘‡ğ‘–ğ‘ğ‘˜ğ‘’ğ‘¡ â†’ ğ¸ğ‘›ğ‘¡ğ‘Ÿğ‘¦ ğ´ğ‘Ÿğ‘Ÿğ‘–ğ‘£ğ‘ğ‘™ Theclassic ticket lock uses grant and ticket fields, where arriving threads atomically fetch-and-increment ticket and then wait for the assigned ticket value to equal grant and the corresponding Release operator increments grant . For retrograde ticket locks we add new per-lock base and top fields. In Figure-4 we show a number line depicting the trajectory of the lock fields. Assigned tickets in the range base -top represent the entry segment and those in range top -ticket represent the arrival segment. Regions colored blue represent ticket values already been granted ownership and no longer waiting, while those colored red are waiting for admission. Crucially, while the entry segment remains populated with waiting threads, the Release operator advances the grant field in a descending fashion, yielding a retrograde order. When grant reaches base , the entry segment is exhausted, and we reprovision the entry segment by setting base to top and top to ticket . That is, the arrival segment becomes the new entry segment. By convention, top and base are accessed only by the current lock holder and only within the Release operation. As we use magnitude-based comparisons, arithmetic rollover and aliasing of the ticket and grant becomes a concern. To address that issue we simply ensure that ticket -related fields are 64-bit integers. Assuming a processor could increment a value at most once per nanosecond, these fields would not overflow and wrap around in less than 200 years, so arithmetic overflow is not a practical concern. Assuming that std::atomic<int64_t>:fetch_add(1) is constant-time - which depends on both the library implementation of std::atomic and on the platform capabilities - our doorway phase is also also constant-time. Only one atomic readmodify-write fetch_add operation is required in Acquire and none in Release , and Release runs in constant-time. We are not constrained to simple retrograde admission order within the entry segment. Lets say our entry segment consists of threads ğ´ -ğµ -ğ¶ -ğ· which hold ticket values 1005 -1004 -1003 -1002, respectively. ğ´ is the most recently arrived thread in the entry segment. Each thread waits for its corresponding ticket value to appear in the Grant field, which confers and conveys ownership. For retrograde admission, which mimics Reciprocating Locks, our admission order is ğ´ then ğµ then ğ¶ then ğ· , using descending ticket values with the entry segment. A prograde admission order of ğ· then ğ¶ then ğµ then ğ´ , with ascending ticket values, is tantamount to simple classic FIFO ticket locks. By using ticket values, we are able, in constant-time, to activate and enable a thread at any arbitrary position (offset) in the entry segment, unlike Reciprocating Locks where the order is dictated, as threads know only the identity of their immediate neighbor. Using ticket-based succession allows a wider variety of admission orders, compared to Reciprocating Locks, as ticket-base successor allows random access to the elements of the entry segment and thus more latitude for succession order. For example, a viable approach is the following. In the Release operator we run a biased Bernoulli trial, and, based on the outcome, select the successor from either the head of the remaining entry segment - the most-recently arrived thread - or the tail, which is the least recently arrived thread, but with the probability favoring the head. This yields a mostly LIFO admission order with the entry segment - mostly retrograde, but occasionally prograde. Crucially, we use the tunable Bernoulli probability to strike a balance between fairness over a period, and aggregate throughput. Such randomization is sufficient to break or perturb long-term unfairness arising from repeating palindromic admission cycles. As an optimization, to reduce the use of the random number generator, we can implement a per-lock CountDown variable. The Release operator always decrements the counter. If the value is found greater than zero, we extract a successor from the head of the entry segment. Otherwise, we extract a successor from the tail, compute a small uniform random integer in the range 1 ..ğ‘€ and then reset CountDown to that value. For our purposes, a simple low-latency low-quality pseudo-random generator number suffices, such as a singleword Marsaglia xor-shift variant[44]. A related technique is, in Release , when detaching a new arrival segment, to run a Bernoulli trial to pick a succession direction - prograde or retrograde but biased toward and favoring retrograde - and then use that direction for the entirety of the segment. These forms use randomization to provide long-term statistical avoidance of both unfair admission and unfair cache residency, but still provide better aggregate cache residency (and throughput) than simple FIFO while also retaining the desirable thread-specific bounded bypass property. For all the above, we can also borrow the waiting scheme from TWA and construct versions that avoid global spinning. MRAT = most rec ntly ar ived thread strikes compromise; trade-off; bal nce; tension derangement, disorder Pick random starting posit on and irection We conjecture that retrograde palindromic order may also provide in ate NUMA ben fits, reducing virtue of the palindromic schedule. If we conceptualize NUMA affinity as a deg lock migration nerate ca he of just one as a cor l ary to improved ca he residency, we also reduce lock migration rates via by lement, hen palindromic schedule. 2025-06-12 Â· Copyright Oracle and or its affiliates Dice and Kogan 1 struct TicketLock { 2 std::atomic < int > Ticket {0} ; 3 std::atomic < int > Grant {0} ; 4 } ; 5 // Classic Ticket lock ... 6 void TicketAcquire (TicketLock * L) { 7 auto ticket = L â†’ Ticket.fetch_add(1) ; 8 while (L â†’ Grant â‰  ticket) { 9 Pause() ; 10 } 11 } 12 void TicketRelease (TicketLock * L) { 13 auto grant = L â†’ Grant.load() + 1 ; 14 L â†’ Grant.store (grant, std::memory_order_release) ; 15 } 16 // \"Retrograde\" ticket lock 17 // provides the same admission order as reciprocating locks 18 // Note that the Acquire() operator is identical to that of 19 // the baseline ticket lock, above. 20 // All changes relative to ticket locks are encapsulated 21 // in the unlock operator. 22 // 23 // Invariant : Ticket >= Top >= Grant >= Base 24 // Threads with tickets values in [Top,Base] are admitted in reverse order 25 // Waiting : Ticket thru Top union with Grant thru Base 26 // Completed : Top thru Grant union with those < Base 27 // EntrySet : Grant thru Base 28 // Arrivals : Ticket thru Top 29 // Note that only the current lock hold accesses Top and Base. 30 // In particular, the lock itself protects Top and Base. 31 struct RetrogradeLock { 32 std::atomic <int64_t> Ticket {0} ; 33 std::atomic <int64_t> Grant {0} ; 34 int64_t Top = 0 ; 35 int64_t Base = 0 ; 36 } ; 37 void RetrogradeAcquire (RetrogradeLock * L) { 38 auto ticket = L â†’ Ticket.fetch_add(1) ; 39 while (L â†’ Grant â‰  ticket) { 40 Pause() ; 41 } 42 } 43 void RetrogradeRelease (RetrogradeLock * L) { 44 auto grant = L â†’ Grant.load() - 1 45 if (grant > L â†’ Base) { 46 // Region of reverse admission order ... 47 // working our way backward through entry set 48 L â†’ Grant = grant ; 49 return ; 50 } 51 auto Hi = L â†’ Top ; 52 L â†’ Base = Hi ; 53 auto tmp = L â†’ Ticket.load() ; 54 L â†’ Top = tmp -1 ; 55 if (tmp == (Hi + 1)) { 56 // Apparently no waiters 57 // Uncontended release --set to \"unlocked\" state with 58 // Ticket == Grant. 59 // Benign if Ticket advances concurrently after we read it. 60 // The store into L â†’ Grant must become visible after the 61 // other stores. 62 L â†’ Top = tmp ; 63 L â†’ Base = tmp ; 64 L â†’ Grant = tmp ; 65 } else { 66 // Contention with waiters --stays locked 67 L â†’ Grant = tmp - 1 ; 68 } 69 } Listing 7. Retrograde Ticket Locks 2025-06-12 Â· Copyright Oracle and or its affiliates Reciprocating Locks",
  "H Alternative 'Gated' Formulation": "1 // @ This variant uses a concurrent pop-stack coupled with a \"LeaderGate\" flag. 2 // @ Aspects of the algorithm are reminiscent of MCSH, Fissile Locks, and QSpinLock. 3 // @ Enjoys the following properties : 4 // * Constant-time arrival and unlock() paths 5 // * bounded bypass : population bounded bypass -- avoids starvation 6 // * single-phase waiting 7 // @ The admission order differs slightly from Reciprocating Locks, 8 // but worst-case lock-induced long-term unfairness remains bounded at 2X 9 // and we also continue to have bounded bypass (anti-starvation). 10 // We provide LIFO admission order within a segment, but 11 // segments themselves are processed in FIFO-FCFS order. 12 // intra-segment â†’ LIFO 13 // inter-segment â†’ FCFS 14 // @ Invariants 15 // At most one thread waits on the LeaderGate at any given time -segment leader 16 // We thus have 1v1 for the next leader vs the retiring (outgoing) leader. 17 // Only leader can transition LeaderGate 0 â†’ 1 18 // Only current owner, at the end of the segment, transitions LeaderGate 1 â†’ 0 19 // Incoming leaders must wait for the previous segment to complete, 20 // and for the outgoing leader to depart. 21 // @ We use LeaderGate as an interlock to separate entry and arrival segment generations. 22 // Each generation has one distinguished thread in the leader role. 23 // At most one such leader thread waits on Gate at any one time. 24 // @ Note that we could replace the LeaderGate with any full lock 25 // as long as that lock was \"Thread-oblivious\" and could be safely 26 // unlocked by some thread other than the one that acquired the lock. 27 // We would want the lock to be FIFO and if possible, to enjoy 28 // local + private spinning, but we also observe that as the LeaderGate 29 // only encounters 1v1 concurrency, a full arbitrary mutex is overkill 30 // and not necessary. 31 // 32 // @ Thread states : we can have participating threads in at most 4 states, 33 // concurrently, when the lock is under contention 34 // * singleton current owner 35 // * Threads waiting on detached entry segmemt 36 // * Singleton leader thread waiting at leader \"gate\" 37 // The leader is 1st thread to arrive on a fresh arrival segment 38 // * Threads waiting on attached arrival segment 39 // 40 // @ Variations that can impact performance 41 // * Placement of LeaderGate and Tail fields 42 // packed & adjacent on the same cache line vs sequestered to 43 // reduce false sharing. 44 // * Employ \"HemLock\" CTR (Coherence Traffic Reduction) optimization 45 // for spin loops. 46 // * Placement of the WaitElemnt : on-frame or TLS singleton 47 // * It is possible to collapse the LeaderGate field into the 48 // low-order bit of the Tail field, resulting in a lock 49 // that requires just one word of storage. 50 struct ReciprocatingGated { 51 52 struct WaitElement { 53 // eos field serves as transfer-of-ownership flag _and to efficiently 54 // convey the End-of-segment address through the chain 55 std::atomic <WaitElement *> eos alignas(128) { nullptr } ; 56 } ; 57 58 // Tail is the arrival segment for the concurrent pop-stack 59 std::atomic <WaitElement *> Tail { nullptr } ; 60 61 // Leader Gate - alfa Prime -Valve -Staging 62 // To reduce false sharing between the Tail and LeaderGate fields we may elect to 63 // sequester -- isolate and segregate --those fields as the sole residents 64 // of their respective cache lines (sectors), via alignas(128). 65 std::atomic < int > LeaderGate alignas(128) {0} ; 66 67 inline auto operator+(std::invocable auto && csfn) â†’ void { 68 // Arrival : Push E onto the concurrent pop-stack 69 // Placement for E : either on-frame or TLS singleton 70 WaitElement E {} ; 71 auto prv = Tail.exchange (&E) ; 72 assert (prv â‰  &E) ; 73 if (prv â‰  nullptr ) { 74 // Slow path - contention --need to wait 75 // We are follower within segment 76 // Consider : could use HemLock CTR optimization here and spin using exchange 77 // That, in turn, would obviate the need to clear Gate at the top of Acquire 78 // and would avoid the MESI/MOESI/MESIF S â†’ M coherence upgrade. 79 // We have private and local busy waiting ... 80 WaitElement * eos ; 81 while ((eos = E.eos.load (std::memory_order_acquire)) == nullptr ) { 82 Pause() ; 2025-06-12 Â· Copyright Oracle and or its affiliates Dice and Kogan 83 } 84 assert (eos â‰  nullptr ) ; 85 assert (eos â‰  &E) ; 86 assert (Tail.load() â‰  &E) ; 87 // At this point E resides on the detached entry segment 88 89 // Execute CS which is expressed as C++ lambda 90 // pass eos and prv as context from lock() to corresponding unlock() phase 91 EnterCSF : 92 assert (LeaderGate.load() â‰  0) ; 93 csfn() ; 94 assert (LeaderGate.load() â‰  0) ; 95 96 if (eos â‰  prv) { 97 // systolic propagation of ownership 98 // relay through detached entry segment chain 99 // propagate wakeup indication and eos value to prv 100 assert (prv â†’ eos.load() == nullptr ) ; 101 prv â†’ eos.store (eos, std::memory_order_release) ; 102 } else { 103 // We reached effective or logical end of the detached entry segment 104 // This is the terminus -omega 105 // The entry segment is exhausted 106 // Re-open the gate to admit the next generation 107 LeaderGate.store (0, std::memory_order_release) ; 108 } 109 return ; 110 } 111 112 // We are the arrival segment leader -- the \"alfa\" role 113 // A segment has only one such thread acting in the leader role 114 // at any given time. 115 // Wait for the the previous generation to complete, depart and vacate 116 // Claim : at most one thread waits here at any one time 117 // We have at most 1v1 contention on LeaderGate at this juncture : 1 waiter vs 1 owner 118 // 119 // We enjoy private but NOT local spinning 120 // Intuitively, LeaderGate seperates generations --entry segment vs arrival 121 // LeaderGate â‰  0 implies lock is held and entry segment is being processed 122 // 123 // In more detail ... 124 // We enjoy private spinning, below, with at most one thread spinning on LeaderGate 125 // at any given time, but LeaderGate is not necessarly local (NUMA co-homed) to the 126 // waiting thread, which can make a performance difference on Intel home-based UPI fabrics. 127 // This is easy to fix via indirection, however, where the waiting thread 128 // installs a pointer to a local waiting location into LeaderGate. 129 // This approach also permits us to retain performance even if we place the 130 // LeaderGate and Arrivals fields on the same cache line, which in turn results in a 131 // very small lock body. 132 // We might then also employ the HemLock CTR \"coherence traffic reduction\" optimization, 133 // where we busy-wait with an atomic exchange. 134 // Currently, to avoid false sharing between LeaderGate and Arrivals, we 135 // sequester those fields via alignment. 136 // This works perfectly well, but bloats the lock body size to 2 sectors (lines). 137 138 while (LeaderGate.load (std::memory_order_acquire) â‰  0) { 139 Pause() ; 140 } 141 LeaderGate.store (1, std::memory_order_release) ; 142 143 // Execute CS which is expressed as C++ lambda 144 // Note that no context needs to be passed from lock() to corresponding unlock(). 145 // NOTA BENE : we can safely detach Tail either early -before 146 // executing the CS --or late, after executing the CS. 147 // Invariant : only the leader can detach the Tail. 148 // 149 // If we decompose into classic lock() and unlock() operators, 150 // and we opt to place E on-frame, in lock(), instead of in TLS, then it 151 // makes sense to detach early, before the CS, while E remains in scope. 152 // This helps mitigate concerns about C++ UB dangling references. 153 // Critically, with early detach, E remains live, extant and in-scope, 154 // in the lock() operator, when we detach and privatize the segment on which E 155 // is resident. 156 // E resides at the distal end of that detached segment and may be buried or 157 // submerged on the chain --zombie. 158 // The segment, once detached, is \"closed\" and no threads can join 159 // by pushing (prepending) onto the concurrent pop-stack. 160 // As no threads can join, we are not exposed to aliasing false-equal 161 // address-based end-of-segment sentinel check pathologies. 162 // Placing E in TLS obviates all such lifecycle concerns. 163 164 EnterCSL : 165 assert (LeaderGate.load() â‰  0) ; 166 csfn() ; 167 assert (LeaderGate.load() â‰  0) ; 2025-06-12 Â· Copyright Oracle and or its affiliates",
  "I Alternative '2 Lanes' Formulation - Imposes Long-Term Fairness": "1 // @ This form uses _two underlying concurrent pop-stack \"lanes\". 2 // Arriving threads pick a lane at random and push themselves on. 3 // The leaders on each lane then must compete for a top-level \"leader lock\". 4 // That leader lock algorithm must be \"thread-oblivious\" in that one 5 acquire thread can // the lock and thread can subsequently other some 6 // release the lock, effectively making it a bounded-binary semaphore. 7 // For illustration, we implement the leader lock as a simple Ticket Lock. 8 // With 2 lanes, at most 2 threads compete for the leader lock at a given 9 // time, so a ticket lock suffices and scales reasonably in that operating regime. 10 use 2 long-term lanes and random lane selection to impose // @ We 11 fairness admission the overcome and specifically to // statistical 12 // unfair admission schedules that reciprocating locks can induce and exhibit. 13 // That is, we use 2 lanes, not for scalability, but rather to perturb the 14 // admission schedule via randomization. 15 // We claim this approach suffices to impose long-term fairness. 16 // Our primary goal for this particular variation was to impose long-term 17 the of properties underlying while preserving fairness desirable // the the 18 or RMR scalability impacting // while not lock, Reciprocating complexity. 19 // @ All the usual desirable Reciprocating Lock properties still hold. 20 Assuming the leader lock is FIFO-FCFS ... // 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 // We continue to have constant-time arrival and unlock paths. // We continue to have bounded-bypass to avoid starvation. // Each thread has single-phase single-condition waiting. // @ We have local and private busy-waiting _except for the // lane leaders that are competing for the ticket lock. // This concern is easy to rectify, however, by switching the leader lock // to an ABQL lock (Anderson's Array-Based Queue Lock) or a // Partitioned Ticket Lock variant, where waiting threads can // install a pointer into a small, bounded waiting array slot and then // spin on a local location reachable by that pointer. // @ Desiderata for the top-level leader lock : criteria // * Thread-oblivious \"TO\" // // // // // // // // // // // // // // // // // // // // // // // // // @ // * Constant-time arrival doorway and unlock paths. * FIFO-FCFS or at minimum very tight bounded bypass * Avoid memory lifecycle entanglements, memory allocation, non-trivial constructors and destructors * local spinning : threads busy-wait on a location co-homed with the thread This is helpful on Intel UPI, with home-based snooping. This particular property is desirable but negotiable. * private spinning : at most one thread spins on a given location (cache line) at any one time * bounded \"low touch\" RMR complexity -- Reasonable * Minimize coherence traffic and false sharing of the leader lock fields vs the lanes array @ Use alignas(128) to sequester --isolate and segregate --the leader lock fields away from the lanes fields. This works well, but increases the size of the lock body to at least two cache sectors, which is not desirable, so this is not our preferred approach. @ Place the leader lock fields immediately abutting the lanes fields but pick a leader lock algorithm and implementation that is parsimonious and minimizes access to the leader lock body fields. This works to minimize false sharing but also yields a small(er) lock body. * Taken together, the above _usually implies some type of indirect waiting and that we avoid busy-waiting directly on fields in the top-level leader lock body. As an optional optimization, we may opt to sequester (isolate and segregate), using the Lanes array away Ticket Lock alignas(128), fields, from the // to reduce false sharing, but at the cost of increasing the lock body size. // @ We have considerable choice on how we select lanes : // * Thread-local random CBRN (Counter-based RNG) // // // // // // // // // * Thread-local random CBRN but with consecutive runs on the same index * Thread-local random CBRN but only advance CBRN when waiting * Apply a hash function to the Ticket value to select a lane. * We can also use balanced vs imbalanced lane selection -biased selection. Our current with equal probablity, picks lanes implementation via a simple Bernoulli trial on the CBRN, but imbalanced lane selection probabilities also suffice. We just need to perturb the Reciprocating admission order. * Bias lane selection based on the geographic position on the 71 // requesting thread within the system topology 72 // + P/E Fire/Ice Big/Little --we might favor lane#1 for the E cores, for instance 73 // This yields a lock that is asymmetry-aware. 74 // + NUMA node 75 // This yields a NUMA-aware lock that reduces lock migration rates 76 77 78 79 80 81 82 // // // // // For uniprocessors with a besh based coherence topology, we might impose an artifical partion of CPUs into abstract hemispheres, where we minimize the average pairwise hop distance between CPUs within a hemisphere. We could then bias lane selection depending which hemisphere thread requesting thread belongs to. struct ReciprocatingGated2XLanes { + 2025-06-12 Â· Copyright Oracle and or its affiliates Reciprocating Locks 83 struct WaitElement { 84 // eos field serves as transfer-of-ownership flag _and to efficiently 85 // convey the End-of-segment address through the chain 86 std::atomic <WaitElement *> eos alignas(128) { nullptr } ; 87 } ; 88 89 struct Lane { 90 // Tail is the arrival segment for the concurrent pop-stack 91 std::atomic <WaitElement *> Tail { nullptr } ; 92 } ; 93 94 Lane Lanes [2] { nullptr , nullptr } ; 95 96 // Top-level Leader Lock -implemented as a ticket lock - prime 97 std::atomic <uint64_t> Ticket {0} ; 98 std::atomic <uint64_t> Grant {0} ; 99 100 static inline constexpr int FastPathEnabled = 0 ; 101 102 int LeaderIsLocked() { 103 auto grn = Grant.load() ; 104 auto tkt = Ticket.load() ; 105 return tkt â‰  grn ; 106 } 107 108 // Golden ratio \"phi\" -- Fibonacci hashing 109 static uint32_t HashPhi32 (uint32_t v) { 110 static const uint64_t invphi = 0x9e3779b9 ; 111 return uint64_t(v * invphi) >> 32 ; 112 } 113 114 inline auto operator +(std::invocable auto && csfn) â†’ void { 115 116 // Optional optimization fast-path for uncontended operation 117 // requires 64-bit Ticket and Grant for safety to avoid roll-over aliasing 118 // The fast-path reflects the usual tension and trade-off and constitutes a \"bet\". 119 // The fast path helps uncontended operation, but generates 120 // additonal coherence traffic and hurts contended performance. 121 // Consider : Polite LD-if-CAS instead of CAS to reduce write invalidation 122 // The fast path does NOT allow barging or change the admission order 123 // or otherwise invalidate our desirable bounded bypass properties. 124 if (FastPathEnabled) { 125 auto grnt = Grant.load (std::memory_order_acquire) ; 126 if (Ticket.cas (grnt, grnt+1) == grnt) { 127 csfn() ; 128 Grant.store (grnt+1, std::memory_order_release) ; 129 return ; 130 } 131 } 132 133 // Select Lane ... 134 // We use cbrn in conjunction with a hash function (HashPhi32) to 135 // construct a _very low-quality but efficient thread-local counter-based PRNG 136 // (CBRN) that we use for Bernoulli trials to randomly select a lane for 137 // a given locking episode. 138 // It is tempting to mix in some thread-specific value into the hash, say, 139 // the address of a thread-local or a dummy on-frame address, to avoid 140 // inter-thread entrainment, but if we have lock-induced unfairness, 141 // then the cbrn values over a set of threads will naturally diverge, 142 // so there is no need. 143 // Consider : defer-avoid update -only recompute lane index at onset of waiting 144 static thread_local constinit int cbrn = 0 ; 145 int ix = (HashPhi32 ((++cbrn) ^ uintptr_t(&cbrn)) & 1) ; 146 auto const Lane = Lanes + ix ; 147 148 // Arrival : Push E onto the concurrent pop-stack 149 // Placement for E : either on-frame or TLS singleton 150 WaitElement E {} ; 151 auto prv = Lane â†’ Tail.exchange (&E) ; 152 assert (prv â‰  &E) ; 153 if (prv â‰  nullptr ) { 154 // Slow path - contention --need to wait 155 // We are follower within segment 156 WaitElement * eos ; 157 while ((eos = E.eos.load (std::memory_order_acquire)) == nullptr ) { 158 Pause() ; 159 } 160 assert (eos â‰  nullptr ) ; 161 assert (eos â‰  &E) ; 162 assert (Lane â†’ Tail.load() â‰  &E) ; 163 // At this point E resides on the detached entry segment 164 165 // Execute CS which is expressed as C++ lambda 166 // pass context from lock() to corresponding unlock() : Lane, eos, prv 167 EnterCSF : 2025-06-12 Â· Copyright Oracle and or its affiliates Dice and Kogan 168 assert (LeaderIsLocked()) ; 169 csfn() ; 170 assert (LeaderIsLocked()) ; 171 172 if (eos â‰  prv) { 173 // systolic propagation of ownership wakeup 174 // relay through detached entry segment chain 175 // propagate eos value to prv -- convey \"Omega\" address 176 assert (prv â†’ eos.load() == nullptr ) ; 177 prv â†’ eos.store (eos, std::memory_order_release) ; 178 } else { 179 // We reached effective end of the detached entry segment 180 // This is the terminus -- entry segment is exhausted 181 // We are omega 182 // Release Leader Lock 183 // Note that we do not require a full atomic fetch-add(1), but 184 // empirically, we find it scales better on Intel UPI. 185 Grant.fetch_add (1) ; 186 } 187 return ; 188 } 189 190 // We are the arrival segment leader - alfa \"prime\" 191 // A segment has only one such thread acting in the leader role 192 // Wait for the the previous generation (on this lane) to depart and vacate 193 // and for the other lane to quiesce and vacate. 194 // Invariant : only leaders compete for the Leader ticket lock. 195 // 196 // Claim : Given 2 lanes, at most two threads wait here at any one time 197 // Claim : at most one thread from a given lane waits here at any one time 198 // 199 // The Ticket lock acquisition combines two modes of waiting : 200 // @ Inter-lane arbitration : occurs at segment alfa-omega points 201 // Acquire at alfa-leader and release at omega detach 202 // @ Intra-lane arbitration : between incoming and outgoing generations within a lane 203 // 204 // Note that we enjoy neither private nor local spinning below, on Grant, 205 // although the degree of global spinning is tightly bounded. 206 // If desired, this is easy to rectify by augmenting Ticket + Grant with 207 // a short array of Waiting \"slots\", indexed by masked ticket values, 208 // where waiters install a pointer to a private waiting element into a slot, 209 // and then wait on that installed element, yielding \"indirect waiting\". 210 // Such indirection confers both local and private waiting. 211 // This also allows us to reduce false sharing and coherence traffic on fields 212 // in the lock body, but without resorting to increasing the lock body size 213 // via padding and sequestration/isolation via alignas(128). 214 // An ABQL variant works nicely here, with N=4, where the waiting array 215 // elements hold pointers to local waiting locations. 216 217 // Acquire Leader Lock -- implemented as Ticket Lock 218 auto tx = Ticket.fetch_add(1) ; 219 while (tx â‰  Grant.load(std::memory_order_acquire)) { 220 Pause() ; 221 } 222 223 // Execute CS which is expressed as C++ lambda 224 // context passed from lock() to corresponding unlock() : tx, Lane, &E 225 // NOTA BENE : we can safely detach Tail either early, before 226 // we execute the CS, or late, after we execute the CS. 227 EnterCSL : 228 assert (LeaderIsLocked()) ; 229 csfn() ; 230 assert (LeaderIsLocked()) ; 231 assert (tx == Grant.load()) ; 232 233 auto detached = Lane â†’ Tail.exchange ( nullptr ) ; 234 assert (detached â‰  nullptr ) ; 235 if (detached â‰  &E) { 236 // While we were executing csfn(), followers have arrived and collected 237 // or accumulated on the arrival segment, and pushed themselves onto Tail. 238 // &E is thus \"submerged\" at distal end of the stack 239 // The E address on the chain is now a \"zombie\" element 240 // Detach the arrival segment, which becomes the next entry segment, 241 // and start relaying ownership through the chain. 242 // Pass &E thru the chain so penultimate waiter can recognize the 243 // effective end-of-segment and treat &E as effectively nullptr. 244 // Passing and escaping &E is technically UB but OK in most environments. 245 // If this is a concern we can move &E into TLS instead of allocating on-frame. 246 // E is live at the time we detach and privatize the chain and pass &E. 247 // The Leader Lock remains held throughout and we pass ownership of 248 // the Leader Lock to the entry segment, where the terminal element 249 // will ultimately surrender that lock. 250 assert (detached â†’ eos.load() == nullptr ) ; 251 detached â†’ eos.store (&E, std::memory_order_release) ; 252 } else { 2025-06-12 Â· Copyright Oracle and or its affiliates",
  "keywords_parsed": [
    "None"
  ]
}