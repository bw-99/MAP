{"Establishing Heuristics for Improving the Usability of GUI Machine Learning Tools for Novice Users": "", "Asma Yamani \u2217": "", "Malak Baslyman \u2020": "Information and Computer Science King Fahd University of Petroleum and Minerals Dhahran, Saudi Arabia 201906630@kfupm.edu.sa Haifa Alshammare \u2217 Information and Computer Science King Fahd University of Petroleum and Minerals Dhahran, Saudi Arabia 202110890@kfupm.edu.sa", "ABSTRACT": "Information and Computer Science King Fahd University of Petroleum and Minerals Dhahran, Saudi Arabia malak.baslyman@kfupm.edu.sa", "1 INTRODUCTION": "Machine learning (ML) tools with graphical user interfaces (GUI) are facing demand from novice users who do not have the background of their underlying concepts. These tools are frequently complex and pose unique challenges in terms of interaction and comprehension by novice users. There is yet to be an established set of usability heuristics to guide and assess GUI ML tool design. To address this gap, in this paper, we extend Nielsen's heuristics for evaluating GUI ML Tools through a set of empirical evaluations. To validate the proposed heuristics, user testing was conducted by novice users on a prototype that reflects those heuristics. Based on the results of the evaluations, our new heuristics set improves upon existing heuristics in the context of ML tools. It can serve as a resource for practitioners designing and evaluating these tools.", "CCS CONCEPTS": "\u00b7 Computing methodologies \u2192 Machine learning ; \u00b7 Humancentered computing \u2192 HCI design and evaluation methods ; \u00b7 Software and its engineering \u2192 Software usability .", "KEYWORDS": "Usable, ML tools, GUI machine learning tools, Weka, Knime, Heuristic evaluation (HE), Cognitive walkthrough (CW), Novice users, User testing, SUS", "ACMReference Format:": "Asma Yamani, Haifa Alshammare, and Malak Baslyman. 2024. Establishing Heuristics for Improving the Usability of GUI Machine Learning Tools for Novice Users. In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI '24), May 11-16, 2024, Honolulu, HI, USA. ACM, New York, NY, USA, 19 pages. https://doi.org/10.1145/3613904.3642087 \u2217 Both authors contributed equally to this research. \u2020 Corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CHI '24, May 11-16, 2024, Honolulu, HI, USA \u00a9 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0330-0/24/05...$15.00 https://doi.org/10.1145/3613904.3642087 Machine learning (ML) is a subset of artificial intelligence that uses computerized methods to solve issues based on historical data and knowledge without requiring unnecessary changes to the core process[4]. With the sheer of data available to analyze and build accurate prediction models, machine learning tools have become necessary to add to a professional's toolkit in various fields. Thus, machine learning has gained immense attraction in recent years. Several software tools have been developed to make machine learning programming easier and faster. These software tools come in the form of programming languages, libraries/packages, graphical user interface-based programs, platforms, toolkits, and combinations of any of these [37]. GUI ML tools, such as Weka, KNIME, and RapidMiner, are easier to use, to some extent, for novice users to use rather than code. According to Dudley and Kristensson [16], machine learning techniques are steadily entering novice users' lives. Thus, enabling individuals to interact with such technologies efficiently will be a significant design problem. This pushed machine learning from being an underlying technological infrastructure unaddressed by HCI researchers to the fore of interface design and user experience [25]. From this perspective, new research practices and design methods are required to address the end-user interaction challenges with ML tools [11, 15]. However, while there is growing acknowledgment of the issues that user experience (UX) and visualization practitioners encounter when working with machine learning [15, 49-51]; the usability of current ML tools has not been studied adequately using a formal process as previous studies are either old and the tools they have studied are outdated or updated [10, 13] or conducted informal evaluations that had key information of the study design missing [7, 37]. Subsequently, the heuristic evaluation method still needs to be updated to address the challenges of ML tools to have a better end-user experience. Although Nielsen heuristics, the most well-known heuristics throughout HCI [35], can be used to evaluate GUI ML tools and was able to identify usability issues in GUI ML tools in the first part of our study, several studies have suggested that Nielsen's heuristics are general and do not address domain-specific issues [29, 31, 33, 44]. The need to extend Nielsen's heuristics to accommodate GUI ML tools for novice users arises since these tools are actually an interface to underlying complex concepts that novice users are not familiar with. This increases the burden on the tool from performing certain tasks to helping the user navigate complex concepts and gain competence in the field over time. Hence, in this work, we sought to extend Nielsen's heuristics for ML tools to enhance their CHI '24, May 11-16, 2024, Honolulu, HI, USA Yamani et al. design and usage concerning novice users. Therefore, we pose the following research question: \u00b7 RQ1: To what degree are the current GUI ML tools usable? \u00b7 RQ2: What are the suitable heuristics for evaluating GUI ML tools usability? To address the first question, we conducted a cognitive walkthrough and heuristic evaluation for two commonly used GUI ML tools (Weka and KNIME). For the second question, we empirically propose 14 heuristics for GUI ML tools by applying a four-phase design process, including heuristics generation, prototyping, validation, and revising. Based on the results, these new heuristics are more effective in enhancing the usability of GUI ML tools. The contribution of this work is as follows: (1) Empirical usability evaluation of Weka and KNIME GUI ML tools; (2) A set of verified heuristics that practitioners and researchers may use to evaluate GUI ML tools formally. The rest of the paper is structured as follows: Section 2 provides a literature review of the related work on GUI ML tools usability evaluations and heuristics. Section 3 emphasizes the fundamental details of this research methodology. The details of establishing the new heuristics stages, as well as their experimental design, results, and analysis, are presented in Sections 4,5,6, and 7. Sections 8 go through the discussion and limitations of the study. The conclusion and future work are covered in Section 9.", "2 RELATED WORK": "", "2.1 Usability of GUI ML tools": "Though the importance of having a highly usable GUI ML tool to foster and encourage adaptations of ML from all sectors, the usability of GUI ML tools or tools in related domains, such as data mining, has not been addressed adequately in the literature. One of the earliest studies conducted to address the usability evaluation of GUI ML tools dated back to 2007 by X. Chen et al. [13]. It covered 12 open-source data mining tools. The authors evaluated the usability of tools based on human interaction, interoperability, and expandability, among other aspects such as functionality and data source. In this study, KNIME and AlphaMiner had the highest scores regarding usability. Seven years later, a study investigated the usability of five data mining tools: KNIME, Orange Canvas, RapidMiner, and Weka. The study conducted a cognitive walkthrough to perform the evaluation on students and lecturers from the CS department. The results concluded that lecturers found Weka was easier to use than KNIME, while students thought the opposite. Both students and lecturers agreed that in many aspects of the evaluation, Weka and KNIME were the best amongst the other GUI data mining tools [10]. As for more recent studies, in 2018, a study attempted to compare the data analytics tools Python, SPSS, R, SAS, and Weka concerning multiple factors, including usability. The study gave all the tools 4 out of 5 when it came to usability, except for R, which got 4.5 [7]. Another recent study in 2019 reported the SUS evaluation of GUI ML tools: Orange Canvas, RapidMiner, Weka, and KNIME against a tool the study authors developed [37]. Aside from the developed software, Weka had a higher satisfaction rate than RapidMiner and KNIME. Based on the surveyed literature regarding GUI ML tools, the early studies contain tools that are now obsolete or updated [10, 13]. Also, some studies based their evaluation on the authors' or participants' perspectives without following any formal definition of usability or following any of the formal usability evaluation methods [13]. As for other studies [7, 37], some important experimental setup details were missing, such as the number of participants, the number of questions, or the tasks they completed before performing the evaluation, hence a reliability issue. Although [10] reported using cognitive walk; the approach was not fully implemented as it was conducted on the task level, not on the action level. In addition, the survey questions were given at the end of the evaluation, not task by task. The question also did not align with the four questions from [45]. Moreover, in [10], the scenario investigated was extremely limited to four tasks only, which is not a representative set of tasks considering all the steps of the ML development life cycle. Therefore, although the results may be valuable for the data mining community, they may not be generalized to the issues faced when developing and deploying an ML model.", "2.2 Interactive Machine Learning": "Another paradigm for designing ML tools for novice users is Interactive Machine Learning (iML) [19]. IML is an interaction paradigm in which a user or user group iteratively builds and refines a mathematical model to describe a concept through iterative cycles of input and review [17]. Current iML tools focus on building a highly accessible tool that provides the basic functionality for a specific problem type that novice users need and it can also be used for educational purposes. Some problems that are currently addressed are images, voices, and pose classification in Teachable Machine [12], regression in TREX [27], and transfer learning in images [32]. The scaffolding approach followed in iML tools allowed the users to focus their mental efforts on integrating their domain knowledge into the models. For example, Teachable Machine [12] was recently evaluated in the context of aiding the development of prototypes by UX practitioners (UXP) to propose a proof-ofconcept design for a new ML-enabled application [21]. Participants found it valuable to interactively explore different combinations of model classes and visualize the data. Although not all were able to interpret the probability scores correctly, coming from no background in ML, it helped in design choices and in identifying limitations and possible biases. The ML module integrated with the app prototype was deemed helpful in communicating with stakeholders and managers [20] A limitation in current iML tools raised in [32] was that overuse of scaffolding which comes with the risk of forcing individuals to follow a pattern conveyed by the interface. Another limitation observed is the narrow purpose and functionality of each of the tool as it came as a trade-off with usability. Also there are no heuristics or guidelines to reproduce such usable tools. Hence, in this work, we aim to evaluate the current most popular GUI ML tools for usability following formal usability evaluation approaches in order to develop heuristics that can be used by designers to develop usable ML tool. Establishing Heuristics for Improving the Usability of GUI Machine Learning Tools for Novice Users CHI '24, May 11-16, 2024, Honolulu, HI, USA", "2.3 The role of AI in GUI generation": "Generative AI is starting to play a significant role in generating prototypes and GUIs. Akin is a UI wireframe generator built using a fine-tuned Self-Attention Generative Adversarial Network trained with the semantic representations of UI images of 500 UI wireframes from different design patterns. Human evaluation of the resulting wireframes against designers' wireframes shows similar quality. In 50% of the occasions, designers identified Akin-generated wireframes as designer-made 50% of the time [24]. In WireGen [23], the researchers leveraged the contextual knowledge in LLMs. They fine-tuned a gpt-3.5-turbo with 1000 user interfaces to generate mid-fidelity wireframes that go beyond the shape and text placeholders of low-fidelity wireframes to include real content, semantic icons, and interactive elements. Aside from the active research in the area, several companies are emerging on this premise, including Semanttic 1 that can be used to regenerate and iterate over previously built interfaces using AI. There is also Builder.io 2 , a Figma plug-in that generates UI based on a text prompt; a similar service is provided by Galileo AI 3 . Dice 4 creates UX strategies and suggests design inspiration based on the project description through a text prompt. With the progression of AI, it could result in enhancing the user experiance and create personalized UI/UX experiences per context.", "3 RESEARCH METHODOLOGY": "To address our research questions, the following goal was formulated according to the Goal Question-Metric (GQM) approach [42]: develop new heuristics for the purpose of enhancing the usability of GUI ML tools with respect to their effectiveness from the point of view of the researcher in the context of Nielsen's ten heuristics, and novice users using Weka, and KNIME ML tools. Figure 1 outlines the methodology we followed to achieve this goal. To address RQ1, we conducted a literature review to identify the most commonly used ML tools in addition to the goals and gaps (as discussed in Section 3.1). We also performed a user and task analysis to evaluate the usability of the selected GUI ML tools (as presented in Sections 3.2 and 3.3). To address RQ2, we followed four phases: (1) heuristics generation, (2) developing a prototype that meets the new proposed heuristics, (3) validation of the developed heuristics, and (4) revising the heuristics. We based Phases 1, 3, and 4 on [3, 29], but made some modifications. The following Sections (4-7) describe each phase in detail.", "3.1 GUI ML tools selection.": "Among multiple GUI ML tools Weka [47] and KNIME [8] were chosen for multiple reasons: (1) They are both widely utilized for data processing [18, 26, 38], and they are among the top 10 machine learning tools for 2021 [14]. (2) KNIME is recommended for individuals who are novices at using such software [18]. It is very robust with built-in components and extra functionality that can be brought 1 https://www.semanttic.com 2 https://www.builder.io/blog/ai-figma 3 https://www.usegalileo.ai/explore 4 https://www.dice.design from third-party libraries[18]. It also comes with a drag-anddrop interface. Based on the analysis, Weka comes second to KNIME because of its many built-in capabilities that do not require any programming or coding experience [18, 43]. (3) Weka and KNIME are both open-source and cross-platform software that appeals to all novice users. (4) Weka and KNIME are two of the common benchmarking tools[1], and to the best of our knowledge, no work evaluated their usability through heuristic evaluation or User testing usability methods. It is worth noting that both Weka and KNIME were used in the first two evaluation methods (Heuristic Evaluation and Cognitive Walkthrough). Only the tool with the least number of issues was chosen afterward to perform user testing evaluation and SUS evaluation to avoid frustration during user testing.", "3.2 Identifying targeted users (User Analysis).": "In the era of data science, the knowledge of implementing the basic algorithms and methods related to machine learning and data analytics is essential for beginners in this field (novice users). Thus, we evaluated the usability of Weka, KNIME, and the developed prototype from the point of view of novice users to help in designing and developing usable ML tools based on the resulting usability issues and suggested recommendations. Fig.12 (in the appendix) shows the persona of our ML tools novice user.", "3.3 Identifying user goal and Task analysis.": "To conduct the usability evaluations of the GUI ML tools, we identified the goal of using ML tools by novice users through a scenario that simulated a real-life case. From this goal, we identified and analyzed the tasks that will be used during usability evaluations (Heuristic evaluation and cognitive walkthrough) to help measure the usability metrics, such as success rate and errors and finding related issues. Scenario: Tiana is a real estate agent in California. She is trying to estimate the best prices for houses in multiple regions. Tiana's friend provided her with a dataset containing 20,000 records in an Excel sheet. The data consisted of the previous sales made by a real estate agent, along with a description of each house. Her friend suggested using machine learning to help her predict outcomes based on the features of the houses. Tiana had never worked with MLor ML tools before, so she found a tutorial and decided to follow it. As she was intimidated by programming Python, she found two cross-platform free ML GUI tools and decided to try them out. Goal: Predict the price of a house based on a set of features with respect to historical data. Task Analysis: The Tasks to achieve this goal follows, with some customization, the nine stages of the machine learning workflow [2], illustrated in Fig.2. We assume the requirements are clear and start by (1) Loading the labled data, and (2) performing exploratory data analysis have a quick look on the features. Then, some necessary data preprocessing including (3a) one-hot encoding that changes categorical data to numerical, (3b) filling in missing data, and (3c) splitting the data for training and testing. No feature engineering is performed since the user is a novice. Then, the model training CHI '24, May 11-16, 2024, Honolulu, HI, USA Yamani et al. Figure 1: Research Methodology Developing New Heuristics Phase 1: Heuristic Generation Conventional Iterature Phase 2: Develop prototype that review 1.1 Cognitie w akhrough and meets te newproposed heuristics Heuristic Evaluation Gap Goal WEKA KNIME Weka Knime Select GUIML tools Phase 3:Heurisucs ation testing of the prototype) 1 2 User testing (Weka) New Revised Results and heuristics analysis Users User anaysis The proposed heuristics set Phase 4:Revising the newheurisucs 1.3 Surveying the needs of GU ML Task analysis tools users Tasks valid starts, including (4a) building the model and (4b) performing hyperparameter tuning and building in an iterative manner. Following that is (5) model evaluation based on the appropriate metrics. Finally, the model deployment step occurs in which the user (6a) saves the model, then the same user or a different one (6b) load the model in inference mode, (6c) makes predictions on an independent dataset, and (6d) saves the prediction results. Model monitoring, from Fig.2, is also left out as it not a task novice users would do. The steps under each task are illustrated in Fig.3. Exact wording of the tasks presented to the participants when performing user testing are in Table 6 (in the appendix) The Tasks are to be completed sequentially.", "4 PHASE 1: HEURISTICS GENERATION": "The first phase of this study was to generate the heuristics after modifying Nielsen's heuristics and adapting them to GUI ML tools based on empirically evaluating two GUI ML tools.", "4.1 Experimental Design": "4.1.1 Heuristic evaluation. As a result of reviewing the literature, there were no heuristics proposed specifically for evaluating the ML tools. Accordingly, Nielsen's Ten usability heuristics [35], shown in Table 1, were used in this heuristic evaluation, which is the most common set of heuristics utilized. In this heuristic evaluation, all the UIs visited for performing each task presented in Fig. 3 were evaluated against the ten heuristics, looking for usability issues in the Weka and KNIME ML tool design. After that, a severity was assigned for all the issues found using Jakob Nielsen's severity rating scale [34], in Table 5 (in the appendix). These severities help rank each issue's significance to the users using those ML tools. 4.1.2 Cognitive walkthrough evaluation. A cognitive walkthrough aims to evaluate the ease of learning a system by exploration through the following four questions [45]: (1) Will the user try to achieve the right effect? (2) Will the user notice that the correct action is available? (3) Will the user associate the correct action with the effect that the user is trying to achieve? (4) If the correct action is performed, will the user see that progress is being made toward the solution of the task? 4.1.3 Surveying the needs of GUI ML tools users. An online questionnaire 5 that consists of twelve items arranged in three parts was used. The first part indicates three items about the experience with ML tools that the respondents have. The second and third parts show four items based on the perception of difficulties while using ML tools and five items based on characteristics(needs) that lead to more usable tools, respectively. Seven items are close-ended. Three of them are scored using a 5-point Likert scale from (1) 'strongly disagree' to (5) 'strongly agree', while four are multiple choices, and the rest five items are open-ended. After designing the questionnaire, it was tested by an expert to increase its validity and distributed through several social media channels to the ML community, targeting people who had previous experience with GUI ML tools. This questionnaire is the only method where participants did have prior experience with GUI ML tools, we focused in the analysis on users with less than 6 months of experience and used the responses of participants with more than 6 months of experience to emphasize the novice users' responses. 4.1.4 User testing evaluation of Weka. Usability testing is one of the most common methods to evaluate product usability by observing users while they accomplish Tasks with that product [39]. As known, it is crucial to test with users to get more usability issues, which could help in developing more comprehensive heuristics to solve them. Thus, we conducted an online moderated user testing of Weka since it is one of the most known GUI ML tools, as mentioned before, and had the least issues when it comes to the cognitive walkthrough and heuristic evaluation. According to Nielsen and Landauer, five users are enough to identify 85% of the issues on most websites [30]. Domas and Redish also supported this argument, claiming that five to twelve participants are sufficient for a test [30]. 5 https://forms.gle/hhHgWXhNFbGQ4fW28 Establishing Heuristics for Improving the Usability of GUI Machine Learning Tools for Novice Users CHI '24, May 11-16, 2024, Honolulu, HI, USA Figure 2: The nine stages of the machine learning workflow [2]. Model Data Data Data Feature Model Model Model Model Requirements Collection Cleaning Labeling Engineering Training Evaluation Deployment Monitoring Figure 3: Task Analysis Goal: Predict a price of a house Do in order: Load Perform Exploratory Data Model Model dataset Data Analysis Preprocessing Model training evaluation deployment Preparing data building the Hyperparameter Saving the Loading the Making the Saving the One-hot Fill in for training and model tuning model model prediction prediction encoding missing data evaluation. cause the cause the cause the click oh save cause the system change the system to open click on save prediction cause the cause the system to cause the system to open the preprocessing to open the hyperparameters the testing the model system to system to open open the results system to open the visualization panel training panel panel file deployment save open the panel panel window' open file cause the cause the window' Fill in missing cause the cause the system choose one- data choose the to train the model choose the system system to hot encoding system to metrics open the save choose the choose the open the 'save choose the chart type open the data algorithm file window' dataset that file window' model file splitting tab contains the choose the choose fill feature dataset file customize the chart missing data cause the choose the choose the choose the appearance choose the target system to Load the columns choose the location and location and choose how to evaluate the name model file make the fill the missing splitting type model prediction name load the file view the chart data cause the system save the save the set the to train the model model view the prediction choose the percentage or results results columns value cause the system to apply the transformation the Therefore, in this evaluation, seven novice participants have been recruited. The evaluation was conducted in separate sessions for each participant, where the sessions were video recorded for analysis purposes(for identifying the errors type and description). Each participant signed the consent form and performed twelve Tasks presented in Table 6 in the appendix. While the participant was performing each task, the evaluator observed and recorded the number of errors made when deviated from the optimal way to perform the task, the time it takes to perform the task in seconds, and the task success with three success levels proposed by Nielsen (S = Success, F = Failure, P = Partial Success). The task success rate was computed as follows[36]:  where the No. of attempts is the number of overall attempts to perform the task, which is equal to the number of participants in the evaluation. These measures will compute the Task completion rate, time spent on each task, error rate, and error types for the analysis. After performing all Tasks, the participants were asked to fill up the System Usability Scale (SUS) to indicate their level of satisfaction with the tool as described in Section 4.1.5 below. Also, an informal interview was conducted with each participant to get some extra views and opinions. 4.1.5 The System Usability Scale (SUS).. The System Usability Scale (SUS) was used to assess the level of users' perceived usability. It is a reliable tool for evaluating the user's satisfaction level with a system, represented as a questionnaire of 10 items with 5-point Likert scale ratings from strongly disagree (1) to strongly agree (5) [9]. This SUS score will be calculated as follows[9]: (1) Subtract one from the score of each odd-numbered question. (2) Subtract the score of the even-numbered question from 5. (3) Sum all the generated values (from all questions) and then multiply the total by 2.5 to obtain the overall SUS score. The SUS score can be transformed into aspects of Adjectives, Grade, and Acceptable[40]. Acceptability ranges are used to interpret the SUS score depending on user satisfaction as Not Acceptable, Marginal, and Acceptable, as shown in Table 5 [40]. On the Grade scale, SUS scores are classified as A, B, C, D, E, and F grades from superior performance to failing performance, with C as an average [40]. Moreover, the Adjective rating is an adjective that converts the SUS numerical score into an absolute usability rating, which includes Worst Imaginable, Awful, Poor, OK, Good, Excellent, and Best Imaginable [6]. Two open-ended questions were also included about what the user liked most about the tool and what the user preferred to change. CHI '24, May 11-16, 2024, Honolulu, HI, USA Yamani et al.", "4.2 Results and Analysis": "4.2.1 Heuristic Evaluation. Heuristic Evaluation was conducted based on Nielsen's Heuristics, in Table 1, by two evaluators. The evaluation revealed 45 issues for Weka, and 85 issues with KNIME, which is 1.8x the issues with Weka. Around 60% of the issues were categorized as major usability problems, 15% as minor usability problems. While Weka sustained minimal catastrophic usability problems, KNIME had 14 catastrophic usability problems. The distribution of the issues based on severity across tools is illustrated in Fig. 13 (in the appendix). An issue that was recognized across tasks and both tools was the lack of adequate help and documentation, as there was no proper offline help or documentation, and even the online help was hard to navigate. Another heuristic that was violated multiple times was the first heuristic related to the visibility of system status, as the systems lacked proper feedback in multiple instances. Also, the 8 \ud835\udc61\u210e heuristic, which is related to Aesthetic and minimalist design, as too much unnecessary information and options are present at the same time for both tools. The distribution of the issues based on heuristic numbers across tools is illustrated in Fig.4. Also, the issues, along with the violated heuristic number, the severity, and the associated tasks, are present in Table 7 (in the appendix). Figure 4: Issues discovered during the Heuristic Evaluation heuristic number. Weka KNIME 30 24 18 12 H1   H2 H3 H4 H5 H6 H7 H8 H9 H1o Nielsen's Heuristics 4.2.2 Cognitive walkthrough. The cognitive walkthrough revealed 12 issues related to the usability of Weka and KNIME in 42 instances out of which 30 instances belonged to KNIME, illustrated in Fig. 14(in the appendix). The user in the scenario followed a tutorial associated with reaching the goal using Python. The reason for choosing a tutorial based on Python, not GUI ML tools, is that it mimics the real-life case as it is the type of tutorial found on the web. This presents an extra challenge in our cognitive walkthrough evaluation. The Despite this, five issues are related to question 2, where many actions were not visible to the user. Five issues are related to question 3, where the step is there on the screen, but the user might have some difficulty associating. Finally, two issues are discovered related to not giving proper feedback when performing an action. In Table 8(in the appendix), we enumerate the issues found, along with the Action item they were discovered in, along Figure 5: Success Rate Ratio Success Partial DFail Task 1 Task 2 Task 3a Task 3b Task 3c Task 4a Task 4b Task 5 Task 6a Task 6b Task 6c Task 6d Participants attempts with the suggestions. 4.2.3 User testing evaluation for the Weka. In this type of evaluation, we aimed to measure the proficiency and error rate when performing the set of 12 tasks that a novice user will most likely encounter when using ML tools. We initially recruited (N = 7) novice participants to the evaluation study, but (N = 2) had to withdraw since Weka on their devices did not recognize the file used for training without showing any error messages. For measuring the proficiency , two metrics were used: The success rate as suggested by Jakob Nielsen [36] and the completion time. The success rate per task is presented in Table 2, while the details of the attempts are illustrated in the stacked bar chart presented in Fig.5. In addition to the success rate for proficiency, we measured the average completion time . The Tasks that were executed successfully across participants were: (1)Task 2 Performing Exploratory Data analysis represented in visualizing the correlations between the different attributes (2) Task 3c: Preparing the data for training and testing by splitting the data into training and test sets. (3)Task 5: Model evaluation represented in reporting the mean absolute error. (4)Task 6b: loading the model. It was notable that Tasks 2, 3c, and 5 were among the least Tasks to violate Nielsen's heuristics during the heuristic evaluation or present issues during the cognitive walkthrough. As for Task 6b, it had a reciprocal Task before it (Task 6a: Saving the model), in which the participants were able to learn how to deal with Task 6b. With an average completion time of 48 minutes for the complete session, the average completion time per Task is 4 minutes. Generally, tasks with higher success rates have lower completion time, illustrated in Fig.15 (in the appendix). As for Error handling, we explored the error rate by task, and then classified whether the error was the result of a user accident or caused by confusion. Fig.16(in the appendix), shows a comparison between the error rates of different Tasks. As for the source of the error, all errors were of error by confusion class . The top source of confusion, confirmed to what was previously found by the heuristic evaluation cognitive walkthrough, was the lack of clear labels or buttons. In addition, with the deep nested lists, the participants Establishing Heuristics for Improving the Usability of GUI Machine Learning Tools for Novice Users CHI '24, May 11-16, 2024, Honolulu, HI, USA Table 1: Nielsen's Ten Heuristics. Table 2: Average success rate across Tasks Table 3: Acceptability ranges of SUS score could not find what they were looking for, which led to moving from the correct path they were initially on. 4.2.4 SUS of Weka. An online SUS questionnaire was filled out by five participants who had been exposed to the Weka tool for the first time. The SUS score was calculated as discussed in section 4.1 using the equation from [41]. Fig.6, illustrates Weka satisfaction measures regarding the SUS score interpretation of all participants. The results of SUS scores were interpreted into different categories: Adjectives, Grade, and Acceptable. As shown in Fig.6, three participants (1,2, and 5) revealed that Weka is not acceptable, with 'poor' usability and failing performance with an 'F' grade. On the other hand, Weka scored a marginal satisfaction level with 'OK' usability for the other two participants (3 and 4) with 'C' and 'D' grades, which indicates that Weka has an average level of satisfaction. On average, Weka has a 49 SUS score among all the participants, so we can conclude that it is considered not acceptable based on Table 3. As for the open-ended question responses, a participant thought Figure 6: SUS evaluation of Weka. that the hint box was helpful, while another participant commented that the hint box was too long and that it should be concise. Another participant suggested improving the associated labels of the buttons and tabs. Also, using buttons instead of right-clicks as they were not being used in an intuitive manner in some places. Finally, to guide the process, one participant suggested that the upper bar should be about the main processes in machine learning.", "4.3 Surveying the needs of GUI ML tools users": "An online questionnaire described in Section 4.1.3 was the method used for collecting qualitative or quantitative data about the users' needs from GUI ML tools. The questionnaire was distributed through several social media channels to those who have an experience in MLtools. In this method we focus on the novice users needs and we corroborate on them using the expert users' opinions Descriptive statistics were used to analyze the collected responses, as shown below. 4.3.1 Descriptive Analysis of Responses. The total number of valid responses was 27. The majority of them have used Weka, as shown in Fig.19 (in the appendix), while only two have used AZURE and RapidMiner tools, and nothing for KNIME. Regarding the experience, 52% are novice users with experience of fewer than six months, and others are considered experts. Additionally, 56% of the respondents have taken 1-3 courses in the field, and 52% of them are Weka users. In terms of perception of difficulties while using ML tools, eight respondents (29 . 6%) disagree that the interaction with ML tools is straightforward and apparent; the bulk of them (63%) have attended 1-3 courses in the field, while 50% are novice users. Seven respondents explained why they thought the interaction with ML tools was not understandable and clear; novice users were concerned about guidance and learnability, while experts were concerned about trust and user interface issues. CHI '24, May 11-16, 2024, Honolulu, HI, USA Yamani et al. Furthermore, as shown in Fig.7, most novice users believe that data cleaning and preprocessing, model selection, and feature engineering Tasks should be outsourced to ML experts rather than working on them themselves. Expert users agreed with novice users when it came to data cleansing and model selection, and they also thought that model deployment should be outsourced. Only seven out of twenty-seven respondents provided reasons for outsourcing the chosen Tasks; time-consuming, lack of available tools, diversity of options, and focusing on more important Tasks are the reported reasons for outsourcing the chosen Tasks. Regarding the character- Figure 7: Tasks to be outsourced to ML experts. Novice Experts Deployment Cleaning Preprocessing Engineeering 'Model Tuning Strategy Monitoring Hyperparameter` Evaluation ' Data ' Choosing Featuree Data istics that lead to usable tools, 28% of novice users do not find that GUI ML tools provide necessary features, and 30% of expert users agreed with them (where 75% of them strongly disagree). None of the experts with more than one year of practice agreed that GUI ML tools are sufficient. Besides that, two of them cited a lack of data cleaning and preprocessing-related features. The survey results revealed that only 11% of respondents disagree that ML tools must propose the most typical action. Furthermore, when the respondents were asked about what they liked most about the ML tool they used, eight of them praised the tool for its functionality aspects, seven praised the tool for its overall ease of use, and the others commended the availability of the tool as a free resource. However, the majority of respondents would prefer to modify the functionality of the ML tool design they've used. Others proposed improving the tool's assistance and documentation, as well as improving its user interface with additional usability features, including personalizing it based on the project, guiding abilities, and trust in the tool.", "4.4 GUI Machine Learning Tool Heuristics": "At this phase, we adapted Jakob Nielsen's 10 Heuristics from 1994 [35], shown in Table 1, to capture the most important issues when developing GUI ML tools for novice users based on our empirical evaluation. The heuristics proposed for GUI ML tools are shown in Table 4. Heuristics 1, 3, 5, 6, and 8 were taken from Nielsen's set without modification as they were well-suited for GUI ML tools. The remaining heuristics (2, 4, 7, 9, and 10) were updated based on empirical evaluations. Heuristics 2 was updated by adding the word novice as during user testing and heuristic evaluation, as some of the issues uncovered could be familiar to an expert user but not a novice user. Heuristics 7 was also updated due to issues uncovered during user testing and heuristic evaluation by explicitly stating common shortcuts as users expected to use the universal shortcuts and were confused when they did not work (e.g., ctr(or cmd) z). Heuristics 4, 9, and 10 were updated based on user testing, survey results, and authors' experience during heuristic evaluation. The Cognitive Walkthrough and SUS results inspired the update of heuristics (4 and 9) and (4 and 10), respectively. As for Heuristic 4, the GUI tools evaluated used jargon associated with statistics or data mining in its early stages that is not understandable by novice users and does not even align with recent tutorials on ML problems. For Heuristics 9, the update was made because error messages and suggestions are more constructive when based on data, not just general errors (e.g., Illegal operation). Finally, for the last updated heuristics, Heuristics 10, observations during user testing show that users need to search YouTube to perform tasks repeatedly, and long hints provided by Weka made the participants impatient and jump from one label to another. Also, in the survey, an average of three processes need to be outsourced to experts to save time as they require background knowledge. Asfor the proposed heuristics in Table 4, Heuristic 11 ( Guidance ) was suggested due to the need to follow a tutorial to build the model resulting from heuristic evaluation, cognitive walkthrough, emphasized by novice users in the survey, informal interviews after user testing, and SUS open-ended questions. Trustworthiness (Heuristic 12) is one of the needs mentioned by more than one respondent during the survey. In addition, trust and explainability are rising topics, and regulations are being implemented to enforce them [48]. The necessity to balance between guidance and a simple aesthetic design needed by novices and the powerful endless functionalities that the user may need as the user grows with the system is the reason behind proposing the adaptation to the growth heuristic (H13), as was mentioned in the survey. Finally, the context relevance heuristic (H14) is treated first as an interpretation for other heuristics. However, it was later emphasized as an independent heuristic during informal interviews and user testing analysis conducted on the prototype in the fourth phase.", "5 PHASE 2: DEVELOP A PROTOTYPE THAT MEETS THE NEW PROPOSED HEURISTICS": "By using Weka as a baseline with extra usable features, a prototype 6 for more usable GUI ML tools was produced in this phase that satisfies the new proposed heuristics resulting from the previous phase. The four steps of the usability engineering lifecycle presented in [5] were followed to produce our prototype sequentially, including analysis of requirements, prototype design, prototype creation, user-test/expert evaluation. These steps were repeated in a new iteration as needed. In the first step, the requirements were identified (e.g., such as the need to load the dataset, perform exploratory data analysis, train the mode, and evaluate the model) 6 In the supporting material Establishing Heuristics for Improving the Usability of GUI Machine Learning Tools for Novice Users CHI '24, May 11-16, 2024, Honolulu, HI, USA", "Table 4: GUI ML Tools Heuristics.In bold proposed heuristics and updates to Nielsen's Heuristics": "with a focus on the 13 heuristics 7 as stated in Table 4. Also, by considering Weka's functionalities refined by going through multiple tutorials for novice users to focus only on the basic functionalities. Following that, in the second step of the usability engineering lifecycle, a prototype was designed to address the identified requirements. As illustrated in Fig. 8, the guidance heuristic was implemented in general by displaying the formal sequence for using any ML tool in the form of sequential taps on the top of the interface, 7 The 14th heuristic mentioned in the table was added during the fourth step of the process additionally by presenting a suggestions box that presents the main tasks regarding each interface the user can perform. The guidance heuristic was also supported by supplying the interfaces with an agent, the octopus, who guides the user by giving comments regarding the essential tasks that can be done through the interface. We chose the octopus as a guidance agent in our prototype since it has the largest brain of any invertebrate. Moreover, the help heuristic was applied in the prototype by providing tutorials and a real-time chatbox with experts in addition to the documentation. Another usable feature is the task log box that presents the tasks performed CHI '24, May 11-16, 2024, Honolulu, HI, USA Yamani et al. by the user with undo and redo buttons to support the user's control and freedom heuristic. In the model-building window, we added an explainability tab to emphasize the system's trustworthiness heuristic to the user by showing how the prediction was made. We also reviewed multiple papers, websites, and blogs to ensure that the terminologies conform to the ML field, supporting the Consistency and standards heuristic. The adaptation to growth heuristic cannot be tested through the prototype as it requires a longitude study on a fully functioning software; however, it was incorporated by adding a custom tab for each module in which users can add code directly. Also, the guidance provided by the octopus will decrease in terms of the frequency of the learned items. After that, in the third step, the prototype was created using one of the most widely-used prototyping tools in academia, Balsamiq [28, 46]. The prototype has two modes: (1) Building mode that caters to Loading and preprocessing the Data and Building and Exporting the Model (Tasks 1-6.a). (2) Deployment mode in which, independently from the Building mode, the user can load a built model and make predictions (Tasks 6.b-6.d). Once the prototype was created as an interactive UI that responds to the user actions, it was tested in the fourth step through expert evaluations initially, where domain experts provided feedback about the interface implementation with Nielsen's heuristics, shown in Table 1. Then, it was used to evaluate the new proposed heuristics in-depth with real users by conducting a user testing evaluation as discussed in the next Section 6.", "6 PHASE 3: VALIDATION OF DEVELOPED HEURISTICS": "", "6.1 Experimental Design": "After developing a prototype for a GUI ML tool (by considering Weka as a baseline) that meets the new proposed heuristics, we conducted a user testing evaluation of that prototype in order to validate the proposed heuristics. The evaluation procedure was identical to the user testing evaluation for Weka described in section 4.1 with another group of thirteen participants recruited through personnel connections.", "6.2 Results and Analysis": "6.2.1 User testing evaluation of prototype. Similar to section 4.1, our goal here was to measure the proficiency and error rate when performing the same set of 12 tasks on the prototype developed that follows the proposed heuristics for evaluating GUI ML tools. Thirteen novice participants completed the experiment. The results of this experiment indicate a highly usable system with 100% success rate across the 12 Tasks, an average completion time for all the Tasks of 11 minutes, and an average completion time of 55 seconds per task. Fig.17 (in the appendix) illustrates the breakdown of the different tasks. The minimum average was about 16 seconds for Task 5 (model evaluation), while the maximum average completion duration for a task was one minute and 36 seconds for Task 3c (splitting the data to training and testing) followed by a one second difference in Task 6b (loading the saved model). Task 6b had the highest error, followed by Task 3c, illustrated in Fig.18 (in the appendix). The cause of the error in Task 3c was that novice users found splitting the data was better to be grouped under data preprocessing rather than model building. As in Task 6b, the errors were caused by the confusion between the Model Deployment tab and Deployment Mode option in the drop-down list. An error by confusion class is the source of errors for the other Tasks also due to similar terminology. It was remarkable that 4 of the 12 classes were error-free or had an average error less than 0 . 1. 6.2.2 SUS analysis for the proposed prototype. Fig.9 demonstrates the proposed prototype satisfaction measures regarding the SUS score interpretation of thirteen participants. It was measured and analyzed in a similar way to Weka. According to the acceptance category, Eight participants (1, 3, 4, 5, 7, 10, 11, and 13) accepted the prototype with a high level of usability, ranging from 'good' to 'best imaginable' with 'A' and 'B' grades, indicating that the prototype has a high performance. However, the remaining participants (2, 6, 8, 9, and 12) reported a marginal satisfaction level for the prototype with 'OK' usability. Participants 2, 6, and 12 reported a 'C' grade, indicating an average level of satisfaction, while participants 8 and 9 reported a 'D' grade, which means their satisfaction level was less than average. On average, the prototype achieved a SUS score of 74.62, which is considered acceptable according to Table 3. When it comes to the open-ended questions, participants did agree that the octopus was helpful, and the sequence of actions was clear. One participant elaborated further on that by saying, \"The most thing I like about the tool is that I became more familiar with what I am doing and what my next steps are. I believe this was accomplished through the horizontal bar that identifies where I am and what I am doing, the task log on the right, which summarises what I have done, and the little octopus that guides me along\". As for what to improve, two participants did not offer suggestions. One participant mentioned similar terminology that caused some confusion. Another participant suggested changing the position of the octopus and finally adding some colors and design elements.", "7 PHASE 4: REVISING THE HEURISTICS": "After generating the heuristics in Phase 1, heuristics 1-13 in Table 4, then validating them by conducting user testing of the developed prototype, we revised them to ensure their effectiveness considering the error types that the users made from Phase 3. As the error source of the errors mentioned in section 6.2, was not due to limitations of the proposed heuristics, there is no need to modify the adapted heuristics. Diving deeper into the cause of the steep drop in error rate, and taking as an example Task 3a: Fill in missing data, one of the reasons for this result is that in the prototype, we provided the user with only the feature that contained missing data and needs to be filled. A similar approach was followed in other tasks and for other types of information, and this approach was highlighted by the participants in the informal interviews. Thus, we added to our proposed heuristics the fourteenth heuristic, which is Context Relevance, with the description of \"Display information relevant to the user's current dataset and problem\". As a result, the final set of our proposed heuristics consists of the 14 heuristics presented in Table 4.", "8 DISCUSSION": "The GUI ML tools team has done a commendable job by providing a range of data science-related functionalities in a GUI format Establishing Heuristics for Improving the Usability of GUI Machine Learning Tools for Novice Users CHI '24, May 11-16, 2024, Honolulu, HI, USA Figure 8: Data preprocessing window of the prototype shows some of the heuristic's features: (1) build mode or deployment mode drop-down list, (2,3,4) guidance and suggestions, (5) displaying information relevant to the user's current dataset, (6) different types of help (tutorials, a real-time chatbot with experts, and documentation), (7) the tasks performed by the user. Octopl Model Budina Oato Proproco Custom Aomovng Data Undo Apoly Figure 9: SUS evaluation of the prototype. Not Acceptable Marginal Acceplable Worsulmaainable Adjective Grade; 40 100 that users can access without coding. However, based on several usability evaluations conducted on two of the most commonly used GUI ML tools (WEKA and KNIME) to answer (RQ1), it was found that the current GUI ML tools have significant and catastrophic usability issues that need to be addressed urgently before the product can be released. Some frequent problems identified include the need for better feedback, adequate help and documentation, and too much unnecessary information and options that may need to be clarified for users. Additionally, the tools require clear labeling and buttons to minimize confusion. During user testing, all errors were categorized as errors by confusion, with the top source of confusion being the lack of clear labels or buttons and missing paths. Regarding the satisfaction usability attribute, WEKA, one of the most popular GUI ML tools, received an average unacceptable satisfaction score of 49. As a result, the usability of those tools needs to be improved. Additionally, it was found that users faced difficulty understanding the interface, requiring more explicit guidance, learnability, trust, functionality, and aesthetic aspects. This led to the need for a more comprehensive set of heuristics. In 1990, Nielsen and Molich developed usability guidelines, but since then, user interfaces have evolved, and researchers have recognized the need to adapt Nielsen's broad set of heuristics to specific interfaces [29]. In this research, to address RQ2, we have proposed new heuristics and updates to Nielsen's heuristics, which can be applied to GUI ML tool interfaces to provide a good user experience using a four-phased approach. In phase one, we generated an initial set of heuristics based on Nielsen's heuristics, using the results of a conducted cognitive walk-through and Nielsen's heuristic evaluation of two popular GUI ML tools (Weka and KNIME), surveying the needs of GUI ML tools users, and user testing evaluation of Weka. This resulted in three new heuristics and five updated heuristics. It is worth noting that the proposed heuristics, although for us originating from the empirical evaluation, were also mentioned when adapting Nielsen's heuristics to other domains or proposing Usability Guidelines. Guidance and trustworthiness (Heuristics 11 and 12) were proposed in the heuristics for conversational agents [29]. In the conversational agents' context, guidance was a result of two heuristics, Recognition rather than recall and Help and Documentation , in which users should be provided with feedback and guidance throughout the conversation to understand the status of the system better. For our context, it is more about guiding the user to what he should do next, either in terms of the upcoming tasks or how to perform the current task [29]. It also presents an alternative to scaffolding, heavily used in current iML tools, and follows a suggestion made in [32] that instead of scaffolding a recommender system that specifically focuses on suggesting helpful next steps based on the user's prior steps in the transfer learning workflow pipeline may assist non-expert users in their progress without the limitations imposed by scaffolding. As for trustworthiness, the conversational agent should be truthful in its interactions to encourage trustworthiness. However, in CHI '24, May 11-16, 2024, Honolulu, HI, USA Yamani et al. our context, the interaction is not the dialog [29]. It is related to how the data is stored and how the predictions are made. As for Adaptation to growth (Heuristic 13), It was proposed in the context of providing Guidelines to Human-AI Interaction [3]. Adaptation to growth has a similar meaning to the \"Learn from user behavior\" guideline, in which in the context of Human-AI Interaction, AI should personalize the experience by learning from the human actions over time [3], whilst in the context of GUI ML tools we personalize the experience by adapting to the learning curve of the user and reducing the intensity of the tips and recommendations as to the learning increases. In our context, the system should allow for customization and extendability with Python code, for example, as the user gains experience. Then, in phase two, we developed a prototype based on the newly proposed heuristics. Then, we validated the proposed heuristics in phase 3, using user testing on the developed prototype. Phase 3 showed significant improvement in task completion and reduction of error rates as comparing the results of user testing of Weka 4.2 and user testing of the prototype, the superior performance of the prototype built based on the heuristics over Weka is obvious, with a completion rate 3.9x faster in favor of the prototype, illustrated in Figure 10. In addition, as illustrated in Figure 11, the error rate was ninth of the errors captured when user testing Weka. The errors are narrowed down to using similar terminology such as 'deployment mode' and 'model deployment', which was a mistake from our side and violated Nielsen's Forth Heuristic. Thus, we replaced the Model Deployment button label with Export the Model. It was interesting too that although splitting the data is usually grouped with modeled building, in Weka, for example, for efficiency of use, novice users did suggest having it with data preprocessing favoring consistency and standard. Figure 10: Tasks Completion Comparison . Weka User Testing Prototype User Testing 3 8 8 1 Phase 3 also showed an increase in user satisfaction as the SUS survey went from a score of 49 for Weka to a score of 74 . 62. Moreover, conversations in the informal interviews and SUS open-ended questions from \"I feel I'm lost\", and \"The labels are confusing, I cannot link them with the tutorial\", to the need to add delighters such as colors, motion to the octopus, all of the aforementioned adds to the validity of the generated heuristics. Finally, in phase 4, we revised the new heuristics by proposing an additional heuristic, raising the total number of the GUI ML tools heuristics to 14. Context Relevance was also proposed in the context of providing Guidelines to Human-AI Interaction and maps Figure 11: Error rate Comparison . Weka Testing Prototype User Testing 16 12 3 3 76 8 User 1 8 8 to Show contextually relevant information in the study [3], with the difference that in Human-AI Interaction, the context is the user's inferred goals and attention during the interaction, while in our GUI ML Tools Heuristics, the context is the current dataset and problem. This becomes increasingly important when building Fair ML models as emphasized by practitioners [22]. To summarize, after conducting a usability evaluation on the prototype of the GUI ML tool that followed the newly proposed set of heuristics, it was found that fewer usability issues were present in the prototype. This indicates that the proposed set is more appropriate for assessing the usability of GUI ML tools than Nielsen's set (RQ2).", "8.1 Threats to Validity": "External validity is threatened by two key factors. First, just two GUI ML tools (Weka and KNIME) were investigated and used in the experiment to identify usability issues with GUI ML tools. To mitigate this threat, Weka and KNIME were tested using the three most common usability evaluation methods: cognitive walkthrough and heuristic evaluation for both tools, as well as user testing, including SUS for Weka. Second, the user testing evaluation for Weka had a limited sample number of participants. As a result, these users may not be a representative sample of all novices. Furthermore, while 27 responses to the survey conducted to collect user needs for GUI ML tools is a reasonable number, additional responses are required to obtain more needs and perceptions. Construct validity is threatened by the fact that the developed prototype didn't meet the adaptation to the growth heuristic. So, the conducted user testing cannot be used to validate this heuristic because it tends to be presented to the novice user after a long period of usage; therefore, it is impossible to validate it from a single user testing session. In terms of the Conclusion validity , the results of this study are adequate to conclude. The intended users and tasks were carefully gathered through user and task analysis. Furthermore, the usability measures were chosen concerning the novice users, and the GUI ML tools were selected based on their popularity and significant features. Finally, all the data were made available for replication, and the methodology of this study is detailed in Section 3.", "9 CONCLUSIONS": "With the rise of ML techniques and them being integrated with many disciplines where professionals are not familiar with the basic concepts of ML or computation techniques in general, the need Establishing Heuristics for Improving the Usability of GUI Machine Learning Tools for Novice Users CHI '24, May 11-16, 2024, Honolulu, HI, USA for usable GUI ML tools arises. However, when addressing RQ1, critical usability issues were identified when evaluating popular GUI ML tools, including inadequate feedback, documentation, and excessive and confusing options. This resulted in an unsatisfactory satisfaction score, emphasizing the need to improve these tools' usability. Therefore, this work provided a set of 14 heuristics adapted and customized from Nielsen's heuristics. This set was developed through a series of empirical evaluations to generate the heuristics. The evaluations included Nielsen's heuristics evaluation, cognitive walkthrough, a survey, user testing, and SUS survey. Upon generating the heuristics, a prototype was developed conforming to those heuristics, and through user testing, those heuristics were validated and revised. Based on the results, it is evident that the prototype designed using the proposed heuristics performs better in terms of usability when compared to the GUI ML tool (WEKA). This indicates that the new heuristics are more effective in enhancing the usability of GUI ML tools and user experience by including new heuristics such as Guidance, Trustworthiness, Adaptation to growth, and Context relevance (RQ2). Furthermore. We expect this work will lay a solid foundation for designers seeking to develop GUI ML tools with a delightful user experience to add to the toolkit of professionals across disciplines. The future direction of this work has three folds: a Machine Learning part concerning building the logic and models behind the suggestion system, the personalization, and the octopus helper. The basic tasks that cater to novice users across various problems should be systematically investigated in cross-disciplinary work between requirement engineering and Machine Learning. Lastly, in terms of usability, a longitude experiment should be conducted to measure the user experience in terms of trust and adaptation to growth, as they are hard to measure from a single experiment.", "ACKNOWLEDGMENTS": "The authors acknowledge the support of King Fahd University of Petroleum and Minerals in the development of this work and acknowledge the support provided by the Deanship of Research Oversight and Coordination at King Fahd University of Petroleum & Minerals (KFUPM). The authors would like to thank all participants in the study.", "REFERENCES": "[1] MA Alsalem, AA Zaidan, BB Zaidan, M Hashim, Osamah Shihab Albahri, Ahmed Shihab Albahri, Ali Hadi, and KI Mohammed. 2018. Systematic review of an automated multiclass detection and classification system for acute Leukaemia in terms of evaluation and benchmarking, open challenges, issues and methodological aspects. Journal of medical systems 42, 11 (2018), 1-36. https://doi.org/10.1007/s10916-018-1064-9 [2] Saleema Amershi, Andrew Begel, Christian Bird, Robert DeLine, Harald Gall, Ece Kamar, Nachiappan Nagappan, Besmira Nushi, and Thomas Zimmermann. 2019. Software engineering for machine learning: A case study. In 2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP) . IEEE, 291-300. https://doi.org/10.1109/ICSE-SEIP.2019.00042 [3] Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen, et al. 2019. Guidelines for human-AI interaction. In Proceedings of the 2019 chi conference on human factors in computing systems . 1-13. https://doi.org/10.1145/3290605. 3300233 [4] Tejenderkaur H. Sandhu and. 2018. MACHINE LEARNING AND NATURAL LANGUAGE PROCESSING - A REVIEW. International Journal of Advanced Research in Computer Science 9, 2 (Feb. 2018), 582-584. https://doi.org/10.26483/ ijarcs.v9i2.5799 [5] Benjamin B\u00e4hr. 2017. Prototyping of user interfaces for mobile applications . Springer. https://doi.org/10.1007/978-3-319-53210-3 [6] Aaron Bangor, Philip Kortum, and James Miller. 2009. Determining what individual SUS scores mean: Adding an adjective rating scale. Journal of usability studies 4, 3 (2009), 114-123. [7] Anmol Bansal and Satyajee Srivastava. 2018. Tools Used in Data Analysis: A Comparative Study. https://api.semanticscholar.org/CorpusID:220513449 [8] Michael R. Berthold, Nicolas Cebron, Fabian Dill, Thomas R. Gabriel, Tobias K\u00f6tter, Thorsten Meinl, Peter Ohl, Kilian Thiel, and Bernd Wiswedel. 2009. KNIME - the Konstanz Information Miner: Version 2.0 and Beyond. SIGKDD Explor. Newsl. 11, 1 (Nov. 2009), 26-31. https://doi.org/10.1145/1656274.1656280 [9] C Shoba Bindu. 2015. Secure usable authentication using strong pass text passwords. International Journal of Computer Network and Information Security 7, 3 (2015), 57-64. https://doi.org/10.5815/ijcnis.2015.03.08 [10] Clodis Boscarioli, Jos\u00e9 Viterbo, Mateus Felipe Teixeira, and Victor Hugo R\u00f6hsig. 2014. Analyzing HCI Issues in Data Clustering Tools. In International Conference on Human Interface and the Management of Information . Springer, 22-33. https: //doi.org/10.1007/978-3-319-07731-4_3 [11] Jenna Burrell. 2016. How the machine 'thinks': Understanding opacity in machine learning algorithms. Big Data & Society 3, 1 (Jan. 2016), 205395171562251. https: //doi.org/10.1177/2053951715622512 [12] Michelle Carney, Barron Webster, Irene Alvarado, Kyle Phillips, Noura Howell, Jordan Griffith, Jonas Jongejan, Amit Pitaru, and Alexander Chen. 2020. Teachable Machine: Approachable Web-Based Tool for Exploring Machine Learning Classification. In Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems (CHI '20) . ACM. https://doi.org/10.1145/3334480.3382839 [13] Xiaojun Chen, Yunming Ye, Graham Williams, and Xiaofei Xu. 2007. A survey of open source data mining systems. In Pacific-Asia Conference on Knowledge Discovery and Data Mining . Springer, 3-14. https://doi.org/10.1007/978-3-54077018-3_2 [14] Madhurjya Chowdhury. 2021. Top 10 machine learning tools 2021. https: //www.analyticsinsight.net/top-10-machine-learning-tools-2021/ [15] Graham Dove, Kim Halskov, Jodi Forlizzi, and John Zimmerman. 2017. UX Design Innovation. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems . ACM. https://doi.org/10.1145/3025453.3025739 [16] John J. Dudley and Per Ola Kristensson. 2018. A Review of User Interface Design for Interactive Machine Learning. ACM Transactions on Interactive Intelligent Systems 8, 2 (June 2018), 1-37. https://doi.org/10.1145/3185517 [17] John J Dudley and Per Ola Kristensson. 2018. A review of user interface design for interactive machine learning. ACM Transactions on Interactive Intelligent Systems (TiiS) 8, 2 (2018), 1-37. https://doi.org/10.1145/3185517 [18] Shraddha Dwivedi, Paridhi Kasliwal, and Suryakant Soni. 2016. Comprehensive study of data analytics tools (RapidMiner, Weka, R tool, Knime). In 2016 Symposium on Colossal Data Analysis and Networking (CDAN) . IEEE, 1-8. https://doi.org/10.1109/CDAN.2016.7570894 [19] Jerry Fails and Dan Olsen. 2003. Interactive Machine Learning. International Conference on Intelligent User Interfaces, Proceedings IUI (05 2003). https://doi. org/10.1145/604045.604056 [20] K. J. Kevin Feng, Maxwell James Coppock, and David W. McDonald. 2023. How Do UX Practitioners Communicate AI as a Design Material? Artifacts, Conceptions, and Propositions. In Proceedings of the 2023 ACM Designing Interactive Systems Conference (Pittsburgh, PA, USA) (DIS '23) . Association for Computing Machinery, New York, NY, USA, 2263-2280. https://doi.org/10.1145/3563657.3596101 [21] K. J. Kevin Feng and David W. Mcdonald. 2023. Addressing UX Practitioners' Challenges in Designing ML Applications: an Interactive Machine Learning Approach. In Proceedings of the 28th International Conference on Intelligent User Interfaces (IUI '23) . ACM. https://doi.org/10.1145/3581641.3584064 [22] K. J. Kevin Feng and David W. Mcdonald. 2023. Addressing UX Practitioners' Challenges in Designing ML Applications: An Interactive Machine Learning Approach. In Proceedings of the 28th International Conference on Intelligent User Interfaces (Sydney, NSW, Australia) (IUI '23) . Association for Computing Machinery, New York, NY, USA, 337-352. https://doi.org/10.1145/3581641.3584064 [23] Sidong Feng, Mingyue Yuan, Jieshan Chen, Zhenchang Xing, and Chunyang Chen. 2023. Designing with Language: Wireframing UI Design Intent with Generative Large Language Models. https://doi.org/10.48550/arXiv.2312.07755 arXiv:2312.07755 [cs.HC] [24] Nishit Gajjar, Vinoth Pandian Sermuga Pandian, Sarah Suleri, and Matthias Jarke. 2021. Akin: Generating UI Wireframes From UI Design Patterns Using Deep Learning. In 26th International Conference on Intelligent User Interfaces - Companion (College Station, TX, USA) (IUI '21 Companion) . Association for Computing Machinery, New York, NY, USA, 40-42. https://doi.org/10.1145/ 3397482.3450727 [25] Marco Gillies, Rebecca Fiebrink, Atau Tanaka, J\u00e9r\u00e9mie Garcia, Fr\u00e9d\u00e9ric Bevilacqua, Alexis Heloir, Fabrizio Nunnari, Wendy Mackay, Saleema Amershi, Bongshin Lee, Nicolas d'Alessandro, Jo\u00eblle Tilmanne, Todd Kulesza, and Baptiste Caramiaux. 2016. Human-Centred Machine Learning. In Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems . ACM. https://doi.org/10.1145/2851581.2856492 CHI '24, May 11-16, 2024, Honolulu, HI, USA Yamani et al. [26] Sadiq Hussain, Neama Abdulaziz Dahan, Fadl Mutaher Ba-Alwib, and Najoua Ribata. 2018. Educational data mining and analysis of students' academic performance using WEKA. Indonesian Journal of Electrical Engineering and Computer Science 9, 2 (2018), 447-459. https://doi.org/10.11591/ijeecs.v9.i2.pp447-459 [27] Jose Maria Santiago III, Giselle Nodalo, Jolene Valenzuela, and Jordan Aiko Deja. 2021. Explore, edit, guess: understanding novice programmers' use of codeblocks for regression experiments. (2021). [28] Jongwan Kim, Jae-wook Jeon, Ki-yeon Kim, et al. 2021. Improving Student's Design Prototyping Skills Using Interactive Prototyping Tool. Journal of Multimedia Information System 8, 1 (2021), 75-78. https://doi.org/10.33851/JMIS.2021.8.1.75 [29] Raina Langevin, Ross J Lordon, Thi Avrahami, Benjamin R Cowan, Tad Hirsch, and Gary Hsieh. 2021. Heuristic evaluation of conversational agents. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems . 1-15. https://doi.org/10.1145/3411764.3445312 [30] Fang Liu. 2008. Usability evaluation on websites. In 2008 9th international conference on computer-aided industrial design and conceptual design . IEEE, 141-144. https://doi.org/10.1109/CAIDCD.2008.4730538 [31] Jennifer Mankoff, Anind K Dey, Gary Hsieh, Julie Kientz, Scott Lederer, and Morgan Ames. 2003. Heuristic evaluation of ambient displays. In Proceedings of the SIGCHI conference on Human factors in computing systems . 169-176. https: //doi.org/10.1145/642611.642642 [32] Swati Mishra and Jeffrey M Rzeszotarski. 2021. Designing Interactive Transfer Learning Tools for ML Non-Experts. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (CHI '21) . ACM. https://doi.org/10.1145/ 3411764.3445096 [33] Christine Murad, Cosmin Munteanu, Benjamin R Cowan, and Leigh Clark. 2019. Revolution or evolution? Speech interaction and HCI design guidelines. IEEE Pervasive Computing 18, 2 (2019), 33-45. https://doi.org/10.1109/MPRV.2019. 2906991 [34] Jakob Nielsen. 2022. Severity Ratings for Usability Problems: Article by Jakob Nielsen. https://www.nngroup.com/articles/how-to-rate-the-severity-ofusability-problems/ [35] Jakob Nielsen and Rolf Molich. 1990. Heuristic evaluation of user interfaces. In Proceedings of the SIGCHI conference on Human factors in computing systems Empowering people - CHI '90 . ACM Press. https://doi.org/10.1145/97243.97281 [36] Nielsen Norman. 2001. Success rate: The simplest usability metric. Retrieved April 16, 2022 from https://www.nngroup.com/articles/success-rate-thesimplest-usability-metric/ [37] GO Ogunleye, SG Fashoto, CY Daramola, LA Ogundele, and TO Ojewumi. 2019. Development of a Simple Graphical Interface Based Software for Machine Learning and Data Visualization. International Journal of Recent Technology and Engineering 8, 2 (2019), 3770-3777. https://doi.org/10.35940/ijrte.B3426.078219 [38] Ritu Ratra and Preeti Gulia. 2020. Experimental evaluation of open source data mining tools (WEKA and Orange). Int. J. Eng. Trends Technol 68, 8 (2020), 30-35. https://doi.org/10.14445/22315381/IJETT-V68I8P206S [39] Sharmistha Roy and Prasant Kumar Pattnaik. 2014. Some popular usability evaluation techniques for websites. In Proceedings of the International Conference on Frontiers of Intelligent Computing: Theory and Applications (FICTA) 2013 . Springer, 535-543. [40] Jeff Sauro. 2022. 5 Ways to Interpret a SUS Score - MeasuringU. https:// measuringu.com/interpret-sus-score/ [41] Ririn Suharsih, Rinanda Febriani, and Sutadi Triputra. 2021. Usability of Jawara Sains Mobile Learning Application Using System Usability Scale (SUS). Jurnal Online Informatika 6, 1 (2021), 41-52. https://doi.org/10.15575/join.v6i1.700 [42] Rini van Solingen, Vic Basili, Gianluigi Caldiera, and H. Dieter Rombach. 2002. Goal Question Metric (GQM) Approach. https://doi.org/10.1002/0471028959. sof142 [43] LENDAVE VIJAYSINH. 2021. Beginner's Guide to WEKA - A Tool for ML and Analytics. https://analyticsindiamag.com/beginners-guide-to-Weka-a-tool-forml-and-analytics/ [44] Zhuxiaona Wei and James A Landay. 2018. Evaluating speech-based smart devices using new usability heuristics. IEEE Pervasive Computing 17, 2 (2018), 84-96. https://doi.org/10.1109/MPRV.2018.022511249 [45] Cathleen Wharton, John Rieman, Clayton Lewis, and Peter Polson. 1994. The cognitive walkthrough method: A practitioner's guide. In Usability inspection methods . 105-140. [46] Balsamiq Wireframes. 2022. \"Balsamiq Wireframes - Industry Standard LowFidelity Wireframing Software | Balsamiq\". https://balsamiq.com/wireframes/ [47] Ian H Witten and Eibe Frank. 2002. Data mining: practical machine learning tools and techniques with Java implementations. Acm Sigmod Record 31, 1 (2002), 76-77. https://doi.org/10.1145/507338.507355 [48] Ben Wolford. 2022. What is GDPR, the EU's new data protection law? https: //gdpr.eu/what-is-gdpr/ [49] Qian Yang, Nikola Banovic, and John Zimmerman. 2018. Mapping machine learning advances from hci research to reveal starting places for design innovation. In Proceedings of the 2018 CHI conference on human factors in computing systems . 1-11. https://doi.org/10.1145/3173574.3173704 [50] Qian Yang, Alex Scuito, John Zimmerman, Jodi Forlizzi, and Aaron Steinfeld. 2018. Investigating How Experienced UX Designers Effectively Work with Machine Learning. In Proceedings of the 2018 Designing Interactive Systems Conference . ACM. https://doi.org/10.1145/3196709.3196730 [51] Qian Yang, Jina Suh, Nan-Chen Chen, and Gonzalo Ramos. 2018. Grounding Interactive Machine Learning Tool Design in How Non-Experts Actually Build Models. In Proceedings of the 2018 Designing Interactive Systems Conference (DIS '18) . ACM. https://doi.org/10.1145/3196709.3196729", "APPENDIX": "Establishing Heuristics for Improving the Usability of GUI Machine Learning Tools for Novice Users CHI '24, May 11-16, 2024, Honolulu, HI, USA", "Table 5: Jakob Nielsen's severity rating.": "Table 6: Tasks for user testing evaluation", "Tiana": "Age: 39 Work State agent Location;USA California Character Rational", "Skills": "Communicaton Social Problem-Solving: Time management Arithmetic", "Bio": "Tlana s agent she has been 0 top agent for 10 years. Her based more than 859 Of referrals {rom saristied cllents Cllents value skills analytical skills. and ability accurately and market home She understands and values the trust customers place her and stives every exceed their expeclallons Therefore with the Increasing demand and supply In the market artificial intelligence, predicting house prices with the highest accuracy and the least knowing her technical background is considered average", "Goal": "To be known for being 0 real estate agent that can always give the best house price", "Frustrations": "Being 0 newble In something Having Figure 12: Persona for the user analysis", "Personality": "Introvert Extrovert Thinking Feeling Sensing Judging Perceiving", "Motivation": "Incentve Fear Growh Power Social CHI '24, May 11-16, 2024, Honolulu, HI, USA Yamani et al. Table 7: Results of Heuristic Evaluation of Weka and KNIME Establishing Heuristics for Improving the Usability of GUI Machine Learning Tools for Novice Users CHI '24, May 11-16, 2024, Honolulu, HI, USA Table 8: Cognitive walkthrough results.Task X Action Y will be referred to as Task X A(Y). CHI '24, May 11-16, 2024, Honolulu, HI, USA Yamani et al. Weka KNIME 50 45 40 35 30 25 20 15 10 5 2 Issue Severity Figure 13: Issues discovered during the Heuristic Evaluation by severity. KNIME Weka 12 10 Q1 Q2 Q3 Cognitive walkthrough results Figure 14: Cognitive walkthrough Results by Question. 3.75 2.5 1.25 3 3 8 66 8 @ @ @ @ 8 3 Figure 15: Average Task completion duration (in minutes). 16 14 12 10 1 8 8 76 8 8 @ @ Figure 16: Error rate of Tasks During User Testing of Weka. Figure 17: Average Task completion duration (in minutes) prototype. 3.75 2.5 1.25 8 8 66 @ Figure 18: Error rate of Tasks During User Testing of Prototype. 16 14 12 10 8 86 8 8 8 Establishing Heuristics for Improving the Usability of GUI Machine Learning Tools for Novice Users CHI '24, May 11-16, 2024, Honolulu, HI, USA Figure 19: Previous experience with ML tools and educational materials No courses courses degree Weka KNIME Azure RapidMiner Less than months months year More than year 1506 406 1596 3096 5206 5696"}
