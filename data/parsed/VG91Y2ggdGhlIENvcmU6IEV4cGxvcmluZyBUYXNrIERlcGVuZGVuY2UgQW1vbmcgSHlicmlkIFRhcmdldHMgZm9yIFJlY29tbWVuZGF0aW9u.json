{
  "Touch the Core: Exploring Task Dependence Among Hybrid Targets for Recommendation": "Xing Tang ‚àó FiT, Tencent Shenzhen, China xing.tang@hotmail.com Yang Qiao ‚àó FiT, Tencent Shenzhen, China sunnyqiao@tencent.com Fuyuan Lyu ‚àó‚Ä† McGill University & MILA Montreal, Canada fuyuan.lyu@mail.mcgill.ca",
  "Dugang Liu ‚Ä°": "Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ) Shenzhen, China dugang.ldg@gmail.com Xiuqiang He ‚Ä° FiT, Tencent Shenzhen, China xiuqianghe@tencent.com",
  "ABSTRACT": "As user behaviors become complicated on business platforms, online recommendations focus more on how to touch the core conversions, which are highly related to the interests of platforms. These core conversions are usually continuous targets, such as watch time , revenue , and so on, whose predictions can be enhanced by previous discrete conversion actions. Therefore, multi-task learning (MTL) can be adopted as the paradigm to learn these hybrid targets. However, existing works mainly emphasize investigating the sequential dependence among discrete conversion actions, which neglects the complexity of dependence between discrete conversions and the final continuous conversion. Moreover, simultaneously optimizing hybrid tasks with stronger task dependence will suffer from volatile issues where the core regression task might have a larger influence on other tasks. In this paper, we study the MTL problem with hybrid targets for the first time and propose the model named Hybrid Targets Learning Network (HTLNet) to explore task dependence and enhance optimization. Specifically, we introduce label embedding for each task to explicitly transfer the label information among these tasks, which can effectively explore logical task dependence. We also further design the gradient adjustment regime between the final regression task and other classification tasks to enhance the optimization. Extensive experiments on two offline public datasets and one real-world industrial dataset are conducted to validate the effectiveness of HTLNet. Moreover, online A/B tests on the financial recommender system also show that our model has improved significantly. Our implementation is available here 1 . ‚àó All authors contributed equally to this research. ‚Ä† This work is done when working at FiT Tencent. ‚Ä° Corresponding Authors 1 https://github.com/fuyuanlyu/HTLNet Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. RecSys '24, October 14-18, 2024, Bari, Italy ¬© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0505-2/24/10...$15.00 https://doi.org/10.1145/3640457.3688101",
  "CCS CONCEPTS": "¬∑ Information systems ‚Üí Recommender systems .",
  "KEYWORDS": "Multi-task Learning, Task Dependence, Hybrid Targets, Recommendation",
  "ACMReference Format:": "Xing Tang, Yang Qiao, Fuyuan Lyu, Dugang Liu, and Xiuqiang He. 2024. Touch the Core: Exploring Task Dependence Among Hybrid Targets for Recommendation. In 18th ACM Conference on Recommender Systems (RecSys '24), October 14-18, 2024, Bari, Italy. ACM, New York, NY, USA, 13 pages. https://doi.org/10.1145/3640457.3688101",
  "1 INTRODUCTION": "Recommender systems (RS) have played a crucial role in online business platforms, which provide personalized candidates (e.g., ads, videos, funds, apps, etc) based on user interests [39]. Usually, the recommendation model in these platforms is required to meet the demand of multiple objectives due to the increasing complexity of user behaviors. For example, the fund recommendation model should consider the likelihood of the user clicking a fund and the likelihood of the user purchasing the fund [17]. However, online platforms have begun to make efforts to touch the core conversion [12, 15, 29]. As shown in Figure 1, video-sharing platforms value most users' watch time, and fund investment platforms care how much the user invests. Along with preceding discrete click or purchase targets, these continuous core targets consist of hybrid targets. Hence, learning tasks with hybrid targets simultaneously in the recommendation model benefits the online platforms. Previously, multi-task learning (MTL) [1] has been introduced in the recommendation to make predictions for multiple targets, attracting much attention and making a success in lots of application [8, 11, 17, 26, 28]. By jointly training multiple tasks in a single model, the MTL can improve performance and decrease computational cost by knowledge sharing and cross-task transferring [30]. A vital issue for the success of MTL models is modeling relationships between multiple tasks. Unlike implicit relationships that require a well-designed learning paradigm, the explicit task dependence must be explored among these targets [24, 31]. As an example illustrated in Figure 1, recommending a video with long watch time is RecSys '24, October 14-18, 2024, Bari, Italy Xing Tang, et al. Figure 1: Illustration of hybrid target learning paradigm in various business platforms. The red one denotes core targets. exposure click purchase amount Inputs ‚Ä¶ view watch time ! ! ! \" ! #$! ! # MTL Networks ‚Ä¶ ‚Ä¶ buying capital exposure click exposure click dependent on the task of whether the user view it, which sequentially depends on whether the user click it. This prior is greatly helpful for modeling these targets simultaneously in the sequential dependence MTL (SDMTL) paradigm. However, the differences between the core and preceding targets make the modeling more challenging. On the one hand, the divergence exists in the modeling objectives. The task of the core target aims to model the distribution of continuous value, compared with preceding tasks modeling the click/conversion rate. Nevertheless, the high conversion rates seldom indicate the high value of core targets, which is contrary to the assumption that a high click rate indicates a high conversion rate in sequential dependence multitask learning [17, 27]. Therefore, exploring task dependence among hybrid targets is more complicated than traditional MTL. On the other hand, the tasks of core targets are usually regression tasks, while the preceding tasks are classification tasks. Obviously, the gradients based on regression loss will not be in accordance with the gradients of classification loss from the view of magnitude and direction. This fact will lead to training stability, and thus degrading the performance [21]. Hence, the optimization strategy designed for hybrid target learning is necessary. There have been plenty of state-of-the-art general MTL models for recommendation. Some mainly investigate how to learn implicit task relationships and extract corresponding representations. Usually, they introduce an ensemble of expert submodules and gating networks to learn task relationships. Although many efforts have been devoted to learning implicit relationships among tasks [11, 16, 20], these approaches are still limited in two aspects. First, the negative information will transfer with the shared structure in an unpredictable manner. No matter whether the two tasks are related, the information will pass between them. The task dependence priors are absent in modeling, which makes the prediction more difficult. As a result, some methods resort to formulating the learning paradigm as SDMTL [17, 24, 27, 31, 37]. Assuming all the targets are discrete, some specific designed architectures and losses are proposed. However, these works neglect the complexity of the dependence between the classification and regression tasks. As SDMTL assumes ÀÜ ùë¶ ùëò -ÀÜ ùë¶ ùëò -1 = ùëÉ ( ùë° ùëò = 0 , ùë° ùëò -1 = 1 ) , implying the difference of prediction scores between adjacent tasks ( ùëò -1 , ùëò ) is the probability of the task ùëò not happening while the task ùëò -1 is observed. However, the equation can no longer hold when the prediction is continuous since we cannot conclude that different continuous values lead to the same probability. Besides, some works focus on optimization strategies to enhance performance. Gradients between tasks should be handled to avoid training instability and performance deterioration [4, 8, 21, 35]. These methods have no special preference for hybrid target learning, which should seriously consider the fact that regression tasks may have a dominant influence on the network gradients. In this paper, we address the hybrid targets learning problem by proposing a Hybrid Targets Learning Net (HTLNet). There are two main challenges identified for our HTLNet. The first challenge is how to explore the dependence among hybrid targets. We tackle this challenge by introducing label embedding and information fusion units in our model. The label embedding unit enables the adjacent tasks to transfer the prediction information following task dependence, and the information fusion unit ensures the information is adaptively transferred. Therefore, HTLNet explores task dependence among hybrid targets through explicit label embedding and implicit task representation. The second challenge is how to optimize the complex model. Optimization is extremely hard to handle as HTLNet aims to train regression and classification tasks in a unified model. Therefore, we further develop an optimization strategy to conquer the gradients in a shared layer. Our major contributions are summarized as follows: ¬∑ This paper first distinguishes the hybrid targets learning problem in the recommendation, which aims to model the sequential auxiliary discrete targets and core continuous targets in an MTL model concerning the core target. ¬∑ We propose a novel model HTLNet that incorporates label and task information to touch the core task. By developing a corresponding optimization strategy, HTLNet can effectively explore the dependence among hybrid targets in a stable way. ¬∑ Extensive offline experiments on public and real-world product datasets are conducted. The results demonstrate the superiority of the proposed model. Furthermore, online experiments also verify the stability and effectiveness of HTLNet. We organize the rest of the paper as follows. In Section 2, we briefly introduce related works. Section 3 formulates the MTL problem with hybrid targets in the recommendation. In section 4, HTLNet is introduced in detail. In Section 5, experimental details are given to verify our HTLNet. Finally, we conclude this paper in Section 6.",
  "2 RELATED WORK": "Multi-task Learning has broad applications in various fields [2, 3, 5, 13, 14, 18, 19, 22, 23, 34, 38]. In recommendation, many MTL network architectures are designed based on Multi-gate Mixtureof-Experts (MMoE) [16]. Progressive Layered Extraction (PLE) [20] Touch the Core: Exploring Task Dependence Among Hybrid Targets for Recommendation RecSys '24, October 14-18, 2024, Bari, Italy separates task-common and task-specific parameters to avoid parameter conflict. AdaTT [11] leverages a residual mechanism and a gating mechanism for task-to-task fusion, which adaptively learns both shared knowledge and specific knowledge. This research line fails to utilize the dependence among targets, which is far from satisfactory in modeling sequential dependence multi-task learning (SDMTL). A typical application of SDMTL is estimating post-click conversion rate (CVR) [17, 27, 37, 40]. These works formulate the problem as ùëùùê∂ùëáùê∂ùëâùëÖ = ùëùùê∂ùëáùëÖ √ó ùëùùê∂ùëâùëÖ and employ an auxiliary click-through rate (CTR) task to enhance the performance. Besides, DCMT [40] and ESCM 2 introduce imputation and inverse propensity weighting tasks to predict unbiased CVR estimation. Despite two dependent tasks, some methods expand the problem into multiple sequential dependence tasks [24, 28, 31]. AITM [31] proposes a novel information transfer mechanism between different conversion steps to explore the dependence among these targets. TAFE [24] investigates the differences between SDMTL and MTL problems and proposes a feature learning module to tackle SDMTL. However, these works assume the targets are discrete conversion steps, neglecting the complexity of hybrid targets learning. Finally, the training stability of multitask ranking models has attracted attention recently [21]. Besides, optimization strategy has been a mainstream way to enhance the performance [4, 8, 35]. These works are all based on the general MTL network, which does not consider the network architecture of hybrid targets learning.",
  "3 PRELIMINARY": "This section will elaborate on the MTL problem with hybrid targets in the recommendation. Generally, the recommendation model will rank items according to many objectives including one or more conversion targets and the core target. First, consider the prediction of the core target in recommendation over an input space X , where X = { x 1 , x 2 , ¬∑ ¬∑ ¬∑ , x ùëõ } represents input features. Given a large dataset of data points { x ùëñ , y ùëñ } where x ùëñ ‚àà X denotes the feature vector and yi ‚àà R is the corresponding core continuous target. Then, we assume it takes the user ùëá conversion steps to complete the core targets. In each step ùë° , y t i ‚àà { 0 , 1 } denotes whether the user completes the conversion and satisfies the constraint in sequential dependence that ùë¶ 1 ùëñ ‚©æ ùë¶ 2 ùëñ ‚©æ ¬∑ ¬∑ ¬∑ ‚©æ ùë¶ ùëá ùëñ . Therefore, { y , y 1 , y 2 , ¬∑ ¬∑ ¬∑ , y ùëá } consists of hybrid targets for all the data points. Meanwhile, we introduce these auxiliary sequential tasks to improve the core target's prediction accuracy and provide predictions of these conversions. The MTL problem with hybrid targets is then formulated as follows:  where Œò denotes the parameters of MTL model ùëì , ÀÜ y ùëñ is the prediction of core target and ÀÜ y ùë° ùëñ , ùë° ‚àà { 1 , ¬∑ ¬∑ ¬∑ , ùëá } are predictions of preceding targets. Note that the first equation denotes the regression task aiming to improve the core target prediction, and the remaining equations are classification tasks, composing a typical sequential dependence MTL (SDMTL) [24] respectively. One critical issue for learning this problem is exploring the dependence among hybrid targets with the MTL model according to Equation.1. The parameters Œò of the MTL model usually consist of three components, i.e. { ùúÉ ùë† , ùúÉ ùëêùëúùëüùëí , { ùúÉ ùë° } ùëá ùë° = 1 } , where ùúÉ ùë† is the shared parameters, ùúÉ ùëêùëúùëüùëí is the parameters for core targets prediction, and ùúÉ ùë° denotes task specific parameters. The Œò is learned by jointly minimizing the core target task loss and other task losses:   where L ùëêùëúùëüùëí can be either mean squared error(MSE) [36], and L ùë° is the Binary Cross-Entropy (BCE) loss due to the hybrid targets. With one optimizing step, the gradient for the MTL model is computed as follows:  As shown in Equation.3, one optimization step relies on the gradient, which is composed of two parts: the gradient for the core targets task and the sum of gradients for other tasks. The gradient component with a larger magnitude can influence the gradient more. Another critical issue for learning the MTL problem with hybrid targets is stabilizing the optimization and enhancing performance [21].",
  "4 METHOD": "To learn the hybrid targets effectively with the MTL model, we propose a novel network architecture, Hybrid Targets Learning Network (HTLNet). The overall framework of HTLNet is illustrated in Figure 2, which mainly consists of three components: (1) task towers for hybrid targets, (2) the label embedding unit (LEU) encoding the labels of auxiliary sequential tasks, (3) the information fusion unit (IFU) utilizing the information of all preceding tasks. Note that the latter two components make our model explore task dependence effectively. Furthermore, a gradient adaption method based on the HTLNet is also introduced, which enables the optimization to be stable and enhances the model performance.",
  "4.1 Framework of HTLNet": "We first introduce the details of HTLNet. The core idea is to explore the task dependence among hybrid targets by introducing label embedding and transferring both implicit and explicit information among tasks. As shown in Figure 2, all the tasks are shared with an embedding layer. With the input x ùëñ , the features usually include the user, item, and context features in the recommendation, which are denoted as a vector with ùëö fields:  Following the input, an embedding matrix E ‚àà ùëÖ ùëÄ √ó ùëë , where ùëÄ is the number of features, and ùëë is the embedding dimension, is used in the shared embedding layer to get the corresponding embedding vector for ùëó -ùë°‚Ñé field:  Then, the vectors are all stacked together as shared embedding vector, ei = [ ùëí ùëñ 1 , ùëí ùëñ 2 , ¬∑ ¬∑ ¬∑ , ùëí ùëñ ùëö ] . RecSys '24, October 14-18, 2024, Bari, Italy Xing Tang, et al. Figure 2: The overall framework of HTLNet. LEU represents the Latent Embedding Unit, which encodes label information, and the Information Fusion Unit, which adaptive infuses all preceding tasks. ! \" Shared Embedding Layer ! \" 012 LEU label emb flow representation flow classification task regression task Task Tower user field ‚Ä¶ ‚Ä¶ item field context field ‚Ä¶ Task Tower !\"# 1\"2 ‚Ä¶ IFU !\"# 1\"3 !\"# 4 $\" 1\"2 ‚Ä¶ IFU $\" 1\"3 $\" 4 ! \" 0 LEU Task Tower !\"# 1 ‚Ä¶ IFU !\"# 1\"2 !\"# 4 $\" 1 ‚Ä¶ IFU $\" 1\"2 $\" 4 4 !\"# 5 1\"2 $\" 5 1\"2 $\" 5 1 !\"# 5 1 concatenate stop gradient prediction Notice that the core continuous target introduces the difficulty of sharing information between hybrid targets. To explore the task dependency relationship, it is not enough to depend solely on shared embedding. Moreover, the prediction of the core target will benefit from the information from the preceding tasks. Therefore, we incorporated explicit and implicit information transferred from preceding tasks as input to the task tower. Specifically, for two adjacent tasks ùë° -1 and ùë° , the input for task ùë° is computed as:  where ùëüùëíùëù ùë° -1 ùëì and ùëôùëí ùë° -1 ùëì are the outputs from the IFUs of task ùë° -1 respectively. Notice that the ùëôùëí ùë° -1 ùëì is the explicitly label information, and the ùëüùëíùëù ùë° -1 ùëì is the implicit information from the task tower, which can be the output from a particular layer of the task tower. One way to encode label information is to use it directly as a constraint in the final loss [24, 31], which is inappropriate in hybrid target learning and will make the prediction of the core target suffer from the deficiency of preceding label information. We introduce the LEU here to address the issue, which will be illustrated in detail afterward. As to the classification tasks, the task tower gives the prediction probability of instance ùëñ :  where the MLP is multi-layer perception, and ùúé ( ùë• ) = 1 1 + ùëí -ùë• is the sigmoid function. The loss for these tasks is usually binary cross entropy:  Figure 3: Illustration of Label Embedding Unit and Information Fusion Unit. ‚äó denotes element-wise multiplication. Note that ùëù ùë° 0 + ùëù ùë° 1 = 1 . $ 3 0 $ 2 0 ! \" 0 Gumbel Softmax %# 3 %# 2 %# 0 (a) Label Embedding Unit # 3 ‚Ä¶ # 012 # 0 % 4 % 1\"2 % 1 # 4 0 Softmax (b) Information Fusion Unit As to the core continuous target, the task tower gives the numerical prediction of instance ùëñ :  and the loss can be MSE:  4.1.1 Label Embedding Unit. The LEU aims to encode the label information explicitly, as shown in Figure 3(a). Unlike the previous study [3], which solely focused on implicit information among task towers, encoding label information can let a task utilize the labels of all its preceding tasks as formulated in Equation 1. However, the discrete 0 / 1 label information can hardly be delivered directly to the core continuous target due to the hybrid targets. Touch the Core: Exploring Task Dependence Among Hybrid Targets for Recommendation RecSys '24, October 14-18, 2024, Bari, Italy Therefore, an embedding table consisting of two rows is introduced for each classification task in our setting, which is ùê∏ ùë° ‚àà ùëÖ 2 √ó ùêø ùëë for label 0 / 1. The embedding table enables a one-to-one mapping from a label to a ùêø ùëë -dimensional trainable vector. Each row contains the information corresponding to the label 0 or 1, denoted ùëôùëí 0 and ùëôùëí 1, respectively. An alternative to mapping the embedding is using the golden labels of tasks, which will introduce the train-test discrepancy. Specifically, the golden labels are only available in the training stage, which is missing in the test set. Moreover, directly using prediction labels for tasks leads to unstable training due to cascading errors between tasks. In our LEU, we sample a label from the predicted probability distribution instead of predefined labels. If the sampling label is a misleading prediction of the final core task, the back-propagated gradients from the core target will penalize the embedding. The only issue is the sampling operation is not differentiable, which is incompatible with our framework. To address this issue, we further introduce the Gumbel-softmax re-parameter operation [9] here to approximate the sampling by:  where ùúè is the decayed temperature parameter to control the smoothness of the Gumbel-softmax, when ùúè approximates to zero, the output will be a two-dimensional one-hot vector. The Gumbel-softmax allows HTLNet to calculate the soft weighted embedding value instead of hard selecting particular embedding, thus significantly reducing cascading errors. Its detailed motivation is further discussed in Appendix A. The output of LEU is then formalized as the weighted sum over the label embedding table:  Unlike directly utilizing the discrete label, this label embedding mechanism can transfer label information from discrete targets to the core continuous target. 4.1.2 Information Fusion Unit. The IFU is proposed to fuse all the information from the preceding tasks. Combined with LEU, there are two kinds of information to fuse, i.e., label embedding and task representation. The IFU adopts a similar attention mechanism in [31, 32] as illustrated in Figure 3(b). Specifically, the preceding task vectors for task ùë° can be denoted as [ e 0 , e 1 , ¬∑ ¬∑ ¬∑ , e ùë° ] , where e is either ùëôùëí from LEU or ùëüùëíùëù from task tower representation. The attention is designed to allocate the weights of these transferred information adaptively.  where ùë§ ùë° is formulated as:  Where < GLYPH<5> , GLYPH<5> > denotes the dot product, ùëò is the hidden dimension, and ‚Ñé 1 ( GLYPH<5> ) , ‚Ñé 2 ( GLYPH<5> ) , ‚Ñé 3 ( GLYPH<5> ) are learnable kernels for transforming input information into a new output space. Finally, we get label information ùëôùëí ùë° ùëì and task representation ùëüùëíùëù ùë° ùëì from IFU to explore the task dependence effectively.",
  "4.2 Optimization Strategy for HTLNet": "Despite exploring the task dependence, the designed network poses some optimization challenges. First, the loss of core continuous targets differs from other tasks, which means the gradients from the core target tower are very far from other tasks, leading to performance deterioration. Second, all the label embeddings and task representations will be transferred to the core target tower, allowing the core target loss to mainly influence the optimization of other tasks with the gradient backpropagation to label prediction and task representation. Hence, we are motivated to propose an optimization strategy for our HTLNet. Two ways for a task to influence other tasks are shared embedding and transferred information. However, as discussed above, the transferred information will disturb the preceding task's label prediction and task representation by LEU and IFU. Hence, we cut off the influence of transferred information, leaving the shared embedding as the only way to influence each other:  Then, we target dealing with the gradient of shared embedding from the view of direction and magnitude. Denoted ùê∫ ùëêùëúùëüùëí = ‚àá Œò L ùëêùëúùëüùëí and ùê∫ ùë° = ‚àá Œò L ùë° in Equation 3, we consider the pairs of task gradients { ùê∫ ùëêùëúùëüùëí , ùê∫ ùë° } ùëá ùë° = 1 to cope with the gradient conflict of shared embedding between discrete tasks and our core continuous task. For every classification task ùë° and core task, we first eliminate the gradient direction conflict with the core task being the target gradient for shared embedding [35]:  where ùõº is a hyperparameter to control the amount of removing the conflict. As a variant of method in [35], when the gradient of preceding task ùë° conflicts with the core task, meaning cosine similarity is negative, the Equation 16 projects the ùê∫ ùë° onto the normal plane of ùê∫ ùëêùëúùëüùëí . After iterations, all the ùê∫ ùë° s will not conflict with ùê∫ ùëêùëúùëüùëí , which means all preceding tasks are optimized towards core tasks. Besides gradient conflict, another issue is the magnitude discrepancy between ùê∫ ùëêùëúùëüùëí and ùê∫ ùë° . Notice that L ùëêùëúùëüùëí is MSE loss, the magnitude of which is usually more significant than the Logloss for L ùë° . Inspired by [8], we adaptive adjust the gradient magnitude of task ùë° toward the core task in a direct way:  where ùõæ is a hyperparameter tuned to balance the gradient magnitude between task ùë° and the core task, ‚à• ùê∫ ùëêùëúùëüùëí ‚à• ‚à• ùê∫ ùë° ‚à• is the calculated weight to adjust the gradients. We introduce a hyperparameter ùê∂ to clip the weight to avoid the explosion of gradients in a batch. When ‚à• ùê∫ ùëêùëúùëüùëí ‚à• ‚à• ùê∫ ùë° ‚à• is too large, it will clip up to ùê∂ , while it is too small, it will clip down to 1 ùê∂ . Combining both direction and magnitude, we summarize the gradient process for shared embedding in the Algorithm 1. RecSys '24, October 14-18, 2024, Bari, Italy Xing Tang, et al.",
  "Algorithm 1: Gradient for shared parameters": "Input: Tasks shared model parameters ùúÉ ùë† , core target task loss L ùëêùëúùëüùëí ( ùúÉ ùë† ) , preceding sequential task losses L ùë° ( ùúÉ ) , projection relax factor ùõº , balance relax factor ùõæ , balance clip threshold C Result: Gradient of shared model parameters ùê∫ ùë† ùê∫ ùëêùëúùëüùëí ( ùúÉ ùë† ) ‚Üê ‚àá ùúÉ L ùëêùëúùëüùëí ( ùúÉ ùë† ) 1 ùë† 2 for ùë° ‚Üê 1 ùë°ùëú ùëá do 3 ùê∫ ùë° ( ùúÉ ùë† ) ‚Üê ‚àá ùúÉ ùë† L ùë° ( ùúÉ ùë† ) 4 if ùê∫ ùë° ¬∑ ùê∫ ùëêùëúùëüùëí < 0 then 5 ùê∫ ùë° = ùê∫ ùë° -ùõº ùê∫ ùë° ¬∑ ùê∫ ùëêùëúùëüùëí ‚à• ùê∫ ùëêùëúùëüùëí ‚à• 2 ùê∫ ùëêùëúùëüùëí 6 end 7 ùë§ùëíùëñùëî‚Ñéùë° = ‚à• ùê∫ ùëêùëúùëüùëí ‚à• ‚à• ùê∫ ùë° ‚à• ; 8 if ùë§ùëíùëñùëî‚Ñéùë° < 1 /C then 9 ùë§ùëíùëñùëî‚Ñéùë° ‚Üê 1 /C 10 end 11 if ùë§ùëíùëñùëî‚Ñéùë° > C then 12 ùë§ùëíùëñùëî‚Ñéùë° ‚ÜêC 13 end 14 ùê∫ ùë° = ùõæ ¬∑ ùë§ùëíùëñùëî‚Ñéùë° ¬∑ ùê∫ ùë° + ( 1 -ùõæ ) ¬∑ ùê∫ ùë° 15 end 16 ùê∫ ùë† = ùê∫ ùëêùëúùëüùëí + Àù ùëá ùë° = 1 ùê∫ ùë° 17 ouput ùê∫ ùë†",
  "5 EXPERIMENTS": "In this section, we first outline the experimental setup for our model. There are two public and one private industry dataset to evaluate the performance. We design experiments to answer the following research questions: ¬∑ RQ1 : Does HTLNet achieve superior performance on MTL for hybrid targets compared with mainstream MTL models? ¬∑ RQ2 : How do the proposed network structure and optimization strategy affect the HTLNet? Can we only rely on one component to achieve the same performance? ¬∑ RQ3 : How well do the proposed LEU and IFU affect the performance of HTLNet? ¬∑ RQ4 : Does the optimization strategy for HTLNet perform better than the previous methods? ¬∑ RQ5 : Can the HTLNet also achieve better performance in the online setting?",
  "5.1 Experimental Setup": "5.1.1 Datasets. The datasets in previous MTL works only consist of tasks with a single type of label, which can not be directly used for evaluation in hybrid target learning. Hence, we define related tasks with hybrid targets from two public datasets, including video and revenue scenarios, and collect one product dataset from a realworld fund recommendation scenario. ¬∑ KuaiRand [6]: This is a sequential recommendation dataset collected from the video-sharing mobile app. Despite providing randomly exposed items, it also contains various user behaviors, such as clicking, sharing, and liking the app. In our experiments, we use the KuaiRand-pure and choose three hybrid targets defined in the dataset: click, long view, and watch time, setting watch time as core target. ¬∑ Kaggle-Revenue [25]: This Kaggle Acquire Valued Shoppers Challenge competition dataset contains a complete basket-level shopping history for customers and companies. The data process follows [25], and we define three hybrid targets: user repurchase in one year, user repurchase in one month, and the amount of user repurchase in one month, which is the core target. ¬∑ Product: The product dataset is collected from the recommendation logs from an online fund recommendation platform in the past three months, which records users' click and purchase fund behaviors. We define three hybrid targets: click, purchase, and the amount of purchase. The statistics of these three datasets are shown in Table 1. The datasets are divided chronologically into training, validation, and test sets, keeping the ratio 8 : 1 : 1. Table 1: Statistics of the two public datasets and one product dataset. 5.1.2 Baselines. We choose baseline models from four aspects. First, the DNN is a three-layer MLP structure for a single task. Second, Shared-Bottom [1], MMoE [16], PLE [20], and AdaTT [11] are SOTA models in MTL for recommendation. Third, ESMM [17] and AITM [31] are MTL models that take the sequential dependence into consideration. Finally, MetaBalance [8] is the recent optimization method for MTL. The implementation details are discussed in Appendix B.1. 5.1.3 Evaluation Metrics. Because our experiments have hybrid targets, we introduce the evaluation metrics for the classification and regression tasks, respectively. Two widely used metrics, AUC and LogLoss, are adopted for all the classification tasks. Regarding regression tasks, we adopt NRMSE (Normalized Root-Mean-Square Error) and NMAE (Normalized Mean Absolute Error) to evaluate watch time and purchase amount following [33, 36]. Besides these two metrics for regression, Gini and Spearman [25] are also adopted to measure purchase amounts because this prediction aims to rank the users according to purchase amounts. Note that AUC, Gini, and Spearman all measure the rank results; a more significant value indicates a better performance. On the contrary, Logloss, NRMSE, and NMAE suggest a smaller value will lead to a better result. Detailed definition of these metrics are listed in Appendix B.2. Touch the Core: Exploring Task Dependence Among Hybrid Targets for Recommendation RecSys '24, October 14-18, 2024, Bari, Italy Table 2: Results on two public and one product datasets. Here the best and second best results are marked in bold and underline fonts, respectively. Each experiment is repeated 10 times for statistical confidence, and ‚àó indicates a significance level of ùëù ‚â§ 0 . 05 based on a two-sample t-test between our method and the best baseline.",
  "5.2 Overall Performance (RQ1)": "The compared experimental results on two public datasets and one product dataset are separately presented in Table 2 due to the space. Overall, our MTLNet achieves the best metrics in all target prediction tasks for all the datasets, which verifies the effectiveness of HTLNet. We summarize our insights as follows. ESSM. We conjecture that these SDMTL models are mainly designed to explore the dependence among classification targets, neglecting the peculiarity of regression tasks. On the contrary, our HTLNet first carefully designs the network architecture to explore the dependence among hybrid targets and provide a corresponding optimization strategy, further improving performance. Firstly, compared with single-task DNN models, most MTL models perform better on all tasks in the corresponding dataset, demonstrating that MTL can predict all hybrid tasks well. This justifies the reason we introduce MTL to learning hybrid targets. Among all the models, our HTLNet can improve performance significantly due to the carefully designed network architecture and corresponding optimization strategy. Secondly, HTLNet is statistically significant on most tasks compared to the other baselines. This necessitates considering both network architecture design and optimization strategy to improve the performance of hybrid target learning. Specifically, HTLNet performs consistently well on core target tasks and preceding sequential tasks compared with ESMM, AITM, and MetaBalance, which indicates that our HTLNet architecture can explore the dependence between hybrid targets. As to MMoE, AdaTT, and PLE, they achieve better performance on core target tasks compared to others. However, because of the optimization difficulty for hybrid targets, they perform worse on the preceding classification tasks. Thirdly, among MTL models except for HTLNet, MetaBalance performs better in auxiliary tasks, demonstrating that optimization strategy is a critical factor in hybrid target learning. On the other hand, the general MTL model performs slightly better than the SDMTL model on core target regression tasks, i.e., AITM and Moreover, an alternative loss to Kaggle-Revenue and product datasets is lognormal loss [25], which has been widely adopted in Lifetime Value Prediction. For the experimental soundness, we refer results in Appendix C.1",
  "5.3 Ablation Study (RQ2)": "We conduct ablation studies on the KuaiRand dataset to analyze the effectiveness of the main components. As discussed in the previous section, our HTLNet contributes to hybrid target learning in network architecture and optimization strategy. Hence, we introduce two variants of HTLNet: (1) HTLNet w/o Architecture replaces the designed architecture with a sharedbottom network, which means there are no special modules for exploring the task dependencies. (2) HTLNet w/o Optimization removes the gradient process described in Algorithm 1, which only relies on the designed architecture. We present the results in Table 3 with the evaluation metrics on all tasks. It can be concluded that HTLNet outperforms all variants consistently. Specifically, HTLNet w/o Optimization is the worst variant in this setting. This phenomenon further justifies our contribution to propose an optimization strategy based on the designed architecture. This is necessary in hybrid target learning, where the objects vary with each other significantly. Moreover, the RecSys '24, October 14-18, 2024, Bari, Italy Xing Tang, et al. Table 3: Ablation Study of HTLNet on KuaiRand dataset Here ‚àó indicates a significance level of ùëù ‚â§ 0 . 05 based on a two-sample t-test between HTLNet and the best-performed baseline. HTLNet w/o Architecture is slightly better than HTLNet w/o Optimization while still worse than HTLNet. This indicates that our LEU and IFU effectively transfer information from task labels and representations. Another critical component of the HTLNet is the optimization strategy. We analyze optimization strategy using several experiments on the KuaiRand dataset.",
  "5.4 Analysis of Network Architecture (RQ3)": "To better understand the effects of architecture in our HTLNet, we further design experiments to investigate the network architecture on the KuaiRand dataset. Notice that two kinds of information are transferred to explore the dependence among hybrid targets. We thus investigate how these two kinds of information affect the performance. We introduce two variants of the network architecture of HTLNet. (1) HTLNet w/o representation removes the IFU for task representations fusion. (2) HTLNet w/o label embedding removes the LEU for each task, equivalent to removing the label information transferred among tasks. The results are summarized in Table 4. Overall, removing any information from the original HTLNet will degrade the performance. HTLNet w/o representation performs slightly better than HTLNet w/o label embedding , which indicates the information is limited from preceding tasks with only label embedding. Figure 4: Comparison of the different operations in IFU. click AUC long view AUC watch time RNMSE watch time NMAE 0.70 0.75 0.80 0.85 0.90 0.95 0.7429 0.7705 0.8986 0.9133 0.7574 0.7787 0.8937 0.9093 HTLNet_concat HTLNet_attention",
  "5.5 Analysis of Optimization Strategy (RQ4)": "We also investigate the effects of information fusion in HTLNet. As the information fusion operation in IFU is attention, we replace it with a simple concatenation operation. The results are given in Figure 4. The performance of classification tasks suffers a drop with the concatenation operation, as directly fusing information from a preceding task cannot avoid the negative transfer. Over the click AUC, HTLNet also performs better with the attention operation, indicating that transferring inappropriate information will influence preceding tasks with shared embedding. As to the core target, the attention mechanism can adaptively learn what and how much information to transfer from preceding tasks. The first one compares HTLNet with relevant variants. (1) HTLNet w/o stop_gradient , HTLNet w/o gradient conflict and HTLNet w/o gradient magnitude represents the HTLNet without any stop gradient operations in Equation.15, HTLNet without gradient direction conflict in Equation.16 and HTLNet without gradient magnitude in Equation.17 respectively. (2) HTLNet-Gradient Surgery employs the Gradient Surgery [35] method on the HTLNet architecture. (3) HTLNet-MetaBalance employs the MetaBalance method on the HTLNet architecture. The results are illustrated in Table 5. From the results, we conclude that the operation of the stop gradient affects the performance significantly among all the gradient operations. We believe that the regression targets interfering with other tasks directly make the optimization volatile. Gradient Surgery and MetaBalance are the optimization methods for general MTL models. The results show that they perform worse than our optimization strategy, which suggests that our optimization method is suitable and customized for hybrid target learning. (a) HTLNet without Gradient Process 0 50 100 150 training steps 0 10 20 30 40 50 L2 norm click long view watch time (b) HTLNet with Gradient Process 0 50 100 150 training steps 0 10 20 30 40 50 L2 norm click long view watch time Figure 5: The illustrative case of the effect of optimization on the KuaiRand dataset. The legend indicates the different tasks. The Y-axis is the average shared embedding gradient magnitude over all mini-batch iterations in one training step. Touch the Core: Exploring Task Dependence Among Hybrid Targets for Recommendation RecSys '24, October 14-18, 2024, Bari, Italy Table 4: Network Architecture Analysis on KuaiRand dataset Here ‚àó indicates a significance level of ùëù ‚â§ 0 . 05 based on a two-sample t-test between HTLNet and the best-performed baseline. Table 5: Optimization Strategy Analysis on KuaiRand dataset Here ‚àó indicates a significance level of ùëù ‚â§ 0 . 05 based on a two-sample t-test between HTLNet and the best-performed baseline. In addition, we give an illustrative case to analyze the effect of the gradient process on shared embedding. As shown in Figure 5, the magnitudes of gradient w.r.t shared embedding layer of the HTLNet without or with gradient process presents a totally different tendency. The upper one (without the gradient process) shows that the gradient from the watch time target is diverging from the click and long view targets. Meanwhile, the magnitude of watch time gradient becomes increasingly more significant than the other two preceding targets. The one with a gradient process can easily control the magnitude of all three tasks, thus making them converge simultaneously. This observation indicates that the convergence of the gradient poses a severe issue in our proposed model and further explains why our model outperforms baselines. Besides, the investigation of two critical parameters ùõº and ùõæ related to performance is conducted in Appendix C.3.",
  "5.6 Online Experiments (RQ5)": "This section investigates whether HTLNet performs better in the online recommendation scenario. The details of online experiments can be referred to Appendix B.3. Figure 6: Online relative performance gains of three online metrics in 14 consecutive days. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Day -1 0 1 2 3 4 5 6 7 Performance Gain(%) click ratio purchase ratio purchase amt Figure 6 shows the relative promotion of three corresponding objectives in 14 consecutive days. On most days, HTLNet achieves significant performance improvements, especially in terms of purchase amount. The accumulated gains of CTR, CVR, and purchase amount are + 0 . 54%, + 1 . 4%, and + 2 . 69%. These significant improvements in the online scenario prove the effectiveness of HTLNet. However, the purchase amount is considered more fluctuant due to the difficulties in the regression target, which further emphasizes our study.",
  "6 CONCLUSION": "In this paper, a novel model, HTLNet, is devoted to learning hybrid targets for the first time. To effectively explore the dependence among the hybrid targets, HTLNet introduces a label embedding unit to map the label into the dense vector containing the information of preceding labels. Then an information fusion unit incorporates preceding information adaptively, which helps the task prediction. Moreover, an optimization strategy is also proposed to solve the gradient issue raised by our network architecture. Compared to several multi-task approaches, the performance gains of HTLNet on both public and product datasets demonstrate its effectiveness in exploring dependence. Besides, we also conduct an online A/B test to further verify our HTLNet performs well in a large-scale fund recommendation scenario. Limitations. Despite HTLNet demonstrating superior effectiveness over other baselines, it requires more training times than other baselines, discussed in Appendix C.2. Additionally, HTLNet posits the hypothesis that the objective of the final core target is to be continuous while those of all the preceding tasks are to be discrete. While this is the most common scenario in the real world, we are interested in generalizing HTLNet into other complex cases.",
  "ACKNOWLEDGMENTS": "We thank the support of the National Natural Science Foundation of China (No.62302310). RecSys '24, October 14-18, 2024, Bari, Italy Xing Tang, et al.",
  "REFERENCES": "[1] Rich Caruana. 1997. Multitask Learning. Machine Learning 28, 1 (1997), 41-75. [2] Heyan Chai, Zhe Yin, Ye Ding, Li Liu, Binxing Fang, and Qing Liao. 2023. A ModelAgnostic Approach to Mitigate Gradient Interference for Multi-Task Learning. IEEE Trans. Cybern. 53, 12 (2023), 7810-7823. [3] Wenqing Chen, Jidong Tian, Liqiang Xiao, Hao He, and Yaohui Jin. 2020. Exploring Logically Dependent Multi-task Learning with Causal Inference. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . Association for Computational Linguistics, Online, 2213-2225. [4] Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. 2018. GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018 (Proceedings of Machine Learning Research, Vol. 80) , Jennifer G. Dy and Andreas Krause (Eds.). PMLR, Stockholmsm√§ssan, Stockholm, Sweden, 793-802. http://proceedings.mlr.press/v80/chen18a.html [5] Chen Gao, Xiangnan He, Dahua Gan, Xiangning Chen, Fuli Feng, Yong Li, TatSeng Chua, and Depeng Jin. 2019. Neural Multi-task Recommendation from Multi-behavior Data. In 35th IEEE International Conference on Data Engineering, ICDE . IEEE, Macao, China, 1554-1557. [6] Chongming Gao, Shijun Li, Yuan Zhang, Jiawei Chen, Biao Li, Wenqiang Lei, Peng Jiang, and Xiangnan He. 2022. KuaiRand: An Unbiased Sequential Recommendation Dataset with Randomly Exposed Videos. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management (CIKM '22) . ACM, Atlanta, GA, USA, 3953-3957. [7] Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (Proceedings of Machine Learning Research, Vol. 9) . PMLR, Chia Laguna Resort, Sardinia, Italy, 249-256. [8] Yun He, Xue Feng, Cheng Cheng, Geng Ji, Yunsong Guo, and James Caverlee. 2022. MetaBalance: Improving Multi-Task Recommendations via Adapting Gradient Magnitudes of Auxiliary Tasks. In WWW'22: The ACM Web Conference 2022 . ACM, New York, NY, USA, 2205-2215. [9] Eric Jang, Shixiang Gu, and Ben Poole. 2017. Categorical Reparameterization with Gumbel-Softmax. In 5th International Conference on Learning Representations, ICLR 2017 . OpenReview.net, Toulon, France. [10] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In 3rd International Conference on Learning Representations, ICLR 2015 , Yoshua Bengio and Yann LeCun (Eds.). OpenReview, San Diego, CA, USA. [11] Danwei Li, Zhengyu Zhang, Siyang Yuan, Mingze Gao, Weilin Zhang, Chaofei Yang, Xi Liu, and Jiyan Yang. 2023. AdaTT: Adaptive Task-to-Task Fusion Network for Multitask Learning in Recommendations. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD . ACM, New York, NY, USA, 4370-4379. [12] Yu Li, Yi Zhang, Lu Gan, Gengwei Hong, Zimu Zhou, and Qiang Li. 2021. RevMan: Revenue-aware Multi-task Online Insurance Recommendation. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI . AAAI Press, Virtual Event, 303-310. [13] Baijiong Lin, Weisen Jiang, Feiyang Ye, Yu Zhang, Pengguang Chen, Ying-Cong Chen, and Shu Liu. 2023. A Scale-Invariant Task Balancing Approach for MultiTask Learning. CoRR abs/2308.12029 (2023). https://doi.org/10.48550/ARXIV. 2308.12029 [14] Baijiong Lin, Weisen Jiang, Feiyang Ye, Yu Zhang, Pengguang Chen, Ying-Cong Chen, Shu Liu, and James Kwok. 2023. Dual-balancing for multi-task learning. (2023). [15] Xiao Lin, Xiaokai Chen, Linfeng Song, Jingwei Liu, Biao Li, and Peng Jiang. 2023. Tree Based Progressive Regression Model for Watch-Time Prediction in ShortVideo Recommendation. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (Long Beach, CA, USA,). Association for Computing Machinery, New York, NY, USA, 4497-4506. [16] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H. Chi. 2018. Modeling Task Relationships in Multi-task Learning with Multi-gate Mixtureof-Experts. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2018 , Yike Guo and Faisal Farooq (Eds.). ACM, London, UK, 1930-1939. [17] Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu, and Kun Gai. 2018. Entire Space Multi-Task Model: An Effective Approach for Estimating Post-Click Conversion Rate. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, SIGIR . ACM, New York, NY, USA, 1137-1140. [18] Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert. 2016. Cross-Stitch Networks for Multi-task Learning. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016 . IEEE Computer Society, Las Vegas, NV, USA, 3994-4003. https://doi.org/10.1109/CVPR.2016.433 [19] Yi Ouyang, Bin Guo, Xing Tang, Xiuqiang He, Jian Xiong, and Zhiwen Yu. 2021. Mobile App Cross-Domain Recommendation with Multi-Graph Neural Network. ACM Trans. Knowl. Discov. Data 15, 4 (2021), 55:1-55:21. https://doi.org/10.1145/ 3442201 [20] Hongyan Tang, Junning Liu, Ming Zhao, and Xudong Gong. 2020. Progressive Layered Extraction (PLE): A Novel Multi-Task Learning (MTL) Model for Personalized Recommendations. In RecSys 2020: Fourteenth ACM Conference on Recommender Systems , Rodrygo L. T. Santos, Leandro Balby Marinho, Elizabeth M. Daly, Li Chen, Kim Falk, Noam Koenigstein, and Edleno Silva de Moura (Eds.). ACM, Virtual Event, Brazil, 269-278. [21] Jiaxi Tang, Yoel Drori, Daryl Chang, Maheswaran Sathiamoorthy, Justin Gilmer, Li Wei, Xinyang Yi, Lichan Hong, and Ed H. Chi. 2023. Improving Training Stability for Multitask Ranking Models in Recommender Systems. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '23) . Association for Computing Machinery, New York, NY, USA, 4882-4893. https://doi.org/10.1145/3580305.3599846 [22] Xing Tang, Qiguang Miao, Yi-Ning Quan, Jie Tang, and Kai Deng. 2015. Predicting individual retweet behavior by user similarity: A multi-task learning approach. Knowledge Based System 89 (2015), 681-688. [23] Xing Tang, Yang Qiao, Yuwen Fu, Fuyuan Lyu, Dugang Liu, and Xiuqiang He. 2023. OptMSM: Optimizing Multi-Scenario Modeling for Click-Through Rate Prediction. In Machine Learning and Knowledge Discovery in Databases: Applied Data Science and Demo Track . Springer Nature Switzerland, Cham, 567-584. [24] Xuewen Tao, Mingming Ha, Qiongxu Ma, Hongwei Cheng, Wenfang Lin, Xiaobo Guo, Linxun Chen, and Bing Han. 2023. Task Aware Feature Extraction Framework for Sequential Dependence Multi-Task Learning. In Proceedings of the 17th ACM Conference on Recommender Systems, RecSys . ACM, New York, NY, USA, 151-160. [25] Xiaojing Wang, Tianqi Liu, and Jingang Miao. 2019. A deep probabilistic model for customer lifetime value prediction. arXiv preprint arXiv:1912.07753 (2019). [26] Yuhao Wang, Ha Tsz Lam, Yi Wong, Ziru Liu, Xiangyu Zhao, Yichao Wang, Bo Chen, Huifeng Guo, and Ruiming Tang. 2023. Multi-Task Deep Recommender Systems: A Survey. CoRR abs/2302.03525 (2023). arXiv:2302.03525 [27] Hong Wen, Jing Zhang, Yuan Wang, Fuyu Lv, Wentian Bao, Quan Lin, and Keping Yang. 2020. Entire Space Multi-Task Modeling via Post-Click Behavior Decomposition for Conversion Rate Prediction. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR . ACM, New York, NY, USA, 2377-2386. [28] Yunpeng Weng, Xing Tang, Liang Chen, and Xiuqiang He. 2023. Curriculum Modeling the Dependence among Targets with Multi-task Learning for Financial Marketing. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR . ACM, New York, NY, USA, 1914-1918. [29] Liang Wu, Diane Hu, Liangjie Hong, and Huan Liu. 2018. Turning Clicks into Purchases: Revenue Optimization for Product Search in E-Commerce. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval . Association for Computing Machinery, New York, NY, USA, 365-374. [30] Sen Wu, Hongyang R. Zhang, and Christopher R√©. 2020. Understanding and Improving Information Transfer in Multi-Task Learning. In 8th International Conference on Learning Representations, ICLR 2020 . OpenReview.net, Addis Ababa, Ethiopia. [31] Dongbo Xi, Zhen Chen, Peng Yan, Yinger Zhang, Yongchun Zhu, Fuzhen Zhuang, and Yu Chen. 2021. Modeling the Sequential Dependence among Audience Multistep Conversions with Multi-task Learning in Targeted Display Advertising. In KDD '21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . ACM, New York, NY, USA, 3745-3755. [32] Dongbo Xi, Fuzhen Zhuang, Bowen Song, Yongchun Zhu, Shuai Chen, Dan Hong, Tao Chen, Xi Gu, and Qing He. 2020. Neural Hierarchical Factorization Machines for User's Event Sequence Analysis. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, China) (SIGIR '20) . Association for Computing Machinery, New York, NY, USA, 1893-1896. https://doi.org/10.1145/3397271.3401307 [33] Mingzhe Xing, Shuqing Bian, Wayne Xin Zhao, Zhen Xiao, Xinji Luo, Cunxiang Yin, Jing Cai, and Yancheng He. 2021. Learning Reliable User Representations from Volatile and Sparse Data to Accurately Predict Customer Lifetime Value. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining (Virtual Event, Singapore) (KDD '21) . Association for Computing Machinery, New York, NY, USA, 3806-3816. [34] Enneng Yang, Junwei Pan, Ximei Wang, Haibin Yu, Li Shen, Xihua Chen, Lei Xiao, Jie Jiang, and Guibing Guo. 2023. AdaTask: A Task-Aware Adaptive Learning Rate Approach to Multi-Task Learning. In Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2023 . AAAI Press, Washington, DC, USA, 10745-10753. [35] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. 2020. Gradient Surgery for Multi-Task Learning. In Advances in Neural Information Processing Systems , H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., virtual, 5824-5836. [36] Ruohan Zhan, Changhua Pei, Qiang Su, Jianfeng Wen, Xueliang Wang, Guanyu Mu, Dong Zheng, Peng Jiang, and Kun Gai. 2022. Deconfounding Duration Bias in Watch-Time Prediction for Video Recommendation. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (Washington DC, USA) (KDD '22) . Association for Computing Machinery, New York, NY, USA, 4472-4481. https://doi.org/10.1145/3534678.3539092 RecSys '24, October 14-18, 2024, Bari, Italy Touch the Core: Exploring Task Dependence Among Hybrid Targets for Recommendation [37] Dandan Zhang, Haotian Wu, Guanqi Zeng, Yao Yang, Weijiang Qiu, Yujie Chen, and Haoyuan Hu. 2022. CTnoCVR: A Novelty Auxiliary Task Making the LowerCTR-Higher-CVR Upper. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '22) . Association for Computing Machinery, New York, NY, USA, 2272-2276. https: //doi.org/10.1145/3477495.3531843 [38] Mingzhu Zhang, Ruiping Yin, Zhen Yang, Yipeng Wang, and Kan Li. 2023. Advances and Challenges of Multi-task Learning Method in Recommender System: A Survey. CoRR abs/2305.13843 (2023). https://doi.org/10.48550/ARXIV.2305.13843 [39] Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. 2019. Deep Learning Based Recommender System: A Survey and New Perspectives. ACM Computing Survey 52, 1 (2019), 5:1-5:38. [40] F. Zhu, M. Zhong, X. Yang, L. Li, L. Yu, T. Zhang, J. Zhou, C. Chen, F. Wu, G. Liu, and Y. Wang. 2023. DCMT: A Direct Entire-Space Causal Multi-Task Framework for Post-Click Conversion Estimation. In 2023 IEEE 39th International Conference on Data Engineering (ICDE) . IEEE Computer Society, Los Alamitos, CA, USA, 3113-3125. RecSys '24, October 14-18, 2024, Bari, Italy Xing Tang, et al.",
  "A MOTIVATION FOR GUMBEL-SOFTMAX AND ITS ALTERNATIVE": "In this section, we discuss why the Gumbel-softmax re-parameterization trick [9] is introduced to differentiate the label information. To understand the gumbel-softmax, the whole process needs to be examined. A straightforward strategy is to use the true labels of all predecessor tasks for label transfer. However, since the true labels are available during training but not during testing, this leads to the training-test discrepancy problem. To this end, proxy labels from the predicted probabilities of the predecessor tasks for label transfer are selected. Simple sampling operations are detrimental to model training because they are non-differentiable. On the one hand, only the label embedding corresponding to one sampled category is updated. This one-sided update quickly leads to high variance in the gradients between different label embeddings and makes the optimization unstable. On the other hand, label transfer based only on information from one category label may be insufficient, especially when the labels of the underlying task are noisy. To alleviate these problems, we introduce gumbel-softmax to perform a differentiable sampling operation. This means that the model can use the information of all category labels in the early stage to perform inference in label transfer and update the label embeddings of all categories simultaneously. Then, as training progresses, the model gradually and smoothly converges to the final proxy labels and corresponding label embeddings.",
  "B EXPERIMENTAL SETUP": "",
  "B.1 Offline Implementation Details": "In our experiments, all models have three hidden layers, the units of which are [ 128 , 64 , 32 ] . Following all previous work, Adam optimizer [10], and Xavier initialization [7] are all adopted. For each model, we tune the following hyper-parameters to get the best performance: shared embedding dimension, batch size, learning rate, and ùëô 2 regularization. For our HTLNet, we set temperature ùúè = 10 and decay with training in 0 . 5. We further tune latent label embedding dimension, the decay step for temperature ùúè ùëë , ùõº , and ùõæ in gradient process with the hyper-parameter search library Optuna 2 . Table 6 summarizes the candidate search space for hyperparameters. In addition, the transferred task representations are extracted from the first layer representation. Table 6: Hyper-parameters tuned in the experiments. 2 https://optuna.org/ Our implementation is based on TensorFlow. As some baseline models are implemented on Pytorch, we re-implement them according to the official implementation for the AITM 3 , AdaTT 4 and MetaBalance 5 . For other baseline models, we re-implement them based on the authors' details, and all experiments are implemented on NVIDIA V100 and Intel(R) Xeon Platinum 8255C CPU@2.50GHz.",
  "B.2 Evaluation Metric": "Here we add more detailed description of evaluation metrics. For discrete tasks (mostly preceding tasks), such as click and long view in KuaiRand dataset, repurchase 1Y and repurchase 1M in Kaggle-Revenue dataset, click and convert in Product dataset, the commonly-adopted metrics, AUC and logloss, are adopted. Moreover, for continuous tasks (mainly core tasks), such as watch time in KuaiRand dataset, repurchase 1M amount in Kaggle-Revenue dataset, and purchase amount in Product dataset, normalized root mean squared error (NRMSE) and normalized mean absolute error (NMAE) are adopted. Additionally, due to the particularity of the amount, Spearman rank-order correlation coefficient (Spearman) and Gini coefficient (Gini) are adopted for repurchase 1M amount in Kaggle-Revenue dataset and purchase amount in Product dataset. Both Gini and Spearman are usually used to measure the ranking of predictions in industrial applications [25].",
  "B.3 Online Implementation Details": "We first introduce the online scenario and setting and then present the online results. We conducted an A/B test in the online fund recommendation scenario to measure the benefits of HTLNet compared with the online baseline MMoE . Unlike other recommendation scenarios, the fund recommendation scenario focuses on not only the click/conversion behaviors but also the users' purchase amount on the platform. We allocate 10% serving traffic for 14 days. Both models are trained in a single cluster, where each node contains a 96-core Intel(R) Platinum 8255C CPU, 256GB RAM, and 8 NVIDIA TESLA A100 GPU cards. Three online metrics, i.e., click ratio (CTR), purchase ratio (CVR), and purchase amount (core target), are measured in the online performance.",
  "C ADDITIONAL EXPERIMENTS": "",
  "C.1 HTLNet with Lognormal Loss": "To handle this special target, we compare ZILN and HTLNet with ZILN architecture on these two datasets as shown in Table 7. Compared with MSE loss in Table 2, the lognormal loss improves the performance of all the tasks, which indicates that the lognormal loss is a better inductive bias for revenue prediction. Notice that our HTLNet still performs better with Ziln, indicating the superiority and generalization of our HTLNet on hybrid target learning.",
  "C.2 Efficiency Analysis": "We conduct a training efficiency analysis of different models in Figure 7. Here, we train each model over the KuaiRand dataset 3 https://github.com/xidongbo/AITM 4 https://github.com/facebookresearch/AdaTT 5 https://github.com/facebookresearch/MetaBalance Touch the Core: Exploring Task Dependence Among Hybrid Targets for Recommendation RecSys '24, October 14-18, 2024, Bari, Italy 1e-5 1e-4 1e-3 1e-2 1e-1 1.0 Value 0.8900 0.8925 0.8950 0.8975 0.9000 NRMSE alpha gamma (a) NRMSE 1e-5 1e-4 1e-3 1e-2 1e-1 1.0 Value 0.86 0.88 0.90 0.92 0.94 0.96 NMAE alpha gamma (b) NMAE Figure 8: Parameter study of optimization hyperparameters ùõº and ùõæ on NRMSE and NMAE of KuaiRand Dataset. Table 7: Results of HTLNet with ZILN Loss on KaggleRevenue and product dataset for 20 epochs, rerun this process ten times, and report the mean training time of each epoch. We observe that HTLNet requires relatively more training time than other baselines. Figure 7: Efficiency Analysis on KuaiRand dataset. DNN Share Bottom ESMM MMoE PLE AITM AdaTT Meta Balance HTLNet 0 1 2 3 4 5 6 7 8 9 Training Time Per Epoch (s)",
  "C.3 Parameters Sensitivity": "We conduct the parameter sensitivity on the two critical hyperparameters in the gradient process of HTLNet, i.e. ùõº and ùõæ , which affects the optimization strategy. The results on the KuaiRand dataset are shown in Figure 8. In Figure 8, we varies the both ùõº and ùõæ in the range [ 1 ùëí -5 , 1 ] with the step of 10. Overall, increasing both ùõº and ùõæ will improve the performance at the beginning, but too large a value will degrade the performance. However, the effect of ùõæ is less sensitive compared with ùõº . We speculate that the gradients of the core target magnitude differ from preceding targets in a certain value, far way from which will be detrimental to the core target performance. While ùõº controlling the direction will be smoother in the tuning process.",
  "keywords_parsed": [
    "Multi-task Learning",
    " Task Dependence",
    " Hybrid Targets",
    " Recommendation"
  ]
}