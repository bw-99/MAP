{
  "AutoML for Large Capacity Modeling of Meta's Ranking Systems": "Hang Yin ‚àó , Kuang-Hung Liu ‚àó , Mengying Sun, Yuxin Chen, Buyun Zhang Jiang Liu, Vivek Sehgal, Rudresh Rajnikant Panchal, Eugen Hotaj, Xi Liu Daifeng Guo, Jamey Zhang, Zhou Wang, Shali Jiang, Huayu Li, Zhengxing Chen Wen-Yen Chen, Jiyan Yang, Wei Wen ‚Ä† {hyin5,khliu,mengyingsun,yuxinc,buyunz,jiangliu,viveksehgal,rudreshp,ehotaj,xliu1}@meta.com {dfguo,jameyz,zhouwang,shalijiang,huayuli,czxttkl,wychen,chocjy,wewen}@meta.com Meta Platforms Inc., USA",
  "ABSTRACT": "",
  "ACMReference Format:": "Web-scale ranking systems at Meta serving billions of users is complex. Improving ranking models is essential but engineering heavy. Automated Machine Learning (AutoML) can potentially release engineers from labor intensive work of tuning ranking models; however, it is unknown if AutoML is efficient enough to meet tight production timeline in real-world applications and, at the same time, bring additional improvements to the already strong baselines. Moreover, to achieve higher ranking performance, there is an ever-increasing demand to scale up ranking models to even larger capacity, which imposes more challenges on the AutoML efficiency. The large scale of models and tight production schedule requires AutoML to outperform human baselines by only using a small number of model evaluation trials ( ‚àº 100). This paper presents a sampling-based AutoML search method, focusing on neural architecture search and hyperparameter optimization, with a particular emphasis on addressing aforementioned challenges in Meta-scale production when building large capacity models. Our approach efficiently handles large-scale data demands. It leverages a lightweight predictor-based searcher and reinforcement learning to explore vast search spaces, significantly reducing the number of model evaluations. Through experiments in large capacity modeling for CTR and CVR applications, we have demonstrated that our method achieves outstanding Return on Investment (ROI) versus human tuned baselines, with up to 0 . 09% Normalized Entropy (NE) loss reduction or 25% Query per Second (QPS) increase by only sampling one hundred models on average from a curated search space. The proposed AutoML method has already made real-world impact where a discovered Instagram CTR model with up to -0 . 36% NE gain (over existing production baseline) was selected for large-scale online A/B test and show statistically significant gain. These production results proved AutoML efficacy and accelerated its adoption in ranking systems at Meta. *These authors contributed equally. ‚Ä† Project lead. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WWW'24 Companion, May 13-17, 2024, Singapore ACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00 ¬© 2023 Association for Computing Machinery. https://doi.org/10.1145/nnnnnnn.nnnnnnn Hang Yin ‚àó , Kuang-Hung Liu ‚àó , Mengying Sun, Yuxin Chen, Buyun Zhang, Jiang Liu, Vivek Sehgal, Rudresh Rajnikant Panchal, Eugen Hotaj, Xi Liu, Daifeng Guo, Jamey Zhang, Zhou Wang, Shali Jiang, Huayu Li, Zhengxing Chen, and Wen-Yen Chen, Jiyan Yang, Wei Wen ‚Ä† . 2023. AutoML for Large Capacity Modeling of Meta's Ranking Systems. In Proceedings of ACM Web Conference 2024 (WWW '24 Companion). ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/nnnnnnn. nnnnnnn",
  "1 INTRODUCTION": "The field of Automated Machine Learning (AutoML) has emerged as a pivotal research area at Meta, driven by the ever increasing complexity and scale of web-based systems, which now serve billions of users. With advancements in hardware, the demand for evolving these vast models has grown exponentially, posing formidable challenges due to constraints in human resources and computational capacities. The goal of AutoML is to automate the process of applying machine learning (e.g., model architecture selection, hyperparameter optimization, etc.) to discover promising models with better Return on Investment (ROI): complex models with accuracy improvement or computationally efficient architecture. This endeavor has garnered considerable attention from both academia and industry dedicated to the advancement of machine learning and artificial intelligence, like Meta. In this work, we present a samplingbased AutoML search method that focuses on neural architecture search and hyperparameter joint optimization and discuss how our proposed method tackles real-world production challenges: ¬∑ Industry-scale ranking model. Meta's ranking models continue to grow both in size and complexity with advancements in hardware and the need for serving high quality Click Through Rate (CTR) and Conversion Rate (CVR) prediction. This poses significant challenge for AutoML as increases in model size leads to increases in search space, and increase in model complexity leads to costly model evaluation. ¬∑ Tight production schedule. In real-world application, production ranking model iteration follows tight development and deployment schedule to ensure the release of highest-quality model. The strict production schedule leaves AutoML limited window of time to search for strong candidates. ¬∑ Compatibility with human baselines and evolving stack. Meta's ranking models are developed by many in-house engineers and undergoes continuous evolution. While it's widely recognized that scaling-up large capacity models is a labor intensive effort and a common pain point for engineers, the strong and dynamic baselines established by expert human sets a high bar WWW'24 Companion, May 13-17, 2024, Singapore Yin and Liu, et al. for AutoML. Compatibility of search space to human ideas and compatibility of discovered model to serving stack are essential for AutoML to deliver meaningful end to end impact. ¬∑ Limited computation resource. The challenge of AutoML lies in the time it takes to discover and validate a new model; during this process, it relies on heavy computation to enable automation. However, the increase in computing power required to run AutoML can incur additional (sometime significant) cost. AutoML needs to demonstrate high ROI (not only effective but also efficient) to justify its deployment in production. In our pursuit of industry-level AutoML at Meta scale, we improved a predictor-based searcher [27]. It serves as an efficient sampling method by mapping model configurations to performance proxies, obviating the need for real-data model evaluations. This State-ofthe-Art (SOTA) method is highly flexible and fully parallelizable. Its compatibility to human baselines and serving stack is also high. To deal with the significant challenges we mentioned above, we advance this method by: ¬∑ Low-fidelity Evaluation with High Ranking Quality. Samplingbased searchers often struggle to gather enough samples to ensure the generalization capacity of the searcher. To overcome this, we employ low-fidelity evaluation to expedite data collection by training a small portion of data. We first evaluate a subset of randomly selected models and ensure their training fully converged (typically require large amount of data). By conducting ranking correlation analysis, we determine the potential for reducing data while maintaining similar ranking quality. This enables us to evaluate subsequent models with reduced data, saving valuable computation resources. Moreover, the dataset of random models we trained can be reused to warm start our searcher. This trait is given by the searcher's ability to reuse historical data. ¬∑ Predictor Optimization with Pairwise Ranking Loss. Even with low-fidelity evaluation, evaluating large number of models is expensive and in practice we only have access to a very small number of examples (denoted by ùëÅ ) to train a predictor. To alleviate label scarcity, we use pairwise ranking loss 1 to train the predictor. This increases the number of labels to O( ùëÅ 2 ) , and is proven more accurate than mean squared error loss. ¬∑ Training Curve Fitting. Our use of low-fidelity evaluation favor models with short-term performance and can sometimes miss models with long term gain but only mediocre short-term performance. To address this challenge, we propose a new proxy that considers both short-term loss and training curve trends. The proposed proxy helps us identify models with both strong short-term performance and favorable long-term trends. ¬∑ Ensemble Modeling and Multi-task Learning. Samplingbased searchers often suffer from poor generalization under limited sampling points. Further compounding this challenge is the noisy and non-stationary production data. To address these challenges, we employ ensemble modeling and multi-task learning for the predictor to improve its generalization capabilities. ¬∑ Predictor and RL Searcher Integration. To find the bestperforming model over a huge search space with an efficient sampler, we adopt the REINFORCE algorithm [28], where we 1 https://pytorch.org/docs/stable/generated/torch.nn.MarginRankingLoss.html learn a stochastic sampling policy to minimize a predefined reward, which combines performance metrics like Normalized Entropy (NE), FLOPs and Queries Per Second (QPS). The efficiency of the RL sampler allows us to greatly reduce the number of models that need to be evaluated compared to a random sampler, while also identifying promising models that satisfy different production requirements.",
  "With the proposed predictor-based RL method, we find that": "¬∑ It can quickly facilitate large capacity modeling with better ROI than manual tuning. Compared to strong human baselines, we demonstrate the proposed method can bring up to 0 . 09% Normalized Entropy (NE) 2 loss reduction or 25% Query per Second (QPS) increase by only sampling one hundred models on average from a curated search space. ¬∑ It outperforms other sampling-based algorithms such as random search and Bayesian optimization. ¬∑ It is able to discover promising model architectures at different levels of training/serving infrastructure capacity effectively.",
  "2 RELATED WORK": "Various deep models have been deployed for ads ranking service to generate high quality CTR and CVR predictions [2, 7, 11, 33]. A main component of ranking model is the feature interaction module which are used to model the relationship among the feature embeddings. Multiple studies have shown a better designed feature interaction module can significantly improve model performance [2, 8, 13-15, 23]. More recently, motivated by the architecture advancement in computer vision and NLP, SOTA ranking models also adopt a deep stacking structure that is composed of repeating blocks of the same interaction module [18, 32]. As the model grow more complex, number of architecture decisions increases and training stability become more sensitive to training hyperparameters and thus require careful fine tuning. Our goal is to apply AutoML to automatically optimize for these modeling decisions. AutoML aims to automate various stages of the machine learning process: data preparation, feature engineering, model generation, and model evaluation [10, 22, 30, 34]. In this work, we focus on the automation of the model generation and model evaluation stages. Hyperparameter optimization (HO) and neural architecture search (NAS) are common optimization methods used in AutoML. Problem wise, HO typically targets tuning training related parameters (e.g., the learning rate, batch size, etc) [1, 3, 6] while NAS focuses on model-related parameters (e.g., the number of layer of NN, size of MLP, etc.) [5, 21, 29]. In this work, we aim to jointly tune both training-related and model-related parameters. Technique wise, HO and NAS share many underlying search method such as Bayesian optimization (BO) [12, 25, 31], evolution-based algorithm (EA) [4, 16, 20, 24], reinforcement learning (RL) [19, 35, 36], etc. Many of existing work were evaluated with small curated public datasets whereas our proposed method were designed for challenging realworld production environment. 2 Normalized Entropy (NE) [9] is the optimization objective used at Meta for ranking systems. AutoML for Large Capacity Modeling of Meta's Ranking Systems WWW'24 Companion, May 13-17, 2024, Singapore",
  "3 METHOD": "At a high level, our proposed AutoML method unfolds in two main parts: train a predictor and sample with RL. The complete workflow is outlined below (also illustrated in Figure 1): ¬∑ Step 1: train a machine learning model (dubbed as 'predictor') which can predict 'NE gain' and FLOPs for a given model architecture ('arch'). 'NE gain' is defined as Normalized Entropy [9] loss improvement versus a baseline and a negative value implies improvement since a smaller NE loss is better. The training dataset include triplets of model arch encoding, NE gain and FLOPs, which are ground-truth labels obtained from historical data or by running model evaluation; ¬∑ Step 3: RL sampled models are fed into the predictor to get predicted NE gain ('pNE') and predicted FLOPs ('pFLOPs'); ¬∑ Step 2: a RL agent samples a set of model 'arch' ; ¬∑ Step 4: 'pNE' and 'pFLOPs' are sent back to the RL agent as reward, such that it can learn to sample better models. In the following, we delve into each step in more details.",
  "3.1 Training the predictor": "The basic idea of the predictor searcher is to build an efficient surrogate model (e.g., MLP), called a \"predictor\", to map any model configuration to the metrics we are interested in. We then perform model selection based on the predicted metrics and finally validate the top candidates (top-k models) with actual online training. The predictor-based searcher is easily compatible with existing production ranking system stack. Formally, at Meta, a model ùúã is configured as JSON file, and each model hyperparameter can be converted to one-hot or float encoding. One-hot encoding is used to encode a parameter ParamA with ùëÅ distinct values as:  where the ùëñ -th position is 1 and all others are 0 if the ùëñ -th value is selected. For float encoding, each parameter can be mapped to a unique number: ParamA ‚Üí R . The final representation of ùúã is the concatenation of all encoding vectors. After defining the search space and converting model configuration, our next task is to collect samples to train the predictor architecture. Real-world production iteration constantly faces strict time constraints and computational resource limitations. Here, we discuss several technical challenges for building an accurate predictor and how we propose to overcome them. Low-fidelity Evaluation with High Ranking Quality. Evaluating a large capacity model end-to-end is expensive and therefore it is too costly to generate large amount samples to train a predictor with good generalization performance. As a result, the accuracy of the predicted metrics derived from the predictor (on unseen data) may suffer from limited training samples. To address this, we use low-fidelity evaluation (i.e. early stop training) to save compute cost for each model evaluation such that we can collect more data points under the same computing budget. More specifically, we first evaluate a small set of pilot models (randomly selected), ensuring they are trained on ample data such that their learning curves converge. We then conduct a ranking correlation analysis to determine how much training data can be reduced (i.e., how early training can be stopped) while still preserving these pilot models' relative ranking order based on each model's reduced learning curve. We then apply the same level of training data reduction to all subsequent model evaluation. We call this process the low-fidelity evaluation . Based on our experience, we find that we can consistently achieve 50% training data reduction from low-fidelity evaluation, i.e., a 50% compute resource saver for each model evaluation Training Curve Fitting . At Meta, building ranking models with long term gain is very critical. While the previously discussed lowfidelity evaluation can bring significant compute saving, selecting model based their reduced learning curve may favor short-term NE gain over long term gain. An example is shown in Fig. 2 where if we naively use raw low-fidelity NE (e.g., short term NE gain at 15B) as the evaluation metric then the blue model is preferred; however, the red model is clearly a better choice long term since it has favorable curve trends (even though it has worse short term NE gain). To remedy this, we propose a new metric that consider both short-term NE and curve trends. More specifically, we select several points near the end of the low-fidelity evaluation to fit a linear regression model. We then predict the value at a future location based on the linear regression model and use this extrapolated value as the label to train the predictor. This approach increases the likelihood of identifying models with both strong short-term performance and favorable long-term trends. Formally, given a short-term NE learning curve, we can represent the predicted long-term NE using the following equations:   Where ùëÅùê∏ ( ùëô ) is the fitted NE gain for a given location ùëô based on the linear regression, ùëö is the slope of the linear regression, ùëÅùê∏ short-term is the observed NE gain at the curve end, Œî ùë• is the step size representing the targeted future location for extrapolation. Training model with Pairwise Ranking Loss . When we use the predictor to estimate the training NE gain of an architecture, to find true top models, our primary focus is its ranking order rather than its proximity to the exact true training NE gain. Therefore, we have found that using a pairwise ranking loss is more effective in identifying top models compared to losses such as MSE. Typically, the pairwise ranking loss is represented as follows:  where ùêø ( ùúÉ ) is the loss function, ( ùëñ, ùëó ) are pairs of samples in the dataset ùê∑ , ùë¶ ùëñ ùëó is the label (indicating which sample in the pair is 'better'), ùëì ( ùë• ; ùúÉ ) is the predictor model, ùë• ùëñ and ùë• ùëó are the features of samples ùëñ and ùëó , respectively, and ùúñ is a margin. This formulation assumes that ùë¶ ùëñ ùëó takes a value of 1 if sample ùëñ has higher NE gain than sample ùëó and -1 otherwise. The loss is zero if the predicted margin is larger than ùúñ , and it increases linearly otherwise. Ensemble Modeling and Multi-task Learning . We employ ensemble modeling and multi-task learning to improve predictor's generalization capability. In this work we propose to build ùëò independently trained predictors where each is a MLP based model WWW'24 Companion, May 13-17, 2024, Singapore Yin and Liu, et al. rank sample arch 1, (NE pNE, search FLOPs 1) pFLOPs (pNE 1, PFLOPs 1), arch space (all predicted NE predicted FLOPs models) arch 2, (NE 2, FLOPs 2) train (pNE 2, pFLOPs 2), arch 2 RL Sampler MutiTaskMLP (pNE M, pFLOPs M), arch M FLOPs N) predict Reward Figure 1: Training and applying the Predictor-based RL searcher. NE and FLOPs are data labels we used to train a predictor. In practice, we apply Eq. (2) instead of NE for model accuracy. For sampling architectures, we combine RL with ROI constraint by following Eq. (6). Training examples NE gain %",
  "3.2 Predictor-based Sampling with Reinforcement Learning": "Figure 2: Top models found through different NE metrics (the blue one: raw NE gain evaluated on low-fidelity data; the red one: predicted long-term NE gain calculated by Eq. 2. that takes a model configuration as input and predicts the corresponding NE gain and FLOPs. Formally, given input encoding ùë• , we have a shared neural architecture ùëÅùëÅ ( ùë• ) with ùêø layers producing a representation vector ‚Ñé ùêø = ùëÅùëÅ ( ùë• ) . For tasks such as prediction of NE gain and FLOPs, the output from the shared architecture is used as input of multiple heads as:  Finally, the ensemble outputs for the tasks are:  Previously discussed predictor allows us to easily estimate the performance of any given model without actual training; however, an efficient search method (sampler) is still needed to find an optimum model. In this work, we adopted a reinforcement learning (RL) method-REINFORCE [28]-to learn a stochastic sampling policy (e.g., learning a configuration sampling distribution) so that the sampled model configuration minimizes some predefined reward. To promote the discovering of models with good performance and cost trade-off, we define the RL reward as a combination of NE gain and cost, e.g., FLOPs. Formally, ROI-constraint search solves the following optimization problem:  where ùëÅùê∏ ensemble and ùêπùêøùëÇùëÉùë† ensemble are generated by a trained predictor, ùúã and Œ† represent the model configuration and the corresponding search space, respectively. The coefficient ùõº is a weighting parameter used for NE and cost tradeoffs. As well be shown later in our experiment results, we consistently find RL sampler to be efficient (converge within 2000 steps and only takes several minutes on a single GPU machine) and effective (converged to a model with predicted NE gain that is better than all of the training data) in finding models with strong predicted performance. The highly efficient RL sampler allows us to iterate multiple round of RL search each with a different random seed that can lead to the discovery of a diverse set of local optimal models. It also allows us to explore different part of NE-cost Pareto front by setting a different NE-cost tradeoffs through weighting coefficient ùõº . Finally, we select the top-k models (from each round of RL search) for final assessment (long term training) on real data. AutoML for Large Capacity Modeling of Meta's Ranking Systems WWW'24 Companion, May 13-17, 2024, Singapore Table 1: AutoML search results across different model types",
  "4 EXPERIMENTS": "(1) The number of DHEN layers. In this section, we conduct experiments to evaluate the effectiveness of AutoML searching for large capacity models for CTR and CVR prediction tasks. Our experiments have the following objectives: (1) Assess the end-to-end model accuracy and efficiency improvement of DHEN [32] architectures discovered by AutoML compared to manual designs. (2) Enablement of each interaction module: a binary decision for each interaction module that applies to all layers, e.g., whether to use/not use a particular interaction module for all DHEN layer. (2) Compare the search efficiency between the Predictor-based RL searcher, random searcher and Bayesian optimization to demonstrate the effectiveness of our searching algorithm in web-scale applications.",
  "4.1 Experiment Setup": "",
  "4.1.1 Search Space Definition.": "In this work, we focus on searching over a Deep Hierarchical Ensemble Network (DHEN) based ranking model. Most state-ofthe-art high-performance model architectures use a deep stacking structure. Here, the deep stacking structures are usually composed of a repeating block containing the same interaction. DHEN also follows this stacking strategy but at the same time builds a novel hierarchical ensemble framework to capture the correlations of multiple interaction modules. It uses the same feature processing layer found in DLRM [18] as feature processing layer. Meanwhile, it proposes a novel hierarchical ensemble framework that includes multiple types of interaction modules and their correlations. Conceptually, a deep hierarchical ensemble network can be described as a deep, fully connected interaction module network, which is analogous to a deep neural network with fully connected neurons. The main goal of hierarchical ensemble is to capture the correlation of the interaction modules. In this work, we largely follow the original DHEN work and consider five types of interaction modules: AdvancedDLRM [17], self-attention, Linear, Deep Cross Net [26], and Convolution. However, due to the potential overlapping with these modules, we also consider the options of removing them in our search space. In summary, our AutoML search space is composed of the following core components: (4) Hyperparameter selection: learning rate, weight initialization scales for the weight matrix and layer normalization. (3) Model architecture selection: architecture selection for all interaction modules, e.g., kernel size for convolution, or size of weight matrix for linear module, etc. Again, the architecture selections apply to all DHEN layers.",
  "4.1.2 Searcher hyper-parameter settings.": "Regarding the hyperparameter settings of the searcher, for the predictor part, we use a four-layer MultiTaskMLP with an architecture [input dim, 50, 50, 2] and 0.5 dropout rate for each layer. The first two layers are a shared architecture, while the last two layers use separate networks for different tasks. Throughout the entire experiment, we use NE gain and FLOPs as the two tasks for metrics, so the output dimension is 2. For the loss function, we use pairwise ranking loss, setting the margin to 0.001. For ensemble modeling, we use 10 individual models to predict together. We use the AdamW algorithm for the optimizer, with a learning rate of 0.001 and a weight decay of 0.005. The learning rate for the RL optimizer is set to 0.01, with an anneal rate of 0.9997, and both the start and min temperature are set at 1.0. Additionally, each time we sample, we use 3 different random seeds to generate top models. More importantly, all those hyperparameters can be tuned by crossvalidation using dataset collected. As the predictor is lightweight and the RL search is efficient, the cost of hyperparameter tuning is negligible compared with training a ranking model.",
  "4.1.3 Training hardware.": "To enable efficient search, we take advantage of the ZionEX fully synchronous training system detailed in [17]. At a high level, the ZionEX system groups 16 hosts into a \"supernode\", called a pod, which contains 128 A100 GPUs (8 per host) with a total HBM WWW'24 Companion, May 13-17, 2024, Singapore Yin and Liu, et al. .136 -0.135 ain data) (train d ta) true NE gain % (test d 0.04 0.00 -0.04 -0.08 NE gain % at 25B 0.04 0.02 0.00 0.02 -0.136 -0.135 -0.134 -0.133 predicted NE on test data(N=30) predicted NE gain (test data) predicted NE gain % (test data) Kendall: 0.77 Pearson: 0.95 Spearman: 0.89 70.02 0,00 NE gain % at 11B 0.06 -0.142 -0.141 -0.140 -0.139 -0.138 predicted NE on training data(N=160) predicted NE gain (train data) predicted NE gain % (train data) Kendall: 0.87 Pearson: 0.95 Spearman: 0.95 0.00 0.02 0.06 NE gain % at 11B w/o curve trends model -0.15 model 2 model 3 -0.25 2 -0.3 -0.35 2B 4B 6B 8B 12B Training examples w/ curve trends (a) Ranking correlation w/ and w/o curve trends (b) NE learning curves of discovered models using predicted longterm NE",
  "Figure 3: Discover model architectures based on both NE and curve trends in CVR scale-up": "capacity of 5TB and 40PF/s BF16 compute capability. Within a host, each GPU is connected through NVLink, and each host in a pod is then connected with a high bandwidth network of up to 200GB/s, shared with 8 GPUs. With ZionEX, we enable efficient and scalable training of DHEN in our cluster [32].",
  "4.2 Experiment Results": "",
  "4.2.1 Ranking correlation study.": "To improve searching efficiency, we applied low-fidelity evaluation in both CTR and CVR scale-up productions to reduce the compute cost (GPU time) required to evaluate each model architectures during the search process. The left-hand-side plot of Figure 3a shows the results of ranking correlation between low-fidelity evaluation and long-term training evaluation. For example, we typically train a CVR model with 25B ('B' is for 'billion') data and expect the NE learning curve to sufficiently converge to a stable value; however, we found through ranking correlation analysis that the NE ranking at 11B data is highly correlated with the results at 25B data (with Kendall Tau 0 . 77). Based on this observation, we propose to only use 11B data to evaluate models which can help us reduce compute resources by more than 50% (of the original cost). As discussed in Section 3.1, incorporating curve trends to predict long-term NE can further improve ranking correlation. This is demonstrated by the right-hand-side plot in Figure 3a where we see Kendall Tau improve from 0 . 77 to 0 . 87. Putting it all together, 0.02 NE gain % at 25B true NE on training data 0.10 0.05 0.00 true NE gain (train dat true NE gain % (train d 0,04 0.02 0,00 true NE on test data true NE gain (test dat true NE gain % (test d 0.04 0.00 -0.04 -0.08 -0.138 -0.137 -0.136 -0.135 -0.134 -0.133 predicted NE on test data(N=30) predicted NE gain (test data) predicted NE gain % Kendall: 0.69 Kendall: 0.55 Pearson: 0.60 Figure 4: Predictor rank correlation between pairwise ranking loss and MSE true NE gain % (test data) predicted NE gain % (train data) predicted NE gain % (test data) predicted NE gain (train data) predicted NE gain (test data) Kendall: 0.45 Pearson: 0.67 Spearman: 0.68 Kendall: 0.58 Pearson: 0.78 Spearman: 0.79 Kendall: 0.55 Pearson: 0.60 Spearman: 0.75 Kendall: 0.69 Pearson: 0.86 Spearman: 0.88 true NE gain % (train data) true NE gain % (train data) true NE gain % (test data) predicted NE gain % (test data) predicted NE gain % (train d ta) (a) w/ pairwise ranking loss Eq. (3) true NE gain % ( est data) predicted NE gain % (train data) predicted NE gain % (test data) predicted NE gain (train data) predicted NE gain (test data) Kendall: 0.45 Pearson: 0.67 Sp arman: 0.68 Kendall: 0.58 Pearson: 0.78 Sp arman: 0.79 Sp arman: 0.75 Pearson: 0.86 Sp arman: 0.88 true NE gain % (tr in data) true NE gain % (tr in data) true NE gain % ( est data) predicted NE gain % (test data) predicted NE gain % (train d ta) (b) w/ MSE 0.10 0.10 0.05 0.05 0.00 0.00 70-10 0.0175 -0.0150 20.026 0.10 0.10 0.05 0.05 0.00 0.00 0.05 0.00 0.00 we applied these proposed techniques to discover three top CVR models in Table 1 and show their learning curves in Figure 3b.",
  "4.2.2 Ablation study of pairwise ranking loss.": "We also compared the impact of different types of loss functions on the performance of the predictor. Specifically, we used pairwise ranking loss as Eq. (3) and MSE. Through our experiments, we found that although MSE can make the predictor's estimation of NE and FLOPs closer to their true values, when considering the rank correlation of sampled models on real data, pairwise ranking loss obviously performs better than the former (Figure 4). This also allows us to more efficiently find the true top models.",
  "4.2.3 Ablation study of ensemble modeling.": "Due to the tight production schedule and limited computational resources, a predictor realistically only have access to a very limited amount of training data. In real-world cases, we found ensemble modeling can significantly improve a predictor's generalization capacity when training data is very limited. An example is illustrated in Figure 5 where we collected 190 historical model evaluations and divide them into a training set and a validation set with 160:30 split. When using a single model to build the predictor, the ranking correlation between the predictor's predictions on the validation set and the ground truth is much lower than the results on the training set, indicating poor generalization performance. In contrast, when we use 10 models to build the predictor through ensemble modeling and make prediction using Eq. (5), the ranking correlation between the predicted values on the validation set and the ground truth improve significantly. Since the MLP model we used to build predictor are very small model, the additional computational cost introduced by ensemble modeling can be considered negligible. 4.2.4 Inference FLOPs and QPS correlation study. AutoML for Large Capacity Modeling of Meta's Ranking Systems WWW'24 Companion, May 13-17, 2024, Singapore Minimum Predicted NE gain % NE gain % at 25B NE gain % -0.139 -0.138 -0.137 -0.136 -0.135 -0.136 -0.135 -0.134 -0.133 0.00 0.05 0.10 0.08 0.04 0.00 -0.04 -0.08 NE gain % at 25B predicted NE on test data(N=30) predicted NE gain (train data) predicted NE gain (test data) predicted NE gain % (train data) predicted NE gain % (test data) Kendall: 0.77 Pearson: 0.95 Spearman: 0.89 Kendall: 0.94 Pearson: 0.73 Spearman: 0.99 true NE gain % (train data) Kendall: 0.38 Pearson: 0.47 Spearman: 0.56 true NE gain % (test data) (a) w/o ensemble modeling -0.142 -0.141 -0.140 -0.139 -0.138 -0.138 -0.137 -0.136 -0.135 -0.134 -0.133 0.10 0.05 0.00 0.08 0.04 0.00 -0.04 -0.08 predicted NE on training data(N=160) predicted NE on test data(N=30) true NE on test data true NE on training data true NE gain (train data) true NE gain (test data) predicted NE gain (train data) predicted NE gain (test data) true NE gain % (train data) predicted NE gain % (train data) true NE gain % (test data) predicted NE gain % Kendall: 0.97 Pearson: 0.67 Spearman: 1.00 Kendall: 0.54 Pearson: 0.70 Spearman: 0.74 Num of Jobs Over Time Minimum Truth WindowNE di ff @11B RL sampler Random sampler the best model in training data -0.02 -0.04 -0.06 -0.08 0 20 40 60 80 100 120 140 samples number Bayesian Optimizer Predictor/RL WindowNE gain (b) w/ ensemble modeling NE gain % at 11B w/o curve trends Kendall: 0.87 Pearson: 0.95 Figure 5: Predictor accuracy performance w/ and w/o ensemble modeling Spearman: 0.95 Figure 6: Correlation between inference FLOPs and train QPS w/ curve trends NE gain % at 11B x train QPS x FLOPs 20 22.8 25.7 28.6 31.4 70 72.5 75 77.5 80 82.5 85 87.5 Pearson:-0.97 Spearman: -0.97 Sample Num predicted flops In terms of hardware infra cost, QPS is the gold metric used at Meta to measure hardware cost instead of FLOPs. However, our training clusters have different GPU types which produce various QPS values for the same model, making the cost incomparable. Moreover, to utilize all the available computing resource, we should not restrict AutoML model evaluation to run on the same specific training hardware type. This makes using QPS as cost metric challenging in real-world AutoML search. To overcome this, we performed study and show that a model's inference FLOPs (in our search space) is highly correlated with train QPS as shown in Figure 6 given the same hardware. Based on this observation, we use FLOPs as the cost metric in this work without sacrificing the ability to search models with high QPS. 0.00 Random -0.08 Minimu 0 Num of Jobs Over Time 20 40 60 80 100 120 140 samples number Spearman: 0.74 -0.134 -0.133 predicted NE on test data(N=30) predicted NE gain (test data) -0.142 -0.141 -0.140 -0.139 -0.138 -0.138 -0.137 -0.136 -0.135 0.10 0.05 0.00 0.08 0.04 0.00 -0.04 -0.08 predicted NE on training data(N=160) true NE on test data true NE on training data true NE gain (train data) true NE gain (test data) predicted NE gain (train data) true NE gain % (train data) predicted NE gain % (train data) true NE gain % (test data) predicted NE gain % Kendall: 0.97 Pearson: 0.67 Spearman: 1.00 Kendall: 0.54 Pearson: 0.70 Minimum Predicted NE gain % Sample Num RL sampler Random sampler the best model in training data Num of Jobs Over Time Minimum Truth WindowNE di ff @30B 0.05 0.00 0.05 0.10 0.15 250 500 750 1750 2000 Kendall: 0.87 Pearson: 0.95 Spearman: 0.95 Figure 7: RL versus random sampling efficiency w/ curve trends NE gain % at 11B NE gain % at 25B predicted flops predicted NE di ff NE gain % prediction flops weight (a) =0 flops weight 0.1 flops weight 0.3 0.15 38.0 38.25 39.0 39.25 39.5 Figure 8: the predicted training NE gain and inference FLOPs of the models identified by RL sampler We demonstrate the sampling efficiency of RL sampler and compare it to a random sampler. In Figure 7, we can see that the predicted NE of the RL sampled model converge within 2000 steps and only takes several minutes on a single GPU machine. The final converged NE is lower (better) than both the best model in training data (indicated by the red curve in Figure 7) and the predicted NE of a random sampler under the same sampling steps which further validates the effectiveness of the RL sampler. The highly efficient and effective RL sampler allows us to iterate multiple rounds of RL search, each with a different random seed that leads to the discovery of a different local optimal models. This can help with global exploration as well as finding a diverse set of strong candidates. Next, to find more promising models with different cost levels, we consider sampling models with cost constraints. Through a study on the correlation among training QPS, inference QPS, and inference FLOPs, we found that these three metrics are highly correlated in CTR and CVR models. Therefore, in AutoML searching, we built a multi-task MLP model to predict training NE and inference FLOPs, and used the weighted sum of the output as the reward in the RL sampler. This helped the predictor effectively search for candidate models at different infra cost levels through adjusting the weighting coefficient ùõº . Consequently, in the iteration of the predictor-based RL searcher, we can obtain promising candidates 4.2.5 Ablation study of RL efficient search. WWW'24 Companion, May 13-17, 2024, Singapore Yin and Liu, et al. Table 2: Different searcher compared to manual design in CVR model with different FLOPs levels. As illustrated in Figure 8, we can explore model architectures of various sizes by adjusting the weight associated with the cost constraint in the RL's reward, referred to as the \"flops weight\". When setting flops weight = 0, it signifies our sole emphasis on NE gain brought about by the model architectures. In this scenario, the RL sampler will recommend many models that prioritize NE gain even if it means sacrificing model efficiency. Conversely, when setting flops weight to a higher value of 0.3, we take into account the impact on model efficiency while striving to enhance NE. Consequently, the recommended models may exhibit relatively lower NE gains, but their model efficiency is significantly improved. Through the utilization of the RL sampler in conjunction with cost constraints for sampling, we have observed that it facilitates the rapid identification of models tailored to different production requirements (as shown in Table 1).",
  "4.2.6 End to end search results.": "To search for a better ROI model for CTR and CVR prediction, we set up a DHEN search space with 18 decisions based on the discussion in Section 4.1.1. Table 1 shows the NE gain and inference QPS improvement of the AutoML discovered DHEN models and a comparison to the manually designed DHEN model and production baseline for CTR and CVR applications. We validated that the predictor-based RL searcher, whether with moderate cost increase or large capacity modeling, can identify promising architectures at different infrastructure cost levels. For example, in Instagram CTR application, our method can improve NE gain from -0.07% (based on manual tuning) to -0.13% while maintaining on-par model efficiency (measured by inference QPS); additionally, we can discover a model that has on-par NE performance but improves inference QPS by 18.96%. In the scale-up production for both Instagram CTR and Ad CVR applications, our method can further improve NE gain to -0.09% without significant model efficiency regression (compared to manual scale-up models). Due to its high ROI, the AutoML generated Instagram CTR scalu-up model with up to -0.36% NE gain was selected for large-scale online A/B test and show statistically significant gain after 12 days' reading. All the search results presented in this work are obtained by only sampling on average one hundred models ('# samples' in Table 1), further proving the efficiency of our AutoML algorithms. 4.2.7 Predictor-based RL searcher versus Bayesian optimizer versuse Random searcher. Lastly, we conduct a searcher comparison between the proposed predictor-based RL searcher, Bayesian optimizer, and random searcher in a CVR model scale-up problem setting. We compare their search performance over the same compute budget of 150 trials. However, to have a fair comparison we reuse the first 75 trails of Figure 9: Sample efficiency comparison among different searcher in CVR model 0.00 -0.02 -0.04 -0.06 -0.08 0 20 40 60 80 100 120 140 samples number NE gain % Random Bayesian Optimizer Predictor/RL random searcher to train predictor-based RL searcher and Bayesian optimizer and then have them generate their remaining 75 candidates. Both predictor-based RL searcher and Bayesian optimizer can be executed in multiple iterations but due to limited compute and time resources in real-world production setting, we typically only perform one iteration of search for both predictor-based RL and BO searchers. We summarize the comparison results in Table 2 where we can see the proposed predictor-based RL searcher greatly improves the model NE performance compared to other searchers. Figure 9 shows the search performance for each searcher as a function of submitted number of trials.",
  "5 CONCLUSION": "In this work, we focused on advancing Automated Machine Learning (AutoML) for efficient productionization of Meta-scale ranking system with ever increasing complexity. We introduced an efficient sampling-based AutoML method that is based on a predictor-based searcher. Through innovative techniques such as low-fidelity evaluation, novel metrics for identifying promising models, enhanced generalization with ensemble modeling, and reinforcement learning sampling integration, our method has demonstrated remarkable results. It facilitates large-capacity modeling with superior Return on Investment (ROI) and effectively discovers promising model architectures across different infrastructure cost levels. This work demonstrates the efficiency and adaptability of AutoML in addressing real-world production challenges, ultimately enhancing the impact of machine learning in web-scale applications.",
  "REFERENCES": "[1] James Bergstra and Yoshua Bengio. 2012. Random Search for Hyper-Parameter Optimization. Journal of Machine Learning Research 13, 10 (2012), 281-305. http://jmlr.org/papers/v13/bergstra12a.html [2] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and Hemal Shah. 2016. Wide & Deep Learning for Recommender Systems. In Proceedings of the 1st Workshop on Deep Learning for Recommender Systems (Boston, MA, USA) (DLRS 2016) . Association for Computing Machinery, New York, NY, USA, 7-10. https://doi.org/10.1145/2988450.2988454 AutoML for Large Capacity Modeling of Meta's Ranking Systems WWW'24 Companion, May 13-17, 2024, Singapore [3] Marc Claesen and Bart De Moor. 2015. Hyperparameter Search in Machine Learning. CoRR abs/1502.02127 (2015). arXiv:1502.02127 http://arxiv.org/abs/ 1502.02127 [5] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. 2019. Neural Architecture Search: A Survey. arXiv:1808.05377 [stat.ML] [4] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. 2019. Efficient Multi-objective Neural Architecture Search via Lamarckian Evolution. arXiv:1804.09081 [stat.ML] [6] Matthias Feurer and Frank Hutter. 2019. Hyperparameter Optimization . Springer International Publishing, Cham, 3-33. https://doi.org/10.1007/978-3-030-05318-5_1 [8] Xiangnan He and Tat-Seng Chua. 2017. Neural Factorization Machines for Sparse Predictive Analytics. CoRR abs/1708.05027 (2017). arXiv:1708.05027 http://arxiv.org/abs/1708.05027 [7] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: A Factorization-Machine based Neural Network for CTR Prediction. CoRR abs/1703.04247 (2017). arXiv:1703.04247 http://arxiv.org/abs/1703.04247 [9] Xinran He, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Yanxin Shi, Antoine Atallah, Ralf Herbrich, Stuart Bowers, et al. 2014. Practical lessons from predicting clicks on ads at facebook. In Proceedings of the eighth international workshop on data mining for online advertising . 1-9. [11] Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. 2019. FiBiNET. In Proceedings of the 13th ACM Conference on Recommender Systems . ACM. https://doi.org/10. 1145/3298689.3347043 [10] Xin He, Kaiyong Zhao, and Xiaowen Chu. 2021. AutoML: A survey of the stateof-the-art. Knowledge-Based Systems 212 (2021), 106622. https://doi.org/10.1016/ j.knosys.2020.106622 [12] Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnab√°s P√≥czos, and Eric P. Xing. 2018. Neural Architecture Search with Bayesian Optimisation and Optimal Transport. CoRR abs/1802.07191 (2018). arXiv:1802.07191 http: //arxiv.org/abs/1802.07191 [14] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong Sun. 2018. XDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (London, United Kingdom) (KDD '18) . Association for Computing Machinery, New York, NY, USA, 1754-1763. https://doi.org/10.1145/3219819.3220023 [13] Lang Lang, Zhenlong Zhu, Xuanye Liu, Jianxin Zhao, Jixing Xu, and Minghui Shan. 2021. Architecture and Operation Adaptive Network for Online Recommendations. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining (Virtual Event, Singapore) (KDD '21) . Association for Computing Machinery, New York, NY, USA, 3139-3149. https: //doi.org/10.1145/3447548.3467133 [15] Bin Liu, Chenxu Zhu, Guilin Li, Weinan Zhang, Jincai Lai, Ruiming Tang, Xiuqiang He, Zhenguo Li, and Yong Yu. 2020. AutoFIS: Automatic Feature Interaction Selection in Factorization Models for Click-Through Rate Prediction. CoRR abs/2003.11235 (2020). arXiv:2003.11235 https://arxiv.org/abs/2003.11235 [17] Dheevatsa Mudigere, Yuchen Hao, Jianyu Huang, Zhihao Jia, Andrew Tulloch, Srinivas Sridharan, Xing Liu, Mustafa Ozdal, Jade Nie, Jongsoo Park, et al. 2022. Software-hardware co-design for fast and scalable training of deep learning recommendation models. In Proceedings of the 49th Annual International Symposium on Computer Architecture . 993-1011. [16] Risto Miikkulainen, Jason Liang, Elliot Meyerson, Aditya Rawal, Dan Fink, Olivier Francon, Bala Raju, Hormoz Shahrzad, Arshak Navruzyan, Nigel Duffy, and Babak Hodjat. 2017. Evolving Deep Neural Networks. arXiv:1703.00548 [cs.NE] [18] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, CaroleJean Wu, Alisson G Azzolini, et al. 2019. Deep learning recommendation model for personalization and recommendation systems. arXiv preprint arXiv:1906.00091 (2019). [20] Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan, Quoc V. Le, and Alexey Kurakin. 2017. Large-Scale Evolution of Image Classifiers. In Proceedings of the 34th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 70) , Doina Precup and Yee Whye Teh (Eds.). PMLR, 2902-2911. https://proceedings.mlr.press/v70/ real17a.html [19] Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. 2018. Efficient Neural Architecture Search via Parameters Sharing. In Proceedings of the 35th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 80) , Jennifer Dy and Andreas Krause (Eds.). PMLR, 4095-4104. https://proceedings.mlr.press/v80/pham18a.html [21] Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Xiaojiang Chen, and Xin Wang. 2020. A Comprehensive Survey of Neural Architecture Search: Challenges and Solutions. CoRR abs/2006.02903 (2020). arXiv:2006.02903 https://arxiv.org/abs/2006.02903 [22] Radwa El Shawi, Mohamed Maher, and Sherif Sakr. 2019. Automated Machine Learning: State-of-The-Art and Open Challenges. CoRR abs/1906.02287 (2019). arXiv:1906.02287 http://arxiv.org/abs/1906.02287",
  "keywords_parsed": [
    "None"
  ]
}