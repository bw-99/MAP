{"title": "#REVAL: A SEMANTIC EVALUATION FRAMEWORK FOR HASHTAG RECOMMENDATION A PREPRINT", "authors": "Areej Alsini; Du Q Huynh; Amitava Datta", "pub_date": "2023-05-31", "abstract": "Automatic evaluation of hashtag recommendation models is a fundamental task in many online social network systems. In the traditional evaluation method, the recommended hashtags from an algorithm are firstly compared with the ground truth hashtags for exact correspondences. The number of exact matches is then used to calculate the hit rate, hit ratio, precision, recall, or F1-score. This way of evaluating hashtag similarities is inadequate as it ignores the semantic correlation between the recommended and ground truth hashtags. To tackle this problem, we propose a novel semantic evaluation framework for hashtag recommendation, called #REval. This framework includes an internal module referred to as BERTag, which automatically learns the hashtag embeddings. We investigate on how the #REval framework performs under different word embedding methods and different numbers of synonyms and hashtags in the recommendation using our proposed #REval-hit-ratio measure. Our experiments of the proposed framework on three large datasets show that #REval gave more meaningful hashtag synonyms for hashtag recommendation evaluation. Our analysis also highlights the sensitivity of the framework to the word embedding technique, with #REval based on BERTag more superior over #REval based on FastText and Word2Vec.", "sections": [{"heading": "Introduction", "text": "It is important to have frameworks that can automatically and accurately evaluate the performances of algorithms. In the hashtag recommendation research area, algorithms that can help recommend hashtags to the user while the tweet is being written have received a great amount of attention in recent years. This is due to the increased numbers of online messaging systems and online users every year. Hashtags are innovative social identifiers prefixed with the # sign. They work as annotations to their corresponding tweets. They enable users to quickly find tweets of a topic in mind and they help to promote engagement among users. Although hashtag recommendation models have received considerable interest due to the increased popularity of social media platforms Alsini et al. [2021], studies on their evaluation methods are still in their infancy.\nCurrent evaluation methods for hashtag recommendation mainly focus on the accuracy-based metrics Kywe et al. [2012], Zhao et al. [2016], Li et al. [2016], Alsini et al. [2017], Kowald et al. [2017], Zhang et al. [2017], Shi et al. [2018], Alsini et al. [2020a,b] such as hit rate, hit ratio Alsini et al. [2020b], precision, recall, and F1 scores. These metrics are dependent on the exact matching of the recommended hashtags with the actual list of hashtags used in the tweets of the user. For evaluation purpose, we refer to this actual list of hashtags as the ground truth hashtags. Some other researchers used human evaluation Mazzia and Juett [2011] to assess their hashtag recommendation models because the suggested hashtags did not match the ground truth hashtags even though they were relevant.\nEvaluating hashtag recommendations is a very challenging task, as illustrated in the following tweets taken from the #Covid19 thread in Twitter:\n(1) Know the facts about #Covid19...Don't panic #StayHomeSA #StaySafe #21DayLockdown.\n(2) #Covid19: A Crucial Impact On #Defence #Industry.\nThere are various automatic language-based evaluation metrics. In the context of machine translation, BiLingual Evaluation Understudy (BLEU) Papineni et al. [2002] is an automatic evaluation method. It matches the n-gram vectors of a given translated sentence (known as candidate) with the n-gram vectors of multiple human translations (known as references) to yield a weighted geometric mean of all the n-gram precisions. Thus, the more is the number of matches, the higher is the BLEU score. METEOR Banerjee and Lavie [2005] is another automatic method for evaluating the outputs of machine translation algorithms. It gives further improvement to BLEU by using additional elements, such as stemming and synonym matching, in the evaluation process. While BLEU scores are defined based on precisions only, METEOR scores are defined as the harmonic means of unigram precisions and recalls, with a heavier weight on the latter. In the context of text summarization, Recall-Oriented Understudy for Gisting Evaluation (ROUGE) Lin [2004] is introduced as a set of automatic metrics, which evaluate the quality of a candidate summary by matching it to the references created by humans using the n-grams, word sequences, and word pairs. Evaluation metrics have also gained increasing interest in the image captioning research area. CIDEr Vedantam et al. [2015] and SPICE Anderson et al. [2016] are image captioning evaluation methods that incorporate human consensus in the description of an image. Precision and recall are considered when estimating the similarity between the candidate sentence and the sentences annotated by humans. CIDEr measures the similarity between the n-gram of words occurring in the candidate sentence and all the reference sentences. SPICE Anderson et al. [2016], on the other hand, constructs a semantic parsing that converts the captions into scene graphs. It has been shown to outperform all previously mentioned metrics. An interesting finding is that the more is the number of reference sentences, the higher is the SPICE score.\nIn all the evaluation methods mentioned above, human references were considered as the base of evaluation. For hashtag recommendation, the ground truth hashtags, which are the hashtags present in the test tweets, are used as references for evaluation. In this regard, the only available method is to perform exact matching between the recommended hashtags and the hashtags from the ground truth. In the literature, accuracy-based metrics such as hit rate, precision, recall, hit ratio, and F1-score are commonly used for evaluating hashtag recommendation models Alsini et al. [2020b]: Hit rate considers the recommendation as a hit if at least a single recommended hashtag has an identical match with a hashtag from the ground truth; precision and recall measure the proportion of correctly matched hashtags in the recommended hashtags and in the ground truth hashtags, respectively; hit ratio calculates the ratio of the hits against the minimum number of hashtags in the recommendation and in the ground truth; F1-score is a harmonic mean of precision and recall. A shortcoming of these metrics is that they do not take into consideration the semantic correlation in the evaluation of hashtag recommendation. The aim of our proposed framework is to overcome this shortcoming.", "publication_ref": ["b0", "b2", "b3", "b4", "b5", "b6", "b7", "b10", "b12", "b14", "b16", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Domain Modeling and Synonym Identification", "text": "Domain modelling schemes represent data in one of the following structured form Gilchrist [2003]: Taxonomy, ontology, and thesaurus. In taxonomy, multiple levels of concept generality are represented using a hierarchical tree structure, with generic terms at the top of the tree to specific terms at the bottom; in ontology of a given domain, of interest is to retrieve a formal and explicit specification of the shared conceptualisation; in thesaurus Schwarz [2005], every term has a reference to all the other terms that have similar meanings to it. The relationship of a concept term with its synonyms in a thesaurus can be hierarchy, association, or equivalence Schwarz [2005]. Synonym expansion has been used in many applications, such as information retrieval Al-Khateeb et al. [2017] and search-related domains Boteanu et al. [2019]. Botenanu et al. Boteanu et al. [2019] enhanced the search for Amazon products via building large taxonomies of shopping items.\nIn the literature, three main methods have been used to identify synonyms: WordNet Boteanu et al. [2019], Named Entity Recognition B\u00f8hn and N\u00f8rv\u00e5g [2010], Sotomayor and Veloz [2017], and word embedding Wang et al. [2015], Leeuwenberg et al. [2016], Landthaler et al. [2017], Ali et al. [2019], Li et al. [2019a]. Handler et al. Handler [2014] highlighted that word embedding generates a larger number of synonyms compared to other methods. Hazem et al. Hazem and Daille [2018] used word embedding to identify synonyms for multi-word terms and Ali et al. Ali et al. [2019] used word embeddings to distinguish between synonyms and antonyms of the embeddings. Both papers used Word2Vec as their word embedding technique.\nIn our paper, our interest is to associate hashtags that have similar meanings. So we investigate various word embedding techniques to represent our hashtags and to build our thesaurus.", "publication_ref": ["b17", "b18", "b18", "b19", "b20", "b20", "b20", "b21", "b22", "b23", "b24", "b25", "b26", "b28", "b29", "b26"], "figure_ref": [], "table_ref": []}, {"heading": "Hashtag Representations", "text": "Hashtags have been used to promote TV-shows De Michele et al. [2019] and utilised by health care professionals to share ideas and policies Ojo et al. [2021]. Being able to semantically compare hashtags help to quickly identify social behaviours in the digital world, such as bullying Calvin et al. [2015] and activism Byrne et al. [2021], and aggregate disaster responses Chowdhury et al. [2020]. Generally, machine learning tasks require the hashtags to be represented as feature vectors (embeddings or representations). However, learning the embeddings of hashtags can be very challenging. As hashtags are user-generated, problems such as sparsity, polysemy and synonymy are common Liu et al. [2018]. Hashtags can be written as acronyms and can include numbers, misspelled words, connected words, and connected words separated by an underscore and/or full stops. Besides, a hashtag can have multiple meanings, and multiple hashtags can share the same meaning.\nHashtags have been dealt with differently in the literature. Some researchers assume that hashtags are independent components and should not be treated as words Weston et al. [2014], Li et al. [2019b], Zhu et al. [2020]. Thus, they preserve hashtags as independent tokens without pre-processing them. Some other researchers assume that the same peculiarities are shared between words in tweets and hashtags. Accordingly, they treat words and hashtags similarly after removing the punctuation, including the # sign. Hashtags are then tokenised into words and sub-words using a tokenisation method.\nVarious methods have been proposed to learn hashtag representations, such as Bag of Words methods (BOW) Tsur et al. [2012Tsur et al. [ , 2013]], Javed and Suk Lee [2018], topic models Zhao et al. [2016], graph models Liu et al. [2018], and word embedding techniques Dey et al. [2017]. Tsur et al. Tsur et al. [2012, 2013] and Javed et al. Javed and Suk Lee [2018] aggregated tweets containing the same hashtag as a document. They then represented each hashtag using BOW of the most frequent words in each document. Zhao et al. Zhao et al. [2016] proposed Hashtag-LDA to generate joint representations of hashtags and words in tweets to discover latent topics and global hashtags. Dey et al. Dey et al. [2017] proposed EmTaggeR that treats hashtags as words in the tweets. They pre-trained a skip-gram Word2Vec model over tweets to derive the embeddings of hashtags. Liu et al. Liu et al. [2018] proposed Hashtag2Vec that creates a heterogeneous hierarchical graph with hashtags, tweets, and words of tweets to derive the embeddings of hashtags.\nAs hashtags are user generated and the free style of creating hashtags change over time, applying the previous methods can be difficult. BOW methods focus on the presence and absence of words in a set of tweets containing hashtags and ignore the contextual meaning. LDA-based models adopt statistical methods where latent parameters need to be recalculated with new data. Using the graph models, the network graph structure can change significantly with the dynamic change of topics over time. Word embedding techniques such as Word2Vec and FastText are trained on the word-level, and pre-trained Word2Vec and FastText models can perform poorly on Twitter data Stewart et al. [2019]. Most researchers re-trained their word embedding models, especially when working on domain-specific Twitter data Alsini et al. [2019Alsini et al. [ , 2020a]].", "publication_ref": ["b30", "b31", "b32", "b33", "b34", "b35", "b36", "b38", "b39", "b40", "b41", "b2", "b35", "b42", "b39", "b40", "b41", "b2", "b42", "b35", "b43", "b44"], "figure_ref": [], "table_ref": []}, {"heading": "Transformer-based Models", "text": "The context vector of a word can be encoded with respect to its surrounding words using an attention mechanism such as the transformer Vaswani et al. [2017]. In 2018, Google's Bidirectional Encoder Representations from Transformers (BERT) Devlin et al. [2019] was a breakthrough in the NLP domain. BERT is trained on extensive corpora using the masked words for predicting the next sentence. BERT comes with two model sizes:\n\u2022 BERT-base, which contains 12 layers (transformer blocks), 12 attention heads, and 110 million parameters; \u2022 BERT-Large, which includes 24 layers, 16 attention heads and 340 million parameters.\nRecently, various transformer-based models, such as RoBERTa Liu et al. [2019], AlBERT Lan et al. [2020], andBERTweet Nguyen et al. [2020], have been proposed. RoBERTa optimizes BERT by training it on larger corpora and more training iterations with different masking patterns. AlBERT optimizes BERT by using fewer parameters and thus shortens the training time. BERTweet, on the other hand, uses a similar architecture as BERT-base but was trained using RoBERTa's pre-training procedure on Twitter data. BERTweet outperformed RoBERTa and XLM-R on two classification datasets.\nBERTweet-base was pre-trained on 850 million English tweets collected from 01/2012 to 08/2019. Pre-trained transformer-based models enable researchers to achieve better results with minimal fine-tuning on various NLP tasks such as classification Antoun et al. [2020], Named Entity Recognition Liang et al. [2020], and Question-Answering Kayesh et al. [2020]. All the previously mentioned transformer-based models generate embeddings at the sentence level. Unlike all the above transformer-based models which generate word embeddings at the sentence level, our module, BERTag, encodes hashtags using a pre-trained BERTweet model.", "publication_ref": ["b45", "b47", "b48", "b49", "b50", "b51"], "figure_ref": [], "table_ref": []}, {"heading": "The #REval Framework", "text": "Given a dataset D = {(t 1 , H 1 ), (t 2 , H 2 ), \u2022 \u2022 \u2022 } that contains a list of tweets and their corresponding hashtags where\nH i = {h 1 , h 2 , \u2022 \u2022 \u2022 }, let {(R, G )} = {(R 1 , G 1 ), (R 2 , G 2 ), \u2022 \u2022 \u2022 }\nbe the collection of tuples representing the test set obtained from a hashtag recommendation model. Here, R i = { \u0125\u2032 1 , \u0125\u2032 2 , ...} is the list of the top-r recommended hashtags (for a user-defined r value) of the i th test tweet and G i = {h \u2032 1 , h \u2032 2 , ...} is the corresponding ground truth hashtags. Our goal is to measure the performance of a given hashtag recommendation model by computing the average #REval-hit-ratio (described later in Section 3.3.1), taking into account the semantic correlation between hashtags. The notations used in the rest of the paper are listed in Table 1.\nOur #REval framework is designed to detect the semantic correspondences of the lists of hashtags between R and G . It comprises the following modules: BERTag, synonym and thesaurus construction, and semantic evaluation. These modules are detailed in the subsections below.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "BERTag", "text": "Let T = {t i | \u2200i} be the set containing all the tweets in D. We consider the hashtags as labels for tweets, where every tweet has a single hashtag label. For a tweet with multiple hashtags, we duplicate the tweet a sufficient number of times to match the number of hashtags. For example, if a tweet t j \u2208 T contains two hashtags {h a , h b }, for some integers a and b, we create the pairs (t\n(1) j , h a ) and (t (2) j , h b ).\nThe superscript of each t j denotes the duplication of the tweet to match the number of hashtags. The absolute ordering of tweets for a given hashtag is not important. The subscripts under h correspond to the unique ID of each distinct hashtag. Retweets with the same label are removed. For each hashtag h, we also keep the index \u03b7 t of the corresponding tweet t, i.e., we store {(\u03b7 t , h) | \u2200t}, for future retrieval of all the tweets for any given hashtag h. h; h i an arbitrary hashtag; the i th hashtag in a set.\n(\u03b7 t , h) the index \u03b7 t of a tweet t for the hashtag h. m total number of clusters (i.e., total number of unique hashtags in the dataset).\nv t ; v h embedding for tweet t; embedding for hashtag h n h total number of tweets having h as a hashtag.\nR i = {h 1 , h 2 , ..., h r }\nthe list of top-r (for some positive integer r) recommended hashtags for the i th test tweet.\nG i = {h \u2032 1 , h \u2032 2 , ..., h \u2032 ni }\nthe ground truth list of hashtags for the i th test tweet; the length n i of the list varies from tweet to tweet.\n\u27e8R, G \u27e9 = {(R 1 , G 1 ), ..., (R n , G n )}\nthe test set for evaluation, where R i is to be verified against G i .\nSyn k (h) the set containing hashtag h and the k closest synonyms of h, i.e., the set has k + 1 elements.\nE = {(h, v h ) | \u2200h}\na dictionary of hashtags and their embeddings.\nT = {(h, Syn(h))\n| \u2200h} a thesaurus of hashtags and their synonyms.\nTweets with their hashtags are the input to our module BERTag. The output of BERTag is a dictionary E = {(h, v h ) | \u2200h}, where each dictionary item is a hashtag h and its semantic representation v h . To generate E , three stages are involved in BERTag: pre-processing, extracting tweet embeddings, and computing hashtag embeddings.\nThe architecture of BERTag comprising these three stages is shown in Fig. 1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Pre-processing", "text": "The pre-processing stage of the BERTag module consists of two steps.\nThe first step is to perform data cleaning on the tweets. This step involves removing stop words, mentions, URLs, and punctuation except for the # sign. Tweets are transformed into lower case letters and non-English tweets are removed.\nThe second step is to load the pre-trained BERTweet models from Hugging Facefoot_0 . These pre-trained models contains at least 16 billion word tokens generated via the TweetTokenizer from NLTKfoot_1 . The tokenization process removes all the mentions from tweets and truncates the length of repeated characters to 3 to reduce the number of unique words (e.g., 'Heeeeelllllllo' becomes 'Heeelllo'). TweetTokenizer keeps hashtags as they are and considers them as independent tokens. We use two versions of BERTweet in the evaluation of our framework:\n\u2022 BERTweet-base, and\n\u2022 BERTweet-covid19-base-uncased.\nBoth versions of BERTweet accept any tweet in the following form: The tweet has been tokenized; the '[CLS]' token has been appended to the start of the tweet; the '[SEP]' token has been appended at the end of the tweet; each token for the tweet has been mapped to a unique token ID; the tweet has been truncated to a pre-defined length; and attention masks for the '[PAD]' tokens have been added to differentiate between padding and non-padding.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Extracting tweet embeddings", "text": "At the end of the second pre-processing step above, each tweet is represented by a BERTweet-base or a BERTweet-covid19-base-uncased embedding vector. Following the dimensions specified in BERTweet, the tweet embedding vectors in BERTag are also 768 dimensional. For any given tweet t, we use v t to denote its tweet embedding vector.\nSince the tweet embeddings in BERTag are learned from a large corpus of tweets, tweets with similar textual contents are closer together in the semantic space. The cosine distance function (i.e., 1 -cos(\u03b8), where \u03b8 is the angle between the two embedding vectors being compared) can therefore be used as a distance measure between tweets.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Pre-processing", "text": "Pre-trained tokenizer ...\n(t (1) 1 , ha) (t (2) 1 , hb) (t (3) 1 , hc) . . . (t (j) i , hd) . . . {(\u03b7t, h) | \u2200h} t1 t2 t3 ti . . . . . . vt 1 vt 2 vt 3 vt i . . . . . . E h 1 h 4 h 3 h 2 h m\nFigure 1: The BERTag Architecture. The inputs are the tweets and their hashtags; the output is the dictionary E = {(h, v h ) | \u2200h} containing all the hashtags and their embeddings expressed in the same semantic space as the tweet embeddings.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Computing hashtag embeddings", "text": "Since every tweet t is now paired with a single hashtag label h, through the index {(\u03b7 t , h) | \u2200t}, all tweets associated with the hashtag h can be retrieved. As described in the previous subsection, the way that the BERTag tweet embeddings are learned ensures that tweets which share the same hashtag are near each other and form a cluster, because these tweets have similar textual contents. Thus, the number of clusters can be set to m, the number of unique hashtags in the dataset.\nLet {v t | \u2200t} be the set of embedding vectors of tweets sharing h as a common hashtag, the embedding v h for hashtag h can be easily calculated by the arithmetic mean: v h = 1 s t v t /n h , where n h is the total number of tweets that use hashtag h and s is a normalisation term to ensure that v h is a unit vector. The centroid of each cluster thus represents a hashtag embedding in the semantic space alongside its tweets. Putting all the embedding vectors v h , \u2200h together yields a dictionary E of hashtags with their embeddings, i.e., E = {(h, v h ) | \u2200h}. From hereon, we refer to the hashtag embeddings computed from BERTweet-based and BERTweet-covid-base-uncased (see Section 3.1.1) as BERTag-base and BERTag-covid.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "BERTag hashtag updates", "text": "Our BERTag module considers the dynamic nature of social media platforms where new tweets and hashtags are continuously posted over time. So, it is designed to avoid recalculating the hashtags' embeddings from scratch when new tweets are added. It is straightforward to incorporate a new tweet embedding and calculate the cumulative average of all the tweet embeddings to get the resultant hashtag embedding. Let n h be the number of existing tweets that have been found to use hashtag h and v h be the corresponding hashtag embedding. Let v t , be the embedding vector of the new tweet t. Then the inclusion of this new tweet t results in the following updates:\nv h \u2190 1 s n h n h + 1 v h + 1 n h + 1 v t ,(1)\nn h \u2190 n h + 1,(2)\nwhere s is a scale factor to normalise v h to a unit vector.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Synonym and Thesaurus Construction", "text": "The process of building a thesaurus starts with identifying the synonyms of each existing hashtag. For every hashtag h \u2208 H, the set of its k closest synonyms are identified from E = {(h, v h )} using the k-nearest neighbours (kNN) algorithm. Here, the value of k is the number of desired synonyms. As kNN includes the hashtag h itself as one of its own synonyms, to get, for instance, n synonyms for h, we set the k value in kNN to n + 1. We use the cosine distance as a measure of closeness of hashtags in the hashtag embedding space. This process gives a list of synonyms Syn(h) for", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm 1 Construct_Synonyms", "text": "INPUT: \u0125: a recommended hashtag. k: an integer denoting the number of desired synonyms. E = {(h, v h )}: the dictionary from BERTag training. OUTPUT: \u015c: the set of synonyms for \u0125.\n1: /* Get the embedding for \u0125 */ 2: for each hashtag h \u2208 E do 3:\nif \u0125 matches h then 4:\nv \u0125 \u2190 v h 5: break 6: end if 7: end for 8: Construct Sv using kNN and E to find k + 1 hashtags having shortest cosine distances from v \u0125 9: Construct \u015c by searching E for all hashtags in Sv 10: return \u015c each hashtag h. These lists of synonyms together form the thesaurus T . That is, T = {(h, Syn(h)) | \u2200h}. Algorithm 1 shows the pseudocode for constructing the list of k synonyms for a given hashtag.\nThroughout the paper, we use Syn(h) to denote the set of synonyms with h itself included. Where appropriate, we use a subscript to denote the number of synonyms (other than the hashtag itself) in the set. For instance,\n\u2022 Syn 0 (h) = {h};\n\u2022 Syn 4 (h) is a list containing five hashtags, including h.\nWhen the argument to Syn() is a set of hashtags, e.g., if\nS = {h 1 , h 2 , \u2022 \u2022 \u2022 }, then Syn(S) \u225c \u222a hi\u2208S Syn(h i ).\nIt should be noted that the synonym relationship between hashtags is not symmetrical. In other words, suppose that a hashtag h i is in the synonym set of another hashtag h j , then it is not necessary that h j is in the synonym set of h i . For instance, using the example shown in Table 2, #sport is one of the three closest synonyms of #swim; however, #swim is outside the three closest synonyms of #sport.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Semantic Evaluation", "text": "The third and last module of the #REval framework is the #REval-hit-ratio calculation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "#REval-hit-ratio", "text": "Let R and G be, respectively, the sets of recommended hashtags and ground truth hashtags. In our previous work Alsini et al. [2020b], the hit ratio is defined as:\nhit-ratio(R, G) = |R \u2229 G| min(|R|, |G|) ,(3)\nwhere |\u2022| denotes the cardinality of the set. In the #REval framework, we modify this formula to incorporate the sets of synonyms extracted from R as follows:\n#REval-hit-ratio(R, G) = 1 min(|R|, |G|) \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 \u0125\u2208R \u03c1 Syn( \u0125) \u2229 G \u0338 = \u2205 if |R| \u2264 |G| h\u2208G \u03c1 h \u2208 Syn(R) otherwise,(4)\nwhere\n\u03c1(s) = 1 if s = True 0 otherwise.\nWe can see the original formula for hit ratio in Eq. ( 3) is just a special case of the formula in Eq. ( 4) with the number of synonyms being zero.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm 2 Match_Synonyms", "text": "INPUT: R: a set containing the recommended hashtags, G: a set containing the groundtruth hashtags, k: the number of desired synonyms, T = {(h, Syn(h)) | \u2200h in the test set}: a thesaurus. OUTPUT: \u03c1: the number of matches.\n1: Initialize: \u03c1 \u2190 0 2: if |R| \u2264 |G| then 3:\nfor \u0125 \u2208 R do 4:\nSyn( \u0125) \u2190 Look up \u0125 in T 5:\nif Syn( \u0125) \u2229 G \u0338 = \u2205 then 6:\n\u03c1 \u2190 \u03c1 + 1 7:\nend if 8:\nend for 9: else 10:\n/* construct the list of synonyms for R */ 11: Syn(R) \u2190 \u2205 12:\nfor \u0125 \u2208 R do 13: Syn( \u0125) \u2190 Look up \u0125 in T 14:\nSyn(R) \u2190 Syn(R) \u222a Syn( \u0125) 15:\nend for 16:\nfor h \u2208 G do 17: if h \u2208 Syn(R) then 18: \u03c1 \u2190 \u03c1 + 1 19: end if 20:\nend for 21: end if 22: return \u03c1 Notice that synonyms are only constructed for the set of recommended hashtags R, and not for the set of ground truth hashtags G. By doing so, we consider each recommended hashtag \u0125 to be a hit only if a ground truth hashtag h in G is synonymous to it, but not the other way round. Consider the case where the recommended hashtag is a more specific term but the ground truth hashtag is a more general term, such as R = {#hockey} and G = {#sport}. In this case, hockey is a kind of sport and it is more likely that Syn(#hockey) \u220b #sport and the #REval-hit-ratio formula will correctly count the recommendation as a hit. On the other hand, if R = {#sport} and G = {#hockey}, then it is unlikely that Syn(#sport) \u220b #hockey as sport is not necessarily about hockey. In this case, the formula will correctly not count the recommendation as a hit. In contrary, if the synonym lists were allowed to grow for both the recommendation set R and ground truth set G, then for extremely large k values, the two synonym lists will overlap even though the starting hashtags in the two sets are very different. By restricting G to remain as is, the above scenario will not occur. It should be noted that k is usually required to be a large number due to the noise in hashtags; however, one should set the value of k sensibly. If we were to let k \u2192 \u221e, then every hashtag in the dataset would be in the list of synonyms of any given hashtag, and it would not be meaningful to use the measure for hashtag recommendation evaluation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Synonym matching", "text": "The pseudo code for computing the number of matches between a set of recommended hashtags and a set of ground truth hashtags is given in Algorithms 2. It is easy to verify that \u03c1, the number of matches between the two sets R and G computed using Eq. ( 4), satisfies the condition: \u03c1 \u2264 min(|R|, |G|). The #REval-hit-ratio is then computed as \u03c1/ min(|R|, |G|). So, like the hit ratio defined in Eq. ( 3), #REval-hit-ratio values are also bounded between 0 and 1, and the higher they are (closer to 1), the better.\nFour small examples showing how synonym matching works are given in Table 2, for different cardinalities of R and G. The last column of the table contains the #REval-hit-ratio values, expressed as a fraction. Note that both the recommended and ground truth hashtags can contain spelling mistakes, such as #keeepfit shown in the examples. Except for the third example where R = {#hockey}, all the other three examples show that the recommended hashtags meaningfully match with the ground truth hashtags; however, an exact matching of the hashtags in R and G will give a hit-ratio of zero for all the examples. The #REval framework comprising all the modules explained above is shown in Figure 2.  \nDataset D = {(t i , H i )} n i=1 A Hashtag Recommendation Model E = {(h, v h ) | \u2200h} T = {( \u0125\u2032 , Syn k ( \u0125\u2032 ) | \u2200 \u0125\u2032 \u2208 \u0124\u2032 } Recommendation R = {(t \u2032 , \u0124\u2032 ) | \u2200t \u2032 , \u0124\u2032 } Average #REval-hit-ratio k Training set {(t, H) | \u2200t, H}; Test set {t \u2032 | \u2200t \u2032 } Ground truth for the test set: G = {(t \u2032 , H \u2032 ) | \u2200t \u2032 , H \u2032 }", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": ["tab_2"]}, {"heading": "Experiments", "text": "We have evaluated the effectiveness of our #REval framework using three datasets, with the last dataset being domainspecific (in health). We have also tested the impact on the average #REval-hit-ratio when embeddings other than BERTag-base and BERTag-covid are used in the first BERTag module of the framework.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Datasets", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The AU Trends dataset", "text": "We used 16 Australian trending hashtags (see Table 3) as seeds for crawling for tweets posted on March 13 th , 2020. As other hashtags were also used in these tweets, the resultant number of unique hashtags is a lot larger than the number of trending hashtags. After the data cleaning step, 272,686 tweets containing one or more hashtags remain. The number of unique hashtags in these tweets is 15,375. As many hashtags were repetitively used multiple times, the total number of hashtags (including the repetitions) is 547,227. Table 4 summarises the statistics of this dataset and the remaining two datasets described below.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_3", "tab_4"]}, {"heading": "The US Trends dataset", "text": "Following the same tweet crawling process above, we used 14 US trending hashtags (see Table 3) to collect tweets, also on March 13 th , 2020. As shown in Fig. 2, the BERTag module (see Section 3.1) outputs BERTag-base embeddings or BERTag-covid embeddings for the dictionary E . Apart from using these embeddings, a different way to learn the hashtag embeddings for building the dictionary E can be evaluated. In our experiments, we tested the Word2Vec and FastText models. The training procedure is summarised below.\nWe firstly trained Word2Vec (similarly for FastText) model using the gensim libraryfoot_2 , where every tweet is considered as a document. Knowing that tweets are short texts, we set the window_size hyperparameter to 2 to determine the dependency of a word to its neighbouring words. We took into account every word in the tweets because some hashtags occur only once. Thus, we set the min_count hyperparameter to 1. The number of training epochs was set to 30. Finally, a dictionary E = {(h, v h )} containing all the hashtags and their embeddings was generated. The hashtag embeddings were finally obtained by taking the arithmetic means of the tweet embeddings obtained from the training process.\nSo, in our experiments, we compared four different variants for the first module of the #REval framework shown in Fig. 2. Each version uses:\n\u2022 hashtag embeddings defined in terms of Word2Vec;\n\u2022 hashtag embeddings defined in terms of FastText;\n\u2022 BERTag-base embeddings; or\n\u2022 BERTag-covid embeddings.", "publication_ref": [], "figure_ref": ["fig_0", "fig_0"], "table_ref": ["tab_3"]}, {"heading": "Generation of R for the test set", "text": "As shown in Fig. 2, #REval is agnostic to the hashtag recommendation model that produces the recommendation set R. This is evident from the figure that the orange rectangular box is outside the light blue shaded region for #REval. However, in order to test the Semantic Evaluation module of #RERval, we need an example hashtag recommendation model. In this paper, we chose the hashtag recommendation method based on tweet similarity proposed in Alsini et al. [2020a] to generate R. This method encoded the tweets as Mean of Word Embedding (MOWE) vectors computed from words that were trained using the Word2Vec model. For a given test tweet, the algorithm computed its MOWE vector and compared it with the training tweets' MOWE vectors based on their cosine similarity measure. A threshold value of 0.5 was used as the cut off for selecting similar tweets from the training set. Hashtags that were used in these selected training tweets were ranked based on their hashtag popularity, and the top-r hashtags, for a pre-defined positive integer r, were put forwarded as the recommended hashtags for the test tweet. Because of the threshold value (0.5) used above, the number of recommended hashtags for any test tweet may be fewer than r.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Top-r recommendations and number of synonyms", "text": "For each dataset mentioned above, only those tweets having at least one hashtag are retained. After the pre-processing stage (see Section 3.1.1), the tweets are split into 90/10% to form the training set and the test set. The training set is used to train the hashtag recommendation method Alsini et al. [2020a] described above to generate R for the test set.\nFor each tweet in the testing set, the top-r recommended hashtags are extracted for the following r values: 1, 5, and 10. This gives three separate R for the three top-r recommendations for comparison with the ground truth test set G (see Fig. 2).\nFor each recommended hashtag, we set the number of synonyms (the value of k in Algorithms 1 and 2) to a range of values: {0, 5, 10, 20, \u2022 \u2022 \u2022 , 70}. The reason for setting k to a large value such as 70 is because hashtags are noisy text strings and can contain spelling mistakes, numbers, etc. For instance, the hashtags #covid, #covid0, #covid19, #covid2019, #coronavirus, and #coronav are all synonymous words and most of them are not in the standard English dictionary. So, when considering a specific target hashtag, we need to enlarge the number of synonyms in the hashtag embedding space. It should be noted that, for the case where k = 0, the #REval-hit-ratio becomes the hit-ratio (Eq. ( 3)) and it can be considered to be the output from the baseline exact-matching method of hashtags.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Results and Discussions", "text": "For each test tweet t \u2032 in the test set, we computed the #REval-hit-ratio between the recommended hashtag set R and the ground truth hashtag set G for t \u2032 . The average #REval-hit-ratio was finally computed for the entire recommendation set R. Figure 3 shows the average #REval-hit-ratios for the four variants of embeddings computed using different numbers of synonyms (the value of k on the horizontal axis) for the three datasets. The top, middle, and bottom rows of the figure correspond to the top-1, top-5, and top-10 recommendations. The #REval-hit-ratio at k = 0 in each subplot denotes the case where no synonyms were used, i.e., Syn 0 (h) = {h} for each hashtag h, and effectively the hit-ratio (Eq. ( 3)) for the baseline exact-matching of hashtags was computed instead.\nComparing the results shown in Figure 3, we summarize the following four observations.\n(i) The exact-matching method underrates the performance of the hashtag recommendation model. As expected, the baseline exact-matching method (where k = 0) has the lowest average #REval-hit-ratio in each subplot, as no synonyms were included in the calculation. For the AU Trends and Health datasets (Fig. 3, columns 1 and 3) we can see that, by increasing k slightly to 5 or 10, the average #REval-hit-ratio values are almost double when the BERTag-base (blue curves) or BERTag-covid (magenta curves) embeddings were used to represent the hashtags. This improvement is consistent for all the top-1, top-5, and top-10 recommendations. For the US Trends dataset, we also observe increases in the average #REval-hit-ratio values when k grows, although the increases are smaller when k is under 10. When Word2Vec or FastText was used as hashtag embeddings, the increase in average #REval-hit-ratio with respect to k is small.\nComparing across the three datasets, the hashtag recommendation model Alsini et al. [2017] performed the best on the US Trends dataset. Even when k = 0, its average #REval-hit-ratio on the US Trends dataset is much higher than those for the other two datasets. It should be noted that hashtag recommendation is not an easy task because of noise in the data and large number of hashtags and variations of tweet contents. With synonyms included, its average #REval-hit-ratio is just under 0.8 for the top-10 recommendations.\n(ii) BERTag-base and BERTag-covid outperform Word2Vec and FastText. Since the same recommendation test set R was used for all the four variants, the only difference is what embeddings were used to represent the hashtags and, consequently, the quality of the generated synonyms for the #REval-hit-ratio calculation. We manually inspected many hashtags and their synonyms extracted from the embedding spaces of these four variants and we summarise six hashtags (two from each dataset) and their five closest synonyms in Table 5. We can see from the table that the synonyms from BERTag-base and BERTag-covid are very similar in most examples. Also, compared to those from Word2Vec and FastText, these synonyms are more related to the original hashtags in the first column. For the hashtag #stockmarket, both BERTag-base and BERTag-covid even picked up the names of two specific shares, #sensex and #nifty, as synonyms. Looking at the Word2Vec column in the table, we see some strange words, such as #lasagna and #grilling as the two closest synonyms of #aging; and #ask as the first synonym of #quinoa. FastText, on the other hand, seemed to group words that have similar substrings together in the hashtag space, e.g., the substring \"market\" appears in all the five synonyms of #stockmarket; the postfix \"ing\" appears in four of the five synonyms of #aging. While these synonymous hashtags from FastText are sometimes quite relevant, there are many cases where FastText did not produce good synonyms, e.g., the five synonyms from FastText for #quinoa are quite poor.\n(iii) Both BERTag-base and BERTag-covid have similar performance and are better embeddings for hashtags. Comparing the #REval-hit-ratios when these embeddings are used in the BERTag module (Fig. 2), we see that BERTag-base (blue curve) outperforms BERTag-covid (magenta curve) slightly for the US Trends dataset, but the results are the other way round for the AU Trends dataset. For the Health dataset, the two curves almost completely overlap, except when k = 20, the magenta curve is a little bit higher.  As discussed above, compared to Word2Vec and FastText, when BERTag-base or BERTag-covid embeddings are used to represent hashtags, the average #REval-hit-ratios are consistently higher for all the datasets. To illustrate how the clusters of tweets sharing the same hashtags look like, we pick the ten most popular hashtags in each dataset (see Table 6) and project all the tweets for these hashtags to a 2D space using the t-SNE algorithm. Due to space limitation, we only show the t-SNE projection for the BERTag-covid embeddings. Some details about Fig. 4 are listed below.  Note that the hashtags in the legend of each plot are sorted in alphabetical order rather than decreasing order of frequency as in Table 6. (figure best viewed in colour)\n\u2022 For the AU Trends dataset, as the tweets were crawled in the period when Covid-19 was widely discussed on Twitter, nine out of these ten hashtags are related to Covid-19, except for #heabreakweather, which is related to the singer Niall Horan's studio album Heartbreak Weather released on the same date. It is clear in Fig. 4(a) that the tweets for this hashtag form a cluster on their own (the deep red dots) while the tweets for the other nine clusters all lump together as they are related to the same topic.\n\u2022 For the US Trends dataset (Fig. 4(b)), the largest cluster is the tweets for the first hashtag #heabreakweather.\nThere is a great overlap between the two clusters for #1 and #2, which are common hashtags that Twitter users like to use when they want to claim they were the first person achieving certain endeavours. As expected, the clusters for the two Covid-related hashtags, #coronavirusinkenya and #covid19, overlap significantly, while the tweets for #bernieisright and #fridaythe13th form two separate clusters. Many tweets for the hashtag #traderjoes (about Trader Joe's grocery store) had cross-topic discussions and they form scattered points, overlapping with other clusters.\n\u2022 Different from the previous two datasets, the Health Dataset is a domain-specific dataset crawled in 2015. For this dataset (Fig. 4(c)), the tweet clusters for most hashtags are well separated, e.g., #ebola, #obamacare, #nhs, and #pharma. However, due to the similarities of the topics discussed in the tweets, the #health cluster has some overlap with #healthtalk and #fitness; the tweet cluster for #weightloss overlaps with those for #latfit and #fitness, and it is also close to the cluster for #getfit.\nWe can see from the t-SNE projection that the hashtags represented using BERTag-covid (similarly for BERTag-base) are more closely related to the topics discussed in the tweets. Consequently, these embeddings give more relevant synonyms for hashtag recommendation evaluation, as confirmed also from the average #REval-hit-ratios shown in Fig. 3. Our #REval framework therefore uses either BERTag-base or BERTag-covid as the default embeddings for the BERTag module.\n(iv) Effectiveness and correctness of the #REval-hit-ratio formula. We discussed in Section 3.3 why the list of synonyms should be constructed for the hashtags in the recommendation set R only, as formulated in Eq. ( 4). It is clear that all the curves in Fig. 3 have a steeper increase of the average #REval-hit-ratio when the number of synonyms is small and only a gentle increase when more synonyms are included in the #REval-hit-ratio calculation. This is what we expect to see in a good hashtag recommendation model. Of course, if we keep growing k indefinitely, we will continue to see a gradual increase of #REval-hit-ratio. So, when comparing the performance of two hashtag recommendation models, one should consider the model that achieves the same #REval-hit-ratio but with a smaller k value as a better model. Our current framework allows the k value to be fed to the Synonym and Thesaurus Construction module (Fig. 2). To impose the criterion that the synonyms must meet a certain level of quality, a threshold value on the cosine distance used in kNN (Section 3.2) can be easily added to the module.", "publication_ref": ["b4"], "figure_ref": ["fig_1", "fig_1", "fig_1", "fig_0", "fig_1", "fig_1", "fig_0"], "table_ref": ["tab_5", "tab_6", "tab_6"]}, {"heading": "Conclusion and Future Work", "text": "We have presented #REval, a novel evaluation framework that measures the performance of hashtag recommendation models by incorporating hashtag synonyms. The framework automatically constructs thesauri of hashtags with their synonyms based on their similarity in the hashtag (and tweet) embedding space. In the paper, we have introduced BERTag, an internal module of #REval that learns hashtag representations from clusters of tweets encoded using a pre-trained transformer-based model. Our experiments show that #REval with BERTag-base and BERTag-covid hashtag embeddings give better and more relevant synonyms than the Word2Vec and FastText embeddings do. We have shown that the exact-matching method is inadequate for matching hashtags and therefore not suitable for evaluating hashtag recommendation models. Our proposed #REval-hit-ratio formula helps to overcome this short-coming.\nWhile we use BERTag embeddings in our #REval framework because of the superior performance of the parent BERTweet embeddings, other improved hashtag embeddings in the future can be used in the BERTag module. Our #REval framework can be trained for different datasets to generate different dictionaries. Once the training is done, the framework can be put in action for evaluation, i.e., the second and third modules do not need any fine-tuning. We think that, with a large number of social media users today, having an evaluation framework such as #REval is important. Our framework can also be adopted for other applications such as product recommendation extracted from messages posted by users. Besides, the resulting hashtags representations encoded using BERTag can be used in many applications such as topic identification, event detection, and information diffusion.\nCurrently, the #REval-hit-ratio formula considers the synonym list for any given hashtag as a set. Our future work is to extend the formula by considering it as an ordered list. By doing so, higher weights can be given to those synonyms appearing near the beginning of list, as they are closer to the hashtag. Different ways of ranking hashtags in the synonym list can also be investigated and compared.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Hashtag recommendation methods for twitter and sina weibo: A review", "journal": "Future Internet", "year": "2021", "authors": "Areej Alsini; Q Du; Amitava Huynh;  Datta"}, {"ref_id": "b1", "title": "On Recommending Hashtags in Twitter Networks", "journal": "Springer", "year": "2012", "authors": "Tuan-Anh Su Mon Kywe; Ee-Peng Hoang; Feida Lim;  Zhu"}, {"ref_id": "b2", "title": "A personalized hashtag recommendation approach using ldabased topic model in microblog environment", "journal": "Future Gener. Comput. Syst", "year": "2016-12", "authors": "Feng Zhao; Yajun Zhu; Hai Jin; Laurence T Yang"}, {"ref_id": "b3", "title": "Tweet modeling with lstm recurrent neural networks for hashtag recommendation", "journal": "IEEE", "year": "2016", "authors": "Jia Li; Hua Xu; Xingwei He; Junhui Deng; Xiaomin Sun"}, {"ref_id": "b4", "title": "Empirical analysis of factors influencing twitter hashtag recommendation on detected communities", "journal": "ADMA", "year": "2017", "authors": "Areej Alsini; Amitava Datta; Jianxin Li; Du Huynh"}, {"ref_id": "b5", "title": "Temporal Effects on Hashtag Reuse in Twitter: A Cognitive-Inspired Hashtag Recommendation Approach", "journal": "", "year": "2017", "authors": "Dominik Kowald; Subhash Chandra Pujari; Elisabeth Lex"}, {"ref_id": "b6", "title": "Hashtag recommendation for multimodal microblog using co-attention network", "journal": "", "year": "2017", "authors": "Qi Zhang; Jiawen Wang; Haoran Huang; Xuanjing Huang; Yeyun Gong"}, {"ref_id": "b7", "title": "Hashtagger+: Efficient high-coverage social tagging of streaming news", "journal": "IEEE Transactions on Knowledge and Data Engineering", "year": "2018", "authors": "Bichen Shi; Gevorg Poghosyan; Georgiana Ifrim; Neil Hurley"}, {"ref_id": "b8", "title": "On utilizing communities detected from social networks in hashtag recommendation", "journal": "IEEE Transactions on Computational Social Systems", "year": "2020", "authors": "Areej Alsini; Amitava Datta; Du Q Huynh"}, {"ref_id": "b9", "title": "Hit ratio: An evaluation metric for hashtag recommendation", "journal": "", "year": "2020", "authors": "Areej Alsini; Q Du; Amitava Huynh;  Datta"}, {"ref_id": "b10", "title": "Suggesting hashtags on twitter", "journal": "", "year": "2011", "authors": "Allie Mazzia; James Juett"}, {"ref_id": "b11", "title": "BERTweet: A pre-trained language model for English tweets", "journal": "", "year": "2020-10", "authors": "Thanh Dat Quoc Nguyen; Anh Tuan Vu;  Nguyen"}, {"ref_id": "b12", "title": "Bleu: a method for automatic evaluation of machine translation", "journal": "", "year": "2002-07", "authors": "Kishore Papineni; Salim Roukos; Todd Ward; Wei-Jing Zhu"}, {"ref_id": "b13", "title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments", "journal": "", "year": "2005-06", "authors": "Satanjeev Banerjee; Alon Lavie"}, {"ref_id": "b14", "title": "ROUGE: A package for automatic evaluation of summaries", "journal": "", "year": "2004-07", "authors": "Chin-Yew Lin"}, {"ref_id": "b15", "title": "Cider: Consensus-based image description evaluation", "journal": "IEEE Computer Society", "year": "2015", "authors": "C Lawrence Ramakrishna Vedantam; Devi Zitnick;  Parikh"}, {"ref_id": "b16", "title": "Spice: Semantic propositional image caption evaluation", "journal": "", "year": "2016", "authors": "Peter Anderson; Basura Fernando; Mark Johnson; Stephen Gould"}, {"ref_id": "b17", "title": "Thesauri, taxonomies and ontologies -an etymological note", "journal": "Journal of Documentation -J DOC", "year": "2003-02", "authors": "Alan Gilchrist"}, {"ref_id": "b18", "title": "Domain model enhanced search-A comparison of taxonomy, thesaurus and ontology", "journal": "", "year": "2005", "authors": "Katharina Schwarz"}, {"ref_id": "b19", "title": "Query reformulation using wordnet and genetic algorithm", "journal": "", "year": "2017", "authors": "B Al-Khateeb; A J Al-Kubaisi; S T Al-Janabi"}, {"ref_id": "b20", "title": "Synonym expansion for large shopping taxonomies", "journal": "", "year": "2019", "authors": "Adrian Boteanu; Adam Kiezun; Shay Artzi"}, {"ref_id": "b21", "title": "Extracting named entities and synonyms from wikipedia", "journal": "", "year": "2010", "authors": "C B\u00f8hn; K N\u00f8rv\u00e5g"}, {"ref_id": "b22", "title": "Thesaurus-based named entity recognition system for detecting spatio-temporal crime events in spanish language from twitter", "journal": "", "year": "2017", "authors": "M Sotomayor; F Veloz"}, {"ref_id": "b23", "title": "Medical synonym extraction with concept space models", "journal": "AAAI Press", "year": "2015", "authors": "Chang Wang; Liangliang Cao; Bowen Zhou"}, {"ref_id": "b24", "title": "A minimally supervised approach for synonym extraction with word embeddings", "journal": "The Prague Bulletin of Mathematical Linguistics", "year": "2016-04", "authors": "Artuur Leeuwenberg; Mihaela Vela; Jon Dehdari; Josef Genabith"}, {"ref_id": "b25", "title": "Extending thesauri using word embeddings and the intersection method", "journal": "", "year": "2017", "authors": "J\u00f6rg Landthaler; Bernhard Waltl; Dominik Huth; Daniel Braun; Florian Matthes"}, {"ref_id": "b26", "title": "Antonym-synonym classification based on new sub-space embeddings", "journal": "AAAI Press", "year": "2019-01-27", "authors": "Muhammad Asif; Ali ; Yifang Sun; Xiaoling Zhou; Wei Wang; Xiang Zhao"}, {"ref_id": "b27", "title": "Multi-view embedding-based synonyms for email search", "journal": "Association for Computing Machinery", "year": "2019", "authors": "Cheng Li; Mingyang Zhang; Michael Bendersky; Hongbo Deng; Donald Metzler; Marc Najork"}, {"ref_id": "b28", "title": "An empirical study of semantic similarity in wordnet and word2vec", "journal": "", "year": "2014", "authors": "Abram Handler"}, {"ref_id": "b29", "title": "Word embedding approach for synonym extraction of multi-word terms", "journal": "European Language Resources Association (ELRA", "year": "2018-05", "authors": "Amir Hazem; B\u00e9atrice Daille"}, {"ref_id": "b30", "title": "On helping broadcasters to promote tv-shows through hashtags", "journal": "Multimedia Tools and Applications", "year": "2019", "authors": "Roberta De; Michele ; Stefano Ferretti; Marco Furini"}, {"ref_id": "b31", "title": "How health care workers wield influence through twitter hashtags: Retrospective cross-sectional study of the gun violence and covid-19 public health crises", "journal": "JMIR Public Health Surveill", "year": "2021-01", "authors": "Ayotomiwa Ojo; Chandra Sharath; Margaret Guntuku; Rinad S Zheng; Megan L Beidas;  Ranney"}, {"ref_id": "b32", "title": "#bully: Uses of hashtags in posts about bullying on twitter", "journal": "Journal of School Violence", "year": "2015-01", "authors": "Angela Calvin; Amy Bellmore; Jun-Ming Xu; Xiaojin Zhu"}, {"ref_id": "b33", "title": "An online occupation of the university hashtag: Exploring how student activists use social media to engage in protest", "journal": "Journal of College and Character", "year": "2021", "authors": "L Virginia; Bridget L Byrne; Alice E Higginbotham; Terah J Donlan;  Stewart"}, {"ref_id": "b34", "title": "On identifying hashtags in disaster twitter data", "journal": "AAAI Press", "year": "2020", "authors": "Jishnu Ray Chowdhury; Cornelia Caragea; Doina Caragea"}, {"ref_id": "b35", "title": "Hashtag2vec: Learning hashtag representation with relational hierarchical embedding model", "journal": "", "year": "2018", "authors": "Jie Liu; Zhicheng He; Yalou Huang"}, {"ref_id": "b36", "title": "#TagSpace: semantic embeddings from hashtags", "journal": "ACL", "year": "2014", "authors": "Jason Weston; Sumit Chopra; Keith Adams"}, {"ref_id": "b37", "title": "Topical co-attention networks for hashtag recommendation on microblogs", "journal": "Neurocomputing", "year": "2019", "authors": "Yang Li; Ting Liu; Jingwen Hu; Jing Jiang"}, {"ref_id": "b38", "title": "Learning improved semantic representations with tree-structured lstm for hashtag recommendation: An experimental study", "journal": "Information", "year": "2020", "authors": "Rui Zhu; Delu Yang; Yang Li"}, {"ref_id": "b39", "title": "Scalable multi stage clustering of tagged micro-messages", "journal": "Association for Computing Machinery", "year": "2012", "authors": "Oren Tsur; Adi Littman; Ari Rappoport"}, {"ref_id": "b40", "title": "Efficient clustering of short messages into general domains", "journal": "", "year": "2013-01", "authors": "O Tsur; A Littman; A Rappoport"}, {"ref_id": "b41", "title": "Hybrid semantic clustering of hashtags", "journal": "Online Social Networks and Media", "year": "2018", "authors": "Ali Javed; Byung Suk; Lee "}, {"ref_id": "b42", "title": "Emtagger: A word embedding based novel method for hashtag recommendation on twitter", "journal": "", "year": "2017", "authors": "Kuntal Dey; Ritvik Shrivastava; Saroj Kaushik; L Venkata;  Subramaniam"}, {"ref_id": "b43", "title": "Word-level Lexical Normalisation using Context-Dependent Embeddings", "journal": "", "year": "2019-11", "authors": "Michael Stewart; Wei Liu; Rachel Cardell-Oliver"}, {"ref_id": "b44", "title": "Community aware personalized hashtag recommendation in social networks", "journal": "Springer", "year": "2019", "authors": "Areej Alsini; Amitava Datta; Du Q Huynh; Jianxin Li"}, {"ref_id": "b45", "title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Illia Kaiser;  Polosukhin"}, {"ref_id": "b46", "title": "", "journal": "Curran Associates Inc", "year": "", "authors": ""}, {"ref_id": "b47", "title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2019-06", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b48", "title": "Roberta: A robustly optimized bert pretraining approach", "journal": "", "year": "2019", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b49", "title": "Albert: A lite bert for self-supervised learning of language representations", "journal": "ICLR", "year": "2020", "authors": "Zhenzhong Lan; Mingda Chen; Sebastian Goodman; Kevin Gimpel; Piyush Sharma; Radu Soricut"}, {"ref_id": "b50", "title": "AraBERT: Transformer-based model for Arabic language understanding", "journal": "European Language Resource Association", "year": "2020-05", "authors": "Fady Wissam Antoun; Hazem Baly;  Hajj"}, {"ref_id": "b51", "title": "Bond: Bert-assisted opendomain named entity recognition with distant supervision", "journal": "Association for Computing Machinery", "year": "2020", "authors": "Chen Liang; Yue Yu; Haoming Jiang; Siawpeng Er; Ruijia Wang; Tuo Zhao; Chao Zhang"}, {"ref_id": "b52", "title": "Answering binary causal questions: A transfer learning based approach", "journal": "", "year": "2020", "authors": "Md Saiful Humayun Kayesh; Junhu Islam;  Wang; A S M Shikha Anirban; Paul Kayes;  Watters"}, {"ref_id": "b53", "title": "Fuzzy approach topic discovery in health and medical corpora", "journal": "International Journal of Fuzzy Systems", "year": "2018", "authors": "Amir Karami; Aryya Gangopadhyay; Bin Zhou; Hadi Kharrazi"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: The #REval framework comprises three major modules: the BERTag module, the Synonym and Thesaurus Construction module, and the Semantic Evaluation module.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: A comparison of the average #REval-hit-ratios for top-1, top-5, and top-10 recommendations from the hashtag recommendation model of Alsini et al. [2020a] on the three datasets, with the hashtag embeddings represented as Word2Vec, FastText, BERTag-base, and BERTag-covid. The number of synonyms (k) varies from 0, 5, 10, 20, \u2022 \u2022 \u2022 , 70. (figure best viewed in colour)", "figure_data": ""}, {"figure_label": "42", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 : 2 -42Figure4: 2-D Visualization of the tweets for the ten most popular hashtags for the three datasets. The tweets are represented by the 768-dimensional BERTag-covid embeddings. Note that the hashtags in the legend of each plot are sorted in alphabetical order rather than decreasing order of frequency as in Table6. (figure best viewed in colour)", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Notations used in the paper.D = {(t 1 , H 1 ), (t 2 , H 2 ), ...}the processed dataset that contains a list of hashtagged tweets t i , each of which has a corresponding list of hashtags H i .", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Four examples showing how \u03c1 and #REval-hit-ratio are computed for different sizes of R and G.", "figure_data": "RG\u03c1#REval-hit-ratio{#hockey, #championship}{#football, #sport}11/2{#football, #sport}{#hockey, #sports}11/2{#hockey}{#football, #rugby}00/1{#swim, #exercise}{#sport}11/1Syn(#hockey) = {#hockey, #bowling, #golf, #sport}Syn(#championship) = {#championship, #champion, #winner, #tournament}Syn(#football) = {#football, #soccer, #footy, #rugby}Syn(#sport) = {#sport, #sports, #exercise, #keeepfit}Syn(#swim) = {#swim, #dive, #paddle, #sport}Syn(#exercise) = {#exercise, #keeepfit, #yoga, #walking}"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Trending hashtags in Australia and US on March 13 th , 2020", "figure_data": "AU TrendsUS Trends#AustralianGP#HeartbreakWeather#Covid_19#FridayThe13th#HeartbreakWeather#QuarantineAndChill#yiayhi#BernieIsRight#Formula1#panicbuying#SeizetheCSWmoment#WeGotThisSeattle#australiangrandprix#EverythingsGonnaBeOkay#WorldSleepDay#selfisolating#Ride2School#MyQuirkiestQuirkReasonsToLeaveTheToiletSeatUp#FlattenTheCurve#Restore4GinKashmir#loona1stwin#peteonkimmel#SocialDistancing#LUVVsTheWorld2#MelbourneGP#NiallCarpool#ScottyFromMarketting#foreverfletcher#tomhanks#My750"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Statistics summary of the four datasets used in our experiments.", "figure_data": "AU Trends (MarUS Trends (MarHealth2020)2020)(2015)"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "The five closest synonyms of a few example hashtags from the three datasets.", "figure_data": "HashtagWord2VecSynonyms from thesaurus using FastText BERTag-baseBERTag-covid#stockmarket (AU Trends)#fearnoevil, #mkonoh, #awareness, #sooryavanshion24thmarch, #faculty#stockmarkets, #stockmarketcrash, #stockmarket2020, #trumpstockmarket, #stockmarketcrash2020#stockmarketcrash, #stockmarketcrash2020, #sensex, #nifty, #stocks#stockmarketcrash2020, #stockmarketcrash, #sensex, #nifty, #commodities#selfisolation (AU Trends)#onthepulse, #coronahitsnoida, #koreandrama, #julialang, #mohfw#selfisolationhelp, #selfisolating, #selfisolate, #italyselfisolation, #selfisolationlevel100#socialdistancing, #selfquarantine, #selfisolate, #quarantine, #isolation#socialdistancing, #selfquarantine, #selfisolate, #quarantine, #isolation#panicbuying (US Trends)#aiea, #petebuttigieg, #powernap, #florida, #ebooks#panicbuyinguk, #panicbuy, #stoppanicbuying, #panickbuying, #costcopanicbuying#cronavirus, #selfisolating, #blessed, #superstore, #foodbank#hoarding, #cronavirus, #panickbuying, #superstore, #asda#selfisolation (US Trends)#lka, #stop, #cinderella, #gaslightingofamerica, #melekoout#selfisolationhelp, #selfisolating, #selfisolate, #selfisolationhelpdrogheda, #isolation#selfquarantine, #selfisolationhelp, #socialdistancing, #lockdown, #quarantine#socialdistancing, #selfquarantine, #selfisolationhelp, #selfisolating, #quarantined#aging (Health)#lasagna, #grilling, #healthforall, #healthyliving, #pet#antiaging, #healthyaging, #agility, #jogging, #breaking#memory, #dementia, #stroke, #autism, #health#memory, #stroke, #autism, #dementia, #health#quinoa#ask, #polio, #hulahooping,#quit, #quiz, #quesadillas, #mojo,#healthy, #recipes, #health,#healthy, #recipes, #health,(Health)#brunch, #cancerfighting#smokers#diet, #food#diet, #food"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "The ten most popular hashtags and their frequencies", "figure_data": "DatasetPopular hashtags and their frequenciesAU#covid19 (81900), #coronavirusupdate (59514), #coronaviruspandemic (52163),Trends#coronavirus (46290), #heabreakweather (27885), #covid2019 (20797),#coronaoutbreak (17403), #corona (9801), #coronav (9150), #flattenthecurve (6595)US#heabreakweather (66262), #niallcarpool (16328), #coronavirusinkenya (10203), #1Trends(10089), #fridaythe13th (8442), #2 (5849), #traderjoes (4671), #bernieisright (4578),#latelateniall (4352), #covid19 (4345)Health#healthtalk (873), #nhs (768), #ebola (392), #getfit (261), #latfit (241), #obamacare (237), #weightloss (235),#health (228), #pharma (209), #fitness (209)"}], "formulas": [{"formula_id": "formula_0", "formula_text": "H i = {h 1 , h 2 , \u2022 \u2022 \u2022 }, let {(R, G )} = {(R 1 , G 1 ), (R 2 , G 2 ), \u2022 \u2022 \u2022 }", "formula_coordinates": [4.0, 72.0, 494.57, 254.59, 9.82]}, {"formula_id": "formula_1", "formula_text": "(1) j , h a ) and (t (2) j , h b ).", "formula_coordinates": [4.0, 184.78, 666.43, 88.16, 14.07]}, {"formula_id": "formula_2", "formula_text": "R i = {h 1 , h 2 , ..., h r }", "formula_coordinates": [5.0, 77.65, 188.8, 66.37, 7.72]}, {"formula_id": "formula_3", "formula_text": "G i = {h \u2032 1 , h \u2032 2 , ..., h \u2032 ni }", "formula_coordinates": [5.0, 77.65, 200.07, 69.92, 9.76]}, {"formula_id": "formula_4", "formula_text": "\u27e8R, G \u27e9 = {(R 1 , G 1 ), ..., (R n , G n )}", "formula_coordinates": [5.0, 77.65, 213.73, 112.87, 7.86]}, {"formula_id": "formula_5", "formula_text": "E = {(h, v h ) | \u2200h}", "formula_coordinates": [5.0, 77.65, 238.81, 59.09, 7.86]}, {"formula_id": "formula_6", "formula_text": "T = {(h, Syn(h))", "formula_coordinates": [5.0, 77.65, 251.34, 57.75, 7.57]}, {"formula_id": "formula_7", "formula_text": "(t (1) 1 , ha) (t (2) 1 , hb) (t (3) 1 , hc) . . . (t (j) i , hd) . . . {(\u03b7t, h) | \u2200h} t1 t2 t3 ti . . . . . . vt 1 vt 2 vt 3 vt i . . . . . . E h 1 h 4 h 3 h 2 h m", "formula_coordinates": [6.0, 124.12, 75.6, 359.7, 164.02]}, {"formula_id": "formula_8", "formula_text": "v h \u2190 1 s n h n h + 1 v h + 1 n h + 1 v t ,(1)", "formula_coordinates": [6.0, 231.42, 579.16, 309.25, 23.23]}, {"formula_id": "formula_9", "formula_text": "n h \u2190 n h + 1,(2)", "formula_coordinates": [6.0, 231.49, 606.98, 309.18, 9.65]}, {"formula_id": "formula_10", "formula_text": "S = {h 1 , h 2 , \u2022 \u2022 \u2022 }, then Syn(S) \u225c \u222a hi\u2208S Syn(h i ).", "formula_coordinates": [7.0, 296.04, 367.3, 193.59, 14.66]}, {"formula_id": "formula_11", "formula_text": "hit-ratio(R, G) = |R \u2229 G| min(|R|, |G|) ,(3)", "formula_coordinates": [7.0, 239.24, 534.87, 301.43, 22.31]}, {"formula_id": "formula_12", "formula_text": "#REval-hit-ratio(R, G) = 1 min(|R|, |G|) \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 \u0125\u2208R \u03c1 Syn( \u0125) \u2229 G \u0338 = \u2205 if |R| \u2264 |G| h\u2208G \u03c1 h \u2208 Syn(R) otherwise,(4)", "formula_coordinates": [7.0, 190.8, 593.69, 349.87, 62.67]}, {"formula_id": "formula_13", "formula_text": "\u03c1(s) = 1 if s = True 0 otherwise.", "formula_coordinates": [7.0, 257.61, 673.98, 95.58, 22.05]}, {"formula_id": "formula_14", "formula_text": "1: Initialize: \u03c1 \u2190 0 2: if |R| \u2264 |G| then 3:", "formula_coordinates": [8.0, 75.78, 152.94, 75.14, 29.92]}, {"formula_id": "formula_15", "formula_text": "for h \u2208 G do 17: if h \u2208 Syn(R) then 18: \u03c1 \u2190 \u03c1 + 1 19: end if 20:", "formula_coordinates": [8.0, 72.0, 309.85, 107.49, 47.97]}, {"formula_id": "formula_16", "formula_text": "Dataset D = {(t i , H i )} n i=1 A Hashtag Recommendation Model E = {(h, v h ) | \u2200h} T = {( \u0125\u2032 , Syn k ( \u0125\u2032 ) | \u2200 \u0125\u2032 \u2208 \u0124\u2032 } Recommendation R = {(t \u2032 , \u0124\u2032 ) | \u2200t \u2032 , \u0124\u2032 } Average #REval-hit-ratio k Training set {(t, H) | \u2200t, H}; Test set {t \u2032 | \u2200t \u2032 } Ground truth for the test set: G = {(t \u2032 , H \u2032 ) | \u2200t \u2032 , H \u2032 }", "formula_coordinates": [9.0, 78.98, 248.44, 451.29, 185.43]}], "doi": "10.3390/fi13050129"}
