{
  "Middleman Bias in Advertising: Aligning Relevance of Keyphrase Recommendations with Search": "Soumik Dey eBay Inc. San Jose, CA, USA sodey@ebay.com Wei Zhang eBay Inc. Shanghai, China wzhang15@ebay.com Hansi Wu eBay Inc. San Jose, CA, USA hanswu@ebay.com Bingfeng Dong eBay Inc. Shanghai, China bidong@ebay.com",
  "Abstract": "E-commerce sellers are recommended keyphrases based on their inventory on which they advertise to increase buyer engagement (clicks/sales). Keyphrases must be pertinent to items; otherwise, it can result in seller dissatisfaction and poor targeting - towards that end relevance filters are employed. In this work, we describe the shortcomings of training relevance filter models on biased click/sales signals. We re-conceptualize advertiser keyphrase relevance as interaction between two dynamical systems - Advertising which produces the keyphrases and Search which acts as a middleman to reach buyers. We discuss the bias of search relevance systems (middleman bias) and the need to align advertiser keyphrases with search relevance signals. We also compare the performance of cross encoders and bi-encoders in modeling this alignment and the scalability of such a solution for sellers at eBay.",
  "CCS Concepts": "",
  "¬∑ Information systems ‚Üí Recommender systems .": "",
  "Keywords": "Information Retrieval, Keyphrase Recommendation, Advertisement",
  "ACMReference Format:": "Soumik Dey, Wei Zhang, Hansi Wu, Bingfeng Dong, and Binbin Li. 2025. Middleman Bias in Advertising: Aligning Relevance of Keyphrase Recommendations with Search . In Companion Proceedings of the ACM Web Conference 2025 (WWW Companion '25), April 28-May 2, 2025, Sydney, NSW, Australia. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3701716. 3717858",
  "1 Introduction": "Within the landscape of e-commerce, sellers utilize online advertising mechanisms such as keyphrase recommendations to overcome low organic search placement, thereby gaining advantageous visibility on the search results page (SRP) and enhancing buyer interaction. Advertiser Keyphrase recommendation at eBay involves This work is licensed under a Creative Commons Attribution-NonCommercialShareAlike 4.0 International License. WWWCompanion '25, Sydney, NSW, Australia ¬© 2025 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-1331-6/2025/04 https://doi.org/10.1145/3701716.3717858 Binbin Li eBay Inc. San Jose, CA, USA binbli@ebay.com three primary steps: retrieval , which collects keyphrases relevant to the item; relevance , which further filters these keyphrases to ensure their pertinence to the item, and ranking , which encourages preferential seller adoption based on the seller's budget and preferences. The significance of keyphrase relevance is pivotal, as it influences seller perception and helps prevent Search systems from being inundated with numerous non-relevant items competing in auctions. Advertiser keyphrase relevance models can be trained to identify general trends in click/sales data. A keyphrase that has garnered a sufficient number of clicks or sales in relation to an item can be considered relevant to the item. In essence, clicks and sales are strong positive indicators of relevance. However, they do not serve well as indicators of irrelevance. E-Commerce data suffer from missing-not-at-random (MNAR) conditions due to several biases [2, 9, 21]. An item lacking clicks for a particular query does not necessarily mean that it is irrelevant. In the context of e-Commerce, buyers act as annotators, but unlike typical annotators, they see a skewed presentation of items influenced by search ranking, which in turn affects their annotations (clicks/sales). An unpopular item will have a poor position on the Search results page (SRP) due to rankings being based on clicks and subsequently may not obtain any clicks or sales. This bias results in the unreliability of negative relevance signals based on clicks or sales. eBay Search shows items in response to buyer queries and also involves the same tasks: retrieval , retrieving items related to the search queries; relevance , a more detailed filtering of the retrieved items to ensure conformity to the query, and ranking , i.e. ordering the retrieved items based on potential to achieve clicks so that buyers interact with them preferentially. From the seller's perspective, when eBay Advertising presents advertised keyphrases to sellers, the sellers choose to bid on them for their items. These items then enter an auction corresponding to the keyphrase. This auction process involves a complex interaction with eBay Search . During this auction, eBay Search acts as a middleman and aligns the items with the queries, which are exactly the advertised keyphrases recommended by us, and further filters the auction items using their relevance filter. This auction mechanism also ensures that the click data logged will only contain keyphrases which pass Search's relevance filter (auction winning impression gaining keyphrases are logged). This introduces an additional bias - training will only be on keyphrases which pass the Search relevance filter . Training on WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia Soumik Dey, Wei Zhang, Hansi Wu, Bingfeng Dong, and Binbin Li (i) Buyer side bay Phone S144.99t0 S299.99 Ico Rated Plus Buyknow Decker Electric Mower Safety Nccathe US 535.00 (ii) Seller Side Figure 1: Screenshot of our keyphrases for manual targeting in Promoted Listings Priority TM [6] for eBay Advertising. click data ensures that the model never experiences keyphrases that are irrelevant to Search, while Advertising does generate such keyphrases and needs to filter them. We coin this bias as middleman bias a form of sample selection bias [7, 18], which the click data additionally suffers from, in the context of advertiser keyphrase recommendation. The interaction between Advertising and Search displays an inherent imbalance, as Search ultimately determines which keyphrases are deemed relevant. For instance, take Itm1 from Figure 2; Advertising considers KP1 and KP4 irrelevant, while eBay Search regards KP2 , KP3 , and KP5 as irrelevant, leading to Itm1 not participating in any auctions. This outcome occurs regardless of the true relevance of the keyphrases, meaning that even if KP2 , KP3 , and KP5 are genuinely relevant, Itm1 will not enter those auctions because Search does not find them relevant . Therefore, it is prudent to align the relevance filter of Advertising with that of Search, which acts as a middleman in this context. In scenarios with optimal outcomes, such as Itm3 and Itm5 , there is absolute agreement between Advertising and Search. As discussed, Advertising requires debiasing the click-based performance data, while still filtering out keyphrases deemed irrelevant by Search. We thus redefine the relevance of keyphrase recommendations as a complex interaction between Advertising Figure 2: Auctions of items (Itm) in relation to keyphrases (KP). Red strikethrough font represents filter of Advertising while gray highlight represents that of Search. Both gray highlight and red-strikethrough represents kephrases eliminated by both Advertising and Search filters. Itmnl KPZ KP 3 KP KP5 KPZ KP3 KP KP5 Itmz KP3 KPA KP5 KP KPZ KP3 KP4 KPI KP? KP3 KPI KP? KP3 KA KPI KP? KP3 KP4 KP5 KPI KP? KP3 KP4 Itmn5 KPI KPZ KP3 KPA KPS KPI KP2 KP3 KPA KPS and Search and propose a scalable solution to align these systems - reminiscent of alignment of dependent systems in cascade ranking [14].",
  "2 Previous Experiments and Learnings": "In this section, we discuss some previous experiments and their shortcomings and describe our learnings from them. A previous BERT model was trained on human judgment (about 80,000 records with 3 annotators). However, after the launch of the model, we received multiple complaints from sellers about irrelevant keyphrases. We had to pull the model from production and a post launch analysis revealed that due to the large number of categories at eBay; some categories had inadequate representation in this dataset of only 80,000 records. In addition, the data suffered from low agreement between the annotators, making it difficult to train on the data. 1 Wealso experimented with an xgboost-based CTR model trained on click data. Offline evaluation of the CTR model failed in terms of search alignment and spot-checked human judgement. An alternate model based on the Jaccard index [11] proved better and was released to production. The Jaccard index was a simple ruledriven token-based algorithm with a threshold to filter out irrelevant keyphrases. This worked, since token similarity is an important feature for relevance. Notably, the CTR model had Jaccard index as a feature but still underperformed. Further analysis indicated that the CTR model was significantly biased, lacking representation from item-query pairs that failed the search relevance filter (required to stay in the auction), and carried forward the described e-Commerce data biases described in Section 1. Jaccard index defined as the token intersection over union has its ownproblems. Since item titles are generally longer than keyphrases, it heavily penalizes shorter keyphrases - which are generally head keyphrases (head keyphrases are keyphrases which drive most clicks/sales, at eBay 10% of keyphrases drive 90% of buyer engagement in clicks/sales). The introduction of the jaccard filter essentially eliminated single-token keyphrases, which were a major source of advertisement for sellers. Additionally, jaccard not being semantically aware; would miss targeting opportunities on those keyphrases semantically aligned with the item. Jaccard is easy to 1 The diversity of eBay items across more than 100,000 categories makes it necessary to collect a lot more than 80,000 records to ensure there is proper representation. Middleman Bias in Advertising: Aligning Relevance of Keyphrase Recommendations with Search WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia implement and integrate in eBay systems; however, it is not the most accurate relevance model and had been adopted as a stop gap.",
  "3 Data Curation and Experiment Design": "Reminiscent of other methods of debiasing by curating data from additional sources [7], to circumvent all the problems described in Section 1 we use a dataset of Search's relevance judgment on our Advertisement recommendations to mitigate middleman bias in Advertising. This ensures that our model does not suffer from the same biases as the click data, while at the same time ensuring that we align with Search's relevance judgment for the most optimal outcomes in auctions for our keyphrase recommendations. We curated a dataset of 24 million and 3 million item-keyphrase pairs with labels 1 and 0 indicating the pass / fail of Search's relevance judgment for training and evaluation, respectively. The data is stratified by categories based on their activity in terms of traffic. This huge data guarantees proper representation of all categories while still maintaining preferential bias of models towards high performing categories [13]. The problem we are trying to solve for is - given an item title, its category information, and the keyphrase associated, can we predict whether the keyphrase would be deemed relevant/irrelevant by Search? For our item data, we take into account the title of the item and the category name . Previous work in [12, 13] has shown how categorical information enriches item data. In terms of representing textual information and modeling for a classification (NLU/NLI); encoders have been the popular choice. The two most popular frameworks for generating text representations using encoders are: 1) bi-encoders , which separately computes the embeddings of item ( ùë¢ ) and the keyphrase ( ùë£ ) with the input of the item encoding being title [SEP] category name , and 2) cross-encoders , where embedding of item and keyphrase are processed together, the input is keyphrase [SEP] category name [SEP] title . In employing bi-encoders as a base model, we utilized the eBay pre-trained BERT model, eBERT [3] and fine-tuned using: ¬∑ Contrastive learning [8] on search relevance scores of itemkeyphrase pairs which minimizes the distance between the embeddings with label 1 and maximizes the distance between the embeddings with label 0. ¬∑ Softmax Classifier trained on concatenation and difference of item embeddings ( ùë¢ ) and keyphrase embeddings ( ùë£ ), i.e. ( ùë¢, ùë£, | ùë¢ -ùë£ |) . This has been proven to work well for NLU tasks[15]. ¬∑ In-batch Random Negative Sampling (IRNS) based on click data [10]. Since click data is biased for negative labels, inbatch-negative sampling paves the way for negative construction in an unsupervised manner from all other data points in the batch. This has been shown to often yield better results [16]. Towards that end we additionally curated a dataset of 24 million records from 30 days of search activity data with at least one click and with a lower lower bound of 0.05 CTR and a minimum of 30 impressions for click-based IRNS training. This is to ensure that we eliminate noisy data points, such as 1 impression, 1 click (accident), or 1000 impressions, 1 click (low CTR), which are misleading. The resulting data points can be construed as positives accumulated from search logs, which will then be used for in-batch negatives sampling. Since cross-encoders are expensive in terms of throughput, we experiment with cross-encoders of two sizes bert-mini 2 with 4 layers and hidden size of 256 and bert-tiny 3 with 2 layers and hidden size of 128 from [1, 17]. These models are chosen because of their lower inference latency; more details to be discussed in Section 4. All models were trained on 4 epochs with a learning rate of 2 √ó 10 -5 with 8 A100-80GB GPUs. The bi-encoder models were trained using a per-device batch size of 384 and fp16 quantization. The bert-mini cross encoder was trained with a batch size of 10240 while the bert-tiny model was trained with a batch size of 40960. The varying batch sizes for different models are designed to maximize the GPU memory usage for each of these models. For our experiments, we evaluated all models on our task of aligning with search relevance. Both bi-encoders and cross-encoders have their pros and cons. Bi-encoder representations are generally more scalable as the embedding generation for items and keyphrases is done independently. Cross encoders, on the other hand, can become expensive for inference as we have to perform it for item-keyphrase combinations. However, previous research [15] has shown that cross encoders have a higher performance in terms of NLU and classification tasks, with the caveat of higher inference latency. For comparison, we also take into account the scalability of cross-encoder models and test cross-encoders of various sizes to account for their performance in relation to their viability. Given the trade-off between performance and scalability, we compare the models based on offline evaluation on Search agreement and select the model which has the best of both worlds. We then present the online A/B test results against the Jaccard model in production.",
  "4 Results": "We present the results of both bi-encoders and cross-encoders described in the previous section. We report results on an evaluation set of 3 million item-keyphrase pairs and their Search relevance judgements. The models are evaluated on precision, recall and F1 scores. While precision is the measure of how aligned Advertising's filtered keyphrases are with Search's relevance filter, recall measures the lost opportunity in terms of targeting potential of keyphrases unnecessarily filtered out. The F1 scores, which are a harmonic mean of precision and recall, are an indicator for the performance of both. Table 1: Offline evaluation on search relevance agreement and the Diff and Full Batch inference latencies in hours. Darker colors represent more favorable scores. 2 https://huggingface.co/prajjwal1/bert-mini 3 https://huggingface.co/prajjwal1/bert-tiny WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia Soumik Dey, Wei Zhang, Hansi Wu, Bingfeng Dong, and Binbin Li Figure 3: Production Serving Architecture for our retrieval and relevance models. Feature Store Online Keyword Reco Service Enrichment KW Enrichment Service Recall A Item Flink New/Revise Windowl Filter Inference KW Relevance Event Service Recall B Filter Upsert KW Recall ‚Ç¨ Store Service Near Real Time Batch KV Store Offline to Online Offline Pipeline Upload KW Recall A Preprocess KW Relevance HDFS Filter HDFS Recall B KW Recall ‚Ç¨ Fromthe results in Table 1 we can infer that of the bi-encoders the most successful one is the contrastive loss trained bi-encoder. This beats the IRNS and the Softmax trained bi-encoders by a significant margin in the F1 scores. For the bi-encoders of IRNS and Constrative we see a very high recall. For cross-encoders we see that the best performance is achieved by bert-mini , beating all the models in their precision and F1 scores, with a very high recall as well. Our smallest model bert-tiny has the 2nd highest precision and F1 score of 0.74 and 0.80 respectively. This shows that even the smallest cross-encoders can beat the relatively more complex bi-encoders. We also see that Contrastive Loss is a better option in training the bi-encoders for the relevance computation than Softmax and IRNS. Bi-encoders and cross-encoders have different serving contracts - bi-encoders perform inference for item and keyphrases independently while cross-encoders work on the combination of the two. Our production architecture illustrated in Figure 3 features two components Near Real-Time (NRT) Inference and Batch Inference. Batch inference primarily serves items with a delay, whereas NRT serves items on an urgent basis, such as items newly created or revised by sellers. The batch inference is done in two parts: 1) for all items and keyphrases, and 2) daily differential (Diff), i.e. the difference of all new items and keyphrases created/revised and then merged with the existing ones. The NRT inference for our relevance model is written into the service code using DeepJava [4] and onnx serving. The NRT service is triggered by the event of creation or revision, behind a Flink processing window and feature enrichment. The full batch inference is around 2.5 billion items, 1 billion keyphrases and around 100 billion item-keyphrase pairs, while for daily differential we serve around 20 million items, 3 million keyphrases and around 7 billion item-keyphrase pairs. Since full batch inference has to be carried out once, the Diff inference latency is what decides if we can put a model in production (we still provide the numbers in Table 1). The Diff inference latency for bi-encoders is 1 hour while for bert-tiny and bert-mini it is 1.5 and 2.5 hours, respectively. All latency numbers are benchmarked using PySpark [20] (1000 executors, 20g memory and 4 cores) with transformers [19] and onnxruntime [5] libraries. While GPU options were considered, they proved to be unsustainable due to their cost. Taking into account our serving requirements, the bert-tiny model, which has a overall decent performance, was chosen for release. In terms of latency, it matches the bi-encoders and offers quality close to that of the top-performing bert-mini model. The bert-tiny model was released into production following an A/B test against the Jaccard model in production. The bert-tiny model delivered on all fronts with impressions, clicks, bought items, CVR or conversion rate (bought items / click) increasing by 20.17%, 13.01%, 26.04%, 11.6%, respectively (all with p-value less than 0.05) while CTR or click-through rate (clicks / impressions) remained neutral (p-value=0.17). Overall, we reduced the number of keyphrases surfaced to sellers by 7% and increased the efficacy of our recommendations, as is evident in these numbers. A follow-up analysis showed that the bert-tiny model decreased the False Positives (i.e., Search irrelevant keyphrases that passed the Advertising filter) and also the False Negatives (Search relevant that were filtered out) Middleman Bias in Advertising: Aligning Relevance of Keyphrase Recommendations with Search WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia while increasing the True Positives (Search relevant keyphrases that pass the Advertising filter). Of the False Negatives that were reduced by bert-tiny , a large portion of them were shorter head keyphrases, which are very desirable for advertisers due to their targeting potential.",
  "5 Conclusions": "In this study, we discuss the limitations of using clicks solely as a training signal and introduce the concept of middleman bias in the context of relevance of advertiser keyphrase recommendations. We reinterpret this task as a complex interaction between two dynamical systems - Advertising, which suggests keyphrases; and Search, which manages the items entering the auction for these keyphrases. Our findings indicate an imbalance in this interaction, with Search ultimately determining relevance of items to keyphrases, thus underscoring the importance of alignment with Search. This research also offers a comparative study of two well-known encoder-based frameworks in modeling this alignment: bi-encoders and crossencoders of varying sizes, training objectives, and losses. We show that even very simple 2-layer cross-encoders significantly outperform more intricate bi-encoders in relevance modeling. Additionally, we highlight that while bi-encoders are considerably faster, utilizing lighter cross-encoders can narrow the latency difference and provide better outcomes.",
  "References": "[1] Prajjwal Bhargava, Aleksandr Drozd, and Anna Rogers. 2021. Generalization in NLI: Ways (Not) To Go Beyond Simple Heuristics. arXiv:2110.01518 [cs.CL] [2] Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan He. 2023. Bias and Debias in Recommender System: A Survey and Future Directions. ACM Trans. Inf. Syst. 41, 3, Article 67 (Feb. 2023), 39 pages. doi:10.1145/3564284 [3] Leonard Dahlmann and Tomer Lancewicki. 2021. Deploying a BERT-based QueryTitle Relevance Classifier in a Production System: a View from the Trenches. arXiv:2108.10197 [cs.CL] https://arxiv.org/abs/2108.10197 [4] Deepjavalibrary. [n. d.]. Deepjavalibrary/djl: An engine-agnostic deep learning framework in Java. https://github.com/deepjavalibrary/djl [5] ONNX Runtime developers. 2021. ONNX Runtime. https://onnxruntime.ai/. Version: 1.20.1. [6] eBay Advertising. 2025. Promoted Listings Priority. https://www.ebayadvertising. com/en/ad-solutions/ad-types/promoted-listings-priority/ [7] Jingyue Gao, Shuguang Han, Han Zhu, Siran Yang, Yuning Jiang, Jian Xu, and Bo Zheng. 2023. Rec4Ad: A Free Lunch to Mitigate Sample Selection Bias for Ads CTR Prediction in Taobao. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management (Birmingham, United Kingdom) (CIKM '23) . Association for Computing Machinery, New York, NY, USA, 4574-4580. doi:10.1145/3583780.3615496 [8] R. Hadsell, S. Chopra, and Y. LeCun. 2006. Dimensionality Reduction by Learning an Invariant Mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) , Vol. 2. 1735-1742. doi:10.1109/CVPR. 2006.100 [9] Jin Huang, Harrie Oosterhuis, Maarten de Rijke, and Herke van Hoof. 2020. Keeping Dataset Biases out of the Simulation: A Debiased Simulator for Reinforcement Learning based Recommender Systems. In Proceedings of the 14th ACM Conference on Recommender Systems (Virtual Event, Brazil) (RecSys '20) . Association for Computing Machinery, New York, NY, USA, 190-199. doi:10.1145/3383313.3412252 [10] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2021. Unsupervised Dense Information Retrieval with Contrastive Learning. doi:10.48550/ARXIV.2112.09118 [11] Paul Jaccard. 1901. √âtude comparative de la distribution florale dans une portion des Alpes et des Jura. Bull Soc Vaudoise Sci Nat 37 (1901), 547-579. [12] Ashirbad Mishra, Soumik Dey, Marshall Wu, Jinyu Zhao, He Yu, Kaichen Ni, Binbin Li, and Kamesh Madduri. 2024. GraphEx: A Graph-based Extraction Method for Advertiser Keyphrase Recommendation. arXiv:2409.03140 [cs.IR] https://arxiv.org/abs/2409.03140 [13] Ashirbad Mishra, Soumik Dey, Jinyu Zhao, Hansi Wu, Binbin Li, and Kamesh Madduri. 2024. Graphite: A Graph-based Extreme Multi-Label Short Text Classifier for Keyphrase Recommendation. In 27th European Conference on Artificial Intelligence, 19-24 October 2024, Santiago de Compostela, Spain - Including 13th Conference on Prestigious Applications of Intelligent Systems (PAIS 2024) (Frontiers in Artificial Intelligence and Applications, Vol. 392) . IOS Press, 4657 - 4664. https://doi.org/10.3233/FAIA241061 [14] Jiarui Qin, Jiachen Zhu, Bo Chen, Zhirong Liu, Weiwen Liu, Ruiming Tang, Rui Zhang, Yong Yu, and Weinan Zhang. 2022. RankFlow: Joint Optimization of MultiStage Cascade Ranking Systems as Flows. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (Madrid, Spain) (SIGIR '22) . Association for Computing Machinery, New York, NY, USA, 814-824. doi:10.1145/3477495.3532050 [15] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (Eds.). Association for Computational Linguistics, Hong Kong, China, 3982-3992. doi:10.18653/v1/D19-1410 [16] Nils Reimers and Iryna Gurevych. 2020. Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, Online, 4512-4525. doi:10.18653/v1/2020.emnlp-main.365 [17] Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. WellRead Students Learn Better: The Impact of Student Initialization on Knowledge Distillation. CoRR abs/1908.08962 (2019). arXiv:1908.08962 http://arxiv.org/abs/ 1908.08962 [18] Francis Vella. 1998. Estimating Models with Sample Selection Bias: A Survey. The Journal of Human Resources 33, 1 (1998), 127-169. http://www.jstor.org/ stable/146317 [19] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R√©mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-Art Natural Language Processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations . Association for Computational Linguistics, Online, 38-45. https://www.aclweb.org/anthology/2020.emnlpdemos.6 [20] Matei Zaharia, Reynold S. Xin, Patrick Wendell, Tathagata Das, Michael Armbrust, Ankur Dave, Xiangrui Meng, Josh Rosen, Shivaram Venkataraman, Michael J. Franklin, Ali Ghodsi, Joseph Gonzalez, Scott Shenker, and Ion Stoica. 2016. Apache Spark: a unified engine for big data processing. Commun. ACM 59, 11 (Oct. 2016), 56-65. doi:10.1145/2934664 [21] Ranran Haoran Zhang, Bensu U√ßar, Soumik Dey, Hansi Wu, Binbin Li, and Rui Zhang. 2025. From Lazy to Prolific: Tackling Missing Labels in Open Vocabulary Extreme Classification by Positive-Unlabeled Sequence Learning. arXiv:2408.08981 [cs.IR] https://arxiv.org/abs/2408.08981",
  "keywords_parsed": [
    "Information Retrieval",
    "Keyphrase Recommendation",
    "Advertisement"
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "Generalization in NLI: Ways (Not) To Go Beyond Simple Heuristics"
    },
    {
      "ref_id": "b2",
      "title": "Bias and Debias in Recommender System: A Survey and Future Directions"
    },
    {
      "ref_id": "b3",
      "title": "Deploying a BERT-based QueryTitle Relevance Classifier in a Production System: a View from the Trenches"
    },
    {
      "ref_id": "b4",
      "title": "Deepjavalibrary/djl: An engine-agnostic deep learning framework in Java"
    },
    {
      "ref_id": "b5",
      "title": "ONNX Runtime"
    },
    {
      "ref_id": "b6",
      "title": "Promoted Listings Priority"
    },
    {
      "ref_id": "b7",
      "title": "Rec4Ad: A Free Lunch to Mitigate Sample Selection Bias for Ads CTR Prediction in Taobao"
    },
    {
      "ref_id": "b8",
      "title": "Dimensionality Reduction by Learning an Invariant Mapping"
    },
    {
      "ref_id": "b9",
      "title": "Keeping Dataset Biases out of the Simulation: A Debiased Simulator for Reinforcement Learning based Recommender Systems"
    },
    {
      "ref_id": "b10",
      "title": "Unsupervised Dense Information Retrieval with Contrastive Learning"
    },
    {
      "ref_id": "b11",
      "title": "√âtude comparative de la distribution florale dans une portion des Alpes et des Jura"
    },
    {
      "ref_id": "b12",
      "title": "GraphEx: A Graph-based Extraction Method for Advertiser Keyphrase Recommendation"
    },
    {
      "ref_id": "b13",
      "title": "Graphite: A Graph-based Extreme Multi-Label Short Text Classifier for Keyphrase Recommendation"
    },
    {
      "ref_id": "b14",
      "title": "RankFlow: Joint Optimization of MultiStage Cascade Ranking Systems as Flows"
    },
    {
      "ref_id": "b15",
      "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"
    },
    {
      "ref_id": "b16",
      "title": "Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation"
    },
    {
      "ref_id": "b17",
      "title": "WellRead Students Learn Better: The Impact of Student Initialization on Knowledge Distillation"
    },
    {
      "ref_id": "b18",
      "title": "Estimating Models with Sample Selection Bias: A Survey"
    },
    {
      "ref_id": "b19",
      "title": "Transformers: State-of-the-Art Natural Language Processing"
    },
    {
      "ref_id": "b20",
      "title": "Apache Spark: a unified engine for big data processing"
    },
    {
      "ref_id": "b21",
      "title": "From Lazy to Prolific: Tackling Missing Labels in Open Vocabulary Extreme Classification by Positive-Unlabeled Sequence Learning"
    }
  ]
}