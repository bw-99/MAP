{
  "Efficient High-Quality Clustering for Large Bipartite Graphs": "",
  "Renchi Yang": "",
  "Jieming Shi ∗": "Hong Kong Baptist University Hong Kong SAR, China renchi@hkbu.edu.hk",
  "ABSTRACT": "A bipartite graph contains inter-set edges between two disjoint vertex sets, and is widely used to model real-world data, such as user-item purchase records, author-article publications, and biological interactions between drugs and proteins. 𝑘 -Bipartite Graph Clustering ( 𝑘 -BGC) is to partition the target vertex set in a bipartite graph into 𝑘 disjoint clusters. The clustering quality is important to the utility of 𝑘 -BGC in various applications like social network analysis, recommendation systems, text mining, and bioinformatics, to name a few. Existing approaches to 𝑘 -BGC either output clustering results with compromised quality due to inadequate exploitation of high-order information between vertices, or fail to handle sizable bipartite graphs with billions of edges. Motivated by this, this paper presents two efficient 𝑘 -BGC solutions, HOPE and HOPE+, which achieve state-of-the-art performance on large-scale bipartite graphs. HOPE obtains high scalability and effectiveness through a new 𝑘 -BGC problem formulation based on the novel notion of high-order perspective (HOP) vectors and an efficient technique for low-rank approximation of HOP vectors. HOPE+ further elevates the 𝑘 -BGC performance to another level with a judicious problem transformation and a highly efficient two-stage optimization framework. Two variants, HOPE+ (FNEM) and HOPE+ (SNEM) are designed when either the Frobenius norm or spectral norm is applied in the transformation. Extensive experiments, comparing HOPE and HOPE+ against 13 competitors on 10 real-world datasets, exhibit that our solutions, especially HOPE+, are superior to existing methods in terms of result quality, while being up to orders of magnitude faster. On the largest dataset MAG with 1.1 billion edges, HOPE+ is able to produce clusters with the highest clustering accuracy within 31 minutes, which is unmatched by any existing solution for 𝑘 -BGC.",
  "CCS CONCEPTS": "· Information systems → Clustering ; · Computing methodologies → Cluster analysis ; · Mathematics of computing → Computations on matrices .",
  "KEYWORDS": "Bipartite Graph, Clustering, Random Walk, Eigenvector",
  "1 INTRODUCTION": "Abipartite graph G contains two disjoint vertex sets, U and V , with only inter-set edges. Bipartite graphs are omnipresent in the real world to model the purchase/view records between customers and products, the publication relationships between authors and articles, the biological interactions between drugs and protein complexes, and the ecological relationships between plants and pollinators. ∗ Corresponding author. Hong Kong Polytechnic University Hong Kong SAR, China jieming.shi@polyu.edu.hk Afundamental task in data mining is 𝑘 -Bipartite Graph Clustering (hereafter 𝑘 -BGC) which aims to group the vertices in the target vertex set, e.g. , U , into 𝑘 disjoint clusters, each of which is tightlyknit, based on the interplay between the two types of vertices U and V in the bipartite graph G (the clustering on the other set V naturally follows). 𝑘 -BGC finds prevalent use in various fields, including social network analysis, recommendation systems, text mining, bioinformatics, and so forth. Practical applications include clustering customers based on customers' purchasing patterns [25, 59], categorizing documents based on document-word associations [11, 12], detecting groups of genes suitable for drug repurposing in gene-drug networks [34], and many others [10, 14, 16, 28, 38, 46, 66]. One simple treatment for 𝑘 -BGC is to regard a bipartite graph as a unipartite graph, and then apply canonical graph clustering methods, e.g. , [20, 39, 55], to get clustering results. However, this brute-force approach overlooks the unique topological properties of bipartite graphs with two disjoint vertex sets, and thus, yields sub-par clustering quality. As reviewed in Section 6, there exists a plethora of studies with dedicated techniques developed particularly for bipartite graph clustering under various settings [8] (see references therein). In particular, one popular methodology, projection-based methods [23, 36, 52], first projects the input bipartite graph G into a unipartite graph containing the same-typed vertices via the one-mode projection [70, 71], and then applies conventional graph clustering methods over the projected graph for vertex clustering. These methods often generate extremely dense projected graphs with 𝑂 (|U| 2 ) or 𝑂 (|V| 2 ) edges in the worst case [62], as shown in Figure 1(a) with a extremely dense projected graph created due to the connections from all users to the phone, rendering them inefficient. Another category of techniques extends spectral clustering [12, 31] or statistical models (e.g., stochastic block models) [32, 67] to simultaneously group vertices of both types in bipartite graphs. To our knowledge, none of these methods effectively exploit the high-order affinities between vertices (vertices can reach each other through multiple hops along the edges), which intuitively is critical for deriving high-quality clustering results and has been shown effective in unipartite graph clustering [43, 65]. Consequently, most of them compromise cluster quality. To illustrate, we consider the customer-product purchase network in Figure 1(b). Suppose that there are two clusters to discover, i.e. , 𝑘 = 2. These methods may assign user D to the r.h.s cluster {E, F, G}, since doing so leads to a good graph partition with solely one cross-cluster edge (in purple), and they cannot discern the strong high-order affinity between users D and A, B, C. But, intuitively, user D is more likely to belong to the l.h.s cluster {A, B, C}, given that user D has as many as nine 3-hop paths to the basketball, football, and game console as users A, B, and C do, reflecting their similar tastes to the products. By contrast, user D possesses only one 3-hop path to either book 1 Renchi Yang and Jieming Shi Figure 1: Limitations of Existing Approaches (b) Overlook High-Order Affinities (a) Extremely Dense Projected Graphs A       B      C        D       E         F       G or skirt and zero 3-hop path to stilettos purchased by users E, F and G, which indicates that they have utterly divergent shopping preferences and should be grouped into disparate clusters. Ontop of that, existing solutions incur prohibitive computational overheads, and are hard to scale to large-scale bipartite graphs with millions of nodes and billions of edges. For instance, on an MAG bipartite graph with 1.1 billion edges, almost all competitors fail to finish clustering within 2 days in experiments. An existing strong approach, biSBM-MCMC [67] takes over one week to process MAG . Summing up, existing 𝑘 -BGC methods suffer from sub-optimal clustering quality, entail tremendous computational overheads and largely fail over massive bipartite graphs, or both. To tackle the above challenges, we propose HOPE and HOPE+ that achieve superior performance for 𝑘 -BGC, via a series of novel algorithmic designs and theoretical analysis over newly designed notion High-Order Perspective VEctors. Given a bipartite graph G with the target vertex set U to be clustered, it is crucial but challenging to capture the rich topological features for effective clustering. To this end, we first construct a weighted projected graph (WPG) GV over the other vertex set V of G with a new weighting scheme. Intuitively, if two vertices 𝑢 𝑖 and 𝑢 𝑗 in U have different (resp. similar) perspectives to WPG GV , they tend to be in different clusters (resp. the same cluster). To realize this, we define a high-order perspective (HOP) per vertex 𝑢 𝑖 ∈ U via a random walk model from 𝑢 𝑖 towards the WPG GV , so as to quantify the personalized high-order semantics from 𝑢 𝑖 to all vertices V in GV . Based thereon, we formulate a new 𝑘 -BGC clustering objective defined using the HOP vectors of vertices in U . The direct materialization of all HOP vectors for U is rather costly, as they constitute a dense HOP matrix H ∈ R | U|×|V| whose calculation involves an infinite series of matrix multiplications with 𝑂 (|U| × |V| 2 ) time. To mitigate the issue, we first present a base method HOPE that builds up a low-rank approximation of H with theoretical accuracy guarantees, without explicitly materializing H , and subsequently conducts clustering using the low-rank approximation as per our proposed objective. HOPE can achieve improved result quality, while being fast, compared with existing methods. Furthermore, we propose HOPE+ that surpasses HOPE and existing methods in terms of both efficiency and effectiveness. Under the hood, through rigorous theoretical analysis, HOPE+ converts our HOP-based 𝑘 -BGC objective into two optimization problems to be solved via our two-stage algorithmic framework. The first problem is optimized by a certain relaxed intermediate result obtained by an efficient partial eigen-decomposition; then in the second optimization, HOPE+ derives the final cluster result by minimizing its difference to the intermediate result via two rounding algorithms, namely FNEM and SNEM, when either Frobenius norm or spectral norm is applied to quantify the difference. The complexities of HOPE, HOPE+ (FNEM), and HOPE+ (SNEM) are all linear to the size of the input bipartite graph. We empirically evaluate HOPE and HOPE+ against 13 competitors on 10 real datasets with ground-truth cluster labels. Extensive experiments exhibit that HOPE and HOPE+ obtain superior clustering quality on most datasets while being up to orders of magnitude faster. Particularly, on the billion-edge dataset MAG , HOPE+ with SNEM achieves a 22.1% conspicuous improvement in clustering accuracy and at least 9 . 5 × speedup over existing methods. To summarize, our contributions in this paper are as follows: · We formulate an effective 𝑘 -BGC objective based on the new High-Order Perspective (HOP) vectors that preserve vertex-specific high-order information over a weighted projected graph. · We present HOPE that efficiently solves the objective via lowrank approximation, without materializing all HOP vectors, to output high-quality clustering results. · We further develop HOPE+ that transforms the 𝑘 -BGC objective into two optimization problems that are solved by efficient technical designs. We present two variants HOPE+ (FNEM) and HOPE+ (SNEM), when Frobenius norm or spectral norm is applied. · Extensive experiments on 10 bipartite graphs validate the superiority of our methods to efficiently obtain high-quality clusters.",
  "2 PROBLEM FORMULATION": "",
  "2.1 Notations": "Let G = (U∪V , E) be a weighted bipartite graph, where U and V are two disjoint vertex sets, and the edge set is E = {( 𝑢 𝑖 , 𝑣 𝑗 ) | 𝑢 𝑖 ∈ U , 𝑣 𝑗 ∈ V} . Every edge ( 𝑢 𝑖 , 𝑣 𝑗 ) ∈ E is associated with a nonnegative weight 𝑤 ( 𝑢 𝑖 , 𝑣 𝑗 ) . The set of neighbors ( i.e. , adjacent vertices) of a vertex 𝑢 𝑖 ∈ U (resp. 𝑣 𝑗 ∈ V ) is denoted by N( 𝑢 𝑖 ) = { 𝑣 𝑗 | ( 𝑢 𝑖 , 𝑣 𝑗 ) ∈ E} (resp. N( 𝑣 𝑗 ) = { 𝑢 𝑖 | ( 𝑢 𝑖 , 𝑣 𝑗 ) ∈ E} ). In this paper, matrices are denoted by bold uppercase letters, e.g. , M ∈ R 𝑛 × 𝑚 with 𝑛 rows and 𝑚 columns. The 𝑖 -th row (resp. 𝑗 -th column) of M is a length𝑚 (resp. length𝑛 ) vector, written as M 𝑖 (resp. M ∗ , 𝑗 ), and the entry at the 𝑖 -th row and 𝑗 -th column of M is denoted by M 𝑖,𝑗 . The Frobenius norm of matrix M is ∥ M ∥ 𝐹 = √︃ ˝ 𝑛 𝑖 = 1 ˝ 𝑚 𝑗 = 1 | M 𝑖,𝑗 | 2 , and the 𝐿 2 norm of vector M 𝑖 is ∥ M 𝑖 ∥ 2 = √︃ ˝ 𝑚 𝑗 = 1 | M 𝑖,𝑗 | 2 . The trace of a square matrix M ∈ R 𝑚 × 𝑚 ⊤ is Tr ( M ) = ˝ 𝑚 𝑖 = 1 M 𝑖,𝑖 . We use M to represent the transpose of M and I to represent the identity matrix with dimension implied by the context. A matrix M is said to have orthogonal columns (resp. rows) if it satisfies M ⊤ M = I (resp. MM ⊤ = I ). Table 1 lists the frequently used notations throughout this paper.",
  "2.2 𝑘 -BGC": "Given a weighted bipartite graph G = (U∪V , E) , the target vertex set to cluster (either U or V ), and the number 𝑘 of clusters, the goal of 𝑘 -Bipartite Graph Clustering ( 𝑘 -BGC) is to partition the target vertex set into 𝑘 disjoint vertex clusters, 𝐶 1 , 𝐶 2 , ..., 𝐶 𝑘 , such that the vertices in the same cluster are closely connected in G via direct or indirect connections, whereas the vertices across clusters are distant from each other [23, 32, 67]. By default, we regard U as the target vertex set to cluster. It naturally follows when V is the target. Choosing 𝑘 is not a focus of this paper, and can be done by an additional step [7]. A critical challenge for 𝑘 -BGC is to leverage the rich semantics hidden in the bipartite graph topology to obtain high-quality 2 Efficient High-Quality Clustering for Large Bipartite Graphs Table 1: Frequently used symbols. features of vertices for effective clustering. In the following subsections, we propose a new way to formulate the 𝑘 -BGC objective. In a nutshell, we first construct a weighted projected graph (WPG) GV built over the counterparty V in Section 2.3, and then, for every vertex 𝑢 𝑖 in U , we define a high-order perspective (HOP) vector (Section 2.4), which captures the personalized multi-hop information from 𝑢 𝑖 over the WPG GV , and then formulate our 𝑘 -BGC objective based on the HOP vectors of U (Section 2.5). Figure 2: Construction of HOP Vectors 𝐇1 𝐇2 𝐇3 𝐇4 𝐇5 HOPVectors Bipartite Graph 𝓖 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒖𝟏 𝒖𝟐 𝒖𝟑 𝒖𝟒 𝒖𝟓 𝒖𝟏 𝒖𝟐 𝒖𝟑 𝒖𝟒 𝒖𝟓 𝒖𝟏 0.71   0.08   0.7    0.08 0.71   0.08   0.7    0.08 0.03   0.98  0.11   0.13 0.15   0.59  0.54   0.59 0.03   0.13  0.11   0.98 𝓤 𝓥 𝒢𝒱 Perspectives to WPG 𝒢𝒱 𝒖𝟑 𝒖𝟐 𝒖𝟒 𝒖𝟓 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 Figure 2 exemplifies the generation of HOP vectors. In the first place, we construct the WPG GV for all vertices in V on the basis of the topology surrounding each 𝑣 𝑗 in V of the input bipartite graph G . Thereafter, each vertex 𝑢 𝑖 in U is represented by an HOP vector computed as per a proposed random walk model over WPG GV from 𝑢 𝑖 's perspective. Intuitively, two vertices ( e.g. , 𝑢 1 and 𝑢 2 in Figure 2) with similar perspectives towards all vertices in the WPG GV tend to have similar HOP vectors, while 𝑢 1 and 𝑢 3 are with relatively different HOP vectors since their perspectives ( i.e. , connections) towards WPG GV are different. Vertices with similar HOP vectors intuitively tend to be in the same cluster.",
  "2.3 Weighted Projected Graph": "Inspired by the foregoing intuition, we construct a weighted projected graph (WPG) that is a unipartite graph GV solely including all vertices in V via a projection over the input bipartite graph G . Specifically, we build the WPG GV with the edge weight matrix W V by a new edge-weighting scheme applied together with the one-mode projection [70]. To facilitate the design, we define the transition probabilities 𝑝 ( 𝑢 𝑖 , 𝑣 𝑗 ) from vertex 𝑢 𝑖 to vertex 𝑣 𝑗 , and 𝑝 ( 𝑣 𝑗 , 𝑢 𝑖 ) from 𝑣 𝑗 to 𝑢 𝑖 on G as follows. Probability 𝑝 ( 𝑢 𝑖 , 𝑣 𝑗 ) (resp. 𝑝 ( 𝑣 𝑗 , 𝑢 𝑖 ) ) connotes the one-hop jump probability of a random walk starting from 𝑢 𝑖 to 𝑣 𝑗 (resp. from 𝑣 𝑗 to 𝑢 𝑖 ).  For every two vertices 𝑣 𝑗 , 𝑣 𝑙 ∈ V , we connect them in GV via an edge ( 𝑣 𝑗 , 𝑣 𝑙 ) if both of them are connected to the same vertices in U of G , i.e. , N( 𝑣 𝑗 ) ∩ N( 𝑣 𝑙 ) ≠ ∅ . The edge weight 𝑤 V( 𝑣 𝑗 , 𝑣 𝑙 ) is obtained as follows. On the input bipartite graph G , apparently it requires a two-hop jump via a certain vertex 𝑢 𝑖 ∈ N( 𝑣 𝑗 ) ∩ N ( 𝑣 𝑙 ) to jump from 𝑣 𝑗 to 𝑣 𝑙 with probability 𝑝 ( 𝑣 𝑗 , 𝑢 𝑖 ) · 𝑝 ( 𝑢 𝑖 , 𝑣 𝑙 ) . Analogously, it needs a two-hop jump with probability 𝑝 ( 𝑣 𝑙 , 𝑢 𝑖 ) · 𝑝 ( 𝑢 𝑖 , 𝑣 𝑗 ) from 𝑣 𝑙 to 𝑣 𝑗 via 𝑢 𝑖 . Accordingly, the probability 𝑞 ( 𝑣 𝑗 , 𝑢 𝑖 , 𝑣 𝑙 ) that the two two-hop jumps both transit via the same vertex 𝑢 𝑖 can be calculated by multiplying the two probabilities above, i.e. , 𝑝 ( 𝑣 𝑗 , 𝑢 𝑖 ) · 𝑝 ( 𝑢 𝑖 , 𝑣 𝑙 ) . Since 𝑣 𝑗 and 𝑣 𝑙 mayshare multiple common neighbors 𝑢 𝑖 ∈ N( 𝑣 𝑗 )∩ N( 𝑣 𝑙 ) of G , we aggregate all such probabilities 𝑞 ( 𝑣 𝑗 , 𝑢 𝑖 , 𝑣 𝑙 ) for 𝑢 𝑖 ∈ U together, resulting in the edge weight 𝑤 V( 𝑣 𝑗 , 𝑣 𝑙 ) of edge ( 𝑣 𝑗 , 𝑣 𝑙 ) in the WPG GV , as shown in Eq. (2). Notice that a square root operator is applied for taking a geometric mean of 𝑞 ( 𝑣 𝑗 , 𝑢 𝑖 , 𝑣 𝑙 ) and 𝑞 ( 𝑣 𝑙 , 𝑢 𝑖 , 𝑣 𝑗 ) and meanwhile alleviating the issue of small values.  Notice that Eq. (2) can be reorganized as follows:  Let Q ∈ R | V | × | U | be a matrix where the ( 𝑗, 𝑖 ) -th element is Q 𝑗,𝑖 = √︁ 𝑝 ( 𝑣 𝑗 , 𝑢 𝑖 ) · 𝑝 ( 𝑢 𝑖 , 𝑣 𝑗 ) if ( 𝑢 𝑖 , 𝑣 𝑗 ) ∈ E , and 0 otherwise. Then Eq. (3) can be re-written as  and accordingly the edge weight matrix W V of GV is W V = QQ ⊤ . Figure 3: Example 𝑝 ( 𝑢 𝑖 , 𝑣 𝑗 ) and Q (𝒖𝒊 , 𝒗 𝒋 ) 𝒑(𝒖𝒊, 𝒗 𝒋 ) 𝒑(𝒗𝒋, 𝒖 𝒊 ) (𝒖𝟏, 𝒗 𝟏 ) 1 ൗ 2 1 ൗ 2 (𝒖𝟏, 𝒗 𝟑 ) 1 ൗ 2 1 ൗ 3 (𝒖𝟐, 𝒗 𝟏 ) 1 ൗ 2 1 ൗ 2 (𝒖𝟐, 𝒗 𝟑 ) 1 ൗ 2 1 ൗ 3 (𝒖𝟑, 𝒗 𝟐 ) 1 1 ൗ 2 (𝒖𝟒, 𝒗 𝟐 ) 1 ൗ 3 1 ൗ 2 (𝒖𝟒, 𝒗 𝟑 ) 1 ൗ 3 1 ൗ 3 (𝒖𝟒, 𝒗 𝟒 ) 1 ൗ 3 1 ൗ 2 (𝒖𝟓, 𝒗 𝟒 ) 1 1 ൗ 2 𝐐 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒖𝟏 𝒖𝟐 𝒖𝟑 𝒖𝟒 𝒖𝟓 1 ൗ 2 1 ൗ 2 0 0 0 0 0 1 ൗ 2 1 ൗ 3 0 0 0 0 0 0 1 ൗ 2 1 ൗ 6 1 ൗ 6 1 ൗ 6 1 ൗ 6 𝐐 T 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒖𝟏 𝒖𝟐 ∙ 𝒖𝟑 𝒖𝟒 𝒖𝟓 1 ൗ 2 1 ൗ 2 0 0 0 0 0 1 ൗ 2 1 ൗ 6 0 1 ൗ 6 1 ൗ 6 1 ൗ 3 1 ൗ 6 1 ൗ 2 0 0 0 0 0 Example 2.1. Suppose that the edge weights 𝑤 ( 𝑢 𝑖 , 𝑣 𝑗 ) in the bipartite graph G of Figure 2 are all 1. Then, by Eq. (1), the one-hop jump probabilities 𝑝 ( 𝑢 𝑖 , 𝑣 𝑗 ) and 𝑝 ( 𝑣 𝑗 , 𝑢 𝑖 ) of every node pair in U× V are presented on the left of Figure 3. Then as an example, Q 3 , 1 = √︁ 𝑝 ( 𝑣 3 , 𝑢 1 )· ( 𝑢 1 , 𝑣 3 ) = 1 / √ 6. After getting Q in Figure 3, the edge weights of GV is calculated by Eq. (4). For example, 𝑤 V( 𝑣 1 , 𝑣 3 ) = ˝ 𝑢 𝑖 ∈U Q 1 ,𝑖 × Q 3 ,𝑖 = 1 2 × 1 √ 6 + 1 2 × 1 √ 6 = 1 √ 6 . 3 Renchi Yang and Jieming Shi",
  "2.4 High-Order Perspective Vectors": "In a bipartite graph G , vertices 𝑢 𝑖 and 𝑢 ℎ in U might have resembling or diverse neighbor sets N( 𝑢 𝑖 ) and N( 𝑢 ℎ ) , both of which are subsets of V . If N( 𝑢 𝑖 ) and N( 𝑢 ℎ ) are radically different, 𝑢 𝑖 and 𝑢 ℎ yield disparate perspectives towards the WPG GV . For instance, in Figure 2, vertex 𝑢 1 is adjacent to 𝑣 1 and 𝑣 3, while vertex 𝑢 3 connects to 𝑣 2, and consequently the HOP vectors of 𝑢 1 and 𝑢 3 are radically different. On the other hand, 𝑢 1 and 𝑢 2 share similar HOP vectors since they have similar perspectives towards WPG GV . Next, we present the formula to compute the HOP vectors for vertices 𝑢 𝑖 ∈ U towards the WPG GV with the consideration of high-order topological connections. For every vertex 𝑢 𝑖 , we first connect itself to all its neighbors 𝑣 𝑗 ∈ N( 𝑢 𝑖 ) in the WPG GV , with weight 𝑝 ( 𝑢 𝑖 , 𝑣 𝑗 ) . Afterwards, we simulate random walks with restart [54] from every 𝑢 𝑖 to the WPG GV to produce the HOP vector of 𝑢 𝑖 w.r.t. GV . We denote by P ∈ R | U|×|V| the transition matrix comprising the one-hop transition probability for each edge ( 𝑢 𝑖 , 𝑣 𝑗 ) ∈ E , i.e. , P 𝑖,𝑗 = 𝑝 ( 𝑢 𝑖 , 𝑣 𝑗 ) . Further, on the WPG GV , ( W V) 𝜆 𝑗,𝑙 , i.e. , ( QQ ⊤ ) 𝜆 𝑗,𝑙 , quantifies the strength of 𝜆 -step connections between vertices 𝑣 𝑗 and 𝑣 𝑙 . Thus, the total strength F 𝑖,𝑗 of connecting 𝑢 𝑖 and 𝑣 𝑗 in GV via any vertex 𝑣 𝑙 ∈ GV through random walks is calculated by  where 𝛼 is a random walk decay factor in ( 0 , 1 ) . Lemma 2.2 indicates that each entry in F is bounded by 1. Further, F converges to an exact solution, which will be theoretically analysed in Section 3 (see Lemma 3.1) 1 .  F is a |U| × |V| matrix, wherein each row vector F 𝑖 accommodates the stopping probabilities of random walks starting from 𝑢 𝑖 to all vertices 𝑣 in GV . Distinct vertices 𝑢 𝑖 and 𝑢 𝑗 correspond to different row vectors F 𝑖 and F 𝑗 . Finally, in Eq. (6), we get the HOP vector H 𝑖 of 𝑢 𝑖 , w.r.t. the WPG GV , by applying 𝐿 2 normalization over F 𝑖 to ensure that H 𝑖 has a unit 𝐿 2 norm ∥ H 𝑖 ∥ 2 = 1.  The HOP vector H 𝑖 summarizes high-order affinities between the given vertex 𝑢 𝑖 and any vertex 𝑣 𝑗 ∈ V from the perspective of 𝑢 𝑖 over the WPG GV , and hence H 𝑖 can act as a structural representation of 𝑢 𝑖 .",
  "2.5 Objective Function": "As aforementioned, two vertices 𝑢 𝑖 and 𝑢 ℎ with similar connections to the WPG GV are likely to have similar HOP vectors, and naturally, they tend to be in the same cluster, and vice versa . Building on this idea, we formulate our 𝑘 -BGC objective using the HOP vectors of all vertices 𝑢 𝑖 in U as follows. The objective in Eq. (7) is to identify 𝑘 disjoint clusters { 𝐶 1 , 𝐶 2 , · · · , 𝐶 𝑘 } such that the overall distance between HOP vectors of vertices in the same cluster and their mean is minimized, similar in spirit to k-Means [24].  1 All proofs appear in Appendix A.  The objective poses two formidable challenges to address. First and foremost, the direct computation and materialization of the dense HOP matrix H ∈ R | U|×|V| incur an exorbitant cost for large bipartite graphs with millions of nodes, given that H has a sophisticated definition that involves summing up an infinite series of matrices to capture high-order affinities as defined in Eq. (5) and (6). On top of that, the ultra-high dimensionality of H ( |V| is often up to millions on large graphs) leads to numerous iterations till convergence in the optimization of Eq. (7). To cope with these technical challenges, we first develop a base method HOPE in Section 3, which achieves superior effectiveness while being efficient. We further propose HOPE+ in Section 4 to circumvent certain limitations of HOPE and expedite the practical efficiency while gaining improved clustering quality.",
  "3 THE HOPE METHOD": "The workflow of HOPE for 𝑘 -BGC is illustrated in Figure 4. More concretely, to mitigate the severe issue of materializing the HOP matrix H and direct clustering of H , HOPE first derives a low-rank approximation X ∈ R | U|× 𝛽 of H ∈ R | U|×|V| ( 𝛽 ≪ |V| ), such that for any two nodes 𝑢 𝑖 and 𝑢 𝑗 in U , the difference between Euclidean distances ∥ X 𝑖 -X 𝑗 ∥ 2 and ∥ H 𝑖 -H 𝑗 ∥ 2 is bounded. Later, rather than H itself, X is used as a representation of H for clustering via optimizing the objective in Eq. (7). For instance, in Figure 4, our task to cluster five 4-dimensional HOP vectors of nodes 𝑢 1𝑢 5 turns to group five length-3 vectors into 2 clusters 𝐶 1 = { 𝑢 1 , 𝑢 2 } and 𝐶 2 = { 𝑢 3 , 𝑢 4 , 𝑢 5 } . Figure 4: A Running Example of HOPE 𝑢1 𝑢2 𝑢4 𝑢3 𝑢5 𝑣1 𝑣3 𝑣2 𝑣4 0.03   0.98   0.11   0.13 0.15   0.59   0.54   0.59 0.03   0.13   0.11   0.98 0.71   0.08    0.7    0.08 𝑢3 𝑢4 𝑢5 0.71   0.08    0.7    0.08 𝑢1 𝑢2 𝑢3 𝑢4 𝑢5 𝑢1 𝑢2 0.6 -0.52 -0.6 0.94 -0.33   0 0.81  0.58    0 0.6 -0.52  0.6 𝐇 ( w/o materialization ) 𝓖 𝐗 0.81  0.58    0 𝐇2 - 𝐇4 2 = 𝟎. 𝟗𝟐𝟕 𝐗2 - 𝐗4 2 = 𝟎. 𝟗𝟏𝟗 𝑢4 𝑢1 𝑢2 0.6 -0.52 -0.6 0.94 -0.33   0 0.81  0.58    0 0.6 -0.52  0.6 0.81  0.58    0 𝑢3 𝑢5 𝐶1 𝐶2 𝒌 -Means Error Bounded However, computing such a low-rank representation X is highly challenging, especially on how to bound the difference between X and H , without actually materializing H for the sake of efficiency. In the following, we first conduct theoretical analysis to derive X , before elaborating on the algorithmic details and asymptotic performance of HOPE. Analysis to Compute X . At first, we derive the following crucial property in Lemma 3.1 for ˝ ∞ 𝜆 = 0 ( 1 -𝛼 ) 𝛼 𝜆 ( QQ ⊤ ) 𝜆 in Eq. (5) by virtue of the symmetry of matrix QQ ⊤ and the semi-orthogonal property of singular vectors. Lemma 3.1. Let U and 𝚺 be the left singular vectors and singular values of Q . Then, U ∗ ,𝑖 and 1 -𝛼 1 -𝛼 · 𝚺 𝒊,𝒊 2 are the 𝑖 -largest eigenvector and eigenvalue of ˝ ∞ 𝜆 = 0 ( 1 -𝛼 ) 𝛼 𝜆 ( QQ ⊤ ) 𝜆 . Lemma 3.1 indicates that the eigenvectors and eigenvalues of the matrix ˝ ∞ 𝜆 = 0 ( 1 -𝛼 ) 𝛼 𝜆 ( QQ ⊤ ) 𝜆 can be readily obtained if the singular vectors U and singular values 𝚺 of Q are given, meaning 4 Efficient High-Quality Clustering for Large Bipartite Graphs",
  "Algorithm 1: HOPE": "Data: Bipartite graph G = (U ∪ V , E) Parameters: The decay factor 𝛼 , the number 𝑘 of clusters and dimensionality 𝛽 Result: 𝑘 cluster sets: 𝐶 1 , 𝐶 2 , · · · , 𝐶 𝑘 1 Perform 𝛽 -truncated SVD over Q ; 2 Let 𝚺 be the top- 𝛽 singular values of Q and U be the corresponding left singular vectors; 3 Calculate b X according to Eq. (8); 4 Normalize b X as X such that each row has a unit 𝐿 2 norm; 5 Invoke 𝑘 -Means to cluster the rows of X ; 6 Let 𝐶 1 , 𝐶 2 , · · · , 𝐶 𝑘 be the output of the 𝑘 -Means; 7 return 𝐶 1 , 𝐶 2 , · · · , 𝐶 𝑘 ; that F in Eq. (5), the vital ingredient of H in Eq. (6), can be calculated by PU 1 -𝛼 1 -𝛼 𝚺 2 U ⊤ , indicating the convergence of F . Let  Then, using the fact of U ⊤ U = I , we can get  as well as  The above equations imply that X with each row X 𝑖 = b X 𝑖 ∥ b X 𝑖 ∥ 2 can be employed as the low-rank approximation of H , when U and 𝚺 are the top𝛽 ( 𝛽 ≪ |V| ) singular vectors and values of Q respectively. Theorem 3.2. Given the above matrix X ∈ R | U|× 𝛽 , ∀ 𝑢 𝑖 , 𝑢 𝑗 ∈ U ,    The accuracy guarantee between the low-rank approximation X and the HOP matrix H is established between ∥ X 𝑖 -X 𝑗 ∥ 2 2 and ∥ H 𝑖 -H 𝑗 ∥ 2 2 by Theorem 3.2. To demonstrate the empirical difference between ∥ X 𝑖 -X 𝑗 ∥ 2 2 and ∥ H 𝑖 -H 𝑗 ∥ 2 2 , in Figure 5, we report the average relative error 𝜖 𝑟 = 1 | U| 2 ˝ 𝑢 𝑖 ,𝑢 𝑗 ∈U | ∥ X 𝑖 -X 𝑗 ∥ 2 2 -∥ H 𝑖 -H 𝑗 ∥ 2 2 | ∥ H 𝑖 -H 𝑗 ∥ 2 2 and absolute error 𝜖 𝑎 = 1 | U| 2 ˝ 𝑢 𝑖 ,𝑢 𝑗 ∈U GLYPH<12> GLYPH<12> ∥ X 𝑖 -X 𝑗 ∥ 2 2 - ∥ H 𝑖 -H 𝑗 ∥ 2 2 GLYPH<12> GLYPH<12> when varying 𝛽 . Note that 𝜖 𝑟 ∈ [ 0 , 1 ] and 𝜖 𝑎 ∈ [ 0 , 2 ] . When increasing 𝛽 from 16 to 256, the errors 𝜖 𝑟 and 𝜖 𝑎 considerably diminish, and dwindle to small values around or below 0.1 when 𝛽 ≥ 64, which showcases that X can accurately approximate H . This observation matches the experimental results in Figure 10 where clustering quality improves and then becomes steady when increasing 𝛽 . The HOPE Algorithm. On the basis of the above analysis, we can obtain the low-rank approximation X of H through an efficient Figure 5: Approximation errors of X when varying 𝛽 . Relative Error Absolute Error 16 32 64 128 256 0 0 . 5 1 1 . 5 2 (a) Cora 16 32 64 128 256 0 0 . 5 1 1 . 5 2 (b) PubMed 16 32 64 128 256 0 0 . 5 1 1 . 5 2 (c) Flickr truncated singular value decomposition (SVD) over the sparse matrix Q using a number of fast matrix operations, without materializing H explicitly, thereby leading to a significant reduction in computation cost. Algorithm 1 displays the pseudo-code of HOPE, which begins with the input of a bipartite graph G , decay factor 𝛼 , the number 𝑘 of clusters, and dimensionality 𝛽 that is usually a multiple of 𝑘 . After that, HOPE performs a 𝛽 -truncated SVD over matrix Q to get its top𝛽 left singular vectors U and a 𝛽 × 𝛽 diagonal matrix 𝚺 containing the top𝛽 singular values (Lines 1-2). At Line 3, we calculate the matrix X according to Eq. (8), and then normalize X as b b X such that each 𝑖 -th row has a unit 𝐿 2 norm, i.e. , X 𝑖 = b X 𝑖 ∥ b X 𝑖 ∥ 2 (Line 4). Lastly, Algorithm 1 invokes k-Means algorithm [24] to cluster the rows of the low-rank approximation matrix X into clusters 𝐶 1 , 𝐶 2 · · · , 𝐶 𝑘 and return them as the final result (Lines 5-7). Complexity. Notice that both P and Q are sparse matrices with |E| non-zero entries. Hence, Line 1 in Algorithm 1 takes 𝑂 (|E| · 𝛽 ) time [49]. The computation of b X by Eq. (8) requires a sparse matrix multiplication, incurring 𝑂 (|E| · 𝛽 ) time, and the normalization of b X at Line 4 takes 𝑂 (|U| · 𝛽 ) time. As stated by [24], k-Means over X runs in 𝑂 (|U| · 𝛽 · 𝑘 · 𝑇 ) time, where 𝑇 stands for the number of iterations in k-Means. In total, the time complexity of HOPE is bounded by 𝑂 ((|E| + |U| · 𝑘 ) · 𝛽 ) . The space overhead of HOPE is chiefly determined by the sizes of b X and X , containing 𝑂 (|U| · 𝛽 ) entries. Together with the matrices P and Q , the overall space cost entailed by HOPE is 𝑂 (|E| + |U| · 𝛽 ) .",
  "4 THE HOPE+ METHOD": "As revealed in experiments in Section 5, HOPE achieves superior clustering quality, while being efficient. Nevertheless, HOPE inherits the defects of k-Means, e.g. , the tendency to get stuck at local optima, and the numerous iterations required for convergence, especially on large bipartite graphs. To alleviate such issues, we further propose HOPE+ which is able to advance clustering performance in both efficiency and effectiveness. In Section 4.1, we first explain how HOPE+ transforms and disassembles the 𝑘 -BGC objective in Section 2.5 into two optimization problems solved one after another via efficient algorithmic designs, as elucidated in Section 4.2 and Section 4.3 respectively. Briefly, the 𝑘 -BGC objective in Eq. (7) is converted to its equivalent form of a matrix trace maximization. The first problem is efficiently optimized with a fractional solution L ∈ R | U|× 𝑘 derived by eigendecomposition, when certain constraints are relaxed (Section 4.2). The second optimization problem is formulated to minimize the difference (quantified by the popular Frobenius norm and spectral norm) between a vertex-cluster membership indicator matrix (VCMI) C and L . Therefore, we devise two rounding algorithms , 5 Renchi Yang and Jieming Shi FNEM and SNEM, to get C when minimizing errors in the aforementioned norms (Section 4.3). Section 4.4 includes the analysis of the overall time and space complexities of HOPE+ with FNEM and SNEM.",
  "4.1 Two-Stage Optimizations": "Recall that h ( 𝑗 ) = ˝ 𝑢 𝑖 ∈ 𝐶 𝑗 H 𝑖 | 𝐶 𝑗 | in Eq. (7) and ∥ H 𝑖 ∥ 2 = 1, ∀ 𝑢 𝑖 ∈ U in Eq. (6), based on which, we establish the equivalence between the objective in Eq. (7) and the trace optimization in Eq. (9).  where C ∈ R | U|× 𝑘 in Eq. (9) is the vertex-cluster membership indicator matrix (VCMI) with each ( 𝑖, 𝑗 ) -th entry to be   where | 𝐶 𝑗 | is the size of cluster 𝐶 𝑗 . In Eq. (10), observe that C is required to fulfill two requirements: (i) each row in C contains only one non-zero and positive entry; and (ii) each column has a unit 𝐿 2 norm in which all entries are equal. Together these two constraints ensure that C has orthogonal columns, i.e. , C ⊤ C = I . Using Ky Fan's trace maximization principle in Lemma 4.1, the optimal solution to Eq. (9) is the 𝑘 -largest eigenvectors of HH ⊤ when the two foregoing constraints on C are relaxed to satisfying C ⊤ C = I . Lemma4.1(KyFan'stracemaximizationprinciple [15]). Given a symmetric real matrix M ∈ R 𝑛 × 𝑛 with distinct eigenvalues 𝜓 1 ( M ) , 𝜓 2 ( M ) , · · · , 𝜓 𝑛 ( M ) , sorted by algebraic value in descending order, eigenvectors L and integer 𝑘 ≤ 𝑛 , we have  According to Lemma 4.1, we can utilize the 𝑘 -largest eigenvectors L of HH ⊤ as a solution in R | U|× 𝑘 to optimize the trace maximization problem in Eq. (9). Afterwards, a VCMI C can be obtained by minimizing the differences between C and L via the optimization problem formulated in Eq. (11).  Summing up, we transform the 𝑘 -BGC objective in Eq. (7) into two optimization problems in Eq. (9) and (11), to be solved in order. The HOPE+ Algorithm. The pseudo-code of HOPE+ is outlined in Algorithm 2, which proceeds in two stages. The first stage is to optimize Eq. (9) via approximate 𝑘 -largest eigenvectors L by Lemma Figure 6: A Running Example of HOPE+ 𝐗 𝑢1 𝑢2 𝑢3 𝑢4 𝑢5 0.81   0.58    0 0.81   0.58    0 0.6 -0.52 -0.6 0.94 -0.33 0 0.6 -0.52 0.6 𝐋 𝑢1 𝑢2 𝑢3 𝑢4 𝑢5 0.48 0.5 0.48 0.5 0.35 -0.45 0.55  -0.29 0.35 -0.45 ∙ ≈ ∙ Top- 𝒌 Singular Vectors 𝐂 𝑢1 𝑢2 𝑢3 𝑢4 𝑢5 0 1/ 2 1/ 3 0 0 1/ 2 1/ 3 0 1/ 3 0 FNEM/SNEM Stage 1: Approximate 𝒌 -Largest Eigenvectors 𝐋 Stage 2: Rounding 𝐋 to get 𝐂 4.1 (Lines 1-5). Note that the construction of matrix HH ⊤ entails a quadratic space overhead of 𝑂 (|U| 2 ) and 𝑂 (|U| 2 · |V| + |E| · |V|) computation time, which is intolerable for massive bipartite graphs. To mitigate this issue, in Section 4.2, we efficiently approximate 𝑘 -largest Eigenvectors L without explicitly materializing HH ⊤ , and we will explain Lines 1-5 shortly. With L at hand, HOPE+ then enters into the second stage, which attends to generating the final clustering result, namely VCMI C , based on the optimization problem in Eq. (11). Prior to that, C is initialized via a simple greedy seeding strategy (Lines 6-10). To be specific, for every vertex 𝑢 𝑖 ∈ U , we first get the column index 𝑗 ∗ corresponding to the maximum entry L 𝑖,𝑗 ∗ in L 𝑖 (Line 8), and assign 𝑢 𝑖 into cluster 𝐶 𝑗 (Line 9). This greedy seeding strategy provides us a high-quality initialization of C to facilitate the convergence of C , when dealing with the optimization problem in Eq. (11). However, it is highly challenging to solve Eq. (11) as it may involve prohibitively expensive time of up to 𝑂 (|U| 2 𝑘 ) . In lieu of directly working on Eq. (11), in Section 4.3, we conduct further analysis to transform Eq. (11) to its equivalent but simplified form, and present two rounding algorithms for the refinement and obtainment of C (Line 11), namely FNEM and SNEM under the settings of the popular Frobenius norm error and spectral norm error, respectively. At Line 12, C is returned as the final clustering result. Example 4.2. Figure 6 presents a running example of HOPE+ over the bipartite graph G with 𝑘 = 2 in Figure 4 for the clustering of five nodes 𝑢 1𝑢 5. Without materializing H and computing HH ⊤ , HOPE+ first transforms the problem of calculating the 2-largest eigenvectors of HH ⊤ as computing the top-2 singular vectors L of X , the 3-dimension approximation of H obtained as in Algorithm 1. After that, HOPE+ conducts a rounding procedure in Algorithm 3 over L ∈ R 5 × 2 to get a VCMI matrix C , in which each row contains only one non-zero entry indicating the cluster membership of the node corresponded by the row. For example, in Figure 6, rows 1 and 2 in C have non-zero elements 1 √ 2 at the second column, while the entries at the first columns of rows 3-4 are nonzero, i.e., 1 √ 3 , resulting in two clusters { 𝑢 1 , 𝑢 2 } and { 𝑢 3 , 𝑢 4 , 𝑢 5 } . In HOPE+, rounding the top-2 singular vectors L to get the VCMI C for producing clusters averts the high computationla costs entailed by using the 𝑘 -Means in HOPE.",
  "4.2 Approximating 𝑘 -Largest Eigenvectors L": "As alluded to earlier, in the first stage of HOPE+, the goal is to efficiently approximate the top𝑘 eigenvectors L to solve Eq. (9) based on Lemma 4.1, but without materializing HH ⊤ . Recall that in Lemma 3.1, we have XX ⊤ = HH ⊤ when 𝛽 = |V| , whereby we can obtain a low-rank approximation X ∈ R | U|× 𝛽 ( 𝑘 < 𝛽 ≪ |V| ) of H efficiently such that XX ⊤ ≈ HH ⊤ . Further, 6 Efficient High-Quality Clustering for Large Bipartite Graphs Algorithm 2: HOPE+ Data: Bipartite graph G = (U ∪ V , E) Parameters: The decay factor 𝛼 , the number 𝑘 of clusters, dimensionality 𝛽 , and the number 𝑇 of iterations Result: The final VCMI C /* Stage 1: Approximate 𝑘 -largest eigenvectors L */ Lines 1-4 are the same as Lines 1-4 in Algorithm 1; 5 Perform the 𝑘 -truncated SVD over X to get the top𝑘 left singular vectors L to solve Eq. (9); /* Stage 2: Rounding L to get C */ 6 Initialize an empty VCMI C ← 0 | U|× 𝑘 ; 7 for 𝑢 𝑖 ∈ U do 8 𝑗 ∗ = arg max 1 ≤ 𝑗 ≤ 𝑘 L 𝑖,𝑗 ; 9 C 𝑖,𝑗 ∗ ← 1; 10 Normalize C such that each column has a unit 𝐿 2 norm; 11 Invoke Algorithm 3 for FNEM/SNEM rounding between L and C for at most 𝑇 iterations; 12 return C ; by leveraging Lemma 4.3, calculating the 𝑘 -largest eigenvectors of HH ⊤ is reduced to obtaining the top𝑘 left singular vectors of the low-rank approximation X . Lemma 4.3 ([49]). Given an SVD of matrix M , the top𝑘 left singular vectors of M are the 𝑘 largest eigenvectors of MM ⊤ . This idea is realized at Lines 1-5 in Algorithm 2 with a detailed procedure of approximating the 𝑘 -largest eigenvectors of HH ⊤ . More precisely, Algorithm 2 initially generates the |U| × 𝛽 -sized low-rank X of the HOP matrix H , identical to Lines 1-4 in Algorithm 1. Subsequently, at Line 5 in Algorithm 2, a 𝑘 -truncated SVD is conducted over X to produce the top𝑘 left singular vectors of X , which are returned as the approximate 𝑘 -largest eigenvectors L of HH ⊤ , according to Lemma 4.2 and the fact XX ⊤ ≈ HH ⊤ . Notice that Lines 1-4 in Algorithm 2 run in 𝑂 ((|U| + |E|) · 𝛽 ) time, and the 𝑘 -truncated SVD at Line 5 in Algorithm 2 is performed over a |U| × 𝛽 matrix, leading to 𝑂 (|U| · 𝛽 2 ) time. X and other intermediate matrices in Algorithm 2 consume 𝑂 (|U| · 𝛽 ) space. In sum, Lines 1-5 in Algorithm 2 avoid materializing HH ⊤ and enable a fast computation of L with 𝑂 (|U| · 𝛽 2 + |E| · 𝛽 ) time and 𝑂 (|E| + |U| · 𝛽 ) space, both of which are linear to graph size.",
  "4.3 Rounding Algorithms": "As explained, after obtaining the approximate L above, the second stage of HOPE+ is to derive C by solving Eq. (11). For efficiency purpose, instead of directly working on Eq. (11), we convert it to its equivalent form that is tractable to solve via the following analysis. In particular, we first capitalize on the cyclic property of matrix trace to get Tr ( C ⊤ HH ⊤ C ) = Tr ( HH ⊤ CC ⊤ ) and Tr ( L ⊤ HH ⊤ L ) = Tr ( HH ⊤ LL ⊤ ) , so as to transform Eq. (11) into  Algorithm 3: FNEM/SNEM Rounding Data: The initial VCMI C and 𝑘 -largest eigenvectors L Parameters: The number 𝑇 of iterations and URT Result: The final VCMI C 1 𝑡 ← 1; 2 does not converges and while C /* 𝑡 Update T when fixing switch URT do case FNEM do Perform a full SVD over L right singular vectors 𝚽 T ; ← ⊤ 𝚽𝚿 case SNEM T L ← do ⊤ C Update /* C C when fixing ← for 0 ; 𝑢 𝑖 𝑗 do ∈ U = arg max 1 ∗ 𝑖,𝑗 ≤ ≤ LT 𝑗 ) ( 𝑘 𝑖,𝑗 ∗ ← 1; C Normalize C 𝑡 3 4 5 6 7 8 9 10 11 12 13 ← 𝑡 + 1; 14 return C ; If there exists a matrix T ∈ R 𝑘 × 𝑘 that minimizes certain matrix norm error ( e.g. , Frobenius norm or spectral norm) between LT and C as in Eq. (13), then Eq. (12) is optimized since CC ⊤ ≈ LT · ( LT ) ⊤ = LTT ⊤ L ⊤ = LL ⊤ .  As such, the optimization problem in Eq. (11) turns into Eq. (13), which can be efficiently solved as shown shortly. The Frobenius norm ∥ · ∥ 𝐹 and spectral norm ∥ · ∥ 2 are two popular choices for the matrix norm ∥ · ∥∗ in Eq. (13). In what follows, we develop two fast rounding algorithms, namely FNEM 2 Rounding and SNEM 3 Rounding , to efficiently solve Eq. (13) to get C , when either Frobenius norm or spectral norm is applied. Algorithm 3 illustrates the pseudo-code of FNEM or SNEM rounding, whose input parameters include an initial VCMI C , the 𝑘 -largest eigenvectors L , the total number of iterations 𝑇 , and a parameter URT (short for Updating Rule for T ) to specify which rounding scheme is adopted, i.e. , FNEM or SNEM. FNEM Rounding. We first delineate Algorithm 3 under the FNEM rounding scheme. From Lines 1 to 13, FNEM rounding works in an iterative process to refine T and C in an alternative fashion until C converges (i.e., C remains unchanged) or 𝑇 iterations are completed. In each iteration, we fix one of T and C , and update the other. Now we explain the updating rules in detail. Note that the objective function in Eq. (13) can be regarded as an orthogonal Procrustes problem [22] when C is fixed. To be specific, given two matrices L and C , the orthogonal Procrustes problem asks to find an orthogonal matrix T ( i.e. , TT ⊤ = I ) which most closely maps L to C such that ∥ LT -C ∥ 𝐹 is minimized. By leveraging our result in Lemma 2 Frobenius Norm Error Minimization 3 Spectral Norm Error Minimization such that each column has a unit ; 𝑇 do ⊤ C and ≤ C T ; 𝐿 */ to get the left and 𝚿 ; */ 2 norm; 7 Renchi Yang and Jieming Shi 4.4, the SVD result of L ⊤ C can be utilized to infer the optimal T , i.e. , T = 𝚽𝚿 ⊤ as Lines 5-6 in Algorithm 3. Since L ⊤ C is a 𝑘 × 𝑘 matrix and 𝑘 is usually small, the SVD can be done efficiently. Lemma 4.4. Let 𝚽 and 𝚿 be the left and right singular vectors of L ⊤ C , respectively. Then T ∗ = arg min T ∥ LT -C ∥ 2 𝐹 = 𝚽𝚿 ⊤ . After obtaining T when C is fixed as described above, we then need to fix T and update C . The problem in Eq. (13) turns into  Recall that C is a VCMI matrix defined in Eq. (10), wherein every 𝑖 -th row has only one non-zero entry. Simply, to optimize Eq. (14), we can determine the cluster id 𝐶 𝑗 ∗ for each vertex 𝑢 𝑖 ∈ U (Lines 8-11 in Algorithm 3), by locating the column 𝑗 ∗ with the maximum entry ( LT ) 𝑖,𝑗 ∗ in the 𝑖 -th row of LT (Line 10) and then setting C 𝑖,𝑗 ∗ to 1 (Line 8). After processing all vertices in U , each column of C is 𝐿 2-normalized at Line 12 so as to ensure Eq. (10). At last, FNEM rounding increases 𝑡 by 1 at Line 13 and executes the next iteration. SNEM Rounding. When spectral norm is considered in Eq. (13) (i.e., URT is set to SNEM) rounding, the optimization problem becomes min T , C ∥ LT -C ∥ 2 s.t. TT ⊤ = I . We also adopt the same iterative process for SNEM as in Algorithm 3, where we update T and C alternatively in an iteration. When C is fixed, SNEM rounding is to find matrix T that solves  The optimal solution to Eq. (15) is simply T ∗ = L ⊤ C as per our carefully-analyzed result in Lemma 4.5.  In Algorithm 3, in every iteration, when C is fixed, the updating rule of T is to set it as L ⊤ C for SNEM (Line 7), and meanwhile, the rule for updating C with a given T in SNEM rounding is consistent with that in FNEM rounding. In comparison with FNEM rounding, SNEM rounding solely requires a simple matrix multiplication L ⊤ C with 𝑂 (|U|· 𝑘 ) time for updating T (Line 7 in Algorithm 3), whereas FNEM rounding needs to conduct a full SVD over L ⊤ C (Line 5) that costs 𝑂 (|U| · 𝑘 2 · 𝑇 𝑆 ) time, where 𝑇 𝑆 is number of iterations needed in SVD and its typical setting could be as large as a few dozens in practice.",
  "4.4 Complexity Analysis": "Here we analyze the overall complexities of HOPE+. We dub HOPE+ with FNEM and SNEM rounding as HOPE+ (FNEM) and HOPE+ (SNEM), respectively. For FNEM rounding, it takes 𝑂 (|U| · 𝑘 2 ) time for the SVD and matrix multiplication at Lines 5-6 of each iteration (Algorithm 3). For SNEM rounding, in each iteration, the matrix T is updated at Line 7 in Algorithm 3, which can be done in 𝑂 (|U| · 𝑘 ) time since C is a sparse matrix with |U| non-zero entries. Both HOPE+ (FNEM) and HOPE+ (SNEM) share the same updating rule of C at Lines 8-12 per iteration, with 𝑂 (|U| · 𝑘 ) time to inspect the maximum element per row of a |U| × 𝑘 matrix. Further, recall that obtaining the top𝑘 eigenvectors of HH ⊤ in Section 4.2 takes 𝑂 (|U|· 𝛽 2 +|E|· 𝛽 ) time and 𝑂 (|E|+|U|· 𝛽 ) space. Therefore, HOPE+ (FNEM) runs in 𝑂 (|E| · 𝛽 + |U| · 𝛽 2 + |U| · 𝑘 2 𝑇 ) time and consumes 𝑂 (|E| + |U| · 𝛽 ) space; the time complexity and space complexity Table 2: Statistics of bipartite graphs. (K = 10 3 , M = 10 6 , B = 10 9 )",
  "Table 3: Evaluated methods.": "of HOPE+ (SNEM) are bounded by 𝑂 (|E| · 𝛽 + |U| · 𝛽 2 + |U| · 𝑘𝑇 ) and 𝑂 (|E| + |U| · 𝛽 ) , respectively.",
  "5 EXPERIMENTS": "We experimentally evaluate HOPE and HOPE+ with FNEM and SNEM against 13 competitors on 10 real-world bipartite graph datasets, in terms of clustering quality and efficiency. The experiments run atop a Linux machine with 2 Intel(R) Xeon(R) Gold 6330 2.00GHz CPUs with 28 cores and 2 TB RAM. The reported results are averaged over 5 runs. For reproducibility, we publish all the codes and datasets at https://github.com/HKBU-LAGAS/HOPE.",
  "5.1 Experimental Setup": "Datasets. Table 2 lists the statistics of the 10 datasets in experiments. There are 5 small or medium-sized graphs and 5 large graphs with millions of edges, up to 1.1 billion edges in MAG . Both CORA and CiteSeer datasets [19] contain scientific publications U with edges to the corresponding keywords V in the publication, and the publications are in 7 and 6 classes (fields of study), respectively. CORA-F [5] is the full version of the CORA network. In Flickr [26], vertices in U (resp. V ) represent users (resp. tags) and edges specify the interest of users in tags, and users are in groups as clusters. In BlogCatalog [53], U is a set of bloggers, and V are the keywords generated from the bloggers' blogs. The labels of vertices in U signify the topic categories provided by the bloggers. The PubMed [47] dataset consists of scientific publications from the PubMed database pertaining to diabetes, classified into three categories, and each edge weight is the TF/IDF weight of the word in the publication. The MIND [58] dataset contains news articles U and users V , and each edge is a click of a user on a news article, and the clustering labels are the categories of the news [63]. LastFM 8 Efficient High-Quality Clustering for Large Bipartite Graphs Table 4: Clustering quality on small or medium-size datasets. Table 5: Clustering quality on large datasets. (Asia) [45] and LastFM [9] are from the LastFM music website. U (resp. V ) in LastFM (Asia) includes users from Asian countries (resp. artists liked by the users), whereas LastFM contains the play count of each music by each user. The clustering labels in both datasets are the locations of users. The MAG dataset is extracted from the Microsoft Academic Graph [48] by [6], with vertices in U and V representing papers and words in the abstracts, respectively, and each edge weight reflects the word occurrence of a word in a paper [63]. The labels correspond to 8 fine-grained fields of study. Baselines and Parameter Settings. Table 3 summarizes the time complexities of our solutions and 13 competitors (the numbers of iterations in them are regarded as constants) evaluated in our experiments when adopted for 𝑘 -BGC. Specifically, we compare HOPE, HOPE+ (FNEM), and HOPE+ (SNEM) against 5 unipartite graph clustering algorithms LE [39], Girvan-Newman [20], SC [55], NRP [64], and PPR [56], 4 data clustering methods including K-Means [24], K-Medoids [29], Birch [69], and NMF [61], as well as 4 BGC approaches, i.e. , SBC [31], SCC [12], biSBM-KL [32] and biSBM-MCMC [67]. Note that in NRP and PPR, the clusters are obtained by applying K-Means over the node embedding vectors and PPR vectors they generate, respectively. The parameters of all competitors are set as suggested in their respective papers. Unless otherwise specified, we set 𝛼 = 0 . 3 and 𝛽 = 5 𝑘 ( 𝑘 is the number of clusters in |U| ) for HOPE and HOPE+. The number 𝑇 of iterations in HOPE+ (FNEM) and HOPE+ (SNEM) is set to 100. Evaluation Metrics. We adopt 4 classic metrics to measure the clustering quality [60], including Clustering Accuracy (Acc), F1 score (F1), Normalized Mutual Information (NMI) [50], and Adjusted Rand Index (ARI) [27]. All metrics are calculated based on the groundtruth clustering labels and predicted clustering labels. Particularly, Acc, F1, and NMI range from 0 to 1 . 0, while ARI ranges from -0 . 5 to 1 . 0. For all of them, the higher values indicate better clustering quality. In terms of clustering efficiency and scalability, we report the running time in seconds (measured in wall-clock time) of each algorithm on each dataset, excluding the time for loading datasets and outputting clusters. A method is excluded if it cannot finish within 2 days.",
  "5.2 Clustering Quality": "Table 4 and Table 5 report the Acc, F1, NMI, ARI scores, as well as the average performance rankings of all methods on 5 small bipartite graph datasets ( i.e. , CORA , CiteSeer , BlogCatalog , Flickr , and PubMed ) and 5 large datasets with more than a million edges, including CORA-F , LastFM (Asia) , MIND , LastFM , and MAG . We report the best performance of HOPE, HOPE+ (FNEM), and HOPE+ (SNEM) on every bipartite graph data when varying 𝛽 from 2 𝑘 to 9 Renchi Yang and Jieming Shi 10 10 Figure 7: Running time in seconds ( ★ marks the competitors with the best clustering quality in Tables 4 and 5). HOPE+ (SNEM) HOPE+ (FNEM) HOPE biSBM-MCMC biSBM-KL SCC SBC Girvan-Newman SC LE K-Means K-Medoids Birch NMF NRP PPR - 2 - 1 10 0 10 1 10 2 10 3 10 4 ★ (a) CORA 10 - 1 10 0 10 1 10 2 10 3 10 4 ★ (b) CiteSeer 10 - 1 10 0 10 1 10 2 10 3 10 4 ★ (c) BlogCatalog 10 - 1 10 0 10 1 10 2 10 3 10 4 ★ (d) Flickr 10 - 1 10 0 10 1 10 2 10 3 10 4 ★ (e) PubMed 10 0 10 1 10 2 10 3 10 4 10 5 ★ ★ (f) CORA-F 10 0 10 1 10 2 10 3 10 4 ★ (g) LastFM (Asia) 10 1 10 2 10 3 ★ (h) MIND 10 2 10 3 10 4 ★ (i) LastFM 10 3 10 4 10 5 ★ (j) MAG Figure 8: Scalability tests. HOPE HOPE+ (FNEM) HOPE+ (SNEM) 2e+5 4e+5 6e+5 8e+5 1e+6 10 1 10 2 10 3 running time (seconds) (a) Varying #nodes 2e+6 4e+6 6e+6 8e+6 1e+7 10 1 10 2 10 3 running time (seconds) (b) Varying #edges 2 10 50 250 1000 10 0 10 2 10 4 running time (seconds) (c) Varying 𝑘 8 𝑘 and 𝛼 from 0 . 1 to 0 . 9. The best, 2nd-best, and 3rd-best results among all methods are highlighted in blue, and darker shades indicate better scores. The best performance of all existing methods is underlined. As shown in the last column of Tables 4 and 5, our methods, HOPE, HOPE+ (FNEM), and HOPE+ (SNEM), achieve the top-3 best average rank (smaller rank is better) on all datasets. Specifically, our proposed solutions considerably outperform all the competitors under most metrics on all 10 datasets. For instance, compared to existing methods, in Table 4, HOPE+ (SNEM) takes a lead by 12 . 7%, 5 . 5%, 13 . 4%, 3 . 5%, and 1 . 4% in terms of clustering accuracy (Acc) on the 5 small bipartite graphs, respectively. On the 5 large datasets in Table 5, HOPE+ (SNEM) outperforms existing approaches by a large margin of 9 . 5%, 5 . 2%, 9 . 7%, 11 . 7%, and 22 . 1% in terms of Acc, respectively. As for the other 3 metrics ( i.e. , F1, NMI, and ARI), we can make qualitatively analogous observations, except on PubMed , where K-Means and SC obtain the highest NMI and ARI scores. The overall results in Table 4 and Table 5 validate the effectiveness of the proposed clustering objective via HOP vectors, which is then solved by the techniques developed in Sections 3 and 4. In addition, in Tables 4 and 5, observe that HOPE+ (SNEM) has the best performance rank on all datasets, whereas HOPE and HOPE+ (FNEM) attain the second-best average performance rank on the small and large datasets, respectively. First, this indicates that HOPE inherits the local optima trap issue caused by the kMeans, and it is accentuated on large bipartite graphs, which is sidestepped by HOPE+ that does not rely on k-Means via a new optimization framework presented in Section 4. Second, HOPE+ (SNEM) outperforms HOPE+ (FNEM) under most cases, which manifests that spectral norm is probably a better choice in Eq. (13) for our methods to handle 𝑘 -BGC.",
  "5.3 Efficiency and Scalability": "Figure 7 depicts the respective running time of all methods required for clustering on the 10 bipartite graphs. The 𝑦 -axis represents the running time (seconds) in the log-scale. Observe that HOPE+ (SNEM) achieves the highest efficiency on all datasets, outperforming existing methods often by up to orders of magnitude. HOPE+ (FNEM) is also faster than all competitors, except on the smallest CORA . Particularly, on small bipartite graphs in Figure 7(a)-7(e), compared with the competitor with the best clustering quality in Table 4, i.e. , biSBM-MCMC, PPR, SCC, or K-Means, HOPE+ achieves over 270 × speedup on CORA , CiteSeer , BlogCatalog , and Flickr , as well as more than 6 . 6 × speedup on PubMed . As an example, on BlogCatalog , the best competitor biSBM-MCMC takes 271 . 8 seconds to complete, while both HOPE+ (SNEM) and HOPE+ (FNEM) need only around 0 . 32 seconds, attaining a significant speedup of 850 × , respectively, while, as mentioned, the clustering quality of HOPE+ (SNEM) and HOPE+ (FNEM) is better than biSBM-MCMC. Compared with the fastest competitor NMF with inferior quality, HOPE+ (SNEM) is still consistently faster. In Figure 7(f)-7(j) on large datasets, the efficiency improvement of our methods maintains or enlarges over existing methods, particularly on the three largest datasets MIND , LastFM , and MAG , where only our methods HOPE, HOPE+ with SNEM and FNEM, and competitors NRP, NMF survive to return within 2 days. Specifically, our solution HOPE+ (SNEM) is able to gain 9 . 2 × , 54 . 7 × , and 44 . 4 × runtime speedup over the best viable competitor NMF or NRP as shown in Figure 7(h), 7(i), and 7(j), respectively. Notice that in spite of the comparable asymptotic complexities of NMF, SBC, and SCC as shown in Table 3, their efficiency significantly falls short compared to HOPE and HOPE+ due to numerous iterations required and/or time-consuming and complex operations involved in them. The superior efficiency of HOPE+ demonstrates the power of the techniques proposed in Section 4 to significantly reduce computational costs, while achieving the state-of-the-art clustering quality as reported in Section 5.2. HOPE+ (SNEM) is consistently faster than HOPE+ (FNEM) since the former has lower time complexity as analyzed in Section 4.3. HOPE+ methods (Section 4) are more efficient than HOPE (Section 3) since HOPE+ avoids the expensive k-Means that needs many iterations to converge, via novel theoretical analysis and algorithm designs. 10 Efficient High-Quality Clustering for Large Bipartite Graphs Figure 10: Accuracy vs. 𝛽 . HOPE HOPE+ (FNEM) HOPE+ (SNEM) 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.52 0.54 0.56 0.58 0.6 Accuracy (a) CORA 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 . 55 0 . 6 0 . 65 Accuracy (b) CiteSeer 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 . 6 0 . 65 0 . 7 0 . 75 Accuracy (c) BlogCatalog 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 . 5 0 . 6 0 . 7 Accuracy (d) Flickr 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 . 54 0 . 56 0 . 58 0 . 6 0 . 62 Accuracy (e) PubMed 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 . 34 0 . 36 0 . 38 Accuracy (f) CORA-F Figure 9: Accuracy vs. 𝛼 . HOPE HOPE+ (FNEM) HOPE+ (SNEM) 𝑘 2 𝑘 3 𝑘 4 𝑘 5 𝑘 6 𝑘 7 𝑘 8 𝑘 0 . 4 0 . 45 0 . 5 0 . 55 0 . 6 Accuracy (a) CORA 𝑘 2 𝑘 3 𝑘 4 𝑘 5 𝑘 6 𝑘 7 𝑘 8 𝑘 0 . 5 0 . 55 0 . 6 0 . 65 Accuracy (b) CiteSeer 𝑘 2 𝑘 3 𝑘 4 𝑘 5 𝑘 6 𝑘 7 𝑘 8 𝑘 0 . 5 0 . 6 0 . 7 Accuracy (c) BlogCatalog 𝑘 2 𝑘 3 𝑘 4 𝑘 5 𝑘 6 𝑘 7 𝑘 8 𝑘 0 . 5 0 . 6 0 . 7 Accuracy (d) Flickr 𝑘 2 𝑘 3 𝑘 4 𝑘 5 𝑘 6 𝑘 7 𝑘 8 𝑘 0 . 58 0 . 6 0 . 62 Accuracy (e) PubMed 𝑘 2 𝑘 3 𝑘 4 𝑘 5 𝑘 6 𝑘 7 𝑘 8 𝑘 0 . 3 0 . 35 Accuracy (f) CORA-F Next, we verify the scalability of HOPE and HOPE+ on synthetic graphs of different sizes by the Erdős-Rényi random graph model [4]. Figure 8(a) shows the running time of HOPE, HOPE+ (FNEM), and HOPE+ (SNEM) on the random graphs generated by fixing the total number of vertices |U| + |V| = 5 × 10 5 with |U| = |V| , and varying the number of edges |E| in { 2 × 10 6 , 4 × 10 6 , 6 × 10 6 , 8 × 10 6 , 10 7 } when 𝑘 = 50. Figure 8(b) displays the efficiency when varying the number of vertices |U| + |V| in { 2 × 10 5 , 4 × 10 5 , 6 × 10 5 , 8 × 10 5 , 10 6 } , while fixing the number of edges |E| = 2 × 10 6 and 𝑘 = 50. In Figure 8(c), we vary 𝑘 in { 2 , 10 , 50 , 250 , 1000 } to evaluate the scalability, while fixing |E| = 2 × 10 6 and |U| + |V| = 2 × 10 5 . Observe that the runtime of our methods increases steadily in proportion to the number of vertices and the number 𝑘 of clusters, but grows modestly as the number of edges increases.",
  "5.4 Parameter Analysis": "We empirically study the effects of the input parameters of HOPE and HOPE+, including the random walk decay factor 𝛼 and the dimensionality 𝛽 . Varying 𝛼 . Figure 9 illustrates the Acc scores of HOPE, HOPE+ (FNEM), and HOPE+ (SNEM) on 6 datasets, including CORA , CiteSeer , BlogCatalog , Flickr , PubMed , and CORA-F , when 𝛼 is varied from 0 . 1 to 0 . 9 with step size 0 . 1. The F1, NMI, and ARI results are qualitatively similar, and thus, are omitted here for the interest of space. From Figure 9, we can see that, on CORA , BlogCatalog , and PubMed , the clustering accuracy scores achieved by HOPE and HOPE+ are stable at first, and then decrease remarkably when 𝛼 roughly is beyond 0 . 5. Additionally, the clustering performance of all three methods witnesses a significant drop on CORA-F after 𝛼 exceeds 0 . 2. Recall that in Eq. (5), a larger 𝛼 results in higher weights ( i.e. , ( 1 -𝛼 ) 𝛼 𝜆 ) to distant vertices in H . Therefore, it can be concluded that on CORA , BlogCatalog , PubMed , and CORA-F , the affinities of the given vertex 𝑢 𝑖 and its proximal vertices play a more important role in the clustering. Interestingly, on CiteSeer , the performance of both HOPE and HOPE+ increases slightly with HOPE HOPE+ (FNEM) HOPE+ (SNEM) 𝑘 2 𝑘 3 𝑘 4 𝑘 5 𝑘 6 𝑘 7 𝑘 8 𝑘 10 0 10 1 10 2 running time (seconds) (a) CORA-F 𝑘 2 𝑘 3 𝑘 4 𝑘 5 𝑘 6 𝑘 7 𝑘 8 𝑘 10 1 10 1 . 5 10 2 running time (seconds) (b) MIND 𝑘 2 𝑘 3 𝑘 4 𝑘 5 𝑘 6 𝑘 7 𝑘 8 𝑘 10 2 10 3 10 4 running time (seconds) (c) LastFM Figure 11: Running time vs. 𝛽 . 𝛼 , implying the consideration of more far-reaching vertices benefits the clustering. A similar observation can be made on Flickr for HOPE+, while the accuracy scores of HOPE slump by more than 10% when 𝛼 > 0 . 2. Further, Figure 9 reveals that HOPE+ yields better stability over HOPE, particularly on CORA and Flickr , when varying 𝛼 . Varying 𝛽 . Figure 10 reports the clustering accuracy of our methods when varying 𝛽 from 𝑘 to 8 𝑘 ( 𝑘 is the number of clusters per dataset). Observe that, on most datasets, the clustering accuracy scores attained by HOPE, HOPE+ (FNEM), and HOPE+ (SNEM) initially experience a rapid increase when 𝛽 is varied from 𝑘 to 3 𝑘 , followed by remaining stable with slight fluctuations when 𝛽 ≥ 3 𝑘 . To explain, recall that 𝛽 is used as the dimensionality of the lowrank matrix X to approximate H . Intuitively, a larger 𝛽 gives a more accurate low-rank approximation of H , thereby leading to higher result quality in 𝑘 -BGC. Furthermore, Figure 11 presents the running time in seconds of HOPE, HOPE+ (FNEM), and HOPE+ (SNEM) when 𝛽 is varied from 𝑘 to 8 𝑘 on 3 representative large datasets CORA-F , MIND , and LastFM . The runtime of HOPE and HOPE+ is roughly proportional to 𝛽 , coinciding with our asymptotic analysis of HOPE and HOPE+ in Sections 3 and 4.4, respectively.",
  "6 RELATED WORK": "This section reviews existing studies germane to the 𝑘 -BGC problem in this paper. A common methodology for 𝑘 -BGC is to first project a bipartite graph G into a unipartite graph G ∗ , i.e. , a graph composed of a single type of vertices U , by connecting two vertices from U if they share common neighborhoods in G . After that, 11 Renchi Yang and Jieming Shi classic graph clustering algorithms can be applied naturally to G ∗ . A variety of methods [70, 71] have been proposed for weighting the connections in G ∗ . Melamed [36] proposed a dual-projection method, which extracts clusters of two one-mode graphs based on the two vertex sets in G independently and then combines the solutions such that the within-community ties are maximized. ComSim [52] detects clusters of one single type of vertex using a similarity measure of common neighbors and maximizes vertices' similarity by identifying cycles of connections. In spite of also relying on projected graphs, our methods differ from previous projectionbased methods. To be more precise, our solutions make use of the high-order affinities between target vertex set U and the vertices in the proposed weighted projected graph GV for dividing U into clusters, whereas prior methods partition vertices in GV based on their direct connections in GV . Recent 𝑘 -BGC solutions propose to simultaneously group vertices in each vertex set of G into clusters. Dhillon [12] extended spectral graph partitioning [44] to bipartite graphs. SBC algorithm [31] finds a blockwise-constant checkerboard matrix as a good approximation of a bipartite graph G for clustering. The state-of-theart solutions to 𝑘 -BGC are based on statistical models. biSBM-KL [32] includes a degree-corrected bipartite stochastic block model for inferring clusters and maximizes the likelihood function using the Kernighan-Lin algorithm [30]. In follow-up work, Yen and Larremore [67] further developed biSBM-MCMC, which adopts a Markov Chain Monte-Carlo sampler for fast optimization. Our methods outperform these approaches, as validated by our experiments, due to our novel designs that efficiently and effectively exploit high-order relationships between vertices. Instead of assuming the number 𝑘 of clusters to be known a priori, another line of research is devoted to discovering 𝑘 automatically during clustering and various techniques have been proposed. Lehmann et al. [33] presented biGcli/q.sc_u.sce for detecting biclique clusters in bipartite graphs by extending the 𝑘 -clique community [41] to bipartite graphs. Considerable efforts have been devoted to extending the notion of modularity [40] in unipartite graphs to bipartite graphs as bi-modularity for jointly clustering two types of vertices based on bi-modularity maximization [2, 23]. Guimera et al. [23] use the cumulative deviation from the random expectation of the number of neighbors shared by vertices in the same cluster to define bi-modularity. Another bi-modularity objective is defined in [2, 3] with the consideration of connectivity of both vertex sets in a bipartite graph, and a BRIM algorithm is presented to solve the objective. Subsequent studies [13, 35, 37, 42, 51, 68] propose more bi-modularity definitions as well as related optimization algorithms. However, these modularity-based methods suffer from the resolution limit issue [18] due to failing to accurately detect small-sized clusters with high modularity scores. In addition, classic graph clustering algorithms [17] and standard data clustering algorithms can be applied to 𝑘 -BGC by considering bipartite graphs as general graphs and data matrices, respectively. Since they are not specially catered for bipartite graphs, these methods cannot capture the unique characteristics of bipartite structures, resulting in a mediocre performance, as shown in Section 5.",
  "7 CONCLUSION": "In this paper, we propose efficient solutions, HOPE and HOPE+ for high-quality 𝑘 -BGC. HOPE achieves high scalability and clustering quality by formulating the 𝑘 -BGC as an optimization problem based on our proposed HOP vectors and an efficient low-rank approximation of the HOP vectors. HOPE+ further improves over HOPE in terms of both empirical efficiency and clustering accuracy through a novel problem transformation and a carefully-crafted two-stage optimization framework, as demonstrated by our extensive experiments. In the future, we intend to extend HOPE and HOPE+ to handle bipartite graphs with vertex attributes.",
  "ACKNOWLEDGMENTS": "Renchi Yang is supported by the NSFC YSF grant (No. 62302414) and Hong Kong RGC ECS grant (No. 22202623). Jieming Shi is supported by Hong Kong RGC ECS (No. 25201221) and NSFC 62202404.",
  "A PROOFS": "Proof of Lemma 2.2. First, we define a matrix 𝛀 ∈ R | V | × | V | where 𝛀 𝑗,𝑙 = ˝ 𝑢 𝑖 ∈U 𝑝 ( 𝑣 𝑗 , 𝑢 𝑖 ) · 𝑝 ( 𝑢 𝑖 , 𝑣 𝑙 ) . It can be seen that 𝛀 is a non-negative row-stochastic matrix, i.e., ˝ 𝑣 𝑙 ∈V 𝛀 𝑗,𝑙 = 1. As per [1] and the definition of 𝛀 , there exists a probability distribution 𝝅 ( 𝑗 ) , such that lim 𝜆 →∞ 𝛀 𝜆 𝑗 = 𝝅 for 1 ≤ 𝑗 ≤ |V| . Next, let 𝚫 be a |V| × |V| diagonal matrix wherein 𝚫 𝑗,𝑗 equals to 𝑤𝑠 ( 𝑣 𝑗 ) = ˝ 𝑢 ℎ ∈U 𝑤 ( 𝑣 𝑗 , 𝑢 ℎ ) . By Eq. (1) and (4), we can derive  which implies that QQ ⊤ = 𝚫 1 2 𝛀𝚫 -1 2 . Thus, each term in Eq. (5) satisfies P ( QQ ⊤ ) 𝜆 = P 𝚫 1 2 𝛀 𝜆 𝚫 -1 2 . Since lim 𝜆 →∞ 𝛀 𝜆 𝑗 = 𝝅 ( 𝑗 ) ∀ 1 ≤ 𝑗 ≤ |V| , each row of P ( QQ ⊤ ) 𝜆 will converge to a vector when 𝜆 → ∞ . In addition, 𝛀 is a non-negative row-stochastic matrix, then 𝛀 𝜆 is also a non-negative row-stochastic matrix, meaning that each entry in 𝛀 is not greater than 1. Therefore, ( P ( QQ ⊤ ) 𝜆 ) 𝑖,𝑗 = ( P 𝚫 1 2 𝛀 𝜆 𝚫 -1 2 ) 𝑖,𝑗 ≤ P 𝑖,𝑗 holds for any 𝜆 ≥ 0, 𝑢 𝑖 ∈ U and 𝑣 𝑗 ∈ V , which further leads to  Since P ( QQ ⊤ ) 𝜆 is non-negative, F 𝑖,𝑗 ≥ 0, completing the proof. □ Proof of Lemma 3.1. By the definition of SVD, Q = U 𝚺 V ⊤ and right singular vectors V have orthogonal columns, i.e., V ⊤ V = I . Further, we have QQ ⊤ = U 𝚺 2 U ⊤ . Notice that the left singular vectors U also have orthogonal columns, i.e. , U ⊤ U = I . Hence,  Continuing forth, we prove that the largest singular value of Q is not greater than 1, namely 𝚺 1 , 1 ≤ 1. Firstly, as per Lemma 4.3, the largest singular value of Q is equal to the largest eigenvalue of QQ ⊤ . Thus, if we can prove that the largest eigenvalue of QQ ⊤ 12 Efficient High-Quality Clustering for Large Bipartite Graphs is less than or equal to 1, then 𝚺 1 , 1 ≤ 1 holds. For any 𝑣 𝑖 ∈ V , let 𝑧 𝑖 ( 𝑢 𝑙 ) = √︁ 𝑝 ( 𝑣 𝑖 , 𝑢 𝑙 ) · 𝑝 ( 𝑢 𝑙 , 𝑣 𝑖 ) . By Eq. (2), we can write ( QQ ⊤ ) 𝑖,𝑗 as  Also, on the basis of Eq. (1), it is easy to prove that  Then, for any vector y ∈ R | V | , the following inequality holds: y · ( I -QQ ⊤ ) · y ⊤ = ∑︁ 𝑣 𝑖 ∈V y 2 𝑖 -2 ∑︁ 𝑣 𝑖 ,𝑣 𝑗 ∈V y 𝑖 · ( QQ ⊤ ) 𝑖,𝑗 · y 𝑗   - 2 y ∑︁ ∑︁ 𝑖 · y 𝑗 · 𝑧 𝑗 ( 𝑢 𝑙 ) · 𝑧 𝑖 ( 𝑢 𝑙 ) 𝑣 𝑖 ,𝑣 𝑗 ∈V 𝑢 𝑙 ∈U  which indicates that QQ ⊤ is positive semidefinite, and hence, gives us the following Rayleigh quotient  The above inequality implies that the largest eigenvalue of QQ ⊤ is less than or equal to 1, and accordingly 𝚺 1 , 1 ≤ 1. Furthermore, we can derive that ( 𝛼 𝚺 2 ) ∞ = 0 and plugging it into Eq. (16) yields  The lemma is proved. □ Proof of Theorem 3.2. Since the 𝐿 2 norm of any HOP vector H 𝑖 is 1 (E.q (6)), we then derive that  Likewise, ∥ X 𝑖 -X 𝑗 ∥ 2 2 = 2 ( 1 -X 𝑖 · X 𝑗 ) since each row of X has a unit 𝐿 2 norm (Line 4 in Algorithm 1). In addition, by Eq. (8) and the fact that ∥ b X 𝑖 ∥ 2 = √︃ b X 𝑖 · b X 𝑖 , we can get X 𝑖 · X 𝑗 = b X 𝑖 · b X 𝑗 ∥ b X 𝑖 ∥ 2 · ∥ b X 𝑗 ∥ 2 =   and particularly, FF ⊤ = PU ( 1 -𝛼 1 -𝛼 · 𝚺 2 ) 2 U ⊤ P ⊤ , where U and 𝚺 are the full left singular vectors and singular values of Q , respectively. By the fact that every element in P is less than 1, we have  Theorem A.1 (Eckart-Young Theorem [21]). Suppose that M 𝑘 ∈ R 𝑛 × 𝑘 is the rank𝑘 approximation to M ∈ R 𝑛 × 𝑛 obtained by exact SVD, then min 𝑟𝑎𝑛𝑘 ( b M ) ≤ 𝑘 ∥ M -b M ∥ 2 = ∥ M -M 𝑘 ∥ 2 = 𝜎 𝑘 + 1 , where 𝜎 𝑖 represents the 𝑖 -th largest singular value of M . As per the definitions, U ( 1 -𝛼 1 -𝛼 · 𝚺 2 ) 2 U ⊤ is the rank𝑘 approximation of U ( 1 -𝛼 1 -𝛼 · 𝚺 2 ) 2 U ⊤ . By Theorem A.1, Inequality (19) becomes  where 𝜎 = ( 1 -𝛼 1 -𝛼 · 𝚺 2 𝛽 + 1 ,𝛽 + 1 ) 2 and 𝚺 𝛽 + 1 ,𝛽 + 1 is the ( 𝛽 + 1 ) -th largest singular value of Q . Since for any matrix M , ∥ M ∥ max ≤ ∥ M ∥ 2, we have ∥ FF ⊤ -b X b X ⊤ ∥ max ≤ 𝜎 , leading to | b X 𝑖 · b X 𝑗 -F 𝑖 · F 𝑗 | ≤ 𝜎 , ∀ 𝑢 𝑖 ∈ U . Recall that H 𝑖 = F 𝑖 ∥ F 𝑖 ∥ 2 in Eq. (6). Consequently, we have  Note that ∥ X 𝑖 -X 𝑗 ∥ 2 2 -∥ H 𝑖 -H 𝑗 ∥ 2 2 = 2 ( H 𝑖 · H 𝑗 -X 𝑖 · X 𝑗 ) . Plugging Eq. (20) into the above equation completes the proof. □ Proof of Lemma 4.4. First, by the definition of matrix Frobenius norm and matrix trace property,  Suppose that 𝚽𝚺𝚿 ⊤ is the exact full SVD of L ⊤ C , where the left and right singular vectors satisfy 𝚽 ⊤ 𝚽 = 𝚿 ⊤ 𝚿 = I . Further, by the cyclic property of matrix trace and the above equation, min T ∥ LT -C ∥ 𝐹 is equivalent to  Observe that the rows of 𝚽 ⊤ T 𝚿 are orthogonal, namely 𝚽 ⊤ T 𝚿 · ( 𝚽 ⊤ T 𝚿 ) ⊤ = I , meaning that -1 ≤ ( 𝚽 ⊤ T 𝚿 ) 𝑖,𝑗 ≤ 1 ∀ 1 ≤ 𝑖, 𝑗 ≤ 𝑘 . Therefore, we have ˝ 𝑘 𝑖 = 1 ( 𝚽 ⊤ T 𝚿 ) 𝑖,𝑖 · 𝚺 𝑖,𝑖 ≤ ˝ 𝑘 𝑖 = 1 𝚺 𝑖,𝑖 . In particular, the upper bound ˝ 𝑘 𝑖 = 1 𝚺 𝑖,𝑖 is attained when 𝚽 ⊤ T 𝚿 = I , implying that T = 𝚽𝚿 ⊤ . Thus, the lemma holds. □ Proof of Lemma 4.5. Suppose that T ∗ is the optimal solution to the objective function in Eq. (15). In the first place, it must hold that ∥ LT ∗ -C ∥ 2 ≤ ∥ LL ⊤ C -C ∥ 2 since T ∗ is the minimizer. On the other hand, by the Pythagorean theorem [57], for any vector v ,  and hence ∥ LT ∗ -C ∥ 2 ≥ ∥ LL ⊤ C -C ∥ 2. As a consequence, ∥ LT ∗ -C ∥ 2 = ∥ LL ⊤ C -C ∥ 2 and  Moreover, recall that L is the 𝑘 -largest eigenvectors, which satisfies L ⊤ L = I . Accordingly, as per Eq. (21), T ∗ = L ⊤ LT ∗ = L ⊤ LL ⊤ C = L ⊤ C , signifying that the optimal such T in Eq. (15) is L ⊤ C . The proof is completed. □ 13 Renchi Yang and Jieming Shi",
  "REFERENCES": "[1] Jac M Anthonisse and Henk Tijms. 1977. Exponential convergence of products of stochastic matrices. J. Math. Anal. Appl. 59, 2 (1977), 360-364. Michael J Barber. 2007. Modularity and community detection in bipartite net- [2] works. Physical Review E 76, 6 (2007), 066102. [3] Michael J Barber, Margarida Faria, Ludwig Streit, and Oleg Strogan. 2008. Searching for communities in bipartite networks. In AIP Conference Proceedings , Vol. 1021. American Institute of Physics, 171-182. [4] Vladimir Batagelj and Ulrik Brandes. 2005. Efficient generation of large random networks. Physical Review E 71, 3 (2005), 036113. [5] Aleksandar Bojchevski and Stephan Günnemann. 2018. Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking. In International Conference on Learning Representations . https://openreview.net/forum? id=r1ZdKJ-0W [6] Aleksandar Bojchevski, Johannes Klicpera, Bryan Perozzi, Amol Kapoor, Martin Blais, Benedek Rózemberczki, Michal Lukasik, and Stephan Günnemann. 2020. Scaling Graph Neural Networks with Approximate PageRank. In SIGKDD . [7] Gabriel Budel and Piet Van Mieghem. 2020. Detecting the number of clusters in a network. Journal of Complex Networks 8, 6 (2020), cnaa047. [8] Paola Gabriela Pesántez Cabrera. 2018. Bipartite Network Community Detection: Algorithms and Applications . Washington State University. [9] O. Celma. 2010. Music Recommendation and Discovery in the Long Tail . Springer. [10] Yizong Cheng and George M Church. 2000. Biclustering of expression data.. In Ismb , Vol. 8. 93-103. [11] Wenyuan Dai, Gui-Rong Xue, Qiang Yang, and Yong Yu. 2007. Co-clustering based classification for out-of-domain documents. In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining . 210-219. [12] Inderjit S Dhillon. 2001. Co-clustering documents and words using bipartite spectral graph partitioning. In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining . 269-274. [13] Carsten F Dormann and Rouven Strauss. 2014. A method for detecting modules in quantitative bipartite networks. Methods in Ecology and Evolution 5, 1 (2014), 90-98. [14] Jennifer A Dunne, Richard J Williams, and Neo D Martinez. 2002. Network structure and biodiversity loss in food webs: robustness increases with connectance. Ecology letters 5, 4 (2002), 558-567. [15] Ky Fan. 1949. On a theorem of Weyl concerning eigenvalues of linear transformations I. Proceedings of the National Academy of Sciences of the United States of America 35, 11 (1949), 652. [16] Yiling Chen Frederico Fonseca. 2003. A bipartite graph co-clustering approach to ontology mapping. In Proceedings of the Workshop on Semantic Web Technologies for Searching and Retrieving Scientific Data. Colocated with the Second International Semantic Web Conference (ISWC-03), CEUR-WS. org . [17] Santo Fortunato. 2010. Community detection in graphs. Physics reports 486, 3-5 (2010), 75-174. [18] Santo Fortunato and Marc Barthelemy. 2007. Resolution limit in community Proceedings of the national academy of sciences detection. 104, 1 (2007), 36-41. [19] Lise Getoor. 2005. Link-based classification. In Advanced methods for knowledge discovery from complex data . Springer, 189-207. [20] Michelle Girvan and Mark EJ Newman. 2002. Community structure in social and biological networks. Proceedings of the national academy of sciences 99, 12 (2002), 7821-7826. [21] Gene H Golub and Charles F Van Loan. 1996. Matrix computations. Johns Hopkins University, Press (1996). [22] John C Gower and Garmt B Dijksterhuis. 2004. Procrustes problems . Vol. 30. OUP Oxford. [23] Roger Guimera, Marta Sales-Pardo, and Luís A Nunes Amaral. 2007. Module identification in bipartite and directed networks. Physical Review E 76, 3 (2007), 036102. [24] John A Hartigan and Manchek A Wong. 1979. Algorithm AS 136: A k-means clustering algorithm. Journal of the royal statistical society. series c (applied statistics) 28, 1 (1979), 100-108. [25] Reinhard Heckel, Michail Vlachos, Thomas Parnell, and Celestine Dünner. 2017. Scalable and interpretable product recommendations via overlapping co-clustering. In 2017 IEEE 33rd International Conference on Data Engineering (ICDE) . IEEE, 1033-1044. [26] Xiao Huang, Jundong Li, and Xia Hu. 2017. Label informed attributed network embedding. In Proceedings of the tenth ACM international conference on web search and data mining . 731-739. [27] Lawrence Hubert and Phipps Arabie. 1985. Comparing partitions. Journal of classification 2 (1985), 193-218. [28] Pedro Jordano, Jordi Bascompte, and Jens M Olesen. 2003. Invariant properties in coevolutionary networks of plant-animal interactions. Ecology letters 6, 1 (2003), 69-81. [29] Leonard Kaufman and Peter J Rousseeuw. 2009. Finding groups in data: an introduction to cluster analysis . Vol. 344. John Wiley & Sons. [30] Brian W Kernighan and Shen Lin. 1970. An efficient heuristic procedure for partitioning graphs. The Bell system technical journal 49, 2 (1970), 291-307. [31] Yuval Kluger, Ronen Basri, Joseph T Chang, and Mark Gerstein. 2003. Spectral biclustering of microarray data: coclustering genes and conditions. Genome research 13, 4 (2003), 703-716. [32] Daniel B Larremore, Aaron Clauset, and Abigail Z Jacobs. 2014. Efficiently inferring community structure in bipartite networks. Physical Review E 90, 1 (2014), 012805. [33] Sune Lehmann, Martin Schwartz, and Lars Kai Hansen. 2008. Biclique communities. Physical review E 78, 1 (2008), 016108. [34] Yvonne Y Li and Steven JM Jones. 2012. Drug repositioning for personalized medicine. Genome medicine 4 (2012), 1-14. [35] Xin Liu and Tsuyoshi Murata. 2010. Community detection in large-scale bipartite networks. Transactions of the Japanese Society for Artificial Intelligence 25, 1 (2010), 16-24. [36] David Melamed. 2014. Community structures in bipartite networks: A dualprojection approach. PloS one 9, 5 (2014), e97823. [37] Tsuyoshi Murata. 2009. Detecting communities from bipartite networks based on bipartite modularities. In 2009 International Conference on Computational Science and Engineering , Vol. 4. IEEE, 50-57. [38] Jose C Nacher and Jean-Marc Schwartz. 2012. Modularity in protein complex and drug interactions reveals new polypharmacological properties. PloS one 7, 1 (2012), e30028. [39] Mark EJ Newman. 2006. Finding community structure in networks using the eigenvectors of matrices. Physical review E 74, 3 (2006), 036104. [40] Mark EJ Newman and Michelle Girvan. 2004. Finding and evaluating community structure in networks. Physical review E 69, 2 (2004), 026113. [41] Gergely Palla, Imre Derényi, Illés Farkas, and Tamás Vicsek. 2005. Uncovering the overlapping community structure of complex networks in nature and society. nature 435, 7043 (2005), 814-818. [42] Paola Pesántez-Cabrera and Ananth Kalyanaraman. 2017. Efficient detection of communities in biological bipartite networks. IEEE/ACM transactions on computational biology and bioinformatics 16, 1 (2017), 258-271. [43] Pascal Pons and Matthieu Latapy. 2005. Computing communities in large networks using random walks. In International symposium on computer and information sciences . Springer, 284-293. [44] Alex Pothen, Horst D Simon, and Kang-Pu Liou. 1990. Partitioning sparse matrices with eigenvectors of graphs. SIAM journal on matrix analysis and applications 11, 3 (1990), 430-452. [45] Benedek Rozemberczki and Rik Sarkar. 2020. Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models. In Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM '20) . ACM, 1325-1334. [46] Risa D Sargent and David D Ackerly. 2008. Plant-pollinator interactions and the assembly of plant communities. Trends in Ecology & Evolution 23, 3 (2008), 123-130. [47] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. 2008. Collective classification in network data. AI magazine 29, 3 (2008), 93-93. [48] Arnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, and Kuansan Wang. 2015. An Overview of Microsoft Academic Service (MAS) and Applications. In The WebConf . [49] Gilbert Strang, Gilbert Strang, Gilbert Strang, and Gilbert Strang. 1993. Introduction to linear algebra . Vol. 3. Wellesley-Cambridge Press. [50] Alexander Strehl and Joydeep Ghosh. 2002. Cluster ensembles-a knowledge reuse framework for combining multiple partitions. Journal of machine learning research 3, Dec (2002), 583-617. [51] Kenta Suzuki and Ken Wakita. 2009. Extracting multi-facet community structure from bipartite networks. In 2009 International Conference on Computational Science and Engineering , Vol. 4. IEEE, 312-319. [52] Raphael Tackx, Fabien Tarissan, and Jean-Loup Guillaume. 2018. ComSim: a bipartite community detection algorithm using cycle and node's similarity. In Complex Networks & Their Applications VI: Proceedings of Complex Networks 2017 (The Sixth International Conference on Complex Networks and Their Applications) . Springer, 278-289. [53] Lei Tang and Huan Liu. 2009. Relational learning via latent social dimensions. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining . 817-826. [54] Hanghang Tong, Christos Faloutsos, and Jia-Yu Pan. 2006. Fast random walk with restart and its applications. In Sixth international conference on data mining (ICDM'06) . IEEE, 613-622. [55] Ulrike Von Luxburg. 2007. A tutorial on spectral clustering. Statistics and computing 17, 4 (2007), 395-416. [56] Sibo Wang, Renchi Yang, Xiaokui Xiao, Zhewei Wei, and Yin Yang. 2017. FORA: simple and effective approximate single-source personalized pagerank. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . 505-514. 14 Efficient High-Quality Clustering for Large Bipartite Graphs [57] David P Woodruff. 2014. Sketching as a Tool for Numerical Linear Algebra. (2014). [58] Fangzhao Wu, Ying Qiao, Jiun-Hung Chen, Chuhan Wu, Tao Qi, Jianxun Lian, Danyang Liu, Xing Xie, Jianfeng Gao, Winnie Wu, and Ming Zhou. 2020. MIND: A Large-scale Dataset for News Recommendation. In ACL . 3597-3606. [59] Yao Wu, Xudong Liu, Min Xie, Martin Ester, and Qing Yang. 2016. CCCF: Improving collaborative filtering via scalable user-item co-clustering. In Proceedings of the ninth ACM international conference on web search and data mining . 73-82. [60] Rongkai Xia, Yan Pan, Lei Du, and Jian Yin. 2014. Robust multi-view spectral clustering via low-rank and sparse decomposition. In Proceedings of the AAAI conference on artificial intelligence , Vol. 28. [61] Wei Xu, Xin Liu, and Yihong Gong. 2003. Document clustering based on nonnegative matrix factorization. In SIGIR . 267-273. [62] Renchi Yang. 2022. Efficient and Effective Similarity Search over Bipartite Graphs. In Proceedings of the ACM Web Conference 2022 . 308-318. [63] Renchi Yang, Jieming Shi, Keke Huang, and Xiaokui Xiao. 2022. Scalable and Effective Bipartite Network Embedding. In Proceedings of the 2022 International Conference on Management of Data . 1977-1991. [64] Renchi Yang, Jieming Shi, Xiaokui Xiao, Yin Yang, and Sourav S Bhowmick. 2020. Homogeneous network embedding for massive graphs via reweighted personalized PageRank. Proceedings of the VLDB Endowment 13, 5 (2020), 670683. [65] Renchi Yang, Jieming Shi, Yin Yang, Keke Huang, Shiqi Zhang, and Xiaokui Xiao. 2021. Effective and scalable clustering on massive attributed graphs. In Proceedings of the Web Conference 2021 . 3675-3687. [66] Mao Ye, Dong Shou, Wang-Chien Lee, Peifeng Yin, and Krzysztof Janowicz. 2011. On the semantic annotation of places in location-based social networks. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining . 520-528. [67] Tzu-Chi Yen and Daniel B Larremore. 2020. Community detection in bipartite networks with stochastic block models. Physical Review E 102, 3 (2020), 032309. [68] Weihua Zhan, Zhongzhi Zhang, Jihong Guan, and Shuigeng Zhou. 2011. Evolutionary method for finding communities in bipartite networks. Physical review E 83, 6 (2011), 066120. [69] Tian Zhang, Raghu Ramakrishnan, and Miron Livny. 1996. BIRCH: an efficient data clustering method for very large databases. SIGMOD (1996), 103-114. [70] Tao Zhou, Jie Ren, Matúš Medo, and Yi-Cheng Zhang. 2007. Bipartite network projection and personal recommendation. Physical review E 76, 4 (2007), 046115. [71] Katharina Anna Zweig and Michael Kaufmann. 2011. A systematic approach to the one-mode projection of bipartite graphs. Social Network Analysis and Mining 1 (2011), 187-218. 15",
  "keywords_parsed": [
    "Bipartite Graph",
    "Clustering",
    "Random Walk",
    "Eigenvector"
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "Exponential convergence of products of stochastic matrices"
    },
    {
      "ref_id": "b2",
      "title": "Modularity and community detection in bipartite networks"
    },
    {
      "ref_id": "b3",
      "title": "Searching for communities in bipartite networks"
    },
    {
      "ref_id": "b4",
      "title": "Efficient generation of large random networks"
    },
    {
      "ref_id": "b5",
      "title": "Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking"
    },
    {
      "ref_id": "b6",
      "title": "Scaling Graph Neural Networks with Approximate PageRank"
    },
    {
      "ref_id": "b7",
      "title": "Detecting the number of clusters in a network"
    },
    {
      "ref_id": "b8",
      "title": "Bipartite Network Community Detection: Algorithms and Applications"
    },
    {
      "ref_id": "b9",
      "title": "Music Recommendation and Discovery in the Long Tail"
    },
    {
      "ref_id": "b10",
      "title": "Biclustering of expression data"
    },
    {
      "ref_id": "b11",
      "title": "Co-clustering based classification for out-of-domain documents"
    },
    {
      "ref_id": "b12",
      "title": "Co-clustering documents and words using bipartite spectral graph partitioning"
    },
    {
      "ref_id": "b13",
      "title": "A method for detecting modules in quantitative bipartite networks"
    },
    {
      "ref_id": "b14",
      "title": "Network structure and biodiversity loss in food webs: robustness increases with connectance"
    },
    {
      "ref_id": "b15",
      "title": "On a theorem of Weyl concerning eigenvalues of linear transformations I"
    },
    {
      "ref_id": "b16",
      "title": "A bipartite graph co-clustering approach to ontology mapping"
    },
    {
      "ref_id": "b17",
      "title": "Community detection in graphs"
    },
    {
      "ref_id": "b18",
      "title": "Resolution limit in community detection"
    },
    {
      "ref_id": "b19",
      "title": "Link-based classification"
    },
    {
      "ref_id": "b20",
      "title": "Community structure in social and biological networks"
    },
    {
      "ref_id": "b21",
      "title": "Matrix computations"
    },
    {
      "ref_id": "b22",
      "title": "Procrustes problems"
    },
    {
      "ref_id": "b23",
      "title": "Module identification in bipartite and directed networks"
    },
    {
      "ref_id": "b24",
      "title": "Algorithm AS 136: A k-means clustering algorithm"
    },
    {
      "ref_id": "b25",
      "title": "Scalable and interpretable product recommendations via overlapping co-clustering"
    },
    {
      "ref_id": "b26",
      "title": "Label informed attributed network embedding"
    },
    {
      "ref_id": "b27",
      "title": "Comparing partitions"
    },
    {
      "ref_id": "b28",
      "title": "Invariant properties in coevolutionary networks of plant-animal interactions"
    },
    {
      "ref_id": "b29",
      "title": "Finding groups in data: an introduction to cluster analysis"
    },
    {
      "ref_id": "b30",
      "title": "An efficient heuristic procedure for partitioning graphs"
    },
    {
      "ref_id": "b31",
      "title": "Spectral biclustering of microarray data: coclustering genes and conditions"
    },
    {
      "ref_id": "b32",
      "title": "Efficiently inferring community structure in bipartite networks"
    },
    {
      "ref_id": "b33",
      "title": "Biclique communities"
    },
    {
      "ref_id": "b34",
      "title": "Drug repositioning for personalized medicine"
    },
    {
      "ref_id": "b35",
      "title": "Community detection in large-scale bipartite networks"
    },
    {
      "ref_id": "b36",
      "title": "Community structures in bipartite networks: A dual-projection approach"
    },
    {
      "ref_id": "b37",
      "title": "Detecting communities from bipartite networks based on bipartite modularities"
    },
    {
      "ref_id": "b38",
      "title": "Modularity in protein complex and drug interactions reveals new polypharmacological properties"
    },
    {
      "ref_id": "b39",
      "title": "Finding community structure in networks using the eigenvectors of matrices"
    },
    {
      "ref_id": "b40",
      "title": "Finding and evaluating community structure in networks"
    },
    {
      "ref_id": "b41",
      "title": "Uncovering the overlapping community structure of complex networks in nature and society"
    },
    {
      "ref_id": "b42",
      "title": "Efficient detection of communities in biological bipartite networks"
    },
    {
      "ref_id": "b43",
      "title": "Computing communities in large networks using random walks"
    },
    {
      "ref_id": "b44",
      "title": "Partitioning sparse matrices with eigenvectors of graphs"
    },
    {
      "ref_id": "b45",
      "title": "Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models"
    },
    {
      "ref_id": "b46",
      "title": "Plant-pollinator interactions and the assembly of plant communities"
    },
    {
      "ref_id": "b47",
      "title": "Collective classification in network data"
    },
    {
      "ref_id": "b48",
      "title": "An Overview of Microsoft Academic Service (MAS) and Applications"
    },
    {
      "ref_id": "b49",
      "title": "Introduction to linear algebra"
    },
    {
      "ref_id": "b50",
      "title": "Cluster ensembles-a knowledge reuse framework for combining multiple partitions"
    },
    {
      "ref_id": "b51",
      "title": "Extracting multi-facet community structure from bipartite networks"
    },
    {
      "ref_id": "b52",
      "title": "ComSim: a bipartite community detection algorithm using cycle and node's similarity"
    },
    {
      "ref_id": "b53",
      "title": "Relational learning via latent social dimensions"
    },
    {
      "ref_id": "b54",
      "title": "Fast random walk with restart and its applications"
    },
    {
      "ref_id": "b55",
      "title": "A tutorial on spectral clustering"
    },
    {
      "ref_id": "b56",
      "title": "FORA: simple and effective approximate single-source personalized pagerank"
    },
    {
      "ref_id": "b57",
      "title": "Sketching as a Tool for Numerical Linear Algebra"
    },
    {
      "ref_id": "b58",
      "title": "MIND: A Large-scale Dataset for News Recommendation"
    },
    {
      "ref_id": "b59",
      "title": "CCCF: Improving collaborative filtering via scalable user-item co-clustering"
    },
    {
      "ref_id": "b60",
      "title": "Robust multi-view spectral clustering via low-rank and sparse decomposition"
    },
    {
      "ref_id": "b61",
      "title": "Document clustering based on nonnegative matrix factorization"
    },
    {
      "ref_id": "b62",
      "title": "Efficient and Effective Similarity Search over Bipartite Graphs"
    },
    {
      "ref_id": "b63",
      "title": "Scalable and Effective Bipartite Network Embedding"
    },
    {
      "ref_id": "b64",
      "title": "Homogeneous network embedding for massive graphs via reweighted personalized PageRank"
    },
    {
      "ref_id": "b65",
      "title": "Effective and scalable clustering on massive attributed graphs"
    },
    {
      "ref_id": "b66",
      "title": "On the semantic annotation of places in location-based social networks"
    },
    {
      "ref_id": "b67",
      "title": "Community detection in bipartite networks with stochastic block models"
    },
    {
      "ref_id": "b68",
      "title": "Evolutionary method for finding communities in bipartite networks"
    },
    {
      "ref_id": "b69",
      "title": "BIRCH: an efficient data clustering method for very large databases"
    },
    {
      "ref_id": "b70",
      "title": "Bipartite network projection and personal recommendation"
    },
    {
      "ref_id": "b71",
      "title": "A systematic approach to the one-mode projection of bipartite graphs"
    }
  ]
}