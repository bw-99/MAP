{"title": "\"Why Here and Not There?\" -Diverse Contrasting Explanations of Dimensionality Reduction", "authors": "Andr\u00e9 Artelt; Alexander Schulz", "pub_date": "", "abstract": "Dimensionality reduction is a popular preprocessing and a widely used tool in data mining. Transparency, which is usually achieved by means of explanations, is nowadays a widely accepted and crucial requirement of machine learning based systems like classifiers and recommender systems. However, transparency of dimensionality reduction and other data mining tools have not been considered in much depth yet, still it is crucial to understand their behavior -in particular practitioners might want to understand why a specific sample got mapped to a specific location. In order to (locally) understand the behavior of a given dimensionality reduction method, we introduce the abstract concept of contrasting explanations for dimensionality reduction, and apply a realization of this concept to the specific application of explaining two dimensional data visualization.", "sections": [{"heading": "I. INTRODUCTION", "text": "Transparency of machine learning (ML) based system, applied in the real world, is nowadays a widely accepted requirement -the importance of transparency was also recognized by the policy makers and therefore made its way into legal regulations like the EU's GDPR [1]. A popular way of achieving transparency is by means of explanations [2] which then gave rise to the field of eXplainable AI (XAI) [3], [4]. Although a lot of different explanation methodologies for ML based systems have been developed [2], [4], it is important to realize that it is still somewhat unclear what exactly makes up a good explanation [5], [6]. Therefore one must carefully pick the right explanation in the right situation, as there are (potentially) different target users with different goals [7] -e.g. ML engineers need explanations that help them to improve the system, while lay users need trust building explanations. Popular explanations methods [2], [4] are feature relevance/importance methods [8], and examples based methods [9] which use a set or a single example for explaining the behavior of the system. Instances of example based methods are contrasting explanations like counterfactual explanations [10], [11] and prototypes & criticisms [12].\nWe gratefully acknowledge funding from the VW-Foundation for the project IMPACT funded in the frame of the funding line AI and its Implications for Future Society.\nAndr\u00e9 Artelt is also affiliated with KIOS Research and Innovation Center of Excellence, University of Cyprus, Nicosia, Cyprus.\nDimensionality reduction methods are a popular tool in data mining -e.g. data visualization -and an often used preprocessing in general ML pipelines [13]. Similar to other ML methods, dimensionality reduction methods itself are not easy to understand -i.e. a high-dimensional sample is \"somehow\" mapped to a low-dimensional sample without providing any explanation/reason of this mapping. A ML pipeline can not be transparent if it contains non-transparent preprocessings like dimensionality reduction, and a proper and responsible use of data analysis tools such as data visualization is not possible if the inner working of the tool is not understood. Therefore, we argue that there is a need for understanding dimensionality reduction methods -we aim to provide such an understanding by means of contrasting explanations.\nRelated work: In the context of explaining dimensionality reduction, only little work exists so far. Some approaches [14], [15] aim to infer global feature importance for a given data projection. Another work [16] estimates feature importance locally for a vicinity around a projected data point, using locally linear models. A recent paper [17] proposes to use local feature importance explanations by computing a local linear approximation for each reduced dimension, extracting feature importances from the weight vectors. Further, saliency map approaches such as the layer-wise relevance propagation (LRP) [18] could in principle be applied to a parametric dimensionality reduction mapping in order to obtain locally relevant features. However, these approaches do not provide contrasting explanations, in which we are interested here.\nOur contributions: First, we make a conceptional contribution by proposing a general formalization of diverse counterfactual explanations for explaining dimensionality reduction methods. Second, we propose concrete realizations of this concept for four popular representatives of parametric dimensionality reduction method classes: PCA (linear mappings), SOM [19] (topographic mappings), autoencoders [20] (neural networks) and parametric t-SNE [21] (parametric extensions of neighbor embedders). Finally, we empirically evaluate them in the particular use-case of two-dimensional data visualization.\nThe remainder of this work is structured as follows: First (Section II) we review the necessary foundations of dimensionality reduction and contrasting explanations. Next (Section III-A), we propose and formalize diverse counterfactual explanations for explaining dimensionality reduction -we first propose a general concept (Section III-A), and then propose practical realizations for popular parametric dimensionality reduction methods (Sections III-B,III-C). We empirically evaluate our proposed explanations in Section IV where we consider two-dimensional data visualization as a popular application of dimensionality reduction. Finally, this work closes with a summary and conclusion in Section V.", "publication_ref": ["b0", "b1", "b2", "b1", "b1", "b3", "b4", "b5", "b6", "b7", "b8", "b9", "b10", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "II. FOUNDATIONS", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A. Dimensionality Reduction", "text": "The common setting for dimensionality reduction (DR) is that data x i , i = 1, . . . , m are given in a high-dimensional input space X -we will assume X = R d in the following. The goal is to project them to lower-dimensional points y i , i = 1, . . . , m in R d -where for data visualization often d = 2 -, such that as much structure as possible is preserved. The precise mathematical formalization of the term \"structure preservation\" is then one of the key differences between different DR methods in literature [22]- [24].\nOne major view for grouping DR methods is whether they provide an explicit function \u03c6 : X \u2192 R d for projection, where the parameters of \u03c6 are adjusted by the according DR method, or whether no such functional form is assumed by the approach. The former methods are referred to as parametric and the latter ones as non-parametric [13], [22].\nSince we require parametric mappings in our work, we recap a few of the most popular parametric DR approaches in the following. However, since there do exist successful extensions for non-parametric approaches to also provide a parametric function, we will consider one of them here as well. We will consider these approaches again in our experiments.\n1) Linear Methods: The most classical DR methods are based on a linear functional form:\n\u03c6( x) = A x + b(1)\nwhere A \u2208 R d \u00d7d and b \u2208 R d . Particular instances are Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA) and also the mappings obtained by metric learning approaches such as the Large Margin Nearest Neighbor (LMNN) method [13]. These constitute different cost function based approaches for estimating the parameters of \u03c6(\u2022), but in the end result in such a linear parametric mapping Eq. (1).\n2) Topographic Mappings: A class of non-linear DR approaches is given by topographic mappings such as the Self Organizing Map (SOM) and the Generative Topographic Mapping (GTM). We consider the SOM as one representative of this class of methods in the following. The SOM [19] consists of a set of prototypes p z \u2208 R d which are mapped to an index set I, \u03c6 : R d \u2192 I -e.g. the prototypes are arranged as a two-dimensional grid: I \u2282 N 2 . The dimensionality reduction maps a given input x to the index of the closest prototype:\n\u03c6( x) = arg min z \u2208 I x -p z 2 .\n(2)\n3) Autoencoder: An autoencoder (AE) f \u03b8 : R d \u2192 R d is a neural network consisting of an encoder, mapping the input to a smaller representation (also called the bottleneck) and a decoder, mapping it back to the original input [20]:\nf \u03b8 ( x) = (dec \u03b8 \u2022 enc \u03b8 )( x),(3)\nwhich are trained to optimize the reconstruction loss. A (typically non-linear) dimensionality reduction \u03c6(\u2022) based on this approach consists of the encoder mapping:\n\u03c6( x) = enc \u03b8 ( x)(4)\n4) Neighbor Embeddings: The class of neighbor embedding methods constitutes a set of non-parametric approaches that are considered as the most successful or state-of-the-art techniques in many cases [13], [25], [26]. Instances are the very popular t-Distributed Stochastic Neighbor Embedding (t-SNE) and Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP) approaches [27], [28]. In the following, we consider t-SNE as a representative of this class of methods and, among its parametric extensions [21], [29], the approach Parametric t-SNE [21].\nParametric t-SNE, uses a neural network f \u03b8 : R d \u2192 R d for mapping a given input x to a lower-dimensional domain:\n\u03c6( x) = f \u03b8 ( x),(5)\nwith respect to the t-SNE cost function.\nWhile there do exist more families of DR approaches, such as manifold embeddings (including MVU and LLE) or discriminative/supervised DR, it would exceed the scope of the present work to investigate all possible choices.", "publication_ref": ["b12", "b14", "b3", "b12", "b3", "b9", "b10", "b3", "b15", "b16", "b17", "b18", "b11", "b19", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "B. Contrasting Explanations", "text": "Contrasting explanations state a change to some features of a given input such that the resulting data point causes a different behavior of the system/model than the original input does. Counterfactual explanations (often just called counterfactuals) are the most prominent instance of contrasting explanations [2]. One can think of a counterfactual explanation as a recommendation of actions that change the model's behavior/prediction. One reason why counterfactual explanations are so popular is that there exists evidence that explanations used by humans are often contrasting in nature [30] -i.e. people often ask questions like \"What would have to be different in order to observe a different outcome?\". It was also shown that such questions are useful to learn about an unknown functionality and exploit this knowledge to achieve some goals [31], [32].\nThe most prominent example from literature for illustrating the concept of a counterfactual explanation is the example of loan application: Imagine you applied for a loan at a bank. Unfortunately, the bank rejects your application. Now, you would like to know why. In particular, you would like to know what would have to be different so that your application would have been accepted. A possible explanation might be that you would have been accepted if you had earned 500$ more per month and if you had not had a second credit card.\nUnfortunately, many explanation methods (including counterfactual explanations) are lacking uniqueness: Often there exists more than one possible & valid explanation -this is called \"Rashomon effect\" [2] -and in such cases, it is not clear which or how many of the possible explanations should be presented to the user. See Figure 2 where we illustrate the concept of a counterfactual explanation, including the existing of multiple possible and valid counterfactuals. Most approaches ignore this problem, however, there exist a few approaches that propose to compute multiple diverse counterfactuals to make the user aware that there exist different possible explanations [33]- [35]. In order to keep the explanation (suggested changes) simple -i.e. we are looking for low-complexity explanations that are easy to understandan obvious strategy is to look for a small number of changes so that the resulting sample (counterfactual) is similar/close to the original sample. This is aimed to be captured by Definition  where (\u2022) denotes a loss function, y cf the target prediction, \u03b8(\u2022) a penalty for dissimilarity of x cf and x, and C > 0 denotes the regularization strength. The counterfactuals from Definition 1 are also called closest counterfactuals because the optimization problem Eq. ( 6) tries to find an explanation x cf that is as close as possible to the original sample x. However, other aspects like plausibility and actionability are ignored in Definition 1, but are covered in other work [36]- [38]. In this work, we refer to counterfactuals in the spirit of Definition 1. Note that counterfactual explanations also exist in the causality domain [39]. Here the knowledge of a structural causal model (SCM), describing the interaction of features, is assumed. This work is not based in the causality domain and we only consider counterfactual explanations as proposed by [10].\narg min xcf \u2208 R d h( x cf ), y cf + C \u2022 \u03b8( x cf , x)(6)", "publication_ref": ["b1", "b20", "b21", "b22", "b1", "b23", "b25", "b26", "b28", "b29"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "III. COUNTERFACTUAL EXPLANATIONS OF DIMENSIONALITY REDUCTION", "text": "In this section, we propose counterfactual explanations of dimensionality reduction -i.e. explaining why a specific point was mapped to a location instead of a requested different location. See Fig. 1 for an illustration of this problem. As it is the nature of counterfactual explanations, the explanations state how we have to (minimally) change the original sample such that it gets mapped to a requested location.\nWe argue that this type of explanation is in particular very well suited for explaining data visualization which is a common application of dimensionality reduction in data mining [13], [23], [40] -e.g. data is mapped to a twodimensional space which is then depicted in a scatter plot. For instance, we could utilize such explanations to explain outliers in the data visualization: I.e. explaining why a point got mapped far away from the other points instead of close to the other ones -a counterfactual explanation states how to change the outlier such that it is no longer an outlier in the visualization, which would allow us to learn something about the particular reasons why this point was flagged as an outlier in the visualization. See Figure 3 for an illustrative example where we explain anomalous pressure measurements in a water distribution network: We consider the hydraulically isolated \"Area A\" in the L-Town network [41] where 29 pressure sensors are installed -we simulate a sensor failure (constant added the original pressure value) in node n105. We pick an outlier x (see left plot in Figure 3) and compute a counterfactual explanation for each normal data point as a target mapping -i.e. asking which sensor measurements must be changed so that the overall measurement vector x cf is mapped to the specified location in the data visualization. When aggregating all explanations by summing up and normalizing the suggested changes for each sensor (see right plot in Figure 3), we are able to identify the faulty sensors and thereby \"explain\" the outlier.\nNote that existing explanation methods for explaining dimensionality reduction methods, which usually focus on feature importances (see Section I), can not provide such an explanation -i.e. answering contrasting questions like \"Why was the point mapped here and not there\". This is because they only highlight important features but do not suggest any changes or magnitude of changes that would yield a different (requested) mapping. However, as already mentioned in Section II-B, the Rashomon effect states that there might exist many possible explanations why a particular point was mapped far away from the others -we therefore aim for a set of diverse counterfactual explanations in order to learn the most about the observed mapping and provide different possibilities for actionable recourse.\nFirst, we formalize the general concept of (diverse) counterfactual explanations of dimensionality reduction in Section III-A. Next, we consider some popular parametric dimensionality reduction methods, and propose methods for efficiently computing single counterfactuals (see Section III-B) and diverse counterfactuals (see Section III-C).", "publication_ref": ["b3", "b13", "b30", "b31"], "figure_ref": ["fig_0", "fig_3", "fig_3", "fig_3"], "table_ref": []}, {"heading": "A. General Modeling", "text": "We assume the DR method is given as a mapping\n\u03c6 : R d \u2192 R d (7) with d > d .\nA counterfactual explanation of a sample x \u2208 R d is a sample x cf in the original domain (i.e. R d ) that differs in a few features only from the given original sample x, but is mapped to a requested location y cf \u2208 R d which is different from the mapping of the original sample x. We formalize this in Definition 2.\nDefinition 2 (Counterfactual Explanation of Dimensionality Reduction): For a given DR method \u03c6(\u2022) Eq. ( 7), a counterfactual explanation x cf \u2208 R d , y cf \u2208 R d of a specific sample x \u2208 R d is given as a solution to the following multi-criteria optimization problem:\nmin xcf \u2208 R d x -x cf 0 , \u03c6( x cf ) -y cf p ,(8)\nwhere p defines the norm that is used. As discussed in Section II-B, there usually exists more than one possible explanation (\"Rashomon effect\") -clearly this is the case for dimensionality reduction as well because dimensionality reduction is a many-to-one mapping (i.e. multiple points are mapped to the same location). In this context, a set of diverse (i.e. highly different) explanations would provide more information than a single explanation only. We therefore extend Definition 2 to a set of diverse counterfactuals explanations instead of a single one (see Definition 3). Definition 3 (Diverse Counterfactual Explanations of Dimensionality Reduction): For a given DR method \u03c6(\u2022) Eq. ( 7), a set of diverse counterfactual explanations\n{ x i cf \u2208 R d }, y cf \u2208 R d of a specific sample x \u2208 R d is\ngiven as a solution to the following multi-criteria optimization problem:\nmin { x i cf \u2208 R d } x -x i cf 0 , \u03c6( x i cf ) -y cf p , \u03c8( x i cf , x j cf )(9)\nwhere \u03c8 : R d \u00d7 R d \u2192 R + denotes a function measuring the pair-wise diversity of two given counterfactuals -i.e. returning a small value if the two counterfactuals are very different and a larger value otherwise. The term \"diversity\" itself is somewhat fuzzy and different use-cases might require different definitions of diverse counterfactuals. In this work we utilize a very general definition of diversity, namely the number of overlapping features -i.e. diverse counterfactuals should not change the same features:\n\u03c8( x j cf , x k cf ) = d i=1 1 ( \u03b4 j cf ) i = 0 \u2227 ( \u03b4 k cf ) i = 0(10)\nwhere \u03b4 j cf = x j cf -x and 1(\u2022) denotes the indicator function that returns 1 if the boolean expression is true and 0 otherwise.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B. Method Specific Computation of a Single Counterfactual", "text": "Hereinafter, we propose practical relaxations for computing a single counterfactual explanation of different parametric dimensionality reduction methods (see Definition 2). While Definition 2 does not make any assumptions on the dimensionality reduction \u03c6(\u2022), we now assume a parametric dimensionality reduction in order to get tractable optimization problems.\nNote that in all cases, we approximate the 0-norm with the 1-norm for measuring closeness between the original sample x and the counterfactual x cf . Furthermore, we use p = 2, the 2-norm for measuring the distance between the mapping of x cf and the requested mapping y cf .\n1) Linear Methods: In the case of linear mappings as defined in Section II-A1, we phrase the computation of a single counterfactual explanations as the following convex quadratic program:\narg min xcf \u2208 R d x -x cf 1 + C \u2022 \u03be s.t. A x cf + b -y cf 2 2 \u2264 \u03be \u03be \u2265 0 (11)\nwhere C > 0 acts as a regularization strength balancing between the two objectives in Eq. (8) -the regularization is necessary because it is numerically difficult (or even impossible) to find a counterfactual x cf that yields the exact mapping \u03c6( x cf ) = y cf , we therefore have to specify how much difference we are willing to tolerate. Note that convex quadratic programs can be solved efficiently [42].\n2) Self Organizing Map: Similar to linear methods, we phrase the computation of a single counterfactual explanations for SOMs (Section II-A2) as the following convex quadratic program, which again can be solved efficiently using standard solvers from convex optimization [42]:\narg min xcf \u2208 R d x -x cf 1 s.t. x cf -p ycf 2 2 + \u2264 x cf -p z 2 2 \u2200 z \u2208 I (12)\nwhere > 0 makes sure that the set of feasible solutions is closed.\n3) Autoencoder: For autoencoders (AEs) as discussed in Section II-A3, we utilize the penalty method to merge the two objectives from Eq. ( 8) into a single objective:\narg min xcf \u2208 R d x -x cf 1 + C \u2022 enc \u03b8 ( x cf ) -y cf 2(13)\nwhere the hyperparameter C > 0 acts as a regularization strength.\nAssuming continuous differentiability of the encoder enc \u03b8 (\u2022), we can solve Eq. ( 13) using a gradient based method. However, due to the non-linearity of enc \u03b8 (\u2022), we might find a local optimum only.\n4) Parametric t-SNE: Although the neural network f \u03b8 (\u2022) of parametric t-SNE (Section II-A4) is trained in a completely different way compared to an autoencoder based dimensionality reduction, the final modeling is the same and consequently, everything from the case of autoencoder based DR applies here as well: \narg min xcf \u2208 R d x -x cf 1 + C \u2022 f \u03b8 ( x cf ) -y cf 2 (14)\nx i cf = CF \u03c6 ( x, y cf , F) Compute next counterfactual 5: R = R \u222a { x i cf } 6: F = F \u222a {j | ( x cf -x) j = 0}\nUpdate set of black-listed features 7: end for", "publication_ref": ["b32", "b32"], "figure_ref": [], "table_ref": []}, {"heading": "C. Computation of Diverse Counterfactuals", "text": "In this section, we propose an algorithm for computing diverse counterfactual explanations (see Definition 3) of the four DR methods considered in the previous section.\nRegarding the formalization of diversity Eq. ( 10), instead of using Eq. ( 10) directly, we propose a more stricter version in order to get a continuous function which then yields tractable optimization problems, similar to the ones we proposed in the previous section: In order to compute a set of diverse counterfactuals instead of a single counterfactual, we utilize our proposed methods for computing a single counterfactual explanations from Section III-B and extend these with a mechanism to forbid or punish changes in black-listed features. We then first compute a single counterfactual explanations using the methodology proposed in Section III-B and then iteratively compute another counterfactual explanation but blacklisting all features that have been changed in the previous counterfactuals -this procedure is illustrated as pseudo-code in Algorithm 1.\nBlack-listing features: We assume we are given an ordered set F of black-listed features. In case of convex programs (e.g. linear methods and SOM), we consider blacklisted features F by means of an additional affine equality constraint:\nM x cf = m(15)\nwhere\nM \u2208 R |F |\u00d7d , m \u2208 R |F | with (M) i,j = 1 if (F) i = j 0 otherwise(16)\nand\nm k = ( x) (F ) k .\nWhereas in all other cases (e.g. autoencoder and parametric t-SNE), where we minimize a (non-convex) cost function, we replace the counterfactual x cf in the optimization problem with an affine mapping undoing any potential changes in blacklisted features -i.e. black-listed features can be changed but have no effect on the final counterfactual because they are reset to their original value:\n\u03c6(M x cf + m) -y cf 2(17)\nwhere\nM \u2208 R d\u00d7d , m \u2208 R d with (M) i,j = 1 if i = j and i \u2208 F 0 otherwise ( m) i = ( x) i if i \u2208 F 0 otherwise(18)\nNote that in both cases, the complexity and type of optimization problem does not change -e.g. convex programs remain convex programs.\nFor convenience, we use CF \u03c6 ( x, y cf , F) to denote the computation of a counterfactual ( x cf , y cf ) of a DR method \u03c6(\u2022) at a given sample x subject to a set F of black-listed features.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "IV. EXPERIMENTS", "text": "We empirically evaluate our proposed explanation methodology of DR methods on the specific use-case of data visualization -i.e. dimensionality reduction to two dimensions. All experiments are implemented in Python and are publicly available on GitHubfoot_0 .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A. Data", "text": "We run all our experiments on a set of different ML benchmark data sets -all data sets are standardized: a) Diabetes: The \"Diabetes Data Set\" [43] is a labeled data set containing recordings from diabetes patients. The data set contains 442 samples and 10 real valued scaled features in [-.2, .2] such as body mass index, age in years and average blood pressure. The labels are integers in [25,346] denoting a quantitative measure of disease progression one year after baseline.\nb) Breast cancer: The \"Breast Cancer Wisconsin (Diagnostic) Data Set\" [44] is used for classifying breast cancer samples into benign and malignant (i.e. binary classification). The data set contains 569 samples and 30 numerical features such as area, smoothness and compactness. c) Toy: An an artificial, self created, toy data set containing 500 ten dimensional samples. Each feature is distributed according to a normal distribution whereby we choose a different random mean for each feature -by this we can guarantee that, in contrast to the other data sets, the features are independent of each other. The binary labelling of the samples is done by splitting the data into two clusters using k-means.", "publication_ref": ["b33", "b15", "b34"], "figure_ref": [], "table_ref": []}, {"heading": "B. Model Agnostic Algorithm for Comparison", "text": "We compare Algorithm 1 to a general model agnostic algorithm (ModelAgnos) for computing diverse counterfactual explanations where we select samples from the training data set D that minimize a weighted combination of Eq. (9)i.e. we make use of the penalty method to solve the multiobjective optimization problem Eq. ( 9) without making any further assumption on the dimensionality reduction \u03c6(\u2022):\nmin { x i cf \u2208 D} C 1 \u2022 x -x i cf 1 +C 2 \u2022 \u03c6( x i cf ) -y cf 2 +C 3 \u2022\u03c8( x i cf , x j cf ) (19)\nwhere C 1 , C 2 , C 3 > 0 denote regularization coefficients that allow us to balance between the different objectives, and \u03c8(\u2022) is implemented as stated in Eq. (10). By limiting the set of feasible solutions to the training data set, we can guarantee plausibility of the resulting counterfactual explanations -note that plausibility of the counterfactuals generated by Algorithm 1 can not be guaranteed.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C. Setup", "text": "For each data set and each of the four parametric DR methods (PCA, Autoencoder, SOM, parametric t-SNE) from Section III-B, we fit the DR method to the entire data set and compute for each sample in the data set a set of three diverse counterfactual explanations -we evaluate and compare the counterfactualsfoot_1 computed by our proposed Algorithm 1 with those from the model agnostic algorithm (see Section IV-B). For the requested target location y cf -recall that in a counterfactual explanation we ask for a change that would lead to a different specified mapping y cf instead of the original mapping y -we either choose the mapping of a different sample (with a different label) from the training data set as y cf or the mapping of the original sample x after perturbing three random features -the same type of perturbation is applied to these three features. Regarding the perturbations, we consider the following ones:\n\u2022 Shift: A constant is added to the feature value.\n\u2022 Gaussian: Gaussian noise is added to feature value. Note that we evaluate each perturbation separately. Furthermore, note that these perturbations could be interpreted as sensor failures and are therefore highly relevant to practice.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D. Evaluation", "text": "For all experimental scenarios, we monitor and evaluate some quantitative measurements:\n\u2022 CfSparse: Sparsity of the counterfactual explanationsi.e. how many (percentage) of the available features are used in the explanation, smaller values are better. \u2022 CfDist: Euclidean distance between the mapping of the counterfactual \u03c6( x cf ) and the requested mapping y cf -i.e. this can be interpreted as a measurement of the error of counterfactual explanations, smaller values are better. \u2022 CfDiv: Diversity of the counterfactual explanations -i.e.\nthe number of overlapping features between the diverse explanations (see Eq. ( 10) in Section III-A), smaller values are better. For the scenarios where we apply a perturbation to the original sample, we also record the recall of the identified perturbed features in the counterfactual explanations -i.e. checking if the used features in the explanation coincide with the perturbed features. By this, we try to measure the usefulness of our explanations for identifying relevant features -however, since dimensionality reduction is a many-to-one mapping, we consider recall only because we do not expect to observe a high precision due to the Rashomon effect.\nNote that each experiment is repeated 100 times in order to get statistically reliable estimates of the quantitative measurements.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E. Results", "text": "The results of the scenario without any perturbations -i.e. randomly selecting the target sample from the training setare shown in Table I and the results of the scenarios with perturbations are shown in Tables V,II -note that, due to space constraints, the latter one is put in the appendix.\nWe observe that Algorithm 1, on average, achieves much sparser and more diverse explanations than the mode agnostic algorithm (Section IV-B) does. Only in case of SOM, the sparsity is often a bit worse than those from the baseline -this might be due to numerical instabilities of the mathematical program Eq. (12). In particular, while Algorithm 1 almost always yields completely diverse explanations, the model agnostic algorithm fails completely -this highlights the strength of our proposed Algorithm 1 for computing diverse explanations. Furthermore, both methods are able to yield counterfactual explanations that are very close to the requested target location. In most cases Algorithm 1 yields counterfactuals that are closer to the target location, only in case of parametric t-SNE the model agnostic algorithm yields \"better\" counterfactuals -however, in both cases the variance is quite large which indicates instabilities of the learned dimensionality reduction. Note that, since the three evaluation metrics IV-D are contradictory, it can be misleading to evaluate the performance under each metric separately without looking at the other metrics at the same time -e.g. a method might yield very sparse counterfactuals but their distance to the requesting mappings is very large. In order to compensate the contradictory nature of the evaluation metrics, we suggest to also consider a ranking over the three metrics when assessing the performance of the two proposed algorithms for computing counterfactuals -we give such a ranking in Tables III,IV,VI.\nAccording to these rankings, Algorithm 1 outperforms the model agnostic algorithm in many cases or is at at least as good as the model agnostic method but never worse. While the recall of the baseline is very good across all DR methods and data sets, the recall of Algorithm 1 is often very good as well, however, there exist some cases (in particular the breast cancer data set) where the recall drops significantly compared to the model agnostic algorithm.\nV. CONCLUSION In this work, we proposed the abstract concept of contrasting explanations for locally explaining dimensionality reduction methods -we considered two-dimensional data visualization as a popular example application. In order to deal with the Rashomon effect -i.e. the fact that there exist more than one possible and valid explanation -we considered a set of diverse explanations instead of a single explanation. Furthermore, we also proposed an implementation of this concept using counterfactual explanations and proposed modelings and algorithms for efficiently computing diverse counterfactual explanations of different parametric dimensionality reduction methods. We empirically evaluated different aspects of our proposed algorithms on different standard benchmark data sets -we observe that our proposed methods consistently yield good results.\nBased on this initial work, there are a couple of potential extensions and directions for future research:\nDepending on the domain and application, it might be necessary to guarantee plausibility of the counterfactualsi.e. making sure that the counterfactual x cf is reasonable and plausible in the data domain. Implausibility or a lack of realism of the counterfactual x cf might hinder successful recourse in practice. In this work, we ignored the aspect of plausibility and it might happen that the computed counterfactuals x cf are not always realistic samples from the data domain -only in case of our model agnostic algorithm (see Section IV-B) we can guarantee plausibility because we only consider samples from the training data set D as potential counterfactuals x cf . In future work, a first approach could be to add plausibility constraints to our proposed modelings (see Section III-A) like it was done for counterfactual explanations of classifiers [36]- [38].\nAnother crucial aspects of transparency & explainability is the human. In particular, quantitative evaluation of algorithmic properties do not necessary coincide with a human evaluation [31]. Therefore we suggest to conduct a user-study to evaluate how \"useful\" our proposed explanation actually are -in particular it would be of interest to compare normal vs. plausible explanations, and to compare diverse explanations vs. a single explanations.", "publication_ref": ["b26", "b28", "b21"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "General data protection regulation: Regulation (eu) 2016/679 of the european parliament", "journal": "", "year": "2016", "authors": "E "}, {"ref_id": "b1", "title": "Interpretable Machine Learning", "journal": "", "year": "2019", "authors": "C Molnar"}, {"ref_id": "b2", "title": "Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models", "journal": "CoRR", "year": "2017", "authors": "W Samek; T Wiegand; K M\u00fcller"}, {"ref_id": "b3", "title": "Data visualization by nonlinear dimensionality reduction", "journal": "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery", "year": "2015", "authors": "A Gisbrecht; B Hammer"}, {"ref_id": "b4", "title": "Metric learning in dimensionality reduction", "journal": "ICPRAM", "year": "2015", "authors": "A Schulz; B Hammer"}, {"ref_id": "b5", "title": "Relevance learning for dimensionality reduction", "journal": "", "year": "2014", "authors": "A Schulz; A Gisbrecht; B Hammer"}, {"ref_id": "b6", "title": "Explaining t-sne embeddings locally by adapting lime", "journal": "", "year": "2020", "authors": "A Bibal; V M Vu; G Nanfack; B Fr\u00e9nay"}, {"ref_id": "b7", "title": "Local explanation of dimensionality reduction", "journal": "", "year": "2022", "authors": "A Bardos; I Mollas; N Bassiliades; G Tsoumakas"}, {"ref_id": "b8", "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation", "journal": "PloS one", "year": "2015", "authors": "S Bach; A Binder; G Montavon; F Klauschen; K.-R M\u00fcller; W Samek"}, {"ref_id": "b9", "title": "The self-organizing map", "journal": "", "year": "1990", "authors": "T Kohonen"}, {"ref_id": "b10", "title": "Deep learning", "journal": "MIT press", "year": "2016", "authors": "I Goodfellow; Y Bengio; A Courville"}, {"ref_id": "b11", "title": "Learning a parametric embedding by preserving local structure", "journal": "PMLR", "year": "2009", "authors": "L Van Der Maaten"}, {"ref_id": "b12", "title": "Dimensionality reduction: a comparative", "journal": "J Mach Learn Res", "year": "2009", "authors": "L Van Der Maaten; E Postma; J Van Den Herik"}, {"ref_id": "b13", "title": "Nonlinear dimensionality reduction", "journal": "Springer", "year": "2007", "authors": "J A Lee; M Verleysen"}, {"ref_id": "b14", "title": "A general framework for dimensionality-reducing data visualization mapping", "journal": "Neural Computation", "year": "2012", "authors": "K Bunte; M Biehl; B Hammer"}, {"ref_id": "b15", "title": "The art of using t-sne for single-cell transcriptomics", "journal": "Nature communications", "year": "2019", "authors": "D Kobak; P Berens"}, {"ref_id": "b16", "title": "Dimensionality reduction for visualizing single-cell data using umap", "journal": "Nature biotechnology", "year": "2019", "authors": "E Becht; L Mcinnes; J Healy; C.-A Dutertre; I W Kwok; L G Ng; F Ginhoux; E W Newell"}, {"ref_id": "b17", "title": "Visualizing data using t-sne", "journal": "Journal of machine learning research", "year": "2008", "authors": "L Van Der Maaten; G Hinton"}, {"ref_id": "b18", "title": "Umap: Uniform manifold approximation and projection for dimension reduction", "journal": "", "year": "2018", "authors": "L Mcinnes; J Healy; J Melville"}, {"ref_id": "b19", "title": "Parametric nonlinear dimensionality reduction using kernel t-sne", "journal": "Neurocomputing", "year": "2015", "authors": "A Gisbrecht; A Schulz; B Hammer"}, {"ref_id": "b20", "title": "Counterfactuals in explainable artificial intelligence (xai): Evidence from human reasoning", "journal": "", "year": "2019", "authors": "R M J Byrne"}, {"ref_id": "b21", "title": "Keep your friends close and your counterfactuals closer: Improved learning from closest rather than plausible counterfactual explanations in an abstract setting", "journal": "", "year": "2022", "authors": "U Kuhl; A Artelt; B Hammer"}, {"ref_id": "b22", "title": "Let's go to the alien zoo: Introducing an experimental framework to study usability of counterfactual explanations for machine learning", "journal": "", "year": "2022", "authors": ""}, {"ref_id": "b23", "title": "Beyond trivial counterfactual explanations with diverse valuable explanations", "journal": "", "year": "2021", "authors": "P Rodriguez; M Caccia; A Lacoste; L Zamparo; I Laradji; L Charlin; D Vazquez"}, {"ref_id": "b24", "title": "Efficient search for diverse coherent explanations", "journal": "", "year": "2019", "authors": "C Russell"}, {"ref_id": "b25", "title": "Explaining machine learning classifiers through diverse counterfactual explanations", "journal": "", "year": "2020", "authors": "R K Mothilal; A Sharma; C Tan"}, {"ref_id": "b26", "title": "Interpretable counterfactual explanations guided by prototypes", "journal": "CoRR", "year": "2019", "authors": "A V Looveren; J Klaise"}, {"ref_id": "b27", "title": "Convex density constraints for computing plausible counterfactual explanations", "journal": "", "year": "2020", "authors": "A Artelt; B Hammer"}, {"ref_id": "b28", "title": "Convex optimization for actionable \\& plausible counterfactual explanations", "journal": "CoRR", "year": "2021", "authors": ""}, {"ref_id": "b29", "title": "Causal inference", "journal": "", "year": "2010", "authors": "J Pearl"}, {"ref_id": "b30", "title": "Dimensionality reduction for data visualization [applications corner", "journal": "IEEE signal processing magazine", "year": "2011", "authors": "S Kaski; J Peltonen"}, {"ref_id": "b31", "title": "Battledim: Battle of the leakage detection and isolation methods", "journal": "", "year": "2020", "authors": "S G Vrachimis; D G Eliades; R Taormina; A Ostfeld; Z Kapelan; S Liu; M Kyriakou; P Pavlou; M Qiu; M M Polycarpou"}, {"ref_id": "b32", "title": "Convex Optimization", "journal": "Cambridge University Press", "year": "2004", "authors": "S Boyd; L Vandenberghe"}, {"ref_id": "b33", "title": "Diabetes data set", "journal": "", "year": "1994", "authors": ""}, {"ref_id": "b34", "title": "Breast cancer wisconsin (diagnostic) data set", "journal": "", "year": "1995", "authors": "O L M William; H Wolberg; W Nick Street"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Fig. 1 .1Fig. 1. Illustration of the investigated topic: a 'high-dimensional' data set (left, with an outlier marked as a star) is mapped to two dimensions (middle), where the question 'why is the central point mapped here and not there' is asked (indicated by the arrow). Possible explanations are depicted (right).", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "1 .1Definition 1 ((Closest) Counterfactual Explanation [10]): Assume a prediction function (e.g. a classifier) h : R d \u2192 Y is given. Computing a counterfactual x cf \u2208 R d for a given input x \u2208 R d is phrased as an optimization problem:", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Fig. 2 .2Fig. 2. \"Rashomon effect\": Illustration of multiple possible counterfactual explanations x cf of a given sample x orig for a binary classifier.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Fig. 3 .3Fig. 3. Explaining anomalous pressure measurements -Left: Two dimensional data visualization of 29-dimensional pressure measurements; Right: Normalized amount of suggested changes per sensor -the faulty sensor is suggested to change the most and is therefore correctly identified.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Algorithm 11Computation of Diverse Counterfactuals Input: Original input x, Target location y cf , k \u2265 1: number of diverse counterfactuals, Dimensionality reduction \u03c6(\u2022) Output: Set of diverse counterfactuals R = { x i cf } 1: F = {} Initialize set of black-listed features 2: R = {} Initialize set of diverse counterfactuals 3: for i = 1, . . . , k do Compute k diverse counterfactuals 4:", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u03c6( x) = A x + b(1)", "formula_coordinates": [2.0, 142.76, 653.87, 157.26, 8.99]}, {"formula_id": "formula_1", "formula_text": "\u03c6( x) = arg min z \u2208 I x -p z 2 .", "formula_coordinates": [2.0, 381.32, 397.01, 112.37, 16.77]}, {"formula_id": "formula_2", "formula_text": "f \u03b8 ( x) = (dec \u03b8 \u2022 enc \u03b8 )( x),(3)", "formula_coordinates": [2.0, 384.43, 477.75, 178.6, 9.65]}, {"formula_id": "formula_3", "formula_text": "\u03c6( x) = enc \u03b8 ( x)(4)", "formula_coordinates": [2.0, 405.29, 539.81, 157.75, 9.65]}, {"formula_id": "formula_4", "formula_text": "\u03c6( x) = f \u03b8 ( x),(5)", "formula_coordinates": [2.0, 408.38, 709.64, 154.66, 9.65]}, {"formula_id": "formula_5", "formula_text": "arg min xcf \u2208 R d h( x cf ), y cf + C \u2022 \u03b8( x cf , x)(6)", "formula_coordinates": [3.0, 97.69, 704.52, 202.33, 17.6]}, {"formula_id": "formula_6", "formula_text": "\u03c6 : R d \u2192 R d (7) with d > d .", "formula_coordinates": [4.0, 48.96, 524.65, 251.06, 33.47]}, {"formula_id": "formula_7", "formula_text": "min xcf \u2208 R d x -x cf 0 , \u03c6( x cf ) -y cf p ,(8)", "formula_coordinates": [4.0, 97.09, 706.46, 202.93, 15.66]}, {"formula_id": "formula_8", "formula_text": "{ x i cf \u2208 R d }, y cf \u2208 R d of a specific sample x \u2208 R d is", "formula_coordinates": [4.0, 311.98, 193.82, 251.06, 23.07]}, {"formula_id": "formula_9", "formula_text": "min { x i cf \u2208 R d } x -x i cf 0 , \u03c6( x i cf ) -y cf p , \u03c8( x i cf , x j cf )(9)", "formula_coordinates": [4.0, 328.26, 236.39, 234.78, 20.36]}, {"formula_id": "formula_10", "formula_text": "\u03c8( x j cf , x k cf ) = d i=1 1 ( \u03b4 j cf ) i = 0 \u2227 ( \u03b4 k cf ) i = 0(10)", "formula_coordinates": [4.0, 347.51, 375.81, 215.52, 30.32]}, {"formula_id": "formula_11", "formula_text": "arg min xcf \u2208 R d x -x cf 1 + C \u2022 \u03be s.t. A x cf + b -y cf 2 2 \u2264 \u03be \u03be \u2265 0 (11)", "formula_coordinates": [4.0, 382.02, 642.55, 181.01, 47.52]}, {"formula_id": "formula_12", "formula_text": "arg min xcf \u2208 R d x -x cf 1 s.t. x cf -p ycf 2 2 + \u2264 x cf -p z 2 2 \u2200 z \u2208 I (12)", "formula_coordinates": [5.0, 73.62, 419.2, 226.4, 33.32]}, {"formula_id": "formula_13", "formula_text": "arg min xcf \u2208 R d x -x cf 1 + C \u2022 enc \u03b8 ( x cf ) -y cf 2(13)", "formula_coordinates": [5.0, 83.86, 527.51, 216.16, 17.6]}, {"formula_id": "formula_14", "formula_text": "arg min xcf \u2208 R d x -x cf 1 + C \u2022 f \u03b8 ( x cf ) -y cf 2 (14)", "formula_coordinates": [5.0, 88.34, 704.52, 211.68, 17.6]}, {"formula_id": "formula_15", "formula_text": "x i cf = CF \u03c6 ( x, y cf , F) Compute next counterfactual 5: R = R \u222a { x i cf } 6: F = F \u222a {j | ( x cf -x) j = 0}", "formula_coordinates": [5.0, 317.73, 376.72, 245.3, 35.29]}, {"formula_id": "formula_16", "formula_text": "M x cf = m(15)", "formula_coordinates": [6.0, 152.24, 87.94, 147.79, 9.83]}, {"formula_id": "formula_17", "formula_text": "M \u2208 R |F |\u00d7d , m \u2208 R |F | with (M) i,j = 1 if (F) i = j 0 otherwise(16)", "formula_coordinates": [6.0, 76.79, 104.57, 223.23, 43.68]}, {"formula_id": "formula_18", "formula_text": "m k = ( x) (F ) k .", "formula_coordinates": [6.0, 66.84, 157.89, 60.35, 10.63]}, {"formula_id": "formula_19", "formula_text": "\u03c6(M x cf + m) -y cf 2(17)", "formula_coordinates": [6.0, 132.22, 259.64, 167.8, 9.81]}, {"formula_id": "formula_20", "formula_text": "M \u2208 R d\u00d7d , m \u2208 R d with (M) i,j = 1 if i = j and i \u2208 F 0 otherwise ( m) i = ( x) i if i \u2208 F 0 otherwise(18)", "formula_coordinates": [6.0, 76.79, 276.24, 223.23, 76.55]}, {"formula_id": "formula_21", "formula_text": "min { x i cf \u2208 D} C 1 \u2022 x -x i cf 1 +C 2 \u2022 \u03c6( x i cf ) -y cf 2 +C 3 \u2022\u03c8( x i cf , x j cf ) (19)", "formula_coordinates": [6.0, 311.98, 251.09, 251.06, 29.28]}], "doi": ""}
