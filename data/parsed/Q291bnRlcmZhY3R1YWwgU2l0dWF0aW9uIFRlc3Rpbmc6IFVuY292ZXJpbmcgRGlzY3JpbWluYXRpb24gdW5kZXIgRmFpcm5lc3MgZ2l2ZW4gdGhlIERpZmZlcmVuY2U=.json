{
  "Counterfactual Situation Testing: Uncovering Discrimination under Fairness given the Difference": "JOSE M. ALVAREZ, Scuola Normale Superiore, University of Pisa, Italy SALVATORE RUGGIERI, University of Pisa, Italy We present counterfactual situation testing (CST), a causal data mining framework for detecting individual discrimination in a dataset of classifier decisions. CST answers the question 'what would have been the model outcome had the individual, or complainant, been of a different protected status?' in an actionable and meaningful way. It extends the legally-grounded situation testing of Thanh et al. [62] by operationalizing the notion of fairness given the difference of Kohler-Hausmann [38] using counterfactual reasoning. In standard situation testing we find for each complainant similar protected and non-protected instances in the dataset; construct respectively a control and test group; and compare the groups such that a difference in decision outcomes implies a case of potential individual discrimination. In CST we avoid this idealized comparison by establishing the test group on the complainant's counterfactual generated via the steps of abduction, action, and prediction. The counterfactual reflects how the protected attribute, when changed, affects the other seemingly neutral attributes of the complainant. Under CST we, thus, test for discrimination by comparing similar individuals within each group but dissimilar individuals across both groups for each complainant. Evaluating it on two classification scenarios, CST uncovers a greater number of cases than ST, even when the classifier is counterfactually fair.",
  "CCS Concepts: · Computing methodologies → Classification ; Machine learning .": "Additional Key Words and Phrases: discrimination discovery, structural causal models, counterfactual fairness",
  "1 INTRODUCTION": "Automated decision making (ADM) is becoming ubiquitous and its societal discontents clearer [4, 19, 31]. There is a shared urgency by regulators [21, 65] and researchers [37, 57] to develop frameworks that can asses these classifiers for potential discrimination based on protected attributes such as gender, race, or religion. Discrimination is often conceived as a causal claim on the effect of the protected attribute over an individual decision outcome [24, 30]. It is, in particular, a conception based on counterfactual reasoning-what would have been the model outcome if the individual, or complainant , were of a different protected status?-where we 'manipulate' the protected attribute of the individual. Kohler-Hausmann [38] calls such conceptualization the counterfactual causal model of discrimination (CMD). Several frameworks for proving ADM discrimination are based on CMD [38]. Central to these frameworks is defining 'similar' instances to the complainant; arranging them based on their protected status into control and test groups; and comparing the decision outcomes of these groups to detect the effect of the protected attribute. Among the available tools [12, 41, 52], however, there is a need for one that is both actionable and meaningful . We consider a framework to be actionable if it can rule out random circumstances for the individual discrimination claim as often required by courts (e.g., [23, 24, 45]), and meaningful if it can account for known links between the protected attribute and all other attributes when manipulating the former as often demanded by social scientists (e.g., [11, 34, 61]). In this paper we present counterfactual situation testing (CST), a causal data mining framework for detecting instances of individual discrimination in the dataset used by a classifier. It combines (structural) 1 counterfactuals [46, 47] with situation testing [62, 70]. Counterfactuals answer to counterfactual reasoning and are generated via structural causal models. Under the right causal knowledge, counterfactuals reflect at the individual level how changing the protected attribute affects other seemingly neutral attributes of a complainant. Situation testing is a data mining method, based 1 Not to be confused with counterfactual explanations [64] from the XAI literature, which are not based on structural causal models. Other works like [33] use 'structural' to differentiate counterfactuals from counterfactual explanations. In this paper, counterfactuals are structural. 1 Alvarez and Ruggieri on the homonymous legal tool [7, 54]. For each complainant, under some search algorithm and distance function for measuring similarity, it finds and compares a control and test group of similar protected and non-protected instances in the dataset, where a difference between the decision outcomes of the groups implies potential discrimination. CST follows the situation testing pipeline with the important exception that it constructs the test group around the complainant's counterfactual instead of the complainant. An illustrative example. Consider the scenario in Fig. 1 (used later in Section 4.1) where a bank uses a classifier to accept or reject ( ˆ 𝑌 ) individual loan applications based on annual salary ( 𝑋 1 ) and account balance ( 𝑋 2 ). Suppose a female applicant ( 𝐴 = 1) with 𝑥 1 = 35000 and 𝑥 2 = 7048 gets rejected and files for discrimination. The bank is using non-sensitive information to calculate ˆ 𝑌 , but according to Fig. 1 there is also a known link between 𝐴 and { 𝑋 1 , 𝑋 2 } that questions the neutrality of such information. Under situation testing, we would find a number of female (protected) and male (non-protected) instances with similar characteristics to the complainant. The resulting control and test groups to be compared would both have similar 𝑋 1 and 𝑋 2 to the complainant. On one hand, comparing multiple instances allows to check whether the complainant's claim is an isolated event or representative of an unfavorable pattern toward female applicants by the model (i.e., actionability). On the other hand, knowing what we know about 𝐴 and its influence, would it be fair to compare the similar female and male instances? As argued by previous works [32, 38], the answer is no. This idealized comparison takes for granted the effect of gender on annual salary and account balance. Under counterfactual situation testing, instead, we would generate the complainant's counterfactual under the causal knowledge provided, creating a 'male' applicant with a higher 𝑥 1 = 50796 and 𝑥 2 = 13852, and use it rather than the complainant to find similar male instances. The resulting control and test groups would have different 𝑋 1 and 𝑋 2 between them. This disparate comparison embodies fairness given the difference , explicitly acknowledging the lack of neutrality when looking at 𝑋 1 and 𝑋 2 based on 𝐴 (i.e., meaningfulness). Here, the control group represents the observed factual world while the test group the hypothetical counterfactual world of the complainant. In addition, with counterfactual situation testing we propose an actionable extension to counterfactual fairness by Kusner et al. [39], which remains the leading causal fairness framework [41]. A classifier is counterfactually fair when the complainant's and its counterfactual's decision outcomes are the same. These are the same two instances used by CST to construct, respectively, the control and test groups, which allows to equip this fairness definition with measures for uncertainty. Hence, CST links counterfactual fairness claims with notions of statistical significance. Further, by looking at the control and test groups rather than the literal comparison of the factual versus counterfactual instances, CST evaluates whether the counterfactual claim is representative of similar instances. Hence, CST detects cases of individual discrimination that are also counterfactually fair, capturing the realistic scenario where a deployed model tends to discriminate when asked to evaluate a borderline instance multiple times. Based on two case studies using synthetic and real data, we evaluate the CST framework using a k -nearest neighbor implementation, k-NN CST, and compare it to its situation testing counterpart, k-NN ST [62], as well as to counterfactual fairness [39]. Here, 𝑘 denotes the number of instances we wish to find for each control and test groups. The experiments show that CST detects a higher number of individual cases of discrimination across the different 𝑘 sizes. Further, the results also show that individual discrimination can occur even when the model is counterfactually fair. The results hold when dealing with multiple protected attributes as well as different implementation parameters. Our main contributions with CST are: (1) a meaningful and actionable framework for detecting individual discrimination; (2) a first operationalization of fairness given the difference for discrimination analysis; and (3) an actionable extension of counterfactual fairness equipped with confidence intervals. With this in mind, Section 2 explores the role 2 Counterfactual Situation Testing of causal knowledge in CST. Section 3 presents the CST framework and its k-NN implementation, while Section 4 showcases CST via two classification scenarios. Section 5 concludes the paper.",
  "1.1 Related Work": "We position CST with current works along the goals of actionability and meaningfulness. Regarding actionability, when proving discrimination, it is important to insure that the framework accounts for sources of randomness in the decision process. Popular non-algorithmic frameworks-such as natural [26] and field [8] experiments, audit [22] and correspondence [9, 53] studies-address this issue by using multiple observations to build inferential statistics. Similar statistics are sometimes asked in court for proving discrimination (e.g., [23, Section 6.3]). Few algorithmic frameworks, instead, address this issue due to model complexity preventing formal inference [5]. An exception are data mining frameworks for discrimination discovery [48, 58] that operationalize the non-algorithmic notions, including situation testing [62, 70]. These frameworks (e.g., [2, 25, 51]) keep the focus on comparing multiple control-test instances for making individual claims, providing evidence similar to that produced by the quantitative tools used in court [38]. To the best of our knowledge, it remains unclear if the same can be said about existing causal fair machine learning methods [41] as these have yet to be used beyond academic circles. Regarding meaningfulness, situation testing and the other methods have been criticized for their handling of the counterfactual question behind the causal model of discrimination [32, 34, 38]. In particular, these actionable methods take for granted the influence of the protected attribute on all other attributes. This can be seen, e.g., in how situation testing constructs the test group, which is equivalent to changing the protected attribute while keeping everything else equal. Such approach goes against how most social scientists interpret the protected attribute and its role as a social construct when proving discrimination [11, 28, 55, 61]. It is in that regard where structural causal models [46] and their ability for conceiving counterfactuals (e.g., [13, 69]), including counterfactual fairness [39], have an advantage. What the criticisms on counterfactuals [32, 34] overlook here is that generating counterfactuals, as long as the causal knowledge is properly specified, accounts modeling-wise for the effects of changing the protected attribute on all other observed attributes. A framework like counterfactual fairness, relative to situation testing and these other methods, is more meaningful in its handling of protected attributes. The novelty in CST is bridging these two lines of work, borrowing the actionability aspects from situation testing and the meaningful aspects from counterfactual fairness.",
  "2 CAUSAL KNOWLEDGE FOR DISCRIMINATION": "Counterfactual situation testing requires access to the dataset of decision records of interest, D , and the algorithmic decision-maker that produced it, 𝑏 () . Let D contain the set of relevant attributes 𝑋 , the set of protected attributes 𝐴 , and the decision outcome ˆ 𝑌 = 𝑏 ( 𝑋 ) . We describe D as a collection of 𝑛 tuples, each ( 𝑥 𝑖 , 𝑎 𝑖 , b 𝑦 𝑖 ) representing the 𝑖 𝑡ℎ individual profile, with 𝑖 ∈ [ 1 , 𝑛 ] . ˆ 𝑌 is binary with ˆ 𝑌 = 1 denoting the positive outcome (e.g., loan granted). For illustrative purposes, we assume a single binary 𝐴 with 𝐴 = 1 denoting the protected status (e.g., female), though we relax this assumption in the experiments of Section 4.2. We also require causal knowledge in the form of a structural causal model that describes the data generating model behind D . We view this requirement as an input space for experts as these models are a convenient way for organizing assumptions on the source of the discrimination, facilitating stakeholder participation and supporting collaborative reasoning about contested concepts [44]. 3 Alvarez and Ruggieri",
  "2.1 Structural Causal Models and Counterfactuals": "A structural causal model (SCM) [46] M = {S , P U } describes how the set of 𝑝 variables 𝑊 = 𝑋 ∪ 𝐴 is determined based on corresponding sets of structural equations S and latent variables 𝑈 with prior distribution P U . Each 𝑊 𝑗 ∈ 𝑊 is assigned a value through a deterministic function 𝑓 𝑗 ∈ S of its causal parents 𝑊 𝑝𝑎 ( 𝑗 ) ⊆ 𝑊 \\ { 𝑊 𝑗 } and noise variable 𝑈 𝑗 with distribution 𝑃 ( 𝑈 𝑗 ) ∈ P U . Formally, for 𝑊 𝑗 ∈ 𝑊 we have that 𝑊 𝑗 ← 𝑓 𝑗 ( 𝑊 𝑝𝑎 ( 𝑗 ) , 𝑈 𝑗 ) , indicating the flow of information in terms of child-parent or cause-effect pairs. We consider the associated causal graph G = (V , E) , where a node 𝑉 𝑗 ∈ V represents a 𝑊 𝑗 variable and a directed edge 𝐸 ( 𝑗,𝑗 ′ ) ∈ E a causal relation. Wemaketwoassumptions on M commonwithin the causal fairness literature [41]. First, we assume causal sufficiency , meaning there are no hidden common causes in M , or confounders. Second, we assume G to be acyclical , which turns G into a directed acyclical graph (DAG), allowing for no feedback loops. We write M under these assumptions as:  where these assumptions are necessary for generating counterfactuals. The causal sufficiency assumption is particularly deceitful as it is difficult to both test and account for a hidden confounder [18, 40, 43]. The risk of a hidden confounder is a general problem to modeling fairness. Here, the dataset D delimits our context. We expect it to contain all relevant information used by the decision-maker 𝑏 () . Input from several stakeholders is needed to derive (1). We see it as a necessary collaborative effort: before we implement CST, we first need to agree over a worldview for the discrimination context . Based on D , a domain-expert motivates a causal graph G . A modelling-expert then translates this graphical information into a SCM M , making model specification decisions on S . We do not cover this process here, but this is how we envision the initial implementation stage of counterfactual situation testing. For a given SCM M we want to run counterfactual queries to build the test group for a complainant. Counterfactual queries answer to what would have been if questions. In CST, we wish to ask such questions around the protected attribute 𝐴 , by setting 𝐴 to the non-protected status 𝛼 using the do-operator 𝑑𝑜 ( 𝐴 : = 𝛼 ) [46] to capture the individuallevel effects 𝐴 has on 𝑋 according to (1). Let 𝑋 𝐶𝐹 denote the set of counterfactual variables obtained via the three step procedure by Pearl et al. [47]. Abduction : for each prior distribution 𝑃 ( 𝑈 𝑖 ) that describes 𝑈 𝑖 , we compute its posterior distribution given the evidence, or 𝑃 ( 𝑈 𝑖 | 𝑋,𝐴 ) . Action : we intervene 𝐴 by changing its structural equation to 𝐴 : = 𝛼 , which gives way to a new SCM M ′ . Prediction : we generate the counterfactual distribution 𝑃 ( 𝑋 𝐶𝐹 𝐴 ← 𝛼 ( 𝑈 ) | 𝑋,𝐴 ) by propagating the abducted 𝑃 ( 𝑈 𝑖 | 𝑋,𝐴 ) through the revised structural equations in M ′ . Note that generating counterfactuals and, thus, CST, unlike, e.g., counterfactual explanations [64] and discrimination frameworks like the FlipTest [10], does not require a change in the individual decision outcome. It is possible for b 𝑌 = b 𝑌 𝐶𝐹 after manipulating 𝐴 . We include a working example on generating counterfactuals in the Appendix.",
  "2.2 Conceiving Discrimination": "The legal setting of interest is indirect discrimination under EU law. It occurs when an apparently neutral practice disadvantages individuals that belong to a protected group. Following [27], we focus on indirect discrimination for three reasons. First, unlike disparate impact under US law [6], the decision-maker can still be liable for it despite lack of premeditation and, thus, all practices need to consider potential indirect discrimination implications. Second, many ADM models are not allowed to use the protected attribute as input, making it difficult for regulators to use the 4",
  "Counterfactual Situation Testing": "[13] Silvia Chiappa. 2019. Path-Specific Counterfactual Fairness. In AAAI . AAAI Press, 7801-7808. [14] Evgenii Chzhen, Christophe Denis, Mohamed Hebiri, Luca Oneto, and Massimiliano Pontil. 2020. Fair regression with Wasserstein barycenters. In NeurIPS . [15] Evgenii Chzhen and Nicolas Schreuder. 2022. A minimax framework for quantifying risk-fairness trade-off in regression. The Annals of Statistics 50, 4 (2022), 2416-2442. [16] Jacob Cohen. 2013. Statistical power analysis for the behavioral sciences . Routledge. [17] Caroline Criado-Perez. 2019. Invisible Women . Vintage. [18] Alexander D'Amour. 2019. On Multi-Cause Causal Inference with Unobserved Confounding: Counterexamples, Impossibility, and Alternatives. CoRR abs/1902.10286 (2019). [19] Jeffrey Dastin. 2018. Amazon scraps secret AI recruiting tool that showed bias against women. Reuters (2018). [20] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard S. Zemel. 2012. Fairness through awareness. In ITCS . ACM, 214-226. [21] European Commission. 2021. The Artificial Intelligence Act. https://artificialintelligenceact.eu/. Accessed on January 2nd, 2023. [22] Michael Fix and Raymond J. Struyk. 1993. Clear and Convincing Evidence: Measurement of Discrimination in America . Urban Institute Press. [23] European Union Agency for Fundamental Rights and Council of Europe. 2018. Handbook on European non-discrimination law. https://fra.europa.eu. Downloaded in 2023.. [24] S. R. Foster. 2004. Causation in antidiscrimination law: Beyond intent versus impact. Houston Law Review 41, 5 (2004), 1469-1548. [25] Sainyam Galhotra, Yuriy Brun, and Alexandra Meliou. 2017. Fairness testing: testing software for discrimination. In ESEC/SIGSOFT FSE . ACM, 498-510. [26] Claudia Goldin and Cecilia Rouse. 2000. Orchestrating Impartiality: The Impact of \"Blind\" Auditions on Female Musicians. American Economic Review 90, 4 (September 2000), 715-741. [27] Philipp Hacker. 2018. Teaching fairness to artificial intelligence: Existing and novel strategies against algorithmic discrimination under EU law. Common Market Law Review 55, 4 (2018). [28] Alex Hanna, Emily Denton, Andrew Smart, and Jamila Smith-Loud. 2020. Towards a critical race methodology in algorithmic fairness. In FAT* . ACM, 501-512. [29] Trevor Hastie, Robert Tibshirani, Jerome H Friedman, and Jerome H Friedman. 2009. The elements of statistical learning: data mining, inference, and prediction . Vol. 2. Springer. [30] James J Heckman. 1998. Detecting Discrimination. Journal of Economic Perspectives 12, 2 (1998), 101-116. [31] Melissa Heikkila. 2022. Dutch scandal serves as a warning for Europe over risks of using algorithms. POLITICO (2022). [32] Lily Hu and Issa Kohler-Hausmann. 2020. What's sex got to do with machine learning?. In FAT* . ACM, 513. [33] Amir-Hossein Karimi, Bernhard Schölkopf, and Isabel Valera. 2021. Algorithmic Recourse: from Counterfactual Explanations to Interventions. In FAccT . ACM, 353-362. [34] Atoosa Kasirzadeh and Andrew Smart. 2021. The Use and Misuse of Counterfactuals in Ethical Machine Learning. In FAccT . ACM, 228-236. [35] Niki Kilbertus, Philip J. Ball, Matt J. Kusner, Adrian Weller, and Ricardo Silva. 2019. The Sensitivity of Counterfactual Fairness to Unmeasured Confounding. In UAI (Proceedings of Machine Learning Research, Vol. 115) . AUAI Press, 616-626. [36] Niki Kilbertus, Mateo Rojas-Carulla, Giambattista Parascandolo, Moritz Hardt, Dominik Janzing, and Bernhard Schölkopf. 2017. Avoiding Discrimination through Causal Reasoning. In NIPS . 656-666. [37] Jon M. Kleinberg, Jens Ludwig, Sendhil Mullainathan, and Cass R. Sunstein. 2019. Discrimination in the Age of Algorithms. CoRR abs/1902.03731 (2019). [38] Issa Kohler-Hausmann. 2018. Eddie Murphy and the Dangers of Counterfactual Causal Thinking about Detecting Racial Discrimination. Nw. UL Rev. 113 (2018), 1163. [39] Matt J. Kusner, Joshua R. Loftus, Chris Russell, and Ricardo Silva. 2017. Counterfactual Fairness. In NIPS . 4066-4076. [40] Christos Louizos, Uri Shalit, Joris M. Mooij, David A. Sontag, Richard S. Zemel, and Max Welling. 2017. Causal Effect Inference with Deep Latent-Variable Models. In NIPS . 6446-6456. [41] Karima Makhlouf, Sami Zhioua, and Catuscia Palamidessi. 2020. Survey on Causal-based Machine Learning Fairness Notions. CoRR abs/2010.09553 (2020). [42] Ron Mallon. 2007. A field guide to social construction. Philosophy Compass 2, 1 (2007), 93-108. [43] Lawrence C McCandless, Paul Gustafson, and Adrian Levy. 2007. Bayesian sensitivity analysis for unmeasured confounding in observational studies. Statistics in Medicine 26, 11 (2007), 2331-2347. [44] Deirdre Mulligan. 2022. Invited Talk: Fairness and Privacy. https://www.afciworkshop.org/afcp2022. At the NeurIPS 2022 Workshop on Algorithmic Fairness through the Lens of Causality and Privacy.. [45] Thomas B Nachbar. 2021. Algorithmic fairness, algorithmic discrimination. Florida State University Law Review 48 (2021), 50. [46] Judea Pearl. 2009. Causality: Models, Reasoning, and Inference (2nd ed.). Cambridge University Press. [47] Judea Pearl, Madelyn Glymour, and Noicholas P. Jewell. 2016. Causal Inference in Statistics: A Primer . John Wiley & Sons. [48] Dino Pedreschi, Salvatore Ruggieri, and Franco Turini. 2008. Discrimination-aware data mining. In KDD . ACM, 560-568. [49] Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. 2017. Elements of Causal Inference: Foundations and Learning Algorithms . The MIT Press. [50] Drago Plecko and Elias Bareinboim. 2022. Causal Fairness Analysis. CoRR abs/2207.11385 (2022). 15 Alvarez and Ruggieri [51] Bilal Qureshi, Faisal Kamiran, Asim Karim, Salvatore Ruggieri, and Dino Pedreschi. 2020. Causal inference for social discrimination reasoning. J. Intell. Inf. Syst. 54, 2 (2020), 425-437. [52] Andrea Romei and Salvatore Ruggieri. 2014. A multidisciplinary survey on discrimination analysis. Knowl. Eng. Rev. 29, 5 (2014), 582-638. [53] Dan-Olof Rooth. 2021. Correspondence testing studies. IZA World of Labor 58 (2021). [54] Isabelle Rorive. 2009. Proving Discrimination Cases: The Role of Situation Testing. Centre for Equal Rights and MPG (2009). https://ec.europa.eu/ migrant-integration/library-document/proving-discrimination-cases-role-situation-testing_en [55] Evan K. Rose. 2022. A Constructivist Perspective on Empirical Discrimination Research. Working Manuscript (2022). https://ekrose.github.io/files/ constructivism.pdf [56] Richard Rothstein. 2017. The Color of Law: A Forgotten History of How our Government Segregated America . Liveright Publishing. [57] Salvatore Ruggieri, José M. Álvarez, Andrea Pugnana, Laura State, and Franco Turini. 2023. Can We Trust Fair-AI?. In AAAI . AAAI Press, 15421-15430. [58] Salvatore Ruggieri, Dino Pedreschi, and Franco Turini. 2010. Data mining for discrimination discovery. ACM Trans. Knowl. Discov. Data 4, 2 (2010), 9:1-9:40. [59] Chris Russell, Matt J. Kusner, Joshua R. Loftus, and Ricardo Silva. 2017. When Worlds Collide: Integrating Different Counterfactual Assumptions in Fairness. In NIPS . 6414-6423. [60] Eric C Schneider. 2008. Smack: Heroin and the American city . University of Pennsylvania Press. [61] Maya Sen and Omar Wasow. 2016. Race as a bundle of sticks: Designs that estimate effects of seemingly immutable characteristics. Annual Review of Political Science 19, 1 (2016), 499-522. [62] Binh Luong Thanh, Salvatore Ruggieri, and Franco Turini. 2011. k-NN as an implementation of situation testing for discrimination discovery and prevention. In KDD . ACM, 502-510. [63] Michael Carl Tschantz. 2022. What is Proxy Discrimination?. In FAccT . ACM, 1993-2003. [64] Sandra Wachter, Brent Mittelstadt, and Chris Russell. 2017. Counterfactual explanations without opening the black box: Automated decisions and the GDPR. Harv. JL & Tech. 31 (2017), 841. [65] White House. 2022. Blueprint for an AI Bill of Rights. https://www.whitehouse.gov/ostp/ai-bill-of-rights/. Accessed on January 2nd, 2023. [66] Linda F Wightman. 1998. LSAC national longitudinal bar passage study . Law School Admission Council. [67] D. Randall Wilson and Tony R. Martinez. 1997. Improved Heterogeneous Distance Functions. J. Artif. Intell. Res. 6 (1997), 1-34. [68] Raphaële Xenidis. 2020. Tuning EU equality law to algorithmic discrimination: Three pathways to resilience. Maastricht Journal of European and Comparative Law 27, 6 (2020), 736-758. [69] Ke Yang, Joshua R. Loftus, and Julia Stoyanovich. 2021. Causal Intersectionality and Fair Ranking. In FORC (LIPIcs, Vol. 192) . Schloss Dagstuhl Leibniz-Zentrum für Informatik, 7:1-7:20. [70] Lu Zhang, Yongkai Wu, and Xintao Wu. 2016. Situation Testing-Based Discrimination Discovery: A Causal Inference Approach. In IJCAI . IJCAI/AAAI Press, 2718-2724. 16 Counterfactual Situation Testing",
  "2.3 Fairness given the Difference: the Kohler-Hausmann Critique": "Here, we make the case-very briefly-that the causal knowledge required for CST makes it a meaningful framework with respect to situation testing [62, 70] and other tools [52] for detecting discrimination. The reference work is Kohler-Hausmann [38]. We refer to the phrase fairness given the difference , 2 which best captures her overall critique toward the causal model of discrimination, as the Kohler-Hausmann Critique (KHC). CST aims to be meaningful by operationalizing the KHC. It builds the test group on the complainant's counterfactual, letting 𝑋 𝐶𝐹 reflect the effects of changing 𝐴 instead of assuming 𝑋 = 𝑋 𝐶𝐹 . This is because we view the test group as a representation of the hypothetical counterfactual world of the complainant. As argued by [38] and others before [11, 61], it is difficult to deny that most protected attributes, if not all of them, are social constructs . That is, these attributes were used to classify and divide groups of people in a systematic way that conditioned the material opportunities of multiple generations [42, 55]. Thus, recognizing 𝐴 as a social constructs means recognizing that its effects can be reflected in seemingly neutral variables in 𝑋 . It is recognizing that 𝐴 , the attribute, cannot capture alone the meaning of belonging to 𝐴 and that we might, as a minimum, have to link it with other attributes to better capture this, such as 𝐴 → 𝑋 where 𝐴 and 𝑋 change in unison. These attributes summarize the historical processes that fairness researchers are trying to address today and should not be treated lightly. 3 The notion of fairness given the difference centers on how 𝐴 is treated in the counterfactual causal model of discrimination (CM). The critique goes beyond the standard manipulation concern [3] in which 𝐴 is an inmutable attribute. Instead, granted that we can or, more precisely, have to manipulate 𝐴 for running a discrimination analysis, the critique goes against how most discrimination frameworks operationalize such manipulation. The KCH emphasizes that when 𝐴 changes, 𝑋 should change as well . 2 A phrase by Kohler-Hausmann during a panel discussion at NeurIPS 2021 workshop on 'Algorithmic Fairness through the Lens of Causal Reasoning. 3 A clear example of this would be the use of race by US policy makers during the early post-WWII era. See, e.g., the historical evidence provided by Rothstein [56] (for housing), Schneider [60] (for narcotics), and Adler [1] (for policing). 5 Alvarez and Ruggieri Based on KHC, we consider two types of manipulations that summarize existing frameworks. The ceteris paribus (CP), or all else equal, manipulation in which 𝐴 changes but 𝑋 remains the same. Examples of it include situation testing [62, 70] but also, e.g., the famous correspondence study by Bertrand and Mullainathan [9]. The mutatis mutandis (MM), or changing what needs to be changed, manipulation in which 𝑋 changes when we manipulate 𝐴 based on some additional knowledge, like a structural causal model, that explicitly links 𝐴 to 𝑋 . Counterfactual fairness [39], e.g, uses this manipulation. The MM is clearly preferred over the CP manipulation when we view 𝐴 as a social construct.",
  "3 COUNTERFACTUAL SITUATION TESTING": "The objective of CST is to construct and compare a control and test group for each 𝑐 protected individual, or complainant , in D in a meaningful and actionable way. Let ( 𝑥 𝑐 , 𝑎 𝑐 , b 𝑦 𝑐 ) ∈ D denote the tuple of interest on which the individual discrimination claim focuses on, where 𝑐 ∈ [ 1 , 𝑛 ] . We assume access to the ADM 𝑏 () , the dataset D , and a structural causal model M describing the discrimination context. There are three key inputs to consider: the number of instances per group , 𝑘 ; the similarity distance function of choice , 𝑑 ; and the strength of the evidence for rejecting the discrimination claim , 𝛼 . A fourth key input that we fix in this paper is the search algorithm of choice , 𝜙 , which we set as the k-nearest neighbors algorithm (k-NN) [29]. We do so as the k-NN is intuitive, easy to implement, and commonly used by existing situation testing frameworks. We discuss the CST implementation used in Section 3.4, including the choice of 𝑑 .",
  "3.1 Building Control and Test Groups": "For complainant 𝑐 , the control and test groups are built on the search spaces and search centers for each group. The search spaces are derived and, thus, delimited by D : we are looking for individuals that have gone through the same decision process as the complainant. The search centers, however, are derived separately: the one for the control group comes from D , while the one for the test group comes from the corresponding counterfactual dataset D 𝐶𝐹 . The test search center represents the what would have been if of the complainant under a mutatis mutandis (MM) manipulation of the protected attribute 𝐴 that motivates the discrimination claim. Definition 3.1 (Search Spaces). Under a binary 𝐴 , where 𝐴 = 1 denotes the protected status, we partition D into the control search space D 𝑐 = {( 𝑥 𝑖 , 𝑎 𝑖 , b 𝑦 𝑖 ) ∈ D : 𝑎 𝑖 = 1 } and the test search space D 𝑡 = {( 𝑥 𝑖 , 𝑎 𝑖 , b 𝑦 𝑖 ) ∈ D : 𝑎 𝑖 = 0 } . Definition 3.2 (Counterfactual Dataset). The counterfactual dataset D 𝐶𝐹 represents the counterfactual mapping of each instance in the dataset D , with known decision maker 𝑏 () and SCM M , via the abduction, action, and prediction steps [47] when setting a binary 𝐴 to the non-protected value, or do ( 𝐴 : = 0). To obtain D 𝐶𝐹 , we consider an SCM M , as in (1), where 𝐴 has no causal parents, or is a root node, 𝐴 affects only the elements of 𝑋 considered by the expert(s), and ˆ 𝑌 = 𝑏 ( 𝑋 ) . Therefore, when generating the counterfactuals on 𝐴 (Section 2.1), under the indirect discrimination setting (Section 2.2), the resulting 𝑋 𝐶𝐹 in D 𝐶𝐹 should reflect an MM manipulation (Section 2.3). Under this structural representation, if 𝐴 changes then 𝑋 changes too. See, e.g., Fig. 1 and Fig. 3. The counterfactual dataset represents the world that the complainants would have experienced under 𝐴 = 0 given our worldview . All three definitions extend to | 𝐴 | > 1. Definition 3.3 (Search Centers). For a complainant 𝑐 , we use 𝑥 𝑐 from the tuple of interest ( 𝑥 𝑐 , 𝑎 𝑐 , ˆ 𝑦 𝑐 ) ∈ D as the control search center for exploring D 𝑐 ⊂ D , and use 𝑥 𝐶𝐹 𝑐 from the tuple of interest's generated counterfactual ( 𝑥 𝐶𝐹 𝑐 , 𝑎 𝐶𝐹 𝑐 , ˆ 𝑦 𝐶𝐹 𝑐 ) ∈ D 𝐶𝐹 as the test search center for exploring D 𝑡 ⊂ D . 6 Counterfactual Situation Testing Given the factual D and counterfactual D 𝐶𝐹 datasets, we construct the control and test groups for 𝑐 using the k-NN algorithm under some distance function 𝑑 ( 𝑥, 𝑥 ′ ) to measure similarity between two tuples 𝑥 and 𝑥 ′ . We want each group or neighborhood to have a size 𝑘 . For the control group ( k-ctr ) we use the (factual) tuple of interest ( 𝑥 𝑐 , 𝑎 𝑐 , ˆ 𝑦 𝑐 ) ∈ D as search center to explore the protected search space D 𝑐 :  where 𝑟𝑎𝑛𝑘 𝑑 ( 𝑥 𝑐 , 𝑥 𝑖 ) is the rank position of 𝑥 𝑖 among tuples in D 𝑐 with respect to the ascending distance 𝑑 from 𝑥 𝐶𝐹 𝑐 . For the test group ( k-tst ) we use the counterfactual tuple of interest ( 𝑥 𝐶𝐹 𝑐 , 𝑎 𝐶𝐹 𝑐 , b 𝑦 𝐶𝐹 𝑐 ) ∈ D 𝐶𝐹 as search center to explore the non-protected search space D 𝑡 :  where 𝑟𝑎𝑛𝑘 𝑑 ( 𝑥 𝐶𝐹 𝑐 , 𝑥 𝑖 ) is the rank position of 𝑥 𝑖 among tuples in D 𝑡 with respect to the ascending distance 𝑑 from 𝑥 𝐶𝐹 𝑐 . We use the same distance function 𝑑 for each group. Neither 𝐴 nor ˆ 𝑌 are used for constructing the groups. Both (2) and (3) can be expanded by including additional constraints, such as a maximum allowed distance. The choice of search centers (Def. 3.3) is what operationalizes fairness given the difference for counterfactual situation testing, making it a meaningful framework for testing individual discrimination. To build k-ctr and k-tst using, respectively, 𝑥 𝑐 and 𝑥 𝐶𝐹 𝑐 is a statement on how we perceive within group ordering as imposed by the protected attribute 𝐴 . This is because the search centers must reflect the 𝐴 -specific ordering of the search spaces that each center targets. Let us consider our illustrative example from Section 1. If being a female ( 𝐴 = 1) in this society imposes certain systematic limitations that hinder 𝑥 𝑐 , then comparing 𝑐 to other female instances in the protected search space preserves the group ordering prescribed by 𝑋 | 𝐴 = 1 as all instances involved experience 𝐴 in the same way. Therefore, given our worldview, the generated counterfactual male instance for 𝑐 should then reflect the group ordering prescribed by 𝑋 | 𝐴 = 0. We expect 𝑥 𝑐 ≠ 𝑥 𝐶𝐹 𝑐 given what we know about the effects of 𝐴 on 𝑋 . Using 𝑥 𝐶𝐹 𝑐 as the test search center would allow us to compare 𝑐 to other male tuples in the non-protected search space without having to reduce 𝐴 to a phenotype. One way to look at the previous statement is by considering the notion of effort. If being female requires a higher individual effort to achieve the same 𝑥 𝑐 , then it is fair to compare 𝑐 to other female instances. However, it is unfair to compare 𝑐 to other male instances without adjusting for the extra effort not incurred by the male instances for being males. The counterfactual 𝑥 𝐶𝐹 𝑐 should reflect said adjustment. See [14, 15] on a similar, more formal critique on individual fairness [20] notions.",
  "3.2 Detecting Discrimination": "For a complainant 𝑐 , we compare the control and test groups by looking at the difference in proportion of negative decision outcomes , or Δ 𝑝 = 𝑝 𝑐 -𝑝 𝑡 , such that:  where 𝑝 𝑐 and 𝑝 𝑡 represents the count of tuples with a negative decision outcome ( ˆ 𝑌 = 0), respectively, in the control and test group. Note that only ˆ 𝑌 is used for deriving the proportions. We compute Δ 𝑝 for all protected tuples in D regardless of the their decision outcome ˆ 𝑌 . CST has the option to include or exclude the search centers when calculating (4). If we exclude them, then 𝑝 𝑐 and 𝑝 𝑡 remain as is; if we include them, then ˆ 𝑦 𝑐 and ˆ 𝑦 𝐶𝐹 𝑐 are counted in 𝑝 𝑐 and 𝑝 𝑡 , leading to a denominator in both of 𝑘 + 1. 7 Alvarez and Ruggieri We add this option to be able to compare CST against standard situation testing [62, 70], which excludes the search centers, and counterfactual fairness [39], which only uses the search centers. Since Δ 𝑝 is a proportion comparison, it is known to be asymptotically normally distributed , which allows to build Wald confidence intervals (CI) around it [62]. Let 𝑧 𝛼 / 2 be the 1 -𝛼 / 2 quantile of the standard normal distribution N for a significance level of 𝛼 (or, conversely, a confidence level ( 1 -𝛼 ) · 100%). We write the two-sided CI for Δ 𝑝 of 𝑐 as:  The confidence interval (5) responds to the hypothesis that there is individual discrimination, providing a measure of certainty on Δ 𝑝 through a range of possible values. For a given claim, if the CI contains the minimum accepted deviation 𝜏 , we cannot reject the hypothesis of no discrimination with ( 1 -𝛼 ) · 100% confidence. In other words, the null hypothesis 𝐻 0 : 𝜋 = 𝜏 cannot be rejected in favor of the alternative hypothesis 𝐻 1 : 𝜋 > 𝜏 , where 𝜋 is the true difference in proportion of negative decision outcomes. 𝜏 , with a default choice of 𝜏 = 0, represents the minimum amount of difference between 𝑝 𝑐 and 𝑝 𝑡 that we need to observe to claim individual discrimination. The overall choice of 𝛼 and 𝜏 will depend on the context of the discrimination claim. It can be motivated, for instance, by legal requirements (set, e.g., by the court [62]), or technical requirements (set, e.g., via power analysis [16]), or both. Definition 3.4 (Individual Discrimination). There is potential 4 individual discrimination toward the complainant 𝑐 if Δ 𝑝 = 𝑝 𝑐 -𝑝 𝑡 > 𝜏 , meaning the negative decision outcomes rate for the control group is greater than for the test group by some minimum deviation 𝜏 ∈ R + . We do not view Def. 3.4 as a matter of individual versus group fairness. When we test whether 𝑏 () discriminates against 𝑐 , we inevitably pass judgement onto the classifier 𝑏 () in fear that this behaviour has happened before. In D we have more than one potential discrimination claim to consider under CST, allowing to draw individual-level conclusions while motivating group-level ones. If 𝑏 () discriminated against 𝑐 , it also discriminated against what 𝑐 represents in terms of membership to 𝐴 . Definition 3.5 (Confidence on the Individual Discrimination Claim). The Wald CI (5) gives a measure of certainty on Δ 𝑝 , which is (asymptotically) normally distributed. For a significance level 𝛼 , we are ( 1 -𝛼 ) % confident on Δ 𝑝 . The claim is said to be statistically valid if the Wald CI excludes 𝜏 . This is a statistical inference extension of Def. 3.4. The many-to-many comparison behind Δ 𝑝 is what makes counterfactual situation testing an actionable framework for testing individual discrimination. Here, the notion of repetition and its relation to representativeness and certainty concerns is important. For proving individual discrimination a single comparison is not enough [23, Sec. 6.3]. This is because we want to ensure, one, that the individual claim is representative of the population, and two, be certain about the individual claim. Implicit to both concerns is finding a pattern of unfavorable decisions against the protected group to which the individual complainant belongs to, i.e., discrimination. Ideally, we would repeat the decision process multiple times for the discriminatory pattern to become apparent. This is not possible in practice. Back to our illustrative example from Section 1, we cannot ask the female complainant to apply multiple times to the same bank. We instead can look at other similar instances under the same process. This is what 𝑝 𝑐 and 𝑝 𝑡 (4) and Def. 3.4 represent. Similarly, if the bank's 𝑏 () is shown to discriminate against the female complainant, what rules out that it has not done it before or that this one time was an exception? Again, we 4 Or prima facie . In practice, discrimination needs to be argued against/for. CST alone, as with any other discrimination analysis tool, cannot claim to prove discrimination. It can, however, provide evidence against/for a discrimination case [52]. 8 Counterfactual Situation Testing cannot repeat the decision process until we are certain of the individual discrimination claim. We instead can assume a theoretical distribution of comparisons with 𝜋 to account for potential randomness in what we detect from the single point estimate that is Δ 𝑝 . This is what the CI (5) and Def. 3.5 represent.",
  "3.3 Connection to Counterfactual Fairness": "There is a clear link between CST and counterfactual fairness [39]. A decision maker is counterfactually fair if it outputs the same outcome for the factual tuple as for its counterfactual tuple, where the latter is generated based on the abduction, action, and prediction steps and the intervention on the protected attribute. 5 The factual ( 𝑥 𝑐 , 𝑎 𝑐 , ˆ 𝑦 𝑐 ) and counterfactual ( 𝑥 𝐶𝐹 𝑐 , 𝑎 𝐶𝐹 𝑐 , ˆ 𝑦 𝐶𝐹 𝑐 ) tuples for 𝑐 used in CST are also the ones used for counterfactual fairness. We view CST, when including the search centers, as an actionable extension of counterfactual fairness. Proposition 3.6 (On Actionable Counterfactual Fairness). Counterfactual fairness does not imply nor it is implied by Individual Discrimination (Def. 3.4). We present a sketch of proof to Prop. 3.6 the in Appendix. Intuitively, it is possible to handle borderline cases where the tuple of interest and its counterfactual both get rejected by 𝑏 () , though the latter is close to the decision boundary. The model 𝑏 () would be considered counterfactually fair, but would that disprove the individual discrimination claim? CST, by constructing the control and test groups around this single comparison, accounts for this actionability concern. CST further equips counterfactual fairness with confidence intervals. Previous works have addressed uncertainty in counterfactual fairness [35, 59], but with a focus on the structure of the SCM M . We instead address certainty on the literal comparison that motivates the counterfactual fairness definition.",
  "3.4 An Implementation: k-NN CST": "Finally, we propose an implementation to our counterfactual situation testing framework. We already defined the search algorithm 𝜙 as the k-NN algorithm. We define as the similarity measure 𝑑 the same distance function between two tuples, 𝑑 ( 𝑥, 𝑥 ′ ) , used in the k-NN situation testing implementation (k-NN ST) [62]. We do so because we want to compare our implementation, k-NN CST, against its standard counterpart, k-NN ST. We summarize the current algorithmic CST implementation in the Appendix. Let us define the distance between two tuples as:  where (6) averages the sum of the per-attribute distances across 𝑋 . A lower (6) implies a higher similarity between the tuples 𝑥 and 𝑥 ′ . Here, 𝑑 𝑖 equals the overlap measurement ( 𝑜𝑙 ) if the attribute 𝑋 𝑖 is categorical; otherwise, it equals the normalized Manhattan distance ( 𝑚𝑑 ) if the attribute 𝑋 𝑖 is continuous, ordinal, or interval. We define each 𝑚𝑑 ( 𝑥 𝑖 , 𝑥 𝑖 ′ ) = | 𝑥 𝑖 -𝑥 𝑖 ′ |/( max ( 𝑋 ) -min ( 𝑋 )) , and 𝑜𝑙 ( 𝑥 𝑖 , 𝑥 𝑖 ′ ) = 1 if 𝑥 𝑖 = 𝑥 𝑖 ′ and 0 otherwise. CST can handle nonnormalized attributes but, unless specified, we normalize them to insure comparable per-attribute distances. The choice of (6) is not restrictive. In subsequent works we hope to explore other distance options like , e.g., heterogeneous distance functions [67], as well as probability-based options, e.g., propensity score weighting [51]. CST is, above all, a framework for detecting discrimination. The choice of 𝑑 as well as 𝜙 are specific to the implementation of CST. What is important is that the test group is established around the complainant's counterfactual while the control group, like in other discrimination frameworks, is established around the complainant. 5 Formally, 𝑃 ( ˆ 𝑌 𝐴 ← 𝑎 ( 𝑈 ) = 𝑦 | 𝑋,𝐴 ) = 𝑃 ( ˆ 𝑌 𝐴 ← 𝑎 ′ ( 𝑈 ) = 𝑦 | 𝑋,𝐴 ) , where the left side is the factual 𝐴 = 𝑎 and the right side the counterfactual 𝐴 = 𝑎 ′ . 9 Alvarez and Ruggieri Fig. 1. The causal knowledge with corresponding SCM M and DAG G behind our (illustrative example) loan application dataset. Let 𝐴 denote an individual's gender, 𝑋 1 annual salary, 𝑋 2 bank balance, and b 𝑌 the loan decision based on the bank's ADM 𝑏 () . 𝐴 𝑋 1 𝑋 2 b 𝑌 M          𝐴 ← 𝑈 𝐴 𝑋 1 ← 𝑓 1 ( 𝐴 ) + 𝑈 1 𝑋 2 ← 𝑓 2 ( 𝑋 1 , 𝐴 ) + 𝑈 2 b 𝑌 = 𝑏 ( 𝑋 1 , 𝑋 2 )",
  "4 EXPERIMENTS": "We now showcase the counterfactual situation testing (CST) framework via its k-NN implementation using synthetic (Section 4.1) and real (Section 4.2) datasets. We contrast it to its situation testing counterpart (k-NN ST) [62], and to counterfactual fairness (CF) [39]. Here, for the structural equations S we assume additive noise. This is a convenient but not necessary assumption that simplifies the abduction step when generating the counterfactuals. 6 Weuse a significance level of 𝛼 = 5%, a minimum deviation of 𝜏 = 0 . 0, and a set of 𝑘 group sizes in { 15 , 30 , 50 , 100 } for CST runs that include and exclude the search centers. Also for comparison, we define individual discrimination as Δ 𝑝 > 𝜏 (Def. 3.4) for a single protected attribute. We still, though, demonstrate the use of confidence intervals (Def. 3.5) and how it would affect the final results. Finally, we assume M and G in both ADM scenarios. 7",
  "4.1 An Illustrative Example": "We create a synthetic dataset D based on the scenario in Fig. 1. It is a modified version of Karimi et al. [33, Fig. 1], where we include the protected attribute gender 𝐴 . Here, gender directly affects both an individual's annual salary 𝑋 1 and bank balance 𝑋 2 , which are used by the bank's ADM 𝑏 () for approving ( b 𝑌 = 1) or rejecting ( b 𝑌 = 0) a loan application. We generate D for 𝑛 = 5000 under 𝐴 ∼ Ber ( 0 . 45 ) with 𝐴 = 1 if the individual is female and 𝐴 = 0 otherwise, and assume: 𝑋 1 ←(-$1500 ) · Poi ( 10 ) · 𝐴 + 𝑈 1 ; 𝑋 2 ←(-$300 ) · X 2 ( 4 ) · 𝐴 + ( 3 / 10 ) · 𝑋 1 + 𝑈 2 ; and b 𝑌 = ✶ { 𝑋 1 + 5 · 𝑋 2 > 225000 } with 𝑈 1 ∼ $10000 · Poi ( 10 ) and 𝑈 2 ∼ $2500 · N ( 0 , 1 ) . Here, D represents a known biased scenario . 8 With 𝐴 we introduce a systematic bias onto the relevant decision attributes for female applicants. To run CST we first generate the counterfactual dataset D 𝐶𝐹 based on the intervention 𝑑𝑜 ( 𝐴 : = 0 ) , or what would have happened had all loan applicants been male ? Comparing D to D 𝐶𝐹 already highlights the unwanted systematic effects of 𝐴 . This can be seen, for instance, in Fig. 2 by the rightward shift experienced in 𝑋 2 for all female applicants when going from the factual to the counterfactual world. The loan rejection rate for females drops from 60.9% in D to 38.7% in D 𝐶𝐹 , which is now closer to the loan rejection rate of 39.2% experienced by males in both worlds. We run CST for all 𝑘 sizes. Results are shown in Table 1, where w/o refers to 'without search centers' for CST. Does 𝑏 () discriminate against female applicants? As Table 1 shows, all three methods detect a number of individual discrimination cases. On one hand, the bank clearly uses information that is neutral and needed for approving a loan request; on the other hand, this information is tainted by the effects of gender on such information and the bank, in turn, continues to perpetuate biases against women in this scenario. The results show how these neutral provisions are harmful toward female applicants, and uncover potential individual discrimination cases. 6 Formally, we assume S = { 𝑊 𝑗 ← 𝑓 𝑗 ( 𝑊 𝑝𝑎 ( 𝑗 ) ) + 𝑈 𝑗 } 𝑝 𝑗 = 1 for 𝑊 = 𝑋 ∪ 𝐴 . 7 The code and data are available at https://github.com/cc-jalvarez/counterfactual-situation-testing. 8 Such 'penalties', e.g., capture the financial burdens female professionals face in the present after having been discouraged in the past from pursuing high-paying, male-oriented fields [17]. 10",
  "4.2 Law School Admissions": "Based on the Law School Success example popularized by Kusner et al. [39, Fig. 2] using US data from the Law School Admission Council survey [66], we create an admissions scenario to a top law school. We consider as protected attributes an applicant's gender, male/female, ( G ) and race, white/non-white, ( R ). We add an ADM 𝑏 () that considers the applicant's undergraduate grade-point average ( UGPA ) and law school admissions test scores (LSAT) for admission. If an applicant is successful, b 𝑌 = 1; otherwise b 𝑌 = 0. We summarize the scenario in Fig. 3. For 𝑏 () , we use the median entry requirements for the top US law school to derive the cutoff 𝜓 . 9 The cutoff is the weighted sum of 60% 3.93 over 4.00 in UGPA and 40% 46.1 over 48 LSAT , giving a total of 20.8; the maximum possible score under 𝑏 () is 22 for an applicant. The structural equations follow (1), as in [39], with 𝑏 𝑈 and 𝑏 𝐿 denoting the intercepts; 𝛽 1 , 𝛽 2 , 𝜆 1 , 𝜆 2 the weights; and 𝑈𝐺𝑃𝐴 ∼ N and 𝐿𝑆𝐴𝑇 ∼ Poi. the probability distributions. R G UGPA LSAT b 𝑌  b ✶ Fig. 3. The causal knowledge with corresponding SCM M and DAG G behind the law school admissions dataset, with 𝑅 denoting race ( 𝑅 = 1 for non-white) and 𝐺 denoting gender ( 𝐺 = 1 for female). 9 That being Yale University Law School: https://www.ilrg.com/rankings/law/index/1/asc/Accept 12",
  "5 CONCLUSION": "We presented counterfactual situation testing (CST), a new framework for detecting individual discrimination in a dataset of classifier decisions. Compared to other methods, CST uncovers more cases even when the classifier is counterfactually fair. It also equips counterfactual fairness with uncertainty measures. CST acknowledges the pervasive effects of the protected attribute by comparing individual instances in the dataset that are observably different in the factual world but hypothetically similar in the counterfactual world. Thus, the results are not too surprising as CST operationalizes fairness given the difference , which is a more flexible take on similarity between individuals for testing discrimination than the standard, idealized comparison of two individuals that only differ on their protected status. 13 Alvarez and Ruggieri Implementation. CST is, above all, a framework for detecting discrimination that advocates for building the test group on the generated counterfactual of the complainant. How similarity is defined, e.g., obviously conditions the implementation. We presented a k-NN version with 𝑑 as (6); other implementations are possible and still loyal to CST as long as the construction of the control and test groups follows fairness given the difference . Similarly, detecting discrimination is a difficult, context-specific task. That is why for CST we emphasized the role of the expert in constructing the causal graph necessary for generating the counterfactual instances. Indeed, this step could be optimized using, e.g., causal discovery methods [49], but proving discrimination is time consuming and should remain as such given its sensitive role in our society. Further, we are aware that, e.g., the experimental setting could be pushed further by considering higher dimensions or more complex causal structures. What is the point in doing so, though, if that is not the case with current ADM tools being deployed and audited in real life like the recent Dutch scandal [31]? Proving discrimination is not a problem exclusive to (causal) modeling. With CST we wanted to create a framework aware of the multiple angles to the problem of proving discrimination. The cases we have tackled here are intended to showcase what is possible implementation-wise. Limitations. As future work, promising directions include extending the framework to cases where causal sufficiency does not hold, which is a common risk, and to cases where the decision maker 𝑏 () is non-binary or of a specific type (e.g., a decision tree). Here, we have also focused on tabular data. Future work should push CST further into more complex datasets to explore the scalability and robustness of the framework. Research Ethics. In this work we used anonymized benchmark (and synthetic) data designed for public use, complying with applicable legal and ethical rules. We disclosed all details of our method in line with the transparency mandates of the ACM Code of Ethics.",
  "ACKNOWLEDGMENTS": "Many thanks to Alejandra Bringas and Ioanna Papageorgiou from NoBIAS for their legal commentary. This work has received funding from the European Union's Horizon 2020 research and innovation program under Marie SklodowskaCurie Actions (grant agreement number 860630) for the project \"NoBIAS - Artificial Intelligence without Bias\". This work reflects only the authors' views and the European Research Executive Agency (REA) is not responsible for any use that may be made of the information it contains.",
  "REFERENCES": "[1] Jeffrey S Adler. 2019. Murder in New Orleans: the creation of Jim Crow policing . University of Chicago Press. [2] Aniya Aggarwal, Pranay Lohia, Seema Nagar, Kuntal Dey, and Diptikalyan Saha. 2018. Automated Test Generation to Detect Individual Discrimination in AI Models. CoRR abs/1809.03260 (2018). [3] Joshua D Angrist and Jörn-Steffen Pischke. 2008. Mostly Harmless Econometrics . Princeton University Press. [4] Julia Angwin, Jef Larson, Surya Mattu, and Lauren Kirchner. 2016. Machine Bias. ProPublica (2016). [5] Susan Athey and Guido W Imbens. 2019. Machine learning methods that economists should know about. Annual Review of Economics 11 (2019), 685-725. [6] Solon Barocas and Andrew D Selbst. 2016. Big data's disparate impact. California Law Review 104, 3 (2016), 671-732. [7] Marc Bendick. 2007. Situation Testing for Employment Discrimination in the United States of America. Horizons stratégiques 3, 5 (2007), 17-39. [8] Marianne Bertrand and Esther Duflo. 2017. Field Experiments on Discrimination. Handbook of Economic Field Experiments 1 (2017), 309-393. [9] Marianne Bertrand and Sendhil Mullainathan. 2004. Are Emily and Greg More Employable than Lakisha and Jamal? A Field Experiment on Labor Market Discrimination. The American Economic Review 94, 4 (2004), 991-1013. [10] Emily Black, Samuel Yeom, and Matt Fredrikson. 2020. FlipTest: fairness testing via optimal transport. In FAT* . ACM, 111-121. [11] Eduardo Bonilla-Silva. 1997. Rethinking Racism: Toward a structural interpretation. American Sociological Review (1997), 465-480. [12] Alycia N. Carey and Xintao Wu. 2022. The Causal Fairness Field Guide: Perspectives From Social and Formal Sciences. Frontiers Big Data 5 (2022), 892837. 14",
  "A SUPPLEMENTARY MATERIAL": "",
  "A.1 Working example for generating counterfactuals": "We present a simple working example for counterfactual generation. Given the assumptions we undertake for (1) plus the additional assumption of an additive noise model (ANM)-i.e., S = { 𝑋 𝑗 ← 𝑓 𝑗 ( 𝑋 𝑝𝑎 ( 𝑗 ) ) + 𝑈 𝑗 } 𝑝 𝑗 = 1 -the generating procedure is straightforward. The ANM assumption is also assumed in Section 4 for the classification scenarios. It is a common model specification assumption that allows to identify more easily the non-random parts of the equation. Suppose we have the following structural causal model M and corresponding directed acyclical graph G :  where 𝑈 1 , 𝑈 2 , 𝑈 3 represent the latent variables, 𝑋 1 , 𝑋 2 , 𝑋 3 the observed variables, and 𝛼, 𝛽 1 , 𝛽 2 the coefficient for the causal effect of, respectively, 𝑋 1 → 𝑋 2 , 𝑋 1 → 𝑋 3 , and 𝑋 2 → 𝑋 3 . Suppose we want to generate the counterfactual for 𝑋 3 , i.e., 𝑋 𝐶𝐹 3 , had 𝑋 1 been equal to 𝑥 1 ∈ 𝑋 1 . In the abduction step, we estimate 𝑈 1 , 𝑈 2 , and 𝑈 3 given the evidence or what is observed under the specified structural equations:  We can generalize this step for (1) as 𝑈 𝑗 = 𝑋 𝑗 -𝑓 𝑗 ( 𝑋 𝑝𝑎 ( 𝑗 ) ) ∀ 𝑋 𝑗 ∈ 𝑋 . This step is an individual-level statement on the residual variation under SCM M . It accounts for all that our assignment functions 𝑓 𝑗 , which are at the population level, cannot explain, or the error terms . In the action step, we intervene 𝑋 1 and set all of its instances equal to 𝑥 1 via 𝑑𝑜 ( 𝑋 1 : = 𝑥 1 ) and obtaining the intervened DAG G ′ and SCM M ′ :  where no edges come out from 𝑋 1 as it has been fixed to 𝑥 1 . Finally, in the prediction step, we combine these two steps to calculate 𝑋 𝐶𝐹 3 under the set of ˆ 𝑈 and the intervened M ′ :  which is done for all instances in 𝑋 3 . This is what is done at a larger scale, for example, in [33] and [47], and also in this paper. The same three steps can apply to 𝑋 2 (also for 𝑋 1 , though it would be trivial as it is a root note). We can view this approach as a frequentist 10 one for generating counterfactuals, in particular, with regard to the Abduction step. A more Bayesian approach is what is done by [39] where they use a Monte Carlo Markov Chain (MCMC) to draw ˆ 𝑈 by updating its prior distribution with the evidence 𝑋 to then proceed with the other two steps. In 10 This is not a formal distinction, but based on talks with other researchers in counterfactual generation. Such a distinction, to the best of our knowledge, remains an open question. 17 Alvarez and Ruggieri Section 4.2, we used both approaches for generating the counterfactuals and found no difference in the results. We only present in this paper the first approach as it is less computationally expensive.",
  "A.2 Sketch of Proof for Proposition 3.6": "Consider the factual tuple ( 𝑥 𝑐 , 𝑎 𝑐 = 1 , b 𝑦 𝑐 = 0 ) and assume the generated counterfactual is ( 𝑥 𝐶𝐹 𝑐 , 𝑎 𝐶𝐹 𝑐 = 0 , b 𝑦 𝐶𝐹 𝑐 = 0 ) . Since b 𝑦 𝑐 = b 𝑦 𝐶𝐹 𝑐 , this is a case where counterfactual fairness holds. However, the decision boundary of the model 𝑏 () can be purposely set such that the 𝑘 -nearest neighbors of 𝑥 𝑐 are all within the decision ˆ 𝑌 = 0, and less than 1 -𝜏 fraction of the 𝑘 -nearest neighbors of 𝑥 𝐶𝐹 𝑐 are within the decision ˆ 𝑌 = 0. This leads to a Δ 𝑝 > 1 - ( 1 -𝜏 ) = 𝜏 , showing that there is individual discrimination. The other way can be shown similarly by assuming b 𝑦 𝑐 ≠ b 𝑦 𝐶𝐹 𝑐 but the sets of 𝑘 -nearest neighbors have rates of negative decisions whose difference is lower than 𝜏 .",
  "B ALGORITHMS FOR K-NN CST IMPLEMENTATION": "We present the relevant algorithms for the k-NN CST implementation (Section 3.4). The algorithm 1 performs CST while algorithm 2 returns the indices of the top𝑘 tuples with respect to the search centers based on the distance function 𝑑 . Notice that the main difference in algorithm 1 when creating the neighborhoods is that the search centers are drawn from the factual dataset for the control group D and the counterfactual dataset D 𝐶𝐹 for the test group. Further, notice that we use the same 𝑐 (i.e., index) for both as these two data-frames have the same structure by construction.",
  "Algorithm 1: run_CST": "Input : D , D 𝐶𝐹 , 𝑘 Output: [ 𝑝 𝑐 -𝑝 𝑡 ] 𝑝𝑟𝑜𝑡 _ 𝑐𝑜𝑛𝑑𝑖𝑡𝑖𝑜𝑛 ← D[ : , 𝑝𝑟𝑜𝑡 _ 𝑎𝑡𝑡𝑟𝑖𝑏𝑢𝑡𝑒 ] == 𝑝𝑟𝑜𝑡 _ 𝑣𝑎𝑙𝑢𝑒 D 𝑐 ← D[ 𝑝𝑟𝑜𝑡 _ 𝑐𝑜𝑛𝑑𝑖𝑡𝑖𝑜𝑛 ] D 𝑡 ← D[¬ 𝑝𝑟𝑜𝑡 _ 𝑐𝑜𝑛𝑑𝑖𝑡𝑖𝑜𝑛 ] 𝑝𝑟𝑜𝑡 _ 𝑖𝑑𝑥 ← D 𝑐 .𝑖𝑛𝑑𝑒𝑥.𝑡𝑜 _ 𝑙𝑖𝑠𝑡 ( ) ; 𝑑𝑖𝑓 𝑓 _ 𝑙𝑖𝑠𝑡 = [ ] for 𝑐, 𝑟𝑜𝑤 ∈ 𝑝𝑟𝑜𝑡 _ 𝑖𝑑𝑥 do 𝑟𝑒𝑠 _1 ← 𝑔𝑒𝑡 _ 𝑡𝑜𝑝 _ 𝑘 ( D [ 𝑐, : ] , D 𝑐 , 𝑘 ) ; // get protected (control) search space // get non-protected (test) search space // get idx for all complainants // idx of the top-k tuples for control group 𝑟𝑒𝑠 _2 ← 𝑔𝑒𝑡 _ 𝑡𝑜𝑝 _ 𝑘 ( D 𝐶𝐹 [ 𝑐, : ] , D 𝑡 , 𝑘 ) ; // idx of the top-k tuples for test group 𝑝 𝑐 ← 𝑠𝑢𝑚 ( D[ 𝑟𝑒𝑠 1 , 𝑡𝑎𝑟𝑔𝑒𝑡 _ 𝑎𝑡𝑡𝑟𝑖𝑏𝑢𝑡𝑒 ] == 𝑛𝑒𝑔𝑎𝑡𝑖𝑣𝑒 _ 𝑜𝑢𝑡𝑐𝑜𝑚𝑒 ) / 𝑙𝑒𝑛 ( 𝑟𝑒𝑠 _1 ) 𝑝 𝑡 ← 𝑠𝑢𝑚 ( D[ 𝑟𝑒𝑠 2 , 𝑡𝑎𝑟𝑔𝑒𝑡 _ 𝑎𝑡𝑡𝑟𝑖𝑏𝑢𝑡𝑒 ] == 𝑛𝑒𝑔𝑎𝑡𝑖𝑣𝑒 _ 𝑜𝑢𝑡𝑐𝑜𝑚𝑒 ) / 𝑙𝑒𝑛 ( 𝑟𝑒𝑠 _2 ) 𝑑𝑖𝑓 𝑓 _ 𝑙𝑖𝑠𝑡 [ 𝑐 ] ← 𝑝 𝑐 -𝑝 𝑡 end return 𝑑𝑖𝑓 𝑓 _ 𝑙𝑖𝑠𝑡",
  "Algorithm 2: get_top_k": "Input : 𝑡 , 𝑡 _ 𝑠𝑒𝑡 , 𝑘 Output: [ 𝑖𝑛𝑑𝑖𝑐𝑒𝑠 ] ( 𝑖𝑑𝑥,𝑑𝑖𝑠𝑡 ) ← 𝑘 _ 𝑁𝑁 ( 𝑡, 𝑡 _ 𝑠𝑒𝑡, 𝑘 + 1 ) ; if without search centers then 𝑟𝑒𝑚𝑜𝑣𝑒 ( 𝑡, 𝑖𝑑𝑥, 𝑑𝑖𝑠𝑡 ) ; end 𝑖𝑑𝑥 ′ ← 𝑠𝑜𝑟𝑡 ( 𝑖𝑑𝑥,𝑑𝑖𝑠𝑡 ) ; return 𝑖𝑑𝑥 ′ // run k-NN algorithm with 𝑘 + 1 // remove the center t from idx // sort idx by the distance Received 10 May 2023; revised 5 September 2023; accepted 20 September 2023 18",
  "keywords_parsed": [
    "None"
  ]
}