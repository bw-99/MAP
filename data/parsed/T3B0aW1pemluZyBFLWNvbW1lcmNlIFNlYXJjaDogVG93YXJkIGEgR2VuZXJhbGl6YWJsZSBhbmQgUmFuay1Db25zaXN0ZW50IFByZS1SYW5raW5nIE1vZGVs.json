{
  "Optimizing E-commerce Search: Toward a Generalizable and Rank-Consistent Pre-Ranking Model": "",
  "ABSTRACT": "In large e-commerce platforms, search systems are typically composed of a series of modules, including recall, pre-ranking, and ranking phases. The pre-ranking phase, serving as a lightweight module, is crucial for filtering out the bulk of products in advance for the downstream ranking module. Industrial efforts on optimizing the pre-ranking model have predominantly focused on enhancing ranking consistency, model structure, and generalization towards long-tail items. Beyond these optimizations, meeting the system performance requirements presents a significant challenge. Contrasting with existing industry works, we propose a novel method: a G eneralizable and RA nkC onsist E nt Pre-Ranking Model ( GRACE ), which achieves: 1) Ranking consistency by introducing multiple binary classification tasks that predict whether a product is within the top-k results as estimated by the ranking model, which facilitates the addition of learning objectives on common point-wise ranking models; 2) Generalizability through contrastive learning of representation for all products by pre-training on a subset of ranking product embeddings; 3) Ease of implementation in feature construction and online deployment. Our extensive experiments demonstrate significant improvements in both offline metrics and online A/B test: a 0.75% increase in AUC and a 1.28% increase in CVR.",
  "CCS CONCEPTS": "· Information systems → Novelty in information retrieval ; Information retrieval ; Retrieval models and ranking ;",
  "KEYWORDS": "Embedding index, neural network, information retrieval",
  "ACMReference Format:": "Enqiang Xu, Yiming Qiu ∗ , Junyang Bai, Ping Zhang, Dadong Miao, Songlin Wang,GuoyuTang,LinLiu, and MingMing Li ∗ . 2024. Optimizing E-commerce ∗ Corresponding author Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '24, July 14-18, 2024, Washington, DC, USA © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0431-4/24/07 https://doi.org/10.1145/3626772.3661343 Search: Toward a Generalizable and Rank-Consistent Pre-Ranking Model. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '24), July 14-18, 2024, Washington, DC, USA. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/ 3626772.3661343",
  "1 INTRODUCTION": "Large-scale E-commerce platforms potentially engage in the sale of billions of items, catering to a user base in the hundreds of millions. To enhance user experience and conversion efficiency, the online search system is employed with a cascading architecture, mainly including recall and ranking. The ranking stage as the downstream component directly influences the efficiency of item sorting. Several superior ranking models have been identified in industrial research, such as MMoE [4], PLE [12], ESMM [5], DeepFM [1], DIN [18], MIMN [8], SDIM [16] , and SIM [12], with a focus on feature engineering, behavioral sequence modeling, and objective function optimization. However, as the scale of products within the search system grows, there is an increasing demand for managing the time complexity of the sorting module. To this end, the industry often divides the sorting process into pre-ranking and ranking models, operating in tandem to balance the trade-off between system processing time and the accuracy of sorting. To some extent, the effectiveness of pre-ranking sets the upper limit for the ranking, thereby affecting the performance of the entire search system. There is an increasing scholarly interest in optimizing pre-ranking models. Commonly, dual-tower models or simple interaction models are employed as the foundational model. Representative dualtower models like DSSM [2] calculate the similarity between queries and documents for scoring and ranking to achieve optimal efficiency. Models like PFD [15] introduce interacted features into the learning of pre-ranking models through the distillation of representations from ranking models. Single-tower models exemplified by COLD [14], which incorporate SE (Squeeze-and-Excitation) blocks in the feature selection process, balance effectiveness, and computational resources, with experimental data showing optimized single-tower models achieving better results without significantly increasing prediction time. Beyond the optimization of ranking models themselves, it is essential to address the distinct challenges that pre-ranking models face compared to ranking models. We identify several key challenges: 1) Consistency with ranking : As previously discussed, the goal of pre-ranking models is to filter out items early in the system to save computational resources for ranking. Achieving closer ranking model performance with fewer model parameters is the SIGIR '24, July 14-18, 2024, Washington, DC, USA Enqiang Xu et al. core intent behind the design of pre-ranking modules. In addition to adopting approximate objective functions, features, and model structures, the industry has proposed direct approaches to model consistency. RankFlow [9] introduces a two-stage model, where the first stage involves self-learning using online feedback labels, and the second stage incorporates downstream task scoring, although learning scores of ranking model is challenging. COPR [17] suggests using relative positioning in ranking as pair-wise supervision signals in pre-ranking systems for advertisements, adding weight factors to penalize incorrect ordering of top pairs. While this method circumvents the issue of learning ranking scores, it requires data transformation to construct pair relationships within the same session. 2) Generalization : It is widely accepted that simply placing ranking models in the position of pre-ranking does not yield positive results, indicating that fitting pre-ranking models to the ranking are not optimal. Pre-ranking models predict a larger array of long-tail items, whereas ranking models aim to sort within the sequence output by pre-ranking. Thus, pre-ranking models require improved item representation and long-tail generalization capabilities. Meta-Embedding [7] proposes a supervised framework to optimize both cold-start and warm-up advertisement representations, but this method is limited by only using attribute features of items to generate cold-start embeddings and is not as straightforward due to its two-stage training process. MWUF [19] introduces optimizations and simplifies the meta-learning process, yet this approach is still not end-to-end and does not fully leverage attribute features for cold-start items. To alleviate the above problems, we propose an innovative approach: a G eneralizable and RA nkC onsist E nt Pre-Ranking Model ( GRACE ). This method is designed to enhance performance by addressing two critical aspects: rank consistency and generalizability. Figure 1 illustrates the overall architecture of the model. Firstly, for rank consistency, GRACE innovatively incorporates a consistency task within the pre-ranking framework, which is built upon a multi-task baseline model [12]. Specifically, it leverages position information from ranking training data (online logs) and learns whether a given sample belongs to the top k positions as determined by the ranking model. This is achieved by introducing an additional loss function, which requires no modification to the existing training data or process, thereby simplifying implementation. Secondly, for generalizability, our model begins by hashing item IDs to generate hash ID embeddings, which are initialized and looked up at the scale of millions, and then fuses these with attributes embeddings (initialized and looked up similarly, for attributes such as brand, shop, category, etc.) to initialize item representations through concatenation. To enhance the accuracy of these embeddings, we introduce supervision signals derived from embeddings generated by a pre-trained Graph Neural Network (GNN) and employ Info-NCE [6] loss for end-to-end learning. Our model, with reduced parameter size, achieves comparable accuracy to models trained directly with embeddings of millions of items. Furthermore, unlike methods relying on pre-trained embeddings of a limited number of items, GRACE supports the generalization across an unlimited range of long-tail items. The optimizations integrated into GRACE do not impose significant storage or computational burdens, whether in offline training Figure 1: The proposed framework, where \"Attrs\" refers to attribute information such as brand, shop, and category. PLE Input Layer Item Embedding Embedding-Net Concat Pretrained Embedding Online Offline Training Hash ID Item ID Attrs Negative Positive Contrastive Learning CTR CVR Rank User Query Item or online deployment, making it an elegant and straightforward adaptation for industrial model enhancements. Empirical validation through offline experiments demonstrated a 0.749% improvement in the area under the ROC curve (AUC [15]). In online A/B testing, GRACE achieved a 1.26% increase in conversion rate (CVR) and a 1.62% rise in gross merchandise value (GMV), with even more pronounced improvements for long-tail items, where we observed a 2.89% boost in CVR and a 10.37% increase in GMV. These results underscore the effectiveness of GRACE in addressing the nuanced demands of pre-ranking models in e-commerce search engines.",
  "2 METHOD": "",
  "2.1 Task Definition": "Given a user 𝑈 entering a query 𝑄 into the search bar, a set of pre-ranking candidate products 𝑆 = { item1 , item2 , . . . , item 𝑀 } is generated, where 𝑀 denotes the number of pre-ranking candidate products. The model computes predictions ˆ 𝑦 = 𝐹 ( 𝑈,𝑄,𝑆 ) , from which the top 𝑁 products are selected and returned to the following ranking phase, with 𝑁 ≪ 𝑀 . In our training dataset, we utilize user feedback from online logs as labels, where the click-through rate (CTR) records whether a user clicks and the conversion rate (CVR) records whether a user converts.",
  "2.2 Base Model": "The baseline is the Progressive Layered Extraction [12] (PLE) multitask model which addresses the trade-off issue between multiple tasks by employing a combination of shared and task-specific expert networks. We input the aforementioned variables 𝑈 , 𝑄 , and 𝑆 into the model to predict the click-through rate (CTR) and conversion rate (CVR), as shown in the following equations:  We employ the cross-entropy loss to compute the loss for each task, where 𝑦 𝑐𝑡𝑟 and 𝑦 𝑐𝑣𝑟 represent the click label and conversion label respectively, denoted as: Optimizing E-commerce Search: Toward a Generalizable and Rank-Consistent Pre-Ranking Model SIGIR '24, July 14-18, 2024, Washington, DC, USA   The total loss of base model could be formulated as:",
  "2.3 Rank Consistency Loss": "For the pre-ranking candidate items, we get their positions after ranking, where 𝑝𝑜𝑠 𝑟𝑎𝑛𝑘 𝑖 represents the position of 𝑖𝑡𝑒𝑚 𝑖 after scoring in the ranking model.  Depending on whether the product's position falls within the top 𝑘 , we establish a binary classification task to determine its membership in the top𝑘 subset.  In the following, we extend the initial pre-ranking task by additionally predicting the objective of rank consistency.  Then, we could get the loss of consistency, denoted as:  To further refine the accuracy of the top𝑘 retrieval task, we employ multiple values of 𝑘 and aggregate the corresponding losses to compute the overall ranking loss 𝐿 rank , where 𝑤 𝑘 is the hyperparameter.",
  "2.4 Generalization Loss": "We incorporate an auxiliary contrastive learning task to enforce the representation of items to align closely with pre-trained embeddings. Let 𝑃 denote the set of pre-trained items, 𝜙 represent the embedding network, and 𝜓 the pre-trained embeddings. Given a batch size 𝑁 and an inner product ⟨· , ·⟩ , with 𝜏 as the temperature coefficient, the contrastive learning loss is computed only for positive instances with existing pre-trained embeddings. Specifically, the aforementioned InfoNCE [6] loss encourages a reduction in the distance between the representations of items and their corresponding pre-trained embeddings, while promoting an increase in the distance between item embeddings across different product categories.  Table 1: Offline metrics. Table 2: Ablation study.",
  "2.5 Total Loss": "The final optimization objective of the model is defined below, where 𝜆 1 and 𝜆 2 are hyper-parameters.",
  "3 EXPERIMENTS": "",
  "3.1 Setup": "3.1.1 Dataset. Our training dataset is compiled from 15 days of search logs, amassing approximately one billion records. Each record is annotated with multiple binary encoded labels indicating whether an item is clicked and ordered (click label and conversion label). As to negative instances, they comprise two categories: one sourced from user feedback, such as items displayed but not clicked, and the other from random sampling of items not displayed to enrich the pool of negative samples. For model validation, we utilized data from the entire 16th day to evaluate the model's performance. 3.1.2 Metrics. · AUC: Area Under Curve, is a performance measurement for classification problems, quantifying the ability of a model to distinguish between classes [15]. · GMV: Gross Merchandise Value, a metric that measures the total value of sales for merchandise sold through a particular marketplace over a specific time period [10, 11]. · CVR: Conversion Rate, widely adopted indicators within E-commerce platforms [3, 13]. · Recall@k: measures the consistency between pre-ranking and ranking results by calculating the intersection of the top-k items from both the pre-ranking and ranking models, divided by the top-k items from the ranking model.  3.1.3 Baseline models. For the sake of a fair comparison, we select the most representative works from the industry, such as PLE and MMoE, against which to benchmark our model. Our model is an optimization directly based on the PLE model. Additionally, to validate our consistency approach, we add a model that employs direct distillation of ranking scores for comparison. Specifically: · PLE: Represents our baseline model deployed online. · MMoE: Stands for the Multi-gate Mixture-of-Experts, a commonly used multi-task model in the industry. · Distillation: Represents the model utilizing the distillation of ranking scores. SIGIR '24, July 14-18, 2024, Washington, DC, USA Enqiang Xu et al. Table 3: Consistency analysis about Recall@k. Table 4: Online A/B test relative improvement. p-value is obtained by t-test, small p-value means statistically significant. 3.1.4 Hyperparameter. As to the top-k value for the consistency loss in our method, we empirically chose 10, 30, 50, and 100 as parameters for our experiments, which are determined by factors such as the browsing depth of users on different e-commerce platforms or the number of products displayed per page on the front-end interface. On our platform, each page displays 10 items, and the average browsing depth of users is 30. In the loss function, 𝜆 1 and 𝜆 2 are both set to 0.1. As for the model parameters: the batch size is 1024, the learning rate is 0.05, the optimizer is Adagrad, and the number of epochs is 20. For the hash ID, the embedding dimension is 32, and the vocabulary size is 3 million.",
  "3.2 Comparison with baseline models": "From Table 1, it is evident that our model shows a significant improvement over the PLE model, with the AUC absolute value increasing by 0.749%. For reference, when compared to another baseline model, MMoE, the increase is even more substantial. This demonstrates that the improvement margin of our model far exceeds the gap between MMoE and PLE. Furthermore, in comparison with the mainstream industry method of distillation, our results show that distillation can bring about a 0.14% increase in AUC. However, the gap remains significant compared to our model, validating our earlier discussion that directly learning from ranking scores is not necessarily the optimal solution.",
  "3.3 Ablation Study": "In our model, we have introduced multiple enhancements over PLE, as detailed in the comprehensive ablation experiments presented in Table 2. This allows us to analyze the sources of AUC gains. The abbreviation \"w/o\" in the table stands for \"without.\" 3.3.1 Generalization module. The ablation results in table 2 demonstrate that the generalization module contributes to a 0.41% increase in AUC, sufficiently proving its effectiveness. Furthermore, we conducted experiments from two perspectives to further validate the significance of the generalization module: · w/o hash id: the straightforward way was to generate generalized representations using product attribute features. However, it became apparent that many product attributes are repetitive, leading to duplicate representations, such as those for products from the same brand or shop, which could potentially conflict and reduce the accuracy of the representations. This duplication posed a significant challenge in learning pretrain embeddings. To address this, we introduced hashed ID as an initialization, which, after a lookup, yielded generalized ID embeddings with a smaller parameter footprint. We posited that hashed ID, combined with product attributes, could serve as unique representations. Upon removing the hashed ID from our model, we observed a 0.13% decrease in AUC, confirming the effectiveness of hashing. · pre-trained item id: We removed the generalization module from our model and replaced it with pre-trained embeddings, which, despite their large parameter size (with a vocabulary size of 30 million and an embedding dimension of 32), still fall short of covering all products. This substitution significantly increased the storage burden for the pre-ranking model. Our findings indicate that GRACE slightly outperforms the one with pre-trained embeddings, demonstrating that we have successfully learned a comparable effect with fewer parameters. Moreover, our model provides more accurate representations for long-tail products, leading to a further increase in AUC. 3.3.2 Rank consistency module. The ablation study on the consistency module presented in Table 2 shows that the experiments lacking this module exhibit an AUC that is 0.35% lower than that of the GRACE model, conclusively demonstrating the efficacy of the consistency module. 3.3.3 Rank consistency analysis. Analysis of Table 3 reveals a marked increase in recall rates for both pre-ranking and ranking sets, with enhancements ranging from 3.50% to 4.43%. Notably, the largest gains were recorded in recall@3 and recall@10, metrics of heightened importance on our platform, which displays a maximum of 10 products per page on the front-end.",
  "3.4 Online A/B Test": "On an e-commerce platform with tens of millions of daily active users, we conducted a 7-day A/B test. Variant A deployed the baseline PLE model, while Variant B deployed GRACE model, with each variant receiving 10% of the traffic. From table 4, we can observe a cumulative increase of 1.28% in conversion rate (CVR) and 1.62% in gross merchandise volume (GMV). By examining the long-tail traffic, which accounts for approximately 10% of the total traffic, we confirmed the significant generalization effects of the proposed model, achieving a 2.89% increase in CVR and a 10.87% increase in GMV within this segment. GRACE was deployed in a production environment at the end of 2022.",
  "4 CONCLUSION": "In our work, we introduce GRACE, a novel pre-ranking model that addresses the dual challenges of rank consistency and generalizability. By innovatively incorporating position information from ranking, GRACE achieves rank consistency without requiring modifications to the training data or process. For generalizability, we fuse hash ID embeddings with attribute-based representations, enabling effective scaling to billions of items without significant storage. Compared with related works, GRACE demonstrates competitive results on both offline studies and online A/B test. Optimizing E-commerce Search: Toward a Generalizable and Rank-Consistent Pre-Ranking Model SIGIR '24, July 14-18, 2024, Washington, DC, USA",
  "REFERENCES": "[1] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. arXiv preprint arXiv:1703.04247 (2017). [2] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. 2013. Learning deep structured semantic models for web search using clickthrough data. In Proceedings of the 22nd ACM international conference on Information & Knowledge Management . 2333-2338. [3] Mingming Li, Chunyuan Yuan, Huimu Wang, Peng Wang, Jingwei Zhuo, Binbin Wang, Lin Liu, and Sulong Xu. 2023. Adaptive Hyper-parameter Learning for Deep Semantic Retrieval. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: EMNLP 2023 - Industry Track, Singapore, December 6-10, 2023 , Mingxuan Wang and Imed Zitouni (Eds.). Association for Computational Linguistics, 775-782. https://aclanthology.org/2023.emnlp-industry.72 [4] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. 2018. Modeling task relationships in multi-task learning with multi-gate mixture-ofexperts. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1930-1939. [5] Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu, and Kun Gai. 2018. Entire space multi-task model: An effective approach for estimating post-click conversion rate. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval . 1137-1140. [6] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018). [7] Feiyang Pan, Shuokai Li, Xiang Ao, Pingzhong Tang, and Qing He. 2019. Warm up cold-start advertisements: Improving ctr predictions via learning to learn id embeddings. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval . 695-704. [8] Qi Pi, Weijie Bian, Guorui Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Practice on long sequential user behavior modeling for click-through rate prediction. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 2671-2679. [9] Jiarui Qin, Jiachen Zhu, Bo Chen, Zhirong Liu, Weiwen Liu, Ruiming Tang, Rui Zhang, Yong Yu, and Weinan Zhang. 2022. RankFlow: Joint Optimization of MultiStage Cascade Ranking Systems as Flows. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval . 814-824. [10] Yiming Qiu, Kang Zhang, Han Zhang, Songlin Wang, Sulong Xu, Yun Xiao, Bo Long, and Wen-Yun Yang. 2021. Query Rewriting via Cycle-Consistent Translation for E-Commerce Search. In 2021 IEEE 37th International Conference on Data Engineering (ICDE) . IEEE, 2435-2446. [11] Yiming Qiu, Chenyu Zhao, Han Zhang, Jingwei Zhuo, Tianhao Li, Xiaowei Zhang, Songlin Wang, Sulong Xu, Bo Long, and Wen-Yun Yang. 2022. Pre-training Tasks for User Intent Detection and Embedding Retrieval in E-commerce Search. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management . 4424-4428. [12] Hongyan Tang, Junning Liu, Ming Zhao, and Xudong Gong. 2020. Progressive layered extraction (ple): A novel multi-task learning (mtl) model for personalized recommendations. In Proceedings of the 14th ACM Conference on Recommender Systems . 269-278. [13] Binbin Wang, Mingming Li, Zhixiong Zeng, Jingwei Zhuo, Songlin Wang, Sulong Xu, Bo Long, and Weipeng Yan. 2023. Learning Multi-Stage Multi-Grained Semantic Embeddings for E-Commerce Search. In Companion Proceedings of the ACMWebConference 2023, WWW 2023, Austin, TX, USA, 30 April 2023 - 4 May 2023 , Ying Ding, Jie Tang, Juan F. Sequeda, Lora Aroyo, Carlos Castillo, and Geert-Jan Houben (Eds.). ACM, 411-415. https://doi.org/10.1145/3543873.3584638 [14] Zhe Wang, Liqin Zhao, Biye Jiang, Guorui Zhou, Xiaoqiang Zhu, and Kun Gai. 2020. COLD: Towards the Next Generation of Pre-Ranking System. CoRR abs/2007.16122 (2020). arXiv preprint arXiv:2007.16122 (2020). [15] Chen Xu, Quan Li, Junfeng Ge, Jinyang Gao, Xiaoyong Yang, Changhua Pei, Fei Sun, Jian Wu, Hanxiao Sun, and Wenwu Ou. 2020. Privileged features distillation at taobao recommendations. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 2590-2598. [16] Zihao Zhao, Zhiwei Fang, Yong Li, Changping Peng, Yongjun Bao, and Weipeng Yan. 2020. Dimension relation modeling for click-through rate prediction. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management . 2333-2336. [17] Zhishan Zhao, Jingyue Gao, Yu Zhang, Shuguang Han, Siyuan Lou, Xiang-Rong Sheng, Zhe Wang, Han Zhu, Yuning Jiang, Jian Xu, et al. 2023. COPR: ConsistencyOriented Pre-Ranking for Online Advertising. arXiv preprint arXiv:2306.03516 (2023). [18] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through rate prediction. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1059-1068. [19] Yongchun Zhu, Ruobing Xie, Fuzhen Zhuang, Kaikai Ge, Ying Sun, Xu Zhang, Leyu Lin, and Juan Cao. 2021. Learning to warm up cold item embeddings for coldstart recommendation with meta scaling and shifting networks. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1167-1176.",
  "COMPANY PORTRAIT": "JD.com, Inc., also known as Jingdong, is a Chinese e-commerce company headquartered in Beijing. It is one of the two massive B2C online retailers in China by transaction volume and revenue, a member of the Fortune Global 500. When classified as a tech company, it is the largest in China by revenue and 7th in the world in 2021.",
  "PRESENTER PROFILES": "Enqiang Xu is a researcher in the Department of Search and Recommendation at JD.com Beijing. He received his master degree in School of Mathematical Sciences, Peking University. His research focuses on information retrieval and natural language processing. Yiming Qiu is a researcher in the Department of Search and Recommendation at JD.com Beijing. He received his master degree in School of Computing and Information System, the University of Melbourne. His research focuses on information retrieval and natural language processing. Mingming Li is a researcher in the Department of Search and Recommendation at JD.com Beijing. He received his doctor degree in Institute of Automation, Chinese Academy of Sciences. His research focuses on information retrieval and natural language processing.",
  "keywords_parsed": [
    "Embedding index",
    " neural network",
    " information retrieval"
  ]
}