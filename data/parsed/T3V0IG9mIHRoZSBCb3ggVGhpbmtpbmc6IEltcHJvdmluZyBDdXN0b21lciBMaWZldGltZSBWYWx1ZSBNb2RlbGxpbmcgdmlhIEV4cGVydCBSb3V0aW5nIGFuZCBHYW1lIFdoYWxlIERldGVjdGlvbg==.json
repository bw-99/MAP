{"Out of the Box Thinking: Improving Customer Lifetime Value Modelling via Expert Routing and Game Whale Detection": "Shijie Zhang \u2217 Tencent Shenzhen, China julysjzhang@tencent.com Xin Yan Tencent Shenzhen, China cyndiyan@tencent.com Xuejiao Yang Tencent Shenzhen, China jerrieyang@tencent.com Binfeng Jia Tencent Shenzhen, China vincejia@tencent.com Shuangyang Wang Tencent Shenzhen, China feymanwang@tencent.com", "ABSTRACT": "Customer lifetime value (LTV) prediction is essential for mobile game publishers trying to optimize the advertising investment for each user acquisition based on the estimated worth. In mobile games, deploying microtransactions is a simple yet effective monetization strategy, which attracts a tiny group of game whales who splurge on in-game purchases. The presence of such game whales may impede the practicality of existing LTV prediction models, since game whales' purchase behaviours always exhibit varied distribution from general users. Consequently, identifying game whales can open up new opportunities to improve the accuracy of LTV prediction models. However, little attention has been paid to applying game whale detection in LTV prediction, and existing works are mainly specialized for the long-term LTV prediction with the assumption that the high-quality user features are available, which is not applicable in the UA stage. In this paper, we propose ExpLTV, a novel multi-task framework to perform LTV prediction and game whale detection in a unified way. In ExpLTV, we first innovatively design a deep neural network-based game whale detector that can not only infer the intrinsic order in accordance with monetary value, but also precisely identify high spenders (i.e., game whales) and low spenders. Then, by treating the game whale detector as a gating network to decide the different mixture patterns of LTV experts assembling, we can thoroughly leverage the shared information and scenario-specific information (i.e., game whales modelling and low spenders modelling). Finally, instead of separately designing a purchase rate estimator for two tasks, we design a shared estimator that can preserve the inner task relationships. The superiority of ExpLTV in terms of its LTV prediction and game whale detection effectiveness is further validated via extensive experiments on three industrial datasets.", "CCS CONCEPTS": "\u00b7 Information systems \u2192 Information systems applications .", "KEYWORDS": "Customer Lifetime Value Prediction; Game Whale Detection; MultiTask Learning", "ACMReference Format:": "Shijie Zhang, Xin Yan, Xuejiao Yang, Binfeng Jia, and Shuangyang Wang. 2023. Out of the Box Thinking: Improving Customer Lifetime Value Modelling via Expert Routing and Game Whale Detection. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management (CIKM '23), October 21-25, 2023, Birmingham, United Kingdom. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3583780.3615002", "1 INTRODUCTION": "For top-grossing games, a well-rounded user acquisition (UA) [35] strategy is privilege to grow their user base. Without an effective strategy, the game publishers will miss out on swathes of opportunities to convert more valuable users. The easiest way to get the attention of new users on online services for promoting games is paid advertising. Most of social media (e.g., Sina Weibo) with massive active users own real-time bidding services for online advertising to deliver paid ads to their users, and thus it is an effective way to connect with target consumers and promoting games. Customer Lifetime Value (LTV) [4, 20, 21, 33] which refers to the total worth that a company can attribute to a customer over a specific period \ud835\udc47 . It is not only essential to estimate the long-term revenue from an established customer for personalized customer relationship management (CRM) [2, 25], but also allowing to predict the short-term worth of a new customer for adjusting the budget of advertising investment. For each ad impression, the advertisers can compute an early estimation of the customer's LTV value in the target games, and then the predicted values will be used to adjust the final bidding price, so as to enhance the success of bidding valuable users and optimize the marketing budget. In this context, LTV prediction accuracy means that the predicted LTV values should be sorted in a manner consistent with the true LTV values, and inaccurate LTV predictions will lead to an extra market budget and decreased success of UA. In this regard, many sorts of LTV prediction models have been developed for various applications. The early LTV prediction methods assume the purchase behaviors as a probability distribution, and then employ the probabilistic Game Whale Users' LTV Low Spenders' LTV Customer Lifetime Values Frequency generative models for predicting the future purchase values and customer churn [15, 16, 19, 32]. Another line of research proposes machine learning-based models to learn a mapping between handcrafted features and monetary value of game players [12, 19, 37]. For example, [37] is designed to predict future value on an individual user basis with a random forest model in Groupon. Recently, the prominent development of deep learning-based techniques brings more opportunities to improve the performance of LTV prediction models. [8] adopts a convolutional neural network to automatically learn temporal representations for LTV prediction. To handle the volatile and sparse data of monetary value, [43] adopts Temporal Trend Encoder and Graph Attention Network [38] to learn the temporal and structure representation respectively. [26] proposes an industrial solution in KUAISHOU, which predicts users' DAU via multiple distribution models to deal with the complex distribution. Despite the efficacy of existing LTV predictions in many realworld applications, the majority of these methods are inapplicable in our scenarios. Most of these methods are only focused on the long-term LTV predictions with the assumption that the rich and high-quality user features are available. For example, in [43], to outright predict day 30 LTV, the proposed model requires access to a large number of users with day 120 LTVs and user features available. Due to app updates, user interest changes and time sensitivity, it is of great practical significance to construct a short-term LTV prediction model for game promotion. Moreover, as the gaming industry has progressed, microtransactions have been a regular fixture in gaming to boost revenues and improve user life cycle, especially for those free-to-play games. Gamers can purchase additional in-game items to decorate their character, upgrade weapons, or gain extra perks. Provided such microtransactions tends to attract many Game Whales (i.e., high spenders) who splurge on in-game purchases[5, 13]. In the presence of such unpredictable and extreme purchase behaviours, these aforementioned LTV prediction models are subject to different levels of performance drop in the mobile game context, since the models are designed without the awareness of game whales and are sensitive to large values. In mobile games, the game whales (GW) represent the smallest percentage of users who are responsible for up to 50% -80% in revenue sales. Thus, it is imperative to spot these game whales in the UA stage. Winning these customers can create a positive feedback loop, where they bring in more profit for game operations. Despite the importance of detecting game whales, most existing solutions [8] straightforwardly use predicted LTV values as the dominant indicators to detect those high-value users. Since the game whales are rare and corresponding LTV values are largely deviated from general users, the pure LTV prediction models trained by heavily imbalanced dataset cannot perform well on the extremely minority labels, and thus lead to inferior performance in game whale detection task. To provide a proof-of-concept, in Figure 1, we show the LTV distribution of GAME A. Clearly, the game whales' LTV values demonstrate a different long-tailed distribution from that of low spenders. Moreover, directly using conventional deeplearning based models in game whale detection task will suffer from Sample Selection Bias (SSB) [45] and Data Sparsity (DS) ![10] problems. SSB problem exists due to a flaw in the training process where a subset of the data is under-sampled, and thus significantly bias the estimates in the inference space. Specifically, GW detection models are trained on dataset composed of paying users, while are utilized to make inference on all samples of convert users. Such biased training will cause severe performance drop in online services. In addition, the game whales with outstanding LTV values are far less than general users, and thus a new paradigm is desired to effectively counteract the long-standing problem of DS problem. To this end, we aim to propose a novel multi-task framework named ExpLTV, which makes full use of GW detection to boost LTV prediction accuracy. Instead of directly using simple constraints to divide users into game whales and general users, we innovatively design a mapping function to calculate the user's probability of being game whales and transfer the binary classification task into a regression task. In this way, the specifically designed detector can not only capture the intrinsic order in accordance with the monetary value, but also generate the probability of being low spenders and high spenders to bucket users for customized game marketing. In addition, to eliminate the aforementioned SSB and DS problems simultaneously, we form a new sequential behavior \" \ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc63\ud835\udc52\ud835\udc5f\ud835\udc61 \u2192 \ud835\udc5d\ud835\udc62\ud835\udc5f\ud835\udc50\u210e\ud835\udc4e\ud835\udc60\ud835\udc52 \u2192 \ud835\udc54\ud835\udc4e\ud835\udc5a\ud835\udc52\ud835\udc64\u210e\ud835\udc4e\ud835\udc59\ud835\udc52\ud835\udc60 \" where the pre-actions are more abundant, and then propose two auxiliary tasks of predicting the purchase-through rate (PTR) and purchase-through&game whale rate (GWPTR) trained via multi-task learning. As such, our model can leverage extra supervisory signals from auxiliary tasks and all samples over the entire space. To allow for accurate LTV prediction, we put forward two novel LTV experts to model users' monetary values instead of one LTV model to serve all users. Inspired by [41], in each LTV expert, we model the distribution of LTV as the zero-inflated lognormal (Ziln) distribution, which consists of three elements, namely purchase rate, mean, and standard deviation parameters. Correspondingly, our LTV prediction model is optimized by zero-inflated lognormal loss, which can minimize the model's sensitivity of the large values from game whales. Moreover, we take GW detector as the gating network to route users into the right expert. By this way, each expert focuses on a specific type of users thus accommodating different scenarios (i.e., high spenders and low spenders modelling) and maintaining discriminative characteristics. Finally, the purchase rate estimator trained in the LTV component is also used in GW detection, which can capture task-relatedness. As a result, these two components are closely hinged and make their complementary merits. Overall, we summarize our contributions in the following: \u00b7 To the best of our knowledge, we are the first to introduce the idea of investigating the mutually beneficial relationship between the LTV prediction task and the game whale detection task. \u00b7 We propose ExpLTV, a novel multi-task framework that bypasses the shortcomings of conventional LTV prediction methods, allowing the game whale detector as a gating network to allocate the users to be trained via the right pattern of LTV experts assembling. \u00b7 To alleviate the SSB and DS problems in GW detection, we form a new sequential behavior \" \ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc63\ud835\udc52\ud835\udc5f\ud835\udc61 \u2192 \ud835\udc5d\ud835\udc62\ud835\udc5f\ud835\udc50\u210e\ud835\udc4e\ud835\udc60\ud835\udc52 \u2192 \ud835\udc54\ud835\udc4e\ud835\udc5a\ud835\udc52\ud835\udc64\u210e\ud835\udc4e\ud835\udc59\ud835\udc52\ud835\udc60 \", and then transfer GW detection into two auxiliary tasks (i.e., PTR and GWPTR). Furthermore, the PTR estimator shares the same model parameters with the purchase rate estimator in the LTV model, which can capture the task relatedness, and thus boost the corresponding performance. \u00b7 Extensive experiments are conducted on three industrial datasets to evaluate the performance of ExpLTV, and the experimental results show that ExpLTV achieves superior performance in both LTV prediction and GW detection tasks.", "2 PRELIMINARY": "In this section, we first revisit key concepts and then mathematically formulate our research problems. Notably, vectors and matrices are denoted by bold lowercase and bold uppercase letters respectively, and sets are calligraphic uppercase letters. Definition 1 (Customer Lifetime Value): \ud835\udc3f\ud835\udc47\ud835\udc49 \ud835\udc47 \ud835\udc62 is the total worth to a mobile game of a customer over a specific period \ud835\udc47 . In this work, \ud835\udc47 is a constant value and thus we let \ud835\udc3f\ud835\udc47\ud835\udc49 \ud835\udc62 replace \ud835\udc3f\ud835\udc47\ud835\udc49 \ud835\udc47 \ud835\udc62 for simplicity. Definition 2 (Game Whale): In mobile games, the game whales represent a group of users who are responsible for the majority of in-app's revenue. Formally, we use \ud835\udc54 \ud835\udc62 to denote the identity label of user \ud835\udc62 . In our case, if user \ud835\udc62 's \ud835\udc3f\ud835\udc47\ud835\udc49 \u2265 \ud835\udc45 , then user \ud835\udc62 is defined as a game whale with \ud835\udc54 \ud835\udc62 = 1. As the purchase behavior of the game whale should be observed, we let \ud835\udc60 \ud835\udc62 = 1 denote it. Definition 3 (Game Whale and Purchase Probability): For user \ud835\udc62 , let \ud835\udc5d \ud835\udc54\ud835\udc64\ud835\udc5d\ud835\udc61\ud835\udc5f \ud835\udc62 denote user \ud835\udc62 's probability of purchase and being a game whale. Specifically, we design a mapping function which calculates \ud835\udc5d \ud835\udc54\ud835\udc64\ud835\udc5d\ud835\udc61\ud835\udc5f \ud835\udc62 based on the available LTV values: Note that \ud835\udc5d \ud835\udc54\ud835\udc64\ud835\udc5d\ud835\udc61\ud835\udc5f \ud835\udc62 = 0, if and only if \ud835\udc3f\ud835\udc47\ud835\udc49 \ud835\udc62 = 0. Definition 4 (Conditional Game Whale Probability): The aim of GW detection is estimating \ud835\udc5d \ud835\udc54\ud835\udc64 \ud835\udc62 , which denotes the conditional probability of being detected as a game whale, given that \ud835\udc62 's purchase behavior is observed. Given a convert user \ud835\udc62 , the \ud835\udc5d \ud835\udc54\ud835\udc64 \ud835\udc62 can be represented as: Note that \ud835\udc5d \ud835\udc54\ud835\udc64 \ud835\udc62 as the conditional probability which is usually obtained via Bayes' theorem. Task 1 . Game Whale Detection : For each user \ud835\udc62 \u2208 U , we construct a feature vector x \ud835\udc62 \u2208 R \ud835\udc5a where x \ud835\udc62 consists of dense features (e.g., purchase frequency), categorical features (e.g., gender) and sequence features (e.g., purchase behaviours). Given a set of samples D = {( x \ud835\udc62 , \ud835\udc5d \ud835\udc54\ud835\udc64\ud835\udc5d\ud835\udc61\ud835\udc5f \ud835\udc62 ) \u2208 X \u00d7 [ 0 , 1 ] : \ud835\udc62 \u2208 U} , game whale detector is trained to estimate users' probability \u02c6 \ud835\udc5d \ud835\udc54\ud835\udc64 \ud835\udc62 \u2208 [ 0 , 1 ] in terms of the definition. With the sortable probability computed, the GW detection task aims to recommend a set of game whales by selecting \ud835\udc3e top-ranked users w.r.t. \u02c6 \ud835\udc5d \ud835\udc54\ud835\udc64 \ud835\udc62 : It can be formulated as: where \u0398 1 represents the parameters of the GW Detector. Note that Section 3.3 will introduce the reasons why we use \ud835\udc5d \ud835\udc54\ud835\udc64\ud835\udc5d\ud835\udc61\ud835\udc5f \ud835\udc62 as the label in this task rather than \ud835\udc54 \ud835\udc62 . Task 2 . Customer Lifetime Value (LTV) Prediction : Given a set of samples D = {( x \ud835\udc62 , \ud835\udc3f\ud835\udc47\ud835\udc49 \ud835\udc62 ) \u2208 X \u00d7 \ud835\udc41 + \u222a { 0 } : \ud835\udc62 \u2208 U} , we aim to predict \ud835\udc3f\ud835\udc47\ud835\udc49 \ud835\udc62 for user \ud835\udc62 who is a new register or new returning user, which can be represented as: where \u0398 2 denotes the parameters of the LTV predictor.", "3 METHODOLOGY": "", "3.1 Overview of ExpLTV": "Our proposed ExpLTV consists of two key components to perform game whale detection and LTV prediction respectively. In the game whale detection task, we carefully design two auxiliary tasks with sufficient supervisory signals to eliminate the impact of SSB and DS problems. To achieve satisfactory performance of LTV prediction, we adopt [41] as the main building block of each LTV expert model, in which the distribution of LTV is modelled as the zero-inflated lognormal (ZILN) distribution, since it is capable of handling the extreme large LTV labels i.e., game whales' LTV values. Note that these two components are mutually enhanced with each other. Specifically, the probability of a user being classified as game whale and low spender via GW detector is taken as a weight to route each user into the right LTV expert, while the purchase rate estimator learned in both two tasks can learn the task relationships, and thus boost the performance of GW detector. In what follows, we will introduce each component in details.", "3.2 Embedding Layer": "As illustrated in Figure 2, we first utilize a fully connected embedding layer to convert the feature vector x \ud835\udc62 into low-dimensional dense representation denoted as lower-level embeddings: where M \u2208 R \ud835\udc5a \u00d7 \ud835\udc51 is the feature transformation matrix and \ud835\udc51 is the dimension of e \ud835\udc62 . In each forward iteration, to obtain the upperlevel embeddings denoted as e \u2217 \ud835\udc62 , the interaction layer \ud835\udc3c\ud835\udc5b\ud835\udc61\ud835\udc3f\ud835\udc4e\ud835\udc66\ud835\udc52\ud835\udc5f (\u00b7) is designed to preserve the feature interaction information from the lower-level embeddings e \ud835\udc62 . Notably, the embedding of sequence features is transformed via sequential \ud835\udc3c\ud835\udc5b\ud835\udc61\ud835\udc3f\ud835\udc4e\ud835\udc66\ud835\udc52\ud835\udc5f (\u00b7) such as BST [9], while non-sequence features will be learned via general encoding methods (i.e., deepFM [18] or WDL [11]). The upper-level embeddings can be represented as: where \ud835\udc51 1 is the dimension of e \u2217 \ud835\udc62 .", "3.3 Game Whale Detection": "The most straightforward way to detect game whale is to train a supervised classifier that can find the meaningful mapping between the user's upper-level embedding e \u2217 \ud835\udc62 and identity label \ud835\udc54 \ud835\udc62 . However, \u01b8 \u01b8 WDL/DeepFM BST categorical features numerical features sequential features Game Whole Detector LTV Predictor \ud835\udf07 \ud835\udc52 [1] \ud835\udf07 \ud835\udc52 [0] \ud835\udf0e \ud835\udc52 [1] \ud835\udf0e \ud835\udc52 [0] \ud835\udc5d \ud835\udc54\ud835\udc64 \ud835\udc5d \ud835\udc5b\ud835\udc54\ud835\udc64 \ud835\udc5d \ud835\udc5d\ud835\udc61\ud835\udc5f Input Features Embedding Layer Interaction Layer \ud835\udf0e \ud835\udf07 \u0de2 \ud835\udc3f\ud835\udc47\ud835\udc49 = \ud835\udc5d \ud835\udc5d\ud835\udc61\ud835\udc5f \u00d7\ud835\udc52 \ud835\udf07 + \ud835\udf0e 2 2 \ud835\udc5d \ud835\udc54\ud835\udc64\ud835\udc5d\ud835\udc61\ud835\udc5f \ud835\udc5d \ud835\udc5b\ud835\udc54\ud835\udc64\ud835\udc5d\ud835\udc61\ud835\udc5f \ud835\udf07 \ud835\udc52 [1] \ud835\udf0e \ud835\udc52 [1] \ud835\udf07 \ud835\udc52 [0] \ud835\udf0e \ud835\udc52 [0] LTV Experts Game whales' LTV expert Low Spenders' LTV expert \u01b8 \u01b8 such solution that simply divides the users into two subgroups (i.e., general users and game whales) based on the threshold value, is infeasible to correctly reflect the order of users by their monetary value. Though the classifier is well-trained, it is not only unable to distinguish non-computation users and low spenders, but also unable to assist the advertising platforms to spot the most valuable users under the limited advertising budget. Therefore, we consider the GW detection as a regression task trained by ground truth \ud835\udc5d \ud835\udc54\ud835\udc64\ud835\udc5d\ud835\udc61\ud835\udc5f \ud835\udc62 (defined in Section 2). Based on the data analysis of our realworld logs, only about 0 . 7% of users are labeled as game whales, which inherently causes data sparsity problem. Intuitively, we found that the data volume of purchase behavior (i.e., about 11% of total users) that is pre-required behavior of being a game whale is much larger. As such, a practical solution to eliminate the data sparsity problem in GW detection is to bind the purchase behavior modelling as an auxiliary task into the GW detection task. Specifically, given the training dataset D = {( x \ud835\udc62 , \ud835\udc3f\ud835\udc47\ud835\udc49 \ud835\udc62 , \ud835\udc5d \ud835\udc54\ud835\udc64\ud835\udc5d\ud835\udc61\ud835\udc5f \ud835\udc62 ) : \ud835\udc62 \u2208 U} , we first model the purchase probability estimator \ud835\udc53 \ud835\udc5d\ud835\udc61\ud835\udc5f (\u00b7) as a deep neural network (DNN) that inputs a learned upper-level embedding e \u2217 , and computes predicted purchase probability \u02c6 \ud835\udc5d \ud835\udc5d\ud835\udc61\ud835\udc5f , which can be represented as: Then, user \ud835\udc62 's game whale and purchase probability \ud835\udc5d \ud835\udc54\ud835\udc64\ud835\udc5d\ud835\udc61\ud835\udc5f \ud835\udc62 can be computed following the Bayes' theorem: Note that Eq (8) holds due to the fact that purchase behavior must be occurred for the game whales. Furthermore, as users' LTV values often demonstrate skewed distributions with a long tail, low spenders are always account for the majority of all spenders, which illustrates the importance of low spenders in LTV prediction. Thus, a critical innovation in our approach is that our proposed GW detector is able to refine the low spenders from the non-consumption users. Specifically, we propose a novel DNN-based GW detector \ud835\udc53 \ud835\udc54\ud835\udc64\ud835\udc51 (\u00b7) that computes a 2-dimensional probability distribution vector \u02c6 y via the final softmax layer. In \u02c6 y , we let the first element \u02c6 y [ 0 ] represent the conditional probability \u02c6 \ud835\udc5d \ud835\udc54\ud835\udc64 \ud835\udc62 , then \u02c6 y [ 1 ] denoted as \u02c6 \ud835\udc5d \ud835\udc5b\ud835\udc54\ud835\udc64 \ud835\udc62 is the conditional probability of being identified as a low spender, given that \ud835\udc62 's purchase behavior is observed. Based on Bayes' theorem, the user \ud835\udc62 's probability of being general users (i.e., low spenders or non-consumption users) \u02c6 \ud835\udc5d \ud835\udc5b\ud835\udc54\ud835\udc64\ud835\udc5d\ud835\udc61\ud835\udc5f \ud835\udc62 can be formulated as: where \u02c6 \ud835\udc5d \ud835\udc5b\ud835\udc54\ud835\udc64 \ud835\udc62 can be treated as the main indicator for identifying low spenders. With the aforementioned operations, we can obtain that \u02c6 \ud835\udc5d \ud835\udc54\ud835\udc64 \ud835\udc62 ( \u02c6 \ud835\udc5d \ud835\udc5b\ud835\udc54\ud835\udc64 \ud835\udc62 ) is the intermediate variable of computing \u02c6 \ud835\udc5d \ud835\udc54\ud835\udc64\ud835\udc5d\ud835\udc61\ud835\udc5f \ud835\udc62 ( \u02c6 \ud835\udc5d \ud835\udc5b\ud835\udc54\ud835\udc64\ud835\udc5d\ud835\udc61\ud835\udc5f \ud835\udc62 ) that is derived over the entire input space X . To avoid the sample selection bias [28] problem, an intuitive way is to simultaneously \u01b8 \u01b8 model related factor \u02c6 \ud835\udc5d \ud835\udc54\ud835\udc64\ud835\udc5d\ud835\udc61\ud835\udc5f \ud835\udc62 ( \u02c6 \ud835\udc5d \ud835\udc5b\ud835\udc54\ud835\udc64\ud835\udc5d\ud835\udc61\ud835\udc5f \ud835\udc62 ) instead of \u02c6 \ud835\udc5d \ud835\udc54\ud835\udc64 \ud835\udc62 ( \u02c6 \ud835\udc5d \ud835\udc5b\ud835\udc54\ud835\udc64 \ud835\udc62 ) by employing a multi-task learning framework. To achieve this, the GWdetection loss is formulated as: where \u0398 \ud835\udc5d\ud835\udc61\ud835\udc5f is the parameter set of estimator \ud835\udc53 \ud835\udc5d\ud835\udc61\ud835\udc5f (\u00b7) , \ud835\udc59 1 (\u00b7) is crossentropy loss function and y is the concatenation of \ud835\udc5d \ud835\udc54\ud835\udc64\ud835\udc5d\ud835\udc61\ud835\udc5f \ud835\udc62 and \ud835\udc5d \ud835\udc5b\ud835\udc54\ud835\udc64\ud835\udc5d\ud835\udc61\ud835\udc5f \ud835\udc62 . Note that \ud835\udc5d \ud835\udc5b\ud835\udc54\ud835\udc64\ud835\udc5d\ud835\udc61\ud835\udc5f \ud835\udc62 = 1 -\ud835\udc5d \ud835\udc54\ud835\udc64\ud835\udc5d\ud835\udc61\ud835\udc5f \ud835\udc62 based on the definition. \ud835\udc37 \ud835\udc3e\ud835\udc3f is KL-Divergence loss function that works as a strict constraint to narrow down the distance of ground truth distributions and resulted distributions generated by the GW detector.", "3.4 LTV Prediction": "Predicting user-level LTV is challenging but undeniably necessary for most user-centered platforms. Especially in online advertising, an LTV forecasting system built upon solid methodology plays a pivotal role in generating reasonable bidding price for each ad space on streaming media. However, users' LTV values often demonstrate long-tail distributions. Trained by those heavily imbalanced data, a conventional regressor optimized by mse loss cannot avoid to be biased towards the majority labels, while the more important minority labels (i.e., game whales' LTV values) will be underperformed. Motivated by [41], we transform the regression task into the task of predicting three elements, namely purchase probability \ud835\udc5d , mean parameter \ud835\udf07 and standard deviation parameter \ud835\udf0e . Each element estimator \ud835\udc53 \ud835\udc3f\ud835\udc47\ud835\udc49 \ud835\udc56 (\u00b7) is designed with the same deep neural network structure that inputs a learned hidden embedding e \u2217 \ud835\udc62 . where the activation logits units of the last layer of the DNN are sigmoid ( \ud835\udc5d ), identity ( \ud835\udf07 ) and softplus ( \ud835\udf0e ) respectively. Moreover, most of mobile games yield significant revenue from a small percentage of game whales, which exhibit different distribution from general users. The training data with significantly large range can impede a model's capability of learning an accurate mapping from users' feature vectors to LTV values inevitably. These limitations motivate us to propose a novel LTV prediction system that can automatically allocate the right LTV expert to model users' monetary values based on users' type (i.e., high or low spenders). Since the estimator \ud835\udc53 \ud835\udc3f\ud835\udc47\ud835\udc49 \ud835\udf07 (\u00b7) and \ud835\udc53 \ud835\udc3f\ud835\udc47\ud835\udc49 \ud835\udf0e (\u00b7) describe the LTV distribution, each LTV expert that contains those two components is designed by the same model structure. Then, to automatically learn the optimal LTV experts assembling, we innovatively take \u02c6 y \ud835\udc62 , i.e., the indicative probability of being high and low spenders as weight to compute the final distribution parameters (i.e., \ud835\udf07 and \ud835\udf0e ): where \ud835\udf07 \ud835\udc52 and \ud835\udf0e \ud835\udc52 is the aggregation of the expert outputs, namely \ud835\udf07 \ud835\udc52 = \ud835\udf07 1 \u2295 \ud835\udf07 2 and \ud835\udf0e \ud835\udc52 = \ud835\udf0e 1 \u2295 \ud835\udf0e 2 . With the aforementioned operations, we can correspondingly compute d \ud835\udc3f\ud835\udc47\ud835\udc49 \ud835\udc62 as: Finally, we adopt the zero-inflated lognormal (Ziln) loss [41] that is designed to handle the zero and extremely large LTV labels to optimize our LTV prediction task: where the first term is the cross entropy loss used to optimize estimator \ud835\udc53 \ud835\udc3f\ud835\udc47\ud835\udc49 \ud835\udc5d (\u00b7) , and the second term ( known as L \ud835\udc59\ud835\udc5c\ud835\udc54\ud835\udc5b\ud835\udc5c\ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc59 ) is a regression loss to quantify the prediction loss.", "3.5 Model Training": "In this section, we define the loss function of ExpLTV for model training. It is worth mentioning that estimator \ud835\udc53 \ud835\udc3f\ud835\udc47\ud835\udc49 \ud835\udc5d and \ud835\udc53 \ud835\udc5d\ud835\udc61\ud835\udc5f designed for two tasks share the same model parameters in our model. The reason is that shared-model design can not only reduce the computation resource, but also boost performance for both GW detection and LTV prediction tasks by capturing the mutual knowledge from different perspectives. As such, we remove the first cross-entropy loss from Eq (10). As all components of our model are end-to-end differentiable, we combine their losses and use joint learning to optimize the following objective function:", "4 EXPERIMENTS": "In this section, we first outline the evaluation protocols for our model and then conduct experiments on three industry datasets to evaluate the performance of our model. Particularly, we aim to answer the following research questions (RQs) via experiments: \u00b7 RQ1 : Is our model the new state-of-the-art in the LTV prediction task? \u00b7 RQ2 : How does our model perform when detecting game whales compared with baseline methods? \u00b7 RQ3 : Can we verify our contribution via the visualization method? \u00b7 RQ4 : How does our model benefit from each key component? \u00b7 RQ5 : How do the hyper-parameters affect the performance of our model in different tasks?", "4.1 Experimental Datasets": "To validate the performance of our proposed model in two tasks, we conducted experiments on three industrial datasets that are collected from Tencent Mobile Games. The user attributes in each dataset contain numerical features (e.g., age), categorical features (e.g., gender) and sequential behaviours features (e.g., purchase records). Then, we process each dataset by taking users' \ud835\udc47 -day cumulative consumption records as labelled LTV values (i.e., \ud835\udc3f\ud835\udc47\ud835\udc49 \ud835\udc62 ). Note that an accurate model requires access to a large number of users with \ud835\udc3f\ud835\udc47\ud835\udc49 \ud835\udc62 available, i.e., model starts using online service at least \ud835\udc47 days ago. Due to the frequent changes in app updates, market and user base, a time-sensitive (i.e., a smaller \ud835\udc47 day LTV) strategy is the best practice in game advertising. Thus, we set \ud835\udc47 = 7 in our case. As the gathered datasets are time-dependent, we take the first 40-day dataset to train the model, 3-day dataset to validate and 3-day dataset to evaluate the performance. The main statistics of our datasets are shown in Table1.", "4.2 Evaluation Protocols": "To evaluate the effectiveness of our LTV prediction model, we adopt two popular metrics, i.e., AUC and normalized GINI (GINI). Larger values indicate better accuracy. AUC measures the performance of estimator \ud835\udc53 \ud835\udc5d\ud835\udc61\ud835\udc5f (\u00b7) that is designed to classify the consumption users and non-consumption users. Similarly, the GINI is purely based on the ranks of the predictions. In our case, the predicted LTV values are used as an essential factor in ad bidding, thus we follow [41] to quantify the ranking accuracy of our model in terms of GINI. For GW detection, we leverage widely-used ranking metric Recall@K (R@K). Suppose we select top-K users as the most possible game whales based on the predicted probability of being identified as game whale (i.e., \u02c6 \ud835\udc5d \ud835\udc54\ud835\udc64 ), R@K is the fraction of selected game whales (i.e., { \ud835\udc62 | \ud835\udc62 \u2208 \ud835\udc3a\ud835\udc4a\ud835\udc37 (D , \u0398 1 ) , \ud835\udc54 \ud835\udc62 = 1 } ) out of all the game whales (i.e., { \ud835\udc62 | \ud835\udc62 \u2208 U , \ud835\udc54 \ud835\udc62 = 1 } ). Correspondingly, larger R@K represents stronger GW detection effectiveness.", "4.3 Baselines": "We compare our model with the following baselines on two tasks, where only the first two are trained with MSE loss and others are trained with Ziln loss. Notably, for those baselines that are only designed for LTV prediction task, top-K possible game whales are selected based on the predicted LTV values (i.e., d \ud835\udc3f\ud835\udc47\ud835\udc49 ) to evaluate the performance in GW detection. Moreover, since TSUR [43] and Marfnet [44] are specifically designed to have a better study of user representation that is applicable in our framework, they are not selected as comparable methods in this paper. \u00b7 Kuaishou (KS) [26]: This work aims to deal with the complex and imbalanced distribution of LTV values by proposing a novel MDME model. \u00b7 WhalesDetector (WD) [8]: It uses a three-layer CNN to predict the LTV values, and then detect the valuable users based on the results. \u00b7 WDL [11]: It is proposed to model both low- and high-order feature interactions. \u00b7 DeepFM [18]: The deep FM combines the FM [31] and the deep neural network to model pair-wise feature interactions. \u00b7 DCN [40]: A novel cross network is proposed to explicitly model feature interaction. \u00b7 Ziln Loss (ZL) [41]: In this work, a novel zero-inflated lognormal loss is designed to handle the imbalanced regression problem. \u00b7 DNN-based regressor (SimGW): It is an upgrade version of ZL by adding a DNN-based regressor to detect GW users. \u00b7 DNN-based classifier (SimGW2): It is an upgrade version of ZL by combining a DNN-based binary classifier to detect GWusers. 0.65 0.66 0.67 0.68 0.69 0.70 AUC OURS SimGW2 SimGW ZL DCN DeepFM WDL KS WD 0.682 0.671 0.677 0.67 0.673 0.68 0.68 0.83 0.84 0.85 0.86 AUC OURS SimGW2 SimGW ZL DCN DeepFM WDL KS WD 0.844 0.84 0.83 0.842 0.835 0.842 0.838 0.66 0.68 0.70 AUC OURS SimGW2 SimGW ZL DCN DeepFM WDL KS WD 0.682 0.677 0.664 0.652 0.682 0.664 0.675 0.65 0.70 0.75 GINI OURS SimGW2 SimGW ZL DCN DeepFM WDL KS WD 0.739 0.69 0.728 0.68 0.711 0.716 0.728 0.636 0.65 0.2 0.4 0.6 GINI OURS SimGW2 SimGW ZL DCN DeepFM WDL KS WD 0.662 0.645 0.634 0.62 0.638 0.633 0.632 0.196 0.581 0.40 0.45 0.50 0.55 GINI OURS SimGW2 SimGW ZL DCN DeepFM WDL KS WD 0.511 0.507 0.491 0.431 0.452 0.481 0.48 0.389 0.479", "4.4 Parameters Settings": "In our model, we set the latent dimension \ud835\udc51 , learning rate and batch size to 8, 0 . 0001 and 128 respectively. Model parameters are randomly initialized using Gaussian distribution. All estimator \ud835\udc53 \ud835\udc65 (\u00b7) is formulated as a 2-layer deep neural network with 8 hidden dimensions for the hidden layer. For the coefficients in loss function L , we set \ud835\udf06 = 15 on GAME A, and \ud835\udf06 = 10 on both GAME B and GAME C.", "4.5 LTV Prediction Effectiveness (RQ1)": "Customer Lifetime value prediction is an essential part in the success of ads bidding platforms, since the advertisers can use the predicted results to intelligently adjust the bidding price for each ads space. We summarize all models' performance on LTV prediction w.r.t. AUC and GINI with Figure 3. Note that WD and KS methods are optimized by MSE loss reported by original paper and thus the AUC results are not available. Based on the experimental results, we discuss our key findings below. Obviously, our proposed model constantly outperforms all baselines in terms of GINI by a large margin in all three datasets, demonstrating that our model is successful at promising the advertisers to offer the reasonable bidding price for each ad space with the limited marketing budget. Specifically, compared with the best baseline, our model has brought 1 . 5%, 3 . 8% and 6 . 2% relative improvements on GAME A, GAME B and GAME C respectively. Additionally, the compared LTV prediction models exhibit significant performance disparity in terms of the GINI. Models optimized by Ziln loss generally perform better than models optimized by MSE loss on all datasets, indicating the superiority of Ziln loss since it is able to handle the heavy-tailedness nature of LTV values and is insensitive to the extremely large values. Though KS model is proved to deal with the complex and imbalanced distribution of \ud835\udc47 -day LTV values (i.e., DAU) in KUAISHOU when the range of training data is limited to a small value \ud835\udc47 , it is not applicable in our scenario. One possible reason is that a small number of distribution experts can KS' expressiveness in modelling the extreme behaviours of game whales, while a large number may cause overfitting problem and excessive computation resources. Finally, our model, which is the most powerful for user ranking, still achieves competitive AUC results, compared with pure LTV prediction methods. It is proved 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% percentage % 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 the percentage of detected GW WD KS WDL DeepFM DCN ZL SimGW SimGW2 ours 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% percentage % 0.0 0.1 0.2 0.3 0.4 the percentage of detected GW WD KS WDL DeepFM DCN ZL SimGW SimGW2 ours Norma Gamno Whalo Norma that the specific design of shared model structure can enhance the performance of estimator \ud835\udc53 \ud835\udc5d\ud835\udc61\ud835\udc5f (\u00b7) , since it can learn the mutual knowledge from two inner-related tasks.", "4.6 Whale Users Detection (RQ2)": "Game whale is a tiny group of users that brings the most revenue for mobile games. Hence, it is imperative for advertisers to spot this type of users among massive new users for game publishers, especially for top-grossing games. To quantitatively evaluate the effectiveness of ExpLTV in GW detection, we provide comprehensive analysis from different perspectives. We first report the overall detection performance of all tested methods in Table 2. Note that by increasing the searching space (i.e., the value of K), it becomes easier for detectors to retrieve game whales. The first observation we can draw is that our model is successful at detecting the game whales. In particular, when \ud835\udc3e = 5000, the GW detector becomes highly confident in its detected results and over 70% of game whales can be spotted accurately in all datasets. Second, our model outperforms all baseline methods consistently. The improvements of ExpLTV significantly increases, with the reduced search space. On GAME A, the 11 . 4% relative improvement of our model with \ud835\udc3e = 500 demonstrates that ExpLTV can capture the most valuable game whales even under the extremely limited searching budget. Furthermore, the models that achieve better results in the LTV prediction task may not still perform well in GW detection. For example, WLD and DeepFM outperform DCN in terms of GINI, while the R@K values are slightly (b) ExpLTV lower. Without the awareness of game whales in the design of pure LTV models, the well-trained models are limited to perform well only on the majority of labels (i.e., low spenders' LTV values), and thus they are unable to always rank game whales at the top of the non-game whales. To prove it, we further calculate the GINI for all spenders ( \ud835\udc3a\ud835\udc3c\ud835\udc41\ud835\udc3c 1) and high spenders ( \ud835\udc3a\ud835\udc3c\ud835\udc41\ud835\udc3c \ud835\udc45 ). Compared with the best baseline (i.e., DCN on GAME A and GAME C, and DeepFM on GAME B), the improvements achieve 0 . 074 ( \ud835\udc3a\ud835\udc3c\ud835\udc41\ud835\udc3c 1) and 0 . 303 ( \ud835\udc3a\ud835\udc3c\ud835\udc41\ud835\udc3c \ud835\udc45 ) on GAME A, 0 . 04 ( \ud835\udc3a\ud835\udc3c\ud835\udc41\ud835\udc3c 1) and 0 . 082 ( \ud835\udc3a\ud835\udc3c\ud835\udc41\ud835\udc3c \ud835\udc45 ) on GAME B, and 0 . 054 ( \ud835\udc3a\ud835\udc3c\ud835\udc41\ud835\udc3c 1) and 0 . 122 ( \ud835\udc3a\ud835\udc3c\ud835\udc41\ud835\udc3c \ud835\udc45 ) on GAME C. The results further validate that the pure LTV models are impractical in the GWdetection task. Moreover, SimGW that makes use of a regressor to detector game whales outperforms SimGW2 that designs the detector as a simple binary classifier, which verifies the effectiveness of our core idea. Since the main use of GW detector is to spot the most valuable users with the limited search budget, we further test the detected results' quality of all baselines from a fine-grained view. Specifically, we partition labelled game whales into ten levels based on the LTV values, and then calculate the number of detected game whales ranking above each level. Figure 4 reports the results with \ud835\udc3e = 500 and \ud835\udc3e = 1000. Clearly, ExpLTV constantly achieves the best performance, verifying that our model can not only detect more game whales, but also can accurately catch those most valuable game whales who have more significantly impact on games' revenue.", "4.7 Visualization Results (RQ3)": "We are the first to bind GW detection as an auxiliary task to boost the LTV prediction performance especially for game whales, since the extremely large and imbalanced labels of them exhibit dramatically varied distribution from general spenders. To verify the necessity of the GW detection in ExpLTV, we visualize the upper latent embeddings e \u2217 \ud835\udc62 in ExpLTV and pure LTV prediction model i.e., ZL via t-SNE in Figure 5. As can be told from Figure 5, the latent embeddings forms several distinct clusters based on the type of users in ExpLTV. Specifically, two large red game whale clusters, one small and one large blue general user clusters. Though several small clusters can be observed, most of the latent embeddings are mixed together in ZL. Furthermore, only several game whales are forced to fit into the wrong clusters in ExpLTV. Hence, the discriminative embeddings encoded by useful information in our model can prove that the application of GW detection has a significant benefit in the LTV prediction.", "4.8 Ablation Study (RQ4)": "To better understand the performance gain from different major components proposed in our model, we implement several degraded versions of ExpLTV for ablation analysis. Table 3 summarizes the outcomes in two tasks in terms of AUC, GINI and R@K. In what follows, we describe all variants and analyse the effectiveness of corresponding model components. Removing LTV Experts (ExpLTV-ne) . In our model, the predicted LTV values are generated by aggregating the outputs of each LTV expert via Eq.(12). To testify the usefulness of LTV experts, we only retain one LTV expert and the result is treated as the final output of the LTV prediction, then we use joint learning to optimize the final objective function. As can be inferred from Table 3, compared with other methods that are designed with multiple LTV experts (i.e., ExpLTV-nssb and ExpLTVs-sp), ExpLTV-ne has the worst performance in LTV prediction, which validates the importance of making full use of LTV experts to capture the distribution differences between high and low spenders. Additionally, a slight performance drop can be observed in the GW detection task. One possible reason is that the user-specific LTV expert by design can boost the LTV prediction accuracy of game whales, which correspondingly enhances the GW detection effectiveness.", "Removing Sequential behaviours learning (ExpLTV-nssb) .": "This variant disables the sequential behaviour \" \ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc63\ud835\udc52\ud835\udc5f\ud835\udc61 \u2192 \ud835\udc5d\ud835\udc62\ud835\udc5f\ud835\udc50\u210e\ud835\udc4e\ud835\udc60\ud835\udc52 \u2192 \ud835\udc54\ud835\udc4e\ud835\udc5a\ud835\udc52\ud835\udc64\u210e\ud835\udc4e\ud835\udc59\ud835\udc52\ud835\udc60 \" learning by first removing estimator \ud835\udc53 \ud835\udc5d\ud835\udc61\ud835\udc5f (\u00b7) , and then rewriting \u02c6 y = [ \u02c6 \ud835\udc5d \ud835\udc54\ud835\udc64 , \u02c6 \ud835\udc5d \ud835\udc5b\ud835\udc54\ud835\udc64 ] , i.e., \u02c6 \ud835\udc5d \ud835\udc54\ud835\udc64 \ud835\udc62 ( \u02c6 \ud835\udc5d \ud835\udc5b\ud835\udc54\ud835\udc64 ) is the approximation of \u02c6 \ud835\udc5d \ud835\udc54\ud835\udc64\ud835\udc5d\ud835\udc61\ud835\udc5f \ud835\udc62 ( \u02c6 \ud835\udc5d \ud835\udc5b\ud835\udc54\ud835\udc64\ud835\udc5d\ud835\udc61\ud835\udc5f ). As ExpLTV-nssb no longer takes the purchase rate prediction as an auxiliary task into the GW detection to counteract the data sparsity and sample selection bias problem, it suffers from inferior performance in GW detection. For R@500, the significant performance drop reaches 20 . 7% on GAME A, 12 . 6% on GAME B and 12 . 9% on GAME C respectively. In addition, the slight performance drop of GINI validates that the shared purchase rate estimator performing for two tasks by design can contribute to the expressiveness of LTV experts. Thus, the novel Sequential behaviours learning showcases its strong contribution to the performance gain in our model. Using Individual Purchase Probability Estimator for Each Task (ExpLTV-sp) . A crucial difference between ExpLTV and ExpLTV-sp is the design of the purchase probability estimator \ud835\udc53 \ud835\udc5d\ud835\udc61\ud835\udc5f (\u00b7) . Since the LTV prediction and GW detection are two closely related tasks, the shared estimator by design can capture the inner task relationships. As a core part in ExpLTV, we further testify the efficacy of shared estimator \ud835\udc53 \ud835\udc5d\ud835\udc61\ud835\udc5f (\u00b7) . ExpLTV constantly outperforms ExpLTV-sp in both two tasks across all datasets. On GAME A, the performance decrease reaches 1 . 64% in AUC, 7 . 1% in GINI, 16% in R@500 and 2 . 4% in R@1000. The results further validate that the estimator trained from two views can learn the mutual knowledge, and thus it can boost the performance of both two tasks.", "4.9 Hyper-Parameter Sensitivity (RQ5)": "To answer RQ4, we further investigate the performance fluctuations of ExpLTV with two varied hyperparameters on GAME A, namely trade-off \ud835\udf06 between LTV prediction loss and GW detection loss in Eq (15), and latent dimension \ud835\udc51 . Based on the standard setting { \ud835\udc51 = 8 , \ud835\udf06 = 15 } of ExpLTV, we tune the value of one hyperparameter while keeping the other unchanged, and report the new results of two tasks achieved in Figure 6. Specifically, We record the performance differences by plotting AUC and GINI for LTV prediction, while demonstrating R@500 and R@1000 for GW detection. Impact of \ud835\udf06 We study our model's sensitivity to the value of \ud835\udf06 in { 4 , 6 , 8 , 10 , 12 , 15 } that controls the trade-off between LTV prediction and GW detection. Within our expectation, as \ud835\udf06 increases from 4 to 15, there is a slight performance drop in LTV prediction, while an upward trend can be observed in GW detection. Luckily, altering this coefficient has less impact on the LTV prediction. Thus, setting \ud835\udf06 = 15 is sufficient for improving the accuracy of GW detection, while ensuring the satisfactory performance of LTV prediction of ExpLTV. Impact of \ud835\udc51 . We vary dimension \ud835\udc51 in { 4 , 6 , 8 , 10 , 12 } . Generally, the value of dimension \ud835\udc51 directly controls our models' expressiveness. As \ud835\udc51 increases from 4 to 10, a fluctuating growth can be observed in ExpLTV performance. However, when \ud835\udc51 exceeds 10, the performance improvement tends to stop. As can be inferred from Figure 6 (c) and (d), our model with \ud835\udc51 = 8 can achieve the best or second best results in both LTV prediction and GW detection, and thus we set \ud835\udc51 = 8 to achieve a balance between the accuracy of both two tasks. 4 6 8 10 12 15 0.60 0.65 0.70 0.75 0.80 0.85 0.90 AUC GINI 4 6 8 10 12 15 0.0 0.2 0.4 0.6 0.8 R@500 R@1000 4 6 8 10 12 dimension d 0.60 0.65 0.70 0.75 0.80 0.85 0.90 AUC GINI 4 6 8 10 12 dimension d 0.0 0.2 0.4 0.6 0.8 R@500 R@1000 (a) LTV Prediction (b) GW Detection (c) LTV Prediction (d) GW Detection", "5 RELATED WORK": "LTV Prediction. Understanding the total revenue that the business can expect from a customer is important in user acquisition [24, 30, 39, 44]. In the literature, many LTV prediction models have been proposed, which can be categorised as probability-based, machine learning- based and deep learning-based methods. The probability-based methods assume the purchase behaviors as a probability distribution, and then propose the probabilistic generative models for predicting the LTV values [15, 16, 19, 32]. [15] proposes a stochastic model that links the RFM paradigm [29] with LTV values. The machine learning-based methods are proposed to learn a mapping between hand-crafted features and monetary value of game players via machine learning techniques [7, 12, 19, 37]. For example, [12] adopts a random forest model to predict user-level LTV value in Groupon. Recent years have witnessed the successful development of deep learning-based techniques in LTV prediction. [8] designs a convolutional neural network in LTV prediction to perform a better modelling of temporal representations. ZL [41] models the distribution of LTV values as Ziln distribution to capture the long-tail nature of training data. Meanwhile, the Ziln loss can be used in deep learning-based neural networks. TSUR [43] is designed to learn a more stable user representation by utilizing wavelet transform and Graph Attention Network, which can alleviate the volatility and sparsity problems of monetary values. In [44], a feature missing-aware routing-and-fusion network (MarfNet) is proposed to reduce the effect of the missing features while training. Recently, [26] is developed to predict users' DAU in KUAISHOU. In [26], the LTV distribution is divided into multiple sub-distributions trained via distribution experts. Inherented the limitations of mse loss, [26] can be proved to success when the range of monetary values is limited to a small value, and thus our scenario impedes its model expressiveness. Most of existing research efforts mainly focus on improving the performance of LTV prediction by enhancing the feature representation, while the potential of integrating GW detection and LTV prediction into a unified framework to make most of their beneficial relationship is always ignored. These limitations motivate us to propose ExpLTV that is able to achieve superior performance in both LTV prediction and GW detection. Multi-Task Learning. Multi-Task learning (MTL) is a widely used training paradigm in machine learning [34, 47-49]. It aims to capture the inner relationships among multiple tasks to improve the performance of each task. The supervised MTL can be classified into five main categories: feature learning-based [6, 27], low rankbased [1, 46], task clustering-based [3, 36], task relation learningbased [14, 23] and decomposition-based [17, 22] approaches. In recent years, many sort of works are successful in solving sample selection bias (SSB) and data sparsity (DS) problems by utilization of MTL in conversation rate prediction (CVR). Specifically, SSB is a bias caused by the varied distribution of training space and inference space. [28] is the first to propose an entire space multitask model (ESMM) to eliminate SSB and DS problems in CVR task. In ESMM, instead of directly optimizing CVR task, two auxiliary tasks of predicting the post-view click-through rate (CTR) and postview click-chrough& conversion rate (CTCVR) are introduced. Inspired by ESMM, ESM 2 [42] decomposes \" \ud835\udc56\ud835\udc5a\ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b \u2192 \ud835\udc5d\ud835\udc62\ud835\udc5f\ud835\udc50\u210e\ud835\udc4e\ud835\udc60\ud835\udc52 \" into several intermediate behaviours as\" \ud835\udc56\ud835\udc5a\ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b \u2192 \ud835\udc50\ud835\udc59\ud835\udc56\ud835\udc50\ud835\udc58 \u2192 \ud835\udc37 ( \ud835\udc42 ) \ud835\udc34\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b \u2192 \ud835\udc5d\ud835\udc62\ud835\udc5f\ud835\udc50\u210e\ud835\udc4e\ud835\udc60\ud835\udc52 \". Based on the novel sequential behavior decomposition, CVR prediction is optimized by modelling multiple auxiliary tasks instead. Similarly, the SSB and DS problems exist in game whale detection. In our work, we form a new sequential behaviour \" \ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc63\ud835\udc52\ud835\udc5f\ud835\udc61 \u2192 \ud835\udc5d\ud835\udc62\ud835\udc5f\ud835\udc50\u210e\ud835\udc4e\ud835\udc60\ud835\udc52 \u2192 \ud835\udc54\ud835\udc4e\ud835\udc5a\ud835\udc52\ud835\udc64\u210e\ud835\udc4e\ud835\udc59\ud835\udc52\ud835\udc60 \", and then trains two decomposition tasks i.e., purchase rate prediction and game whale & purchase rate prediction via multi-task learning. Consequently, investigating by the entire samples and abundant auxiliary supervisory signals, ExpLTV can efficiently address the SSB and DS issues.", "6 CONCLUSION": "In this paper, we propose a novel multi-task framework named ExpLTV to perform LTV prediction and game whale detection. By investigating the beneficial relationship of LTV prediction and game whale detection, these two tasks can exert mutually. In game whale detection, the carefully designed DNN-based detector is expected to precisely refine game whales and low spenders, which can be treated as a gating network to decide the optimized patterns of LTV experts assembling. Meanwhile, the purchase rate estimator trained by the LTV predictor is used in the GW detector as an auxiliary task to eliminate SSB and DS problems. The extensive experiments conducted on three industrial datasets confirm the effectiveness of ExpLTV over the state-of-the-art baselines on both LTV prediction and GW detection tasks.", "REFERENCES": "[1] Rie Kubota Ando, Tong Zhang, and Peter Bartlett. 2005. A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learning Research 6, 11 (2005). [2] Muhammad Anshari, Mohammad Nabil Almunawar, Syamimi Ariff Lim, and Abdullah Al-Mudimigh. 2019. Customer relationship management and big data enabled: Personalization & customization of services. Applied Computing and Informatics 15, 2 (2019), 94-101. [3] BJ Bakker and TM Heskes. 2003. Task clustering and gating for bayesian multitask learning. (2003). [4] Paul D Berger and Nada I Nasr. 1998. Customer lifetime value: Marketing models and applications. Journal of interactive marketing 12, 1 (1998), 17-30. [5] Brian C Britt and Rebecca K Britt. 2021. From waifus to whales: The evolution of discourse in a mobile game-based competitive community of practice. Mobile Media & Communication 9, 1 (2021), 3-29. [6] Rich Caruana. 1998. Multitask learning . Springer. [7] Benjamin Paul Chamberlain, Angelo Cardoso, CH Bryan Liu, Roberto Pagliari, and Marc Peter Deisenroth. 2017. Customer lifetime value prediction using embeddings. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining . 1753-1762. [8] Pei Pei Chen, Anna Guitart, Ana Fern\u00e1ndez del R\u00edo, and Africa Peri\u00e1nez. 2018. Customer lifetime value in video games using deep learning and parametric models. In 2018 IEEE international conference on big data (big data) . IEEE, 21342140. [9] Qiwei Chen, Huan Zhao, Wei Li, Pipei Huang, and Wenwu Ou. 2019. Behavior sequence transformer for e-commerce recommendation in alibaba. In Proceedings of the 1st International Workshop on Deep Learning Practice for High-Dimensional Sparse Data . 1-4. [10] Tong Chen, Hongzhi Yin, Jing Long, Quoc Viet Hung Nguyen, Yang Wang, and MengWang.2022. Thinking inside The Box: Learning Hypercube Representations for Group Recommendation. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1664-1673. [11] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems . 7-10. [12] Anders Drachen, Mari Pastor, Aron Liu, Dylan Jack Fontaine, Yuan Chang, Julian Runge, Rafet Sifa, and Diego Klabjan. 2018. To be or not to be... social: Incorporating simple social features in mobile game customer lifetime value predictions. In Proceedings of the Australasian Computer Science Week Multiconference . 1-10. [13] Michael Dreier, Klaus W\u00f6lfling, Eva Duven, Sebasti\u00e1n Giralt, Manfred E Beutel, and Kai W M\u00fcller. 2017. Free-to-play: About addicted Whales, at risk Dolphins and healthy Minnows. Monetarization design and Internet Gaming Disorder. Addictive behaviors 64 (2017), 328-333. [14] Theodoros Evgeniou, Charles A Micchelli, Massimiliano Pontil, and John ShaweTaylor. 2005. Learning multiple tasks with kernel methods. Journal of machine learning research 6, 4 (2005). [15] Peter S Fader, Bruce GS Hardie, and Ka Lok Lee. 2005. RFM and CLV: Using iso-value curves for customer base analysis. Journal of marketing research 42, 4 (2005), 415-430. [16] Peter S Fader, Bruce GS Hardie, and Ka Lok Lee. 2005. 'Counting your customers' the easy way: An alternative to the Pareto/NBD model. Marketing science 24, 2 (2005), 275-284. [17] Pinghua Gong, Jieping Ye, and Changshui Zhang. 2012. Robust multi-task feature learning. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining . 895-903. [18] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. arXiv preprint arXiv:1703.04247 (2017). [19] Sunil Gupta, Dominique Hanssens, Bruce Hardie, Wiliam Kahn, V Kumar, Nathaniel Lin, Nalini Ravishanker, and S Sriram. 2006. Modeling customer lifetime value. Journal of service research 9, 2 (2006), 139-155. [20] Janny C Hoekstra and Eelko KRE Huizingh. 1999. The lifetime value concept in customer-based marketing. Journal of Market-Focused Management 3 (1999), 257-274. [21] Dipak Jain and Siddhartha S Singh. 2002. Customer lifetime value research in marketing: A review and future directions. Journal of interactive marketing 16, 2 (2002), 34-46. [22] Ali Jalali, Sujay Sanghavi, Chao Ruan, and Pradeep Ravikumar. 2010. A dirty model for multi-task learning. Advances in neural information processing systems 23 (2010). [23] Tsuyoshi Kato, Hisashi Kashima, Masashi Sugiyama, and Kiyoshi Asai. 2007. Multi-task learning via conic programming. Advances in Neural Information Processing Systems 20 (2007). [24] V Kumar, Girish Ramani, and Timothy Bohling. 2004. Customer lifetime value approaches and best practice applications. Journal of interactive Marketing 18, 3 (2004), 60-72. [25] Vineet Kumar and Werner Reinartz. 2018. Customer relationship management . Springer. [26] Kunpeng Li, Guangcui Shao, Naijun Yang, Xiao Fang, and Yang Song. 2022. Billionuser Customer Lifetime Value Prediction: An Industrial-scale Solution from Kuaishou. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management . 3243-3251. [27] Xuejun Liao and Lawrence Carin. 2005. Radial basis function network for multitask learning. Advances in Neural Information Processing Systems 18 (2005). [28] Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu, and Kun Gai. 2018. Entire space multi-task model: An effective approach for estimating post-click conversion rate. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval . 1137-1140. [29] John A McCarty and Manoj Hastak. 2007. Segmentation approaches in datamining: A comparison of RFM, CHAID, and logistic regression. Journal of business research 60, 6 (2007), 656-662. [30] Phillip E Pfeifer, Mark E Haskins, and Robert M Conroy. 2005. Customer lifetime value, customer profitability, and the treatment of acquisition spending. Journal of managerial issues (2005), 11-25. [31] Steffen Rendle. 2010. Factorization machines. In 2010 IEEE International conference on data mining . IEEE, 995-1000. [32] David C Schmittlein, Donald G Morrison, and Richard Colombo. 1987. Counting your customers: Who-are they and what will they do next? Management science 33, 1 (1987), 1-24. [33] Jos\u00e9-Ram\u00f3n Segarra-Moliner and Miguel-\u00c1ngel Moliner-Tena. 2022. Engaging in customer citizenship behaviours to predict customer lifetime value. Journal of Marketing Analytics (2022), 1-14. [34] Ozan Sener and Vladlen Koltun. 2018. Multi-task learning as multi-objective optimization. Advances in neural information processing systems 31 (2018). [35] Yanhui Su. 2022. Data-driven method development and evaluation for indie mobile game publishing. Multimedia Tools and Applications (2022), 1-32. [36] Sebastian Thrun and Joseph O'Sullivan. 1996. Discovering structure in multiple learning tasks: The TC algorithm. In ICML , Vol. 96. Citeseer, 489-497. [37] Ali Vanderveld, Addhyan Pandey, Angela Han, and Rajesh Parekh. 2016. An engagement-based customer lifetime value system for e-commerce. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining . 293-302. [38] Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint arXiv:1710.10903 (2017). [39] Rajkumar Venkatesan and Vita Kumar. 2004. A customer lifetime value framework for customer selection and resource allocation strategy. Journal of marketing 68, 4 (2004), 106-125. [40] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network for ad click predictions. In Proceedings of the ADKDD'17 . 1-7. [41] Xiaojing Wang, Tianqi Liu, and Jingang Miao. 2019. A deep probabilistic model for customer lifetime value prediction. arXiv preprint arXiv:1912.07753 (2019). [42] Hong Wen, Jing Zhang, Yuan Wang, Fuyu Lv, Wentian Bao, Quan Lin, and Keping Yang. 2020. Entire space multi-task modeling via post-click behavior decomposition for conversion rate prediction. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval . 2377-2386. [43] Mingzhe Xing, Shuqing Bian, Wayne Xin Zhao, Zhen Xiao, Xinji Luo, Cunxiang Yin, Jing Cai, and Yancheng He. 2021. Learning Reliable User Representations from Volatile and Sparse Data to Accurately Predict Customer Lifetime Value. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining . 3806-3816. [44] Xuejiao Yang, Binfeng Jia, Shuangyang Wang, and Shijie Zhang. 2023. Feature Missing-aware Routing-and-Fusion Network for Customer Lifetime Value Prediction in Advertising. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining . 1030-1038. [45] Bianca Zadrozny. 2004. Learning and evaluating classifiers under sample selection bias. In Proceedings of the twenty-first international conference on Machine learning . 114. [46] Jian Zhang, Zoubin Ghahramani, and Yiming Yang. 2005. Learning multiple related tasks using latent independent component analysis. Advances in neural information processing systems 18 (2005). [47] Shijie Zhang, Hongzhi Yin, Tong Chen, Zi Huang, Quoc Viet Hung Nguyen, and Lizhen Cui. 2022. Pipattack: Poisoning federated recommender systems for manipulating item promotion. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining . 1415-1423. [48] Shijie Zhang, Hongzhi Yin, Tong Chen, Quoc Viet Nguyen Hung, Zi Huang, and Lizhen Cui. 2020. Gcn-based user representation learning for unifying robust recommendation and fraudster detection. In Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval . 689698. [49] Yu Zhang and Qiang Yang. 2021. A survey on multi-task learning. IEEE Transactions on Knowledge and Data Engineering 34, 12 (2021), 5586-5609."}
