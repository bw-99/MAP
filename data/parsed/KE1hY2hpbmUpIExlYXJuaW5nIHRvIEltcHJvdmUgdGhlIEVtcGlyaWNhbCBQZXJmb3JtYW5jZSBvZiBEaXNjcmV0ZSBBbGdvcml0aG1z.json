{"title": "(Machine) Learning to Improve the Empirical Performance of Discrete Algorithms", "authors": "Imran Adham; Jes\u00fas A De Loera; Zhenyang Zhang", "pub_date": "2021-09-29", "abstract": "This paper discusses a data-driven, empirically-based framework to make algorithmic decisions or recommendations without expert knowledge. We improve the performance of two algorithmic case studies: the selection of a pivot rule for the Simplex method and the selection of an all-pair shortest paths algorithm. We train machine learning methods to select the optimal algorithm for given data without human expert opinion. We use two types of techniques, neural networks and boosted decision trees. We concluded, based on our experiments, that: 1) Our selection framework recommends various pivot rules that improve overall total performance over just using a fixed default pivot rule. Over many years experts identified steepestedge pivot rule as a favorite pivot rule. Our data analysis corroborates that the number of iterations by steepest-edge is no more than 4 percent more than the optimal selection which corroborates human expert knowledge, but this time the knowledge was obtained using machine learning. Here our recommendation system is best when using gradient boosted trees. 2) For the all-pairs shortest path problem, the models trained made a large improvement and our selection is on average .07 percent away from the optimal choice. The conclusions do not seem to be affected by the machine learning method we used. We tried to make a parallel analysis of both algorithmic problems, but it is clear that there are intrinsic differences. For example, in the all-pairs shortest path problem the graph density is a reasonable predictor, but there is no analogous single parameter for decisions in the Simplex method.", "sections": [{"heading": "Introduction", "text": "What is the best way to select an algorithm? Two different algorithms for the same computational task have difference performances: one algorithm is better on some inputs, but worse on the others. Over the years there have been various theoretical frameworks answering this question. Worst-case analysis aims to find the extreme instances that strain the performance the most. Average-case analysis on the other hand assumes that input instances come from a fixed probability distribution, thus we can talk about average running time or average complexity. More recently, the Smooth analysis is a hybrid of the worst-case and average-case analysis of algorithms where one measures the maximum over inputs of the expected performance of an algorithm under small random perturbations of that input. The performance of many algorithms varies dramatically on the types of input one provides, thus the theoretical evaluations often say nothing useful for the non-expert user. How is a non-expert user supposed to make the right algorithmic choices when a large number of choices are possible? How can someone make reasonable consistent choices of parameters for tuning complicated algorithms? Here we propose an ML-based methodology to make those choices in the absence of an expert or to corroborate what an expert can suggest.\nConsider for example the very famous Simplex method of G. Dantzig [17]. This is a well-studied algorithm, researchers have found the worst-case behavior of Simplex algorithm is exponential, for most known deterministic pivot rules [2,4,26,28,32,37,46] and randomized pivot rules [30,34]. On the other hand, under a specific probability distribution for input instances, the average running time of Simplex algorithm is polynomial in terms of the input size [14]. Similarly, the smooth analysis shows that the Simplex method is efficient [16]. Despite the theoretical success, neither of the three theoretical evaluations matches the empirical performance of the Simplex method, which is known to be very fast in practice. Today the Simplex method has been investigated and improved enormously from its original version [12]. It is known that the running time or number of iterations for the Simplex method depends not just on the input data, but how we tune the algorithm itself. E.g., what choice of pivot rule shall we make? This is a question that has been answered by experts by fixing a default pivot rule, which often performs well, but may not be always the optimal choice (this choice is often steepest edge pivot rule). As a proof of concept we demonstrate how machine learning can recover the hard-won wisdom of experts.\nThe purpose of our paper is to discuss a pragmatic framework for empirical algorithm selection tuning and comparison. In the present article we demonstrate a machine learning-based selection and tuning of algorithms. Our framework is data-driven, empirically-based, and can help non-experts make reasonable consistent algorithmic decisions without prior knowledge of the algorithms. Users of algorithmic methods often have no knowledge of the worst-case examples, nor can they assume to know the exact distribution of their data. Users only have access to data sets. The simple principle we propose here is that, if one has sufficiently many data instances, one can create a practical machine learning recommendation system to efficiently automate the selection of algorithms or their parameter configurations for concrete data sets, with the intention to speed up computation. We picked two case studies to illustrate the framework, but it would apply almost in the same way to other algorithms where the input is based on matrices.\nAlgorithm selection has seen a strong surge in both practical and theoretical research and we only touch the fraction of the literature that we know deals with algorithm similar to our case studies (for much more we recommend [6,29,40,42,53] and the many references therein). Several authors have been directly concerned with algorithm selection and tuning for discrete algorithmic problems (see e.g., [3,5,36] and the many references therein). The papers [9,52] are great surveys of uses of learning in combinatorial optimization. In [10] the authors redefine mixed integer convex optimization problems as a multi-class classification problem where the machine learning predictor gives insights on the optimal solution. Dai et al. [35] develop a method to learn heuristics over graph problems. Several authors have proposed ways to use machine learning to select the best branching rules (see [1,36]). Machine learning methods have also been useful in aiding the selection of reformulations and decompositions for mixed-integer optimization [13,39]. Some libraries organize data for various NP-hard tasks (where the aim is to predict how long an algorithm will take to solve concrete instances of NP-complete problems, or to choose best approximation schemes tailored by instances) [11,38,48]. In fact the approach we present here is a simplification of the empirical hardness model to predict the running time of algorithms applied to improve logic satisfiability (SAT) solvers [20,41]. There are also now a number of well-established software implementations for algorithm tuning (see [21,23] and the many references therein).", "publication_ref": ["b16", "b1", "b3", "b25", "b27", "b31", "b36", "b45", "b29", "b33", "b13", "b15", "b11", "b5", "b28", "b39", "b41", "b52", "b2", "b4", "b35", "b8", "b51", "b9", "b34", "b0", "b35", "b12", "b38", "b10", "b37", "b47", "b19", "b40", "b20", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Our contributions", "text": "In this work we present two case studies of ML-algorithm selection, where we see the behavior is clearly dependent on the right choice of algorithm or algorithm version:\nFirst, the Simplex method. It is widely used in solving linear programming (LP) problems. Geometrically, Simplex algorithm starts on a vertex of the feasible region (which is a polytope), and generates a path via improving edges until optimum is reached. A pivot rule helps to decide which improving edge to pick if there are multiple choices. In this case, we are interested in applying different machine learning models to study and improve the choice among five pivoting rules for the Simplex algorithms on linear programming based on features of different LP instances.\nSecond, we do algorithm selection on the problem of computing the shortest paths between all pairs of vertices on a graph. The All-Pairs Shortest Path (APSP) algorithms that we consider for the comparison are All-Pairs Dijkstra [18]; Floyd-Warshall [24], which iteratively improves the lengths of shortest paths using dynamic programming; and finally an algorithm proposed by Peng et al. [50], which is a dynamic programming improvement of All-Pairs Dijkstra to skip extraneous computations.\nWe demonstrate that the total performance of algorithms, when guided by Machine Learning (ML) decision-making, is clearly faster than using a single static choice for these algorithms. We implemented two ML methodologies, boosted decision trees and neural networks. We tested two different schemes of predicting the fastest algorithm: direct classification and run time prediction. In direct classification a machine learning method is trained to predict which algorithm will run the fastest. In the run time prediction setting, a machine learning method is trained to estimate how long an algorithm will run on a particular instance, then we pick the algorithm that is expected to run the quickest. In addition, we tested different data representations and features. We discuss the details in each of the two situations. Next we present the details and in the end we discuss conclusions.\n2 Case study 1: The Simplex Method", "publication_ref": ["b17", "b23", "b49"], "figure_ref": [], "table_ref": []}, {"heading": "Algorithms", "text": "We begin with some standard definitions related to linear programming and the Simplex algorithm. This introduction is meant to be brief, and we refer to textbooks (see [17,51]) for more extensive background knowledge. For the remainder of this section we let\nA \u2208 R m\u00d7n , b \u2208 R m , c \u2208 R n be given.\nDefinition 1. A linear program in standard form is the optimization problem of maximizing c T x subject to Ax = b and x \u2265 0. Definition 2. We say that B \u2286 [n] with |B| = m is a basis if and only if the columns of A B are linearly independent, or equivalently A B is non-singular. We say x B a basic feasible solution with basis B if Ax B = b, x B \u2265 0 and for all j \u2208 B: x j = 0. Definition 3. The vector of reduced costs for a basis B is defined as\nz B = c -AA -1 B c B .\nWe say j \u2208 [n] is an improving pivot with respect to B if and only if z B j > 0.\nWith the definition of an improving pivot, the Simplex method can be summarized as a process of starting with a feasible basis B and updating with improving pivots until no such improving pivots exist. Now we present three basic pivot rules that our experiments consider:\n1. Dantzig: this rule was suggested by Dantzig [17]. In every iteration Dantzig's rule picks the non-basic variable with the largest positive reduced cost to be the entering variable. 2. Greatest Improvement: this rule picks the improving pivot that results in the largest increment of the objective function. 3. Steepest edge: this rule performs the improving pivot with the largest rate of increment of objective function per distance traveled along the improving edge.\nIn the following example we briefly explain how different pivot rules choose different pivots using tableaux.\nVariables\nz w 1 w 2 w 3 \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 z x 1 x 2 x 3 w 1 w 2 w 3 b 1 -5 -4 -3 0 0 0 0 0 2 3 1 1 0 0 5 0 4 1 2 0 1 0 11 0 3 4 2 0 0 1 8 \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb\nWe can see that x 1 , x 2 , x 3 have negative coefficients and are the potential entering variables. For Dantzig's rule, we pick x 1 . For greatest improvement, we pick x 2 since the increment of objective function by each variable is x 1 : 12.5, x 2 : 20 3 , x 3 : 12. And for steepest edge, we pick x 3 since the rate of each variable is\nx 1 : 5 \u221a 30 , x 2 : 4 \u221a 27 , x 3 : 3 \u221a 10 .\nWe study the pivoting strategies for primal Simplex algorithm implemented in DOcplex [49]. These include Dantzig's rule, hybrid (DOcplex's default), greatest improvement, steepest edge and devex. Hybrid is a pivot rule DOcplex implemented as default, which uses Dantzig's rule in the earlier iterations when there are a lot of choices of improving pivots and switch to steepest edge later. Devex is an approximate version of steepest edge developed by P. Harris [31]. DOcplex also implemented a steepest edge with slack initial norms, which is slightly cheaper in computation. But in our testing, it usually is not better than steepest edge. As a consequence it was not included in the algorithm portfolio.", "publication_ref": ["b16", "b50", "b16", "b48", "b30"], "figure_ref": [], "table_ref": []}, {"heading": "Data Generation", "text": "The existing libraries (MIPLIB 2017 [45], NETLIB [19] etc.) of linear programming or integer programming are too small for our training purpose. Hence we generated our own data for training and testing. We adapted the algorithms introduced by Bowly et al [15]. Their method involves generating constraint matrix A, and a solution pair (\u03b1, \u03b2). They used A, \u03b1, \u03b2 to generate the final linear problem maximizing c T x subject to Ax \u2264 b. For simplicity, we replaced the generation of variable constraint graph by generating Erd\u0151s-R\u00e9nyi (ER) random graphs.\nFor training and validation set, we generated 24634 instances of linear programming problems with number of constraints ranging from 120 to 200 and number of variables ranging from 50 to 100. For testing, we generate 7279 more instances. Note that these linear programs will most likely be characterized as \"easy\" problems by MIPLIB 2017. For the ER random graphs, the parameter p was drawn from U{0.2, 0.8}. For other hyperparameters in generating the LP instances, we draw the coefficient mean \u00b5 A from normal distribution N (0, 1), coefficient standard deviation \u03c3 A from uniform distribution U{1, 10}, primal versus slack basis \u03b3 from U{0.2, 0.8}, fractional primal \u03bb from N (0, 1) and Beta fraction a = 0.5.\nAfter generating the LP instances, we solve our LP problems using primal Simplex solver in DOcplex with default initialization. We store the number of iterations for each instance using different pivot rules. Note that the LP instances we generate may have degeneracy, and empirically there is a high likelihood of degeneracy where the constraint matrix is low-density.", "publication_ref": ["b44", "b18", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Feature selection", "text": "We have two different ways of choosing features for the linear programming instances. The first method we use is a bag-of-features, where we add features based on heuristics from previous studies on the Simplex method. Apart from m, n the number of constraints and the number of variable, we add three sets of features: variable constraint graph features, coefficient values, and normalized coefficients. Variable constraint graph features include the minimum, maximum, mean, and standard deviation of the degree sequences of variable nodes and constraint nodes. Coefficient values include the statistics of the coefficient matrix A, the constraint vector b, and the objective function c (i.e. he minimum, maximum, mean, standard deviation, norm of the vector, and the smallest non-zero absolute value). Finally, normalized coefficients are the statistics of row and column normalized coefficients ({ The other way we have implemented features related to the coefficient matrix A, is the Truncated Singular Value Decomposition (SVD), which is a method of dimension reduction [43]. The truncated SVD of a matrix A \u2208 R m\u00d7n returns three matrices U, \u03a3, V such that:\nA \u2248 U \u03a3V\nwhere U \u2208 R m\u00d7k , \u03a3 \u2208 R k\u00d7k , and V \u2208 R k\u00d7n , where k is the number of top singular values to keep. Multiplying U by \u03a3 allows for the computation of an m \u00d7 k matrix. Applying this procedure again to (U \u03a3) T will then compute a k \u00d7 k matrix with similar features to the original matrix A. We choose k = 20 in this experiment for the best performance. We still include the features of statistics of the constraint vector b and objective function c.", "publication_ref": ["b42"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We train four models to choose which pivoting strategies will perform the best on each LP instance. Two models use the bag of features that we choose for LP problems, and the other two use the SVD to replace the features of the coefficient matrix A.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Boosted Trees", "text": "We train two boosted trees to predict the best pivot rule for each LP instance. The first one is an empirical hardness model, that is, for all five pivot rules, we use regression on the features we selected to predict number of iterations that the solver will take using certain pivot rule. The second model is a boosted tree classifier using truncated SVD as features.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Bag-of-features boosted trees", "text": "The first model is an empirical hardness model, where we use gradient boosted trees to do regression and predict the number of iterations each pivot rule would cost. Table 1 shows the hyperparameters for different pivot rules. This model results in a 67.78% accuracy on the test set with 178.5934 iterations on average.  Figure 1 shows the gain of features for boosted tree regressors. We can see that apart from number of constraints and number of variables, some of the common features that are important are: maximum number of constraint degree, max and mean of variable degree, min and mean of coefficient matrix A, min, mean, norm and standard deviation of objective function c etc. One could take the subset of important features to train smaller models, which makes the training much faster, but the accuracy will drop to 66.44% with 178.7804 iterations on average.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": ["tab_0"]}, {"heading": "Boosted tree classifier", "text": "The other boosted tree uses the truncated SVD with k = 20 as part of the features while keeping the features of constraint vector and objective function. This random forest contains 102 trees with minimum child weight of 5, maximum depth of 5, learning rate of 0.1, subsample and column subsample by tree ratio of 0.8. This model results in a 67.15% accuracy on the test set with 179.0714 iterations on average. The feature importance is shown in Figure 2. As we can see, number of variables and constraints (the first and second feature), as well as features related to constraint vector and objective function are of great importance. Meanwhile, the diagonal entries of the SVD matrix have a relatively high importance.\nNeural Networks We train two models to classify which pivoting strategies will perform the best on each LP instance. The first model uses the bag of features that we choose for LP problems, and the second model uses the truncated SVD matrix to replace the features of the coefficient matrix A.\nBag-of-features Neural Network We first train a neural network using features of LP instances we pre-selected. The architecture of the network consists of four hidden layers of ReLU activation function with 64 neurons. Each hidden layer has a dropout of 0.1. The output layer contains five neurons with the softmax activation function. We train the neural network to minimize the categorical cross-entropy loss with the RMSProp optimizer with a learning rate of 0.01 and momentum of 0.2. We train with a batch size of 64 for 50 epochs. This model results in a 62.2% accuracy on the test set with 179.279 iterations on average. Figure 6 (in Appendix) plots the accuracy and loss during each epoch.\nTruncated SVD Neural Network We then train a neural network using truncated SVD matrices as features for coefficient matrix A while keeping the features of constraint vector and objective function. The architecture consists of four layers of 512 hidden units with ReLU activation function. The output layer contains 5 neurons with the softmax activation function. We train the neural network to minimize the categorical cross-entropy loss with the ADAM optimizer with a learning rate of 0.001. We train with a batch size of 64 for 100 epochs. This model results in a 72.78% accuracy on the test set with 179.18 iterations on average. Figure 7 (in Appendix) plots the accuracy and loss during each epoch.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Comparison of models", "text": "Here we summarize the performance of our models. Table 2 shows the average number of iterations (from the most to the fewest) if we use certain pivot rule or follow our models to solve the LP instances in the test set. It also demonstrates the prediction accuracy of our models. Table 3 shows the instance-wise comparison between our model recommendations with the most popular steepest edge pivot rule. We can see that the best performance of our four models is 69.06% of the number of iterations steepest edge will take. And they vary on the worst case behavior, with our gradient boosted tree regressor being the most consistent: their worst case will only cost 174.19% of what steepest edge will perform. We run the Wilcoxon Signed Rank test on each of the test instances between our models and steepest edge pivoting strategy. We can see in Table 3 that except for SVD-20 NN model, the other three models have significant improvement compared to using steepest edge pivoting strategy on all test instances.  3. Comparison between our models and steepest edge pivot rule on test set per instance.\n3 Case study 2: All-Pairs Shortest Path Problem", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Algorithms", "text": "The All-Pairs Shortest Path (APSP) algorithms that we consider for the portfolio are:\n1. All-Pairs Dijkstra's Algorithm [18]: This algorithm simply applies the standard Dijkstra's algorithm for every possible starting node to calculate every possible pair of shortest paths. 2. Floyd-Warshall [24]: This algorithm is a dynamic program that stores all the lengths of shortest paths between every pair of nodes. It begins by initializing the path lengths to the weight of the path connecting every node, or \u221e if no edge exists. Then it loops through all possible pairs of nodes n times to incrementally update the shortest path between them by checking all intermediary nodes. 3. Peng [50]: Applies All-Pairs Dijkstra's using dynamic programming to store solutions in order to reduce redundant calculations. In addition, the nodes in the graph are sorted in decreasing order by their degrees in order to maximize the number of calculations that are skipped. The standard Dijkstra's algorithm may be faster if the overhead cost of sorting the nodes is too high on a particular graph.\nTwo other commonly used APSP algorithms were also considered initially: All-Pairs Bellman-Ford [8,25] and Johnson's algorithm [33]. These algorithms have the benefit of being applicable to graphs with negative edge weights, making them more versatile. However, in our testing we noted that these algorithms were always the slowest for all test cases, so these algorithms were not included in the algorithm portfolio.", "publication_ref": ["b17", "b23", "b49", "b7", "b24", "b32"], "figure_ref": [], "table_ref": []}, {"heading": "Data Generation", "text": "Graphs for the training and test sets were generated randomly using four methods: Erd\u0151s-R\u00e9nyi (ER) random graphs [22], Barab\u00e1si-Albert (BA) random graphs [7], Watts-Strogatz (WS) small world random graphs [47], and finally geometric random graphs. Graphs were generated with nodes between 20 and 1250. After a graph is generated, every edge within it is given a random integer weight between 1 and 100. The parameters for the generation of the graphs were chosen in order to produce a variety in the densities of the graphs while also trying to ensure the graphs are connected. The training set consists of 2309 graphs and the test set contains 1125 graphs with nodes between 20 and 500.\nFor ER graphs, the parameter p was set to be a random number between ln n n and 1. p = ln n n is a transition where the graph will likely be connected. For BA graphs, the parameter m is an integer chosen uniformly between 5 and n -1. In the WS model, the mean degree K was picked randomly between ln n and n -1, and the parameter \u03b2 was chosen randomly between 0 and 1.\nThe geometric graphs were constructed by first generating n points within the unit cube. If two points are within a distance of of each other, an edge is added between them. The value of was chosen to be between 20  n and 1. After generating each graph, we run all three algorithms on them and record how long each algorithm takes to run, along with the algorithm that runs the fastest. In the classification setting, the label for each sample is the algorithm that performed best, and in the regression setting the label is the runtime for the respective algorithm. 10% of the training data is set aside for validation. Assuming 100% accuracy in predicting the fastest algorithm, the total runtime of the best algorithm on each graph of the test set is 4356 seconds.\nIn addition, we test the neural network on a real-world Facebook social network, provided by Stanford [44]. It contains 4039 nodes and 88,234 edges, and has a topology that is not very well represented by the training set alone.\nFigure 3 displays each graph from the test set plotted by its density vs its number of nodes, labeled by the algorithm that runs fastest on it. The figure shows that Dijkstra only runs fastest on graphs with a low number of nodes and low density, Floyd-Warshall tends to run fastest on graphs with high density, and Peng's algorithm is fastest on most lower density graphs. ", "publication_ref": ["b21", "b6", "b46", "b43"], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Feature Selection", "text": "We represent graphs for training the models in two ways. The first method we use is the Truncated Singular Value Decomposition (SVD) of the adjacency matrix as described earlier in the pivot rule selection. The second representation is a sampling of the degree sequence of the graph.\nIn the Truncated SVD representation, we use k = 20 as the parameter. We lose information on the number of nodes and edges of the graph, so we add 1/n and the density as features. It has been shown that the density of a graph is an important feature to determine when a shortest path algorithm is faster [27].\nGiven a degree sequence of a graph, and some parameter q, we wish to reduce the degree sequence down to q elements. We take elements with indices length of degree sequence q i, for i = 0, 1, . . . , q -1. This representation does not maintain the size of the graph and the values are not normalized. So we add 1/n as a feature and divide every element of the sequence by n. Peng's algorithm is optimized for graphs with few high-degree nodes and many low-degree nodes, so this representation could capture the information necessary to distinguish when an algorithm will be faster.", "publication_ref": ["b26"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "Boosted Trees We train two different boosted trees to predict the fastest APSP algorithm on a given graph. The first model uses the truncated SVD of the adjacency matrix as its features. The second model uses a sample of the degree sequence of the graph. Hyperparameters were tuned via grid search on the parameters of the representation, the maximum depth of the trees, minimum child weight, learning rate, and subsample rate.\nTruncated SVD Boosted Tree We begin by training a boosted tree random forest with truncated SVD parameter k = 5. The forest consists of 64 boosted trees, with maximum depth 6, minimum child weight of 1, and learning rate 0.1. This random forest results in an accuracy of 93.6% and the total time taken on all the graphs using its predictions is 4359 seconds. Figure 4 plots the gain for each feature. We see that the most important feature is the graph density (second feature), with the diagonal elements of the matrix having relatively high importance. Degree Sequence Boosted Tree Another boosted tree is trained using the sampled degree sequence representation. This random forest contains of 32 trees with a depth of 8, minimum child weight of 1, and learning rate 0.1. The parameter q is set to 50. This forest achieves 93.4% accuracy with a total time taken of 4359 seconds. The total gain for the features is shown in Figure 5. The number of nodes (first feature) has high importance, as well as the degrees of the nodes about 3/4 the way through the degree sequence. Fig. 5. The gain for the degree sequence boosted tree with q = 50. First feature is the number of nodes, the remaining are the elements of the reduced degree sequence.\nNeural Networks In total we train three models to classify which algorithm will perform the best on each instance. The first model is based on a collection of neural networks that predicts the running time of each algorithm. The second uses the truncated SVD as its representation, and the final model uses the degree sequence for its representation.", "publication_ref": [], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "Runtime Prediction Model", "text": "The first model is based on runtime prediction. For each of the three algorithms, a neural network is trained to predict its runtime on a given instance, then the algorithm with the fastest predicted runtime is chosen as the label for the classification. The networks have four hidden layers, the first layer uses the ELU activation function, and the rest use the ReLU activation function. The first layer consists of 512 neurons, and the other three have 256 neurons each. Dropout with p = 0.25 is added to all the layers. The networks are trained to minimize the mean squared error. The classification accuracy for these networks is 75.4%. The total runtime of all the graphs in the test set using the algorithms predicted by this classifier is 4594 seconds. Figure 8 (in Appendix) plots the accuracy and loss during each epoch.\nTruncated SVD Neural Network The next model trained is a neural network to classify which algorithm will run fastest on each graph using the truncated SVD representation with k = 20. The architecture of the network consists of five hidden dense layers with 128 neurons each. Dropout with p = 0.5 is added to each layer to prevent overfitting. The first hidden layer has the ELU activation function, and the other four use the sigmoid activation. The output layer contains three neurons with the softmax activation function. We train the neural network to minimize the categorical cross-entropy loss with the ADAM optimizer with a learning rate of 0.001. We train with a batch size of 64 for 300 epochs. Figure 9 (in Appendix) plots the loss and accuracy of the model at every epoch. This model achieves an accuracy of 93.7% and using its predictions on the test set causes a total running time of 4360 seconds on the test set.\nDegree Sequence Neural Network The final model we trained is one that uses the sampled degree sequence with q = 50 as the representation. The architecture for this neural network is two hidden layers with 128 neurons in the first layer, and 64 neurons in the second layer. The first hidden layer uses the exponential linear activation function as before, and the other one uses the sigmoid activation function. Every layer has dropout added with p = 0.5. The neural network is trained to minimize the categorical cross-entropy loss using the ADAM optimizer with a learning rate of 0.001. The network is trained for 300 epochs with a batch size of 64. The accuracy and loss for this model is plotted in Figure 10 (attached in Appendix). The network has a test accuracy of 93.3% and the total time taken on the test set is 4364 seconds.", "publication_ref": [], "figure_ref": ["fig_5", "fig_1"], "table_ref": []}, {"heading": "Comparison of Models", "text": "To compare models and determine which one performs best, we run the Wilcoxon Signed-Rank test on each of the 30 instances of each model. First, comparing the SVD NN model to the Degree Sequence NN model, we find that the degree sequence neural network has a higher accuracy of 93.0% compared to the SVD's accuracy of 92.1%. With a p value of p = 0.002, we conclude that the results are not due to chance. The SVD model takes 4365s to run all test cases while the degree sequence models takes 4366s. Running the Wilcoxon test returns p = 0.2, and so we determine that they perform similarly even though the degree sequence model has a higher accuracy. We now compare the SVD NN model to the SVD Boosted Tree model. The tree model has an accuracy of 93.1%. The Wilcoxon test returns p = 0.00002, so we can conclude that there is a significant difference in the accuracies of these models.\nA Real-World Graph We tested the SVD classification neural network on the Facebook social network [44] to verify if the output is correct and to test the generalization of the neural network. When the algorithms are applied to the graph, Dijkstra's algorithm ran in 532s, Peng's algorithm took 40s, and Floyd-Warshall ran in 12,670s. So Peng's algorithm was considerably faster on this graph than the other algorithms. Inputting this graph into the neural network, the outputted probability vector is (0.0000164, 0.999, 0.0000148). The first coordinate represents the probability that Dijkstra's algorithm is fastest, the second coordinate corresponds to Peng's algorithm, and the third coordinate is for Floyd-Warshall. So the neural network is very confident that Peng will run the fastest on the graph, which is supported by the actual runtime of only 40s.\nTable 6 summarizes the results of all the classifiers. We note that all the ML models largely improves on the performance over just using one algorithm, and they perform similarly. Table 7 summarizes the largest time saved from correct classifications and largest time lost from incorrect classifications for each model by instance. We can see that every model was able to correctly classify the test instance that had the largest impact on overall time saved. Although the neural networks had the highest test accuracy, they were slightly slower compared to the boosted trees due to the fact that they misclassified the more important test cases; boosted trees were able to correctly classify the more important test cases and had overall better performance.", "publication_ref": ["b43"], "figure_ref": [], "table_ref": ["tab_4", "tab_5"]}, {"heading": "Conclusion", "text": "In this paper, we show one can rely on ML-methods to predict the performance of different algorithms in different input instances. We can then make recommendations and decide the best algorithm to use in a particular situation.\nFor the different pivoting strategies for the Simplex algorithm, we find that gradient boosting decision trees work the best in predicting the correct number of iterations. Our ML-method corroborates what human experts have recovered from their experience that most frequently the steepest-edge pivot rule is a great choice. Tuning hyperparameters helps to improve the performance of the models, but it is feature engineering that actually improves the models by a huge amount. Throughout the process we learn that certain features, such as variable constraint graph degrees, and coefficients in A and c, are more important than other features that people empirically believe (row and column normalized features). convenient way to encode the matrix A into features. This improves the prediction accuracy of our models, but might not necessarily enhance the performance in number of iterations. All of our four models are able to outperform the popular steepest edge pivoting rule by a small edge, and their performance on the test set are pretty close. Although there is a gap between our model and the theoretical optimum, our experiments show that machine learning can help to improve the choice of pivoting strategy. With a proper way to encode linear programs of different dimensions, we might be able to improve the performance of the Simplex algorithm further.\nFor the problem of computing all-pairs shortest paths we found applying ML techniques to perform algorithm selection vastly improves on the overall performance over selecting an individual algorithm. We found that the method for classification did not greatly affect the performance, neural networks and boosted trees both had very similar performance. We discovered that the density of a graph and the degrees of the nodes are the most important features in selecting algorithms for APSP. Based on Figure 3 it seems as if it might be possible to select Peng's algorithm for graphs with density less than 0.5, and Floyd-Warshall otherwise. But if we use this rule as a classifier, we get a test accuracy of only 78.6% and a total time taken on the test set of 4508 seconds. This heuristic largely under-performs our ML models, showing that ML can be used to discover deeper, useful patterns in the data to improve results.\nWe have presented a very simple machine learning data-driven approach for empirical algorithm selection or parameter tuning that is widely applicable. Given data and a collection of algorithms or parameters from which to choose, our empirical algorithm selection and tuning approach can be utilized to obtain automatic recommendations by almost anyone. We must of course discuss the benefits and shortcomings of our empirical algorithm selection.\nOur approach does not formally prove our selection is optimal for all input instances, but instead only with respect to available data. Although our methods apply to any algorithmic problem with matrices as input we lack a theoretical recipe to choose the features used for training, and will likely change depending on the algorithm. We cannot answer questions such as \"Is there a canonical best ML method?\" or \"What is the optimal neural network architecture (number of layers, activation functions, etc) for a particular algorithm selection problem?\". But this is not a limitation of our approach, rather it is a drawback of the entire theory of machine learning. On the other hand, there are multiple advantages to using our approach. Foremost, it is very basic and simple but improves computation. A user does not require expert-level knowledge of algorithms to make reasonable decisions. Our approach is a pragmatic way to justify algorithmic choices based on available data, and we also provide some consistency and rigor for evaluating algorithms' performance. Moreover, human experts tend to narrow algorithmic choices to one popular default setup which leads to a one-size-fits-all situation. Our approach allows variability in the choice of algorithm or parameters depending on the concrete instance and, most importantly, results in a clear improvement of running time or computational cost.  ", "publication_ref": [], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Appendix", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "A machine learning-based approximation of strong branching", "journal": "INFORMS J. Comput", "year": "2017", "authors": "A M Alvarez; Q Louveaux; L Wehenkel"}, {"ref_id": "b1", "title": "Deformed products and maximal shadows of polytopes", "journal": "", "year": "1999", "authors": "N Amenta; G Ziegler"}, {"ref_id": "b2", "title": "Learning to learn by gradient descent by gradient descent", "journal": "", "year": "2016", "authors": "M Andrychowicz; M Denil; S Gomez; M W Hoffman; D Pfau; T Schaul; B Shillingford; N De Freitas"}, {"ref_id": "b3", "title": "Notes on Bland's pivoting rule", "journal": "Springer", "year": "2009", "authors": "D Avis; V "}, {"ref_id": "b4", "title": "Learning to branch", "journal": "", "year": "2018", "authors": "M Balcan; T Dick; T Sandholm; E Vitercik"}, {"ref_id": "b5", "title": "Learning-theoretic foundations of algorithm configuration for combinatorial partitioning problems", "journal": "", "year": "2017", "authors": "M Balcan; V Nagarajan; E Vitercik; C White"}, {"ref_id": "b6", "title": "Emergence of scaling in random networks", "journal": "Science", "year": "1999", "authors": "A Barab\u00e1si; R Albert"}, {"ref_id": "b7", "title": "On a routing problem", "journal": "Quart. Appl. Math", "year": "1958", "authors": "R E Bellman"}, {"ref_id": "b8", "title": "Machine learning for combinatorial optimization: a methodological tour d'horizon", "journal": "", "year": "2018", "authors": "Y Bengio; A Lodi; A Prouvost"}, {"ref_id": "b9", "title": "The voice of optimization", "journal": "", "year": "2020-07", "authors": "D Bertsimas; B Stellato"}, {"ref_id": "b10", "title": "ASlib: A benchmark library for algorithm selection", "journal": "Artif. Intell", "year": "2016", "authors": "B Bischl; P Kerschke; L Kotthoff; M T Lindauer; Y Malitsky; A Fr\u00e9chette; H H Hoos; F Hutter; K Leyton-Brown; K Tierney; J Vanschoren"}, {"ref_id": "b11", "title": "Solving real-world linear programs: A decade and more of progress", "journal": "Operations Research", "year": "2001", "authors": "R Bixby"}, {"ref_id": "b12", "title": "Learning a classification of mixed-integer quadratic programming problems", "journal": "Springer", "year": "2018-06-26", "authors": "P Bonami; A Lodi; G Zarpellon"}, {"ref_id": "b13", "title": "The average number of pivot steps required by the simplex-method is polynomial", "journal": "Zeitschrift for Operations Research", "year": "1982", "authors": "K H Borgwardt"}, {"ref_id": "b14", "title": "Generation techniques for linear programming instances with controllable properties", "journal": "Mathematical Programming Computation", "year": "2020", "authors": "S Bowly; K Smith-Miles; D Baatar; H Mittelmann"}, {"ref_id": "b15", "title": "A friendly smoothed analysis of the simplex method", "journal": "", "year": "2018", "authors": "D Dadush; S Huiberts"}, {"ref_id": "b16", "title": "Linear Programming and Extensions", "journal": "Princeton University Press", "year": "1998", "authors": "G B Dantzig"}, {"ref_id": "b17", "title": "A note on two problems in connexion with graphs", "journal": "Numerische Mathematik", "year": "1959", "authors": "E W Dijkstra"}, {"ref_id": "b18", "title": "Netlib", "journal": "", "year": "1997", "authors": "J Dongarra"}, {"ref_id": "b19", "title": "Neural networks for predicting algorithm runtime distributions", "journal": "", "year": "2018", "authors": "K Eggensperger; M Lindauer; F Hutter"}, {"ref_id": "b20", "title": "Pitfalls and best practices in algorithm configuration", "journal": "J. Artif. Intell. Res", "year": "2019", "authors": "K Eggensperger; M Lindauer; F Hutter"}, {"ref_id": "b21", "title": "On the evolution of random graphs", "journal": "Publication of the Mathematical Institute of the Hungarian Academy of Sciences", "year": "1960", "authors": "P Erd\u00f6s; A R\u00e9nyi"}, {"ref_id": "b22", "title": "Auto-sklearn: Efficient and robust automated machine learning", "journal": "Springer", "year": "2019", "authors": "M Feurer; A Klein; K Eggensperger; J T Springenberg; M Blum; F Hutter"}, {"ref_id": "b23", "title": "Algorithm 97: Shortest path", "journal": "Commun. ACM", "year": "1962-06", "authors": "R W Floyd"}, {"ref_id": "b24", "title": "Flows in Networks", "journal": "Princeton University Press", "year": "1962", "authors": "L R Ford; D R Fulkerson"}, {"ref_id": "b25", "title": "A subexponential lower bound for zadeh's pivoting rule for solving linear programs and games", "journal": "Springer", "year": "2011", "authors": "O Friedmann"}, {"ref_id": "b26", "title": "Shortest path algorithms", "journal": "Annals of Operations Research", "year": "1988-12", "authors": "G Gallo; S Pallotino"}, {"ref_id": "b27", "title": "Worst case behavior of the steepest edge simplex method", "journal": "Discrete Applied Mathematics", "year": "1979", "authors": "D Goldfarb; W Y Sit"}, {"ref_id": "b28", "title": "A PAC approach to application-specific algorithm selection", "journal": "SIAM J. Comput", "year": "2017", "authors": "R Gupta; T Roughgarden"}, {"ref_id": "b29", "title": "Two new bounds for the random-edge simplex-algorithm", "journal": "SIAM Journal on Discrete Mathematics", "year": "2007-01", "authors": "B G\u00e4rtner; V Kaibel"}, {"ref_id": "b30", "title": "Pivot selection methods of the devex lp code", "journal": "Mathematical Programming", "year": "1973", "authors": "P M J Harris"}, {"ref_id": "b31", "title": "The simplex algorithm with the pivot rule of maximizing criterion improvement", "journal": "Discrete Mathematics", "year": "1973", "authors": "R G Jeroslow"}, {"ref_id": "b32", "title": "Efficient algorithms for shortest paths in sparse networks", "journal": "J. Assoc. Comput. Mach", "year": "1977", "authors": "D B Johnson"}, {"ref_id": "b33", "title": "Linear programming, the simplex algorithm and simple polytopes", "journal": "Mathematical Programming", "year": "1997", "authors": "G Kalai"}, {"ref_id": "b34", "title": "Learning combinatorial optimization algorithms over graphs", "journal": "", "year": "2017-12-09", "authors": "E B Khalil; H Dai; Y Zhang; B Dilkina; L Song"}, {"ref_id": "b35", "title": "Learning to run heuristics in tree search", "journal": "", "year": "2017", "authors": "E B Khalil; B Dilkina; G L Nemhauser; S Ahmed; Y Shao"}, {"ref_id": "b36", "title": "How good is the simplex algorithm", "journal": "", "year": "1972", "authors": "V Klee; G J Minty"}, {"ref_id": "b37", "title": "The ICON challenge on algorithm selection", "journal": "AI Magazine", "year": "2017", "authors": "L Kotthoff; B Hurley; B O'sullivan"}, {"ref_id": "b38", "title": "Learning when to use a decomposition", "journal": "Springer", "year": "2017-06-05", "authors": "M Kruber; M E L\u00fcbbecke; A Parmentier"}, {"ref_id": "b39", "title": "Algorithm selection using reinforcement learning", "journal": "", "year": "2000", "authors": "M G Lagoudakis; M L Littman"}, {"ref_id": "b40", "title": "Understanding the empirical hardness of NPcomplete problems", "journal": "Commun. ACM", "year": "2014", "authors": "K Leyton-Brown; H H Hoos; F Hutter; L Xu"}, {"ref_id": "b41", "title": "Hyperband: A novel bandit-based approach to hyperparameter optimization", "journal": "Journal of Machine Learning Research", "year": "2018", "authors": "L Li; K Jamieson; G Desalvo; A Rostamizadeh; A Talwalkar"}, {"ref_id": "b42", "title": "Introduction to Information Retrieval", "journal": "Cambridge University Press", "year": "2008", "authors": "C Manning; P Raghavan; H Sch\u00fctze"}, {"ref_id": "b43", "title": "Learning to discover social circles in ego networks", "journal": "", "year": "2012", "authors": "J Mcauley; Jure Leskovic"}, {"ref_id": "b44", "title": "", "journal": "MIPLIB", "year": "2017", "authors": ""}, {"ref_id": "b45", "title": "Computational complexity of parametric linear programming", "journal": "Mathematical Programming", "year": "1980", "authors": "K G Murty"}, {"ref_id": "b46", "title": "Random graph models of social networks", "journal": "Proceedings of the National Academy of Sciences", "year": "2002", "authors": "M E J Newman; D J Watts; S H Strogatz"}, {"ref_id": "b47", "title": "Understanding random SAT: Beyond the clauses-to-variables ratio", "journal": "", "year": "2004", "authors": "E Nudelman; A Devkar; Y Shoham; K Leyton-Brown"}, {"ref_id": "b48", "title": "The IBM Decision Optimization on Cloud team", "journal": "Docplex", "year": "", "authors": ""}, {"ref_id": "b49", "title": "A fast algorithm to find all-pairs shortest paths in complex networks", "journal": "", "year": "2012", "authors": "W Peng; X Hu; F Zhao; J Su"}, {"ref_id": "b50", "title": "Theory of Linear and Integer Programming", "journal": "Wiley Series in Discrete Mathematics & Optimization", "year": "1998", "authors": "A Schrijver"}, {"ref_id": "b51", "title": "Neural networks for combinatorial optimization: A review of more than a decade of research", "journal": "INFORMS Journal on Computing", "year": "1999", "authors": "K A Smith"}, {"ref_id": "b52", "title": "Oboe: Collaborative filtering for automl model selection", "journal": "", "year": "2019-07", "authors": "C Yang; Y Akimoto; D W Kim; M Udell"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Aij bj |b j = 0} and { Aij cj |c j = 0}) and degree normalized coefficients ({ bj deg(uj ) } and { ci deg(vi) }).", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Fig. 1 .1Fig. 1. The gain of features for boosted tree regressors.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Fig. 2 .2Fig. 2. The gain of features for boosted tree classifier.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Fig. 3 .3Fig. 3. Graphs from the test set plotted as density vs number of nodes and labeled with the fastest algorithm", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Fig. 4 .4Fig. 4. The gain for the SVD boosted tree with k = 5. First two features are number of nodes followed by graph density. The remaining are elements of truncated SVD matrix.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Fig. 8 .8Fig. 8. The accuracy and loss of the runtime prediction models at each epoch", "figure_data": ""}, {"figure_label": "910", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Fig. 9 .Fig. 10 .910Fig. 9. The accuracy and loss of the SVD classification model plotted against the number of epochs during training", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Hyperparameters for each regressor.", "figure_data": "Hyperparameters Dantzig Hybrid Devex Steepest Greatestlearning rate0.10.10.10.10.05# estimators271137173173371max depth56466min child weight66541\u03b300000.3subsample ratio10.80.80.90.8column subsample1110.80.9regularization \u03b1100101e-51001e-5"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Summary of average number of iterations and accuracy of each model.", "figure_data": "ClassifierAverage iterations on test set AccuracyGreatest Improvement326.1419-Dantzig319.7501-Devex262.2335-Hybrid217.2856-Steepest edge179.4161-Bag-of-features NN179.27962.2%SVD-20 NN179.1872.78%XGBClassifier179.071470.15%XGBRegressor178.593467.78%Best in theory173.1783100%ClassifierBest Worst Wilcoxon test p-valueBag-of-features NN 69.06% 258.91%5.37085 \u00d7 10 -13SVD-20 NN69.06% 210.25%0.91014XGBClassifier69.06% 318.06%6.12158 \u00d7 10 -17XGBRegressor69.06% 174.19%1.4 \u00d7 10 -31Table"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Summary of p-values for accuracy from Wilcoxon-Signed Rank Test.", "figure_data": "-SVD NN Deg. Seq. NN Runtime NN SVD Tree Deg. Seq. TreeSVD NN-0.002-0.00001970.000008Deg. Seq. NN 0.002--0.5720.0571Runtime NN-6053s--SVD tree 0.00001970.57293.1%-0.005Deg. seq. tree 0.0000080.057193.3%0.005--SVD NN Deg. Seq. NN Runtime NN SVD Tree Deg. Seq. TreeSVD NN-0.206-0.00000173 0.00000522Deg. Seq. NN0.206--0.00000192 0.00000388Runtime NN-6053s--SVD tree 0.00000173 0.0000019293.1%-0.329Deg. seq. tree 0.00000522 0.0000038893.3%0.329-"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Summary of p-values for total time from Wilcoxon-Signed Rank Test.", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Truncated Singular Value Decomposition gives us a Summary of total time and accuracy of each model", "figure_data": "ClassifierTotal time on test set AccuracyDijkstra9165s-ClassifierLargest improvement Largest deficitPeng Floyd-Warshall SVD tree Deg. seq. tree Runtime NN SVD NN Deg. seq. NN Density heuristic6658s 6053s 4359s 4359s 4594s 4365s 4366s 4508s--93.1% 93.3% 75.4% 92.1% 93.0% 78.6%SVD tree Deg. seq. tree Runtime NN SVD NN Deg. seq. NN Density heuristic Best in theory51.6s 51.6s 51.6s 51.6s 51.6s 51.6s 51.6s0.76s 0.79s 13.7s 1.8s 1.04s 12s 0sBest in theory4356s100%"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Largest improvement and largest deficit in time for each model by instance", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "A \u2208 R m\u00d7n , b \u2208 R m , c \u2208 R n be given.", "formula_coordinates": [3.0, 73.44, 549.83, 428.4, 22.27]}, {"formula_id": "formula_1", "formula_text": "z B = c -AA -1 B c B .", "formula_coordinates": [4.0, 281.74, 118.0, 85.24, 13.3]}, {"formula_id": "formula_2", "formula_text": "z w 1 w 2 w 3 \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 z x 1 x 2 x 3 w 1 w 2 w 3 b 1 -5 -4 -3 0 0 0 0 0 2 3 1 1 0 0 5 0 4 1 2 0 1 0 11 0 3 4 2 0 0 1 8 \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb", "formula_coordinates": [4.0, 255.91, 307.67, 152.45, 59.04]}, {"formula_id": "formula_3", "formula_text": "x 1 : 5 \u221a 30 , x 2 : 4 \u221a 27 , x 3 : 3 \u221a 10 .", "formula_coordinates": [4.0, 211.84, 409.07, 117.65, 14.6]}, {"formula_id": "formula_4", "formula_text": "A \u2248 U \u03a3V", "formula_coordinates": [5.0, 264.54, 435.66, 43.99, 8.77]}], "doi": ""}
