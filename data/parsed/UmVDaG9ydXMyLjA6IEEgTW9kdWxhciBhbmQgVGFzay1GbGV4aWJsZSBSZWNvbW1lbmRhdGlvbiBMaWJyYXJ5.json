{"ReChorus2.0: A Modular and Task-Flexible Recommendation Library": "Jiayu Li*, Hanyu Li* {jy-li20,hanyuli23}@mails.tsinghua.edu.cn DCST, Tsinghua University Quan Cheng Laboratory Beijing, China Peijie Sun sun.hfut@gmail.com DCST, Tsinghua University Beijing, China", "Zhiyu He": "", "Weizhi Ma\u2020": "hezy22@mails.tsinghua.edu.cn DCST, Tsinghua University Beijing, China", "Min Zhang\u2020": "z-m@tsinghua.edu.cn DCST, Tsinghua University Quan Cheng Laboratory Beijing, China", "ABSTRACT": "mawz@tsinghua.edu.cn AIR, Tsinghua University Beijing, China Shaoping Ma msp@tsinghua.edu.cn DCST, Tsinghua University Beijing, China", "CCS CONCEPTS": "With the applications of recommendation systems rapidly expanding, an increasing number of studies have focused on every aspect of recommender systems with different data inputs, models, and task settings. Therefore, a flexible library is needed to help researchers implement the experimental strategies they require. Existing open libraries for recommendation scenarios have enabled reproducing various recommendation methods and provided standard implementations. However, these libraries often impose certain restrictions on data and seldom support the same model to perform different tasks and input formats, limiting users from customized explorations. To fill the gap, we propose ReChorus2.0 , a modular and task-flexible library for recommendation researchers. Based on ReChorus, we upgrade the supported input formats, models, and training&evaluation strategies to help realize more recommendation tasks with more data types. The main contributions of ReChorus2.0 include: (1) Realization of complex and practical tasks, including reranking and CTR prediction tasks; (2) Inclusion of various context-aware and rerank recommenders; (3) Extension of existing and new models to support different tasks with the same models; (4) Support of highly-customized input with impression logs, negative items, or click labels, as well as user, item, and situation contexts. To summarize, ReChorus2.0 serves as a comprehensive and flexible library better aligning with the practical problems in the recommendation scenario and catering to more diverse research needs. The implementation and detailed tutorials of ReChorus2.0 can be found at https://github.com/THUwangcy/ReChorus. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. RecSys'24, October 14-18, 2024, Bari, Italy https://doi.org/XXXXXXX.XXXXXXX \u00b7 Information systems \u2192 Recommender systems ; \u00b7 Software and its engineering \u2192 Software libraries and repositories .", "KEYWORDS": "Recommendation library, Reproducibility, Reranking, CTR", "ACMReference Format:": "Jiayu Li*, Hanyu Li*, Zhiyu He, Weizhi Ma\u2020, Peijie Sun, Min Zhang\u2020, and Shaoping Ma. 2024. ReChorus2.0: A Modular and Task-Flexible Recommendation Library. In Proceedings of RecSys (RecSys'24). ACM, New York, NY, USA, 10 pages. https://doi.org/XXXXXXX.XXXXXXX", "1 INTRODUCTION": "Recommendation systems have gained increasing interest from the academic community, with research addressing a wide range of issues throughout the recommendation system workflow. These studies, from theoretical analyses to practical applications, have significantly contributed to the development in this domain. As research advances, various recommendation tasks, experimental settings, and optimization targets emerge. Recommendation tasks, starting from rating predictions [24] and top-k recommendations [42, 48], have extended to recent interests in CTR/CVR prediction [41, 60, 65] and re-ranking tasks [38, 39]. Meanwhile, various contexts are considered in recommendation modeling, including users' historical interactions [18, 23], user and item profiles [9, 15], and the interaction-varying situational context [28, 35]. Different requirements of candidate set construction also emerged accordingly, such as negative sampling for Top-k recommendation [42], labeled data for CTR prediction, and impression-based logs for reranking [28, 38]. Implementing these diverse configurations requires significant time cost. A flexible standard framework that can support flexible settings can free researchers from these engineering details and allow them to focus on theoretical and methodological research, while also effectively avoiding implementation bugs. Recently, there have been a number of excellent libraries to facilitate standard implementations of numerous recommendation algorithms and tasks. These libraries significantly contribute to the reproducibility of the recommendation research community. For instance, BARS [71], including BART-CTR and BARS-Batch, targets open benchmarks for both CTR prediction and candidate item matching tasks on a variety of public datasets. RecBole [68] provides a unified and comprehensive recommendation framework supporting 43 datasets and 91 algorithms. There are also other famous libraries, such as the venerable LensKit [12] and Microsoft Recommenders [3], Cornac for multimodal recommendations [44], and LibRerank for reranking methods [33]. However, as shown in Table 1, they mostly impose constraints on input data and candidate set construction, and supported tasks are also limited. This lack of flexibility in altering model and data settings makes it less convenient for researchers to conduct experiments, especially exploratory studies, during research progress. To tackle these challenges, we provide ReChorus2.0 , a modular and task-flexible library for research in recommendation . The updated library is built on ReChorus1.0 [57], a swift and efficient framework focusing on sequential recommendations, one of the frameworks recommended by ACM RecSys2024 1 . Leveraging and enhancing the modular nature of ReChorus, we extend it to encompass more complex recommendation tasks in practice, including CTR prediction tasks with various context metadata and ranking/reranking with customized candidate sets for training and evaluations (i.e., users can customize positive and negative samples with variable lengths in each candidate set such as impression logs). By enabling the free assembly of modules akin to building blocks, ReChorus2.0 allows one recommender to support diverse tasks and input formats, helping users implement desired tasks. To be specific, in this upgrade to ReChorus2.0, several major modifications are implemented: (1) The realization of more complex tasks closely aligned with practice, such as reranking and CTR prediction tasks; (2) To accomplish the extended tasks, more flexible training and evaluation strategies are implemented, including training and evaluations on highly customized candidate sets with variable length and multiple positive instances, as well as binary label classification for CTR; (3) We expand existing recommenders and introduced new ones, making a single model able to support multiple tasks; (4) Various input formats are accommodated, including dense and discrete metadata, interaction-varying situation contexts, and impression logs. ReChorus2.0 marks a substantial update toward a more modular and task-flexible framework. It better aligns with the practical recommendation systems, catering to more diverse research needs. We hope ReChorus2.0 will serve as a highly customized and user-friendly tool for more researchers to form a 'Chorus' of recommendation tasks and algorithms.", "2 RELATED WORK": "", "2.1 Reranking in Recommendations": "Serving as the last step of a recommender system, the re-ranking stage obtains and permute top items from the ranking stage as candidates. The importance of re-ranking models has gained increasing attention [33], especially in the industry, since its output directly affects the revenue. Even ranking algorithms [40] begin to implicitly consider the relations among the items to provide high-quality candidate sets for re-ranking. Re-ranking algorithms consider the inter-dependencies among items in a candidate list, which can be categorized as two groups: two-step models (evaluator and generator) and unified models (maintaining permutation invariance in the candidate list). Two-step models typically utilize reinforcement learning techniques to maximize the utility. For instance, Aliexpress [19] employs an LSTM-based evaluator to assess the utility of re-ranked item lists, guiding the generator to determine the optimal permutation of candidate items. Other approaches [13, 62] also focus on maximizing list-wise utility through similar methods. Unified re-ranking models treat input items as a set, ensuring the permutation of items does not affect the order of re-ranked results. PRM [39] was the first to apply transformer blocks in reranking to model global relationships in the candidate list. Setrank [38] introduced the concept of permutation invariance and proposed to use induced self-attention to extract item correlations. And MIR [63] extracted fine-grained interactions between candidate items and historical interactions. Given the importance of reranking tasks in practical applications and the recent trend towards unified reranking models, we support unified reranking models and corresponding training&evaluations on customized candidate sets in ReChorus2.0.", "2.2 Context-aware Recommendation": "Context-aware recommendations consider contextual attributes to capture user preferences more accurately [25]. The broad definition of context, which is adopted in this paper, encompasses user profiles, item content, and interaction-varying situations. Early works on context-aware RecSys explored various structures to model the relations of context features, such as interaction terms in FM [41], a combination of FM and deep neural networks in DeepFM [15] and xDeepFM [31], and attention-based pooling in AFM [64]. More recent models, such as FinalMLP [36] and SAM [10], are still exploring better structures to capture relations between features. As RecSys becomes increasingly complex, users' historical interactions are also considered in context modeling. For instance, DIN [70] utilized an activation unit to learn the relationships between candidate items&context and user behavior history. DIEN [69] adopted an interest-evolving layer to capture user interest from history. CAN [4] enhanced sequential recommenders with Co-Action Units between target items, context, and history features. Recently, situations, i.e., context varying with user interactions, have attracted attention [28, 35]. In these works, users' location and time are considered for dynamic user preference modeling. In summary, context-aware recommendation is an evolving research hotspot. The majority of existing work has been conducted under the CTR prediction task [10, 36, 41, 45, 70], while some recent exploration [4, 28] has also applied context information for ranking tasks. Therefore, in ReChorus2.0, we implement various contextaware models and set them to support both CTR prediction and top-k recommendation tasks. Additionally, we expand the reader to accommodate different formats of context information.", "2.3 Libraries for Recommendation Algorithms": "The increasing complexity and diversity of recommendation systems have raised concerns about reproducibility and fair comparison in recommendation research [14, 46, 50]. To address the issues, researchers have made great efforts to design tools and frameworks that promote reproducible research. Table 1 compares our proposed ReChorus2.0 framework with various existing libraries. While most existing frameworks provide support for basic Top-k recommendation and CTR prediction tasks, few fully support context information for different tasks. For instance, only three of the frameworks support context-aware models for Top-k recommendation task. Furthermore, most context-aware frameworks focus on the side information from users and items while neglecting the situational context that varies with interactions. Moreover, ReChorus2.0 can also handle different interaction formats. Besides the ordinary click and rating feedback, the impression logs (exposure but non-click) are also considered. ReChorus2.0 also distinguishes itself by fully adopting the reranking task and corresponding customized candidate sets, which has gained increasing attention in both academics and industry. Only Librerank [33] provides a comprehensive re-ranking algorithm platform, but it only offers two fixed base-ranker options, which limits comparing ranking and re-ranking models based on the same evaluation system. ReChorus2.0 enables the re-ranking setting by inducing the customized candidate set. Apart from the conventional negative sampling operation for training and evaluation, researchers can also customize the positive and negative samples with variable lengths for each candidate set, both in the training and evaluation datasets. Moreover, in ReChorus2.0, ranking and re-ranking models can be flexibly combined as they would be in real-world scenarios. Note that we only compare with libraries that provide generic recommenders aiming at performance improvement. Frameworks with other targets, experiment settings, or specific scenarios are excluded in our discussions, such as reinforcement learning-based recommender framework EasyRL4Rec [66], fairness-aware component in Recbole2.0 [68], recommender evaluation library RecList [11], and news recommendation library [20].", "3 THE RECHORUS2.0 FRAMEWORK": "", "3.1 Supported Task Categories": "As a task-flexible library, ReChorus2.0 is able to support a variety of tasks with different types of input, models, and training&evaluation strategies. We introduce the tasks in general in this section. \u00b7 Top-k Recommendation Task : Top-k recommendation is a common task where the goal is to suggest a list of \ud835\udc58 items that they are most likely to be interested in for users [42, 57]. During both training and evaluation, a fixed number of randomly sampled unseen items are treated as the negative items for each positive interaction. List-wise evaluation metrics, such as Hit Ratio and NDCG, are calculated on lists containing one positive item and a fixed number of negative items. In ReChorus2.0, general, sequential, and context-aware recommenders all support the Top-k recommendation task. \u00b7 Impression-based Ranking/Reranking Task : Different from top-k recommendation, impression-based tasks do not involve negative sampling. Instead, the candidate item lists shown to the user (i.e., the impression item list) are used, and the non-clicked items are viewed as negative in each impression. In practice, impression lists may be variable in length, and the number of positive and negative items in one impression list also varies. The evaluation metrics are calculated over all positive interactions in the same candidate list before being averaged among lists. In ReChorus2.0, general, sequential, and reranking models all support the impression-based task. \u00b7 Click-Through Rate (CTR) Prediction Task : Click-Through Rate (CTR) prediction aims to estimate the probability that a user will click on a given item, and the estimation is compared with a binary label representing click/non-click. Evaluation metrics, such as LogLoss and AUC, are calculated based on the prediction probabilities and ground truth labels of all interactions or each user. Generally, rich context information is utilized for CTR prediction tasks. In ReChorus2.0, context-aware models can support this task, where user profiles, item metadata, and situation context (i.e., context varying with interactions) can be utilized. Calculate statistics         Organize corpus Input Interactions Context features Model Checkpoint Impression logs Reader Base Sequence Context Impression Rec. Model General Sequential Context- aware Re-rank Runner Base CTR Pred. Impression Train & Evaluate Formatted file Utilities Saver : Models Predictions Logger : Args Metrics Process \u2026 Layers : Attention Transformer MLP block Activations \u2026 (b) Task Implementation Examples Task Example2: CTR Prediction Task Example3: Reranking Impression logs Impression Reader Re-rank Model Impression Runner Interactions Context Reader Context- aware Model CTR Pred. Runner Task Example1: Top-k Recommendation Context features Interactions Sequence Reader Sequential Model Base Runner Base Runner Context- aware Model Interactions Context Reader Context features Module Assembly Model Checkpoint Select model & task", "3.2 Overall Framework Design": "As shown in Figure 1(a), ReChorus2.0 follows the flexible modular design and introduces numerous new features within each part of module to support a wider variety of models and task settings. In general, Rechorus2.0 supports various recommendation tasks from different Input formats by assembly of three main modules: Reader , Model , and Runner . Reader s deal with inputs, calculate the dataset statistics, and organize data as a unified corpus for models. Basic interactions, click labels, impressions, model checkpoints, and abundant context information are all acceptable inputs. Diverse recommendation Model s are supported, including generic and sequential models, context-aware recommender systems (CARS), and re-rank recommenders. Up to 37 recommendation methods are supported in ReChorus2.0. Depending on the data and the specific task, the corresponding Runner is invoked: Base Runner for top-k recommendation, CTR Runner for CTR prediction, and Impression Runner for impression-based ranking & re-ranking tasks. Note that readers and runners are automatically assembled to models once users select the recommendation model and mode (i.e., task), freeing users from implementation details and allowing them to focus on methodological and experimental exploration. Specifically, functions about the context-aware CTR predictions and recommendations, as well as impression-based (re-) ranking are all newly introduced in ReChorus2.0. Besides the main modules, utility modules are adopted to back all modules, including a logger to write the running processes, a saver for model checkpoints and prediction, and several common neural network layers. Moreover, an experiment script, exp.py , is designed to automatically conduct repeat experiments with different random seeds and save the average performances.", "3.3 Data Readers": "Reader modules are utilized to transfer input files to a unified corpus for training and evaluation. During the transfer, basic dataset statistics are recorded. After processing, the entire corpus is stored user_id item_id time 1 1 1573747200 user_id item_id time neg_items 1 100 1573782111 [3, 5, 6, \u2026] user_id Item_id time c_location_c c_temp_f 1 1 1573747200 1 25 Situation context user_id u_gender_c u_age_f 1 0 25.0 item_id i_type_c i_price_f 1 15 99.9 Figure 2: Examples of data input formats supported by ReChorus2.0. Note that impression_id is only necessary when conducting impression-based tasks in Figure(b). as a .pkl file to facilitate direct loading in future uses, thereby saving time. Various types of readers are designed to support flexible inputs with different formats and content, as shown in Figure 2: \u00b7 Base reader : It is the basic class for all readers and is directly called by Top-k recommendation tasks. Base readers support input of interactions with user and item IDs, with negative items assigned to validation and test sets, as shown in Figure 2(a). \u00b7 Sequential reader : Built on the base reader, sequential readers capture and save users' interaction history for sequential recommender systems to support Top-k recommendation tasks. \u00b7 Impression reader : It enables impression data reading of the format in Figure 2(b). Interactions with the same \ud835\udc56\ud835\udc5a\ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b _ \ud835\udc56\ud835\udc51 are grouped, and later, they will be trained and evaluated together. In this way, users can customize their desired ways to construct any type of candidate set. \u00b7 Context reader : It helps read contextual information of users, items, and interaction situations, as shown in Figure 2(c). Besides these four fundamental readers, ReChorus2.0 provides their combinations for various data format requirements, such as Context-SeqReader for context-aware sequential recommenders and Context-ImpReader for context-aware impression-based tasks. It is worth noting that, unlike some popular frameworks [68, 71], ReChorus does not provide dataset filtering and split processes. On the contrary, training, validation, and test sets are fed into the readers separately, with post-processing to combine them if necessary (e.g., for user history or impression construction). This allows users to freely customize dataset settings, facilitating deep investigation of datasets in academic research.", "3.4 Supported Models": "In this upgrade of ReChorus, we add numerous context-aware and re-ranking recommenders and expand existing models to cover more tasks, adding up to 37 recommenders. In the implementation, we enable many models to effectively support more than one task by model inheritance and code reuse. Users can switch to different tasks easily by changing the \ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59 _ \ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52 configuration. Models and their supported modes (i.e., tasks) are listed as follows: \u00b7 General models : This model group includes CF-based recommenders considering user and item IDs, which support Top-k recommendation and impression-based ranking tasks, including: Most popular, BPR [42], Neural MF [17], CFKG [67], LightGCN [16], BUIR [26], and DirectAU [55]. \u00b7 Sequential models : Sequential models utilize users' historical item interaction sequences to recommend users' next interaction, which support Top-k recommendation and impression-based ranking tasks, including: FPMC [43], GRU4Rec [18], NARM [27], Caser [51], SASRec [23], SLRCPlus [56], Chorus [57], ComiRec [7], KDA [53], TiSASRec [30], TiMiRec [54], and ContraRec [52]. \u00b7 Re-rank models : Re-rank models incorporate pre-trained backbone recommenders to provide ranking lists and re-sort them. Both general and sequential backbones are supported for impressionbased ranking with PRM [39], SetRank [38], and MIR [63]. \u00b7 Context-aware models : Context-aware recommender systems (CARS) adopt diverse structures to capture the influences of contextual information on users' preferences. Both interaction-based and sequential CARS support Top-k recommendation and CTR prediction tasks, including: FM [41], Wide&Deep [9], DeepFM [15], AFM[64], DCN [58], xDeepFM [31], AutoInt [45], DCNv2 [59], FinalMLP [36], SAM [10], SARE [28], DIN [70], DIEN [69], CAN [4] (based on DIEN), ETA [8], and SDIM [5]. It is important to note that since ReChorus2.0 targets at offering a modular and flexible framework, we only include classic and some state-of-the-art recommenders in each model group. Owing to the extensibility of ReChorus 2.0, users can easily implement new models (Details are shown in Section 4.4). We are also planning to add more models in the future.", "3.5 Training and Evaluation Runners": "Runners are responsible for training and evaluating the models based on a specified task, providing appropriate training strategies and evaluation metrics. \u00b7 Base Runner : The base runner is adopted for Top-k recommendation tasks, where BPR losses are used for optimization, and Hit Rate (HR) and NDCG [21] are used as metrics. \u00b7 Impression Runner : It contributes to the variable-length and multiple-positive recommendation tasks, where multiple loss functions can be adopted, including list-level BPR loss [29], listnet loss [6], softmax cross-entropy loss, and attention rank [1]. Evaluation metrics include HR, NDCG, and MAP, where we adopt matrix operations to optimize the time efficiency of calculations. \u00b7 CTR Runner : CTR runner is designed for CTR prediction tasks, with BPR/BCE loss for training and AUC/Log loss for evaluation. For expression consistency, loss functions are introduced here, along with the runners. In the implementation, loss functions are actually implemented in the task-oriented base model s. By inheriting different base models in different modes, each recommender, as introduced in the previous section, can be applied to varied tasks.", "4 TASK-BASED USAGE GUIDELINES": "Given that the update of ReChorus2.0 is centered on task enhancement, we present several examples to demonstrate the flexible implementation of various tasks using ReChorus2.0 as user guidelines. Examples of configurations to run all supported task categories (in Section 3.1) are shown in Figure 3, and details about usage tutorials for all these existing and new tasks can be found in the repository 2 . In the following subsections, we will focus on three main task categories newly supported in ReChorus2.0 and illustrate the data preparation and configuration settings for each task: Impression-based Ranking/Re-ranking in Figure 3(e)-(g), CTR prediction in Figure 3(d), and context-aware Top-k recommendation in Figure 3(c). Moreover, extensions to new tasks and models will also be briefly introduced.", "4.1 Impression-based Ranking/Re-ranking": "In impression-based ranking and re-ranking tasks, each sample's candidate set may vary in length and contain arbitrary number of positive and negative instance. To standardize processing in the library, ReChorus2.0 requires that interaction data be formatted with each interaction on one line, identified by an impression ID to denote interactions under the same impression, as shown in Figure 2(b). The impression reader will automatically aggregate multiple interactions sharing the same impression ID for training and testing. Examples of running configurations are illustrated in Figure 3(e)-(g). For ranking tasks, users simply need to set \ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59 _ \ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52 to 'Impression'. For re-ranking tasks, it is also necessary to specify the name, configuration, and checkpoint of the backbone ranker to obtain ranking inferences from the backbone.", "4.2 CTR Prediction Task": "As defined in Section 3.1, the CTR prediction task aims to predict whether a user will click on a given item, essentially a binary classification problem. To undertake a CTR prediction task with ReChorus 2.0, a dataset comprising interactions and corresponding labels (click or not) should be prepared, as depicted in python main.py ~~model_name BPRMF ~-dataset topkData ~~num_neg metric NDCG, HR ~topk 5,10 loss n BPR model mode TopK (a) General model for Top-k recommendation max 20 topkData num neg ~-metric NDCG, HR ~-topk 5,10 loss BPR -model_mode TopK (b) Sequential model for Top-k recommendation python main.py ~-model_name FM ~~dataset topkData neg ~metric NDCG,HR ~include_situation_features include user_features loss BPR 'mode mode TopK (c) Context-aware model for Top-k recommendation python main.py name FM ~lr le-3 le-6 ~~dataset labeledData neg ~metric AUC loss ~include_item_features ~include_situation_features include user features -loss BCE model mode CTR Log python main.py name BPRMF le-3 le-6 ~~loss_n BPRsession ~~dataset impData neg ~metric NDCG , HR model_mode Impression (e) General model for Impression-based Ranking python main.py ~model_name SASRec -history le-3 le-6 n BPRsession ~-dataset impData metric NDCG HR ~-topk 3,5 mode mode Impression python main.py model name PRM le-3 le-6 =sloss BPRsession ~dataset impData ~metric NDCG HR , MAP ~stopk 3,5 ~~ranker name BPRMF ranker config_file BPRMF .yaml ranker model_file BPRMF . pt model mode General Figure 2(b) (Note that \ud835\udc56\ud835\udc5a\ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b _ \ud835\udc56\ud835\udc51 is not necessary). Furthermore, users can input context by appending situation information in the interaction file, as well as extra \ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc5f _ \ud835\udc5a\ud835\udc52\ud835\udc61\ud835\udc4e\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4e.\ud835\udc50\ud835\udc60\ud835\udc63 and \ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc5a _ \ud835\udc5a\ud835\udc52\ud835\udc61\ud835\udc4e\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4e.\ud835\udc50\ud835\udc60\ud835\udc63 files, as shown in Figure 2(c). Once the dataset is prepared, users simply need to specify the desired recommendation model and set the \ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59 _ \ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52 parameter to CTR in the execution command. An example of the main parameters is illustrated in Figure 3(d), where the \ud835\udc56\ud835\udc5b\ud835\udc50\ud835\udc59\ud835\udc62\ud835\udc51\ud835\udc52 _ \ud835\udc4b\ud835\udc4b\ud835\udc4b _ \ud835\udc53 \ud835\udc52\ud835\udc4e\ud835\udc61\ud835\udc62\ud835\udc5f\ud835\udc52\ud835\udc60 parameter allows for flexible input of partial or all prepared context, while the \ud835\udc5b\ud835\udc62\ud835\udc5a _ \ud835\udc5b\ud835\udc52\ud835\udc54 parameter should be set to 0, since negative sampling is unnecessary for CTR prediction tasks.", "4.3 Context-aware Top-k Recommendation Task": "Top-k recommendation, where recommender models rank either the entire item set or a randomly sampled subset of items and provide Top-k items as recommendation results, was the only task supported by ReChorus1.0. In ReChorus2.0, we enable the newly added context-aware models to support the Top-k recommendation task as well. The dataset preparation for this task includes a training set containing interactions and validation&test sets containing interactions and corresponding negative item candidates, as shown in Figure 2(a) (for full-set evaluation, \ud835\udc5b\ud835\udc52\ud835\udc54 _ \ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc5a\ud835\udc60 is not needed). Additionally, context information, as depicted in Figure 2(c), is also permitted as input. An example for task running configurations is shown in Figure 3(c), where \ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59 _ \ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52 should be set to TopK and \ud835\udc5b\ud835\udc62\ud835\udc5a _ \ud835\udc5b\ud835\udc52\ud835\udc54 should be greater than zero for training with negative sampling and the BPR loss function [42].", "4.4 Extension to New Models and Tasks": "Since ReChorus2.0 follows the flexible modular design, users can conveniently extend to new models and tasks in ReChorus2.0. To implement a new model for an existing task, users simply need to create a new model class, inherit the corresponding base model, and specify the runner name corresponding to the task and the reader name matching the data. Within the new model class, users only need to define the model's required parameters and construct a forward function that accepts the input as a feed dictionary. The framework will then automatically handle the model initialization, the construction of the data batches, and the training&evaluation process. If extra information is needed for feed-in batches, users can inherit a dataset class from base models to the new model, and define their own training and evaluation batches. If users plan to introduce a new task, new runners should be defined to configure training and evaluation, and a new reader is needed to accommodate a novel data format if necessary. Subsequently, users can develop new models for the novel task or extend existing models to the new task by inheritance and overriding. In the future, we will continue to update the library to include more models and tasks.", "5 EXPERIMENTS": "In this section, we present experiments for three mainly updated tasks in ReChorus2.0: (1)Impression-based ranking and reranking (Section 4.1), (2) CTR Prediction (Section 4.2), and (3) Contextaware Top-k Recommendation (Section 4.3) over all recommendation methods in Section 3.4 that can support corresponding tasks. Through extensive experiments, we aim to demonstrate how ReChorus2.0 flexibly executes various tasks using diverse models on the same datasets, rather than provide a standard benchmark for the recommenders.", "5.1 Experiment Settings": "5.1.1 Datasets. For consistent comparisons, we choose the same datasets for three task settings, so the datasets should include both rich context information and complete impression logs with exposure and click records. Based on these requirements, we select two datasets with different scales and scenarios: \u00b7 MIND-Large [61]: It is a large-scale dataset of impression-based click and exposure logs of Microsoft News from 1 million users during one-week interactions, including rich content and context about news. \u00b7 MovieLens-1M 3 : It contains 1 million rating interactions spanning over 20 years from the MovieLens website with genre and title categories of movie items. Similar preprocesses are conducted for both datasets: Firstly, fivecore filtering on users and items is conducted until all remaining users and items have at least five positive interactions. Secondly, situation context is extracted from the interaction timestamps, including hour-of-day, day-of-week, and period-of-day. Thirdly, interactions with the same timestamp are taken as an impression in MIND-Large, and for MovideLens-1M, we follow previous practice [22, 32, 40] to split user rating sessions into impression lists of length 20 and take the ratings of 4-5 as positive and 1-3 as negative. Finally, training, validation, and test sets are split along the global timeline [47]: In MIND, the first six days are treated as training set, followed by a half-day validation set and half-day test set; In MovieLens-1M, training, validation, and test sets are split with 80%, 10%, 10% of the time. The detailed preprocessing scripts are publicly available 4 , and final statistics of two datasets are shown in Table 2. We refer to the datasets as MIND and ML-1M for short when the context is clear. 5.1.2 Training and Evaluation Settings. Since the three tasks are naturally different, distinct settings are adopted for each task: \u00b7 For impression-based ranking and reranking tasks, list-level BPR loss [29] is adopted as training loss, and evaluations are conducted on the ranking/reranking results of impression lists. Evaluation metrics are HR@k, MAP@k, and NDCG@k. Since the impression lists are generally short, results with k=2 and k=5 are reported. \u00b7 For CTR prediction tasks, BCE loss is used for training. We evaluate by AUC and Log loss; both are commonly used evaluation metrics for CTR prediction. \u00b7 For context-aware Top-k recommendation, negative sampling is utilized for both training and evaluation: one random negative sample is chosen for BPR loss during training, and 99 negative samples are randomly sampled for evaluation, with HR@k and NDCG@k as metrics (k=5, 10, and 20 are reported). Note that for Top-k recommendations, efficiency considerations led us to sample only 99 negative instances due to the necessity of running and tuning numerous models. However, ReChorus2.0 supports evaluation with more negative instances or even the full item set. 5.1.3 Implementation Details. ReChorus2.0 is implemented entirely in Python and based on PyTorch. For a lightweight interface, all user interactions are facilitated via command lines. Parameter tuning is conducted for each model and task: for fair comparison, the embedding size is fixed at 64; learning rate is chosen from {5e-3, 2e-3, 1e-3, 5e-4, 2e-4}; L2 regularization term is chosen from {1e-4, 1e-6, 0}; for models with hidden layers, the size of each hidden layer is chosen from {64, 128, 256}; the length of user history for sequential models is selected from {10, 20, 30}; and other model-specific parameters are also fine-tuned accordingly. Training employs early stopping, which stops when there is no performance improvement on the validation set for 10 epochs. The optimal parameters for each model on each task are recorded within the library 5 . Experiments were repeated five times with different random seeds using the exp.py module, and the average performance is reported as follows.", "5.2 Experiment Results": "5.2.1 Impression-based Ranking&Re-ranking. Table 3 and Table 4 shows the performance comparison for ranking and re-ranking models on the MIND and ML-1M dataset, respectively. For ranking models, we consider two classical general models, BPR and LightGCN, and two sequential models, GRU4Rec and SASRec. For the reranking task, each reranker is conducted with one general model and one sequential model as base rankers, respectively, where base rankers are all previously trained on the same dataset and fixed. In the MIND dataset, reranking the candidate item list improves the ranking performance in most cases, especially for BPRMF, the simpler base ranker. Sequential base rankers can also provide more useful information, and they tend to achieve continuously better performance for every reranking model. The promising results indicate that considering the candidate list context induces a performance boost. For the ML-1M dataset, the candidate list is shorter, and the proportion of positive interactions is much larger on average, which makes the ranking task easier (i.e., the metrics are higher for base rankers). Hence, limited improvement can be achieved by adding sequential information and reranking models. 5.2.2 CARS for Top-k Recommendation and CTR Prediction. For the Context-aware recommender systems (CARS) mentioned in Section 3.4, we investigate their performance on both Top-k recommendation and CTR prediction tasks in Table 5 and Table 6. Models are divided into two group by whether they utilize user history, and they are listed in chronological order in each group. Despite the difference in the data size and the ratio of positive interactions between the two datasets, Table 5 reveals that the sequential CARS achieves much better performance in Topk recommendation. Apart from the information gained from the historical interaction, this result is also explained by the fact that sequential models can better leverage the newly emerged interactions in the validation and test set. The results of CARS for the CTR prediction task are shown in Table 6, where sequential recommenders also achieve slightly better performance on average. Note that the Log loss metric is not compatible between two datasets: Because the ratio of positive interactions is much lower for the MIND dataset, models are prone to give smaller prediction scores to achieve smaller log loss values. The above experiment results for various recommenders on the three newly implemented tasks highlight the flexibility of ReChorus2.0 framework, demonstrating its ability to handle various types of tasks, inputs, settings, and metrics comprehensively.", "6 CONCLUSIONS": "In this paper, we provide ReChorus2.0 , a modular task-flexible recommender library. Different from existing popular libraries, ReChorus2.0 provides highly customized ways to implement tasks and consturct datasets. ReChorus2.0 concurrently supports three types of recommendation tasks: top-k recommendation, CTR prediction, and impression-based ranking/re-ranking. It also enables the same recommender to flexibly support multiple different tasks. Moreover, utilizing the well-separated reader module, ReChorus2.0 accommodates customized input settings, including various dataset splitting configurations, different interaction formats (e.g., negative item lists, impression logs, and click labels), and various types of context information. With these characteristics, ReChorus2.0 offers researchers in the recommender system scenario a convenient platform that enables them to assemble different modules like building blocks, facilitating the creation of customized experiment settings. This is particularly crucial for users to conduct in-depth analyses of methods or data during the research process. In the future, we plan to provide more task configurations and models, offering users a more convenient, powerful, and comprehensive tool for recommender system research.", "ACKNOWLEDGMENTS": "We sincerely thank Chenyang Wang's excellent contributions to ReChorus1.0. It is his elegant work that makes ReChorus2.0 possible. This work is supported by the Natural Science Foundation of China (Grant No. U21B2026, 62372260) and Quan Cheng Laboratory (Grant No. QCLZD202301).", "REFERENCES": "[1] Qingyao Ai, Keping Bi, Jiafeng Guo, and W. Bruce Croft. 2018. Learning a Deep Listwise Context Model for Ranking Refinement. In SIGIR . ACM, Ann Arbor MI USA, 135-144. [2] Vito Walter Anelli, Alejandro Bellog\u00edn, Antonio Ferrara, Daniele Malitesta, Felice Antonio Merra, Claudio Pomo, Francesco Maria Donini, and Tommaso Di Noia. 2021. Elliot: A Comprehensive and Rigorous Framework for Reproducible Recommender Systems Evaluation. In SIGIR '21: The 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, Virtual Event, Canada, July 11-15, 2021 , Fernando Diaz, Chirag Shah, Torsten Suel, Pablo Castells, Rosie Jones, and Tetsuya Sakai (Eds.). ACM, 2405-2414. https://doi.org/10.1145/3404835.3463245 [3] Andreas Argyriou, Miguel Gonz\u00e1lez-Fierro, and Le Zhang. 2020. Microsoft recommenders: best practices for production-ready recommendation systems. In Companion Proceedings of the Web Conference 2020 . 50-51. [4] Weijie Bian, Kailun Wu, Lejian Ren, Qi Pi, Yujing Zhang, Can Xiao, Xiang-Rong Sheng, Yong-Nan Zhu, Zhangming Chan, Na Mou, et al. 2022. CAN: feature co-action network for click-through rate prediction. In Proceedings of the fifteenth ACM international conference on web search and data mining . 57-65. [5] Yue Cao, Xiaojiang Zhou, Jiaqi Feng, Peihao Huang, Yao Xiao, Dayao Chen, and Sheng Chen. 2022. Sampling is all you need on modeling long-term user behaviors for CTR prediction. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management . 2974-2983. [6] Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. 2007. Learning to rank: from pairwise approach to listwise approach. In International Conference on Machine Learning . https://api.semanticscholar.org/CorpusID:207163577 [7] Yukuo Cen, Jianwei Zhang, Xu Zou, Chang Zhou, Hongxia Yang, and Jie Tang. 2020. Controllable multi-interest framework for recommendation. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 2942-2951. [8] Qiwei Chen, Changhua Pei, Shanshan Lv, Chao Li, Junfeng Ge, and Wenwu Ou. 2021. End-to-end user behavior retrieval in click-through rateprediction model. arXiv preprint arXiv:2108.04468 (2021). [9] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems . 7-10. [10] Yuan Cheng and Yanbo Xue. 2021. Looking at CTR Prediction Again: Is Attention All You Need?. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1279-1287. [11] Patrick John Chia, Jacopo Tagliabue, Federico Bianchi, Chloe He, and Brian Ko. 2022. Beyond NDCG: Behavioral Testing of Recommender Systems with RecList (WWW'22 Companion) . Association for Computing Machinery, New York, NY, USA, 99-104. https://doi.org/10.1145/3487553.3524215 [12] Michael D Ekstrand. 2020. Lenskit for python: Next-generation software for recommender systems experiments. In Proceedings of the 29th ACM international conference on information & knowledge management . 2999-3006. [13] Yufei Feng, Binbin Hu, Yu Gong, Fei Sun, Qingwen Liu, and Wenwu Ou. 2021. GRN: Generative Rerank Network for Context-wise Recommendation. arXiv (April 2021). arXiv. [14] Maurizio Ferrari Dacrema, Simone Boglio, Paolo Cremonesi, and Dietmar Jannach. 2021. A troubling analysis of reproducibility and progress in recommender systems research. ACM Transactions on Information Systems (TOIS) 39, 2 (2021), 1-49. [15] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. arXiv preprint arXiv:1703.04247 (2017). [16] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for recommendation. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval . 639-648. [17] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural collaborative filtering. In Proceedings of the 26th international conference on world wide web . 173-182. [18] Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2015. Session-based recommendations with recurrent neural networks. arXiv preprint arXiv:1511.06939 (2015). [19] Guangda Huzhang, Zhen-Jia Pang, Yongqing Gao, Yawen Liu, Weijie Shen, WenJi Zhou, Qing Da, An-Xiang Zeng, Han Yu, Yang Yu, and Zhi-Hua Zhou. 2020. AliExpress Learning-To-Rank: Maximizing Online Model Performance without Going Online. arXiv (Dec. 2020). [20] Andreea Iana, Goran Glava\u0161, and Heiko Paulheim. 2023. NewsRecLib: A PyTorchLightning Library for Neural News Recommendation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations . 296-310. [21] Kalervo J\u00e4rvelin and Jaana Kek\u00e4l\u00e4inen. 2002. Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems (TOIS) 20, 4 (2002), [44] Aghiles Salah, Quoc-Tuan Truong, and Hady W Lauw. 2020. Cornac: A Comparative Framework for Multimodal Recommender Systems. Journal of Machine Learning Research 21, 95 (2020), 1-5. [45] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, and Jian Tang. 2019. Autoint: Automatic feature interaction learning via selfattentive neural networks. In Proceedings of the 28th ACM international conference on information and knowledge management . 1161-1170. [46] Aixin Sun. 2023. On Challenges of Evaluating Recommender Systems in an Offline Setting. In Proceedings of the 17th ACM Conference on Recommender Systems . 1284-1285. [47] Aixin Sun. 2023. Take a Fresh Look at Recommender Systems from an Evaluation Standpoint. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval . 2629-2638. [48] Peijie Sun, Le Wu, Kun Zhang, Xiangzhi Chen, and Meng Wang. 2023. Neighborhood-Enhanced Supervised Contrastive Learning for Collaborative Filtering. IEEE Transactions on Knowledge and Data Engineering (2023). [49] Zhu Sun, Hui Fang, Jie Yang, Xinghua Qu, Hongyang Liu, Di Yu, Yew-Soon Ong, and Jie Zhang. 2022. DaisyRec 2.0: Benchmarking Recommendation for Rigorous Evaluation. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) (2022). [50] Zhu Sun, Di Yu, Hui Fang, Jie Yang, Xinghua Qu, Jie Zhang, and Cong Geng. 2020. Are we evaluating rigorously? benchmarking recommendation for reproducible evaluation and fair comparison. In Proceedings of the 14th ACM Conference on Recommender Systems . 23-32. [51] Jiaxi Tang and Ke Wang. 2018. Personalized top-n sequential recommendation via convolutional sequence embedding. In Proceedings of the eleventh ACM international conference on web search and data mining . 565-573. [52] Chenyang Wang, Weizhi Ma, Chong Chen, Min Zhang, Yiqun Liu, and Shaoping Ma. 2023. Sequential recommendation with multiple contrast signals. ACM Transactions on Information Systems 41, 1 (2023), 1-27. [53] Chenyang Wang, Weizhi Ma, Min Zhang, Chong Chen, Yiqun Liu, and Shaoping Ma. 2020. Toward dynamic user intention: Temporal evolutionary effects of item relations in sequential recommendation. ACM Transactions on Information Systems (TOIS) 39, 2 (2020), 1-33. [54] Chenyang Wang, Zhefan Wang, Yankai Liu, Yang Ge, Weizhi Ma, Min Zhang, Yiqun Liu, Junlan Feng, Chao Deng, and Shaoping Ma. 2022. Target interest distillation for multi-interest recommendation. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management . 2007-2016. [55] Chenyang Wang, Yuanqing Yu, Weizhi Ma, Min Zhang, Chong Chen, Yiqun Liu, and Shaoping Ma. 2022. Towards representation alignment and uniformity in collaborative filtering. In Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data mining . 1816-1825. [56] Chenyang Wang, Min Zhang, Weizhi Ma, Yiqun Liu, and Shaoping Ma. 2019. Modeling item-specific temporal dynamics of repeat consumption for recommender systems. In The world wide web conference . 1977-1987. [57] Chenyang Wang, Min Zhang, Weizhi Ma, Yiqun Liu, and Shaoping Ma. 2020. Make it a chorus: knowledge-and time-aware item modeling for sequential recommendation. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . 109-118. [58] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network for ad click predictions. In Proceedings of the ADKDD'17 . 1-7. [59] Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong, and Ed Chi. 2021. Dcn v2: Improved deep & cross network and practical lessons for web-scale learning to rank systems. In Proceedings of the web conference 2021 . 1785-1797. [60] Yifan Wang, Peijie Sun, Min Zhang, Qinglin Jia, Jingjie Li, and Shaoping Ma. 2023. Unbiased Delayed Feedback Label Correction for Conversion Rate Prediction. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 2456-2466. [61] Fangzhao Wu, Ying Qiao, Jiun-Hung Chen, Chuhan Wu, Tao Qi, Jianxun Lian, Danyang Liu, Xing Xie, Jianfeng Gao, Winnie Wu, et al. 2020. Mind: A large-scale dataset for news recommendation. In Proceedings of the 58th annual meeting of the association for computational linguistics . 3597-3606. [62] Yunjia Xi, Weiwen Liu, Xinyi Dai, Ruiming Tang, Weinan Zhang, Qing Liu, Xiuqiang He, and Yong Yu. 2022. Context-aware Reranking with Utility Maximization for Recommendation. arXiv (Feb. 2022). [63] Yunjia Xi, Weiwen Liu, Jieming Zhu, Xilong Zhao, Xinyi Dai, Ruiming Tang, Weinan Zhang, Rui Zhang, and Yong Yu. 2022. Multi-Level Interaction Reranking with User Behavior History. [64] Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua. 2017. Attentional factorization machines: Learning the weight of feature interactions via attention networks. arXiv preprint arXiv:1708.04617 (2017). [65] Yanwu Yang and Panyu Zhai. 2022. Click-through rate prediction in online advertising: A literature review. Information Processing & Management 59, 2 (2022), 102853. [66] Yuanqing Yu, Chongming Gao, Jiawei Chen, Heng Tang, Yuefeng Sun, Qian Chen, Weizhi Ma, and Min Zhang. 2024. EasyRL4Rec: A User-Friendly Code Library for Reinforcement Learning Based Recommender Systems. arXiv preprint arXiv:2402.15164 (2024). [67] Yongfeng Zhang, Qingyao Ai, Xu Chen, and Pengfei Wang. 2018. Learning over knowledge-base embeddings for recommendation. arXiv preprint arXiv:1803.06540 (2018). [68] Wayne Xin Zhao, Yupeng Hou, Xingyu Pan, Chen Yang, Zeyu Zhang, Zihan Lin, Jingsen Zhang, Shuqing Bian, Jiakai Tang, Wenqi Sun, Yushuo Chen, Lanling Xu, Gaowei Zhang, Zhen Tian, Changxin Tian, Shanlei Mu, Xinyan Fan, Xu Chen, and Ji-Rong Wen. 2022. RecBole 2.0: Towards a More Up-to-Date Recommendation Library. In CIKM . ACM, 4722-4726. [69] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Deep interest evolution network for click-through rate prediction. In Proceedings of the AAAI conference on artificial intelligence , Vol. 33. 5941-5948. [70] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through rate prediction. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1059-1068. [71] Jieming Zhu, Quanyu Dai, Liangcai Su, Rong Ma, Jinyang Liu, Guohao Cai, Xi Xiao, and Rui Zhang. 2022. BARS: Towards Open Benchmarking for Recommender Systems. In SIGIR '22: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022 , Enrique Amig\u00f3, Pablo Castells, Julio Gonzalo, Ben Carterette, J. Shane Culpepper, and Gabriella Kazai (Eds.). ACM, 2912-2923. https://doi.org/10.1145/ 3477495.3531723"}
