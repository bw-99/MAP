{"MCNet: Monotonic Calibration Networks for Expressive Uncertainty Calibration in Online Advertising": "Quanyu Dai Huawei Noah's Ark Lab Shenzhen, China daiquanyu@huawei.com Jiaren Xiao \u2217 The HK PolyU Hong Kong, China xiaojr@connect.hku.hk Zhaocheng Du Huawei Noah's Ark Lab Shenzhen, China zhaochengdu@huawei.com Jieming Zhu Huawei Noah's Ark Lab Shenzhen, China jiemingzhu@ieee.org", "Chengxiao Luo": "Tsinghua University Shenzhen, China luocx21@mails.tsinghua.edu.cn Zhenhua Dong Huawei Noah's Ark Lab Shenzhen, China dongzhenhua@huawei.com", "Abstract": "", "Keywords": "In online advertising, uncertainty calibration aims to adjust a ranking model's probability predictions to better approximate the true likelihood of an event, e.g., a click or a conversion. However, existing calibration approaches may lack the ability to effectively model complex nonlinear relations, consider context features, and achieve balanced performance across different data subsets. To tackle these challenges, we introduce a novel model called Monotonic Calibration Networks, featuring three key designs: a monotonic calibration function (MCF), an order-preserving regularizer, and a field-balance regularizer. The nonlinear MCF is capable of naturally modeling and universally approximating the intricate relations between uncalibrated predictions and the posterior probabilities, thus being much more expressive than existing methods. MCF can also integrate context features using a flexible model architecture, thereby achieving context awareness. The order-preserving and field-balance regularizers promote the monotonic relationship between adjacent bins and the balanced calibration performance on data subsets, respectively. Experimental results on both public and industrial datasets demonstrate the superior performance of our method in generating well-calibrated probability predictions.", "CCS Concepts": "\u00b7 Information systems \u2192 Computational advertising . \u2217 Corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WWW'25, Sydney, NSW, Australia \u00a9 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-1274-6/25/04 https://doi.org/10.1145/3696410.3714802 Uncertainty Calibration, Online Advertising, Monotonic Networks", "ACMReference Format:": "Quanyu Dai, Jiaren Xiao, Zhaocheng Du, Jieming Zhu, Chengxiao Luo, Xiao-Ming Wu, and Zhenhua Dong. 2025. MCNet: Monotonic Calibration Networks for Expressive Uncertainty Calibration in Online Advertising. In Proceedings of the ACM Web Conference 2025 (WWW '25), April 28-May 2, 2025, Sydney, NSW, Australia. ACM, New York, NY, USA, 12 pages. https: //doi.org/10.1145/3696410.3714802", "1 Introduction": "In recent years, machine learning models have been extensively used to assist or even replace humans in making complex decisions in practical scenarios. Numerous applications, such as medical diagnosis [2], autonomous driving [1], and online advertising [13], have significant implications for people's safety or companies' economic incomes. Therefore, the performance of these models is of paramount importance. Apart from classification accuracy or ranking performance, it is also crucial for the predicted probability to accurately reflect the true likelihood of an event [10]. However, modern neural networks often struggle to produce accurate probability estimate, despite excelling at classification or ranking tasks. This limitation, known as miscalibration, is affected by their model architectures and training (e.g., model capacity, normalization, and regularization) [10, 24], significantly hindering their applications in real-world scenarios. In this paper, we focus on the scenario of online advertising. Typically, ECPM (Effective Cost Per Mille) serves as a key metric for measuring the revenue earned by the platform for every 1,000 ad impressions. In Pay-Per-Click advertising, ECPM is estimated as the product of the predicted Click-Through Rate (CTR) and the bidding price, i.e., \ud835\udc36\ud835\udc47\ud835\udc45 \u00d7 \ud835\udc4f\ud835\udc56\ud835\udc51 \u00d7 1000, which is used for ad ranking. This requires a model to output the predicted CTR that precisely reflects the probability of a user clicking on a given advertisement, as it directly influences bidding results and, consequently, the platform's revenue. However, as highlighted in [6, 19], the widely used deep Xiao-Ming Wu The HK PolyU Hong Kong SAR, China xiao-ming.wu@polyu.edu.hk WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia Quanyu Dai et al. ranking models heavily suffer from miscalibration, meaning the predicted CTR does not accurately represent the true probability. To tackle this challenge, uncertainty calibration has been widely studied [6, 19]. The goal is to train a well-calibrated predictor that can generate predictions accurately reflecting the actual probability of an event [7]. In this paper, we focus on the post-hoc calibration paradigm [6, 31] due to its flexibility and widespread adoption in practice. Post-hoc methods fix the base predictor and learn a new calibration function to transform the predictions from the base predictor into calibrated probabilities. As a result, they can be conveniently used as a model-agnostic plug-and-play module placed on top of the base predictor in real recommender systems. Post-hoc methods mainly contain binning-based methods [31, 32], scaling-based methods [15, 20], and hybrid methods [6, 19]. Binning-based methods, such as Histogram Binning [31] and Isotonic Regression [32], first divide data samples into multiple bins and then directly use the empirical posterior probability as the calibrated probability for each bin. Consequently, their calibration function is a piecewise constant function (see Fig. 1a), making the samples within a bin lose their order information. To alleviate this, scaling-based methods employ parametric functions for calibration. While preserving the order information among samples, they still face limitations in expressiveness due to their strong assumptions on the distributions of the class-conditional predicted scores, such as the Gaussian distribution in Platt Scaling [20]. Moreover, hybrid methods integrate binning-based and scaling-based methods into a unified solution to fully leverage their advantages. Representative methods, including SIR [6], NeuCalib [19], and AdaCalib [28], typically learn a piecewise linear calibration function (see Fig. 1 (b) and (c)). Therefore, they are incapable of learning a perfect calibration function when dealing with complex nonlinear relationships between uncalibrated scores and the true data distribution, which is often encountered in real-world applications. We defer to Section 4.1 for a more detailed and intuitive analysis. In addition to limited expressiveness, existing post-hoc methods also struggle to adaptively capture varying miscalibration issues across contexts, and fail to achieve a balanced performance across different data fields. Specifically, NeuCalib introduces an additional module to capture context information, which heavily relies on an additional dataset. On the other hand, AdaCalib learns independent calibration functions for different data subsets, but it suffers from the data-efficiency issue. In summary, existing methods fall short in developing an effective calibration function due to several key issues: their limited expressiveness, lack of context-awareness, and failure to consider field-balance calibration performance . In this paper, we propose a novel hybrid approach, M onotonic C alibration Net work ( MCNet ), designed to address the aforementioned challenges in uncertainty calibration. Like other hybrid methods, MCNet comprises a binning phase for dividing samples into multiple bins and a scaling phase for learning the calibration function for each bin. The success of MCNet hinges on three key designs, including a monotonic calibration function (MCF), an order-preserving regularizer, and a field-balance regularizer. Firstly, the MCF serves as a powerful approximator for capturing the intricate relationship between uncalibrated scores and the true data distribution (see Fig. 1 (d)). It is constructed using monotonic neural networks, ensuring the monotonically increasing property by enforcing the positivity of its derivative. Additionally, MCF can effectively model uncalibrated scores and context features with a flexible model architecture, thus achieving context-awareness efficiently. Secondly, the order-preserving regularizer is intended to promote monotonicity between different bins by penalizing calibration functions of two adjacent bins that violate the relative order at the split point of the two bins. The proposed calibration function, along with this regularizer, effectively address the first two issues. Additionally, we introduce a field-balance regularizer to address the third issue, which penalizes the variance of the calibration performance across different fields. By incorporating this regularizer, MCNet can attain a more balanced calibration performance. In summary, this paper makes the following contributions: \u00b7 We propose a novel hybird approach, Monotonic Calibration Networks, to achieve expressive, monotonically increasing, context-aware, and field-balanced uncertainty calibration. \u00b7 We design a monotonic calibration function, constructed using monotonic neural networks, to capture the complex relationship between uncalibrated scores and the true data distribution. \u00b7 We propose an order-preserving regularizer and a field-balance regularizer, which can significantly preserve order information and effectively promote balanced calibration performance among different fields, respectively. \u00b7 We conduct extensive experiments on two large-scale datasets: a public dataset AliExpress and a private industrial dataset from the advertising platform of Huawei browser, encompassing both click-through rate and conversion rate (CVR) prediction tasks.", "2 Related Work": "", "2.1 Uncertainty Calibration": "In real-world scenarios, the learned model must handle data samples from diverse contexts with varying data distributions. Therefore, it is crucial for the calibration function to consider the contextual information to enable adaptive calibration across contexts. Here, we broadly categorize existing uncertainty calibration approaches into two types: context-agnostic calibration [6, 10, 17, 20, 31] and context-aware calibration [19, 28]. Context-agnostic calibration. The uncalibrated score is the unique input of the calibration function. The binning-based methods, such as Histogram Binning [31] and Isotonic Regression [32], divide the samples into multiple bins, according to the sorted uncalibrated probabilities. In these non-parametric methods, the calibrated probability within each bin is the bin's posterior probability. Isotonic Regression merges adjacent bins to ensure the bins' posterior probabilities are non-decreasing. The scaling-based approaches, such as Platt Scaling [20] and Gamma Scaling [15], propose parametric functions that map the uncalibrated scores to calibrated ones. These parametric functions assume that the class-conditional scores follow Gaussian distribution (Platt Scaling) or Gamma distribution (Gamma Scaling). Smoothed Isotonic Regression (SIR) [6] learns a monotonically increasing calibration function with isotonic regression and linear interpolation, thus jointly exploiting the strengths of the binning- and scaling-based methods. Context-aware calibration. In addition to the uncalibrated score, the context information, such as the field id denoting the source of data samples, has also been considered recently. NeuCalib, MCNet: Monotonic Calibration Networks for Expressive Uncertainty Calibration in Online Advertising WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia as the pioneering work, uses a univariate calibration function to transform the uncalibrated logits, and an auxiliary neural network that considers the sample features for context-adaptive calibration. However, it is important to note that the calibration function itself in NeuCalib does not directly consider the context information. AdaCalib [28], on the other hand, divides the validation set into several fields and learns an isotonic calibration function for each field using the field's posterior statistics. This approach, however, suffers from data efficiency issues due to the need for field-specific calibration. Both NeuCalib and AdaCalib adopt piecewise linear calibration functions, which may lack expressiveness when modeling the complex nonlinear relationships between the uncalibrated scores and the true data distribution. Our approach, MCNet, addresses this issue by learning nonlinear calibration functions with monotonic neural networks. Additionally, MCNet can naturally achieve context awareness by incorporating the context feature as input to its monotonic calibration function. Moreover, MCNet is equipped with a novel field-balance regularizer to ensure more balanced calibration performance across various fields.", "2.2 Monotonic Neural Networks": "Monotonic neural networks are models that exhibit monotonicity with respect to some or all inputs [5, 21]. Pioneering methods like MIN-MAXnetworks [22] achieve monotonicity through monotonic linear embeddings and max-min-pooling. Daniels and Velikova [5] extended this approach to construct partially monotone networks that are monotonic with respect to a subset of inputs. However, these methods can be challenging to train, which limits their practical adoption. Further, deep lattice networks (DLNs) [30] is designed to combine linear embeddings, lattices, and piecewise linear functions to build partially monotone models. Other recent work, such as UMNN [27] and LMN [18], ensures monotonicity by learning a function whose derivative is strictly positive. Our work is inspired by monotonic neural networks, and is generally applicable to any implementation of monotonic neural networks.", "3 Preliminaries": "", "3.1 Problem Formulation": "In this paper, we study the problem of uncertainty calibration and formulate it from the perspective of binary classification. In binary classification, the aim is to predict the label \ud835\udc66 \u2208 { 0 , 1 } of a data sample given its feature vector \ud835\udc99 \u2208 X by learning a predictor \ud835\udc54 (\u00b7) with a labeled training dataset. Then, given a data sample \ud835\udc99 , the predicted probability of positive label is \u02c6 \ud835\udc5d = \ud835\udc54 ( \ud835\udc99 ) , where the positive label refers to 1 and the negative label refers to 0. Both click-through rate prediction [4, 33] and conversion rate prediction [3, 23] can be considered as binary classification task. Currently, the most widely used predictors such as logistic regression [16] and deep neural networks [34] are not well calibrated [6, 13]. It means that the predicted probability \u02c6 \ud835\udc5d could not accurately represent the true probability of the event \ud835\udc38 [ \ud835\udc4c | \ud835\udc4b ] defined as  To tackle this problem, the post-hoc calibration, as a common paradigm, fixes the base predictor and learns a new mapping function to transform the raw model output \u02c6 \ud835\udc5d into the calibrated probability. Specifically, the aim of uncertainty calibration in post-hoc calibration is to find a function \ud835\udc53 \u2217 that takes the predicted score from \ud835\udc54 as input such that the calibration error could be minimized, i.e.,  In practice, the calibration function \ud835\udc53 is learned based on a validation dataset D val = {( \ud835\udc99 ( \ud835\udc56 ) , \ud835\udc66 ( \ud835\udc56 ) )} \ud835\udc41 \ud835\udc56 = 1 with \ud835\udc41 samples. It can be a parametric or non-parametric function. To evaluate the calibration performance, Eq. (2) is not a feasible metric due to the unobservable event likelihood \ud835\udc38 [ \ud835\udc4c | \ud835\udc4b = \ud835\udc99 ] . A common practice is to utilize the empirical data to approximate the true likelihood and quantify the calibration performance. Many metrics have been proposed. Predicted click over click (PCOC) [9, 11], as the most commonly used metric, calculates the ratios of the average calibrated probability and the posterior probability as  where D = {( \ud835\udc99 ( \ud835\udc56 ) , \ud835\udc66 ( \ud835\udc56 ) )} | D| \ud835\udc56 = 1 is the test dataset. PCOC is insufficient to evaluate the calibration performance, since it neither considers the distribution of calibrated probabilities nor takes into account the field information. To improve it, many more fine-grained metrics have been proposed. For example, calibration\ud835\udc41 [6] and probabilitylevel calibration error (Prob-ECE) [17] make use of the calibrated distribution based on binning method. Further, a more reliable metric, i.e., field-level relative calibration error (F-RCE) is proposed [19], which is a weighted sum of the average bias of predictions in each data subset divided by the true outcomes as  where \ud835\udc50 represents a specific field feature (which is usually a part of feature vector \ud835\udc99 ), \ud835\udc41 \ud835\udc50 is the number of samples of field \ud835\udc50 with \u02dd | C | \ud835\udc50 = 1 \ud835\udc41 \ud835\udc50 = |D| , \ud835\udf16 is a small positive number (e.g., \ud835\udf16 = 0 . 01) to avoid division by zero, and I \ud835\udc50 (\u00b7) is an indicator function with value as 1 if the input is \ud835\udc50 otherwise 0.", "3.2 Key Properties of Calibration": "To learn a well-performed calibration function, several key characteristics should be carefully considered and balanced. Expressiveness . The underlying data distribution in real scenarios can be highly complex, and the discrepancy between this distribution and the learned base predictor \ud835\udc54 can be substantial. Therefore, the mapping function \ud835\udc53 must be sufficiently expressive to facilitate the complex nonlinear transformations required to calibrate uncalibrated scores to the true data distribution. Order-Preserving . This suggests that the calibrated probability output by \ud835\udc53 should preserve the order of the original scores produced by the uncalibrated model \ud835\udc54 . Typically, the base predictor \ud835\udc54 is a strong deep neural network that excels in ranking tasks. For instance, sophisticated deep models are widely used in CTR prediction in industry [33, 35]. This property allows us to improve the predicted probability while maintaining the ranking performance. WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia Quanyu Dai et al. Context-Awareness . In many applications, the trained model needs to handle data samples from various contexts (e.g., domains or categories) with significantly different distributions. The calibration function \ud835\udc53 should incorporate context information to achieve adaptive calibration. However, this property can conflict with the order-preserving property. Specifically, for samples in different contexts, ground-truth probabilities can differ despite the same uncalibrated scores due to different miscalibration issues, thus violating the order-preserving property. Consequently, careful balancing of these two properties is essential. Field-Balance . It is crucial for the calibration model to perform consistently across different fields (e.g., domains or categories). Inconsistent performance can lead to various issues. For example, in an online advertising platform, if the calibration model \ud835\udc53 overestimates the probabilities for samples from certain fields while underestimating them for others, it can result in overexposure in some fields and underexposure in others. This imbalance can cause unfairness and negatively impact the ad ecosystem.", "4 Our Approach": "In this section, we propose a novel hybrid approach, M onotonic C alibration Net works ( MCNet ), for uncertainty calibration. We begin by motivating the proposal of MCNet through an analysis of existing methods, followed by a detailed description of our method.", "4.1 Analysis and Motivation": "To begin with, we provide a discussion of some representative methods in post-hoc paradigm in terms of the key properties. We also intuitively demonstrates the calibration function of different methods with a toy example in Figure 1. Binning-based methods. The binning-based methods, such as Histogram Binning [31], first divide the samples into multiple bins according to the sorted uncalibrated probabilities (in ascending order), and then obtain the calibrated probability within each bin by computing the bin's posterior probability. Suppose the samples are divided into \ud835\udc3e bins as {[ \ud835\udc4f 0 , \ud835\udc4f 1 ) , \u00b7 \u00b7 \u00b7 , [ \ud835\udc4f \ud835\udc58 -1 , \ud835\udc4f \ud835\udc58 ) , \u00b7 \u00b7 \u00b7 , [ \ud835\udc4f \ud835\udc3e -1 , \ud835\udc4f \ud835\udc3e )} . Then, the calibration function can be formulated as  where I [ \ud835\udc4f \ud835\udc58 -1 ,\ud835\udc4f \ud835\udc58 ) (\u00b7) is an indicator with value as 1 if the input falls into [ \ud835\udc4f \ud835\udc58 -1 , \ud835\udc4f \ud835\udc58 ) otherwise 0. Essentially, the calibration function is a piecewise constant function (see Fig. 1a). It gives the same calibrated probability to all samples of the same bin, and thus loses the ranking information. Besides, they could not achieve context-awareness, and do not have any mechanism to enhance field-balance. Hybrid methods. The hybrid methods integrate the binning and scaling methods into a unified solution so as to make full use of their advantages, which have achieved state-of-the-art performance. Representative approaches, such as SIR [6], NeuCalib [19], and AdaCalib [28], have piecewise linear calibration functions (see Fig. 1b and 1c). They first obtain multiple bins similarly as the binning-based methods, and then learn a linear calibration function for each bin. The calibration function can be formalized as  Figure 1: Illustration of different calibration functions. (a) Histogram Binning Probability Uncalibrated Score Uncalibrated Score (b) SIR (c) NeuCalib or AdaCalib Uncalibrated Score Uncalibrated Score (d) MCNet (ours) Ground Truth Calibration Error Posterior Probability Probability Probability where { \ud835\udc4f \ud835\udc58 } \ud835\udc3e + 1 \ud835\udc58 = 0 are binning boundaries and { \ud835\udc4e \ud835\udc58 } \ud835\udc3e + 1 \ud835\udc58 = 0 are learned differently for hybrid models. Specifically, SIR directly calculates { \ud835\udc4e \ud835\udc58 } \ud835\udc3e + 1 \ud835\udc58 = 0 from the statistics of the validation dataset, NeuCalib denotes them as learnable model parameters, and AdaCalib applies neural networks to learn them, respectively. Although these existing methods gradually use more and more complicated functions to learn { \ud835\udc4e \ud835\udc58 } \ud835\udc3e + 1 \ud835\udc58 = 0 , their calibration functions are essentially linear and have limited expressiveness. Besides, NeuCalib and AdaCalib rely on similar additional order-preserving constraints to keep the order information, and do not consider field-balance. Motivation. Figure 1 provides an intuitive illustration of the calibration errors of existing methods. As we can see, these methods, because of their limited expressiveness, are unable to achieve perfect uncertainty calibration when confronted with complex nonlinear transformation relations. This motivates us to design a more expressive calibration function with stronger modeling capabilities.", "4.2 Monotonic Calibration Networks": "As motivated above, we propose an expressive Monotonic Calibration Network that has the capacity to learn a perfect uncertainty calibration function, as shown in Figure 1d. MCNet is a hybrid method consisting of two phases, i.e., the binning phase and the scaling phase. In the binning phase, the validation samples in D \ud835\udc63\ud835\udc4e\ud835\udc59 are divided into \ud835\udc3e bins with equal frequency, and the interval for the \ud835\udc58 -th bin is [ \ud835\udc4f \ud835\udc58 -1 , \ud835\udc4f \ud835\udc58 ) . \ud835\udc4f 0 and \ud835\udc4f \ud835\udc3e are two pre-defined numbers to ensure that all samples can be assigned to a specific bin. In the scaling phase, \ud835\udc3e calibration functions are designed and learned for the \ud835\udc3e bins, respectively. Figure 2 shows the architecture of our proposed Monotonic Calibration Network, which aims to achieve accurate calibration. MCNet relies on three key designs: a monotonic calibration function, an order-preserving regularizer, and a field-balance regularizer. Next, we provide a detailed introduction to each of these components. 4.2.1 Monotonic Calibration Function. The monotonic calibration function (MCF) is built upon monotonic neural networks (MNNs) [5, MCNet: Monotonic Calibration Networks for Expressive Uncertainty Calibration in Online Advertising WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia Figure 2: Model architecture of MCNet. MCNet jointly models the uncalibrated score and the context feature to learn a monotonic calibration function. Given a specific context feature (e.g., context 1 and 2), MCNet generates the calibrated probabilities that are context-adaptive and monotonically increasing with the corresponding uncalibrated probabilities. Sample Feature Uncalib Prob Calibrated Prob Uncalibrated Model Monotonic Calibration Network (MCNet) Context Feature Sample Uncalib Prob Uncalib Prob Sample Uncalib Prob Context Feature Monotonic Non-monotonic Calibrated Prob Uncalib Prob Calibrated Prob Uncalib Prob \u0db1 \u0db1 Context 1 Context 2 18, 27], which inherently possess the property of monotonicity and serve as strong approximators of the true data distribution. In this work, we implement the MCF based on an unconstrained MNN [27], which is designed with an architecture that ensures its derivative is strictly positive, thereby achieving the desired monotonicity. Denote a sample as ( \ud835\udc99 , \ud835\udc50, \ud835\udc66 ) where \ud835\udc99 represents all features and \ud835\udc50 ( \ud835\udc50 \u2208 C = { \ud835\udc50 1 , \ud835\udc50 2 , \u00b7 \u00b7 \u00b7 , \ud835\udc50 | C | } ) represents a specific field feature (which is usually a part of \ud835\udc99 ). Here, the field feature \ud835\udc50 is used as the context feature without loss of generality, and can be readily replaced with any other features. The proposed monotonic calibration function can jointly model the uncalibrated scores and context features with a flexible model architecture, thus achieving context-awareness. Compared with AdaCalib, which learns the calibration function for different fields independently, the MCF is shared by all fields within the same bin, making it more data-efficient. Specifically, the calibration function MCF of MCNet for the \ud835\udc58 -th bin is formulated as follows:  where \ud835\udc53 \ud835\udc58 1 (\u00b7 ; \ud835\udeaf \ud835\udc58 1 ) , \ud835\udc53 \ud835\udc58 2 (\u00b7 ; \ud835\udeaf \ud835\udc58 2 ) , and \u210e \ud835\udc58 (\u00b7 ; \ud835\udebd \ud835\udc58 ) are parametric functions, and \ud835\udeaf \ud835\udc58 1 , \ud835\udeaf \ud835\udc58 2 , and \ud835\udebd \ud835\udc58 are the corresponding model parameters. \u210e \ud835\udc58 (\u00b7 ; \ud835\udebd \ud835\udc58 ) is an embedding function that transforms the input context feature id into embedding vectors. Both \ud835\udc53 \ud835\udc58 1 and \ud835\udc53 \ud835\udc58 2 can be implemented with any neural networks. The MCF consists of an integration term and a bias term. Most importantly, the integration term jointly considers the uncalibrated score \ud835\udc54 ( \ud835\udc99 ) and the context feature \ud835\udc50 in a natural way to achieve both monotonicity and context-awareness. Figure 2 illustrates this term within the dashed box on the right side and demonstrates its properties intuitively. Specifically, \ud835\udc53 \ud835\udc58 1 is the derivative of \ud835\udc53 \ud835\udc58 with respect to the input \ud835\udc54 ( \ud835\udc99 ) , and it is designed to be a strictly positive parametric function, thus ensuring the function is monotonically increasing given the same context and contextually adaptive across different contexts. In experiments, we leverage the sigmoid as the activation function for \ud835\udc53 \ud835\udc58 1 to ensure the positiveness of its outputs. The bias term \ud835\udc53 \ud835\udc58 2 is designed to further capture the contextual information, and does not impact the monotonicity with respect to \ud835\udc54 ( \ud835\udc99 ) . It should be noted that the context feature \ud835\udc50 is an optional input depending on whether contextual information plays an important role in the application. Then, the overall calibration function of \ud835\udc3e bins is formulated as  where I [ \ud835\udc4f \ud835\udc58 -1 ,\ud835\udc4f \ud835\udc58 ) (\u00b7) is an indicator with value as 1 if the input falls into [ \ud835\udc4f \ud835\udc58 -1 , \ud835\udc4f \ud835\udc58 ) otherwise 0. Theorem 4.1 ( Expressiveness ). If the uncalibrated scores possess accurate order information and the ground truth calibration function is continuously differentiable, then the monotonic calibration function \ud835\udc53 (\u00b7) serves as a universal approximator of the ground truth function. This theorem suggests that MCNet is capable of learning the perfect nonlinear calibration function under certain conditions. We provide the proof of Theorem 4.1 in Appendix A.1. To train MCNet, we quantify the instance-level calibration error with the negative log-likelihood loss, which is formulated as  where \ud835\udc5d ( \ud835\udc56 ) = \ud835\udc53 GLYPH<0> \ud835\udc54 ( \ud835\udc99 ( \ud835\udc56 ) ) , \ud835\udc50 ( \ud835\udc56 ) GLYPH<1> is the calibrated probability obtained via the MCF. 4.2.2 Order-Preserving Regularizer. The proposed monotonic calibration function (Eq. (7)) can ensure the monotonicity within samples of the same bin, while samples across different bins are not constrained. To address this issue, we design a regularizer to encourage the monotonicity between different bins as follows  It penalizes calibration functions of the two adjacent bins that violate the monotonicity, i.e., the ending of the \ud835\udc58 -th calibration curve (which is located between \ud835\udc4f \ud835\udc58 -1 and \ud835\udc4f \ud835\udc58 ) is greater than the beginning of the ( \ud835\udc58 + 1 ) -th one. With this regularizer, MCNet can preserve the order information of all samples if no context feature WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia Quanyu Dai et al. is given, i.e., the output of \u210e \ud835\udc58 is set to an all-zero vector. If the field id is used as the context feature for calibration, MCNet can also maintain the order-preserving property within each field, while it might achieve better calibration performance for each specific field due to the consideration of context features. 4.2.3 Field-Balance Regularizer. In many real scenarios, it is highly important to keep a balance of the calibration performance among different fields. For example, in online advertising platform, a balanced performance can enhance fairness of different bidders and improve the healthiness of the business ecosystem. PCOC and its variants are widely leveraged to quantify the calibration performance [6]. However, it can easily cause high variance issue if directly used as the evaluation metric in our regularizer due to its division operation. To avoid this issue, we design a new metric for quantifying the calibration performance for field \ud835\udc50 as follows  where I \ud835\udc50 (\u00b7) is an indicator function with value as 1 if the input is \ud835\udc50 otherwise 0. Different from PCOC, it computes the diference between the calibrated probability and the posterior probability of the data through subtraction, which can quantify both overestimation and underestimation as PCOC. To enhance the balance of calibration performance among different fields, we use the standard variance of DIFF of different fields as the field-balance regularizer  where \ud835\udc37\ud835\udc3c\ud835\udc39\ud835\udc39 is the mean of { \ud835\udc37\ud835\udc3c\ud835\udc39\ud835\udc39 \ud835\udc56 } | C | \ud835\udc56 = 1 . The overall training loss for MCNet is formulated as  where \ud835\udefd and \ud835\udefc are hyperparameters to control the importance of the regularization terms. 4.2.4 Discussion. MCNet flexibly balances the properties of orderpreserving and context-awareness. Specifically, the calibration function \ud835\udc53 \ud835\udc58 is strictly monotonically increasing with respect to the input uncalibrated score, thereby perfectly preserving the order information for samples of the same context within the same bin. Further, the relative order of samples across bins can be easily constrained using the order-preserving regularizer. For samples of different contexts, the ground-truth probability can differ even with the same uncalibrated score, due to different miscalibration issues. Our MCNet enables context-adaptive calibration by naturally modeling the contextual information. Fine-grained context features will possibly benefit the calibration performance, while posing challenges for preserving the ranking information of the base predictor. For scenarios where context features convey limited information, MCNet can directly disregard them and preserve the order information across contexts.", "4.3 Training Algorithm": "The proposed MCNet cannot be trivially trained with stochastic gradient descent (SGD) methods due to the existence of integration operation in Eq. (7). To tackle this problem, we use Clenshaw-Curtis quadrature (CCQ) [8, 25] to compute the forward and backward integration. In CCQ, the integration is computed by constructing a polynomial approximation, involving \ud835\udc47 forward operation of \ud835\udc53 1 . Thus, the complexity of MCNet is a constant times of a normal neural network positively depending on \ud835\udc47 . Luckily, the \ud835\udc47 forward operations can be computed in parallel, making it time efficient. In the backward step, we integrate the gradient instead of computing the gradient of the integral to avoid storing additional results, making it memory efficient. Thus, CCQ enables the computation of gradients of MCNet efficiently and thus the optimization of it with SGD effectively. Appendix A.2 provides the detailed training algorithms based on CCQ as well as the empirical analysis of timeand memory-efficiency.", "5 Experiments": "In this section, extensive experiments are conducted to investigate the following research questions (RQs). \u00b7 RQ1: How does MCNet perform on the calibration tasks compared with the state-of-the-art baseline approaches? \u00b7 RQ2: What are the effects of the auxiliary neural network and the field-balance regularizer on the performance of MCNet? \u00b7 RQ3: What are the strengths of nonlinear calibration functions learned by MCNet? In the Appendix, we further provide the hyperparameter sensitivity analysis (Appendix B.3) and the analysis on MCNet's robustness against overfitting (Appendix B.4).", "5.1 Experimental Setup": "5.1.1 Datasets. The experiments are conducted on two large-scale datasets: one public dataset (AliExpress 1 ) and one private industrial dataset (Huawei Browser). Both datasets are split into three subsets for training, validation, and testing, respectively. For AliExpress, the field feature \ud835\udc50 is set as the country where the data are collected. With such a categorical feature as the field information, AliExpress can be divided into 4 fields (i.e., 4 disjoint subsets), representing the 4 countries. The Huawei Browser dataset is extracted directly from the Huawei online advertising system with samples across 9 days. It is partitioned into 3 fields, indicating the 3 advertisement sources. More detailed descriptions are provided in Appendix B.1. 5.1.2 Baselines. We make comparisons with three categories of baselines. (1) Binning-based methods: Histogram Binning [31] and Isotonic Regression [32]. (2) Scaling-based methods: Platt Scaling [20], Gaussian Scaling [15], and Gamma Scaling [15]. (3) Hybrid methods: SIR [6], NeuCalib [19], and AdaCalib [28]. More detailed decriptions of them are provided in Appendix B.2. Our method, MCNet , is also a hybrid approach, including two variants: MCNet-None and MCNet-Field . MCNet-None is a variant without considering the context feature. The output of embedding function \u210e \ud835\udc58 is set to an all-zero vector. MCNet-Field is a variant that takes the field information as the context feature. 5.1.3 Implementation Details. In our methods, the parametric functions \ud835\udc53 1 (\u00b7) and \ud835\udc53 2 (\u00b7) for each bin are implemented as multilayer perceptions (MLPs) with two 128-dimensional hidden layers. \u210e (\u00b7) 1 https://tianchi.aliyun.com/dataset/74690 MCNet: Monotonic Calibration Networks for Expressive Uncertainty Calibration in Online Advertising WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia Table 1: Results on the AliExpress and Huawei Browser datasets. The highest results in each column are in boldface, the second-best values are underlined, and the values inside '()' represent the standard deviation across three different runs. is an embedding lookup table with an embedding dimension of 128. The balance coefficient \ud835\udefd in Eq. (13) is set as 1. The proposed calibration models are trained using the Adam optimizer [14] with a batch size of 2048. The default learning rates for MCNet-None and MCNet-Field are 1e-5 and 1e-4, respectively. The base predictor \ud835\udc54 (\u00b7) is deep click-through rate prediction model [26]. The baseline calibration approaches are implemented based on their papers. We set the number of bins to 20 for all methods that require binning. The base predictor is trained on the training set. All calibration methods are trained using the validation set and evaluated on the test set. We conduct both the CTR and CVR calibration on the AliExpress and Huawei Browser datasets. We choose PCOC as the field-agnostic calibration metric, F-RCE as the field-level calibration metric, and AUC score as the ranking metric.", "5.2 Performance Study (RQ1)": "best baseline, indicating MCNet-None is more expressive to approximate the posterior probabilities. On the Huawei Browser dataset, MCNet-None still obtains competitive calibration performance on both tasks. With the field information incorporated, MCNet-Field achieves the lowest F-RCE scores on both CTR and CVR tasks and the second-best PCOC on the CTR task. Thus, MCNet-None and MCNet-Field are more favorable on AliExpress and Huawei Browser, respectively, and they can be chosen based on their performance on a specific task. Since MCNet-Field incorporates the field features, priority can be given to MCNet-Field if the field features are vital. Moreover, on these two datasets, both MCNet-None and MCNet-Field maintain AUC scores the same as or close to those of the base predictor, thus largely preserving the original ranking of uncalibrated probabilities. Therefore, our methods, MCNet-None and MCNet-Field, excel in uncertainty calibration by learning nonlinear calibration functions with monotonic neural networks. As shown in Table 1, compared with the base predictor (i.e., Base), all approaches have improved calibration metrics (i.e., PCOC and F-RCE) on both datasets. For example, on the AliExpress dataset, MCNet-None significantly reduces the F-RCE from 16.10% to 1.77% on the CTR task and from 34.21% to 10.07% on the CVR task. In comparison with the baseline calibration approaches, except for the PCOC on Huawei Browser, our method MCNet yields the best PCOC and F-RCE scores in all other cases. As the base predictor has already achieved a PCOC score close to 1 (the optimal value) on Huawei Browser, there is not much room for further improvement by calibration methods, including our method MCNet. MCNet-None yields the best PCOC and F-RCE on both CTR and CVR tasks based on results of the paired-t-test compared with the", "5.3 Ablation Study (RQ2)": "5.3.1 Study on the auxiliary neural network. An auxiliary neural network can be incorporated into MCNet as NeuCalib and AdaCalib, serving as an optional component to improve the ranking performance. A detailed illustration is provided in Appendix A.3. By comparing the results in Table 2 and Table 1, it is observed that the auxiliary network increases the AUC scores of MCNetNone and MCNet-Field in most cases. For instance, on the CTR task of Huawei Browser dataset, the relative AUC improvements obtained by the auxiliary network are 1.10% for MCNet-None and 0.82% for MCNet-Field, respectively. Similar observations can be found in NeuCalib and AdaCalib. Hence, the auxiliary network can WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia Quanyu Dai et al. Table 2: Ablation study on the auxiliary neural network. Table 3: Ablation study on the field-balance regularizer with CTR task on AliExpress. 'All' and 'STD' denote the overall PCOC and the PCOC standard deviation of the four fields, respectively. * M-N: MCNet-None, M-F: MCNet-Field, N: NeuCalib, A: AdaCalib, L \ud835\udc4f : L \ud835\udc4f\ud835\udc4e\ud835\udc59\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52 . improve the ranking ability of a calibration model. However, the calibration metrics (i.e., PCOC and F-RCE) sometimes get worse with the auxiliary network integrated. For example, worse values of PCOC and F-RCE can be seen in MCNet-None-Aux compared with MCNet-None. It reveals that the auxiliary network may have a negative impact on a model's calibration ability. The reason is that, as a multi-layer perception, the auxiliary network also suffers from the miscalibration issue like the base predictor. 5.3.2 Study on the field-balance regularizer. Table 3 shows the results with the field-balance regularizer (i.e., L \ud835\udc4f\ud835\udc4e\ud835\udc59\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52 , see Section 4.2.3) applied. A field's PCOC reveals the calibration performance on this field. It can be seen that the field-balance regularizer reduces the PCOC standard deviations of all calibration approaches. For example, the PCOC standard deviations of MCNet-None and MCNet-Field decrease from 3.56% to 2.93% and from 2.36% to 1.81%, respectively. Such observations demonstrate that the field-balance regularizer can improve the balance of calibration performance on different fields, thus promoting the fairness of the product ecosystem. In addition, under the field-balance regularizer, the overall PCOC is improved in most cases. Moreover, the field-balance regularizer achieves a reduced or comparable F-RCE, and maintains the AUC score. Thus, the field-balance regularizer can keep the calibration and ranking metrics while promoting the field-balance.", "5.4 Calibration Function Analysis (RQ3)": "Figure 3 shows the calibration functions of MCNet-None and three baselines, i.e., SIR, NeuCalib, and AdaCalib. These calibration functions are learned using the validation set. The black bar is the posterior probability of test samples in each bin. The bin number Figure 3: Visualization of calibration functions. Method PCOC F-RCE SIR 0.7178 20.00% NeuCalib 0.8477 10.78% AdaCalib 0.8990 7.14% MCNet-None 1.0035 1.02% is set as 10. As introduced in Section 4.1, the piecewise linear calibration function of SIR is constructed directly using the posterior probabilities of the validation set. Consequently, the two ends of each line are within the bins. The ordinate value of each endpoint is the bin's posterior probability. By comparing the calibration curve of SIR and the posterior statistics of the test set, it can be observed that the validation and test sets have distinct distributions of posterior probabilities. In comparison with the baseline approaches, the calibration function of MCNet-None can more closely approximate the posterior probabilities of the test set. This point can also be supported by the preferable PCOC and F-RCE of MCNet-None. Note that, in MCNet-None, the gap between two adjacent calibration curves is caused by the order-preserving regularizer (see Section 4.2.2). With such an order-preserving regularizer, the calibrated probabilities are non-decreasing, thus keeping the original ranking of uncalibrated probabilities.", "6 Conclusion": "We have proposed a novel hybrid method, Monotonic Calibration Networks (MCNet), to tackle the current challenges in uncertainty calibration for online advertising. MCNet is equipped with three key designs: a monotonic calibration function (MCF), an orderpreserving regularizer, and a field-balance regularier. The proposed MCF is capable of learning complex nonlinear relations by leveraging expressive monotonic neural networks. In addition, its flexible architecture enables efficient joint modeling of uncalibrated scores and context features, facilitating effective context-awareness. The two proposed regularizers further enhance MCNet by improving the monotonicity increasing property for preserving order information and the field-balanced calibration performance, respectively. Finally, extensive experiments are conducted on both public and industrial datasets to demonstrate the superiority of MCNet in CTR and CVR tasks. MCNet: Monotonic Calibration Networks for Expressive Uncertainty Calibration in Online Advertising", "References": "[1] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al. 2016. End to end learning for self-driving cars. arXiv preprint arXiv:1604.07316 (2016). [2] Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. 2015. Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission. In Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining . 1721-1730. [3] Quanyu Dai, Haoxuan Li, Peng Wu, Zhenhua Dong, Xiao-Hua Zhou, Rui Zhang, Rui Zhang, and Jie Sun. 2022. A generalized doubly robust learning framework for debiasing post-click conversion rate prediction. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 252-262. [4] Quanyu Dai, Yalei Lv, Jieming Zhu, Junjie Ye, Zhenhua Dong, Rui Zhang, Shu-Tao Xia, and Ruiming Tang. 2022. LCD: Adaptive label correction for denoising music recommendation. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management . 3903-3907. [5] Hennie Daniels and Marina Velikova. 2010. Monotone and partially monotone neural networks. IEEE Transactions on Neural Networks 21, 6 (2010), 906-917. [6] Chao Deng, Hao Wang, Qing Tan, Jian Xu, and Kun Gai. 2021. Calibrating user response predictions in online advertising. In Machine Learning and Knowledge Discovery in Databases: Applied Data Science Track: European Conference, ECML PKDD 2020, Ghent, Belgium, September 14-18, 2020, Proceedings, Part IV . Springer, 208-223. [7] Jakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi, Mohsin Ali, Jongseok Lee, Matthias Humt, Jianxiang Feng, Anna Kruspe, Rudolph Triebel, Peter Jung, Ribana Roscher, et al. 2023. A survey of uncertainty in deep neural networks. Artificial Intelligence Review 56, Suppl 1 (2023), 1513-1589. [8] W Morven Gentleman. 1972. Implementing Clenshaw-Curtis quadrature, I methodology and experience. Commun. ACM 15, 5 (1972), 337-342. [9] Thore Graepel, Joaquin Quinonero Candela, Thomas Borchert, and Ralf Herbrich. 2010. Web-scale bayesian click-through rate prediction for sponsored search advertising in microsoft's bing search engine. Omnipress. [10] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. 2017. On calibration of modern neural networks. In International conference on machine learning . PMLR, 1321-1330. [11] Xinran He, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Yanxin Shi, Antoine Atallah, Ralf Herbrich, Stuart Bowers, et al. 2014. Practical lessons from predicting clicks on ads at facebook. In Proceedings of the eighth international workshop on data mining for online advertising . 1-9. [12] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. 1989. Multilayer feedforward networks are universal approximators. Neural networks 2, 5 (1989), 359-366. [13] Siguang Huang, Yunli Wang, Lili Mou, Huayue Zhang, Han Zhu, Chuan Yu, and Bo Zheng. 2022. MBCT: Tree-Based Feature-Aware Binning for Individual Uncertainty Calibration. In Proceedings of the ACM Web Conference 2022 . 22362246. [14] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , Yoshua Bengio and Yann LeCun (Eds.). [15] Wonbin Kweon, SeongKu Kang, and Hwanjo Yu. 2022. Obtaining Calibrated Probabilities with Personalized Ranking Models. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 36. 4083-4091. [16] H. Brendan McMahan, Gary Holt, David Sculley, Michael Young, Dietmar Ebner, Julian Grady, Lan Nie, Todd Phillips, Eugene Davydov, Daniel Golovin, Sharat Chikkerur, Dan Liu, Martin Wattenberg, Arnar Mar Hrafnkelsson, Tom Boulos, and Jeremy Kubica. 2013. Ad Click Prediction: a View from the Trenches. In Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD) . 1222-1230. WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia [17] Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. 2015. Obtaining well calibrated probabilities using bayesian binning. In Proceedings of the AAAI conference on artificial intelligence , Vol. 29. [18] Niklas Nolte, Ouail Kitouni, and Mike Williams. 2023. Expressive Monotonic Neural Networks. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 . [19] Feiyang Pan, Xiang Ao, Pingzhong Tang, Min Lu, Dapeng Liu, Lei Xiao, and Qing He. 2020. Field-aware calibration: a simple and empirically strong method for reliable probabilistic predictions. In Proceedings of The Web Conference 2020 . 729-739. [20] John Platt et al. 1999. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in large margin classifiers 10, 3 (1999), 61-74. [21] Davor Runje and Sharath M Shankaranarayana. 2023. Constrained monotonic neural networks. In International Conference on Machine Learning . PMLR, 2933829353. [22] Joseph Sill. 1997. Monotonic networks. Advances in neural information processing systems 10 (1997). [23] Yumin Su, Liang Zhang, Quanyu Dai, Bo Zhang, Jinyao Yan, Dan Wang, Yongjun Bao, Sulong Xu, Yang He, and Weipeng Yan. 2021. An attention-based model for conversion rate prediction with delayed feedback via post-click calibration. In Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence . 3522-3528. [24] Sunil Thulasidasan, Gopinath Chennupati, Jeff A Bilmes, Tanmoy Bhattacharya, and Sarah Michalak. 2019. On mixup training: Improved calibration and predictive uncertainty for deep neural networks. Advances in Neural Information Processing Systems 32 (2019). [25] J\u00f6rg Waldvogel. 2006. Fast Construction of the Fej\u00e9r and Clenshaw-Curtis Quadrature Rules. Seminar for Applied Mathematics, Swiss Federal Institute of Technology ETH, CH-8092 Zurich (2006). [26] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network for ad click predictions. In Proceedings of the ADKDD'17 . 1-7. [27] Antoine Wehenkel and Gilles Louppe. 2019. Unconstrained monotonic neural networks. Advances in neural information processing systems 32 (2019). [28] Penghui Wei, Weimin Zhang, Ruijie Hou, Jinquan Liu, Shaoguo Liu, Liang Wang, and Bo Zheng. 2022. Posterior Probability Matters: Doubly-Adaptive Calibration for Neural Predictions in Online Advertising. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval . 2645-2649. [29] Xue Ying. 2019. An overview of overfitting and its solutions. In Journal of physics: Conference series , Vol. 1168. IOP Publishing, 022022. [30] Seungil You, David Ding, Kevin Canini, Jan Pfeifer, and Maya Gupta. 2017. Deep lattice networks and partial monotonic functions. Advances in neural information processing systems 30 (2017). [31] Bianca Zadrozny and Charles Elkan. 2001. Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers. In Proceedings of the Eighteenth International Conference on Machine Learning , Vol. 1. 609-616. [32] Bianca Zadrozny and Charles Elkan. 2002. Transforming classifier scores into accurate multiclass probability estimates. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining . 694-699. [33] Jieming Zhu, Quanyu Dai, Liangcai Su, Rong Ma, Jinyang Liu, Guohao Cai, Xi Xiao, and Rui Zhang. 2022. Bars: Towards open benchmarking for recommender systems. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval . 2912-2923. [34] Jieming Zhu, Qinglin Jia, Guohao Cai, Quanyu Dai, Jingjie Li, Zhenhua Dong, Ruiming Tang, and Rui Zhang. 2023. Final: Factorized interaction layer for ctr prediction. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval . 2006-2010. [35] Jieming Zhu, Jinyang Liu, Shuai Yang, Qi Zhang, and Xiuqiang He. 2021. Open Benchmarking for Click-Through Rate Prediction. In The 30th ACM International Conference on Information and Knowledge Management (CIKM) . 2759-2769. WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia Quanyu Dai et al.", "A Model Details": "", "A.1 Proof of Theorem 4.1": "Theorem A.1 ( Expressiveness ). If the uncalibrated scores possess accurate order information and the ground truth calibration function is continuously differentiable, then the monotonic calibration function \ud835\udc53 (\u00b7) serves as a universal approximator of the ground truth function. Proof. If the uncalibrated scores possess accurate order information, then the true probability is monotonically increasing with respect to the uncalibrated scores. This implies that the ground truth calibration function has a strictly positive derivative. The derivative of the proposed calibration function \ud835\udc53 \ud835\udc58 ( \u02c6 \ud835\udc5d, \ud835\udc50 ) is given by \ud835\udc51 \ud835\udc51 \u02c6 \ud835\udc5d \ud835\udc53 \ud835\udc58 ( \u02c6 \ud835\udc5d, \ud835\udc50 ) = \ud835\udc53 \ud835\udc58 1 ( \u02c6 \ud835\udc5d, \u210e \ud835\udc58 ( \ud835\udc50 )) , which is strictly positive, as guaranteed by the output activation. Based on [12], a multi-layer feedforward network with sufficient hidden units is a universal approximator for our problem. Thus, the function \ud835\udc53 \ud835\udc58 1 (\u00b7) can be implemented with multi-layer feedforward networks with sufficient hidden units, allowing it to readily accommodate any positive continuous functions. Hence, \ud835\udc53 \ud835\udc58 ( \u02c6 \ud835\udc5d, \ud835\udc50 ) can precisely match the groundtruth calibration function of the \ud835\udc58 -th bin. \u25a1", "A.2 Algorithm and Complexity": "Algorithm 1 and Algorithm 2 show the detailed training procedures for both forward and backward integration with Clenshaw-Curtis quadrature (CCQ) [25, 27], respectively. In CCQ, the integration is computed by constructing a polynomial approximation, involving \ud835\udc47 (set to 50 empirically) forward operation of \ud835\udc53 1 . Thus, the complexity of MCNet is a constant times of a normal neural network positively depending on \ud835\udc47 . Luckily, the \ud835\udc47 forward operations can be computed in parallel, making it time efficient. In the backward step, we integrate the gradient instead of computing the gradient of the integral to avoid storing additional results, making it memory efficient. In Table 4, we provide the training time per epoch and the GPU memory consumption of closely-related baselines and our methods, verifying that MCNet is both time- and memory-efficient. Although the training time of MCNet is several times longer than the baseline methods, it is acceptable in practical applications because the calibration dataset is usually much smaller than the training dataset. In addition, the inference time per sample of MCNet-None ( \ud835\udc47 = 50), AdaCalib, and NeuCalib are 12.2e-3, 2.02e-3, and 0.48e-3 milliseconds, respectively. All methods have low inference delay, thereby meeting the requirements for online real-time inference in realworld scenarios.", "A.3 Auxiliary Neural Network": "An auxiliary neural network can be incorporated into MCNet as NeuCalib and AdaCalib, serving as an optional component to improve the ranking performance. The auxiliary neural network takes the sample feature vectors as inputs. When the auxiliary network is used, MCNet takes uncalibrated logit \u02c6 \ud835\udc59 as the input and outputs calibrated logit \ud835\udc59 . The calibrated probability is calculated by adding the outputs of the calibration function and the auxiliary neural network. Figure 4 illustrates the model architecture with an additional auxiliary network incorporated into the MCNet model. Note that the auxiliary network relies on an independent validation set. If the validation set for calibration is a subset of the training set, the auxiliary network should be removed. Table 2 reports the calibration and ranking metrics with an auxiliary neural network (Aux) incorporated into each model, including AdaCalib, NeuCalib, MCNet-None, and MCNet-Field. The auxiliary neural network is implemented as a 2-layer MLP. The experimental settings are the same as those of Section 5.2. Figure 4: Model architecture with an additional auxiliary network. Sample Feature \ud835\udc99 Uncalibrated Logit \u1218 \ud835\udc59 Calibrated Prob \ud835\udc5d Monotonic Calibration Network (MCNet) Uncalibrated Model Auxiliary Neural Network Context Feature \ud835\udc50 Calibrated Logit \ud835\udc59 activation Table 4: Training time per epoch (min) and GPU memory consumption (MiB).", "B More Experiments": "", "B.1 Experimental Datasets": "The experiments are conducted on two real-world datasets: one public dataset (AliExpress 2 ) and one private industrial dataset (Huawei Browser). We provide the detailed statistics of experimental datasets, i.e., AliExpress and Huawei Browser, in Table 5. For AliExpress, the provided training and test sets are split along the time sequence. Since the timestamp information is not available in the original training set, following AdaCalib [28], we split the original training set to be a new training set and a validation set with a proportion of 3:1. Field feature \ud835\udc50 (see Section 4.2.1) is set as the country where the data are collected. With such a categorical feature as the field 2 https://tianchi.aliyun.com/dataset/74690 MCNet: Monotonic Calibration Networks for Expressive Uncertainty Calibration in Online Advertising WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia", "Algorithm 2: Backward Integration for MCNet with Clenshaw-Curtis quadrature": "Input : \ud835\udc5d : The superior integration bounds. \u210e : The vector that representing embeddings of the input feature id, and \u210e is denoted as \u210e \ud835\udc58 for the \ud835\udc58 -th bin. \u2207 \ud835\udc5c\ud835\udc62\ud835\udc61 : The derivatives of the loss function with respect to \u222b \ud835\udc5d 0 \ud835\udc53 1 ( \ud835\udc61 ; \u210e ; \ud835\udebd ) d \ud835\udc61 for all \ud835\udc5d . : \u2207 \ud835\udebd : The gradient of \u222b \ud835\udc5d 0 \ud835\udc53 1 ( \ud835\udc61 ; \u210e ; \ud835\udebd ) d \ud835\udc61 with respect to the parameters \ud835\udebd of \ud835\udc53 1 . Output \u2207 \u210e : The gradient of \u222b \ud835\udc5d 0 \ud835\udc53 1 ( \ud835\udc61 ; \u210e ; \ud835\udebd ) d \ud835\udc61 with respect to feature embeddings \u210e . Hyperparameters: \ud835\udc53 1 : A derivable function R \u2192 R with model parameters as \ud835\udebd . \ud835\udc47 : The number of integration steps. 1 \ud835\udc64 , \ud835\udeff \ud835\udc5d = compute_clenshaw_curtis_weights( \ud835\udc47 ) // Compute Clenshaw-Curtis weights and evaluation steps 2 \ud835\udc53 , \u2207 \ud835\udebd , \u2207 \u210e = 0 , 0 , 0 3 for \ud835\udc56 = 1 , . . . , \ud835\udc47 do 4 \ud835\udc5d \ud835\udc56 = \ud835\udc5d 0 + 1 2 ( \ud835\udc5d -\ud835\udc5d 0 )( \ud835\udeff \ud835\udc5d [ \ud835\udc56 ] + 1 ) 5 \ud835\udeff \ud835\udc39 = \ud835\udc53 1 ( \ud835\udc5d \ud835\udc56 ; \u210e ; \ud835\udebd ) 6 \ud835\udeff \u2207 \u210e = \u02dd \ud835\udc35 \ud835\udc57 = 1 \u2207 \u210e \ud835\udc57 GLYPH<16> \ud835\udeff \ud835\udc57 \ud835\udc53 GLYPH<17> \u2207 \ud835\udc57 \ud835\udc5c\ud835\udc62\ud835\udc61 ( \ud835\udc5d \ud835\udc57 -\ud835\udc5d \ud835\udc57 0 ) 7 \ud835\udeff \u2207 \ud835\udebd = \u02dd \ud835\udc35 \ud835\udc57 = 1 \u2207 \ud835\udebd GLYPH<16> \ud835\udeff \ud835\udc57 \ud835\udc53 GLYPH<17> \u2207 \ud835\udc57 \ud835\udc5c\ud835\udc62\ud835\udc61 ( \ud835\udc5d \ud835\udc57 -\ud835\udc5d \ud835\udc57 0 ) 8 \u2207 \u210e = \u2207 \u210e + \ud835\udc64 [ \ud835\udc56 ] \ud835\udeff \u2207 \u210e 9 \u2207 \ud835\udebd = \u2207 \ud835\udebd + \ud835\udc64 [ \ud835\udc56 ] \ud835\udeff \u2207 \ud835\udebd 10 end 11 return \u2207 \ud835\udebd , \u2207 \u210e information, AliExpress can be divided into 4 fields (i.e., 4 disjoint subsets), representing the 4 countries. The Huawei Browser dataset is extracted directly from the Huawei online advertising system, which has samples across 9 days. To simulate the real scenario, we split this dataset by the date, i.e., the first 7 days for training, the 8th day for validation, and the 9th day for testing. Huawei Browser dataset is partitioned into 3 fields, indicating the 3 advertisement sources.", "B.2 Baseline Methods": "Wemakecomparisonswiththree categories of baselines. (1) Binningbased methods : Histogram Binning [31] partitions the ranked uncalibrated scores into multiple bins, and assigns the calibrated probability of each bin to be the bin's posterior probability. Isotonic Regression [32] improves over Histogram Binning by merging the adjacent bins to ensure the bin's posterior probability keeps increasing. (2) Scaling-based methods : They design parametric calibration functions with the assumption that the class-conditional scores follow the Gaussian distribution ( Platt Scaling [20] and Gaussian Scaling [15]) or Gamma distribution ( Gamma Scaling [15]). (3) Hybrid methods : These methods borrow ideas from both the binning- and scaling-based methods. SIR [6] constructs calibration functions using isotonic regression and linear interpolation. NeuCalib [19] computes the calibrated probabilities with a linear calibration function and a field-aware auxiliary neural network. AdaCalib [28] learns one linear calibration function for each field using the field's posterior statistics. // Compute the next point to evaluate // Sum up for all samples of the batch the gradients with respect to inputs \u210e // Sum up for all samples of the batch the gradients with respect to parameters \ud835\udebd WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia Quanyu Dai et al. Table 5: Statistics of the AliExpress and Huawei Browser datasets. Table 6: PCOC under every 2 training epochs (10 epochs in total). Compared with MCNet-None, MCNet-Field is more sensitive to bin number while less sensitive to the learning rate and the balance coefficient \ud835\udefd . When varying the bin number, the F-RCE and PCOCofMCNet-None only slightly change. The calibration metrics of MCNet-Field become worse under larger bin numbers while getting improved when the learning rate increases. The optimal learning rates are 1e-5 and 1e-3 for MCNet-None and MCNet-Field, respectively. When changing the value of \ud835\udefd , the calibration metrics of MCNet-Field remain stable, while those of MCNet-None fluctuate. The optimal setting of \ud835\udefd is 1.0 for MCNet-None. Figure 5: Calibration metrics across bin numbers, learning rates, and balance coefficients \ud835\udefd . \u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000 \u0000 \u0000 \u0000\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000\u0000 \u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000 \u0000 \u0000 \u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000", "B.3 Hyperparameter Sensitivity Analysis": "Figure 5 shows the calibration metrics of MCNet under three key hyperparameters, i.e., bin number, learning rate, and balance coefficient \ud835\udefd . Bin number is the number of bins that the validation set is divided. In each bin, a monotonic calibration function is learned for calibration. The learning rate is used to train the calibration models. \ud835\udefd is the balance coefficient in the overall loss function (i.e., Eq. (13)). The experiments are conducted on the AliExpress datasets with the F-RCE and PCOC of the CTR task reported. When investigating one hyperparameter, the remaining hyperparameters are set as the default values described in Section 5.1.3.", "B.4 Model Robustness against Overfitting": "MCNet is designed to be robust against overfitting via three strategies: 1) constraining the calibration function of each bin to be monotonic regarding the uncalibrated scores, 2) applying \ud835\udc3f 2 regularization and employing a small number of training epochs, 3) taking simple inputs, i.e., only the uncalibrated scores (MCNetNone) or together with the context features (MCNet-Field) [29]. The auxiliary neural network is an optional module of MCNet to enhance the ranking performance. To avoid overfitting, the auxiliary network is implemented as a simple 2-layer MLP. Table 6 and Table 7 report the PCOC and AUC scores on the CVR task under every 2 training epochs (10 epochs in total), demonstrating that a training epoch within the range of 2 to 10 has a negligible impact on the final calibration and ranking performance. Therefore, MCNet is robust against overfitting, even with the auxiliary network incorporated. Table 7: AUC under every 2 training epochs (10 epochs in total)."}
