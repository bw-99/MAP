{"M ulti -A uxiliary A ugmented C ollaborative V ariational A uto -encoder for T ag R ecommendation": "Jing Yi 1 , Xubin Ren 1 , and Zhenzhong Chen 1,2,* 1 School of Computer Science, Wuhan University 2 School of Remote Sensing and Information Engineering, Wuhan University", "A bstract": "Recommending appropriate tags to items can facilitate content organization, retrieval, consumption and other applications, where hybrid tag recommender systems have been utilized to integrate collaborative information and content information for better recommendations. In this paper, we propose a multi-auxiliary augmented collaborative variational auto-encoder (MA-CVAE) for tag recommendation, which couples item collaborative information and item multi-auxiliary information, i.e., content and social graph, by defining a generative process. Specifically, the model learns deep latent embeddings from di GLYPH<11> erent item auxiliary information using variational auto-encoders (VAE), which could form a generative distribution over each auxiliary information by introducing a latent variable parameterized by deep neural network. Moreover, to recommend tags for new items, item multi-auxiliary latent embeddings are utilized as a surrogate through the item decoder for predicting recommendation probabilities of each tag, where reconstruction losses are added in the training phase to constrict the generation for feedback predictions via di GLYPH<11> erent auxiliary embeddings. In addition, an inductive variational graph auto-encoder is designed where new item nodes could be inferred in the test phase, such that item social embeddings could be exploited for new items. Extensive experiments on MovieLens and citeulike datasets demonstrate the e GLYPH<11> ectiveness of our method.", "1 INTRODUCTION": "The activity that users annotate items, such as movies and articles, with some key words is called tagging. Online systems like Movielens 1 allow users to tag items. Appropriate tags can facilitate content organization, retrieval, consumption and so on [1, 2, 3, 4, 5]. Tag recommenders aim to recommend suitable tags for items to help users tag more easily, which also facilitates the spread of the items by creating convenient annotations. Therefore, it is very important to recommend suitable tags for items, especially in the Internet era that contains a great number of items. collaborative information and content information have been less explored. Tag recommenders assist the tagging process by utilizing historical interactions between items and tags which are given to the object by online users. Collaborative information contained in item-tag interactions reflects the users' common perception of the object, which has been shown the e GLYPH<11> ectiveness for tag recommendations [6, 7, 8]. Among them, Rendle and SchmidtThieme [8] proposed a pairwise interaction tensor factorization (PITF) model using pairwise interaction tensor factorization and adapted the Bayesian Personalized Ranking (BPR) framework. However, the sparse feedback of item-tag interactions and the cold-start problem in tag recommendations make systems using only collaborative information hard to work. Content-based tag recommendations [9, 10] have been widely studied. Hassan et al. [11] modeled textual content in scientific articles using bidirectional gated recurrent units (bi-GRUs) with word-level and sentence-level attention mechanisms. Tang et al. [10] proposed a content-based tag recommender system that utilized GRU layers to extract semantic and structural features in text content, where tag correlations and tag-content overlapping were considered. However, hybrid methods which could benefit from both Corresponding author: Zhenzhong Chen, E-mail:zzchen@ieee.org How to couple item content information and collaborative information is a challenge. Moreover, the item has multiple auxiliary information. Beyond the content information ( e.g., textual or visual contents of the item), the social graph information, as illustrated in Fig 1, contains plenty of social-oriented information and fully exploiting them can maximally benefit the tag recommendation process. Collaborative topic regression (CTR) [12] used a Bayesian probabilistic model to couple the textual information learned by latent Dirichlet allocation (LDA) and the collaborative information learned by probabilistic matrix factorization (PMF), which has achieved good results. CTRSR [13] extended it by integrating social graph information of items using a linear model with limited representation abilities. Considering the flexibility to integrate di GLYPH<11> erent information of Bayesian probabilistic generative process and the powerful feature learning capabilities of deep learning methods, in this paper, we propose to use deep generative models to learn the latent variables of item collaborative and multi-auxiliary information by defining a generative process to couple them. In this way, the collaborative information contained in item-tag interactions and relevant item auxiliary features could be comprehensively extracted with a deep generative model for tag recommendations. Variational auto-encoder (VAE) and variational graph auto-encoder (VGAE) are employed to model the content and social graph information of items respectively, where a generative distribution over each auxiliary information can be obtained by introducing a latent variable parameterized by deep neural network. Moreover, we substitute the linear PMF model to a multinomial VAE (Mult-VAE) model inspired by [14] to better capture the item collaborative information from the sparse itemtag ratings. Specifically, we utilize a multinomial likelihood variational auto-encoder which assumes the item-by-tag interactions 1 https://movielens.org/ of each item follow a multinomial distribution such that higher probability mass could be put on the observed interactions. The deep latent variables learned from item multi-auxiliary information are injected into the modeling of item latent embeddings that contains collaborative information, which bridges the hybrid information in a unified framework. Variational inference are utilized for parameter estimation. Another challenge comes to how to recommend tags for new items which do not have collaborative information during the test phase. Especially for tag recommendations, new items spring up soon. Therefore, extending hybrid recommenders to deal with totally new items is necessary. To solve the above challenges, we propose a Multi-auxiliary Augmented Collaborative Variational Auto-encoder (MACVAE) for tag recommendations. Specifically, we define a probabilistic generative process to couple item collaborative information and multiple auxiliary information into a unified framework, where deep generative models are utilized to model di GLYPH<11> erent information. Item auxiliary information consists of various available side information of items, such as content information ( e.g. , textual or visual content of the item), as well as the social graph information ( e.g. , co-consumption of users between items, citations between articles, or co-director between movies). The Product-of-experts (PoE) principle is exploited to integrate item multi-auxiliary Gaussian variables since the mean vector of the product embedding is a weighted sum of the semantic information in content and social graph according to their informative levels for the recommendation. In addition, to solve the item cold-start problem, we extend VGAE to inductive VGAE through sub-graph sampling and neighborhood aggregation so that new item nodes can be inferred during testing. We further add the generative losses of each auxiliary embeddings through the item decoder, where the generation of recommendation probabilities for each tag could be further strengthened via multiple auxiliary information for new items. The main contributions of our method are as follows: GLYPH<136> By defining a probabilistic generative process, item multiple auxiliary information (content and social graph) are integrated into the modeling of collaborative information, where the closeness of the latent item variable and each auxiliary variable is deduced to be achieved by an mean square error (MSE) loss. Therefore, item auxiliary information could be exploited to make better tag recommendations. GLYPH<136> Generative losses of di GLYPH<11> erent item auxiliary embeddings through the item decoder are added to enhance the prediction ability of interaction feedback. Therefore, the generation of recommendation probabilities for each tag could be well inferred via multi-auxiliary item embeddings for new item scenarios where collaborative information does not exist. Moreover, an inductive variational graph auto-encoder (VGAE) is proposed to solve the problem of adding nodes in the graph by sub-graph sampling and neighborhood aggregation. In this way, item social graph information along with the item content information could be fully exploited to better infer the embeddings of new items for cold-start recommendations. GLYPH<136> As a Bayesian probabilistic model, the tight coupling between item collaborative information and multiple auxiliary information is realized with deep generative models utilized to better model the multiple data sources. Moreover, extensive experiments have verified the e GLYPH<11> ectiveness of our method in tag recommendations. The rest of the paper is organized as follows. Section 2 reviews the related work. Section 3 describes the proposed MA-CVAE model in detail, and Section 4 presents the experimental evaluations. Section 5 summarizes the paper with conclusions.", "2 RELATED WORK": "In this section, we first review the related work about tag recommendation (TR) from three aspects, i.e. , collaborative-based TR that merely relies on the item-tag interaction matrix, contentbased TR that utilizes item content information for modeling content filtering, and hybrid methods that merge two information sources. Then, we summarize the previous work on variational recommender systems where collaborative-based, content-based, and hybrid methods are all included. Specifically, collaborativebased methods utilize variational auto-encoder (V AE) for reconstructing sparse user-aware interactions among items, contentbased methods apply the powerful feature extraction ability of VAE for item-side or user-side feature extraction. Hybrid methods seek to utilize both rating information and auxiliary information in a unified framework.", "2.1 Tag Recommendation": "Existing tag recommendation tasks mainly include objectcentered and personalized tasks [3, 15]. This paper focuses on the objective-centered tag recommendation without taking users' preferences into account. From the perspective of exploited data sources, tag recommendation can be roughly categorized into: collaborative-based, content-based, and hybrid methods. Collaborative-based methods utilize the existing tagging history as input. Rendle and Schmidt-Thieme [8] presented the factorization model PITF (Pairwise Interaction Tensor Factorization) and adapted the Bayesian Personalized Ranking (BPR) framework. Fang et al. [16] exploited the Gaussian radial basis function to increase the model's capacity which could be considered as a nonlinear extension of Canonical Decomposition. Chen et al. [17] integrated the graph neural networks into the pairwise interaction tensor factorization model to better capture the tagging patterns in item-tag interaction graph. Content-based methods model the semantic and structural content of items to recommend suitable tags. Maity et al. [18] learned the content representation from question title and body to recommend appropriate question tags on Stack Overflow. Yu et al. [19] extended labeled latent Dirichlet allocation (LLDA) [20] by explicitly specifying several relevant words for a given tag, and allowing to generate the content directly using these words by treating the tags as the supervision information of the corresponding content. Khezrian et al. [21] used the BERT pre-training technique in tag recommendation task for online Q&A and open-source communities for the first time. Hassan et al. [11] adopted deep recurrent neural networks, i.e. , bi-GRUs, to encode titles and abstracts of scientific articles into semantic vectors for enhancing the recommendation task. Tang et al. [10] combined RNN with topical distributions to learn text representations, where the content-tag overlapping and the tag correlation were further considered. Nie et al. [4] constructed a 2 Figure 1: On the left is exemplars of item tagging in articles. On the right is the illustration of social interactions between items. Auxiliary information could be obtained by social links among items except for item content information. Abstract Tags article1 article2 article3 We proposed a variational auto-encoder  network, where tighter ELBO ... VAE Network Abstract Tags Variational auto-encoder has been utilized for the recommender, ... VAE Recommender Abstract Tags We proposed a graph network for recommenders, where collaborative information ... GNN Recommender ELBO collaborative ELBO collaborative author1 author2 user group1 user group2 article1 article2 article3 Co- author Co- author Consumed-by Written-by Co- consume Co- consume hypergraph by integrating multiple facets, including QuestionAnswer content analytics, tag-sharing information, as well as user connections, and then selected candidate tags by simultaneously considering informativeness, stability, and closeness. Nie et al. [5] further considered the newly-posted question tagging problem by learning the question and topic embeddings from deep neural networks and projecting them into the same space for a similarity measure. Hybrid methods integrate collaborative and content information for better recommendations. Song et al. [22] introduced a graph-based method, which represented the tagged data into two bipartite graphs of (document, tag) and (document, word) and found document topics by leveraging graph partitioning algorithms. Zhang et al. [23] proposed an optimization model to integrate item contents into user interests where di GLYPH<11> erent impacts of item features on user preference toward an item have been extracted for item recommendation. Sun et al. [24] proposed a hierarchical attention model, where collaborative embeddings and content embeddings were fused through an attention module. Wang et al. [12] proposed a CTR model to recommend articles to users, which combined a collaborative filtering matrix decomposition algorithm based on hidden factors and a content analysis algorithm based on probabilistic topic models. These two models were unified into a probabilistic generative framework, which could weigh the importance of the content of the article and the collaborative information. For an article that is rarely tagged, it depends more on the content information, and vice versa. Wang et al. [13] adapted the framework of CTR for tag recommendation problems to seamlessly integrate both item-tag matrix information and item content information, where social networks between items are integrated into the framework for better tag recommendation. Considering the limited representational capabilities of the linear method and topic model to learn interactions and content for the recommendation task, we utilize deep generative models to learn the hidden variables of item collaborative information, content, and social graph. Moreover, by introducing multinomial VAE [14], the generative process could be revised to item-based only with an item decoder to predict recommendation probabilities of each tag, where tag latent embeddings are excluded which has more flexibility and conciseness.", "2.2 Variational-based Recommendation": "Variational auto-encoders (VAEs) [25] generate observation x via the latent variable z . VAEs construct a variational posterior of the unobserved variable z given the input x to approximate the true posterior p ( z j x ) due to the nonlinearity of the conditional likelihood where the generative model uses a decoder network to reconstruct x from z . Specifically, the inference model also uses a neural network as an encoder to learn parameters of approximated posterior distribution q GLYPH<30> ( z j x ). By minimizing the Kullback-Leibler divergence between the parametric posterior and the true posterior, the goal of V AEs to maximize the log marginal likelihood log p ( x ) deduces to minimize an Evidence Lower BOund (ELBO):  where x ( i ) is a sample of the observed variable x , GLYPH<18> and GLYPH<30> are parameters of the decoder and encoder. The first term on the right-hand side (RHS) of Eq. (1) represents the reconstruction error, which calculates the approximation of observed data and estimated ones to improve the quality of generated data. The second term on the RHS of Eq. (1) is a regularization term, which penalizes the approximated posterior probability q GLYPH<30> GLYPH<16> z j x ( i ) GLYPH<17> to be far away from prior probability p ( z ). The reparameterization trick is applied to remove the stochastic sampling from the formation, and thus the gradient could be calculated and back-propagation could be performed. Furthermore, by introducing a hyperparameter GLYPH<12> before the second term of RHS of Eq. (1), V AE can be extended to beta-V AE [26] which controls the balance of reconstruction and regularization terms. VAEs [25] have been extended for recommendations and other tasks [27, 28] due to their advances in modeling highdimensional data as probabilistic latent variables. V AE-CF [14] presented a generative model to fit a user's interactions of items to follow a multinomial likelihood based on beta-VAE. The au- 3 thors utilized the implicit feedback for items of a user as an input and make predictions based on reconstructed multinomial probability. MacridVAE [29] extended VAE-CF by learning disentangled representations of users' latent preferences at high ( e.g. , a user's intention towards di GLYPH<11> erent categories of items) and low ( e.g. , a user's preference of the size of clothing) levels. Askari et al. [30] proposed a Joint Variational Auto-encoder (JoVA), which was an ensemble of two VAEs to jointly learn both user and item representations to predict user preferences. On the other hand, Li and She [31] exploited V AE to learn item content variables, and the probabilistic matrix factorization was utilized to model collaborative variables from user-item interactions. By coupling collaborative information and item content information through a Bayesian generative process, better recommendations could be made. Chen et al. [32] proposed a deep generative model, LVSM, to address the item cold-start top-N recommendation problem. The model could capture local aspects of items and measure global item similarity based on deep representations extracted from item features through a variational EM procedure. VAEs are also utilized for the modeling of hybrid recommendations. Chen and de Rijke [33] proposed to simultaneously recover user ratings and side information of items by using a VAE, where user ratings and side information were encoded and decoded collectively through the same inference network and generation network. Due to the heterogeneity of user ratings and side information, the final layer of the generation network followed di GLYPH<11> erent distributions. Lee et al. [34] proposed to encompass variational auto-encoders through augmenting structures to model the auxiliary information and to model the implicit user feedback, where a conditional V AE [35] for modeling the conditional distribution given another modality and a joint multimodal VAE (JMVAE) [36] for modeling the joint distribution of di GLYPH<11> erent modalities by a single latent variable were utilized for integrating multiple item auxiliary features. Ma et al. [37] introduced a partial V AE, which could e GLYPH<14> ciently handle the missing ratings using amortized partial inference technique without relying on ad-hoc assumptions such as Zero Imputation. The authors further designed a partial inference network for auxiliary distribution by a permutation invariant set function to encode auxiliary information on user side and item side for better hybrid recommendations. Wang et al. [38] proposed a personalized online course recommender system, where the extracted latent representations of the employees' competencies from their skill profiles with auto-encoding variational inference based topic modeling and the personal demands of employees were integrated into a unified Bayesian inference view. The graphical model comprehensively combined the conventional latent factor models (LFM)-based collaborative filtering method with auto-encoding variational inference for topic modeling.", "3 M ulti -A uxiliary A ugmented C ollaborative V ariational A uto -encoder": "In this section, we describe the proposed multiple auxiliary information augmented variational auto-encoder (MA-CVAE) for tag recommendations as shown in Fig. 2. MA-CVAE is a generative model where di GLYPH<11> erent item auxiliary information, i.e., content and social graph, are generated through item auxiliary variables, and item-tag ratings are generated by item latent vari- ables. Di GLYPH<11> erent item auxiliary information is injected into the modeling of item latent variables through adding multi-auxiliary variables and collaborative variables, which bridges hybrid information together into a unified framework. Notations used in this article are summarized in Table 1. Note that we use Capital non-boldface symbols such as R to denote the corresponding random vectors of r , R m is used to denote the random matrix for stacked r . Capital boldface symbols such as R are used to denote matrices.", "3.1 Problem Statements": "Let V and T denote the set of I items and J tags, respectively. Assume we have C = [ c 1 ; c 2 ; GLYPH<1> GLYPH<1> GLYPH<1> ; c I ], where c i denotes the content of item i . The item social graph G is a multigraph where multiple edges are distinguished by the attributes of the links. Here, we denote the intrinsic links as the edges between items once after the item is produced such as citations between scientific articles and co-star information between movies. While extrinsic links are denoted as edges between items after user co-consumption of the items such as two items with 5 or more users interacting in common could be linked to an edge. The adjacency matrix is A 2 R I GLYPH<2> I where we merge these two types of links between items and normalize the edge weights to be {0, 1}. The collaborative information is represented by an item-tag tagging matrix R = [ ri j ] I GLYPH<2> J , where ri j = 1 means that the tag j is tagged to the item i . Given the item i , the task of tag recommendation is to find a list of tags T ( i ) GLYPH<18> T that is likely to be annotated to the item i . In this paper, we utilize item content information X , item social graph G , and collaborative information R for hybrid tag recommendations.", "3.2 Generative Process": "In our proposed model, we consider item content and social graph to be generated by their latent content variables and social variables through generative networks respectively. Specifically, to utilize the content information of items, we assign a latent content variable c for each item. To further employ the social network of items, which contains plenty of contiguous relations between items, we assign a latent social variable s for each item. We draw the latent item content variable and item social variable in a latent low-dimensional space of K dimensions from Gaussian distributions:   where GLYPH<21> C and GLYPH<21> S are precision vectors of item content variable and social variable. The content of the item x is then generated from its latent content variable c through a generation neural network, e.g. , an MLP as with variational auto-encoder [25]. For the generation of item social graph, sub-graph sampling as in [39] is utilized for inductive learning and mini-batch training. The sub-graph G s of the item (Here, we focus on the generation of the edges, i.e., the adjacency matrix A s of the sub-graph) can be generated by latent item social variables S s , which consist of variables of the item and its sampled neighbors, through an inner product decoder as with variational graph auto-encoder [40]. Exactly, the content and social graph of the item are generated from its latent content variable c and social variables S s through 4 Item Social Graph 5 Sampler G sub \u02c6 sub G A Figure 2: On the left is the framework of the proposed MA-CVAE. Item content x and social graph G s are di GLYPH<11> erent auxiliary information that are augmentations of collaborative information contained in item-tag ratings r . On the right is the zoom-in of the inference network and generation network of VAE for content information and inductive VGAE for social information. V R X C G S X G R Item Content Item Social Graph C \uf06c S \uf06c ( | ) V p R C \uf071 ( | ) V p R S \uf071 C \uf06d C \uf073 c x A S \uf06d S \uf073 s \u02c6 x Inference Network Generation Network X Sampler sub G \u02c6 sub G Content VAE Social VGAE C E V E S E C D V D S D generation neural networks parameterized by GLYPH<18> :   The latent item variable v is then transformed via a non-linear function, which is parameterized by a neural network, to produce a probability distribution over J tags as in [14]. The item-by-tag interaction of the item is assumed to be: where G s is a sampled graph from the social graph G consisting of l -hop of neighborhoods of item node, which will be further discussed in the following inductive variational graph auto-encoder section. To comprehensively utilize the item content and social network information, a product-of-experts (PoE) principle is employed to fuse c and s as a multi-auxiliary variable. For Gaussian variables, the product is also Gaussian where the new mean becomes GLYPH<22> m = ( GLYPH<22> c GLYPH<21> C + GLYPH<22> s GLYPH<21> S ) = ( GLYPH<21> C + GLYPH<21> S ), and the new variance becomes GLYPH<21> m = ( GLYPH<21> C GLYPH<21> S ) = ( GLYPH<21> C + GLYPH<21> S ). Since the mean of a Gaussian variable depicts its semantic structure and variance denotes uncertainty theoretically, the mean vector of the item embedding is a weighted sum of the semantic information in content and social graph according to their informative levels for the recommendation. To fully explore the collaborative information, we explicitly introduce v y to embed the item collaborative information for item, and draw it from a Gaussian distribution as:  Then, we set the latent item variable v to be composed of both item collaborative and multi-auxiliary latent variables as follows:  Given the product of c and s , then, the latent item variable v follows the conditional distribution N ( PoE ( c ; s ; GLYPH<21> C GLYPH<0> 1 I K ; GLYPH<21> S GLYPH<0> 1 I K ) ; I K ), which is the key to introduce mutual regularization between v and PoE ( c ; s ; GLYPH<21> C GLYPH<0> 1 I K ; GLYPH<21> S GLYPH<0> 1 I K ) ; I K ) in the Maximum A Posterior (MAP) objective. Moreover, with the PoE principle, we could define the conditional generative process of v given c and s to be:    where GLYPH<25> ( v ) is computed by a neural network NN( v ; GLYPH<18> v ) with the output normalized via a Softmax function. GLYPH<18> v is trainable weights of the deep generation network. Given the total number of interactions N # from the item, r is sampled from the Multinomial distribution parameterized by GLYPH<25> ( v ). The multinomial distribution has been proved to be good at modeling implicit feedback data since the model would assign more probability mass to tags that are more likely to be interacted, which will perform well under the topN ranking loss that recommender systems are commonly evaluated on. With the above generative process defined, the joint distribution of observable and hidden variables of the item and its neighbors in the social graph can be formulated as follows:  where f R ; X ; G m s g is the set of all observed variables, f V ; C ; S m s g is the set of all latent variables needed to be inferred. The joint distribution of item content variable p GLYPH<18> c ( X ; C ) is factorized as p GLYPH<18> c ( X j C ) p ( C ), with the prior distribution p ( C ) of latent item content variable to be Gaussian distribution and the stochastic content decoder p GLYPH<18> c ( X j C ) to be parameterized by neural networks. Similarly, the joint distribution of item social variables p GLYPH<18> s ( S m s ; G m s ) is factorized as p ( S m s ) and the stochastic social decoder p GLYPH<18> s ( G m s j S m s ). The joint distribution of item variable p GLYPH<18> v ( R ; V ) is factorized as the conditional generative distribution p ( V j C ; S ) of latent item variable, and the item decoder p GLYPH<18> v ( R j V ). Moreover, p GLYPH<18> s ( G m s j S m s ) can be factorized into the product of per node distributions GLYPH<5> ip GLYPH<18> s ( G j S ), due to the assumption of marginal independence among items in the sub-graph. \uf073 S Table 1: Notations used in our method.", "3.3 Inference Process": "as: Since the generative processes of the latent item variable and multiple auxiliary variables are nonlinear ( i.e. , parameterized as deep neural networks), the posterior distributions for V , C , and S m s are intractable. Therefore, we resort to variational inference with an approximated posterior as q GLYPH<30> GLYPH<16> V ; C ; S m s j R ; X ; G m s GLYPH<17> , where we assume the approximated posterior to come from tractable families of distributions (which are also parameterized by deep neural networks) and in those families find the distribution closest to the true posterior measured by the KL-divergence [41]. According to the conditional independence, the joint posterior of all hidden variables can be factorized into the product of three compact parts with as follows:  where q GLYPH<30> v ( V j R ) denotes the variational posterior of item latent variable, q GLYPH<30> c ( C j X ) and q GLYPH<30> s GLYPH<16> S m s j G m s GLYPH<17> represent the variational posteriors of item content variable and social variables, respectively. Previous work [41] proves that the minimization of the KLdivergence is equivalent to the maximization of the Evidence Lower BOund (ELBO) [25], since the marginal likelihood of inputs could be deduced to the KL divergence of the approximate from the true posterior and the ELBO term, where the KLdivergence term is non-negative. The ELBO could be formalized   By substituting the joint distribution of Eq. (11), we can rewrite the ELBO as in Eq. (13). GLYPH<30> consists of the parameters of the inference networks where q GLYPH<30> is an abbreviation for q GLYPH<30> GLYPH<16> V ; C ; S m s j R ; X ; G m s GLYPH<17> . As with the CVAE paper [31], the entropy of q GLYPH<30> ( V j R ) is regarded as a constant and omitted from the ELBO.", "3.4 Maximum A Posterior Estimation": "Maximum A Posterior (MAP) estimation can be performed by considering the variational distributions of q GLYPH<30> c ( C j X ), q GLYPH<30> s GLYPH<16> S m s j G m s GLYPH<17> and q GLYPH<30> v ( V j R ), as well as maximizing the objective with respect to C , S m s and V by an EM-style algorithm using block coordinate ascent. Since each variable in our paper is defined as a Gaussian distribution, and the negative logarithmic likelihood of the Gaussian distribution is L2-norm loss, the 6  objective in Eq. (13) could further be rewrote as:  where GLYPH<21> C and GLYPH<21> S are precision vectors of item content variables and social variables respectively. F is the Frobenius norm and L2-norm are utilized here. E q GLYPH<30> v ( V j R )[log p GLYPH<18> v ( R j V )], E q GLYPH<30> c ( C j X )[log p GLYPH<18> c ( X j C )] and E q GLYPH<30> s ( S m s j G m s ) [log p GLYPH<18> s ( G m s j S m s )] represent the inference and generation process of item variable, content variable and social variables respectively, in the form of variational auto-encoders. We first fix the parameters related to item auxiliary variables C , S and optimize the parameters related to item variable V of matrimonial VAE (Mult-VAE). The objective for V thus becomes:  where \u02c6 C equals to the mean vector produced by the item content inference network, \u02c6 S equals to the mean vector produced by the item social graph inference network. From Eq. (15), we could see that the closeness between item latent embeddings and each latent auxiliary embeddings is achieved by a mean square error (MSE) loss, which could leverage multiple item auxiliary information into the embedding process of collaborative information. In this way, multiple auxiliary information is incorporated to further alleviate the sparsity of the implicit feedback. Furthermore, to better strengthen the closeness of two components and infer representations for new items using only item auxiliary information, we introduce an additional constraint. Specifically, we add the implementation of the p GLYPH<18> v ( R j C ) and p GLYPH<18> v ( R j S ) to the objective of the item end, using the same decoder as the item decoder p GLYPH<18> v ( R j V ). By adding the additional reconstruction losses, the generation of recommendation probabilities for each tag could be well inferred via item multi-auxiliary embeddings for new items. Therefore, cold-start items that do not contain any collaborative information could be recommended with suitable tags. Then, we fix the parameters of variable V and item social latent variable S , and optimize the item content VAE objective. We isolate the terms related to C and the objective thus becomes:  Finally, we fix the parameters of item variable V and content variable C , the item social inductive VGAE objective can be optimized. The objective for VGAE after isolating the terms 7 related to S becomes:  where \u02c6 V m s denotes the stacked mean vector for item variables, which is inferred by the item inference network. For both L content and L social , the objective consists of three parts: 1) the reconstruction part learns a latent Gaussian variable to reconstruct the input; 2) the KL-divergence part assumes a Normal Gaussian as the prior of the latent variable, which penalizes learned latent variable to encode excessive noisy information; 3) the MSE part constricts the closeness of the item variable and each auxiliary variable, which tightly couples the collaborative information and multi-auxiliary information by mutual constraints.", "3.5 Reparameterization Trick": "We utilize reparameterization trick [25] to make the sampling outside the model for amendable gradient-based optimization. For the three Gaussian latent variables z 2 f v ; c ; s g , we calculate their mean GLYPH<22> z and logarithm of standard deviation GLYPH<27> z via the corresponding encoder network, which could then be scaled by samples drawn from a fixed Gaussian distribution. The transformation is as:  where GLYPH<15> GLYPH<24> N ( 0 ; I K ). Through the reparameterization, the random elements are separated, where di GLYPH<11> erentiation is amendable in the backward propagation. According to previous work [25], we utilize the Monte Carlo gradient estimator to sample for the expectations, which has shown that the variance of the sampling is small and one sample for each data point su GLYPH<14> ces for convergence. With the reparameterization trick, we can iteratively optimize the Mult-VAE part, the item content VAE part, and the item social inductive VGAE part for tag recommendation, which forms a tightly coupled hybrid model. That is, collaborative information can guide the learning of item multiple auxiliary feature mapping, and also each auxiliary representation can further improve the performance of interaction-based VAE, especially on items with sparse interactions. In this way, the item content variable and social variable are coupled into the latent item variable with the MSE term. Moreover, the latent item variable that contains collaborative information is further exploited to constrain the updating of the parameters of networks for item content variable and item social variable. Therefore, for sparse datasets which contain less collaborative filtering information, more item content and social network information could be extracted to assist the expression of items, and vice versa. In this way, better recommendations could be made by mutual restraint. Furthermore, GLYPH<21> S and GLYPH<21> C could be viewed as hyper-parameters that balance the item social and auxiliary components. The training procedure of MA-CVAE is summarized in Algorithm 1.", "3.6 Inductive Variational Graph Auto-encoder": "To handle the transductive characteristic of variational graph auto-encoder [40] which cannot generalize to unseen nodes by", "Algorithm 1: Training MA-CVAE with SGD.": "Input: Item-tag interaction matrix R ; Item content matrix X ; Item social graph G = f A ; X g Randomly initialize GLYPH<18> , GLYPH<30> . while not converged do // Update item Mult-VAE. forall v 2 V do Randomly sample a batch of items with R . Infer the content mean C and social mean S . Sample V via reparametrization trick. Compute the item loss via Eq. (15).", "end": "Compute the gradient of the social loss. Update GLYPH<18> s , GLYPH<30> s by taking stochastic gradient steps. end return GLYPH<18>; GLYPH<30> learning embeddings for each node in the graph during training, we propose an inductive VGAE framework inspired by [39]. Specifically, we utilize a graph sampler to sample a subgraph of the nodes which are to be embedded in the mini-batch, and then an aggregation function is employed to fuse features from the nodes' local neighbors. During training, the parameters of the aggregation function could be well learned through mini-batch gradient descent. Therefore, the embeddings of new nodes which are unseen for the training set could be obtained by leveraging node features of their neighbors using the learned aggregation operation. Meanwhile, the inductive variational graph auto-encoder can be applied for large graphs in recommendation systems such as the user-item bipartite graph through the mini-batch training and inference. We exploit the features of items in the item social graph, where textual Term Frequency-Inverse Document Frequency (TF-IDF) attributes of items are utilized as features. We employ an inner-product decoder as the generative process of our inductive VGAE. Inspired by Bayesian personalized ranking (BPR) loss [42] which is proved to be more suitable for recommendations, the decoder becomes:  where GLYPH<16> is an activation function to increase nonlinearities and R = f ( v ; v + ; v GLYPH<0> ) j ( v ; v + ) 2 E ; ( v ; v GLYPH<0> ) < Eg is a set of triplet of three items. v is the target item to be embedded, v + is the l -hop neighbor of item v which are sampled through Random Walk [43] and v GLYPH<0> is the randomly sampled negative item which are not interacted with item v . For the encoder, we utilize a mean aggregation function to leverage features from neighbors, which can be presented as:  where N ( v ) is the neighbors of item v in the sampled sub-graph. In this way, the item's previous layer embedding s k GLYPH<0> 1 v is concatenated with the aggregated neighborhood vector s k GLYPH<0> 1 u which can be viewed as a skip connection [44]. It is worth noting that for new items, links between items contain only intrinsic relations ( e.g., citations for articles and co-star for movies) among items without extrinsic links (co-interactions of users for items), since a newly uploaded item is more likely to have no interactions by users. In this way, tag recommendations suggest suitable tags for items and then improve the click-through rate of the items.", "forall v 2 V do": "Randomly sample a batch of items for G s . Infer the item mean V s and content mean C s . Conduct the neighbor aggregation via Eq. (20). Sample S s via reparametrization trick. Compute the social loss via Eq. (17).", "3.7 Prediction": "", "3.7.1 For Existing Items": "Let D be the observed data. With the MA-CVAE trained, the weights of the inference networks and generation networks of item implicit feedback, content and social graph, are learned. Then, the prediction for existing items which are existing in the training set becomes:  where GLYPH<22> v denotes the mean vector of the latent item variables through the Mult-VAE encoder. ( GLYPH<22> c GLYPH<21> C + GLYPH<22> s GLYPH<21> S ) = ( GLYPH<21> C + GLYPH<21> S ) calculates the mean vector of the product of item content variables and social variables.", "3.7.2 For New Items": "For totally new items, the product-of-experts of item content embeddings and social embeddings, which could be easily obtained by the inference network of item content and item social graph, are utilized to make predictions via the item decoder. The predictions can be represented as:", "4 EXPERIMENTS": "To evaluate our proposed MA-CVAE, we conduct extensive experiments to answer the following research questions: GLYPH<136> RQ1 How does MA-CVAE perform compared with the stateof-the-art methods for tag recommendations? Among them, collaborative-based, content-based, and hybrid methods are all included to make comprehensive comparisons. 8 Table 2: Statistics of evaluation datasets after preprocessing. GLYPH<136> RQ2 How does MA-CVAE perform under cold-start item scenarios? Comparisons are made among our proposed method and other content-based baselines for new items which are added to the catalog without any interaction. In this situation, collaborative-based models and hybrid methods that are not specifically designed for new items fail to recommend. GLYPH<136> RQ3 Look inside the proposed MA-CVAE, how is the performance of MA-CVAE a GLYPH<11> ected by the parameters GLYPH<21> C and GLYPH<21> S which influence the balance of item content information and social information? How is the interpretability of MA-CV AE when visualizing some examples?", "4.1 Experimental Settings": "", "4.1.1 Datasets": "In our experiments, three real-world datasets are utilized to evaluate the e GLYPH<11> ectiveness of our method. GLYPH<136> MovieLens 20M It is a stable baseline dataset 2 for recommender systems, abbreviated as ml-20m, which contains 20 million ratings rated by 138,000 users on 27,000 movies. We focus on the tagging information provided by the dataset and collected the plot of movies as the textual content of items (movies) using the links in IMDB. The crew information is provided by IMDB 3 and we use the match information provided by ml-20m to correspond a movieId to an imdbId for constructing the item social graph. GLYPH<136> citeulike-a This dataset is from CiteULike, which helps users manage academic articles by creating their own collections of articles. On this platform, you can tag and rate your collected references which is the source of our article-tag interactions. We use the collected tags and articles released by [13] which contains tagging information, textual information ( i.e., title and abstract) and information needed for item social graph ( i.e., citations and user-item ratings) in the dataset. GLYPH<136> citeulike-t It is an extension of citeulike-a dataset collected by [13] which contains more items (articles) and tags. On the other hand, the tagging interactions are more sparse which makes the methods using only collaborative information even harder to perform. We preprocess the content of items ( i.e. , text information) as in [13], where we convert all characters to lowercase, remove the stop words, and conduct lemmatization. Finally, the top 8,000 distinct words are selected as our vocabulary for the citeulike-a dataset. For citeulike-t and ml-20m, the vocabulary sizes are 20,000 and 24,453. We then utilize the TF-IDF of the textual content as our content features for three datasets. 2 https://grouplens.org/datasets/movielens/ 3 https://www.imdb.com/interfaces/ To exploit social information between items, we construct an item social graph using two types of attributes of links ( i.e., intrinsic and extrinsic edges). For intrinsic links, we employ citation links existing among items that are available in CiteULike to construct the citation social network (which is a directed graph) for academic articles. While on MovieLens, we exploit co-actor, co-writer, and co-director information as the inherent attributes of movies, where we assign a link between two items if they have one common actor, writer, or director. On the other hand, the extrinsic links are applied to construct the item co-consumption social graph. Specifically, we employ the cointeracted pattern of users where two items with 4 or more users interacting in common are linked an edge for citeulike, and 50 for ml-20m. We then merge these two networks between items and normalize the weights to be 1 if there exists one edge in any of the two networks. After constructing the item social graph, the number of edges is 368,603, 355,181, and 3,496,719 for citeulike-a, citeulike-t, and ml-20m, respectively. More specifically, we remove the tags used less than 3 times and the items without any links in the item social graph. The statistics after preprocessing of our three datasets are listed in Table 2.", "4.1.2 Evaluation Protocols": "For each dataset, we firstly split out 1000 items for new items which do not exist in the training set. Then, we randomly select tags for each remaining item as training, validation, and test sets with a 6:2:2 ratio. Specifically, items in the training are treated as existing items, and items excluded in the training set are as new items. The tags in the validation and test set are all included in the training set. We use Recall@ N , NDCG@ N , and MRR@ N as our evaluation metrics as in [24], where we do not choose Precision@ N for the reason that an unobserved tag for an item may be since the tag is not suitable for the item, or that the tag is not considered by users to annotate to the item[12]. Recall@ N calculates the hit ratio of the tested models:  where j yv j denotes the number of all interacted tags in test set for item v . Hence, Recall v @ N calculates the number of hit tags in topN recommendation list among Ground Truth of item v and Recall@ N averages Recall v @ N for all items in the test set. Normalized Discounted Cumulative Gain (NDCG) assigns different importance to di GLYPH<11> erent ranks which calculates as:  where ri represents the relevance degree of the item at position i and it is assigned as binary ( i.e. , 0 or 1) in implicit recommendation scenario. The numerator of the RHS of Eq. (24) calculates the discounted cumulative gain (DCG) score, which would increase according to its ranking position when the item in topN list hits the Ground Truth yv . Here, the denominator j yv j log 2 is to normalize NDCG v @ N to be in the range [0, 1]. Mean reciprocal rank (MRR) measures where the first correctly predicted tag in the recommendation list appears which calcu9 lates as:  where T is a set of recommended topN tags, yv is a set of tags that the item v interacted, and r ( T ; yv ) denotes the position of the first correctly recommended tag in the recommendation list for item v . MRR@ N calculates the average of MRRv @ N for all items in the test set.", "4.1.3 Baseline Methods": "To evaluate the e GLYPH<11> ectiveness of our model, we compare it with the following state-of-the-art methods for tag recommendations: GLYPH<136> CF [45] This is a matrix-factorization-based collaborative filtering method that factorizes the training matrix into two low-rank matrices and recovers the original matrix by the inner product of them. It only uses the item-tag matrix information GLYPH<136> PITF [8] It explicitly models the pairwise interaction between users, items, and tags, which is a strong competitor in the field of personalized tag recommendation. GLYPH<136> GNN-PTR [17] It is a graph neural networks boosted personalized tag recommendation model, which integrates the graph neural networks into the pairwise interaction tensor factorization model. GLYPH<136> Bi-GRU + Att [11] This is a content-based tag recommendation method, where deep learning methods are utilized for capturing semantic meanings in the text. Bidirectional gated recurrent units (bi-GRUs) with attention mechanisms are employed to encode text information into semantic vectors. GLYPH<136> ITAG [10] This content-based tag recommendation method takes tag correlation and content-tag overlapping modeling into consideration beyond capturing textual semantic embeddings using Recurrent Neural Networks. GLYPH<136> CTR-SR [13] It is a hybrid method that combines the item-tag matrix, item content information, and item social information into a unified framework through a hierarchical Bayesian model. GLYPH<136> HAM-TR [24] It models two important attentive aspects with a hierarchical attention model, which exploits two levels of attention to e GLYPH<11> ectively aggregate di GLYPH<11> erent elements and different information of content information and collaborative information respectively. Here, the first three baselines are collaborative-based methods, where CF and PITF are matrix-based and GNN-PTR is graphbased. Bi-GRU + Att and ITAG are content-based methods. The last two baselines are hybrid methods that exploit both collaborative information and item content information.", "4.1.4 Parameter Settings": "The validation set is utilized to select the best parameters. Empirically, we set batch_size and embedding_size both to be 64, other parameters are found by performing a grid search as follows: GLYPH<21> C ; GLYPH<21> S 2 f 0 : 1 ; 0 : 5 ; 1 ; 2 ; 10 ; 100 g , learning_rate 2 f 0 : 01 ; 0 : 001 ; 0 : 0001 ; 0 : 0005 ; 0 : 00001 g . The models for collaborative information, content, and social graph are pre-trained in a plain Mult-VAE, VAE, and VGAE manner to first learn initial starting points for the network weights. We set the learn-ing_rate of pre-trained V AE, pre-trained VGAE, and pre-trained Mult-VAE to be 0.001, empirically. However, the learning_rates of our MA-CVAE model are carefully turned which controls the balance of the learning speed of collaborative information, content information, and social information. By default, we set our learning_rate to be 1e-5, 5e-4 for Mult-VAE, and VGAE, respectively for three datasets. And the learning_rate of content VAE model are set to be 1e-4, 1e-3, and 5e-4 for citeulike-a, citeulike-t, and ml-20m datasets. The overall architecture for the Mult-VAE is [ J ! 600 ! 64 ! 600 ! J ] with J tags. Both the inference network and the generation network are chosen to be a two-hidden-layer network architecture ([64 ! 64]) for content VAE network. For VGAE, we set the neighbors to sample for each node in VGAE to be [20, 20] for citeulike-a dataset and [10, 10] for citeulike-t and ml-20m datasets with a two-layer neighborhood aggregation. Moreover, we turn the parameters GLYPH<21> C and GLYPH<21> S which control the balance of content information and social information. We set GLYPH<21> C and GLYPH<21> S both to be 10 by default and further discussion of these two parameters is included in the following section of sensitiveness to parameters.", "4.2 Overall Comparison": "The overall performance of our proposed MA-CVAE and the state-of-the-art baselines for tag recommendations are summarized in Table 3, where the upper part is various baselines and the bottom part is our proposed method with ablations. MA-CVAEw / o only conduct Mult-VAE on collaborative information, MA-CVAEcontent and MA-CVAEsocial perform with augmented item content information and social information, respectively. Each metric is averaged across all test users and we report top-20 evaluations. First of all, by comparing collaborative-based methods ( i.e., CF, PITF, and GNN-PTR) and content-based methods ( i.e., BiGRU-Att and ITAG), we can see that content-based methods perform better on sparse datasets like citeulike. However, on more dense datasets like ml-20m, collaborative-based methods perform much better, which demonstrates the e GLYPH<11> ectiveness of collaborative filtering by using co-occurrence information of collaborative-based methods. For the hybrid method CTRSR, it achieves satisfactory results on the citeulike-a dataset, while on the other two datasets whose content features are highdimensional and sparse, it cannot perform well. This is because of the weakness of LDA in modeling high-dimensional and sparse features. On the other hand, the other hybrid method HAM-TR, which fuses collaborative and content information with weighted average using latent factor methods for modeling both information, achieves somewhat satisfactory results. However, our proposed MA-CVAE significantly outperforms all baselines. Compared to collaborative-based methods, MA-CVAEw / o with only collaborative information outperforms the strong GNN-PTR by a large margin mainly due to the modeling of collaborative information via a deep generative model and applying a multinomial likelihood for the implicit interaction data which has been proved to be good for topN recommendations. For content-based methods, MA-CVAEcontent performs better than them due to the e GLYPH<11> ective modeling of collaborative information and content information. MA-CVAEcontent also out- 10 Table 3: Overall performance comparisons between the proposed MA-CVAE model and various baselines. Table 4: Performance comparisons between the proposed MA-CVAE model and other baselines on cold-start items. performs the hybrid method HAM-TR by modeling content with deep generative models and tight coupling of item content information and collaborative information. CTR-SR, which though makes use of item content and social graph information, applies linear methods to model the item auxiliary information. Therefore, it achieves inferior performance compared to our MA-CVAE method. We further attribute the performance improvements to the following two reasons: 1) By defining a probabilistic generative process, the tight coupling of collaborative information and item auxiliary information is well achieved by MSE losses between the item latent embeddings and multiple auxiliary embeddings. In this way, the two types of information could be maximally utilized in a tuning way. 2) The modeling of collaborative information and multi-auxiliary information are conducted with deep generative models, which have good representational abilities on sparse data. Moreover, the variational-based methods could be robust to noises. training set. Collaborative-based ( i.e., CF, PITF, and GNN-PTR) and hybrid methods ( i.e., CTR-SR and HAM-TR) in our baselines are dependent on interactions when making predictions for items, and thus they cannot be applied to item cold-start scenarios. However, the spring up of new items in tag recommendation scenarios is common which is a problem that needs to be solved. For our proposed MA-CVAE, by modeling item auxiliary information with deep generative models and constraining the generation of implicit feedback using di GLYPH<11> erent auxiliary embeddings, the new items could be recommended using item multi-auxiliary information. Here, the results between contentbased baselines and our proposed MA-CVAE are listed in Table 4. As an ablation study of our method, MA-CVAEw / o with only collaborative information achieves inferior results compared to MA-CVAEcontent and MA-CVAEsocial with item content information and social information, respectively. It demonstrates that di GLYPH<11> erent item auxiliary information all play vital roles in tag recommendations, especially the social information on citeulike-t dataset. Moreover, by defining a generative process, we fuse multiple auxiliary information via a product-of-experts module and tightly couple the collaborative information and multi-auxiliary information, which achieves the best performance.", "4.3 Recommendation for Cold-start Items": "We explore the e GLYPH<11> ectiveness of our proposed method when recommending tags to totally new items, which do not exist in the Results show that Bi-GRU + Att could make good recommendations on citeulike datasets whose vocabulary sizes are smaller, however, it cannot perform well on the ml-20m dataset with a straightforward multi-class classification setting. ITAG could achieve good results on ml-20m due to the modeling of tag correlations with GRU layers, while it performs badly on citeulike datasets. These two methods cannot perform well on all datasets due to specifically designed modules for content extractions. However, our proposed method performs much better than content-based methods on three datasets which illustrates the e GLYPH<11> ectiveness of our method to recommend new items using item multi-auxiliary embeddings as surrogates for the item latent embeddings. This is achieved by constraining the closeness of the item latent embeddings and multi-auxiliary embeddings by MSE losses and additional generation losses. Moreover, through our ablation study, we can observe that di GLYPH<11> erent item auxiliary information all play an important role in tag recommendations for new items. 11 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000 \u0000 \u0000\u0000 \u0000\u0000\u0000 C \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 (a) ml-20m \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000 \u0000 \u0000\u0000 \u0000\u0000\u0000 C \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 (b) citeulike-a (c) citeulike-t \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000 \u0000 \u0000\u0000 \u0000\u0000\u0000 C \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 Figure 3: Sensitivity to parameters. (a), (d) The e GLYPH<11> ect of GLYPH<21> C and GLYPH<21> S on ml-20m. (b), (e) The e GLYPH<11> ect of GLYPH<21> C and GLYPH<21> S on citeulike-a. (c), (f) The e GLYPH<11> ect of GLYPH<21> C and GLYPH<21> S on citeulike-t. \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000 \u0000 \u0000\u0000 \u0000\u0000\u0000 S \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 (d) ml-20m \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000 \u0000 \u0000\u0000 \u0000\u0000\u0000 S \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 (e) citeulike-a \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000 \u0000 \u0000\u0000 \u0000\u0000\u0000 S \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 (f) citeulike-t", "4.4 Further Discussion on MA-CVAE": "", "4.4.1 Sensitive to Parameters": "The parameters GLYPH<21> C and GLYPH<21> S influence the balance between the utilization of content and social graph of item multiple auxiliary information. We set GLYPH<21> C 2 f 0 : 1 ; 0 : 5 ; 1 ; 2 ; 10 ; 1000 g when fixing GLYPH<21> S to be 10, and set GLYPH<21> S 2 f 0 : 1 ; 0 : 5 ; 1 ; 2 ; 10 ; 100 g when fixing GLYPH<21> C to be 10. From Figure 3, we have the following observations: 1) For both GLYPH<21> C and GLYPH<21> S , the performance increases first and begins to decrease at some point, which demonstrates that when fixing the utilization of one auxiliary information, further exploiting another auxiliary information could improve the performance by injecting more information. However, if one kind of auxiliary information overweights the multiple information, the performance decreases much because the other information could not be utilized well and the facilitation e GLYPH<11> ect between each information is reduced. 2) For citeulike datasets, the impact of performance by varying GLYPH<21> S is larger than varying GLYPH<21> C , and vice-versa on ml-20m. It demonstrates that di GLYPH<11> erent content and social information have di GLYPH<11> erent importance degrees on di GLYPH<11> erent datasets, and therefore, it is useful to choose appropriate GLYPH<21> C and GLYPH<21> S . to the article, 'pred tags' to indicate the tags recommended to the user, 'Hit C' to indicate the partial content of the article itself, and 'Hit S' to indicate the partial content of the adjacency articles in the item social graph.Among them, the first two items are existing items in the training set, and the last one is a new item.", "4.4.2 Interpretability": "In order to further intuitively analyze the performance of our proposed model, we show several visualization results of MACVAE in this section. These results are chosen from the test set of the citeulike-a data set, where the items are articles. We use 'true tags' to indicate the tags that the user actually assigns From Table 5, we can see that our proposed method can recommend suitable tags for items with a good hit ratio of our predicted tags on true tags. Moreover, MA-CVAE could recommend some appropriate tags which are not included in the true tags, such as 'ppi (protein-protein-interaction)\" for Article I. Furthermore, by observing 'Hit C\" and 'Hit S\" which represent the partial content of the hit sentence of the content itself and the content of its adjacent neighborhood items, we can figure out that using item content only cannot always hit the right tag. However, through properly modeling the content of the social network, the content of neighborhoods could be aggregated which contributes to making much better recommendations. For example, the tag 'graph\" needs to su GLYPH<14> ciently model item social network information to be properly recommended, which is performed by a variational graph auto-encoder model in our proposed MACVAE. On the other hand, for Article IV which is a new item, we can see that our method can still recommend suitable tags with a high hit ratio, and the item social network information ( i.e., co-author) performs a significant role in recommendations. Since the intrinsic links in the item social graph naturally represent the proximity between items, and similar tags are well recommended for totally new items. 12 Table 5: Example articles with recommended tags. 13", "5 CONCLUSIONS": "In this paper, we seamlessly integrate the collaborative information and item multi-auxiliary information by defining a probabilistic generative process, where the item latent embedding that contain collaborative information and multiple auxiliary embeddings are tightly coupled by constraining their closeness with MSE losses. Specifically, implicit item-by-tag feedback, item content and social graph information are well modeled by specific-designed variational auto-encoders respectively. Moreover, new items could be recommended by constraining the generation of implicit interactions through the item decoder by multiple auxiliary embeddings in the training phase, as well as designing an inductive variational graph auto-encoder to enable the inference of new items. Extensive experiments on real-world datasets, MovieLens and citeulike, successfully demonstrate the e GLYPH<11> ectiveness of our proposed models on both existing and new items. In future work, we would like to extend MA-CVAE to disentangled settings, where personality and matching are considered as two aspects of tag recommendations. Specifically, personality represents taggers' personal preference among tags, while matching denotes the representative and properly-matched tags of items. The disentanglement for the two aspects should be adaptively learned for users since di GLYPH<11> erent demands that whether personality is more important or not is di GLYPH<11> erent among users.", "R eferences": "[1] Peng Hao, Guangquan Zhang, Luis Martinez, and Jie Lu. Regularizing knowledge transfer in recommendation with tag-inferred correlation. IEEE Trans. Cybern. , 49(1):8396, 2019. [2] Ping Yang, Yan Song, and Yang Ji. Tag-based user interest discovery though keywords extraction in social network. In Proc. Int. Con. Big Data Comput. Commun. , pages 363372, 2015. [3] Fabiano M Bel\u00e9m, Jussara M Almeida, and Marcos A Gon\u00e7alves. A survey on tag recommendation methods. J. Assoc. Inf. Sci. Technol. , 68(4):830-844, 2017. [4] Liqiang Nie, Yongqi Li, Fuli Feng, Xuemeng Song, Meng Wang, and Yinglong Wang. Large-scale question tagging via joint question-topic embedding learning. ACM Trans. Inf. Syst. , 38(2):1046-8188, 2020. [5] Liqiang Nie, Yi-Liang Zhao, Xiangyu Wang, Jialie Shen, and Tat-Seng Chua. Learning to recommend descriptive tags for questions in social forums. ACM Trans. Inf. Syst. , 32(1):1046-8188, 2014. [6] Panagiotis Symeonidis, Alexandros Nanopoulos, and Yannis Manolopoulos. Tag recommendations based on tensor dimensionality reduction. In Proc. ACM Conf. Recommender Syst. , pages 43-50, 2008. [7] Ste GLYPH<11> en Rendle and Lars Schmidt-Thieme. Factor models for tag recommendation in bibsonomy. In Proc. Int. Conf. ECML PKDD Discov. Challenge , pages 235-242, 2009. [8] Ste GLYPH<11> en Rendle and Lars Schmidt-Thieme. Pairwise interaction tensor factorization for personalized tag recommendation. In Proc. ACM Int. Conf. Web Search and Data Mining , pages 81-90, 2010. [9] Jia Li, Hua Xu, Xingwei He, Junhui Deng, and Xiaomin Sun. Tweet modeling with LSTM recurrent neural networks for hashtag recommendation. In Proc. Int. Jt. Conf. Neural Netw. , pages 1570-1577, 2016. [10] Shijie Tang, Yuan Yao, Suwei Zhang, Feng Xu, Tianxiao Gu, Hanghang Tong, Xiaohui Yan, and Jian Lu. An integral tag recommendation model for textual content. In Proc. AAAI Conf. Artif. Intell. , pages 5109-5116, 2019. [11] Hebatallah A. Mohamed Hassan, Giuseppe Sansonetti, Fabio Gasparetti, and Alessandro Micarelli. Semanticbased tag recommendation in scientific bookmarking systems. In Proc. ACM Conf. Recommender Syst. , pages 465-469, 2018. [12] Chong Wang and David M. Blei. Collaborative topic modeling for recommending scientific articles. In Proc. ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining , pages 448-456, 2011. [13] Hao Wang, Binyi Chen, and Wu-Jun Li. Collaborative topic regression with social regularization for tag recommendation. In Proc. Int. Jt. Conf. Artif. Intell. , pages 27192725, 2013. [14] Dawen Liang, Rahul G. Krishnan, Matthew D. Ho GLYPH<11> man, and Tony Jebara. Variational autoencoders for collaborative filtering. In Proc. Int. Conf. World Wide Web , pages 689-698, 2018. [15] Jiahao Yuan, Yuanyuan Jin, Wenyan Liu, and Xiaoling Wang. Attention-based neural tag recommendation. In Proc. Int. Conf. Database Syst. Adv. Appl. , pages 350-365, 2019. [16] Xiaomin Fang, Rong Pan, Guoxiang Cao, Xiuqiang He, and Wenyuan Dai. Personalized tag recommendation through nonlinear tensor factorization using gaussian kernel. In Proc. AAAI Conf. Artif. Intell. , pages 439-445, 2015. [17] Xuewen Chen, Yonghong Yu, Fengyixin Jiang, Li Zhang, Rong Gao, and Haiyan Gao. Graph neural networks boosted personalized tag recommendation algorithm. In Proc. Int. Jt. Conf. Neural Netw. , pages 1-8, 2020. [18] Suman Kalyan Maity, Abhishek Panigrahi, Sayan Ghosh, Arundhati Banerjee, Pawan Goyal, and Animesh Mukherjee. DeepTagRec: A content-cum-user based tag recommendation framework for stack overflow. In Eur. Conf. Inf. Retrieval , pages 125-131, 2019. [19] Yong Wu, Shengqu Xi, Yuan Yao, Feng Xu, Hanghang Tong, and Jian Lu. Guiding supervised topic modeling for content based tag recommendation. Neurocomputing , 314: 479-489, 2018. [20] Daniel Ramage, David Hall, Ramesh Nallapati, and Christopher D. Manning. Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora. In Proc. Conf. Empir. Methods Nat. Lang. Process. , pages 248-256, 2009. [21] Navid Khezrian, Jafar Habibi, and Issa Annamoradnejad. Tag recommendation for online q&a communities based on bert pre-training technique. arXiv preprint arXiv:2010.04971 , 2020. 14 [22] Yang Song, Lu Zhang, and C Lee Giles. Automatic tag recommendation algorithms for social recommender systems. ACM Trans. Web , 5(1):1-31, 2011. [23] Haijun Zhang, Yanfang Sun, Mingbo Zhao, Tommy W. S. Chow, and Q. M. Jonathan Wu. Bridging user interest to item content for recommender systems: An optimization model. IEEE Trans. Cybern. , 50(10):4268-4280, 2020. [24] Jianshan Sun, Mingyue Zhu, Yuanchun Jiang, Yezheng Liu, and Le Wu. Hierarchical attention model for personalized tag recommendation. J. Assoc. Inf. Sci. Technol. , 72(2): 173-189, 2021. [25] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Proc. Int. Conf. Learn. Representations , 2014. [26] I. Higgins, L. Matthey, A. Pal, Christopher P. Burgess, Xavier Glorot, M. Botvinick, S. Mohamed, and Alexander Lerchner. Beta-vae: Learning basic visual concepts with a constrained variational framework. In Proc. Int. Conf. Learn. Representations , 2017. [27] Chaojie Wang, Bo Chen, Sucheng Xiao, Zhengjue Wang, Hao Zhang, Penghui Wang, Ning Han, and Mingyuan Zhou. Multimodal weibull variational autoencoder for jointly modeling image-text data. IEEE Trans. Cybern. , pages 1-16, 2021. [28] Jian Chen, Lan Du, and Leiyao Liao. Discriminative mixture variational autoencoder for semisupervised classification. IEEE Trans. Cybern. , pages 1-15, 2020. [29] Jianxin Ma, Chang Zhou, Peng Cui, Hongxia Yang, and Wenwu Zhu. Learning disentangled representations for recommendation. In Proc. Adv. Neural Inf. Process. Syst. , pages 5711-5722, 2019. [30] Bahare Askari, Jaroslaw Szlichta, and Amirali SalehiAbari. Variational autoencoders for top-k recommendation with implicit feedback. In Proc. Int. ACM SIGIR Conf. Res. Devt. in Inf. Retrieval , pages 2061-2065, 2021. [31] Xiaopeng Li and James She. Collaborative variational autoencoder for recommender systems. In Proc. ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining , pages 305-314, 2017. [32] Yifan Chen, Yang Wang, Xiang Zhao, Hongzhi Yin, Ilya Markov, and MAARTEN De Rijke. Local variational feature-based similarity models for recommending top-n new items. ACM Trans. Inf. Syst. , 38(2):1046-8188, 2020. [33] Yifan Chen and Maarten de Rijke. A collective variational autoencoder for top-n recommendation with side information. In Proc. Workshop Deep Learn. Recommender Syst. , pages 3-9, 2018. [34] Wonsung Lee, Kyungwoo Song, and Il-Chul Moon. Augmented variational autoencoders for collaborative filtering with auxiliary information. In Proc. ACM Conf. Inf. Knowl. Manage. , pages 1139-1148, 2017. [35] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional generative models. In Proc. Adv. Neural Inf. Process. Syst. , pages 3483-3491, 2015. [36] Masahiro Suzuki, Kotaro Nakayama, and Yutaka Matsuo. Joint multimodal learning with deep generative models. In Proc. Int. Conf. Learn. Representations , 2017. [37] Chao Ma, Wenbo Gong, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Noam Koenigstein, Sebastian Nowozin, and Cheng Zhang. Partial vae for hybrid recommender system. In Proc. NIPS Workshop Bayesian Deep Learn. , volume 2018, 2018. [38] Chao Wang, Hengshu Zhu, Peng Wang, Chen Zhu, Xi Zhang, Enhong Chen, and Hui Xiong. Personalized and explainable employee training course recommendations: A bayesian variational approach. ACM Trans. Inf. Syst. , 40 (4):1046-8188, 2021. [39] William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Proc. Adv. Neural Inf. Process. Syst. , pages 1025-1035, 2017. [40] Thomas N Kipf and Max Welling. Variational graph autoencoders. In Proc. Adv. Neural Inf. Process. Syst. Workshop , 2016. [41] David M Blei, Alp Kucukelbir, and Jon D McAuli GLYPH<11> e. Variational inference: A review for statisticians. J. Am. Stat. Assoc. , 112(518):859-877, 2017. [42] Ste GLYPH<11> en Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. BPR: Bayesian personalized ranking from implicit feedback. In Proc. Conf. Uncertain. Artif. Intell. , pages 452-461, 2009. [43] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. DeepWalk: Online learning of social representations. In Proc. ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining , pages 701-710, 2014. [44] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In Proc. Eur. Conf. Comput. Vis. , pages 630-645, 2016. [45] Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender systems. Computer , 42(8):30-37, 2009. 15"}
