{"title": "printf: Preference Modeling Based on User Reviews with Item Images and Textual Information via Graph Learning", "authors": "Jyun-Yu Jiang; Ming-Hao Juan; Pu-Jen Cheng", "pub_date": "2023-08-19", "abstract": "Nowadays, modern recommender systems usually leverage textual and visual contents as auxiliary information to predict user preference. For textual information, review texts are one of the most popular contents to model user behaviors. Nevertheless, reviews usually lose their shine when it comes to top-N recommender systems because those that solely utilize textual reviews as features struggle to adequately capture the interaction relationships between users and items. For visual one, it is usually modeled with naive convolutional networks and also hard to capture high-order relationships between users and items. Moreover, previous works did not collaboratively use both texts and images in a proper way. In this paper, we propose printf, preference modeling based on user reviews with item images and textual information via graph learning, to address the above challenges. Specifically, the dimensionbased attention mechanism directs relations between user reviews and interacted items, allowing each dimension to contribute different importance weights to derive user representations. Extensive experiments are conducted on three publicly available datasets. The experimental results demonstrate that our proposed printf consistently outperforms baseline methods with the relative improvements for NDCG@5 of 26.80%, 48.65%, and 25.74% on Amazon-Grocery, Amazon-Tools, and Amazon-Electronics datasets, respectively. The in-depth analysis also indicates the dimensions of review representations definitely have different topics and aspects, assisting the validity of our model design.", "sections": [{"heading": "INTRODUCTION", "text": "In the era of information overload, recommender systems play a crucial role in satisfying users and keeping them engaged by providing personalized recommendations. With the increasing demand for customized contents on modern E-commerce and entertainment platforms, such as Yahoo News, Amazon Shopping, and Yelp, the effectiveness of recommendations is limited by existing user-item interactions and model capacity because of data sparsity. To address the sparsity issues, previous studies leverage higher-order relationships to mitigate the gaps between users and items. For example, as one of the most widely adopted and successful techniques, collaborative filtering (CF) [32] assumes low-rank relations between users and items, thereby establishing their relevance based on past interactions. Specifically, CF assumes that users with similar preferences will also consume similar items, thereby identifying items for recommendation [13,21,29,31,45,48]. In recent years, graph-based models, such as knowledge graph learning [1,39,41,42,47] and graph convolution networks (GCN) [10,12,38,46], are powerful approaches to improve the accuracy and scalability of collaborative filtering in recommendation systems by representing users and items as nodes in a graph with edges of their interactions. In other words, the graph structure enables the capability of capturing highorder relations among users and items beyond direct interactions.\nFor most modern e-commerce applications, besides structured data like product categories and brands, the majority of items are also accompanied by crucial unstructured data, such as product titles and images. Specifically, textual and image contents are the most popular unstructured data formats. However, the potential of those unstructured data has not been fully unleashed. For instance, conventional methods tackle texts and images with large-scale neural language models [7,25] and convolutional neural networks [16] or transformer-based visual models [8], respectively, but different data formats are modeled independently so that their knowledge cannot be aligned with each other. Moreover, to the best of our knowledge, none of the previous studies focus on simultaneously incorporating image and textual contents.\nAmong various textual contents, user reviews can become valuable resources to model their preferences and derive user representations [2,5,11,20,43] because detailed user opinions and item attributes can alleviate the data sparsity [33,49]. However, although some studies like RGCL [34] utilize reviews as edge features in graph-based models, the relations between reviews and item contents are not considered. This represents an unprecedented opportunity to improve the effectiveness of recommendation systems by leveraging both user reviews and item contents to generate more accurate personalized results.\nThe attention mechanism [37] is one of the most popular ways to model user interests from interacted items [51]. Previous studies like AGTM [14] also utilize attention to conquer the semantic gap between users and items, further differentiating the importance of interacted items toward the user. However, the previous attention mechanisms are based on vector similarities, where each latent dimension of item representations shares the same weight in computations. In other words, they lose the flexibility to learn distinct similarities of diversified topics within multiple dimensions of different semantics. Therefore, specifying different attention weights for each latent dimension can be a more suitable approach for assigning importance weights with better explanatory power.\nIn this paper, we present a novel framework, preference model based on user reviews with item images and textual information via graph learning (printf), which considers item images and textual contents as the primary source of item representations and leverages user reviews to determine user representations. Our framework consists of three sub-modules, including (1) Cross-Modality Item Modeling (CMIM), (2) Review-Aware User Modeling (RAUM), and (3) User-Item Embedding Propagation and Interaction Modeling (EPIM). CMIM fine-tunes our cross-modality feature encoder to extract image and text representations for items. RAUM leverages user reviews to distill user interests and preferences. User reviews are treated as an indicator to derive user embeddings. Compared to previous methods, instead of using a vector-wise inner product to compute the attention scores, we propose to compute multiple dimension-wise attention scores to learn diversified topics. EPIM employs the state-of-the-art graph convolutional network to propagate user-item embeddings and model high-order connectivity of user-item interactions.\nTo conclude, the main contributions of this work can be summarized as follows:\n\u2022 To the best of our knowledge, we are the first study to innovatively incorporate unstructured contents, such as user reviews, and the images and titles of items, as supplementary sources of multi-modality information.", "publication_ref": ["b31", "b12", "b20", "b28", "b30", "b44", "b47", "b0", "b38", "b40", "b41", "b46", "b9", "b11", "b37", "b45", "b6", "b24", "b15", "b7", "b1", "b4", "b10", "b19", "b42", "b32", "b48", "b33", "b36", "b50", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "\u2022 The novel dimension-based attention technique enables us", "text": "to learn varied topics within multiple embedding dimensions while modeling the connection between review embeddings and item embeddings.\n\u2022 Our proposed framework, printf, generates precise recommendations based on high-order relationships in user-item interactions through graph networks. Through rigorous experiments, we showcase the significance and potency of our suggested printf, leading to the significant relative improvements for NDCG@5 of 26.80%, 48.65%, and 25.74% on the Amazon-Grocery, Amazon-Tools, and Amazon-Electronics datasets, respectively.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "RELATED WORK 2.1 Graph-Based Recommendations", "text": "In recent years, graph neural networks (GNNs) have shown promising results in various recommendation tasks. For instance, GraphRec [9] incorporates user-item interactions and item contents to learn a joint user-item representation via graph convolutional neural networks (GCNs) and propagate messages between connected nodes to predict ratings. PinSage [46] uses a two-level GNN architecture to capture the structural and content-based information of items, and performs neighborhood aggregation for top-N recommendations. To further reduce the model complexity and improve the performance, LightGCN [12] is the state-of-the-art GNN-based model that minimizes the number of computational resources, only applies graph convolutional operations without non-linear activation to learn user and item embeddings. These previous studies demonstrate the effectiveness of GNNs in top-N tasks by leveraging high-order relationships successfully.", "publication_ref": ["b8", "b45", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Content-Based Recommendations", "text": "2.2.1 Item Content Utilization. Item content features usually come in unstructured forms, such as item titles, descriptions, and images. To the best of our knowledge, the majority of studies such as TPR [6] and AGTM [14] specifically focus on modeling unstructured item textual contents. TPR models token-level information between users and items by constructing an enormous knowledge graph including tokens, items, and users nodes. However, the information provided by the token-level is limited; therefore, AGTM tries to leverage transformer-based language models [7,28,37] to extract item textual contents at the sentence-level. Even though both methods utilized item titles and descriptions, there is no previous model dedicated to model item textual contents and images simultaneously. However, in the field of vision-language (V-L) tasks, they have reached notable improvement and success in retrieving the information from (image,text) pairs. Vision-Language Representation Learning In recent years, the existing work on Vision-Language (V-L) representation learning can be classified into 3 main categories. The first category [4,27] involves learning separate uni-modal encoders for images and texts and utilizing a contrastive loss to pre-train on large, noisy web data. They perform exceptionally well on image-text retrieval tasks but lack the capacity to model more intricate interactions between image and text for other V-L tasks. The second category [19,24] utilizes transformer-based multi-modal encoders to model interactions between image and text features, which are highly effective in downstream V-L tasks that require intricate reasoning. However, these methods often require high-resolution input images and pre-trained object detectors. Although some recent approaches aim to improve inference speed by eliminating object detectors, they result in lower performance. Therefore, hybrid models, such as ALBEF [18] and BLIP [17], have emerged as a third category, which unifies the first two categories to create robust uni-modal and multimodal representations with superior performance on both retrieval and reasoning tasks. Furthermore, most of these recent approaches do not require object detectors, which were previously a significant bottleneck for many conventional methods.\n2.2.2 User Review Utilization. Historical reviews have been widely used to improve the learning of user and item embeddings in the field of recommendation systems [2,5,11,20,33,34,43,49]. Generally speaking, the users can be represented as the reviews they have written, and the items can be represented as the reviews they have received. For instance, DeepCoNN [50] utilized the TextCNN [3] to encode features and generate user and item embeddings for rating prediction. In order to differentiate the importance of different reviews, the attention mechanism [37] has also been introduced to improve review-based recommendation performance.\nIn some studies such as NARRE [2], after the feature extraction by CNN, attention is employed to select important reviews in learning user and item representations, improving model interpretability. HUITA [44] designs a three-hierarchy attention mechanism to leverage word-level, sentence-level, and review-level information in user reviews. We can also concatenate all reviews generated by a single user into a long document, for instance, NRCA [22] integrates the document-level and review-level modeling for learning better representations. Last but not least, previous studies such as RGCL [34] also combine the review information during the propagation of the graph-based model. It employs user reviews to interact with the corresponding users and items, and the rating-specific weights, to further get better representations for rating prediction tasks. In summary, from the aforementioned previous works, it is evident that user reviews play a crucial role in recommendation scenarios.", "publication_ref": ["b5", "b13", "b6", "b27", "b36", "b3", "b26", "b18", "b23", "b17", "b16", "b1", "b4", "b10", "b19", "b32", "b33", "b42", "b48", "b49", "b2", "b36", "b1", "b43", "b21", "b33"], "figure_ref": [], "table_ref": []}, {"heading": "PRINTF FOR PREFERENCE MODELING 3.1 Overview", "text": "Referring to Figure 1, printf is composed of three main components, including (1) Cross-Modality Item Modeling (CMIM), (2) Review-Aware User Modeling (RAUM), and (3) User-Item Embedding Propagation and Interaction Modeling (EPIM). CMIM fine-tunes our cross-modality feature encoder to extract image and text representations for items. RAUM leverages user reviews to distill user interests and preferences. User reviews are treated as an indicator to derive user embeddings. Compared to previous methods, instead of using a vector-wise inner product operator to compute the attention scores, we propose to compute multiple dimension-wise attention scores to reflect diversified topics between user reviews and item contents. EPIM employs the state-of-the-art graph convolutional network to propagate user-item embeddings and model high-order connectivity of user-item interactions. Problem Statement. Suppose we have a user set U, an item set I, and the review matrix R, where the entry \ud835\udc5f \ud835\udc62,\ud835\udc56 specifies the review written by the user \ud835\udc62 \u2208 U toward the item \ud835\udc56 \u2208 I. The observedinteractions list can be designated as E = {(\ud835\udc62, \ud835\udc56, \ud835\udc5f \ud835\udc62,\ud835\udc56 )|\ud835\udc62 \u2208 U, \ud835\udc56 \u2208 I, \ud835\udc5f \ud835\udc62,\ud835\udc56 \u2208 R}, where each (\ud835\udc62, \ud835\udc56, \ud835\udc5f \ud835\udc62,\ud835\udc56 ) tuple represents an interaction between the user \ud835\udc62 and the item \ud835\udc56 with the corresponding review \ud835\udc5f \ud835\udc62,\ud835\udc56 . Moreover, we have a set of item text contents T , and a set of item image contents M. For each item \ud835\udc56 \u2208 I, its corresponding textual and image contents are denoted as \ud835\udc61 \ud835\udc56 \u2208 T and \ud835\udc5a \ud835\udc56 \u2208 M respectively. The goal of our proposed model is to learn a function that can predict how likely a user will interact with an unseen item given interaction data E and item contents {T , M}. Note that since the review feature \ud835\udc5f \ud835\udc62,\ud835\udc56 implies the positive interaction between the user \ud835\udc62 \u2208 U and the item \ud835\udc56 \u2208 I, the review feature of the given user toward the item \ud835\udc5f \ud835\udc62,\ud835\udc56 will be not be fed into the model while making the prediction, i.e., inference stage.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Cross-Modality Item Modeling (CMIM)", "text": "3.2.1 Cross-Modality Feature Alignment Encoder. Inspired by the ALBEF [18] model, we propose to leverage the image-text contrastive loss [15] and an additional multi-modal encoder to obtain aligned image and text representations. Specifically, a 12layer visual transformer [8] and a 6-layer BERT model [7] derive the image and text embeddings h \ud835\udc63 \ud835\udc56 and h \ud835\udc61 \ud835\udc56 . In the learning stage, the contrastive loss will align embeddings to each other while an additional 6-layer cross-attentive multi-modal encoder will enforce their multi-modality with self-supervised learning. The detailed loss functions can be found in Section 3.5.", "publication_ref": ["b17", "b14", "b7", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Item Embedding Compressor.", "text": "Graph networks favor using lower-dimensional embeddings to model high-order connectivity [40] while higher-dimensional embeddings usually lead to severe over-fitting [26]. For instance, both LightGCN [12] and NGCF [40] employ a fixed embedding dimension size of 64 to achieve the state-of-the-art performance at the time. However, most of the Transformer-based text and image encoders tend to derive embeddings with a higher dimension number, e.g., 768 of BERT embeddings. To connect the dots from Transformers to GCN, here we propose an item embedding compressor based on Auto-Encoder (AE) [30] to reduce the number of embedding dimensions.\nFormally, given the original image and text embeddings h \ud835\udc63 \ud835\udc56 and h \ud835\udc61 \ud835\udc56 of an item \ud835\udc56 \u2208 I, we can have AEs as:\ne \ud835\udc63 \ud835\udc56 = F \ud835\udc63 encoder (h \ud835\udc63 \ud835\udc56 ), \u0125\ud835\udc63 \ud835\udc56 = F \ud835\udc63 decoder (e \ud835\udc63 \ud835\udc56 ), e \ud835\udc61 \ud835\udc56 = F \ud835\udc61 encoder (h \ud835\udc61 \ud835\udc56 ), \u0125\ud835\udc61 \ud835\udc56 = F \ud835\udc61 decoder (e \ud835\udc61 \ud835\udc56 ),(1)\nwhere e \ud835\udc63 \ud835\udc56 \u2208 R \ud835\udc51 \u2032 and e \ud835\udc61 \ud835\udc56 \u2208 R \ud835\udc51 \u2032 are the dimension reduced image and text embeddings; \ud835\udc51 \u2032 is the reduced embedding dimension size, which is set to 64 in this work following previous studies [12,40]; F \ud835\udc63 encoder (\u2022) and F \ud835\udc63 decoder (\u2022) as the AE of image embeddings are both two 2-layer multi-layer perceptron (MLP) modules. Similarly, two 2-layer MLP modules F \ud835\udc61 encoder (\u2022) and F \ud835\udc61 decoder (\u2022) are applied for the AE of text embeddings.\nLast but not least, the embedding of the item \ud835\udc56 as the input of graph networks can be derived by concatenating the dimensionreduced item image and text embeddings together as:\ne (0) \ud835\udc56 = concat(e \ud835\udc63 \ud835\udc56 , e \ud835\udc61 \ud835\udc56 )(2)", "publication_ref": ["b39", "b25", "b11", "b39", "b29", "b11", "b39"], "figure_ref": [], "table_ref": []}, {"heading": "Review-Aware User Modeling (RAUM)", "text": "To preserve the information from item contents during propagation and to avoid the semantic gap between items and users, we propose utilizing user review features as auxiliary knowledge to overcome this issue. This approach can ultimately assist in modeling users better and help bridge the semantic gap between user and item embeddings via user reviews, leading to a more robust and explainable recommendation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "3.3.1", "text": "User Review Feature Extraction. To consider the sentencelevel and paragraph-level semantics, the Sentence-Transformer [28] is used as an encoder to extract contextualized global semantics from user reviews. For the review \ud835\udc5f \ud835\udc62,\ud835\udc56 written by a user \ud835\udc62 toward an item \ud835\udc56, we can derive the contextualized representation h \ud835\udc5f \ud835\udc62,\ud835\udc56 as the review embedding.", "publication_ref": ["b27"], "figure_ref": [], "table_ref": []}, {"heading": "Review Embedding", "text": "Compressor. As we mentioned in section 3.2, we favor downsizing the embedding dimension for better usage in the training phase. Therefore, here we employ a similar approach to do dimension reduction via an AE. The formula can be defined in this fashion:\ne \ud835\udc5f \ud835\udc62,\ud835\udc56 = F \ud835\udc5f encoder (h \ud835\udc5f \ud835\udc62,\ud835\udc56 ), \u0125\ud835\udc5f \ud835\udc62,\ud835\udc56 = F \ud835\udc5f decoder (e \ud835\udc5f \ud835\udc62,\ud835\udc56 ),(3)\nwhere e \ud835\udc5f \ud835\udc62,\ud835\udc56 \u2208 R \ud835\udc51 \u2032 is the dimension reduced review embedding; \ud835\udc51 \u2032 is the reduced embedding dimension size (i.e., 64 in this work); F \ud835\udc5f encoder (\u2022) and F \ud835\udc5f decoder (\u2022) as two 2-layer MLP modules are the AE of review embeddings.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Dimension-based", "text": "Review-Aware User Modeling. In this paper, we assume each dimension of the review embedding represents different perspectives and aspects of the user's feedback on the interacted item. In order to establish the connection between reviews and items, we first generate the average review embeddings of items, and calculate the dimension-based relations between user reviews and the corresponding item products as:\nD = R\ud835\udc47 e (0) \ud835\udc56 (4)\nwhere R \u2208 R | I | \u00d7\ud835\udc51 is the average review embedding of the item over valid reviews written by users; D \u2208 R \ud835\udc51 \u00d7\ud835\udc51 is the cross-relation matrix that captures the relationship between the dimensions of the review embeddings and the item embeddings. The matrix can also be treated as a projection matrix from the latent space of user reviews toward the space of item contents. Once we have the relationship matrix between reviews and items, we can determine the dimension-based attention weight of each review towards the interacted items by multiplying the relationship matrix with the user review. Eventually, we utilize this pre-computed dimensionbased attention weight to determine the significance of different interacted items by the weighted sum of the initial item embeddings for initializing user representations.\ne (0) \ud835\udc62 = \u2211\ufe01 \ud835\udc56 \u2208 N \ud835\udc62 exp(e \ud835\udc5f \ud835\udc62,\ud835\udc56 \u00d7 D) \ud835\udc57 \u2208 N \ud835\udc62 exp(e \ud835\udc5f \ud835\udc62,\ud835\udc57 \u00d7 D) \u2299 e (0) \ud835\udc56(5)\nwhere \u00d7 denotes matrix multiplication, \u2299 represents element-wise multiplication (dot), and e\n(0)\n\ud835\udc62 denotes the user embedding for user \ud835\udc62 as the input of the graph network.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "User-Item Embedding Propagation and Interaction Modeling (EPIM)", "text": "For the user-item embedding propagation phase, we leverage the structure of the graph convolutional network [12] to learn user and item embeddings via pair-wise interactions.", "publication_ref": ["b11"], "figure_ref": [], "table_ref": []}, {"heading": "Message", "text": "Passing and Message Aggregation. We empirically follow message passing and aggregation methods similar to previous works [12,36] to adopt mean aggregation in each graph convolutional layer. The formula is defined as follows:\nx\n(\ud835\udc59+1) \ud835\udc56\u2192\ud835\udc62 = e (\ud835\udc59 ) \ud835\udc56 \u221a\ufe01 |N \ud835\udc62 ||N \ud835\udc56 | , x (\ud835\udc59+1) \ud835\udc62\u2192\ud835\udc56 = e (\ud835\udc59 ) \ud835\udc62 \u221a\ufe01 |N \ud835\udc62 ||N \ud835\udc56 |(6)\nwhere\nx (\ud835\udc59+1) \ud835\udc56\u2192\ud835\udc62 and x (\ud835\udc59+1)\n\ud835\udc62\u2192\ud835\udc56 represents the message propagated from item \ud835\udc56 toward user \ud835\udc62 and from user \ud835\udc62 toward item \ud835\udc56 through the l-th layer to l+1-th layer, respectively. \ud835\udc41 \ud835\udc62 and \ud835\udc41 \ud835\udc56 denote the set of items interacted by user \ud835\udc62 and the set of users interacted with item \ud835\udc56.\ne (\ud835\udc59+1) \ud835\udc62 = \u2211\ufe01 \ud835\udc56 \u2208 N \ud835\udc62 x (\ud835\udc59+1)\n\ud835\udc56\u2192\ud835\udc62 , e\n(\ud835\udc59+1) \ud835\udc56 = \u2211\ufe01 \ud835\udc62 \u2208 N \ud835\udc56 x (\ud835\udc59+1) \ud835\udc62\u2192\ud835\udc56(7)\n3.4.2 Layer Combination. After computing \ud835\udc3f layers of propagation by Equation 6 and 7, we would like to aggregate different high-order relationships extracted by disparate graph convolutional layers. We simply follow a similar strategy as LightGCN [12] does, using mean pooling to combine the embeddings distilled at each layer to form the final user and item representations. \nwhere e \ud835\udc62 and e \ud835\udc56 are the final representations of user \ud835\udc62 and item \ud835\udc56.", "publication_ref": ["b11", "b35", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Interaction Modeling Layer.", "text": "For the final layer of the proposed model, the naive inner product operation is applied to estimate a user's preference toward a target item, which can be formulated as:\n\u0177\ud835\udc62\ud835\udc56 = e \ud835\udc62 \u2022 e \ud835\udc56(9)\nwhere \u0177\ud835\udc62\ud835\udc56 denotes the matching score of the user \ud835\udc62 and item \ud835\udc56.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Learning and Optimization", "text": "To optimize printf, we introduce the loss functions used by each component in the framework, including alignment loss, Auto-Encoder loss, and Bayesian Personalized Ranking (BPR) loss.\n3.5.1 Alignment Loss. The alignment loss is from our crossmodality feature alignment encoder in CMIM. It has three subobjectives: image-text contrastive loss and masked language modeling in uni-modality encoders, as well as image-text matching applied to the multi-modal encoder.\n\u2022 Image-text Contrastive Learning Loss. Image-text contrastive learning is applied to the uni-modality image encoder and text encoder respectively. It aligns image and textual features, and also trains the uni-modality encoders to better understand the semantic meaning of images and texts. Specifically, a learn-able similarity function s is utilized.\ns = \ud835\udc4a \ud835\udc56\ud835\udc5a\ud835\udc54 e \ud835\udc56\ud835\udc5a\ud835\udc54 \ud835\udc50\ud835\udc59\ud835\udc60 \u2022 \ud835\udc4a \ud835\udc61\ud835\udc65\ud835\udc61 e \ud835\udc61\ud835\udc65\ud835\udc61 \ud835\udc50\ud835\udc59\ud835\udc60 (10\n)\nwhere \ud835\udc4a \ud835\udc56\ud835\udc5a\ud835\udc54 and \ud835\udc4a \ud835\udc61\ud835\udc65\ud835\udc61 denotes the image projection layer and text projection layer as the pretraining phase respectively. For each (image,text) pair, we compute its image-to-text and text-to-image similarity with instances via the Softmax function:\n\u0177\ud835\udc562\ud835\udc61 (I) = Softmax( s(\ud835\udc3c,\ud835\udc47 ) \ud835\udf0b ), \u0177\ud835\udc612\ud835\udc56 (T) = Softmax( s(\ud835\udc47 , \ud835\udc3c ) \ud835\udf0b )(11)\nwhere \ud835\udf0b is a trainable parameter for controlling temperature. Therefore, the image-text contrastive loss from each (image,text) can be written as follows:\nL \ud835\udc56\ud835\udc61\ud835\udc50 = 1 2 * CrossEntropyLoss(\ud835\udc66 \ud835\udc562\ud835\udc61 (I), \u0177\ud835\udc562\ud835\udc61 (I))(12)\n+ 1 2 * CrossEntropyLoss(\ud835\udc66 \ud835\udc61 2\ud835\udc56 (T), \u0177\ud835\udc612\ud835\udc56 (T))\nwhere \ud835\udc66 \ud835\udc562\ud835\udc61 (I) and \ud835\udc66 \ud835\udc61 2\ud835\udc56 (T) denote the ground-truth one-hot similarity, where negative pairs have a probability of 0 and the positive pair has a probability of 1, separately. \u2022 Masked Language Modeling Loss. Second, following the same pretraining objective in BERT [7], masked language modeling (MLM) is applied to the multi-modal encoder. The model randomly masks out the input text tokens with a probability of 15% and replaces them with [MASK] tokens, and predicts them using both the image and the masked text.\nL \ud835\udc5a\ud835\udc59\ud835\udc5a = CrossEntropyLoss(\ud835\udc66 \ud835\udc5a\ud835\udc59\ud835\udc5a , \u0177\ud835\udc5a\ud835\udc59\ud835\udc5a (I, T masked ))\nwhere \ud835\udc66 \ud835\udc5a\ud835\udc59\ud835\udc5a is a one-hot encoded distribution where the groundtruth token has a value of 1. \u0177\ud835\udc5a\ud835\udc59\ud835\udc5a (I, T masked ) is the model's predicted probability distribution for a masked token with given the image representation I and the masked token T masked . \u2022 Image-Text Matching Loss. Third, image-text matching is employed in the multi-modal encoder, which predicts whether a (image, text) pair is positive or negative, i.e., matched or not matched. The output embedding of the [CLS] token from the multi-modal encoder is treated as the joint representation of the image-text pair, and is passed into a fully-connected network ensuing with Softmax to predict a binary classification problem.\nL \ud835\udc56\ud835\udc61\ud835\udc5a = CrossEntropyLoss(\ud835\udc66 \ud835\udc56\ud835\udc61\ud835\udc5a , \u0177\ud835\udc56\ud835\udc61\ud835\udc5a (I, T)) (14\n)\nwhere the \ud835\udc66 \ud835\udc56\ud835\udc61\ud835\udc5a is a 2d one-hot encoded ground truth, and \u0177\ud835\udc56\ud835\udc61\ud835\udc5a (I, T) represents the prediction output probabilities with the given (image,text) pair.\nTo conclude, the overall alignment loss can be concisely formulated as:\nL Align = L \ud835\udc56\ud835\udc61\ud835\udc50 + L \ud835\udc5a\ud835\udc59\ud835\udc5a + L \ud835\udc56\ud835\udc61\ud835\udc5a(15)\n3.5.2 Auto-Encoder Loss. In our proposed model, the AEs are used to condense user review embeddings, item image embeddings, and item text embeddings. All of these AEs are applied with mean square error (MSE) loss,\nL AE = MSELoss(e, \u00ea) + \ud835\udf06 * |\u0398 AE | 2(16)\nwhere e and \u00ea denote original input embeddings and reconstructed embeddings. Note that we also add a regularization term on trainable parameters \u0398 AE in the AE with the \ud835\udc3f 2 regularization coefficient \ud835\udf06. We have \ud835\udf06 \ud835\udc62\ud835\udc5f , \ud835\udf06 \ud835\udc56\ud835\udc56 , \ud835\udf06 \ud835\udc56\ud835\udc61 of three embedding compressors for user review, item image, and item text representations, correspondingly.", "publication_ref": ["b6"], "figure_ref": [], "table_ref": []}, {"heading": "BPR Loss.", "text": "For the user-item interaction graph in EPIM, we employ Bayesian Personalized Ranking (BPR) [29] loss, a pairwise loss assuming the observed interactions reflect a user's preference more than the unobserved ones. The prediction score of an observed interaction should be higher than an unobserved one on it.\nL BPR = - \u2211\ufe01 \ud835\udc62 \u2208 U \u2211\ufe01 \ud835\udc56 \u2208 N \ud835\udc62 \u2211\ufe01 \ud835\udc57\u2209N \ud835\udc62 ln \ud835\udf0e ( \u0177\ud835\udc62\ud835\udc56 -\u0177\ud835\udc62 \ud835\udc57 )+\ud835\udf06 \ud835\udc4f\ud835\udc5d\ud835\udc5f * (|e \ud835\udc62 | 2 +|e \ud835\udc56 | 2 ) (17\n)\nwhere {(\ud835\udc56, \ud835\udc57)|\ud835\udc56 \u2208 N \ud835\udc62 , \ud835\udc57 \u2209 N \ud835\udc62 } is the pair-wise training data for user \ud835\udc62, \ud835\udf0e (\u2022) is the Sigmoid function, adn \ud835\udf06 \ud835\udc4f\ud835\udc5d\ud835\udc5f is the \ud835\udc3f 2 regularization coefficient.", "publication_ref": ["b28"], "figure_ref": [], "table_ref": []}, {"heading": "EXPERIMENTS 4.1 Experimental Datasets", "text": "We apply printf model on three real-world Amazon datasets: Grocery, Tools, and Electronics, which are all publicly accessible 1 . All of the user-item interactions are thoroughly collected in the dataset file, it also contains review comments, which can be perfectly employed in the scenario of our work. Furthermore, we take images and titles from the metadata file as input features of items. In our task scenario, we sample the interactions user by user, and split the datasets 75% for train, 5% for validation, and 20% for testing. By adopting the approach that samples the interactions user by user, we can ensure that all users receive training during the training process, eliminating the possibility of a user being included in the testing set without being part of the training set. The detailed data statistics are summarized in Table 1. Cumulative Gain (NDCG). We set all the metric rankings at the top-{5, 10} to evaluate and prove efficiency. For those items that have no interaction with the user are treated as negative ones, and the interacted items in the test set which is not seen in the train set are treated as positive ones. In this paper, we abbreviate R for Recall, and N for NDCG, respectively.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Dataset", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "4.2.2", "text": "Baseline Methods. We consider five baseline models for comparisons in the experiments, including (1) BPRMF [29] as a pure CF model, (2) LightGCN [12] as the state-of-the-art GCNbased recommendation model, (3) RGCL [34] as the state-of-the-art review-based rating prediction model, (4) TPR [6] incorporating item titles and descriptions at the token-level with a knowledge graph learning, and (5) AGTM [14] leveraging LightGCN and models both item textual contents at the sentence level and high-order connectivity in the user-item graph for the top-N recommendation.\nFor TPR 2 and RGCL 3 , we directly use their official codes which are publicly accessible on GitHub. Here, we would like to explain the reasons why we tend not to showcase the review-based baselines for model comparison. If we directly transform the rating prediction tasks, such as DeepCoNN [50] and NARRE [2], into top-N recommendation tasks, it empirically leads to terrible performance. Even though we perform the top-N task on the state-of-the-art review-based rating prediction model, i.e., RGCL [34], the Recall and NDCG metric are less than 0.015, which are still incredibly low. Therefore, we neglect the review-based recommendation part except for RGCL.", "publication_ref": ["b28", "b11", "b33", "b5", "b13", "b49", "b1", "b33"], "figure_ref": [], "table_ref": []}, {"heading": "Implementation Details.", "text": "The printf is mainly implemented in PyTorch. In CMIM, the 12-layer visual transformer (ViT-B/16 4 ) is adopted for the image encoder and initialized with the pre-trained weights well-trained via ImageNet-1k from DEIT [35]. For the text encoder and the multi-modal encoder, the first 6-layer and the last 6-layer of the BERT [7] model (bert-base-uncased 5 ) with additional cross-attention layers applied respectively. In preactical, the cross-modality feature alignment encoder is initialized with the pre-trained weights 6 from ALBEF [18]. We fine-tune the crossmodality feature alignment encoder with item (image,text) pairs extracted from our dataset for 3 epochs using a batch size of 32 on 1 NVIDIA RTX3090 GPU. We use the AdamW optimizer with a learning rate of 10 -4 and a weight decay of 10 -2 . In RAUM, we use the pre-trained weights 7 of Sentence-Transformer for review embedding generation. For each representation compressor, i.e., AE, in CMIM and RAUM, the AdamW [23] optimizer with a learning rate of 10 -3 and a weight decay of 10 -2 is applied. For the graph in EPIM, we train it using a batch size of 4096 on 1 NVIDIA RTX3090 GPU via the AdamW optimizer with a learning rate of 10 -3 and a weight decay of 10 -2 .", "publication_ref": ["b34", "b6", "b17", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Overall Performance", "text": "The complete main results among three Amazon datasets are summarized in Table 2. According to the table, we can obtain the following observations: BPRMF is a naive approach to learn low-dimensional latent representations of users and items. Its simplicity lies in the fact that it solely relies on collaborative filtering signals without considering item contents or user profiles. As a result, BPRMF generally yields sub-optimal performance in user preference modeling, highlighting the insufficiency of solely modeling direct user-item interactions. For LightGCN, as we expected, it consistently outperforms BPRMF, verifying that stacking GCN layers is an effective way to model high-order relationships which is vital to capture user preference. Moreover, in some of the metrics, LightGCN can reach better performance than TPR, this again highlights the importance of high-order connectivity modeling. Nevertheless, it still relies on the information without any user profile and item contents. RGCL, the state-of-the-art review-based recommendation model in rating prediction, utilizes the user reviews and GCN structure to model high-order connectivity for users and items; however, it performs terribly worse in top-N tasks, probably not only because of the intrinsic gap between rating predictions and top-N tasks, but also the exclusion of content information such as titles and images.\nTPR generally has competitive results with the LightGCN. In the Grocery dataset with Recall@5 and Recall@10, TPR performs significantly better than LightGCN. We suggest this could be that the item textual contents in Grocery data tend to contain much more information, which assists the model in capturing additional collaborative filtering signals. This again proves the importance of modeling item textual contents for representation learning. However, their tremendous KG structure, i.e., treating word tokens, items, and users as different nodes in the KG, makes them unable to learn information effectively when the token-level information is not enough.\nAGTM surpasses the preceding models due to its utilization of item modeling that incorporates textual contents at the sentencelevel, resulting in improved performance compared to TPR. AGTM", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Amazon-Grocery", "text": "Amazon-Tools Amazon-Electronics R@5 N@5 R@10 N@10 R@5 N@5 R@10 N@10 R@5 N@5 R@10 N@10 Table 2: Top-N recommendation performance of different methods. N@5 and R@5 denote the metrics of Recall@5 and NDCG@5. Note that the best results are highlighted in bold while the improvement indicates relative gains over the best baseline method.\nBPRMF\nAmazon-Grocery Amazon-Tools Amazon-Electronics R@5 N@5 R@10 N@10 R@5 N@5 R@10 N@10 R@5 N@5 R@10 N@10 printf 0. effectively captures high-order connectivity through the use of a GCN-based model, further highlighting the potency of GCN in recommendation tasks. However, in their model design, AGTM does not directly train user embeddings. Instead, the user is modeled using an aspect attentive network. As a consequence, the user embedding does not undergo direct training during the backward propagation of the model, resulting in information loss and a deficiency in learning representations. Lastly, printf model consistently achieves exceptional performance across all datasets, providing strong evidence for the effectiveness of our model design. The observed improvements are statistically significant, with all \ud835\udc5d-values being less than 0.01. These improvements can be attributed to three key factors:\n\u2022 The cross-modality feature alignment encoder in CMIM enables us to extract more potent and robust image and text features with global semantics. \u2022 Using dimension-based attention via extracted review representations for user embeddings enables us to effectively determine the prominence relationship between user preferences and item contents. Moreover, the user embeddings are directly trained during the EPIM stage to avoid information loss. \u2022 The bipartite graph structure in EPIM allows printf model to successfully distill higher-order relationships and precisely model user preferences.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ablation Study", "text": "In our printf model, we have incorporated two crucial components: CMIM and RAUM. We aim to demonstrate the effectiveness of each component individually; a comprehensive ablation study experiment is conducted. The results presented in Table 3 showcase the performance of our model when each component is excluded, allowing us to evaluate the impact and efficiency of CMIM and RAUM in isolation. The constructed variants are as follows: Based on Table 3, it's evident that every component of our model design contributes positively to the recommendation. Removing any of these components would result in a performance decrease. Our experimental findings provide validation for the effectiveness of our design, proving that incorporating both image and textual contents as initial item embeddings is a great starting point. This integration allows CMIM to proficiently capture and align the meaning of item images and titles, enhancing item representation. It's worth noting that eliminating either textual content or image content leads to a significant decline in performance, underscoring the importance of modeling both modalities. Additionally, RAUM excels in capturing user preferences. Removing this component results in a substantial drop in performance, indicating the dimension-based attention in RAUM to differentiate the significance of interacted items is an excellent approach for user preference modeling. To conclude, all of the components play a crucial role in printf. Note that in our ablation test, we did not include the variant of removing Amazon-Grocery Amazon-Tools Amazon-Electronics R@5 N@5 R@10 N@10 R@5 N@5 R@10 N@10 R@5 N@5 R@10 N@10 \n\u2022 w/o CMIM (", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2", "tab_2"]}, {"heading": "Analysis and Discussions", "text": "4.5.1 The Effect of GCN Layers. We investigate the impact of varying the number of graph convolution layers on the performance of our model. The comprehensive experimental results can be found in Figure 2. The model performance generally improves as the number of layers is increased from 1 to 7, except for the metrics@5 on Tools and Electronics, which perform better with 5 gcn layers. In most cases, the printf model with 7 GCN layers demonstrates competitive and satisfactory results. This finding highlights that our proposed model requires a higher number of GCN layers compared to other GCN-based models, such as LightGCN [12] and AGTM [14], which typically employ 3 GCN layers. The need for more layers in printf may be attributed to the sparsity and diversity of textual and image features, necessitating additional layers to effectively extract user preferences and capture high-order connectivity. However, it's important to note that excessive propagation, i.e., an excessive number of GCN layers such as 8 or 9, can result in the over-smoothing of information and have a negative impact on predictions.  this leads to negative effects and instead creates noise that interferes with the originally generated representation, since the image embedding we obtained did not participate in the training of the recommendation task. Therefore, we design a method that uses the item image representation and user image representation we just get, as the initial embedding of BPRMF, and conducts model training with the current recommendation task, then takes the user and item embeddings which have been fully trained. We concatenate the embeddings individually behind the user and item representation to predict the top-N items, and compare the performance. From the Table 4, all the above models have positive effects when image contents are taken into consideration.", "publication_ref": ["b11", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Dimension-Based Attention in RAUM.", "text": "Here we would like to validate and evaluate the assumptions and designs we made in RAUM. The assumption posits that each dimension in the representation corresponds to specific topics and aspects that can be easily differentiated. To investigate this hypothesis, we leveraged the attention scores calculated for each dimension to observe if such a phenomenon exists as shown in Figure 3, in which we identified the top frequently attended words for each dimension. It conclusively demonstrated that indeed, each dimension possessed words that were distinctly associated with it, as evident through their diagonal co-cluster effect. Furthermore, these words encompassed diverse types and themes. For instance, within the word cluster positioned in the middle (bound with a green rectangle and arrows pointed), the words are \"light, day, night, bulb, room,\" it can be easily inferred that these particular dimensions pertain to items related to indoor electric lights, encompassing discussions about the brightness and color characteristics of the products. We also explored the direct relationships between different dimensions of the review representations and item representations since we hypothesized that the text within a review would have a certain connection with the item's title or picture. To apply the concept to the embedding representation within the latent space, each dimension of the review embedding should have a specific relationship with the corresponding dimension in the item embedding. By employing the formula developed in Section 3.3, we obtained matrix \ud835\udc37 successfully. Subsequently, through spectral clustering analysis, we were able to confirm the presence of a specific correlation between the review and the embedding dimensions of the item. Referring to Figure 4, we can initially observe the successful differentiation between review embedding dimensions and item embedding dimensions. This differentiation enables us to trace back the importance of the current review embedding dimension and its corresponding dimensions in item embeddings. By doing so, we gain insights into the frequent transmission of specific information between these two latent spaces and understand the type of information that is transparent in-between.\nTo conclude, for RAUM, we can treat the matrix \ud835\udc37 not only as a projection for the review from its space to the item space, but also as an attention matrix to further select the significance of users' reviews toward different interacted items. The design of RAUM not only effectively assists the user preference modeling, but also increases the interpretability of attention.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "CONCLUSION", "text": "In this work, we proposed a new method to effectively model the common scenario in the recommendation. In the scenario, users click the item based on the image and text information provided and write down review comments. The printf jointly models the item's image and textual features and determines user preference based on review comments via a bipartite user-item interaction graph. The crucial keys in our model include not only the utilization of a cross-modality feature alignment encoder in CMIM to extract global semantics from item information but also the effectiveness of dimension-based attention in RAUM to make up a deficiency of the semantic gap, making the information have a better starting position of propagation in EPIM stage. Extensive experiments on three real-world datasets demonstrate the effectiveness, power, and significant improvement of our printf.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Unifying Knowledge Graph Learning and Recommendation: Towards a Better Understanding of User Preferences", "journal": "", "year": "2019", "authors": "Yixin Cao; Xiang Wang; Xiangnan He; Zikun Hu; Tat-Seng Chua"}, {"ref_id": "b1", "title": "Neural Attentional Rating Regression with Review-Level Explanations", "journal": "WWW", "year": "2018", "authors": "Chong Chen; Min Zhang; Yiqun Liu; Shaoping Ma"}, {"ref_id": "b2", "title": "Convolutional neural network for sentence classification", "journal": "", "year": "2015", "authors": "Yahui Chen"}, {"ref_id": "b3", "title": "UNITER: UNiversal Image-TExt Representation Learning", "journal": "", "year": "2020", "authors": "Yen-Chun Chen; Linjie Li; Licheng Yu; Ahmed El Kholy; Faisal Ahmed; Zhe Gan; Yu Cheng; Jingjing Liu"}, {"ref_id": "b4", "title": "ANR: Aspect-based neural recommender", "journal": "", "year": "2018", "authors": "Jin Yao Chin; Kaiqi Zhao; Shafiq Joty; Gao Cong"}, {"ref_id": "b5", "title": "TPR: Text-Aware Preference Ranking for Recommender Systems", "journal": "", "year": "2020", "authors": "Yu-Neng Chuang; Chih-Ming Chen; Chuan-Ju Wang; Ming-Feng Tsai; Yuan Fang; Ee-Peng Lim"}, {"ref_id": "b6", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "journal": "", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b7", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "journal": "", "year": "2021", "authors": "Alexey Dosovitskiy; Lucas Beyer; Alexander Kolesnikov; Dirk Weissenborn; Xiaohua Zhai; Thomas Unterthiner; Mostafa Dehghani; Matthias Minderer; Georg Heigold; Sylvain Gelly; Jakob Uszkoreit; Neil Houlsby"}, {"ref_id": "b8", "title": "Graph neural networks for social recommendation", "journal": "", "year": "2019", "authors": "Wenqi Fan; Yao Ma; Qing Li; Yuan He; Eric Zhao; Jiliang Tang; Dawei Yin"}, {"ref_id": "b9", "title": "Attentionbased Graph Convolutional Network for Recommendation System", "journal": "", "year": "2019", "authors": "Chenyuan Feng; Zuozhu Liu; Shaowei Lin; Tony Q S Quek"}, {"ref_id": "b10", "title": "Attentive Aspect Modeling for Review-Aware Recommendation", "journal": "TOIS, Article", "year": "2019", "authors": "Zhiyong Xinyu Guan; Xiangnan Cheng; Yongfeng He; Zhibo Zhang; Qinke Zhu; Tat-Seng Peng;  Chua"}, {"ref_id": "b11", "title": "LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation", "journal": "", "year": "2020", "authors": "Xiangnan He; Kuan Deng; Xiang Wang; Yan Li; Yongdong Zhang; Meng Wang"}, {"ref_id": "b12", "title": "Neural Collaborative Filtering", "journal": "WWW", "year": "2017", "authors": "Xiangnan He; Lizi Liao; Hanwang Zhang; Liqiang Nie; Xia Hu; Tat-Seng Chua"}, {"ref_id": "b13", "title": "Attentive Graph-based Text-aware Preference Modeling for Top-N Recommendation", "journal": "", "year": "2023", "authors": "Ming-Hao Juan; Pu-Jen Cheng; Hui-Neng Hsu; Pin-Hsin Hsiao"}, {"ref_id": "b14", "title": "Supervised Contrastive Learning", "journal": "", "year": "2021", "authors": "Prannay Khosla; Piotr Teterwak; Chen Wang; Aaron Sarna; Yonglong Tian; Phillip Isola; Aaron Maschinot; Ce Liu; Dilip Krishnan"}, {"ref_id": "b15", "title": "Imagenet classification with deep convolutional neural networks", "journal": "Commun. ACM", "year": "2017", "authors": "Alex Krizhevsky; Ilya Sutskever; Geoffrey E Hinton"}, {"ref_id": "b16", "title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation", "journal": "PMLR", "year": "2022", "authors": "Junnan Li; Dongxu Li; Caiming Xiong; Steven Hoi"}, {"ref_id": "b17", "title": "Align before fuse: Vision and language representation learning with momentum distillation", "journal": "Advances in neural information processing systems", "year": "2021", "authors": "Junnan Li; Ramprasaath Selvaraju; Akhilesh Gotmare; Shafiq Joty; Caiming Xiong; Steven Chu; Hong Hoi"}, {"ref_id": "b18", "title": "VisualBERT: A Simple and Performant Baseline for Vision and Language", "journal": "", "year": "2019", "authors": "Liunian Harold; Li ; Mark Yatskar; Cho-Jui Da Yin; Kai-Wei Hsieh;  Chang"}, {"ref_id": "b19", "title": "Aspect-based fashion recommendation with attention mechanism", "journal": "IEEE Access", "year": "2020", "authors": "Weiqian Li; Bugao Xu"}, {"ref_id": "b20", "title": "Amazon. com recommendations: Item-to-item collaborative filtering", "journal": "IEEE Internet computing", "year": "2003", "authors": "Greg Linden; Brent Smith; Jeremy York"}, {"ref_id": "b21", "title": "Neural Unified Review Recommendation with Cross Attention", "journal": "", "year": "2020", "authors": "Hongtao Liu; Wenjun Wang; Hongyan Xu; Qiyao Peng; Pengfei Jiao"}, {"ref_id": "b22", "title": "Decoupled Weight Decay Regularization", "journal": "", "year": "2019", "authors": "Ilya Loshchilov; Frank Hutter"}, {"ref_id": "b23", "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks", "journal": "", "year": "2019", "authors": "Jiasen Lu; Dhruv Batra; Devi Parikh; Stefan Lee"}, {"ref_id": "b24", "title": "Deep contextualized word representations", "journal": "", "year": "2018", "authors": "Matthew E Peters; Mark Neumann; Mohit Iyyer; Matt Gardner; Christopher Clark; Kenton Lee; Luke Zettlemoyer"}, {"ref_id": "b25", "title": "Rethinking the item order in session-based recommendation with graph neural networks", "journal": "", "year": "2019", "authors": "Ruihong Qiu; Jingjing Li; Zi Huang; Hongzhi Yin"}, {"ref_id": "b26", "title": "Learning Transferable Visual Models From Natural Language Supervision", "journal": "", "year": "2021", "authors": "Alec Radford; Jong Wook Kim; Chris Hallacy; Aditya Ramesh; Gabriel Goh; Sandhini Agarwal; Girish Sastry; Amanda Askell; Pamela Mishkin; Jack Clark; Gretchen Krueger; Ilya Sutskever"}, {"ref_id": "b27", "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks", "journal": "", "year": "2019", "authors": "Nils Reimers; Iryna Gurevych"}, {"ref_id": "b28", "title": "BPR: Bayesian Personalized Ranking from Implicit Feedback", "journal": "", "year": "2009", "authors": "Steffen Rendle; Christoph Freudenthaler; Zeno Gantner; Lars Schmidt-Thieme"}, {"ref_id": "b29", "title": "Learning Internal Representations by Error Propagation", "journal": "", "year": "1986", "authors": "D E Rumelhart; G E Hinton; R J Williams"}, {"ref_id": "b30", "title": "Item-based collaborative filtering recommendation algorithms", "journal": "", "year": "2001", "authors": "Badrul Sarwar; George Karypis; Joseph Konstan; John Riedl"}, {"ref_id": "b31", "title": "Collaborative filtering recommender systems", "journal": "", "year": "2007", "authors": "Ben Schafer; Dan Frankowski; Jon Herlocker; Shilad Sen"}, {"ref_id": "b32", "title": "Deep collaborative filtering with multi-aspect information in heterogeneous networks", "journal": "IEEE transactions on knowledge and data engineering", "year": "2019", "authors": "Chuan Shi; Xiaotian Han; Li Song; Xiao Wang; Senzhang Wang; Junping Du; S Yu; Philip "}, {"ref_id": "b33", "title": "A Review-aware Graph Contrastive Learning Framework for Recommendation", "journal": "ACM", "year": "2022", "authors": "Jie Shuai; Kun Zhang; Le Wu; Peijie Sun; Richang Hong; Meng Wang; Yong Li"}, {"ref_id": "b34", "title": "Training data-efficient image transformers distillation through attention", "journal": "", "year": "2021", "authors": "Hugo Touvron; Matthieu Cord; Matthijs Douze; Francisco Massa; Alexandre Sablayrolles; Herve Jegou"}, {"ref_id": "b35", "title": "Graph Convolutional Matrix Completion", "journal": "", "year": "2018", "authors": "Rianne Van Den; Thomas N Berg; Max Kipf;  Welling"}, {"ref_id": "b36", "title": "Attention is all you need", "journal": "Advances in neural information processing systems", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b37", "title": "Knowledge graph convolutional networks for recommender systems", "journal": "", "year": "2019", "authors": "Hongwei Wang; Miao Zhao; Xing Xie; Wenjie Li; Minyi Guo"}, {"ref_id": "b38", "title": "KGAT: Knowledge Graph Attention Network for Recommendation", "journal": "", "year": "2019", "authors": "Xiang Wang; Xiangnan He; Yixin Cao; Meng Liu; Tat-Seng Chua"}, {"ref_id": "b39", "title": "Neural Graph Collaborative Filtering", "journal": "", "year": "2019", "authors": "Xiang Wang; Xiangnan He; Meng Wang; Fuli Feng; Tat-Seng Chua"}, {"ref_id": "b40", "title": "Learning Intents behind Interactions with Knowledge Graph for Recommendation", "journal": "WWW", "year": "2021", "authors": "Xiang Wang; Tinglin Huang; Dingxian Wang; Yancheng Yuan; Zhenguang Liu; Xiangnan He; Tat-Seng Chua"}, {"ref_id": "b41", "title": "CKAN: Collaborative Knowledge-Aware Attentive Network for Recommender Systems", "journal": "", "year": "2020", "authors": "Ze Wang; Guangyan Lin; Huobin Tan; Qinghong Chen; Xiyang Liu"}, {"ref_id": "b42", "title": "Neural News Recommendation with Attentive Multi-View Learning", "journal": "", "year": "2019", "authors": "Chuhan Wu; Fangzhao Wu; Mingxiao An; Jianqiang Huang; Yongfeng Huang; Xing Xie"}, {"ref_id": "b43", "title": "Hierarchical user and item representation with three-tier attention for recommendation", "journal": "", "year": "2019", "authors": "Chuhan Wu; Fangzhao Wu; Junxin Liu; Yongfeng Huang"}, {"ref_id": "b44", "title": "HOP-Rec: High-Order Proximity for Implicit Recommendation", "journal": "", "year": "2018", "authors": "Jheng-Hong Yang; Chih-Ming Chen; Chuan-Ju Wang; Ming-Feng Tsai"}, {"ref_id": "b45", "title": "Graph Convolutional Neural Networks for Web-Scale Recommender Systems", "journal": "", "year": "2018", "authors": "Rex Ying; Ruining He; Kaifeng Chen; Pong Eksombatchai; William L Hamilton; Jure Leskovec"}, {"ref_id": "b46", "title": "Collaborative Knowledge Base Embedding for Recommender Systems", "journal": "", "year": "2016", "authors": "Fuzheng Zhang; Nicholas Jing Yuan; Defu Lian; Xing Xie; Wei-Ying Ma"}, {"ref_id": "b47", "title": "Discrete collaborative filtering", "journal": "", "year": "2016", "authors": "Hanwang Zhang; Fumin Shen; Wei Liu; Xiangnan He; Huanbo Luan; Tat-Seng Chua"}, {"ref_id": "b48", "title": "Joint deep modeling of users and items using reviews for recommendation", "journal": "", "year": "2017", "authors": "Lei Zheng; Vahid Noroozi; Philip S Yu"}, {"ref_id": "b49", "title": "Joint Deep Modeling of Users and Items Using Reviews for Recommendation", "journal": "", "year": "2017", "authors": "Lei Zheng; Vahid Noroozi; Philip S Yu"}, {"ref_id": "b50", "title": "Deep Interest Network for Click-Through Rate Prediction", "journal": "", "year": "2018", "authors": "Guorui Zhou; Chengru Song; Xiaoqiang Zhu; Ying Fan; Han Zhu; Xiao Ma; Yanghui Yan; Junqi Jin; Han Li; Kun Gai"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: The overall framework of our proposed \"printf\" for preference model.", "figure_data": ""}, {"figure_label": "522", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "4. 5 . 2 Figure 2 :522Figure 2: effect of different numbers of GCN layers.", "figure_data": ""}, {"figure_label": "34", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :Figure 4 :34Figure 3: Bi-clusters of top frequently attended words between different review embedding dimensions derived from the Amazon-Tools dataset.original baseline model, so that the prediction score can also be obtained through the dot-product as original model settings. However,", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "The statistics of the experimental datasets.", "figure_data": "Grocery Tools ElectronicsK-core101016Users12,46419,97318,119Items6,64411,5099,952Items w/text6,63911,4969,946Items w/image6,6398,7698,759Reviews (Interactions) 205,739 304,439440,672Density0.00248 0.001320.00244"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "The results of the ablation study about the contribution of different components in printf model.", "figure_data": "1233 0.1331 0.1536 0.1426 0.0466 0.0468 0.0631 0.0530 0.0390 0.0465 0.0574 0.0522w/o CMIM (image) 0.0970 0.1025 0.1298 0.1137 0.0291 0.0273 0.0460 0.0339 0.0263 0.0310 0.0434 0.0370w/o CMIM (title)0.0950 0.1010 0.1256 0.1114 0.0276 0.0259 0.0441 0.0324 0.0248 0.0292 0.0411 0.0348w/o RAUM0.1139 0.1250 0.1424 0.1346 0.0422 0.0427 0.0569 0.0485 0.0376 0.0452 0.0549 0.0505w/o Both0.0901 0.0956 0.1141 0.1038 0.0266 0.0254 0.0383 0.0299 0.0233 0.0250 0.0374 0.0318"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "image): We only use the item textual features, i.e. The dimension-based attention in RAUM for selecting different importance of interacted items for users is removed. The user embeddings here are randomly initialized.\u2022 w/o Both: Both CAUM and RAUM are removed, and we directly randomly initialized the user and item embeddings in the graph for training, which is equal to LightGCN.", "figure_data": "item titles, to encode the initial item representations without us-ing image features, removing the part of information constructedfrom cross-modality item modeling (CMIM).\u2022 w/o CMIM (title): Only item image features are included toencode the initial item representations.\u2022 w/o RAUM:"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Performance of different methods with and without using image contents.the entire CMIM, which entails not using item textual and image content simultaneously, while still utilizing the review-aware user modeling. The reason for this omission is that when employing dimension-based attention between review embeddings and randomly initialized item embeddings, it lacks any meaningful semantics. Consequently, when conducting ablation on CMIM, we can only eliminate either the textual contents or the visual ones, but not both simultaneously.", "figure_data": "TPR (text only)0.0923 0.0918 0.1228 0.1031 0.0269 0.0248 0.0405 0.0302 0.0222 0.0251 0.0362 0.0301TPR w/ image0.1077 0.1112 0.1390 0.1222 0.0346 0.0333 0.0497 0.0393 0.0284 0.0323 0.0438 0.0376AGTM (text only) 0.1001 0.1050 0.1337 0.1167 0.0336 0.0315 0.0519 0.0386 0.0309 0.0370 0.0498 0.0433AGTM w/ image0.1141 0.1220 0.1429 0.1312 0.0395 0.0385 0.0558 0.0448 0.0345 0.0407 0.0528 0.0466printf w/o image 0.0970 0.1025 0.1298 0.1137 0.0291 0.0273 0.0460 0.0339 0.0263 0.0310 0.0434 0.0370printf0.1233 0.1331 0.1536 0.1426 0.0466 0.0468 0.0631 0.0530 0.0390 0.0465 0.0574 0.0522"}], "formulas": [{"formula_id": "formula_0", "formula_text": "e \ud835\udc63 \ud835\udc56 = F \ud835\udc63 encoder (h \ud835\udc63 \ud835\udc56 ), \u0125\ud835\udc63 \ud835\udc56 = F \ud835\udc63 decoder (e \ud835\udc63 \ud835\udc56 ), e \ud835\udc61 \ud835\udc56 = F \ud835\udc61 encoder (h \ud835\udc61 \ud835\udc56 ), \u0125\ud835\udc61 \ud835\udc56 = F \ud835\udc61 decoder (e \ud835\udc61 \ud835\udc56 ),(1)", "formula_coordinates": [4.0, 102.56, 264.41, 192.03, 29.28]}, {"formula_id": "formula_1", "formula_text": "e (0) \ud835\udc56 = concat(e \ud835\udc63 \ud835\udc56 , e \ud835\udc61 \ud835\udc56 )(2)", "formula_coordinates": [4.0, 136.21, 415.34, 158.37, 13.12]}, {"formula_id": "formula_2", "formula_text": "e \ud835\udc5f \ud835\udc62,\ud835\udc56 = F \ud835\udc5f encoder (h \ud835\udc5f \ud835\udc62,\ud835\udc56 ), \u0125\ud835\udc5f \ud835\udc62,\ud835\udc56 = F \ud835\udc5f decoder (e \ud835\udc5f \ud835\udc62,\ud835\udc56 ),(3)", "formula_coordinates": [4.0, 93.31, 668.65, 201.28, 13.28]}, {"formula_id": "formula_3", "formula_text": "D = R\ud835\udc47 e (0) \ud835\udc56 (4)", "formula_coordinates": [4.0, 416.38, 201.85, 142.36, 13.12]}, {"formula_id": "formula_4", "formula_text": "e (0) \ud835\udc62 = \u2211\ufe01 \ud835\udc56 \u2208 N \ud835\udc62 exp(e \ud835\udc5f \ud835\udc62,\ud835\udc56 \u00d7 D) \ud835\udc57 \u2208 N \ud835\udc62 exp(e \ud835\udc5f \ud835\udc62,\ud835\udc57 \u00d7 D) \u2299 e (0) \ud835\udc56(5)", "formula_coordinates": [4.0, 363.56, 378.15, 195.18, 26.84]}, {"formula_id": "formula_5", "formula_text": "(0)", "formula_coordinates": [4.0, 413.09, 425.25, 8.95, 6.25]}, {"formula_id": "formula_6", "formula_text": "(\ud835\udc59+1) \ud835\udc56\u2192\ud835\udc62 = e (\ud835\udc59 ) \ud835\udc56 \u221a\ufe01 |N \ud835\udc62 ||N \ud835\udc56 | , x (\ud835\udc59+1) \ud835\udc62\u2192\ud835\udc56 = e (\ud835\udc59 ) \ud835\udc62 \u221a\ufe01 |N \ud835\udc62 ||N \ud835\udc56 |(6)", "formula_coordinates": [4.0, 363.63, 582.67, 195.11, 26.79]}, {"formula_id": "formula_7", "formula_text": "x (\ud835\udc59+1) \ud835\udc56\u2192\ud835\udc62 and x (\ud835\udc59+1)", "formula_coordinates": [4.0, 343.1, 620.71, 61.08, 13.12]}, {"formula_id": "formula_8", "formula_text": "e (\ud835\udc59+1) \ud835\udc62 = \u2211\ufe01 \ud835\udc56 \u2208 N \ud835\udc62 x (\ud835\udc59+1)", "formula_coordinates": [4.0, 359.42, 675.51, 71.33, 21.86]}, {"formula_id": "formula_9", "formula_text": "(\ud835\udc59+1) \ud835\udc56 = \u2211\ufe01 \ud835\udc62 \u2208 N \ud835\udc56 x (\ud835\udc59+1) \ud835\udc62\u2192\ud835\udc56(7)", "formula_coordinates": [4.0, 447.93, 675.51, 110.81, 21.86]}, {"formula_id": "formula_11", "formula_text": "\u0177\ud835\udc62\ud835\udc56 = e \ud835\udc62 \u2022 e \ud835\udc56(9)", "formula_coordinates": [5.0, 153.14, 251.78, 141.45, 8.43]}, {"formula_id": "formula_12", "formula_text": "s = \ud835\udc4a \ud835\udc56\ud835\udc5a\ud835\udc54 e \ud835\udc56\ud835\udc5a\ud835\udc54 \ud835\udc50\ud835\udc59\ud835\udc60 \u2022 \ud835\udc4a \ud835\udc61\ud835\udc65\ud835\udc61 e \ud835\udc61\ud835\udc65\ud835\udc61 \ud835\udc50\ud835\udc59\ud835\udc60 (10", "formula_coordinates": [5.0, 129.18, 466.6, 161.99, 12.08]}, {"formula_id": "formula_13", "formula_text": ")", "formula_coordinates": [5.0, 291.16, 468.77, 3.42, 7.94]}, {"formula_id": "formula_14", "formula_text": "\u0177\ud835\udc562\ud835\udc61 (I) = Softmax( s(\ud835\udc3c,\ud835\udc47 ) \ud835\udf0b ), \u0177\ud835\udc612\ud835\udc56 (T) = Softmax( s(\ud835\udc47 , \ud835\udc3c ) \ud835\udf0b )(11)", "formula_coordinates": [5.0, 64.37, 530.97, 230.22, 18.66]}, {"formula_id": "formula_15", "formula_text": "L \ud835\udc56\ud835\udc61\ud835\udc50 = 1 2 * CrossEntropyLoss(\ud835\udc66 \ud835\udc562\ud835\udc61 (I), \u0177\ud835\udc562\ud835\udc61 (I))(12)", "formula_coordinates": [5.0, 89.04, 588.67, 205.54, 20.17]}, {"formula_id": "formula_16", "formula_text": "+ 1 2 * CrossEntropyLoss(\ud835\udc66 \ud835\udc61 2\ud835\udc56 (T), \u0177\ud835\udc612\ud835\udc56 (T))", "formula_coordinates": [5.0, 107.26, 610.17, 151.27, 20.17]}, {"formula_id": "formula_18", "formula_text": "L \ud835\udc56\ud835\udc61\ud835\udc5a = CrossEntropyLoss(\ud835\udc66 \ud835\udc56\ud835\udc61\ud835\udc5a , \u0177\ud835\udc56\ud835\udc61\ud835\udc5a (I, T)) (14", "formula_coordinates": [5.0, 359.19, 255.42, 196.13, 8.96]}, {"formula_id": "formula_19", "formula_text": ")", "formula_coordinates": [5.0, 555.32, 256.43, 3.42, 7.94]}, {"formula_id": "formula_20", "formula_text": "L Align = L \ud835\udc56\ud835\udc61\ud835\udc50 + L \ud835\udc5a\ud835\udc59\ud835\udc5a + L \ud835\udc56\ud835\udc61\ud835\udc5a(15)", "formula_coordinates": [5.0, 383.13, 328.57, 175.61, 9.87]}, {"formula_id": "formula_21", "formula_text": "L AE = MSELoss(e, \u00ea) + \ud835\udf06 * |\u0398 AE | 2(16)", "formula_coordinates": [5.0, 375.25, 393.94, 183.49, 11.75]}, {"formula_id": "formula_22", "formula_text": "L BPR = - \u2211\ufe01 \ud835\udc62 \u2208 U \u2211\ufe01 \ud835\udc56 \u2208 N \ud835\udc62 \u2211\ufe01 \ud835\udc57\u2209N \ud835\udc62 ln \ud835\udf0e ( \u0177\ud835\udc62\ud835\udc56 -\u0177\ud835\udc62 \ud835\udc57 )+\ud835\udf06 \ud835\udc4f\ud835\udc5d\ud835\udc5f * (|e \ud835\udc62 | 2 +|e \ud835\udc56 | 2 ) (17", "formula_coordinates": [5.0, 322.62, 528.86, 232.7, 21.99]}, {"formula_id": "formula_23", "formula_text": ")", "formula_coordinates": [5.0, 555.32, 532.68, 3.42, 7.94]}, {"formula_id": "formula_24", "formula_text": "BPRMF", "formula_coordinates": [7.0, 60.38, 112.39, 24.84, 7.24]}, {"formula_id": "formula_25", "formula_text": "\u2022 w/o CMIM (", "formula_coordinates": [7.0, 317.96, 387.91, 57.76, 8.41]}], "doi": "10.1145/3583780.3615012"}
