{
  "Helen: Optimizing CTR Prediction Models with Frequency-wise Hessian Eigenvalue Regularization": "",
  "Zirui Zhu": "",
  "Yong Liu": "",
  "Zangwei Zheng": "zirui@comp.nus.edu.sg National University of Singapore Singapore liuyong@comp.nus.edu.sg National University of Singapore Singapore Huifeng Guo huifeng.guo@huawei.com Huawei Noah's Ark Lab Shenzhen, China",
  "ABSTRACT": "Click-Through Rate (CTR) prediction holds paramount significance in online advertising and recommendation scenarios. Despite the proliferation of recent CTR prediction models, the improvements in performance have remained limited, as evidenced by open-source benchmark assessments. Current researchers tend to focus on developing new models for various datasets and settings, often neglecting a crucial question: What is the key challenge that truly makes CTR prediction so demanding? In this paper, we approach the problem of CTR prediction from an optimization perspective. We explore the typical data characteristics and optimization statistics of CTR prediction, revealing a strong positive correlation between the top hessian eigenvalue and feature frequency. This correlation implies that frequently occurring features tend to converge towards sharp local minima, ultimately leading to suboptimal performance. Motivated by the recent advancements in sharpness-aware minimization (SAM), which considers the geometric aspects of the loss landscape during optimization, we present a dedicated optimizer crafted for CTR prediction, named Helen. Helen incorporates frequency-wise Hessian eigenvalue regularization, achieved through adaptive perturbations based on normalized feature frequencies. Empirical results under the open-source benchmark framework underscore Helen's effectiveness. It successfully constrains the top eigenvalue of the Hessian matrix and demonstrates a clear advantage over widely used optimization algorithms when applied to seven popular models across three public benchmark datasets on BARS. Our code locates at github.com/NUS-HPC-AI-Lab/Helen.",
  "ACMReference Format:": "Zirui Zhu, Yong Liu, Zangwei Zheng, Huifeng Guo, and Yang You. 2024. Helen: Optimizing CTR Prediction Models with Frequency-wise Hessian Eigenvalue Regularization. In Proceedings of the ACM Web Conference 2024 (WWW'24), May 13-17, 2024, Singapore, Singapore. ACM, New York, NY, USA, 13 pages. https://doi.org/10.1145/3589334.3645463 Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). WWW'24, May 13-17, 2024, Singapore, Singapore © 2024 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-0171-9/24/05. https://doi.org/10.1145/3589334.3645463 zangwei@comp.nus.edu.sg National University of Singapore Singapore",
  "Yang You": "youy@comp.nus.edu.sg National University of Singapore Singapore Figure 1: Evolution of AUC Performance in CTR Prediction Models on the Criteo Dataset, with Time Progression Indicated on the X-axis (Reported by BARS [79]). CCPM W&D HOFM PNN DNN DeepFM AutoInt xDeepFM DCN FGCNN HFM AFN Fi-GNN InterHAt FwFM",
  "1 INTRODUCTION": "Click-through rate (CTR) prediction holds significant importance in the realm of online advertising and marketing [57, 58, 78]. CTR provides insights into the effectiveness of advertising campaigns by measuring the ratio of users who click on a specific link or advertisement to the total number of users exposed to it. Accurate CTR prediction is crucial for advertisers as it enables them to assess the performance of their campaigns, allocate budgets effectively, and optimize ad creatives to maximize return on investment (ROI) [41, 57, 78]. For companies with extremely large user bases like Facebook and Google, even a slight improvement in AUC can result in a significant increase in revenue [38, 65, 79]. The pursuit of accurate CTR prediction has garnered substantial attention from various stakeholders since the early 2000s [41, 46, 55]. Efforts to develop sophisticated models to improve CTR prediction accuracy have been persistent in both academic and industrial communities [7, 12, 20, 37, 65, 66, 77, 78]. However, as depicted by Figure 1, despite the surge of new CTR prediction models and architectures, they continue to encounter challenges in improving the performance on benchmark settings. Besides model structure, the choice of optimization algorithms also has a significant impact on model performance [54, 72]. Adaptive learning rate techniques adpoted by Adam [32] and AdamW [44] have consistently outperformed classic methods such as SGD [56] WWW'24, May 13-17, 2024, Singapore, Singapore Zirui Zhu, Yong Liu, Zangwei Zheng, Huifeng Guo, and Yang You and its variants [48, 52] over various machine learning tasks and reigned over the field of optimization for the past decade. Yet, recent attention has been directed towards the customization of optimization algorithms to suit specific tasks and new models, shining a vibrant spotlight on the possibility of inventing new optimizers to further improve model performance. For example, Lion [10] is developed through automated program search and empirically showcased to perform well in diverse computer vision tasks, and Sophia [40] demonstrates distinctive expertise in training large language models (LLM) by employing a lightweight estimate of the diagonal Hessian. Notably, Sharpness-Aware Minimization (SAM) [19] demonstrates that decreasing the sharpness of the loss function will increase models' ability to generalize. Amidst this rise in recognition of task-tailored optimization methodologies, the field of optimization in CTR prediction has received relatively limited attention. Diverging from many other machine learning tasks, CTR prediction stands out due to its highdimensional, skewed distribution of categorical features. Crafting a customized optimization approach for this task necessitates special attention to these characteristics. This paper presents a thorough exploration of how feature frequency influences the optimization process of CTR prediction models. Our investigation reveals a noteworthy correlation: embeddings of more frequently occurring features tend to converge towards sharper local minima, as evidenced by a larger top eigenvalue of the Hessian matrix. In response to these findings, we introduce Helen, a specialized optimizer designed for CTR prediction models. Helen incorporates frequency-wise Hessian eigenvalue regularization, drawing inspiration from SAM, which introduces perturbations to the updating gradient. Our contributions can be summarized as follows: · We are the first to unveil a robust positive correlation between feature frequency and the top eigenvalue of feature embeddings. This correlation highlights an imbalanced distribution of loss sharpness across the parameter space, making it challenging for the optimizer to discover flat minima that generalize effectively. · We introduce a specialized optimizer for CTR prediction models, known as Helen. Drawing inspiration from SAM, which has been proven to possess the ability to regularize Hessian eigenvalues, Helen leverages frequency information to estimate the sharpness of feature embeddings and adjusts the regularization strength accordingly. · With thorough testing under an open-source benchmark setup, we provide empirical evidence across three public datasets and seven established CTR prediction models. The results consistently demonstrate Helen's effectiveness in regularizing the sharpness of the loss function, thereby significantly enhancing the performance of CTR prediction models.",
  "2 PRELIMINARIES": "",
  "2.1 CTR Prediction": "Let S = {( 𝒙 𝑖 , 𝑦 𝑖 )} 𝑛 𝑖 = 1 represent the training dataset, where each sample ( 𝑥 𝑖 , 𝑦 𝑖 ) follows the distribution D and 𝑦 𝑖 ∈ { 0 , 1 } denotes whether the user clicked or not, 𝒙 𝑖 = [ 𝒙 1 𝑖 , 𝒙 2 𝑖 , . . . , 𝒙 𝑚 𝑖 ] encodes categorical information regarding the user, the product, and their interaction, where 𝑚 indicates the number of feature fields. The 𝑗 -th categorical field is converted through one-hot encoding into a vector denoted as 𝒙 𝑗 𝑖 ∈ { 0 , 1 } 𝑠 𝑗 , where 𝑠 𝑗 represents the number of total feature within the 𝑗 -th categorical field and 𝒙 𝑗 𝑖 [ 𝑘 ] = 1 only if the 𝑘 -th feature in field 𝑗 is present in the 𝑖 -th sample. Given the predictive network denoted as 𝑓 , predictions are generated using the function 𝑓 ( 𝒙 ; 𝒘 ) , where 𝒘 = [ 𝒉 , 𝒆 ] comprises two parts: 𝒆 for the embeddings of sparse features and 𝒉 for all remaining dense network's hidden layers. The embedding weights 𝒆 can be further subdivided into 𝑚 parts, represented as 𝒆 = [ 𝒆 1 , 𝒆 2 , . . . , 𝒆 𝑚 ] , where 𝒆 𝑗 signifies the embedding weights associated with the 𝑗 -th field. Within each field, the embedding weights 𝒆 𝑗 can be broken down into 𝑠 𝑗 components, denoted as 𝒆 𝑗 = [ 𝒆 𝑗 1 , 𝒆 𝑗 2 , . . . , 𝒆 𝑗 𝑠 𝑗 ] , with 𝒆 𝑗 𝑘 representing the weights for the 𝑘 -th feature in the 𝑗 -th field. Assume that 𝒉 ∈ R 𝑑 ℎ and 𝒆 𝑗 𝑘 ∈ R 𝑑 𝑒 , where 𝑑 ℎ and 𝑑 𝑒 denote the dimensions of the dense network and embeddings, respectively.",
  "2.2 Model Optimization": "Given the predicting network 𝑓 , for a specific sample ( 𝒙 , 𝑦 ) and model parameter 𝒘 , the loss function L( 𝒙 , 𝑦, 𝑓 ( 𝒙 ; 𝒘 )) measures the difference between the prediction 𝑓 ( 𝒙 ; 𝒘 ) and the ground truth 𝑦 . The loss function L is usually defined as the cross-entropy loss for CTR prediction task. The ultimate goal of model optimization is to solve the following optimization problem:  where 𝒘 ∗ is the optimal model parameter. However, this optimization problem is intractable since the distribution D is unknown. Instead, we can solve the following empirical risk minimization problem based on the training dataset S :",
  "2.3 Sharpness-Aware Minimization": "The objective of the SAM algorithm [19] is to identify the parameters that minimize the training loss LS ( 𝒘 ) while considering neighboring points within the ℓ 𝑝 ball. This is achieved through the utilization of the following modified objective function:  where | 𝝐 | 𝑝 ≤ 𝜌 ensures that the magnitude of the perturbation 𝝐 remains within a specified threshold. As calculating the optimal solution of inner maximization is infeasible, SAM employs a one-step gradient to approximate the maximization problem, as demonstrated below:  Finally, SAM computes the gradient with respect to perturbed model 𝒘 + ˆ 𝝐 for the update:  Helen: Optimizing CTR Prediction Models with Frequency-wise Hessian Eigenvalue Regularization WWW'24, May 13-17, 2024, Singapore, Singapore Figure 2: Feature Distribution on three public datasets. The y-axis is in the logarithm scale. 0 500 1000 1500 2000 2500 10 2 10 4 10 6 C14 in Avazu 0 500 1000 1500 2000 2500 3000 10 2 10 4 10 6 C13 in Criteo 0 1000 2000 3000 4000 5000 10 2 10 4 10 6 cate_id in T aobao",
  "3 METHOD": "We begin by establishing the key characteristics of the CTR prediction task in Section 3.1, primarily focusing on skewed feature distributions. Subsequently, we analyze the intricate connection between feature frequencies and the Hessian matrix in Section 3.2. Drawing from our analytical insights, we introduce Helen, a novel and tailored optimizer designed specifically for CTR prediction models in Section 3.3.",
  "3.1 Skewed Feature Distribution": "A prominent feature that sets CTR prediction apart from other traditional machine learning tasks, such as image classification, is its distinctive input data format-often comprised of multi-hot encoded categorical features. Within CTR prediction models, the possible number of features can grow immensely, potentially comparable to the total number of users or items. This results in the creation of an input vector that is both highly dimensional and sparse. To compound this challenge, the distribution of these features exhibits a significant skew. As shown in Figure 2, the distribution of feature frequency is highly skewed. Popular features are much more frequent than unpopular features. Many previous works [3, 49, 71, 73] have realized the challenge caused by the skewed feature distribution and propose different techniques such as counterfactual reasoning [5, 39, 73, 75] and model regularization [2, 29, 60]. However, these works slide over the intrinsic difficulty that a skewed feature distribution brings to the optimization of CTR prediction models, which leads to suboptimal recommendation performance. A recent work, CowClip [76], proposes to clip the gradient of feature embedding according to the frequencies of features and successfully scales the batch size to reduce the training time. Cowclip first considers the impact of skewed feature distribution from the perspective of optimization. However, its discussion is limited to the first-order gradient and fails to unveil the second-order information of the loss landscape, i.e., Hessian matrix. In the following section, we will show that even if feature frequencies do affect the feature embedding gradient norm, the impact 0 100 200 300 400 500 Frequency of Features 0.0 1.0 2.0 3.0 Top Eigenvalue max ×10 2 (a) Top Hessian Eigenvalue of PNN 0 100 200 300 400 500 Frequency of Features 0.0 1.0 2.0 3.0 4.0 5.0 Gradient Norm | g | ×10 3 (b) Top Hessian Eigenvalue of DeepFM 0 100 200 300 400 500 Frequency of Features 0.0 0.5 1.0 1.5 2.0 2.5 Top Eigenvalue max ×10 2 (c) Gradient Norm of PNN 0 100 200 300 400 500 Frequency of Features 0.0 1.0 2.0 3.0 4.0 5.0 Gradient Norm | g | ×10 3 (d) Gradient Norm of DeepFM Figure 3: PNN and DeepFM trained on Criteo dataset with Adam optimizer. is not as significant as the correlation between feature frequency and the dominant eigenvalue of the Hessian matrix.",
  "3.2 Hessian and Frequency of Features": "Previous works [35, 50, 59] have shown that under sufficient regularity conditions, gradient-based methods are guaranteed to converge to local minimizer 𝒘 ∗ of the loss function, such that  and the Hessian matrix 𝑯 = ∇ 2 𝒘 LS ( 𝒘 ∗ ) is positive semi-definite, i.e., all eigenvalues of 𝑯 are non-negative. Bottou [6] demonstrates that the largest eigenvalue 𝜆 of the Hessian matrix 𝑯 of the loss function LS (·) indicates whether the optimization is ill-conditioned. If the dominant eigenvalue is too large, the gradient-based optimization methods will converge to some deep ravines in the loss landscape, which leads to poor generalization performance [8, 25, 30, 31]. In the context of CTR prediction, we delve deeper into examining the connection between feature frequencies and the dominant eigenvalues of the Hessian matrix associated with the respective feature embedding. The frequency of each feature 𝑘 in the 𝑗 -th feature field is counted according to the following equation,  As discussed in Section 2.1, the parameters of CTR prediction models can typically be categorized as  WWW'24, May 13-17, 2024, Singapore, Singapore Zirui Zhu, Yong Liu, Zangwei Zheng, Huifeng Guo, and Yang You Table 1: Correlation between the top eigenvalue of Hessian matrix and the frequency of features. Similarly, the gradient can be decomposed as:  Regarding the Hessian matrix 𝑯 , our primary focus centers on the diagonal block matrix 𝑑𝑖𝑎𝑔 ( 𝑯 ) , which is defined as:  Here, the diagonal block matrix 𝑯 𝒉 is a 𝑑 𝒉 × 𝑑 𝒉 matrix and 𝑯 𝑗 𝑘 is a 𝑑 𝑒 × 𝑑 𝑒 matrix for 𝑗 ∈ { 1 , 2 , . . . , 𝑚 } and 𝑘 ∈ { 1 , 2 , . . . , 𝑠 𝑗 } . Given our primary focus on the feature distribution, we use the notation 𝜆 𝑗 𝑘 to represent the top eigenvalue of the Hessian matrix 𝑯 𝑗 𝑘 . We choose two popular CTR prediction models, PNN [53] and DeepFM [20], and train them with Adam [32] optimizer. Figures 3a and 3b visually represent the relationship between the highest eigenvalue of the Hessian matrix, 𝜆 𝑗 𝑘 , and the frequency of the corresponding feature 𝑁 𝑗 𝑘 within the feature field labeled as ' C13 ' in the Criteo dataset. To offer a point of comparison, we also include graphical representations of the gradient norm of feature embedding GLYPH<12> GLYPH<12> GLYPH<12> 𝒈 𝑗 𝑘 GLYPH<12> GLYPH<12> GLYPH<12> plotted against the frequency of the corresponding feature 𝑁 𝑗 in Figures 3c and 3d. 𝑘 When comparing the correlation between the gradient norm of feature embeddings and feature frequency to the correlation involving the top eigenvalue of the diagonal block of the Hessian matrix, a striking difference emerges. The latter exhibits a considerably more pronounced association, signifying an exceptionally strong and positive linear relationship. To further substantiate this observation, we conduct an analysis by computing the Pearson correlation coefficient 𝑟 ( 𝜆, 𝑁 ) between the top eigenvalue of the diagonal block in the Hessian matrix and the corresponding feature's frequency. The results, as depicted in Table 1, also include the Pearson correlation coefficient between the gradient norm and feature frequency. Across the three benchmark public datasets, it is evident that the correlation coefficient 𝑟 ( 𝜆, 𝑁 ) between the top Hessian eigenvalue and the corresponding feature's frequency consistently exceeds 0.8. This value signifies a nearly perfect positive linear relationship between these two variables. This strongly implies that features with higher frequencies are more likely to converge to sharper local minima.",
  "3.3 Helen: Frequency-wise Hessian Eigenvalue Regularization": "SAM [19] has proven its efficacy in enhancing the generalization performance of deep learning models by concurrently minimizing both the loss value and sharpness. Recent advancements in both experimental studies [9, 19] and theoretical research [67] have established SAM's ability to effectively reduce the dominant eigenvalue of the Hessian matrix. Lemma 1. Minimizing the SAM loss function  introduces a bias  among the minimizers of the original loss L 𝑆 ( 𝒘 ) in an O( 𝜌 ) neighborhood manifold, where 𝜆 GLYPH<0> ∇ 2 𝒘 L 𝑆 ( 𝒘 ) GLYPH<1> denotes the maximum eigenvalue of the Hessian matrix. In particular, Wen et al. [67] demonstrates that SAM diminishes the largest eigenvalue of the Hessian matrix of the loss function in the local vicinity of the manifold bounded by the perturbation radius 𝜌 , as summarized in Lemma 1. This finding has prompted our exploration into the regularization of the Hessian matrix for feature embedding using SAM. However, a limitation of the native SAM lies in its application of a uniform perturbation radius 𝝐 to all model parameters. As previously demonstrated in Section 3.1 and Section 3.2, we have underscored the notable skew in the distribution of feature frequencies and the robust correlation existing between these frequencies and the top eigenvalue of the Hessian matrix. When the uniform perturbation radius 𝝐 is relatively small, it inadequately regularizes Hessian eigenvalues for high-frequency features, leading to suboptimal performance, which deviates from our goals. Conversely, when utilizing a large uniform perturbation radius 𝝐 , the top eigenvalue of the Hessian matrix for features with higher frequencies decreases, indicating a convergence toward flatter local minima. However, this strategy overly regularizes features with lower frequencies, redirecting their optimization toward flat regions at the cost of refining the original loss function L 𝑆 ( 𝒘 ) . This trade-off between high-frequency and low-frequency features ultimately leads to a suboptimal solution. Inspired by the aforementioned insights, we propose Helen, a novel optimization algorithm with frequency-wise Hessian eigenvalue regularization. Helen is founded on the SAM with an adaptive perturbation radius 𝝐 for each feature embedding. During the training process, Helen first calculates the frequency of each feature 𝑘 in any feature field 𝑗 , denoted as 𝑁 𝑗 𝑘 . Then the perturbation radius 𝜌 𝑗 𝑘 is calculated as follows,  Given the relatively small proportion of infrequent features in relation to the most frequent feature, we have introduced a lowerbound parameter denoted as 𝜉 to avoid setting the perturbation radius 𝜌 𝑗 𝑘 to excessively small values. Helen: Optimizing CTR Prediction Models with Frequency-wise Hessian Eigenvalue Regularization WWW'24, May 13-17, 2024, Singapore, Singapore And then approximate Equation 3 with first-order Taylor expansion, we have  For the dense network parameters ℎ , we use the same perturbation radius 𝜌 as SAM. After we have all the perturbation vectors  wecan calculate the Helen gradient with respect to perturbed model 𝒘 + ˆ 𝝐 similar with Equation 5 for the update:  The culmination of our work yields the ultimate Helen algorithm, achieved by implementing a conventional numerical optimizer, such as stochastic gradient descent (SGD), and replacing the conventional gradient with the Helen gradient. Algorithm 1 provides pseudocode for the complete Helen algorithm, with SGD as the underlying optimization method.",
  "4 EXPERIMENTS": "",
  "4.1 Experimental Setup": "4.1.1 Datasets. Weevaluate the proposed optimizer on three benchmark public datasets, namely Avazu, Criteo, and Taobao. For more statistics of these three datasets, please refer to Appendix A.2. A brief introduction to them is as follows: · Avazu : The Avazu dataset [4] employed in this research comprises approximately 10 days of labeled click-through data associated with mobile advertisements. · Criteo : The Criteo dataset [34] serves as a prominent benchmark dataset widely utilized for CTR prediction, specifically in the context of display advertising. · Taobao : The Taobao dataset [63] comprises a collection of advertisement display and click records, which were randomly selected from Taobao over a span of 8 days. For a comprehensive and equitable comparison, we use the datasets preprocessed by the BARS benchmark [79]. In particular, we adopt the officially recommended ' Criteo_x4_001 ' for the Criteo dataset and the ' Avazu_x4_001 ' for the Avazu dataset among all the available preprocessed versions. 4.1.2 CTR Prediction Models. To assess Helen's performance, we selected seven well-established CTR prediction models, specifically DNN [12], WideDeep [11], PNN [53], DeepFM [20], DCN [65], DLRM [47], and DCNv2 [66]. The choice of these models is primarily influenced by two key considerations: their impact within both the academic and industry communities and their high performance on the BARS benchmark leaderboard. Each of these seven models has accumulated over 200 citations and has consistently showcased robust performance on the BARS benchmark. For further details of these models, please refer to Appendix A.5. 4.1.3 Baselines. To demonstrate the effectiveness of our proposed method, we compare Helen with seven commonly used optimizers, namely Adam [32], Nadam [15], Radam [42], SAM [19], and ASAM [33]. Nadam and Radam are both variants of Adam, and ASAM is an adaptive version of SAM. For more details, please refer to Appendix A.6",
  "4.2 Performance Analysis": "We evaluate the Helen optimizer alongside Adam, Nadam, Radam, SAM, and ASAM using seven CTR prediction models across three datasets. The summarized outcomes for the Avazu and Criteo datasets are presented in Table 2 and Table 3, respectively. For results of Taobao, please refer to Appendix A.3. The analysis of these tables leads to the following insights:",
  "4.2.1 Narrow Performance Disparity Among CTR Predic-": "tion Models . While a multitude of CTR prediction models have emerged since the advent of DNN in 2016, proclaiming their superiority over predecessors, the actual performance disparity among these models remains relatively modest. The performance gap between the best and worst-performing models in terms of AUC stands at 2.30%, 0.13%, and 0.61% for the Avazu, Criteo, and Taobao datasets, respectively. When excluding the highest and lowest AUC scores, this performance gap further narrows to 0.35%, 0.07%, and 0.51% in the Avazu, Criteo, and Taobao datasets, respectively. Notably, model performance exhibits inconsistency across different datasets. For instance, DCNv2 excels in the Criteo dataset but ranks as the least effective model in the Avazu dataset, showcasing a significant performance contrast relative to other models. Among the seven models evaluated, PNN and DeepFM demonstrate the highest stability, with PNN claiming the top position in two datasets and DeepFM consistently outperforming the average in all three datasets.",
  "4.2.2 Limited Performance Enhancement with Adam Vari-": "ants . Although Nadam and Radam are theoretically proven to have superior convergence bounds or lower variance, their actual performance improvements are rather limited. The average enhancement over seven models achieved by Nadam compared to Adam amounts to a mere 0.15%, 0.03%, and -0.04% in terms of AUC for the Avazu, Criteo, and Taobao datasets, respectively. Similarly, the average improvement with Radam over Adam stands at 0.03%, 0.03%, and -0.22% in AUC for the Avazu, Criteo, and Taobao datasets, respectively. Notably, the performance of Nadam and Radam exhibits inconsistency across different datasets, proving beneficial in the Avazu and Criteo datasets but detrimental in the Taobao dataset.",
  "4.2.3 Performance Impact of SAM in Comparison to Adam .": "Deploying SAM directly to CTR prediction models does not guarantee performance improvement. SAM does yield positive results in the Criteo dataset, with a performance gain of 0.03% in terms of AUC. However, on average, models trained with SAM exhibit slightly lower AUC scores compared to those trained with Adam, resulting in performance declines of 0.04% and 0.05% in the Avazu and Taobao datasets, respectively. Conversely, ASAM consistently outperforms Adam in all three datasets, with an average performance gain of 0.03‰, 0.01%, and 0.04% in the Avazu, Criteo, and Taobao datasets, respectively. These WWW'24, May 13-17, 2024, Singapore, Singapore Zirui Zhu, Yong Liu, Zangwei Zheng, Huifeng Guo, and Yang You Table 2: Overall performance on the Avazu dataset. Table 3: Overall performance on the Criteo dataset. results underscore the significance of employing an adaptive perturbation radius, as demonstrated by ASAM, to achieve enhanced performance, at least in the context of CTR prediction tasks.",
  "4.2.4 Helen's Consistent Superiority Across Three Datasets .": "Helen consistently outperforms Adam, Nadam, Radam, SAM, and ASAM across all three datasets, securing the top position in 20 out of 21 experiments. It delivers an average performance improvement of 0.36%, 0.07%, and 0.42% in terms of AUC for the Avazu, Criteo, and Taobao datasets, respectively. In terms of LogLoss, Helen records an average performance reduction of 0.22%, 0.07%, and 0.11% compared to Adam in the Avazu, Criteo, and Taobao datasets. Furthermore, we conducted paired 𝑡 -tests to compare the differences in AUC scores across all seven models and three datasets. The null hypothesis ( 𝐻 0 ) posited no significant difference between the two optimizers. The resulting 𝑝 -value is 8 × 10 -3 , which falls below the conventional significance threshold of 0.05. Thus, we reject the null hypothesis and conclude that Helen significantly outperforms Adam.",
  "4.2.5 Helen's Role in Narrowing the Performance Gap Among": "CTR Prediction Models . As discussed in Section 4.2.1, the performance gap between the best and worst-performing models remains relatively small. A notable instance is evident in the results presented in Table 2, where the convergence of DCNv2 and DLRM to exceedingly sharp local optima creates a significant performance disparity when compared to other models. As shown in Figure 5, Helen effectively addresses this issue, substantially reducing this performance gap, making them comparable to other models. (a) PNN in Criteo 0 100 200 300 400 500 Frequency of Features 0.0 1.0 2.0 3.0 Top Eigenvalue max ×10 2 Adam SAM Helen 0.0 0.5 1.0 1.5 2.0 2.5 Top Eigenvalue max 0 100 200 300 400 500 Frequency of Features ×10 2 Adam SAM Helen (b) DeepFM in Criteo Figure 4: Dominant Hessian eigenvalues of PNN and DeepFM trained with Adam and Helen on the Criteo dataset. To elaborate, we calculated the variance of AUC scores across all seven models. The variances for Adam are 6 . 54 × 10 -1 , 2 . 03 × 10 -3 , and 6 . 57 × 10 -2 for the Avazu, Criteo, and Taobao datasets, respectively. In contrast, Helen's variances are 1 . 36 × 10 -2 , 1 . 07 × 10 -3 , and 5 . 55 × 10 -3 for the Avazu, Criteo, and Taobao datasets, respectively. These results indicate that Helen can effectively reduce the performance variance of different models, and uniformly improve the performance of CTR prediction models.",
  "4.3 Hessian Eigenspectrum Analysis": "In alignment with the motivations delineated in Section 3.2, we conduct a comprehensive exploration to investigate how the relationship between Hessian eigenvalues and feature frequencies Helen: Optimizing CTR Prediction Models with Frequency-wise Hessian Eigenvalue Regularization WWW'24, May 13-17, 2024, Singapore, Singapore Table 4: Comparing the Performance of Helen Variants. Notably, both Helen and SAM introduce perturbations to the embedding weights but Helen is characterized with frequency-wise perturbation radius. \u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 Figure 5: Helen exhibits a lower variance across various models in comparison to Adam on the Taobao dataset. evolves when training CTR prediction models with different optimizers, namely Adam, SAM, and Helen. wetrained PNN and DeepFM on the Criteo dataset, utilizing each of the three mentioned optimizers-Adam, SAM, and Helen. For the sake of fair comparison, we maintained a uniform perturbation radius ( 𝜌 ) of 0.05 for both SAM and Helen. It's worth emphasizing that, as specified in Equation 7, the perturbation radius 𝜌 𝑗 𝑘 for a specific embedding in Helen is normalized by the maximum frequency, ensures that it does not exceed the value of 𝜌 .",
  "4.3.1 SAM's Effective Regularization of Hessian Eigenval-": "ues . As depicted in Figure 4, it is evident that SAM effectively reduces the top Hessian eigenvalues across all embeddings. The average top eigenvalue experiences a notable decline, decreasing from 4 . 45 × 10 -4 to 2 . 86 × 10 -4 in PNN and from 4 . 59 × 10 -4 to 4 . 14 × 10 -4 in DeepFM. This observation aligns with findings reported in previous studies [9, 19, 67]. Furthermore, a closer examination of the results in Table 3 affirms that the reduction in the top eigenvalue of the Hessian matrix correlates with an enhancement in the generalization performance of CTR prediction models.",
  "4.3.2 Helen's Superior Hessian Eigenvalue Regularization .": "Remarkably, the results depicted in Figure 4 demonstrate that Helen regularizes the top Hessian eigenvalues more effectively than SAM, with the average top eigenvalue decreasing from 4 . 45 × 10 -4 to 1 . 66 × 10 -4 in PNN, and from 4 . 59 × 10 -4 to 2 . 67 × 10 -4 in DeepFM. Also the standard variance of the top eigenvalues decreases from 1 . 11 × 10 -3 to 3 . 07 × 10 -4 in PNN, and from 1 . 06 × 10 -3 to 6 . 21 × 10 -4 in DeepFM. This indicates a more uniform distribution of sharpness across different embeddings. For more details, please refer to the Appendix A.4. This phenomenon comes from Helen's distinctive perturbation strategy, which places a special emphasis on the frequent features. These frequently occurring features naturally tend to guide the optimization process into sharper local optima, making them substantial contributors to the overall model sharpness. The prioritization of regularization for these frequent features effectively guides the model towards minima that are more generalizable. Wecontend that this frequency-wise Hessian eigenvalue regularization plays a pivotal role in driving Helen's superior performance, in perfect alignment with our overarching objective of enhancing generalization.",
  "4.4 Ablation Study": "In this section, we conduct a comprehensive dissection and evaluation of the individual components and variants comprising the Helen optimizer. Specifically, we assess the impact of perturbations applied to embedding parameters and network parameters individually as the perturbations to embedding parameters and network parameters can be operated independently. When uniform perturbations are applied to all embedding and network parameters, the Helen optimizer essentially reverts to the SAM optimizer. Additionally, we investigate the role of the lower-bound parameter 𝜉 on the perturbation radius 𝜌 𝑗 𝑘 . Wedenote the variant of the original Helen optimizer as Helen𝑚 (indicating Helen-minimal), which exclusively implements frequencywise Hessian eigenvalue regularization for the embedding parameters. Similarly, we introduce Helen𝑏 (representing Helen-base) as another deviation from the original Helen optimizer. Helen𝑏 combine frequency-wise embedding perturbation in conjunction with normal SAM perturbation for network parameters, omitting the lower-bound 𝜉 constraint on the perturbation radius 𝜌 𝑗 𝑘 . To maintain simplicity and clarity, our ablation study is exclusively performed on the Criteo dataset, and the results are thoughtfully summarized in Table 4. And we have the following observations:",
  "4.4.1 Enhanced Performance Through Feature-wise Hessian": "Eigenvalue Regularization . SAMand Helen𝑏 both apply perturbations to both embedding and network parameters. Nevertheless, the key distinction between them lies in their perturbation strategies. Helen𝑏 employs frequency-wise embedding perturbation, WWW'24, May 13-17, 2024, Singapore, Singapore Zirui Zhu, Yong Liu, Zangwei Zheng, Huifeng Guo, and Yang You while SAM employs uniform embedding perturbation. The results, as presented in Table 4, unambiguously demonstrate Helen𝑏 's superiority over SAM in terms of both LogLoss and AUC. This compelling evidence underscores the pivotal role of frequency-wise Hessian eigenvalue regularization in enhancing generalization performance.",
  "4.4.2 The Significance of Embedding Parameter Perturba-": "tion . While Helen𝑚 , exclusively perturbing the embedding parameters, demonstrates its superiority over SAM in both LogLoss and AUC, it is crucial to recognize the added value that regularization of network parameters brings to the table. As evidenced in Table 4, Helen𝑏 surpasses Helen𝑚 by a substantial margin of 0.5‰in terms of AUC, showing a notable enhancement in performance. This highlights the importance of perturbing both embedding and network parameters in optimizing model generalization.",
  "5 RELATED WORKS": "",
  "5.1 Click-Through Rate Prediction": "In the realm of online advertising, metrics like clicks and clickthrough rate (CTR) serve as indicators of the relevance of advertisements from the users' perspective. It is widely acknowledged by both researchers and industry professionals that improving CTR is essential for the sustainable growth of online advertising ecosystems [57, 58]. As a result, there has been a significant research focus on advertising CTR prediction in recent decades [7, 12, 20, 37, 46, 55, 65, 66, 69, 77, 78]. Early works in CTR and user preference prediction were predominantly based on linear regression (LR) [46] and matrix factorization (MF) [45]. In recent years, deep learning has demonstrated remarkable success in CTR prediction, leading to numerous applications in the field [41, 58, 77]. A typical industrial CTR prediction model, as exemplified in studies such as [68, 74, 77], comprises two main types of parameters: sparse embedding and dense network. However, sparse embedding parameters often account for more than 99% of the total parameter count [76].",
  "5.2 Optimizer for Machine Learning": "Optimizers for machine learning algorithms could largely influence the performance and convergence of neural networks. SGD [56] is the earliest and also the most representative optimizer for neural network training, which update model parameters based on gradients computed from randomly sampled subsets of training data. More recently, adaptive gradient algorithms have been proposed and widely used in various tasks, such as computer vision [22] and natural language processing [13]. For example, AdaGrad [17] and RMSProp [23] can dynamically adjust learning rate of each parameter based on their historical gradients. Then, Kingma and Ba proposed Adam, which combines the benefits of both momentumbased and adaptive learning rate approaches for effective optimization. After that, many variants of Adam [15, 42] are proposed and can further improve the performance. While these methods can achieve great performance in most areas. However, to the best of our knowledge, there is still no optimizer specifically for the CTR task. That also motivates us to design a novel optimizer Helen for the CTR task.",
  "5.3 Sharpness and Generalization": "The presence of sharp local minima in deep networks can significantly impact their generalization performance [8, 18, 26, 28, 33, 62]. As a result, numerous recent studies have sought to investigate these sharp local minima and address the associated optimization challenges [8, 14, 19, 21, 33, 36, 64, 70]. Recently, Sharpness-Aware Minimization (SAM) [19] presented a novel approach that simultaneously minimizes loss value and loss sharpness to narrow the generalization gap. SAM and its variants demonstrated state-of-the-art performance and achieved rigorous empirical results across various benchmark experiments [1, 16, 33, 43, 81]. However, it is still not clear whether SAM can benefit to CTR task. Therefore, the primary focus of this paper is to further explore the potential of SAM in enhancing the performance of CTR task.",
  "6 CONCLUSION": "In this study, we have unveiled an intriguing and consistent pattern in CTR prediction models, demonstrating a strong positive correlation between feature frequencies and the top Hessian eigenvalues associated with each feature. Given that the top Hessian eigenvalue serves as a crucial indicator of local minima sharpness, this observation implies that frequent features play a guiding role in steering the optimization process towards sharper local minima. To leverage this insight, we introduce a novel optimizer, Helen, which prioritizes the regularization of the top Hessian eigenvalues of embedding parameters based on their respective feature frequencies. Through an extensive series of experiments, we have illustrated Helen's remarkable effectiveness, consistently outperforming benchmark optimizers including Adam, Nadam, Radam, SAM, and ASAM across three prominent datasets. Our in-depth analysis of Hessian eigenvalues has revealed that Helen's frequency-wise Hessian eigenvalue regularization effectively reduces the sharpness of model parameters and facilitates the optimization process towards more generalizable local minima. Furthermore, we have conducted a comprehensive ablation study to dissect the individual components of Helen and assess their specific contributions to the optimizer's overall performance. This holistic exploration provides valuable insights into the inner workings of Helen and its capacity to enhance generalization in CTR prediction models.",
  "7 ACKNOWLEDGEMENT": "Yang You's research group is being sponsored by NUS startup grant (Presidential Young Professorship), Singapore MOE Tier-1 grant, ByteDance grant, ARCTIC grant, SMI grant (WBS number: A-8001104-00-00), Alibaba grant, and Google grant for TPU usage. This work is sponsored by Huawei Noah's Ark Lab. Helen: Optimizing CTR Prediction Models with Frequency-wise Hessian Eigenvalue Regularization WWW'24, May 13-17, 2024, Singapore, Singapore",
  "REFERENCES": "[1] Momin Abbas, Quan Xiao, Lisha Chen, Pin-Yu Chen, and Tianyi Chen. 2022. Sharp-MAML: Sharpness-Aware Model-Agnostic Meta Learning. In Proceedings of the 39th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 162) , Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (Eds.). PMLR, 10-32. [2] Himan Abdollahpouri, Robin Burke, and Bamshad Mobasher. 2017. Controlling popularity bias in learning-to-rank recommendation. In Proceedings of the eleventh ACM conference on recommender systems . 42-46. [3] Himan Abdollahpouri, Masoud Mansoury, Robin Burke, and Bamshad Mobasher. 2019. The unfairness of popularity bias in recommendation. In Proceedings of the RecSys Workshop on Recommendation in Multistakeholder Environments (RMSE) . [4] Avazu. 2015. Avazu Click-Through Rate Prediction. [5] Stephen Bonner and Flavian Vasile. 2018. Causal embeddings for recommendation. In Proceedings of the 12th ACM conference on recommender systems . 104-112. [6] Léon Bottou. 1991. Stochastic gradient learning in neural networks. Proceedings of Neuro-Nımes 91, 8 (1991), 12. [7] Jianxin Chang, Chenbin Zhang, Yiqun Hui, Dewei Leng, Yanan Niu, Yang Song, and Kun Gai. 2023. Pepnet: Parameter and embedding personalized network for infusing with personalized prior information. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 3795-3804. [8] Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. 2017. Entropy-sgd: Biasing gradient descent into wide valleys. In The 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings . OpenReview.net. https://openreview. net/forum?id=B1YfAfcgl [9] Xiangning Chen, Cho-Jui Hsieh, and Boqing Gong. 2022. When vision transformers outperform resnets without pre-training or strong data augmentations. In International Conference on Learning Representations . [10] Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, et al. 2023. Symbolic discovery of optimization algorithms. arXiv preprint arXiv:2302.06675 (2023). [11] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishikesh B. Aradhye, Glen Anderson, Gregory S. Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and Hemal Shah. 2016. Wide & Deep Learning for Recommender Systems. In Proceedings of the 1st Workshop on Deep Learning for Recommender Systems . [12] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep Neural Networks for YouTube Recommendations. In Proceedings of the 10th ACM Conference on Recommender Systems (RecSys) . 191-198. [13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018). [14] Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. 2017. Sharp minima can generalize for deep nets. In International Conference on Machine Learning . PMLR, 1019-1028. [15] Timothy Dozat. 2016. Incorporating Nesterov Momentum into Adam. In Proceedings of the 4th International Conference on Learning Representations . 1-4. [16] Jiawei Du, Hanshu Yan, Jiashi Feng, Joey Tianyi Zhou, Liangli Zhen, Rick Siow Mong Goh, and Vincent YF Tan. 2021. Efficient sharpness-aware minimization for improved training of neural networks. arXiv preprint arXiv:2110.03141 (2021). [17] John C. Duchi, Elad Hazan, and Yoram Singer. 2010. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning Research 12, 7 (2010). [18] Gintare Karolina Dziugaite and Daniel M Roy. 2017. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. arXiv preprint arXiv:1703.11008 (2017). [19] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. 2021. Sharpness-aware Minimization for Efficiently Improving Generalization. In International Conference on Learning Representations . https://openreview.net/forum? id=6Tm1mposlrM [20] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: A Factorization-Machine based Neural Network for CTR Prediction. In International Joint Conference on Artificial Intelligence (IJCAI) . 1725-1731. [21] Haowei He, Gao Huang, and Yang Yuan. 2019. Asymmetric Valleys: Beyond Sharp and Flat Local Minima. In Advances in Neural Information Processing Systems , Vol. 32. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/ 2019/file/01d8bae291b1e4724443375634ccfa0e-Paper.pdf [22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition . 770-778. [23] Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. 2012. Neural networks for machine learning lecture 6a overview of mini-batch gradient descent. Cited on 14, 8 (2012), 2. [24] Geoffrey Hinton, N Srivastava, K Swersky, T Tieleman, and A Mohamed. 2012. Coursera: Neural networks for machine learning. Lecture 9c: Using noise as a regularizer (2012). [25] Sepp Hochreiter and Jrgen Schmidhuber. 1997. Flat minima. Neural computation 9, 1 (1997), 1-42. [26] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. 2018. Averaging weights leads to wider optima and better generalization. arXiv preprint arXiv:1803.05407 (2018). [27] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. 1991. Adaptive mixtures of local experts. Neural computation 3, 1 (1991), 79-87. [28] Chi Jin, Lydia T Liu, Rong Ge, and Michael I Jordan. 2018. On the local minima of the empirical risk. arXiv preprint arXiv:1803.09357 (2018). [29] Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh, and Jun Sakuma. 2014. Correcting Popularity Bias by Enhancing Recommendation Neutrality. RecSys Posters 10 (2014). [30] Simran Kaur, Jeremy Cohen, and Zachary Chase Lipton. 2023. On the Maximum Hessian Eigenvalue and Generalization. In Proceedings on \"I Can't Believe It's Not Better! - Understanding Deep Learning Through Empirical Falsification\" at NeurIPS 2022 Workshops (Proceedings of Machine Learning Research, Vol. 187) , Javier Antorán, Arno Blaas, Fan Feng, Sahra Ghalebikesabi, Ian Mason, Melanie F. Pradier, David Rohde, Francisco J. R. Ruiz, and Aaron Schein (Eds.). PMLR, 51-65. [31] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. 2017. On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings . OpenReview.net. https://openreview.net/forum?id=H1oyRlYgg [32] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings . http://arxiv.org/abs/1412.6980 [33] Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. 2021. Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks. In International Conference on Machine Learning . PMLR, 59055914. [34] Criteo Labs. 2014. Display Advertising Challenge. [35] Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. 2016. Gradient descent only converges to minimizers. In Conference on learning theory . PMLR, 1246-1257. [36] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. 2017. Visualizing the loss landscape of neural nets. arXiv preprint arXiv:1712.09913 (2017). [37] Zeyu Li, Wei Cheng, Yang Chen, Haifeng Chen, and Wei Wang. 2020. Interpretable Click-Through Rate Prediction through Hierarchical Attention. In The International Conference on Web Search and Data Mining (WSDM) . 313-321. [38] Xiaoliang Ling, Weiwei Deng, Chen Gu, Hucheng Zhou, Cui Li, and Feng Sun. 2017. Model ensemble for click prediction in bing search ads. In Proceedings of the 26th international conference on world wide web companion . 689-698. [39] Dugang Liu, Pengxiang Cheng, Zhenhua Dong, Xiuqiang He, Weike Pan, and ZhongMing. 2020. A general knowledge distillation framework for counterfactual recommendation via uniform data. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . 831-840. [40] Hong Liu, Zhiyuan Li, David Hall, Percy Liang, and Tengyu Ma. 2023. Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training. arXiv preprint arXiv:2305.14342 (2023). [41] Hu Liu, Jing Lu, Hao Yang, Xiwei Zhao, Sulong Xu, Hao Peng, Zehua Zhang, Wenjie Niu, Xiaokun Zhu, Yongjun Bao, et al. 2020. Category-Specific CNN for Visual-aware CTR Prediction at JD. com. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining . 2686-2696. [42] Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han. 2020. On the Variance of the Adaptive Learning Rate and Beyond. In International Conference on Learning Representations . [43] Yong Liu, Siqi Mai, Xiangning Chen, Cho-Jui Hsieh, and Yang You. 2022. Towards efficient and scalable sharpness-aware minimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 12360-12370. [44] Ilya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regularization. In International Conference on Learning Representations . https://openreview.net/ forum?id=Bkg6RiCqY7 [45] Jing Lu, Steven Hoi, and Jialei Wang. 2013. Second order online collaborative filtering. In Asian Conference on Machine Learning . PMLR, 325-340. [46] H. B. McMahan, Gary Holt, D. Sculley, Michael Young, Dietmar Ebner, Julian Grady, Lan Nie, Todd Phillips, Eugene Davydov, Daniel Golovin, Sharat Chikkerur, Dan Liu, Martin Wattenberg, Arnar Mar Hrafnkelsson, Tom Boulos, and Jeremy Kubica. 2013. Ad click prediction: a view from the trenches. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining . [47] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean WWW'24, May 13-17, 2024, Singapore, Singapore Zirui Zhu, Yong Liu, Zangwei Zheng, Huifeng Guo, and Yang You Wu, Alisson G. Azzolini, Dmytro Dzhulgakov, Andrey Mallevich, Ilia Cherniavskii, Yinghai Lu, Raghuraman Krishnamoorthi, Ansha Yu, Volodymyr Kondratenko, Stephanie Pereira, Xianjie Chen, Wenlin Chen, Vijay Rao, Bill Jia, Liang Xiong, and Mikhail Smelyanskiy. 2019. Deep Learning Recommendation Model for Personalization and Recommendation Systems. arXiv abs/1906.00091 (2019). [48] Yurii Nesterov. 1983. A method for unconstrained convex minimization problem with the rate of convergence O (1/k2). In Dokl. Akad. Nauk. SSSR , Vol. 269. 543. [49] Gal Oestreicher-Singer and Arun Sundararajan. 2012. Recommendation networks and the long tail of electronic commerce. Mis quarterly (2012), 65-83. [50] Robin Pemantle. 1990. Nonconvergence to unstable points in urn models and stochastic approximations. The Annals of Probability 18, 2 (1990), 698-712. [51] Boris T Polyak. 1964. Some methods of speeding up the convergence of iteration methods. Ussr computational mathematics and mathematical physics 4, 5 (1964), 1-17. [52] Ning Qian. 1999. On the momentum term in gradient descent learning algorithms. Neural networks 12, 1 (1999), 145-151. [53] Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, and Jun Wang. 2016. Product-Based Neural Networks for User Response Prediction. In Proceedings of the IEEE 16th International Conference on Data Mining (ICDM) . 1149-1154. [54] Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. 2019. On the convergence of adam and beyond. arXiv preprint arXiv:1904.09237 (2019). [55] Matthew Richardson, Ewa Dominowska, and Robert Ragno. 2007. Predicting clicks: estimating the click-through rate for new ads. In Proceedings of the 16th international conference on World Wide Web . 521-530. [56] Herbert Robbins and Sutton Monro. 1951. A stochastic approximation method. The annals of mathematical statistics (1951), 400-407. [57] Helen Robinson, Anna Wysocka, and Chris Hand. 2007. Internet advertising effectiveness: the effect of design on click-through rates for banner ads. International journal of advertising 26, 4 (2007), 527-541. [58] Rómer Rosales, Haibin Cheng, and Eren Manavoglu. 2012. Post-click conversion modeling and analysis for non-guaranteed delivery display advertising. In Proceedings of the fifth ACM international conference on Web search and data mining . 293-302. [59] Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. 2018. Empirical analysis of the hessian of over-parametrized neural networks. In International Conference on Learning Representations (Workshop Track) . [60] Tobias Schnabel, Adith Swaminathan, Ashudeep Singh, Navin Chandak, and Thorsten Joachims. 2016. Recommendations as Treatments: Debiasing Learning and Evaluation. In Proceedings of The 33rd International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 48) , Maria Florina Balcan and Kilian Q. Weinberger (Eds.). PMLR, New York, New York, USA, 1670-1679. [61] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538 (2017). [62] Samuel L Smith and Quoc V Le. 2017. A bayesian perspective on generalization and stochastic gradient descent. arXiv preprint arXiv:1710.06451 (2017). [63] Taobao. 2018. Alibaba Ad Display/Click Data Prediction. [64] Yusuke Tsuzuku, Issei Sato, and Masashi Sugiyama. 2020. Normalized flat minima: Exploring scale invariant definition of flat minima for neural networks using pac-bayesian analysis. In International Conference on Machine Learning . PMLR, 9636-9647. [65] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & Cross Network for Ad Click Predictions. In Proceedings of the 11th International Workshop on Data Mining for Online Advertising (ADKDD) . 12:1-12:7. [66] Ruoxi Wang, Rakesh Shivanna, Derek Zhiyuan Cheng, Sagar Jain, Dong Lin, Lichan Hong, and Ed H. Chi. 2021. DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems. In Proceedings of the Web Conference 2021 . [67] Kaiyue Wen, Tengyu Ma, and Zhiyuan Li. 2022. How Sharpness-Aware Minimization Minimizes Sharpness?. In International Conference on Learning Representations . [68] Minhui Xie, Kai Ren, Youyou Lu, Guangxu Yang, Qingxing Xu, Bihai Wu, Jiazhen Lin, Hongbo Ao, Wanhong Xu, and Jiwu Shu. 2020. Kraken: memory-efficient continual learning for large-scale real-time recommendations. In International Conference for High Performance Computing, Networking, Storage, and Analysis (SC) . [69] Ling Yan, Wu-Jun Li, Gui-Rong Xue, and Dingyi Han. 2014. Coupled group lasso for web-scale ctr prediction in display advertising. In International conference on machine learning . PMLR, 802-810. [70] Mingyang Yi, Qi Meng, Wei Chen, Zhi-ming Ma, and Tie-Yan Liu. 2019. Positively scale-invariant flatness of relu neural networks. arXiv preprint arXiv:1903.02237 (2019). [71] Hongzhi Yin, Bin Cui, Jing Li, Junjie Yao, and Chen Chen. 2012. Challenging the long tail recommendation. Proceedings of the VLDB Endowment 5, 9 (2012), 896-907. [72] Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank Reddi, Sanjiv Kumar, and Suvrit Sra. 2020. Why are adaptive methods good for attention models? Advances in Neural Information Processing Systems [81] Juntang Zhuang, Boqing Gong, Liangzhe Yuan, Yin Cui, Hartwig Adam, Nicha Dvornek, Sekhar Tatikonda, James Duncan, and Ting Liu. 2022. Surrogate gap minimization improves sharpness-aware training. arXiv preprint arXiv:2203.08065 (2022). Helen: Optimizing CTR Prediction Models with Frequency-wise Hessian Eigenvalue Regularization WWW'24, May 13-17, 2024, Singapore, Singapore",
  "A APPENDIX": "",
  "A.1 Pseudocode of Helen": "For a practical and comprehensive understanding, we present the pseudocode of the Helen optimizer here.",
  "Algorithm 1 Helen": "Require: number of steps 𝑇 , batch size 𝑏 , learning rate 𝜂 , perturbation radius 𝜌 > 0, lower-bound 𝜉 1: for 𝑡 ← 1 to 𝑇 do 2: Draw 𝑏 samples B from S 3: 𝒈 ← 1 𝑏 ˝ 𝒙 ∈ 𝐵 ∇ 𝒘 LB( 𝒘 ) // Dense weights perturbation 4: ˆ 𝝐 ( 𝒘 ) ← [ 𝜌 · 𝒈 𝒉 ∥ 𝒈 𝒉 ∥ ] 5: for each field 𝑗 and each feature 𝑘 in the field do 6: 𝑁 𝑗 𝑘 (S) ← ˝ 𝑛 𝑖 = 1 𝒙 𝑗 𝑖 [ 𝑘 ]    // Embedding perturbation 9: Append ˆ 𝝐 ( 𝒆 𝑗 𝑘 ) to ˆ 𝝐 ( 𝒘 ) 10: end for 11: Compute Helen gradient 𝒘 𝐻𝑒𝑙𝑒𝑛 12: ← 𝒘 - 𝜂 · 𝒈 13: end for",
  "A.2 Dataset Description": "We present essential statistics for three datasets in Table 5. Table 5: Key Statistics of Three Benchmark Datasets. Datasets are preprocessed by BARS, which includes the filtering of infrequent categorical features.",
  "A.3 Overall Performance On The Taobao Dataset": "Table 6 summarizes the overall performance of seven models trained with different optimizers on the Taobao dataset.",
  "A.4 Hessian Eigenspectrum Analysis": "Figure 6 visualizes the relationship between the top eigenvalue of the Hessian matrix and the frequency of the features on all three datasets.",
  "A.5 CTR Prediction Models Description": "· DNN : DNN [12] pioneers a large-scale recommendation system powered by deep neural networks. In contrast to traditional linear models, DNN employs a fully-connected network to create dense 𝒈 𝐻𝑒𝑙𝑒𝑛 𝑤 𝒘 + ˆ 𝝐 ( 𝒘 ) = ∇ LB( 𝒘 )| vector representations for users. Candidate items are selected via top-N nearest neighbor search, and a neural network, incorporating candidate representations and contextual data, produces the final predictions. · Wide&Deep : Wide&Deep [11] innovatively combines the advantages of traditional shallow linear models and deep neural networks within a unified learning framework. It comprises two integral components: the Wide Component, featuring a generalized linear model with cross-product transformation, and the Deep Component, which is a feed-forward neural network. · PNN : PNN [53] consists of three key elements: an embedding layer for learning distributed representations of categorical features, a product layer for capturing feature interactions, and final fully-connected layers to make the prediction. This streamlined architecture enhances its capacity for handling multi-field categorical data with minimal computational overhead. · DeepFM : DeepFM [20] is a deep learning-based CTR prediction model that combines the power of factorization machines (FM) and deep neural networks (DNN). The FM component of DeepFM is responsible for learning the low-order feature interactions, while the DNN component is responsible for learning the highorder feature interactions. · DCN : The DCN [65] introduces an additional cross-network that explicitly generates feature interactions alongside the standard DNNarchitecture. This approach applies feature crossing at each layer, enabling high-degree interactions across features without the need for manual feature engineering, all while introducing minimal extra complexity to the DNN model. · DLRM : DLRM [47] utilizes an MLP for continuous feature processing, computes second-order interactions between categorical embeddings and dense features, and produces predictions through a sigmoid-activated MLP. It efficiently reduces dimensionality by only considering cross-terms created via dot-products between embedding pairs in the final MLP layer, resembling factorization machines. · DCNv2 : DCNv2 [66] builds upon the straightforward crossnetwork architecture of DCN. It enhances this architecture by integrating it with a deep neural network to facilitate the discovery of complementary implicit interactions. Moreover, DCNv2 incorporates low-rank techniques and adopts the Mixture-ofExpert architecture [27, 61] to enhance computational efficiency and reduce cost.",
  "A.6 Optimizers Description": "We will cover the mentioned seven optimizers in detail in this section. · Adam : Adam [32] is a highly acclaimed optimization algorithm in the field of deep learning, renowned for its robustness and effectiveness. This algorithm combines the advantages of RMSprop [24] and Momentum [51] by tracking the moving averages of both first-order and second-order gradient moments to dynamically adapt the learning rate, making it a pivotal tool in the training of deep neural networks. · Nadam : Nadam [15] is an extension of Adam optimizer, further enhances the Adam optimizer by replacing momentum component with Nesterov's accelerated gradient (NAG) [48], which has WWW'24, May 13-17, 2024, Singapore, Singapore Zirui Zhu, Yong Liu, Zangwei Zheng, Huifeng Guo, and Yang You Table 6: Overall performance on the Taobao dataset. 0 100 200 300 400 Frequency of Features 0.0 0.5 1.0 1.5 Top Eigenvalue max ×10 2 Adam Helen 0 100 200 300 400 500 Frequency of Features 0.0 1.0 2.0 3.0 Top Eigenvalue max ×10 2 Adam Helen 0 500 1000 1500 Frequency of Features 0.0 0.5 1.0 1.5 2.0 Top Eigenvalue max ×10 2 Adam Helen (a) PNN in Avazu (c) PNN in Taobao 0 100 200 300 400 Frequency of Features 0.0 0.2 0.4 0.6 0.8 1.0 Top Eigenvalue max ×10 2 Adam Helen",
  "(b) PNN in Criteo": "0 100 200 300 400 500 Frequency of Features 0.0 0.5 1.0 1.5 2.0 2.5 Top Eigenvalue max ×10 2 Adam Helen (e) DeepFM in Criteo 0 500 1000 1500 Frequency of Features 0.0 0.3 0.6 0.9 1.2 1.5 Top Eigenvalue max ×10 Adam Helen 2 (d) DeepFM in Avazu (f) DeepFM in Taobao Figure 6: Dominate Hessian eigenvalues of PNN and DeepFM in three datasets trained with Adam and Helen. a provably better bound. This adaptation allows Nadam to make more informed updates to model parameters and accelerates convergence, making it a robust and efficient choice for training neural networks. approach is equivalent to optimizing the loss function while concurrently penalizing a measure of the model's sharpness without calculating the Hessian matrix, which is computationally expensive. · Radam : Radam [42], or Rectified Adam, is a variant of Adam optimizer, by introducing a term to rectify the variance of the adaptive learning rate, especially in the early stage of the training process. Its motivation comes from the consistent improvement of the warmup training strategy, which is widely used in the training of deep neural networks. · SAM : SAM [19] represents a recent advance in enhancing the generalization performance of neural networks. SAM takes into account the geometry of the loss landscape by minimizing the loss function with respect to perturbed model parameters. This · ASAM : ASAM [33] stands as an adaptive iteration of SAM, tailored to dynamically fine-tune the perturbation radius for each layer's parameters. ASAM is claimed to possess scale-invariant property, achieved by adjusting the perturbation radius in relation to the weights' scale. This adaptive adjustment consistently yields improved generalization performance across various neural network architectures and tasks. A.6.1 Implementation details. Our experiment is conducted using FuxiCTR 1 [79, 80], an open-source framework for CTR prediction. 1 https://github.com/xue-pai/FuxiCTR Helen: Optimizing CTR Prediction Models with Frequency-wise Hessian Eigenvalue Regularization WWW'24, May 13-17, 2024, Singapore, Singapore We strictly adhere to the benchmark's coding standards for CTR model implementation, data processing, and model evaluation. In our experiment, we rely on the official PyTorch implementations of Adam, Nadam, Radam, and an open-source PyTorch implementation 2 of SAM. For ASAM, we employ the official implementation 3 provided by the authors. Regarding model hyperparameters, we adopt the benchmark settings reported by FuxiCTR and keep them constant for all the optimizers. As for optimizer hyperparameters, the learning rate remains fixed at 𝜂 = 1 × 10 -3 since all the tested optimizers incorporate an adaptive learning rate mechanism and are not particularly sensitive to changes in this parameter. In terms of L2-regularization coefficients, we explore values from the set { 1 × 10 -4 , 1 × 10 -5 , 0 } , maintaining consistency with the search space employed in BARS. For SAM, ASAM, and Helen, we perform tuning on the perturbation radius 𝜌 , considering values from the set { 0 . 05 , 0 . 01 , 0 . 005 , 0 . 001 } . In the case of Helen, the lower bound 𝜉 is varied within { 0 , 0 . 5 } . 2 https://github.com/davda54/sam 3 https://github.com/SamsungLabs/ASAM",
  "keywords_parsed": [
    "None"
  ]
}