{
  "Unified Low-rank Compression Framework for Click-through Rate Prediction": "Hao Yu yuh@lamda.nju.edu.cn Nanjing University Nanjing, Jiangsu, China Minghao Fu fumh@lamda.nju.edu.cn Nanjing University Nanjing, Jiangsu, China Jiandong Ding jdding.cs@gmail.com Researcher Shanghai, China Yusheng Zhou zhou-yusheng@foxmail.com Researcher Shanghai, China Jianxin Wu wujx2001@nju.edu.cn Nanjing University Nanjing, Jiangsu, China",
  "ABSTRACT": "Deep Click-Through Rate (CTR) prediction models play an important role in modern industrial recommendation scenarios. However, high memory overhead and computational costs limit their deployment in resource-constrained environments. Low-rank approximation is an effective method for computer vision and natural language processing models, but its application in compressing CTR prediction models has been less explored. Due to the limited memory and computing resources, compression of CTR prediction models often confronts three fundamental challenges, i.e., (1). How to reduce the model sizes to adapt to edge devices? (2). How to speed up CTR prediction model inference? (3). How to retain the capabilities of original models after compression? Previous low-rank compression research mostly uses tensor decomposition, which can achieve a high parameter compression ratio, but brings in AUC degradation and additional computing overhead. To address these challenges, we propose a unified low-rank decomposition framework for compressing CTR prediction models. We find that even with the most classic matrix decomposition SVD method, our framework can achieve better performance than the original model. To further improve the effectiveness of our framework, we locally compress the output features instead of compressing the model weights. Our unified low-rank compression framework can be applied to embedding tables and MLP layers in various CTR prediction models. Extensive experiments on two academic datasets and one real industrial benchmark demonstrate that, with 3-5 × model size reduction, our compressed models can achieve both faster inference and higher AUC than the uncompressed original models. Our code is at https://github.com/yuhao318/Atomic_Feature_Mimicking.",
  "CCS CONCEPTS": "· Information systems → Recommender systems . Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. KDD '24, August 25-29, 2024, Barcelona, Spain. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0490-1/24/08...$15.00 https://doi.org/10.1145/3637528.3671520",
  "KEYWORDS": "Recommendation Systems, CTR Prediction, Model Compression, Low-rank Approximation",
  "ACMReference Format:": "Hao Yu, Minghao Fu, Jiandong Ding, Yusheng Zhou, and Jianxin Wu. 2024. Unified Low-rank Compression Framework for Click-through Rate Prediction. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '24), August 25-29, 2024, Barcelona, Spain. ACM,NewYork,NY,USA,14pages.https://doi.org/10.1145/3637528.3671520",
  "1 INTRODUCTION": "Benefiting from the large model sizes and powerful neural architectures, deep learning recommendation models (DLRMs) show unparalleled advantages in mining users' potential interests. To improve the capabilities of such models, a common method in practice is to increase the model sizes. However, these recommender systems rely heavily on abundant storage, huge memory access, and large computing resources for frequent and swift inference requested by millions of concurrent users. Although enlarging the model size is effective, it can also become a major obstacle to model deployment and real-time predictions, especially for devices with limited resources. Therefore, recommendation model compression has attracted immense interest in recent years. Generally speaking, modern deep CTR prediction models can be divided into three main modules, i.e., the embedding tables process categorical features by encoding sparse, high-dimensional inputs into a dense vector representation (i.e. embeddings), the MLP layers are used for feature interaction modeling and prediction, and the feature interaction module learns cross representations of highorder features. Each feature field has a unique embedding, which is stored in the embedding tables. However, in actual industrial scenarios, there are usually billions or even trillions of categorical features, and the embedding tables may require hundreds of GB or even TB storage space. As a result, the embedding layers occupy most of the parameters in a CTR prediction model. Moreover, the MLP layers correspondingly occupy most of the computational cost during the inference process, but the effort to compress it remains scarce. Therefore, we believe that to efficiently and economically deploy a CTR prediction model in an actual production system, its embedding tables and MLP layers both need to be compressed. Low-rank decomposition methods, including matrix decomposition and tensor decomposition methods, are widely used in various KDD '24, August 25-29, 2024, Barcelona, Spain. Hao Yu, Minghao Fu, Jiandong Ding, Yusheng Zhou & Jianxin Wu model compression tasks. The most classic method of matrix decomposition is Singular Value Decomposition (SVD). However, it is rarely explored in the field of compressing deep CTR prediction models. Traditional tensor decomposition methods, such as Tensor-Train Decomposition (TTD), need to re-multiply all tensors to obtain the embedding during inference, so it will introduce abundant additional computational overhead. However, the embedded tables of industrial DLRMs may reach hundreds of GB or even TB levels, far exceeding the maximum tens of GB memory capacity of a single GPU, so most companies deploy their recommendation models on CPUs. Therefore, the decompression calculation caused by tensor decomposition will greatly slow down the model's inference speed. Besides, tensor decomposition is also a lossy compression method that reduces the models' AUC, and every 1‰ drop in AUC represents a significant loss of economic benefits. Moreover, Tensor-Train Decomposition is only applicable to the embedding layer, and there is currently a lack of research on applying tensor decomposition in the MLP layers. Hence, we believe that in order to successfully perform lowrank approximation to compressing CTR prediction models, we need an effective framework that can be easily generalized into various DLRMs, reduces the embedding layer sizes to achieve a higher parameter compression ratio, further compresses the MLP layers to obtain faster model acceleration, and still maintains or even improves the models' capacity after compression. To fulfill these goals, we propose a unified low-rank decomposition method, which can be used on both the MLP layers and embedding tables of the CTR prediction models. With our compression framework, even the classical matrix decomposition SVD method can obtain better performances than the original models. To further improve the AUC of the compressed models, we design a novel way to replace SVD with Atomic Feature Mimicking (AFM) [31]. AFM's idea stems from a simple but crucial realization: when compressing a deep learning model, we should focus on minimizing the loss of the model outputs, rather than the weight [16, 31]. Therefore, AFM utilizes the PCA technology to low-rank approximate MLP outputs. Inspired by this idea, to compress the MLP layers, we introduce and improve AFM to make it work properly for MLP in CTR prediction models. To further improve the model's performance, we add an extra activation function between the two decomposed linear layers. Then, for the large-scale embedding layers of the CTR prediction models, we further extend AFM for the embedding tables to reduce its dimensions. The compressed weights can be incorporated into the original model to achieve further acceleration. Since these two compressing methods are orthogonal, we can naturally combine them to obtain higher compression ratios. Our contributions are as follows: · Our paper analyzes issues with traditional low-rank decomposition, i.e., its limited scope of application and poor performance. Therefore, we propose a unified low-rank decomposition framework for compressing the MLP layers and the embedding tables of the CTR prediction models. · Unlike standard low-rank decomposition, our plug-and-play framework can be easily generalized to mainstream CTR prediction models and greatly improve their AUC by mimicking feature distributions to achieve better output approximation. · Abundant experiments prove the effectiveness of our framework. On two academic datasets and a real industry recommendation dataset, our methods achieve 3-5 × compression ratios, significantly outperform the original models' AUC scores, and largely reduce the models' inference time.",
  "2 RELATED WORK": "Our work is connected to several themes in the literature, which we describe next.",
  "2.1 Deep CTR Prediction Models": "Many deep learning recommendation models have been proposed over the past years and have achieved state-of-the-art performances in CTR prediction tasks. Wide & Deep [2] jointly trains wide linear models and deep neural networks to combine the benefits of memorization and generalization for recommender systems. DCN [25] introduces a novel cross-network to capture certain bounded-degree feature interactions efficiently. DeepFM [7] combines the power of Factorization Machines (FM) [19] for recommendation and Multilayer Perceptron (MLP) layers for feature learning. NFM [9] introduces the Bi-Interaction pooling operation in neural network modeling. AutoInt [22] applies multi-head self-attention [23] to automatically learn high-order feature interactions and efficiently handle large-scale high-dimensional sparse data. FiBiNet [12] applies the SENet [11] mechanism to dynamically learn the weights of features. AFN [3] proposes the logarithmic transformation layer to learn the power of each feature in a feature combination. DCNv2 [26] leverages low-rank techniques to approximate feature crosses in a subspace for better performance. GDCN [24] proposes a gated deep cross network and a field-level dimension optimization approach. In this paper, we will show that our compression approach can handle multiple deep learning recommendation models and generally achieve better performances than the original models.",
  "2.2 Low-Rank Compression for CTR Prediction Models": "CTR prediction models tend to have a large number of parameters and are computationally intensive. To reduce parameter sizes and speed up model inference, a natural idea is to factorize one weight into two or more smaller matrices. There are two pipelines for low-rank decomposing the CTR prediction models, i.e., directly performing matrix decomposition, or first reshaping the weight matrix into a tensor, and then applying tensor decomposition on it. For the matrix decomposition methods, a common technique for low-rank factorization is SVD [1, 6], which can be used in all linear layers. The dimension of Mixed Dimension (MD) Embeddings [5] varies with its query frequency. For the tensor decomposition method, TT-Rec [30] applies Tensor-Train Decomposition (TTD) [10] to compress the embedding layers. Xia et al. [28] introduce semitensor product based tensor-train decomposition (STTD) for higher compression rates of the embedding table. In previous low-rank approaches such as TT-Rec, a high parameter compression ratio can be reached. Nevertheless, the flexibility of previous tensor decomposition methods is limited, because it can only be applied to the embeddings. However, the most computation-intensive module in DLRMs is the Unified Low-rank Compression Framework for Click-through Rate Prediction KDD '24, August 25-29, 2024, Barcelona, Spain. MLP layer. Besides, it introduces abundant extra computational overhead and takes a too long time for inference. By contrast, our unified framework is the first attempt to simultaneously decompose both embedding tables and MLP weights and achieve higher speeds by low-rank compression in CTR prediction models.",
  "2.3 Other Compression Methods for CTR Prediction Models": "Parameter pruning is another useful technique for striking a balance between model accuracy and inference speed by cutting out redundant parameters. Plug-in Embedding Pruning (PEP) [14] obtains a mixed-dimension embedding scheme by adaptively learning pruning threshold from data. UMEC [20] formulates the joint input feature selection and model compression task as a constrained optimization problem. Then it solves the DLRMs compression task by the alternating direction method of the multipliers algorithm. SSEDS [18] proposes a single-shot embedding pruning method. It first pre-trains a traditional CTR prediction model with unified embedding dimensions. Then it utilizes the proposed criterion which could measure the importance of embedding dimensions only in one forward-backward pass. Besides network pruning, quantization and hashing methods are also commonly used compression recommendation system methods. Product quantization [13] decomposes the space into a Cartesian product of low-dimensional subspaces and quantizes each subspace separately. Random Offset Block Embedding (ROBE) [4] uses hash functions on embedding tables to locate it in a small circular array of memory. Binary Hash (BH) [29] uses a binary code based hash embedding method to reduce the size of the embedding table in arbitrary scale. Those methods have achieved a high compression ratio in recommendation systems and our approach may be potentially combined with them. Note that though some modern CTR prediction model compression algorithms can achieve higher compression ratios, they often bring a degree of performance degradation, which leads to a large decline in industry revenue. Therefore, if there is no suitable compression algorithm that can improve AUC, algorithm engineers often do not compress the recommended models.",
  "3 METHODS": "In this section, we first describe our framework, starting by introducing traditional matrix and tensor decomposition methods. Then we design two different low-rank approximation algorithms for introducing AFM into the embedding tables and the MLP modules of CTR prediction models, respectively. Throughout our compression process, both algorithms can generally improve the AUC with both fewer parameters and higher speeds.",
  "3.1 Classical Low-Rank Compression Methods": "Traditional low-rank decomposition methods, including matrix and tensor decomposition, are widely used in model compression tasks. The most classical method of matrix decomposition is SVD. Specifically, let us consider a matrix 𝑀 ∈ R 𝑑 1 × 𝑑 2 , a typical way to compress this matrix is to perform SVD on 𝑀 , i.e., 𝑀 = 𝑈𝑆𝑉 ⊤ , where 𝑈 ∈ R 𝑑 1 × 𝑑 1 and 𝑉 ∈ R 𝑑 2 × 𝑑 2 are orthonormal matrices. 𝑆 ∈ R 𝑑 1 × 𝑑 2 is a diagonal rectangular matrix containing singular values in the decreasing order. If we only use the largest 𝑘 terms of the singular values, the resulting matrix is an optimal approximation of 𝑀 with a lower rank 𝑘 < min ( 𝑑 1 , 𝑑 2 ) : 𝑀 ≈ 𝑀 1 𝑀 2, where 𝑀 1 ∈ R 𝑑 1 × 𝑘 and 𝑀 2 ∈ R 𝑘 × 𝑑 2 are the rank𝑘 approximation matrices by taking 𝑀 1 = 𝑈𝑆 1 2 𝑘 and 𝑀 2 = 𝑆 1 2 𝑘 𝑉 ⊤ , and 𝑆 1 2 𝑘 is a diagonal matrix formed by the square-roots of the corresponding top 𝑘 singular values in 𝑆 . After this low-rank approximation, the number of parameters in this matrix decreases from 𝑂 ( 𝑑 1 𝑑 2 ) to 𝑂 (( 𝑑 1 + 𝑑 2 ) 𝑘 ) . Similar to matrix decomposition, Tensor-Train Decomposition (TTD) is a simple and robust approach to decompose tensor representation of multidimensional data into a product of smaller tensors. Assume a tensor 𝑇 ∈ R 𝑑 1 × 𝑑 2 ×···× 𝑑 𝑛 is a 𝑛 -dimensional tensor, then we can apply TTD on 𝑇 , i.e., 𝑇 ≈ 𝑇 1 𝑇 2 · · · 𝑇 𝑛 , where 𝑇 𝑖 ∈ R 𝑟 𝑖 -1 × 𝑑 𝑖 × 𝑟 𝑖 and 𝑟 0 = 𝑟 𝑛 = 1 to keep the product of the sequence of tensors a scalar. The sequence { 𝑟 𝑖 } 𝑛 𝑖 = 0 is referred to as TT-ranks, and each 3-dimension tensor 𝑇 𝑖 is called a TT-core. The TTD method can also be generalized to compress a matrix 𝑀 ∈ R 𝑚 × 𝑛 . We assume that 𝑚 and 𝑛 can be factorized into sequences of integers, i.e., 𝑚 = ˛ 𝑘 𝑖 = 1 𝑚 𝑖 and 𝑛 = ˛ 𝑘 𝑖 = 1 𝑛 𝑖 . Correspondingly, we reshape the matrix 𝑀 as a 2 𝑛 -dimensional tensor 𝑀 ′ ∈ 𝑅 ( 𝑚 1 × 𝑛 1 ) × ( 𝑚 2 × 𝑛 2 ) · · · ( 𝑚 𝑘 × 𝑛 𝑘 ) . Then 𝑀 ′ ≈ 𝑀 ′ 1 𝑀 ′ 2 · · · 𝑀 ′ 𝑘 , where 𝑀 ′ 𝑖 ∈ R 𝑟 𝑖 -1 × 𝑚 𝑖 × 𝑛 𝑖 × 𝑟 𝑖 and 𝑟 0 = 𝑟 𝑛 = 1. Let ¯ 𝑟 , ¯ 𝑚 , and ¯ 𝑛 be the maximal values of sequences { 𝑟 𝑖 } , { 𝑚 𝑖 } and { 𝑛 𝑖 } for 𝑖 in { 1 , · · · , 𝑑 } , then TTD reduces the space for storing the matrix from 𝑂 ( 𝑚𝑛 ) to 𝑂 ( 𝑘 ¯ 𝑟 2 ¯ 𝑚 ¯ 𝑛 ) . Please note that in many practical scenarios, it is difficult to find a suitable sequence { 𝑚 𝑖 } and { 𝑛 𝑖 } to accurately decompose 𝑚 and 𝑛 , so researchers often add some extra rows and columns to M and then perform TTD compression. To the best of our knowledge, low-rank decomposition has not been fully studied in the area of compressing CTR prediction models. Traditional matrix decomposition methods, such as SVD, have rarely been explored in the field of compressing recommendation systems. Besides, traditional tensor decomposition methods (e.g., TTD) are difficult to apply in MLP layers because they will damage the original structure of the MLP weight, which causes trouble for inference and leads to lower inference speed. In addition, the MLP layer tends to have fewer parameters, and using tensor decomposition methods is not helpful in this aspect either. Besides, embedding table lookup is originally a low computational cost operation. However, to obtain the embeddings, TTD needs to collect all tensors and recalculate embedding tables again. Therefore, although TTD can heavily reduce the model's parameter sizes (e.g., 100 × ) [30], it will greatly increase the inference overhead, which limits its practicality. These pressing difficulties prompt us to come up with novel low-rank compression solutions for CTR prediction tasks.",
  "3.2 Atomic Feature Mimicking for MLP Layers": "Let us consider a fully-connected layer 𝑦 = 𝑊𝑥 + 𝑏 in CTR prediction models, whose 𝑥 ∈ R 𝑛 × 𝑐 , 𝑦 ∈ R 𝑚 × 𝑐 and 𝑊 ∈ R 𝑚 × 𝑛 , 𝑏 ∈ R 𝑚 . The optimization target of previous low-rank decomposition algorithms is often aimed at a single matrix or tensor, i.e., min ∥ 𝑊 -𝑊 𝑟 ∥ 2 , where 𝑊 𝑟 is the low-rank approximation of 𝑊 . However, compared with only decomposing 𝑊 , we also need to consider the distribution of the input 𝑥 , that is, min ∥ 𝑊𝑥 -𝑊 𝑟 𝑥 ∥ 2 is a better choice [8, 31]. Therefore, following the notation described in [27, 31], now we introduce Atomic Feature Mimicking (AFM [31]), which seeks to factorize the output features as opposed to decomposing the model KDD '24, August 25-29, 2024, Barcelona, Spain. Hao Yu, Minghao Fu, Jiandong Ding, Yusheng Zhou & Jianxin Wu weights. Let us treat the output feature in R 𝑚 × 𝑐 as 𝑐 instantiations of the random feature vector 𝑦 (each in R 𝑚 ), and compute the covariance matrix:  where E [·] is the expectation operator. Since Cov ( 𝑦 ) is positive semi-definite, its eigendecomposition (i.e., the principal component analysis or PCA) is Cov ( 𝑦 ) = 𝑈𝑆𝑈 ⊤ . We only keep the top 𝑘 eigenvalues and extract the first 𝑘 columns of 𝑈 ∈ R 𝑚 × 𝑚 into 𝑈 𝑘 ∈ R 𝑚 × 𝑘 and 𝑈 𝑘 𝑈 ⊤ 𝑘 ≈ 𝐼 . The classic PCA knowledge tells us the low-rank representation of 𝑦 is 𝑈 ⊤ 𝑘 ( 𝑦 -E [ 𝑦 ]) , and 𝑦 can be approximated as E [ 𝑦 ] + 𝑈 𝑘 𝑈 ⊤ 𝑘 ( 𝑦 -E [ 𝑦 ]) . Hence,   This approximation is proved optimal [27]. Then, one linear layer can be transformed into two:   where the first FC layer has weights 𝑈 ⊤ 𝑘 𝑊 ∈ R 𝑘 × 𝑛 , and the second one has weights 𝑈 𝑘 ∈ R 𝑚 × 𝑘 and bias E [ 𝑦 ] + 𝑈 𝑘 𝑈 ⊤ 𝑘 ( 𝑏 -E [ 𝑦 ]) ∈ R 𝑚 . Note that because collecting output 𝑦 during inference would be a memory-greedy process, in practice we adaptively update E [ 𝑦𝑦 ⊤ ] and E [ 𝑦 ] in a streaming fashion instead of storing all output features. We will perform inference on the entire training set, and then collect the E [ 𝑦𝑦 ⊤ ] and E [ 𝑦 ] of each FC layer in the MLP of the recommendation system to compress them. Furthermore, surprisingly, our subsequent experiments will show that for the MLP module in CTR prediction models, after initializing the compressed model parameters by AFM, adding an extra activation function between 𝑈 𝑇 𝑘 𝑊 and 𝑈 𝑘 only slightly degrades AUC. Moreover, this strategy can obtain higher AUC scores after fine-tuning. Therefore, we improve AFM by adding ReLU [17] activation functions into the model structure after compression and then fine-tune the whole compressed model for 1 epoch. For CTR prediction models, compared with traditional matrix decomposition methods (such as SVD), because AFM imitates the output 𝑦 rather than the weight 𝑊 , the AUC drop it brings will be much smaller. Therefore, AFM is a better matrix-decomposition initialization method. Compared with traditional tensor decomposition methods like TTD, AFM obtains higher speed acceleration because it requires fewer multiplication calculations.",
  "3.3 Compressing the Embedding Tables": "The embedding tables map every sparse categorical feature to realvalued dense vectors. In this subsection, we will show how AFM can be made to compress the embedding tables. Let us denote an input vector as 𝑣 ∈ R 𝑑 , in which 𝑣 𝑖 is its 𝑖 -th feature field. 𝑣 𝑖 can be either a continuous or nominal variable. For notational simplicity, we assume 𝑣 𝑖 has an embedding e 𝑖 . When 𝑣 𝑖 is continuous, e 𝑖 = 𝑣 𝑖 ∈ R and it does not require embedding tables, so we do not consider continuous variables. When it is nominal, 𝑣 𝑖 is in fact an index value and there is an embedding table 𝐷 𝑖 ∈ R 𝑡 × 𝑠 𝑖 associated with 𝑣 𝑖 . Here both 𝑡 and 𝑠 𝑖 are constant values, i.e., 𝑡 is the embedding dimensionality shared by all fields, and 𝑠 𝑖 is the number of possible items in the dictionary such that 𝑣 𝑖 ∈ { 1 , 2 , · · · , 𝑠 𝑖 } . Normally 𝐷 𝑖 is a dense matrix, and e 𝑖 = 𝐷 𝑖 ( : , 𝑣 𝑖 ) ∈ R 𝑡 × 1 . Now the output 𝑦 of the embedding layer is  The nominal feature embedding layer 𝐷 = { 𝐷 1 , 𝐷 2 , · · · , 𝐷 𝑑 } occupies most of the parameters in the models and now our goal is to compress the embedding dimensionality 𝑡 . Here, we compute the covariance matrix independently for each single categorical field. Let us consider the embedding table 𝐷 𝑖 . In particular, the embedding table lookup operation also can be regarded as a fullyconnected layer, i.e., e 𝑖 = 𝐷 𝑖 ( : , 𝑣 𝑖 ) = 𝐷 𝑖 𝑥 𝑣 𝑖 . Here 𝑥 𝑣 𝑖 ∈ R 𝑠 𝑖 × 1 is a one-hot vector, i.e., the 𝑣 𝑖 -th element of 𝑥 𝑣 𝑖 is 1, and the others are 0. Therefore, we can perform atomic feature mimicking on the embedding table 𝐷 𝑖 . First, we collect embedding output e 𝑖 in the whole training dataset. Then we calculate E [ e 𝑖 ] and E [ e 𝑖 e 𝑇 𝑖 ] . After that, we compute the covariance matrix of e 𝑖 :  Similarly, we extract the first 𝑘 columns of 𝑈 𝑖 ∈ R 𝑡 × 𝑡 into 𝑈 𝑖 𝑘 ∈ R 𝑡 × 𝑘 . Therefore, we can get  Hence, e 𝑖 = 𝐷 𝑖 𝑥 𝑣 𝑖 can be approximated as   Therefore, the 𝑖 -th embedding table 𝐷 𝑖 ∈ R 𝑡 × 𝑠 𝑖 can be replaced by ( 𝑈 𝑖 𝑘 ) ⊤ 𝐷 𝑖 ∈ R 𝑘 × 𝑠 𝑖 . For other embedding tables, we can also perform similar operations. Then the embedding dimension of the entire recommendation model will be reduced from 𝑡 to 𝑘 . Now any input vector 𝑣 has its compressed embedding e ′ = [ e ′ 1 , e ′ 2 , · · · , e ′ 𝑑 ] , where e ′ ∈ R ˝ 𝑑 𝑖 = 1 𝑘 . It is worth noting that after compressing embeddings, we add an extra fully-connected layer after each embedding table. For the 𝑖 -th embedding table, the weight is 𝑊 𝑖 = 𝑈 𝑖 𝑘 ∈ R 𝑡 × 𝑘 and bias is 𝑏 𝑖 = ( 𝐼 -𝑈 𝑖 𝑘 ( 𝑈 𝑖 𝑘 ) ⊤ ) E [ e 𝑖 ] ∈ R 𝑡 . Therefore, the final output 𝑦 𝑒 of the embedding layer after AFM is  where 𝑦 𝑒 ∈ R ˝ 𝑑 𝑖 = 1 𝑡 . Hence, we reduce the space for storing the embedding tables from 𝑂 ( 𝑡 ˝ 𝑑 𝑖 = 1 𝑠 𝑖 ) to 𝑂 ( 𝑘 ( ˝ 𝑑 𝑖 = 1 𝑠 𝑖 + 𝑑𝑡 )) . It is worth noting that 𝑊 𝑖 is often very lightweight, which means the cost of this calculation can be ignored. In particular, the layers after the embedding tables are usually an MLP layer and a feature interaction module. Then, to obtain a higher compression ratio, we can further merge the first linear layer in the MLP and the linear layer brought by compressing embedding tables. In particular, we express the computation of the first linear layer in the MLP as 𝑦 ′ = 𝑊 ′ 𝑥 + 𝑏 ′ . Suppose the output of this layer has 𝑜 dimensions, then 𝑊 ′ ∈ R 𝑜 × ˝ 𝑑 𝑖 = 1 𝑡 and 𝑏 ′ ∈ R 𝑜 . First, let us rewrite 𝑊 ′ in the block matrix notation, i.e., 𝑊 ′ = [ 𝑊 ′ 1 , 𝑊 ′ 2 , · · · , 𝑊 ′ 𝑑 ] , in which 𝑊 ′ 𝑖 ∈ R 𝑜 × 𝑡 . Besides, we can also rewrite 𝑦 𝑒 as [ 𝑦 𝑒 1 , 𝑦 𝑒 2 , · · · , 𝑦 𝑒 𝑑 ] , where 𝑦 𝑒 𝑖 = 𝑊 𝑖 e ′ 𝑖 + 𝑏 𝑖 . Then, it is Unified Low-rank Compression Framework for Click-through Rate Prediction KDD '24, August 25-29, 2024, Barcelona, Spain. obvious that the input 𝑥 of the first MLP layer 𝑦 ′ = 𝑊 ′ 𝑥 + 𝑏 ′ is 𝑦 𝑒 , i.e.,  Then we can replace 𝑊 ′ with [ 𝑊 ′ 1 𝑊 1 , 𝑊 ′ 2 𝑊 2 , · · · , 𝑊 ′ 𝑑 𝑊 𝑑 ] , and 𝑏 ′ with [ 𝑊 ′ 1 𝑏 1 , 𝑊 ′ 2 𝑏 2 , · · · , 𝑊 ′ 𝑑 𝑏 𝑑 ] + 𝑏 ′ . After this replacement, the size of the first dense layer (mostly in 𝑊 ′ ) in the MLP layers will be reduced, too. Although this reduction is negligible when compared to the reduction in the embedding tables, it will compress the input dimension size of 𝑊 ′ from ˝ 𝑑 𝑖 = 1 𝑡 to ˝ 𝑑 𝑖 = 1 𝑘 and will further improve the inference speed. Note that at this time we still need to retain [ 𝑊 1 , 𝑊 2 , · · · , 𝑊 𝑑 ] to calculate the embedding output and use this as the input of the feature interaction module. Moreover, for some specific models (such as FiBiNet [12]), which will add an additional interaction layer between the embedding tables and the MLP layers, this fusion method is not applicable. After compressing embedding tables, we also fine-tune the whole compressed model for 1 epoch. Later we will show that after finetuning, the compressed model's AUC is also significantly better than the original model. It is obvious that our two compression strategies can be jointly applied and achieve higher inference speed with lower parameter sizes. In total, our entire compression process is as follows. First, we use AFM to compress the MLP module of the CTR prediction models and fine-tune the models by one epoch. Then we continue to reduce the embedding dimensionality based on the previous compressed model. According to the structure of each CTR prediction model, we judge whether to merge the projection weight of embeddings into the first linear layer of MLP or not. Note that throughout the compression process, we apply the whole training dataset to calculate the weights of AFM and fine-tune the models. It can be seen that our plug-and-play framework has very little intrusiveness to the entire recommendation system. After compressing the MLP layers, without making any changes to the codebase, algorithm engineers only need to add some extra fully-connected layers to the model weights. After further compressing the embedding, only a few lightweight dense layers need to be added after the embedding tables. These features make our framework very easy to implement and user-friendly. Later we will demonstrate that our framework can attain even higher AUC with significantly fewer parameters and faster inference speed on these tasks.",
  "4 EXPERIMENTS": "In this section, we start by describing the datasets, evaluation metrics, and baseline models used for our experiments. Then, we perform our methods on those pre-trained models and compare our algorithms to state-of-the-art previous low-rank decomposition works. We also list several ablation studies and end this section with online experiments. All the experiments are conducted with PyTorch. More training details and further experimental results are shown in the appendix. Table 1: Statistics of the three recommendation datasets used in our experiments.",
  "4.1 Datasets, Metrics and Models": "Datasets. We evaluate the proposed method on two public datasets for CTR prediction tasks, i.e., Criteo and Avazu. Criteo contains click logs with 45.8 million data instances, and Avazu consists of several days of 40.4 million ad click-through data which is ordered chronologically. Besides, we also evaluate our algorithms on a private industry dataset, i.e., the XYZ AppGallery dataset. 1 Its training dataset consists of user click information on the XYZ AppGallery within 12 days, and its testing dataset contains user click logs on the 13th day. We follow the hyperparameter settings in FiBiNet [12], i.e., we split the Criteo dataset randomly into two parts: 90% is for training, while the rest is for testing. We also split Avazu randomly into two parts: 80% is for training, while the rest is for testing. We shown the statistics of the Criteo, Avazu and XYZ AppGallery datasets in Table 1. Evaluation Metrics. We adopt AUC (Area Under the ROC Curve) and Logloss to measure the performance of models. We also report the parameter sizes of the recommendation models and average inference throughput in the test dataset. Because the inference bottleneck of the recommendation systems is memory access, to show the benefits brought by our compressing algorithms clearly, we calculate the models' throughput on an Intel Xeon Gold 5220R CPU with a fixed 10000 mini-batch size. Note that an improvement of 1‰ in AUC is usually regarded as significant for CTR prediction tasks, because it will bring a large increase in a company's revenue if the company has a substantial user base. Baseline models. To validate the effectiveness of our compression method, we deploy our framework to seven representative CTR prediction models: DCN [25], DeepFM [7], NFM [9], AutoInt [22], FiBiNet [12], DCNv2 [26] and GDCN [24]. To show the influences of AFM, we compare it with recent advances in low-rank approximation: SVD [6] and TTD [30].",
  "4.2 Main Experiments": "We summarize the performances on Criteo and Avazu test sets in Table 2, and show the results on XYZ AppGallery test set in Table 3. Implementation details. We first train the baseline models with uniform embedded dimensions. For the Criteo and Avazu datasets, we follow the model hyperparameters settings in FiBiNet, i.e., we set 400 dimensions per layer for the Criteo dataset and 2000 neurons per layer for the Avazu dataset. The embedding dimension is 16 for the Criteo dataset and 50 for the Avazu dataset. For the XYZ AppGallery dataset, we also set the MLP hidden sizes to 400 and the embedding dimension to 16. For all models, the number of hidden layers is set to 3 and we apply the ReLU activation function. In particular, we also add a linear weight for each categorical field, 1 Please note that the company name is anonymized as XYZ. KDD '24, August 25-29, 2024, Barcelona, Spain. Hao Yu, Minghao Fu, Jiandong Ding, Yusheng Zhou & Jianxin Wu Table 2: Results on the Criteo (columns 2-5) and Avazu (columns 6-9) test sets. Table 3: Results on the XYZ AppGallery test set. whose input and output dimensions are the number of possible items and 1, respectively. Then we compress these base models by our framework. We first compress the MLP layer and set the compression dimensions to 64 and 320 for the Criteo and Avazu datasets, respectively. For example, on the Criteo dataset, we low-rank decompose the original 400 × 400 linear layer into a 400 × 64 and a 64 × 400 linear layer. For the XYZ AppGallery dataset, the compression dimension is 64, too. Then we apply the entire training set to fine-tune the compressed model for 1 epoch. We only compress the second and third linear layers and do not deal with the first layer. This is because the input dimension of the first dense layer is related to the embedded layer, and in some cases, the input dimension is shallow. For simplicity, we do not consider the first dense linear for all models. Then we continue to reduce the embedding dimensions of those MLP-compressed models. For the Criteo and Avazu datasets, we uniformly compress the embedding dimension from 16 to 2, and 50 to 8, respectively. For the XYZ AppGallery dataset, we reduce the dimension from 16 to 4. After compressing the embedding dimension, we still fine-tune the sub-model for 1 epoch. When compressing NFM and FiBiNet, due to their unique structure, we do not merge the compression weights into the first dense layer in the MLP module. Results. We name our improved atomic feature mimicking for embedding tables as AFM EMB, and for MLP layers as AFM MLP. After compressing the MLP layers, except the NFM and FiBiNet on the XYZ AppGallery dataset, all models' AUC increases significantly, and the LogLoss also largely decreases. After further compressing the embedding tables, the parameter sizes of those models are generally reduced by 3-5 × , and the AUC also exceeds the original model by 0.010-0.003, which is a very high improvement in CTR prediction tasks. Because each model contains a linear weight, although the embedding dimension is reduced by 6-8 × , the total parameter sizes of those models are reduced by only 3-5 × . However, it is still a large compression ratio. Because the attention module of AutoInt and the SENet module of FiBiNet are very time-consuming, although we have achieved a higher parameter compression rate, the throughput improvement of the two models on the Criteo is about 10%. Except for these two models, our embedding tables and MLP compression Unified Low-rank Compression Framework for Click-through Rate Prediction KDD '24, August 25-29, 2024, Barcelona, Spain. Table 4: Results of AFM, SVD, and TTD on the Criteo (columns 3-6) and Avazu (columns 7-10) test sets with DeepFM. Table 5: Results of AFM, SVD, and TTD on the XYZ AppGallery test set with DeepFM. methods can increase the inference speed by 35%-170%, which is a very significant improvement. On the XYZ AppGallery dataset, our framework also generally achieves a 10%-30% speed increase.",
  "4.3 Ablation Studies": "To explore the influence of different modules of our method, we perform three analyses in this section. We take DeepFM on the Criteo, Avazu, and XYZ AppGallery datasets as examples. For a fair comparison, we adopt the same training strategies as in Section 4.2. 4.3.1 Compare AFM with TTD and SVD. First, we explore the advantages of our method compared with the traditional low-rank decomposition algorithm SVD and TTD. For the convenience of comparison, when applying SVD, we apply the same model structure as AFM but use SVD to initialize the compressed model. When performing TTD, we follow the model hyperparameter settings in the TT-Rec paper [30] and denote the TT-ranks as 16. Table 4 shows the performances of those compressed models on the Criteo and Avazu datasets. We name the results of direct compression and further fine-tuning 1 epoch as 'Pre-train' and 'Fine-tune'. We then display the results of the XYZ AppGallery dataset in Table 5. We refer to 'Pre-train' as 'PT' and 'Fine-tune' as 'FT'. As we can conclude from those tables, though AFM indeed performs better than SVD, they both achieve better than the original models. This indicates our framework does not hinge on AFM. In fact, it is our low-rank compression framework method as a whole, rather than the single AFM, that is effective in compressing CTR prediction models. We believe that our framework reveals the great potential of matrix decomposition (not just AFM) in compressed CTR prediction models, which is our greatest contribution. In addition, though TTD can achieve a higher parameter compression rate, it will greatly slow down the inference. 4.3.2 Add Activation Functions When Compressing MLP Layers. When compressing the MLP layers, we will add an additional ReLU activation layer between the two decomposed linear layers. Here we explore the effect of this ReLU layer on the results. We follow the previous MLP compression settings. Table 6 shows the results of adding additional activation functions. We report the results of direct compression and further finetuning. Surprisingly, after compressing, the ReLU function has only a slight impact on the AUC. After fine-tuning, the AUCs of those models with additional ReLU functions are generally higher than those of models without ReLU. Therefore, when low-rank decomposing one linear layer into two linear layers, we will add an additional activation function between the two linear layers. 4.3.3 Dimensionality of Embedding and MLP Layers. In the previous experiments, we fix the compression dimension of those models. In this subsection, we continue to explore the influence of different compression dimensionalities on model performances. Note that unlike previous experiments, we compress the embedding tables or the MLP layers directly on the baseline model. We first explore the performances of different embedding dimensions. On the Criteo and XYZ AppGallery datasets, the original embedding dimension is 16, and here we compress the dimensions to { 2 , 4 , 8 , 12 } . On the Avazu dataset, the original dimension is 50, and we reduce the dimensions into { 4 , 8 , 16 , 32 } . The results are shown in Table 7. When directly compressing, the more the embedding dimensions, the higher the AUC of the model. After further fine-tuning, as the dimension increases, the AUC of the model on the Criteo and XYZ AppGallery datasets first increases and then decreases slightly. On the Avazu dataset, the AUC keeps increasing. We also explore the influence of different MLP hidden sizes. On the Criteo and XYZ AppGallery datasets, the original hidden layer dimension is 400, and here we low-rank decompose it into { 32 , 64 , 96 , 128 } , respectively. On the Avazu dataset, the hidden layer KDD '24, August 25-29, 2024, Barcelona, Spain. Hao Yu, Minghao Fu, Jiandong Ding, Yusheng Zhou & Jianxin Wu Table 6: Results with or without ReLU functions. Table 7: Results of different embedding dimensionalities. Table 8: Results of different MLP hidden sizes. dimension is 2000, and we compress it to { 160 , 320 , 480 , 640 } . The results are shown in Table 8. Surprisingly, no matter what the size of the hidden layer is, the AUC is always the same when directly compressing. This indicates that the representation dimensionality required by the MLP layers of the CTR prediction models may be very low . After further fine-tuning, compressed models with different MLP sizes obtained similar AUC, too. Table 9: Results of multi-epoch training with DeepFM on the XYZ AppGallery dataset. In summary, we find that compared with MLP sizes, the embedding dimensionalities have a higher influence on the models' performances. Note that on the Avazu dataset, when the embedding dimension is 4, the compression ratio is 10.2 × , but it will slightly reduce the AUC. Here our goal is to reduce the dimensions as much as possible to achieve faster speeds, and we also want to ensure that the AUC and Logloss of the compressed model are comparable or even better than the original model. Therefore, we choose the present compression dimensions for DeepFM. For simplicity and uniformity, we generalize DeepFM's hyperparameters to other CTR prediction models. 4.3.4 Combine Our Framework with Mutil-Epoch Training . In our original setting, we only train the baseline model 1 epoch because of the one-epoch phenomenon [15]. To show that our method is also applicable to multi-epoch training, we imitate MEDA [15] and reinitialize the embedding layer in each epoch and only keep other parts of the checkpoint (such as MLP layers). In this way, we can also achieve multi-epoch training. We train DeepFM with the XYZ AppGallery dataset. All hyperparameters are the same as in the original settings. The results are shown in Table 9. Therefore, even the multi-epoch training is still inferior to our framework when converging. Note that our framework is compressing the model, which requires less training time than multi-epoch training and consumes fewer resources during inference. For deep Unified Low-rank Compression Framework for Click-through Rate Prediction KDD '24, August 25-29, 2024, Barcelona, Spain.",
  "Table 10: Results of performing our framework after multiepoch training with DeepFM on the XYZ AppGallery dataset.": "Table 11: The number of top-k singular values needed when keeping 90%, 95%, and 99% of the sum of total singular values on the Criteo dataset with DeepFM. learning tasks, smaller model parameter sizes and computational overhead mean that the overall system requires fewer storage devices, computing units, and carbon emissions, especially for CTR models, which are widely used in the industry and inference millions of times a day, and bring direct economic benefits. Furthermore, our approach does not conflict with multi-epoch training, and we can also continue to perform our compression framework on multi-epoch trained models. Here we continue to compress the model based on 3-epoch training. The results are shown in Table 10. As the results show, when we use some additional techniques to multi-epoch train the model, we can continue to use our framework to further compress the model and achieve better results. This is what multi-epoch training and other state-of-the-art compression methods cannot achieve. 4.3.5 One Possible Reason why the Compressed Model Outperforms the Original Model. It is an exceptional phenomenon that the compression model consistently outperforms the original model, and here we try to give a possible explanation. In CTR fields, although the output features' dimensionality is large, there is a high linear correlation among them. Here is a concrete example. We collect the output features of three MLP layers on a pre-trained DeepFM with the Criteo dataset, and calculate the singular values of the output features. We report how many top-k singular values are needed when keeping 90%, 95%, and 99% of the sum of total singular values. The results are shown in Table 11. Note that the feature dimensionality is 400. It shows in the MLP layers, only a few dimensions are needed to achieve 90% singular values, and in the second and third FC, even only 1 or 2 dimensions are needed. So the output feature space is highly redundant and can be represented by fewer dimensions without loss of useful information. Discarding redundant principal components (by our framework) can help identify and extract the most important features, thereby improving processing efficiency and keeping the model accuracy. Besides, in CTR prediction tasks, models are prone to overfitting, hence they are often trained by one-epoch. With little or no decrease in AUC, our compression framework reduces dimensions with fewer principal components to remove unimportant redundant information, which is more like a regularization Table 12: Online experiment results for 7 days in comparison with non-compressed baseline. term to prevent the model from overfitting. Therefore, with our framework, the model can find better initial points and even better results can be obtained after further fine-tuning. Similar results can be found in previous papers, such as UMEC [20], which achieves a 50% compression ratio and slightly increased AUC after further finetuning.",
  "4.4 Online Experiments": "To enhance the assessment of our method's efficacy, we integrated it into the online advertising system and executed a comprehensive 7-day online A/B test, aiming at contrasting our approach with the established baseline. We take the average throughput (TP Avg ) and the 99th percentile of throughput (TP P99 , which is usually used to assess the performance of a system under high load) to measure the inference efficiency. In addition to AUC, the Advertiser Value (AV) is selected to quantify the efficacy of the advertising investments. As illustrated in Table 12, similar to the observation in offline experiments, our method consistently improves both the AUC and throughput metrics. Note that compared to the baseline models, our method only improves online AUC by 0.079‰. This is because first, the original model's AUC is already very high, and second, the online experiment contains a large user base. The two difficulties make it difficult for the AUC to increase even slightly. However, our method also can achieve higher online AUC with faster speed. In addition, there is a substantial increase in Advertiser Value, marked by a significant boost of 2.34%. This highlights that even when confronted with the complexities of online environments, our method maintains its efficiency and is proved to be highly effective.",
  "5 CONCLUSION": "In this paper, we believe that traditional low-rank decomposition methods tend to focus too much on model weights and ignore the distribution of feature maps. In contrast, we proposed a novel and unified low-rank decomposition framework for compressing CTR prediction models. Our framework mimics feature distribution and can be used in both the embedding tables and MLP layers of the CTR prediction models. Extensive experiments confirm that our framework can significantly increase the CTR prediction models' AUC while effectively reducing the parameter sizes and improving the throughput. We find that since the inference bottleneck of CTR prediction models is mainly memory access, our framework cannot bring significant acceleration on GPU. Therefore, applying model compression in a reasonable way to accelerate the model's inference speed on GPU is an interesting future direction. Furthermore, our method can theoretically be applied to a wide variety of recommendation models, so we will continue to extend our method to these models in the future. KDD '24, August 25-29, 2024, Barcelona, Spain. Hao Yu, Minghao Fu, Jiandong Ding, Yusheng Zhou & Jianxin Wu",
  "6 ACKNOWLEDGMENTS": "We acknowledge the funding provided by the National Natural Science Foundation of China under Grant 62276123 and Grant 61921006. J. Wu is the corresponding author.",
  "REFERENCES": "[1] Rohan Anil, Sandra Gadanho, Da Huang, Nijith Jacob, Zhuoshu Li, Dong Lin, Todd Phillips, Cristina Pop, Kevin Regan, Gil I Shamir, et al. 2022. On the factory floor: ML engineering for industrial-scale ads recommendation models. arXiv:2209.05310 [2] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems . ACM, Boston, USA, 7-10. [3] Weiyu Cheng, Yanyan Shen, and Linpeng Huang. 2020. Adaptive factorization network: Learning adaptive-order feature interactions. In Proceedings of the 34th AAAI Conference on Artificial Intelligence . AAAI, New York, USA, 3609-3616. [4] Aditya Desai, Li Chou, and Anshumali Shrivastava. 2022. Random Offset Block Embedding (ROBE) for compressed embedding tables in deep learning recommendation systems. In Annual Conference on Machine Learning and Systems . MLSys, Santa Clara, USA, 762-778. [5] Antonio A Ginart, Maxim Naumov, Dheevatsa Mudigere, Jiyan Yang, and James Zou. 2021. Mixed dimension embeddings with application to memory-efficient recommendation systems. In IEEE International Symposium on Information Theory . IEEE, Melbourne, Australia, 2786-2791. [6] Gene H Golub and Charles F Van Loan. 2013. Matrix computations . Johns Hopkins University Press, Baltimore, USA. [7] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: A factorization-machine based neural network for CTR prediction. In Proceedings of the 26th International Joint Conference on Artificial Intelligence . IJCAI, Melbourne, Australia, 1725-1731. [8] Babak Hassibi, David G. Stork, and Gregory J. Wolff. 1993. Optimal Brain Surgeon and general network pruning. In IEEE International Conference on Neural Networks . IEEE, San Francisco, USA, 293-299. [9] Xiangnan He and Tat-Seng Chua. 2017. Neural factorization machines for sparse predictive analytics. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval . ACM, Tokyo, Japan, 355364. [10] Oleksii Hrinchuk, Valentin Khrulkov, Leyla Mirvakhabova, Elena Orlova, and Ivan Oseledets. 2020. Tensorized embedding layers. In Findings of the Association for Computational Linguistics: EMNLP 2020 . ACL, Virtual, 4847-4860. [11] Jie Hu, Li Shen, and Gang Sun. 2018. Squeeze-and-excitation networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . IEEE, Salt lake city, USA, 7132-7141. [12] Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. 2019. FiBiNET: Combining feature importance and bilinear feature interaction for click-through rate prediction. In Proceedings of the 13th ACM Conference on Recommender Systems . ACM, Copenhagen, Denmark, 169-177. [13] Herve Jégou, Matthijs Douze, and Cordelia Schmid. 2011. Product quantization for nearest neighbor search. IEEE Transactions on Pattern Analysis and Machine Intelligence 33, 1 (2011), 117-128. [14] Siyi Liu, Chen Gao, Yihong Chen, Depeng Jin, and Yong Li. 2021. Learnable embedding sizes for recommender systems. In International Conference on Learning Representations . OpenReview, Virtual, 17 pages. [15] Zhaocheng Liu, Zhongxiang Fan, Jian Liang, Dongying Kong, and Han Li. 2023. Multi-epoch learning for deep Click-Through Rate prediction models. arXiv:2305.19531 [16] Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. 2017. ThiNet: A filter level pruning method for deep neural network compression. In Proceedings of the IEEE Conference on Computer Vision . IEEE, Venice, Italy, 5058-5066. [17] Vinod Nair and Geoffrey E Hinton. 2010. Rectified linear units improve restricted Boltzmann machines. In Proceedings of the 27th international Conference on Machine Learning . Omnipress, Haifa, Israel, 807-814. [18] Liang Qu, Yonghong Ye, Ningzhi Tang, Lixin Zhang, Yuhui Shi, and Hongzhi Yin. 2022. Single-shot embedding dimension search in recommender system. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval . ACM, Madrid, Spain, 513-522. [19] Steffen Rendle. 2010. Factorization machines. In IEEE International Conference on Data Mining . IEEE, Sydney, Australia, 995-1000. [20] Jiayi Shen, Haotao Wang, Shupeng Gui, Jianchao Tan, Zhangyang Wang, and Ji Liu. 2021. UMEC: Unified model and embedding compression for efficient recommendation systems. In International Conference on Learning Representations . OpenReview, Virtual, 13 pages. [21] Hao-Jun Michael Shi, Dheevatsa Mudigere, Maxim Naumov, and Jiyan Yang. 2020. Compositional embeddings using complementary partitions for memory-efficient recommendation systems. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . ACM, Virtual, 165-175. [22] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, and Jian Tang. 2019. AutoInt: Automatic feature interaction learning via selfattentive neural networks. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management . ACM, Beijing,China, 1161-1170. [23] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems , Vol. 30. Curran Associates, Inc., Long Beach, USA, 5998-6008. [24] Fangye Wang, Hansu Gu, Dongsheng Li, Tun Lu, Peng Zhang, and Ning Gu. 2023. Towards deeper, lighter and interpretable cross network for CTR prediction. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management . ACM, Birmingham, UK, 2523-2533. [25] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & Cross Network for ad click predictions. In Proceedings of the ADKDD'17 . ACM, New York, USA, 1-7. [26] Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong, and Ed Chi. 2021. DCN V2: Improved Deep & Cross Network and practical lessons for Web-scale learning to rank systems. In Proceedings of the ACM Web Conference 2021 . ACM, Ljubljana, Slovenia, 1785-1797. [27] Jianxin Wu. 2020. Essentials of pattern recognition: An accessible approach . Cambridge University Press, Cambridge, UK. [28] Xin Xia, Hongzhi Yin, Junliang Yu, Qinyong Wang, Guandong Xu, and Quoc Viet Hung Nguyen. 2022. On-device next-item recommendation with selfsupervised knowledge distillation. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval . ACM, Madrid, Spain, 546-555. [29] Bencheng Yan, Pengjie Wang, Jinquan Liu, Wei Lin, Kuang-Chih Lee, Jian Xu, and Bo Zheng. 2021. Binary code based hash embedding for Web-scale applications. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management . ACM, Gold Coast, Australia, 3563-3567. [30] Chunxing Yin, Bilge Acun, Carole-Jean Wu, and Xing Liu. 2021. TT-Rec: Tensor train compression for deep learning recommendation models. In Annual Conference on Machine Learning and Systems . MLSys, Santa Clara, USA, 448-462. [31] Hao Yu and Jianxin Wu. 2023. Compressing Transformers: Features are lowrank, but weights are not!. In Proceedings of the AAAI Conference on Artificial Intelligence . AAAI Press, Washing DC, USA, 11007-11015. Unified Low-rank Compression Framework for Click-through Rate Prediction KDD '24, August 25-29, 2024, Barcelona, Spain.",
  "A APPENDIX": "",
  "A.1 Illustration of the Compression Process": "We show the process of compressing the MLP layer and embedding tables in Figures 1 and 2, respectively. The figures include the calculation of low-rank weights and fusion methods. It is worth noting that when compressing embedding tables, if there is an additional interaction layer between embedding and the first FC weight, then the fusion of 𝑊 and 𝑈 𝑘 is not applicable.",
  "A.2 Detailed Training Settings": "When training the baseline model, we follow the training settings in FiBiNet, i.e., we use Adam with a mini-batch size of 2000, 1000, and 500 for the XYZ AppGallery, Criteo, and Avazu datasets, respectively. The learning rate is set to 0.0001 and we also apply dropout and set the rate to 0.5. When fine-tuning the MLP-compressed models, the learning rate is 0.001. On the Criteo and XYZ AppGallery datasets, we set the batch size to 20000, and on Avazu the batch size is 10000. When further fine-tuning the compressed models with reduced embedding dimensions, we set the batch size to 10000, 5000, and 3000 on the Criteo, Avazu, and XYZ AppGallery datasets, respectively. We also set the learning rate to 0.001 and remove dropout on the Criteo and Avazu datasets. On the XYZ AppGallery datasets, we set the dropout rate as 0.3. In all experiments, the weight decay is 1e-3, and we sum the binary cross-entropy loss and the l2-norm of model weights as loss targets, i.e.,  In the first item of the right hand side, 𝑦 𝑖 is the ground truth of the 𝑖 -th instance, ˆ 𝑦 𝑖 is the model's prediction, and 𝑁 is the total size of samples. Correspondingly, in the second item, 𝑟 is the loss ratio, 𝑊 𝑖 is the 𝑖 -th model weight and 𝑀 is the number of model weights. When compressing the embedding dimension on the XYZ AppGallery dataset, we set 𝑟 to 1e-2, and in other cases, the ratio 𝑟 is 1e-5. In all experiments, we set the random seed as 0. It is worth noting that we did not mention validation dataset. This is because we find that the model consistently behaves the same on the training and test set when there are different training hyperparameters. Taking DeepFM with AFM MLP on the Criteo dataset as an example, we set the learning rate in {1e-2,5e-3,1e-3,5e4,1e-4}. The results are shown in Table 13. As we can see from the table, it is enough that we only need to adjust the hyperparameters so that the model performs well on the training set. Besides, to prove the practicality and ease-to-use of our method, we do not deliberately tune the hyperparameters of the model training and simply generalize DeepFM's training hyperparameters to other models.",
  "A.3 Training Times of the Baseline and Our Compression Framework": "To prove the effectiveness of our compression framework, we also report the GPU times of the baseline model and our compression Figure 1: Illustration of compressing MLP layers. /g58 /g72/g76/g74/g75/g87 /g38/g82/g80/g83/g85/g72/g86/g86 /g47/g82/g90/g16/g85/g68/g81/g78 /g58 /g72/g76/g74/g75/g87 /g44/g81/g83/g88/g87/g3 /g50/g88/g87/g83/g88/g87/g3 /g58 /g72/g76/g74/g75/g87 /g44/g81/g83/g88/g87/g3 /g50/g88/g87/g83/g88/g87/g3 /g48/g72/g85/g74/g72 /g44/g81/g83/g88/g87/g3 /g50/g88/g87/g83/g88/g87/g3 /g53/g72/g47/g56 Figure 2: Illustration of compressing embedding tables. /g58 /g72/g76/g74/g75/g87 /g38/g82/g80/g83/g85/g72/g86/g86 /g47/g82/g90/g16/g85/g68/g81/g78 /g58 /g72/g76/g74/g75/g87 /g44/g81/g83/g88/g87/g3 /g50/g88/g87/g83/g88/g87/g3 /g48/g72/g85/g74/g72 /g40/g80/g69 /g58 /g72/g76/g74/g75/g87 /g44/g81/g83/g88/g87/g3 /g50/g88/g87/g83/g88/g87/g3 /g40/g80/g69 /g44/g81/g83/g88/g87/g3 /g50/g88/g87/g83/g88/g87/g3 /g40/g80/g69 Table 13: DeepFM's performances of different learning rate on the Criteo dataset with AFM MLP. framework during the training process. Note that AFM for MLP and embedding tables processes only infer the training dataset once and then directly compute the compression weights. The inference computational resources it requires are small and the time it takes is negligible. As for the further fine-tuning process, our framework only need to fine-tune the sub-model 1 epoch after each compression process, so the training time required for the whole framework is short. Taking DeepFM as an example, we train the model on a single 3090 GPU on the Criteo, Avazu, and XYZ datasets. The results are shown in Table 14. Note that the training time is proportional to the batch size. It can be seen that the training time our framework needs is very short, and a compression time of tens of minutes is acceptable. It takes longer to train AFM EMB on the XYZ AppGallery dataset than AFM MLP, because we set batch sizes as 20000 when compressing the KDD '24, August 25-29, 2024, Barcelona, Spain. Hao Yu, Minghao Fu, Jiandong Ding, Yusheng Zhou & Jianxin Wu Table 14: Results of training times with DeepFM. Table 15: Comparison of our framework and other compression techniques on the Criteo dataset with DeepFM. Table 16: Transfer the XYZ compression weight into Criteo dataset with DeepFM. MLP layer and batch sizes as 3000 when reducing the embedding dimensions.",
  "A.4 Compare Our Framework with Other Compression Techniques": "To demonstrate the superiority of our approach, we train DeepFM with Criteo and compare our framework with other three stateof-the-art methods, i.e., UMEC (pruning method) [20], Product Quantization (PQ, quantization method) [13] and QR (hashing method) [21]. The results are shown in Table 15. It can be seen that previous works hardly achieve AUC improvement with a 3-5x compression ratio and faster inference. But, when AUC decreases even 0.1%, it will cause revenue loss and will not be adopted. Our method can achieve both higher AUC and faster speed in compressing CTR models.",
  "A.5 Transfer the Compression Weight into Different Datasets.": "Here we use MLP compression weights calculated on the XYZ dataset to compress DeepFM trained on Criteo, and then fine-tune the compressed model with Criteo. All hyperparameters are the same as in the previous experiments. These results in Table 16 show that transferring the weight from another dataset causes a large drop in AUC, but AUC also recovers after fine-tuning. Although this result is not as good as using the original dataset directly (0.8016 v.s. 0.8009), it is better than the original model (0.7964). Table 17: Parameter sizes and throughput on GPU of AFM and TTD on the Criteo, Avazu, and XYZ AppGallery datasets with DeepFM.",
  "A.6 Model Throughput on the GPU Device": "In this section, we report the throughput of the original model and various compression models on the GPU device. Here we take DeepFM on GTX 3090 GPU as examples. Note that because SVD and AFM share the same model structure, here we ignore SVD and report the throughput of the AFM and TTD compression models. The mini-batch size is still 10000. The results are shown in Table 17. During the throughput testing process, we find that the GPU utilization is less than 20%, which is very low. This phenomenon shows that the throughput on the GPU cannot truly reflect the effectiveness of our algorithms. However, we still achieve a 3-5x reduction in memory usage and comparable throughput. In contrast, TTD needs to recalculate the embedding layer on each field before it can obtain the corresponding feature embedding. This operation seriously affects its speed. These results indicate that our framework is far superior to traditional tensor decomposition methods, both on the GPU and CPU devices.",
  "A.7 The Influence of Different Random Seeds": "To prove that our method has a statistical improvement, we use 5 different random seeds to divide the Criteo and Avazu datasets and compress the DeepFM model (Note that XYZ training and test datasets are already partitioned and cannot be changed). We report the AUC and LogLoss's mean and standard deviation of the original compressed models. The results are shown in Table 18. With different random seeds, our method has always improved steadily. This shows that our framework is statistically significant enough to report an improvement in accuracy.",
  "A.8 The Detailed Performances of AFM in Each Model": "In this subsection, we present detailed performances of direct compression and further fine-tuning on the other six DLRMs. Similarly, the results of AFM on the Criteo, Avazu, and XYZ AppGallery datasets are shown in Table 19. Since we have already reported Unified Low-rank Compression Framework for Click-through Rate Prediction KDD '24, August 25-29, 2024, Barcelona, Spain. Table 18: DeepFM's performances on the Criteo and Avazu datasets with different random seeds. Table 19: Results of AFM on the Criteo (columns 3-4), Avazu (columns 5-6), and XYZ AppGallery (columns 7-8) datasets with each model. parameter sizes and throughput in the main text, we ignore these results here.",
  "A.9 Compare Our Framework with Train from Scratch": "We can draw a similar conclusion to the above text from those results, i.e., when compressing the MLP layers and embedding tables, our algorithm will only bring a slight drop in AUC, or even no decrease in it. After further fine-tuning, the AUC of the model will further increase. It is worth noting that on the XYZ AppGallery dataset, when we perform AFM directly on AutoInt, FiBiNet and GDCN,sometimesthe model's AUC even increases, but after further fine-tuning, the AUC also may decrease. This phenomenon shows that in some cases, directly applying AFM to compress model can already achieve good performances, and the sub-models no longer need fine-tuning. Our framework first trains a large network first and then compress it into a small network. We now compare our 'train-compressfinetune' framework and directly 'train from scratch' (refer to TFS) a small network. The results are shown in Table 20. The first two lines mean the AUC and LogLoss of only performing AFM for MLP layers, and last two lines mean the AUC and LogLoss of performing AFM both for MLP layers and embedding tables. We can see that our 'train-compress-finetune' framework consistently outperforms the 'training from scratch' approach. KDD '24, August 25-29, 2024, Barcelona, Spain. Hao Yu, Minghao Fu, Jiandong Ding, Yusheng Zhou & Jianxin Wu Table 20: Comparison of our framework and training from scratch. Table 21: Results of different compression orders.",
  "A.10 The Order of Compressing Embedding and MLP Layers.": "In previous experiments, we first compress the MLP layers and then continue to reduce the embedding dimensions. In this subsection, we research the influence of the compression order. Table 21 shows the influence of different compression orders on the DLRMs' performances. The first four lines represent that we compress MLP first and then reduce the embedding dimension. Correspondingly, the last four lines represent that we first compress embedding. It can be seen that the final AUC difference between the two strategies is not large, and each strategy does not show a sustained lead over the other strategy.",
  "keywords_parsed": [
    "Recommendation Systems",
    "CTR Prediction",
    "Model Compression",
    "Low-rank Approximation"
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "On the factory floor: ML engineering for industrial-scale ads recommendation models"
    },
    {
      "ref_id": "b2",
      "title": "Wide & deep learning for recommender systems"
    },
    {
      "ref_id": "b3",
      "title": "Adaptive factorization network: Learning adaptive-order feature interactions"
    },
    {
      "ref_id": "b4",
      "title": "Random Offset Block Embedding (ROBE) for compressed embedding tables in deep learning recommendation systems"
    },
    {
      "ref_id": "b5",
      "title": "Mixed dimension embeddings with application to memory-efficient recommendation systems"
    },
    {
      "ref_id": "b6",
      "title": "Matrix computations"
    },
    {
      "ref_id": "b7",
      "title": "DeepFM: A factorization-machine based neural network for CTR prediction"
    },
    {
      "ref_id": "b8",
      "title": "Optimal Brain Surgeon and general network pruning"
    },
    {
      "ref_id": "b9",
      "title": "Neural factorization machines for sparse predictive analytics"
    },
    {
      "ref_id": "b10",
      "title": "Tensorized embedding layers"
    },
    {
      "ref_id": "b11",
      "title": "Squeeze-and-excitation networks"
    },
    {
      "ref_id": "b12",
      "title": "FiBiNET: Combining feature importance and bilinear feature interaction for click-through rate prediction"
    },
    {
      "ref_id": "b13",
      "title": "Product quantization for nearest neighbor search"
    },
    {
      "ref_id": "b14",
      "title": "Learnable embedding sizes for recommender systems"
    },
    {
      "ref_id": "b15",
      "title": "Multi-epoch learning for deep Click-Through Rate prediction models"
    },
    {
      "ref_id": "b16",
      "title": "ThiNet: A filter level pruning method for deep neural network compression"
    },
    {
      "ref_id": "b17",
      "title": "Rectified linear units improve restricted Boltzmann machines"
    },
    {
      "ref_id": "b18",
      "title": "Single-shot embedding dimension search in recommender system"
    },
    {
      "ref_id": "b19",
      "title": "Factorization machines"
    },
    {
      "ref_id": "b20",
      "title": "UMEC: Unified model and embedding compression for efficient recommendation systems"
    },
    {
      "ref_id": "b21",
      "title": "Compositional embeddings using complementary partitions for memory-efficient recommendation systems"
    },
    {
      "ref_id": "b22",
      "title": "AutoInt: Automatic feature interaction learning via self-attentive neural networks"
    },
    {
      "ref_id": "b23",
      "title": "Attention is All you Need"
    },
    {
      "ref_id": "b24",
      "title": "Towards deeper, lighter and interpretable cross network for CTR prediction"
    },
    {
      "ref_id": "b25",
      "title": "Deep & Cross Network for ad click predictions"
    },
    {
      "ref_id": "b26",
      "title": "DCN V2: Improved Deep & Cross Network and practical lessons for Web-scale learning to rank systems"
    },
    {
      "ref_id": "b27",
      "title": "Essentials of pattern recognition: An accessible approach"
    },
    {
      "ref_id": "b28",
      "title": "On-device next-item recommendation with self-supervised knowledge distillation"
    },
    {
      "ref_id": "b29",
      "title": "Binary code based hash embedding for Web-scale applications"
    },
    {
      "ref_id": "b30",
      "title": "TT-Rec: Tensor train compression for deep learning recommendation models"
    },
    {
      "ref_id": "b31",
      "title": "Compressing Transformers: Features are low-rank, but weights are not!"
    }
  ]
}