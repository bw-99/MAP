{
  "RecDCL: Dual Contrastive Learning for Recommendation": "Dan Zhang Tsinghua University zd21@mails.tsinghua.edu.cn Yangliao Geng Beijing Jiaotong University gyarenas@gmail.com Wenwen Gong Tsinghua University wenweng@mail.tsinghua.edu.cn Zhongang Qi ARC Lab, Tencent PCG zhongangqi@tencent.com Ying Shan ARC Lab, Tencent PCG yingsshan@tencent.com Zhiyu Chen FiT, Tencent colinzychen@tencent.com Yuxiao Dong ‚àó Tsinghua University yuxiaod@tsinghua.edu.cn",
  "ABSTRACT": "Xing Tang FiT, Tencent xing.tang@hotmail.com Jie Tang ‚àó Tsinghua University jietang@tsinghua.edu.cn",
  "KEYWORDS": "Self-supervised learning (SSL) has recently achieved great success in mining the user-item interactions for collaborative filtering. As a major paradigm, contrastive learning (CL) based SSL helps address data sparsity in Web platforms by contrasting the embeddings between raw and augmented data. However, existing CL-based methods mostly focus on contrasting in a batch-wise way, failing to exploit potential regularity in the feature dimension. This leads to redundant solutions during the representation learning of users and items. In this work, we investigate how to employ both batch-wise CL (BCL) and feature-wise CL (FCL) for recommendation. We theoretically analyze the relation between BCL and FCL, and find that combining BCL and FCL helps eliminate redundant solutions but never misses an optimal solution. We propose a dual contrastive learning recommendation framework-RecDCL. In RecDCL, the FCL objective is designed to eliminate redundant solutions on user-item positive pairs and to optimize the uniform distributions within users and items using a polynomial kernel for driving the representations to be orthogonal; The BCL objective is utilized to generate contrastive embeddings on output vectors for enhancing the robustness of the representations. Extensive experiments on four widely-used benchmarks and one industry dataset demonstrate that RecDCL can consistently outperform the state-ofthe-art GNNs-based and SSL-based models (with an improvement of up to 5.65% in terms of Recall@20). The source code is publicly available 1 .",
  "CCS CONCEPTS": "¬∑ Information systems ‚Üí Recommender systems. ‚àó Corresponding authors: YD and JT 1 https://github.com/THUDM/RecDCL Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WWW'24, May 13-17, 2024, Singapore, Singapore ¬© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0171-9/24/05...$15.00 https://doi.org/10.1145/3589334.3645533 Recommender Systems, Self-supervised Learning, Batch-wise Contrastive Learning, Feature-wise Contrastive Learning",
  "ACMReference Format:": "Dan Zhang, Yangliao Geng, Wenwen Gong, Zhongang Qi, Zhiyu Chen, Xing Tang, Ying Shan, Yuxiao Dong, and Jie Tang. 2024. RecDCL: Dual Contrastive Learning for Recommendation. In Proceedings of the ACM Web Conference 2024 (WWW '24), May 13-17, 2024, Singapore, Singapore. ACM, New York, NY, USA, 17 pages. https://doi.org/10.1145/3589334.3645533",
  "1 INTRODUCTION": "Contrastive learning (CL) [20] is known as a major branch of selfsupervised learning. The fundamental idea behind CL is to artificially augment more supervised instances and conduct a pretext task with augmented data, addressing the issue of data sparsity. In recent years, CL-based collaborative filtering (CF) has been proposed and achieved great success to mine interacted records from Web systems for recommendation [24, 36, 40, 45, 46, 53]. In general, CL-based collaborative filtering methods focus on batch-wise objective functions. Batch-wise objectives aim to maximize the similarity of embedding between positive pairs (diagonal) while minimizing that of negative pairs (off-diagonal). A typical kind of batch-wise based CF methods apply BPR loss [31] to predict the users' preferences on multiple interacted Web platforms (e.g., e-commerce, music, video), such as graph neural networks (GNNs)-based models (e.g., PinSAGE [43], LightGCN [16], and ApeGNN [50]) and self-supervised learning (SSL)-based models (e.g., SGL [40] and SimGCL [45]). In particular, these methods usually require both the user-item interacted pairs and the negative counterparts generated by negative sampling. However, since the negative sampling scheme may mistakenly treat 'positive but unobserved' pairs as negative pairs, there exist critical limitations in the performance of these methods. Moreover, some recent batch-wise CL (BCL) recommendation methods (e.g., BUIR [24], CLRec [53], and DirectAU [36]) point out that more robust improvements can be obtained without negative sampling for Web recommendation systems. However, these BCL methods may lead to trivial constant solutions since they fail to leverage embedding information of users and items from Web platforms, which is illustrated in Figure 1. To address this issue, we investigate CL-based methods in various domains and conclude their critical differences in Table 1. The WWW'24, May 13-17, 2024, Singapore, Singapore Zhang et al. Figure 1: The motivating example that shows the effect for a negative pair in BCL, FCL, and BCL+FCL, where the lightshaded symbols indicate potentially possible solutions. In this example, BCL (top right) tends to align the negative pair on a straight line, i.e., even distribution in a circle; FCL (bottom left) mostly encourages the two representations to be orthogonal; BCL+FCL (bottom right) drives the two samples to saturate on either the ùë• axis or the ùë¶ axis. Note that using either BCL alone or FCL alone will result in infinite potential solutions. In contrast, combining BCL and FCL yields only four possible solutions: {( 0 , 1 ) , ( 0 , -1 )} , {( 0 , -1 ) , ( 0 , 1 )} , {( 1 , 0 ) , (-1 , 0 )} and {(-1 , 0 ) , ( 1 , 0 )} , which cancels the redundant solutions but never misses an optimal solution, and thus is intuitively a more reasonable regularization compared with BCL alone or FCL alone. High-dimensional cases are analogous. Negative pair Raw embedding Optimization with FCL Optimization with BCL Optimization with BCL+FCL objective functions of CL usually fall into two categories: batchwise objectives and feature-wise objectives. As for the feature-wise objectives, existing works have attracted full attention in the computer vision (CV) domain. In particular, feature-wise CL methods such as Barlow Twins [47] and VICREG [1] have been devoted to investigating the importance of embedding vectors and proposing novel feature-wise objective functions. These methods maximize the variability of the embeddings by decorrelating the components in the feature-wise dimension, which can avoid collapse [38] and yield the desired performance. However, as shown in Figure 1, these FCL methods ignore important information provided in batch-wise objectives and lead to orthogonal distribution. In light of this, a meaningful question then arises: Is there an effective optimization objective between batch-wise CL and feature-wise CL for self-supervised recommendations? Regarding this, CL4CTR [37] proposes feature alignment and field uniformity and masks feature and dimension information to address the \"long tail\" distribution of feature frequencies for CTR prediction. However, previous studies [37, 38] only explored the connection between BCL and FCL, there has been a lack of a native interpretation to connect them and little effort has been made to understand the effect of combining them. Table 1: Critical comparison between BCL methods, FCL methods, and RecDCL. To answer the above question, we investigate a native connection of the objective between the batch-wise CL and the feature-wise CL (Figure 1 and Observation 3.1) and present a perspective to show a cooperative benefit by using them both (Observation 3.2) from the perspective of theory and experiment. Based on these analyses, we propose a dual CL method, referred to as RecDCL. RecDCL joints feature-wise objectives and batch-wise objectives for selfsupervised recommendations. On one hand, RecDCL optimizes the feature-wise CL objective (FCL) by eliminating redundancy between users and items. Especially, FCL captures the distribution of user-item positive pairs by measuring a cross-correlation matrix and optimizes user (items) distribution via a polynomial kernel. On the other hand, as a batch-wise dimension, we design basic BCL and advanced BCL to enhance the robustness of the representations, which the latter combines historical embedding with current embeddings and generates contrastive views via online and target networks. Extensive experiments validate that RecDCL outperforms the state-of-the-art GNN-based and SSL-based models (by up to 5.34% on Beauty), showing the effectiveness of jointly optimizing the feature-wise and batch-wise objectives. The main contributions of this work are as follows: ¬∑ We theoretically reveal a native connection of the objective between feature-wise CL and batch-wise CL, and demonstrate a cooperative benefit by using them both from theoretical and experimental perspectives. ¬∑ Based on the above analysis, we propose a dual CL method named RecDCL with joint training objectives in feature-wise and batchwise ways to learn informative representations. ¬∑ We conduct extensive experiments on four public datasets and one industrial dataset to show the effectiveness of the proposed RecDCL. Compared to multiple state-of-the-art methods, RecDCL achieves significant performance improvements up to 5.34% in terms of NDCG@20 on the Beauty dataset and 5.65% in terms of Recall@20 on the Yelp dataset.",
  "2 PRELIMINARIES": "This section briefly introduces three closely related technologies: batch-wise collaborative filtering, batch-wise contrastive learning, and feature-wise contrastive learning.",
  "2.1 Batch-wise Collaborative Filtering": "Let U , I and R of a user-item bipartite graph G be the user set, item set and interactions between users and items. A collaborative filtering method aims to rank all items and predict the possible items that the user will interact with next. A popular architecture for collaborative filtering is GCN. Given the initial embedding e ùë¢ and e ùëñ for user ùë¢ and item ùëñ , GCN aims to iteratively perform neighborhood aggregation and update node representation on G . In each RecDCL: Dual Contrastive Learning for Recommendation WWW'24, May 13-17, 2024, Singapore, Singapore iteration, a user node ùë¢ aggregates the neighbors' representation of the ( ùëô -1 ) -th layer, with which e ( ùëô -1 ) ùë¢ is updated into e ( ùëô ) ùë¢ . The same update rule applies to an item node ùëñ . After the ùêø -layer iteration, the final ranking score between ùë¢ and ùëñ is calculated via the inner product of their representations, i.e., ÀÜ ùë¶ ùë¢,ùëñ = e ( ùêø ) ùë¢ e ( ùêø ) ùëñ . The optimization objective is to push ÀÜ ùë¶ ùë¢,ùëñ closer to the ground truth ùë¶ ùë¢ùëñ in a point-wise manner. As a representative implementation, pairwise Bayesian Personalized Ranking (BPR) [31] enforces the predicted score of positive interaction is higher than its negative counterpart as follows:  where O is the training data, ( ùë¢, ùëñ ) is the observed interaction, ( ùë¢, ùëó ) is the unobserved interaction, and ùúé is sigmoid function.",
  "2.2 Batch-wise Contrastive Learning (BCL)": "Due to the marginal improvements in existing BPR loss, a selfsupervised learning paradigm has achieved great success in computer vision and natural language understanding [5, 8, 13]. Some recent studies often focus on contrastive learning dominant in the SSL-based paradigm, typically InfoNCE Loss in SGL [40], which mainly relies on the idea of applying graph augmentations on graph structure. Suppose that there are two augmented views of nodes, the views of the same node are regarded as the positive pairs, i.e., ùëß ‚Ä≤ ùë¢ , ùëß ‚Ä≤‚Ä≤ ùë¢ , and the views of any different nodes as the negative pairs, i.e., ùëß ‚Ä≤ ùë¢ , ùëß ‚Ä≤‚Ä≤ ùë£ ( ùë¢, ùë£ ‚àà U , ùë¢ ‚â† ùë£ ) . InfoNCE Loss that aims to maximize the consistency of different views of the same node and minimize that of the views among different nodes are as follows:  in which ùë† (¬∑) means the similarity between two representation vectors. Afterwards, Cosine Contrastive Loss in BUIR [24] and SelfCF [54], Alignment and Uniformity Loss in DirectAU [36]) bring more significant recommendation performance improvements. Among them, it should be particularly pointed out that the nowadays CL-based models (e.g., BUIR and DirectAU) perform topùêæ recommendation tasks without negative sampling, which enforces faster convergence and better recommendation performance.",
  "2.3 Feature-wise Contrastive Learning (FCL)": "Recent studies (e.g., Barlow Twins [47] and VICREG [1]) in contrastive representation learning emphasize the importance of representations in feature-wise objectives. Given two batches of perturbed views, Barlow Twins first feeds these inputs into an encoder and produces batches of representations Z and ÀÜ Z , respectively. Then, it optimizes an innovative loss function by building a cross-correlation matrix C between Z and ÀÜ Z as follows  where 1 ‚â§ ùëö,ùëõ ‚â§ ùêπ denote the feature-wise indices of the embedding matrices. C is a square matrix that has the same dimensions as the encoder's output.  The first term, also called the invariance term, aims to make the diagonal values close to 1 and keep the embedding invariant even if applying distortions. On the other hand, the redundancy reduction term is defined to make the off-diagonal elements close to 0 and decorrelate each dimension representation of the embedding. In our work, we will explore contrastive representation learning via these two items for collaborative filtering.",
  "3 UNDERSTANDING BCL AND FCL": "Since BCL and FCL serve as the cornerstones of this paper, a more in-depth discussion about them may be warranted before diving into methodological details. To this end, we first review existing perspectives and then present our interpretation. We reveal a native connection between the two CL principles (Observation 3.1) and discover that combining BCL and FCL intuitively forms a better regularization which can benefit from high embedding dimensions (Observation 3.2).",
  "3.1 Existing Perspectives": "Many BCL objectives follow a basic form known as InfoNCE [29]. The core idea of InfoNCE is to estimate model parameters by contrasting the embedding between raw samples and some randomly augmented samples, which can be approximately viewed to maximize the mutual information estimation between the two input signals. Towards a more intuitive interpretation, Wang et al. [38] show that BCL optimizes two quantifiable metrics of representation quality: alignment of features from positive pairs, and uniformity of the induced distribution of the (normalized) features on the hypersphere. The alignment and uniformity principle provides a nice perspective to understand BCL, which has motivated many follow-up works [7, 10, 30, 37, 41]. To achieve the desired performance, BCL usually requires some careful implementation designs such as large batch sizes [5] and memory banks [13]. To avoid these implementation details, Zbontar et al. [47] develops the first FCL method named Barlow Twins (BT), which trains the model via minimizing the redundancy between the components of representation instead of directly optimizing the geometry of embedding distribution. Kalantidis et al. [21] indicate that BT can be seen as a more modern, simpler way to optimize deep canonical correlation analysis. Tsai et al. [35] relate Barlow Twins to the Hilbert-Schmidt independence criterion, which suggests a possibility to bridge the two major families of self-supervised learning philosophies: non-contrastive and contrastive approaches. BCL and FCL can be regarded as two dimensions of contrastive learning. Some recent works have been devoted to revealing the underlying connection between them. Towards the generalization ability of CL, Huang et al. [19] develop a mathematical analysis framework to prove that BCL and FCL enjoy some common advantages. Zhang et al. [51] propose a negative-free contrastive learning method that combines BCL and FCL, but they fail to clarify the insight for doing so. Following the principle of maximum entropy in information theory, Liu et al. [28] propose a maximum entropy WWW'24, May 13-17, 2024, Singapore, Singapore Zhang et al. coding loss, which provides a unified perspective to understand BCL and FCL objectives. Summary. The explanation for BCL alone [12, 29, 38] or FCL alone [21, 35, 47] has been well established. In addition, the connection between BCL and FCL has also been explored from the generalization view [19] and the maximum entropy coding theory [28]. However, there still lacks a native interpretation to connect them without introducing extra knowledge. Furthermore, efforts to understand the effect of combining BCL and FCL are almost missing. We try to answer these two questions next.",
  "3.2 Our Interpretation": "This part aims to approach two questions: what is the relationship between BCL and FCL (Cf. Observation 3.1), and why does combining them work (Cf. Observation 3.2). Intuitively, BCL and FCL share the same mechanism, i.e., drawing positive pairs close while pushing negative pairs away. The difference lies in the objects that make up their pairs. The considered objects are samples for BCL while features for FCL. This difference seems to endow the two CL objectives with different effects when optimizing a model. Interestingly, they lead the model to optimize in similar directions under some conditions. Formally, we conclude our observation as follows with theoretical analyses provided in A.1. Observation 3.1. If the two embedding matrices are standardized (i.e., they have mean zero and standard deviation one), then the objectives of BCL and FCL can be approximately transformed to each other 2 . Observation 3.1 shows that there exists an inherent connection between BCL and FCL. A follow-up question would be whether it is necessary to use them both? We provide a perspective below to partially answer this question. The key to our perspective is to consider the role that negative pairs play in the two objectives. For BCL, it has been known that pushing the negative pairs away actually approximately encourages the samples to be evenly distributed in the embedding space, which together with the positive pair constraint implicitly enlarges the classification margin [38]. For FCL, the reason for pushing negative pairs away in the feature-wise space may be not obvious. Although some explanations have been made from the information theory [1, 23, 28, 47], a more intuitive explanation can help to understand how this regularization contributes to final embedding. Therefore, we provide an illustrative example to interpret the effect of BCL, FCL, and BCL+FCL in Figure 1, with main observations concluded in Observation 3.2. Observation 3.2. For normalized sample embedding, pushing negative pairs away has different influences on embedding learning between BCL and FCL. For BCL, it encourages samples to be evenly distributed in the embedding space. For FCL, it tends to drive the representations of samples to be orthogonal. This difference is mainly due to that BCL encourages the inner product of negative pairs (in the batch dimension) to be as small as possible; but FCL only enforces the inner product of negative pairs (in the feature dimension) to be close to zero, which implicitly encourages the representations of samples (in 2 The theoretical analysis for Observation 3.1 is provided in Appendix A.1. the batch dimension) to be orthogonal. If we combine BCL and FCL, pushing negative pairs away will not only encourage sample representations to be evenly distributed in the embedding space but also help eliminate redundant solutions (Cf. Figure 1) 3 . This regularity can benefit embedding learning as the embedding dimension increases 4 .",
  "3.3 Recommendation Intuition": "To validate the effectiveness of regularity in Section 3.2, we conduct an ablation study to see whether combining BCL and FCL results in a more desirable embedding distribution. Specifically, we compare the average entropy of the embeddings among FCL, BCL, and BCL+FCL on the Yelp dataset. Let x denote the embedding of a sample. We select the topùêæ (1024 and 2048) absolute values of x via two approaches and normalize them into a K-dimensional probability distribution. The first one is we sort the embedding values in a descending way for each sample and obtain the topùêæ values, called each-sample. The second one is we calculate the mean values in each dimension for all samples, sort the values, obtain the topùêæ indices, and extract the topùêæ values for each sample, called mean-sample. Based on the above step, the entropy of each sample can be calculated and we average it throughout all samples. The results are shown in Table 2 and Table 3 (lower average entropy indicates sharper embedding distribution). Table 2: The result of each-sample method. Table 3: The result of the mean-sample method. We can observe that BCL+FCL achieves the smallest average entropy, which demonstrates the embedding distribution of BCL+FCL is sharper (Cf. the bottom right of Figure 1). Note that no contradiction arises between Observation 3.1 and Observation 3.2. The former reveals the connection between BCL and FCL through some theoretical approximations and assumptions, while the latter states that pushing negative pairs away in BCL and FCL can have complementary benefits. These consequences afford insights into a dual CL design which motivates our RecDCL.",
  "4 THE RECDCL METHOD": "Motivated by the analyses in Section 3, we develop a dual CL framework for recommendation, referred to as RecDCL. RecDCL is mainly characterized by two recommendation-fitted CL objectives: a RecFCL objective (Section 4.1) for driving the representations to be orthogonal, and a Rec-BCL objective (Section 4.2) for enhancing the robustness of the representations. Throughout this section, we use E ùëà ‚àà R ùêµ √ó ùêπ ( E ùêº ‚àà R ùêµ √ó ùêπ ) to denote the user (item) embedding 3 The theoretical and experimental analysis for Observation 3.2 is provided in Appendix A.2. 4 Empirical evidence will be provided in Section 3.3. RecDCL: Dual Contrastive Learning for Recommendation WWW'24, May 13-17, 2024, Singapore, Singapore Figure 2: The overall framework of RecDCL. In this framework, (a) denotes the cross-correlation matrix to an identity matrix between users and items in FCL; (b) and (c) stand for the distribution uniformity within users and items in FCL; (d) denotes the random distribution on user-item positive pairs in BCL; (e) demonstrates the final distribution on users and items produced by a dual CL. User-item Positive  Pairs G N N ùë¢ ! ùë¢ \" ùë¢ # ùë£ $ ùë£ ! ùë£ # ùë¢ $ Recall ùëí % (‚àó) ùëí ) (‚àó) Feature-wise CL Objective Batch-wise CL Objective ‚Ñí **++ = ùëì(ùë¨ ùëº ) + ùëì(ùë¨ ùë∞ (.) ) ‚Ñí *+/0 = ùëì(ùë¨ ùëº , ùë¨ ùë∞ (.) ) ‚Ñí /12 = ùëì ùë¨ ùëº , ùë¨ ùë∞ (.) +ùëì ùë¨ ùë∞ (.) , ùë¨ ùëº ùê∏ * Batch Samples ùê∏ + . Batch Samples Users and Items Users Items RecDCL ùê∏ * Batch Samples ùê∏ + . Batch Samples Stop gradient ùë£ \" (a) (b) (c) (d) (e) matrix, and E ùëö, : ùëà and E : ,ùëõ ùëà to respectively denote the ùëö -th row and the ùëõ -th column of E ùëà , where ùêµ stands for the number of samples in a batch and ùêπ represents the embedding dimension.",
  "4.1 FCL Objective for Recommendation": "Eliminate redundancy between users and items. To explore the alignment in an FCL way, we propose to extend the Barlow Twins objective function, namely UIBT, for self-supervised recommendations. More specially, we build a cross-correlation matrix computed from user and item embedding, and make it close to the identity matrix via invariance term and variance term as shown in Figure 2. Formally, the cross-correlation matrix C between E ùëà and E ùêº can be computed. C is a square matrix that has the same dimensions as the encoder's output. Note that we define the invariance term and redundancy reduction term with a factor 1 / ùêπ that scales the criterion as a function of the dimension:  In Eq. (5), the invariance term aims to make the diagonal elements of the matrix C equal to 1 and meet the vector invariance to the distorted samples. Besides, the redundancy reduction term aims to make the off-diagonal elements of the matrix C equal to 0 and reduce the redundancy within the output representation. Eliminate redundancy within users and items. In addition, to further enhance the embedding diversity between different features, wepropose a feature-wise uniformity optimization via a polynomial kernel, namely UUII. The polynomial kernel is the natural featurewise way to present samples on the hypersphere. Considering the possible distribution gap between users and items, we calculate uniformity separately within the user embedding and the item embedding. The joint objective can be formulated as  (6) where ùëé , ùëê and ùëí are parameters of the polynomial kernel and set to 1, 1 ùëí -7 and 4 by default, respectively. Note that feature-wise uniformity loss is only calculated via representations of in-batch samples since in-batch instances are more consistent with the actual user and item data distribution. As a result, the user/item distribution will be uniform, as shown in Figure 2. In addition, the exposure bias can be reduced by incorporating user/item distribution, which is illustrated in CLRec [53].",
  "4.2 BCL Objective for Recommendation": "Indeed, the proposed BCL objective and the proposed FCL objective (UIBT and UUII) are designed and evaluated for recommendations. Basic BCL. To validate the generality, we propose a baseline that solely combines the basic BCL (DirectAU) and FCL (only off-diagonal elements) methods. We directly design a summing loss function called DCL and conduct experiments on four public datasets. The summing loss function is described as:  Advanced BCL. As analyzed in Section 3.2, combining the two dimensions FCL and BCL can intuitively benefit model learning. It motivates us to further improve optimization via a BCL objective. Generally, data augmentation can be achieved via a series of meaningful perturbations in many scenarios such as computer vision and neural language processing. However, positive user-item pairs in CF need to be preserved for representation invariance and are difficult to distort for data augmentation. To avoid this issue and achieve the same effect, as shown in Figure 2, we conduct data augmentation on output representation and generate contrastive but related views for representation learning, namely BCL. Owing to the simple design and effectiveness of LightGCN, we adopt it as the graph encoder ùëì ùúÉ to conduct node aggregation and propagation. After generating embedding of each layer and stacking multi-layer representations, we use historical embeddings [3, 9, 54] to perform augmentation on output embedding. For the objective function, a trivial choice would be directly applying the original InfoNCE introduced in Eq. (2). A recent work WWW'24, May 13-17, 2024, Singapore, Singapore Zhang et al. [48] indicates that InfoNCE can be understood under a more general framework. It provides a perspective to unify InfoNCE with another popular SSL method SimSiam, which implicitly performs CL through the stop-gradient and asymmetric trick. We empirically find that this implicit CL design achieves better performance and choose it as our implementation. Suppose E ùëà ( E ùêº ) is the current embedding generated by the encoder ùëì ùúÉ and E ( ‚Ñé ) ùëà ( E ( ‚Ñé ) ùêº ) is the historical embedding. The perturbed representation ÀÜ E ùëà is calculated by combining E ( ‚Ñé ) ùëà and E ( ‚Ñé ) ùëà :  where ùúè is a hyper-parameter that controls the embedding information preservation ratio from a prior training iteration. Note that we perform representation distortion on the historical embedding from prior training iterations in the target network instead of perturbing the input commonly [24] or the current node embedding [44, 45] directly as used in previous CL-based recommendation methods. Besides, online and target networks share the same graph encoder ùëì ùúÉ in this component, which can reduce the additional memory and computation cost. Therefore, the optimization of batch-wise output augmentation can be formulated as follows:  where ‚Ñé (¬∑) is a multi-layer perceptron network; ùë†ùëî (¬∑) is the stopgradient operation; and ùëÜ (¬∑ , ¬∑) denotes the cosine distance.",
  "4.3 Training Objective": "In a word, the main goal of RecDCL is to design an FCL objective and an advanced BCL objective that capture embedding importance and maximize the benefits from a dual CL to encourage embedding learning, as shown in Figure 2. In practice, we jointly optimize these three objectives with trade-off hyperparameters ùõº and ùõΩ as follows:  Finally, we calculate the ranking score function by using the inner product between user and item representations.",
  "5 EXPERIMENTS": "In this section, we conduct extensive experiments on four public datasets and one industrial dataset in real-world application to validate the effectiveness of RecDCL. We introduce the overall performance and study the influence of each design of RecDCL. We also describe experimental settings, analyze the efficiency, and give detailed hyper-parameter sensitivity analysis in Appendix D.",
  "5.1 Overall Performance": "In Table 4, we show the overall top-20 performance of all the baselines and our RecDCL. Specifically, we highlight the best result in bold and the second best result in underline, calculate the relative improvement (%Improv.) for our methods, and show the ùëù -value in t-test experiments on each dataset. We give detailed observations based on these experimental results. Comparison with MF-based models. From Table 4, we can observe that SSL-based models except BUIR on Game, particularly our RecDCL, can obtain superior results on most conditions. This demonstrates that SSL-based models have remarkable advantages in the problem of extremely sparsity data. More specifically, BPR-MF outperforms VAE-based models and GNNs-based models on Beauty and Food datasets, while the performance of NeuMF is the poorest on all datasets. This indicates that the VAE-based models and GNNsbased models can better capture the interactions between users and items via variational autoencoders and graph neural networks. Comparison with VAE-based models. Compared to GNNsbased models, VAE-based models obtain inferior results on Beauty and Game even though they are effective on Yelp. In contrast, our RecDCL as well as contrastive learning-based DirectAU are robust and achieve significantly better on all four datasets. This suggests that SSL-based models represented by our RecDCL can address the sample distribution of users and items in user-item interaction well. Comparison with GNNs-based models. As shown in Table 4, SSL-based models (CLRec and our RecDCL) are higher than GNNsbased models in terms of Recall@20 and NDCG@20 on all four datasets, while BUIR performs well on Beauty and Food datasets. Especially, our proposed RecDCL exceeds the state-of-the-art GNNsbased model LightGCN by 17.06%, 17.87%, 6.46%, and 37.49% in terms of Recall@20 on all four datasets respectively, which verifies the effectiveness of constructing SSL-based loss function instead of BPR loss with negative sampling. Moreover, this advantage also answers the question mentioned in Section 1 that CF-based recommendation models without negative sampling can still be effective. Comparison with SSL-based models. Particularly, in Table 4, we show existing batch-wise CL objectives based BUIR, CLRec, and DirectAU in SSL-based recommendation outperforms the state-ofthe-art GNNs-based methods in CF, although only using positive user-item pairs to construct contrastive learning loss function. Particularly, our proposed RecDCL consistently derives promising results on all four datasets and improves by 1.33%-5.20% and 3.35%5.34% in terms of Recall@20, and NDCG@20, which verifies the effectiveness of incorporating the feature-wise objectives for selfsupervised recommendation in RecDCL. It also meets the viewpoint in Section 1 that feature-wise CL is worth considering and the theoretical analysis in Section 3 about the effective combination of feature-wise objectives and batch-wise objectives for SSR.",
  "5.2 Study of RecDCL": "To investigate the reasons for RecDCL's effectiveness, we perform comprehensive ablation experiments to study the necessity of each component in RecDCL. Considering the limited space, we only show the analysis on representative datasets (Beauty and Yelp). The result analysis on Food and Game datasets is represented in Table 11 of Appendix D.5. Effect of feature-wise objective UIBT. As discussed in Section 4, UIBT achieves approximate results compared to the base encoder model LightGCN in feature-wise objectives. Table 5 shows the comparison between LightGCN and the components of UIBT on Beauty and Yelp. We can find that (1) only \"w/ UIBT\" is better than LightGCN on Beauty in terms of Recall@20 and NDCG@20/ (2) To validate the effectiveness of UIBT, we compare results between \"w/ UUII\" and \"w/ UIBT & UUII\". From Table 5, we find that the RecDCL: Dual Contrastive Learning for Recommendation WWW'24, May 13-17, 2024, Singapore, Singapore Table 4: Overall top-20 performance (% is omitted) comparison with representative models on four datasets (R and N are the abbreviations for Recall and NDCG). 1 Note that we tune embedding size from 32 to 2048 and report the best results for all baselines and our method RecDCL. Generally, the embedding size is set by default to 64. 2 Indeed, RecDCL is the very first work to explore the effectiveness of FCL for recommendations. We have looked and found that there are no appropriate baselines for FCL. To comprehensively compare, we conduct the experiments in ablation studies, that is UIBT for FCL. latter value is not just higher than LightGCN, and higher than \"w/ UUII\" on Beauty and Yelp. (3) Similarly, the comparison between \"w/ BCL\" and \"w/ UIBT & BCL\" also shows this ascending trend on two datasets, which again demonstrates the importance of interactions between users and items either in FCL or BCL. Table 5: Performance comparison of different designs of RecDCL on Beauty and Yelp. sample distribution within users and items for recommendation. Besides, addressing feature-wise interaction distribution and user (item) data distribution is vital for performance improvements. (3) The comparison between \"w/ BCL\" and \"w/ UUII & BCL\" shows the performance improvement given by \"w/ UUII\". (4) In fact, the UUII component simulates the real distribution among users (items) and meets the essential characteristic of contrastive learning. In combination with the above analysis, the UIBT maximizes the similar representation while minimizing the dissimilar embedding between users and items of in-batch, and the UUII nomial minimizes the similarity among users (items) in a feature-wise way. Effect of feature-wise objective UUII. As shown in Table 5, (1) Compared to \"w/ UIBT\" and \"w/ BCL\", the performance of only feature-wise UUII, namely \"w/ UUII\", is extremely poor on Beauty and Yelp. This finding is similar to DirectAU which only optimizes uniformity, which just goes to show the importance of optimizing uniformity. (2) Compared to \"w/ UIBT\" and \"w/ UIBT & UUII\", the performance of \"w/ UIBT & UUII\" consistently outperforms \"w/ UIBT\" on Beauty and Yelp datasets. This shows the importance of Effect of batch-wise augmentation BCL. From Table 5, we have some observations: (1) we first conduct experiments solely on batchwise augmentation and find that it is better than base LightGCN on the Beauty dataset, which shows the benefits of data augmentation that reinforces the interaction between users and items. (2) To further study the influence of output augmentation, we compare \"w/ UUII\" and \"w/ UUII & BCL\", which demonstrates the effectiveness of BCL. Note that the performance of \"w/ UUII & BCL\" is better than \"w/ BCL\" even if the performance of only \"w/ UUII\" is poor, which again shows the importance of combining interactions with user (item) distribution. (3) Meanwhile, the comparison between \"w/ UIBT\" and \"w/ UIBT & BCL\" also shows the effectiveness of BCL on these two datasets.",
  "5.3 Industrial Results": "To further study the effectiveness of our method, we employ RecDCL on an industrial dataset, which is collected from an online payment platform. We first extract links between users and retailers from user payment behavior logs in a month, December 2022. WWW'24, May 13-17, 2024, Singapore, Singapore Zhang et al. Then, we define users who conduct more than 3 payments as core users and subsample the users from the core user set. The general statistics of this dataset are summarized in Table 6. Table 6: Statistics of payment dataset. Table 7: Performance of RecDCL on payment platform data. We conducted an offline experiment on a worker equipped with NVIDIA-A100 GPU, and 40GB memory. Following the experimental setting in Section 5, the users' behaviors are also split into training/validation/test sets with a ratio of 0.8/0.1/0.1. The main task focuses on retrieving topùëÅ business from the item pool and then recommending them to users. To evaluate the task, we introduce Recall@20 and NDCG@20 as metrics, which are carefully considered in the real industrial platform. Notice that although the baseline of this platform is BPR-MF, we also conduct experiments on LightGCN and DirectAU to further compare our method with others. As shown in Table 4, the overall effectiveness of DirectAU is better on the private dataset than on the public datasets. Regarding this, we speculate that the reason behind this is the long-tail effect of this task is more serious, and DirectAU has a uniform loss, which may be a little bad in the case of such a serious long-tail effect. Table 7 demonstrates that our method performs significantly compared with other baselines in terms of two metrics Recall@20 and NDCG@20, improving by 3.40% and 7.53% respectively.",
  "6 RELATED WORK": "",
  "6.1 Collaborative Filtering": "Collaborative filtering plays a vital role in modern recommendation systems [33]. The key idea of CF is that similar users tend to have similar preferences on items. To address the issue of data sparsity in CF, Matrix Factorization (MF) decomposes the original user-item interacted matrix to the low-dimensional matrix with user (item) latent features. NeuMF [17] based on a deep neural network is proposed to learn rich information and compress latent features. Furthermore, based on the viewpoint that the modeling capacity of linear factor models often is limited, Liang et al. [26] introduce the generative model - variational autoencoders (VAEs) with multinomial likelihood into collaborative filtering tasks and propose Mult-VAE, which shows excellent performance for topùëÅ recommendations mainly owing to well modeling user-item implicit feedback data. Afterward, RecVAE [34] improves several aspects of the Mult-VAE, mainly including a novel encoder, novel approach to setting hyperparameter, and new training strategy, and then performs well. With the development of graph neural networks (GNNs) [39, 49], GNNs-based CF models are widely studied and proposed. The observed user-item interaction matrix can be built as a bipartite graph, and GNNs-based models [4, 16, 39, 55] aggregate information from neighbors and capture high-order connection information on graph-structure data. Take LightGCN [16] as an example, it simplifies the GNN architecture by removing nonlinear activation as well as feature transformation based on NGCF [39]. Such methods do produce relatively ideal results, however, there still remains a tricky challenge when we are faced with extremely sparse data.",
  "6.2 Contrastive Learning for Recommendation": "Recently, self-supervised learning for recommendations has attracted increasing attention and several SSL-based CF [2, 15, 24, 27, 36, 40, 42, 53, 54] has achieved competitive results. According to the learning objective and negative sampling strategy, we divide existing SSL-based CF into two categories: (1) With sampling: the state-of-the-art model SimGCL [45] perturbed the current representation of each node via random and different noise-based data augmentation. (2) Without sampling: inspired by MoCo [14], BUIR [24] proposes an asymmetric structure to learn user and item representation solely on user-item positive pairs. SelfCF [54] inherits the Siamese network structure of SimSiam's architecture [6] and optimizes Cosine Similarity loss on output augmentation obtained in in-batch positive-only data. CLRec [53] adopts InfoNCE loss to tackle exposure bias for recommendation. DirectAU [36] focuses on the desired properties of representations from the alignment and uniformity metrics and optimizes these two properties via a new loss function in CF. However, none of these works considered CL-based recommendations from the perspective of feature-wise, resulting in limited performance. Barlow Twins [47] is the first to investigate the feature-wise objective in CV domains. Motivated by Barlow Twins, we study the feasibility of feature-wise objectives and design a dual CL with BCL and FCL objectives. Due to the prominent results on SSL-based models without sampling, we follow this design principle to optimize the learning objective.",
  "7 CONCLUSION": "In this work, we theoretically reveal the connection of the objective between BCL and FCL and show a cooperative benefit by using them both, which motivates us to develop a dual CL called RecDCL that jointly optimizes BCL and FCL training objectives to learn informative representations for recommendation. Then we investigate the two desired properties of representation - alignment and uniformity - in FCL objectives for CF, and perform data augmentation on output vectors for robustness in BCL objective. Extensive experiments on four public datasets and an industrial dataset show the superiority of our proposal considering the FCL objectives. We hope RecDCL could attract the CF community's attention to the learning paradigm toward FCL perspective representation properties. In the future, we will investigate other CL-based training objectives that also favor feature-wise perspectives to improve effectiveness and efficiency.",
  "ACKNOWLEDGMENTS": "This work has been supported by Technology and Innovation Major Project of the Ministry of Science and Technology of China under Grant 2022ZD0118600, Natural Science Foundation of China (NSFC) 62276148, NSFC for Distinguished Young Scholar 61825602, the New Cornerstone Science Foundation through the XPLORER PRIZE, and a research fund from Zhipu AI. RecDCL: Dual Contrastive Learning for Recommendation WWW'24, May 13-17, 2024, Singapore, Singapore",
  "REFERENCES": "[1] Adrien Bardes, Jean Ponce, and Yann LeCun. 2021. Vicreg: Varianceinvariance-covariance regularization for self-supervised learning. arXiv preprint arXiv:2105.04906 (2021). [2] Xuheng Cai, Chao Huang, Lianghao Xia, and Xubin Ren. 2023. LightGCL: Simple Yet Effective Graph Contrastive Learning for Recommendation. arXiv preprint arXiv:2302.08191 (2023). [3] Jianfei Chen, Jun Zhu, and Le Song. 2017. Stochastic training of graph convolutional networks with variance reduction. arXiv preprint arXiv:1710.10568 (2017). [4] Lei Chen, Le Wu, Richang Hong, Kun Zhang, and Meng Wang. 2020. Revisiting graph based collaborative filtering: A linear residual graph convolutional network approach. In Proceedings of the AAAI conference on artificial intelligence , Vol. 34. 27-34. [5] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A simple framework for contrastive learning of visual representations. In International Conference on Machine Learning . PMLR, 1597-1607. [6] Xinlei Chen and Kaiming He. 2021. Exploring simple siamese representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition . 15750-15758. [7] Ching-Yao Chuang, Joshua Robinson, Yen-Chen Lin, Antonio Torralba, and Stefanie Jegelka. 2020. Debiased contrastive learning. In Advances in Neural Information Processing Systems . 8765-8775. [8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics . [9] Matthias Fey, Jan E Lenssen, Frank Weichert, and Jure Leskovec. 2021. Gnnautoscale: Scalable and expressive graph neural networks via historical embeddings. In International Conference on Machine Learning . PMLR, 3294-3304. [10] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple Contrastive Learning of Sentence Embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing . 6894-6910. [11] Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics . JMLR Workshop and Conference Proceedings, 249-256. [12] Michael Gutmann and Aapo Hyv√§rinen. 2010. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of the thirteenth international conference on artificial intelligence and statistics . JMLR Workshop and Conference Proceedings, 297-304. [13] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition . 9729-9738. [14] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition . 9729-9738. [15] Wei He, Guohao Sun, Jinhu Lu, and Xiu Susie Fang. 2023. Candidate-aware Graph Contrastive Learning for Recommendation. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (<conf-loc>, <city>Taipei</city>, <country>Taiwan</country>, </confloc>) (SIGIR '23) . Association for Computing Machinery, New York, NY, USA, 1670-1679. https://doi.org/10.1145/3539618.3591647 [16] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for recommendation. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval . 639-648. [17] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural collaborative filtering. In Proceedings of the 26th international conference on world wide web . 173-182. [18] Roger A Horn and Charles R Johnson. 2012. Matrix Analysis . Cambridge university press. [19] Weiran Huang, Mingyang Yi, and Xuyang Zhao. [n. d.]. Towards the generalization of contrastive self-supervised learning. arXiv preprint arXiv:2111.00743 , year=2021 ([n. d.]). [20] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. 2020. A survey on contrastive self-supervised learning. Technologies 9, 1 (2020), 2. [21] Y. Kalantidis, C. Lassance, J. Almaz√°n, and D. Larlus. 2022. TLDR: Twin learning for dimensionality reduction. Transactions of Machine Learning Research (2022). https://openreview.net/forum?id=86fhqdBUbx [22] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014). [23] Julius Von K√ºgelgen, Yash Sharma, Luigi Gresele, Wieland Brendel, Bernhard Sch√∂lkopf, Michel Besserve, and Francesco Locatello. 2021. Self-Supervised Learning with Data Augmentations Provably Isolates Content from Style. In Advances in Neural Information Processing Systems , A. Beygelzimer, Y. Dauphin, [47] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St√©phane Deny. 2021. Barlow twins: Self-supervised learning via redundancy reduction. In International WWW'24, May 13-17, 2024, Singapore, Singapore Zhang et al. Conference on Machine Learning . PMLR, 12310-12320. [48] Chaoning Zhang, Kang Zhang, Chenshuang Zhang, Trung X. Pham, Chang D. Yoo, and In So Kweon. 2022. How Does SimSiam Avoid Collapse Without Negative Samples? A Unified Understanding with Self-supervised Contrastive Learning. In International Conference on Learning Representations . https://openreview.net/ forum?id=bwq6O4Cwdl [49] Dan Zhang, Wenzheng Feng, Yuandong Wang, Zhongang Qi, Ying Shan, and Jie Tang. 2024. DropConn: Dropout Connection Based Random GNNs for Molecular Property Prediction. IEEE Transactions on Knowledge and Data Engineering 36, 2 (2024), 518-529. https://doi.org/10.1109/TKDE.2023.3290032 [50] Dan Zhang, Yifan Zhu, Yuxiao Dong, Yuandong Wang, Wenzheng Feng, Evgeny Kharlamov, and Jie Tang. 2023. ApeGNN: Node-Wise Adaptive Aggregation in GNNs for Recommendation. In Proceedings of the ACM Web Conference 2023 . 759-769. [51] Shaofeng Zhang, Feng Zhu, Junchi Yan, Rui Zhao, and Xiaokang Yang. 2022. Zero-CL: Instance and Feature decorrelation for negative-free symmetric contrastive learning. In International Conference on Learning Representations . https: //openreview.net/forum?id=RAW9tCdVxLj [52] Wayne Xin Zhao, Shanlei Mu, Yupeng Hou, Zihan Lin, Yushuo Chen, Xingyu Pan, Kaiyuan Li, Yujie Lu, Hui Wang, Changxin Tian, et al. 2021. Recbole: Towards a unified, comprehensive and efficient framework for recommendation algorithms. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management . 4653-4664. [53] Chang Zhou, Jianxin Ma, Jianwei Zhang, Jingren Zhou, and Hongxia Yang. 2021. Contrastive learning for debiased candidate generation in large-scale recommender systems. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining . 3985-3995. [54] Xin Zhou, Aixin Sun, Yong Liu, Jie Zhang, and Chunyan Miao. 2021. SelfCF: A Simple Framework for Self-supervised Collaborative Filtering. arXiv preprint arXiv:2107.03019 (2021). [55] Yifan Zhu, Qika Lin, Hao Lu, Kaize Shi, Donglei Liu, James Chambua, Shanshan Wan, and Zhendong Niu. 2023. Recommending Learning Objects Through Attentive Heterogeneous Graph Convolution and Operation-Aware Neural Network. IEEE Transactions on Knowledge and Data Engineering 35, 4 (2023), 4178-4189. https://doi.org/10.1109/TKDE.2021.3125424",
  "A MORE DETAILED EXPLANATION": "",
  "A.1 Connection Between BCL and FCL": "The part provides the theoretical analyses on Observation 3.1 in the main text, which is restated as follows for convenience. Observation A.1. If the two embedding matrices are standardized (i.e., they have mean zero and standard deviation one), then the objectives of BCL and FCL can be approximately transformed to each other. Our main idea is to transform both BCL and FCL objectives into matrix forms and then connect them through the algebraic properties of matrices. Formally, let Z ‚àà R ùëÅ √ó ùê∑ and ÀÜ Z ‚àà R ùëÅ √ó ùê∑ denote the embedding matrices of raw samples and the corresponding augmented samples in a batch, respectively. The BCL objective can be formulated as:   Here, we assume that z ‚ä§ ùëñ ÀÜ z ùëó ‚àà [ 0 , 1 ] for any 1 ‚â§ ùëñ, ùëó ‚â§ ùëÅ , as the negative terms contribute little to the log-sum-exponential function. With this assumption, let us consider a surrogate of R.H.S. in Eq. (11):  where the push-away term is a reasonable approximation to the counterpart in Eq. (11) since both the sum-square function and the log-sum-exponential function relatively enhance larger terms while weakening smaller terms for non-negative values. With this surrogate, we can transform the BCL objective into a matrix form:  On the other hand, with the condition that both Z and ÀÜ Z are standardized, setting the redundancy reduction weight ùúÜ = 1 in the FCL objective yields  = ‚à• I - Z Z ‚à• ‚ä§ ÀÜ 2 ÀÜ ‚ä§ ‚ä§ ÀÜ ÀÜ ‚ä§ ‚ä§ ÀÜ ùêπ = ùê∑ + Tr ( Z ZZ Z ) - Tr ( Z Z ) - Tr ( Z Z ) . Comparing Eq. (13) and Eq. (14), we find that they differ by only a constant | ùëÅ -ùê∑ | if Z ‚ä§ ÀÜ Z = ÀÜ Z ‚ä§ Z and Z ÀÜ Z ‚ä§ = ÀÜ ZZ ‚ä§ 5 , i.e., Tr ( ÀÜ ZZ ‚ä§ Z ÀÜ Z ‚ä§ ) - Tr ( ÀÜ ZZ ‚ä§ ) - Tr ( Z ÀÜ Z ‚ä§ )  where ùúÜ ùëñ stands for the ùëñ -th eigenvalue, and the equation from the second row to the third row holds because ÀÜ Z ‚ä§ Z , Z ‚ä§ ÀÜ Z , ÀÜ Z ‚ä§ Z and Z ‚ä§ ÀÜ Z share exactly the same non-zero eigenvalues [18]. Note that the conditions Z ‚ä§ ÀÜ Z = ÀÜ Z ‚ä§ Z and Z ÀÜ Z ‚ä§ = ÀÜ ZZ ‚ä§ can be satisfied if Z ‚âà ÀÜ Z , which is mild requirement since the objective of CL naturally leads to Z ‚âà ÀÜ Z . Eq. (15) reveals that, under the assumed conditions, the objectives of BCL and FCL can be approximately transformed to each other.",
  "A.2 Joint BCL and FCL": "Observation A.2. For normalized sample embedding, pushing negative pairs away has different influences to embedding learning between BCL and FCL. For BCL, it encourages samples to be evenly distributed in the embedding space. For FCL, it tends to drive the representations of samples to be orthogonal. This difference is mainly due to that BCL encourages the inner product of negative pairs (in the batch dimension) to be as small as possible; but FCL only enforces the inner product of negative pairs (in the feature dimension) to be close to zero, which implicitly encourages the representations of samples (in the batch dimension) to be orthogonal. If we combine BCL and FCL, pushing negative pairs away will not only encourage sample representations to be evenly distributed in the embedding space but also help eliminate redundant solutions (Cf. Figure 1). This regularity can benefit embedding learning as the embedding dimension increases 6 . 5 These two conditions imply that Tr ( ÀÜ ZZ ‚ä§ Z ÀÜ Z ‚ä§ ) = Tr ( ÀÜ Z ‚ä§ ZZ ‚ä§ ÀÜ Z ) , and all ÀÜ Z ‚ä§ Z , Z ‚ä§ ÀÜ Z , ÀÜ Z ‚ä§ Z and Z ‚ä§ ÀÜ Z can be similarly diagonalized. 6 Empirical evidence will be provided in Section D.7. RecDCL: Dual Contrastive Learning for Recommendation WWW'24, May 13-17, 2024, Singapore, Singapore The key to our interpretation for Observation 3.2 is to consider the quality of the solution sets resulting from the push-away objectives (for negative pairs) in BCL and FCL: using only BCL or only FCL would lead to a large number of redundant solutions while combining BCL and FCL would effectively reduce this redundancy and never miss an optimal solution. We start with the special case of one negative pair (the example shown in Figure 1), followed by a general case of an arbitrary number of negative samples. Theory-wise. Suppose that z 1 ‚àà R ùê∑ and z 2 ‚àà R ùê∑ are representations of a negative pair residing on a hyper-sphere surface S ùê∑ -1 ‚âú { z ‚àà R ùê∑ | | | z | | 2 = 1 } . Then the push-away objective in BCL and FCL can be respectively formulated as  and  where off-diag (¬∑) denotes a projection operator which preserves the off-diagonal elements of a matrix. It can be verified that the optimal solution to (16) and (17) is given by  and  respectively. In general, a solution in Z ‚òÖ B (cf. the top right of Figure 1) does not admit a solution in Z ‚òÖ F (cf. the bottom left of Figure 1). However, if z 1 is a one-hot vector and z 2 = -z 1 (cf. the bottom right of Figure 1), substituting them into (16) and (17) respectively yields the optimal value -1 and 0, implying that they exactly fall into the joint solution set Z ‚òÖ B ‚à© Z ‚òÖ F ‚â† ‚àÖ . The above analysis explains the derivation of Figure 1 in our paper. Regarding \"implicitly encourages the representations of samples (in the batch dimension) to be orthogonal\", let us first consider a special case where the normalized embedding matrices Z , ÀÜ Z ‚àà R ùêµ √ó ùêπ and the feature dimension ùêµ equals the batch dimension ùêπ . In this case, the FCL objective which optimize Z ‚ä§ ÀÜ Z to approach the identity matrix I indeed encourages the representations of samples to be orthogonal (i.e., Z ÀÜ Z ‚ä§ ‚âà I ) since Z ÀÜ Z ‚ä§ = I ‚áê‚áí Z ‚ä§ ÀÜ Z = I . For the case ùêµ > ùêπ , FCL encourages feature embeddings to fall into ùêπ clusters which are orthogonal to each other. For the case ùêµ < ùêπ , FCL encourages feature embeddings to be orthogonal in a subspace. Furthermore, for the general case where ùëÅ negative samples constitute a representation matrix Z ‚àà R ùëÅ √ó ùê∑ , the push-away objective in BCL and FCL can be respectively formulated as  where 1 ‚àà R ùëÅ denotes a all-one vector. It is easy to verify that ùëì B and ùëì F are respectively invariant under the right rotation and the left rotation, i.e.,  where R B ‚àà R ùê∑ √ó ùê∑ and R F ‚àà R ùëÅ √ó ùëÅ denote any rotation matrices. FCL inherently tends to make each feature dimension orthogonal to each other. However, this does not necessarily imply that the samples are positioned on the axes. These rotation-invariances induce redundancy when using BCL only or FCL only. Positioning the samples on the axes represents merely one specific instance of many redundant solutions. In contrast, combing BCL and FCL eliminates this redundancy since ùëì B+F ( Z ) ‚âú ùëì B ( Z ) + ùëì F ( Z ) is neither left-rotation invariant nor right-rotation invariant in general. On the other hand, one can construct an optimal solution to ùëì B+F that also admits the optimality of both ùëì B and ùëì F , so using ùëì B+F never miss the optimal solution. In summary, combining the push-away objectives in BCL and FCL would reduce redundant solutions but never miss an optimal solution, thus qualifying as a more reasonable regularization. These analyses support our claims in Observation 3.2. Experiment-wise. Remarkably, in the theoretical part, we consider the case where the vanilla BCL and FCL are directly combined by means of addition for easy of analysis. In practice, RecDCL incorporates more advanced BCL and FCL techniques in a soft way. This creates a slight discrepancy between theory and experience: the gap in average entropy between RecDCL and BCL is not very obvious. Nevertheless, they are consistent in trend.",
  "B EFFICIENCY ANALYSES": "RecDCL explores the influence of representation size from low dimension 32 to high dimension 2048 in feature-wise objectives. Therefore, we compare the training efficiency of RecDCL with the two methods that are most related to RecDCL, i.e., representative LightCCN and the state-of-the-art DirectAU. Considering space constraints and simplicity, here, we only present the largest dataset Food by Table 8. As we can see, Table 8 shows the reported results of memory consume (Memory), training time per epoch (Time/epoch), total epochs (#Epochs) and final performance (NDCG@20) when embedding size is setting as 2048. To fairly compare, we set the training batch size as 1024 and implement representative methods under the same framework RecBole. Table 8: Efficiency comparison of largest dataset Food (h: hour, m: minute, s: second). The statistic results in Table 8 show that LightGCN is the slowest in terms of the training time per epoch, which is since it performs multi-hop neighborhood aggregation and linear propagation and optimizes objective by time-consuming BPR loss. By contrast, our WWW'24, May 13-17, 2024, Singapore, Singapore Zhang et al. RecDCL consumes the least amount of time per epoch, but more total training time due to more epochs required than LightGCN and DirectAU. Surprisingly, these two methods take roughly the same amount of space, which indicates the advantage of RecDCL over LightGCN. DirectAU is indeed the most efficient of the two indicators, i.e., training time per epoch and total epochs, but it consumes a little more memory than LightGCN and our RecDCL. Though RecDCL requires more training epochs, it consistently brings performance improvements to RecDCL. However, possibly due to that RecDCL uses a comprehensive loss function leading to a more intricate \"optimization path\". In contrast, the performance of LightGCN and DirectAU plateaus quickly but remained unchanged after that. Thus, we believe the performance benefits and running costs in practice are indeed justified and acceptable.",
  "C MORE DETAILED DISCUSSION": "As for the relation and differences between batch-wise and featurewise CL, we will perform a more detailed discussion. Relation. Traditional CL is characterized by an anchor, a positive sample, and several negative samples, with the objective being to draw the anchor closer to the positive and further from the negatives, establishing a contrastive dynamic. In the case of DirectAU, the principles of Alignment and Uniformity (AU) mirror these roles: Alignment propels a user embedding (serving as the anchor) towards the item embedding (the positive sample) with which it has interacted, while Uniformity functions to spatially separate this user embedding from other user embeddings, akin to the role of negative samples in standard CL. This framework essentially positions the user embedding in a contrastive landscape relative to other embeddings. Despite the necessity of supervised information for positive user-item pairs, we posit that the foundational contrastive notion inherent in AU merits its classification under the umbrella of CL. This viewpoint is bolstered by a body of work suggesting a fundamental congruence between AU and CL. For example, in [38], the authors proved that optimizing for contrastive loss amplifies AU's efficacy, which will bring positive effects to downstream tasks. Furthermore, uCTRL [25] acknowledges the influence of DirectAU and its use of contrastive representation learning. Therefore, in light of these considerations and the evolving understanding of CL, particularly in the context of recommendation systems, we have included DirectAU and similar methodologies in the category of contrastive learning-based methods in our research. We believe this broader interpretation of CL fosters a more expansive and nuanced comprehension of these techniques and their mechanics. Additionally, as formulated in Eq. (4), the objective function of Barlow Twins essentially plays a role similar to the contrastive term in existing objective functions for self-supervised learning, such as the representative InfoNCE in SGL [40]. In this regard, we have analyzed it theoretically in A.1. Differences. Compared to batch-wise CL methods, there exist certain advantages in our method or other batch-wise CL methods due to important conceptual differences between batch-wise and feature-wise CL. Through a detailed analysis, the main differences between batch-wise and feature-wise CL focus on two aspects. ¬∑ FCL methods represented by Barlow Twins strongly benefit from pretty high-dimensional embeddings, which still is acceptable although a rather high computational cost. ¬∑ BCL , intriguingly, the batch-wise objective maximizes the variability of the embedding vectors via maximizing the pairwise distance between all interacted user-item pairs, while feature-wise CL methods do this by decorrelating each component of these embedding vectors. Thus, to provide better performance of recommendation, we develop RecDCL by combining the strengths of feature-wise and batch-wise objectives which can maximize the benefits of CL. ¬∑ FCL+BCL (CL4CTR) focus is on addressing the \"long tail\" distribution of feature frequencies by directly learning accurate feature representations through BCL and FCL. RecDCL focuses on combining the FCL objective and BCL objective whose effectiveness is to reduce redundant solutions but never miss an optimal solution, making it a more reasonable regularization technique.",
  "D MORE DETAILED EXPERIMENTS": "",
  "D.1 Algorithm and Complexity": "Training Algorithm. Let |U| , |I| and |E| denote the number of user nodes, item nodes, and edges in a user-item bipartite graph G , respectively. ùêµ and ùêπ stand for the batch size and the feature size. ùë† represents the number of epochs and ùêø is the number of GCN layers. The training process is shown in Algorithm 1. Time Complexity. We analyze how the feature-wise or batchwise objectives impact the complexity of CL-based recommendation methods with LightGCN as the based encoder. The time complexity of RecDCL training mainly includes the following parts: ¬∑ Encoding. The time complexity of the graph convolution module in the based encoder is ùëÇ ( 2 |E| ùë†ùêøùêπ | E | ùêµ ) . ¬∑ Evaluating CL loss. For feature-wise alignment, we use only user-item positive pair, thus the complexity is linear to | E | ùêµ , i.e., ùëÇ ( ùë†ùêπ | E | ùêµ | E | ùêµ ) . As defined in Eq. (6), we calculate feature-wise uniformity for users and items. The complexity of user side and item side are both ùëÇ ( ùë†ùêπ | E | ùêµ | E | ùêµ ) during whole training phase. Within a batch, the complexity of output augmentation is ùëÇ ( ùë†ùêπ ) . RecDCL: Dual Contrastive Learning for Recommendation WWW'24, May 13-17, 2024, Singapore, Singapore Therefore, the total complexity of RecDCL is linear to the feature size ùêπ , i.e., ùëÇ ( ( 2 ùë†ùêøùêµ + 3 ) ùêπ | E | | E | ùêµùêµ ) .",
  "D.2 Comparison with Recent Baselines": "To further validate the effectiveness of RecDCL, we select the representative and popular SGL and conduct its three variants on Beauty and Yelp datasets. Experimental results are shown as follows. From this table, we see that RecDCL still outperforms SGL which demonstrates the actual effectiveness of RecDCL. Table 9: Comparison with recent baselines.",
  "D.3 Experimental Settings": "Baselines. We compare various representative baselines including pop model, MF-based models (BPR-MF and NeuMF), VAE-based models (Mult-VAE and RecVAE), GNNs-based models (NGCF and LightGCN), and SSL-based models (BUIR, CLRec, and DirectAU). The detailed description is presented as follows: ¬∑ Pop recommends the most popular items to each user. ¬∑ BPR-MF [32] optimizes MF with Bayesian personalized ranking (BPR) loss by sampling negatives of interactions between users and items. ¬∑ NeuMF [17] utilizes a multi-layer perceptron to model users and items based on implicit feedbacks. ¬∑ Mult-VAE [26] is a typical item-based CF method based on variational autoencoders (VAEs) to maximum entropy discrimination. ¬∑ RecVAE [34] is also based on VAEs and improves the performance of Mult-VAE [26] by reconstructing user representations. ¬∑ NGCF [39] proposes a message-passing framework and performs graph convolution for collaborative filtering in first-order and high-order propagation way. ¬∑ LightGCN [16] is the state-of-the-art GNNs-based model, which removes the activation function and feature transformation of NGCF to simplify the design of graph convolution for recommendation. ¬∑ BUIR [24] is the first work that only uses user-item positive pair without any negative sampling in contrastive learning for collaborative filtering. Inspired by SimSiam [6], BUIR trains online encoder and target encoder for one-class collaborative filtering. ¬∑ CLRec [53] addresses exposure bias via InfoNCE loss for recommendation. ¬∑ DirectAU [36] focuses on the representation of users and items obtained from representation learning. It considers the alignment of user-item positive pairs to minimize the distance of positive pairs and calculates the uniform metric between users and items to maximize the distance between dissimilar users. Datasets. We adopt four widely public datasets with different scales and one real-world dataset, in which the specific information of them are described in Table 10 and Table 6, respectively. Table 10: Statistics of datasets. ¬∑ Beauty 7 is one of the types collected from Amazon and includes product review data. We use the interactions and setting following the previous work [36]. ¬∑ Food 8 is also one of the series of grocery and gourmet food of Amazon that has more than 120K users and 40K items in a largescale user-item interaction graph. ¬∑ Game 9 is an amazon-video-games review dataset that is released in 2018. ¬∑ Yelp 10 includes interaction between users and stores (e.g., restaurants, bars, and so on) in business domains. We use the Yelp2018 dataset and follow the default split setting of DirectAU [36]. Evaluation Metrics. We use widely adopted two evaluation metrics Recall@ K and Normalized Discounted Cumulative Gain (NDCG@ K ) to evaluate topK recommendation performance. Here, we conduct evaluated experiments for the value of ùêæ in the range of {10, 20, 50}, and report the results of ùêæ = 20 for simplicity. We split the interactions of each user into training/validation/test sets with a ratio of 0.8/0.1/0.1 following the previous work. In the evaluation process, we select all items except the training items for each user to calculate the evaluation metrics and report the average value of all test users' results as the final result. To validate the significant improvement of our method, we repeat our method and the suboptimal method 5 times with different random seeds and calculate the t-test value shown in Table 4 that are less than 0.05. 7 https://jmcauley.ucsd.edu/data/amazon/links.html 8 https://nijianmo.github.io/amazon/index.html 9 https://nijianmo.github.io/amazon/index.html 10 https://www.yelp.com/dataset WWW'24, May 13-17, 2024, Singapore, Singapore Zhang et al. We use two wide metrics to conduct topK experiments for RecDCL. The description of evaluated metrics can be described as follows: ¬∑ Recall demonstrates the ratio of recommendation items to test items. The calculation process is described as:  where | ùëÖ ( ùë¢ )| and | ùëá ( ùë¢ )| are the recommendation and test item set for user ùë¢ , respectively. ¬∑ NDCG indicates the importance of higher-ranked true positives. The NDCG@ K is calculated by Eq. (23)  where IDCG@K denotes the ideal cumulative gain. The DCG@K can be calculated as:  where ùëüùëíùëô ùëò,ùë¢ is 1 if ùëò -th item ùëò is positive for user ùë¢ else it is 0. Implementation Details. For fair comparison, we use the RecBole [52] framework to implement all experiments. Specifically, we initialize the parameters by Xavier initialization [11] and use the Adam optimizer [22] with a learning rate of 0.001 for all methods. The training batch size is set to 256 on Beauty and 1024 on Food, Game, and Yelp datasets. Considering the trade-off between memory cost and performance improvement, the embedding size is tuned among {32, 64, 128, 256, 512, 1024 and 2048}. In RecDCL, the default encoder is a 2-layer LightGCN that propagates the interactions between users and items. The weight ùõæ of L UIBT , coefficient ùõº of L UUII and the coefficient ùõΩ of L AUG are tuned in the range of {0.005, 0.01, 0.05, 0.1}, {0.2, 0.5, 1, 2, 5, 10} and {1, 5, 10, 20}, respectively. We tuned the momentum value ùúè of L ùê¥ùëàùê∫ within {0.1, 0.3, 0.5, 0.7, 0.9}. For all baselines, the setting of specific hyper-parameters refers to their respective original work.",
  "D.4 Implementation Note": "Running Environment. We implement RecDCL on PyTorch 1.9.1+cu111 and Python 3.9.7. All experiments on the Food dataset are conducted on Ubuntu with NVIDIA-A100 GPU, other experiments on the other three datasets are performed on a worker equipped with GeForce-RTX-3090. Projector in Figure 2. Especially, the components of Projector in Figure 2 for feature-wise objectives are shown in Figure 3. Figure 3: The components of Projector. MLP BatchNorm ReLU MLP BatchNorm Detailed Functions We list the detailed implementation process of each objective function in Algorithm 2. Algorithm 2: The training process of the RecDCL. Input: ùêµ : batch_size, ùëë : dimension size, ùëí : exponent, ùõæ : coefficient of L ùëàùêºùêµùëá , ùõº : coefficient of L ùëàùëàùêºùêº , ùõΩ : coefficient of L ùê¥ùëàùê∫ , ‚Ñé : multi-layer perceptron, ùë†ùëî : stop-gradient network, ùëÜ : cosine similarity, ùúè : value. Output: encoder parameters ùúÉ . Data: e ùë¢ : user embedding, e ùëñ : item embedding /* Calculate feature-wise alignment loss */  /* Calculate feature-wise uniformity loss */ Function UUII( e ùëñ ) :  /* Calculate batch-wise augmentation loss  for each mini-batch with positive pairs ( ùë¢, ùëñ ) do",
  "D.5 Study of RecDCL on Food and Game": "Similarly, to verify the effectiveness of each component, we show the statistical results of the ablation study on two datasets, Food and Game, as presented in Table 11. From this table, we can find that removing either or both of the three components in RecGCL leads to decreasing performance, while the five variants (except \"w/ UUII\") can do better than the baseline LightGCN in general. It demonstrates that the dual CL design of RecDCL will benefit the performance in graph collaborative filtering. Besides, feature-wise CL and batch-wise CL complement each other and improve the performance in different aspects.",
  "D.6 Hyper-parameter of Batch Size": "To analyze the impact of batch size on the FCL component, we use the Beauty dataset as an example and explore various batch size options within the range of {128, 256, 512, 1024, 2048, 4096}, which aligns with the default list used by Barlow Twins. Simultaneously, we conduct experiments using these batch size settings across different embedding sizes within the range of {256, 512, 1024, 2048}. The results, measured in terms of Recall@20 and NDCG@20 are detailed as follows: */ RecDCL: Dual Contrastive Learning for Recommendation WWW'24, May 13-17, 2024, Singapore, Singapore Table 11: Performance comparison of different designs of RecDCL on Food and Game. Table 12: Recall@20/NDCG@20 results of different ùêµ in RecDCL on Beauty dataset. Based on the data presented in the table, it is evident that FCL demonstrates its most effective recommendation performance when the batch size is set at 256 in most conditions. Furthermore, it showcases the robustness towards smaller batch sizes with its performance remaining relatively unaffected even with batch sizes as small as 256. This observation aligns with the concept of 'robustness to batch size' as described in the Barlow Twins paper.",
  "D.7 Hyper-parameter Sensitivity of RecDCL on Beauty and Yelp": "Effect of embedding size ùêπ . Generally, the embedding size is set as 64 by default in most CF methods [16, 36]. To validate the impact of feature-wise objectives, we run all methods with different embedding sizes from 32 to 2048 on four datasets and show the best results of each dimension on Beauty and Yelp in Figure 4. We have the following observations: (1) as embedding size increases, RecVAE and our RecDCL continue to grow, while the performance of other models in terms of Recall@20 first improves, and then holds the line even degrades, even though we provided these methods with exactly the same experimental settings. For larger F ( > 2048), the results of RecDCL still increase, while LightGCN degrades the performance and DirectAU causes the out-of-memory problem. In this sense, instead of an unsuitable comparison, the above phenomenon may also be understood as a property of RecDCL, i.e., it can benefit from increasing embedding size but the baseline methods failed. The above setting has been used in literature. For example, in [47], although a promising self-supervised method BarlowTwins only surpasses the baselines at a large embedding size, it has received extensive attention and inspired many follow-up works. (2) Though RecVAE has the same trend, RecDCL exceeds RecVAE on these two datasets and has an improvement of up to 23.67% on Beauty. This again demonstrates that RecDCL can capture the information of high-dimensional representation well. (3) Indeed, LightGCN enjoying the joint advantages of high-order neighborhood information and low-rank MF should be expected to achieve better performance than BPR-MF, which is consistent with our experimental observation when the embedding size F is not large (e.g., 32 and 64 on Beauty). Furthermore, these experimental results for small embedding sizes are also in agreement with that reported in the literature [36]. However, since this paper focuses more on the FCL objective whose effectiveness in empowering CF necessitates a larger embedding size, we regard the embedding dimension as a tunable hyperparameter. In this situation, LightGCN does not benefit much from the increase in embedding size despite our best efforts in parameter tuning. As a result, it lags behind BPR-MF at large embedding sizes on two (out of four) datasets (Beauty and Food). On the other hand, by searching throughout the hyperparameter space of embedding size, NGCF showcases slight advantages over LightGCN on the Food dataset. However, on the other three benchmarks, the performance of NGCF is only comparable to or inferior to that of LightGCN. 32 64 128 256 512 1024 2048 8.5 10.3 12.1 13.9 15.7 Recall@20 Beauty BPR-MF RecVAE LightGCN DirectAU RecGCL (ours) 32 64 128 256 512 1024 2048 6.1 7.4 8.7 10.0 11.3 Yelp F F Figure 4: Recall@20 results of different embedding sizes of representative baselines and RecDCL on Beauty and Yelp. Effect of coefficient ùõº of UUII. Similarly, we analyze the impact of coefficient ùõº in the range of {0.2, 0.5, 1, 2} of UUIInomial within users and items on the Beauty and Yelp dataset in Figure 5. (a) and in Figure 5. (b), respectively. The small value, i.e., 0.2, will promote the model performance on the Beauty dataset, but the trend is just the opposite on Yelp. The best value of ùõº is 0.2 on Beauty and 2 on Yelp, which indicates that different datasets have different optimal situations. Effect of coefficient ùõΩ of BCL. As verified in Section 4, batch-wise output augmentation plays a critical role in SSL-based recommendation. Consequently, it is necessary for us to show the model performance ùë§.ùëü.ùë°. ùõΩ in Figure 6. According to this figure, we can observe that: (1) after adding batch-wise augmentation, the whole performance is better than only \"with UIBT & UUII\". (2) Increasing WWW'24, May 13-17, 2024, Singapore, Singapore Zhang et al. 0.2 0.5 1 2 15.0 15.2 15.4 15.6 15.8 16.0 Recall@20 7.60 7.65 7.70 7.75 7.80 7.85 7.90 NDCG@20 Recall@20 NDCG@20 (a) Beauty Figure 5: Influence of different ùõº of UUII on Beauty and Yelp. 0.2 0.5 1 2 11.0 11.2 11.4 11.6 11.8 12.0 Recall@20 6.9 7.0 7.1 7.2 7.3 7.4 7.5 NDCG@20 Recall@20 NDCG@20 (b) Yelp the value of ùõΩ , e.g., 20, will keep consistent or lead to poor performance. (3) ùõΩ is consistent on different datasets, which is setting as 5 on Beauty and Yelp. Figure 6: Influence of different ùõΩ of BCL on Beauty and Yelp. 0 1 5 10 20 14.8 15.0 15.2 15.4 15.6 15.8 16.0 16.2 Recall@20 7.4 7.5 7.6 7.7 7.8 7.9 8.0 8.1 NDCG@20 Recall@20 NDCG@20 (a) Beauty 0 1 5 10 20 10.6 10.8 11.0 11.2 11.4 11.6 11.8 12.0 Recall@20 6.6 6.8 7.0 7.2 7.4 7.6 NDCG@20 Recall@20 NDCG@20 (b) Yelp Effect of coefficient ùõæ of UIBT. We explore the sensitivity of RecDCL to the coefficient ùõæ of UIBT, which trades off the desiderata of invariance term and informativeness of the representations. Figure 7 shows that the influence of UIBT's coefficient ùõæ in range of {0.005, 0.01, 0.05, 0.1} in terms of Recall@20 and NDCG@20 on Beauty and Yelp. We can find that our RecDCL is not very sensitive to this hyperparameter. Besides, UIBT's coefficient varies on different datasets. More specially, the optimal coefficient is 0.01 on Beauty dataset in Figure 7. (a) while the best ùõæ is 0.1 on the Yelp dataset in Figure 7. (b). Figure 7: Influence of different ùõæ of UIBT on Beauty and Yelp datasets. 0.005 0.01 0.05 0.1 14.0 14.5 15.0 15.5 16.0 16.5 Recall@20 6.5 6.9 7.3 7.7 8.1 8.5 NDCG@20 Recall@20 NDCG@20 (a) Beauty 0.005 0.01 0.05 0.1 10.8 11.0 11.2 11.4 11.6 11.8 Recall@20 6.6 6.8 7.0 7.2 7.4 7.6 NDCG@20 Recall@20 NDCG@20 (b) Yelp Effect of ùúè of BCL. We study the effect of ùúè in rang of {0.1, 0.3, 0.5, 0.7, 0.9} of batch-wise output augmentation and report the results in Figure 8. We find that: (1) ùúè is sensitive to Beauty. For example, the ùúè set as 0.7 performs better than other values. (2) In contrast, fixing ùúè to three values will rarely affect performance on the Yelp dataset. In a nutshell, we suggest tuning ùúè in the range of 0 . 1 ‚àº 1 . 0 carefully. Figure 8: Influence of different ùúè of BCL on Beauty and Yelp. 0.1 0.3 0.5 0.7 0.9 15.5 15.6 15.7 15.8 15.9 16.0 Recall@20 7.70 7.75 7.80 7.85 7.90 7.95 8.00 NDCG@20 Recall@20 NDCG@20 (a) Beauty 0.1 0.3 0.5 0.7 0.9 10.4 10.6 10.8 11.0 11.2 11.4 11.6 11.8 Recall@20 7.0 7.1 7.2 7.3 7.4 7.5 NDCG@20 Recall@20 NDCG@20 (b) Yelp",
  "D.8 Hyper-parameter Sensitivity of RecDCL on Food and Game": "Effect of embedding size ùêπ . Considering that feature-wise objectives are a key point of our method, embedding size ùêπ is a critical hyper-parameter that affects the performance. Thus, it is essential to show the changing trends of embedding size ùêπ from 32 to 2048 on the remaining two datasets Food and Game in Figure 9. As we expected, the performance of almost all methods increases with ùêπ , which also verifies that the larger ùêπ value can indeed provide richer representation information and feature-wise CL method can achieve ideal performance while embedding size is high-dimensional. Among them, the most significant improvement is our RecDCL. More specifically, when ùêπ is 32, our method lags behind other methods (especially on Food), but our method is the best when d is 2048, improving by 110.34% on Food and 36.67% on Game. 32 64 128 256 512 1024 2048 14.5 18.5 22.5 26.5 Recall@20 Food BPR-MF RecVAE LightGCN DirectAU RecGCL 32 64 128 256 512 1024 2048 13.0 15.5 18.0 20.5 Game F F Figure 9: The experimental results of different embedding sizes of baselines and our RecDCL on Food and Game. Effect of coefficient ùõæ of UIBT. As described in Figure 10, we test the effect of ùõæ in UIBT on the performance of RecDCL on Food and Game in terms of Recall@20 and NDCG@20, in which ùõæ is in the range of {0.005, 0.01, 0.05, 0.1} like the other two datasets Beauty and Yelp. Although Recall@20 and NDCG@20 on both datasets Food and Game basically show a trend of rising first and then falling, the magnitude of the change is not particularly obvious, which indicates that it is not very sensitive to the parameter ùõæ . Moreover, we can find optimal ùõæ for different datasets. Specifically, the optimal ùõæ is 0.05 on Food and 0.01 on Game, respectively. Effect of coefficient ùõº of UUII. To more comprehensively investigate the influence of ùõº on the effectiveness of RecDCL, we RecDCL: Dual Contrastive Learning for Recommendation WWW'24, May 13-17, 2024, Singapore, Singapore 0.005 0.01 0.05 0.1 28.80 28.85 28.90 28.95 29.00 29.05 29.10 Recall@20 22.9 23.0 23.1 23.2 23.3 23.4 23.5 NDCG@20 Recall@20 NDCG@20 (a) Food 0.005 0.01 0.05 0.1 19.9 20.0 20.1 20.2 20.3 20.4 20.5 20.6 9.5 9.6 9.7 9.8 9.9 10.0 NDCG@20 Recall@20 NDCG@20 (b) Game Recall@20 Figure 10: Influence of different ùõæ of UIBT on Food and Game datasets. further vary ùõº in {0.2, 0.5, 1, 2} on Food and Game two datasets, and statistical results are displayed in Figure 11. (a) and in Figure 11. (b), respectively. We can see that the performance of the two datasets shows opposite trends. Specifically, increasing ùõº generally improves the performance on Food while leading to poor performance on Game. Therefore, the optimal performance on Food is obtained when ùõº is 1, while the best value of ùõº on Game is 0.2. Figure 11: Influence of different ùõº of UUII on Food and Game. 0.2 0.5 1 2 28.7 28.8 28.9 29.0 29.1 Recall@20 23.0 23.1 23.2 23.3 23.4 NDCG@20 Recall@20 NDCG@20 (a) Food 0.2 0.5 1 2 20.1 20.2 20.3 20.4 20.5 20.6 Recall@20 9.70 9.75 9.80 9.85 9.90 9.95 10.00 NDCG@20 Recall@20 NDCG@20 (b) Game Effect of ùúè of BCL. Similarly, we also consider varying ùúè in the range of 0.1 to 0.9 in steps of 0.2 and then illustrate the changing curves of Recall@20 and NDCG@20 on Food and Game in Figure 12. From Figure 12.(a) and Figure 12.(b), we can observe that: (1) Compared with hyperparameters ùõæ and ùõº , ùúè of BCL is relatively sensitive on two datasets. Thus, we believe that it is necessary for us to carefully tune ùúè in the range of 0 . 1 ‚àº 0 . 9 carefully. (2) The changing trends in the two datasets are the same. Especially, setting ùúè to 0.1, 0.5, 0.7, or 0.9 will rarely affect the performance of RecDCL. Besides, the ùúè is 0.3 and performs better than other values. NDCG@20 Figure 12: Influence of different ùúè of BCL on Food and Game. 0.1 0.3 0.5 0.7 0.9 28.80 28.85 28.90 28.95 29.00 29.05 29.10 Recall@20 23.10 23.15 23.20 23.25 23.30 23.35 23.40 Recall@20 NDCG@20 (a) Food 0.1 0.3 0.5 0.7 0.9 20.30 20.35 20.40 20.45 20.50 20.55 20.60 Recall@20 9.70 9.75 9.80 9.85 9.90 NDCG@20 Recall@20 NDCG@20 (b) Game Effect of coefficient ùõΩ of BCL. As analyzed in the previous body text, batch-wise output augmentation is essential for SSLbased recommendation. Thus, it is also necessary to test the effect of coefficient ùõΩ on the performance of RecDCL. Illustratively, we present the results ùë§.ùëü.ùë°. different ùõΩ in the range of 0 to 20 in variable steps in Figure 13. According to Figure 13a and Figure 13b, we can observe that: (1) Compared with only \"with UIBT & UUII\" (i.e., ùõΩ = 0), batch-wise augmentation (i.e., ùõΩ > ùëô 0) improve the whole performance by 8.64%, 9.43% on Food and 12.64%, 17.02% on Game in terms of Recall@20 and NDCG@20, respectively. (2) Unlike Beauty and Yelp two datasets, larger values first lead to better performance on Food and Game and then decrease in general. (3) The impact of ùõΩ on RecDCL is consistent on different datasets, which are both set as 10 on Food and Game datasets. Figure 13: Influence of different ùõΩ of BCL on Food and Game. 0 1 5 10 20 26.3 26.9 27.5 28.1 28.7 29.3 29.9 Recall@20 20.4 21.0 21.6 22.2 22.8 23.4 24.0 NDCG@20 Recall@20 NDCG@20 (a) Food 0 1 5 10 20 18 19 20 21 22 23 Recall@20 8.6 9.0 9.4 9.8 10.2 NDCG@20 Recall@20 NDCG@20 (b) Game",
  "D.9 Trade-off hyperparameters ùõº and ùõΩ": "As shown in Eq. (10), ùõº and ùõΩ act as factors that adjust the influence of two different loss functions, L UUII and L BCL , which correspond to the Feature-wise and Batch-wise objectives, respectively. In Section 3.2, we explore how these goals help create a distribution characterized by both orthogonality (or more specifically, dimension-wise independence) and uniformity. Therefore, in theory, a larger ùõº encourages orthogonality, whereas a higher ùõΩ enhances uniformity. In practice, although both orthogonality and uniformity contribute positively to model performance (as shown in Table 5), their joint effects on the overall quality of recommendations can be intricate. For instance, Figures 5 and 6 demonstrate how ùõº and ùõΩ differently affect performance in the Beauty and Yelp datasets. ùõΩ tends to show a more stable pattern, while ùõº varies more in its correlation with performance. As a result, we used a grid search technique to tune these parameters, aiming to maximize recommendation performance (Recall@20) on the validation set. We list the respective hyper-parameter on each dataset with the best performance in Table 13. Table 13: Best hyperparameter setting.",
  "keywords_parsed": [
    "Recommender Systems",
    "Self-supervised Learning",
    "Batch-wise Contrastive Learning",
    "Feature-wise Contrastive Learning"
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "Vicreg: Varianceinvariance-covariance regularization for self-supervised learning"
    },
    {
      "ref_id": "b2",
      "title": "LightGCL: Simple Yet Effective Graph Contrastive Learning for Recommendation"
    },
    {
      "ref_id": "b3",
      "title": "Stochastic training of graph convolutional networks with variance reduction"
    },
    {
      "ref_id": "b4",
      "title": "Revisiting graph based collaborative filtering: A linear residual graph convolutional network approach"
    },
    {
      "ref_id": "b5",
      "title": "A simple framework for contrastive learning of visual representations"
    },
    {
      "ref_id": "b6",
      "title": "Exploring simple siamese representation learning"
    },
    {
      "ref_id": "b7",
      "title": "Debiased contrastive learning"
    },
    {
      "ref_id": "b8",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding"
    },
    {
      "ref_id": "b9",
      "title": "Gnnautoscale: Scalable and expressive graph neural networks via historical embeddings"
    },
    {
      "ref_id": "b10",
      "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings"
    },
    {
      "ref_id": "b11",
      "title": "Understanding the difficulty of training deep feedforward neural networks"
    },
    {
      "ref_id": "b12",
      "title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models"
    },
    {
      "ref_id": "b13",
      "title": "Momentum contrast for unsupervised visual representation learning"
    },
    {
      "ref_id": "b14",
      "title": "Momentum contrast for unsupervised visual representation learning"
    },
    {
      "ref_id": "b15",
      "title": "Candidate-aware Graph Contrastive Learning for Recommendation"
    },
    {
      "ref_id": "b16",
      "title": "Lightgcn: Simplifying and powering graph convolution network for recommendation"
    },
    {
      "ref_id": "b17",
      "title": "Neural collaborative filtering"
    },
    {
      "ref_id": "b18",
      "title": "Matrix Analysis"
    },
    {
      "ref_id": "b19",
      "title": "Towards the generalization of contrastive self-supervised learning"
    },
    {
      "ref_id": "b20",
      "title": "A survey on contrastive self-supervised learning"
    },
    {
      "ref_id": "b21",
      "title": "TLDR: Twin learning for dimensionality reduction"
    },
    {
      "ref_id": "b22",
      "title": "Adam: A method for stochastic optimization"
    },
    {
      "ref_id": "b23",
      "title": "Self-Supervised Learning with Data Augmentations Provably Isolates Content from Style"
    },
    {
      "ref_id": "b47",
      "title": "Barlow twins: Self-supervised learning via redundancy reduction"
    },
    {
      "ref_id": "b48",
      "title": "How Does SimSiam Avoid Collapse Without Negative Samples? A Unified Understanding with Self-supervised Contrastive Learning"
    },
    {
      "ref_id": "b49",
      "title": "DropConn: Dropout Connection Based Random GNNs for Molecular Property Prediction"
    },
    {
      "ref_id": "b50",
      "title": "ApeGNN: Node-Wise Adaptive Aggregation in GNNs for Recommendation"
    },
    {
      "ref_id": "b51",
      "title": "Zero-CL: Instance and Feature decorrelation for negative-free symmetric contrastive learning"
    },
    {
      "ref_id": "b52",
      "title": "Recbole: Towards a unified, comprehensive and efficient framework for recommendation algorithms"
    },
    {
      "ref_id": "b53",
      "title": "Contrastive learning for debiased candidate generation in large-scale recommender systems"
    },
    {
      "ref_id": "b54",
      "title": "SelfCF: A Simple Framework for Self-supervised Collaborative Filtering"
    },
    {
      "ref_id": "b55",
      "title": "Recommending Learning Objects Through Attentive Heterogeneous Graph Convolution and Operation-Aware Neural Network"
    }
  ]
}