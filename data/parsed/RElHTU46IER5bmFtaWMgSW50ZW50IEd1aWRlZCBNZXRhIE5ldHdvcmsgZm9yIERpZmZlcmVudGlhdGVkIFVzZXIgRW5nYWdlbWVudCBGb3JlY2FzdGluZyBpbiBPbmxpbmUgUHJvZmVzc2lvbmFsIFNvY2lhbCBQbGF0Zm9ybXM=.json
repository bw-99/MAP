{"DIGMN: Dynamic Intent Guided Meta Network for Differentiated User Engagement Forecasting in Online Professional Social Platforms": "Feifan Li \u2217 feifan@mail.dlut.edu.cn Dalian University of Technology Qiang Fu qifu@microsoft.com Microsoft Research Shi Han shihan@microsoft.com Microsoft Research Lun Du \u2020 lun.du@microsoft.com Microsoft Research Yushu Du yusdu@linkedin.com LinkedIn Corporation", "Zi Li": "zili@linkedin.com LinkedIn Corporation", "ABSTRACT": "User engagement prediction plays a critical role in designing interaction strategies to grow user engagement and increase revenue in online social platforms. Through the in-depth analysis of the real-world data from the world's largest professional social platforms, i.e., LinkedIn, we find that users expose diverse engagement patterns, and a major reason for the differences in user engagement patterns is that users have different intents. That is, people have different intents when using LinkedIn, e.g., applying for jobs, building connections, or checking notifications, which shows quite different engagement patterns. Meanwhile, user intents and the corresponding engagement patterns may change over time. Although such pattern differences and dynamics are essential for user engagement prediction, differentiating user engagement patterns based on user dynamic intents for better user engagement forecasting has not received enough attention in previous works. In this paper, we proposed a D ynamic I ntent G uided M eta N etwork (DIGMN), which can explicitly model user intent varying with time and perform differentiated user engagement forecasting. Specifically, we derive some interpretable basic user intents as prior knowledge from data mining and introduce prior intents to explicitly model dynamic user intent. Furthermore, based on the dynamic user intent representations, we propose a meta-predictor to perform differentiated user engagement forecasting. Through a comprehensive evaluation of LinkedIn anonymous user data, our method outperforms stateof-the-art baselines significantly, i.e., 2.96% and 3.48% absolute error reduction, on coarse-grained and fine-grained user engagement Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WSDM '23, February 27-March 3, 2023, Singapore, Singapore \u00a9 2023 Association for Computing Machinery. ACM ISBN 978-1-4503-9407-9/23/02...$15.00 https://doi.org/10.1145/3539597.3570420 prediction tasks, respectively, demonstrating the effectiveness of our method.", "CCS CONCEPTS": "\u00b7 Information systems \u2192 Enterprise applications ; \u00b7 Computing methodologies \u2192 Neural networks ;", "KEYWORDS": "User Intent, User Engagement Forecasting, Meta Learning", "ACMReference Format:": "Feifan Li, Lun Du, Qiang Fu, Shi Han, Yushu Du, Guangming Lu, and Zi Li. 2023. DIGMN: Dynamic Intent Guided Meta Network for Differentiated User Engagement Forecasting in Online Professional Social Platforms. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining (WSDM '23), February 27-March 3, 2023, Singapore, Singapore. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3539597.3570420", "1 INTRODUCTION": "Online professional social platforms like LinkedIn have become a significant part of today's lives. People use these platforms to socialize, apply for jobs, read industry news, etc. Maintaining a highlevel user engagement is vital for these platforms, which can lead to more revenue (e.g., more ad exposure). For the purpose of increasing user engagement in the future, these platforms need to formulate appropriate user interaction strategies, such as delivering content that satisfies user intents or interests. Accurate user engagement prediction is one of the core technologies for developing these strategies, which can help the platform conduct user modeling, understand user needs, and provide personalized services. Real-world users often exhibit different behaviors in online social platforms, leading to diverse engagement patterns. Through data mining and analysis in the scenario of LinkedIn, we found that the diversity of user engagement patterns is related to the multiple intents of users using LinkedIn, as shown in Figure 1. For instance, some users intend to look for jobs recently, and they will frequently visit LinkedIn to seek and apply for jobs, increasing their engagement level rapidly in a period. As another example, some users use LinkedIn to view industry news. Their engagement pattern Guangming Lu glu@linkedin.com LinkedIn Corporation Check Notifications Find New Jobs View Industry News Time # sessions \u22ef\u22ef Day 1 Day 2 Day 3 \u22ef Day 4 Day 1 Day 2 Day 3 \u22ef Day 4 Day 1 Day 2 Day 3 \u22ef Day 4 is usually maintained at a relatively high level because they regularly check the industry news on LinkedIn. These insights suggest that user intents can be signals to differentiate user engagement patterns. However, user intents are usually not directly observable because they exist implicitly in human consciousness. How to extract implicit user intents is a challenging problem. At the same time, the user's intent may change over time. For example, some users initially use LinkedIn to look for jobs, and when they finish looking for jobs, they may use LinkedIn to socialize (e.g., make new connections at a new company). Explicitly modeling dynamic user intent is vital. It can help the platform understand users' recent intents (or interests) and provide users with content that matches their intents to increase user engagement. Recently, there have been many works on user engagement forecasting in social network platforms. [1] firstly groups new users into some clusters and then uses an LSTM-based model to predict user churn rate. [2] constructs a user action graph to characterize and forecast new user engagement. [3] considers user interaction actions and builds a user graph that evolves to predict user engagement. Although engagement patterns vary between users, these works use a model with static parameters for all users to predict their future engagement, which cannot sufficiently model diverse user engagement patterns and perform differentiated user engagement forecasting. Meanwhile, some works show that user intent can impact user engagement. [4] shows that user intent can influence user engagement (e.g., usage time and return time) at Pinterest. [5] demonstrates that user primary intents are associated with how likely the user is to re-engage in activity-tracking applications. On the one hand, these works have limitations in extracting user intent. In [4], the user's intent is obtained through a survey when the app is just opened, which may affect the user's subsequent behaviors [6]. [5] adopts the activity that the user most commonly uses as a proxy for the user's intent. However, the user's intent may be diverse when using applications [7]. On the other hand, these works do not explicitly model changes in user intent over time. To address the above challenges, we first use Latent Dirichlet Allocation (LDA) [8] to perform user intent mining on large-scale session data and identify basic user intents. Then, we propose a Dynamic Intent Guided Meta Network (DIGMN), which can capture the user's dynamic intent and perform differentiated user engagement prediction. Specifically, DIGMN infers multiple user intents during each session based on similarity computation with the basic user intents and captures the variation of user intents over time by the sequence model. Besides, DIGMN contains a prediction network based on meta-learning, which adopts a dynamic intent guided attention mechanism to adjust network parameters by performing a linear combination of basic parameters shared by all users for differentiated user engagement forecasting. Extensive experiments conducted on coarse-grained and fine-grained user engagement forecasting tasks verify the effectiveness of our DIGMN method. The major contributions of this paper can be summarized as follows: \u00b7 We find that user intent can be beneficial for differentiated user engagement forecasting in online professional social platforms. \u00b7 We develop a D ynamic I ntent G uided M eta N etwork (DIGMN) which explicitly model user intent's evolution over time and leverage intent guided attention mechanism to adjust model parameters for differentiated user engagement modeling and forecasting. \u00b7 Through evaluation experiments on anonymous data from LinkedIn, our proposed model DIGMN has improvements of 2.96% (Macro F1-score) and 3.48% (AUROC) on coarse-grained and fine-grained user engagement prediction tasks when compared to the state-of-the-art model, showing the effectiveness of our proposed method.", "2 RELATED WORK": "In this section, we present related work on user engagement forecasting, user intent modeling, and meta-learning for dynamic network parameters. Userengagementforecasting. Recently, there have been many works on user engagement predicting from different perspectives on the social platform. Such as user behaviors and social attributes [1], user action graphs [2], interaction actions between users [3], periodicity of user behaviors [9], and causal effects of social influence [10]. These works learn a model with static parameters for all users to make predictions, which cannot sufficiently model differentiated user engagement patterns. [11] leverages a decision tree model to divide users into disjoint groups and then learns a separate Logistic regression model for each group of users to predict user churn. However, such separate modeling compromises the model's ability to capture similarities between users. [12] adopts the matrix factorization to predict the personalized user's participation in mobile video. However, our scenario has a large amount of user behavior sequence data. This method can not effectively deal with the user's behavior sequence and its change over time. User intent modeling. Some previous works exploit LDA [2, 13, 14], n-gram [2, 15] and deep learning model [16, 17] to mine user intent from user behavior. At the same time, modeling user intents can help us understand user needs better and is significant in many scenarios: for instance, web searching [18, 19], e-commerce application [4], image sharing social platform [20], activity tracking application [5] and recommender systems [21-25]. However, to our best knowledge, no related work has explicitly modeled user intent and its variation for differentiated user engagement forecasting in online social platforms. Meta-learning for dynamic network parameters. Meta-learning (also known as learning to learn) can be used to learn dynamic model parameters, which is widely used in scenarios and tasks with diverse data distributions. There are usually two ways to learn dynamic model parameters [26]: dynamically generate model parameters or dynamically adjust model parameters. For dynamically generating model parameters: [27] utilizes a meta-knowledge learner to generate model parameters for modeling diverse spatial-temporal correlations in urban traffic prediction. [28] adopts a meta-network to learn a personalized mapping function for each user in the crossdomain recommendation. [29], [30] and [31] adopt meta networks to generate dynamic parameters of scenario-specific models for CTR prediction. For dynamically adjusting model parameters: [32] incorporates meta-information learned from a supplementary neural network into a fixed base-level neural network to realize the generalization for each type of input image. [33], [34], and [35], in the light of different input images, use a separate network to learn different weights for adjusting the parameters of convolution kernels, which can obtain dynamic convolution kernels for feature extraction. However, no existing works utilize meta-learning for predicting user engagement with diverse patterns in online social platforms.", "3 PRELIMINARY": "In this section, we introduce the relevant definitions and formulate the user engagement forecasting tasks used in this work.", "3.1 Basic Definitions": "We define \ud835\udc48 = { \ud835\udc62 1 , \ud835\udc62 2 , ..., \ud835\udc62 \ud835\udc41 } as a set of different users and \ud835\udc3c = { \ud835\udc56 1 , \ud835\udc56 2 , ..., \ud835\udc56 \ud835\udc40 } as a set of different event types, where \ud835\udc41, \ud835\udc40 are the number of users and event types respectively. User event. The user's behaviors on the platform can be abstracted as events for collection. A user event \ud835\udc52 can be denoted by \ud835\udc52 = ( \ud835\udc62, \ud835\udc56, \ud835\udc61 ) , where \ud835\udc62 \u2208 \ud835\udc48 is the user, \ud835\udc56 \u2208 \ud835\udc3c is the event type, and \ud835\udc61 is the time when the event happens. We collect ten major event types on the platform, as shown in Table 1. These ten events can completely cover users' activities on the platform. User session. A user session can be viewed as a set of continuous browsing activities with gaps between full-page views not more than a threshold. Formally, a session can be denoted as S = ( \ud835\udc62, c , E) , where \ud835\udc62 \u2208 \ud835\udc48 is the user who starts the session, c is the session context information (e.g., the session start time, the session duration time and the software client) and E is the sequence of events that take place during the session. User engagement. User engagement measures how often and how long users interact with the website or application, reflecting how much value users get from the website or application. Different platforms use different metrics to track user engagement. In this paper, we adopt two metrics to measure the engagement of each user on LinkedIn: average active days (coarse-grained, which is the same as the definition of active rate in [2]) and average session numbers (fine-grained). The average session numbers \u00af \ud835\udc60 = # \ud835\udc63\ud835\udc4e\ud835\udc59\ud835\udc56\ud835\udc51 \ud835\udc60\ud835\udc52\ud835\udc60\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60 # \ud835\udc61\ud835\udc5c\ud835\udc61\ud835\udc4e\ud835\udc59 \ud835\udc51\ud835\udc4e\ud835\udc66\ud835\udc60 , where a valid session is a session that contains at least one event in Table 1. The average active days \u00af \ud835\udc51 = # \ud835\udc4e\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc63\ud835\udc52 \ud835\udc51\ud835\udc4e\ud835\udc66\ud835\udc60 # \ud835\udc61\ud835\udc5c\ud835\udc61\ud835\udc4e\ud835\udc59 \ud835\udc51\ud835\udc4e\ud835\udc66\ud835\udc60 , where an active day is defined as a day that the user has at least one valid session. Platform actions. Previous studies have demonstrated that user engagement on the platform can be increased through notifications [36, 37], incentives including badges [38, 39], etc. Therefore, we need to consider the impact of the interaction actions between the platform and the user when forecasting user engagement. For simplicity, we only consider the platform delivery messages. Formally, a platform delivery message can be denoted as D = ( \ud835\udc62, \ud835\udc64, \ud835\udc5f, \ud835\udc61 ) , where \ud835\udc62 \u2208 \ud835\udc48 is the user being delivered, \ud835\udc64 is the way the message is delivered (i.e., email, SMS and in-app push), \ud835\udc5f is the content of the delivery message (i.e., Feed relevant, Jobs relevant, PYMK relevant, Message relevant and others), \ud835\udc61 is the delivery time.", "3.2 Task Formulation": "In this work, we predict the trend of user engagement instead of directly predicting the future engagement of users because, in actual business scenarios, we pay more attention to the change in user engagement. For example, when designing a platform message delivery strategy in the scenario of user retention, the change value ( \u0394 \ud835\udc60 or \u0394 \ud835\udc51 ) of the user engagement caused by the delivered message is an essential indicator of business decisions. Specifically, we define two user engagement trend forecasting tasks as follows: \u00b7 Day-level Task (coarse-grained) : We predict the trend of the average active days of the user, which can be viewed as a 3-class (i.e., increase, decrease, or stay the same) classification task. \u00b7 Session-level Task (fine-grained) : We predict the trend of the average session numbers of the user, which can be viewed as a 2class (i.e., decrease or not decrease) classification task. The reason for setting it as a binary classification task is that the number of user sessions has a broader distribution than the number of active days, and users with a large number of sessions are less likely to have the same number of sessions over a while. Based on the above definitions, for any user \ud835\udc62 \u2208 \ud835\udc48 , given the user's macroscopic features \ud835\udc74 \ud835\udc62 (e.g., the number of connections, the average number of sessions per day in the past period), session sequence \ud835\udc7a \ud835\udc62 = < S 1 , S 2 , ..., S \ud835\udc47 > , and the latest platform delivery message \ud835\udc6b \ud835\udc62 in the past period, forecasting user engagement trend \ud835\udc66 \ud835\udc62 in the next period can be formulated as follows: where \ud835\udc47 is the number of sessions, \ud835\udc66 \ud835\udc62 is the user engagement trend label, and \u0398 is the parameters we need to learn.", "4 INTENT MINING": "In this section, we introduce how to perform user intent mining and show the basic user intents we obtain from data mining. We assume that every user has at least one intent within each session [40]. The user's intents are usually not directly observed and exist implicitly in the user's consciousness. It is difficult for us to collect a large number of user behavior samples with user intent labels, so using unsupervised methods for user intent mining is a more feasible option. Intuitively, the user's intents influence the user's behaviors, and the user's behaviors constitute the set of events in the session. This process is similar to generating documents with an unsupervised topic model LDA. Like previous work [2], we treat each session as a document and each event type as a word, then apply LDA to mine basic user intents. We use Spark MLlib 1 to perform LDA on approximately 6 million anonymous user session data for intent mining. To determine the optimal number of intents with semantic meaning, we adopt perplexity as the evaluation metric to search for the optimal intent number in { 2 , 3 , 4 , ..., 10 } . When the number of intents equals 7, the LDA model has the least perplexity. These 7 topics can be regarded as 7 basic user intents (denoted as \ud835\udc95 1 , \ud835\udc95 2 , ..., \ud835\udc95 7 \u2208 R 10 ), each of which is composed of events with different weights (also can be view as probability) as shown in Figure 2. The meaning of each intent can be explained by the topweight events that make up them. For example, we can find that the two events with top weight making up intent 1 are PYMK and Profile View, indicating that the users want to expand their connections on LinkedIn. Notification Search Profile View PYMK Jobs Message Feed Profile Edit Content Share Follow Intent 1 Intent 2 Intent 3 Intent 4 Intent 5 Intent 6 Intent 7", "5 DYNAMIC INTENT GUIDED META NETWORK": "In this section, we introduce our proposed model, DIGMN. The framework of our proposed model is illustrated in Figure 3. It consists of three main components: a behavior evolution layer, an intent evolution layer, and a meta-predictor.", "5.1 Behavior Evolution Layer": "Users' behaviors change over time, and the length of the behavior sequence of different users may be different. LSTM[41] has been widely used for modeling variable-length user behavioral sequences. In this paper, we utilize a 1-layer LSTM to model user behavior evolution. For each session S , we represent it by concatenating the session context features \ud835\udc84 and session event frequency \ud835\udf42 = ( \ud835\udf08 1 , \ud835\udf08 2 , ..., \ud835\udf08 10 ) \u2208 R 10 , where \ud835\udf08 \ud835\udc56 is the frequency of event \ud835\udc56 (e.g., if there are 100 events in a session, and the Feed event occurs 10 times, then \ud835\udf08 1 = 10 / 100 = 0 . 1). We take \ud835\udc94 = ( \ud835\udf42 , \ud835\udc84 ) as the input of the session LSTM unit at each timestep. Since the user's behavior sequences are of different lengths, we take the last timestep output of LSTM as the representation of the user's behavior features in the past period.", "5.2 Intent Evolution Layer": "A user may have multiple intents during one session. For example, the users can use LinkedIn to view updates on industry news (i.e., intent 3) and share content related to themself (i.e., intent 5) at the same time. We can infer the user's possible intents based on the events happening during this session. In this paper, we exploit a simple and effective way to infer user intent in each session. Given a session's event frequency \ud835\udf42 and the basic user intents \ud835\udc95 1 , \ud835\udc95 2 , ..., \ud835\udc95 7 obtained by LDA in the intent mining stage, we can infer the user intents \ud835\udc8a = ( \ud835\udc56 1 , \ud835\udc56 2 , ..., \ud835\udc56 7 ) during this session. To be specific, \ud835\udc56 \ud835\udc58 is the cosine similarity between the session event frequency \ud835\udf42 and the \ud835\udc58 -th basic user intent \ud835\udc95 \ud835\udc58 , which measures the degree to which the current session contains the \ud835\udc58 -th intent and can be calculated as follows: By performing the above intent extraction operation on each user session, we can obtain an intent sequence for each user with the same length as the user session sequence. Another 1-layer LSTM is adopted to model the variation of user intent over time. We take each session's intent representation \ud835\udc8a as the input of the intent LSTM unit at each timestep. We also take the LSTM's last timestep output to represent the user's dynamic intent over the past period.", "5.3 Meta-Predictor": "Different users have different engagement patterns, and the user engagement pattern may change in different periods. Previous works [1-3] use a model with the static parameters \u0398 for different users to predict their future engagement in the social platforms, which can be formulated as \ud835\udc9a = \ud835\udc39 ( \ud835\udc99 , \u0398 ) , where \ud835\udc99 is the model input (i.e., user features) and \ud835\udc9a is the model output. To make a good prediction, the model tends to capture the common patterns of all users, which is not optimal for modeling and predicting diverse user engagement patterns in real scenarios. The most straightforward idea is to learn a unique predictor for each user or a group of similar users. However, there is a certain similarity between users, while separate modeling may impair the model's ability to capture the similarity between users. Therefore, we need a predictor that can better model the diversity in user engagement patterns while capturing such similarity. Dynamic parameters neural networks have shown promising results in various real-world tasks and can dynamically adjust or generate model parameters \u0398 \u2217 according to different inputs \ud835\udc99 , which have a more powerful representation ability. It can be formulated as \ud835\udc9a = \ud835\udc39 ( \ud835\udc99 , \u0398 \u2217 ) = \ud835\udc39 ( \ud835\udc99 , \ud835\udf19 ( \ud835\udc99 , \u0398 )) , where \ud835\udf19 (\u00b7 , \u0398 ) is the operation that adjusts or generates model parameters according to input \ud835\udc99 . Acommon technique in dynamic parameters neural networks is attention on parameters [33, 34]. It assumes that there are some basic learnable parameters with the same shape in the dynamic parameter layer. Given different inputs, the meta network can generate different attention weights to combine these basic learnable parameters v 1 c 1 v 2 c 2 v T c T session LSTM session LSTM session LSTM \u22ef Compute Similarity with Priori LDA Intents intent LSTM intent LSTM \u22ef Session Event Frequency ( c ) Session Context Features ( ` ) Session Intents ( 9 ) User Macroscopic Features ( a ) b & b ' b 3 9 & 9 ' 9 3 Behavior Evolution Layer Intent Evolution Layer Output ReLU Meta-Predictor \u22ee \u22ee \u00d7& intent LSTM Softmax l -th fully connected layer with dynamic parameters Linear Mapping Data flow Parameter adjustment M Meta Network d @ % @ % # connections average session time \u22ee Linear Mapping delivery method delivery time delivery content Platform Delivery Message ( e ) D in the dynamic parameter layer to obtain dynamic parameters and make transformations. In our scenario, the meta network can adjust the model parameters according to different user feature inputs for differentiated modeling. At the same time, all users share the basic learnable parameters of the dynamic parameter layer, which enables the model to capture the similarities between users. According to the previous analysis, we can use the user dynamic intent representation obtained from the intent evolution layer as the input signal to parameterized adjustment operation \ud835\udf19 (\u00b7 , \u0398 ) in the meta-network to perform differentiated user engagement forecasting. Specifically, we design a meta-predictor that contains a meta network and multiple stacked fully connected layers with dynamic parameters (FC-D layer), as illustrated in Figure 3. l -th fully connected layer with dynamic parameters F X = 3 \u2217 1 @ % Meta Network d @ % 3 1 h % Dynamic User Intent \u00d7 3 = h %4& 1 5.3.1 Fully Connected Layers with Dynamic Parameters . As illustrated in Figure 4, assuming that the \ud835\udc59 -th FC-D layer transforms the input feature \ud835\udc89 \ud835\udc59 \u2208 R \ud835\udc5a into \ud835\udc89 \ud835\udc59 + 1 \u2208 R \ud835\udc5b , and correspondingly assuming there are \ud835\udc51 basic learnable parameters \ud835\udc7e \ud835\udc59 1 , \ud835\udc7e \ud835\udc59 2 , ..., \ud835\udc7e \ud835\udc59 \ud835\udc51 \u2208 R ( \ud835\udc5a \u2217 \ud835\udc5b ) in it, denoted as \ud835\udc7e \ud835\udc59 = [ \ud835\udc7e \ud835\udc59 1 , \ud835\udc7e \ud835\udc59 2 , ..., \ud835\udc7e \ud835\udc59 \ud835\udc51 ] \ud835\udc47 \u2208 R \ud835\udc51 \u00d7( \ud835\udc5a \u2217 \ud835\udc5b ) . The \ud835\udc59 -th FC-D layer computes the following transformation: where b \ud835\udc7e \ud835\udc59 \u2208 R \ud835\udc5a \u00d7 \ud835\udc5b , \u02c6 \ud835\udc83 \ud835\udc59 \u2208 R \ud835\udc5b are the weights and bias of the \ud835\udc59 -th FC-D layer adjusted by the meta network. 5.3.2 Meta Network . The adjustment process of b \ud835\udc7e \ud835\udc59 in the meta network is shown in Figure 5. The combination weights \ud835\udc82 = [ \ud835\udc4e 1 , ..., \ud835\udc4e \ud835\udc51 ] \ud835\udc47 \u2208 R \ud835\udc51 (which can be seen as meta-knowledge) of basic learnable parameters \ud835\udc7e \ud835\udc59 is derived by applying a non-linear transformation and a Softmax operation to user dynamic intent (denoted as e \ud835\udc8a ). In this paper, we utilize two fully connected layers with the ReLU activation function to perform non-transformation, which can be formulated as follows: where \ud835\udc7e 1 , \ud835\udc7e 2 , \ud835\udc83 1 , \ud835\udc83 2 are learnable parameters. Then we can combine the basic learnable parameters \ud835\udc7e \ud835\udc59 = [ \ud835\udc7e \ud835\udc59 1 , \ud835\udc7e \ud835\udc59 2 , ..., \ud835\udc7e \ud835\udc59 \ud835\udc51 ] \ud835\udc47 through \ud835\udc82 , and obtain the adjusted transformation matrix b \ud835\udc7e \ud835\udc59 of \ud835\udc59 -th FC-D layer through the reshape operation: \u02c6 \ud835\udc83 \ud835\udc59 can be calculated in the similar way as b \ud835\udc7e \ud835\udc59 . Each row of \ud835\udc7e \ud835\udc59 (i.e., \ud835\udc7e \ud835\udc59 \ud835\udc56 , \ud835\udc56 = 1 , 2 , .., \ud835\udc51 ) can be regarded as independent basic learnable parameters. b \ud835\udc7e \ud835\udc59 is a linear combination of \ud835\udc7e \ud835\udc59 1 , \ud835\udc7e \ud835\udc59 2 , ..., \ud835\udc7e \ud835\udc59 \ud835\udc51 , and we hope that \ud835\udc7e \ud835\udc59 1 , \ud835\udc7e \ud835\udc59 2 , ..., \ud835\udc7e \ud835\udc59 \ud835\udc51 are as orthogonal as possible to reduce redundant information. [42] summarizes some orthogonal regularization methods for parameters, including \"selective\" Soft Orthogonality Regularization, which can be written as follows: where \ud835\udf06 is the regularization coefficient, \ud835\udc70 \u2208 R \ud835\udc51 \u00d7 \ud835\udc51 is the identity matrix. Here we do not limit the norm of \ud835\udc7e \ud835\udc59 \ud835\udc56 to 1, and adopt the Meta Network F X = 3 \u2217 1 @ % F \u00d7 g X = 3 \u2217 1 d @ % 3 1 l -th fully connected layer with dynamic parameters Reshape to matrix Non-linear Transformation Softmax Dynamic User Intent = following orthogonal regularization term: where 1 \u2208 R \ud835\udc51 \u00d7 \ud835\udc51 is a matrix whose elements are all 1, \u2299 is Hadamard product, and \ud835\udc3f is the number of FC-D layers.", "5.4 End-to-end model training": "We adopt an end-to-end learning strategy and use error backpropagation to train our proposed model. We choose cross-entropy as a classification loss function, which can be formulated as follows: where \ud835\udc5b is the number of samples, \ud835\udc56 represents the \ud835\udc56 -th sample, \ud835\udc66 \ud835\udc56\ud835\udc50 equals to 1 if \ud835\udc56 belongs to class \ud835\udc50 otherwise 0, \u02c6 \ud835\udc66 \ud835\udc56\ud835\udc50 is the predicted probability that sample \ud835\udc56 belongs to class \ud835\udc50 . After adding the regularization term for the parameters of the meta-predictor, the loss function can be written as: where \ud835\udefd is the hyperparameter that trades off classification loss L \ud835\udc36 and regularization term L \ud835\udc45 .", "6 EVALUATION": "In this section, we describe the dataset used in this work and introduce the detailed experimental settings. Then, we show that DIGMN outperforms current state-of-the-art models on the user engagement prediction task, demonstrating the effectiveness of DIGMN. To be more specific, we aim to answer the following research questions: \u00b7 RQ1 : Can DIGMN outperform state-of-the-art baselines in user engagement forecasting tasks at different granularities? \u00b7 RQ2 : Does DIGMN perform better than a network with static parameters and a similar number of parameters? \u00b7 RQ3 : How does each part in DIGMN affect the performance? \u00b7 RQ4 : Is dynamic user intent a good signal to differentiate diverse user engagement patterns? \u00b7 RQ5 : How do hyperparameters affect the performance? \u00b7 RQ6 : What about the impact of using other methods to obtain user intent representations on DIGMN performance? \u00b7 RQ7 : What is the influence of different methods implementing dynamic parameters on the prediction effect of DIGMN? \u00b7 RQ8 : Can DIGMN learn interpretable user dynamic intent representations?", "6.1 Experimental Setup": "6.1.1 Datasets . To evaluate the performance of our proposed model, we conduct experiments on real-world anonymous users' data from LinkedIn. To protect user privacy, we collect coarsegrained behavioral data (only the types of events, as shown in Table 1, not detailed user behaviors) of random anonymous users during four weeks. We filter out users with less than 7 (median) sessions during the first two weeks to make the extracted dynamic intent information more meaningful. As shown in Table 2, the user engagement trend label \ud835\udc66 is obtained by comparing the user engagement in the first two weeks (i.e., \ud835\udc51 \u210e , \ud835\udc60 \u210e ) and the following two weeks (i.e., \ud835\udc51 \ud835\udc53 , \ud835\udc60 \ud835\udc53 ). The label distribution of the day-level task is approximately \ud835\udc66 = -1 : \ud835\udc66 = 0 : \ud835\udc66 = 1 \u2248 2 : 2 : 1, and the label distribution of the session-level task is approximately \ud835\udc66 = 0 : \ud835\udc66 = 1 \u2248 3 : 2. We randomly sample 200K users for each experiment and split them into three parts: 80% of the samples (i.e., users) for training, 10% for validation, and the rest 10% for testing. \uf8f3 6.1.2 Evaluation Metrics. For the day-level task (3-class classification task), we adopt Macro F1-score as our metric following Liu et al.[2]'s work. Macro F1-score is the average F1-score for each class and can evaluate the classifier in the case of class imbalance. Furthermore, for the session-level task (2-class classification task), AUROC is adopted as our metric, which is a popular metric for the binary classification task. 6.1.3 Comparison Methods. We do not collect the interaction actions between users in this work. Considering comparability, we did not compare with the FATE [3] and CFChurn [10], which uses interaction behaviors between users to predict user engagement or user churn. We compare our proposed model against the following baselines: \u00b7 Logistic Regression (LR) : We use user macroscopic features and the latest platform delivery message to make predictions. \u00b7 XGBoost [43]: Use the same input features as Logistic Regression to classify. \u00b7 Multilayer Perceptron (MLP) : We implement an MLP with two hidden layers using the same features as Logistic Regression and XGBoost as input. \u00b7 Activity LSTM [1]: We count the number of events in Table 1 that occurred each day for the user in the past two weeks and get a time series \ud835\udc68 \u2208 R 10 \u00d7 14 as the input of the activity LSTM. \u00b7 Temporal GCN-LSTM [2]: We treat events as actions and construct the user's action graph according to [2]. Temporal GCNLSTM applies GCN on the action graph to extract user finegrained features, then feeds it into LSTM to capture its temporal dynamics. \u00b7 Deep Multi-channel [2]: To achieve the best performance, it combines user macroscopic features, the latest platform delivery message, activity LSTM, and Temporal GCN-LSTM. It can be viewed as the state-of-the-art model for predicting user engagement that does not consider the interaction behaviors between users. 6.1.4 Evaluation Settings. The output dimension of the linear mapping layer is set to be half of the input dimension. The number of hidden units in session LSTM and intent LSTM is empirically set to 32. We implement DIGMN with \ud835\udc3f FC-D layers, where \ud835\udc3f is searched from 1 to 5. The hidden units of the non-linear transformation layer in the meta network are set to 32. The dimension of \ud835\udc82 is searched from 2 to 10 and the hyperparameter \ud835\udefd is searched in [ 10 , 1 , 10 -1 , 10 -2 , 10 -3 , 10 -4 , 10 -5 , 0 ] . We adopt Adam [44] as the optimizer with an initial learning rate 10 -3 and learning rate decay half every 20 epochs. According to [45], the authors argue against using \u2113 2 weight decay and the orthogonal regularization term together, so we only use \u2113 2 weight decay for learnable parameters other than \ud835\udc7e \ud835\udc59 ( \ud835\udc59 = 1 , 2 , ..., \ud835\udc3f ) in FC-D layers with a decay rate 10 -5 . Anearly stopping strategy is also used on the validation set to avoid overfitting. Our experiments are conducted on a single machine with a 2.4GHz 12-core CPU and 64GB of memory. Each experiment is repeated 5 times with different random seeds.", "6.2 Performance Comparison (RQ1, RQ2)": "Table 3 lists the experiment results of all compared methods. We observe that DIGMN performs best on both day-level and session-level user engagement prediction tasks, which shows the effectiveness of DIGMN. The possible reason for the poor performance of activity LSTM and Temporal GCN-LSTM on our dataset is that our user session data is sparse (each user averages around 2 sessions a day in our dataset, but 7 in Liu et al.[2]'s dataset), which results in sparse activity sequences and action graphs, reducing their performance. Compared with the FC layer, the FC-D layer used in DIGMN increases the learnable parameters. Take one FC-D layer as an example, assuming that it maps the input \ud835\udc89 \u2208 R \ud835\udc5a to \ud835\udc89 \u2032 \u2208 R \ud835\udc5b , then the learnable parameters of FC-D layer is \ud835\udc51 \u2217 \ud835\udc5a \u2217 \ud835\udc5b (for brevity, ignoring bias). However, for the FC layer, its number of learnable parameters is \ud835\udc5a \u2217 \ud835\udc5b . For comparability, we use the FC layers (Static) with the same shape or a similar number of parameters to replace the FC-D layers (Dynamic) in DIGMN. As shown in Table 4, we observe that DIGMN with dynamic parameters outperforms DIGMN with static parameters in two tasks, demonstrating the effectiveness of leveraging dynamic parameters network to achieve differentiated user engagement prediction.", "6.3 Model Ablation Study (RQ3, RQ4)": "We conduct the following ablation study on the session-level user engagement prediction task. We use an MLP with two hidden layers (# hidden units are 64 and 32) as the predictor to investigate the effectiveness of various components in our model. As shown in Figure 6(a), we find that simultaneously using the user's macroscopic features, platform delivery message, session features, and intent features can achieve the best results. We also study the influence of different adjust signals of DIGMN, as shown in Figure 6(b). We find that using intent as a parameter adjustment signal performs best, indicating that intent can distinguish AUROC 0.63 0.64 0.65 Component S+D M+S+D I+S+D M+I+S+D AUROC 0.63 0.64 0.65 0.66 Parameter adjustment signal M S I M+I M+S I+S M+I+S (a) Ablation study on different model (b) Ablation study on different parame- components. ter adjustment signals of DIGMN. different engagement levels well. Introducing too much information to adjust parameters may impair the model classification ability.", "6.4 Hyperparameter Sensitivity (RQ5)": "We conduct the following hyperparameter sensitivity study on the session-level user engagement prediction task. \u00b7 Hyperparameter sensitivity of \ud835\udc82 : As shown in Figure 7(a), we search for the optimal dimension of \ud835\udc82 between 2 and 10, and find that when the dimension of \ud835\udc82 is too small or too large, it will hurt the model's performance. The possible reason is that when the dimension of \ud835\udc82 is small, the expressive ability of the model is limited and cannot sufficiently capture the diverse engagement patterns of users. However, when the dimension of \ud835\udc82 is large, the number of parameters of the model will rapidly increase, which may cause overfitting and weaken generalization ability. On our dataset, the model performs best when the dimension of \ud835\udc82 is equal to 4. \u00b7 Hyperparameter sensitivity of \ud835\udefd : As illustrated in Figure 7(b), we adopt grid search in [ 10 , 1 , 10 -1 , 10 -2 , 10 -3 , 10 -4 , 10 -5 , 0 ] , and find that the model performs best when \ud835\udefd = 10 -2 . This indicates that too strong orthogonal constraints on the FC-D layer will damage the classification ability of the model, and adding proper orthogonal constraints to the FC-D layer can improve the model's performance. \u00b7 Hyperparameter sensitivity of \ud835\udc3f : As illustrated in Figure 7(c), we vary the number of FC-D layers (i.e., \ud835\udc3f ) in the meta predictor from 1 to 5 and find that the model achieves the best performance when \ud835\udc3f = 3. The possible reason is that when the number of FC-D layers is small, the learning ability of the model is insufficient, and the model is prone to underfitting. In contrast, when the number of FC-D layers is large, the model is prone to overfitting, which damages the model's generalization ability. AUROC 0.645 0.650 0.655 Dimension of a 2 3 4 5 6 7 8 9 10 AUROC 0.635 0.640 0.645 0.650 0.655 0.660 \u03b2 10 1 10 -1 10 -2 10 -3 10 -4 10 -5 0 AUROC 0.645 0.650 0.655 The number of L 1 2 3 4 5 (b) Hyperparameter sensitivity of \ud835\udefd . (c) Hyperparameter sensitivity of \ud835\udc3f .", "6.5 Further Analysis (RQ6, RQ7)": "In order to compare with other methods for obtaining representations of user intent, we replace the intent inference operation based on cosine similarity computation in the intent evolution layer with a linear mapping (end-to-end learning), which can be formulated as \ud835\udc8a = tanh ( \ud835\udc7e\ud835\udf42 + \ud835\udc83 ) , where \ud835\udc7e \u2208 R 10 \u00d7 7 , \ud835\udc83 \u2208 R 7 are learnable parameters, tanh (\u00b7) is hyperbolic tangent activation functions used to limit the output value between -1 and 1 (same range as cosine similarity). As shown in Table 5, we can observe that introducing basic prior user intents obtained by LDA to mine user intents in DIGMN performs better than directly learning the implicit intent representation. The possible reason is that the basic prior intents are learned on a larger amount of data, while the end-to-end learning user intent representation usually only uses a part of the data, and the former contains more information. There are usually two ways to implement dynamic parameters: one is to dynamically adjust the model's parameters (which we use in this paper), and the other is to generate the model's parameters directly. The latter method generate transform matrix b \ud835\udc7e \ud835\udc59 of \ud835\udc59 -th FC-D layer, which can be formulated as b \ud835\udc7e \ud835\udc59 = Reshape ( \ud835\udc7e 4 ( ReLU ( \ud835\udc7e 3 e \ud835\udc8a + \ud835\udc83 3 )) + \ud835\udc83 4 ) , where \ud835\udc7e 3 , \ud835\udc7e 4 , \ud835\udc83 3 , \ud835\udc83 4 are learnable parameters. As shown in Table 6, directly generating parameters of FC-D layers usually requires a meta network with more parameters. Meanwhile, during the training process, we find that directly generating model parameters is prone to overfitting which reduces the model's generalization ability.", "6.6 Visualization (RQ8)": "We use PCA to reduce the dimensions of dynamic user intent representation from 32 to 3 and 2 in the test dataset of the session-level user engagement forecasting task. The label of each sample is the user's most frequent intent in the past two weeks. Figure 8(a) and 8(b) show that the intent evolution layer can learn meaningful and discriminative user dynamic intent representation. Subsequent meta-predictor can leverage user dynamic intent representation to adjust network parameters and perform differentiated user engagement prediction. ler Figure 8: The visualization of dynamic user intent representation. We apply zero-mean normalization to the input highdimensional vectors before using PCA for dimension reduction. Best viewed in color.", "7 CONCLUSIONS": "In this work, we use LDA on user session data to mine basic user intents. Meanwhile, we propose a dynamic intent guided meta network (DIGMN) to explicitly model user intent evolution over time and perform differentiated user engagement forecasting. Experiments on the real-world dataset from LinkedIn demonstrate the effectiveness of our method. Future work will focus on the interpretability of the method and apply our model to more business scenarios, such as the platform message delivery system. Besides, we will validate our method on more datasets.", "ACKNOWLEDGMENTS": "Thanks to Xu Chen and Yanbin Kang for their insightful discussions. Thanks to Lu Chen and Yihan Cao for their help in the data process and building the dataset. Thanks to Yiping Yuan, Huichao Xue, and Yanming Shen for their comprehensive review feedback.", "REFERENCES": "[1] Carl Yang, Xiaolin Shi, Luo Jie, and Jiawei Han. I know you'll be back: Interpretable new user clustering and churn prediction on a mobile social application. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , pages 914-922, 2018. [2] Yozen Liu, Xiaolin Shi, Lucas Pierce, and Xiang Ren. Characterizing and forecasting user engagement with in-app action graph: A case study of snapchat. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , pages 2023-2031, 2019. [3] Xianfeng Tang, Yozen Liu, Neil Shah, Xiaolin Shi, Prasenjit Mitra, and Suhang Wang. Knowing your fate: Friendship, action and temporal explanations for user engagement prediction on social apps. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , pages 2269-2279, 2020. [4] Caroline Lo, Dan Frankowski, and Jure Leskovec. Understanding behaviors that lead to purchasing: A case study of pinterest. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining , pages 531-540, 2016. [5] Zhiyuan Lin, Tim Althoff, and Jure Leskovec. I'll be back: on the multiple lives of users of a mobile activity tracking application. In Proceedings of the 2018 World Wide Web Conference , pages 1501-1511, 2018. [6] Vicki G Morwitz, Eric Johnson, and David Schmittlein. Does measuring intent change behavior? Journal of consumer research , 20(1):46-61, 1993. [7] Martin Fishbein and Icek Ajzen. Belief, attitude, intention, and behavior: An introduction to theory and research. Philosophy and Rhetoric , 10(2), 1977. [8] David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. the Journal of machine Learning research , 3:993-1022, 2003. [9] Farhan Asif Chowdhury, Yozen Liu, Koustuv Saha, Nicholas Vincent, Leonardo Neves, Neil Shah, and Maarten W Bos. Ceam: The effectiveness of cyclic and ephemeral attention models of user behavior on social platforms. In Proceedings of the International AAAI Conference on Web and Social Media , volume 15, pages 117-128, 2021. [10] Guozhen Zhang, Jinwei Zeng, Zhengyue Zhao, Depeng Jin, and Yong Li. A counterfactual modeling framework for churn prediction. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining , pages 1424-1432, 2022. [11] Arno De Caigny, Kristof Coussement, and Koen W De Bock. A new hybrid classification algorithm for customer churn prediction based on logistic regression and decision trees. European Journal of Operational Research , 269(2):760-772, 2018. [12] Lin Yang, Mingxuan Yuan, Yanjiao Chen, Wei Wang, Qian Zhang, and Jia Zeng. Personalized user engagement modeling for mobile videos. Computer Networks , 126:256-267, 2017. [13] Guandong Xu, Yanchun Zhang, and Xun Yi. Modeling user behavior for web recommendation using lda model. In 2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology , volume 3, pages 529-532. IEEE, 2008. [14] Mark J Carman, Fabio Crestani, Morgan Harvey, and Mark Baillie. Towards query log based personalization using topic models. In Proceedings of the 19th ACM international conference on Information and knowledge management , pages 1849-1852, 2010. [15] Jimmy Lin and W John Wilbur. Modeling actions of pubmed users with n-gram language models. Information retrieval , 12(4):487-503, 2009. [16] Pablo Loyola, Chen Liu, and Yu Hirate. Modeling user session and intent with an attention-based encoder-decoder architecture. In Proceedings of the Eleventh ACM Conference on Recommender Systems , pages 147-151, 2017. [17] Rakshit Agrawal, Anwar Habeeb, and Chih-Hsin Hsueh. Learning user intent from action sequences on interactive systems. In Workshops at the Thirty-Second AAAI Conference on Artificial Intelligence , 2018. [18] Qi Guo and Eugene Agichtein. Ready to buy or just browsing? detecting web searcher goals from interaction data. In Proceedings of the 33rd international ACMSIGIR conference on Research and development in information retrieval , pages 130-137, 2010. [19] Wenchao Gu, Yanlin Wang, Lun Du, Hongyu Zhang, Shi Han, Dongmei Zhang, and Michael Lyu. Accelerating code search with deep hashing and code classification. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 2534-2544, 2022. [20] Justin Cheng, Caroline Lo, and Jure Leskovec. Predicting intent using activity logs: How goal specificity and temporal range affect user behavior. In Proceedings of the 26th International Conference on World Wide Web Companion , pages 593-601, 2017. [21] Haoyang Li, Xin Wang, Ziwei Zhang, Jianxin Ma, Peng Cui, and Wenwu Zhu. Intention-aware sequential recommendation with structured intent transition. IEEE Transactions on Knowledge and Data Engineering , 2021. [22] Yongjun Chen, Zhiwei Liu, Jia Li, Julian McAuley, and Caiming Xiong. Intent contrastive learning for sequential recommendation. In Proceedings of the ACM Web Conference 2022 , pages 2172-2182, 2022."}
