{
  "AdSEE: Investigating the Impact of Image Style Editing on Advertisement Attractiveness": "Liyao Jiang University of Alberta Edmonton, AB, Canada liyao1@ualberta.ca Chenglin Li University of Alberta Edmonton, AB, Canada ch11@ualberta.ca Haolan Chen Platform and Content Group, Tencent Shenzhen, China haolanchen@tencent.com Xiaodong Gao Xinwang Zhong Platform and Content Group, Tencent Shenzhen, China cshiudawn@gmail.com visionzhong@tencent.com Yang Qiu Shani Ye Platform and Content Group, Tencent Shenzhen, China rickyqqiu@tencent.com lisaniye@tencent.com",
  "Di Niu": "University of Alberta Edmonton, AB, Canada dniu@ualberta.ca",
  "ABSTRACT": "",
  "KEYWORDS": "Online advertisements are important elements in e-commerce sites, social media platforms, and search engines. With the increasing popularity of mobile browsing, many online ads are displayed with visual information in the form of a cover image in addition to text descriptions to grab the attention of users. Various recent studies have focused on predicting the click rates of online advertisements aware of visual features or composing optimal advertisement elements to enhance visibility. In this paper, we propose Advertisement Style Editing and Attractiveness Enhancement (AdSEE), which explores whether semantic editing to ads images can affect or alter the popularity of online advertisements. We introduce StyleGANbased facial semantic editing and inversion to ads images and train a click rate predictor attributing GAN-based face latent representations in addition to traditional visual and textual features to click rates. Through a large collected dataset named QQ-AD, containing 20,527 online ads, we perform extensive offline tests to study how different semantic directions and their edit coefficients may impact click rates. We further design a Genetic Advertisement Editor to efficiently search for the optimal edit directions and intensity given an input ad cover image to enhance its projected click rates. Online A/B tests performed over a period of 5 days have verified the increased click-through rates of AdSEE-edited samples as compared to a control group of original ads, verifying the relation between image styles and ad popularity. We open source the code for AdSEE research at https://github.com/LiyaoJiang1998/adsee.",
  "CCS CONCEPTS": "¬∑ Information systems ‚Üí Display advertising ; Computational advertising ; ¬∑ Computing methodologies ‚Üí Image representations . Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. KDD '23, August 6-10, 2023, Long Beach, CA, USA. ¬© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0103-0/23/08...$15.00 https://doi.org/10.1145/3580305.3599770 Advertisement Image Editing; StyleGAN; Click-through Rate Prediction; Genetic Algorithms",
  "ACMReference Format:": "Liyao Jiang, Chenglin Li, Haolan Chen, Xiaodong Gao, Xinwang Zhong, Yang Qiu, Shani Ye, and Di Niu. 2023. AdSEE: Investigating the Impact of Image Style Editing on Advertisement Attractiveness. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '23), August 6-10, 2023, Long Beach, CA, USA. ACM, New York, NY, USA, 13 pages. https://doi.org/10.1145/3580305.3599770",
  "1 INTRODUCTION": "Online or digital advertisements are crucial elements in e-commerce sites, social media platforms, and search engines. With the increasing popularity of mobile browsing, many online ads are displayed on cellphones with visual information frequently in the form of a cover image in addition to text description, since visual information is not only more direct but can also grab people's attention compared to text-only ads . In fact, previous studies [4, 9] have shown that appealing cover images lead to a higher Click-Through Rate (CTR) in online ads . Therefore, a number of recent studies on online ads have focused on extracting visual features for visual-aware CTR prediction [7, 23, 40]. Furthermore, while many online ad images contain human faces, previous studies [2, 20, 44] have verified that incorporating human faces in online ad correlates to more attention towards the ads , as well as that eye gaze directions have an impact on user response. Another related research direction focuses on Advertisement Creatives selection [8], which searches from a large pool of creative elements and templates to compose a good ad design. Thanks to recent advancements in generative adversarial networks (GANs), e.g., SytleGAN [30-32], image editing has been made possible, especially with respect to facial semantics. However, existing studies have not investigated the impact of style editing in recommender systems. In this paper, we propose the Advertisement Style Editing and Attractiveness Enhancement (AdSEE) system, which aims to do a reality check to answer a long-standing question in AI ethicswhether editing the facial style in an online ad can enhance its KDD '23, August 6-10, 2023, Long Beach, CA, USA. Liyao Jiang et al. attractiveness? AdSEE consists of two parts: 1) a Click Rate Predictor (CRP) to predict the averaged click rate (CR) for any given ad in an ad category, based on its cover image and text information, and 2) a Genetic Advertisement Editor (GADE) to search for the optimal face editing dimensions and directions, e.g., smile, eye gaze direction, as well as the corresponding editing intensity coefficients. Our main contributions are summarized as follows: ¬∑ We study the impact of face editing on online ad enhancement, which edits the facial features in an ad cover image by changing its latent face representations. We use a pre-trained StyleGAN2FFHQ [32] model as well as its corresponding pre-trained GAN inversion model e4e [52] for face generation and embedding. Specifically, coupled with facial landmark detection techniques, a face image detected from an ad is first encoded into the latent space of the GAN generator through the GAN inversion encoder e4e. We then modify the face representation in the latent embedding space and feed it into the generator to obtain a semantically edited version, which is finally replacing the original face to generate the enhanced ad . ¬∑ We collect the QQ-AD dataset which contains 20,527 online ads with visual and textual information as well as their click rates information, based on which we train a new click rates prediction model based on six types of features, including Style-GAN-based facial latent vectors, in addition to image and text embeddings. This is the first online ad CTR predictor that takes into account latent embeddings from GAN. Offline tests have verified the superiority of our predictor to a range of baselines only using image embeddings or using NIMA image quality assessment [51], implying the important connections of facial characteristics to ad popularity. We open source the implementation of AdSEE 1 . ¬∑ We use the SeFa [50] model to find ùëû semantic editing directions in the latent space of the GAN generator through eigenvalue decomposition of the weight matrix of the generator. Each selected direction corresponds to a semantic facial characteristic, e.g., smile, age, etc. Then, we use a genetic algorithm to search for the best editing intensities for all the identified directions. With the identified directions and their corresponding optimal intensities, we adjust an ad to the best appearance that may lead to higher click rates. We further perform extensive analysis to offer insights on what directions and intensities of semantic edits may improve ad click rates. We found that a face oriented slightly downward, a smiling face, and a face with feminine features are more attractive to clicks according to the analysis. AdSEE was integrated into the Venus distributed processing platform at Tencent and deployed for an online A/B test in the recommendation tab of the QQ Browser mobile app (a major browser app by Tencent for smartphones and tablets). We report the test results of AdSEE in the traffic of QQ Browser mobile app for a period of 5 days in 2022. As click rate is an important metric to gauge user satisfaction and efficiency of the business, with human-aided ethics control and censoring, the online A/B testing results show that AdSEE improved the average click rate of general ads in the QQ Browser recommendation tab, verifying the existence of the relationship between image style editing and ad popularity. 1 Code available at https://github.com/LiyaoJiang1998/adsee.",
  "2 RELATED WORK": "Click-Through Rate Prediction. ACTRpredictor aims to predict the probability that a user clicks an ad given certain contexts which play an important role in improving user experience for many online services, e.g., e-commerce sites, social media platforms, and search engines. Recent studies extract visual features from the cover image of ad for better CTR predicting [7, 40, 41, 63, 66]. Chen et al. [7] apply deep neural network (DNN) on ad image for CTR prediction. Liu et al. propose the CSCNN [40] model to encode ad image and its category information, and predict the personalized CTR with user embeddings. Li et al. [37] utilize multimodal features including categorical features, image embeddings, and text embeddings to predict the CTR of E-commerce products. The sparsity and dimensionality of features vary drastically among different modalities. Therefore, it is crucial to effectively model the interactions among the features from different modalities [56]. AutoInt is shown to achieve great performance improvement on the prediction tasks on multiple real-world datasets. Thus, in this paper, we build a click rate predictor to estimate the averaged click rate of an ad among advertising audience based on the best-performing AutoInt [60] model compared with many state-of-the-art models in Appendix Section A. Creatives Selection . Another research direction of display advertising focuses on creatives selection. Previous studies in this line of research use the bandit algorithm for the news recommendations [36], page optimization [59], and real online advertising [25]. Chen et al. [8] propose an automated creative optimization framework to search for the optimal creatives from a pool. In this work, instead of choosing from various creatives, we enhance an existing ad through direct facial feature editing. Face Image Generation and Editing . Generative Adversarial Networks (GANs) [18] have achieved impressive results on a range of image generation tasks. Style transfer [16] is the task of rendering the content of one image in the style of another. StyleGAN [31] proposes a style-based generator using the AdaIN [27] operation and can generate higher quality photo-realistic images compare to other alternatives [6, 29]. Based on the StyleGAN2 [32] model, Tov et al. propose the e4e [52] encoder to map real face images to the latent embedding space of the StyleGAN2-FFHQ [32] model. Shen and Zhou propose a closed-form factorization method to find the latent directions of face image editing without supervision. Following the style transfer [16] direction, Durall et al. propose FacialGAN [14] to transfer the style of a reference face image to the target face image. However, FacialGAN requires a standard face image as reference, which can not be satisfied when we have arbitrary faces in ùëéùëë cover images. Instead, our work utilizes the SeFa [50] image editing method to find the face editing directions without any supervision or reference images which is automated and efficient. To adjust the ùëéùëëùë† to their best appearances that may lead to higher click rates, we find the optimal face editing intensity through the guidance of the predicted click rate. We adopt StyleGAN2 as our backbone image generation model because StyleGAN2 offers state-of-the-art generation quality and is applicable to many domains including faces, cars, animals, etc. Many works have chosen to extend StyleGAN2 including [28, 48, 50, 52] thus allowing many possible applications including image editing with SeFa [50]. AdSEE: Investigating the Impact of Image Style Editing on Advertisement Attractiveness KDD '23, August 6-10, 2023, Long Beach, CA, USA.",
  "3 METHOD": "In this section, we describe the detailed model adopted in the AdSEE framework. We consider advertisement ( ad ) data with category information, cover image, and query text. Each advertisement is displayed to the user within the app feed as a card which includes a cover image and a query text as the advertisement title. Specifically, for a given advertisement ùëéùëë ùëñ = ( ùê∂ ùëñ , ùêº ùëñ , ùëá ùëñ ) , ùê∂ ùëñ ‚àà C where ùê∂ ùëñ represents the category, e.g., 'Sports', 'Game', that ùëéùëë ùëñ belongs to, C denotes the set of all the considered categories, and ùêº ùëñ , ùëá ùëñ represent the cover image and query text of ùëéùëë ùëñ , respectively. An impression refers to the event when an ad is shown/exposed to a target user by the online advertising system. Therefore, to assess the attractiveness of ùëéùëë ùëñ , we calculate its averaged click rate as  where ùëêùëôùëñùëêùëò ùëñ and ùëñùëöùëùùëüùëíùë†ùë†ùëñùëúùëõ ùëñ denote the total numbers of clicks and impressions of ùëéùëë ùëñ , respectively. The averaged click rate ùê∂ùëÖ ùëñ indicates the overall attractiveness of ùëéùëë ùëñ among the ad audience.",
  "3.1 System Overview": "Figure 1 provides an overview of our proposed AdSEE framework. First, we build a Click Rate Predictor (CRP) which takes an ad as input and predicts its averaged click rate defined in (1). Trained with a regression task, the CRP estimates the click rate of any given ùëéùëë which can be used to guide the ad editing module. Second, we build the Genetic Advertisement Editor (GADE) module to enhance the overall attractiveness indicated by ùê∂ùëÖ ùëñ of ùëéùëë ùëñ through editing its cover image ùêº ùëñ . The GADE module utilizes genetic algorithm to explore human facial feature editing directions in the form the face latent codes. It aims to find the best editing direction and editing intensities which may lead to the highest attractiveness enhancement reflected by the increase in predicted Click Rate with guidance from the CRP.",
  "3.2 Click Rate Predictor": "As shown in Figure 1, we extract sparse and dense features from the raw input ad data, i.e., ( ùê∂ ùëñ , ùêº ùëñ , ùëá ùëñ ) and use the AutoInt [60] model structure to predict the average click rate for ùëéùëë ùëñ . Sparse Features . The category information of an ad , e.g., 'Game' is encoded as a one-hot vector, e.g. '[0,1,0]'. The length of the encoded vector depends on the size of the category set, i.e., |C| . The content of the cover image is also crucial to its overall attractiveness. Therefore, apart from the ad category, we further extract sparse features from the cover image of an ad . Specifically, we adopt the SOLO instance segmentation model [57, 58] to identify the segmentation masks of all instances which belong to the COCO [39] class, e.g., person, cat, etc. Formally, for an advertisement ùëéùëë ùëñ = ( ùê∂ ùëñ , ùêº ùëñ , ùëá ùëñ ) , we have  where Instance ùëñ is the list of all detected instances by the SOLO model from the cover image ùêº ùëñ , and Unique ( Instance ùëñ ) identifies all the unique COCO classes, Class ùëñ , from the instance list. The SOLO model supports the detection of 80 classes of COCO object labels. Therefore, we convert the detected Class ùëñ to a multi-hot encoded vector of size 80, e.g., [ 0 , 1 , 0 , ..., 0 , 1 ] , where each 1 indicates the presence of a certain COCO class in the cover image. For instances that fall in the 'Person' COCO class, we extract their corresponding person images according to their segmentation masks. Specifically, for a person instance, we apply Gaussian Blur[17] to the unmasked area (non-person area) to blur the background out and isolate individual person to obtain person image, i.e., ùëÉ ùëñ,ùëó for the j-th person in the cover image ùêº ùëñ . Then, we feed all the person images, i.e., ùëÉ ùëñ = { ùëÉ ùëñ,ùëó } , ùëó = 1 , ¬∑ ¬∑ ¬∑ , ùêæ ùëñ , where ùêæ ùëñ is the total number of persons in cover image ùêº ùëñ , into the Dlib [34] face alignment model to align the facial landmarks and crop to face which yields face images ùêπ ùëñ = { ùêπ ùëñ,ùëó } , ùëó = 1 , ¬∑ ¬∑ ¬∑ , ùëÄ ùëñ , where ùëÄ ùëñ ‚â§ ùêæ ùëñ represents the number of detected faces from the ùêæ ùëñ person images ùëÉ ùëñ . That is,  Note that, we remove ùëéùëëùë† that do not contain a face image because we cannot perform facial feature editing if there is no face in a cover image. In addition, we remove ads with more than ùëÄ = 5 faces from the dataset to avoid extracting low-resolution and unrecognizable face images from a cover image. Thereafter, we encode the face count, i.e., ùëÄ ùëñ where 1 ‚â§ ùëÄ ùëñ ‚â§ 5, into a one-hot sparse vector with the length of 5, for example, a face count vector [ 0 , 1 , 0 , 0 , 0 ] indicates 2 faces are detected from a cover image. Dense Features . We further extract dense features from the cover image and query text of an ad for the click rate prediction. First, we adopt the e4e model [52] to encode each face image, ùêπ ùëñ,ùëó , into a real-valued dense vector representation ùëß ùëñ,ùëó . Formally, for the j-th face image of cover image ùêº ùëñ , we have  where ùê∏ denotes the pre-trained e4e encoder for GAN inversion [61] to the StyleGAN2-FFHQ [32] face latent space, and ùëß ùëñ,ùëó ‚àà R ùëë √ó ùëô is the corresponding two-dimensional latent representation of face ùêπ ùëñ ùëó , and ùëÄ ùëñ is the number of detected faces in cover image ùêº ùëñ . Then, we stack the ùëÄ ùëñ latent embeddings of shape [ ùëë, ùëô ] into one tensor of shape [ ùëÄ ùëñ , ùëë, ùëô ] , i.e.,  We apply the best-performing max-pooling operation (among maxpooling, average-pooling, and concatenation operations) on ùëß ùëñ along its first dimension to obtain the latent face code ¬Ø ùëß ùëñ with the shape of [ ùëë, ùëô ] . Then, ¬Ø ùëß ùëñ is flattened and used as a dense feature for the click rate prediction. With the latent representation ¬Ø ùëß ùëñ , the CRP encodes the attractiveness of ads from their facial features which enables using it to guide the GADE module for face style editing and cover image enhancement of ad . Second, apart from facial features, the attractiveness of an ad also relies on the overall content and quality of the cover image. Therefore, we encode the whole cover image into a latent image representation to boost the click rate prediction. Specifically, we use two different image embedding methods to get more comprehensive and effective embeddings of the cover images. 1) We adopt the image embedding model which is pre-trained with the multi-label image classification task on the open image dataset [35]. With more than 9.7 million images and around 20 thousand labels, the embedding provided by the multi-label classifier carries fine-grained image KDD '23, August 6-10, 2023, Long Beach, CA, USA. Liyao Jiang et al. Figure 1: The system architecture of the proposed framework. GADE Module Click Rate Predictor Click Rate Predictor Overall Framework of AdSEE Edited Estimated Raw Input : original ad GADE Module Original Estimated Output: Enhanced Cover Image Output: Face Segmentation Module Substitute with Edited Face Latent Code Cover Image Person(s) SOLO Instance Segmentation Model Dlib Face Alignment Model Face(s) ad Category Cover Image Query Text Image Embedding Models e4e Encoder Face Segmentation Module Maxpool and Flatten Face(s) Latent Code Image Embedding Bert Model Text Embedding One-hot Face Count Multi-hot Class Labels Dense Features One-hot ad Category One-hot Face Count Multi-hot Class Labels Sparse Features Substitute with AutoInt Model AutoInt Model Output: Original Estimated Output: Edited Estimated Edited Estimated Original Estimated Sparse Features Dense Features Embedding Layer Output Layer: Estimated Click Rate Multi-head Self-Attention Interacting Layer Output: Edited Cover Image Output: Semantic Face Editing Genetic Algorithm Optimization SeFa Editing directions StyleGAN2 Generator Original Face Latent Code Face Swap Output: Best Edited Cover Image Click Rate Predictor Substitute with Raw Input: Random Population Initialization Fitness Evaluation Parent Selection Crossover and Mutation loop Survivor Selection content information. 2) Another method of cover image embedding is provided by Sogou 2 . Sogou provides the service of searching pictures through text, in which both text and pictures are encoded into latent vectors for picture and text matching. Therefore, the embedding of the cover image provided by Sogou contains semantic information which is useful in judging the attractiveness of the cover image. For a given ad , we concatenate the image embeddings from the above two models to obtain the final embedding of the cover image. Finally, we use a pre-trained Bert-Chinese model [13] to extract text embedding from the query text, ùëá ùëñ , associated with ùëéùëë ùëñ . The Bert model takes the query text as input and outputs the embeddings of the words in the text. Then, we apply the max-pooling operation on the word embeddings to get the embedding of the query text. Click Rate Prediction Let ùë• ùëñ, 1 , ¬∑ ¬∑ ¬∑ , ùë• ùëñ, 6 denotes the 6 extracted features, including sparse features, ad category, multi-hot class label, one-hot face count, and dense features, latent face representation, cover image embedding, and text embedding, for ùëéùëë ùëñ . Then, We apply the AutoInt [60] model to the extracted features, ùë• ùëñ, 1 , ¬∑ ¬∑ ¬∑ , ùë• ùëñ, 6, to predict the averaged click rate. We selected the best-performing AutoInt model in our evaluation of many SOTA models in Appendix Section A. For a given advertisement ùëéùëë ùëñ , to allow the interaction between sparse and dense features, the Embedding layer of the AutoInt model maps all 6 extracted features into a fix-length and lowdimensional space through embedding matrices, i.e.,  where ùíâ ùëñ,ùëò denotes the low-dimensional feature of ùë• ùëñ,ùëò , and Embed (¬∑) is the standard embedding layer seen in almost all recommenders which learns a set of Embedding Weight Matrices, one for each 2 A technology subsidiary of Tencent that provides search services. input feature. Then, self-attention layers are adopted to model highorder feature interactions in an explicit fashion:  where Attention (¬∑) denotes multiple self-attention layers, and ÀÜ ùíâ ùëñ 1 represents the high-order interaction features. Finally, the firstorder features and their high-order interactions are fed into an output layer for click rate prediction.  where ùíâ ‚å¢ ùëñ, 1 ùíâ ‚å¢ ùëñ, 2 ¬∑ ¬∑ ¬∑ ‚å¢ ùíâ ùëñ, 6 denotes the vector after concatenating the vectors ùíâ ùëñ, 1 , ¬∑ ¬∑ ¬∑ , ùíâ ùëñ, 6, and ‚äï represent point-wise addition. The output, ÀÜ ùê∂ùëÖ ùëñ , of the fully-connected layer, FC (¬∑) , denotes the predicted average click rate of ùëéùëë ùëñ . The loss function of the Click Rate Predictor (CRP) is defined as the mean square error (MSE) between the predicted click rate and the target click rate:  where ùëÅ is the number of ads , and ùëä denotes the model weights. The first term represents the averaged square error between predicted click rates and the target click rates on the whole dataset. The second term is a regularizer to prevent over-fitting. The ùõº is a hyperparameter that controls the influence of regularization.",
  "3.3 Genetic Advertisement Editor": "As shown in Figure 1, the Genetic Advertisement Editor (GADE) module takes the original faces latent code ùëß as input, iterates over generations of Face Image Enhancement guided by the CRP, then outputs the best-edited cover image ùêº ‚àó ùëñ and the best change in predicted click rate denoted as Œî ÀÜ ùê∂ùëÖ ‚àó ùëñ . We describe the Semantic Face AdSEE: Investigating the Impact of Image Style Editing on Advertisement Attractiveness KDD '23, August 6-10, 2023, Long Beach, CA, USA. Editing module and the Genetic Algorithm Optimization (GAO) module in detail below. Semantic Face Editing. Following Tov et al. [52], we adopt the closed-form semantic factorization method SeFa [50] to identify a set of edit directions ùëõ from the latent space of the pre-trained StyleGAN2-FFHQ [32] face image generator ùê∫ (¬∑) . SeFa utilizes eigen-decomposition on the matrix ùê¥ ùëá ùê¥ , where ùê¥ is the weight matrix of ùê∫ (¬∑) , to find a set of edit directions, i.e., ùëõ = { ùëõ ùëù } ùëû ùëù = 1 where ùëõ ùëù corresponds to the eigenvector associated with the ùëù -th largest eigenvalue of the matrix ùê¥ ùëá ùê¥ . Each edit direction ùëõ ùëù ‚àà ùëÖ 512 corresponds to some face semantic concept, e.g. smile, eyeopenness, age. With the identified edit directions ùëõ , we apply the ùëíùëëùëñùë° (¬∑) operation to the face set ùêπ ùëñ to edit the facial image styles and enhance the attractiveness of a given cover image ùêº ùëñ . Formally, we have  where we alter the face image ùêπ ùëñ,ùëó by linearly moving its original face latent codes ùëß ùëñ,ùëó along the identified direction ùõº ùëñ,ùëó ùëõ . Then, we use ùê∫ (¬∑) to generate edited face images ùêπ ‚Ä≤ ùëñ,ùëó from edited face style vectors ùëß ‚Ä≤ ùëñ,ùëó . In addition, ùõº ùëñ,ùëó ‚àà R ùëû is the editing intensity coefficients given by a genetic algorithm for face image ùêπ ùëñ,ùëó , and ùõº ùëñ,ùëó ùëõ denotes the linear combination of the ùëû edit directions ùëõ . Cover Image Editing . Then, we use the OpenCV [5] and Dlib [34] libraries to swap the edited face images ùêπ ‚Ä≤ ùëñ back into ùêº ùëñ to obtain the edited cover image ùêº ‚Ä≤ ùëñ . The face swap operation ùëÜùë§ùëéùëù (¬∑) can be formulated as  where the edited face images ùêπ ‚Ä≤ ùëñ = { ùêπ ùëñ,ùëó } ùëÄ ùëñ ùëó = 1 are defined in (10). We measure the attractiveness enhancement of the edited faces ùêπ ‚Ä≤ ùëñ over the original faces ùêπ ùëñ using the difference in the predicted click rates of ùêπ ùëñ and ùêπ ‚Ä≤ ùëñ , i.e.,  where ÀÜ ùê∂ùëÖ ‚Ä≤ ùëñ is the predicted average click rate of the edited cover image ùêº ‚Ä≤ ùëñ , and ÀÜ ùê∂ùëÖ ùëñ is the predicted average click rate of the original cover image ùêº ùëñ defined in (8). Therefore, the enhancement of the attractiveness depends on the editing intensity coefficients ùõº ùëñ = { ùõº ùëñ,ùëó } ùëÄ ùëñ ùëó = 1 and the identified editing directions ùëõ . Genetic Algorithm . To maximize the attractiveness enhancement Œî ÀÜ ùê∂ùëÖ ùëñ defined in (12), we adopt the genetic algorithm to search for the optimal editing intensity coefficients ùõº ‚àó ùëñ for all the detected faces ùêπ ùëñ in cover image ùêº ùëñ . We selected the genetic algorithm to optimize the editing intensities because of its efficiency and effectiveness in a large search space. Alternatively, a gradient-based optimization approach will require backward passes through many components including the large generator model which is prohibitively expensive. Then, we generate the best-edited cover image ùêº ‚Ä≤‚àó ùëñ according to to (11). We summarize the searching procedure of the genetic algorithm in Algorithm 1. We set the editing intensity coefficients ùõº ùëñ for ùëéùëë ùëñ as the genotype in the genetic algorithm. Then, the fitness measurement ùõΩ ùëñ ( ùõº ùëñ ) for genotype ùõº ùëñ is set to be the predicted click rate ÀÜ ùê∂ùëÖ ‚Ä≤ ùëñ defined in (8). That is,",
  "Algorithm 1: Genetic Advertisement Editor (GADE)": "Input: Given ùëéùëë ùëñ = ( ùê∂ ùëñ , ùêº ùëñ , ùëá ùëñ ) ; Set of original face latent codes ùëß ùëñ = { ùëß ùëñ,ùëó } ùëÄ ùëñ ùëó = 1 ; Set of SeFa Edit directions: ùëõ = { ùëõ ùëù } ùëû ùëù = 1 . Parameters: ùëÉùëúùëùùë¢ùëôùëéùë°ùëñùëúùëõùëÜùëñùëßùëí , ùëÅùë¢ùëöùê∫ùëíùëõùëíùëüùëéùë°ùëñùëúùëõ , ùëÅùë¢ùëöùëÉùëéùëüùëíùëõùë°ùë† , ùëÉùëíùëüùëêùëíùëõùë°ùëÄùë¢ùë°ùëéùë°ùëñùëúùëõ . Genotype: ùõº ùëñ = { ùõº ùëñ,ùëö } ùëÄ ùëñ ùëö = 1 , ùõº ùëñ,ùëö ‚àà R ùëû . Fitness Function: Fitness measurement ùõΩ ( ùõº ùëñ ) defined in (13). Initialization: Generate the initial population ùëùùëúùëù 1 by randomly generating ùëÉùëúùëùùë¢ùëôùëéùë°ùëñùëúùëõùëÜùëñùëßùëí of genotypes. Generation Loop: for ‚àÄ ùëî ‚àà [ 1 , ùëÅùë¢ùëöùê∫ùëíùëõùëíùëüùëéùë°ùëñùëúùëõ ] do Fitness Evaluation: evaluate the fitness for each genotype in ùëùùëúùëù ùëî with ùõΩ (¬∑) . Parent Selection: use rank selection method to select ùëÅùë¢ùëöùëÉùëéùëüùëíùëõùë°ùë† parents from ùëùùëúùëù ùëî for mating. Crossover: apply the uniform crossover operation among the parents to create off-springs. Mutation: apply the random mutation operation to ùëÉùëíùëüùëêùëíùëõùë°ùëÄùë¢ùë°ùëéùë°ùëñùëúùëõ percent of off-spring genotypes. Survivor Selection: keep all ùëÅùë¢ùëöùëÉùëéùëüùëíùëõùë°ùë† parents, and keep at most ùëÉùëúùëùùë¢ùëôùëéùë°ùëñùëúùëõùëÜùëñùëßùëí -ùëÅùë¢ùëöùëÉùëéùëüùëíùëõùë°ùë† fit genotypes from the off-springs. All the kept genotypes are treated as the next population ùëùùëúùëù ùëî + 1. Output: The best genotype ùõº ‚àó ùëñ . where the Predictor (¬∑) is the CRP, and ùêº ‚Ä≤ ùëñ , defined in (11), is the edited cover image of ad ùêº ùëñ . Thus, guided with ùõΩ ùëñ (¬∑) , the genetic algorithm is supposed to search for the best genotype, i.e., editing intensity coefficients. At the initialization step, we create an initial population denoted as ùëùùëúùëù 1 by randomly generating PopulationSize number of genotypes, each with the same shape as ùõº ùëñ . Then, we repeat the generation loop ùëÅùë¢ùëöùê∫ùëíùëüùëíùëüùëéùë°ùëñùëúùëõ times. Each iteration consists of five steps including Fitness Evaluation, Parent Selection, Crossover, Mutation, and Survivor Selection . After ùëÅùë¢ùëöùê∫ùëíùëüùëíùëüùëéùë°ùëñùëúùëõ generations, we return the best genotype ùõº ‚àó ùëñ with the highest fitness value, which also results in the best improvement of the predicted click rate Œî ÀÜ ùê∂ùëÖ ‚àó ùëñ defined in (12). Finally, we use the best genotype ùõº ‚àó ùëñ to generate the best cover image ùêº ùëñ according to (11).",
  "4 EVALUATION": "In this section, we conduct extensive experiments to evaluate the effectiveness of the proposed click rate predictor and the AdSEE framework. We also perform offline and online analyses based on the introduced QQ-AD dataset to offer insights on the connection between style editing and possible click rate enhancement. Furthermore, we also evaluate AdSEE on the public CreativeRanking [55] dataset. Due to space constraints, we qualitatively evaluate AdSEE edited images by putting examples in the Appendix Section B.2. KDD '23, August 6-10, 2023, Long Beach, CA, USA. Liyao Jiang et al. Table 1: Statistics of the Collected QQ-AD Dataset.",
  "4.1 Datasets": "QQ-AD Dataset. To evaluate our proposed approach, we collected real advertisement data from the QQ Browser mobile app. Note that the common recommender model datasets such as Avazu [3] and Criteo [12] do not apply to our work because they do not contain any image. Each ùëéùëë record consists of its category information, cover image, and query text. In addition, we also collected the number of impressions, i.e., the number of times an ad is shown to an audience, and the number of clicks, i.e, the number of times that an ad was clicked by an audience. Shown in Table 1, we collected a total number of 158,829 ads from December 19, 2021, to January 18, 2022. As our goal is to enhance the attractiveness of ad images through facial feature editing, we remove ads that do not contain a face in their cover image. In addition, we also remove ads with more than m=5 faces in its cover image from the collected dataset to avoid extracting low-resolution and unrecognizable face images from the cover image of an ad . Finally, we have 20,527 ads with a valid number of faces in the collected QQ-AD dataset. That is, around 12.92% of the collected ads from the QQ Browser mobile environment contain 1-5 faces that can be enhanced with our AdSEE framework. The number of impressions and clicks for AdSEE applicable images in the QQ-AD dataset accounts for 19.12% and 19.48% of the total number of impressions and clicks, respectively. This suggests that an ad image with 1 to 5 faces is common in the QQ Browser mobile environment, and editing the facial features can potentially have a significant impact on the overall user clicks, impressions, and click rates. We randomly split the ads in QQ-AD dataset into three parts for training (64%), validation (16%), and testing (20%). CreativeRanking Dataset. We further evaluate AdSEE on the relevant public dataset CreativeRanking 3 published by Wang et al. [55]. We process the CreativeRanking dataset to be similar to our image enhancement task. Each row in CreativeRanking dataset contains an e-commerce image, a product name, a number of clicks, a number of shows, and a show date. We aggregate the total clicks and the total shows for the same product and the same image over different dates resulting in each row corresponding to an imageproduct pair and the corresponding total show, total click, and average click rate. Similar to the features used in Section 3, we use the same one-hot face count (from 0 to 5) and multi-hot class label as the sparse feature, and we use face latent code and image embedding as the dense feature. Differently, we replace the ùëéùëë category sparse feature with the product name index as a sparse feature and we do not use any text embedding feature since there is no text data in CreativateRanking. In this dataset, there can be different images that are for the same product so each row in our dataset 3 Dataset available at https://tianchi.aliyun.com/dataset/93585. is a product-image pair. We remove any product-image pair with less than 100 total impressions, with more than 1000 total impressions, or with 0 total clicks. This yields 267,362 product-images pairs which we split into three parts for training (60%), validation (20%), and testing (20%). We use the train set which contains both images with face and images with no face to train the CRP model. However, the GADE model should be applied to images containing faces, so we further filter any images with no faces or more than 5 faces resulting in a total of 23,713 valid images with a desirable number of faces in the entire dataset.",
  "4.2 Evaluation on QQ-AD and CreativeRanking": "In the offline evaluation, we first evaluate the proposed CRP model on the click rate prediction task and compare it against a wide range of baseline methods on both of the QQ-AD dataset and the CreativeRanking dataset. Then, we want to analyze whether style editing using the GADE module is linked to attractiveness and ùëéùëë popularity improvements. Thus, we edit the ads with the GADE module and evaluate the improvement of the attractiveness, measured by Œî ÀÜ ùê∂ùëÖ , of the edited ads through the CRP model on both of the QQ-AD dataset and the CreativeRanking dataset. Finally, we perform case studies to analyze the more attractive face editing directions on the QQ-AD dataset. Evaluation of the CRP model In this experiment, we compare the proposed CRP method with the following state-of-the-art baseline methods that use different features for average click rate prediction on the QQ-AD dataset. CRP-NIMA: This is the baseline method of [51] where the NIMA score mean and standard-deviation are used as dense features. CRP-OpenImage: Use the image embeddings obtained from the multi-label image classification model pre-trained on Open Image dataset [35] as dense features. CRPSogou: Use the image embeddings obtained from the Sogou model for searching pictures through text as dense features. CRP-e4e: Use the max-pooled face latent codes obtained from the pre-trained e4e FFHQ encoder [31, 52] model as dense features. The implementation details including hyperparameters, pretrained models, and environment are introduced in Appendix Section C. Note that, we train the proposed model and all the other baseline methods with the same MSE loss for a fair comparison. Moreover, the sparse features, i.e., ad category, multi-hot class label, and one-hot face count, are used in all methods. Furthermore, we adopt the mean absolute error (MAE), mean absolute percentage error (MAPE), normalized discounted cumulative gain (NDCG), Spearman's ùúå , and Kendall's ùúè to evaluate the performance of different models for the average click rate prediction task. Table 2 summarizes the performances of the proposed CRP model and all the baseline methods on the QQ-AD dataset. We can clearly see that our proposed CRP significantly outperforms all the other baselines on all the evaluation metrics. The superiority of the proposed method over other baselines can be attributed to the adoption of multi-modal dense features, i.e., face latent code, image embedding, and text embedding. Note that, the CRP-NIMA baseline method from [51], which uses image quality NIMA score as a dense feature, is the worst model in terms of NDCG@10 and NDCG@50 when compared against the rest of the methods where image embedding and face latent code is adopted as the dense feature. This, AdSEE: Investigating the Impact of Image Style Editing on Advertisement Attractiveness KDD '23, August 6-10, 2023, Long Beach, CA, USA. Table 2: Comparing the proposed CRP predictor with other baselines using different types of features on the QQ-AD dataset. Table 3: Comparing the proposed feature combination C5 with other combinations on the CreativeRanking [55] dataset. We consider features including Face Count (FC), Product Name (PN), Class Label (CL), Face Latents (FL), and Image Embedding (IE). (a) Distribution of Œî ÀÜ ùê∂ùëÖ in the offline test on the QQ-AD dataset. 0.025 0,050 0.075 0.100 0.125 0.150 0.175 0.200 Predicted Average Click Rate Difference ACR 0.007 0.006 1 0.005 0.004 0.003 g 002 0.001 Allowed Edit Direction 0.0012,0 = 0.0308 0.00025 0,00o5o 0,00075 00100 0.00125 0,00150 0,00175 00200 Predicted Average Click Rate Difference ACR (b) Distribution of Œî ÀÜ ùê∂ùëÖ for 10 different edit directions in the offline test on the QQ-AD dataset. (c) Distribution of Œî ÀÜ ùê∂ùëÖ in the offline test on the CreativeRanking [55] dataset. Figure 2: Analysis of predicted average click rate difference Œî ÀÜ ùê∂ùëÖ in the offline evaluations. again, demonstrates the importance of our extracted dense features for the accurate click rate prediction of an ad and the correlation between image style and ad popularity. Table 3 summarizes the performances of the proposed CRP model and all the baseline methods on the CreativeRanking [55] dataset. We train our CRP predictor on the preprocessed CreativeRanking dataset with MSE loss. We call the proposed combined set of features C5, which includes sparse features one-hot face count, one-hot product name, and multi-hot class label. In addition, C5 includes dense features face latent code, and image embedding. First, we compare the performance of different feature combinations on the CreativeRanking dataset and we found our proposed combined set of features (C5) outperforms all other feature combinations on 5 out of 6 metrics. All instances use the AutoInt model and share the same training settings for a fair comparison. Results on both QQ-AD and CreativeRanking demonstrate the benefits of the multi-modal features we proposed to use. Specifically, using a combination of face latent vectors, image embeddings, and text embeddings can achieve better performance than the baseline features. Evaluation of the GADE model . In this experiment, we want to answer the question: does editing facial styles of an ùëéùëë using our AdSEE model improve the attractiveness of an ad ? An edited ad and its corresponding original ad will form an evaluation pair. In Figure 2(a), we can observe that the values of the Œî ÀÜ ùê∂ùëÖ are positive for all the evaluation pairs in the test set of QQ-AD dataset. This shows that facial style editing do improve the attractiveness of an ùëéùëë through using our AdSEE framework. Furthermore, the Œî ÀÜ ùê∂ùëÖ has a mean of 0.049 and is right skewed which means that most of the samples have a relatively small positive increase in the predicted click rate, i.e., ÀÜ ùê∂ùëÖ , after being edited by the AdSEE model. Whereas, a few ads have a large increase in the predicted click rate. This is reasonable because most of the cover images of the ads are already well-designed and have decent attractiveness. In addition, we use the GADE module together with the CRP predictor to optimize a random sample of 500 images from the CreativeRanking [55] dataset test set (keeping images with 1 to 5 faces). We summarize the predicted average click rate difference Œî ÀÜ ùê∂ùëÖ in Figure 2(c). We observe the Œî ÀÜ ùê∂ùëÖ for all 500 test images are all KDD '23, August 6-10, 2023, Long Beach, CA, USA. Liyao Jiang et al. positive and have a mean of 0 . 0012 which is a 3.9% increase relative to the 0.0308 mean ùê∂ùëÖ of these test images. This demonstrates that our method can enhance image attractiveness when applied to other image recommendation scenarios like e-commerce besides our own advertisement QQ-AD dataset. This shows AdSEE can be used to extract knowledge and can enhance image attractiveness by facial image style editing. Moreover, the results show the existence of a correlation between image style editing and click rates in ùëéùëëùë† . Semantic Editing Directions. To figure out the most important semantic editing directions that improve the attractiveness of a cover image the most, we sample 1000 images from the QQ-AD test set and run AdSEE 10 times. In each run, we allow editing in only one out of the top ten directions discovered by SeFa [50]. In Figure 2(b), we observe that editing on directions ùëõ 4, ùëõ 7, ùëõ 1 results in the largest increase in ÀÜ ùê∂ùëÖ . That is, these directions have the largest impact on the attractiveness of an ad among the other editing directions. We further analyze the details on semantic editing directions in Figure 6 of the Appendix Section B.1.",
  "4.3 Online A/B Tests": "We further report the results of an online A/B test, by comparing 250 ad images edited and altered by the AdSEE model as well as the 250 original ad images tested over the QQ Browser mobile app users in a 5-day period. All of the 250 original images and the corresponding 250 edited images contain faces. These ùëéùëëùë† fall into 19 categories (genres), including photography, sports, fashion, show, game, TV, movie, education, science, culture, food, life, comic, inspiration, other, pics, folk arts, novel, and career. The online A/B tests were performed over a period of 5 days from Feb 5th, 2022 to Feb 9th, 2022, where we collected the number of impressions and clicks, and click rates to compare AdSEE with the control group of unaltered images. Recall that an impression refers to the event when an ad is shown/exposed to a user by the online advertising system and that click rate equals to the number of clicks divided by the number of impressions. The result shows that images edited by facial style editing with AdSEE received a significant increase in attractiveness compared to the original images in every metric. When performing online split tests, the AdSEE images and the original images were uploaded at the same time and presented on QQ Browser. After 5 days, we collected the number of impressions and clicks, and conducted the following comparisons. Figure 3 shows that AdSEE images were presented a larger number of times and showed a higher click rate hence attracting more clicks on ads each day separately. By conducting Paired Sample T-Test, we validated that the experiment group was significantly better than the control group. A larger number of impressions indicate that AdSEE-enhanced images are recommended more times by the recommender model, which means the independent production recommendation model that is not trained on AdSEE-edited images \"believes\" AdSEE-edited images are more attractive to users and may lead to increase in click rates. In addition, a higher click rate indicates better attractiveness to users. Figure 4 shows the difference in performance in each of the 19 categories, we largely improved the popularity and attractiveness of images in the photograph category and sports category in terms of every metric. Figure 5 shows the cumulative frequency distribution of the number of impressions, click rate, and the number of clicks, the results indicate that AdSEE images outperformed the control group in different bins of images.",
  "5 LIMITATIONS AND DISCUSSION": "This work represents one of the first efforts to explore the potential impact of art and image synthesis on recommender systems. Specifically, we aim to investigate if there is a linkage/correlation between popularity and image styles through a data-science approach, which we believe is a valuable question to ask for the AI community as well as AI ethics community. We verified the existence of this linkage with both offline experiments and online A/B testing. However, we do not aim to commercialize AdSEE as a traffic booster at the moment. In addition, any exploitation of the research results for commercial use is subject to further consideration of ethical requirements and regulations. Asimilar case also applies to recent advancements in content generation like image generation models StyleGAN3 [30], DALLE ¬∑ 2 [46], and Imagic [33], which are widely popular in AI research because they can automatically generate state-of-the-art synthetic images that may match the quality of real images created by cameras and human artists. Meanwhile, we recognize that the nature of synthetic image generation tasks inherently brings risks to areas such as information objectivity, misleading information, copyright, data privacy, data fairness, etc. Therefore, we believe it is crucial that any research in the image generation area should be performed with broader societal and ethical impacts in mind. Wehold copyright protection, data privacy, information objectivity, user consent, and right-to-correct as our core ethical values. The potential ethical issues are related to the specific application context and we adopt a series of ethical protection measures throughout the design, development, and evaluation of AdSEE. During the collection of our QQ-AD dataset, we check the copyright licenses for each image and only select images with appropriate licenses that allow commercial use and free modification. We do not publish our QQ-AD dataset to ensure that copyright licenses are not violated and data privacy is protected. As a normal process, the platform advertisement censoring team censors every image on the platform for legal compliance and ethical control, which also includes all original and edited images used in the online experiments. The users participating in the online experiments are beta testers and internal employees who provided consent to opt into the beta testing program. The users have the option to provide feedback or opt out of the program at any time.",
  "6 CONCLUSION": "We present the AdSEE system which aims at finding out whether online advertisement visibility and attractiveness can be affected by semantic edits to human facial features in the ad cover images. Specifically, we design a CRP module to predict the click rate of an ad based on the face latent embeddings offered by a StyleGANbased encoder in addition to traditional image and textual embeddings. We further design a GADE module to efficiently search for optimal editing directions and coefficients using a genetic algorithm. Based on analyzing the introduced QQ-AD dataset, we identify semantic edit directions that are key to popularity enhancement. From AdSEE: Investigating the Impact of Image Style Editing on Advertisement Attractiveness KDD '23, August 6-10, 2023, Long Beach, CA, USA. (a) Click Rate per day (p-value 3 . 68 √ó 10 - 5 ) control 0.10 0.08 0.06 0.04 Day Day Day Day Day (b) Number of clicks per day (p-value 1 . 86 √ó 10 - 2 ) 14000 Control 12000 10000 8000 6000 1 4000 2000 Day Day 2 Day 3 Day 4 Day 120000 100000 1 80000 60000 1 40000 20000 Day Day Day Day Day (c) Number of impressions per day (p-value 2 . 11 √ó 10 - 2 ) Figure 3: Comparison between AdSEE and control group in terms of the number of impressions, clicks and click rates. (a) Click Rate Difference 0.04 0.02 0.00 0.04 1 8 9 \" 1 Category (b) The difference of number of impressions 1 150000 125000 100000 75000 50000 25000 1 9 1 8 Category (c) The difference of number of clicks ' 20000 15000 ' 5000 1 3 3 8 58 9 8 Category Figure 4: The difference (increase) after applying AdSEE to each category of ads in terms of the click rate, number of impressions, and number of clicks. (a) CDF of the click rate 1.0 method AdSEE Control 0.8 1 0.6 1 8 0.2 00 0.05 0.10 0.15 0.20 Click Through Rate 1.0 method AdSEE Control 0.8 1 0.6 0.4 1 0 0.2 0.0 10 The Logarithm of the Number of Clicks 1.0 method AdSEE Control 0.8 0.6 0.4 1 0.2 0.0 10 12 The Logarithm of the Number of Impressions (b) CDF of the logarithm of the number of clicks (c) CDFofthelogarithm of the number of impressions Figure 5: The cumulative frequency distribution of click rates, the logarithm of the number of impressions, and the logarithm of the number of clicks. the analysis, we observe that a face oriented slightly downward, a smiling face, and a face with more feminine features are more attractive to users. Evaluation results on two offline datasets and online A/B tests demonstrate the existence of correlation between style editing and click rates in online ads . KDD '23, August 6-10, 2023, Long Beach, CA, USA. Liyao Jiang et al.",
  "REFERENCES": "[1] Paszke Adam, Gross Sam, Chintala Soumith, and Chanan Gregory. 2017. Pytorch:Tensors and dynamic neural networks in python with strong gpu acceleration. https://pytorch.org/. [2] Safaa Adil, Sophie Lacoste-Badie, and Olivier Droulers. 2018. Face presence and gaze direction in print advertisements: How they influence consumer responses-An eye-tracking study. Journal of Advertising Research 58, 4 (2018), 443-455. [3] Avazu. 2015. The Avazu Dataset. https://www.kaggle.com/c/avazu-ctrprediction [4] Javad Azimi, Ruofei Zhang, Yang Zhou, Vidhya Navalpakkam, Jianchang Mao, and Xiaoli Fern. 2012. The impact of visual appearance on user response in online display advertising. In proceedings of the 21st international conference on World Wide Web . 457-458. [5] G. Bradski. 2000. The OpenCV Library. Dr. Dobb's Journal of Software Tools (2000). [6] Andrew Brock, Jeff Donahue, and Karen Simonyan. 2018. Large scale GAN training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096 (2018). [7] Junxuan Chen, Baigui Sun, Hao Li, Hongtao Lu, and Xian-Sheng Hua. 2016. Deep ctr prediction in display advertising. In Proceedings of the 24th ACM international conference on Multimedia . 811-820. [8] Jin Chen, Ju Xu, Gangwei Jiang, Tiezheng Ge, Zhiqiang Zhang, Defu Lian, and Kai Zheng. 2021. Automated Creative Optimization for E-Commerce Advertising. In Proceedings of the Web Conference 2021 . 2304-2313. [9] Haibin Cheng, Roelof van Zwol, Javad Azimi, Eren Manavoglu, Ruofei Zhang, Yang Zhou, and Vidhya Navalpakkam. 2012. Multimedia features for click prediction of new ads in display advertising. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining . 777-785. [10] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems . 7-10. [11] Weiyu Cheng, Yanyan Shen, and Linpeng Huang. 2020. Adaptive factorization network: Learning adaptive-order feature interactions. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 34. 3609-3616. [12] Criteo. 2014. The Criteo Dataset. https://www.kaggle.com/competitions/criteodisplay-ad-challenge/overview [13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018). [14] Ricard Durall, Jireh Jam, Dominik Strassel, Moi Hoon Yap, and Janis Keuper. 2021. FacialGAN: Style Transfer and Attribute Manipulation on Synthetic Faces. arXiv preprint arXiv:2110.09425 (2021). [15] Kun Gai, Xiaoqiang Zhu, Han Li, Kai Liu, and Zhe Wang. 2017. Learning piecewise linear models from large scale data for ad click prediction. arXiv preprint arXiv:1704.05194 (2017). [16] Leon A Gatys, Alexander S Ecker, and Matthias Bethge. 2016. Image style transfer using convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition . 2414-2423. [17] Estev√£o S Gedraite and Murielle Hadad. 2011. Investigation on the effect of a Gaussian Blur in image filtering and segmentation. In Proceedings ELMAR-2011 . IEEE, 393-396. [18] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. Advances in neural information processing systems 27 (2014). [19] Chunhui Gu, Chen Sun, David A Ross, Carl Vondrick, Caroline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan, George Toderici, Susanna Ricco, Rahul Sukthankar, et al. 2018. Ava: A video dataset of spatio-temporally localized atomic visual actions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 6047-6056. [20] Gianluigi Guido, Marco Pichierri, Giovanni Pino, and Rajan Nataraajan. 2019. Effects of face images and face pareidolia on consumers' responses to print advertising: an empirical investigation. Journal of Advertising Research 59, 2 (2019), 219-231. [21] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: A Factorization-Machine based Neural Network for CTR Prediction. 1725-1731. https://doi.org/10.24963/ijcai.2017/239 [22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition . 770-778. [23] Ruining He and Julian McAuley. 2016. VBPR: visual bayesian personalized ranking from implicit feedback. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 30. [24] Xiangnan He and Tat-Seng Chua. 2017. Neural factorization machines for sparse predictive analytics. In Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval . 355-364. [25] Daniel N Hill, Houssam Nassif, Yi Liu, Anand Iyer, and SVN Vishwanathan. 2017. An efficient bandit algorithm for realtime multivariate optimization. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . 1813-1821. [26] Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. 2019. FiBiNET: combining feature importance and bilinear feature interaction for click-through rate prediction. In Proceedings of the 13th ACM Conference on Recommender Systems . 169-177. [27] Xun Huang and Serge Belongie. 2017. Arbitrary style transfer in real-time with adaptive instance normalization. In Proceedings of the IEEE international conference on computer vision . 1501-1510. [28] Erik H√§rk√∂nen, Aaron Hertzmann, Jaakko Lehtinen, and Sylvain Paris. 2020. GANSpace: Discovering Interpretable GAN Controls. In Proc. NeurIPS . [29] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. 2017. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196 (2017). [30] Tero Karras, Miika Aittala, Samuli Laine, Erik H√§rk√∂nen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. 2021. Alias-Free Generative Adversarial Networks. In Proc. NeurIPS . [31] Tero Karras, Samuli Laine, and Timo Aila. 2019. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 4401-4410. [32] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. 2020. Analyzing and Improving the Image Quality of StyleGAN. In Proc. CVPR . [33] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. 2022. Imagic: Text-based real image editing with diffusion models. arXiv preprint arXiv:2210.09276 (2022). [34] Davis E. King. 2009. Dlib-ml: A Machine Learning Toolkit. Journal of Machine Learning Research 10 (2009), 1755-1758. [35] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, Tom Duerig, and Vittorio Ferrari. 2020. The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale. IJCV (2020). [36] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. 2010. A contextualbandit approach to personalized news article recommendation. In Proceedings of the 19th international conference on World wide web . 661-670. [37] Xiang Li, Chao Wang, Jiwei Tan, Xiaoyi Zeng, Dan Ou, Dan Ou, and Bo Zheng. 2020. Adversarial multimodal representation learning for click-through rate prediction. In Proceedings of The Web Conference 2020 . 827-836. [38] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong Sun. 2018. xdeepfm: Combining explicit and implicit feature interactions for recommender systems. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1754-1763. [39] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r, and C Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In European conference on computer vision . Springer, 740-755. [40] Hu Liu, Jing Lu, Hao Yang, Xiwei Zhao, Sulong Xu, Hao Peng, Zehua Zhang, Wenjie Niu, Xiaokun Zhu, Yongjun Bao, et al. 2020. Category-Specific CNN for Visual-aware CTR Prediction at JD. com. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 2686-2696. [41] Qiang Liu, Shu Wu, and Liang Wang. 2017. Deepstyle: Learning user preferences for visual recommendation. In Proceedings of the 40th international acm sigir conference on research and development in information retrieval . 841-844. [42] Qiang Liu, Feng Yu, Shu Wu, and Liang Wang. 2015. A convolutional click prediction model. In Proceedings of the 24th ACM international on conference on information and knowledge management . 1743-1746. [43] Wantong Lu, Yantao Yu, Yongzhe Chang, Zhen Wang, Chenhui Li, and Bo Yuan. 2021. A dual input-aware factorization machine for CTR prediction. In Proceedings of the twenty-ninth international conference on international joint conferences on artificial intelligence . 3139-3145. [44] Sepideh Nasiri, Negar Sammaknejad, and Mohamad Ali Sabetghadam. 2020. The effect of human face and gaze direction in advertising. International Journal of Business Forecasting and Marketing Intelligence 6, 3 (2020), 221-237. [45] Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, and Jun Wang. 2016. Product-based neural networks for user response prediction. In 2016 IEEE 16th international conference on data mining (ICDM) . IEEE, 1149-1154. [46] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125 (2022). [47] Steffen Rendle. 2010. Factorization machines. In 2010 IEEE International conference on data mining . IEEE, 995-1000. [48] Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or. 2021. Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . [49] Weichen Shen. 2017. DeepCTR: Easy-to-use,Modular and Extendible package of deep-learning based CTR models. https://github.com/shenweichen/deepctr. AdSEE: Investigating the Impact of Image Style Editing on Advertisement Attractiveness KDD '23, August 6-10, 2023, Long Beach, CA, USA. [50] Yujun Shen and Bolei Zhou. 2021. Closed-Form Factorization of Latent Semantics in GANs. In CVPR . [51] Hossein Talebi and Peyman Milanfar. 2018. NIMA: Neural image assessment. IEEE transactions on image processing 27, 8 (2018), 3998-4011. [52] Omer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, and Daniel Cohen-Or. 2021. Designing an encoder for stylegan image manipulation. ACM Transactions on Graphics (TOG) 40, 4 (2021), 1-14. [53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017). [54] Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong, and Ed Chi. 2021. Dcn v2: Improved deep & cross network and practical lessons for web-scale learning to rank systems. In Proceedings of the web conference 2021 . 1785-1797. [55] Shiyao Wang, Qi Liu, Tiezheng Ge, Defu Lian, and Zhiqiang Zhang. 2021. A Hybrid Bandit Model with Visual Priors for Creative Ranking in Display Advertising. In Proceedings of the 30th international conference on World wide web . [56] Xinfei Wang. 2020. A Survey of Online Advertising Click-Through Rate Prediction Models. In 2020 IEEE International Conference on Information Technology,Big Data and Artificial Intelligence (ICIBA) , Vol. 1. 516-521. https://doi.org/10.1109/ ICIBA50161.2020.9277337 [57] Xinlong Wang, Tao Kong, Chunhua Shen, Yuning Jiang, and Lei Li. 2020. SOLO: Segmenting Objects by Locations. In Proc. Eur. Conf. Computer Vision (ECCV) . [58] Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. 2020. SOLOv2: Dynamic and Fast Instance Segmentation. Proc. Advances in Neural Information Processing Systems (NeurIPS) (2020). [59] Yue Wang, Dawei Yin, Luo Jie, Pengyuan Wang, Makoto Yamada, Yi Chang, and Qiaozhu Mei. 2016. Beyond ranking: Optimizing whole-page presentation. In Proceedings of the Ninth ACM International Conference on Web Search and Data Mining . 103-112. [60] Song Weiping, Shi Chence, Xiao Zhiping, Duan Zhijian, Xu Yewen, Zhang Ming, and Tang Jian. 2018. AutoInt: Automatic Feature Interaction Learning via SelfAttentive Neural Networks. arXiv preprint arXiv:1810.11921 (2018). [61] Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, and MingHsuan Yang. 2021. GAN inversion: A survey. arXiv preprint arXiv:2101.05278 (2021). [62] Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua. 2017. Attentional factorization machines: Learning the weight of feature interactions via attention networks. arXiv preprint arXiv:1708.04617 (2017). [63] Xiao Yang, Tao Deng, Weihan Tan, Xutian Tao, Junwei Zhang, Shouke Qin, and Zongyao Ding. 2019. Learning compositional, visual and relational representations for CTR prediction in sponsored search. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management . 2851-2859. [64] Yi Yang, Baile Xu, Shaofeng Shen, Furao Shen, and Jian Zhao. 2020. Operationaware neural networks for user response prediction. Neural Networks 121 (2020), 161-168. [65] Yantao Yu, Zhen Wang, and Bo Yuan. 2019. An Input-aware Factorization Machine for Sparse Prediction.. In IJCAI . 1466-1472. [66] Zhichen Zhao, Lei Li, Bowen Zhang, Meng Wang, Yuning Jiang, Li Xu, Fengkun Wang, and Weiying Ma. 2019. What You Look Matters? Offline Evaluation of Advertising Creatives for Cold-start Problem. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management . 2605-2613. KDD '23, August 6-10, 2023, Long Beach, CA, USA. Liyao Jiang et al.",
  "A BASE RECOMMENDER MODELS": "To select the best-performing base recommender model for our task, wecompare the performances among many SOTA models using our proposed set of features and on our dataset. For these experiments, we use the same set of features and the same experiment settings for each base recommender model for a fair comparison. Webriefly introduce the most important base recommender models in our comparison due to the vast amount of models compared. Rendle propose the Factorization Machine (FM) [47] model to learn the first- and second-order interactions of features. To model the interactions of both the sparse and dense features, Wide & Deep [10] uses DNN to extract dense features and adopts a Logistic Regression (LR) model to learn the interactions between the dense and the sparse features. However, the Wide & Deep [10] model requires manual feature engineering for the sparse features, which needs domain expertise. To alleviate this downside, Guo et al. propose the DeepFM [21] model to learn the first-order and high-order interactions automatically with an FM module and a DNN module, respectively. Recently, the AutoInt [60] model was proposed which utilizes state-of-the-art deep learning techniques including attention mechanism [53], and residual connections [22] to learn both the first-order and high-order interactions automatically. Table 4 summarizes the performances of the different base recommender models on the QQ-AD dataset. Each model shares the same set of features described in Section 3.2 to ensure a fair comparison, i.e. ad category, multi-hot class label, one-hot face count, latent face representation, cover image embedding, and text embedding. We can clearly see that AutoInt [60] model significantly outperforms all the other baselines on all the evaluation metrics. The high performance of the AutoInt base recommender model is likely due to its adoption of a powerful multi-head self-attentive neural network with residual connections to model both the loworder and high-order feature interactions. These experiments also demonstrate that our method and features used for CR prediction are scalable to many models of various sizes and different designs. Furthermore, we repeat this base recommend model comparison on the CreativeRanking dataset and found the AutoInt model also provides the most robust performance among the compared models.",
  "B QUALITATIVE STUDY": "",
  "B.1 Semantic Editing Directions": "In Figure 2(b) from Section 4, we observe that editing on directions ùëõ 4, ùëõ 7, ùëõ 1 results in the largest increase in ÀÜ ùê∂ùëÖ . That is, these directions have the largest impact on the attractiveness of an ad among the other editing directions. We further analyze the semantics of the editing directions in Figure 6. We visualize each editing direction by generating images from a range of editing intensity coefficients. Each row in the figure corresponds to one editing direction, and each column corresponds to a particular editing intensity value in the range of -5 , -2 . 5 , 0 , 2 . 5 , 5 from left to right. We can see that for direction ùëõ 4, which corresponds to the vertical orientation of the face, the best average editing coefficient value found by the AdSEE model is -2.77 which means a face slightly facing downward is found to be more attractive. Similarly, for direction ùëõ 7, which corresponds to the gender of the face, the best average editing coefficient value found by the AdSEE model is 2.26 which means a face with Table 4: Comparing the CRP predictor with our proposed combined set of features on different base recommender models on the QQ-AD dataset. more feminine features is more attractive. With editing direction ùëõ 1, which corresponds to the smilingness of the face, the best average editing coefficient value found by AdSEE is -2.63 which shows that a person with a smiling face is more attractive.",
  "B.2 AdSEE Edited Images": "In this section, we qualitatively evaluate AdSEE by showing examples of AdSEE-enhanced images. Figure 7 shows some examples of the enhanced ads by the AdSEE model. The two examples are from two different ad categories, i.e., others and sports. Nevertheless, the AdSEE model consistently chooses to enhance the attractiveness of the face by making it smile. In addition, the eyes in Examples 1 and 2 are edited to look downwards. These observations match our analysis of the average editing coefficient, where smilingness and vertical face orientation are attractive editing directions.",
  "C REPRODUCIBILITY": "Environment. We open source the implementation of AdSEE 4 so our method can be easily studied, reproduced, and extended. For all the experiments, we implement our model with PyTorch 1.7.0 [1] in Python 3.7.16 environment and train on a Tesla P40 GPU with a memory size of 24 GB. We also try our system on an RTX 2080Ti GPU with 11GB of memory, which can still handle our entire AdSEE system efficiently when tuning down the batch size hyperparameters. We provide the virtual environment and dependency setup script in our code repository for reproducibility. Pre-trained Models. Besides the important e4e [52] encoder model, the StyleGAN2-FFHQ [32] generator, and the SOLO instance segmentation model [57, 58] described in Section 3, we enumerate all the pre-trained models in our system. We adopt the pretrained Bert-Chinese [13] model to extract the 768-dimensional text embedding of the ad query texts as a dense feature. Within the 4 Code available at https://github.com/LiyaoJiang1998/adsee. AdSEE: Investigating the Impact of Image Style Editing on Advertisement Attractiveness KDD '23, August 6-10, 2023, Long Beach, CA, USA. Dircction Best Coefficent: -2.63 Direction 2 Best Coefficent: 1.19 Direction 3 Best Coefficent: -1.43 Direction Best Coefficent: Direction 5 Best Cocfficent: 1.77 Direction Best Coefficent: 2.00 Direction Best Coefficent: 2.69 Direction 8 Best Cocfficent: 2.26 Direction 9 Best Cocfficent: 2.06 Direction Coefficent: 10 Best 1.20 Figure 6: Case study analysis of edit directions 1 to 10.",
  "Example 1": "ad text: 'What does a dimple on the forehead mean? ' Category: other",
  "Example 2": "ad text: 'How many steps can you think ahead in Go?' Category: sports Example 3 ad text: 'The five most popular dramas' Figure 7: Examples of ùëéùëë s enhanced by AdSEE where we show the ùëéùëë category, text, and cover image. Left: Original cover image, Right: Enhanced cover image. Category: TV Face Segmentation Module, we utilize the Dlib [34] face alignment model to extract aligned human faces. As for the Image Embedding Model, we use the Tencent internal Sougou Image Embedding Model and multi-label classification model described in Section 3 for experiments on QQ-AD dataset. In addition, we adopt the publicly available ResNet-18 [22] model as the image embedding model for experiments on the public CreativeRanking [55] dataset. For the CRP-NIMA baseline model, we use the NIMA image quality assessment model [51] pre-trained on the AVA [19] dataset to obtain the ad cover image quality score mean and standard-deviation for all cover images in the QQ-AD dataset. Hyper-parameters and Implementation Details. For all of the base recommender models including AutoInt, we adopt the implementations from the DeepCTR [49] library and use the default hyperparameters of each model. For both of the QQ-AD and CreativeRanking [55], we use the train set to train the model and use the validation set to tune the hyper-parameters, select features and determine early stop, and evaluate the performance on the test set. For the CRP model training on both datasets, we find a learning rate of 1 ùëí -4 performs well and we use a batch size of 256. For the CRP model trained on the QQ-AD dataset and CreativeRanking dataset, we train them for 37 epochs and 18 epochs respectively. For the GADE module, we use the following settings for experiments on the QQ-AD dataset. In Algorithm 1, we set the PopulationSize to 75 and set the NumGenerations to 20. In the Parent Selection step, we select 10 genotypes as parents by performing the rank selection method. In the Crossover step, the parents in the mating pool will create 65 off-springs using the uniform crossover operation. Then, the 10 parents are combined with the 65 offspring to form a new population of 75 genotypes. Next, we randomly select 20% of the 75 genotypes to mutate in the Mutation step. For each genotype selected for mutation, we randomly change one of its genes by perturbing its value by a value in the range of [-0 . 1 , 0 . 1 ] . The search space for each gene value is limited to a value between the range of [-3 , 3 ] with a step of 0.1. The gene values are randomly initialized to a value in the range of [-1 , 1 ] . In each genotype, we have 20 genes that correspond to the top 20 editing directions found by SeFa. On the CreativeRanking [55] dataset, we use a slightly different set of settings for the GADE module that is suitable for this dataset. We use a PopulationSize of 30 and set the NumGenerations to 5. In the Parent Selection step, we select 10 genotypes as parents by performing the rank selection method. In the Crossover step, the parents in the mating pool will create 20 off-springs using the uniform crossover operation. Then, the 10 parents are combined with the 20 offspring to form a new population of 30 genotypes. Next, we randomly select 20% of the 30 genotypes to mutate in the Mutation step. For each genotype selected for mutation, we randomly change one of its genes by perturbing its value by a value in the range of [-0 . 01 , 0 . 01 ] . The search space for each gene value is limited to a value between the range of [-1 . 5 , 1 . 5 ] with a step of 0.01. The gene values are randomly initialized to a value in the range of [-0 . 1 , 0 . 1 ] . In each genotype, we have 20 genes that correspond to the top 20 editing directions found by SeFa. For the operation used to convert face latent codes to a fixed length, we compare four operations including max-pooling, averagepooling, aggregation and concatenation with padding to a fixed length. We found max-pooling to be the best performer on both datasets and use the max-pooling operation throughout our experiments. In the implementation, we apply standardization to the click rate label and we predict the standardized click rate. Even more detailed implementation-related settings and their values can be found in the code.",
  "keywords_parsed": [
    "Online advertisements are important elements in e-commerce sites",
    " social media platforms",
    " and search engines. With the increasing popularity of mobile browsing",
    " many online ads are displayed with visual information in the form of a cover image in addition to text descriptions to grab the attention of users. Various recent studies have focused on predicting the click rates of online advertisements aware of visual features or composing optimal advertisement elements to enhance visibility. In this paper",
    " we propose Advertisement Style Editing and Attractiveness Enhancement (AdSEE)",
    " which explores whether semantic editing to ads images can affect or alter the popularity of online advertisements. We introduce StyleGANbased facial semantic editing and inversion to ads images and train a click rate predictor attributing GAN-based face latent representations in addition to traditional visual and textual features to click rates. Through a large collected dataset named QQ-AD",
    " containing 20",
    "527 online ads",
    " we perform extensive offline tests to study how different semantic directions and their edit coefficients may impact click rates. We further design a Genetic Advertisement Editor to efficiently search for the optimal edit directions and intensity given an input ad cover image to enhance its projected click rates. Online A/B tests performed over a period of 5 days have verified the increased click-through rates of AdSEE-edited samples as compared to a control group of original ads",
    " verifying the relation between image styles and ad popularity. We open source the code for AdSEE research at https://github.com/LiyaoJiang1998/adsee."
  ]
}