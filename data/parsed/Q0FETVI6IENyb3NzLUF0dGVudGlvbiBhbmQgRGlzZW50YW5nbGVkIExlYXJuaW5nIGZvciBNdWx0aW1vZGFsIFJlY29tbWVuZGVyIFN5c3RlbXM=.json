{"title": "CADMR: Cross-Attention and Disentangled Learning for Multimodal Recommender Systems", "authors": "Yasser Khalafaoui; Martino Lovisetto; Basarab Matei; Nistor Grozavu", "pub_date": "2024-12-03", "abstract": "The increasing availability and diversity of multimodal data in recommender systems offer new avenues for enhancing recommendation accuracy and user satisfaction. However, these systems must contend with high-dimensional, sparse user-item rating matrices, where reconstructing the matrix with only small subsets of preferred items for each user poses a significant challenge. To address this, we propose CADMR, a novel autoencoder-based multimodal recommender system framework. CADMR leverages multi-head cross-attention mechanisms and Disentangled Learning to effectively integrate and utilize heterogeneous multimodal data in reconstructing the rating matrix. Our approach first disentangles modality-specific features while preserving their interdependence, thereby learning a joint latent representation. The multi-head cross-attention mechanism is then applied to enhance user-item interaction representations with respect to the learned multimodal item latent representations. We evaluate CADMR on three benchmark datasets, demonstrating significant performance improvements over state-of-the-art methods.", "sections": [{"heading": "Introduction", "text": "Recommender systems have become a cornerstone of modern digital experiences, playing a critical role in various domains such as streaming services and social media. By filtering and presenting relevant content from an overwhelming array of options, these systems enhance user satisfaction and engagement. Recommender systems can be classified into three main groups: content-based, collaborative filtering (CF), and hybrid algorithms [? ]. Content-based filtering uses item features to recommend items similar to those a user has liked based on previous actions or explicit feedback. This approach assumes that users are likely to choose items with features similar to those they have previously liked. However, this method struggles when recommending new items with novel features. On the other hand, collaborative filtering predicts a user's interests based on their historical behavior and the preferences of a large number of other users [1]. Hybrid approaches combine techniques from both collaborative filtering and content-based filtering to achieve more accurate results [2].\nCF remains one of the most widely utilized approaches for recommendation tasks. However, it is limited by the sparsity of the user-item rating matrix and the cold start problem, where new users lack historical behavior data. Various approaches have been proposed to overcome these limitations. Some methods [3,4] leverage dimensionality reduction and clustering to mitigate the effects of sparsity. These approaches aim to use a low-dimensional latent representation of the rating matrix instead of the original sparse matrix to estimate user-item interactions. Common approaches include Matrix Factorization and Singular Value Decomposition [? ]. Other methods employ autoencoders (AE) to learn the low-dimensional feature space of a given sparse rating matrix. AEs are particularly useful for capturing non-linear relationships and encoding complex features into a compact latent representation. For instance, I-AutoRec [5] utilizes an item-based AE to project high-dimensional matrix entries into a low-dimensional latent hidden space, reconstructing the entries in the output space to predict missing ratings. GLocal-K [1] presents a two-stage framework that pre-trains the autoencoder using a local kernel matrix and then fine-tunes the model using a global convolution kernel. However, these approaches primarily focus on the rating matrix and often overlook the potential of incorporating side information.\nRecently, multimodal-based recommender systems have gained attention, leveraging diverse data sources to improve recommendations. Zhang et al. [6] propose a method that uses a Multi-Layer Perceptron (MLP) to project multimodal data into a unified latent space, with the learned features transferred to user and item embeddings, which are optimized using a reconstruction loss. Similarly, Zheng et al. [7] introduce DeepCoNN, where text review embeddings are fed into two parallel CNN architectures to learn both user and item representations. Despite these advances, existing multimodal approaches typically learn user and item embeddings separately, neglecting the crucial relationship between the user-item rating matrix and the various data modalities. This oversight is significant because user-item interactions are inherently influenced by multiple factors present in multimodal data, which should be integrated into the recommendation process. However, current methods fail to leverage these rich interactions, potentially limiting their effectiveness in capturing the full complexity of user preferences.\nTo address the limitations of existing approaches, this paper introduces the framework CADMR (Cross-Attention and Disentangled Learning for Multimodal Recommender Systems). It begins with a pretraining phase where modality-specific features are disentangled and projected into a shared latent space. Simultaneously, the user-item rating matrix is processed through an autoencoder to capture a compact representation of user-item interactions. During the fine-tuning phase, a multi-head cross-attention mechanism is employed to integrate the rating matrix with the multimodal latent representations. This mechanism allows each user-item interaction to attend to the most relevant complementary information from the multimodal data. Finally, the enhanced rating matrix is reintroduced into the autoencoder for the reconstruction of the final, refined rating matrix. Our contributions can be summarized as follows:\n\u2022 We propose CADMR, a multimodal recommender system framework incorporating disentangled learning and cross-attention. \u2022 We introduce a novel approach for integrating multimodal information into the reconstruction of the rating matrix using a cross-attention mechanism, ensuring that user-item interactions are informed by the most relevant multimodal features. \u2022 We conduct extensive experiments on benchmark datasets, validating the effectiveness and efficiency of CADMR. The results demonstrate superior performance compared to other state-of-the-art methods.\n2 Related Work", "publication_ref": ["b0", "b1", "b2", "b3", "b4", "b0", "b5", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Multimodal Collaborative Filtering", "text": "The multimodal collaborative filtering-based recommender system aims to learn informative representations of users and items by leveraging multimodal features. He et al. [8] extracted visual features using a CNN, which they concatenated with ID embeddings to produce a refined item representation, which is then fed to a scalable factorization model to learn users' opinions. Joint Representation Learning for top-n recommendations (JRL) [6] leverages the MLP architecture to learn a unified latent representation for the multimodal data. A mapping matrix is then learned to transfer the unified multimodal representation into user and item embeddings, while the latter are learned using a reconstruction loss. Kang et al. [9] build on existing BPR recommender systems by introducing a Siamese CNN where the last layer of the CNN produces the item representation. Moreover, the framework learns pixel-level representations by training the image representation and the recommender system jointly. While these methods successfully incorporate multimodal features into collaborative filtering frameworks, they often suffer from several limitations. One significant weakness is that these approaches typically treat multimodal data as supplementary to the core user-item interaction, rather than as integral components of the interaction itself. This can lead to suboptimal utilization of the rich information contained in the various modalities.", "publication_ref": ["b7", "b5", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Attention Networks", "text": "Traditional multimodal recommender systems often struggle to effectively integrate and utilize multimodal information. The attention mechanism offers a solution by allowing the recommender system to selectively focus on different aspects of the multimodal data, thereby better capturing user preferences. For instance, ACF [10] introduces a framework that incorporates both item-level and component-level attention, enabling the system to attend to high-level information (e.g., photos, videos) as well as finer details, such as specific regions of an image or frames within a video. Similarly, Disentangled Multimodal Representation Learning for Recommendation (DMRL) [12] takes a novel approach by dividing each modality into equal factors and learning a disentangled representation of these different modality feature factors. The attention mechanism is then applied to each factor to effectively model user preferences. UVCAN [11] leverages the attention mechanism in micro-video recommendations by learning multimodal representations for both users and micro-videos through a co-attention mechanism between items and micro-videos. Additionally, it refines video representations by using them as input queries to capture user attention through multi-step reasoning.\nBuilding upon existing approaches that utilize attention mechanisms, our method introduces a novel strategy starting with a pretraining phase where disentangled representations for the different modalities are learned, and the user-item rating matrix is reconstructed via an autoencoder. Following this, we apply a cross-attention mechanism between the user-item rating matrix and the learned multimodal representation. The multimodal-enhanced rating matrix is then projected back into the autoencoder for fine-tuning, ensuring a more integrated and robust modeling of user preferences.", "publication_ref": ["b9", "b11", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Methodology", "text": "In this section, we present CADMR, a novel framework for multimodal recommender systems. Our approach advances existing methods by integrating cross-attention between the rating matrix and multimodal representations. We begin by detailing the learning process for disentangled multimodal representations. Next, we delve into the cross-attention mechanism. Finally, we describe the complete pipeline, from pre-training to the final reconstruction of the rating matrix.\nFig. 1 illustrates the CADMR architecture for a multimodal recommender system, which operates in two distinct phases: pretraining and fine-tuning. During the pretraining phase, an autoencoder is trained on the user-item rating matrix while modality-specific feature extractors learn disentangled representations for textual and visual features. These extracted features undergo layer normalization and are processed through feedforward networks, in order to learn the unified multimodal representation. In the fine-tuning phase, a multi-head cross-attention mechanism is employed, allowing the user-item rating matrix, the query vector, to interact with the unified multimodal representation-treated as key, and value vectors-to generate attention weights. These weights are used to refine the rating matrix, which is then reintroduced into the autoencoder for final reconstruction, resulting in a more accurate and refined rating matrix.\nFig. 1 CADMR architecture for a multimodal recommender system. In the pretraining phase, the autoencoder and modality-specific feature extractors are trained separately. In the fine-tuning phase, a cross-attention mechanism integrates the user-item rating matrix with the unified multimodal representation, refining the matrix, which is then processed through the trained autoencoder to produce the final reconstructed rating matrix.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries", "text": "In what follows, we denote R \u2208 R I\u00d7U as the user-item rating or interaction matrix, with I and U representing the sets of items and users, respectively. We consider a user u to have interacted with an item i, if the corresponding entry r i,u \u2208 R is nonzero, otherwise, there was no interaction, and r i,u = 0. In our setting, each user and item have a unique ID, and we assume that each item is associated with a set of multimodal information, such as item descriptions and images. Unlike existing approaches that treat the item ID as an independent modality, we chose to exclude it from our analysis, focusing instead on textual and visual data. These modalities provide richer, more descriptive features that are sufficient to capture the nuanced preferences of users and the intrinsic characteristics of items.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Feature Extraction", "text": "The extracted multimodal features are fed separately into their respective two-layer neural networks to learn task-specific features relevant to the recommendation. We introduce a layer normalization before each of these neural networks to stabilize the learning process, improve the convergence speed, and ensure that the features fed into the networks are on a consistent scale. The projection neural network is defined as,\nh = f (W 1 g(W 0 N (x) + b 0 ) + b 1 ), (1\n)\nwhere x is the multimodal data features, f and g are the activation functions, W 0 , W 1 and b 0 , b 1 the corresponding layer weights and biases, and N (.) is the layer normalization. It should be noted that the output h = {h t , h v } can represent either the visual or textual feature representation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Disentangled Representation Learning", "text": "Let h t \u2208 R Dt and h v \u2208 R Dv represent the textual and visual feature embeddings obtained using the feature extraction module, respectively, where D t and D v denote the dimensions of these feature spaces. One of the main challenges in multimodal learning is that features extracted from different modalities often contain redundant or entangled information. This means that certain dimensions of the latent space might capture overlapping or correlated features between modalities, leading to inefficiencies and potential overfitting in downstream tasks. To address this issue, we employ disentangled learning, which aims to ensure that each dimension of the latent representations captures distinct and non-overlapping factors of variation. By disentangling the features, we improve the generalization capability of the model and reduce the risk of redundancy.\nThe disentangled loss is applied to both h t and h v , encouraging each dimension of the latent representations to capture distinct, non-overlapping information. This is achieved through a Total Correlation (TC) loss, which measures the dependency along the dimensions of the latent representation. The TC loss can be expressed as,\nL T C (h) = D d=1 E log p(h d ) p(h d |h d \u2032 ) ,(2)\nwhere p(h d ) is the marginal probability of the d-th dimension of the latent variable h, and p(h\nd |h d \u2032 ) is the conditional probability of h d given the other dimensions h d \u2032 .\nBy minimizing this loss, the learned representations h t and h v are encouraged to have minimal redundancy, thus disentangling the underlying factors of variation.\nFollowing the disentangled learning, we fuse the disentangled representations of the two modalities via a feed-forward network composed of one layer. The fused representation h f is given by,\nh f = f (W N ([h t ; h v ]) + b),(3)\nwhere W and b are the weights and bias of the fusion layer respectively, [.;.] denotes the concatenation operation, f the activation function and N (.) the layer normalization. This fused representation h f is then used in the matrix reconstruction step, to leverage the complementary information from both textual and visual modalities.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Cross-Attention for User's Preference Modeling", "text": "In our framework, we employ a cross-attention mechanism to model user preferences by integrating the fused multimodal representations learned during the pre-training phase. This approach aims to capture the nuanced interactions between users and items by attending to various aspects of the multimodal features, thus improving the accuracy of rating predictions. Given the fused disentangled embeddings of text and image h f , we apply the cross-attention mechanism in conjunction with the rating matrix R. Specifically, let Q, K and V represent the query, key and value matrices respectively. In our crossattention setup, the rating matrix R serves as the query Q, while the fused multimodal representations h f forms the keys K and values V . The cross-attention operation is defined as,\nCross-Attention(Q, K, V ) = softmax QK T \u221a d V,(4)\nwhere Q, K, V \u2208 R I\u00d7l , with I being the number of items and l the latent dimension of the query, key and value. This mechanism computes a weighted sum of the values V based on the similarity (i.e., dot product) between the query Q and the keys K, allowing the model to focus on the most relevant aspects of the item's multimodal features for each user. The output of the cross-attention is an intermediate representation of the user-item rating matrix. We then apply a feed-forward layer to project this representation back to its original dimension.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Rating Matrix Reconstruction", "text": "After generating the cross-attention-based rating matrix, we follow the approach proposed by [1] which involves refining the predictions using the AE. The rating matrix produced in the previous steps serves as input to the trained AE model, which has been pretrained to capture the underlying structure of user-item interactions. In this phase, the AE model takes its pretrained weights and fine-tunes them based on the cross-attention-modified rating matrix, as depicted in Fig. 1.\nThis adjustment allows the model to integrate the additional information encapsulated by the cross-attention mechanism, leading to a more accurate reconstruction of user preferences. The output of this fine-tuned AE corresponds to the final predicted ratings in the proposed recommender system. By reconstructing the rating matrix with the enhanced input, the system ensures that the final predictions are informed by both the original latent factors learned during pretraining and the nuanced multimodal interactions captured through cross-attention.", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "Discussion", "text": "We identified two main reasons for choosing cross-attention in our recommendation framework: its ability to model the heterogeneity of user preferences and item characteristics, and its contribution to model interpretability.\nFrom a modeling perspective, cross-attention excels at addressing the diverse and complex nature of user preferences and item attributes. Users often display varied tastes that are influenced by different factors depending on the context. For instance, a user might prioritize certain visual cues in an image when selecting a movie to watch. Similarly, items in a recommender system can be highly diverse, each possessing a unique set of attributes that appeal to different users. Cross-attention allows the model to selectively align the most relevant item features with each user's preferences. From an interpretability perspective, cross-attention mechanisms offer valuable insights into which features are most influential in driving user decisions. By explicitly modeling the interactions between user preferences and item features, cross-attention highlights the aspects of the data that contribute most to the recommendations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Training 4.1 Regularization", "text": "To prevent overfitting and enhance generalization, we employ two types of regularization during training:\nL 2 Regularization\nFollowing [1], we apply L 2 regularization (Ridge regularization) to both the weight and kernel matrices. This is controlled by separate penalty parameters \u03bb 2 and \u03bb s .", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "Dropout", "text": "We use a dropout rate of Dr rate = 0.2 during the initial step of multimodal feature extraction. Dropout is applied to the output of each sub-layer before it is normalized and passed as input to the subsequent layer.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Hardware and Schedule", "text": "We trained our CADMR model on a single NVIDIA A100 GPU. With the hyperparameters discussed throughout the paper, each training step took approximately 1 second on average. For comparison, training the model on a machine with only a CPU took around 10 seconds per step.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Optimization", "text": "We trained our model using a squared errors loss with the aformentionned L 2 regularization term plus the additional Total Correlation loss for the disentangled representation learning. The final objective function of CADMR is defined as,\nL = L M SE + \u03bbL T C ,(5)\nwhere L M SE and L T C are the mean squared and total correlation losses respectively. \u03bb is a hyperparameter that controls the relative importance of the disentangled representation learning total correlation loss with respect to the mean squared loss, which is set to 0.5. We utilized the Adam optimizer [15] with \u03b2 1 = 0.9, \u03b2 2 = 0.99, and \u03f5 = 10 -6 to ensure efficient and stable convergence.", "publication_ref": ["b14"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "To validate the effectiveness of our proposed multimodal recommender system, we conducted extensive experiments on three benchmark datasets. In what follows, we introduce the experimental setup and then report and analyze the experimental results.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Setup", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Datasets", "text": "The Amazon products dataset [16] is a widely-used benchmark in the field of multimodal recommender systems, providing a rich and diverse set of information for model training and evaluation. It is divided into 24 product categories and contains 48 million items, and 571 million reviews from 54 million users represented as reviews; i.e., ratings, text, helpfulness votes. Additionally most of the items are accompanied by the corresponding metadata; i.e., descriptions, category information, price, brand, and image features. For our experimentations, three product subsets are selected: Baby, Sports and Electronics. Table 1 presents the general information about each of the selected products subsets.\nFollowing [13,17,18], we reduce the Amazon datasets to the 5-core setting, that is each of the users and items have 5 reviews each. The resulting datasets are then randomly split into training, validation and test sets using the ratio 8:1:1.\nThe available metadata is used as our multimodal information. Specifically, for the text modality, we concatenate the item's title, description, brand, and categories. We then apply a pretrained SBERT model [14] to extract the textual features, resulting in a 384-dimensional vector. Visual features from each item's image are extracted using a Deep CNN, producing a 4096-dimensional vector as presented by [19]. Finally, we construct the rating matrix based on the user-item interactions.", "publication_ref": ["b15", "b12", "b16", "b17", "b13", "b18"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Metrics", "text": "There are several widely-used evaluation metrics for recommendation tasks, including accuracy, recall, precision, Normalized Discounted Cumulative Gain (NDCG), and hit rate. These metrics are commonly used for evaluating recommender systems, where higher values signify better performance in identifying relevant items. Given that the rating matrix represents user-item interactions, we define an interaction as r ij = 1 (positive) if a user has interacted with an item, and r ij = 0 (negative) otherwise. Consequently, the dataset contains both positive (interaction) and negative (no interaction) labels.\nDue to the typically high sparsity of recommendation datasets, often exceeding 90%, the dataset is inherently unbalanced, with the majority of labels being negative. In such cases, accuracy can be misleading, as a model could achieve a high accuracy score by predominantly predicting negative labels. To address this imbalance, we focus primarily on two metrics in our experiments: Recall and NDCG, specifically their variants Recall@K and NDCG@K, where K \u2208 {10, 20}.\nRecall@K measures the proportion of correctly predicted relevant items within the top K recommendations relative to the total number of relevant items in the dataset. However, this metric does not account for the order or relevance of the items within the list.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "NDCG@K", "text": "(Normalized Discounted Cumulative Gain) evaluates the quality of the ranking by considering the relevance of the recommended items and their positions in the list. It computes the relevance score of items in the recommendation list and discounts it logarithmically, penalizing relevant items that are not ranked at the top. The ideal ordered recommendation list (iDCG) serves as the ground truth for this metric. NDCG@K is defined as:\nNDCG@K = 1 iDCG K k=1 2 relevance k -1 log 2 (k + 1)(6)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Compared methods", "text": "We compare CADMR with seven multimodal recommender systems baselines, namely,\n\u2022 LATTICE [19] introduces a modality-aware structure learning layer that generates item-item structures for each modality and combines them to create latent item graphs. Graph convolutions on these graphs explicitly embed high-order item affinities into item representations, which can be integrated into existing collaborative filtering models to enhance recommendation accuracy. \u2022 BM3 [20] begins by bootstrapping latent contrastive views from user and item representations using simple dropout augmentation. It then simultaneously optimizes three multimodal objectives to learn user and item representations by reconstructing the user-item interaction graph and aligning modality features across both interand intra-modality perspectives. \u2022 The two primary components of SLMRec [21] are (1) data augmentation of multimodal content, which generates multiple item views using three operators: feature dropout (FD), feature masking (FM), and fine and coarse feature spaces (FAC); and (2) contrastive learning, which separates an item's view from others' views to provide extra supervisory signals.\n\u2022 ADDVAE [22] introduces a second set of disentangled user representations learned from textual content and aligns these with the original set, improving both recommendation effectiveness and representation interpretability.\n\u2022 FREEDOM [23] proposes a straightforward yet effective model that freezes the item-item graph while simultaneously denoising the user-item interaction graph to enhance multimodal recommendations. \u2022 DRAGON [24] builds a user-user graph based on common item interactions and an item-item graph from multimodal item features. It uses graph learning on both the ", "publication_ref": ["b18", "b19", "b20", "b21", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Performance Comparison Analysis", "text": "Our proposed model, CADMR, demonstrates a substantial improvement over stateof-the-art multimodal recommender systems across key metrics, including NDCG@10, NDCG@20, Recall@10, and Recall@20, as demonstrated in Table 2. Specifically, CADMR significantly outperforms competing models in terms of both NDCG and Recall. For instance, in the context of NDCG@10, CADMR achieves a notable improvement, with values such as 0.1693, 0.1719, and 0.1245 across the evaluated datasets, far surpassing the closest competitor, MG, which reaches 0.0431 in the best case.\nSimilarly, for Recall@10, it attains values such as 0.2640, 0.2754, and 0.2253, which are more than three times higher than the best-performing baseline model. This increase in recall indicates CADMR's proficiency in retrieving a higher proportion of relevant items within the top recommendations, thus enhancing the user experience by presenting more relevant suggestions.\nThe consistent outperformance across various datasets and metrics demonstrates CADMR's generalizability and efficiency in leveraging multimodal information to refine recommendations. This success can be attributed to the novel integration of cross-attention mechanisms, which enable the model to capture intricate interactions between users and items, leading to more accurate and personalized recommendations. Overall, CADMR sets a new benchmark in multimodal recommendation, significantly advancing the state of the art.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Cold start Analysis", "text": "We wanted to evaluate the model's resilience to the cold-start problem by varying the size of the training data from 80% to 20%. The results, depicted in Fig. 2, show the NDCG@10 performance on the Sports and Baby datasets.\nAs expected, the NDCG@10 score decreases as the training data size diminishes, which is a common manifestation of the cold-start problem. The reduction in available training data limits the model's ability to learn user-item interactions effectively, leading to a decline in recommendation quality. For the Sports dataset, the NDCG@10 progressively drops to approximately 13% when the training data is reduced to 20%. Similarly, the Baby dataset follows a similar outcome, with NDCG@10 decreasing to just below 13% over the same range.\nDespite this decrease in performance, it is important to note that our model, CADMR, still outperforms existing methods across all training sizes. Even at the lowest training size of 20%, CADMR maintains a higher NDCG@10 than what many competing models achieve with a full dataset. This demonstrates the robustness of CADMR in handling sparse data scenarios and mitigating the cold-start problem better than prior state-of-the-art models. These results underline CADMR's strong generalization capability, making it a competitive solution even in data-constrained environments.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Ablation Study", "text": "In this section, we present an ablation study to evaluate the contribution of two key components in our model, CADMR: the cross-attention mechanism and the disentangled representation learning (DRL) module. By systematically removing these components, we measure their individual impact on the overall performance. We compare the following variants:  The results of the ablation study are summarized in Table 3. It is evident that removing either of these components leads to a significant decline in performance across all datasets (Baby, Sports, and Electronics). Specifically, when DRL is removed, the performance drops notably, with NDCG@10 scores of 0.1215 on the Baby dataset, 0.1376 on the Sports dataset, and 0.1083 on the Electronics dataset. Similarly, excluding the cross-attention mechanism (CADMR w/o CA ) results in reduced performance, with NDCG@10 scores of 0.0639, 0.0681, and 0.0527 for Baby, Sports, and Electronics datasets, respectively. This demonstrates the substantial contribution of cross-attention to our model's ability to enhance user-item interaction with multimodal data effectively. Our base CADMR model, which integrates both cross-attention and disentangled representation learning, achieves the highest scores on all datasets. This confirms that both components are essential for maximizing performance, with their combined effect yielding the best results.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Effect of Cross-Attention heads number", "text": "We varied the number of cross-attention heads to assess their effect on our model's performance across the Baby, Sports, and Electronics datasets. As the number of heads increased from 1 to 4, a clear improvement in NDCG@10 scores was observed in all datasets. This steady improvement indicates that adding more cross-attention However, when the number of heads was increased beyond 4, the performance gains began to taper off. On all three datasets, moving from 4 to 8 heads resulted in only marginal improvements, signaling a plateau in performance. These results suggest that while cross-attention heads are beneficial up to a certain point, adding more heads beyond 4 yields diminishing results.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "In this paper, we introduced CADMR, an autoencoder-based multimodal recommender system designed to address the challenges of high-dimensional, sparse user-item rating matrices. CADMR integrates multi-head cross-attention mechanisms and disentangled learning to effectively leverage diverse multimodal data, enhancing both the reconstruction of the rating matrix and the representation of user-item interactions. By disentangling modality-specific features while preserving their interdependence, CADMR learns a joint latent representation that significantly improves recommendation performance. Extensive evaluations on multiple benchmark datasets demonstrated its superior performance over state-of-the-art methods, as well as the critical role of the cross-attention and disentangled learning modules. In future work, we will further analyze CADMR's scalability and explore potential avenues for enhancing its effectiveness.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Conference On Multimedia. pp. 935-943 (2023) [24] Zhou, H., Zhou, X., Zhang ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Glocal-k: Global and local kernels for recommender systems", "journal": "", "year": "2021", "authors": "S Han; T Lim; S Long; B Burgstaller; J Poon"}, {"ref_id": "b1", "title": "A comprehensive survey on multimodal recommender systems: Taxonomy, evaluation, and future directions", "journal": "", "year": "2023", "authors": "H Zhou; X Zhou; Z Zeng; L Zhang; Z Shen"}, {"ref_id": "b2", "title": "A non negative matrix factorization for collaborative filtering recommender systems based on a Bayesian probabilistic model", "journal": "Knowledge-Based Systems", "year": "2016", "authors": "A Hernando; J Bobadilla; F Ortega"}, {"ref_id": "b3", "title": "Using singular value decomposition approximation for collaborative filtering", "journal": "", "year": "2005", "authors": "S Zhang; W Wang; J Ford; F Makedon; J Pearlman"}, {"ref_id": "b4", "title": "Autoencoders meet collaborative filtering", "journal": "", "year": "2015", "authors": "S Sedhain; A Menon; S Sanner; L Xie;  Autorec"}, {"ref_id": "b5", "title": "Joint representation learning for topn recommendation with heterogeneous information sources", "journal": "", "year": "2017", "authors": "Y Zhang; Q Ai; X Chen; W Croft"}, {"ref_id": "b6", "title": "Joint deep modeling of users and items using reviews for recommendation", "journal": "", "year": "2017", "authors": "L Zheng; V Noroozi; P Yu"}, {"ref_id": "b7", "title": "VBPR: visual bayesian personalized ranking from implicit feedback", "journal": "", "year": "2016", "authors": "R He; J Mcauley"}, {"ref_id": "b8", "title": "Visually-aware fashion recommendation and design with generative image models", "journal": "", "year": "2017", "authors": "W Kang; C Fang; Z Wang; J Mcauley"}, {"ref_id": "b9", "title": "Attentive collaborative filtering: Multimedia recommendation with item-and component-level attention", "journal": "", "year": "2017", "authors": "J Chen; H Zhang; X He; L Nie; W Liu; T Chua"}, {"ref_id": "b10", "title": "User-video co-attention network for personalized micro-video recommendation", "journal": "", "year": "2019", "authors": "S Liu; Z Chen; H Liu; X Hu"}, {"ref_id": "b11", "title": "Disentangled representation learning for recommendation", "journal": "IEEE Transactions On Pattern Analysis And Machine Intelligence", "year": "2022", "authors": "X Wang; H Chen; Y Zhou; J Ma; W Zhu"}, {"ref_id": "b12", "title": "Mmrec: Simplifying multimodal recommendation", "journal": "", "year": "2023", "authors": "X Zhou"}, {"ref_id": "b13", "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks", "journal": "", "year": "2019", "authors": "N Reimers"}, {"ref_id": "b14", "title": "A method for stochastic optimization", "journal": "", "year": "2014", "authors": "D Kingma;  Adam"}, {"ref_id": "b15", "title": "Bridging language and items for retrieval and recommendation", "journal": "", "year": "2024", "authors": "Y Hou; J Li; Z He; A Yan; X Chen; J Mcauley"}, {"ref_id": "b16", "title": "Graph-refined convolutional network for multimedia recommendation with implicit feedback", "journal": "", "year": "2020", "authors": "Y Wei; X Wang; L Nie; X He; T Chua"}, {"ref_id": "b17", "title": "Multi-modal graph convolution network for personalized recommendation of micro-video", "journal": "", "year": "2019", "authors": "Y Wei; X Wang; L Nie; X He; R Hong; T Chua;  Mmgcn"}, {"ref_id": "b18", "title": "Mining latent structures for multimedia recommendation", "journal": "", "year": "2021", "authors": "J Zhang; Y Zhu; Q Liu; S Wu; S Wang; L Wang"}, {"ref_id": "b19", "title": "Bootstrap latent representations for multi-modal recommendation", "journal": "", "year": "2023", "authors": "X Zhou; H Zhou; Y Liu; Z Zeng; C Miao; P Wang; Y You; F Jiang"}, {"ref_id": "b20", "title": "Selfsupervised learning for multimedia recommendation", "journal": "IEEE Transactions On Multimedia", "year": "2022", "authors": "Z Tao; X Liu; Y Xia; X Wang; L Yang; X Huang; T Chua"}, {"ref_id": "b21", "title": "Aligning dual disentangled user representations from ratings and textual content", "journal": "", "year": "2022", "authors": "N Tran; H Lauw"}, {"ref_id": "b22", "title": "A tale of two graphs: Freezing and denoising graph structures for multimodal recommendation", "journal": "", "year": "", "authors": "X Zhou; Z Shen"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Fig. 22Fig. 2 CADMR performance with respect to the training set size.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Fig. 33Fig. 3 Impact of the cross-attention number of heads on the overall performance of our model", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "", "figure_caption": "", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Datasets used in our experiments", "figure_data": "Dataset# Users # Items # InteractionsBaby194457050160,792Sports3559818357296,337Electronics192,40363,0011,689188"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Performance comparison by different multimodal recommender system models in terms of NDCG and Recall. The best results are in bold.", "figure_data": "DatasetModelNDCG@10 NDCG@20 Recall@10 Recall@20LATTICE0.02920.03700.05470.0850BM30.03010.03830.05640.0883BabySLMRec0.02900.03530.05290.0775ADDVAE0.03230.04040.05980.091FREEDOM0.03300.04240.06270.0992DRAGON0.03450.04350.06620.1021DRAGON + MG0.03690.04650.07010.1082CADMR0.16930.17790.26400.2940LATTICE0.03350.04210.06200.0953BM30.03550.04380.06560.0980SportsSLMRec0.03650.04500.06630.0990ADDVAE0.03890.04730.07090.1035FREEDOM0.03850.04810.07170.1089DRAGON0.04030.05000.07490.1124DRAGON + MG0.04310.05350.07930.1191CADMR0.17190.17960.27540.2977LATTICE----BM30.02470.03020.04370.0648Elec.SLMRec0.02490.03030.04430.0651ADDVAE0.02530.03080.04510.0665FREEDOM0.02200.02730.03960.0601DRAGON0.02920.03570.05220.0781DRAGON + MG0.03120.03810.05530.0827CADMR0.12450.13100.22530.2489heterogeneous user-item graph and the homogeneous user-user and item-item graphsto derive dual user and item representations. Finally, an attentive concatenationmethod is utilized for fusing user and item representations.\u2022 MG [25] introduces Mirror Gradient (MG), a concise yet effective gradient strategythat enhances model robustness during optimization, reducing instability risks frommultimodal inputs."}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Performance of the different variants of our CADMR model over all three datasets. Best results are in bold. CADM R w/o DRL : This variant removes the disentangled representation learning module, retaining only the cross-attention mechanism. \u2022 CADM R w/o CA : In this version, we exclude the cross-attention mechanism while keeping the disentangled learning module.", "figure_data": "MethodBabySportsElec.CADM R w/o DRL0.12150.13760.1083CADM R w/o CA0.06390.06810.0527CADM R base0.1693 0.1719 0.1245\u2022"}], "formulas": [{"formula_id": "formula_0", "formula_text": "h = f (W 1 g(W 0 N (x) + b 0 ) + b 1 ), (1", "formula_coordinates": [5.0, 215.14, 568.66, 251.3, 9.65]}, {"formula_id": "formula_1", "formula_text": ")", "formula_coordinates": [5.0, 466.44, 568.66, 4.24, 8.74]}, {"formula_id": "formula_2", "formula_text": "L T C (h) = D d=1 E log p(h d ) p(h d |h d \u2032 ) ,(2)", "formula_coordinates": [6.0, 239.88, 347.25, 255.32, 30.63]}, {"formula_id": "formula_3", "formula_text": "d |h d \u2032 ) is the conditional probability of h d given the other dimensions h d \u2032 .", "formula_coordinates": [6.0, 158.59, 400.84, 318.43, 11.96]}, {"formula_id": "formula_4", "formula_text": "h f = f (W N ([h t ; h v ]) + b),(3)", "formula_coordinates": [6.0, 252.69, 487.93, 242.52, 9.65]}, {"formula_id": "formula_5", "formula_text": "Cross-Attention(Q, K, V ) = softmax QK T \u221a d V,(4)", "formula_coordinates": [7.0, 180.02, 170.6, 290.67, 25.25]}, {"formula_id": "formula_6", "formula_text": "L 2 Regularization", "formula_coordinates": [8.0, 124.6, 215.18, 87.46, 9.68]}, {"formula_id": "formula_7", "formula_text": "L = L M SE + \u03bbL T C ,(5)", "formula_coordinates": [8.0, 266.16, 472.59, 229.05, 9.65]}, {"formula_id": "formula_8", "formula_text": "NDCG@K = 1 iDCG K k=1 2 relevance k -1 log 2 (k + 1)(6)", "formula_coordinates": [10.0, 227.14, 271.55, 268.07, 30.63]}], "doi": ""}
