{"Helping the Helper : Supporting Peer Counselors via AI-Empowered Practice and Feedback": "SHANG-LING HSU, University of Southern California, USA RAJ SANJAY SHAH, Georgia Institute of Technology, USA PRATHIK SENTHIL, Georgia Institute of Technology, USA ZAHRA ASHKTORAB, IBM Research, USA CASEY DUGAN, IBM Research, USA WERNER GEYER, IBM Research, USA DIYI YANG, Stanford University, USA Millions of users come to online peer counseling platforms to seek support. However, studies show that online peer support groups are not always as effective as expected, largely due to users' negative experiences with unhelpful counselors. Peer counselors are key to the success of online peer counseling platforms, but most often do not receive appropriate training. Hence, we introduce CARE : an AI-based tool to empower and train peer counselors through practice and feedback. Concretely, CARE helps diagnose which counseling strategies are needed in a given situation and suggests example responses to counselors during their practice sessions. Building upon the Motivational Interviewing framework, CARE utilizes large-scale counseling conversation data with text generation techniques to enable these functionalities. We demonstrate the efficacy of CARE by performing quantitative evaluations and qualitative user studies through simulated chats and semi-structured interviews, finding that CARE especially helps novice counselors in challenging situations. The code is available at https://github.com/SALT-NLP/CARE. CCS Concepts: \u00b7 Human-centered computing \u2192 Collaborative and social computing systems and tools . Additional Key Words and Phrases: Human-AI collaboration, online support platforms, peer counseling", "ACMReference Format:": "Shang-Ling Hsu, Raj Sanjay Shah, Prathik Senthil, Zahra Ashktorab, Casey Dugan, Werner Geyer, and Diyi Yang. 2025. Helping the Helper : Supporting Peer Counselors via AI-Empowered Practice and Feedback. Proc. ACM Hum.-Comput. Interact. 9, 2, Article CSCW095 (April 2025), 45 pages. https://doi.org/10.1145/3710993", "1 Introduction": "Millions of people (nearly 1 in 5 American adults) are touched by mental illness [37]. The majority of people affected never access care [42], due to structural barriers including the high cost of treatment and the lack of trained professionals to meet the demand [67, 69, 86]. One promising remedy for under-treatment and, to meet the demand for mental healthcare, is the use of online peer counseling platforms, which provide the benefits of anonymity, empowerment, and ease of access [57, 82, 102]. Online peer counseling platforms have been proven effective in reducing people's Authors' Contact Information: Shang-Ling Hsu, hsushang@usc.edu, University of Southern California, USA; Raj Sanjay Shah, rajsanjayshah@gatech.edu, Georgia Institute of Technology, USA; Prathik Senthil, psenthil3@gatech.edu, Georgia Institute of Technology, USA; Zahra Ashktorab, Zahra.Ashktorab1@ibm.com, IBM Research, USA; Casey Dugan, cadugan@us.ibm.com, IBM Research, USA; Werner Geyer, werner.geyer@us.ibm.com, IBM Research, USA; Diyi Yang, diyiy@stanford.edu, Stanford University, USA. This work is licensed under a Creative Commons Attribution-NoDerivatives 4.0 International License. \u00a9 2025 Copyright held by the owner/author(s). ACM 2573-0142/2025/4-ARTCSCW095 https://doi.org/10.1145/3710993 I am so stressed. It's overwhelming Jane (Support Seeker) Type something Alex (Listener) 2. Suggest possible response Sounds like you're going through a challenging time. 1. Diagnose which skill to use Ask an open-ended question without CARE with CARE Sounds really bad Sounds like you are going through a challenging time. Can you tell me more? self-reported anxiety and depression and improving their quality of life [31, 38]. Nevertheless, studies show that online support groups are not always as effective as expected [55], largely due to support seekers' negative experiences with unhelpful supporters 1 [18, 19, 129]. Supporters are the key to the success of online peer counseling platforms [3], and the quality of the support that seekers receive within a community heavily depends on those providing support. Thus, finding ways to help peer counselors be more helpful and supportive is important. Evidence from volunteer-based support services indicates that even brief training may improve a supporter's ability to help seekers [114] and may ultimately lead to improved mental health outcomes in seekers [6]. Thus, there is a growing trend in scaling effective peer support training online. For instance, 7 Cups [90], an online peer-to-peer counseling platform, provides users with training on active listening techniques [10], and SAHAR , an Israeli-based suicide-prevention initiative, trains its online counselors extensively during weeks of in-person training to handle supportive chats [8]. However, supporters typically do not have the rigorous professional training of their offline counterparts [103], with very abbreviated training or no training at all for most online peer support groups [45, 140]. Thus, even if supporters want to help others in need, they might lack the necessary set of skills or the knowledge to use evidence-based counseling strategies [134]. Moreover, unlike traditional counseling settings, online supporters often do not have systematic ways to receive supervision or guidance, especially during their conversations with seekers and when there are high levels of uncertainty about how to respond. Without appropriate guidance, supporters might develop biased or even inappropriate helping skills without being aware of it. This can lead to ineffective, or at worst, harmful, support, which might prevent seekers from receiving effective support or other types of support options sooner [111, 130] and may also expose supporters to self-doubts [134], increased stress and decreased self-efficacy [33, 117]. Existing mechanisms of training or scaffolding largely rely on human supervision, which requires an extensive amount of resources such as cost, time, labor, and expertise, making it hard to scale up to help the large number of supporters [7] who support millions of people in need of care in text-based online peer counseling platforms. Social computing research on online peer support groups mainly looks at how supporters' language use or counseling strategies are associated with better outcomes for seekers [4, 28, 91, 100, 101, 103]. In contrast, there have been relatively fewer studies on building scalable systems and tools to empower supporters to offer the most effective support in online peer counseling platforms, with a few exceptions [99, 125]. More importantly, the majority of research has specifically been about designing virtual counselors that emulate a human counselor in a chatbot-like setting [48, 49]. In contrast to prior research on chatbots focused on virtual counselors or client-clinician interaction [32], our approach involves the development of an automated assistant designed to enhance and support individuals by providing exposure to a diverse array of counseling strategies during training, with the intention of augmentation rather than replacement [107]. Concretely, we propose to design an interactive training agent CARE to empower peer counselors in text-based, online peer-to-peer counseling platforms. Here, we use 7 Cups as a research site and build contextualized language generation approaches to provide tailored assistance for supporters by highlighting which counseling strategies are needed in a given situation and suggesting example responses. Building upon psychotherapy theories and empirical studies on Motivational Interviewing (MI) [87, 122], we select a set of counseling strategies from MI to help volunteer counselors. We build machine learning classifiers on top of the pre-trained language model BERT [35] to predict the most suitable counseling strategies in a given context and then develop language generation approaches via a pre-trained generation model DialoGPT [150] for generating example responses that counselors can further use and edit. As shown in Figure 1, we integrate different components into an interactive interface for CARE to enable supporters at scale. We work closely with different stakeholders from 7 Cups in the development process of CARE : platform developers and volunteer counselors. To evaluate whether the proposed system CARE works for peer counselors, we perform quantitative and qualitative evaluations, including system log analyses, questionnaires, and semi-structured interviews, and demonstrate the impacts of CARE on counselors. We find that CARE especially helps beginner and novice counselors to better deal with these challenging situations. To sum up, we make the following contributions: \u00b7 Develop contextualized language generation techniques that provide tailored assistance for peer counselors by highlighting which counseling strategies may be needed in a given situation and suggesting example responses. \u00b7 Create an interactive tool CARE to empower peer counselors by offering on-the-fly suggestions during practice sessions on an online peer counseling platform. \u00b7 Evaluate CARE via both quantitative system log analyses and qualitative user studies to demonstrate the efficacy of CARE on a set of representative counseling scenarios.", "2 Related Work": "", "2.1 Peer Support and Online Peer Counseling Platforms": "Peer support has been shown to be effective in a wide range of mental health services [110], and is a major motivation for people who go for online mental health support [102]. Prior work has provided some evidence for the effectiveness of online peer support groups. For instance, a recent survey on 7 Cups reported higher user satisfaction with the support provided by 7 Cups counselors, and users who indicated receiving psychotherapy in the past marked the peer counselors' support to be as helpful as psychotherapy [9]. Most studies on online text-based peer counseling context have focused on the automatic modeling of behaviors from peer counselors and seekers and the correlations between the behaviors and successful outcomes [4, 71, 80, 144, 147], user engagement [5, 125], or support provision [70, 126]. An increasing volume of automated text-based analytical work has been carried out to identify behavioral codes derived from the Motivational Interviewing (MI) Skill Codes [20, 39, 59, 83, 84, 100, 101, 121, 132]. Some works specifically concentrate on automating the scoring of reflections , a key listening skill, to offer numeric feedback on counselors' responses [20, 84, 100, 127]. Others explore the relation between the use of multiple MI strategies and the satisfaction of support seekers in counseling sessions, showcasing the effectiveness of MI strategies through extensive data analysis [39, 101, 122]. In a recent study, Chen et al. [26] delve into the challenges faced and MI strategies employed by novice peer counselors through qualitative user studies, discussing design implications to better prepare for the skill development of novice therapists. Building upon these prior works, we focus on a set of widely used MI strategies and further leverage these to guide the design of an interactive tool aimed at empowering peer counselors, especially inexperienced ones, with contextual examples for emulation during training phases.", "2.2 Systems and Interventions for Peer Counseling Platforms": "Interventions for support are generally valued by people with mental health concerns [91]. For example, people with schizophrenia showed a positive attitude toward using technology for care [44]. Supporters show a substantial interest in learning helping skills and using web-disseminated peer support interventions to help one another online [12]. Supportive chats guided by prompts in a design study were found to be associated with reduced anxiety, perceived as 'deeper' with solutions to problems and new perspectives [92] compared to unguided ones, calling for natural language processing empowered systems to provide appropriate scaffolds at scale. This is in line with a recent study on the impact of technology in psychotherapy [60], which has identified the development of machine learning technologies for counselors' training and feedback as important needs where technology has a significant impact. To address this, our work leverages natural language processing methods to provide peer counselors with contextualized training and to help them acquire or improve their counseling skills. Additionally, there has been increased attention towards building automatic tools for online health communities [47, 99], however, the majority of research has specifically been about designing virtual counselors that emulate humans in a chatbot-like setting. For instance, Han et al. [48, 49] introduce a counseling dialogue system that engages users by recognizing their input and generating responses incorporating basic counseling techniques through rule-based decisions and extractionbased response templates. Shen et al. [127] and O'neil et al. [93] employ GPT-based models to generate reflections. These automation approaches are efficient and labor-free [107]. However, in high-stakes and high-touch scenarios like counseling, automation poses risks for support seekers and raises ethical and legal concerns [60, 79, 85, 143, 153]. In contrast to automation approaches, Saha et al. [119] and Sharma et al. [124] devise response rewriting methods to enhance empathy to augment trainee's candidate response. Similarly, to further increase interactiveness, Sharma et al. [125] propose HAILEY, a tool that suggests modifications to peer supporters' responses to help them respond more empathically to support seekers. Given the potential risks associated with automation , CARE also focuses on augmentation of peer counselor support by proactively diagnosing counseling strategies to use and suggesting possible responses in a contextualized manner. Overall, we argue that an augmentation approach offers a balanced solution between support at scale and risk management.", "2.3 Contextualized Language Generation": "To empower peer supporters, automatic assistants must generate situationally appropriate responses. Contextualized language generation is rooted in generating text that is closely tied to the surrounding context, which is particularly important in tasks that heavily rely on context, such as text summarization, machine translation, and chatbots. Various strategies are employed for contextualized language generation. These strategies involve training recurrent models like RNNs [118, 123], LSTMs [54], and GRUs [30] using sequence-to-sequence frameworks. Moreover, a prevalent approach involves utilizing pre-trained Transformer-based models [136] like BART [77], T5 [106], and the GPT series [14, 94, 95, 105], where these models can be fine-tuned, trained from scratch, or queried for specific tasks. The choice of context in contextualized language generation depends on the task and design considerations. Specifically, in generating responses for conversations, it's common to incorporate the conversation history, comprising prior utterances, to ensure coherent dialogues. In scenarios involving single-turn responses, researchers often employ RNNs [123] or LSTMs [137], using the most recent utterance as input. For more extensive and ongoing conversations, approaches like XiaoIce [151] introduce encoded dialog states to the input of GRUs [30]. Addressing multi-turn conversations and aiming for diverse responses, DialoGPT [150] builds upon GPT-2 [105], reranking generation options using a backward model. BlenderBot [115] enhances context by adding conversation topics and personas to the dialog history, incorporating a retrieval step to mitigate hallucinations. Its subsequent iteration [145] continuously maintains a summary of conversation history for reference across chat sessions. InstructGPT [95] adopts reinforcement learning to finetune GPT-3 [14], aligning responses better with human intent. More recent models such as ChatGPT and GPT-4 [94] push the contextualized language generation to a new height by generating very human-like text based on context and past conversations. However, such models are only accessible via API calls, which raises concerns over data security and confidentiality and makes them difficult to tailor to mental health settings with model fine-tuning or few-shot learning. Thus to generate contextually relevant suggestions for mental health, we extend a controllable version of DialoGPT to fine-tune it to build CARE , which incorporates conversation history and integrates an MI strategy into the context for language generation. Many studies have looked at language generation techniques to support real-time writing assistants, covering diverse applications such as keyboard prediction [51], grammar and spelling error correction [41, 65], auto-completion [24, 25, 68, 89, 138], and response suggestion for messaging [56] and emails [66, 113]. Building on these studies, our interface design resembles that of response suggestion assistants. Similar to prior research on writing assistants, we quantitatively evaluate CARE with utilization-based metrics, such as click-through rate, visibility of suggestions, and response lengths. Hence, certain findings from previous studies, such as the trade-offs between suggestion savings and cognitive workload [73, 104, 135], may be relevant to CARE . Unlike other similar writing assistants, such as commercial software to generate suggestions in email [113] or controlling the diversity of suggestions through clustering [66], CARE aims to help novices gain skills by providing them with domain-aware generation conditioned on different Motivational Interviewing (MI) Skill Codes.", "3 The Design of CARE": "Realistic practice and tailored feedback are key processes for training individuals with therapy skills [11, 72, 109]. As discussed earlier, existing mechanisms of providing feedback largely depend on human supervision. It often takes well-trained human raters up to ten times as long as the duration of the session itself to finish labeling that session on the involved counseling strategies [7], let alone providing detailed feedback, making it difficult to scale up to help a large number of volunteer counselors who use online communities. Our work aims to use natural language processing tools to automatically provide suggestions to assist volunteer counselors during their practice sessions, especially for novices at scale [53]. Research Site. Our design of CARE will use the online peer-counseling platform 7 Cups as a case study. 7 Cups is an online peer support service, where support seekers with a variety of mental health problems participate in text-based chats with peer counselors who have completed active listening and other therapeutic training. As of July 2024, 7 Cups had over 429,000 trained peer counselors , supporting over a million people a month. CARE uses the conversational data obtained from 7 Cups to train suggestive models described in sections 3.2 and 3.3.2. The data used in this study to train our models was provided via a partnership with 7 Cups, following the Health Insurance Portability and Accountability Act (HIPAA) and confidential data use agreements. Scope of CARE . Wedesign CARE as a prototype system that can be used in a training or simulated environment where counselors can practice their skills in a safe environment with no risk to real people, and receive feedback or assistance. While prior research on chatbots for mental health support focuses on building virtual counselors (replacement) [1, 50, 62], we develop a scaffolding tool to aid in training (augmentation) for the following reasons. Firstly, the replacement of humans with AI carries serious ethical concerns [60, 79, 85, 153, 153] and massive risks [13, 143] to the support seekers despite potential for scalability. Secondly, CARE allows for human oversight and control. We require counselors to engage with and review the suggested content if they want to use it during the training stage. We envision systems like CARE to be mainly used to train counselors who have already received very abbreviated default training on 7 Cups, not seekers or the vulnerable members who suffer from mental health issues; thus, there is no direct communication between support seekers or patients and CARE . In its current prototype stage, CARE aims to serve as an invaluable tool in a training or simulated environment, providing counselors with a risk-free interaction environment. Our controlled setting ensures safety and allows for essential real-time human oversight. There are many complexities involved in the transition from a prototype to a real-world application beyond CARE 's current scope, which we will discuss in depth in Section 6.3. Overview of CARE . CARE is an interactive tool that works in a private text-based session consisting of two speakers: a support seeker and a peer counselor. CARE aids users by recommending up to three possible responses to counselors in a given situation. These suggestions are built upon psychotherapy literature on Motivational Interviewing [87, 122], which is a client-centered counseling style for eliciting positive behavior change by helping support seekers to explore and resolve ambivalence. We design the tool interface to be the same as the UI of 7 Cups [90] (figure 3) to simulate real-life user studies and obtain genuine user feedback. CARE comprises a backend system with suggestion-generation modules and a front-end user interface. They together form a full stack system. Note that our backend system is designed to be independent of the frontend client for generalizable deployment. The same backend system can be connected to other front-end clients in a plug-and-play fashion.", "3.1 Design Considerations": "We start the design of CARE by engaging with peer counselors on 7 Cups. Our main considerations for the design of CARE are summarized as follows, and we discuss the limitation of our design process in Section 6.3. Design and Evaluation Choices. We concurrently consider the design and evaluation of the tool while building CARE . We evaluate CARE by recruiting peer counselors from 7 Cups as participants. To achieve this, we designed the trainees' experience with CARE to closely mirror the look and feel of 7 Cups sessions, minimizing the need for them to learn a new user interface. This ensures that the study focuses solely on changes in the user behavior due to the addition of AI suggestions, rather than the impact of a new user interface. To achieve these objectives, we aim to integrate CARE into the existing 7 Cups interface rather than designing the user interface from scratch. This approach leads us to design CARE as an add-on panel that extends the 7 Cups chat interface. We discuss the interface design in Section 3.4.1 and suggestions for future work in Section 6.3. Challenges in existing training methods. In the typical educational process of a peer counselor on 7 Cups, a supporter seeking advice on a 1-1 chat shares an anonymized summary of a challenging situation in a chatroom or with a more experienced counselor, keeping the session confidential. Other counselors then provide feedback on handling the situation. While this promotes peer learning, it has some drawbacks: (1) It might be time-consuming for experienced counselors to give feedback, and sometimes there might be a mismatch between the expertise of a specific experienced counselor and the seeker's . (2) Trainees must wait for an experienced counselor to be available, which often delays the process of receiving feedback. Such issues around the efficiency of training and the quality of feedback leads us to design CARE in its current way.", "3.2 Motivational Interviewing Strategies": "Motivational Interviewing (MI) Strategies are a set of client-centric techniques used by therapists for client betterment [87]. The underlying principle of MI techniques focuses on a therapeutic alliance between a patient and a therapist with an emphasis on patient autonomy. These techniques are suitable for online peer counseling platforms given the established effectiveness in reducing drug and alcohol abuse [17, 52, 64, 81], decrease in smoking [46], and reduction of sexual risk behaviors [40, 74]. We used the corpus released by prior work on 7 Cups [122], which identifies and annotates 17 categories of counseling techniques on 14,797 utterances from 7 Cups chats (referred to as 7C-MI hereafter). In this work, we selected a set of MI strategies mainly based on whether there are enough annotations from previous work to build reasonably accurate machine-learning models. This led us to 8 strategies, as shown in Table 1. Future work can easily annotate other types of counseling strategies or employ other psychotherapy frameworks to expand this set. Data Preparation. To find out which MI strategies are suitable in a context or which example response might be appropriate, we can not ask annotators on crowdsourcing platforms to simply perform the annotation, given the sensitive nature of mental health data and data use agreement with 7 Cups. Furthermore, labeling which MI strategies are suitable or producing example responses in a given context by a third party requires a significant amount of effort and expertise. Thus, we make a strong assumption that if a conversation is of high quality as indicated by their satisfaction ratings from support seekers, then these MI strategies used by support providers in their utterances are appropriate. To this end, we collected all conversations in January 2021 that received a rating of 5 on a 5-point Likert scale from support seekers by working with 7 Cups, resulting in 20,445,517 utterances. With written permission from [122], we use their released strategy classifiers trained on utterances between January 2020 and August 2020 to label these utterances in terms of their MI strategies, to construct our corpus ( 7C-HQ ) for predicting the suitable response a support provider should use in response to a message from a support seeker. We randomly split the dataset into training set (%80), development set (%10), and test set (%10). Given the sensitive nature of the domain and the confidentiality of data, we will only release the datasets used in the study with researchers who have obtained data use agreements from 7 Cups.", "3.3 Generating Suggestions": "CARE suggests counseling strategies and potential responses for a conversation in real-time by using a multi-step process described as follows (Figure 2): Step 1. For each of the 8 counseling strategies described in Table 1, CARE predicts the probability of which strategy should be used in the next peer counselor' response using the 5 latest utterances in the chat and a strategy-specific fine-tuned machine learning classifier. Step 2. CARE generates a potential counselor response for each of the top three most probable counseling strategies. This generation is conditioned with the 5 latest utterances in the chat and uses a DialoGPT model fine-tuned with one of the 8 counseling strategies. Step 3. CARE ensures another layer of support seeker safety by filtering all potentially inappropriate generations using a finetuned HateBERT [22]. Step 4. The de-duplicated responses are rendered on the front end in descending order of the probabilities from Step 1. We now describe in detail the machine learning models involved in Step 1, Step 2, and Step 3. 3.3.1 Step 1: Predicting the Most Suitable Counseling Strategy. We use one of the most popular pretrained language models-Bidirectional Encoder Representations from Transformers (BERT) [35], to predict the appropriate counseling strategy that a counselor should use in response to the utterance from the support seeker. We do not use closed-source API-based models like ChatGPT, given that such uses might raise concerns over data security and confidentiality and violate our data use agreement. Moreover, supervised fine-tuned models (like BERT) show state-of-the-art performance compared to in-context learning methods for most classification tasks in social domains [152]. Mathematically, we frame it as a multi-label classification task: given the current conversation between a support seeker and a peer counselor, and a set of possible counseling strategies, identify a set of strategies that are most suitable for the given context. Practically, we first use our automated models of identifying MI strategies mentioned in Section 3.2, to identify (i.e., label) the strategies used in counselors' responses across all conversations. Then, we convert this newly labeled data into a classification task of predicting the strategies in a counselor's response given a seeker's message (context). During the implementation stage, we transform this multi-label classification task into eight binary classification tasks, following the binary relevance paradigm for multi-label classification. We used the most recent five messages as the context for the current conversation, as suggested by prior work [122]. To deal with any class imbalance issue, we used downsampling by sampling the negative class size to be the same as the positive automatic annotations. The final classifiers are built by fine-tuning a pre-trained BERT-base-uncased checkpoint from Hugging Face [142] on 7C-HQ. The results are reported on 7C-MI, the human-annotated instances from [122] as the test set. We show the number of instances, model accuracy, and the F1 scores in Table 1. We found that our machine learning classifiers performed reasonably well, with an overall F1 score larger than 0.705. Given these reasonable predictions of the most appropriate counseling strategies, we then can move forward with the detailed feedback generation. 3.3.2 Step 2: Generating Example Responses. Supporters, especially beginners, might not be able to identify how to respond to a given situation [88], or they may not feel confident about providing an ideal response. To this end, we introduce the task of contextualized suggestion generation with humans in the loop. The goal here is to augment them when they experience high uncertainty in conversation, as well as to provide help to these supporters in formulating their responses. We formulate this task as a conditional text generation problem: given an input (a seekers' post and the context) and a suitable strategy, we generate a statement that provides a sample response so that volunteer counselors can review and use-or edit and then use-in a given session. Similar to the setup in Section 3.2, we identified a set of pairs (post, response) from high-quality conversations filtered by satisfaction ratings as our corpus. Auto-regressive models like GPT-2 [105] and BART [77] have shown great performance on generation tasks. Here, we choose a variant of GPT-2, DialoGPT [150], trained on multi-turn dialog conversations, as its domain shares a considerable quantity of similarities with the counseling setting and is open-sourced thus supporting the fine-tuning on our dataset. Specifically, our model generates responses conditioned on (1) the counseling strategy, and (2) the most recent five utterances in the conversation. We do this by appending the predicted MI strategy token to the dialog history at both training and inference time. By experimenting with different models, we found that DialoGPT 2 outperforms the GPT-2 and BART in terms of our automatic evaluation metrics [78, 96, 149], and thus use it to generate the example response when conditioning on a specific counseling strategy. Table 2 presents the performances of our generation models, with an overall semantic similarity score between a generated text and the ground truth text larger than 0.87 (indicated by BERTScore). The column of Adherence , which refers to whether the generated text exhibits the type of counseling (1) P redict Next S trategy X Context Affirm Prob: 0.8 Support Prob: 0.6 Persuade Prob: 0.2 (2) G enerate responses conditioned on strategies Affirm Response Support Response Persuade Response Affirm Support It is  g ood that you have reached out. I am so sorry to hear that! (4) Order responses by decreasing order of probabilities (3) F ilter out inappropriate responses Low Probability strategy that it is supposed to have, demonstrates relatively good agreement with an overall score of 78%. We also find that the quality of generation differs in strategies, with Grounding , Open Questions , and Introduction being the strategies with the highest Adherence rates, and Reflection , Persuade , and Affirm given the lowest scores. 3.3.3 Step 3: Filtering Out Inappropriate Responses. As an extra layer of safety protection, CARE filters out inappropriate and undesired responses before presenting them to peer counselors. A response is defined as inappropriate if it consists of abusive behavior, the inquiry of unnecessary support seeker personal information/ personal identifiers, or contains unnecessary and excessive swear words. To avoid outputting inappropriate example responses, CARE employs the inappropriate classifier [122], which is a HateBERT model [22] finetuned on human annotations of inappropriate messages on 7 Cups. The classifier is recall-heavy to ensure that CARE refrains from suggesting possibly inappropriate responses. It gives an F1 score of 84.21% on the human-annotated test set of 7C-MI.", "3.4 System Development": "Building upon the aforementioned steps, we design an interactive system CARE to empower counselors during their chats with seekers. CARE integrates front-end and back-end components to optimize the user experience and runtime efficiency 3 . The Server-side event handlers are flexible to connect with any type of front-end client. This allows CARE to be used with any 7 Cups-like online mental health platform and provides training to peer counselors in a platform-agnostic manner. A client-side API reference is used to facilitate the processing of function calls to the back-end API. This enables functionalities such as the initialization of new chat sessions, the addition of new messages to the chat log, and the clearing of sessions. With this architecture, the chat system can return suggestions and generations in real-time. We ensure that the client-side API is lightweight for quick loading of the system on the client end. 3.4.1 Design of the System Front-End. The front end has two views, one for the support seeker and one for the peer counselor. The view for the peer counselor is given in Figure 3. The UI contains the four major components to replicate a chat environment like 7 Cups. These components are the chat information toolbar on the top, the chat window, the collapsible panel for suggestions and responses generated by CARE , and the text input box. The support seeker view of the platform is Cups M Member Chat Information M sort of? told him hope that he gets real job, but he doesn't listen to unfortunately It sounds like he makes you worried. What do you think about this situation? Chat M hate that. the fact that he doens\" have a job makes me so financially stressed. Window M it's been 3 years _ think i've done everything possible to improve this situation Member is typing .. Hide example Affirm Closed Question Support Suggestion am glad you are taking steps Do you want to talk more about understand It seems as Panel towards improving your life. your relationship with your though you are really struggling boyfriend? and it is causing you stress Text Input Type your message here .. Box me, the same as the peer counselor view except for the absence of the collapsible panel. Verbal feedback on the interface was obtained by working with counselors from 7 Cups to simulate the look and feel of the platform. For instance, based on insights from domain users, we have introduced several features: (1) an indicator of ' a user is typing ', and (2) the option for users to hide the tool window. When a peer counselor clicks on one of the potential responses in the tool, the text input box gets populated with the response. The peer counselor can then choose to modify the response before sending it. 3.4.2 Logging of Peer Counselor Actions. Welog 4 peer counselor actions on the suggested responses to understand the impact of our tool in chats. For each utterance, we log the CARE suggestions, the peer counselor's click (choice) in the suggested responses, the peer counselor's click to show and hide the collapsible panel, and the final peer counselor response. We use these logs for our subsequent quantitative evaluation.", "4 Evaluation of CARE via User Studies": "Evaluating the proposed system CARE requires rigorous examination and ethical consideration based on its sensitive and high-stakes cases. We conduct user studies to ensure that our tool can Table 3. Participant Information . ID is the ID of the participant in this study. Tenure denotes the duration a participant has been a peer counselor on 7 Cups by the date they complete their user study session. These quantities are rounded to the closest integer in the largest non-zero unit. CARE Chat marks whether the participant sees CARE 's AI assistance in the first (1) or the second (2) chat in our user study session, which is randomly assigned. Category is the mock chat situation chosen by a participant from A nxiety and R elationship Stress . Information about the Device(s) Used for 7 Cups is collected because 7 Cups provides UIs for computers, tablets, and cellphones, but for consistency, we conduct our user studies using computers only. Half-and-half for this column indicates participants use computers and mobile devices in a Half-and-Half duration fashion. Most Recent Background denotes the participant's most recent, external, relevant academic or professional background. be used widely in practice by working closely with peer counselors from 7 Cups-the end-users of CARE .", "4.1 Recruitment of Participants": "Werecruit peer counselors who have extensive experience with the platform and its expectations, as well as first-hand knowledge of the learning process. Our recruitment criteria required participants to have completed at least 25 conversations on 7 Cups to ensure familiarity with the platform, and to have a peer counselor rating of 4 or higher (out of 5) to ensure they have a moderate understanding of the expectations for effective peer counseling and can assess appropriate responses in various situations. Additionally, we excluded any peer counselor who had been blocked by a support seeker in the past year. This set of inclusion and exclusion criteria positions them in a good position to effectively evaluate our training tool. Besides these criteria, we did not introduce any other exclusion criteria in order to obtain a natural sample of peer counselors. By collaborating with community moderators and platform owners on 7 Cups, we recruited 15 peer counselors from the United States on a first-come, first-served basis (11 females, 3 males, 1 unknown, ages 18-69). 5 All of the participants were recruited via notifications on the 7 Cups mobile/web app and were compensated $20 for their time (the average time duration of the study was a little under 1 hour). The experience level of these participants ranged from 5 days to 8 years on the platform at the time of their interview. We present their non-identifiable information in Table 3.", "4.2 User Study Details": "We test how CARE empowers volunteer counselors by conducting user studies with two conditions: (1) no support, and (2) support provision with CARE . We now describe in detail the different components involved in this user study. 4.2.1 Procedure and Method. For each user study session, based on their familiarity and comfort, a participant (peer counselor) first selects a chat category between anxiety and relationship stress (the two most frequent topics on 7Cups [139]). The participant then tests the system by interacting with a researcher who acts as a support seeker. Both of the systems mimic the chat interface of 7 Cups as described in Section 3.4.1. The order of the chats is randomly assigned using a computer randomizer: one with CARE , and the other without CARE . 4.2.2 Onboarding Participants with CARE . To ensure that each participant is familiar with all the features of CARE , we briefly introduce the tool before they experiment with the system. The participants are also asked to watch a tutorial video emphasizing the voluntary use of CARE and explicitly teaching how to hide and show CARE 's AI assistance (video transcript in Appendix Section E). The video also describes the different (8) counseling strategies along with their examples. The researchers also clarify any doubts and confusions a participant may have about the study. During the chat, when CARE shows AI assistance for the first time, the researchers verbally instruct the participants again that they may show/hide the tool, ignore/modify generated responses, or use it as they prefer. 4.2.3 Conducting Simulated Chats. We select a larger set of representative seeker-supporter conversations from 7 Cups, remove any personal identifiers in these conversations, and manually paraphrase and fuse them, to obtain a smaller set of representative chats. In total, we devise four representative scenarios, two for each category and their corresponding scripts. In each simulated chat, the researchers act as support seekers by sending messages modified from a pre-defined script. In detail, for our focused two categories: relationship stress and anxiety, the research team read roughly 30+ conversations from actual chats in 7 Cups. Researchers involved in the user study in our team have signed up as volunteer counselors and support seekers and conducted observations with more than 100 cumulative hours of counseling time on the platform. This enables the researchers to be well-equipped to interact with participants while conducting the studies. While the prepared scripts help maintain a coherent theme, we also allow for variability in the conversation due to the variability in the peer counselor's responses. The scenarios are summarized in Table 4, and the scripts are provided in Section H.", "4.3 Measure and Evaluation": "We perform quantitative analyses on counselors' use of suggested counseling strategies when they interact with seekers, and qualitative analyses on the participants' feedback on what they like and dislike about each session. We understand the participants' perceptions of CARE by administering a post-task, anonymous questionnaire (Table 7 in Appendix), and also conducting a semi-structured interview. The questionnaire does not ask for any personal user identifiers to allow participants to freely disagree/ agree to the use of CARE and leave their unbiased opinions and comments. The questionnaire contains a mixture of Likert scales, checkboxes, and an open-ended response textbox, depending on the nature of the question. The participants complete the questionnaire during the session and may opt to unmute and ask questions whenever the questionnaire is unclear to them. In the semi-structured interview, we prepare a list of questions 6 and ask follow-up questions upon detecting a potential theme. Afterward, the research team inductively develops a codebook and codes all interview transcriptions and questionnaire responses to the open-ended questions.", "4.4 Ethical Considerations and Safety Protocols": "The Institutional Review Board has been approved for this study at the researchers' institution. The data for the study was collected in partnership with 7 Cups, following HIPAA and confidentiality data use agreements. To protect the privacy of the participants, the data has been anonymized and additional steps have been taken to ensure that it cannot be linked to any specific user. At all times, actual user data was present in secure servers, which could be only accessed by the researchers of this study. All the researchers involved in this study have completed CITI Program certifications on responsible code of conduct in research. All the participants in the user study are older than 18 and have signed a consent form stating their explicit consent. Safety Protocols: We take careful steps to ensure the safety of our participants. First, we note that CARE is designed for training and not for real-time use; thus, supporters are involved in a simulated environment with little risk. We collect feedback and refine CARE in an interactive process based on user insights to make sure it works as expected. Second, at all times, the participants were free to leave the study (with the guarantee of full payment) if they felt uncomfortable with the topics discussed in the chats. Third, our research team was involved in every support provision conversation transcript; for any signs of negative impacts (e.g., negative mood, frustration), an established protocol was to be implemented - the study would be stopped immediately and the participants would then be referred to the consulting clinician and information about available national resources and help-lines was to be shared. None of the participants expressed discontent and seemed uncomfortable in the study.", "5 Results": "As described in Section 4.3, our participants fill out an anonymous questionnaire after they complete the first 30 utterances of each of the two system-testing chats. There are 30 system testing chats from the 15 user studies, which result in 926 utterances sent in those mock chats, where 470 utterances (50.76%) are sent by peer counselors, and the remaining 456 utterances (49.24%) are sent by us, the simulated help seekers. On top of the obtained numerical data from log analysis (Section 5.1) and Survey Responses (Section 5.2), we also conduct semi-structured interviews to understand the impact of CARE (Section 5.3). Detailed results are presented below.", "5.1 Log Analysis": "We analyze the system logs to understand whether CARE has effects on peer counselors and show that peer counselors make use of CARE when it is provided. We report the aggregated numbers at both, the utterance level and the chat level, to give a sense of the overall statistics and account for the difference between individual preferences. 5.1.1 CARE provides assistance 84% of the time. CARE 's AI assistance is frequently provided to peer counselors during the experiment chats. CARE starts suggesting strategies and responses when a chat reaches at least 5 utterances. After that, CARE presents potential responses if the confidence score of the next strategy prediction is greater than a threshold of 0.5. Our analysis shows that across all utterances, CARE gives suggestions for 374 out of 467 utterances (80.09%) that were sent in chats, which is 84.38% of the time if we compute the median at the chat level. 5.1.2 Peer counselors choose to see CARE 93% of the time. Despite being told that they have the choice to see the tool or not see the tool, most peer counselors almost always keep the CARE panel open during the mock chats. The peer counselors choose to turn on CARE for 340 utterances out of 374 utterances (90.91%) when CARE is available during the experimental chats, which takes up 93.20% of the time span on average across chats (std=23.376%, median=100%). Notably, 13/15 peer counselors keep the AI assistance panel shown throughout their experimental chat. 5.1.3 Peer counselors check CARE 's suggestions before sending in around 47% of responses. We join system logs on clicks and messages together to analyze their usage of CARE . When it is the peer counselors' turn, and they see CARE 's suggestion, they click them before sending their response 75 out of 199 times, making the total click-through rate 37.69%. The median across the 15 experimental chats with CARE yields a 46.67% click-through rate. Appendix Section B and C summarize the situations where CARE is clicked and not clicked, respectively. 5.1.4 Peer counselors use a CARE 's response directly or edit it. We compare the text similarity between what CARE suggests and what peer counselors finally send out. From the utterances peer counselors sent after clicking responses suggested by CARE (click-through rate 7 =37.69%), we found that most (45/75; 60%) of the responses were sent without modification from CARE 's suggestion , 8 while the others were altered before being sent. For the 30/75 (40%) edited responses, we calculate the length of the Longest Common Subsequence (LCS) between the clicked suggested response and the actual utterance sent by the peer counselor. This calculation yields a median difference of 41.5 characters between the two strings. On one hand, if we divide the length of each LCS by the length of its corresponding generated response, we get the median 99.3% (mean=87.7%, std=19.6%). On the other hand, if each LCS length is instead divided by the length of the corresponding actual response peer counselors sent, the resulting median is 47.7% (mean=50.8%, std=20.6%). These two ratios together advise that when peer counselors modify a CARE 's suggestion, they usually build upon it by preserving almost the entire suggestion and adding more content to it. 5.1.5 Peer counselors send longer responses with CARE .. The median length of peer counselors' responses increases with CARE compared with its counterpart without CARE . Concretely, when peer counselors see CARE 's suggestions, the median length of responses (77 characters) is significantly longer than the response length without CARE (61 characters) (N = (199, 271), p < 0.01; Mann-Whitney U rank test). (b) Perceptions of CARE (a) Like Aspects (b) Dislike Aspects (c) Helpful Occasions", "5.2 Survey Responses": "5.2.1 Overall Perceptions. As shown in Figure 4a, the responses to the model perception questions show that for most participants, the strategy prediction and suggestion generation of CARE are helpful in terms of whether the counseling strategies suit the situation, and whether the generated responses fit the conversation topic, and help peer counselors. As shown in Figure 4b, all participants agree that CARE is straightforward to use, while more than half of them think it is overall helpful. The median response to 'Will you use CARE ' ( If the tool is provided to you, how often do you think you will make use of it? in the questionnaire) tilts towards positive. Lastly, responses to the question 'Do the examples contain *harmful* message' are: Never (4/15), Very Rarely (6/15), Rarely (3/15), Occasionally (1/15), Frequently (1/15), and Very Frequently (0/15). This indicates that in general the example responses CARE suggested are unlikely to cause harm, but more advanced filters are still needed. 5.2.2 Like & Dislike. To deeply understand what features or aspects of CARE are more appreciated by peer counselors, we visualized their preference responses in Figure 5. We find that more than half of the participants agree that CARE reminds them of counseling strategies (12/15), inspires them to better responses (10/15), and increases their confidence (8/15). While about half of the participants think CARE can disrupt the thought process during conversations, as discussed in Section 5.3.4, its seamlessness may increase as users become more familiar with it in the training environment. 5.2.3 When Is CARE Helpful? As shown in Figure 5c, for the question of when CARE is helpful, we saw from the top three selections to our question ' CARE helps more when...?' that, 13/15 said YES to the peer counselor is new to listening chats , 10/15 mentioned the peer counselor feels more stressed , and 10/15 emphasized when the peer counselor is less confident in the chat's category . This result suggests the effectiveness of CARE in helping, especially novice peer counselors.", "5.3 In-Depth Interview Results": "To provide an in-depth understanding of our findings from the log analyses and questionnaires, this section takes a deep dive into what participants shared with us during their semi-structured interviews.", "5.3.1 How Peer Counselors Use CARE .": "Preference for and against showing CARE . Participant P7 attributed why they keep CARE open throughout the simulated chat to potential inspirations, which also explains the result about the major user preference of showing CARE more often than hiding it in Section 5.1.2. 'I think I would probably have it on more often than not . I'm, just in case it suggested something that I hadn't, that hadn't popped into my mind. ' - P7 Alternatively, P6 and P14 prefer to use CARE as a safety net and refer to CARE when there are uncertainties. 'Only if I'm really on, like, lost on what to say, I don't have any ideas or strategies to go, I would just go there [ CARE 's suggestions]. Other than that, I prefer to go with my train of thought without looking at it.' - P14 . Importance of Optionality. Although most (13/15) participants chose to show CARE instead of hiding it, giving peer counselors an option to hide the tool is still essential in the training process. Some peer counselors (P3, P4, P6, P10) pointed out the need for this option to switch the tool on/off. 'You can switch it on and off as needed. If you pick up a chat with a subject you're not as familiar with, if you decide you want to put it on, you have that option . If you don't want to, you don't have to.' - P4 Adopting a CARE Response. Section 5.1.3 shows that peer counselors click CARE 's suggestion before sending in around 47% of responses. P7 demystifies this behavior by illustrating their usage of CARE : 'I might take a quick glance at it [CARE] and see if it's the same thing I was going to type anyway, and then click on it [the suggestion]. That's definitely a time saver . ' - P7 In terms of intentions behind the modification, P8 mentioned their rationale for doing so is because 'There were some open-ended questions that the tool suggested that I didn't think of, that I thought is really good, and sometimes I use exactly what was suggested . ' . At other times, P8 modifies the suggestion for personalization : 'The personalization part, I still think that the listener would have to be able to write the words out themselves.' Thus, when P8 wants to make the response conform to their style more, they 'fix up the wording to make it a bit warmer and less cold' once in a while. One such modification P8 makes is shown in Fig. 10 in Appendix B. 5.3.2 Peer counselors think CARE is straightforward and helpful. Participants' interview comments on the straightforwardness and helpfulness of CARE are consistent with the survey response results in Section 5.2.1. Straightforwardness of CARE . When being asked to comment on the UI/UX of CARE , P10 compliments on the UI design as they 'like how the suggestions are right above the chat bar.' . P12 describes the intuitive nature of CARE as 'Even if you didn't know about it [how to use the tool] before, you can [directly] use this technology here.' . Helpfulness of CARE . Many participants (P1, P6, P7, P8, P9, P15) also agree that CARE is helpful and also mention the utility of the strategies. Others show satisfaction in the quality of CARE 's suggested responses. 'I think it could be a great tool . I really do. I think it's something that we need in 7 Cups because we don't really train listeners very much. We, it's very minimal training, so we kind of grab listeners once problems are occurring. So if we could put this at the front end, that would be phenomenal.' - P1 Risks of CARE . While CARE is perceived rarely harmful (Section 5.2.1), peer counselors who see CARE 's mistakes think CARE may not actually cause negative consequences during training : 'I did notice just a couple of suggested responses that didn't quite fit, but um, I mean, I think the average listener would know not to click on them. I don't think that they would just randomly start clicking. ' - P7 5.3.3 Interpretation of Likes. This part extends Section 5.2.2 from what to why to gauge the reason behind peer counselors' preference for CARE . Reminds peer counselors of counseling strategies. The top questionnaire answer for what peer counselors like about CARE finds that it reminds peer counselors of counseling strategies (Section 5.2.2). 80% of peer counselors like the fact that CARE reminds them of MI strategies, indicating that the tool's ability to suggest relevant MI strategies is a helpful feature that supplements the short training these counselors receive. Peer counselors P6 and P12 mentioned in their interview how CARE helped them with learning MI strategies . \"The tool is using what the member [support seeker] is saying to come up with these prompts, and it's giving me open question options, it's giving me the different active listening skills that you're supposed to use when you're listening. And people often forget that as listeners. They get stuck on asking questions all the time, or just validating all the time, or just reflecting all the time. And I think this gives me the confidence to explore the different skills that you should have as a listener.\" - P6 \"I think there were a lot of good [strategy] suggestions, and it helped remind me of different coping mechanisms that were appropriate to like, mention in the use with the member [support seeker]. For example, the whole challenging negative thought thing. The suggestions, you know, reminded me of that, because with affirmation, and as, being one tool, for example, it really helped me.\" - P12 Inspire better responses. In Section 5.2.2, we bring out that 10/15 of the surveyed participants reported that they liked how CARE inspires better responses . CARE 's dual-functioning of providing the MI strategy and potential response of what to say is a helpful feature. Even if peer counselors know which MI strategy they want to use, their training and experience may not be extensive enough to adequately apply the strategy to the situation. This is where CARE is able to support counselors by providing examples of using MI in the specific scenarios at hand. Even if counselors do not directly use the suggested utterance, it gives insight into what an alternative/better response might be. Peer counselor P6 mentioned multiple times how CARE helped write better responses, even in situations where the generated responses are not a perfect fit , they inspire the choice of response and the listener can then modify the suggestions, as P6 mentioned: \"[it] inspires me to use different strategies and gives me ideas and then I take that and modify it.\" Increases peer counselors' confidence. More than half of the participants mentioned that using CARE increases their confidence (8/15). During the interviews, many peer counselors (P1, P3, P6, P7, P8, P9, P10, P11) supported that the tool would be greatly beneficial to novice counselors by exposing them to new situations and would help build confidence. Peer counselor P3, who often coaches peer counselors to be better, also mentioned how it helps in new situations. Peer counselor P10 stated that CARE would help provide better service to the support seekers. \"I liked it honestly. I do listener coaching and mentoring. And I liked that this really prompted, I think, a nice variety of responses depending on how comfortable the person is, and diving more deeply into whatever the issue that the member [support seeker] has. So I thought that as I was going through, I thought, this is a great tool for listeners who are newer or are not feeling comfortable with the topic. \" - P3 \"I feel like it [deploying CARE on 7 Cups as part of the training program] would make them [new peer counselors] more knowledgeable. It would make it easier for them to provide better service in a way for the members [support seekers].\" - P10 5.3.4 Interpretation of Dislikes and Potential Failure Modes of CARE . Similar to Section 5.3.4, we also highlight why some peer counselors are inclined to disfavor CARE . We will discuss mitigation strategies in Section 6.3. Disrupts thinking. Some participants (8/15) pointed out that the addition of an interactive tool may distract a peer counselor from the actual conversation. The same peer counselor, P6, who said that the tool gave ideas, also thinks that it leads to an unconscious comparison between the peer counselor's response and the suggested response, which can lead to distractions\"I am always comparing what it was saying to what I was going to say . And just like having another thing to see. \" However, a few participants (P3, P8) disagree. This is supported by prior research that shows similar user feedback on the introduction of any new AI assistance tools: users feel that the tool disrupts thinking initially; over time, they feel that the tool is indispensable as they get used to it [63, 128]. P3 attributes this to friction during the process of getting used to AI assistance. 'It's interesting that you say that [whether CARE disrupts thinking] because at first, I was thinking, 'oh, this is going to be distracting.' But then when I didn't have it, I was like, well, I have to work a little harder because I have to come up with everything on my own. ' - P3 Does not capture nuances. Several participants (P1, P2, P3, P6, P8, P10, P11) mention responses not apprehending the message underlying the conversation. While P1 thinks 'it just that-, it just seemed that there were certain situations or certain, like, nuances of the conversation that the AI just didn't really pick up on.' , P6 sees the difficulties as an inherent property of AI 'As a listener, we're able to detect what this person might mean based on what they've already said, based on our whole conversation, but it's more difficult for AI to do that. They [AI] just take the words interpreted differently. ' Despite this, P10 still thinks 'having that general basis of knowledge is useful for new listeners when they are building their confidence' , and P3 also believes that for new peer counselors CARE is already more humanlike than the existing assistance provided by 7 Cups, which are a set of pre-defined, generic tips randomly prompted to peer counselors during chats: ' In 7 Cups, we have something sort of like this [ CARE ], but it's always the same, and people complain that it sounds like a bot because it doesn't sound very authentic. It [the random tip suggestion in 7 Cups] sounds pretty, you know, like a scripted type of thing. And people will always say like don't send those bot messages, and, so it's hard when you're new, for someone [new] to be comfortable enough to do this. And I liked that these messages [from CARE ] sounded very genuine and related to the issue and not so abstract almost.' - P3 Contains made-up information. A few participants (P8, P9, P11) noticed some false information in CARE 's suggestions. For example, the suggested responses may be sometimes based on inaccurate assumptions. This may be a consequence of the insufficient dialogue context given to CARE . For example, if the gender of the support seeker is not mentioned in the latest message context, CARE may infer one. However, peer counselors can refer to the earlier context of the chat and correct this. Future work may incorporate a dialog summarization model [23] to integrate the conversation context.", "5.3.5 CARE helps both new and experienced peer counselors.": "CARE helps inexperienced peer counselors learn and grow . As most peer counselors (13/15), experienced and inexperienced, believe that CARE helps more the peer counselor is newer to listening chats as it 'gives them suggestions about what to say or how to respond' (P1). Aiding new peer counselors is also a common theme mentioned in interviews and open-ended questionnaire responses. 'New people haven't really dealt with the situation before. And they're not listening and they don't really know exactly what to say yet. [ CARE ] can be quite a guide . ' - P2 A couple of peer counselors, especially those who are involved in the new peer counselor training and mentoring efforts, think CARE will be useful in training new peer counselors . Some compare CARE with the current training modules and reach-out resources on 7 Cups, saying that learning through real-time experience with CARE is more natural and practical than current static training modules : 'With this tool, I think it'll be really helpful because this helps, like, when they are taking the chat instead of, like, help before or after that. Because this is real-time help , so it will be a lot more helpful for the new listeners to get familiar with the process.' - P8 'As most listeners, you know, they don't devote the time [on training modules]. Maybe not most, but some of them at least don't. So this way, you'll just, it's much better to pick up through experience . ' - P12 Others even make recommendations on how to incorporate CARE into the peer counselor training process. P3 mentions how suggested responses give new peer counselors a sense of the expected language and the degree of formality on 7 Cups, making it a helpful tool for new peer counselors to refer to during the practice chats before they are verified. Still, others, especially those who join 7 Cups newly, think other than helping them to learn, hypothetically, the fact that CARE is available on 7 Cups will itself increase their willingness to contribute more to chats. This can potentially help mitigate the high dropout rate or low engagement of peer counselors. 'It makes the whole process likely easier. It could only be helpful. So I guess it would make me feel more open to doing more chats than I usually do . ' - P15 Peer counselors also foresee that new peer counselors' dependence on CARE can decrease over time since they gradually pick up what and how to use different strategies and respond with CARE , which meet our goal of helping new peer counselors to grow. One listener depicts their personal experience with CARE during the mock chats. 'When I started listening, it was helpful because when, when you first get into the listening, it's kind of hard to figure out immediately what you're going to say. So I did read the suggestion that was there . But once I'm more comfortable, I was more comfortable listening, and I kind of got the idea of how to be empathetic, and how to ask open or closed questions, or just sympathize.' - P14 CARE helps experienced peer counselors in difficult situations . In terms of the CARE 's effect on experienced peer counselors, other than the aforementioned responses in the questionnaire, peer counselors think of other occasions where CARE can help, such as dealing with culture/ language barriers, unfamiliar topics, or context switches when being in multiple chats. By suggesting suitable responses, CARE can help mitigate the language barrier or age gap between a peer counselor and a help seeker. 'It [CARE] could be really helpful. So I know for some people on 7 Cups, they don't always have the same native language. Sometimes there are language differences or, like slang words that aren't always understood.' - P1 'Sometimes you have trouble connecting with people who are not your age or even closer. I want to feel like a colleague and not a mother. You know, so maybe the tool would help in that in that instance.' - P4 Peer counselors often specialize in a set of topics, yet they often have to converse on topics beyond that. A few participants describe difficulties in new topics which CARE helps mitigate: '[listeners often] feel stuck in a chat, which happens quite often' (P9). Specifically, P4 encountered this situation during the simulated chat and reflects that: 'My specialty is grief. Relationship stress, [the category of the simulated chat], is not something that I'm as familiar with. I mean, I am, but I'm better on certain topics. So I think it would help me, you know, with the category being as not as familiar . ' -P4 Many peer counselors manage multiple chats concurrently, P3 talks about how CARE can help in such context switches of peer counselors by hinting according to their experience from peer counselor coaching. 'I have some people that I coach who take multiple chats and have a hard time managing them. This [CARE] would allow them, I think, to manage them more properly because they would have some kind of preset options to choose from and not have to come up with everything originally.' - P3 To sum up, via our system log analyses, questionnaires, and semi-structured interviews, we found that: \u00b7 General Perception : Overall, peer counselors think CARE is helpful and often use the tool when provided. The tool also leads to longer peer counselor responses on average. Peer counselors also agree that the tool is easy to use and these suggestions are natural. Analysis of system logs and surveys both showed that users choose to use CARE more often than not when given an option. \u00b7 Situations where CARE helps : Peer counselors think that CARE inspires better responses and reminds them of relevant counseling strategies. This helps novice counselors gain experience and confidence. Additionally, experienced peer counselors agree that CARE will help them with difficult and new topics.", "6 Conclusion and Discussion": "This work develops an interactive training agent CARE for empowering volunteer counselors on online peer-to-peer counseling platforms For any given chat between a support seeker and a peer counselor, CARE automatically suggests counseling strategies and provides tailored responses based on the conversation context to assist counselors. We built upon the motivational interviewing framework and focused on a set of representative counseling strategies. We then develop contextualized classifiers to highlight which counseling strategies are needed in a given situation and scalable language generation approaches to generate example responses on the fly. We integrate different components into the interactive counseling support tool CARE to enable a realistic user experience. Through both quantitative analysis and qualitative user studies, we demonstrate the efficacy of CARE on a set of representative counseling scenarios.", "6.1 Implications and Design Recommendation": "This work combines psychotherapy theories and advances in natural language processing to build an interactive assistant CARE that empowers peer counselors on online peer counseling platforms. Our contributions consist of operationalizing theory-driven motivational interviewing strategies, developing contextualized natural language models, designing an interactive HCI system, analyzing observational data from system logs and questionnaire results, as well as performing semi-structured interviews-all of these demonstrate an example paradigm on how to build both theory-driven and model-driven systems to augment human capabilities via AI. One straightforward design recommendation is to use CARE to complement the current training programs on 7 Cups and many other similar online peer counseling platforms. As we discussed earlier, existing mechanisms of training or scaffolding largely rely on human supervision, which is costly and difficult to scale. Volunteer counselors often do not have the rigorous professional training as their offline counterparts and most online counseling platforms only provide very abbreviated training. By evaluating it with 15 stakeholders both quantitatively and qualitatively, we demonstrate the possibility of CARE as an initial effective effort to improve these training mechanisms. For instance, integrating CARE -assisted practice chats into the training curriculum for new peer counselors could facilitate simulation-based learning [75, 76]. This approach can be further enhanced by incorporating support seeker bots [34, 133]. With appropriate ethical consideration, CARE also holds great potential to be deployed for realtime scaffolding which supporters can turn to during their interactions with seekers. This provides supporters with actionable suggestions on which skill best fits in a given scenario, and how to formulate what they are going to say, with the long-term goal of increasing the number of people they can help and the effectiveness of their interactions in addressing the mental health needs of support seekers. We design CARE to be primarily used as a training or simulated roleplay environment for support providers so that they can practice their skills in a safe environment, without harming real people [116]. While the user studies and findings are grounded in the context of 7 Cups as the primary platform, findings on the impact of such an AI tool, design recommendations for the deployment of the tool, and the general perception of the tool are broadly applicable to other scaffolding and online communities with similar setups of private text-based communication channels. These channels include active listening skills, cultural sensitivity, negotiation, conflict resolution, empathy, persuasion, public relations, feedback delivery, and crisis communication. In other words, this system is generic and agnostic to any online peer counseling platform and can be used in many similar environments[146]. From a technical perspective, CARE consists of two main functions: highlighting which counseling strategies are most suitable and suggesting example responses in a given context. Each function can be used as an individual sub-tool to help specific use cases in online support groups. For instance, the resulting machine learning classifiers in detecting the most suitable strategies can also be used to understand the relationship between different strategies and their influence on conversation success or the well-being of support seekers.", "6.2 Ethical and Societal Considerations for Tools like CARE": "While this work shows promise in using AI to provide feedback to peer counselors, societal and ethical considerations must also be considered as we deploy systems like CARE : Distributional Shift. We design CARE as a scaffolding practice tool that peer counselors can use during training, with the hope that they will be able to transfer these skills to the real world. As with other training environments, we acknowledge this shift between training and the real world. We have communicated to our users that our training system is only intended for practices and that instead of completely relying on this feedback, they should use it to learn how to use these strategies effectively. Deployment Considerations. This work demonstrates how CARE can empower peer counselors as proof of concept. The leap from a prototype to a real-world application involves complexities that transcend the current scope of CARE . For one, the dynamics of counselor-seeker interactions in real-world settings are far more nuanced and unpredictable than those used in our simulated environment. Furthermore, real-world applications' ethical considerations, data privacy, and security aspects demand rigorous scrutiny and adaptations beyond what we have introduced to CARE . We urge future work to investigate these dimensions seriously when considering the deployment of tools like CARE , such as how to maintain privacy and prevent data leakage when using AI models, how to continuously update the model to correspond to real-world events like the pandemic, as well as how to moderate and audit the processes of how these models are used so that norms and policies are updated accordingly. Over-Reliance. In addition, users may be heavily reliant on our tools, which may make them uncomfortable upon lack of access to the tool in real-world situations [97]. Furthermore, we recommend tools like CARE as a complement to existing training tools, as AI blind reliance can lead to cognitive atrophy [2]. To encourage peer counselors to learn with CARE rather than to rely on it, developers can incorporate various measures like warnings and cognitive forcing functions to ensure user engagement with CARE with an emphasis on the unavailability of CARE in actual chats[16]. Personalized Feedback. To improve the effectiveness and user satisfaction of feedback generation tools like CARE , they can be further improved to learn user preferences over time, similar to recommender systems, which adapt based on user behavior patterns. Just as some participants suggested-' every listener has a different way of responding and showing empathy ', adaptive feedback generation based on individual counseling styles could make CARE more responsive and personalized. Biases and Stereotypes with AI Suggestion. Given the sensitive nature of this study on online peer counseling platforms, any use of AI models in providing assistance might introduce concerns about user safety and suffer from the inherent limitation of unwanted biases. We have followed responsible AI and safety best practices in this work by implementing appropriate guardrails and filtering unwanted output before presenting any content to supporters, as shown in Figure 2 and by maintaining safety protocols as described in Section 4.4 to disclose risks to users. We recommend the development and use of better algorithms for debiasing and content filtering [61], as well as introducing more real-time human oversight and control when deploying similar systems. While the exact safety requirements vary by task configuration, we recommend (1) model validation with experienced peer counselors and platform developers, (2) AI and human content moderation and filtering, and (3) humans at the center of control. Augmentation and Automation. Prior studies have explored the use of chatbots or large language models as peer counselor surrogates [1, 50, 62], to make peer counseling labor-free and scalable. Nevertheless, in high-stake and high-touch scenarios like counseling, such automation can pose significant risks for support seekers who suffer from mental health issues and raise ethical and legal concerns [60, 79, 85, 143, 153]. Despite extensive discussions and reported uses 9 for using LLMs as therapists, recent research has shown that LLM simulated therapists suffer from hallucination, caricature [27], stereotypes, and low-quality output [29]. Given all of these risks associated with automation , our work takes an augmentation perspective to use AI to help the helper. Stade et al. [131] used an analogy of building an autonomous vehicle to discuss the role of AI automation for counseling and categorized the space into assistive, collaborative , and fully autonomous ; our augmentation perspective is similar to the assistive dimension. Overall, we argue that such an augmentation route in the current form of AI tools can provide a balance between scale and risk management, and automation would require more advanced AI capabilities, stronger interdisciplinary collaboration, and attention to issues such as evaluation, risk, bias, and transparency, which remain challenges for LLMs today.", "6.3 Limitations and Future Work": "There are a few limitations to this work, which we discuss in detail here with recommendations for future work. Multi-party and Long Conversations. We design CARE as a training tool primarily for one-to-one conversations and evaluate it with relatively short conversations. However, some online support provisions might happen in multi-user chat forums like Reddit or public chat groups. Furthermore, conversations between support seekers and peer counselors can be very long in the real world; such long-term dependencies may not be captured well in the current setup when providing assistance. Generalization of CARE to multi-party and long conversations requires further investigation. Support Seekers' Perspectives. The primary goal of CARE is the training of novice peer counselors and not support seekers. Therefore, various experiments focus on supporter perception and preferences for the use of CARE , including Wizard-of-oz operationalization of the end-to-end system. While future studies could include support seekers - especially their perceptions about peer counselors' AI-supported training, we believe that is beyond the scope of this study. Design Choices. Although we utilized the current state-of-the-art models for recommending counseling strategies and example responses, some generation results are still not very satisfactory as pointed out by participants. Such limited quality of generations may negatively affect the training or suggestion provision to peer counselors. Future work can further improve these models by performing extensive parameter searches and utilizing more recent language models. In terms of multiple design choices such as counseling strategies, we mainly focus on a subset of motivational interviewing strategies, which provide a solid foundation for CARE . However, there are other types of MI strategies or other prominent counseling techniques such as cognitive behavioral therapy which could be strong alternatives for empowering peer counselors. While the chosen subset of strategies covers most messages in the 7 Cups dataset, its potential influence on the trainees' variety of strategies remains unknown, such as whether they may be biased against the strategies exclusive of our chosen subset. Similarly, for multiple threshold choices such as the number of responses to show on the CARE interface, we chose them based on the researchers' expertise in design and feedback from stakeholders. We plan to iteratively refine them and perform more robustness checks for the next version of CARE . Long-Term Impact. As an initial effort to empower peer counselors, our work mainly focuses on the development and design of CARE as a prototype system, and the evaluation of it with domain users via interviews and system logs in a simulated one-time chat environment. Our work has not evaluated any long-term effects of CARE , such as its influence on certain skill acquisition of peer counselors, on the mental health outcome changes in support seekers, or the change in listener engagement to a conversation with the availability of CARE . We plan to continue improving CARE and evaluate its long-term impact by longitudinally working with more domain users. Generalization to Other Platforms. We designed and evaluated CARE by working with domain users-peer counselors from 7 Cups, as a result, our system design principles can be applied to similar online peer counseling platforms, especially those that share similar training and chatting feature, such as Supportiv 10 and TalkLife 11 . While we expect many findings to be platform-agnostic, it is difficult to generalize without rigorous testing with specific domain users. We leave this to future work to examine the generalizability of similar tools across different peer counseling platforms. Representativeness. During our user studies, we asked peer counselors to focus on two of the most popular topics: relationship stress and anxiety. However, online peer counseling platforms cover a wide variety of issues, including depression, health, LGBTQ+ matters, dissociative identity, home life, self-improvement, and more. Future research could explore the efficacy of our system across a broader range of topics to improve generalizability. Additionally, our user studies included only 15 peer counselors, located in the United States, and using English in their sessions. We do not have data on the actual distribution of gender, location, and language of peer counselors on 7 Cups. Significant differences between our participants and the actual counselor population could introduce biases into our observations. Furthermore, the participating peer counselors varied in their length of service, with most lacking formal counseling training. Expert opinions on the quality of CARE may more accurately reflect its potential impacts. Expanding this work to include more participants with diverse demographics, counseling backgrounds, languages, locations, and expertise could provide more nuanced and generalizable findings.", "Acknowledgments": "The authors would like to thank the reviewers and members of SALT Lab for their feedback. This work was supported by an NSF grant IIS-2247357.", "References": "[1] Alaa A Abd-Alrazaq, Mohannad Alajlani, Nashva Ali, Kerstin Denecke, Bridgette M Bewick, and Mowafa Househ. 2021. Perceptions and opinions of patients about mental health chatbots: scoping review. Journal of medical Internet research 23, 1 (2021), e17828. [2] Sayed Fayaz Ahmad, Heesup Han, Muhammad Mansoor Alam, Mohd Rehmat, Muhammad Irshad, Marcelo Arra\u00f1oMu\u00f1oz, Antonio Ariza-Montes, et al. 2023. Impact of artificial intelligence on human loss in decision making, laziness and safety in education. Humanities and Social Sciences Communications 10, 1 (2023), 1-14. [3] Kathina Ali, Louise Farrer, Amelia Gulliver, and Kathleen M Griffiths. 2015. Online peer-to-peer support for young people with mental health problems: a systematic review. JMIR mental health 2, 2 (2015), e4418. [4] Tim Althoff, Kevin Clark, and Jure Leskovec. 2016. Large-scale analysis of counseling conversations: An application of natural language processing to mental health. Transactions of the Association for Computational Linguistics 4 (2016), 463-476. [5] Nazanin Andalibi, Oliver L Haimson, Munmun De Choudhury, and Andrea Forte. 2018. Social support, reciprocity, and anonymity in responses to sexual abuse disclosures on social media. ACM Transactions on Computer-Human Interaction (TOCHI) 25, 5 (2018), 1-35. [6] Joe Armstrong. 2010. How effective are minimally trained/experienced volunteer mental health counsellors? Evaluation of CORE outcome data. Counselling and Psychotherapy Research 10, 1 (2010), 22-31. [7] David C Atkins, Mark Steyvers, Zac E Imel, and Padhraic Smyth. 2014. Scaling up the evaluation of psychotherapy: evaluating motivational interviewing fidelity via statistical text classification. Implementation Science 9, 1 (2014), 1-11. [8] Azy Barak. 2007. Emotional support and suicide prevention through the Internet: A field project report. Computers in Human Behavior 23, 2 (2007), 971-984. [9] Amit Baumel. 2015. Online emotional support delivered by trained volunteers: users' satisfaction and their perception of the service compared to psychotherapy. Journal of mental health 24, 5 (2015), 313-320. [10] Amit Baumel and Stephen M Schueller. 2016. Adjusting an available online peer support platform in a program to supplement the treatment of perinatal depression and anxiety. JMIR mental health 3, 1 (2016), e5335. [11] James Bennett-Levy. 2006. Therapist skills: A cognitive model of their acquisition and refinement. Behavioural and Cognitive Psychotherapy 34, 1 (2006), 57-78. [12] Samantha L Bernecker, Kaitlin Banschback, Gennarina D Santorelli, and Michael J Constantino. 2017. A webdisseminated self-help and peer support program could fill gaps in mental health care: Lessons from a consumer survey. JMIR mental health 4, 1 (2017), e4751. [13] Charlotte Blease and John Torous. 2023. ChatGPT and mental healthcare: balancing benefits with risks of harms. BMJ Ment Health 26, 1 (2023). [14] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877-1901. [15] Roger Bruning, Michael Dempsey, Douglas F Kauffman, Courtney McKim, and Sharon Zumbrunn. 2013. Examining dimensions of self-efficacy for writing. Journal of educational psychology 105, 1 (2013), 25. [16] Zana Bu\u00e7inca, Maja Barbara Malaya, and Krzysztof Z Gajos. 2021. To trust or to think: cognitive forcing functions can reduce overreliance on AI in AI-assisted decision-making. Proceedings of the ACM on Human-computer Interaction 5, CSCW1 (2021), 1-21. [17] Brian L Burke, Hal Arkowitz, and Marisa Menchola. 2003. The efficacy of motivational interviewing: a meta-analysis of controlled clinical trials. J Consult Clin Psychol 71, 5 (Oct. 2003), 843-861. [18] Moira Burke and Robert Kraut. 2008. Mind your Ps and Qs: the impact of politeness and rudeness in online communities. In Proceedings of the 2008 ACM conference on Computer supported cooperative work . 281-284. [19] Moira Burke, Robert Kraut, and Elisabeth Joyce. 2010. Membership claims and requests: Conversation-level newcomer socialization strategies in online groups. Small group research 41, 1 (2010), 4-40. [20] Do\u011fan Can, Panayiotis G Georgiou, David C Atkins, and Shrikanth S Narayanan. 2012. A case study: Detecting counselor reflections in psychotherapy for addictions using linguistic features. In Thirteenth Annual Conference of the International Speech Communication Association . [21] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. 2021. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21) . 2633-2650. [22] Tommaso Caselli, Valerio Basile, Jelena Mitrovi\u0107, and Michael Granitzer. 2021. HateBERT: Retraining BERT for Abusive Language Detection in English. In Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021) . 17-25. [23] Jiaao Chen, Mohan Dodda, and Diyi Yang. 2022. Human-in-the-loop Abstractive Dialogue Summarization. arXiv e-prints (2022), arXiv-2212. [24] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021). study. Alcohol Alcohol 40, 6 (Aug. 2005), 535-539. [47] Stephanie Greer, Danielle E. Ramo, Yin-Juei Chang, Michael Fu, Judith Tedlie Moskowitz, and Jana Haritatos. 2019. Use of the Chatbot 'Vivibot' to Deliver Positive Psychology Skills and Promote Well-Being Among Young People After Cancer Treatment: Randomized Controlled Feasibility Trial. JMIR mHealth and uHealth 7 (2019). [48] Sangdo Han, Jeesoo Bang, Seonghan Ryu, and Gary Geunbae Lee. 2015. Exploiting knowledge base to generate responses for natural language dialog listening agents. In Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue . 129-133. [49] Sangdo Han, Kyusong Lee, Donghyeon Lee, and Gary Geunbae Lee. 2013. Counseling dialog system with 5w1h extraction. In Proceedings of the SIGDIAL 2013 Conference . 349-353. [50] MDRomael Haque and Sabirat Rubya. 2023. An overview of chatbot-based mobile mental health apps: insights from app description and user reviews. JMIR mHealth and uHealth 11, 1 (2023), e44838. [51] Andrew Hard, Kanishka Rao, Rajiv Mathews, Swaroop Ramaswamy, Fran\u00e7oise Beaufays, Sean Augenstein, Hubert Eichner, Chlo\u00e9 Kiddon, and Daniel Ramage. 2018. Federated learning for mobile keyboard prediction. arXiv preprint arXiv:1811.03604 (2018). [52] Jennifer Hettema, Julie Steele, and William R Miller. 2005. Motivational interviewing. Annu Rev Clin Psychol 1 (2005), 91-111. [53] Clara E Hill, Jessica Stahl, and Melissa Roffman. 2007. Training novice psychotherapists: Helping skills and beyond. Psychotherapy: Theory, Research, Practice, Training 44, 4 (2007), 364. [54] Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural computation 9, 8 (1997), 1735-1780. [55] Brenda E Hogan, Wolfgang Linden, and Bahman Najarian. 2002. Social support interventions: Do they work? Clinical psychology review 22, 3 (2002), 381-440. [56] Jess Hohenstein and Malte Jung. 2018. AI-supported messaging: An investigation of human-human text conversation with AI support. In Extended abstracts of the 2018 CHI conference on human factors in computing systems . 1-6. [57] Thomas K Houston, Lisa A Cooper, and Daniel E Ford. 2002. Internet support groups for depression: a 1-year prospective cohort study. American Journal of Psychiatry 159, 12 (2002), 2062-2068. [58] Hui-Wen Huang, Zehui Li, and Linda Taylor. 2020. The effectiveness of using grammarly to improve students' writing skills. In Proceedings of the 5th International Conference on Distance Education and Learning . 122-127. [59] Xiaolei Huang, Lixing Liu, Kate Carey, Joshua Woolley, Stefan Scherer, and Brian Borsari. 2018. Modeling temporality of human intentions by domain adaptation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing . 696-701. [60] Zac E Imel, Derek D Caperton, Michael Tanana, and David C Atkins. 2017. Technology-enhanced human interaction in psychotherapy. Journal of counseling psychology 64, 4 (2017), 385. [61] Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, et al. 2023. Llama guard: Llm-based input-output safeguard for human-ai conversations. arXiv preprint arXiv:2312.06674 (2023). [62] Muhammad Nazrul Islam, Shahriar Rahman Khan, Noor Nafiz Islam, Md Rezwan-A-Rownok, Syed Rohit Zaman, and Samiha Raisa Zaman. 2021. A mobile application for mental health care during covid-19 pandemic: Development and usability evaluation with system usability scale. In Computational Intelligence in Information Systems: Proceedings of the Computational Intelligence in Information Systems Conference (CIIS 2020) . Springer, 33-42. [63] Maurice Jakesch, Megan French, Xiao Ma, Jeffrey T. Hancock, and Mor Naaman. 2019. AI-Mediated Communication: How the Perception That Profile Text Was Written by AI Affects Trustworthiness. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (Glasgow, Scotland Uk) (CHI '19) . Association for Computing Machinery, New York, NY, USA, 1-13. https://doi.org/10.1145/3290605.3300469 [64] Chad D Jensen, Christopher C Cushing, Brandon S Aylward, James T Craig, Danielle M Sorell, and Ric G Steele. 2011. Effectiveness of motivational interviewing interventions for adolescent substance use behavior change: a meta-analytic review. J Consult Clin Psychol 79, 4 (Aug. 2011), 433-440. [65] Tuo Ji, Hang Yan, and Xipeng Qiu. 2021. s. In Proceedings of the 2021 conference on empirical methods in natural language processing . 3544-3551. [66] Anjuli Kannan, Karol Kurach, Sujith Ravi, Tobias Kaufmann, Andrew Tomkins, Balint Miklos, Greg Corrado, Laszlo Lukacs, Marina Ganea, Peter Young, et al. 2016. Smart reply: Automated response suggestion for email. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining . 955-964. [67] Viktoria Kantor, Matthias Knefel, and Brigitte Lueger-Schuster. 2017. Perceived barriers and facilitators of mental health service utilization in adult trauma survivors: A systematic review. Clinical Psychology Review 52 (2017), 52-68. [68] Stavroula Karapapa and Maurizio Borghi. 2015. Search engine liability for autocomplete suggestions: personality, privacy and the power of the algorithm. International Journal of Law and Information Technology 23, 3 (2015), 261-289. [69] Alan E Kazdin and Stacey L Blase. 2011. Rebooting psychotherapy research and practice to reduce the burden of mental illness. Perspectives on psychological science 6, 1 (2011), 21-37. [91] Kathleen O'Leary, Arpita Bhattacharya, Sean A Munson, Jacob O Wobbrock, and Wanda Pratt. 2017. Design opportunities for mental health peer support technologies. In Proceedings of the 2017 ACM conference on computer", "A Appendix": "", "B Positive Example Chat Excerpts": "We selected one excerpt for each common type of situation in which CARE was shown to the participant, and the participant checked it. The text in the bottom input box is the actual response the participant sent. Texts adopted by the participants are highlighted in yellow or green . Hi, how can help? M have problem getting along with my boyfriend Oh sorry to hear that Would you like to tell me more about the problems you are experiencing? M Okay. My boyfriend started studying abroad. feel so lonely, so we make video calls very frequently Hide example Affirm Closed Question Open Question understand see, do you think it is because of do you think your relationship your relationship stress or going? something else? How do you think your relationship is going? How That is greatl You seem very determined to get through this situation and be able help yourself: You're a very strong individual to be so resilient and brave to reach out! think that your therapist would definitely be a great step to bringing you both closer again! M Aw thank you for the kind words. really don't want to break up with him. That's why am trying very hard to resolve the problem. It sounds like you've been doing an amazing job at handling your emotions and Im glad that you're still willing to take steps to address them. M Hmm that will probably mean, talking to the therapist about this M Thanks for that Hide example Affirm Support Persuade hope it goes well for you! understand it can be difficult when believe in you! people are not in our control but they need support and love. It might sound cliche but if he was truly understand it can be difficult when people are not in our control but they need support and love It might sound cliche, but if he is truly so dear to you; you are doing everything possible and I'm sure that in the end you will reach a place that is happiest for you both! work. M Not really. We have been in a relationship for 3 years with no problem. but our relationship became distance 3 months ago M and that was when everything bad started. Do you think it's possible to talk about these feelings and things to him? M Hmmmm tried, but he wasn't listening. He insisted on reducing the frequency of calls . Hide example v Reflection Grounding Closed Question understand how difficult that can see what you mean Have ever considered reaching especially if your partner is not out to someone else who can help to listen you feel better? see what you mean Have you ever considered reaching out to someone else who can help you feel better? long you be, willing Have you talked with anyone else in your life who knows more about the situation? M unfortunately no. am afraid of letting people know this support system can sometimes be helpful when you start to feel discouraged. understand why you would feel scared. You came here s0 that was a good step to takel M Thanks for the kind words! our negative emotions affect each other. Its really difficult for me Hide example Affirm Support Closed Question You are welcome. And it is great It is definitely hard on everyone and Do you have any hobbies or that you have reached out to it must have been very hard for you. interests? someone It is definitely hard on everyone and it must have been very hard for you. am honored to be listening to you in this moment. M it's very complicated. hard to explain.. Ah see, do you want to try explaining what's on your mind? M okay, my lover has become a real trouble to me M Well, he lost his job three years ago, playing video games every feel so financially stressed Hide example Support Reflection Closed Question Oh wow, that must be really tough wow, that sounds like a lot of stress Ohh understand, have you talked for you about this with him? Oh see, that sounds like a lot of stress on both of you well._ day .", "C Negative Example Chat Excerpts": "We selected excerpts for situations in which CARE was shown to the participant, but the participant did not select it. M No. feel so lonely, so we make video calls very frequently, but he hates that see, sounds like a lot of stress M It does. | feel devastated. he means a lot to me What do you think could help you get through this situation? M Discussing my feeling with him? tried, but he isn't listening Hide example Affirm Closed Question Grounding That's greatl Talking about your Do you have any ideas on how to That's understandable feelings is always a good start approach the situation better? That's definitely hard when you want to communicate but he doesn't seem like he wants to listen. M don't know. feel so lonely Yeah can totally understand that. When you say loneliness do you mean in terms of family , friends, friends or something else? M do hang out with people once in a while_ but we don't talk about real struggles in life see, is that something you wish you had a friend to talk to about then? M Exactly . feel alone but don't know who to talk to Hide example Support Closed Question Reflection That's perfectly normal to feel Maybe your parents or siblings? Or Thats difficult It must be hard to be alone even friends? understand that: Feelings of loneliness can be challenging to deal with. Have you considered reaching out to maybe a family member? very", "D Details of System Development": "Specifically, the system is hosted on an Amazon AWS server with an Ubuntu Operating system (32 Threads, 480GB RAM) having 8 Tesla k80 GPUs (12 GB RAM). These specifications enable CARE to interact seamlessly with humans. The back end is implemented with a Flask API that May ask who is putting high expectations on M My friends and family-I have claimed that I'll be a surgeon literally to everyone; its so shameful if eventually don't become one understand that a lot of pressure comes from family, but you have to do what's right for you! hope your family would want you to do what makes you happiest! There's no shame in changing your mind at any time! Have you tried talking to them about how you are feeling? have definitely changed my mind on my career so many times :P M Oh really?! though have no choice-If don't get into a med school, don't know what my value is Hide example Affirm Reflection Support That sounds like a great idea! Yeah can imagine. It must feel like It can feel like there isn't much out struggle to think of what you there. But sometimes we just need could possibly achieve if you were to find the right path to success. to g0 through this! You always have a choice! Maybe we could talk about some things that you like about yourself that don't involve med school? There are so many great things about you! you? That must be hard, I'm sorry you had to go through all of this Do you think it's possible to talk about these feelings and things to him? M Hmmmm tried, but he wasn't listening. He insisted on reducing the frequency of calls. see what you mean , Have you ever considered reaching out to someone else who can help you feel better? M Oh haven't tried that. could probably talk to my bestie? Hide example Affirm Closed Question Persuade Yes thats a good idea Have you thought about talking with Maybe you should try talking with your boyfriend or anyone close to your friend you in general? Im glad you're thinking of alternatives; what else can you do uses WebSockets to enable continuous bi-directional communication with the chat client. The functions provided by the back-end API fulfill the requirements of the system such as posting and pushing log-in credentials, posting chat logs for post-hoc analysis, and pushing generations from the back-end models. Server-side event handlers are invoked through API function calls made by the front-end interface. All models (DialoGPT, BERT) are hosted on the same server to enable faster communication. Given the confidentiality of the training data of our models, since releasing a language model is subject to potential data leakage [21], we do not publicly release our fine-tuned models.", "E Transcript": "This video introduces a new system for listeners to chat with members. It's similar to the Seven Cups one, except that it will suggest generated strategies and example responses to you. We will discuss them in detail later. Please note that the use of the suggestions and generations is voluntary. In other words, you can use them if they help, but if they don't, feel free to disregard them. First of all, you will receive a chat ID and a user ID to log in. Then, you can start the chat with us. It looks and feels just like Seven Cups. When the tool feels confident, it will suggest strategies and example responses. When the generations are available, you can click on the box over the message input field to see the strategies and example responses. Each suggestion comes with the corresponding example response. For example, this response uses this strategy. Click a response to fill the input box with it. You may edit the response before sending it if you wish. Now, let's look at the strategies one by one. [Pause] That's it. Please feel free to ask questions if anything is unclear as we continue.", "F Supplemental Discussion": "", "F.1 Utilization and Learning Outcomes": "The effectiveness of writing assistants is often assessed using click-through rate [25, 56, 66, 104, 113, 135]. The click-through rate (CTR) helps understand the user engagement, content effectiveness on which the participant prefers strategies and acts as an overall proxy for the generation quality. However, it remains unclear whether this metric reflects changes in learning outcomes due to its use in writing assistants. Therefore, studies in educational contexts employ additional metrics to evaluate their impact. For instance, Dewi et al. [36] and Huang et al. [58] assess students' grades in writing exams, while Nazari et al. [89] use established tools to analyze learners' cognitive engagement [43], self-efficacy for writing [15], and achievement emotions [98]. Although evaluating the long-term outcomes of CARE is beyond the scope of this study, we recommend that future researchers measure the influence of CARE on trainees and their support seekers using peer counseling metrics (e.g., [139]).", "F.2 Representativeness of Participants": "The participant pool in this study consists predominantly of females (11 females, 3 males, 1 unknown). This gender distribution is consistent with prior research on 7 Cups [9, 148] and other mental health platforms [112, 120]. We hypothesize that this skewed distribution may mirror the actual population, as care work has historically been dominated by females [141]. We recommend that future research endeavors seek to include a more diverse participant pool to provide a broader perspective (Section 6.3).", "G Semi-Structure Interview Scripts": "Other than what you might have typed in the questionnaires, What do you like about the tool? What do you dislike about the tool? What are the advantages of using this tool? What are the disadvantages of using this tool? In what circumstances will this tool help more? In what circumstances did this tool help less? Did the tool affect you in any way during the Listening chat? In what way? Comments on components: suggestion, generation, UI, UX? Did \"whether the tool was available\" affect how you feel during the chat? In what way? Other than whether the tool was available, did you notice anything else that affected how you feel during the chat? What and how? Does \"whether such a tool is available on 7 Cups\" affect how you feel about conducting Listening chats in general? Do you want to see such a tool deployed? In what way? \"Any background in psychology, counseling, or therapy? -> How long have you been providing therapy?\" Mentor new listeners? Do you do counseling other than volunteering as a listener on 7 Cups? Any other thoughts, comments, etc. on anything you'd like to share?", "H Scripts for Mock Chats": "We show the scripts for mock chats used in our user studies in Table 8-11.", "I Access to the training datasets in the study": "7Cups retains ownership of the training and evaluation data used in this study. Researchers interested in accessing these datasets are encouraged to contact 7 Cups to arrange the necessary data use agreements to access the train datasets for the study. Received January 2024; revised July 2024; accepted October 2024", "Table 8. Script for Anxiety 1": "", "Introduction": "hi there nice to meet you pretty bad, and that's why I'm here i'm doing badly What happened hmm both sound fine for me. well, i'll take the exam for medical school admission next month but I don't think I can get into any at all I feel like I have sacrificed a lot, but no school on earth will admit me anyways I don't know what the purpose of living like this is College life I don't have time to enjoy my college life either I just study all day no friends, no life, not like other college students I feel like I don't deserve a break because I'm not good enough Expectation i have claimed that I'll be a surgeon literally to everyone it's so shameful if I eventually don't become one People's expectations on me are so high, making me feel so stressful... People make me feel that if i get good grades in school, I have to be a surgeon. This feeling is awful. I believe everyone is standing there, watching me fail. I hate this. Emotion I hate studying all day. I can't stand with it anymore i have no choice. If I don't get into a med school, I don't know what my value is what can i do? i literally have no idea I feel so bad Unconfidence i don't think i'm that good i am not as brilliant as those medical school students i don't think i can make it", "Table 9. Script for Anxiety 2": "", "Table 10. Script for Relationship Stress 1": "", "Table 11. Script for Relationship Stress 2": ""}
