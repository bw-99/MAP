{"Fair multilingual vandalism detection system for Wikipedia": "Mykola Trokhymovych Pompeu Fabra University Barcelona, Spain mykola.trokhymovych@upf.edu Muniza Aslam Wikimedia Foundation San Francisco, USA muniza-ctr@wikimedia.org Ai-Jou Chou Wikimedia Foundation San Francisco, USA aiko@wikimedia.org Ricardo Baeza-Yates EAI, Northeastern University Silicon Valley, USA rbaeza@acm.org", "ABSTRACT": "This paper presents a novel design of the system aimed at supporting the Wikipedia community in addressing vandalism on the platform. To achieve this, we collected a massive dataset of 47 languages, and applied advanced filtering and feature engineering techniques, including multilingual masked language modeling to build the training dataset from human-generated data. The performance of the system was evaluated through comparison with the one used in production in Wikipedia, known as ORES. Our research results in a significant increase in the number of languages covered, making Wikipedia patrolling more efficient to a wider range of communities. Furthermore, our model outperforms ORES, ensuring that the results provided are not only more accurate but also less biased against certain groups of contributors.", "ACMReference Format:": "Mykola Trokhymovych, Muniza Aslam, Ai-Jou Chou, Ricardo Baeza-Yates, and Diego Saez-Trumper. 2023. Fair multilingual vandalism detection system for Wikipedia. In ,. ACM, New York, NY, USA, 10 pages.", "1 INTRODUCTION": "Being the most popular online knowledge source and the eighth most visited website in the world, 1 Wikipedia has a central role in the Web ecosystem. Its content is frequently used for powering other websites and products, from educational purposes, such as incorporating verified facts into curricula, to artificial intelligence solutions, such as training large language models [4, 12]. Wikipedia is an online encyclopedia that is edited by volunteers. Unlike traditional encyclopedias written by a closed group of experts, Wikipedia relies on the contributions of a large community of users across the globe. This collaborative approach allows a wide range of viewpoints to appear so that information on the site is constantly updated. However, it also requires significant manual work to maintain the site's accuracy and integrity. This includes 1 Similarweb Wikipedia.org statistics overview https://www.similarweb.com/website/ wikipedia.org. Accessed 05 December 2022. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). To appear in KDD'23 ,", "Diego Saez-Trumper": "Wikimedia Foundation San Francisco, USA diego@wikimedia.org reviewing and editing new content, monitoring for vandalism, and enforcing community guidelines. The collaborative nature of Wikipedia is one of its strengths. However, it also presents some challenges. Not all of the content added to the site is of high quality, and it can be difficult to ensure that the information is accurate, true, and/or unbiased. This is partly because many of the site's contributors are inexperienced users who may make mistakes when adding or editing content. Additionally, Wikipedia is also vulnerable to vandalism, which can take the form of harmful edits or the insertion of misinformation [20]. To address these issues, Wikipedia has a group of dedicated volunteer editors, known as patrollers, 2 who work to ensure the accuracy and integrity of the information on the site. These patrollers review and edit articles, monitor for vandalism, and enforce community guidelines. However, their work is not easy, as they have to keep up with the fast pace of Wikipedia, where on average, around 16 pages are edited per second. 3 To support the work of patrollers, Wikipedia has developed machine learning (ML) systems such as ORES (Objective Revision Evaluation Service) to assist in identifying potentially damaging changes [8]. Even though ORES implements various ML solutions, we focus on the damaging model in this work. This model helps patrollers to easily discover harmful contributions, helping to prioritize the content that requires patrolling. While ML systems can greatly assist patrollers, there are still open problems to be solved. The current system still has limitations regarding language coverage, precision, and bias against anonymous users. The main goal of this work is to create a tool that addresses these open problems. The main contributions of this work are: \u00b7 Introduction of an open-source, 4 multilingual model for content patrolling on Wikipedia, outperforming the state-of-theart models; \u00b7 Significantly increasing the number of languages covered by more than 60%; \u00b7 Study the biases of different models and discuss the trade-offs between performance and fairness, \u00b7 Model inference productionalization and deployment. 2 https://en.wikipedia.org/wiki/Wikipedia:Patrolling 3 Wikimedia Statistics https://stats.wikimedia.org/. Accessed 05 December 2022. 4 GitHub repository: https://github.com/trokhymovych/KI_multilingual_training \u00a9 2023 Copyright held by the owner/author(s). To appear in KDD'23 , Mykola Trokhymovych, Muniza Aslam, Ai-Jou Chou, Ricardo Baeza-Yates, and Diego Saez-Trumper The rest of the paper is organized as follows. Section 2 presents related work, while Section 3 presents the architecture of our system. In Section 4, we explain our data setup giving the evaluation results in Section 5. We finish with some conclusions in Section 6. To evaluate possible user biases, we are using group fairness metrics implemented in AI Fairness 360 open-source toolkit, such as Disparate Impact Ratio (DIR) and Difference in metrics for privileged and unprivileged groups of users [2].", "2 RELATED WORK": "In this section, we cover the related work, categorizing it into three main areas: (i) Knowledge integrity in Wikipedia; (ii) biases and explainability; and (iii) language modeling.", "2.1 Knowledge integrity in Wikipedia": "With the rise of Wikipedia, a problem of inappropriate content occurred rapidly after the platform's creation [16]. The nature of harmful revisions varies from trolling and hate speech to intended disinformation and even paid promotions [11, 17, 21]. Initial research papers presented model-based methods for vandalism prevention, defined a problem as a binary classification task, and used generic features like upper case ratio and term frequency [16]. Further works also observed the relationship between editing behavior, link structure, and article quality on Wikipedia [19]. The study of vandalism detection in other open-source platforms like Wikidata and OpenStreetMap, which observe vandalism patterns and propose detection approaches, also provide valuable insights applicable to our tasks due to shared similarities [10, 15]. Even the first technologies fundamentally transformed the way of editing and administration in Wikipedia [1, 3, 6]. More refined works appeared that rethought the problem from different aspects. As a result, the most significant work presents an Objective Revision Evaluation Service (ORES) currently used as a main tool that helps the Wikipedia community evaluate changes to the encyclopedia and assess the article's quality [8]. It uses a combination of metadata about the edit and the editor to evaluate each revision, including bad words and informal word features, page, edit, and user metadata. Although nowadays ORES is a key tool that helps the community to improve KI, it has drawbacks that we try to address in our paper: (i) limited number of languages support and difficulty to extend it; (ii) limited to fixed dictionaries content semantics features; (iii) biases against new and not registered users [8, 23]. The behavior of editors who are not registered on Wikipedia has been controversial [14]. At the same time, the quality control process and the algorithmic tools used to reject contributions are identified previously as significant reasons for the decline in newcomer retention and active users [7]. It shows the importance of analyzing and preserving biases of new algorithms, especially for new and anonymous users. Our work is inspired by insights and recommendations provided in the paper by Estelle Smith et al., which observes the alignment of model design with the values of communities based on the ORES example [22].", "2.2 Biases and explainability": "Evaluation of content moderation requires not only calculating their accuracy but also taking into account possible biases. For example, model bias against new and anonymous users can cause the active editor's number to decline in long-term perspective [7]. Moreover, it is important not just to prevent vandalism but also to provide users with an explanation of the decision made by the model. It can be used for further model enhancement and for editors' skills improvement. As for that, we are using tools for local interpretation. They can help to explain every single prediction of a model independently from each other [13, 18].", "2.3 Language modeling": "To build a new system for vandalism detection, we want to extract both meta-features of revision and user and semantic features of content changes. Previous works approached it using fixed lists of bad or informal words [8]. The problem is that it doesn't consider the context where those words were used. Moreover, such lists are not available for all languages, and their creation is costly, making it difficult to cover more languages in Wikipedia. At the same time, recent discoveries in the area of Natural Language Processing show the effectiveness of using Masked Language Models (MLMs) for various tasks [4]. Moreover, it is valuable that one model can be used for multiple languages, because it is easier to maintain and update. In our work, we are using the pretrained bert-base-multilingual-cased model and fine-tuning it for our particular task. This model allows working with 104 languages, chosen from the top 100 largest Wikipedias. 5", "3 SYSTEM DESIGN": "In this section, we will provide an overview of the overall architecture and workflow of the system, selecting the appropriate algorithms and technologies, training, testing, and deploying the solution.", "3.1 Architecture overview": "The proposed system takes the Wikipedia revision as input and aims to define if it will be reverted. It returns the probability of the revert event for a given specific revision. The key idea of our solution is to analyze the changes in text content, which is the main component of the encyclopedia page. Having the features that represent the content changes, we combine them with the revision and creator metadata to pass in the final classifier to get the revert score. To sum up, the system pipeline consists of two main steps: (i) features preparation; (ii) final classification. The system inference logic schema is presented in Figure 1. 3.1.1 Features preparation. We start feature preparation with the text-processing steps observed in Section 4.3. In the case of the mwedittypes package standard output, 6 we use it directly as a feature of the final models. As for inserted, removed, and changed texts, we apply the finetuned MLMs that were trained to predict the event of revision to be reverted. Training details will be observed in Section 3.2.2. Later we 5 mBERT: https://github.com/google-research/bert/blob/master/multilingual.md 6 mwedittypes python package: https://github.com/geohci/edit-types Fair multilingual vandalism detection system for Wikipedia To appear in KDD'23 , Figure 1: System inference logic schema. process Revision and user wikitexts metadata Scores outputs Mean pooling List of text changes pre-trained text (before softmax layer) Max pooling REVISION [text1 , text2, classification model Content Final classification model features Probability outputs Mean pooling (after softmax layer) Max pooling Pass each text separately use the final model output before (scores) and after (probabilities) the softmax layer as features for the final classifier. We also do the same for page titles that can include valuable semantic information. As each revision can include multiple changes, inserts, or removes, we are calculating features for each and later aggregating them using mean and max pooling. Calculated aggregated scores from each of the four models are later used as input for the final classifier. 3.1.2 Final classification. As for the final classifier model, we use the Catboost classification model [5]. It is fitted with both text content features observed in the previous subsection and revision metadata. Revision metadata includes the difference in bytes made by revision, users' information, and the language of the page. We are training the final model with different feature configurations. The basic configuration includes revisions metadata along with the mwedittypes package output. An example of it is presented later in Figure 6. Later we are adding MLMs features and user metadata to the basic configuration. The reasoning for separating those features type is that MLMs features require much more computational power to extract, and user features can introduce additional bias to the model. So, we observe the influence of those features on the model performance in different aspects to assess their impact. Finally, the classifier returns the probability of the revision being reverted. That score can be further used by patrollers to prioritize work and review the most suspicious revisions at first, constantly improving the quality of content in Wikipedia.", "3.2 Training process details": "As stated previously, the general system consists of multiple trainable parts related to each other. So, it is crucial to design the training process to avoid any data leakage and finally result in a complex model of high quality. Next, we will observe the process of data splitting and training each part of the complex system. 3.2.1 Data splitting logic. Initially, we have two training datasets of six months of anonymous users and all users' revisions. Also, we have the independent test set of the following week after the training data period. The time-based splitting procedure is needed to avoid time-related anomalies that can bias evaluation results. The overview of the data splitting procedure is presented in Figure 2. This validation data will help to evaluate the final solution in a realworld scenario and compare our system with existing solutions. As for the training sets, we separated them into two independent parts that were used for fine-tuning MLMs and training the final Figure 2: Training-testing split strategy. 01.2022-07.2022 Train 0.6 of data (revisions used for text models) 01.07.2022 08.07.2022 Hold-out test All users: anonymous authorized Train 0.4 of data (revisions used for final revert classifier model training) random split based on articles titles (in order to minimise article context sharing between models) classifier. We utilized a proportion of six to four, giving more data for fine-tuning MLMs, as they required more data to train, and the procedure observed in section 3.2.2 required heavy filtering. Although splitting was random with the fixed seed, it was secured that revisions of the same articles were not shared between different training datasets. The aim of this condition was to prevent the leakage of article context bias between MLMs and the final classifier. 3.2.2 Text classification models. The initial approach to modeling was to fine-tune 4 MLMs that will independently evaluate revision features like text changes , text inserts , text removes , and pages title semantics . Basically, we fine-tune MLMs in three different problem formulations: (i) pair of texts classification (text changes), (ii) single text classification (text inserts, text removes), (iii) regression based on text (pages title semantics) . As for model training, we have resources limited to one AMD Radeon Pro WX 9100 16GB GPU. We use the dataset collected for text model training defined in Section 3.2.1. As for changes and insert model training, we also filter out revisions with more than one change, insert or remove. The aim of this process is to get rid of noise in training data. For example, having two text changes in revision, we can't be sure which of them caused the revision to be reverted. Also, we are doing class balancing for all classification models using undersampling of not reverted revisions. Data preprocessing for the regression model differs from other models. This model aims to define the revert rate of the page based on the title. For this purpose, we take only those pages that have five or more revisions, whereas other language pages are considered different ones. We calculate the revert rate for those pages and use it as a target for model tuning. We use the transformers package for further model tuning [24]. It allows us to load pretrained models directly from the hub and To appear in KDD'23 , Mykola Trokhymovych, Muniza Aslam, Ai-Jou Chou, Ricardo Baeza-Yates, and Diego Saez-Trumper tune them for our specific needs. As for tokenization, we use each model's specific tokenizer. As we have multilingual problem formulation, we use MLM that was initially trained on multilingual datasets bert-base-multilingual-cased [4]. During the training process, we use \ud835\udc59\ud835\udc52\ud835\udc4e\ud835\udc5f\ud835\udc5b\ud835\udc56\ud835\udc5b\ud835\udc54 _ \ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc52 = 0 . 00002, \ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 _ \ud835\udc51\ud835\udc52\ud835\udc50\ud835\udc4e\ud835\udc66 = 0 . 01, and tune models for five epochs. All other parameters are left to be the default. We independently tune models on two different training sets: train anon and train all , presenting their performance in Section 5.1. 3.2.3 Final classifier model. Having all features for content changes (Section 3.1.1) and revision meta-features, we build a classification model to define whether the revision will be reverted. We are using an independent training data subset. With an imbalanced dataset, weapply class weighting proportionally to the ratio between classes. So, we are giving more weight to data samples of underrepresented classes. Also, we have tried out under-sampling of the training data for balancing, but this strategy showed worse results on the final evaluation. As a classification model, we use the CatBoost classifier with \ud835\udc59\ud835\udc52\ud835\udc4e\ud835\udc5f\ud835\udc5b\ud835\udc56\ud835\udc5b\ud835\udc54 _ \ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc52 = 0 . 01 [5]. We train each model for 5 thousand iterations. Validation of the model is performed using hold-out testing data presented in Section 3.2.1, and the final results will be presented in Section 5.", "4 DATA PREPARATION": "The modeling stage and further inference performance depend highly on the data used. In this section, we observe the data collection process and explain our technical decisions. Also, we provide the initial exploratory data analysis for the collected dataset.", "4.1 Data collection overview": "Our data source is the Wikimedia Data Lake, 7 specifically, we used mediawiki history and mediawiki wikitext history datasets. In particular, the mediawiki history dataset stores the metadata about edits to all pages in the form of monthly snapshots. It includes information about the page, the user that performs the change, and details of the individual edit. 8 On the other hand, the wikitext history contains snapshots of the text content for each of the versions (a.k.a., revisions) made up to the date of the specific snapshot creation. 9 The collected data have the following characteristics: \u00b7 Contains the 47 most edited languages in Wikipedia to support generalizability. We didn't collect data for Kazakh and Portuguese as they don't allow anonymous revisions, so we could not use them. Also, we omitted Simple Wikipedia . \u00b7 Contains only records for event entity = 'revision', corresponding to page changes. We limited the number of revisions per language to 300K for training and 100K for testing. There are two main reasons for this restriction: (i) wewanted to avoid a huge imbalance in the number of revisions for different languages, (ii) we had limited resources for further processing of collected data. 7 Wikimedia Data Lake: https://wikitech.wikimedia.org/wiki/Analytics/Data_Lake 8 Mediawiki history dumps: https://wikitech.wikimedia.org/wiki/Analytics/Data_Lake/ Edits/Mediawiki_history_dumps 9 Mediawiki wikitext history dumps: https://wikitech.wikimedia.org/wiki/Analytics/ Data_Lake/Content/Mediawiki_wikitext_history \u00b7 Uses the period from 2022-01-01 to 2022-07-01 for training and the following week for a hold-out test set. \u00b7 Includes two training datasets: The first contains only revisions created by anonymous users, and the other contains all users' revisions. In general, the data collection process can be divided into three stages: (i) aggregating edits history data; (ii) filtering of data explained in the Section 4.2 (iii) merging text data and extracting content difference. Text processing details will be explained later in Section 4.3. The final dataset contains revisions for various languages, their metadata, creator information, and features that correspond to text changes in the revision.", "4.2 Data filtering": "Wikipedia data is both diverse and noisy. We applied multiple filters to the collected data to construct the proper signal for further modeling. In this section, we describe those filters along with the reasoning behind using them. 4.2.1 Content type filter. In this research, we are concentrating on reducing vandalism on content pages. That is why we leave only 'revision' event entity that corresponds to encyclopedia content page changes. Moreover, we omit changes corresponding to discussions, user pages, or other non-encyclopedic content for our research. It is important to mention that we filter out revisions that create new pages, so they are out of the scope of the proposed solution. 4.2.2 User type filter. We are removing bots edits from our dataset because we are interested in human-made changes. Also, bot edits present bias towards the lower revert rate and duplicated edits, which can harm model training and evaluation. One of the biggest changes differentiating our research from others is leaving only anonymous users edits in the training dataset. Previous works have shown that possible vandalism detection solutions can be biased against anonymous and newly registered users [8]. This issue contributes to the decreasing retention rate of newcomers. This is part of an even bigger problem of the declining number of active contributors to Wikipedia [9]. Our research contributes to solving this problem that is represented in Section 5.4. 4.2.3 \"Edit wars\" filter. Wikipedia article updating is not always a peaceful and collaborative process. There are sometimes strong disagreements in the community. It results in sequential revisions that revert one another, called edit wars. Although those revisions usually include vandalism, we filter them out because it is a source of strong noise in data. The problem is that in the case of edit wars, wehave half of the reverted revisions, which are good-faith changes aimed at removing vandalism. At the same time, it is difficult to automatically differentiate vandalism and good-faith changes in the case of edit wars. So we apply a specific filtering technique and remove all reverting revisions that are further reverted by the next revision. The filtering logic is presented in Figure 3.", "4.3 Text processing": "Another important step of data preparation is getting features from content changes. Initially, we have two raw strings representing the page's full wikitext content for each revision. We are using the Fair multilingual vandalism detection system for Wikipedia To appear in KDD'23 , Figure 3: Technique used to filter out \"revision-wars.\" We are not considering those reverts as they part of revert wars we dont know if they good' So, we leave only reverted revisions that are reverted by NOT reverted revisions Reverted change Reverted change NOT Reverted change NOT Reverted change Revision Revisi\u00f3n 2 Revision 3 Revision 4 Revision 5 revert revert LABEL; LABEL: LABEL: LABEL; LABEL: NOT REVERTED NaN (filtered out) REVERTED NOT REVERTED NOT REVERTED are Figure 4: Text content changes extraction. Parsed text changes from wikitext: (1) text inserts: I]. texts removed: [I Laurit Krasniqi: Difference between text changes: [('Born in Belgium; he has chosen to represent the Kosovo national team' Born in Kosovo; he chosen to represent the Hovision Kosovo national team:) Linc 21 2022 ([Royal [ [Kosovol ] choson choscn Uikosovo tootboll nacional revis histon has aforementioned mwedittypes as the main tool for text change extraction. From a wikitext pair (parent revision and current revision), we extract the inserts , changes , and removes (see Figure 4). Also, we extract the standard output of mwedittypes in the form of count for changes, inserts, and removes for different types of data (Text, References, Tables, etc.) In the case of inserts or removes , we add all whole parsed texts to the list. They usually represent sentences or phrases. In case of changes, we get the whole paragraph where the change occurred, even if that change is in one character. To extract valuable change signals, we perform the additional step of text processing. We split the extracted paragraph with changes into sentences. Then we compare sentences pairwise using Levenshtein Distance 10 between the parent and the current revision. If there is a minor change in a sentence compared to parent revision, we pass the pair of sentences to the changes list. If a sentence exists in the parent revision and there is no similar one in the current revision, we add that sentence to the removed list. If a sentence exists in the current revision and not in the parent one, we add that sentence to the insert list. Figure 4 presents an example of text change parsing.", "4.4 Dataset description": "In this section, we provide a general overview of our data. We collected three datasets: (i) anonymous users training (train anon ), (ii) all users training (train all ), and (iii) all users hold-out test set (test). Each of them contains records from 47 languages. The main fields of data schema are presented in Figure 5. Fields texts_removed , 10 fuzzywuzzy python package: https://github.com/seatgeek/fuzzywuzzy {'wiki_db': 'enwiki', 'event_comment': 'grammar', 'event_user_text_historical': '2603:6010:D800:395B:A03F:E9CD:9BE1:65D7', 'event_user_seconds_since_previous_revision': nan, 'revision_id': 1096535207, 'page_title': 'Balaclava (clothing)', 'revision_text_bytes_diff': 45, 'revision_is_identity_reverted': 1, 'event_timestamp': '2022-07-05 02:46:04.0', 'revision_parent_id': 1096535065, 'is_mobile_edit': 0, 'is_mobile_web_edit': 0, 'is_visualeditor': 0, 'is_wikieditor': 1, 'is_mobile_app_edit': 0, 'is_android_app_edit': 0, 'is_ios_app_edit': 0, 'texts_removed': '[]', 'texts_insert': '[\"So, people don\\'t have to see your ugly face\"]', 'texts_change': '[]', 'actions': \"{...}\", 'is_anonymous': 1 } Figure 5: Data sample: this revision contains vandalism on the inserted text. {'change_Media': 0, 'insert_Media': 0, 'move_Media': 0, 'remove_Media': 0, 'change_Punctuation': 0, 'insert_Punctuation': 3, 'move_Punctuation': 0, 'remove_Punctuation': 0, ..., 'change_Whitespace': 0, 'insert_Whitespace': 9, 'move_Whitespace': 0, 'remove_Whitespace': 0, 'change_Word': 0, 'insert_Word': 9, 'move_Word': 0, 'remove_Word': 0 } Figure 6: Example of actions features. texts_insert , texts_change contain the text content change parsed from wikitext for the specific revision. When actions field corresponds to the output of mwedittypes and is treated as metadata for the revision. An example of actions features is presented in Figure 6. Fundamental data characteristics are presented in Table 1. Both training sets represent the same period of 6 months. We observe a slightly higher rate of anonymous users in testset. Also, that number differs across the languages. For example, anonymous users' rates vary from 0.04 for urwiki/eowiki/uzwiki to 0.39 for kowiki and 0.46 for mswiki in train all . As for the revert rate, we observe a significantly higher value in train anon than in train all , as expected. This value deviates from 0.01 for eowiki and 0.02 for urwiki to 0.18 for arwiki and 0.27 for hiwiki for train all . We can observe the relation between the revert rate and anonymous users rate across the languages in train all in Figure 7. We also calculated the Spearman correlation coefficient which is 0.62, which supports our reasoning that the anonymous user rate relates to the revert rate. To appear in KDD'23 , Mykola Trokhymovych, Muniza Aslam, Ai-Jou Chou, Ricardo Baeza-Yates, and Diego Saez-Trumper Table 1: Data characteristics. revert rate Figure 7: Revert rate vs. anonymous users rate across languages.", "5 EVALUATION": "In this section, we observe the performance of the proposed solution and compare it to other existing models. The goal of our models is to predict whether a given revision will be reverted or not. We consider the accuracy, efficiency, and fairness of models. Moreover, we evaluate our model trained on data with different configurations: (i) using different feature sets; (ii) training only on anonymous revisions, or using both registered and anonymous users' revisions as training data.", "5.1 Masked Language Models metrics": "In this section, we evaluate the standalone performance of finetuned MLMs for the task of revert prediction. In particular, we evaluate the models trained to predict reverts based on changes, removes, and inserts of the text to the content of the pages. We should mention that using those models in standalone mode is inappropriate, as they will not cover all the revisions. The reason is that it is possible that revisions don't have any text changes, and the difference is only in media, references, or other content. In particular, for our hold-out test set, the rate of revisions with text changes is 38.4%, text inserts are 24.3%, and text removes 14.4%. In general, we have the situation that 55.6% of revisions have at least one of the text manipulation types. However, in this section, we want to show that MLMs can provide a powerful signal that can be used side-by-side with meta-features, improving the general system performance that is shown in the next Section 5.2. The results are presented in Table 2. Wecalculate the AUC metric separately for all users (AUC all ) and for anonymous user (AUC anon ) revisions. We use the aggregation logic observed previously in case of multiple text modifications for one revision. In particular, we are using mean aggregation of probability scores obtained from MLMs. We are only considering revisions having a specific text manipulation used for model training. Table 2: Standalone MLM performance for changes, inserts and removes on the test set. Weuse bert-base-multilingual-cased as a base model and fine-tune it using all or anonymous only users' revisions. For models fine-tuned on different user group revisions, we present anonymous and all users metrics. The first outcome of the analysis is that the model learns the signal from different text manipulations, thus, can be used to improve the complete anti-vandalism system. Another insight is that models fitted on anonymous or all users' revisions perform better on specific types of revisions that were used for training. This leads us to conclude that anonymous revisions have specific patterns of text changes that differ from general cases. Those models' output will be used later for building the final classifier.", "5.2 Final model": "The following section presents an examination of the performance of the complete system. The results are analyzed and discussed in detail to provide insight into the performance of the model and its potential applications. We compare our model with the existing SOTA solution - ORES. Also, we use a baseline, a biased rule-based model that reverts all the anonymous revisions. We use the AUC score on a full unbalanced holdout test set as the primary metric for model comparison. It can be interpreted as a probability of the model ranking a random positive example higher than a random negative example, which directly corresponds to our goal. As an alternative metric, we use precision at a recall level of 0.75 ( \ud835\udc43\ud835\udc5f @ \ud835\udc45 0 . 75) on an unbalanced dataset. The reasoning is that patrollers can't always review all revisions in real-life usage scenarios, so they select the most suspicious ones. At the same time, various models can have different optimal thresholds, and they are usually selected based on the patrollers' workload for different communities. That is why precision at a recall level is a sub-optimal approach to comparing models' performance in a practical setup. We have also calculated MacroF1 on unbalanced along with Accuracy and F1 on the balanced holdout test set. We use a default threshold (0.5) and balance by downsampling the overrepresented class across each language. With this threshold, our best configuration shows a performance of 4.5% worse than ORES on all users testing using MacroF1 score. However, those metrics depend highly on the classification threshold, which is inadequate for model comparison in our setup. The full results of the models' evaluation are presented in Table 3 for all users and Table 4 for anonymous-only users. Only revisions Fair multilingual vandalism detection system for Wikipedia To appear in KDD'23 , Table 3: System performance on test set of all users. for languages implemented in ORES are used in the presented comparison tables. We are comparing models trained on anonymous users' revisions only and on revisions of all users. Also, we experiment with adding user-specific features, like user group and is_anonymous, and MLMs features. It is important to mention that the MLM used for the anonymous user training set was fine-tuned only on anonymous users' revisions. Similarly, all users' training data uses MLMs tuned on all user's revisions. Finally, we present the precision-recall graphs in Figure 8 and Figure 9 that show how the precision and recall metrics differ for different thresholds, which can be useful for selecting the model for specific community conditions. For example, if the patrollers have significantly limited resources, they will select only the most suspicious revisions to check. It is clear that recall, in that case, will not be high, but at least we want to maximize the precision to spend the patrollers' time efficiently. The final results show that the multilingual model trained on all users' revisions with users and MLMs features outperforms all competitors, including ORES, using the AUC metric. Figure 9 supports our reasoning and shows that the mentioned model performs the best for any classification threshold. Moreover, we also observe a significant improvement in performance for anonymous users. In particular, we see that MLMs features contribute to all model configurations, especially for anonymous users. Furthermore, we evaluate the performance of the best-performing model for languages that are not implemented in ORES. The AUC value for those languages is 0.873, which is similar to those used in the comparison table. Another important observation is that models trained on anonymous user revisions can only be beneficial for the classification of anonymous revisions. However, they have significantly worse performance considering all users' revisions. Even though the bestperforming model for anonymous users appears to be the one fitted with only anonymous users' revisions, we observe that the configuration performing the best for all users has comparable results also on anonymous users. As a result, we select the configuration trained on all revisions with user and MLMs features as the best model and use it for more detailed analysis in the next sections. Table 4: System performance on test set of anonymous users. ORES Mult. Anonymous Mult Anonymous MLM Mult. AII MLM 1 user feat. Mult, Aii user feat. MLM 0.2 0.4 0.6 1.0 Recall Figure 8: Precision-recall graph for all users. ORES Mult. Anonymous Mult_Anonymous MLM Mult. AII MLM 1 Mult. AII user feat. user feat. MLM 0.0 0.4 0.8 Recall Figure 9: Precision-recall graph for anonymous users.", "5.3 Model productionalization": "In this section, we analyze deployment details and the efficiency of the complete system. It includes data collection using the MediaWiki API, 11 data processing, feature engineering, and final model prediction. The whole model inference pipeline is open-source and can be found in the repository. 12 Service was experimentally deployed using Lift Wing infrastructure. 13 Lift Wing is a set of production Kubernetes clusters at Wikimedia based on KServe, 14 a serverless inference platform. Lift Wing hosts machine learning models and responds to requests for predictions. Machine learning models are hosted on Lift Wing as Inference Services, which are asynchronous microservices and containerized applications. Each inference service has production images that are published in the WMF Docker Registry. 15 The final model was deployed to the ML 11 MediaWiki API https://www.mediawiki.org/wiki/API:Main_page 12 https://gitlab.wikimedia.org/repos/research/knowledge_integrity 13 Lift Wing https://wikitech.wikimedia.org/wiki/Machine_Learning/LiftWing 14 KServe https://github.com/kserve/kserve 15 WMFDocker Registry https://docker-registry.wikimedia.org/ To appear in KDD'23 , Mykola Trokhymovych, Muniza Aslam, Ai-Jou Chou, Ricardo Baeza-Yates, and Diego Saez-Trumper staging cluster and allocated up to 4 CPUs and 6 Gi memory resources to the pod. The service endpoint is exposed using Istio as an ingress to the API consumers. As for the efficiency experiment, we select a random sample of 1K revisions and pass them sequentially to the Lift Wing API. We use the model with the best configuration for the test. We used the model that requires both revision metadata and MLM-based features. The inference is tested on CPU-only instances. As a result, we got the median response time of 1.0 seconds, 1.7, and 4.0 seconds for 75%, and 95% percentile, respectively. It should be mentioned that revision processing time highly depends on the size of text changes, as it requires more processing using MLMs. We didn't reproduce the performance with the original ORES model, as it uses caching in its API and is deployed on different instances. At the same time, the original ORES paper report median, 75%, and 95% percentile response timings are 1.1, 1.2, and 1.9 seconds respectively, for single score speed [8]. Having those values as a reference, we can say that our solution is comparable in time performance with the ORES model but can be slightly less efficient in case of revisions with major text changes.", "5.4 Fairness and biases": "Although anonymous edits tend to be reverted more than the ones done by registered users, the big majority of Wikipedia editions keep allowing anonymous editors (a.k.a., IP Editors). 16 Therefore, it is important that ML-systems try to mitigate potential biases against this group of editors. This section presents a thorough analysis of the biases present in our proposed model and a comparison to existing solutions. As for analysis, we are using Disparate Impact Ratio ( \ud835\udc37\ud835\udc3c\ud835\udc45 ) and Difference in AUC score for anonymous and registered users to analyze the bias against (unprivileged) anonymous users [2]. Equation 1 shows the calculation logic of \ud835\udc37\ud835\udc3c\ud835\udc45 , where \ud835\udc43\ud835\udc5f corresponds to probability, \u02c6 \ud835\udc4c - predicted value, \ud835\udc37 - a group of users (anonymous or registered). The results are presented in Table 5.  We use the same unbalanced dataset as in Section 5.2 for the following experiment. Also, we calculate \ud835\udc37\ud835\udc3c\ud835\udc45 \ud835\udc4f\ud835\udc4e\ud835\udc60\ud835\udc52 = 7 . 93 where we use \ud835\udc4c - true values instead of \u02c6 \ud835\udc4c , which shows the ratio of base rates (disparate impact of the original dataset). We compare this value with the ones calculated for each algorithm. The first insight is that the current model (ORES) has a DIR equal to 20.02, much bigger than the base one. It means ORES presents a significant bias against the unprivileged class (anonymous users). Also, we observe the difference in performance for anonymous and registered users is -0.043. At the same time, the best-performing configuration of our solution shows a DIR value of 9.54, which is much closer to the base value. It means that we still introduce the bias against the anonymous user, but it is significantly lower. Also, the model's performance for anonymous and registered users is more similar based on the difference in AUC, which is -0.017 for the presented model. Moreover, we observe the absolute difference in AUC decrease after adding MLMs features from -0.035 to -0.017. 16 https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_is_anonymous Table 5: Fairness metrics evaluation.", "5.5 Performance on different languages": "Previously, the system design implemented independent model training for each of the languages [8]. In our setup, we train one model that takes the language as a feature. Such a design makes it easier to maintain and extend models. Moreover, we extend the number of supported languages from 28 to 47. At the same time, in Section 4.4, we show that languages significantly differ in their characteristics like revert rate and rate of anonymous users. In this section, we observe and compare models' performance across languages supported by the ORES model and have at least 10K observations in the test dataset. We compare the models using the AUC score, and the results are presented in Figure 10. We not only significantly increase the language coverage, but also our solution performs better or equal to ORES for all previous languages. Moreover, we can observe that for some languages, we can achieve better performance than ORES even without user-based features that introduce bias against anonymous users. It supports the claim that our model usage can benefit each independent language community. Moreover, we show how the general model performance depends on the rate of anonymous users per language. We observe the negative correlation between the anonymous user rate and AUC metric both for ORES and our model. At the same time, the new model shows significantly better performance for most languages.", "5.6 Model selection and explainability": "In recent years, there has been a growing interest in developing machine learning models that are accurate and interpretable. Explainability, or the ability to understand the reasoning behind a model's predictions, is crucial for building trust in the model and for identifying potential biases or errors. Here we focus on two key aspects of explainability: model feature importance and singlesample prediction explanation. We are analyzing the best model based on the evaluation in the previous sections and we use SHAP (SHapley Additive exPlanations) values for it. SHAP values calculation is a method for interpreting the output of machine learning models by attributing each feature's contribution to the model's prediction. SHAP values provide a way to 17 The closer to the the \ud835\udc37\ud835\udc3c\ud835\udc45 \ud835\udc4f\ud835\udc4e\ud835\udc60\ud835\udc52 = 7 . 93, the better To appear in KDD'23 , Figure 10: AUC score per model and language. Fair multilingual vandalism detection system for Wikipedia Mult. AII MLM Mult. AII user feat. user feat. MLM Mult. Anonymous Mult_Anonymous MLM ORES 0.85 g 0.80 0.75 0.70 language Figure 11: Anonymous rate vs. AUC per language. Multilingual all user features MLM ORES 0,925 0,875 0.825 0.825 0.800 0775 0.775 0,725 0 25 0 25 0.30 anonymous ate decompose the model's prediction into a sum of the contributions of each feature. It allows the user to understand how each part is contributing to the final result [13]. Firstly, we observe the feature importance of the new model shown in Figure 12. We use mean absolute SHAP values for this experiment. We can see that the most influential feature is the indicator if the user is anonymous or not. This insight was not surprising, as we have shown in Section 5.2, that baseline, that using only this feature can achieve 0.75 in AUC, meaning that the feature has strong predictive power. On the other hand, it introduces bias against anonymous users, so we aimed to reduce it by adding other features with a different nature that also have predictive power, like MLMs features. So, we can see that 4 out of 20 most important features of the model are MLM-based when the model has 137 features in total. Moreover, we also observe a local explanation of the prediction for the sample shown before in Section 4.4 in Figure 13. One of the most influential features for this particular example is MLMs features based on inserted text. Looking at the inserted text in Figure 5, we can observe abuse text that definitely should be reverted. So, for this example the model explanation using the SHAP value provided a correct insight into the possible reason for the problem with a revision. It can define more granular problems for each revision and indicate possible problems for the patrollers and contributors.", "6 CONCLUSIONS AND FUTURE WORK": "To sum up, this paper presents a new design for a system aimed to help in preventing vandalism on Wikipedia. The model was trained Is_anonymous wiki_db is_editor is_autopatrolled is_mobile_edit bert_title_score is_extendedconfirmed is_sysop is_visualeditor insert Wikilink revision_text_bytes_diff bert_change_pO_max is_patroller change_Table autoreview insert_Template change_Section bert_change_s0_max bert_change_s1 mean 0.00 0.10 0.15 0.20 0.25 mean(ISHAP valuel) (average impact on model output magnitude) Figure 12: Top-20 feature importance based on SHAP values for all users test set. 4 249 3,249 2 249 21 240 0 7514 1 751 2 751 bertinsed mear 21,959 onyrnous Figure 13: SHAP explanation for data sample from Figure 5. using a large dataset of 47 different languages that utilize sophisticated filtering and feature engineering techniques. The presented solution takes advantage of modern NLP techniques that rely on pre-trained multilingual MLMs. It also differs from the previous solution introducing a single model for all languages, making it easier to maintain. The results showed that the system outperformed the current state-of-the-art ORES system in terms of both the number of languages covered and accuracy. This research contributes to the field of multilingual Wikipedia patrolling, making it more efficient and less biased against anonymous users worldwide. Improving model accuracy directly influences the encyclopedia's knowledge integrity because patrollers can cover more damaging pages with the same resources. Moreover, reducing bias against anonymous users aims to help extend the active editors pool, which is the core of the Wikipedia movement. Nevertheless, the current study has several limitations that should be considered when interpreting the results. Data Limitations : Our study focused on analyzing changes in the text content of pages. However, we have observed that only 56% of revisions have at least some changes in the text. We are not considering changes in references, media, tables, etc. Analyzing other types of content can be a great extension of our work in the future. We restricted our data to the most frequent languages, which may not represent the full range of languages used on Wikipedia. Data sampling limitations : We make our analysis on the fixed time frame. We don't consider time-related patterns that could affect the amount of vandalism on Wikipedia and model performance. Modeling limitations : We only tested one language model and did not explore other language models that could potentially provide different results. We leave this for future work.", "ACKNOWLEDGMENTS": "This work has been funded by MCIN/AEI /10.13039/501100011033 under the Maria de Maeztu Units of Excellence Programme (CEX2021001195-M) To appear in KDD'23 , Mykola Trokhymovych, Muniza Aslam, Ai-Jou Chou, Ricardo Baeza-Yates, and Diego Saez-Trumper", "REFERENCES": "NY, USA, 2899-2905. https://doi.org/10.1145/3366423.3380055 [1] B. Thomas Adler, Luca de Alfaro, Santiago M. Mola-Velasco, Paolo Rosso, and Andrew G. West. 2011. Wikipedia Vandalism Detection: Combining Natural Language, Metadata, and Reputation Features. In Computational Linguistics and Intelligent Text Processing , Alexander Gelbukh (Ed.). Springer Berlin Heidelberg, Berlin, Heidelberg, 277-288. [2] Rachel K. E. Bellamy, Kuntal Dey, Michael Hind, Samuel C. Hoffman, Stephanie Houde, Kalapriya Kannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta, Aleksandra Mojsilovic, Seema Nagar, Karthikeyan Natesan Ramamurthy, John Richards, Diptikalyan Saha, Prasanna Sattigeri, Moninder Singh, Kush R. Varshney, and Yunfeng Zhang. 2018. AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias. https: //arxiv.org/abs/1810.01943 [3] Si-Chi Chin, W. Nick Street, Padmini Srinivasan, and David Eichmann. 2010. Detecting Wikipedia Vandalism with Active Learning and Statistical Language Models. In Proceedings of the 4th Workshop on Information Credibility (Raleigh, North Carolina, USA) (WICOW '10) . Association for Computing Machinery, New York, NY, USA, 3-10. https://doi.org/10.1145/1772938.1772942 [4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers) , Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computational Linguistics, Minneapolis, MN, USA, 4171-4186. https://doi.org/10.18653/v1/n19-1423 [5] Anna Veronika Dorogush, Andrey Gulin, Gleb Gusev, Nikita Kazeev, Liudmila Ostroumova, and Aleksandr Vorobev. 2017. Fighting biases with dynamic boosting. [6] R. Stuart Geiger and David Ribes. 2010. The Work of Sustaining Order in Wikipedia: The Banning of a Vandal. In Proceedings of the 2010 ACM Conference on Computer Supported Cooperative Work (Savannah, Georgia, USA) (CSCW '10) . Association for Computing Machinery, New York, NY, USA, 117-126. https://doi.org/10.1145/1718918.1718941 [7] Aaron Halfaker, R.Stuart Geiger, Jonathan Morgan, and John Riedl. 2013. The Rise and Decline of an Open Collaboration System How Wikipedia's Reaction to Popularity Is Causing Its Decline. American Behavioral Scientist 57 (05 2013), 664-688. https://doi.org/10.1177/0002764212469365 [8] Aaron Halfaker and R Stuart Geiger. 2020. Ores: Lowering barriers with participatory machine learning in wikipedia. Proceedings of the ACM on Human-Computer Interaction 4, CSCW2 (2020), 1-37. [9] Aaron Halfaker, R. Stuart Geiger, Jonathan T. Morgan, and John Riedl. 2013. The Rise and Decline of an Open Collaboration System: How Wikipedia's Reaction to Popularity Is Causing Its Decline. American Behavioral Scientist 57, 5 (2013), 664-688. https://doi.org/10.1177/0002764212469365 arXiv:https://doi.org/10.1177/0002764212469365 [10] Stefan Heindorf, Matthias Potthast, Benno Stein, and Gregor Engels. 2016. Vandalism Detection in Wikidata. In Proceedings of the 25th International Conference on Information and Knowledge Management (CIKM 2016) . Association for Computing Machinery, New York, NY, USA, 327-336. https://doi.org/10.1145/2983323. 2983740 [11] Nikesh Joshi, Francesca Spezzano, Mayson Green, and Elijah Hill. 2020. Detecting Undisclosed Paid Editing in Wikipedia. In Proceedings of The Web Conference 2020 (Taipei, Taiwan) (WWW'20) . Association for Computing Machinery, New York, [12] Florian Lemmerich, Diego S\u00e1ez-Trumper, Robert West, and Leila Zia. 2019. Why the World Reads Wikipedia: Beyond English Speakers. In Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining (Melbourne VIC, Australia) (WSDM '19) . Association for Computing Machinery, New York, NY, USA, 618-626. https://doi.org/10.1145/3289600.3291021 [13] Scott M. Lundberg and Su-In Lee. 2017. A Unified Approach to Interpreting Model Predictions. In Proceedings of the 31st International Conference on Neural Information Processing Systems (Long Beach, California, USA) (NIPS'17) . Curran Associates Inc., Red Hook, NY, USA, 4768-4777. [14] Nora McDonald, Benjamin Mako Hill, Rachel Greenstadt, and Andrea Forte. 2019. Privacy, Anonymity, and Perceived Risk in Open Collaboration: A Study of Service Providers. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (Glasgow, Scotland Uk) (CHI '19) . Association for Computing Machinery, New York, NY, USA, 1-12. https://doi.org/10.1145/3290605.3300901 [15] Pascal Neis, Marcus Goetz, and Alexander Zipf. 2012. Towards Automatic Vandalism Detection in OpenStreetMap. ISPRS International Journal of Geo-Information 1, 3 (2012), 315-332. https://doi.org/10.3390/ijgi1030315 [16] Martin Potthast, Benno Stein, and Robert Gerling. 2008. Automatic Vandalism Detection in Wikipedia. In Advances in Information Retrieval , Craig Macdonald, Iadh Ounis, Vassilis Plachouras, Ian Ruthven, and Ryen W. White (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 663-668. [17] Charu Rawat, Arnab Sarkar, Sameer Singh, Rafael Alvarado, and Lane Rasberry. 2019. Automatic Detection of Online Abuse and Analysis of Problematic Users in Wikipedia. In 2019 Systems and Information Engineering Design Symposium (SIEDS) . IEEE, Charlottesville, VA, USA, 1-6. [18] Marco Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. 'Why Should I Trust You?': Explaining the Predictions of Any Classifier. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations . Association for Computational Linguistics, San Diego, California, 97-101. https://doi.org/10.18653/v1/N16-3020 [19] Thorsten Ruprechter, Tiago Santos, and Denis Helic. 2020. Relating Wikipedia article quality to edit behavior and link structure. Applied Network Science 5 (09 2020), 61. https://doi.org/10.1007/s41109-020-00305-y [20] Diego Saez-Trumper. 2019. Online disinformation and the role of wikipedia. arXiv preprint arXiv:1910.12596 (2019). [21] Diego S\u00e1ez-Trumper. 2019. Online Disinformation and the Role of Wikipedia. arXiv:1910.12596 http://arxiv.org/abs/1910.12596 [22] C. Estelle Smith, Bowen Yu, Anjali Srivastava, Aaron Halfaker, Loren Terveen, and Haiyi Zhu. 2020. Keeping Community in the Loop: Understanding Wikipedia Stakeholder Values for Machine Learning-Based Systems. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI '20) . Association for Computing Machinery, New York, NY, USA, 1-14. https://doi.org/10.1145/3313831.3376783 [23] Nathan TeBlunthuis, Benjamin Mako Hill, and Aaron Halfaker. 2020. The effects of algorithmic flagging on fairness: quasi-experimental evidence from Wikipedia. arXiv:2006.03121 https://arxiv.org/abs/2006.03121 [24] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-Art Natural Language Processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations . Association for Computational Linguistics, Online, 38-45. https://www.aclweb.org/anthology/2020.emnlp-demos.6"}
