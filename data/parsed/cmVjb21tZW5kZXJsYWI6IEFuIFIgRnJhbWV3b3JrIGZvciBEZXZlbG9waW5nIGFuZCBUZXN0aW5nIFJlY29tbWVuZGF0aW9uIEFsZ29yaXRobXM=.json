{"title": "recommenderlab: An R Framework for Developing and Testing Recommendation Algorithms", "authors": "Michael Hahsler", "pub_date": "2022-05-24", "abstract": "Algorithms that create recommendations based on observed data have significant commercial value for online retailers and many other industries. Recommender systems has a significant research community and studying such systems is part of most modern data science curricula. While there is an abundance of software that implements recommendation algorithms, there is little in terms of supporting recommender system research and education. This paper describes the open-source software recommenderlab which was created with supporting research and education in mind. The package can be directly installed in R or downloaded from https://github.com/mhahsler/recommenderlab.", "sections": [{"heading": "Introduction", "text": "Recommender systems apply statistical and knowledge discovery techniques to the problem of making product recommendations based on previously recorded usage data (Sarwar, Karypis, Konstan, and Riedl 2000). Creating such automatically generated personalized recommendations for products including books, songs, TV shows and movies using collaborative filtering have come a long way since Information Lense, the first system using social filtering was created more than 30 years ago (Malone, Grant, Turbak, Brobst, and Cohen 1987). Today, recommender systems are a successful technology used by market leaders in several industries (e.g., by Amazon, Netflix, and Pandora). In retail, such recommendations can improve conversion rates by helping the customer to find products she/he wants to buy faster, promote cross-selling by suggesting additional products and can improve customer loyalty through creating a value-added relationship (Schafer, Konstan, and Riedl 2001).\nEven after 30 years, recommender systems still have a very active research community. It is often not clear which of the many available algorithms is appropriate for a particular application and new approaches are constantly proposed. Many commercially available software applications implement recommender algorithms, however, this paper focuses on software support for recommender systems research which includes rapid prototyping algorithms and thorough evaluation and comparison of algorithms. For this purpose, access to the source code is paramount. Many open-source projects implementing recommender algorithms have been initiated over the years. Table 1 provides links to several popular open source implementations which provide code which can be used by researchers. The extent of (currently available) functionality as well as the target usage of the available software packages vary greatly and many projects have been abandoned over the years. A comprehensive list of package was employed by several researchers to develop and test their own algorithms (e.g., Chen, Chao, and Shah 2013;Buhl, Famulare, Glazier, Harris, McDowell, Waldrip, Barnes, and Gerber 2016;Beel, Breitinger, Langer, Lommatzsch, and Gipp 2016;Lombardi and Vernero 2017).\nPackage recommenderlab focuses on collaborative filtering which is based on the idea that given rating data by many users for many items (e.g., 1 to 5 stars), one can predict a user's rating for an item not known to her or him (see, e.g., Goldberg, Nichols, Oki, and Terry 1992) or create for each user a so called top-N lists of recommended items (see, e.g., Sarwar, Karypis, Konstan, and Riedl 2001;Deshpande and Karypis 2004). The premise is that users who agreed on the rating for some items typically also tend to agree on the rating for other items.\nrecommenderlab provides implementations of many popular algorithms, including the following.\n\u2022 User-based collaborative filtering (UBCF) predicts ratings by aggregating the ratings of users who have a similar rating history to the active user (Goldberg et al. 1992;Resnick, Iacovou, Suchak, Bergstrom, and Riedl 1994;Shardanand and Maes 1995).\n\u2022 Item-based collaborative filtering (IBCF) uses item-to-item similarity based on user ratings to find items that are similar to the items the active user likes (Kitts, Freed, and Vrieze 2000;Sarwar et al. 2001;Linden, Smith, and York 2003;Deshpande and Karypis 2004).\n\u2022 Latent factor models use singular value decomposition (SVD) to estimate missing ratings using methods like SVD with column-mean imputation, Funk SVD or alternating least squares (Hu, Koren, and Volinsky 2008;Koren, Bell, and Volinsky 2009).\n\u2022 Association rule-based recommender (AR) uses association rules to find recommended items (Fu, Budzik, and Hammond 2000;Mobasher, Dai, Luo, and Nakagawa 2001;Geyer-Schulz, Hahsler, and Jahn 2002;Lin, Alvarez, and Ruiz 2002;Demiriz 2004).\n\u2022 Popular items (POPULAR) is a non-personalized algorithm which recommends to all users the most popular items they have not rated yet.\n\u2022 Randomly chosen items (RANDOM) creates random recommendations which can be used as a baseline for recommender algorithm evaluation.\n\u2022 Re-recommend liked items (RERECOMMEND) recommends items which the user has rated highly in the past. These recommendations can be useful for items that are typically consumed more than once (e.g., listening to songs or buying groceries).\n\u2022 Hybrid recommendations (HybridRecommender) aggregates the recommendations of several algorithms (\u00c7ano and Morisio 2017).\nWe will discuss some of these algorithms in the rest of the paper. Detailed information can be found in the survey book by Desrosiers and Karypis (2011).\nThis rest of this paper is structured as follows. Section 2 introduces collaborative filtering and some of its popular algorithms. In section 3 we discuss the evaluation of recommender algorithms. We introduce the infrastructure provided by recommenderlab in section 4. In section 5 we illustrate the capabilities on the package to create and evaluate recommender algorithms. We conclude with section 6.", "publication_ref": ["b36", "b29", "b38", "b5", "b3", "b1", "b28", "b12", "b37", "b7", "b12", "b34", "b39", "b20", "b37", "b27", "b7", "b18", "b23", "b10", "b31", "b11", "b26", "b6", "b4", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Collaborative Filtering", "text": "To understand the use of the software, a few formal definitions are necessary. We will often give examples for a movie recommender, but the examples generalize to other types of items as well. Let U = {u 1 , u 2 , . . . , u m } be the set of users and I = {i 1 , i 2 , . . . , i n } the set of items.\nRatings are stored in a m \u00d7 n user-item rating matrix R = (r jk ), where r jk represents the rating of user u j for item i k . Typically, only a small fraction of ratings are known and for many cells in R, the values are missing. Missing values represent movies that the user has not rated and potentially also not seen yet.\nCollaborative filtering aims to create recommendations for a user called the active user u a \u2208 U.\nWe define the set of items unknown to user u a as I a = I \\ {i l \u2208 I|r al is not missing}. The two typical tasks are to predict ratings for all items in I a or to create a list containing the best N recommended items from I a (i.e., a top-N recommendation list) for u a . Predicting all missing ratings means completing the row of the rating matrix where the missing values for items in I a are replaced by ratings estimated from other data in R. From this point of view, recommender systems are related to matrix completion problem. Creating a top-N list can be seen as a second step after predicting ratings for all unknown items in I a and then taking the N items with the highest predicted ratings. Some algorithms skip predicting ratings first and are able to find the top N items directly. A list of top-N recommendations for a user u a is an partially ordered set T N = (X , \u2265), where X \u2282 I a and |X | \u2264 N (| \u2022 | denotes the cardinality of the set). Note that there may exist cases where top-N lists contain less than N items. This can happen if |I a | < N or if the CF algorithm is unable to identify N items to recommend. The binary relation \u2265 is defined as x \u2265 y if and only if rax \u2265 ray for all x, y \u2208 X . Furthermore we require that \u2200 x\u2208X \u2200 y\u2208Ia rax \u2265 ray to ensure that the top-N list contains only the items with the highest estimated rating.\nTypically we deal with a very large number of items with unknown ratings which makes first predicting rating values for all of them computationally expensive. Some approaches (e.g., rule based approaches) can predict the top-N list directly without considering all unknown items first.\nCollaborative filtering algorithms are typically divided into two groups, memory-based CF and model-based CF algorithms (Breese, Heckerman, and Kadie 1998). Memory-based CF use the whole (or at least a large sample of the) user database to create recommendations. The most prominent algorithm is user-based collaborative filtering. The disadvantages of this approach is scalability since the whole user database has to be processed online for creating recommendations. Model-based algorithms use the user database to learn a more compact model (e.g, clusters with users of similar preferences) that is later used to create recommendations.\nIn the following we will present the basics of well known memory and model-based collaborative filtering algorithms. Further information about these algorithms can be found in the recent survey book chapter by Desrosiers and Karypis (2011).", "publication_ref": ["b2", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "User-based Collaborative Filtering", "text": "User-based CF (Goldberg et al. 1992;Resnick et al. 1994;Shardanand and Maes 1995) is a memory-based algorithm which tries to mimics word-of-mouth by analyzing rating data from many individuals. The assumption is that users with similar preferences will rate items similarly. Thus missing ratings for a user can be predicted by first finding a neighborhood of similar users and then aggregate the ratings of these users to form a prediction.\nThe neighborhood is defined in terms of similarity between users, either by taking a given number of most similar users (k nearest neighbors) or all users within a given similarity threshold. Popular similarity measures for CF are the Pearson correlation coefficient and the Cosine similarity. These similarity measures are defined between two users u x and u y as\nsim Pearson ( x, y) = i\u2208I ( x i \u00af x)( y i \u00af y) (|I| -1) sd( x) sd( y) (1) and sim Cosine ( x, y) = x \u2022 y x y ,(2)\nwhere x = r x and y = r y represent the row vectors in R with the two users' profile vectors. sd(\u2022) is the standard deviation and \u2022 is the l 2 -norm of a vector. For calculating similarity using rating data only the dimensions (items) are used which were rated by both users.\nNow the neighborhood for the active user N (a) \u2282 U can be selected by either a threshold on the similarity or by taking the k nearest neighbors. Once the users in the neighborhood are found, their ratings are aggregated to form the predicted rating for the active user. The easiest form is to just average the ratings in the neighborhood.\nraj = 1 |N (a)| i\u2208N (a) r ij (3)\nAn example of the process of creating recommendations by user-based CF is shown in Figure 1.\nTo the left is the rating matrix R with 6 users and 8 items and ratings in the range 1 to 5 (stars). We want to create recommendations for the active user u a shown at the bottom of the matrix. To find the k-neighborhood (i.e., the k nearest neighbors) we calculate the similarity between the active user and all other users based on their ratings in the database and then select the k users with the highest similarity. To the right in Figure 1 we see a 2-dimensional representation of the similarities (users with higher similarity are displayed closer) with the active user in the center. The k = 3 nearest neighbors (u 1 , u 2 and u 3 ) are selected and marked in the database to the left. To generate an aggregated estimated rating, we compute the average ratings in the neighborhood for each item not rated by the active user. To create a top-N recommendation list, the items are ordered by predicted rating. In the small example in Figure 1 the order in the top-N list (with N \u2265 4) is i 2 , i 1 , i 7 and i 5 . However, for a real application we probably would not recommend items i 7 and i 5 because of their low ratings. ? 4.0 4.0 2.0 1.0 2.0 ? ? 3.0 ? ? ? 5.0 1.0 ? ? 3.0 ? ? 3.0 2.0 2.0 ? 3.0 4.0 ? ? 2.0 1.0 1.0 2.0 4.0 1.0 1.0 ? ? ? ? ? 1.0 ? 1.0 ? ? 1.0 1.0 ? 1.0 ? ? 4.0 3.0 ? 1.0 ? 5.0 3.5 4.0 2.3 2.0\ni 1 i 2 i 3 i 4 i 5 i 6 i 7 i 8 u 1 u 2 u 3 u 4 u 5 u 6 u a r\u0302a (a) (b) 0.3 1.0 0.2 0.3 0.1 0.1 u a u 1 u 2 u 3 u 4 u 5 u 6 0.3 1.0 0.2 0.3 0.1 0.1 u a u 1 u 2 u 3 u 4 u 5 u 6 (c)", "publication_ref": ["b12", "b34", "b39"], "figure_ref": [], "table_ref": []}, {"heading": "R s a", "text": "Figure 1: User-based collaborative filtering example with (a) rating matrix R and estimated ratings for the active user, (b), similarites between the active user and the other users s a (Euclidean distance converted to similarities), and (b) the user neighborhood formation.\nThe fact that some users in the neighborhood are more similar to the active user than others (see Figure 1 (b)) can be incorporated as weights into Equation (3).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "raj = 1", "text": "i\u2208N (a) s ai i\u2208N (a)\ns ai r ij (4)\ns ai is the similarity between the active user u a and user u i in the neighborhood.\nFor some types of data the performance of the recommender algorithm can be improved by removing user rating bias. This can be done by normalizing the rating data before applying the recommender algorithm. Any normalization function h : R n\u00d7m \u2192 R n\u00d7m can be used for preprocessing. Ideally, this function is reversible to map the predicted rating on the normalized scale back to the original rating scale. Normalization is used to remove individual rating bias by users who consistently always use lower or higher ratings than other users. A popular method is to center the rows of the user-item rating matrix by\nh(r ui ) = r ui -ru ,\nwhere ru is the mean of all available ratings in row u of the user-item rating matrix R.\nOther methods like Z-score normalization which also takes rating variance into account can be found in the literature (see, e.g., Desrosiers and Karypis 2011).\nThe two main problems of user-based CF are that the whole user database has to be kept in memory and that expensive similarity computation between the active user and all other users in the database has to be performed.", "publication_ref": ["b8"], "figure_ref": [], "table_ref": []}, {"heading": "Item-based Collaborative Filtering", "text": "Item-based CF (Kitts et al. 2000;Sarwar et al. 2001;Linden et al. 2003;Deshpande and Karypis 2004) is a model-based approach which produces recommendations based on the relationship between items inferred from the rating matrix. The assumption behind this approach is that users will prefer items that are similar to other items they like.  To reduce the model size to n \u00d7 k with k n, for each item only a list of the k most similar items and their similarity values are stored. The k items which are most similar to item i is denoted by the set S(i) which can be seen as the neighborhood of size k of the item. Retaining only k similarities per item improves the space and time complexity significantly but potentially sacrifices some recommendation quality (Sarwar et al. 2001).\nTo make a recommendation based on the model we use the similarities to calculate a weighted sum of the user's ratings for related items.", "publication_ref": ["b20", "b37", "b27", "b7", "b37"], "figure_ref": [], "table_ref": []}, {"heading": "rai = 1", "text": "j\u2208S(i)\u2229{l ; r al =?} s ij j\u2208S(i)\u2229{l ; r al =?} s ij r aj (5) Figure 2 shows an example for n = 8 items with k = 3. For the similarity matrix S only the k = 3 largest entries are stored per row (these entries are marked using bold face). For the example we assume that we have ratings for the active user for items i 1 , i 5 and i 8 . The rows corresponding to these items are highlighted in the item similarity matrix. We can now compute the weighted sum using the similarities (only the reduced matrix with the k = 3 highest ratings is used) and the user's ratings. The result (below the matrix) shows that i 3 has the highest estimated rating for the active user.\nSimilar to user-based recommender algorithms, user-bias can be reduced by first normalizing the user-item rating matrix before computing the item-to-item similarity matrix.\nItem-based CF is more efficient than user-based CF since the model (reduced similarity matrix) is relatively small (N \u00d7 k) and can be fully precomputed. Item-based CF is known to only produce slightly inferior results compared to user-based CF and higher order models which take the joint distribution of sets of items into account are possible (Deshpande and Karypis 2004). Furthermore, item-based CF is successfully applied in large scale recommender systems (e.g., by Amazon.com).", "publication_ref": ["b7"], "figure_ref": [], "table_ref": []}, {"heading": "User and Item-Based CF using 0-1 Data", "text": "Less research is available for situations where no large amount of detailed directly elicited rating data is available. However, this is a common situation and occurs when users do not want to directly reveal their preferences by rating an item (e.g., because it is to time consuming). In this case preferences can only be inferred by analyzing usage behavior. For example, we can easily record in a supermarket setting what items a customer purchases. However, we do not know why other products were not purchased. The reason might be one of the following.\n\u2022 The customer does not need the product right now.\n\u2022 The customer does not know about the product. Such a product is a good candidate for recommendation.\n\u2022 The customer does not like the product. Such a product should obviously not be recommended. Mild and Reutterer (2003) and Lee, Jun, Lee, and Kim (2005) present and evaluate recommender algorithms for this setting. The same reasoning is true for recommending pages of a web site given click-stream data. Here we only have information about which pages were viewed but not why some pages were not viewed. This situation leads to binary data or more exactly to 0-1 data where 1 means that we inferred that the user has a preference for an item and 0 means that either the user does not like the item or does not know about it. Pan, Zhou, Cao, Liu, Lukose, Scholz, and Yang (2008)  In the 0-1 case with r jk \u2208 0, 1 where we define:\nr jk = 1 user u j is known to have a preference for item i k 0 otherwise.(6)\nTwo strategies to deal with one-class data is to assume all missing ratings (zeros) are negative examples or to assume that all missing ratings are unknown. In addition, Pan et al. (2008) propose strategies which represent a trade-off between the two extreme strategies based on wighted low rank approximations of the rating matrix and on negative example sampling which might improve results across all recommender algorithms.\nIf we assume that users typically favor only a small fraction of the items and thus most items with no rating will be indeed negative examples. then we have no missing values and can use the approaches described above for real valued rating data. However, if we assume all zeroes are missing values, then this lead to the problem that we cannot compute similarities using Pearson correlation or Cosine similarity since the not missing parts of the vectors only contains ones. A similarity measure which only focuses on matching ones and thus prevents the problem with zeroes is the Jaccard index:\nsim Jaccard (X , Y) = |X \u2229 Y| |X \u222a Y| , (7\n)\nwhere X and Y are the sets of the items with a 1 in user profiles u a and u b , respectively. The Jaccard index can be used between users for user-based filtering and between items for item-based filtering as described above.", "publication_ref": ["b30", "b24", "b32", "b32"], "figure_ref": [], "table_ref": []}, {"heading": "Recommendations for 0-1 Data Based on Association Rules", "text": "Recommender systems using association rules produce recommendations based on a dependency model for items given by a set of association rules (Fu et al. 2000;Mobasher et al. 2001;Geyer-Schulz et al. 2002;Lin et al. 2002;Demiriz 2004). The binary profile matrix R is seen as a database where each user is treated as a transaction that contains the subset of items in I with a rating of 1. Hence transaction k is defined as T k = {i j \u2208 I|r jk = 1} and the whole transaction data base is D = {T 1 , T 2 , . . . , T U } where U is the number of users. To build the dependency model, a set of association rules R is mined from R. Association rules are rules of the form X \u2192 Y where X , Y \u2286 I and X \u2229 Y = \u2205. For the model we only use association rules with a single item in the right-hand-side of the rule (|Y| = 1). To select a set of useful association rules, thresholds on measures of significance and interestingness are used. Two widely applied measures are:\nsupport(X \u2192 Y) = support(X \u222a Y) = Freq(X \u222a Y)/|D| = P (E X \u2229 E Y ) confidence(X \u2192 Y) = support(X \u222a Y)/support(X ) = P (E Y |E X )\nFreq(I) gives the number of transactions in the data base D that contains all items in I. E I is the event that the itemset I is contained in a transaction.\nWe now require support(X \u2192 Y) > s and confidence(X \u2192 Y) > c and also include a length constraint |X \u222a Y| \u2264 l. The set of rules R that satisfy these constraints form the dependency model. Although finding all association rules given thresholds on support and confidence is a hard problem (the model grows in the worse case exponential with the number of items), algorithms that efficiently find all rules in most cases are available (e.g., Agrawal and Srikant 1994;Zaki 2000;Han, Pei, Yin, and Mao 2004). Also model size can be controlled by l, s and c.\nTo make a recommendation for an active user u a given the set of items T a the user likes and the set of association rules R (dependency model), the following steps are necessary:\n1. Find all matching rules X \u2192 Y for which X \u2286 T a in R.\n2. Recommend N unique right-hand-sides (Y) of the matching rules with the highest confidence (or another measure of interestingness).\nThe dependency model is very similar to item-based CF with conditional probability-based similarity (Deshpande and Karypis 2004). It can be fully precomputed and rules with more than one items in the left-hand-side (X ), it incorporates higher order effects between more than two items.", "publication_ref": ["b10", "b31", "b11", "b26", "b6", "b0", "b41", "b16", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Other collaborative filtering methods", "text": "Over time several other model-based approaches have been developed. A popular simple item-based approach is the Slope One algorithm (Lemire and Maclachlan 2005). Another family of algorithms is based on latent factors approach using matrix decomposition (Koren et al. 2009). More recently, deep learning has become a very popular method for flexible matrix completion, matrix factorization and collaborative ranking. A comprehensive survey is presented by Zhang, Yao, Sun, and Tay (2019).\nThese algorithms are outside the scope of this introductory paper.", "publication_ref": ["b25", "b23", "b42"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation of Recommender Algorithms", "text": "Evaluation of recommender systems is an important topic and reviews were presented by Herlocker, Konstan, Terveen, and Riedl ( 2004) and Gunawardana and Shani (2009). Typically, given a rating matrix R, recommender algorithms are evaluated by first partitioning the users (rows) in R into two sets U train \u222a U test = U. The rows of R corresponding to the training users U train are used to learn the recommender model. Then each user u a \u2208 U test is seen as an active user. Before creating recommendations some items are withheld from the profile r ua\u2022 and it measured either how well the predicted rating matches the withheld value or, for top-N algorithms, if the items in the recommended list are rated highly by the user. Finally, the evaluation measures calculated for all test users are averaged.\nTo determine how to split U into U train and U test we can use several approaches (Kohavi 1995).\n\u2022 Splitting: We can randomly assign a predefined proportion of the users to the training set and all others to the test set.\n\u2022 Bootstrap sampling: We can sample from U test with replacement to create the training set and then use the users not in the training set as the test set. This procedure has the advantage that for smaller data sets we can create larger training sets and still have users left for testing.\n\u2022 k-fold cross-validation: Here we split U into k sets (called folds) of approximately the same size. Then we evaluate k times, always using one fold for testing and all other folds for leaning. The k results can be averaged. This approach makes sure that each user is at least once in the test set and the averaging produces more robust results and error estimates.\nThe items withheld in the test data are randomly chosen. Breese et al. (1998) introduced the four experimental protocols called Given 2, Given 5, Given 10 and All-but-1. For the Given x protocols for each user x randomly chosen items are given to the recommender algorithm and the remaining items are withheld for evaluation. For All but x the algorithm gets all but x withheld items.\nIn the following we discuss the evaluation of predicted ratings and then of top-N recommendation lists.", "publication_ref": ["b15", "b21", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation of predicted ratings", "text": "A typical way to evaluate a prediction is to compute the deviation of the prediction from the true value. This is the basis for the Mean Average Error (MAE) where K is the set of all user-item pairings (i, j) for which we have a predicted rating rij and a known rating r ij which was not used to learn the recommendation model.\nMAE = 1 |K| (i,j)\u2208K |r ij -rij |,(8)\nAnother popular measure is the Root Mean Square Error (RMSE).\nRMSE = (i,j)\u2208K (r ij -rij ) 2 |K| (9)\nRMSE penalizes larger errors stronger than MAE and thus is suitable for situations where small prediction errors are not very important.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation Top-N recommendations", "text": "The items in the predicted top-N lists and the withheld items liked by the user (typically determined by a simple threshold on the actual rating) for all test users U test can be aggregated into a so called confusion matrix depicted in table 2 (see Kohavi and Provost (1998)) which corresponds exactly to the outcomes of a classical statistical experiment. The confusion matrix shows how many of the items recommended in the top-N lists (column predicted positive; d + b) were withheld items and thus correct recommendations (cell d) and how many where potentially incorrect (cell b). The matrix also shows how many of the not recommended items (column predicted negative; a + c) should have actually been recommended since they represent withheld items (cell c).\nFrom the confusion matrix several performance measures can be derived. For the data mining task of a recommender system the performance of an algorithm depends on its ability to learn significant patterns in the data set. Performance measures used to evaluate these algorithms have their root in machine learning. A commonly used measure is accuracy, the fraction of correct recommendations to total possible recommendations.", "publication_ref": ["b22"], "figure_ref": [], "table_ref": []}, {"heading": "Accuracy = correct recommendations total possible recommendations", "text": "= a + d a + b + c + d (10)\nA common error measure is the mean absolute error (MAE, also called mean absolute deviation or MAD).\nMAE = 1 N N i=1 | i | = b + c a + b + c + d , (11\n)\nwhere N = a + b + c + d is the total number of items which can be recommended and | i | is the absolute error of each item. Since we deal with 0-1 data, | i | can only be zero (in cells a and d in the confusion matrix) or one (in cells b and c). For evaluation recommender algorithms for rating data, the root mean square error is often used. For 0-1 data it reduces to the square root of MAE.\nRecommender systems help to find items of interest from the set of all available items. This can be seen as a retrieval task known from information retrieval. Therefore, standard information retrieval performance measures are frequently used to evaluate recommender performance. Precision and recall are the best known measures used in information retrieval (Salton and McGill 1983;van Rijsbergen 1979).\nPrecision = correctly recommended items total recommended items = d b + d (12) Recall = correctly recommended items total useful recommendations = d c + d (13)\nOften the number of total useful recommendations needed for recall is unknown since the whole collection would have to be inspected. However, instead of the actual total useful recommendations often the total number of known useful recommendations is used. Precision and recall are conflicting properties, high precision means low recall and vice versa. To find an optimal trade-off between precision and recall a single-valued measure like the E-measure (van Rijsbergen 1979) can be used. The parameter \u03b1 controls the trade-off between precision and recall.\nE-measure = 1 \u03b1(1/Precision) + (1 -\u03b1)(1/Recall)(14)\nA popular single-valued measure is the F-measure. It is defined as the harmonic mean of precision and recall.\nF-measure = 2 Precision Recall Precision + Recall = 2 1/Precision + 1/Recall (15)\nIt is a special case of the E-measure with \u03b1 = .5 which places the same weight on both, precision and recall. In the recommender evaluation literature the F-measure is often referred to as the measure F1.\nAnother method used in the literature to compare two classifiers at different parameter settings is the Receiver Operating Characteristic (ROC). The method was developed for signal detection and goes back to the Swets model (van Rijsbergen 1979). The ROC-curve is a plot of the system's probability of detection (also called sensitivity or true positive rate TPR which is equivalent to recall as defined in formula 13) by the probability of false alarm (also called false positive rate FPR or 1 -specificity, where specificity = a a+b ) with regard to model parameters. A possible way to compare the efficiency of two systems is by comparing the size of the area under the ROC-curve, where a bigger area indicates better performance.", "publication_ref": ["b35", "b40"], "figure_ref": [], "table_ref": []}, {"heading": "Recommenderlab Infrastructure", "text": "recommenderlab is implemented using formal classes in the S4 class system. Figure 3 shows the main classes and their relationships.\nThe package uses the abstract ratingMatrix to provide a common interface for rating data. ratingMatrix implements many methods typically available for matrix-like objects. For example, dim(), dimnames(), colCounts(), rowCounts(), colMeans(), rowMeans(), colSums() and rowSums(). Additionally sample() can be used to sample from users (rows) and image() produces an image plot.\nFor ratingMatrix we provide two concrete implementations realRatingMatrix and binaryRatingMatrix to represent different types of rating matrices R. realRatingMatrix implements a rating matrix with real valued ratings stored in sparse format defined in package Matrix. Sparse matrices in Matrix typically do not store 0s explicitly, however for realRatingMatrix we use these sparse matrices such that instead of 0s, NAs are not explicitly stored.\nbinaryRatingMatrix implements a 0-1 rating matrix using the implementation of itemMatrix defined in package arules. itemMatrix stores only the ones and internally uses a sparse representation from package Matrix. With this class structure recommenderlab can be easily extended to other forms of rating matrices with different concepts for efficient storage in the future.\nClass Recommender implements the data structure to store recommendation models. The creator method\nRecommender(data, method, parameter = NULL)\ntakes data as a ratingMatrix, a method name and some optional parameters for the method and returns a Recommender object. Once we have a recommender object, we can predict top-N recommendations for active users using predict(object, newdata, n=10, type=c(\"topNList\", \"ratings\", \"ratingMatrix\"), ...).\nPredict can return either top-N lists (default setting) or predicted ratings. object is the recommender object, newdata is the data for the active users. For top-N lists n is the maximal number of recommended items in each list and predict() will return an objects of class topNList which contains one top-N list for each active user. For \"ratings\" and \"ratingMatrix\", n is ignored and an object of realRatingMatrix is returned. Each row contains the predicted ratings for one active user. The difference is, that for \"ratings\", the items for which a rating exists in newdata have a NA instead of a predicted/actual ratings.\nThe actual implementations for the recommendation algorithms are managed using the registry mechanism provided by package registry. The registry called recommenderRegistry and stores recommendation method names and a short description. Generally, the registry mechanism is hidden from the user and the creator function Recommender() uses it in the background to map a recommender method name to its implementation. However, the registry can be directly queried by recommenderRegistry$get_entries() and new recommender algorithms can be added by the user. We will give and example for this feature in the examples section of this paper.\nTo evaluate recommender algorithms package recommenderlab provides the infrastructure to create and maintain evaluation schemes stored as an object of class evaluationScheme from rating data. The creator function evaluationScheme(data, method=\"split\", train=0.9, k=10, given=3)\ncreates the evaluation scheme from a data set using a method (e.g., simple split, bootstrap sampling, k-fold cross validation). Testing is perfomed by withholding items (parameter given). Breese et al. (1998) introduced the four experimental witholding protocols called Given 2, Given 5, Given 10 and All-but-1. During testing, the Given x protocol presents the algorithm with only x randomly chosen items for the test user, and the algorithm is evaluated by how well it is able to predict the withheld items. For All-but-x, a generalization of All-but-1, the algorithm sees all but x withheld ratings for the test user. given controls x in the evaluations scheme. Positive integers result in a Given x protocol, while negative values produce a All-but-x protocol.\nThe function evaluate() is then used to evaluate several recommender algorithms using an evaluation scheme resulting in a evaluation result list (class evaluationResultList) with one entry (class evaluationResult) per algorithm. Each object of evaluationResult contains one or several object of confusionMatrix depending on the number of evaluations specified in the evaluationScheme (e.g., k for k-fold cross validation). With this infrastructure several recommender algorithms can be compared on a data set with a single line of code.\nIn the following, we will illustrate the usage of recommenderlab with several examples.", "publication_ref": ["b2"], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Examples", "text": "This fist few example shows how to manage data in recommender lab and then we create and evaluate recommenders. First, we load the package.\nR> library(\"recommenderlab\")", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Coercion to and from rating matrices", "text": "For this example we create a small artificial data set as a matrix.\nR> m <-matrix(sample(c(as.numeric(0:5), NA), 50, + replace=TRUE, prob=c(rep(.4/6,6),.6)), ncol=10, + dimnames=list(user=paste(\"u\", 1:5, sep= ), + item=paste(\"i\", 1:10, sep= ))) R> m With coercion, the matrix can be easily converted into a realRatingMatrix object which stores the data in sparse format (only non-NA values are stored explicitly; NA values are represented by a dot).\nR> r <-as(m, \"realRatingMatrix\") R> r 5 x 10 rating matrix of class ???realRatingMatrix??? with 19 ratings.\nR> getRatingMatrix(r) 5 x 10 sparse Matrix of class \"dgCMatrix\" u1 . 2 3 5 . 5 . 4.000e+00 . . u2 2 . . . . . . . 2 3 u3 2 . . . . 1 . . . . u4 2 2 1 . . 5 . 2.225e-308 2 . u5 5 . . . . . . 5.000e+00 . 4\nThe realRatingMatrix can be coerced back into a matrix which is identical to the original matrix.\nR> identical(as(r, \"matrix\"),m)\n[1] TRUE It can also be coerced into a list of users with their ratings for closer inspection or into a data.frame with user/item/rating tuples. The data.frame version is especially suited for writing rating data to a file (e.g., by write.csv()). Coercion from data.frame (user/item/rating tuples) and list into a sparse rating matrix is also provided. This way, external rating data can easily be imported into recommenderlab.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "R> as(r, \"list\")", "text": "$ 0 i2 i3 i4 i6 i8 2 3 5 5 4 $ 1 i1 i9 i10 2 2 3 $ 2 i1 i6 2 1 $ 3 i1 i2 i3 i6 i8 i9 2.000e+00 2.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Normalization", "text": "An important operation for rating matrices is to normalize the entries to, e.g., centering to remove rating bias by subtracting the row mean from all ratings in the row. This is can be easily done using normalize().\nR> r_m <-normalize(r) R> r_m 5 x 10 rating matrix of class ???realRatingMatrix??? with 19 ratings. Normalized using center on rows. R> getRatingMatrix(r_m) Normalization can be reversed using denormalize().\n5\nR> denormalize(r_m) 5 x 10 rating matrix of class ???realRatingMatrix??? with 19 ratings.\nSmall portions of rating matrices can be visually inspected using image().\nR> image(r, main = \"Raw Ratings\") R> image(r_m, main = \"Normalized Ratings\")\nFigure 4 shows the resulting plots.", "publication_ref": [], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "Binarization of data", "text": "A matrix with real valued ratings can be transformed into a 0-1 matrix with binarize() and a user specified threshold (min_ratings) on the raw or normalized ratings. In the following only items with a rating of 4 or higher will become a positive rating in the new binary rating matrix.\nR> r_b <-binarize(r, minRating=4) R> r_b 5 x 10 rating matrix of class ???binaryRatingMatrix??? with 7 ratings.\nR> as(r_b, \"matrix\") \ni1 i2 i3 i4 i5 i6 i7 i8 i9 i10", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Inspection of data set properties", "text": "We will use the data set Jester5k for the rest of this section. This data set comes with recommenderlab and contains a sample of 5000 users from the anonymous ratings data from the Jester Online Joke Recommender System collected between April 1999and May 2003(Goldberg, Roeder, Gupta, and Perkins 2001). The data set contains ratings for 100 jokes on a scale from -10 to +10. All users in the data set have rated 36 or more jokes.\nR> data(Jester5k) R> Jester5k 5000 x 100 rating matrix of class ???realRatingMatrix??? with 363209 ratings.\nJester5k contains 363209 ratings. For the following examples we use only a subset of the data containing a sample of 1000 users (we set the random number generator seed for reproducibility). For random sampling sample() is provided for rating matrices.\nR> set.seed(1234) R> r <-sample(Jester5k, 1000) R> r 1000 x 100 rating matrix of class ???realRatingMatrix??? with 74323 ratings.\nThis subset still contains 74323 ratings. Next, we inspect the ratings for the first user. We can select an individual user with the extraction operator.", "publication_ref": ["b13"], "figure_ref": [], "table_ref": []}, {"heading": "R> rowCounts(r[1,])", "text": "u20648 74\nR> as(r[1,], \"list\") $ 0 j1 j2 j3 j4 j5 j6 j7 j8 j9 j10 j11 j12 -2.86 1. 75 -4.03 -5.78 2.23 -5.44 -3.40 8.74 -4.51 3.74  The user has rated 74 jokes, the list shows the ratings and the user's rating average is -0.423243243243243 .\nNext, we look at several distributions to understand the data better. getRatings() extracts a vector with all non-missing ratings from a rating matrix.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "R> hist(getRatings(r), breaks=100)", "text": "In the histogram in Figure 5 shoes an interesting distribution where all negative values occur with a almost identical frequency and the positive ratings more frequent with a steady decline towards the rating 10. Since this distribution can be the result of users with strong rating bias, we look next at the rating distribution after normalization.", "publication_ref": [], "figure_ref": ["fig_6"], "table_ref": []}, {"heading": "R> hist(getRatings(normalize(r)), breaks=100)", "text": "R> hist(getRatings(normalize(r, method=\"Z-score\")), breaks=100)\nFigure 6 shows that the distribution of ratings ins closer to a normal distribution after row centering and Z-score normalization additionally reduces the variance to a range of roughly -3 to +3 standard deviations. It is interesting to see that there is a pronounced peak of ratings between zero and two.  R> hist(colMeans(r), breaks=20)\nFigure 7 shows that there are unusually many users with ratings around 70 and users who have rated all jokes. The average ratings per joke look closer to a normal distribution with a mean above 0.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Creating a recommender", "text": "A recommender is created using the creator function Recommender(). Available recommendation methods are stored in a registry. The registry can be queried. Here we are only interested in methods for real-valued rating data.\nR> recommenderRegistry$get_entries(dataType = \"realRatingMatrix\") Next, we create a recommender which generates recommendations solely on the popularity of items (the number of users who have the item in their profile). We create a recommender from the first 1000 users in the Jester5k data set.\n$\nR> r <-Recommender(Jester5k[1:1000], method = \"POPULAR\") R> r\nRecommender of type ???POPULAR??? for ???realRatingMatrix??? learned using 1000 users.\nThe model can be obtained from a recommender using getModel().", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "R> names(getModel(r))", "text": "[1] \"topN\" \"ratings\" \"normalize\" [4] \"aggregationRatings\" \"aggregationPopularity\" \"verbose\"", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "R> getModel(r)$topN", "text": "Recommendations as ???topNList??? with n = 100 for 1 users.\nIn this case the model has a top-N list to store the popularity order and further elements (average ratings, if it used normalization and the used aggregation function).\nRecommendations are generated by predict() (consistent with its use for other types of models in R). The result are recommendations in the form of an object of class TopNList.\nHere we create top-5 recommendation lists for two users who were not used to learn the model.\nR> recom <-predict(r, Jester5k[1001:1002], n=5) R> recom\nRecommendations as ???topNList??? with n = 5 for 2 users.\nThe result contains two ordered top-N recommendation lists, one for each user. The recommended items can be inspected as a list.\nR> as(recom, \"list\")", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "$u15553", "text": "[1] \"j89\" \"j72\" \"j93\" \"j76\" \"j87\" $u7886\n[1] \"j89\" \"j72\" \"j93\" \"j76\" \"j1\"\nSince the top-N lists are ordered, we can extract sublists of the best items in the top-N . For example, we can get the best 3 recommendations for each list using bestN().\nR> recom3 <-bestN(recom, n = 3) R> recom3\nRecommendations as ???topNList??? with n = 3 for 2 users.\nR> as(recom3, \"list\")", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "$u15553", "text": "[1] \"j89\" \"j72\" \"j93\" $u7886\n[1] \"j89\" \"j72\" \"j93\"\nMany recommender algorithms can also predict ratings. This is also implemented using predict() with the parameter type set to \"ratings\". Predicted ratings are returned as an object of realRatingMatrix. The prediction contains NA for the items rated by the active users. In the example we show the predicted ratings for the first 10 items for the two active users.\nAlternatively, we can also request the complete rating matrix which includes the original ratings by the user.\nR> recom <-predict (r, Jester5k[1001(r, Jester5k[ :1002]], type=\"ratingMatrix\") R> recom 2 x 100 rating matrix of class ???realRatingMatrix??? with 200 ratings. R> as (recom, \"matrix\") ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation of predicted ratings", "text": "Next, we will look at the evaluation of recommender algorithms. recommenderlab implements several standard evaluation methods for recommender systems. Evaluation starts with creating an evaluation scheme that determines what and how data is used for training and testing. Here we create an evaluation scheme which splits the first 1000 users in Jester5k into a training set (90%) and a test set (10%). For the test set 15 items will be given to the recommender algorithm and the other items will be held out for computing the error.\nR> e <-evaluationScheme(Jester5k[1:1000], method=\"split\", train=0.9, + given=15, goodRating=5) R> e Evaluation scheme with 15 items given Method: ???split??? with 1 run(s). Training set proportion: 0.900 Good ratings: >=5.000000 Data set: 1000 x 100 rating matrix of class ???realRatingMatrix??? with 74164 ratings.\nWe create two recommenders (user-based and item-based collaborative filtering) using the training data. R> r1 <-Recommender(getData(e, \"train\"), \"UBCF\") R> r1\nRecommender of type ???UBCF??? for ???realRatingMatrix??? learned using 900 users. R> r2 <-Recommender(getData(e, \"train\"), \"IBCF\") R> r2\nRecommender of type ???IBCF??? for ???realRatingMatrix??? learned using 900 users.\nNext, we compute predicted ratings for the known part of the test data (15 items for each user) using the two algorithms.\nR> p1 <-predict(r1, getData(e, \"known\"), type=\"ratings\") R> p1 100 x 100 rating matrix of class ???realRatingMatrix??? with 8357 ratings. R> p2 <-predict(r2, getData(e, \"known\"), type=\"ratings\") R> p2 100 x 100 rating matrix of class ???realRatingMatrix??? with 8417 ratings.\nFinally, we can calculate the error between the prediction and the unknown part of the test data.\nR> error <-rbind( + UBCF = calcPredictionAccuracy(p1, getData(e, \"unknown\")), + IBCF = calcPredictionAccuracy(p2, getData(e, \"unknown\")) + ) R> error 4.571 20.89 3.585 IBCF 4.538 20.60 3.440 In this example user-based collaborative filtering produces a smaller prediction error.\nRMSE MSE MAE UBCF", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation of a top-N recommender algorithm", "text": "For this example we create a 4-fold cross validation scheme with the the Given-3 protocol, i.e., for the test users all but three randomly selected items are withheld for evaluation.\nR> scheme <-evaluationScheme(Jester5k[1:1000], method=\"cross\", k=4, given=3, + goodRating=5) R> scheme Evaluation scheme with 3 items given Method: ???cross-validation??? with 4 run(s). Good ratings: >=5.000000 Data set: 1000 x 100 rating matrix of class ???realRatingMatrix??? with 74164 ratings.\nNext we use the created evaluation scheme to evaluate the recommender method popular. We evaluate top-1, top-3, top-5, top-10, top-15 and top-20 recommendation lists.\nR> results <-evaluate(scheme, method=\"POPULAR\", type = \"topNList\", + n=c (1,3,5,10,15,20)) \nPOPULAR", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "R> results", "text": "Evaluation results for 4 folds/samples using method ???POPULAR???.\nThe result is an object of class EvaluationResult which contains several confusion matrices. getConfusionMatrix() will return the confusion matrices for the 4 runs (we used 4-fold cross evaluation) as a list. In the following we look at the first element of the list which represents the first of the 4 runs. Evaluation results can be plotted using plot(). The default plot is the ROC curve which plots the true positive rate (TPR) against the false positive rate (FPR).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "R> getConfusionMatrix(results)[[1]]", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "R> plot(results, annotate=TRUE)", "text": "For the plot where we annotated the curve with the size of the top-N list is shown in Figure 8. By using \"prec/rec\" as the second argument, a precision-recall plot is produced (see Figure 9). R> plot(results, \"prec/rec\", annotate=TRUE)", "publication_ref": [], "figure_ref": ["fig_8"], "table_ref": []}, {"heading": "Comparing recommender algorithms", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Comparing top-N recommendations", "text": "The comparison of several recommender algorithms is one of the main functions of recommenderlab. For comparison also evaluate() is used. The only change is to use evaluate() with a list of algorithms together with their parameters instead of a single method name. In the following we use the evaluation scheme created above to compare the five recommender algorithms: random items, popular items, user-based CF, item-based CF, and SVD approximation. Note that when running the following code, the CF based algorithms are very slow.\nFor the evaluation we use a \"all-but-5\" scheme. This is indicated by a negative number for given.\nR> set.seed(2016) R> scheme <-evaluationScheme(Jester5k[1:1000], method=\"split\", train = .9, + k=1, given=-5, goodRating=5) R> scheme Evaluation scheme using all-but-5 items Method: ???split??? with 1 run(s). Training set proportion: 0.900 Good ratings: >=5.000000 Data set: 1000 x 100 rating matrix of class ???realRatingMatrix??? with 74164 ratings.\nR> algorithms <-list( + \"random items\" = list(name=\"RANDOM\", param=NULL), + \"popular items\" = list(name=\"POPULAR\", param=NULL), + \"user-based CF\" = list(name=\"UBCF\", param=list(nn=50)), + \"item-based CF\" = list(name=\"IBCF\", param=list(k=50)), + \"SVD approximation\" = list(name=\"SVD\", param=list(k = 50)) + ) R> ## run algorithms R> results <-evaluate(scheme, algorithms, type = \"topNList\", + n=c (1,3,5,10,15,20)) The result is an object of class evaluationResultList for the five recommender algorithms.\nRANDOM", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "R> results", "text": "List of evaluation results for 5 recommenders:\n$ random items Evaluation results for 1 folds/samples using method ???RANDOM???.\n$ popular items Evaluation results for 1 folds/samples using method ???POPULAR???.\n$ user-based CF Evaluation results for 1 folds/samples using method ???UBCF???.\n$ item-based CF Evaluation results for 1 folds/samples using method ???IBCF???.\n$ SVD approximation Evaluation results for 1 folds/samples using method ???SVD???.\nIndividual results can be accessed by list subsetting using an index or the name specified when calling evaluate().", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "R> names(results)", "text": "[1] \"random items\" \"popular items\" \"user-based CF\" [4] \"item-based CF\" \"SVD approximation\"", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "R> results[[\"user-based CF\"]]", "text": "Evaluation results for 1 folds/samples using method ???UBCF???.\nAgain plot() can be used to create ROC and precision-recall plots (see Figures 10 and11). Plot accepts most of the usual graphical parameters like pch, type, lty, etc. In addition annotate can be used to annotate the points on selected curves with the list length.\nR> plot(results, annotate=c(1,3), legend=\"bottomright\") R> plot(results, \"prec/rec\", annotate=3, legend=\"topleft\")\nFor this data set and the given evaluation scheme popular items and the user-based CF methods clearly outperform the other methods. In Figure 10 we see that they dominate (almost completely) the other method since for each length of top-N list they provide a better combination of TPR and FPR.", "publication_ref": [], "figure_ref": ["fig_9", "fig_10", "fig_9"], "table_ref": []}, {"heading": "Comparing ratings", "text": "Next, we evaluate not top-N recommendations, but how well the algorithms can predict ratings.\nR> ## run algorithms R> results <-evaluate(scheme, algorithms, type = \"ratings\")   The result is again an object of class evaluationResultList for the five recommender algorithms.\nRANDOM", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "R> results", "text": "List of evaluation results for 5 recommenders:\n$ random items Evaluation results for 1 folds/samples using method ???RANDOM???.\n$ popular items Evaluation results for 1 folds/samples using method ???POPULAR???.\n$ user-based CF Evaluation results for 1 folds/samples using method ???UBCF???.\n$ item-based CF Evaluation results for 1 folds/samples using method ???IBCF???.\n$ SVD approximation Evaluation results for 1 folds/samples using method ???SVD???.\nR> plot(results, ylim = c(0,100))\nPlotting the results shows a barplot with the root mean square error, the mean square error and the mean absolute error (see Figures 12).", "publication_ref": [], "figure_ref": ["fig_11"], "table_ref": []}, {"heading": "Using a 0-1 data set", "text": "For comparison we will check how the algorithms compare given less information. We convert the data set into 0-1 data and instead of a all-but-5 we use the given-3 scheme.\nR> Jester_binary <-binarize(Jester5k, minRating=5) R> Jester_binary <-Jester_binary[rowCounts(Jester_binary)>20] R> Jester_binary 1840 x 100 rating matrix of class ???binaryRatingMatrix??? with 67728 ratings.\nR> scheme_binary <-evaluationScheme(Jester_binary[1:1000], + method=\"split\", train=.9, k=1, given=3) R> scheme_binary Evaluation scheme with 3 items given Method: ???split??? with 1 run(s). Training set proportion: 0.900 Good ratings: NA Data set: 1000 x 100 rating matrix of class ???binaryRatingMatrix??? with 36619 ratings. R> results_binary <-evaluate(scheme_binary, algorithms, + type = \"topNList\", n=c (1,3,5,10,15,20)) Note that SVD does not implement a method for binary data and is thus skipped.\nRANDOM\nR> plot(results_binary, annotate=c(1,3), legend=\"topright\")\nFrom Figure 13 we see that given less information, the performance of user-based CF suffers the most and the simple popularity based recommender performs almost a well as item-based CF.\nSimilar to the examples presented here, it is easy to compare different recommender algorithms for different data sets or to compare different algorithm settings (e.g., the influence of neighborhood formation using different distance measures or different neighborhood sizes).", "publication_ref": [], "figure_ref": ["fig_12"], "table_ref": []}, {"heading": "Implementing a new recommender algorithm", "text": "Adding a new recommender algorithm to recommenderlab is straight forward since it uses a registry mechanism to manage the algorithms. To implement the actual recommender algorithm we need to implement a creator function which takes a training data set, trains a model and provides a predict function which uses the model to create recommendations for new data. The model and the predict function are both encapsulated in an object of class Recommender.\nFor examples look at the files starting with RECOM in the packages R directory. A good examples is in RECOM_POPULAR.R.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "In this paper we described the R extension package recommenderlab which is especially geared towards developing and testing recommender algorithms. The package allows to create evaluation schemes following accepted methods and then use them to evaluate and compare recommender algorithms. recommenderlab currently includes several standard algorithms and adding new recommender algorithms to the package is facilitated by the built in registry mechanism to manage algorithms. In the future we will add more and more of these algorithms to the package and we hope that some algorithms will also be contributed by other researchers. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "filtering Java http://mahout.apache.org/ Crab Components to create recommender systems Python https://github.com/muricoca/crab LensKit Collaborative filtering algorithms from GroupLens Research Python http://lenskit.grouplens.org/ MyMediaLite factorization C++ https://www.jmlr.org/papers/v13/chen12a.html Vogoo PHP LIB //sourceforge.net/projects://github.com/mhahsler/recommenderlab", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Affiliation:", "text": "Michael Hahsler Computer Science Lyle School of Engineering Southern Methodist University P.O. Box 750122 Dallas, TX 75275-0122 E-mail: mhahsler@lyle.smu.edu URL: michael@hahsler.net", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Fast Algorithms for Mining Association Rules in Large Databases", "journal": "VLDB", "year": "1994", "authors": "R Agrawal; R Srikant"}, {"ref_id": "b1", "title": "Towards reproducibility in recommender-systems research User Model", "journal": "User Modeling and User-Adapted Interaction", "year": "2016", "authors": "J Beel; C Breitinger; S Langer; A Lommatzsch; B Gipp"}, {"ref_id": "b2", "title": "Empirical Analysis of Predictive Algorithms for Collaborative Filtering", "journal": "", "year": "1998", "authors": "J S Breese; D Heckerman; C Kadie"}, {"ref_id": "b3", "title": "Optimizing multi-channel health information delivery for behavioral change", "journal": "", "year": "2016", "authors": "M Buhl; J Famulare; C Glazier; J Harris; A Mcdowell; G Waldrip; L Barnes; M Gerber"}, {"ref_id": "b4", "title": "Hybrid recommender systems: A systematic literature review", "journal": "Intell. Data Anal", "year": "2017", "authors": "E \u00c7ano; M Morisio"}, {"ref_id": "b5", "title": "Hybrid recommendation system for tourism", "journal": "IEEE Computer Society", "year": "2013", "authors": "J Chen; K Chao; N Shah"}, {"ref_id": "b6", "title": "Enhancing Product Recommender Systems on Sparse Binary Data", "journal": "Data Minining and Knowledge Discovery", "year": "2004", "authors": "A Demiriz"}, {"ref_id": "b7", "title": "Item-based top-N recommendation algorithms", "journal": "ACM Transations on Information Systems", "year": "2004", "authors": "M Deshpande; G Karypis"}, {"ref_id": "b8", "title": "A Comprehensive Survey of Neighborhood-based Recommendation Methods", "journal": "Springer US", "year": "2011", "authors": "C Desrosiers; G Karypis"}, {"ref_id": "b9", "title": "UML Distilled: A Brief Guide to the Standard Object Modeling Language", "journal": "Addison-Wesley Professional", "year": "2004", "authors": "M Fowler"}, {"ref_id": "b10", "title": "Mining navigation history for recommendation", "journal": "ACM", "year": "2000", "authors": "X Fu; J Budzik; K J Hammond"}, {"ref_id": "b11", "title": "A Customer Purchase Incidence Model Applied to Recommender Systems", "journal": "Springer-Verlag", "year": "2001", "authors": "A Geyer-Schulz; M Hahsler; M Jahn"}, {"ref_id": "b12", "title": "Using collaborative filtering to weave an information tapestry", "journal": "Communications of the ACM", "year": "1992", "authors": "D Goldberg; D Nichols; B M Oki; D Terry"}, {"ref_id": "b13", "title": "Eigentaste: A Constant Time Collaborative Filtering Algorithm", "journal": "Information Retrieval", "year": "2001", "authors": "K Goldberg; T Roeder; D Gupta; C Perkins"}, {"ref_id": "b14", "title": "Building a Recommendation System with R", "journal": "Packt Publishing", "year": "2015", "authors": "S K Gorakala; M Usuelli"}, {"ref_id": "b15", "title": "A Survey of Accuracy Evaluation Metrics of Recommendation Tasks", "journal": "Journal of Machine Learning Research", "year": "2009", "authors": "A Gunawardana; G Shani"}, {"ref_id": "b16", "title": "Mining frequent patterns without candidate generation", "journal": "Data Mining and Knowledge Discovery", "year": "2004", "authors": "J Han; J Pei; Y Yin; R Mao"}, {"ref_id": "b17", "title": "Evaluating collaborative filtering recommender systems", "journal": "ACM Transactions on Information Systems", "year": "2004", "authors": "J L Herlocker; J A Konstan; L G Terveen; J T Riedl"}, {"ref_id": "b18", "title": "Collaborative Filtering for Implicit Feedback Datasets", "journal": "IEEE Computer Society", "year": "2008", "authors": "Y Hu; Y Koren; C Volinsky"}, {"ref_id": "b19", "title": "A List of Recommender Systems and Resources", "journal": "", "year": "2019", "authors": "G Jenson"}, {"ref_id": "b20", "title": "Cross-sell: a fast promotion-tunable customer-item recommendation method based on conditionally independent probabilities", "journal": "ACM", "year": "2000", "authors": "B Kitts; D Freed; M Vrieze"}, {"ref_id": "b21", "title": "A study of cross-validation and bootstrap for accuracy estimation and model selection", "journal": "", "year": "1995", "authors": "R Kohavi"}, {"ref_id": "b22", "title": "Glossary of Terms", "journal": "Machine Learning", "year": "1998", "authors": "R Kohavi; F Provost"}, {"ref_id": "b23", "title": "Matrix Factorization Techniques for Recommender Systems", "journal": "Computer", "year": "2009", "authors": "Y Koren; R Bell; C Volinsky"}, {"ref_id": "b24", "title": "Classification-based collaborative filtering using market basket data", "journal": "Expert Systems with Applications", "year": "2005", "authors": "J S Lee; C H Jun; J Lee; S Kim"}, {"ref_id": "b25", "title": "Slope One Predictors for Online Rating-Based Collaborative Filtering", "journal": "", "year": "2005", "authors": "D Lemire; A Maclachlan"}, {"ref_id": "b26", "title": "Efficient Adaptive-Support Association Rule Mining for Recommender Systems", "journal": "Data Mining and Knowledge Discovery", "year": "2002", "authors": "W Lin; S A Alvarez; C Ruiz"}, {"ref_id": "b27", "title": "Amazon.com Recommendations: Item-to-Item Collaborative Filtering", "journal": "IEEE Internet Computing", "year": "2003", "authors": "G Linden; B Smith; J York"}, {"ref_id": "b28", "title": "What and who with: A social approach to double-sided recommendation", "journal": "International Journal of Human-Computer Studies", "year": "2017", "authors": "I Lombardi; F Vernero"}, {"ref_id": "b29", "title": "Intelligent informationsharing systems", "journal": "Communications of the ACM", "year": "1987", "authors": "T W Malone; K R Grant; F A Turbak; S A Brobst; M D Cohen"}, {"ref_id": "b30", "title": "An improved collaborative filtering approach for predicting cross-category purchases based on binary market basket data", "journal": "Journal of Retailing and Consumer Services", "year": "2003", "authors": "A Mild; T Reutterer"}, {"ref_id": "b31", "title": "Effective Personalization Based on Association Rule Discovery from Web Usage Data", "journal": "", "year": "2001", "authors": "B Mobasher; H Dai; T Luo; M Nakagawa"}, {"ref_id": "b32", "title": "One-Class Collaborative Filtering", "journal": "IEEE Computer Society", "year": "2008", "authors": "R Pan; Y Zhou; B Cao; N N Liu; R Lukose; M Scholz; Q Yang"}, {"ref_id": "b33", "title": "R: A Language and Environment for Statistical Computing", "journal": "", "year": "2018", "authors": "Team Core"}, {"ref_id": "b34", "title": "GroupLens: an open architecture for collaborative filtering of netnews", "journal": "ACM", "year": "1994", "authors": "P Resnick; N Iacovou; M Suchak; P Bergstrom; J Riedl"}, {"ref_id": "b35", "title": "Introduction to Modern Information Retrieval", "journal": "McGraw-Hill", "year": "1983", "authors": "G Salton; M Mcgill"}, {"ref_id": "b36", "title": "Analysis of recommendation algorithms for e-commerce", "journal": "ACM", "year": "2000", "authors": "B Sarwar; G Karypis; J Konstan; J Riedl"}, {"ref_id": "b37", "title": "Item-based collaborative filtering recommendation algorithms", "journal": "ACM", "year": "2001", "authors": "B Sarwar; G Karypis; J Konstan; J Riedl"}, {"ref_id": "b38", "title": "E-Commerce Recommendation Applications", "journal": "Data Mining and Knowledge Discovery", "year": "2001", "authors": "J B Schafer; J A Konstan; J Riedl"}, {"ref_id": "b39", "title": "Social Information Filtering: Algorithms for Automating 'Word of Mouth", "journal": "ACM Press/Addison-Wesley Publishing Co", "year": "1995", "authors": "U Shardanand; P Maes"}, {"ref_id": "b40", "title": "Information retrieval", "journal": "", "year": "1979", "authors": "C Van Rijsbergen"}, {"ref_id": "b41", "title": "Scalable Algorithms for Association Mining", "journal": "IEEE Transactions on Knowledge and Data Engineering", "year": "2000", "authors": "M J Zaki"}, {"ref_id": "b42", "title": "Deep learning based recommender system: A survey and new perspectives", "journal": "ACM Computing Surveys (CSUR)", "year": "2019", "authors": "S Zhang; L Yao; A Sun; Y Tay"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "call this type of data in the context of collaborative filtering analogous to similar situations for classifiers one-class data since only the 1-class is pure and contains only positive examples. The 0-class is a mixture of positive and negative examples.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure3: UML class diagram for package recommenderlab(Fowler 2004).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "NA NA NA 1 NA NA NA NA u4 2 2 1 NA NA 5 NA 0 2 NA u5 5 NA NA NA NA NA NA 5 NA 4", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 4 :4Figure 4: Image plot the artificial rating data before and after normalization.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "FinallyFigure 5 :5Figure 5: Raw rating distribution for as sample of Jester.", "figure_data": ""}, {"figure_label": "67", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 6 :Figure 7 :67Figure 6: Histogram of normalized ratings using row centering (left) and Z-score normalization (right).", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 9 :9Figure 9: Precision-recall plot for method POPULAR.", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 10 :10Figure 10: Comparison of ROC curves for several recommender methods for the given-3 evaluation scheme.", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 11 :11Figure 11: Comparison of precision-recall curves for several recommender methods for the given-3 evaluation scheme.", "figure_data": ""}, {"figure_label": "12", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure 12 :12Figure 12: Comparison of RMSE, MSE, and MAE for recommender methods for the given-3 evaluation scheme.", "figure_data": ""}, {"figure_label": "13", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "Figure 13 :13Figure 13: Comparison of ROC curves for several recommender methods for the given-3 evaluation scheme.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ": 2x2 confusion matrixactual / predicted negative positivenegativeabpositivecd"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "u1 FALSE FALSE FALSE TRUE FALSE TRUE FALSE TRUE FALSE FALSE u2 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE u3 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE u4 FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE u5 TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "For the first run we have 6 confusion matrices represented by rows, one for each of the six different top-N lists we used for evaluation. n is the number of recommendations per list. TP, FP, FN and TN are the entries for true positives, false positives, false negatives and true negatives in the confusion matrix. The remaining columns contain precomputed performance measures. The average for all runs can be obtained from the evaluation results directly using avg().", "figure_data": "200.4150.310TPR0.250.1310.050.100.15FPRFigure 8: ROC curve for recommender method POPULAR.R> avg(results)TPFPFNTN N precision recallTPRFPR nTP [1,] 0.453 0.547 18.12 77.88 97 FP FN TN N precision recall 0.4530 0.03428 0.03428 0.006685 1 TPR FPR n [1,] 0.464 0.536 19.25 76.75 97 [2,] 1.299 1.701 17.28 76.72 97 0.4330 0.09435 0.09435 0.020704 3 0.4640 0.03458 0.03458 0.00678 1 [2,] 1.372 1.628 18.34 75.66 97 [3,] 2.119 2.881 16.46 75.54 97 0.4238 0.14437 0.14437 0.034955 5 0.4573 0.08912 0.08912 0.01988 3 [3,] 2.228 2.772 17.49 74.51 97 [4,] 4.127 5.873 14.45 72.55 97 0.4127 0.27452 0.27452 0.071829 10 0.4456 0.13867 0.13867 0.03379 5 [4,] 4.352 5.648 15.36 71.64 97 [5,] 5.922 9.078 12.65 69.35 97 0.3948 0.38732 0.38732 0.111440 15 0.4352 0.28256 0.28256 0.06890 10 [5,] 6.272 8.728 13.44 68.56 97 0.4181 0.40248 0.40248 0.10828 15 [6,] 7.347 12.653 11.23 65.77 97 0.3673 0.46375 0.46375 0.155834 20[6,] 7.600 12.400 12.12 64.88 970.3800 0.45762 0.45762 0.15522 20"}], "formulas": [{"formula_id": "formula_0", "formula_text": "sim Pearson ( x, y) = i\u2208I ( x i \u00af x)( y i \u00af y) (|I| -1) sd( x) sd( y) (1) and sim Cosine ( x, y) = x \u2022 y x y ,(2)", "formula_coordinates": [5.0, 81.0, 304.22, 441.0, 81.23]}, {"formula_id": "formula_1", "formula_text": "raj = 1 |N (a)| i\u2208N (a) r ij (3)", "formula_coordinates": [5.0, 249.41, 512.75, 272.59, 29.32]}, {"formula_id": "formula_2", "formula_text": "i 1 i 2 i 3 i 4 i 5 i 6 i 7 i 8 u 1 u 2 u 3 u 4 u 5 u 6 u a r\u0302a (a) (b) 0.3 1.0 0.2 0.3 0.1 0.1 u a u 1 u 2 u 3 u 4 u 5 u 6 0.3 1.0 0.2 0.3 0.1 0.1 u a u 1 u 2 u 3 u 4 u 5 u 6 (c)", "formula_coordinates": [6.0, 127.11, 123.41, 274.32, 145.51]}, {"formula_id": "formula_3", "formula_text": "s ai r ij (4)", "formula_coordinates": [6.0, 346.74, 397.99, 175.26, 10.7]}, {"formula_id": "formula_4", "formula_text": "h(r ui ) = r ui -ru ,", "formula_coordinates": [6.0, 260.29, 547.94, 82.41, 10.7]}, {"formula_id": "formula_5", "formula_text": "r jk = 1 user u j is known to have a preference for item i k 0 otherwise.(6)", "formula_coordinates": [8.0, 158.08, 504.14, 363.92, 25.89]}, {"formula_id": "formula_6", "formula_text": "sim Jaccard (X , Y) = |X \u2229 Y| |X \u222a Y| , (7", "formula_coordinates": [8.0, 235.56, 723.52, 281.8, 24.43]}, {"formula_id": "formula_7", "formula_text": ")", "formula_coordinates": [8.0, 517.35, 730.84, 4.65, 9.63]}, {"formula_id": "formula_8", "formula_text": "support(X \u2192 Y) = support(X \u222a Y) = Freq(X \u222a Y)/|D| = P (E X \u2229 E Y ) confidence(X \u2192 Y) = support(X \u222a Y)/support(X ) = P (E Y |E X )", "formula_coordinates": [9.0, 132.58, 348.24, 337.85, 38.41]}, {"formula_id": "formula_9", "formula_text": "MAE = 1 |K| (i,j)\u2208K |r ij -rij |,(8)", "formula_coordinates": [10.0, 233.83, 717.8, 288.17, 29.32]}, {"formula_id": "formula_10", "formula_text": "RMSE = (i,j)\u2208K (r ij -rij ) 2 |K| (9)", "formula_coordinates": [11.0, 227.4, 262.97, 294.6, 27.11]}, {"formula_id": "formula_11", "formula_text": "= a + d a + b + c + d (10)", "formula_coordinates": [11.0, 369.65, 577.91, 152.35, 24.5]}, {"formula_id": "formula_12", "formula_text": "MAE = 1 N N i=1 | i | = b + c a + b + c + d , (11", "formula_coordinates": [11.0, 218.55, 641.16, 298.6, 31.85]}, {"formula_id": "formula_13", "formula_text": ")", "formula_coordinates": [11.0, 517.15, 651.78, 4.85, 9.63]}, {"formula_id": "formula_14", "formula_text": "Precision = correctly recommended items total recommended items = d b + d (12) Recall = correctly recommended items total useful recommendations = d c + d (13)", "formula_coordinates": [12.0, 181.77, 193.48, 340.23, 66.92]}, {"formula_id": "formula_15", "formula_text": "E-measure = 1 \u03b1(1/Precision) + (1 -\u03b1)(1/Recall)(14)", "formula_coordinates": [12.0, 184.54, 363.48, 337.46, 24.5]}, {"formula_id": "formula_16", "formula_text": "F-measure = 2 Precision Recall Precision + Recall = 2 1/Precision + 1/Recall (15)", "formula_coordinates": [12.0, 161.93, 437.64, 360.07, 24.5]}, {"formula_id": "formula_17", "formula_text": "$ 0 i2 i3 i4 i6 i8 2 3 5 5 4 $ 1 i1 i9 i10 2 2 3 $ 2 i1 i6 2 1 $ 3 i1 i2 i3 i6 i8 i9 2.000e+00 2.", "formula_coordinates": [16.0, 81.0, 111.64, 372.27, 200.6]}, {"formula_id": "formula_18", "formula_text": "5", "formula_coordinates": [17.0, 81.0, 393.61, 5.73, 10.91]}, {"formula_id": "formula_19", "formula_text": "i1 i2 i3 i4 i5 i6 i7 i8 i9 i10", "formula_coordinates": [18.0, 115.36, 207.96, 320.73, 10.91]}, {"formula_id": "formula_20", "formula_text": "$", "formula_coordinates": [21.0, 81.0, 572.31, 5.73, 10.91]}, {"formula_id": "formula_21", "formula_text": "RMSE MSE MAE UBCF", "formula_coordinates": [26.0, 81.0, 615.94, 126.0, 24.46]}, {"formula_id": "formula_22", "formula_text": "POPULAR", "formula_coordinates": [27.0, 81.0, 320.3, 40.09, 10.91]}, {"formula_id": "formula_23", "formula_text": "RANDOM", "formula_coordinates": [30.0, 81.0, 384.02, 34.36, 10.91]}, {"formula_id": "formula_24", "formula_text": "RANDOM", "formula_coordinates": [31.0, 81.0, 612.96, 34.36, 10.91]}, {"formula_id": "formula_25", "formula_text": "RANDOM", "formula_coordinates": [36.0, 81.0, 155.75, 34.36, 10.91]}], "doi": "10.1145/138859.138867"}
