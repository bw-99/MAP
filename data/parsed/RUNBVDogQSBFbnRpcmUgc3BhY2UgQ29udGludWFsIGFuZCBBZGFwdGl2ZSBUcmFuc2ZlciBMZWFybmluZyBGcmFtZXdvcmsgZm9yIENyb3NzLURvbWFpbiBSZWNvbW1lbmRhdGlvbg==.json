{"ECAT: A Entire space Continual and Adaptive Transfer Learning Framework for Cross-Domain Recommendation": "Chaoqun Hou \u2217\u2020 hcq.hcq@taobao.com Alibaba Group Hangzhou, China Yuanhang Zhou \u2020 zhouyuanhang.zyh@taobao.com Alibaba Group Hangzhou, China Yi Cao dylan.cy@taobao.com Alibaba Group Hangzhou, China", "ABSTRACT": "In industrial recommendation systems, there are several mini-apps designed to meet the diverse interests and needs of users. The sample space of them is merely a small subset of the entire space, making it challenging to train an efficient model. In recent years, there have been many excellent studies related to cross-domain recommendation aimed at mitigating the problem of data sparsity. However, few of them have simultaneously considered the adaptability of both sample and representation continual transfer setting to the target task. To overcome the above issue, we propose a E ntire space C ontinual and A daptive T ransfer learning framework called ECAT which includes two core components: First, as for sample transfer, we propose a two-stage method that realizes a coarse-tofine process. Specifically, we perform an initial selection through a graph-guided method, followed by a fine-grained selection using domain adaptation method. Second, we propose an adaptive knowledge distillation method for continually transferring the representations from a model that is well-trained on the entire space dataset. ECAT enables full utilization of the entire space samples and representations under the supervision of the target task, while avoiding negative migration. Comprehensive experiments on realworld industrial datasets from Taobao show that ECAT advances state-of-the-art performance on offline metrics, and brings +13.6% CVR and +8.6% orders for Baiyibutie, a famous mini-app of Taobao.", "CCS CONCEPTS": "\u00b7 Information systems \u2192 Retrieval models and ranking .", "KEYWORDS": "cross domain, continual transfer learning, adaptive knowledge distillation, graph guided \u2217 Corresponding author. \u2020 Both authors contributed equally to this research. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '24, July 14-18, 2024, Washington, DC, USA. \u00a9 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0431-4/24/07 https://doi.org/10.1145/3626772.3661348 Tong Liu yingmu@taobao.com Alibaba Group Hangzhou, China", "ACMReference Format:": "Chaoqun Hou, Yuanhang Zhou, Yi Cao, and Tong Liu. 2024. ECAT: A Entire space Continual and Adaptive Transfer Learning Framework for CrossDomain Recommendation. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '24), July 14-18, 2024, Washington, DC, USA. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3626772.3661348", "1 INTRODUCTION": "Recommendation systems (RS) have played a significant role in e-commerce platforms, and their efficiency is closely related to the accuracy of click-through rate (CTR) prediction. In recent years, thanks to the continuous improvements in computational power and the increasing volume of datasets, numerous outstanding singledomain CTR models [3, 4, 10, 21, 29] have achieved impressive results. At large e-commercial companies, there are several miniapps designed to meet the diverse interests and needs of users. However, these mini-apps all encounter a common issue: the target domain has relatively sparse samples, making it challenging to train the complex CTR model, especially the representations of ID categorical features (i.e., item ID and user ID). Take Taobao for instance, Baiyibutie is a mini-app that contributes billions of daily page views by exclusively selling brand-discounted products. The sample size of Baiyibutie is less than 1% of the entire Taobao domain. Therefore, exploring how cross-domain transfer learning can utilize the abundant information available in data-rich domains to enhance the data-sparse domains has emerged as an important research focus in the industry. The traditional cross-domain [1, 9, 14, 16, 19, 25, 28, 31] recommendation can be categorized into two paradigms: sample transfer and parameter transfer from the well-trained source model. In the sample transfer paradigm , multi-task learning methods [11, 18, 20, 23, 24, 27, 30, 32] are typically employed to enhance performance across all domains by combining the source and the target samples. However, despite the fact that this paradigm has achieved commendable results in many scenarios, it still has some evident limitations in certain situations. For instance, in scenarios where the sample size of source domain is hundreds of times larger than that of the target domain, the training process can be easily dominated by the source domain, resulting in insufficient training in the target domain. Another issue is that introducing the source domain samples of such a large scale could significantly increase complexity. Therefore, the core objective should be to enhance the SIGIR '24, July 14-18, 2024, Washington, DC, USA. Chaoqun Hou, Yuanhang Zhou, Yi Cao, & Tong Liu performance of the target task by selecting samples that are deemed valuable. In the parameter transfer paradigm , pre-training & finetuning methods [2, 12, 13] are more efficient and effective. Specifically, the initialization parameters of the target model are obtained by loading a pre-trained source model, followed by fine-tuning with samples from the target domain. However, an evident issue is that merely fine-tuning with sparse samples from the target domain can easily lead the target model to settle into a sub-optimal local minimum. Therefore, it's crucial to measure the value of the source model's parameters for the target task. Another issue is that few studies considering the setting of Continual Transfer Learning (CTL) [6, 17, 22, 26], resulting in an inability to continuously utilize the newest information of the source model. CTNet [17] accomplishes continuous transfer by treating the latest source domain representations through an adapter layer. However, in most real-world RSs, user behavioral sequences hold great potential in boosting the performance of the CTR model. CTNet ignores the representations of user behavior sequences in the source model. Furthermore, the target model outperforms the source model on certain samples. Therefore, we need the source model to provide valuable incremental information for these samples that the target model cannot handle well. For these samples that the target model can predict more accurately, we should minimize intervention. To better solve the above issues in cross-domain modeling, we propose a E ntire space C ontinual and A daptive T ransfer learning framework ( ECAT ). As shown in Figure 1, the ECAT framework mainly includes two parts: sample transfer and representations continual transfer. Specifically, we perform a coarse sample selection through a graph guided method, followed by a fine-grained selection using domain adaptation method. During the training process, we continuously transfer valuable information from the source model using an adaptive knowledge distillation method. During the online inference process, the only additional component introduced is the adapter layers, which have a very small complexity. To summarize, the main contributions include: \u00b7 We propose an ECAT framework, which enables full utilization of the source domain samples and representations under the supervision of the target task, while alleviating negative migration. \u00b7 We propose a two-stage method that realizes a coarse-to-fine process for sample transfer (GST & DA), which enables ECAT to efficiently select samples that are valuable for the target task. \u00b7 Wepropose an adaptive knowledge distillation method (AKD-CT) for continually transferring the representations from a source model that is well-trained on the entire space dataset, which allows the ECAT framework to adaptively decide whether to incorporate representational information from the source model. \u00b7 We evaluate ECAT on the Taobao industrial dataset. Comprehensive experiments show that ECAT advances state-of-the-art performance on offline metrics, and brings +13.6% CVR and +8.6% orders for Baiyibutie, a mini-app of Taobao.", "2 METHODS": "", "2.1 Problem Definition": "Mathematically, we represent samples from the source domain and the target domain as \ud835\udc37 \ud835\udc60 = ( \ud835\udc65 \ud835\udc60 \ud835\udc56 , \ud835\udc66 \ud835\udc60 \ud835\udc56 ) and \ud835\udc37 \ud835\udc61 = ( \ud835\udc65 \ud835\udc61 \ud835\udc56 , \ud835\udc66 \ud835\udc61 \ud835\udc56 ) respectively, where \ud835\udc65 \ud835\udc60 \u2208 \ud835\udc45 \ud835\udc51 \ud835\udc60 and \ud835\udc65 \ud835\udc61 \u2208 \ud835\udc45 \ud835\udc51 \ud835\udc61 . Label \ud835\udc66 \ud835\udc60 \ud835\udc56 and \ud835\udc66 \ud835\udc61 \ud835\udc56 \u2208 { 0 , 1 } indicate whether the \ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc5a \ud835\udc56 was purchased or not. It is worth mentioning that we have established the capability to acquire samples from the entire domain of Taobao. \ud835\udc46 is a continually well-trained model on \ud835\udc37 \ud835\udc60 , capable of learning new distributions in a timely manner. In this study, our goal is to train a model \ud835\udc47 using \ud835\udc37 \ud835\udc61 while considering the incremental information in areas including sample transfer from \ud835\udc37 \ud835\udc60 and representation transfer from \ud835\udc46 . Furthermore, we represent \ud835\udc37 \ud835\udc60 through a graph \ud835\udc3a \ud835\udc60 = ( \ud835\udc49 \ud835\udc60 , \ud835\udc38 \ud835\udc60 ) , where \ud835\udc49 \ud835\udc60 = { \ud835\udc62 \ud835\udc60 1 , \ud835\udc56 \ud835\udc60 1 , ..., \ud835\udc62 \ud835\udc60 \ud835\udc5b , \ud835\udc56 \ud835\udc60 \ud835\udc5b } denotes the user and item node in the graph of source domain. Edge \ud835\udc52 \ud835\udc56 \ud835\udc57 \u2208 \ud835\udc38 \ud835\udc60 denotes that \ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc5f \ud835\udc56 has clicked or purchased on \ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc5a \ud835\udc57 . In other words, we can identify the corresponding samples \ud835\udc37 \ud835\udc60 through the nodes and edges of \ud835\udc3a \ud835\udc60 . Similarly, we define \ud835\udc3a \ud835\udc61 = ( \ud835\udc49 \ud835\udc61 , \ud835\udc38 \ud835\udc61 ) according to \ud835\udc37 \ud835\udc61 .", "2.2 Model Overview": "We have decomposed the ECAT framework process into two serial stages. Initially, figure 1(a) shows a simple yet effective method called G raph-guided based S ample T ransfer (GST), which aims to select samples from \ud835\udc37 \ud835\udc60 with a similar distribution to \ud835\udc37 \ud835\udc61 . The GST can incorporate sample relevance by leveraging prior heuristic insights or measure it through representational learning via graph neural networks. In this paper, we focus on the area of e-commerce recommendation, there inherently exists plenty of valuable prior knowledge. For example, a direct browse, click or purchase of an item by a user acts as a one-hop link, while a two-hop link can be established between two items through a co-click relationship by users. Moreover, GST is versatile and capable of employing suitable strategies based on the specific domain, or even training a graph representation network model. Subsequently, from left to right in figure 1(b), the diagram sequentially illustrates the Domain Adaption (DA) module for assessment of incremental value that samples from \ud835\udc37 \ud835\udc54\ud835\udc60\ud835\udc61 contribute to \ud835\udc47 , and the Adaptive Knowledge Distillation (AKD-CT) module is designed to assess the incremental value that representations from the well-trained source model \ud835\udc46 . More detailed exposition will be delineated in the subsequent discourse.", "2.3 Graph guided and Domain Adaptation based Sample Transfer": "Graph guided Module : Incorporating the findings of many related studies [5, 27], the evidence suggests that a model trained using samples from the combined source and target domains, typically results in insufficient training to the target domain. This phenomenon is particularly pronounced in the context of this study, where the sample size of \ud835\udc37 \ud835\udc60 is hundreds of times greater than \ud835\udc37 \ud835\udc61 . Considering that, in our scenario, the target domain is a sub-channel of the source domain ( \ud835\udc49 \ud835\udc61 \u2282 \ud835\udc49 \ud835\udc60 ), the heuristic GST approach is markedly effective and appropriate for our domain-specific challenges. Specifically, as shown in figure 1(a), we initiate the process by directly mapping \ud835\udc49 \ud835\udc60\ud835\udc52\ud835\udc52\ud835\udc51 = \ud835\udc49 \ud835\udc61 \u2229 \ud835\udc49 \ud835\udc60 onto \ud835\udc3a \ud835\udc60 , anchoring the alignment through the correspondence of the same IDs between the domains. Subsequently, within \ud835\udc3a \ud835\udc60 = ( \ud835\udc49 \ud835\udc60 , \ud835\udc38 \ud835\udc60 ) , we expand to include more nodes \ud835\udc49 \ud835\udc54\ud835\udc52\ud835\udc5b\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc59\ud835\udc56\ud835\udc67\ud835\udc52\ud835\udc51 , which are similar to the target domain, by exploiting one-hop (i.e., click or pay relationships) and two-hop (i.e., co-click or group cluster) connectivity. Finally, within the context of e-commerce recommendation systems, the relevant sample can be identified by specifying a distinct \ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc5f \ud835\udc56 and \ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc5a \ud835\udc57 . In other words, we can convert \ud835\udc3a \u2032 \ud835\udc60 = ( \ud835\udc49 \ud835\udc60\ud835\udc52\ud835\udc52\ud835\udc51 \u222a \ud835\udc49 \ud835\udc54\ud835\udc52\ud835\udc5b\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc59\ud835\udc56\ud835\udc67\ud835\udc52\ud835\udc51 , \ud835\udc38 \ud835\udc60 ) to \ud835\udc37 \ud835\udc54\ud835\udc60\ud835\udc61 . ECAT: A Entire space Continual and Adaptive Transfer Learning Framework for Cross-Domain Recommendation SIGIR '24, July 14-18, 2024, Washington, DC, USA. User Item G(Vt, Et) Target domain (Information-poor) Inter-channel User-Item Item-Item Gs(Vs, Es) Vseed Source domain (Information-rich) Vgeneralized 1-hop: user-item direct interactive 2-hop: user-item(click, pay, cart), 1-hop item-item(co-click, similarity), 2-hop uscr-uscr(cluster group) Vfinal Vseed U Vgeneralized Gfinal (Vfinal, Es) Dgst Graph-guided based Sample Transfer module User- Uscr (b) Domain Adaptation based Sample Transfer(left) and Adaptive Knowledge Distillation based Continual Transfer(right) Domain Loss CVR Loss Distill Loss Entropy Weight Target Domain Source Domain Module (untrainable) (trainable) Module (trainable) Logit Layers Logit Laycrs Logit (esequence' esequence) Classifier(MLP) Layers Classifier(MLP) Layers Classifier(MLP) Layers sequence esequence Fusion Domain Discriminate context e{tem esequence Adapter Feature Encoder Layer Ldi Sequence Layers Adaptive Knowledge Sequence Layers Distillation Module domain-independent X Lda Embedding layer Embedding layer stop gradient Xi \u20ac Dt U Dgst Layers efser Laycr Figure 1: The illustration of ECAT (Entire space Continual and Adaptive Transfer) framework. ECAT is composed of three parts: First, the Graph-guided module (a) and the Domain Adaption (b-left) are aimed to transfer samples. Second, the target model is trained daily. Third, the Adaptive Knowledge Distillation (b-right) module is for transferring representations continually. Target Domain Module : The structure of the target model \ud835\udc47 is similar to ETA [3], which includes four parts. First, the embedding layer maps features to representations of a specific dimension, primarily including categorical features and numerical features. It is worth mentioning that the categorical features are extremely important and require a substantial number of samples for effective training. It is the significant reason why we introduce the welltrained model \ud835\udc46 across the entire space. Subsequently, long and short-term user behavioral sequences are mapped into higher semantic representations through the sequence layers. Finally, we can get the score after successively passing through the classification and logit layers. \ud835\udc3f \ud835\udc66 is usually a binary cross-entropy loss function.  where \ud835\udc5b \ud835\udc61 and \ud835\udc5b \ud835\udc54\ud835\udc60\ud835\udc61 are the sample size of \ud835\udc37 \ud835\udc61 and \ud835\udc37 \ud835\udc54\ud835\udc60\ud835\udc61 respectively. \ud835\udc3a \ud835\udc61 denotes the output of samples pass sequentially from the representation to the logit layers of \ud835\udc47 . \u03a6 \ud835\udc61 is designed to map samples from different feature dimensions to the same feature space, such as attention maps [15]. Domain Adaptation Module : To select samples from \ud835\udc37 \ud835\udc54\ud835\udc60\ud835\udc61 that better fit the distribution of \ud835\udc37 \ud835\udc61 , as shown in figure 1(b), we refine the sample selection by incorporating a Domain Adaption module. The training dataset is \ud835\udc37 \ud835\udc51\ud835\udc4e = \ud835\udc37 \ud835\udc61 \u222a \ud835\udc37 \ud835\udc54\ud835\udc60\ud835\udc61 = ( \ud835\udc65 \ud835\udc51\ud835\udc4e \ud835\udc56 , \ud835\udc66 \ud835\udc51\ud835\udc4e \ud835\udc56 ) and \u03a6 ( \ud835\udc65 \ud835\udc51\ud835\udc4e \ud835\udc56 ) denotes domain-independent features. Label \ud835\udc66 \ud835\udc51\ud835\udc4e \ud835\udc56 \u2208 { 0 , 1 } indicates whether the sample \ud835\udc65 \ud835\udc51\ud835\udc4e \ud835\udc56 belongs to \ud835\udc37 \ud835\udc61 or \ud835\udc37 \ud835\udc54\ud835\udc60\ud835\udc61 . The optimization objective \ud835\udc3f \ud835\udc51\ud835\udc4e is a binary cross-entropy loss function. The DA module is effective due to three key factors: First, to avoid feature bias, we ensure the effectiveness of the discriminator by solely using domain-independent features. Second, to avoid model bias towards the source domain, we select samples similar to the target domain distribution through GST. Third, to prevent the target model from being influenced by irrelevant gradients, we stop the gradients produced by the DA on the target model. Up to this point in our discussion, we have been able to achieve satisfactory results in sample transfer. However, as time progresses, target model \ud835\udc47 will gradually forget the representations obtained through one-time warm up from \ud835\udc46 , while the representation of \ud835\udc46 also continues to update. We will solve this issue in the next section.", "2.4 Adaptive Knowledge Distillation based Continual representation Transfer": "Source Domain Module : To provide incremental information, we introduce a source model \ud835\udc46 that has been well-trained in the entire space. During the training process of \ud835\udc47 , \ud835\udc46 only executes forward propagation, which entails a low computational complexity. As shown in Figure 1(b-right), \ud835\udc46 and \ud835\udc47 have identical architecture. AKD-CT Module : Inspired by CTNet, ECAT endeavors to enhance the target model performance through CTL setting. ECAT differs in that it further transfers all layers from the embedding layers to the logit layers, particularly sequence layer representations. To achieve this, we propose an Adaptive Knowledge Distillation based Continual Transfer (AKD-CT) method. Figure 1(b-right) illustrates the training process of AKD-CT that showcases the representations distillation of the sequence layers. Specifically, we obtain the representations of various behavioral sequence features after passing through the embedding and sequence layers. \ud835\udc52 \ud835\udc61 \ud835\udc60\ud835\udc52\ud835\udc5e and \ud835\udc52 \ud835\udc60 \ud835\udc60\ud835\udc52\ud835\udc5e denote the sequence representations obtained from \ud835\udc47 and \ud835\udc46 , respectively. Subsequently, \ud835\udc52 \ud835\udc61 \ud835\udc60\ud835\udc52\ud835\udc5e is the input of adapter layers to obtain \ud835\udc52 \ud835\udc61 \u2032 \ud835\udc60\ud835\udc52\ud835\udc5e , which then distill knowledge from \ud835\udc46 under the supervision of \ud835\udc3f \ud835\udc51\ud835\udc56 . We use cosine similarity loss to pull \ud835\udc52 \ud835\udc61 \u2032 \ud835\udc60\ud835\udc52\ud835\udc5e and \ud835\udc52 \ud835\udc60 \ud835\udc60\ud835\udc52\ud835\udc5e more similar. To prevent noise from the distillation process, we stop conducting gradient to \ud835\udc47 . We have obtained the incremental information \ud835\udc52 \ud835\udc61 \u2032 \ud835\udc60\ud835\udc52\ud835\udc5e , all that remains is appropriately fusing \ud835\udc52 \ud835\udc61 \u2032 \ud835\udc60\ud835\udc52\ud835\udc5e into \ud835\udc47 . The Design of Adaptive Knowledge Distillation is threefold: First, considering that \ud835\udc47 may have better discrimination for certain samples than \ud835\udc46 . We introduce an adaptive gate network to assess the value of \ud835\udc52 \ud835\udc61 \u2032 \ud835\udc60\ud835\udc52\ud835\udc5e for \ud835\udc47 . Specifically, we concatenate \ud835\udc52 \ud835\udc61 \u2032 \ud835\udc60\ud835\udc52\ud835\udc5e , \ud835\udc52 \ud835\udc61 \ud835\udc60\ud835\udc52\ud835\udc5e SIGIR '24, July 14-18, 2024, Washington, DC, USA. Chaoqun Hou, Yuanhang Zhou, Yi Cao, & Tong Liu and the entropy from \ud835\udc47 as the input of the adaptive gate network to generate fusion weight. With the supervision of loss \ud835\udc3f \ud835\udc66 , the fusion weight could indicate the importance of \ud835\udc52 \ud835\udc61 \u2032 \ud835\udc60\ud835\udc52\ud835\udc5e . Second, our objective is not to distill representations from the source model that are merely similar, but rather those are more suitably adapted to \ud835\udc47 . Therefore, each sample is associated with a distillation intensity \ud835\udc64 \ud835\udc5d\ud835\udc5c\ud835\udc64 \ud835\udc56 that governs the degree to which \ud835\udc52 \ud835\udc61 \u2032 \ud835\udc60\ud835\udc52\ud835\udc5e approximates \ud835\udc52 \ud835\udc60 \ud835\udc60\ud835\udc52\ud835\udc5e . Ideally, the distillation intensity would be higher for samples that \ud835\udc47 finds hard to predict. After numerous experiments, we adopt the cos similarity to calculate \ud835\udc64 \ud835\udc5d\ud835\udc5c\ud835\udc64 \ud835\udc56 . Third, our primary task is to enhance the performance of \ud835\udc47 . Therefore, the adapter layers responsible for generating \ud835\udc52 \ud835\udc61 \u2032 \ud835\udc60\ud835\udc52\ud835\udc5e are also subject to supervision from \ud835\udc3f \ud835\udc66 . In summary, the final loss of ECAT is as follows:  where \ud835\udefc\ud835\udc4e\ud835\udc5b\ud835\udc51\ud835\udefd are hyperparameter that controls the weight of corresponding loss, w \ud835\udc51\ud835\udc4e is the entropy value from the DA module for each sample and w \ud835\udc5d\ud835\udc5c\ud835\udc64 is the distillation intensity for each sample.", "3 EXPERIMENTS": "", "3.1 Experimental Setup": "3.1.1 Dataset. In the absence of suitable public benchmarks for evaluating continual cross-domain prediction, we adopt Taobao industrial datasets to comprehensively compare ECAT and baselines. Therefore, we use the Baiyibutie from Taobao as the target domain, which generates millions CVR samples every day, accounting for less than 1% of the entire space of Taobao. The users and items in the source domain and the target domain partially overlap, but the data distribution is very different. In this study, we utilize target domain samples spanning 90 days, amounting to a total of 120 million samples. Similarly, taking the entire space as the source domain, we have accumulated a total of 66 billion samples. During A/B testing, ECAT serves over hundreds of thousands of users daily. 3.1.2 Baseline Models. We compare the samples transfer methods like simple merge \ud835\udc37 \ud835\udc60 and \ud835\udc37 \ud835\udc61 , DANN [8], the representations transfer methods including Shared Bottom, MMoE [18], PLE [24], and the continual learning setting method like CTNet [17]. 3.1.3 Implementation Details. To ensure the fairness of the experiments, all single domain methods employs the ETA [3] as the architecture, including the target model and source model. Specifically, we use AdagradDecayV2 [7] as the optimizer. Learning rate is set to 0.01 and the batch size is 1024. The dimension of MLP Layers is set to 1024, 512 and 256. Following previous work, we adopt AUC to measure the CVR prediction performance in offline evaluation.", "3.2 Offline Evaluation": "As shown in Table 1, our ECAT (GST & DA and AKD-CT) achieves the best performance among all baselines. More specifically, (1) ECAT achieves the best performance (AUC=0.8348) compared to both single-domain and cross-domain methods, with its core advantage being that ECAT simultaneously considers the adaptability of both sample and representation for the target task. (2) In terms of sample transfer: ECAT enhances the performance of each method, including CTNet (AUC from 0.8307 to 0.8327), by transferring valuable samples from \ud835\udc37 \ud835\udc60 through GST & DA. Besides, directly merging \ud835\udc37 \ud835\udc60 with \ud835\udc37 \ud835\udc61 leads to performance degradation. (3) In terms of continual representation transfer: ECAT further improves performance through the adaptive capabilities of AKD-CT module, which under the supervision of the target task, continuously transfers valuable representation information from \ud835\udc46 . Even with the same sample transfer strategy, the effectiveness of AKD-CT (AUC=0.8348) surpasses that of CTNet (AUC=0.8327). Table 1: Offline results of various methods.", "3.3 Research Questions": "3.3.1 RQ1: How to prove the adaptive capability of AKD-CT model? Table 2 shows that (1) AKD-CT drops performance without gate. The reason is that the gate network assesses the importance of incremental information for \ud835\udc47 , providing valuable incremental representation information for samples with higher uncertainty and lower confidence. (2) The absence of distillation intensity in AKD-CT results in poorer result, which suggests that an intensitybased strategy facilitates the distillation of representations more suited to \ud835\udc47 . Table 2: Ablation experiments on adaptive capability. 3.3.2 RQ2: How to prove the necessity of CTL setting? We compare the performance between AKD-CT and CTNet under continuous transfer and one-time transfer setting. Table 3 shows that continuous transfer is better than one-time transfer, which illustrates the necessity of CTL. \u0394 t is 30 days in this study. Table 3: The comparisons between different transfer setting.", "4 CONCLUSIONS": "In this paper, we introduce the ECAT framework for cross-domain prediction, which not only considers the continual transfer of both samples and representations but also the adaptability of incremental information to the target task. Experiments conducted on a largescale industrial dataset, along with online A/B testing, confirm its effectiveness in real-world applications. It is noteworthy that ECAT has been deployed in the RS of Taobao to serve numerous marketing channels, including Baiyibutie. ECAT: A Entire space Continual and Adaptive Transfer Learning Framework for Cross-Domain Recommendation SIGIR '24, July 14-18, 2024, Washington, DC, USA.", "REFERENCES": "[1] Liyue Chen, Linian Wang, Jinyu Xu, Shuai Chen, Weiqiang Wang, Wenbiao Zhao, Qiyu Li, and Leye Wang. 2023. Knowledge-inspired Subdomain Adaptation for Cross-Domain Knowledge Transfer. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management . 234-244. [2] Lei Chen, Fajie Yuan, Jiaxi Yang, Xiangnan He, Chengming Li, and Min Yang. 2021. User-specific adaptive fine-tuning for cross-domain recommendations. IEEE Transactions on Knowledge and Data Engineering (2021). [3] Qiwei Chen, Yue Xu, Changhua Pei, Shanshan Lv, Tao Zhuang, and Junfeng Ge. 2022. Efficient Long Sequential User Data Modeling for Click-Through Rate Prediction. arXiv preprint arXiv:2209.12212 (2022). [4] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems . 7-10. [5] Michael Crawshaw. 2020. Multi-task learning with deep neural networks: A survey. arXiv preprint arXiv:2009.09796 (2020). [6] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ale\u0161 Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. 2021. A continual learning survey: Defying forgetting in classification tasks. IEEE transactions on pattern analysis and machine intelligence 44, 7 (2021), 3366-3385. [7] John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. Journal of machine learning research 12, 7 (2011). [8] Yaroslav Ganin and Victor Lempitsky. 2015. Unsupervised domain adaptation by backpropagation. In International conference on machine learning . PMLR, 11801189. [9] Jingtong Gao, Xiangyu Zhao, Bo Chen, Fan Yan, Huifeng Guo, and Ruiming Tang. 2023. AutoTransfer: Instance Transfer for Cross-Domain Recommendations. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1478-1487. [10] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. arXiv preprint arXiv:1703.04247 (2017). [11] Guangneng Hu, Yu Zhang, and Qiang Yang. 2018. Conet: Collaborative cross networks for cross-domain recommendation. In Proceedings of the 27th ACM international conference on information and knowledge management . 667-676. [12] Jian Hu, Hongya Tuo, Chao Wang, Lingfeng Qiao, Haowen Zhong, and Zhongliang Jing. 2019. Multi-Weight Partial Domain Adaptation.. In BMVC . 5. [13] Jian Hu, Hongya Tuo, Chao Wang, Lingfeng Qiao, Haowen Zhong, Junchi Yan, Zhongliang Jing, and Henry Leung. 2020. Discriminative partial domain adversarial network. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXVII 16 . Springer, 632-648. [14] Zhaoxin Huan, Ang Li, Xiaolu Zhang, Xu Min, Jieyu Yang, Yong He, and Jun Zhou. 2023. SAMD: An Industrial Framework for Heterogeneous Multi-Scenario Recommendation. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 4175-4184. [15] Nikos Komodakis and Sergey Zagoruyko. 2017. Paying more attention to attention: improving the performance of convolutional neural networks via attention transfer. In ICLR . [16] Chenglin Li, Yuanzhen Xie, Chenyun Yu, Bo Hu, Zang Li, Guoqiang Shu, Xiaohu Qie, and Di Niu. 2023. One for All, All for One: Learning and Transferring User Embeddings for Cross-Domain Recommendation. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining . 366-374. [17] Lixin Liu, Yanling Wang, Tianming Wang, Dong Guan, Jiawei Wu, Jingxu Chen, Rong Xiao, Wenxiang Zhu, and Fei Fang. 2023. Continual Transfer Learning for Cross-Domain Click-Through Rate Prediction at Taobao. In Companion Proceedings of the ACM Web Conference 2023 . 346-350. [18] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. 2018. Modeling task relationships in multi-task learning with multi-gate mixture-ofexperts. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1930-1939. [19] Shanlei Mu, Penghui Wei, Wayne Xin Zhao, Shaoguo Liu, Liang Wang, and Bo Zheng. 2023. Hybrid Contrastive Constraints for Multi-Scenario Ad Ranking. arXiv preprint arXiv:2302.02636 (2023). [20] Wentao Ouyang, Xiuwu Zhang, Lei Zhao, Jinmei Luo, Yu Zhang, Heng Zou, Zhaojie Liu, and Yanlong Du. 2020. Minet: Mixed interest network for crossdomain click-through rate prediction. In Proceedings of the 29th ACM international conference on information & knowledge management . 2669-2676. [21] Qi Pi, Guorui Zhou, Yujing Zhang, Zhe Wang, Lejian Ren, Ying Fan, Xiaoqiang Zhu, and Kun Gai. 2020. Search-based user interest modeling with lifelong sequential behavior data for click-through rate prediction. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management . 2685-2692. [22] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. 2016. Progressive neural networks. arXiv preprint arXiv:1606.04671 (2016). [23] Xiang-Rong Sheng, Liqin Zhao, Guorui Zhou, Xinyao Ding, Binding Dai, Qiang Luo, Siran Yang, Jingshan Lv, Chi Zhang, Hongbo Deng, et al. 2021. One model to serve all: Star topology adaptive recommender for multi-domain ctr prediction. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management . 4104-4113. [24] Hongyan Tang, Junning Liu, Ming Zhao, and Xudong Gong. 2020. Progressive layered extraction (ple): A novel multi-task learning (mtl) model for personalized recommendations. In Proceedings of the 14th ACM Conference on Recommender Systems . 269-278. [25] Yu Tian, Bofang Li, Si Chen, Xubin Li, Hongbo Deng, Jian Xu, Bo Zheng, Qian Wang, and Chenliang Li. 2023. Multi-Scenario Ranking with Adaptive Feature Learning. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval . 517-526. [26] Hao Wang, Hao He, and Dina Katabi. 2020. Continuously indexed domain adaptation. arXiv preprint arXiv:2007.01807 (2020). [27] Yufeng Xie, Mingchu Li, Kun Lu, Syed Bilal Hussain Shah, and Xiao Zheng. 2022. Multi-task Learning Model based on Multiple Characteristics and Multiple Interests for CTR prediction. In 2022 IEEE Conference on Dependable and Secure Computing (DSC) . IEEE, 1-7. [28] Xuanhua Yang, Jianxin Zhao, Shaoguo Liu, Liang Wang, and Bo Zheng. 2023. Gradient Coordination for Quantifying and Maximizing Knowledge Transference in Multi-Task Learning. arXiv preprint arXiv:2303.05847 (2023). [29] Weinan Zhang, Jiarui Qin, Wei Guo, Ruiming Tang, and Xiuqiang He. 2021. Deep learning for click-through rate estimation. arXiv preprint arXiv:2104.10584 (2021). [30] Yu Zhang and Qiang Yang. 2021. A survey on multi-task learning. IEEE Transactions on Knowledge and Data Engineering 34, 12 (2021), 5586-5609. [31] Pengyu Zhao, Xin Gao, Chunxu Xu, and Liang Chen. 2023. M5: Multi-Modal Multi-Interest Multi-Scenario Matching for Over-the-Top Recommendation. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 5650-5659. [32] Xinyu Zou, Zhi Hu, Yiming Zhao, Xuchu Ding, Zhongyi Liu, Chenliang Li, and Aixin Sun. 2022. Automatic expert selection for multi-scenario and multi-task search. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1535-1544."}
