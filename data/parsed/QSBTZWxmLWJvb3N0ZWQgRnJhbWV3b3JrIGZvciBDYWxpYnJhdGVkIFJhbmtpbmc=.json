{"A Self-boosted Framework for Calibrated Ranking": "Shunyu Zhang Kuaishou Technology Beijing, China zhangshunyu@kuaishou.com Hu Liu \u2217 Kuaishou Technology Beijing, China hooglecrystal@126.com Wentian Bao Columbia University Beijing, China wb2328@columbia.edu Enyun Yu Northeasten University Beijing, China yuenyun@126.com Yang Song Kuaishou Technology Beijing, China yangsong@kuaishou.com", "ABSTRACT": "Scale-calibrated ranking systems are ubiquitous in real-world applications nowadays, which pursue accurate ranking quality and calibrated probabilistic predictions simultaneously. For instance, in the advertising ranking system, the predicted click-through rate (CTR) is utilized for ranking and required to be calibrated for the downstream cost-per-click ads bidding. Recently, multi-objective based methods have been wildly adopted as a standard approach for Calibrated Ranking, which incorporates the combination of two loss functions: a pointwise loss that focuses on calibrated absolute values and a ranking loss that emphasizes relative orderings. However, when applied to industrial online applications, existing multi-objective CR approaches still suffer from two crucial limitations. First, previous methods need to aggregate the full candidate list within a single mini-batch to compute the ranking loss. Such aggregation strategy violates extensive data shuffling which has long been proven beneficial for preventing overfitting, and thus degrades the training effectiveness. Second, existing multi-objective methods apply the two inherently conflicting loss functions on a single probabilistic prediction, which results in a sub-optimal trade-off between calibration and ranking. To tackle the two limitations, we propose a Self-Boosted framework for Calibrated Ranking (SBCR). In SBCR, the predicted ranking scores by the online deployed model are dumped into context features. With these additional context features, each single item can perceive the overall distribution of scores in the whole ranking list, so that the ranking loss can be constructed without the need for sample aggregation. As the deployed model is a few versions older than the training model, the dumped predictions reveal what was failed to learn and keep boosting the model to correct previously mis-predicted items. Moreover, a calibration module is introduced to decouple the point loss and ranking loss. The two losses are applied before and after the calibration module separately, which \u2217 Corresponding Author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. KDD '24, August 25-29, 2024, Barcelona, Spain \u00a9 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0490-1/24/08...$15.00 https://doi.org/10.1145/3637528.3671570 elegantly addresses the sub-optimal trade-off problem. We conduct comprehensive experiments on industrial scale datasets and online A/B tests, demonstrating that SBCR can achieve advanced performance on both calibration and ranking. Our method has been deployed on the video search system of Kuaishou, and results in significant performance improvements on CTR and the total amount of time users spend on Kuaishou.", "CCS CONCEPTS": "\u00b7 Information systems \u2192 Learning to rank ; Personalization .", "KEYWORDS": "Calibrated Ranking; Learning-to-rank; Search Ranking System", "ACMReference Format:": "Shunyu Zhang, Hu Liu, Wentian Bao, Enyun Yu, and Yang Song. 2024. A Selfboosted Framework for Calibrated Ranking. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '24), August 25-29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3637528.3671570", "1 INTRODUCTION": "As one of the most popular video-sharing apps in China, Kuaishou strongly relies on its leading personalized ranking system, which serves hundreds of millions of users with fingertip connection to billions of attractive videos. Once the ranking system receives a request from a user (aka. query in literature), it will predict a ranking score for each of the retrieved candidate videos (aka. items, documents ). These ranking scores are not only used in sorting the candidate items to fit the user's personalized interest, but also essential for many downstream applications. For example, we use the click-through rate (CTR) to guide ads bidding and the probability of effectively watching to estimate the video quality. This suggests that industrial ranking systems should emphasize two matters simultaneously: 1). the relative orders between scores, namely the ranking quality evaluated by GAUC and NDCG [16], and 2). the accurate absolute values of scores which should be calibrated to some actual likelihood when mapped to probabilistic predictions. To meet this practical demand in industrial ranking systems, there have been emerging studies on a paradigm known as Calibrated Ranking (CR) [1, 33]. The standard approach of CR usually incorporates a multi-objective loss function: a pointwise loss that focuses on calibrated absolute values and a ranking loss that emphasizes relative orderings. Sculley [27] combines regression and KDD '24, August 25-29, 2024, Barcelona, Spain Zhang and Liu, et al. pairwise loss for CTR prediction with offline experiments. Further studies [1, 28] combine pointwise loss and listwise loss for CR in real-world systems. Although encouraging progress has been made on both calibration and ranking ability, existing multi-objective CR approaches still suffer from two crucial limitations in industrial online applications. First, existing multi-objective CR approaches usually contradict extensive data shuffling which has long been proven beneficial in preventing the overfitting issue and is almost the default setting in industrial sequential gradient descent (Fig 1). Specifically, one component of the multi-objective loss, the ranking loss, is defined to make comparisons between scores of candidate items retrieved for the same request. This naturally requires the aggregation of the whole candidate list in a single mini-batch, making extensive itemlevel data shuffling inapplicable. As the result, the training process suffers from many terrible issues that could otherwise be avoided by extensive shuffling, including the non-IID data, and overfitting caused by the aggregation of similar samples. We will demonstrate the degradation of training effectiveness from insufficient shuffling in our experiments. Second, conventional multi-objective CR applies point loss and ranking loss jointly on a single probabilistic prediction. However, the two objectives are not necessarily compatible, or even inherently conflicting, making the best trade-off sub-optimal for both calibration and ranking. And this trade-off is also sensitively dependent on the relative weights of the two objectives, leading to a challenging hyper-parameter choosing issue. How to decouple the two objects, namely optimizing one without sacrificing the other, still remains an open question. To address the first limitation, we proposed a self-boosted pairwise loss that enables extensive data shuffling, while achieving high ranking quality like what conventional ranking loss does. Our strategy is to dump the ranking scores predicted by the deployed model on our online server into the context features of a specific query. With these contexts and negligible additional cost (only a few real numbers), each single candidate item can perceive the overall distribution of ranking scores under the same query. So there is no need to aggregate the whole candidate list in a single mini-batch for score comparison. More importantly, as the deployed model is a few versions older than the training model, the dumped scores actually reveal what the model failed to learn and further direct the following update to pay extra attention to previously mispredicted items. We term this Self-Boosted . We further tackle the second limitation by introducing a calibration module that decouples the ranking and calibration losses. Ranking and calibration losses are applied before and after the calibration module separately, which elegantly addresses the sub-optimal multi-objective trade-off problem. Finally, we propose the Self-Boosted framework for Calibrated Ranking (SBCR). Our architecture includes two modules, 1). a ranking module termed Self-Boosted Ranking (SBR) trained by a multiobjective loss consisting of the pointwise and proposed self-boosted pairwise losses, and 2). a following calibration module trained by a calibration loss. Our main contributions are summarized as follows: 1 The experiments are conducted on the pointwise production baseline with standard Logloss. The training details are described in Sec.4.1 Figure 1: The performance comparison of different data shuffling strategies. Evaluation metrics include: Logloss, NDCG@10 (widely used in ranking), and the total amount of time users spend on Kuaishou (the most important metric in our online A/B test). Extensive item-level data shuffling (upper) significantly outperforms the query-level data shuffling (bottom) where the whole candidate item list retrieved for a single request is aggregated in a single mini-batch. Theoretical explanation will be discussed in Sec 3.2.2. This experimental result validates the advantage of extensive data shuffling and motivates us to propose a novel ranking loss that enables extensive shuffling. 1 query 1 query 4 query 2 Extensive Shuffle Query-level Shuffle NDCG@10 LogLoss 0.5612 0.5625 0.7269 0.7267 User Time Spend -0.25% \u00b7 Wehighlight the two limitations of conventional multi-objective CR approaches in industrial online applications, namely, the contradiction with extensive data shuffling and the suboptimal trade-off between calibration and ranking. \u00b7 We propose a novel SBCR framework that successfully addresses the two limitations. In SBCR, ranking quality is emphasized without the need for data aggregating, and the two objectives are decoupled to avoid the conflict. \u00b7 We validate SBCR on the video search system of Kuaishou. Extensive offline experiments show that our method can achieve advanced performance on both calibration and ranking. In online A/B tests, SBCR also outperforms the strong production baseline and brings significant improvements on CTR and the total amount of time users spend on Kuaishou. SBCR has now been deployed on the video search system of Kuaishou, serving the main traffic of hundreds of millions of active users.", "2 RELATED WORK": "Wemainly focus on the related work concerning these aspects: CTR Prediction, Learning-to-Rank (LTR), and Calibrated Ranking. CTR Prediction aims to predict a user's personalized click tendency, which is crucial for nowadays information systems. In the last decades, the field of CTR prediction has evolved from traditional shallow models, e.g. Logistic Regression (LR), Gradient Boosting Decision Tree (GBDT) [17], to deep neural models e.g. Wide & Deep [7], DCN [30]. Most researches are dedicated to improving model architectures: Wide & Deep [7] and DeepFM [13] combine low-order and high-order features to improve model expressiveness, and DCN [30, 31] replace FM of DeepFM with Cross Network. DIN [35] and SIM [24] employ the attention mechanism to extract user interest. Despite recent progress, the loss function is still not well-explored and the dominant pointwise LogLoss [35] can't well satisfy the ranking quality highly desired in practice [19, 20, 27]. A Self-boosted Framework for Calibrated Ranking KDD '24, August 25-29, 2024, Barcelona, Spain Learning-To-Rank (LTR) generally learns a scoring function to predict and sort a list of objects [2, 20, 32]. The evaluation is based on ranking metrics considering the sorted order, such as Area Under the Curve [9] and Normalized Discounted Cumulative Gain [16]. The pointwise approach [10] learns from the label of a single item. And the pairwise methods learn from the relative ordering of item pairs [4, 11], which is further used in real-world LTR systems [3, 15]. Some others propose the ranking-metric-based optimization including the listwise approaches, which directly target on aligning the loss with the evaluation metrics [5, 26]. However, the poor calibration ability limits non-pointwise LTRs for wider applications. Multi-Objective CR is a natural idea to address the above problems, where ranking loss is calibrated by a point loss [19, 33] to preserve both calibration and ranking ability. Sculley [27] conducts an early study to combine regression with pairwise ranking, and Li et al. [19] shows it can yield promising results in real-world CTR prediction systems. Recently, multi-objective CR has been deployed in deep models and more methods are proposed to better combine the two losses. Yan et al. [33] address the training divergence issues and achieve calibrated outputs. Bai et al. [1] further proposes a regression-compatible ranking approach to balance the calibration and ranking accuracy. Sheng et al. [28] propose a hybrid method using two logits corresponding to click and non-click states, to jointly optimize the ranking and calibration. However, existing multi-objective methods are still sub-optimal facing the trade-off between calibration and ranking, and contradict extensive data shuffling as we have stated. Another line of works on CR revolves around post-processing methods including Platt-scaling, Isotonic Regression, and etc. Tagami et al. [29] adopt pairwise squared hinge loss for training, and then used Platt-scaling [25] to convert the ranking scores to probabilities. Chaudhuri et al. [6] compare post-processing methods including Platt-scaling and Isotonic Regression, to calibrate the outputs of an ordinal regression model. Apart from the calibration tailored for ranking, there are some others that only aim for more accurate probabilistic predictions, including binning [21] and hybrid approaches [18], for e.g., Smooth Isotonic Regression [8], Neural Calibration [22]. While they all require extra post-processing, our method is jointly learned during training.", "3 METHODOLOGY": "We start from reviewing the general preliminaries of CR in Section 3.1. Then we describe its standard approach multi-objective CR and analyze the two main drawbacks in Section 3.2. To address these issues, we finally dig into the details of our proposed SBCR in Section 3.3. The notations used are summarized in Table 1.", "3.1 Preliminaries": "The aim of Calibrated Ranking [1, 33] is to predict ranking scores for a list of candidate items (or documents, objects) properly given a certain query (request with user, context features). Besides the relative order between the ranking scores emphasized by conventional LTR [5], CR also focuses on the calibrated absolute values of scores simultaneously. To be specific, ranking scores should not only improve ranking metrics, such as GAUC and NDCG [16], but Table 1: Important Notations Used in Section 3. also be scale-calibrated to some actual likelihood when mapped to probabilistic predictions. Formally, our goal is to learn a scoring function \ud835\udc60 : Q \u00d7 X \u2192 R , given a training dataset D consisting of a list of samples ( \ud835\udc5e, \ud835\udc65, \ud835\udc66 ) . Here, we denote Q as the query space, \ud835\udc5e \u2208 Q as a query , X as the item feature space, \ud835\udc65 \u2208 X as a candidate item, and \ud835\udc66 \u2208 Y as a ground-truth label under specific business settings. For example, in our video search system at Kuaishou, \ud835\udc66 can indicate whether a video is clicked, liked, or watched effectively (beyond a predefined time threshold). Without loss of generality, we assume the label space Y = { 1 , 0 } throughout this paper. The model first predicts a ranking score (also known as logit ),  for each item \ud835\udc65 associated with the same query \ud835\udc5e , and then ranks the items according to the descending order of the scores. In addition, the ranking score is mapped to a probabilistic prediction by a sigmoid function,  which will be used for many downstream applications. And this probabilistic prediction should be well calibrated, i.e., agree with the actual likelihood that the item is clicked, liked, or watched effectively, i.e., \u02c6 \ud835\udc66 \ud835\udc5e,\ud835\udc65 = E [ \ud835\udc66 | \ud835\udc5e, \ud835\udc65 ] .", "3.2 Multi-objective Calibrated Ranking": "In literature and industrial ranking systems, multi-objective methods have been wildly adopted as standard approaches for CR [1]. 3.2.1 Existing Methods. The key idea of multi-objective CR is to take the advantage of two loss functions: 1). a pointwise loss that calibrates the absolute probabilistic prediction values, and 2). a ranking loss that emphasizes the relative orders of items associated with the same query. KDD '24, August 25-29, 2024, Barcelona, Spain Zhang and Liu, et al. Specifically, the pointwise loss is defined as the average of Cross Entropy overall training samples,  which is shown to be calibrated [33] since the minima is achieved at the point \u02c6 \ud835\udc66 \ud835\udc5e,\ud835\udc65 = [ \ud835\udc66 | \ud835\udc5e, \ud835\udc65 ] . \u274a The ranking loss is usually defined on pairs of training samples. We denote D + \ud835\udc5e = { \ud835\udc65 | ( \ud835\udc5e, \ud835\udc65, \ud835\udc66 ) \u2208 D , \ud835\udc66 = 1 } , D \ud835\udc5e = { \ud835\udc65 | ( \ud835\udc5e, \ud835\udc65, \ud835\udc66 ) \u2208 D , \ud835\udc66 = 0 } as the set of positive and negative items associate to the same query \ud835\udc5e . And the pairwise loss is defined to promote a large margin between items across the two sets,  where the outer average is taken over all queries in D 2 , \ud835\udc44 denotes the number of unique queries, and the inner average is taken over pairs inside each query. Although achieving high ranking quality, this pairwise loss suffers from the miscalibration problem due to translation-invariant [33]. To combine the advantages of both, a multi-objective loss is defined as  where \ud835\udefc \u2208 ( 0 , 1 ) controls the trade-off between the quality of calibration and ranking. Note that besides the pairwise loss, there are many other widely used ranking losses, such as listwise softmax [2, 23] and listwise ApproxNDCG [26]. Similarly, these listwise losses also sacrifice calibration for ranking performance, and could be used as a component in multi-objective CR. We skip the discussion for conciseness. 3.2.2 Limitations of Existing Multi-Objective CR. Despite being extensively studied, existing multi-objective CR approaches still suffer from two crucial limitations: contradiction to extensive data shuffling and sub-optimal trade-off between calibration and ranking abilities. In the default setting of most industrial training systems, samples are first extensively shuffled, and then divided into mini-batches for sequential gradient descent. This shuffling simulates the Independent and Identically Distribution (IID), and consequently prevents the overfitting problem caused by aggregated similar samples inside each mini-batch. We show the performance gain from data shuffling in our experiments in Fig. 1. While in multi-objective CR, extensive data shuffling is not applicable due to the definition of the pairwise loss. Specifically, to calculate \u2113 \ud835\udc5d\ud835\udc4e\ud835\udc56\ud835\udc5f ( \ud835\udc5e ) , we need to go through all positive-negative pairs associated with \ud835\udc5e , indicating that all samples with the same query 2 We slightly abuse the notation for conciseness of Eq. 4: \ud835\udc5e \u2208 D represents any unique query in the training dataset, namely, \ud835\udc5e \u2208 { \ud835\udc5e | ( \ud835\udc5e, \ud835\udc65, \ud835\udc66 ) \u2208 D} . have to be gathered inside a single mini-batch. As a result, in training pairwise (or listwise) losses, data can only be shuffled at query level, not sample level. This aggregation of similar samples (under the same query, with identical context, user features) inside each mini-batch contradicts the IID assumption and thus heavily degrades the performance gain from data shuffling. Another limitation of conventional CR lies in the trade-off nature of the multi-objective loss. To be specific, we introduce the following theorem. Theorem 3.1. L \ud835\udc5d\ud835\udc4e\ud835\udc56\ud835\udc5f and L \ud835\udc5d\ud835\udc5c\ud835\udc56\ud835\udc5b\ud835\udc61 have distinct optimal solutions. Proof. As mentioned in Sec 3 of [1], L \ud835\udc5d\ud835\udc5c\ud835\udc56\ud835\udc5b\ud835\udc61 is minimized when,  And L \ud835\udc5d\ud835\udc4e\ud835\udc56\ud835\udc5f is minimized when,  In the case \ud835\udc66 1 and \ud835\udc66 2 are independent, we rewrite Eq. 7 as,  Supposing the two losses share the same optimal solution, we use Eq. 6 to further rewrite Eq. 8 as,  Thus,  Ultimately, we have derived an infeasible equation, \ud835\udc52 -\ud835\udc60 \ud835\udc5e,\ud835\udc65 1 + \ud835\udc52 \ud835\udc60 \ud835\udc5e,\ud835\udc65 2 = 0, indicating that the two losses have distinct optimal solution. \u25a1 Intuitively, L \ud835\udc5d\ud835\udc4e\ud835\udc56\ud835\udc5f emphasizes the relative order of items, while failing to predict the absolute probabilistic value accurately. And L \ud835\udc5d\ud835\udc5c\ud835\udc56\ud835\udc5b\ud835\udc61 vice versa. In Eq 5, however, the two inherently conflicting losses are applied on the prediction \ud835\udc60 \ud835\udc5e,\ud835\udc65 , and thus the best trade-off may be sub-optimal for both calibration and ranking. In addition, this trade-off also sensitively depends on \ud835\udefc , leading to a challenging hyper-parameters choosing issue. In contrast, a better design would decouple the ranking and calibration losses with separated network structures and gradient cut-off strategies, which elegantly addresses the objective conflict.", "3.3 Self-Boosted Calibrated Ranking": "To address the two limitations, our proposed SBCR mainly consists of two modules: 1). a self-boosted ranking module (SBR) that enables extensive data shuffling, while achieving high ranking quality like what conventional pairwise loss does, and 2). an auxiliary calibration module that decouples the ranking and calibration losses and thus successfully addresses the sub-optimal trade-off problem. We describe our model architecture in Fig. 2. 3.3.1 The Self-Boosted Ranking Module. Asmentioned earlier, \u2113 \ud835\udc5d\ud835\udc4e\ud835\udc56\ud835\udc5f ( \ud835\udc5e ) is calculated on all possible positive-negative item pairs associated with \ud835\udc5e , making it inseparable for sample-level data shuffling. To address this issue, we propose a novel loss function,  A Self-boosted Framework for Calibrated Ranking KDD '24, August 25-29, 2024, Barcelona, Spain Self-Boosted Pair-wise Loss -log - ) \ud835\udd5d( - ) \ud835\udc60 !,# \ud835\udc2c * ! \ud835\udc66 \ud835\udc32 * ! -log - ) \ud835\udd5d( - ) \ud835\udc60 !,# \ud835\udc2c * ! \ud835\udc66 \ud835\udc32 * ! + Query Item \u03c3 \ud835\udc60 !,# Self-Boosted Ranking Module \ud835\udc60(\ud835\udc5e, \ud835\udc65) ( \ud835\udc66 !,# ( \ud835\udc66 $%&',!,# Calibration Module g ( ( \ud835\udc66 ;q ) Training Model Query Candidate Items \u03c3 Self-Boosted Ranking Module \ud835\udc32 * ! Calibration Module g ( ( \ud835\udc66 ;q ) Deployed Model \ud835\udc2c * ! \ud835\udc60(\ud835\udc5e, \ud835\udc65) Model Loading Dump Scores & Labels \ud835\udc32 * ! \ud835\udc2c * ! \u0303 Figure 2: The architecture of the proposed Self-Boosted framework for Calibrated Ranking. Middle: SBCR consists of two modules: a self-boosted ranking module (SBR) trained by a multi-objective loss (pointwise and self-boosted pairwise loss) and a calibration module. Left: the details of the proposed self-boosted pairwise loss. Using dumped ranking scores e s \ud835\udc5e from the online deployed model, we enable both comparisons between samples associated with the same query and extensive sample-level data shuffling. Right: the proposed calibration module that decouples the ranking and calibration objectives to avoid the conflict. Calibration Module \ud835\udc4f ( (\ud835\udc5e) ( \ud835\udc66 $%&',!,# ( \ud835\udc66 !,# Query \u2211(Softmax(NN)) Different from L \ud835\udc5d\ud835\udc4e\ud835\udc56\ud835\udc5f (Eq. 4), here each component \u2113 \ud835\udc5d\ud835\udc4e\ud835\udc56\ud835\udc5f _ \ud835\udc4f\ud835\udc5c\ud835\udc5c\ud835\udc60\ud835\udc61 only depends on a single training sample. Note this design is nontrivial due to the conflict between two facts: With the extra e s \ud835\udc5e and f y \ud835\udc5e as context features in \ud835\udc5e , we define, \u00b7 In order to enhance the ranking ability, comparisons between the current item and its peers, i.e., other items associated with the same query, are essential for ranking order learning. So \u2113 \ud835\udc5d\ud835\udc4e\ud835\udc56\ud835\udc5f _ \ud835\udc4f\ud835\udc5c\ud835\udc5c\ud835\udc60\ud835\udc61 should be able to perceive the overall score distribution under \ud835\udc5e . \u00b7 Since only one item's feature \ud835\udc65 is used as the input, it is impossible to run the network forward and backward for the other items associated with \ud835\udc5e . We now dig into the details of \u2113 that solves this conflict. \ud835\udc5d\ud835\udc4e\ud835\udc56\ud835\udc5f _ \ud835\udc4f\ud835\udc5c\ud835\udc5c\ud835\udc60\ud835\udc61 Our system mainly consists of two parts: 1). an online Server that receives a user's request, makes real-time responses by scoring and ranking candidate items, and dumps logs including the user's feedback. and 2). a near-line Trainer that sequentially trains the scoring function \ud835\udc60 (\u00b7 , \u00b7) on latest logs. For every few minutes, the Server continuously loads the latest scoring function from the Trainer. We denote the deployed model on the Server as e \ud835\udc60 (\u00b7 , \u00b7) , which would be a bit older than the training model \ud835\udc60 (\u00b7 , \u00b7) . Formally, when the Server receives a query \ud835\udc5e and candidates [ \ud835\udc65 1 , ..., \ud835\udc65 \ud835\udc5b ] , it (1) first predicts ranking scores e s \ud835\udc5e = [ e \ud835\udc60 \ud835\udc5e,\ud835\udc65 1 , ..., e \ud835\udc60 \ud835\udc5e,\ud835\udc65 \ud835\udc5b ] for candidates using the deployed model e \ud835\udc60 (\u00b7 , \u00b7) , (2) then presents the ranked items to the user and collects her feedback, f y \ud835\udc5e = [ e \ud835\udc66 \ud835\udc5e,\ud835\udc65 1 , ..., e \ud835\udc66 \ud835\udc5e,\ud835\udc65 \ud835\udc5b ] . (3) finally dumps the scores e s \ud835\udc5e \u2208 R \ud835\udc5b and labels f y \ud835\udc5e \u2208 R \ud835\udc5b into context features of the current query \ud835\udc5e . Although only adding negligible cost, e s \ud835\udc5e and f y \ud835\udc5e actually provide rich knowledge on the score distribution to enhance the model's ranking ability. More importantly, the dumped scores from an older model reveal what the Trainer failed to learn and further direct the following update to pay extra attention to the previously mispredicted samples. We thus term this Self-Boosted .  where the indicator I ( \ud835\udc67 ) = 1 if \ud835\udc67 > 0 and 0 otherwise. We slightly abuse notations by broadcasting scalars to fit vectors' dimensions. Remarks : Similar to \u2113 \ud835\udc5d\ud835\udc4e\ud835\udc56\ud835\udc5f , our \u2113 \ud835\udc5d\ud835\udc4e\ud835\udc56\ud835\udc5f _ \ud835\udc4f\ud835\udc5c\ud835\udc5c\ud835\udc60\ud835\udc61 also compares items under the same query and encourages large margins. It is different that the self-boost mechanism keeps lifting the model performance by focusing on previously missed knowledge. We take the first term in \u2113 \ud835\udc5d\ud835\udc4e\ud835\udc56\ud835\udc5f _ \ud835\udc4f\ud835\udc5c\ud835\udc5c\ud835\udc60\ud835\udc61 for example. After masked by the indicator, it promotes the margins between the positive item's score \ud835\udc60 \ud835\udc5e,\ud835\udc65 and its negative peers' dumped scores e s \ud835\udc5e . If the items under \ud835\udc5e were poorly predicted previously, the unmasked elements in e s \ud835\udc5e would be larger, resulting in a larger loss value. To achieve a satisfactory margin, \ud835\udc60 \ud835\udc5e,\ud835\udc65 would be lifted more aggressively in the backpropagation. 3.3.2 The Calibration Module. Theprobabilistic prediction \u02c6 \ud835\udc66 trained using multi-objective CR (Eq. 5) usually suffers from the sub-optimal trade-off between calibration and ranking ability. To address this issue, we propose a calibration module to decouple the two losses. The pairwise loss and point loss are applied before and after the calibration module separately, which elegantly addresses the objective conflicting problem. The calibration module maps \u02c6 \ud835\udc66 into a calibrated one:  where \u02c6 \ud835\udc66 cali \u2208 ( 0 , 1 ) is the calibrated probability and \ud835\udc54 is the proposed calibration module. We make several considerations in the design of \ud835\udc54 (\u00b7) : First, to be flexible enough to capture various functional distributions, \ud835\udc54 (\u00b7) is set as a continuous piece-wise linear function. Without loss of generality, we partition the function domain ( 0 , 1 ) into 100 equal-width intervals and set \ud835\udc54 (\u00b7) to be a linear function inside each interval:  KDD '24, August 25-29, 2024, Barcelona, Spain Zhang and Liu, et al. where I \ud835\udc58 ( \u02c6 \ud835\udc66 ) indicates whether \u02c6 \ud835\udc66 lies inside the \ud835\udc58 -th interval. Namely I \ud835\udc58 ( \u02c6 \ud835\udc66 ) = 1 if \u02c6 \ud835\udc66 \u2208 [ 0 . 01 \u2217 \ud835\udc58, 0 . 01 \u2217 ( \ud835\udc58 + 1 )) and 0 otherwise. And \ud835\udc4f \ud835\udc58 , \ud835\udc58 \u2208 { 0 , ..., 100 } are the function values at all interval ends. Obviously, \ud835\udc4f 0 = 0 and \ud835\udc4f 100 = 1. And the other 99 which control the function property will be adjusted during modeling training. Second, to keep the knowledge learnt previously from pairwise loss, our calibration module should preserve the relative orders of items under the same query. Namely, given any two predictions \u02c6 \ud835\udc66 \ud835\udc5e,\ud835\udc56 < \u02c6 \ud835\udc66 \ud835\udc5e,\ud835\udc57 , the calibrated probability \u02c6 \ud835\udc66 cali ,\ud835\udc5e,\ud835\udc56 \u2264 \u02c6 \ud835\udc66 cali ,\ud835\udc5e,\ud835\udc57 . We thus require the piece-wise linear function to be non-decreasing, i.e., \ud835\udc4f 0 \u2264 \ud835\udc4f 1 \u2264 ... \u2264 \ud835\udc4f 100 . And the parameters \ud835\udc4f 1 , ..., \ud835\udc4f 99 are learnt only from the features of the current query, such as the user features (user id, gender, age, behavior sequences) and context features (timestamp, page index, date). So Eq. 11 only includes \ud835\udc5e as the input but excludes any item features in \ud835\udc65 . The learning process is defined:  where a \u2208 R 100 represents the learnt 100 interval heights, normalized by the softmax function and \ud835\udc4f \ud835\udc58 is calculated as the accumulated heights from the first to \ud835\udc58 -th interval. Finally, our calibration module is trained using pointwise loss to ensure accurate absolute prediction values:  3.3.3 The Overall Architecture of SBCR and Training Tricks. Our model consists of two networks, as summarized in Fig 2. The first deep network defines the scoring function \ud835\udc60 ( \ud835\udc5e, \ud835\udc65 ) with two groups of inputs: 1). \ud835\udc5e , features shared by samples under the same query, including user features, long-term user behaviors and context features; 2). \ud835\udc65 , features of a specific item. In the industrial ranking system of Kuaishou, we adopt QIN [14] as \ud835\udc60 ( \ud835\udc5e, \ud835\udc65 ) due to its SOTA performance. This network is trained using,  We replace the conventional pairwise loss in Eq. 5 by our proposed self-boosted pairwise loss, which enables sample-level shuffling. Our second deep network defines the function for calculating the interval heights a in Eq. 13, which is trained by Eq. 14. In our system, the network consists of 4 FC layers of size ( 255 , 127 , 127 , 100 ) . To avoid a sub-optimal trade-off between calibration and ranking, we further stop the gradient back propagation for all inputs to the calibration module (i.e., \u02c6 \ud835\udc66, \ud835\udc5e ). Thus \ud835\udc60 ( \ud835\udc5e, \ud835\udc65 ) focuses on the ranking quality and the calibration module only deals with calibration. We will discuss more on parameter sensitivity in our experiments.", "4 EXPERIMENTS": "In this section, to validate the effectiveness of our proposed SBCR framework, we compare SBCR with many state-of-the-art CR algorithms. We also provide an in-depth analysis to investigate the impact of each building block in SBCR. Experiments are conducted based on the Kuaishou video search system, including both offline evaluations on the billion-scale production dataset, and online A/B Table 2: Statistics of the real production dataset. Query means a request from the user. M and B are short for million and billion. testing on the real traffic of millions of active users. We did not include experiments on the public datasets, since our method is designed for the online training system.", "4.1 Experiment Setup": "4.1.1 Datasets. In our offline experiments, all compared algorithms are initialized from the same checkpoint, trained online using 5 days' data, and then frozen to test on the 6th day's data. The dataset is collected from the user log on the video search system of Kuaishou and the statistics of the dataset are shown in Tab. 2. Our method is designed for the online training systems that are widely deployed in industrial scenarios, so we only conduct experiments on the real production dataset. 4.1.2 Implementation Details. In our experiments, we adopt QIN [14] as our architecture for all compared methods. With efficient user behavior modeling, QIN is a strong production baseline latest deployed on the KuaiShou video search system. For feature engineering, ID features are converted to dense embeddings and concatenated with numerical features. All models are trained and tested using the same optimization settings, i.e., Adam optimizer, learning rate of 0.001, and batch size of 512. All models are trained with one epoch following Zhang et al. [34], which is widely adopted in the production practice. For the relative ranking weight ( 1 -\ud835\udefc )/ \ud835\udefc , we chose the best one from [0.01, 0.1, 1.0, 10, 100] for each compared algorithm and report the performance of them in their own optimal hyper-parameter settings for a fair comparison. For our proposed SBCR, we simply set the relative weight to 1 consistently across all experiments. 4.1.3 Evaluation Metrics. In this work, we consider both ranking and calibration performances. For evaluating the ranking performance, we choose NDCG@10 and GAUC (Group AUC). NDCG@10 is consistent with other metrics like NDCG@5. GAUC is widely employed to assess the ranking performance of items associated with the same query, and has demonstrated consistent with online performance in previous studies [24, 35]. GAUC is computed by:  where AUC \ud835\udc5e represent AUC within the same query \ud835\udc5e . For evaluating the calibration performance, we include LogLoss , expected calibration error ( ECE ) [22], and predicted CTR over the true CTR ( PCOC ) [12]. LogLoss is calculated the same way as in Eq. 3, which measures the logarithmic loss between probabilistic A Self-boosted Framework for Calibrated Ranking KDD '24, August 25-29, 2024, Barcelona, Spain Table 3: The comparison of SOTAs on the production dataset. The best results are highlighted in bold and the second-best are underlined. * indicates significant improvement with p-value=0.05 than previous best-performing Point + ListCE. Shuffle denotes compatibility with extensive sample-level data shuffling. SBCR outperforms all due to 2 key advantages: compatibility with extensive data shuffling and effective structure to decouple ranking and calibration losses. predictions and true labels. ECE and PCOC are computed by  where I \ud835\udc58 is defined in the same way as Eq. 12. Among the three calibration metrics, LogLoss provides sample-level measurement, whereas ECE and PCOC provide subset level and dataset level measurement, respectively. There are some other metrics like Cal-N and GC-N [8] and here we mainly follow the setting of calibrated ranking [28, 33] for a fair comparison. Lower LogLoss or ECE indicates better performance, and PCOC is desired to be close to 1.0. These five metrics serve as reliable indicators for evaluating both ranking and calibration abilities. 4.1.4 Compared Methods. As in Table 3, we include several important baseline methods for a comprehensive comparison. These baseline methods are divided into two groups based on whether the loss function is single-objective or multi-objective. The Singleobjective group consists of these four methods: \u00b7 Pointwise refers to the standard LogLoss (Eq. 3), which is widely adopted for binary targets. It is also the production baseline for most industrial ranking systems. \u00b7 RankNet [3] adopts pairwise loss (Eq. 4) to optimize the relative ranking of pairs of samples. \u00b7 ListNet [5] defines a listwise loss to maximize the likelihood of the correct ordering of the whole list. \u00b7 ListCE [1] proposes a regression compatible ranking approach where the two ranking and regression components are mutually aligned in a modified listwise loss. In the Multi-objective group, we include several advanced methods with both calibrated point loss and ranking-oriented loss for the comprehensive comparison: \u00b7 Point + RankNet [19] combines the pointwise and pairwise loss in a multi-objective paradigm (Eq. 5) to improve both calibration and ranking. \u00b7 Point + ListNet [33] is the combination of the pointwise and the listwise loss, which is proved as a strong calibrated ranking baseline and termed 'multi-ojbective\" in [33]. \u00b7 Multi-task [33] is a multi-task method that uses multi-head of DNN for the ranking and calibration scores. \u00b7 Calibrated Softmax [33] is a reference-based method where an anchor candidate with label \ud835\udc66 0 to a query is introduced to control the trade-off. \u00b7 JRC [28] proposes a hybrid method that employs two logits corresponding to click and non-click states and jointly optimizes the ranking and calibration abilities. \u00b7 Point + ListCE [1] combines the pointwise loss with ListCE that makes ranking and regression components compatible and achieves advanced results.", "4.2 Main Experimental Results": "The main experimental results are shown in Table 3. The methods are compared on both ranking and calibration metrics. From the results, we have the following observations: First, in the single-objective group, pointwise achieves the best calibration performance but inferior ranking performance. In contrast, RankNet and ListNet outperform the Pointwise model on the ranking ability, at the expense of being completely uncalibrated. Among these methods, ListCE can reach a tradeoff for it makes regression compatible with ranking, but it still suffers from poor calibration. This validates the necessity of calibrated ranking. Second, the multi-objective methods incorporate the two losses and achieve a better trade-off. And several recent studies (JRC and Point + ListCE) have achieved encouraging progress on both ranking and calibration metrics. Note that LogLoss is a stronger calibration metric than PCOC and ECE. For example, if a model predicts KDD '24, August 25-29, 2024, Barcelona, Spain Zhang and Liu, et al. Table 4: Comparison on different sample aggregations. Shuf denotes extensive sample-level shuffle, Aggre means aggregating the whole candidate list in a single mini-batch. Best are marked bold. Shuf consistently outperforms Aggre on both Point and Point + Pair models. averaged prediction over the whole dataset, the model achieves perfect PCOC and ECE but poor LogLoss. Considering LogLoss as the main calibration metric, all compared multi-objective methods still suffer from sub-optimal trade-off between calibration and ranking, when compared to the best Single-objective methods, validating our second motivation. When comparing the three variants of [33], we find that cal-softmax achieves slightly better NDCG but suffers from unacceptable worst calibration performance, while multi-objective gets the best trade-off between ranking and calibration. Our observation are also consistent with the previous results reported in the original paper. Third, our proposed SBCR outperforms all compared algorithms on both ranking and calibration performance. This validates the two key advantages of SBCR, i.e., compatibility with extensive datashuffling and effective structure to avoid trade-off between ranking and calibration. Specifically, SBCR is trained with the dumped predictions by the online deployed model, making it the only method that needs no sample aggregation when computing the ranking loss. And a calibration module is introduced to decouple the point loss and ranking loss to address the sub-optimal trade-off problem. The gain of the two key advantages will be further analyzed in section 4.3.1 and 4.3.2.", "4.3 Ablation Study and Analysis": "We conduct ablation studies to investigate the contribution of each SBCR building block: the self-boosted pairwise loss to address the data-shuffling problem, and the calibration module to address the trade-off problem. We also include hyper-parameter analysis to show the sensitivity. 4.3.1 Analysis on Data-shuffling. As mentioned, extensive datashuffling that simulates the Independent and Identical Distribution has long been proven beneficial in preventing the overfitting problem. We re-validate the performance gain of data-shuffling on both point loss and point + pair loss. For point + pair loss, Point + RankNet [19] is used as the aggregation method and our SBR (Eq. 15) is used as the shuffling method, results shown in Tab. 4. Shuffling achieves consistent improvement over sample aggregation, which validates that extensive data-shuffling is essential for performance and supports our first motivation. Conventional multi-objective CR algorithms require the aggregation of the whole candidate list in a single mini-batch for computing the ranking loss, Table 5: Comparison on different Calibration Modules. 'Calib' is our proposed one. We observe consistent improvement when the ranking module is trained by ListNet and SBR, validating our adaptability. which is incompatible with extensive data-shuffling. SBCR solves this problem and achieves superior performance. 4.3.2 The Impact of Calibration Module. In order to analyze the impact of the Calibration Module, we compared several methods in Tab. 5. ListNet-Platt [33] applies Platt-scaling post-processing after a ListNet model, which is a strong baseline. Hence, we compared our calibration module with Platt-scaling on the same ListNet model. We also apply Platt-scaling and ours upon the same SBR model as defined in Eq. 15. As shown in Tab. 5, we observed that both Platt-scaling and ours preserve the relative orders and show the same GAUC. Our proposed calibration module achieves consistently better calibration ability on both ListNet and SBR, which validates the strong adaptability of our method. 4.3.3 Effects of Hyper-Parameter. The only hyper-parameter introduced in our method is \ud835\udefc , the trade-off parameter between point loss and self-boosted pair loss in Eq. 15. We define ( 1 -\ud835\udefc )/ \ud835\udefc as the relative ranking weight and examine the sensitivity in Fig. 3. First, surprisingly, an extremely small value of relative ranking weight is not optimal for calibration and an extremely large value is not optimal for ranking. This validates that the two losses collaborate with each other. Point loss is necessary for ranking, especially in queries where all items are negative and pair loss is necessary for calibration by giving auxiliary guild for model training. Second, SBCR is robust to the setting of \ud835\udefc . When the relative weight is set 10 times larger /smaller, the performance of SBCR is still comparable to that of SOTA. We own this robustness to our calibration module which is specially designed to be monotonic for a given \ud835\udc5e . Namely, it preserves the learned orderings from our ranking module. Optimizing the calibration module will not degrade the ranking performance. Third, we still observe slightly trade-off between calibration and ranking. This trade-off has been greatly improved by the calibration module. We tried to remove the calibration module and found that when ( 1 -\ud835\udefc )/ \ud835\udefc = 100, ECE will be increased to 0.1796, which is 35 times worse than the current case.", "4.4 Online Performance": "We validate the proposed SBCR with online A/B testing on the video search system of Kuaishou (Tab. 6). We compare SBCR with our latest production baseline Point and the strongest compared A Self-boosted Framework for Calibrated Ranking KDD '24, August 25-29, 2024, Barcelona, Spain Figure 3: The sensitivity of relative ranking weight ( 1 -\ud835\udefc )/ \ud835\udefc for SBCR (Eq. 15). Higher GAUC and lower ECE indicate better performance. 0.01 0.1 1 10 100 1- \u03b1 \u03b1 0.606 0.607 0.608 0.609 0.610 0.611 0.612 0.613 GAUC GAUC 0.0044 0.0046 0.0048 0.0050 0.0052 0.0054 ECE ECE algorithm reported in Table 3, Point+ListCE . All three algorithms share the same backbone QIN [14], and the same features, model structures and optimizer. The online evaluation metrics are CTR, View Count and User Time Spend. View count measures the total amount of video users watched, and time spend measures the total amount of time users spend on viewing the videos. As shown in Tab. 6, SBCR contributes to the +4.81% increase in CTR, +3.15% increase in View Count and +0.85% increase in Time Spend. Note that the 0.2% increase is a significant improvement in our system. SBCR is also efficient for online serving. Compared to our baseline QIN+Point, the only additional module is the calibration network that adds 1.41% parameters and 0.96% floating point operations, which is negligible for the model. Note that there is an important issue that is commonly faced in the real production systems. Usually, there are several different algorithms under A/B test simultaneously. So the dumped scores used for training SBCR are actually from deployed models in different A/B tests, not only SBCR's corresponding deployed model. This is not equivalent to the standard self-boosted mechanism in Sec 3. We should check the impact. In A/B test, the pointwise baseline serves the main traffic, thus SBCR is mostly trained with dumped scores from the pointwise baseline. In this sub-optimal implementation, we still observe significant gain (the 3rd two in Table 6). And in A/B backtest (last row in Table 6), SBCR serves the main traffic and the old pointwise model serves the small traffic. The dumped scores are mostly from SBCR itself. We observe a larger improvement compared to that in AB test: an additional gain of +0.63% View Count and +0.43% Time Spent. We conclude that the dumped scores from other models also work, with a slightly smaller gain compared to standard self-boost. This is because the mis-predicted samples by other models are also hard and informative and focusing on other strong model's mistakes is also beneficial.", "5 CONCLUSION AND FUTURE WORKS": "We proposed a Self-Boosted framework for Calibrated Ranking in industrial online applications. SBCR addressed the two limitations of conventional multi-objective CR, namely, the contradiction with Table 6: The improvements of SBCR in online A/B test compared to the production baseline. In the video search system of Kuaishou, 0.2% increase is a significant improvement for CTR, View Count and User Time spend. extensive data shuffling and the sub-optimal trade-off between calibration and ranking, which contributes to significant performance gain. SBCR outperformed our highly optimized production baseline and has been deployed on the video search system of Kuaishou, serving the main traffic of hundreds of millions of active users. Note that we restricted our calibration module \ud835\udc54 (\u00b7) to piece-wise linear function since it is easy to guarantee the monotonicity, which is necessary to preserve the relative orders of items under the same query. A promising future direction is to improve the flexibility of \ud835\udc54 by upgrading it to monotonic neural networks.", "REFERENCES": "[1] Aijun Bai, Rolf Jagerman, Zhen Qin, Le Yan, Pratyush Kar, Bing-Rong Lin, Xuanhui Wang, Michael Bendersky, and Marc Najork. 2023. Regression Compatible Listwise Objectives for Calibrated Ranking with Binary Relevance. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management . 4502-4508. [2] Sebastian Bruch, Xuanhui Wang, Michael Bendersky, and Marc Najork. 2019. An analysis of the softmax cross entropy loss for learning-to-rank with binary relevance. In Proceedings of the 2019 ACM SIGIR international conference on theory of information retrieval . 75-78. [3] Christopher JC Burges. 2010. From ranknet to lambdarank to lambdamart: An overview. Learning 11, 23-581 (2010), 81. [4] Christopher J. C. Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Gregory N. Hullender. 2005. Learning to rank using gradient descent. In Proceedings of the 22nd international conference on Machine learning , Vol. 119. 89-96. [5] Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. 2007. Learning to rank: from pairwise approach to listwise approach. In Proceedings of the 24th international conference on Machine learning . 129-136. [6] Sougata Chaudhuri, Abraham Bagherjeiran, and James Liu. 2017. Ranking and calibrating click-attributed purchases in performance display advertising. In Proceedings of the ADKDD'17 . 1-6. [7] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st Workshop on Deep Learning for Recommender Systems . ACM, 7-10. [8] Chao Deng, Hao Wang, Qing Tan, Jian Xu, and Kun Gai. 2021. Calibrating user response predictions in online advertising. In Machine Learning and Knowledge Discovery in Databases: Applied Data Science Track: European Conference, ECML PKDD 2020, Ghent, Belgium, September 14-18, 2020, Proceedings, Part IV . Springer, 208-223. [9] Tom Fawcett. 2006. An introduction to ROC analysis. Pattern Recognit. Lett. 27, 8 (2006), 861-874. [10] Fredric C Gey. 1994. Inferring probability of relevance using the method of logistic regression. In Proceedings of the 17th Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval. 222-231. [11] Bin Gu, Victor S. Sheng, KengYeow Tay, Walter Romano, and Shuo Li. 2015. Incremental Support Vector Learning for Ordinal Regression. IEEE Transactions on Neural networks and learning systems 26, 7 (2015), 1403-1416. [12] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. 2017. On Calibration of Modern Neural Networks. In Proceedings of the 34th International Conference on Machine Learning , Vol. 70. 1321-1330. [13] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. Deepfm: a factorization-machine based neural network for ctr prediction. In Proceedings of the 26th International Joint Conference on Artificial Intelligence . KDD '24, August 25-29, 2024, Barcelona, Spain Zhang and Liu, et al. Melbourne, Australia., 2782-2788. [14] Tong Guo, Xuanping Li, Haitao Yang, Xiao Liang, Yong Yuan, Jingyou Hou, Bingqing Ke, Chao Zhang, Junlin He, Shunyu Zhang, et al. 2023. Query-dominant User Interest Network for Large-Scale Search Ranking. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management . 629638. [15] Jui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip Pronin, Janani Padmanabhan, Giuseppe Ottaviano, and Linjun Yang. 2020. Embeddingbased retrieval in facebook search. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 2553-2561. [16] Kalervo J\u00e4rvelin and Jaana Kek\u00e4l\u00e4inen. 2002. Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems (TOIS) 20, 4 (2002), 422-446. [17] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. 2017. Lightgbm: A highly efficient gradient boosting decision tree. Advances in neural information processing systems 30 (2017). [18] Ananya Kumar, Percy S Liang, and Tengyu Ma. 2019. Verified uncertainty calibration. Advances in Neural Information Processing Systems 32 (2019). [19] Cheng Li, Yue Lu, Qiaozhu Mei, Dong Wang, and Sandeep Pandey. 2015. Clickthrough prediction for advertising in twitter timeline. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . 1959-1968. [20] Tie-Yan Liu et al. 2009. Learning to rank for information retrieval. Foundations and Trends\u00ae in Information Retrieval 3, 3 (2009), 225-331. [21] Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. 2015. Obtaining well calibrated probabilities using bayesian binning. In Proceedings of the AAAI conference on artificial intelligence , Vol. 29. [22] Feiyang Pan, Xiang Ao, Pingzhong Tang, Min Lu, Dapeng Liu, Lei Xiao, and Qing He. 2020. Field-aware calibration: a simple and empirically strong method for reliable probabilistic predictions. In Proceedings of The Web Conference 2020 . 729-739. [23] Rama Kumar Pasumarthi, Sebastian Bruch, Xuanhui Wang, Cheng Li, Michael Bendersky, Marc Najork, Jan Pfeifer, Nadav Golbandi, Rohan Anil, and Stephan Wolf. 2019. Tf-ranking: Scalable tensorflow library for learning-to-rank. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 2970-2978. [24] Qi Pi, Guorui Zhou, Yujing Zhang, Zhe Wang, Lejian Ren, Ying Fan, Xiaoqiang Zhu, and Kun Gai. 2020. Search-based user interest modeling with lifelong sequential behavior data for click-through rate prediction. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management . 2685-2692. [25] John Platt et al. 1999. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in large margin classifiers 10, 3 (1999), 61-74. [26] Tao Qin, Tie-Yan Liu, and Hang Li. 2010. A general approximation framework for direct optimization of information retrieval measures. Information retrieval 13 (2010), 375-397. [27] David Sculley. 2010. Combined regression and ranking. In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining . 979-988. [28] Xiang-Rong Sheng, Jingyue Gao, Yueyao Cheng, Siran Yang, Shuguang Han, Hongbo Deng, Yuning Jiang, Jian Xu, and Bo Zheng. 2023. Joint optimization of ranking and calibration with contextualized hybrid model. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 4813-4822. [29] Yukihiro Tagami, Shingo Ono, Koji Yamamoto, Koji Tsukamoto, and Akira Tajima. 2013. Ctr prediction for contextual advertising: Learning-to-rank approach. In Proceedings of the Seventh International Workshop on Data Mining for Online Advertising . 1-8. [30] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network for ad click predictions. In Proceedings of the ADKDD'17 . 1-7. [31] Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong, and Ed Chi. 2021. Dcn v2: Improved deep & cross network and practical lessons for web-scale learning to rank systems. In Proceedings of the web conference 2021 . 1785-1797. [32] Fen Xia, Tie-Yan Liu, Jue Wang, Wensheng Zhang, and Hang Li. 2008. Listwise approach to learning to rank: theory and algorithm. In Proceedings of the 25th international conference on Machine learning , Vol. 307. 1192-1199. [33] Le Yan, Zhen Qin, Xuanhui Wang, Mike Bendersky, and Marc Najork. 2022. Scale Calibration of Deep Ranking Models. In Proceedings of the 28th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . 4300-4309. [34] Zhao-Yu Zhang, Xiang-Rong Sheng, Yujing Zhang, Biye Jiang, Shuguang Han, Hongbo Deng, and Bo Zheng. 2022. Towards Understanding the Overfitting Phenomenon of Deep Click-Through Rate Models. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management . 2671-2680. [35] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through rate prediction. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . ACM, 1059-1068."}
