{"title": "SC-OTGM: SINGLE-CELL PERTURBATION MODELING BY SOLVING OPTIMAL MASS TRANSPORT ON THE MANIFOLD OF GAUSSIAN MIXTURES", "authors": "Andac Demir; Elizaveta Solovyeva; James Boylan; Mei Xiao; Fabrizio Serluca; Sebastian Hoersch; Jeremy Jenkins; Murthy Devarakonda; Bulent Kiziltan", "pub_date": "", "abstract": "Influenced by recent breakthroughs in Large Language Models (LLMs), single-cell foundation models are emerging. While these models demonstrate successful performance in cell type clustering, phenotype classification, and gene perturbation response prediction, it remains to be seen if a simpler model could achieve comparable or better results, especially with limited data. This is important, as the quantity and quality of single-cell data typically fall short of the standards in textual data used for training LLMs. Single-cell sequencing often suffers from technical artifacts, dropout events, and batch effects. These challenges are compounded in a weakly supervised setting, where the labels of cell states can be noisy, further complicating the analysis. To tackle these challenges, we present sc-OTGM, streamlined with less than 500K parameters, making it approximately 100x more compact than the foundation models, offering an efficient alternative. sc-OTGM is an unsupervised model grounded in the inductive bias that the scRNAseq data can be generated from a combination of the finite multivariate Gaussian distributions. The core function of sc-OTGM is to create a probabilistic latent space utilizing a Gaussian mixture model (GMM) as its prior distribution and distinguish between distinct cell populations by learning their respective marginal probability density functions (PDFs). It uses a Hit-and-Run Markov chain sampler to determine the optimal transport (OT) plan across these PDFs within the GMM framework. We evaluated our model against a CRISPR-mediated perturbation dataset, called CROP-seq, consisting of 57 one-gene perturbations. Our results demonstrate that sc-OTGM is effective in cell state classification, aids in the analysis of differential gene expression, and ranks genes for target identification through a recommender system. It also predicts the effects of single-gene perturbations on downstream gene regulation and generates synthetic scRNA-seq data conditioned on specific cell states.", "sections": [{"heading": "INTRODUCTION", "text": "The molecular mechanisms that drive diseases are complex, often reflected in the high-dimensional profiles of gene expression. Conducting detailed analyses of these gene expression matrices-across various cell types, disease states, and control versus experimental subjects-is essential to understand disease progression and identify targets for potential drug interventions. scRNA-seq technology facilitates the detailed profiling of transcriptomes across a vast range of cells, from thousands to millions, allowing for the exploration of cellular heterogeneity and the understanding of disease pathogenesis at the single-cell level.\nIn recent years, Perturb-seq has emerged as a powerful high-throughput method that combines CRISPR-based genetic perturbation with scRNA-seq Dixit et al. (2016); Adamson et al. (2016). It enables the analysis of how specific gene modulations impact gene expression across numerous individual cells. Within this framework, CRISPRi and CRISPRa allows for targeted downregulation and upregulation of gene expression, respectively, offering insights into gene interactions and regulatory networks at the granularity of single-cells.\nA significant hurdle in this research is that cellular responses to genetic perturbations are highly heterogeneous Elsasser (1984); Rubin (1990). This variability arises from differences in mRNA and protein levels Sonneveld et al. (2020), cell states Kramer et al. (2022), and the microenvironment among single-cells Snijder et al. (2009). Given the heterogeneity of potential perturbations, and the complexity of possible cell states, understanding the inherent data geometries and distributions of distinct cell populations becomes crucial for effective analyses. sc-OTGM employs a GMM to parametrize the marginal PDFs of diverse cell states and states within a reduced subspace. Not all subpopulations are well modeled by Gaussian distributions, and some subpopulations may cover large regions of feature space due to skewed class proportions and need further subdivisions. However, the use of GMM as priors enables the detection of local, high-density regions of phenotype space and effectively incorporates an inductive bias to overcome the limitations of data quantity and quality. This approach aligns with some recent methodologies in single-cell genomics, where Gaussian mixture priors are adopted to address the challenges of noise and data scarcity inherent in scRNA-seq datasets Wen et al. (2023); Xu et al. (2023); Gr\u00f8nbech et al. (2020); Slack et al. (2008).\nWhile mixture modeling is not a new concept in scRNA-seq data analysis, we break new ground from following perspective: First, sc-OTGM effectively learns the OT plan that facilitates the mapping from one cell population to another on the manifold of Gaussian mixtures. To capture the transformation from unperturbed to perturbed cell states, the model employs a Hit-and-Run Markov chain sampler. This sampler guarantees a globally optimal solution to generate samples from the target distribution and offers faster convergence than small-step random walks Smith (1996). Second, modeling the PDF of perturbations allows us to identify perturbed genes and predict changes in the expression of other genes following perturbation. Third, sc-OTGM provides a scalable and unified framework for cell state classification, differential gene expression analysis, gene ranking for target identification, perturbation response prediction, and the generation of synthetic scRNA-seq data by sampling from the posteriors of Gaussian components. It offers improved efficiency with reduced runtime and memory requirements compared to LLMs and Variational Autoencoder (VAE) variants, while maintaining competitive accuracy.", "publication_ref": ["b9", "b0", "b10", "b19", "b23", "b14", "b22", "b29", "b31", "b11", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "RELATED WORK", "text": "Recent advances in single-cell transcriptomics have led to the development of models such as Geneformer Theodoris et al. (2023), scGPT Cui et al. (2023), and scBERT Yang et al. (2022), which utilize masked language modeling (MLM) to learn gene embeddings. However, their effectiveness, particularly in zero-shot learning and in addressing batch effects, is still questioned Kedzierska et al. (2023); Boiarsky et al. (2023). Additionally, there have been significant developments in generative models, including Compositional Perturbation Autoencoder (CPA) Lotfollahi et al. (2021), which uses distinct encoder networks for cell states and perturbations, and further advancements by Lopez et al. (2023) and SAMS-VAE Bereket & Karaletsos (2023), which focus on disentangling representations with causal semantics for perturbation analysis. Tejada-Lapuerta et al. (2023) has criticized these approaches for oversimplifying complex causal relationships. Further details on these models and their comparative analyses can be found in the Appendix A.1.", "publication_ref": ["b25", "b6", "b32", "b13", "b4", "b17", "b16", "b3", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "DATASET", "text": "To evaluate the performance of sc-OTGM, we used the CROP-seq dataset from in-vitro experiments on human-induced pluripotent stem cell (iPSC)-derived neurons subjected to genetic perturbations Tian et al. (2021). These perturbations were executed via CRISPRi, enabling targeted gene knockdown to investigate its effects on neuronal survival and oxidative stress. Using the rank genes groups method from scanpy package Wolf et al. (2018) for differential expression analysis, we scrutinized the effects of knocking down 185 genes identified as potentially relevant to neuronal health and disease states. Of these, only 57 genes met our significance threshold (adjusted p-value less than 0.05), indicating a significant alteration in expression levels post-perturbation. These findings are summarized in Table 2, which includes the genes that presented significant differential expression, highlighting their potential roles in neuronal function and susceptibility to oxidative stress-a key factor in the pathogenesis of neurodegenerative diseases. Additional information regarding the pre-processing procedures can be found in Appendix A.4. Raw published data is available from the Gene Expression Omnibus under accession code GSE152988.\nz i x proj,i \u03c0 \u00b5 k b \u03a3 k b + e k h k * z pi N K K\nFigure 1: sc-OTGM represented as a generative graphical model.\nWe define the complete generative model for sc-OTGM as illustrated in Figure 1. Let X proj denote the gene expression matrix, with rows representing individual cells and columns representing features in a reduced-dimensional space. The gene expression profile of cell i is X proj, i . \u03c0 represents the cluster probabilities. Each \u03c0 k , where k specifies a particular cell state, indicates the prior probability of the k-th component in the mixture, subject to k \u03c0 k = 1. The latent variable z i \u2208 R K determining the component generating each data point x proj, i , is one-hot encoded and follows a categorical distribution parameterized by \u03c0. For each Gaussian component in the mixture model, \u00b5 k b \u2208 R m and \u03a3 k b \u2208 R m\u00d7m define the mean and covariance matrix for unperturbed cells of a specific cell type, respectively. We specify perturbations and heterogeneous cellular responses as multivariate Gaussiandistributed variables e \u223c N (\u00b5 ke , \u03a3 ke ) and h \u223c N (\u00b5 km , \u03a3 km ), respectively. We model perturbation as a dynamic system, where the cell outputs an impulse response function h, when presented with a brief perturbation signal e. The convolution of these variables represents the combined effect on the latent state z pi , which is also distributed as a multivariate Gaussian:\nz pi = (e k * h k )(z) = +\u221e -\u221e e k (\u03c4 )h k (z-\u03c4 )d\u03c4 \u223c N (\u00b5 ke +\u00b5 km , \u03a3 ke +\u03a3 km ) \u223c N (\u00b5 kp , \u03a3 kp ),(1)\nIn the proposed generative mixture model, the joint probability distribution for observed data X, and latent variables Z i , E, and H conditioned on \u03c0, \u00b5, \u03a3 is formulated as:\np(X, Z i , E, H | \u03c0, \u00b5, \u03a3) = N i=1 p(z i |\u03c0)p(x proj,i |z i , \u00b5, \u03a3) N i=1 e * h (2) = N i=1 K k=1 \u03c0 z ik k N (w bi ; \u00b5 k b , \u03a3 k b ) z ik N (w pi ; \u00b5 kp , \u03a3 kp ) ,(3)\nwhere Z \u2208 {0, 1} N \u00d7K denotes latent class indicators for N cells across K cell states, E \u2208 R N \u00d7m captures perturbation effects, and H \u2208 R N \u00d7m represents the cellular responses to these perturbations. Each datum w bi , w pi \u2208 R m is drawn independently and identically distributed (i.i.d.) from their respective marginal PDFs. To prevent numerical instability due to arithmetic underflow or overflow during likelihood calculations in the E-step of the Expectation-Maximization (EM) algorithm, we use the log-sum-exp trick. This method transforms the product of Gaussian probabilities into a sum, ensuring more stable computations. Let L denote p(X,\nZ i , E, H | \u03c0, \u00b5, \u03a3): log L = N i=1 K k=1 z ik log \u03c0 k + z ik log N (w bi ; \u00b5 k b , \u03a3 k b ) + log N (w pi ; \u00b5 kp , \u03a3 kp )(4)\nThe Maximum-a-Posteriori (MAP) parameter updates for the GMM via the EM algorithm are detailed in Algorithm 3. To address numerical instability in \u03a3's inversion due to its near singularity or nonpositive semi-definiteness, Tikhonov regularization is applied Alberti et al. (2021). See Appendix A.6 for additional details.", "publication_ref": ["b26", "b30", "b1"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "PLANNING OPTIMAL TRANSPORT VIA HIT-AND-RUN MARKOV CHAIN SAMPLER", "text": "OT problems, central to measuring the cost of optimally transporting mass from one distribution to another, are traditionally solved via the Monge (1781) and Kantorovich (1942) formulations, which, however, scale poorly for large datasets due to their reliance on linear programming (LP) Bunne et al. (2023). A breakthrough by Cuturi (2013) introduces entropic regularization into OT, resulting in the Sinkhorn algorithm, which significantly reduces computational complexity, enabling efficient largescale applications. This methodological advancement, detailed in Section A.2.3, represents a pivotal shift towards practical OT computation in machine learning. While OT is conventionally represented by a scalar value indicating the minimum cost required for such transport under specific constraints, sc-OTGM conceptualizes OT as a distribution to focus on the distribution of transportation costs and paths rather than summarizing these costs into a single scalar. A distribution-based approach encapsulates more information about the transport process, such as the variance of transport costs, providing not just the minimum cost but also how costs are distributed across different transport paths. Additionally, it provides a more robust measure of similarity between distributions, as it does not collapse the transport problem into a single metric but rather considers the entire cost landscape, potentially mitigating the influence of outliers.\nWe model the latent states of unperturbed and perturbed cells as Gaussian distributions, with unperturbed cells described by X = N (\u00b5 k b , \u03a3 k b ) and perturbed cells by Y = N (\u00b5 ky , \u03a3 ky ). MAP estimates for \u00b5 k b , \u03a3 k b , \u00b5 ky , \u03a3 ky were derived using the EM Algorithm within a GMM framework.\nTo quantify the perturbation effect, we introduce a distribution, Z = N (\u00b5 kp , \u03a3 kp ), resulting from the linear displacement between X and Y . Specifically, Z is characterized by\n\u00b5 kp = \u00b5 ky -\u00b5 k b and \u03a3 kp = \u03a3 k b + \u03a3 ky -2\u03a3 cross .\nZ captures the OT cost distribution required to transition between these states. The coupling (joint distribution) of X and Y is unknown, therefore we approximate \u03a3 cross via Hit-and-Run Markov Chain Monte Carlo (MCMC). This generates a Markov chain that, in its stable state, converges to the uniform distribution over a convex polytope van Valkenhoef & Tervonen (2015), and under certain regularity conditions, converges in distribution to the target distribution Smith (1996). The steps to compute \u03a3 cross are elaborated in Algorithm 1. For recursive updates of \u03a3 cross we use follow-the-leader (FTL) strategy which is prominently used in online density estimation and active learning Azoury & Warmuth (2001); Dasgupta & Hsu (2007). Details on the implementation and synthetic data experiments are provided in the Appendix A.8.\nsc-OTGM samples transportation paths (vectors) directly from the OT landscape, rooted in the perturbation distribution Z, within a dimensionally reduced subspace: x path \u223c N (\u00b5 kp , \u03a3 kp ), which effectively captures the essence of gene expression dynamics under perturbation. The high-dimensional gene expression profiles corresponding to these paths are reconstructed via inverse PCA, expressed as:\nx \u2032\u2032 path = x path V T m\n, where V m represents the matrix comprising the top m eigenvectors derived from the covariance matrix of the pre-processed gene expression data. See Appendix A.4 and A.5 for more details. For each gene expression profile within the OT landscape, we derive a parametric representation, as follows:\n\u00b5 gene,i = the i th element of (\u00b5 kp V T m ) and \u03c3 2 gene,i = the i th diagonal entry of (V m \u03a3 kp V T m ),\nwhere \u00b5 gene,i denotes the degree of change in a gene's expression in response to perturbation, indicating potential activation or suppression of the gene. \u03c3 gene,i quantifies the confidence in these expression changes, providing insights into the variability of our computations. While biological systems are often complex and nonlinear, under specific conditions or within certain ranges, linear approximations can provide valuable insights and simplify modeling efforts. van Someren et al. (2000) presents a methodology for modeling genetic networks that employs clustering to tackle the dimensionality problem and a linear model to infer the regulatory interactions. By analyzing the covariance matrix \u03a3 kp of the perturbation distribution, we can explore the interconnected behavior between genes, examining how they co-vary or influence each other. Consider the scenario where the expression level of gene i changes, denoted as \u2206X i . The covariance matrix \u03a3 kp , upon transformation via V m , becomes\n\u03a3 \u2032\u2032 kp = V m \u03a3 kp V T m .\nThe expected change in gene j's expression level, \u2206X j , resulting from a change in gene i, is linearly approximated as follows:\n\u2206X j = \u03a3 \u2032\u2032 kp,ij \u03a3 \u2032\u2032 kp,ii \u2206X i (5)\nAlgorithm 1 Estimation of Cross-Covariance Matrix via Hit-and-Run Markov Chain Monte Carlo 1: Input: Domains D X and D Y with non-zero density for X and Y , number of iterations N , mean vectors \u00b5 X , \u00b5 Y , covariance matrices \u03a3 X , \u03a3 Y , and confidence interval for the bounds \u03b1. 2: Output: Estimated cross-covariance matrix \u03a3 XY .\n3: Initialize (x (0) , y (0) ) uniformly from D X \u00d7 D Y . 4: Initialize \u03a3 (0)\nXY as a random matrix, ensuring it is symmetric and positive definite. 5: Compute the z-score, z, for the specified confidence interval \u03b1 using the inverse of the standard normal CDF:\nz = \u03a6 -1 1+\u03b1 2 . 6: for i = 1 to N do 7: Calculate non-zero density bounds [a X , b X ] for X: 8: Generate a random unit direction d X in X's space. 9: Normalize d X to unit length: d X,normalized = d X \u2225d X \u2225 10: Project x (i-1) onto d X,normalized : p X,projection = x (i-1) \u2022 d X,normalized 11: Compute standard deviation \u03c3 X,projection = d T X,normalized \u03a3 X d X,normalized 12: Determine [a X , b X ] using p X,projection \u00b1 z \u2022 \u03c3 X,projection 13: Calculate non-zero density bounds [a Y , b Y ] for Y : 14: Generate a random unit direction d Y in Y 's space. 15: Normalize d Y to unit length: d Y,normalized = d Y \u2225d Y \u2225 16: Project y (i-1) onto d Y,normalized : p Y,projection = y (i-1) \u2022 d Y,normalized 17: Compute standard deviation \u03c3 Y,projection = d T Y,normalized \u03a3 Y d Y,normalized 18: Determine [a Y , b Y ] using p Y,projection \u00b1 z \u2022 \u03c3 Y,projection 19: Update \u03a3 XY : 20: Sample x (i) \u223c Uniform(a X , b X ) and Sample y (i) \u223c Uniform(a Y , b Y ) 21: \u2206x (i) = x (i) -\u00b5 X , \u2206y (i) = y (i) -\u00b5 Y 22: \u03a3 (i) XY = i i+1 \u03a3 (i-1) XY + i (i+1) 2 \u2206x (i) (\u2206y (i) ) T\n\u25b7 FTL Online Density Estimation 23: end for", "publication_ref": ["b12", "b5", "b7", "b28", "b21", "b8", "b27"], "figure_ref": [], "table_ref": []}, {"heading": "RESULTS", "text": "Not all cells receiving the CRISPR components will have the target gene successfully knockeddown, and even among those that do, the degree of knockdown can vary substantially due to differences in Cas9 activity, guide RNA efficiency, and individual cell responses. Additionally, the downstream effects of knocking down a particular gene leads to compensatory mechanisms in the cell, altering the expression of other genes. This necessitates benchmarking computational models based on their accuracy to identify knockeddown genes and predict subsequent changes in the expression of other genes. We evaluated statistical models for differential gene expression in CRISPRi experiments, including the Mann-Whitney U test, t-test, and sc-OTGM, as shown in Table 1. We assessed how often each method placed the true knockeddown gene within the top-k results. Lower p-values from the Mann-Whitney U test and t-test correlate with higher rankings. sc-OTGM performs exceptionally well in Top-1 accuracy, showing its effectiveness in identifying the most likely perturbed gene. The performance advantage of sc-OTGM decreases as the ranking threshold increases. In our experiments, we investigated the impact of 57 single-gene knockdowns, featuring BIN1 as a case study in the paper. The dataset for this case study, comprising 382 cells (315 control and 67 target), was split into an 80-20 train-test ratio. The extremely limited size of the dataset presented significant challenges in training our model from scratch and fine-tuning existing foundation models for the CRISPRi experiment. Our results confirm that sc-OTGM accurately predicts the direction of expression level changes among DEGs, distinguishing between upregulated and downregulated genes following BIN1 knockdown. However, the differences between the magnitudes of model predictions and actual values highlight the potential advantages of nonlinear models in capturing the complex dynamics of gene regulation. Furthermore, based on sc-OTGM's ranking, a cutoff of 100 is set to select the top predicted DEGs.\nPredicted DEGs are obtained from the top of the ranked gene list, and compared against the list of known DEGs. We conducted Fisher's exact test which yielded a p-Value of 6.39e-08, significantly below the standard threshold of 0.05. This confirms a strong statistical correlation between known DEGs after BIN1 knockdown and those predicted by sc-OTGM. While this analysis focuses on BIN1, Table 3 presents results for all targeted gene knockdowns, showing p-Values well below 0.05, validating the performance on DEG enrichment. Additionally, sc-OTGM demonstrated a mean accuracy of 75.2% \u00b1 15.4% and F1-score of 0.79 \u00b1 0.14, predicting the directionality of expression changes (upregulation or downregulation) in DEGs.  scGPT introduces a variant of masked language modeling (MLM) that mimics the auto-regressive generation in natural language processing, where the masked genes are iteratively predicted according to the model's confidence. Geneformer completely abandons the precise expression levels of genes. Instead, it models the rank of gene expressions and constructs sequences of genes according to their relative expression levels within each cell. The assessment of Geneformer and scGPT revealed significant limitations in their zero-shot performance Kedzierska et al. (2023). In various tasks, particularly in cell type annotation, these models are outperformed by simpler models, such as scVI Lopez et al. (2018) and strategies focusing on highly variable genes (HVGs). Kedzierska et al. ( 2023) also highlighted that aligning the pretraining dataset's tissue of origin with the target task did not consistently improve scGPT's performance, and broader pretraining datasets sometimes resulted in decreased effectiveness. Additionally, both Geneformer and scGPT showed inadequate handling of batch effects in zero-shot settings. Comparisons of scBERT and scGPT with L1-regularized logistic regression in cell type annotation under limited training data suggest that logistic regression performs more accurately, questioning the complexity needed for cell type annotation and the efficacy of MLM in learning gene embeddings Boiarsky et al. (2023). The failure of these models to accurately predict gene expression in zero-shot or limited data scenarios emphasizes the necessity for advancements in model design and training methodologies. CellPLM aggregates gene embeddings since gene expressions are bag-of-word features and leverages spatially-resolved transcriptomic (SRT) data in pre-training to facilitate learning cell-cell relations and introduce a Gaussian mixture prior as an additional inductive bias to overcome data limitation Wen et al. (2023).\nThe field has also seen advancements in generative modeling. Compositional Perturbation Autoencoder (CPA) has been developed to learn embeddings for both cell states and perturbations Lotfollahi et al. (2021). This is achieved within a unified framework, utilizing input data comprising two main components: gene expression data and perturbation data, such as drug types and dosages. CPA employs distinct encoder networks for cell states and perturbations, respectively. These encoders map input data into a latent space, from which the decoder network reconstructs the expected cellular response to a specific perturbation. Building on that concept, Lopez et al. (2023) leverages sparse mechanism shift assumption in order to learn disentangled representations with a causal semantic to the analysis of single-cell genomics data with genetic or chemical perturbations. However, in single-cell transcriptomics, such a setting might be overly simplistic. Similarly, SAMS-VAE adopts the idea to disentangle cellular latent spaces into basal and perturbation latent variables Bereket & Karaletsos (2023). Specifically, the proposed method models the latent state of perturbed samples as a combination of a local latent variable capturing sample-specific variation and sparse global variables of latent intervention. These global variables are formulated as the point-wise product between latent perturbation variables and a binary mask. Tejada-Lapuerta et al. ( 2023) argues that disentanglement simplifies the problem of recovering meaningful causal representations assuming independence among latent variables by the use of mean field approximation to ease the computation of variational inference.", "publication_ref": ["b13", "b15", "b4", "b29", "b17", "b16", "b3"], "figure_ref": [], "table_ref": ["tab_0", "tab_3"]}, {"heading": "A.2 OPTIMAL TRANSPORT", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.2.1 MONGE'S FORMULATION", "text": "Monge's formulation of the OT problem seeks a mapping T : X \u2192 Y that transports a mass distribution \u00b5 on a space X to a mass distribution \u03bd on a space Y in the most cost-effective way. The cost of transporting a unit mass from point x \u2208 X to point y \u2208 Y is given by a cost function c(x, y).\nThe goal is to minimize the total transportation cost:\nmin T X c(x, T (x)) d\u00b5(x)\nsubject to the constraint that T # \u00b5 = \u03bd, where T # \u00b5 is the pushforward measure of \u00b5 by T , ensuring that the mass distribution after transportation is \u03bd.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.2.2 KANTOROVICH'S RELAXATION", "text": "Kantorovich's formulation relaxes Monge's problem by considering a probabilistic coupling \u03c0 in the product space X \u00d7 Y , which represents a joint distribution of source and target points that respects the marginal distributions \u00b5 and \u03bd. The problem is formulated as:\nmin \u03c0\u2208\u03a0(\u00b5,\u03bd) X\u00d7Y c(x, y) d\u03c0(x, y)\nwhere \u03a0(\u00b5, \u03bd) is the set of all couplings \u03c0 with marginals \u00b5 on X and \u03bd on Y . This formulation is more flexible than Monge's because it allows for mass splitting, making it possible to find solutions in situations where Monge's problem has none.\nThe Kantorovich problem leads to a dual formulation that expresses the OT cost as:\nsup (f,g)\u2208\u03a6 X f (x) d\u00b5(x) + Y g(y) d\u03bd(y)\nwhere \u03a6 consists of all pairs of functions (f, g) such that f (x)+g(y) \u2264 c(x, y) for all (x, y) \u2208 X \u00d7Y .\nThe p-Wasserstein distance between \u00b5 and \u03bd for p \u2265 1 is derived from Kantorovich's problem with the cost function c(x, y) = \u2225x -y\u2225 p , providing a metric on the space of probability measures:\nW p (\u00b5, \u03bd) = min \u03c0\u2208\u03a0(\u00b5,\u03bd) X\u00d7Y \u2225x -y\u2225 p d\u03c0(x, y) 1/p\nThis distance measures the minimal amount of work required to transform the distribution \u00b5 into the distribution \u03bd under the given cost function.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.2.3 REGULARIZED TRANSPORT WITH SINKHORN'S ALGORITHM", "text": "The Sinkhorn distance introduces an entropic regularization to the OT problem, making it computationally more tractable by allowing the use of efficient algorithms. The entropic regularization term, \u03f5 KL(\u03c0\u2225\u00b5 \u2297 \u03bd), where \u03f5 is a positive regularization parameter and KL denotes the Kullback-Leibler divergence between \u03c0 and the product distribution \u00b5 \u2297 \u03bd, adds strong convexity to the optimization problem. This convexity ensures that the smoothness of the objective function allows for the application of gradient-based optimization methods that are known to converge quickly.   ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.4 PRE-PROCESSING", "text": "Cell Filtering Cells expressing fewer than \u03b8 g genes are excluded. Formally, cell i is removed if its gene count j 1 {Xij >0} is less than \u03b8 g , where X ij denotes the expression level of gene j in cell i, and 1 represents the indicator function.\nGene Filtering Similarly, genes expressed in fewer than \u03b8 c cells are discarded. A gene j is eliminated if i 1 {Xij >0} falls below \u03b8 c .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Normalization and Logarithmic Scaling", "text": "The gene expression dataset undergoes normalization to equalize expression levels across cells, parameterized by counts per cell after. Postnormalization, logarithmic scaling is applied:\nX \u2032 ij = log(1 + X ij )\n, where X ij represents the normalized expression of gene j in cell i.\nIdentification of Highly Variable Genes Genes are deemed highly variable based on their mean expression \u00b5 j and dispersion \u03c3 2 j , constrained within specified thresholds. A gene j qualifies as highly variable if it meets the criteria \u03b8 \u00b5,min < \u00b5 j < \u03b8 \u00b5,max and \u03c3 2 j > \u03b8 \u03c3 . Scaling Subsequently, gene expression levels are standardized, ensuring zero mean and unit variance for each gene:\nX \u2032 ij = Xij -Xj \u03c3j\n, where X j and \u03c3 j denote the mean and standard deviation of gene j's expression, respectively.\nFinal Steps After the selection of highly variable genes, further scaling is applied:\nX \u2032\u2032 ij = min(max(X \u2032 ij , -\u03b8 max ), \u03b8 max ).\nThis step ensures that the data range remains within the predefined limits, specified by \u03b8 max .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.5 PROJECTION TO REDUCED SUBSPACE", "text": "In downstream analysis, Principal Component Analysis (PCA) is utilized to reduce the dimensionality of a gene expression matrix, X \u2032\u2032 \u2208 R N \u00d7d , comprising N cells and d genes. This process involves the eigendecomposition of the covariance matrix \u03a3 of pre-processed data X \u2032\u2032 , to compute eigenvectors v i and their corresponding eigenvalues \u03bb i , satisfying the equation \u03a3v i = \u03bb i v i . The top m eigenvectors, selected based on the magnitude of their eigenvalues, are represented as a matrix V m \u2208 R d\u00d7m . The data X \u2032\u2032 is then projected onto the lower-dimensional subspace defined by V m , resulting in the projected data X proj = X \u2032\u2032 V m , where X proj \u2208 R N \u00d7m and m \u226a d.\nAlgorithm 2 K-S Test for Each Principal Component of a High-Dimensional Sample against a Gaussian Reference Distribution 1: Null Hypothesis H 0 : The data for principal component i follows the reference distribution f (x).\n2: Significance Level \u03b1: Threshold (\u03b1 = 0.05) to decide on the null hypothesis based on the K-S statistic. 3: for each principal component i in x proj \u2208 R N \u00d7m do 4: Extract all data points in dimension i to form S i = {x 1,i proj , x 2,i proj , . . . , x N,i proj }", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "5:", "text": "Compute mean \u00b5 i and variance \u03c3 2 i of S i 6:\nDefine the Gaussian reference distribution f (x) = N (x; \u00b5 i , \u03c3 2 i )\n7:\nSort S i in ascending order 8:\nfor j = 1 to N do 9:\nF emp (x j,i proj ) = j N \u25b7 Empirical Cumulative Distribution Function (ECDF) 10: F ref (x j,i proj ) = x j,i proj -\u221e f (x) dx \u25b7 CDF of the Reference Gaussian Distribution 11: D j,i = |F emp (x j,i proj ) -F ref (x j,i proj )| \u25b7 K-S Statistic 12:\nend for 13:\nD i = max j D j,i 14:\nif D i is greater than the critical value at significance \u03b1 for dimension i then end if 19: end for To determine if GMM is appropriate to model the scRNA-seq data, we apply Kolmogorov-Smirnov (K-S) Test to all principal components as detailed in Algorithm 2. If the K-S test suggests that the data is not drawn from a single Gaussian, this can be an initial hint (though not definitive proof) that a GMM might be suitable. Although this approach may not capture the true multivariate nature of the data or account for inter-feature dependencies, it is useful in identifying the presence of multiple modes or clusters in the data.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.6 MAP PARAMETER ESTIMATION", "text": "Algorithm 3 MAP Parameter Estimation for the GMM via the EM Algorithm 1: Input: {k, x}. k denotes the number of Gaussian components. x \u2208 R N \u00d7m denotes N samples, each with m dimensions after PCA. 2: Output: {\u03c0 1:k , \u00b5 1:k , \u03a3 1:k } MAP parameter estimates, where \u03c0 k is the prior probability, \u00b5 k \u2208 R m is the mean vector, and \u03a3 k \u2208 R m\u00d7m is the covariance matrix of component k.\n3: Initialize \u03c0 i , \u00b5 i , \u03a3 i for i = 1 : k. 4: repeat 5:\nfor i = 1 : k do \u25b7 Iterate through each Gaussian component in the mixture.\n6:\nfor j = 1 : N do \u25b7 Calculate log-likelihood & posterior for each sample.\n7:\nlog P (x j | \u03c0 i ) = -m 2 log(2\u03c0) -1 2 log |\u03a3 i | -1 2 (x j -\u00b5 i ) T \u03a3 -1 i (x j -\u00b5 i ) 8:\na i,j = log P (x j | \u03c0 i ) + log P (\u03c0 i )\n9:\nlog P (x j ) = log k l=1 exp(a l,j -max l a l,j ) + max l a l,j \u25b7 Log-Sum-Exp trick 10:\nP (\u03c0 i | x j ) = exp(a i,j -log P (x j )) \u25b7 E-Step 11:\nend for 12:\n\u00b5 i = N j=1 P (\u03c0i|xj )xj N j=1 P (\u03c0i|xj )\n13:\n\u03a3 i = N j=1 P (\u03c0i|xj )(xj -\u00b5i)(xj -\u00b5i) T N j=1 P (\u03c0i|xj )\n\u25b7 M-Step 14:\n\u03c0 i = 1 N N j=1 P (\u03c0 i | x j )\n\u25b7 Update the priors for each component 15:\n\u03b1 = 0.01 \u00d7 mean(diag(\u03a3 i ))\n16:\n\u03a3 i = \u03a3 i + \u03b1I \u25b7 Tikhonov regularization 17:\nend for 18: until Convergence or maximum number of EM iterations We introduce a bias to stabilize the covariance matrix by adding a scaled identity matrix:\n\u03a3 = \u03a3 + \u03b1I (6)\nwhere \u03b1 is a small positive regularization parameter and I is the identity matrix of the same dimension as \u03a3. The choice \u03b1 = 0.01 \u00d7 mean(diag(\u03a3 i )) is a heuristic wherein the covariance matrix is regularized by adding 1% of its average variance to its diagonal. This represents an arbitrary yet small perturbation. Benefits of covariance regularization include:\n\u2022 Invertibility: When the number of dimensions is close to or exceeds the number of data points, \u03a3 might be singular. Regularization ensures its invertibility. \u2022 Stability: For ill-conditioned matrices, their inversion can be highly sensitive to slight changes in the data. Regularization stabilizes the inversion process. The condition number is a commonly used measure to gauge the stability of a matrix, especially when it comes to measuring how a matrix will amplify errors in problems involving matrix inversion. The condition number of a matrix A in terms of its norm is defined as:\n\u03ba(A) = \u2225A\u2225 \u2022 \u2225A -1 \u2225\nFor the 2-norm (or Euclidean norm), it can be expressed in terms of the singular values \u03c3 max and \u03c3 min of A as: \u03ba 2 (A) = \u03c3 max \u03c3 min Given a linear system Ax = b: If \u03ba(A) \u2248 1, then A is well-conditioned. For small relative changes in b or \u03b4A, the corresponding changes in the solution x are also small. If \u03ba(A) \u226b 1, then A is ill-conditioned. Minor relative perturbations in b or \u03b4A can lead to significant variations in the solution x.\n\u2022 Eigenvalue Shrinkage: The regularization effectively increases each eigenvalue of \u03a3 by \u03b1, beneficial when there is a need to dampen the influence of high variance variables.  We implemented this code to simulate the performance of Hit-and-Run sampling in estimating the cross-covariance matrix between two distributions. Two sets of parameters (means and covariances) for two different multivariate normal distributions are used to generate two sets of samples: base samples and noise. base samples are generated using the first set of parameters (mean1, cov1) by sampling from the corresponding multivariate normal distribution. Similarly, noise is generated using the second set of parameters (mean2, cov2). To create a correlation between these two sets of samples, the function uses a correlation factor. This factor determines how much of the final synthetic data (correlated samples) will be influenced by the base samples versus the noise. This introduces a controlled amount of dependence between the two sets of samples, simulating correlated data. By controlling the correlation factor, the code can simulate different degrees of correlation between the two sets of data. This synthetic data is then used in the code to evaluate the performance of the Hit-and-Run sampler in estimating the cross-covariance of correlated distributions. It evaluates the accuracy of the estimation by comparing it to a ground truth and measures performance using Root Mean Square Error (RMSE) and Frobenius norm.\n# Calculate the variance of the projection variance_projection = np.dot( direction_normalized.T, np.dot(cov, direction_normalized) ) # Calculate the standard deviation of the projection std_deviation = np.sqrt(variance_projection) # Find the z-scores for the specified confidence interval z_score = norm.ppf((1 + confidence) / 2)   ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Source code and documentation are available at: https://github.com/Novartis/scOTGM.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "import numpy as np from scipy.stats import multivariate_normal, norm import matplotlib.pyplot as plt def generate_random_params(dimension: int) -> tuple:\n\"\"\" Generate random means and covariances for a given dimension.\nArgs: dimension (int): The dimensionality of the mean and covariance.\nReturns: tuple: A tuple containing the mean vector and covariance matrix.   ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "A multiplexed single-cell crispr screening platform enables systematic dissection of the unfolded protein response", "journal": "Cell", "year": "2016", "authors": "Britt Adamson; Marco Thomas M Norman; Min Y Jost; James K Cho; Yuwen Nu\u00f1ez; Jacqueline E Chen; Luke A Villalta; Max A Gilbert; Marco Y Horlbeck;  Hein"}, {"ref_id": "b1", "title": "Learning the optimal tikhonov regularizer for inverse problems", "journal": "Advances in Neural Information Processing Systems", "year": "2021", "authors": "Ernesto De Giovanni S Alberti; Matti Vito; Luca Lassas; Matteo Ratti;  Santacesaria"}, {"ref_id": "b2", "title": "Relative loss bounds for on-line density estimation with the exponential family of distributions", "journal": "Machine learning", "year": "2001", "authors": "S Katy; Manfred K Azoury;  Warmuth"}, {"ref_id": "b3", "title": "Modelling cellular perturbations with the sparse additive mechanism shift variational autoencoder", "journal": "", "year": "2023", "authors": "Michael Bereket; Theofanis Karaletsos"}, {"ref_id": "b4", "title": "A deep dive into single-cell rna sequencing foundation models", "journal": "bioRxiv", "year": "2023", "authors": "Rebecca Boiarsky; M Nalini; Alejandro Singh; Gad Buendia; David Getz;  Sontag"}, {"ref_id": "b5", "title": "Learning single-cell perturbation responses using neural optimal transport", "journal": "Nature Methods", "year": "2023", "authors": "Charlotte Bunne; Stefan G Stark; Gabriele Gut; Jacobo Sarabia; Del Castillo; Mitch Levesque; Kjong-Van Lehmann; Lucas Pelkmans; Andreas Krause; Gunnar R\u00e4tsch"}, {"ref_id": "b6", "title": "scgpt: towards building a foundation model for single-cell multi-omics using generative ai", "journal": "bioRxiv", "year": "2023", "authors": "Haotian Cui; Chloe Wang; Hassaan Maan; Kuan Pang; Fengning Luo; Bo Wang"}, {"ref_id": "b7", "title": "Sinkhorn distances: Lightspeed computation of optimal transport", "journal": "Advances in neural information processing systems", "year": "2013", "authors": "Marco Cuturi"}, {"ref_id": "b8", "title": "On-line estimation with the multivariate gaussian distribution", "journal": "Springer", "year": "2007", "authors": "Sanjoy Dasgupta; Daniel Hsu"}, {"ref_id": "b9", "title": "Perturb-seq: dissecting molecular circuits with scalable single-cell rna profiling of pooled genetic screens", "journal": "cell", "year": "2016", "authors": "Atray Dixit; Oren Parnas; Biyu Li; Jenny Chen; Charles P Fulco; Livnat Jerby-Arnon; Nemanja D Marjanovic; Danielle Dionne; Tyler Burks; Raktima Raychowdhury"}, {"ref_id": "b10", "title": "Outline of a theory of cellular heterogeneity", "journal": "Proceedings of the National Academy of Sciences", "year": "1984", "authors": "M Walter;  Elsasser"}, {"ref_id": "b11", "title": "scvae: variational auto-encoders for single-cell gene expression data", "journal": "Bioinformatics", "year": "2020", "authors": "Maximillian Fornitz Christopher Heje Gr\u00f8nbech; Pascal N Vording; Casper Kaae Timshel; Tune H S\u00f8nderby; Ole Pers;  Winther"}, {"ref_id": "b12", "title": "On the transfer of masses (in russian)", "journal": "Doklady Akademii Nauk", "year": "1942", "authors": " Kantorovich"}, {"ref_id": "b13", "title": "Assessing the limits of zero-shot foundation models in single-cell biology", "journal": "bioRxiv", "year": "2023", "authors": "Zofia Kasia; Lorin Kedzierska; Ava Pardis Crawford; Alex X Amini;  Lu"}, {"ref_id": "b14", "title": "Multimodal perception links cellular state to decision-making in single cells", "journal": "Science", "year": "2022", "authors": "Bernhard A Kramer; Jacobo Sarabia Del Castillo; Lucas Pelkmans"}, {"ref_id": "b15", "title": "Deep generative modeling for single-cell transcriptomics", "journal": "Nature methods", "year": "2018", "authors": "Romain Lopez; Jeffrey Regier; Michael I Michael B Cole; Nir Jordan;  Yosef"}, {"ref_id": "b16", "title": "Learning causal representations of single cells via sparse mechanism shift modeling", "journal": "PMLR", "year": "2023", "authors": "Romain Lopez; Natasa Tagasovska; Stephen Ra; Kyunghyun Cho; Jonathan Pritchard; Aviv Regev"}, {"ref_id": "b17", "title": "Compositional perturbation autoencoder for single-cell response modeling", "journal": "BioRxiv", "year": "2021", "authors": "Anna Klimovskaia Lotfollahi; Carlo De Susmelj; Yuge Donno; Ignacio L Ji; Alexander Ibarra; Nafissa Wolf; Fabian J Yakubova; David Theis;  Lopez-Paz"}, {"ref_id": "b18", "title": "M\u00e9moire sur la th\u00e9orie des d\u00e9blais et des remblais", "journal": "Mem. Math. Phys. Acad. Royale Sci", "year": "1781", "authors": " Gaspard Monge"}, {"ref_id": "b19", "title": "The significance of biological heterogeneity", "journal": "Cancer and Metastasis Reviews", "year": "1990", "authors": "Harry Rubin"}, {"ref_id": "b20", "title": "Characterizing heterogeneous cellular responses to perturbations", "journal": "Proceedings of the National Academy of Sciences", "year": "2008", "authors": "Elisabeth D Michael D Slack; Lani F Martinez; Steven J Wu;  Altschuler"}, {"ref_id": "b21", "title": "The hit-and-run sampler: a globally reaching markov chain sampler for generating arbitrary multivariate distributions", "journal": "", "year": "1996", "authors": "L Robert;  Smith"}, {"ref_id": "b22", "title": "Population context determines cell-to-cell variability in endocytosis and virus infection", "journal": "Nature", "year": "2009", "authors": "Berend Snijder; Raphael Sacher; Pauli R\u00e4m\u00f6; Eva-Maria Damm; Prisca Liberali; Lucas Pelkmans"}, {"ref_id": "b23", "title": "Heterogeneity in mrna translation", "journal": "Trends in cell biology", "year": "2020", "authors": "Stijn Sonneveld; M P Bram; Marvin E Verhagen;  Tanenbaum"}, {"ref_id": "b24", "title": "Causal machine learning for single-cell genomics", "journal": "", "year": "2023", "authors": "Alejandro Tejada-Lapuerta; Paul Bertin; Stefan Bauer; Hananeh Aliee; Yoshua Bengio; Fabian J Theis"}, {"ref_id": "b25", "title": "Transfer learning enables predictions in network biology", "journal": "Nature", "year": "2023", "authors": "Christina V Theodoris; Ling Xiao; Anant Chopra; Zeina R Al Mark D Chaffin; Matthew C Sayed; Helene Hill; Elizabeth M Mantineo; Zexian Brydon; X Zeng;  Shirley Liu"}, {"ref_id": "b26", "title": "Genome-wide crispri/a screens in human neurons link lysosomal failure to ferroptosis", "journal": "Nature neuroscience", "year": "2021", "authors": "Ruilin Tian; Anthony Abarientos; Jason Hong; Sayed Hadi Hashemi; Rui Yan; Nina Dr\u00e4ger; Kun Leng; Mike A Nalls; Andrew B Singleton; Ke Xu"}, {"ref_id": "b27", "title": "Linear modeling of genetic networks from experimental data", "journal": "Citeseer", "year": "2000", "authors": "Eugene P Van Someren; Lodewyk Fa Wessels; Marcel Jt Reinders"}, {"ref_id": "b28", "title": "hitandrun:\" hit and run\" and\" shake and bake\" for sampling uniformly from convex shapes", "journal": "CRAN", "year": "2015", "authors": "G Van Valkenhoef; T Tervonen"}, {"ref_id": "b29", "title": "Cellplm: Pre-training of cell language model beyond single cells", "journal": "bioRxiv", "year": "2023", "authors": "Hongzhi Wen; Wenzhuo Tang; Xinnan Dai; Jiayuan Ding; Wei Jin; Yuying Xie; Jiliang Tang"}, {"ref_id": "b30", "title": "Scanpy: large-scale single-cell gene expression data analysis", "journal": "Genome biology", "year": "2018", "authors": "Alexander Wolf; Philipp Angerer; Fabian J Theis"}, {"ref_id": "b31", "title": "Graph embedding and gaussian mixture variational autoencoder network for end-to-end analysis of single-cell rna sequencing data", "journal": "Cell Reports methods", "year": "2023", "authors": "Junlin Xu; Jielin Xu; Yajie Meng; Changcheng Lu; Lijun Cai; Xiangxiang Zeng; Ruth Nussinov; Feixiong Cheng"}, {"ref_id": "b32", "title": "scbert as a large-scale pretrained deep language model for cell type annotation of single-cell rna-seq data", "journal": "Nature Machine Intelligence", "year": "2022", "authors": "Fan Yang; Wenchuan Wang; Fang Wang; Yuan Fang; Duyu Tang; Junzhou Huang; Hui Lu; Jianhua Yao"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure2shows both sc-OTGM's perturbation distribution and a confusion matrix for cell state classification. Here, clustering around the center suggests minimal impact on most genes, whereas known differentially expressed genes (DEGs) (blue points) significantly diverge. sc-OTGM accurately ranked BIN1 as a knockeddown gene, placing it at the top of the recommendation list. In the presented confusion matrix, cells are classified as either Control-indicating no CRISPR-induced changes-or Perturbed-where changes are expected. True negatives (56) correctly identify cells as Control; false positives (18) erroneously label cells as Perturbed; false negatives (5) miss the classification of Perturbed cells as Control; and true positives (8) correctly detect Perturbed cells. Additionally, we used in silico perturbation response predictions to analyze gene expression changes between BIN1-knockdown and control cells, as shown in Figure3.", "figure_data": ""}, {"figure_label": "23", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :Figure 3 :23Figure 2: Perturbation distribution from the OT landscape, and ranking genes for target identification through a recommender system.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "min \u03c0\u2208\u03a0(\u00b5,\u03bd) X\u00d7Y c(x, y) d\u03c0(x, y) + \u03f5 KL(\u03c0\u2225\u00b5 \u2297 \u03bd) A.3 DATASET STATISTICS", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "for i in range(1, num_iterations + 1):# Hit-and-Run sampling for X direction_x = np.random.randn(len(mean1)) lb_x, ub_x = get_non_zero_density_bounds( cov1, x_current, direction_x ) x_current = np.random.uniform(lb_x, ub_x) # Hit-and-Run sampling for Y direction_y = np.random.randn(len(mean2)) lb_y, ub_y = get_non_zero_density_bounds( cov2, y_current, direction_y ) y_current = np.random.uniform(lb_y, ub_y) # Sequential update on cross-covariance matrix deviation_x = x_current -mean1 deviation_y = y_current -mean2 cross_covariance = (i / (i + 1)) * cross_covariance + ( i / (i + 1) ** 2 ) * np.outer(deviation_x, deviation_y) # Update rmse values and frobenius_norms rmse = np.sqrt(np.mean((cross_covariance -ground_truth) ** 2)) rmse_values.append(rmse) fro_norm = np.linalg.norm(cross_covariance -ground_truth, \"fro\") normalized_fro_norm = fro_norm / np.sqrt((dimension ** 2)) frobenius_norms.append(normalized_fro_norm) return cross_covariance, rmse_values, frobenius_norms cross-covariance for a given dimension with an imposed correlation factor. Args: dimension (int): The dimensionality of the distribution. num_iterations (int, optional): Number of iter. for sampling. Default is 1000. correlation_factor (float, optional): Factor to impose correlation between the two distributions. Default is 0. Returns: list: List of RMSE values over iterations and list of Frobenius norms for iterations. \"\"\" mean1, cov1 = generate_random_params(dimension) mean2, cov2 = generate_random_params(dimension) # Generate correlated samples _, _, correlated_samples = generate_non_independent_data( mean1, cov1, mean2, cov2, num_iterations, correlation_factor ) # Calculate the ground truth cross-covariance matrix ground_truth = np.cov(correlated_samples, rowvar=False) _, rmse, frobenius_norms = estimate_cross_covariance( mean1, cov1, mean2, cov2, ground_truth, num_iterations ) return rmse, frobenius_norms if __name__ == \"__main__\": dimensions = [100] num_iterations = 1000correlation_factors = [0.0, 0.1, 0.2, 0.5] # Dictionary to store results for each correlation factor all_results = {} # Compute results for each correlation factor and dimension for factor in correlation_factors: all_results[factor] = { dim: main(dim, num_iterations, factor) for dim in dimensions } plt.figure(figsize=(16, 5)) lines = [] # To store line objects for the legend labels = [] # To store label strings for the legend for i, factor in enumerate(correlation_factors): plt.subplot(2, 4, i + 1) # Retrieve results for this correlation factor results = all_results[factor]", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Benchmarking differential gene expression analysis techniques. The highest performance is highlighted in bold, the best baseline method is underlined. . Expression Analysis Top-1 Acc. \uf8e6 Top-5 Acc. \uf8e6 Top-10 Acc. \uf8e6 Top-50 Acc. \uf8e6 Top-100 Acc. \uf8e6", "figure_data": "DiffMann-Whitney U test0.370.400.420.580.60t-test0.390.670.700.790.86sc-OTGM0.560.680.740.820.91"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Quantitative Analysis of Gene Expression Alterations Post-CRISPRi Knockdown", "figure_data": "Genelog 2 (fold change)p-ValueAdjusted p-ValueNum. Control CellsNum. Targeted CellsTUBB4A-5.601.30 \u00d7 10 -29 1.06 \u00d7 10 -25 31786ATP1A3-5.571.11 \u00d7 10 -25 9.08 \u00d7 10 -22 35487KIFAP3-5.341.50 \u00d7 10 -26 1.23 \u00d7 10 -22 358100MAPT-5.132.26 \u00d7 10 -23 1.84 \u00d7 10 -19 368101CASP3-4.731.53 \u00d7 10 -20 1.25 \u00d7 10 -16 30783APEX1-4.728.08 \u00d7 10 -18 6.60 \u00d7 10 -14 34676COX10-4.565.61 \u00d7 10 -14 3.28 \u00d7 10 -10 9755NDUFS8-4.521.57 \u00d7 10 -15 1.29 \u00d7 10 -11 35273ZNF292-4.364.91 \u00d7 10 -13 4.01 \u00d7 10 -933960Continued on next page"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Continued from previous page", "figure_data": "Genelog 2 (fold change)p-ValueAdjusted p-ValueNum. Control CellsNum. Targeted CellsGSTA4-4.123.74 \u00d7 10 -14 3.06 \u00d7 10 -10 33773STX1B-4.013.34 \u00d7 10 -16 2.72 \u00d7 10 -12 23572OPTN-3.954.06 \u00d7 10 -16 3.32 \u00d7 10 -12 33888SOD1-3.843.71 \u00d7 10 -15 3.03 \u00d7 10 -11 36485NDUFV1-3.728.89 \u00d7 10 -11 5.80 \u00d7 10 -732062CALB1-3.624.02 \u00d7 10 -78.21 \u00d7 10 -45064EEF2-3.602.61 \u00d7 10 -12 5.32 \u00d7 10 -936763BIN1-3.573.12 \u00d7 10 -11 2.55 \u00d7 10 -731567SCFD1-3.562.38 \u00d7 10 -63.60 \u00d7 10 -424932PON2-3.506.35 \u00d7 10 -11 5.18 \u00d7 10 -79974BAX-3.453.23 \u00d7 10 -14 2.63 \u00d7 10 -10 23086SCAPER-3.071.37 \u00d7 10 -91.12 \u00d7 10 -531171CYB561-3.061.15 \u00d7 10 -51.57 \u00d7 10 -313832AKAP9-3.001.13 \u00d7 10 -99.21 \u00d7 10 -636679VPS35-2.971.38 \u00d7 10 -83.76 \u00d7 10 -533465PRNP-2.928.65 \u00d7 10 -92.35 \u00d7 10 -525372AP2A2-2.872.51 \u00d7 10 -11 2.05 \u00d7 10 -733299SOD2-2.806.16 \u00d7 10 -85.03 \u00d7 10 -518861BECN1-2.797.42 \u00d7 10 -51.89 \u00d7 10 -214827SNCB-2.713.35 \u00d7 10 -89.12 \u00d7 10 -514468CDH11-2.711.55 \u00d7 10 -61.58 \u00d7 10 -39761ELOVL5-2.661.36 \u00d7 10 -91.11 \u00d7 10 -517392NTRK2-2.662.21 \u00d7 10 -41.62 \u00d7 10 -223731DAP-2.658.90 \u00d7 10 -83.63 \u00d7 10 -415670EIF4G1-2.491.17 \u00d7 10 -61.63 \u00d7 10 -328168TRPM7-2.465.46 \u00d7 10 -74.46 \u00d7 10 -310085COASY-2.371.06 \u00d7 10 -74.32 \u00d7 10 -4120101TRAP1-2.341.82 \u00d7 10 -77.44 \u00d7 10 -420686CYP46A1-2.325.03 \u00d7 10 -74.11 \u00d7 10 -316386PARP1-2.252.66 \u00d7 10 -79.08 \u00d7 10 -432088FOXRED1-2.254.29 \u00d7 10 -51.78 \u00d7 10 -213953AFG3L2-2.242.13 \u00d7 10 -68.69 \u00d7 10 -327479RAB7A-2.149.64 \u00d7 10 -83.94 \u00d7 10 -434495PPP2R2B-2.138.95 \u00d7 10 -68.84 \u00d7 10 -332477RGS2-2.101.73 \u00d7 10 -51.09 \u00d7 10 -211878AMFR-2.081.61 \u00d7 10 -44.88 \u00d7 10 -29669MRPL10-2.067.03 \u00d7 10 -51.34 \u00d7 10 -212769ANO10-1.941.86 \u00d7 10 -58.91 \u00d7 10 -314592DMXL1-1.946.13 \u00d7 10 -52.15 \u00d7 10 -210789HYOU1-1.912.91 \u00d7 10 -51.83 \u00d7 10 -217786HTT-1.899.44 \u00d7 10 -73.85 \u00d7 10 -3200125ECHS1-1.882.57 \u00d7 10 -66.12 \u00d7 10 -333296CYCS-1.851.45 \u00d7 10 -63.04 \u00d7 10 -336565CEP63-1.843.53 \u00d7 10 -51.92 \u00d7 10 -215887FARP1-1.806.95 \u00d7 10 -53.14 \u00d7 10 -232774FRMD4A-1.727.80 \u00d7 10 -42.36 \u00d7 10 -324667RPL6-1.663.29 \u00d7 10 -92.69 \u00d7 10 -536887PFN1-1.502.87 \u00d7 10 -52.93 \u00d7 10 -236676"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Quantitative Analysis of Gene Perturbation Responses in CRISPRi Experiments. This table presents the effectiveness of sc-OTGM in identifying known differentially expressed genes (DEGs) following targeted gene knockdowns. Statistical validation is provided through p-Values obtained from Fisher's exact test, confirming a significant correlation between predicted and known DEGs for all targeted gene knockdown experiments, well below the significance threshold of 0.05. The columns labeled 'Accuracy (%)' and 'F1-score' evaluate sc-OTGM's performance to accurately predict the direction of expression changes-either upregulation or downregulation-in DEGs. The mean accuracy of 75.2% \u00b1 15.4% and mean F1-score of 0.79 \u00b1 0.14 show the model's performance in predicting gene expression dynamics.", "figure_data": "GeneDEG Enrichment Perturbation Response Predictionp-ValueAcc. (%)F1-scoreTUBB4A7.37 \u00d7 10 -6100.01.00ATP1A33.12 \u00d7 10 -2078.70.85KIFAP36.39 \u00d7 10 -1087.50.67MAPT9.01 \u00d7 10 -4100.01.00CASP36.93 \u00d7 10 -6100.01.00APEX12.61 \u00d7 10 -16100.01.00COX101.43 \u00d7 10 -1683.70.87NDUFS82.29 \u00d7 10 -3682.70.83ZNF2924.91 \u00d7 10 -1665.40.72GSTA45.50 \u00d7 10 -2189.50.92STX1B4.95 \u00d7 10 -3872.10.76OPTN1.51 \u00d7 10 -6100.01.00SOD11.16 \u00d7 10 -788.90.90NDUFV14.33 \u00d7 10 -3073.90.79CALB11.38 \u00d7 10 -440.00.46EEF25.08 \u00d7 10 -2992.20.92BIN16.39 \u00d7 10 -888.90.89SCFD11.59 \u00d7 10 -4256.40.66PON28.41 \u00d7 10 -5553.60.60BAX1.27 \u00d7 10 -3078.30.83SCAPER1.48 \u00d7 10 -3187.20.89CYB5615.66 \u00d7 10 -3360.40.66AKAP93.69 \u00d7 10 -14100.01.00VPS359.36 \u00d7 10 -1480.00.85PRNP3.95 \u00d7 10 -5359.40.72AP2A22.48 \u00d7 10 -5775.40.82SOD22.01 \u00d7 10 -790.50.91BECN16.22 \u00d7 10 -473.10.75SNCB6.79 \u00d7 10 -4187.70.89CDH111.77 \u00d7 10 -1466.70.70ELOVL52.28 \u00d7 10 -1492.30.93NTRK24.50 \u00d7 10 -2958.70.61DAP1.27 \u00d7 10 -4581.70.86EIF4G13.00 \u00d7 10 -2476.40.84TRPM73.66 \u00d7 10 -1466.70.80COASY1.51 \u00d7 10 -683.30.84TRAP12.11 \u00d7 10 -3580.00.89CYP46A14.56 \u00d7 10 -450.00.33PARP15.51 \u00d7 10 -2457.70.67FOXRED11.49 \u00d7 10 -2575.80.78AFG3L21.01 \u00d7 10 -1575.00.81RAB7A1.21 \u00d7 10 -1283.30.84PPP2R2B3.10 \u00d7 10 -3563.90.75RGS25.03 \u00d7 10 -3063.00.68AMFR4.06 \u00d7 10 -1262.50.74MRPL101.11 \u00d7 10 -2243.60.57Continued on next page"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Continued from previous page", "figure_data": "GeneDEG Enrichment Perturbation Response Predictionp-ValueAcc. (%)F1-scoreANO101.38 \u00d7 10 -440.00.46DMXL16.72 \u00d7 10 -3366.70.76HYOU17.43 \u00d7 10 -3555.30.65HTT8.81 \u00d7 10 -2461.10.66ECHS15.44 \u00d7 10 -971.40.79CYCS7.80 \u00d7 10 -377.80.80CEP632.80 \u00d7 10 -1475.00.77FARP11.35 \u00d7 10 -2283.30.88FRMD4A8.53 \u00d7 10 -3183.10.84RPL62.74 \u00d7 10 -3667.70.77PFN13.91 \u00d7 10 -1680.00.85A.8 IMPLEMENTATION OF HIT-AND-RUN MARKOV CHAIN SAMPLER"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "# Plotting RMSE values for dim, (rmse, _) in results.items():(line,) = plt.plot(rmse, label=f\"d={dim}\", linewidth=1) if i == 0: # Only add to legend for the first subplot lines.append(line) labels.append(f\"d={dim}\")", "figure_data": "plt.title(f\"$\\\\rho$: {factor}\", fontsize=12)plt.xlabel(\"# of iterations\", fontsize=10)plt.ylabel(\"RMSE\", fontsize=10)plt.xlim(0, num_iterations)plt.grid(True)for i, factor in enumerate(correlation_factors):plt.subplot(2, 4, i + 5)# Retrieve results for this correlation factorresults = all_results[factor]# Plotting Frobenius normsfor dim, (_, fro_norm) in results.items():plt.plot(fro_norm, label=f\"d={dim}\", linewidth=1)plt.title(f\"$\\\\rho$: {factor}\", fontsize=12)plt.xlabel(\"# of iterations\", fontsize=10)plt.ylabel(\"Frobenius norm\", fontsize=10)plt.xlim(0, num_iterations)plt.grid(True)plt.figlegend(lines,labels,loc=\"upper center\",ncol=len(dimensions),fontsize=10,)plt.tight_layout(rect=[0, 0, 1, 0.95])plt.show()"}], "formulas": [{"formula_id": "formula_0", "formula_text": "z i x proj,i \u03c0 \u00b5 k b \u03a3 k b + e k h k * z pi N K K", "formula_coordinates": [3.0, 194.97, 119.21, 217.7, 88.88]}, {"formula_id": "formula_1", "formula_text": "z pi = (e k * h k )(z) = +\u221e -\u221e e k (\u03c4 )h k (z-\u03c4 )d\u03c4 \u223c N (\u00b5 ke +\u00b5 km , \u03a3 ke +\u03a3 km ) \u223c N (\u00b5 kp , \u03a3 kp ),(1)", "formula_coordinates": [3.0, 112.98, 400.86, 391.69, 26.29]}, {"formula_id": "formula_2", "formula_text": "p(X, Z i , E, H | \u03c0, \u00b5, \u03a3) = N i=1 p(z i |\u03c0)p(x proj,i |z i , \u00b5, \u03a3) N i=1 e * h (2) = N i=1 K k=1 \u03c0 z ik k N (w bi ; \u00b5 k b , \u03a3 k b ) z ik N (w pi ; \u00b5 kp , \u03a3 kp ) ,(3)", "formula_coordinates": [3.0, 139.07, 480.16, 365.6, 65.5]}, {"formula_id": "formula_3", "formula_text": "Z i , E, H | \u03c0, \u00b5, \u03a3): log L = N i=1 K k=1 z ik log \u03c0 k + z ik log N (w bi ; \u00b5 k b , \u03a3 k b ) + log N (w pi ; \u00b5 kp , \u03a3 kp )(4)", "formula_coordinates": [3.0, 141.41, 622.96, 363.25, 57.86]}, {"formula_id": "formula_4", "formula_text": "\u00b5 kp = \u00b5 ky -\u00b5 k b and \u03a3 kp = \u03a3 k b + \u03a3 ky -2\u03a3 cross .", "formula_coordinates": [4.0, 108.0, 320.41, 395.0, 21.28]}, {"formula_id": "formula_5", "formula_text": "x \u2032\u2032 path = x path V T m", "formula_coordinates": [4.0, 123.39, 478.18, 64.44, 14.27]}, {"formula_id": "formula_6", "formula_text": "\u00b5 gene,i = the i th element of (\u00b5 kp V T m ) and \u03c3 2 gene,i = the i th diagonal entry of (V m \u03a3 kp V T m ),", "formula_coordinates": [4.0, 121.03, 533.77, 369.95, 12.85]}, {"formula_id": "formula_7", "formula_text": "\u03a3 \u2032\u2032 kp = V m \u03a3 kp V T m .", "formula_coordinates": [4.0, 182.65, 670.5, 79.75, 14.2]}, {"formula_id": "formula_8", "formula_text": "\u2206X j = \u03a3 \u2032\u2032 kp,ij \u03a3 \u2032\u2032 kp,ii \u2206X i (5)", "formula_coordinates": [4.0, 265.35, 704.08, 239.32, 30.15]}, {"formula_id": "formula_9", "formula_text": "3: Initialize (x (0) , y (0) ) uniformly from D X \u00d7 D Y . 4: Initialize \u03a3 (0)", "formula_coordinates": [5.0, 112.98, 131.22, 205.57, 24.42]}, {"formula_id": "formula_10", "formula_text": "z = \u03a6 -1 1+\u03b1 2 . 6: for i = 1 to N do 7: Calculate non-zero density bounds [a X , b X ] for X: 8: Generate a random unit direction d X in X's space. 9: Normalize d X to unit length: d X,normalized = d X \u2225d X \u2225 10: Project x (i-1) onto d X,normalized : p X,projection = x (i-1) \u2022 d X,normalized 11: Compute standard deviation \u03c3 X,projection = d T X,normalized \u03a3 X d X,normalized 12: Determine [a X , b X ] using p X,projection \u00b1 z \u2022 \u03c3 X,projection 13: Calculate non-zero density bounds [a Y , b Y ] for Y : 14: Generate a random unit direction d Y in Y 's space. 15: Normalize d Y to unit length: d Y,normalized = d Y \u2225d Y \u2225 16: Project y (i-1) onto d Y,normalized : p Y,projection = y (i-1) \u2022 d Y,normalized 17: Compute standard deviation \u03c3 Y,projection = d T Y,normalized \u03a3 Y d Y,normalized 18: Determine [a Y , b Y ] using p Y,projection \u00b1 z \u2022 \u03c3 Y,projection 19: Update \u03a3 XY : 20: Sample x (i) \u223c Uniform(a X , b X ) and Sample y (i) \u223c Uniform(a Y , b Y ) 21: \u2206x (i) = x (i) -\u00b5 X , \u2206y (i) = y (i) -\u00b5 Y 22: \u03a3 (i) XY = i i+1 \u03a3 (i-1) XY + i (i+1) 2 \u2206x (i) (\u2206y (i) ) T", "formula_coordinates": [5.0, 108.5, 166.73, 350.46, 231.67]}, {"formula_id": "formula_11", "formula_text": "min T X c(x, T (x)) d\u00b5(x)", "formula_coordinates": [9.0, 254.92, 718.96, 102.16, 17.23]}, {"formula_id": "formula_12", "formula_text": "min \u03c0\u2208\u03a0(\u00b5,\u03bd) X\u00d7Y c(x, y) d\u03c0(x, y)", "formula_coordinates": [10.0, 242.74, 184.42, 126.53, 17.23]}, {"formula_id": "formula_13", "formula_text": "sup (f,g)\u2208\u03a6 X f (x) d\u00b5(x) + Y g(y) d\u03bd(y)", "formula_coordinates": [10.0, 218.58, 273.2, 167.37, 17.23]}, {"formula_id": "formula_14", "formula_text": "W p (\u00b5, \u03bd) = min \u03c0\u2208\u03a0(\u00b5,\u03bd) X\u00d7Y \u2225x -y\u2225 p d\u03c0(x, y) 1/p", "formula_coordinates": [10.0, 198.43, 345.84, 214.64, 27.18]}, {"formula_id": "formula_15", "formula_text": "X \u2032 ij = log(1 + X ij )", "formula_coordinates": [12.0, 303.54, 133.26, 87.89, 12.33]}, {"formula_id": "formula_16", "formula_text": "X \u2032 ij = Xij -Xj \u03c3j", "formula_coordinates": [12.0, 166.61, 215.33, 58.91, 14.6]}, {"formula_id": "formula_17", "formula_text": "X \u2032\u2032 ij = min(max(X \u2032 ij , -\u03b8 max ), \u03b8 max ).", "formula_coordinates": [12.0, 108.0, 246.9, 396.0, 24.76]}, {"formula_id": "formula_18", "formula_text": "F emp (x j,i proj ) = j N \u25b7 Empirical Cumulative Distribution Function (ECDF) 10: F ref (x j,i proj ) = x j,i proj -\u221e f (x) dx \u25b7 CDF of the Reference Gaussian Distribution 11: D j,i = |F emp (x j,i proj ) -F ref (x j,i proj )| \u25b7 K-S Statistic 12:", "formula_coordinates": [12.0, 108.5, 537.94, 396.17, 57.58]}, {"formula_id": "formula_19", "formula_text": "D i = max j D j,i 14:", "formula_coordinates": [12.0, 108.5, 596.16, 97.89, 22.39]}, {"formula_id": "formula_20", "formula_text": "3: Initialize \u03c0 i , \u00b5 i , \u03a3 i for i = 1 : k. 4: repeat 5:", "formula_coordinates": [13.0, 112.98, 205.34, 145.38, 30.69]}, {"formula_id": "formula_21", "formula_text": "log P (x j | \u03c0 i ) = -m 2 log(2\u03c0) -1 2 log |\u03a3 i | -1 2 (x j -\u00b5 i ) T \u03a3 -1 i (x j -\u00b5 i ) 8:", "formula_coordinates": [13.0, 112.98, 246.98, 357.57, 21.92]}, {"formula_id": "formula_22", "formula_text": "P (\u03c0 i | x j ) = exp(a i,j -log P (x j )) \u25b7 E-Step 11:", "formula_coordinates": [13.0, 108.5, 284.79, 395.5, 19.73]}, {"formula_id": "formula_23", "formula_text": "\u00b5 i = N j=1 P (\u03c0i|xj )xj N j=1 P (\u03c0i|xj )", "formula_coordinates": [13.0, 154.82, 303.72, 87.08, 20.19]}, {"formula_id": "formula_24", "formula_text": "\u03a3 i = N j=1 P (\u03c0i|xj )(xj -\u00b5i)(xj -\u00b5i) T N j=1 P (\u03c0i|xj )", "formula_coordinates": [13.0, 154.82, 324.91, 144.46, 20.19]}, {"formula_id": "formula_25", "formula_text": "\u03c0 i = 1 N N j=1 P (\u03c0 i | x j )", "formula_coordinates": [13.0, 154.82, 346.11, 105.68, 14.56]}, {"formula_id": "formula_26", "formula_text": "\u03b1 = 0.01 \u00d7 mean(diag(\u03a3 i ))", "formula_coordinates": [13.0, 154.82, 361.89, 115.4, 9.65]}, {"formula_id": "formula_27", "formula_text": "\u03a3 i = \u03a3 i + \u03b1I \u25b7 Tikhonov regularization 17:", "formula_coordinates": [13.0, 108.5, 372.82, 395.5, 19.73]}, {"formula_id": "formula_28", "formula_text": "\u03a3 = \u03a3 + \u03b1I (6)", "formula_coordinates": [13.0, 281.4, 437.12, 223.27, 11.48]}, {"formula_id": "formula_29", "formula_text": "\u03ba(A) = \u2225A\u2225 \u2022 \u2225A -1 \u2225", "formula_coordinates": [13.0, 280.43, 589.99, 87.01, 10.81]}], "doi": ""}
