{"title": "iFixR: Bug Report driven Program Repair", "authors": "Anil Koyuncu; Kui Liu; Tegawend\u00e9 F Bissyand\u00e9; Dongsun Kim; Martin Monperrus; Jacques Klein; Yves Le Traon; Mar- Tin Monperrus;  Ca Munawar Hafiz", "pub_date": "2019-07-12", "abstract": "Issue tracking systems are commonly used in modern software development for collecting feedback from users and developers. An ultimate automation target of software maintenance is then the systematization of patch generation for user-reported bugs. Although this ambition is aligned with the momentum of automated program repair, the literature has, so far, mostly focused on generate-andvalidate setups where fault localization and patch generation are driven by a well-defined test suite. On the one hand, however, the common (yet strong) assumption on the existence of relevant test cases does not hold in practice for most development settings: many bugs are reported without the available test suite being able to reveal them. On the other hand, for many projects, the number of bug reports generally outstrips the resources available to triage them. Towards increasing the adoption of patch generation tools by practitioners, we investigate a new repair pipeline, iFixR, driven by bug reports: (1) bug reports are fed to an IR-based fault localizer; (2) patches are generated from fix patterns and validated via regression testing; (3) a prioritized list of generated patches is proposed to developers. We evaluate iFixR on the Defects4J dataset, which we enriched (i.e., faults are linked to bug reports) and carefullyreorganized (i.e., the timeline of test-cases is naturally split). iFixR generates genuine/plausible patches for 21/44 Defects4J faults with its IR-based fault localizer. iFixR accurately places a genuine/plausible patch among its top-5 recommendation for 8/13 of these faults (without using future test cases in generation-and-validation).", "sections": [{"heading": "INTRODUCTION", "text": "Automated program repair (APR) has gained incredible momentum in the last decade. Since the seminal work by Weimer et al. [83] who relied on genetic programming to evolve program variants until one variant is found to satisfy the functional constraints of a test suite, the community has been interested in test-based techniques to repair programs without specifications. Thus, various approaches [12,13,18,19,25,26,32,33,38,45,48,49,52,53,55,62,65,83,84,92,93] have been proposed in the literature aiming at reducing manual debugging efforts through automatically generating patches. Beyond fixing syntactic errors, i.e., cases where the code violates some programming language specifications [16], the current challenges lie in fixing semantic bugs, i.e., cases where implementation of program behavior deviates from developer's intention [22,61].\nTen years ago, the work of Weimer et al. [83] was explicitly motivated by the fact that, despite significant advances in specification mining (e.g., [40]), formal specifications are rarely available. Thus, test suites represented an affordable approximation to program specifications. Unfortunately, the assumption that test cases are readily available still does not hold in practice [7,27,68]. Therefore, while current test-based APR approaches would be suitable in a test-driven development setting [6], their adoption by practitioners faces a simple reality: developers majoritarily (1) write few tests [27], (2) write tests after the source code [7], and (3) write tests to validate that bugs are indeed fixed and will not reoccur [23].\nAlthough APR bots [79] can come in handy in a continuous integration environment, the reality is that bug reports remain the main source of the stream of bugs that developers struggle to handle daily [5]. Bugs are indeed reported in natural language, where users tentatively describe the execution scenario that was being carried out and the unexpected outcome (e.g., crash stack traces). Such bug reports constitute an essential artifact within a software development cycle and can become an overwhelming concern for maintainers. For example, as early as in 2005, a triager of the Mozilla project was reported in [5, page 363] to have commented that: \"Everyday, almost 300 bugs appear that need triaging. This is far too much for only the Mozilla programmers to handle.\" However, very few studies [9,43] have undertaken to automate patch generation based on bug reports. To the best of our knowledge, Liu et al. [43] proposed the most advanced study in this direction. Unfortunately, their R2Fix approach carries several caveats: as illustrated in Figure 1, it focuses on perfect bug reports [43,  The trailing zero (`\\0') will be written to state [4] which is out of bound.\nlinux/net/mac80211/debugfs_sta.c: -strcpy(state, \"off \"); + strcpy(state, \"off\");\n(a) Linux Kernel Bug Report (b) Patch to Fix the Bug", "publication_ref": ["b82", "b11", "b12", "b17", "b18", "b24", "b25", "b31", "b32", "b37", "b44", "b47", "b48", "b51", "b52", "b54", "b61", "b64", "b82", "b83", "b91", "b92", "b15", "b21", "b60", "b82", "b39", "b6", "b26", "b67", "b5", "b26", "b1", "b6", "b22", "b78", "b4", "b8", "b42", "b42"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Remove the space character", "text": "Figure 1. Converting a bug report to a patch. \"-\" denotes a line to be deleted; \"+\" denotes a line to be added; and \" \" is one space character.\ndevelopers forget to check if the array is long enough to hold the content before the assignment; or was the bug caused by more complex reasons? The developers then need to check out the buggy version, modify the buggy code to fix the bug, and generate the patch that can be applied to the shared source code repository. The result of this challenging and time-consuming process by developers for bug 11975 is the patch in Figure 1(b). The patch deletes the line that writes 5 bytes to buffer state (denoted by -strcpy(state, \"off \");), and adds a new line to write only 4 bytes to state (+ strcpy(state, \"off\");), which fixes the overflow bug.\nDevelopers often need to fix more bugs than their time and resources allow [6]. Although developers spend almost half of their time fixing bugs [21], bugs take years to be fixed on average [9], [19].\nTherefore, support to make it easier and faster for developers to fix bugs is in high demand. The capability to automatically generate patches (e.g., Figure 1(b)) from bug reports (e.g., Figure 1(a)) could: (1) save programmers' time and effort in diagnosing bugs and generating patches, allowing developers to fix more bugs or focus on other development tasks; and (2) shorten the bug-fixing time, thus improve software reliability and security.", "publication_ref": ["b5", "b20", "b8", "b18", "b0"], "figure_ref": ["fig_1", "fig_1", "fig_1", "fig_1"], "table_ref": []}, {"heading": "A. Ideal Goal Versus Realistic Goal", "text": "Ideally, we want to automatically generate patches for all bug reports. Realistically, it is impossible. We found that only 16.7-33.5% of bug reports in the Linux kernel, Mozilla, and Apache bug databases are fixed. This is because, many bug reports are invalid, unreproducible, incomplete, etc. Even among bugs that can be fixed, some are too complex to be fixed automatically because they require redesign of the algorithm, addition of new features, etc.  (1) which explicitly include localization information, (2) where the symptom (e.g., buffer overrun) is explicitly indicated by the reporter, and ( 3) which are about one of the following three simple bug types: Buffer overflow, Null Pointer dereference or memory leak. R2Fix runs a straightforward classification to identify the bug category and uses a match and transform engine (e.g., Coccinelle [66]) to generate patches. As the authors admitted, their target space represents <1% of bug reports in their dataset. Furthermore, it should be noted that, given the limited scope of the changes implemented in its fix patterns, R2Fix does not need to run tests for verifying that the generated patches do not break any functionality.\nThis paper. We propose to investigate the feasibility of a program repair system driven by bug reports, thus we replace classical spectrum-based fault localization with Information Retrieval (IR)-based fault localization. Eventually, we propose iFixR, a new program repair workflow which considers a practical repair setup by imitating the fundamental steps of manual debugging. iFixR works under the following constraint:\nWhen a bug report is submitted to the issue tracking system, a relevant test case reproducing the bug may not be readily available.\nTherefore, iFixR is leveraged in this study to assess to what extent an APR pipeline is feasible under the practical constraint of limited test suites. iFixR uses bug reports written in natural language as the main input. Eventually, we make the following contributions:\n\u2022 We present the architecture of a program repair system adapted to the constraints of maintainers dealing with user-reported bugs.\nIn particular, iFixR replaces classical spectrum-based fault localization with Information Retrieval (IR)-based fault localization. \u2022 We propose a strategy to prioritize patches for recommendation to developers. Indeed, given that we assume only the presence of regression test cases to validate patch candidates, many of these patches may fail on the future test cases that are relevant to the reported bugs. We order patches to present correct patches first. \u2022 We assess and discuss the performance of iFixR on the Defects4J benchmark to compare with the state-of-the-art APR tools. To that end, we provide a refined Defects4J benchmark for APR targeting bug reports. Bugs are carefully linked with the corresponding bug reports, and for each bug we are able to dissociate future test cases that were introduced after the relevant fixes. Overall, experimental results show that there are promising research directions to further investigate towards the integration of automatic patch generation in actual software development cycles. In particular, our findings suggest that IR-based fault localization errors lead less to overfitting patches than spectrum-based fault localization errors. Furthermore, iFixR offers provides comparable results to most state-of-the-art APR tools, although it is run under the constraint that post-fix knowledge (i.e., future test cases) is not available. Finally, iFixR's prioritization strategy tends to place more correct/plausible patches on top of the recommendation list.", "publication_ref": ["b1", "b65"], "figure_ref": [], "table_ref": []}, {"heading": "MOTIVATION", "text": "We motivate our work by revisiting two essential steps in APR:\n(1) During fault localization, relevant program entities are identified as suspicious locations that must be changed. Commonly, stateof-the-art APR tools leverage spectrum-based fault localization (SBFL) [12,25,32,33,39,50,52,53,62,65,83,92,93], which uses execution coverage information of passing and failing test cases to predict buggy statements. We dissect the construction of the Defects4J dataset to highlight the practical challenges of fault localization for user-reported bugs. (2) Once a patch candidate is generated, the patch validation step ensures that it is actually relevant for repairing the program. Currently, widespread test-based APR techniques use test suites as the repair oracle. This however is challenged by the incompleteness of test suites, and may further not be inline with developer requirements/expectations in the repair process.", "publication_ref": ["b11", "b24", "b31", "b32", "b38", "b49", "b51", "b52", "b61", "b64", "b82", "b91", "b92"], "figure_ref": [], "table_ref": []}, {"heading": "Fault Localization Challenges", "text": "Defects4J is a manual curated dataset widely used in the APR literature [12,18,73,84,90,91]. Since Defects4J was not initially built for APR, the real order of precedence between the bug report, the patch and the test case is being overlooked by the dataset users. Indeed, Defects4J offers a user-friendly way of checking out buggy versions of programs with all relevant test cases for readily benchmarking test-based systems. We propose to carefully examine the actual bug fix commits associated with Defects4J bugs and study how the test suite is evolved. Table 1 provides detailed information. Overall, for 96%(=381/395) bugs, the relevant test cases are actually future data with respect to the bug discovery process. This finding suggests that, in practice, even the fault localization may be challenged in the case of user-reported bugs, given the lack of relevant test cases. The statistics listed in Table 2 indeed shows that if future test cases are dropped, no test case is failing when executing buggy program versions for 365 (i.e., 92%) bugs. In the APR literature, fault localization is generally performed using the GZoltar [11] testing framework and a SBFL formula [88] (e.g., Ochiai [2]). To support our discussions, we attempt to perform fault localization without the future test cases to evaluate the performance gap. Experimental results (see details forward in Table 6 of Section 5) expectedly reveal that the majority of the Defects4J bugs (i.e., 375/395) cannot be localized by SBFL at the time the bug is reported by users.\nIt is necessary to investigate alternate fault localization approaches that build on bug report information since relevant test cases are often unavailable when users report bugs. Step 0\nStep 1\nStep 2\nStep 3\nStep 4\nStep 5\nIterative folding", "publication_ref": ["b11", "b17", "b72", "b83", "b89", "b90", "b10", "b87", "b1"], "figure_ref": [], "table_ref": ["tab_1", "tab_2", "tab_9"]}, {"heading": "Patch Candidates", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Patch Generation Patch Validation", "text": "Regression Testing  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Fix Pattern Matching", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Patch Validation in Practice", "text": "The repair community has started to reflect on the acceptability [26,63] and correctness [76,91] of the patches generated by APR tools. Notably, various studies [10,34,69,76,94] have raised concerns about overfitting patches: a typical APR tool that uses a test suite as the correctness criterion can produce a patched program that actually overfits the test-suite (i.e., the patch makes the program pass all test cases but does not actually repair it). Recently, new research directions [89,97] are being explored in the automation of test case generation for APR to overcome the overfitting issue. Nevertheless, so far they have had minimal positive impact due to the oracle problem [98] in automatic test generation. At the same time, the software industry takes a more systematic approach for patch validation by developers. For instance, in the open-source community, the Linux development project has integrated a patch generation engine to automate collateral evolutions that are validated by maintainers [29,66]. In proprietary settings, Facebook has recently reported on their Getafix [75] tool, which automatically suggests fixes to their developers. Similarly, Ubisoft developed Clever [64] to detect risky commits at commit-time using patterns of programming mistakes from the code history.\nPatch recommendation for validation by developers is acceptable in the software development communities. It may thus be worthwhile to focus on tractable techniques for recommending patches in the road to fully automated program repair.", "publication_ref": ["b25", "b62", "b75", "b90", "b9", "b33", "b68", "b75", "b93", "b88", "b96", "b97", "b28", "b65", "b74", "b63"], "figure_ref": [], "table_ref": []}, {"heading": "THE IFIXR APPROACH", "text": "Figure 2 overviews the workflow of the proposed iFixR approach. Given a defective program, we consider the following issues:\n(1) Where is the bug? We take as input the bug report in natural language submitted by the program user. We rely on the information in this report to localize the bug positions. (2) How should we change the code? We apply fix patterns that are recurrently found in real-world bug fixes. Fix patterns are selected following the structure of the abstract syntax tree representing the code entity of the identified suspicious code. (3) Which patches are valid? We make no assumptions on the availability of positive test cases [83] that encode functionality requirements at the time the bug is discovered. Nevertheless, we leverage existing test cases to ensure, at least, that the patch does not regress the program. (4) Which patches do we recommend first? In the absence of a complete test suite, we cannot guarantee that all patches that pass regression tests will fix the bug. We rely on heuristics to re-prioritize the validated patches in order to increase the probability of placing a correct patch on top of the list.", "publication_ref": ["b82"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Input: Bug reports", "text": "Issue tracking systems (e.g., Jira) are widely used by software development communities in the open source and commercial realms. Although they can be used by developers to keep track of the bugs that they encounter and the features to be implemented, issue tracking systems allow for user participation as a communication channel for collecting feedback on software executions in production. Table 3 illustrates a typical bug report when a user of the LANG library code has encountered an issue while using the NumberUtils API. A description of erroneous behavior is provided. Occasionally, the user may include in the bug description some information on how to reproduce the bug. Oftentimes, users simply insert code snippets or dump the execution stack traces.\nIn this study, among our dataset of 162 bug reports, we note that only 27 (i.e., \u223c17%) are reported by users who are also developersfoot_0 contributing to the projects. 15 (i.e., \u223c9%) bugs are reported and again fixed by the same project contributors. These percentages suggest that, for the majority of cases, the bug reports are indeed genuinely submitted by users of the software who require project developers' attention.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Table 3: Example bug report (Defects4J Lang-7).", "text": "Issue No.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "LANG-822 Summary", "text": "NumberUtils#createNumber -bad behaviour for leading \"-\" Description NumberUtils#createNumber checks for a leading \"-\" in the string, and returns null if found. This is documented as a work round for a bug in BigDecimal.\nReturning nulll is contrary to the Javadoc and the behaviour for other methods which would throw NumberFormatException. It's not clear whether the BigDecimal problem still exists with recent versions of Java. However, if it does exist, then the check needs to be done for all invocations of BigDecimal, i.e. needs to be moved to createBigDecimal.\nGiven the buggy program version and a bug report, iFixR must unfold the workflow for precisely identifying (at the statement level) the buggy code locations. We remind the reader that, in this step, future test cases cannot be relied upon. We consider that if such test cases could have triggered the bug, a continuous integration system would have helped developers deal with the bug before the software is shipped towards users.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Fault Localization w/o Test Cases", "text": "To identify buggy code locations within the source code of a program, we resort to Information Retrieval (IR)-based fault localization (IRFL) [67,80]. The general objective is to leverage potential similarity between the terms used in a bug report and the source code to identify relevant buggy code locations. The literature includes a large body of work on IRFL [57,72,81,85,87,96,99] where researchers systematically extract tokens from a given bug report to formulate a query to be matched in a search space of documents formed by the collections of source code files and indexed through tokens extracted from source code. IRFL approaches then rank the documents based on a probability of relevance (often measured as a similarity score). Highly ranked files are predicted to be the ones that are likely to contain the buggy code.\nDespite recurring interest in the literature, with numerous approaches continuously claiming new performance improvements over the state-of-the-art, we are not aware of any adoption in program repair research or practice. We postulate that one of the reasons is that IRFL techniques have so far focused on file-level localization, which is too coarse-grained (in comparison to spectrumbased fault localization output). Recently, Locus [85] and BLIA [96] are state-of-the-art techniques which narrow down localization, respectively to the code change or the method level. Nevertheless, to the best of our knowledge, no IRFL technique has been proposed in the literature for statement-level localization.\nIn this work, we develop an algorithm to rank suspicious statements based on the output (i.e., files) of a state-of-the-art IRFL tool, thus yielding a fine-grained IR-based fault localizer which will then be readily integrated into a concrete patch generation pipeline.", "publication_ref": ["b66", "b79", "b56", "b71", "b80", "b84", "b86", "b95", "b98", "b84", "b95"], "figure_ref": [], "table_ref": []}, {"heading": "Ranking Suspicious Files.", "text": "We leverage an existing IRFL tool. Given that expensive extractions of tokens from a large corpus of bug reports is often necessary to tune IRFL tools [41], we selected a tool for which the authors provide datasets and pre-processed data. We use the D&C [30] as the specific implementation of file-level IRFL available online [1] , which is a machine learning-based IRFL tool using a similarity matrix of 70-dimension feature vectors (7 features from bug reports and 10 features from source code files): D&C uses multiple classifier models that are trained each for specific groups of bug reports. Given a bug report, the different predictions of the different classifiers are merged to yield a single list of suspicious code files. Our execution of D&C (Line 2 in Algorithm 1) is tractable given that we only need to preprocess those bug reports that we must localize. Trained classifiers are already available. We ensure that no data leakage is induced (i.e., the classifiers are not trained with bug reports that we want to localize in this work).", "publication_ref": ["b40", "b29", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Ranking Suspicious", "text": "Statements. Patch generation requires fine-grained information on code entities that must be changed. For iFixR, we propose to produce a standard output, as for spectrumbased fault localization, to facilitate integration and reuse of stateof-the-art patch generation techniques. To start, we build on the conclusions on a recent large-scale study [47] of bug fixes to limit the search space of suspicious locations to the statements that are more error-prone. After investigating in detail the abstract syntax tree (AST)-based code differences of over 16 000 real-world patches from Java projects, Liu et al. [47] reported that the following specific AST statement nodes were significantly more prone to be faulty than others: IfStatements, ExpressionStatements, FieldDeclarations, ReturnStatements and VariableDeclara-tionStatements. Lines 7-17 in Algorithm 1 detail the process to produce a ranked list of suspicious statements.\nAlgorithm 1 describes the process of our fault localization approach used in iFixR. Top k files are selected among the returned list of suspicious files of the IRFL along with their computed suspiciousness scores. Then each file is parsed to retain only the relevant error-prone statements from which textual tokens are extracted. The summary and descriptions of the bug report are also analyzed  return Sscor e (lexically) to collect all its tokens. Due to the specific nature of stack traces and other code elements which may appear in the bug report, we use regular expressions to detect stack traces and code elements to improve the tokenization process, which is based on punctuations, camel case splitting (e.g., findNumber splits into find, number) as well as snake case splitting (e.g., find_number splits into find, number). Stop word removalfoot_1 is then applied before performing stemming (using the PorterStemmer [24]) on all tokens to create homogeneity with the term's root (i.e., by conflating variants of the same term). Each bag of tokens (for the bug report, and for each statement) is then eventually used to build a feature vector. We use cosine similarity among the vectors to rank the file statements that are relevant to the bug report.\nGiven that we considered k files, the statements of each having their own similarity score with respect to the bug report, we weight these scores with the suspiciousness score of the associated file. Eventually, we sort the statements using the weighted scores and produce a ranked list of code locations (i.e., statements in files) to be recommended as candidate fault locations.", "publication_ref": ["b46", "b46", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "Fix Pattern-based Patch Generation", "text": "A common, and reliable, strategy in automatic program repair is to generate concrete patches based on fix patterns [26] (also referred to as fix templates [51] or program transformation schemas [18]). Several APR systems [14,18,26,31,46,48,49,51,60,73] in the literature implement this strategy by using diverse sets of fix patterns obtained either via manual generation or automatic mining of bug fix datasets. In this work, we consider the pioneer PAR system by Kim et al. [26]. Concretely, we build on kPAR [48], an open-source Java implementation of PAR in which we included a diverse set of fix patterns collected from the literature. Table 4 provides an enumeration of fix patterns used in this work. For more implementation details, we refer the reader to our replication package. All tools and data are released as open source to the community to foster further research into these directions. As illustrated in Figure 3, a fix pattern encodes the recipe of change actions that should be applied to mutate a code element.  For a given reported bug, once our fault localizer yields its list of suspicious statements, iFixR iteratively attempts to select fix patterns for each statement. The selection of fix patterns is conducted in a na\u00efve way based on the context information of each suspicious statement (i.e., all nodes in its abstract syntax tree, AST). Specifically, iFixR parses the code and traverses each node of the suspicious statement AST from its first child node to its last leaf node in a breadth-first strategy (i.e, left-to-right and top-to-bottom). If a node matches the context a fix pattern (i.e., same AST node types), the fix pattern will be applied to generate patch candidates by mutating the matched code entity following the recipe in the fix pattern. Whether the node matches a fix pattern or not, iFixR keeps traversing its children nodes and searches fix patterns for them to generate patch candidates successively. This process is iteratively performed until leaf nodes are encountered.\nConsider the example of bug Math-75 illustrated in Figure 4. iFixR parses the buggy statement (i.e., statement at line 302 in the file Frequency.java) into an AST as illustrated by Figure 5. First, iFixR matches a fix pattern that can mutate the expression in the return statement with other expression(s) returning data of type double. It further selects fix patterns for the direct child node (i.e., method invocation: getCumPct((Comparable<?> v))) of the return statement. This method invocation can be matched against fix patterns with two contexts: method name and parameter(s). With the breadth-first strategy, iFixR assigns a fix pattern, calling another method with the same parameters (cf. PAR [26, page 804]), to mutate the method name, and then selects fix patterns to mutate the parameter. Furthermore, iFixR will match fix patterns for the type and variable of the cast expression respectively and successively. ", "publication_ref": ["b25", "b50", "b17", "b13", "b17", "b25", "b30", "b45", "b47", "b48", "b50", "b59", "b72", "b25", "b47"], "figure_ref": ["fig_5"], "table_ref": ["tab_6"]}, {"heading": "Patch Validation with Regression Testing", "text": "For every reported bug, fault localization followed by pattern matching and code mutation will yield a set of patch candidates. In a typical test-based APR system, these patch candidates must let the program pass all test cases (including some positive test cases [83], which encode the actual functional requirements relevant to the bug). Thus, the patch candidates set is actively pruned to remove all patches that do not meet these requirements. In our work, in accordance with our investigation findings that such test cases may\nReturnStatement \"raw_code\" MethodInvocation \"raw_code\" MethodName \"getCumPct\" CastExpression \"raw_code\"\nType \"Comparable<?>\" VariableName \"v\" \u2460 \u2461 \u2462 \u2463 \u2464\n*\"raw_code\" denotes the corresponding source code at the related node position.\nFigure 5: AST of bug Math-75 source code statement. not be available at the time the bug is reported (cf. Section 2), we assume that iFixR cannot reason about future test cases to select patch candidates.\nInstead, we rely only on past test cases, which were available in the code base, when the bug is reported. Such test cases are leveraged to perform regression testing [95], which will ensure that, at least, the selected patches do not obstruct the behavior of the existing, unchanged part of the software, which is already explicitly encoded by developers in their current test suite.", "publication_ref": ["b82", "b94"], "figure_ref": [], "table_ref": []}, {"heading": "Output: Patch Recommendation List", "text": "Eventually, iFixR produces a ranked recommendation list of patch suggestions for developers. Until now, the order of patches is influenced mainly by two steps in the workflow:\n(1) localization: our statement-level IRFL yields a ranked list of statements to modify in priority. (2) pattern matching: the AST node of the buggy code entity is broken down into its children and iteratively navigated in a breadth-first manner to successively produce candidate patches. Eventually, the produced list of patches has an order, which carries the biases of fault localization [48], and is noised by the pre-set breadth-first strategy for matching fix patterns. We thus design an ordering process with a function 3 , f r cmd : 2 P \u2192 P k , as follows:\nf r cmd (patches) = (pri type \u2022 pri susp \u2022 pri chan\u0434e )(patches) (1)\nwhere pri * are three heuristics-based prioritization functions used in iFixR. f r cmd takes a set of patches validated via regression testing (cf. Section 3.4) and produces an ordered sequence of patches (f r cmd (patches) = seq r cmd \u2208 P k ). We propose the following heuristics to re-prioritize the patch candidates:\n(1) [Minimal changes]: we favor patches that minimize the differences between the patched program and the buggy program. To that end, patches are ordered following their AST edit script sizes. Formally, we define pri chan\u0434e : 2\nP \u2192 P n where n = |patches |, pri chan\u0434e (patches) = [p i , p i+1 , p i+2 , \u2022 \u2022 \u2022 ] and holds \u2200p \u2208 patches, C chan\u0434e (p i ) \u2264 C chan\u0434e (p i+1 ).\nHere, C chan\u0434e (p) is a function that counts the number of deleted and inserted AST nodes by the change actions of p.\n(2) [Fault localization suspiciousness]: when two patch candidates have equal edit script sizes, the tie is broken by using the suspiciousness scores (of the associated statements) yielded during IR-based fault localization. Thus, when C chan\u0434e (p i ) == C chan\u0434e (p i+1 ), pri susp re-orders the two patch candidates. We define pri susp :\nP n \u2192 P n such that pri susp (seq chan\u0434e ) = [\u2022 \u2022 \u2022 , p i , p i+1 , \u2022 \u2022 \u2022 ] holds S susp (p i ) \u2265 S susp (p i+1 )\n, where seq chan\u0434e is the result of pri chan\u0434e and S susp returns a suspicious score of the statement that a given patch p i changes. (3) [Affected code elements]: after a manual analysis of fix patterns and the performance of associated APR in the literature, we empirically found that some change actions are irrelevant to bug fixing. Thus, for the corresponding pre-defined patterns, iFixR systematically under-prioritizes their generated patches against any other patches, although among themselves the ranking obtained so far (through pri chan\u0434e and pri susp ) is preserved for those under-prioritized patches. These are patches generated by (i) mutating a literal expression, (ii) mutating a variable into a method invocation or a final static variable, or (iii) inserting a method invocation without parameter. This prioritization, is defined by pri type : P n \u2192 P k , which returns a sequence of top k ordered patches (k \u2264 n = |patches |). To define this prioritization function, we assign natural numbers j 1 , j 2 , j 3 , j 4 \u2208 N to each patch generation types (i.e., j 1 \u2190(i), j 2 \u2190(ii), and j 3 \u2190(iii), respectively) and (j 4 \u2190) everything else, which strictly hold j 4 > j 1 , j 4 > j 2 , j 4 > j 3 . This prioritization function takes the result of pri susp and returns another sequence\n[p i , p i+1 , p i+2 , \u2022 \u2022 \u2022 ] that holds \u2200p i , D type (p i ) \u2265 D type (p i+1\n). D type is defined as D type : 2 P \u2192 {j 1 , j 2 , j 3 , j 4 } and determines how a patch p i has been generated as defined above. From the ordered sequence, the function returns the leftmost (i.e., top) k patches as a result.", "publication_ref": ["b47"], "figure_ref": [], "table_ref": []}, {"heading": "EXPERIMENTAL SETUP", "text": "We now provide details on the experiments that we carry out to assess the iFixR patch generation pipeline for user-reported bugs. Notably, we discuss the dataset and benchmark, some implementation details before enumerating the research questions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Dataset & Benchmark", "text": "To evaluate iFixR we propose to rely on the Defects4J [21] dataset which is widely used as a benchmark in the Java APR literature. Nevertheless, given that Defects4J does not provide direct links to the bug reports that are associated with the benchmark bugs, we must undertake a fairly accurate bug linking task [78]. Furthermore, to realistically evaluate iFixR, we must reorganize the dataset test suites to accurately simulate the context at the time the bug report is submitted by users.", "publication_ref": ["b20", "b77"], "figure_ref": [], "table_ref": []}, {"heading": "Bug linking.", "text": "To identify the bug report describing a given bug in the Defects4J dataset we focus on recovering the links between the bug fix commits and bug reports from the issue tracking system. Unfortunately, projects Joda-Time, JFreeChart and Closure have migrated their source code repositories and issue tracking systems into GitHub without a proper reassignment of bug report identifiers. Therefore, for these projects, bug IDs referred to in the commit logs are ambiguous (for some bugs this may match with the GitHub issue tracking numbering, while in others, it refers to the original issue tracker). To avoid introducing noise in our validation data, we simply drop these projects. For the remaining projects (Lang and Math), we leverage the bug linking strategies implemented in the Jira issue tracking software. We use a similar approach to Fischer et al. [15] and Thomas et al. [78] to link to commits to corresponding bug reports. Concretely, we crawled the bug reports related to each project and assessed the links with a two-step search strategy: (i)\nwe check commit logs to identify bug report IDs and associate the corresponding changes as bug fix changes; then (ii) we check for bug reports that are indeed considered as such (i.e., tagged as \"BUG\") and are further marked as resolved (i.e., with tags \"RESOLVED\" or \"FIXED\"), and completed (i.e., with status \"CLOSED\"). Eventually, our evaluation dataset includes 156 faults (i.e., De-fects4J bugs). Actually, for the considered projects, Defects4J enumerates 171 bugs associated with 162 bug reports: 15 bugs are indeed left out because either (1) the corresponding bug reports are not in the desired status in the bug tracking system, which may lead to noisy data, or (2) there is ambiguity in the buggy program version (e.g., some fixed files appear to be missing in the repository at the time of bug reporting).", "publication_ref": ["b14", "b77"], "figure_ref": [], "table_ref": []}, {"heading": "Test suite reorganization.", "text": "We ensure that the benchmark separates past test cases (i.e., regression test cases) from future test cases (i.e., test cases that encode functional requirements specified after the bug is reported). This timeline split is necessary to simulate the snapshot of the repository at the time the bug is reported. As highlighted in Section 2, for over 90% cases of bugs in the De-fects4J benchmark, the test cases relevant to the defective behavior was actually provided along the bug fixing patches. We have thus manually split the commits to identify test cases that should be considered as future test cases for each bug report.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Implementation Choices", "text": "During implementation, we have made the following parameter choices in the iFixR workflow:\n\u2022 IR fault localization considers the top 50 (i.e., k = 50 in Algorithm 1) suspicious files for each bug report, in order to search for buggy code locations. \u2022 For patch recommendation experiments, we limit the search space to the top 20 suspected buggy statements yielded by the fine-grained IR-based fault localization. \u2022 For comparison experiments, we implement spectrum-based fault localization using the GZoltar testing framework with the Ochiai ranking strategy. Unless otherwise indicated, GZoltar version 0.1.1 is used (as it is widely adopted in the literature, by Astor [59], ACS [92], ssFix [90] and CapGen [84] among others).", "publication_ref": ["b58", "b91", "b89", "b83"], "figure_ref": [], "table_ref": []}, {"heading": "Research Questions", "text": "The assessment objective is to assess the feasibility of automating the generation of patches for user-reported bugs, while investigating the foreseen bottlenecks as well as the research directions that the community must embrace to realize this long-standing endeavor. To that end, we focus on the following research questions associated with the different steps in the iFixR workflow.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "\u2022 RQ1 [Fault localization] :", "text": "To what extent does IR-based fault localization provide reliable results for an APR scenario? In particular, we investigate the performance differences when comparing our fine-grained IRFL implementation against the classical spectrumbased localization. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "ASSESSMENT RESULTS", "text": "In this section, we present the results of the investigations for the previously-enumerated research questions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "RQ1: [Fault Localization]", "text": "Fault localization being the first step in program repair, we evaluate the performance of the IR-based fault localization developed within iFixR. As recently thoroughly studied by Liu et al. [48], an APR tool should not be expected to fix a bug that current fault localization systems fail to localize. Nevertheless, with iFixR, we must demonstrate that our fine-grained IRFL offers comparable performance with SBFL tools used in the APR literature.\nTable 5 provides performance measurements on the localization of bugs. SBFL is performed based on two different versions of the GZoltar testing framework, but always based on the Ochiai ranking metric. Finally, because fault localization tools output a ranked list of suspicious statements, results are provided in terms of whether the correct location is placed under the top-k suspected statements. In this work, following the practice in the literature [48,56], we consider that a bug is localized if any buggy statement is localized. Overall, the results show that our IRFL implementation is strictly comparable to the common implementation of spectrum-based fault localization when applied on the Defects4J bug dataset. Note that the comparison is conducted for 171 bugs of Math and Lang, given that these are the projects for which the bug linking can be reliably performed for applying the IRFL. Although performance results are similar, we remind the reader that SBFL is applied by considering future test cases. To highlight a practical interest of IRFL, we compute for each bug localizable in the top-10, the elapsed time between the bug report date and the date the relevant test case is submitted for this bug. Based on the distribution shown in Figure 6, on mean average, IRFL could reduce this time by 26 days. Finally, to stress the importance of future test cases for spectrumbased fault localization, we consider all Defects4J bugs and compute localization performance with and without future test cases.\nResults listed in Table 6 confirms that in most bug cases, the localization is impossible: Only 10 bugs (out of 395) can be localized among the top-10 suspicious statements of SBFL at the time the bug is reported. In comparison, our IRFL locates 72 bugs under the same conditions of having no relevant test cases to trigger the bugs. Fine-grained IR-based fault localization in iFixR is as accurate as Spectrum-based fault localization in localizing Defects4J bugs. Additionally, it does not have the constraint of requiring test cases that may not be available when the bug is reported.", "publication_ref": ["b47", "b47", "b55"], "figure_ref": ["fig_6"], "table_ref": ["tab_8", "tab_9"]}, {"heading": "RQ2: [Overfitting]", "text": "Patch generation attempts to mutate suspected buggy code with suitable fix patterns. Aside from having adequate patterns or not (which is out of the scope of our study), a common challenge of APR lies in the effective selection of buggy statements. In typical test-based APR, test cases drive the selection of these statements. The incompleteness of test suites is however currently suspected to often lead to overfitting of generated patches [94].\nWe perform patch generation experiments to investigate the impact of localization bias. We compare our IRFL implementation against commonly-used SBFL implementations in the literature of test-based APR. We recall that the patch validation step in these experiments makes no assumptions about future test cases (i.e., all test cases are leveraged as in classical APR pipeline). For each bug, depending on the rank of the buggy statements in the suspicious statements yielded the fault localization system (either IRFL or SBFL), the patch generation can produce more or less relevant patches. Table 7 details the repair performance in relation to the position of buggy statements in the output of fault localization. Results are provided in terms of numbers of plausible and correct [69] patches that can be found by considering top-k statements returned by the fault localizer. x is the number of bugs for which a correct patch is generated; y is the number of bugs for which a plausible patch is generated.\nOverall, we find that IRFL and SBFL localization information lead to similar repair performance in terms of the number of fixed bugs plausibly/correctly. Actually IRFL-supported APR outperforms SBFL-supported APR on the Lang project bugs and vice-versa for Math project bugs: overall, 6 bugs that are fixed using IRFL output, cannot be fixed using SBFL output (although assuming the availability of the bug triggering test cases to run the SBFL tool).\nWe investigate the cases of plausible patches in both localization scenarios to characterize the reasons why these patches appear to only be overfitting the test suites. Table 8 details the overfitting reasons for the two scenarios. Table 8: Dissection of reasons why patches are plausible * but not correct.", "publication_ref": ["b93", "b68"], "figure_ref": [], "table_ref": ["tab_10"]}, {"heading": "Localization Error", "text": "Pattern Prioritization Lack of Fix ingredients w/ IRFL 6 1 16 w/ SBFL 15 1 10 * A plausible patch passes all test cases, but may not be semantically equivalent to developer patch (i.e., correct). We consider a plausible patch to be overfitted to the test suite (1) Among the 23(= 44 -21) plausible patches that are generated based on IRFL identified code locations and that are not found to be correct, 6 are found to be caused by fault localization errors: these bugs are plausibly fixed by mutating irrelevantlysuspicious statements that are placed before the actual buggy statements in the fault localization output list. This phenomenon has been recently investigated in the literature as the problem of fault localization bias [48]. Nevertheless, we note that patches generated based on SBFL identified code locations suffer more of fault localization bias: 15 of the 26 (= 50-24) plausible patches are concerned by this issue. (2) Pattern prioritization failures may also lead to plausible patches: while a correct patch could have been generated using a specific pattern at a lower node in the AST, another pattern (leading to an only plausible patch) was first found to be matching the statement during the iterative search of matching nodes (cf. Section 3.3). ( 3) Finally, we note that both configurations yield plausible patches due to the lack of suitable patterns or due to a failed search for the adequate donor code (i.e., fix ingredient [44]).\nExperiments with the Defects4J dataset suggest that code locations provided by IR-based fault localization lead less to overfitted patches than the code locations suggested by Spectrum-based fault localization: cf. \"Localization error\" column in Table 8.", "publication_ref": ["b47", "b43"], "figure_ref": [], "table_ref": []}, {"heading": "RQ3: [Patch Ordering]", "text": "While the previous experiment focused on patch generation, our final experiment assesses the complete pipeline of iFixR as it was imagined for meeting the constraints that developers can face in practice: future test cases, i.e., those which encode the functionality requirements that are not met by the buggy programs, may not be available at the time the bug is reported. We thus discard the future test cases of the Defects4J dataset and generate patches that must be recommended to developers. The evaluation protocol thus consists in assessing to what extent correct/plausible patches are placed in the top of the recommendation list.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "5.", "text": "3.1 Overall performance. Table 9 details the performance of the patch recommendation by iFixR: we present the number of bugs for which a correct/plausible patch is generated and presented among the top-k of the list of recommended patches. In the absence of future test cases to drive the patch validation process, we use heuristics (cf. Section 4.2) to re-prioritize the patch candidates towards ensuring that patches which are recommended first will eventually be correct (or at least plausible when relevant test cases are implemented). We present results both for the case where we do not re-prioritize and the case where we re-prioritize.\nRecall that, given that the re-organized benchmark separately includes the future test cases, we can leverage them to systematize the assessment of patch plausibility. The correctness (also referred to as correctness [69]) of patches, however, is still decided manually by comparing against the actual bug fix provided by developers and available in the benchmark. Overall, we note that iFixR performance is promising as it manages, for 13 bugs, to present a plausible patch among its top-5 recommended patches per bug. Among those plausible patches, 8 are eventually found to be correct.  [19] which uses sophisticated techniques to improve the fault localization accuracy and search for fix ingredients. It should be noted that in this case, our prioritization strategy is not applied to the generated patches. iFixR opt represents the reference performance for our experiment which assesses the prioritization. Table 10 provides the comparison matrix. Information on stateof-the-art results are excerpted from their respective publications.\niFixR offers a reasonable performance in patch recommendation when we consider the number of Defects4J bugs that are successfully patched among the top-5 (in a scenario where we assume not having relevant test cases to validate the patch candidates). Performance results are even comparable to many state-of-the-art test-based APR tools in the literature.", "publication_ref": ["b68", "b18"], "figure_ref": [], "table_ref": ["tab_11", "tab_12"]}, {"heading": "5.3.3", "text": "Properties of iFixR's patches. In Table 11, we characterize the correct and plausible patches recommended by iFixR top5 . Overall, update and insert changes have been successful; most patches affect a single statement, and impact precisely an expression entity within a statement.   ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_13"]}, {"heading": "5.3.4", "text": "Diversity of iFixR's fixed bugs. Finally, in Table 12 we dissect the nature of the bugs for which iFixR t op5 is able to recommend a correct or a plausible patch. Priority information about the bug report is collected from the issue tracking systems, while the root cause is inferred by analyzing the bug reports and fixes. Overall, we note that 9 out of the 13 bugs have been marked as Major issues. 12 different bug types (i.e., root causes) are addressed. In contrast, R2Fix [43] only focused on 3 simple bug types.", "publication_ref": ["b42"], "figure_ref": [], "table_ref": ["tab_14"]}, {"heading": "DISCUSSION", "text": "This study presents the conclusions of our investigation into the feasibility of generating patches automatically from bug reports. We set strong constraints on the absence of test cases, which are used in test-based APR to approximate what the program is actually supposed to do and when the repair is completed [83]. Our experiments on the widely-used Defects4J bugs eventually show that patch generation without bug-triggering test cases is promising.\nManually looking at the details of failures and success in generating patches with iFixR, several insights can be drawn: Test cases can be buggy: During manual analysis of results, we noted that iFixR actually fails to generate correct patches for three bugs (namely, Math-5, Math-59 and Math-65) because even the test cases were buggy. Figure 7 illustrates the example of bug Math-5 where its patch also updated the relevant test case. This example supports our endeavor, given that users would find and report bugs for which the appropriate test cases were never properly written.\n// Patched Source Code: ---a/src/main/java/org/apache/commons/math3/complex/Complex.java +++ b/src/main/java/org/apache/commons/math3/complex/Complex.java @@ -304,3 +304,3 @@ public class Complex if (real == 0.0 && imaginary == 0.  Bug reports deserve more interest: With iFixR, we have shown that bug reports could be handled automatically for a variety of bugs. This is an opportunity for issue trackers to add a recommendation layer to the bug triaging process by integrating patch generation techniques. There are, however, several directions to further investigation, among which: (1) help users write proper bug reports; and (2) re-investigate IRFL techniques at a finer-grained level that is suitable for APR. Prioritization techniques must be investigated: In the absence of complete test suites for validating every single patch candidate, a recommendation system must ensure that patches presented first to the developers are the most likely to be plausible and even correct. There are thus two directions of research that are promising: (1) ensure that fix patterns are properly prioritized to generate good patches and be able to early-stop for not exploding the search space; and (2) ensure that candidate patches are effectively re-prioritized. These investigations must start with a thorough dissection of plausible patches for a deep understanding of plausibility factors. More sophisticated approaches to triaging and selecting fix ingredients are necessary: In its current form, iFixR implements a na\u00efve approach to patch generation, ensuring that the performance is tractable. However, the literature already includes novel APR techniques that implement strategies for selecting donor code and filters patterns. Integrating such techniques into iFixR may lead to performance improvement. More comprehensive benchmarks are needed: Due to bug linking challenges, our experiments were only performed on half of the Defects4J benchmark. To drive strong research in patch generation for user-reported bugs, the community must build larger and reliable benchmarks, potentially even linking several artifacts of continuous integration (i.e, build logs, past execution traces, etc.). In the future, we plan to investigate the dataset of Bugs.jar [71]. Automatic test generation techniques could be used as a supplement: Our study tries to cope radically with the incompleteness of test suites. In the future, however, we could investigate the use of automatic test generation techniques to supplement the regression test cases during patch validation.\nThreats to external validity: The bug reports used in this study may be of low quality (i.e., wrong links for corresponding bugs). We reduced this threat by focusing only on bugs from the Lang and Math projects, which kept a single issue tracking system. We also manually verified the links between the bug reports and the Defects4J bugs. Table 13 characterizes the bug reports of our dataset following the criteria enumerated by Zimmermann et al. [100] in their study of \"what makes a good bug report\". Notably, as illustrated by the distribution of comments in Figure 8, we note that the bug reports have been actively discussed before being resolved. This suggests that they are not trivial cases (cf. [17] on measuring bug report significance). Code-related terms such as package/class names found in the summary and description, in addition to stack traces and code blocks, as separate features referred to as hints. Another threat to external validity relates to the diversity of the fix patterns used in this study. iFixR currently may not implement a reasonable number of relevant fix patterns. We minimize this threat by surveying the literature and considering patterns from several pattern-based APR. Threats to internal validity: Our implementation of fine-grained IRFL carries some threats: during the search of buggy statements, we considered top-50 suspicious buggy files from the file-level IRFL tool, to limit the search space. Different threshold values may lead to different results. We also considered only 5 statement types as more bug-prone. This second threat is minimized by the empirical evidence provided by Liu et al. [47].\nAdditionally, another internal threat is in our patch generation steps: iFixR only searches for donor code from the local code files, which contain the buggy statement. The adequate fix ingredient may however be located elsewhere. Threats to construct validity: In this study, we assumed that patch construction and test case creation are two separated tasks for developers. This may not be the case in practice. The threat is however mitigated given that, in any case, we have shown that the test cases are often unavailable when the bug is reported.", "publication_ref": ["b82", "b0", "b70", "b99", "b16", "b46"], "figure_ref": ["fig_8", "fig_9"], "table_ref": ["tab_16"]}, {"heading": "RELATED WORK", "text": "Fault Localization. As stated in a recent study [48], fault localization is a critical task affecting the effectiveness of automated program repair. Several techniques have been proposed [67,80,88] and they use different information such as spectrum [4], text [85], slice [58], and statistics [42]. The first two types of techniques are widely studies in the community. SBFL techniques [3,20] are widely adopted in APR pipelines since they identify bug positions at a fine-grained level (i.e., statements). However, they have limitations on localizing buggy locations since it highly relies on the test suite [48]. Information retrieval based fault localization (IRFL) [41] leverages textual information in a bug report. It is mainly used to help developers narrow down suspected buggy files in the absence of relevant test cases. For the purpose of our study, we have proposed an algorithm for further localizing the faulty code entities at the statement level. Patch Generation. Patch generation is another key process of APR pipeline, which is, in other words, a task searching for another shape of a program (i.e., a patch) in the space of all possible programs [37,54]. To improve repair performance, many APR systems have been explored to address the search space problem by using different information and approaches: stochastic mutation [38,83], synthesis [53,92,93], pattern [14,18,19,26,32,36,49,51,52,73], contract [12,82], symbolic execution [65], learning [8,16,55,70,77,86], and donor code searching [25,62]. In this paper, patch generation is implemented with fix patterns presented in the literature since it may make the generated patches more robust [74]. Patch Validation. The ultimate goal of APR systems is to automatically generate a correct patch that can actually resolve the program defects rather than satisfying minimal functional constraints. At the beginning, patch correctness is evaluated by passing all test cases [26,36,83]. However, these patches could be overfitting [34,69] and even worse than the bug [76]. Since then, APR systems are evaluated with the precision of generating correct patches [19,49,84,92]. Recently, researchers explore automated frameworks that can identify patch correctness for APR systems automatically [35,91]. In this paper, our approach validates generated patches with regression test suites since fail-inducing test cases are readily available for most of bugs as described in Section 2.", "publication_ref": ["b47", "b66", "b79", "b87", "b3", "b84", "b57", "b41", "b2", "b19", "b47", "b40", "b36", "b53", "b37", "b82", "b52", "b91", "b92", "b13", "b17", "b18", "b25", "b31", "b35", "b48", "b50", "b51", "b72", "b11", "b81", "b64", "b7", "b15", "b54", "b69", "b76", "b85", "b24", "b61", "b73", "b25", "b35", "b82", "b33", "b68", "b75", "b18", "b48", "b83", "b91", "b34", "b90"], "figure_ref": [], "table_ref": []}, {"heading": "CONCLUSION", "text": "In this study, we have investigated the feasibility of automating patch generation from bug reports. To that end, we implemented iFixR, an APR pipeline variant adapted to the constraints of test cases unavailability when users report bugs. The proposed system revisits the fundamental steps, notably fault localization, patch generation and patch validation, which are all tightly-dependent to the positive test cases [83] in a test-based APR system.\nWithout making any assumptions on the availability of test cases, we demonstrate, after re-organizing the Defects4J benchmark, that iFixR can generate and recommend priority correct (and more plausible) patches for a diverse set of user-reported bugs. The repair performance of iFixR is even found to be comparable to that of the majority of test-based APR systems on the Defects4J dataset.\nWe open source iFixR's code and release all data of this study to facilitate replication and encourage further research in this direction which is promising for practical adoption in the software development community: https://github.com/SerVal-DTF/iFixR ", "publication_ref": ["b82"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "D&C", "journal": "", "year": "2019", "authors": ""}, {"ref_id": "b1", "title": "On the accuracy of spectrum-based fault localization", "journal": "IEEE", "year": "2007", "authors": "Rui Abreu; J C Arjan; Peter Van Gemund;  Zoeteweij"}, {"ref_id": "b2", "title": "A practical evaluation of spectrum-based fault localization", "journal": "JSS", "year": "2009", "authors": "Rui Abreu; Peter Zoeteweij; Rob Golsteijn; Arjan Jc Van Gemund"}, {"ref_id": "b3", "title": "Spectrum-based multiple fault localization", "journal": "", "year": "2009", "authors": "Rui Abreu; Peter Zoeteweij; Arjan Jc Van Gemund"}, {"ref_id": "b4", "title": "Who should fix this bug?", "journal": "", "year": "2006", "authors": "John Anvik; Lyndon Hiew; Gail C Murphy"}, {"ref_id": "b5", "title": "Test-driven development: by example", "journal": "Addison-Wesley Professional", "year": "2003", "authors": "Kent Beck"}, {"ref_id": "b6", "title": "When, how, and why developers (do not) test in their IDEs", "journal": "", "year": "2015", "authors": "Moritz Beller; Georgios Gousios; Annibale Panichella; Andy Zaidman"}, {"ref_id": "b7", "title": "Neuro-symbolic program corrector for introductory programming assignments", "journal": "", "year": "2018", "authors": "Sahil Bhatia; Pushmeet Kohli; Rishabh Singh"}, {"ref_id": "b8", "title": "Harvesting Fix Hints in the History of Bugs", "journal": "", "year": "2015", "authors": "F Tegawend\u00e9;  Bissyand\u00e9"}, {"ref_id": "b9", "title": "Where is the bug and how is it fixed? an experiment with practitioners", "journal": "", "year": "2017", "authors": "Marcel B\u00f6hme; Sudipta Ezekiel O Soremekun; Emamurho Chattopadhyay; Andreas Ugherughe;  Zeller"}, {"ref_id": "b10", "title": "Gzoltar: an eclipse plug-in for testing and debugging", "journal": "", "year": "2012", "authors": "Jos\u00e9 Campos; Andr\u00e9 Riboira; Alexandre Perez; Rui Abreu"}, {"ref_id": "b11", "title": "Contract-based program repair without the contracts", "journal": "", "year": "2017", "authors": "Liushan Chen; Yu Pei; Carlo A Furia"}, {"ref_id": "b12", "title": "Program transformations to fix C integers", "journal": "", "year": "2013", "authors": "Zack Coker; Munawar Hafiz"}, {"ref_id": "b13", "title": "Dynamic patch generation for null pointer exceptions using metaprogramming", "journal": "", "year": "2017", "authors": "Thomas Durieux; Benoit Cornu; Lionel Seinturier; Martin Monperrus"}, {"ref_id": "b14", "title": "Populating a release history database from version control and bug tracking systems", "journal": "", "year": "2003", "authors": "Michael Fischer; Martin Pinzger; Harald Gall"}, {"ref_id": "b15", "title": "DeepFix: Fixing Common C Language Errors by Deep Learning", "journal": "AAAI Press", "year": "2017", "authors": "Rahul Gupta; Soham Pal; Aditya Kanade; Shirish Shevade"}, {"ref_id": "b16", "title": "Modeling bug report quality", "journal": "", "year": "2007", "authors": "Pieter Hooimeijer; Westley Weimer"}, {"ref_id": "b17", "title": "Towards practical program repair with on-demand candidate generation", "journal": "", "year": "2018", "authors": "Jinru Hua; Mengshi Zhang; Kaiyuan Wang; Sarfraz Khurshid"}, {"ref_id": "b18", "title": "Shaping Program Repair Space with Existing Patches and Similar Code", "journal": "ACM", "year": "2018", "authors": "Jiajun Jiang; Yingfei Xiong; Hongyu Zhang; Qing Gao; Xiangqun Chen"}, {"ref_id": "b19", "title": "Empirical evaluation of the tarantula automatic fault-localization technique", "journal": "", "year": "2005", "authors": "A James; Mary Jones; Harrold Jean"}, {"ref_id": "b20", "title": "Defects4J: A database of existing faults to enable controlled testing studies for Java programs", "journal": "ACM", "year": "2014", "authors": "Ren\u00e9 Just; Darioush Jalali; Michael D Ernst"}, {"ref_id": "b21", "title": "Comparing developer-provided to user-provided tests for fault localization and automated program repair", "journal": "ACM", "year": "2018", "authors": "Ren\u00e9 Just; Chris Parnin; Ian Drosos; Michael D Ernst"}, {"ref_id": "b22", "title": "Guest editors' introduction: Software testing practices in industry", "journal": "IEEE Software", "year": "2006", "authors": "Natalia Juristo; Juzgado ; Ana Mar\u00eda Moreno; Wolfgang Strigel"}, {"ref_id": "b23", "title": "Information retrieval with porter stemmer: a new version for English", "journal": "Springer", "year": "2013", "authors": "Wahiba Ben; Abdessalem Karaa; Nidhal Grib\u00e2a"}, {"ref_id": "b24", "title": "Repairing programs with semantic code search (t)", "journal": "", "year": "2015", "authors": "Yalin Ke; Kathryn T Stolee; Claire Le Goues; Yuriy Brun"}, {"ref_id": "b25", "title": "Automatic patch generation learned from human-written patches", "journal": "", "year": "2013", "authors": "Dongsun Kim; Jaechang Nam; Jaewoo Song; Sunghun Kim"}, {"ref_id": "b26", "title": "An empirical study of adoption of software testing in open source projects", "journal": "", "year": "2013", "authors": "Pavneet Singh Kochhar; F Tegawend\u00e9; David Bissyand\u00e9; Lingxiao Lo;  Jiang"}, {"ref_id": "b27", "title": "Elements of the Theory of Functions and Functional Analysis (dover books on mathematics", "journal": "Dover Publications", "year": "1999", "authors": "A N Kolmogorov; S V Fomin"}, {"ref_id": "b28", "title": "Impact of tool support in patch construction", "journal": "ACM", "year": "2017", "authors": "Anil Koyuncu; F Tegawend\u00e9; Dongsun Bissyand\u00e9; Jacques Kim; Martin Klein; Yves Monperrus;  Le Traon"}, {"ref_id": "b29", "title": "D&C: A Divide-and-Conquer Approach to IR-based Bug Localization", "journal": "", "year": "2019", "authors": "Anil Koyuncu; F Tegawend\u00e9; Dongsun Bissyand\u00e9; Kui Kim; Jacques Liu; Martin Klein; Yves Monperrus;  Le Traon"}, {"ref_id": "b30", "title": "FixMiner: Mining Relevant Fix Patterns for Automated Program Repair", "journal": "", "year": "2018", "authors": "Anil Koyuncu; Kui Liu; Tegawend\u00e9 F Bissyand\u00e9; Dongsun Kim; Jacques Klein; Martin Monperrus; Yves Le Traon"}, {"ref_id": "b31", "title": "S3: syntax-and semantic-guided repair synthesis via programming by examples", "journal": "", "year": "2017", "authors": "Xuan-Bach D Le; Duc-Hiep Chu; David Lo; Claire Le Goues; Willem Visser"}, {"ref_id": "b32", "title": "Enhancing automated program repair with deductive verification", "journal": "", "year": "2016", "authors": "Xuan-Bach D Le; Quang Loc Le; David Lo; Claire Le Goues"}, {"ref_id": "b33", "title": "Overfitting in semantics-based automated program repair", "journal": "EMSE Journal", "year": "2018", "authors": "Xuan Bach; D Le; Ferdian Thung; David Lo; Claire Le Goues"}, {"ref_id": "b34", "title": "On Reliability of Patch Correctness Assessment", "journal": "", "year": "2019", "authors": "D Xuan-Bach; Lingfeng Le; David Bao; Xin Lo; Shanping Xia;  Li"}, {"ref_id": "b35", "title": "History Driven Program Repair", "journal": "IEEE", "year": "2016", "authors": "D Xuan-Bach; David Le; Claire Le Lo;  Goues"}, {"ref_id": "b36", "title": "A systematic study of automated program repair: Fixing 55 out of 105 bugs for $8 each", "journal": "", "year": "2012", "authors": "Claire Le Goues; Michael Dewey-Vogt; Stephanie Forrest; Westley Weimer"}, {"ref_id": "b37", "title": "GenProg: A generic method for automatic software repair", "journal": "TSE", "year": "2012", "authors": "Claire Le Goues; Thanhvu Nguyen; Stephanie Forrest; Westley Weimer"}, {"ref_id": "b38", "title": "GenProg: A Generic Method for Automatic Software Repair", "journal": "TSE", "year": "2012", "authors": "Claire Le Goues; Thanhvu Nguyen; Stephanie Forrest; Westley Weimer"}, {"ref_id": "b39", "title": "Specification mining with few false positives", "journal": "Springer", "year": "2009", "authors": "Claire Le; Goues ; Westley Weimer"}, {"ref_id": "b40", "title": "Bench4bl: reproducibility study on the performance of ir-based bug localization", "journal": "ACM", "year": "2018", "authors": "Jaekwon Lee; Dongsun Kim; F Tegawend\u00e9; Woosung Bissyand\u00e9; Yves Jung;  Le Traon"}, {"ref_id": "b41", "title": "Scalable statistical bug isolation", "journal": "", "year": "2005", "authors": "Ben Liblit; Mayur Naik; Alice X Zheng; Alex Aiken; Michael I Jordan"}, {"ref_id": "b42", "title": "R2Fix: Automatically generating bug fixes from bug reports", "journal": "", "year": "2013", "authors": "Chen Liu; Jinqiu Yang; Lin Tan; Munawar Hafiz"}, {"ref_id": "b43", "title": "LSRepair: Live Search of Fix Ingredients for Automated Program Repair", "journal": "", "year": "2018", "authors": "Kui Liu; Koyuncu Anil; Kisub Kim; Dongsun Kim; Tegawend\u00e9 F Bissyand\u00e9"}, {"ref_id": "b44", "title": "Learning to Spot and Refactor Inconsistent Method Names", "journal": "", "year": "2019", "authors": "Kui Liu; Dongsun Kim; Tegawend\u00e9 F Bissyand\u00e9; Taeyoung Kim; Kisub Kim; Anil Koyuncu; Suntae Kim; Yves Le Traon"}, {"ref_id": "b45", "title": "Mining fix patterns for findbugs violations", "journal": "TSE", "year": "2018", "authors": "Kui Liu; Dongsun Kim; F Tegawend\u00e9; Shin Bissyand\u00e9; Yves Yoo;  Le Traon"}, {"ref_id": "b46", "title": "A closer look at real-world patches", "journal": "", "year": "2018", "authors": "Kui Liu; Dongsun Kim; Anil Koyuncu; Li Li; Tegawend\u00e9 F Bissyand\u00e9; Yves Le Traon"}, {"ref_id": "b47", "title": "You Cannot Fix What You Cannot Find! An Investigation of Fault Localization Bias in Benchmarking Automated Program Repair Systems", "journal": "", "year": "2019", "authors": "Kui Liu; Anil Koyuncu; F Tegawend\u00e9; Dongsun Bissyand\u00e9; Jacques Kim; Yves Klein;  Le Traon"}, {"ref_id": "b48", "title": "AVATAR: Fixing Semantic Bugs with Fix Patterns of Static Analysis Violations", "journal": "", "year": "2019", "authors": "Kui Liu; Anil Koyuncu; Dongsun Kim; Tegawend\u00e9 F Bissyand\u00e9"}, {"ref_id": "b49", "title": "TBar : Revisiting Template-based Automated Program Repair", "journal": "ACM", "year": "2019", "authors": "Kui Liu; Anil Koyuncu; Dongsun Kim; Tegawend\u00e9 F Bissyand\u00e9"}, {"ref_id": "b50", "title": "Mining stackoverflow for program repair", "journal": "", "year": "2018", "authors": "Xuliang Liu; Hao Zhong"}, {"ref_id": "b51", "title": "Automatic inference of code transforms for patch generation", "journal": "", "year": "2017", "authors": "Fan Long; Peter Amidon; Martin Rinard"}, {"ref_id": "b52", "title": "Staged program repair with condition synthesis", "journal": "", "year": "2015", "authors": "Fan Long; Martin Rinard"}, {"ref_id": "b53", "title": "An analysis of the search spaces for generate and validate patch generation systems", "journal": "", "year": "2016", "authors": "Fan Long; Martin Rinard"}, {"ref_id": "b54", "title": "Automatic patch generation by learning correct code", "journal": "ACM", "year": "2016", "authors": "Fan Long; Martin Rinard"}, {"ref_id": "b55", "title": "Are Faults Localizable?", "journal": "", "year": "2012", "authors": "Lucia Lucia; Ferdian Thung; David Lo; Lingxiao Jiang"}, {"ref_id": "b56", "title": "Bug localization using latent Dirichlet allocation", "journal": "IST", "year": "2010", "authors": "Nicholas A Stacy K Lukins; Letha H Kraft;  Etzkorn"}, {"ref_id": "b57", "title": "Slice-based statistical fault localization", "journal": "JSS", "year": "2014", "authors": "Xiaoguang Mao; Yan Lei; Ziying Dai; Yuhua Qi; Chengsong Wang"}, {"ref_id": "b58", "title": "Astor: A program repair library for java", "journal": "ACM", "year": "2016", "authors": "Matias Martinez; Martin Monperrus"}, {"ref_id": "b59", "title": "Ultra-Large Repair Search Space with Automatically Mined Templates: The Cardumen Mode of Astor", "journal": "Springer", "year": "2018", "authors": "Matias Martinez; Martin Monperrus"}, {"ref_id": "b60", "title": "Semantic Program Repair Using a Reference Implementation", "journal": "", "year": "2018", "authors": "Sergey Mechtaev; Manh-Dung Nguyen; Yannic Noller; Lars Grunske; Abhik Roychoudhury"}, {"ref_id": "b61", "title": "Directfix: Looking for simple program repairs", "journal": "IEEE Press", "year": "2015", "authors": "Sergey Mechtaev; Jooyong Yi; Abhik Roychoudhury"}, {"ref_id": "b62", "title": "A critical review of automatic patch generation learned from human-written patches: essay on the problem statement and the evaluation of automatic software repair", "journal": "", "year": "2014", "authors": "Martin Monperrus"}, {"ref_id": "b63", "title": "CLEVER: combining code metrics with clone detection for just-in-time fault prevention and resolution in large industrial projects", "journal": "ACM", "year": "2018", "authors": "Mathieu Nayrolles; Abdelwahab Hamou-Lhadj"}, {"ref_id": "b64", "title": "SemFix: program repair via semantic analysis", "journal": "", "year": "2013", "authors": "Hoang Duong; Thien Nguyen; Dawei Qi; Abhik Roychoudhury; Satish Chandra"}, {"ref_id": "b65", "title": "Documenting and automating collateral evolutions in Linux device drivers", "journal": "ACM", "year": "2008", "authors": "Yoann Padioleau; Julia Lawall; Ren\u00e9 Rydhof Hansen; Gilles Muller"}, {"ref_id": "b66", "title": "Are automated debugging techniques actually helping programmers?", "journal": "ACM", "year": "2011", "authors": "Chris Parnin; Alessandro Orso"}, {"ref_id": "b67", "title": "How Effectively Is Defective Code Actually Tested?: An Analysis of JUnit Tests in Seven Open Source Systems", "journal": "ACM", "year": "2018", "authors": "Jean Petri\u0107; Tracy Hall; David Bowes"}, {"ref_id": "b68", "title": "An analysis of patch plausibility and correctness for generate-and-validate patch generation systems", "journal": "", "year": "2015", "authors": "Zichao Qi; Fan Long; Sara Achour; Martin Rinard"}, {"ref_id": "b69", "title": "Learning syntactic program transformations from examples", "journal": "", "year": "2017", "authors": "Reudismam Rolim; Gustavo Soares; Loris D 'antoni; Oleksandr Polozov; Sumit Gulwani; Rohit Gheyi; Ryo Suzuki; Bj\u00f6rn Hartmann"}, {"ref_id": "b70", "title": "Bugs.jar: a large-scale, diverse dataset of real-world java bugs", "journal": "", "year": "2018", "authors": "Ripon Saha; Yingjun Lyu; Wing Lam; Hiroaki Yoshida; Mukul Prasad"}, {"ref_id": "b71", "title": "Improving bug localization using structured information retrieval", "journal": "", "year": "2013", "authors": "K Ripon; Matthew Saha; Sarfraz Lease; Dewayne E Khurshid;  Perry"}, {"ref_id": "b72", "title": "ELIXIR: Effective object-oriented program repair", "journal": "", "year": "2017", "authors": "K Ripon; Yingjun Saha; Hiroaki Lyu; Mukul R Yoshida;  Prasad"}, {"ref_id": "b73", "title": "Software mutational robustness", "journal": "Genetic Programming and Evolvable Machines", "year": "2014", "authors": "Eric Schulte; Zachary P Fry; Ethan Fast; Westley Weimer; Stephanie Forrest"}, {"ref_id": "b74", "title": "Getafix: Learning to fix bugs automatically", "journal": "", "year": "2019", "authors": "Andrew Scott; Johannes Bader; Satish Chandra"}, {"ref_id": "b75", "title": "Is the cure worse than the disease? Overfitting in automated program repair", "journal": "", "year": "2015", "authors": "K Edward; Earl T Smith; Claire Le Barr; Yuriy Goues;  Brun"}, {"ref_id": "b76", "title": "Using a probabilistic model to predict bug fixes", "journal": "", "year": "2018", "authors": "Mauricio Soto; Claire Le; Goues "}, {"ref_id": "b77", "title": "The impact of classifier configuration and classifier combination on bug localization", "journal": "TSE", "year": "2013", "authors": "Meiyappan Stephen W Thomas; Dorothea Nagappan; Ahmed E Blostein;  Hassan"}, {"ref_id": "b78", "title": "How to design a program repair bot?: insights from the repairnator project", "journal": "", "year": "2018", "authors": "Simon Urli; Zhongxing Yu; Lionel Seinturier; Martin Monperrus"}, {"ref_id": "b79", "title": "Evaluating the usefulness of IR-based fault localization techniques", "journal": "", "year": "2015", "authors": "Qianqian Wang; Chris Parnin; Alessandro Orso"}, {"ref_id": "b80", "title": "Version History, Similar Report, and Structure: Putting Them Together for Improved Bug Localization", "journal": "ACM", "year": "2014", "authors": "Shaowei Wang; David Lo"}, {"ref_id": "b81", "title": "Automated fixing of programs with contracts", "journal": "ACM", "year": "2010", "authors": "Yi Wei; Yu Pei; Carlo A Furia; Lucas S Silva; Stefan Buchholz; Bertrand Meyer; Andreas Zeller"}, {"ref_id": "b82", "title": "Automatically finding patches using genetic programming", "journal": "", "year": "2009", "authors": "Westley Weimer; Thanhvu Nguyen; Claire Le Goues; Stephanie Forrest"}, {"ref_id": "b83", "title": "Context-Aware Patch Generation for Better Automated Program Repair", "journal": "", "year": "2018", "authors": "Ming Wen; Junjie Chen; Rongxin Wu; Dan Hao; Shing-Chi Cheung"}, {"ref_id": "b84", "title": "Locus: Locating bugs from software changes", "journal": "", "year": "2016", "authors": "Ming Wen; Rongxin Wu; Shing-Chi Cheung"}, {"ref_id": "b85", "title": "Sorting and Transforming Program Repair Ingredients via Deep Learning Code Similarities", "journal": "IEEE", "year": "2019", "authors": "Martin White; Michele Tufano; Matias Martinez; Martin Monperrus; Denys Poshyvanyk"}, {"ref_id": "b86", "title": "Boosting Bug-Report-Oriented Fault Localization with Segmentation and Stack-Trace Analysis", "journal": "", "year": "2014", "authors": "Chu-Pan Wong; Yingfei Xiong; Hongyu Zhang; Dan Hao; Lu Zhang; Hong Mei"}, {"ref_id": "b87", "title": "A survey on software fault localization", "journal": "TSE", "year": "2016", "authors": "Eric Wong; Ruizhi Gao; Yihao Li; Rui Abreu; Franz Wotawa"}, {"ref_id": "b88", "title": "Identifying test-suite-overfitted patches through test case generation", "journal": "ACM", "year": "2017", "authors": "Qi Xin; Steven P Reiss"}, {"ref_id": "b89", "title": "Leveraging syntax-related code for automated program repair", "journal": "", "year": "2017", "authors": "Qi Xin; Steven P Reiss"}, {"ref_id": "b90", "title": "Identifying patch correctness in test-based program repair", "journal": "", "year": "2018", "authors": "Yingfei Xiong; Xinyuan Liu; Muhan Zeng; Lu Zhang; Gang Huang"}, {"ref_id": "b91", "title": "Precise condition synthesis for program repair", "journal": "", "year": "2017", "authors": "Yingfei Xiong; Jie Wang; Runfa Yan; Jiachen Zhang; Shi Han; Gang Huang; Lu Zhang"}, {"ref_id": "b92", "title": "Nopol: Automatic repair of conditional statement bugs in java programs", "journal": "TSE", "year": "2017", "authors": "Jifeng Xuan; Matias Martinez; Favio Demarco; Maxime Clement; Sebastian Lamelas Marcote; Thomas Durieux; Daniel Le Berre; Martin Monperrus"}, {"ref_id": "b93", "title": "Better test cases for better automated program repair", "journal": "", "year": "2017", "authors": "Jinqiu Yang; Alexey Zhikhartsev; Yuefei Liu; Lin Tan"}, {"ref_id": "b94", "title": "Regression testing minimization, selection and prioritization: a survey", "journal": "STVR", "year": "2012", "authors": "Shin Yoo; Mark Harman"}, {"ref_id": "b95", "title": "Improved bug localization based on code change histories and bug reports", "journal": "IST", "year": "2017", "authors": "Klaus Changsun Youm; June Ahn; Eunseok Lee"}, {"ref_id": "b96", "title": "Test case generation for program repair: A study of feasibility and effectiveness", "journal": "", "year": "2017", "authors": "Zhongxing Yu; Matias Martinez; Benjamin Danglot; Thomas Durieux; Martin Monperrus"}, {"ref_id": "b97", "title": "Alleviating patch overfitting with automatic test generation: a study of feasibility and effectiveness for the Nopol repair system", "journal": "EMSE Journal", "year": "2018", "authors": "Zhongxing Yu; Matias Martinez; Benjamin Danglot; Thomas Durieux; Martin Monperrus"}, {"ref_id": "b98", "title": "Where should the bugs be fixed? more accurate information retrieval-based bug localization based on bug reports", "journal": "", "year": "2012", "authors": "Jian Zhou; Hongyu Zhang; David Lo"}, {"ref_id": "b99", "title": "What makes a good bug report", "journal": "TSE", "year": "2010", "authors": "Thomas Zimmermann; Rahul Premraj; Nicolas Bettenburg; Sascha Just; Adrian Schroter; Cathrin Weiss"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "282", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: Example of Linux bug report addressed by R2Fix.(1) which explicitly include localization information,(2) where the symptom (e.g., buffer overrun) is explicitly indicated by the reporter, and (3) which are about one of the following three simple bug types: Buffer overflow, Null Pointer dereference or memory leak. R2Fix runs a straightforward classification to identify the bug category and uses a match and transform engine (e.g., Coccinelle[66]) to generate patches. As the authors admitted, their target space represents <1% of bug reports in their dataset. Furthermore, it should be noted that, given the limited scope of the changes implemented in its fix patterns, R2Fix does not need to run tests for verifying that the generated patches do not break any functionality.This paper. We propose to investigate the feasibility of a program repair system driven by bug reports, thus we replace classical spectrum-based fault localization with Information Retrieval (IR)-based fault localization. Eventually, we propose iFixR, a new program repair workflow which considers a practical repair setup by imitating the fundamental steps of manual debugging. iFixR works under the following constraint:", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: The iFixR Program Repair Workflow.", "figure_data": ""}, {"figure_label": "123", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Algorithm 1 : 2 F 3 F123Statement-level IR-based Fault Localization. Input :br : a bug report Input : irT ool : IRFL tool Output : Sscor e : Suspicious Statements with weight scores 1 Function main (br ,irT ool ) \u2190 fileLocalizations (irT ool ,br ) \u2190 selectTop (F ,k )4 c b \u2190 bagOfTokens (br ) /* c b : Bag of Tokens of bug report */ 5 c \u2032 b \u2190 preprocess (c b ) /* tokenization,stopword removal, stemming */ 6 v b \u2190 tfIdfVectorizer(c \u2032 b ) /* v b", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "iFixR: Bug Report driven Program Repair ESEC/FSE '19, August 26-30, 2019, Tallinn, Estonia", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "FileFigure 4 :4Figure 4: Buggy code of Defects4J bug Math-75.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 6 :6Figure 6: Distribution of elapsed time (in days) between bug report submission and test case attachment.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "iFixR: Bug Report driven Program Repair ESEC/FSE '19, August 26-30, 2019, Tallinn, Estonia", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 7 :7Figure 7: Patched source code and test case of fixing Math-5.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 8 :8Figure 8: Distribution of # of comments per bug report.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Test case changes in fix commits of Defects4J bugs.", "figure_data": "Test case related commits# bugsCommit does not alter test cases14Commit is inserting new test case(s) and updating previous test case(s)62Commit is updating previous test case(s) (without inserting new test cases)76Commit is inserting new test case(s) (without updating previous test cases)243"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": "Failing test cases# bugsFailing test cases exist (and no future test cases are committed)14Failing test cases exist (but future test cases update the test scenarios)9Failing test cases exist (but they are fewer when considering future test cases)4Failing test cases exist (but they differ from future test cases which trigger the bug)3No failing test case exists (i.e., only future test cases trigger the bug)365"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": ": Bug report Feature Vector */", "figure_data": "7for f in F do8S \u2190 parse(f )/* S : List of statements */9for s in S do10cs \u2190 bagOfTokens (s )/* cs : Bag of Tokens of statements */11 12c \u2032 s \u2190 preprocess (cs ) vs \u2190 tfIdfVectorizer(c \u2032 s )/* vs : Statements Feature Vector */13/* Cosine similarity between bug report and statement*/14simcos \u2190 similarity cosine (v b ,vs )15wscor e \u2190 simcos \u00d7 f .score;/* score: Suspicious Value */18"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Fix patterns implemented in iFixR.", "figure_data": "Pattern descriptionused by  *Pattern descriptionused by  *Insert Cast CheckerGenesisMutate Literal ExpressionSimFixInsert Null Pointer CheckerNPEFixMutate Method InvocationELIXIRInsert Range CheckerSOFixMutate OperatorjMutRepairInsert Missed StatementHDRepairMutate Return StatementSketchFixMutate Conditional ExpressionssFixMutate VariableCapGenMutate Data TypeAVATARMove Statement(s)PARRemove Statement(s)FixMiner*  We mention only one example tool even when several tools implement it.+ if (exp instanceof T) {...(T) exp...; ......+ }Figure 3: Illustration of \"Insert Cast Checker\" fix pattern."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "What is the effectiveness of iFixR's patch ordering strategy? In particular, we investigate the overall workflow of iFixR, by re-simulating the real-world cases of software maintenance cycle when a bug is reported: future test cases are not available for patch validation.", "figure_data": "iFixR: Bug Report driven Program RepairESEC/FSE '19, August 26-30, 2019, Tallinn, Estonia\u2022 RQ3 [Patch ordering] :"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Fault localization results: IRFL (IR-based) vs. SBFL (Spectrum-based) on Defects4J (Math and Lang) bugs.", "figure_data": "(171 bugs)Top-1Top-10Top-50Top-100Top-200AllIRFL2572102117121139SBFLGZ v 1 GZ v 226 2375 79106 119110 135114 150120 156\u2020 GZ v 1 and GZ v 2 refer to GZoltar 0.1.1 and 1.6.0 respectively, which are widely used in APRsystems for Java programs."}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Fault localization performance.", "figure_data": "GZoltar + Ochiai (395 bugs)Top-1Top-10Top-50Top-100Top-200Allwithout future tests51017171920with future tests45140198214239263"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "IRFL vs. SBFL impacts on the number of generated correct/plausible patches for Defects4J bugs.", "figure_data": "LangMathTotalIRFL Top-11/43/44/8SBFL Top-11/46/87/12IRFL Top-53/67/1410/20SBFL Top-52/711/1713/24IRFL Top-104/99/1713/26SBFL Top-104/1116/2720/38IRFL Top-207/129/1816/30SBFL Top-204/1118/3022/41IRFL Top-507/1510/2217/37SBFL Top-504/1319/3423/47IRFL Top-1008/1810/2318/41SBFL Top-1005/1419/3624/50IRFL All11/1910/2521/44SBFL All5/1419/3624/50*  We indicate x/y numbers of patches:"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Overall performance of iFixR for patch recommendation on the Defects4J benchmark. * x/y: x is the number of bugs for which a correct patch is generated; y is the number of bugs for which a plausible patch is generated.5.3.2 Comparison with the state-of-the-art test-based APR systems.To objectively position the performance of iFixR (which does not require future test cases to localize bugs, generate patches and present a sorted recommendation list of patches), we count the number of bugs for which iFixR can propose a correct/plausible patch. We consider three scenarios with iFixR:(1) [iFixR top5 ] -developers will be provided with only top 5 recommended patches which have been validated only with regression tests: in this case, iFixR outperforms about half of the state-of-the-art in terms of numbers bugs fixed with both plausible or correct patches.", "figure_data": "Recommendation rankTop-1Top-5Top-10Top-20Allwithout patch re-prioritization3/34/56/106/1013/27with patch re-prioritization3/48/139/1410/1513/27(2) [iFixR all ] -developers are presented with all (i.e., not onlytop-5) generated patches validated with regression tests: in thiscase, only four (out of sixteen) state-of-the-art APR techniquesoutperform iFixR.(3) [iFixR opt ] -developers are presented with all generated patcheswhich have been validated with augmented test suites (i.e., opti-mistically with future test cases): with this configuration, iFixRoutperforms all state-of-the-art, except SimFix"}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "iFixR vs state-of-the-art APR tools. /y: x is the number of bugs for which a correct patch is generated; y is the number of bugs for which a plausible patch is generated. iFixRopt : the version of iFixR where available test cases are relevant to the bugs. iFixR al l : all recommended patches are considered. iFixR t op5 : only top 5 recommended patches are considered.", "figure_data": "APR toolLang  *Math  *Total  *jGenProg [59]0/05/185/18jKali [59]0/01/141/14jMutRepair [59]0/12/112/12HDRepair [36]2/64/76/13Nopol [93]3/71/214/28ACS [92]3/412/1615/20ELIXIR [73]8/1212/1920/31JAID [12]1/81/82/16ssFix [90]5/1210/2615/38CapGen [84]5/512/1617/21SketchFix [18]3/47/810/12FixMiner [31]2/312/1414/17LSRepair [44]8/147/1415/28SimFix [19]9/1314/2623/39kPAR [48]1/87/188/26AVATAR [49]5/116/1311/24iFixRopt11/1910/2521/44iFixR al l6/117/1613/27iFixR t op53/75/68/13*  x"}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Change properties of iFixR's correct patches. * x/y -\u2192 for x bugs the patches are correct, while for y bugs they are plausible.", "figure_data": "Change action#bugs  *Impacted statement(s)#bugs  *Granularity#bugs  *Update5/7Single-statement8/12Statement1/2Insert3/5Multiple-statement0/1Expression7/11Delete0/1"}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "Dissection of bugs successfully fixed by iFixR.", "figure_data": "Patch TypeDefect4J Bug IDIssue IDRoot CausePriorityGL-6LANG-857String index out of bounds exceptionMinorGL-24LANG-664Wrong behavior due missing conditionMajorGL-57LANG-304Null pointer exceptionMajorGM-15MATH-904Double precision floating point format errorMajorGM-34MATH-779Missing \"read only access\" to internal listMajorGM-35MATH-776Range checkMajorGM-57MATH-546Wrong variable type truncates double valueMinorGM-75MATH-329Method signature mismatchMinorPL-13LANG-788Serialization error in primitive typesMajorPL-21LANG-677Wrong Date Format in comparisonMajorPL-45LANG-419Range checkMinorPL-58LANG-300Number formatting errorMajorPM-2MATH-1021Integer overflowMajor\"G\" denotes correct patch and \"P\" means plausible patch."}, {"figure_label": "13", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "Dissection of bug reports related to Defects4J bugs.", "figure_data": "Proj.Unique Bug Reportsw/ Patch AttachedAverage Commentsw/ Stack Tracesw/ Hintsw/ Code BlocksLang62114.5346231Math100235.1559251"}], "formulas": [{"formula_id": "formula_0", "formula_text": "Type \"Comparable<?>\" VariableName \"v\" \u2460 \u2461 \u2462 \u2463 \u2464", "formula_coordinates": [5.0, 322.43, 88.75, 227.95, 57.08]}, {"formula_id": "formula_1", "formula_text": "f r cmd (patches) = (pri type \u2022 pri susp \u2022 pri chan\u0434e )(patches) (1)", "formula_coordinates": [5.0, 327.42, 440.38, 230.78, 10.4]}, {"formula_id": "formula_2", "formula_text": "P \u2192 P n where n = |patches |, pri chan\u0434e (patches) = [p i , p i+1 , p i+2 , \u2022 \u2022 \u2022 ] and holds \u2200p \u2208 patches, C chan\u0434e (p i ) \u2264 C chan\u0434e (p i+1 ).", "formula_coordinates": [5.0, 331.12, 543.86, 227.08, 34.67]}, {"formula_id": "formula_3", "formula_text": "P n \u2192 P n such that pri susp (seq chan\u0434e ) = [\u2022 \u2022 \u2022 , p i , p i+1 , \u2022 \u2022 \u2022 ] holds S susp (p i ) \u2265 S susp (p i+1 )", "formula_coordinates": [5.0, 331.45, 654.38, 226.75, 22.91]}, {"formula_id": "formula_4", "formula_text": "[p i , p i+1 , p i+2 , \u2022 \u2022 \u2022 ] that holds \u2200p i , D type (p i ) \u2265 D type (p i+1", "formula_coordinates": [6.0, 67.3, 295.68, 226.75, 20.74]}], "doi": "10.1145/3338906.3338935"}
