{"title": "-Lipschitz Layers Compared: Memory, Speed, and Certifiable Robustness", "authors": "Bernd Prach; Fabio Brau; Giorgio Buttazzo; Christoph H Lampert", "pub_date": "2023-11-28", "abstract": "The robustness of neural networks against input perturbations with bounded magnitude represents a serious concern in the deployment of deep learning models in safety-critical systems. Recently, the scientific community has focused on enhancing certifiable robustness guarantees by crafting 1-Lipschitz neural networks that leverage Lipschitz bounded dense and convolutional layers. Although different methods have been proposed in the literature to achieve this goal, understanding the performance of such methods is not straightforward, since different metrics can be relevant (e.g., training time, memory usage, accuracy, certifiable robustness) for different applications. For this reason, this work provides a thorough theoretical and empirical comparison between methods by evaluating them in terms of memory usage, speed, and certifiable robust accuracy. The paper also provides some guidelines and recommendations to support the user in selecting the methods that work best depending on the available resources.", "sections": [{"heading": "Introduction", "text": "Modern artificial neural networks achieve high accuracy and sometimes superhuman performance in many different tasks, but it is widely recognized that they are not robust to tiny and imperceptible input perturbations [3,33] that, if properly crafted, can cause a model to produce the wrong output. Such inputs, known as Adversarial Examples, represent a serious concern for the deployment of machine learning models in safety-critical systems [21]. For this reason, the scientific community is pushing towards guarantees of robustness. Roughly speaking, a model f is said to be \u03b5-robust for a given input x if no perturbation of magnitude bounded by \u03b5 can change its prediction. Recently, in the context of image classification, various approaches * Joined first authors. Scores are assigned from 1 (worst) to 5 (best) to every method based on the results reported in Sections 3 and 5.\nhave been proposed to achieve certifiable robustness, including Verification, Randomized Smoothing, and Lipschitz bounded Neural Networks.\nVerification strategies aim to establish, for any given model, whether all samples contained in a l 2 -ball with radius \u03b5 and centered in the tested input x are classified with the same class as x. In the exact formulation, verification strategies involve the solution of an NP-hard problem [15]. Nevertheless, even in a relaxed formulation, [38], these strategies require a huge computational effort [37].\nRandomized smoothing strategies, initially presented in [9], represent an effective way of crafting a certifiablerobust classifier g based on a base classifier f . If combined with an additional denoising step, they can achieve state-ofthe-art levels of robustness, [6]. However, since they require multiple evaluations of the base model (up to 100k evaluations) for the classification of a single input, they cannot be used for real-time applications.\nFinally, Lipschitz Bounded Neural Networks represent a valid alternative to produce certifiable classifiers, since they only require a single forward pass of the model at inference time [5,8,19,22,24,28,34]. Indeed, the difference between the two largest output components of the model directly provides a lower-bound, in Euclidean norm, of the minimal adversarial perturbation capable of fooling the model. Lipschitz-bounded neural networks can be obtained by the composition of 1-Lipschitz layers [1]. The process of parameterizing 1-Lipschitz layers is fairly straightforward for fully connected layers. However, for convolutionswith overlapping kernels -deducing an effective parameterization is a hard problem. Indeed, the Lipschitz condition can be essentially thought of as a condition on the Jacobian of the layer. However, the Jacobian matrix can not be efficiently computed.\nIn order to avoid the explicit computation of the Jacobian, various methods have been proposed, including parameterizations that cause the Jacobian to be (very close to) orthogonal [22,30,34,40] and methods that rely on an upper bound on the Jacobian instead [28]. Those different methods differ drastically in training and validation requirements (in particular time and memory) as well as empirical performance. Furthermore, increasing training time or model sizes very often also increases the empirical performance. This makes it hard to judge from the existing literature which methods are the most promising. This becomes even worse when working with specific computation requirements, such as restrictions on the available memory. In this case, it is important to choose the method that better suits the characteristics of the system in terms of evaluation time, memory usage as well and certifiable-robust-accuracy.\nThis works aims at giving a comprehensive comparison of different strategies for crafting 1-Lipschitz layers from both a theoretical and practical perspective. For the sake of fairness, we consider several metrics such as Time and Memory requirements for both training and inference, Accuracy, as well as Certified Robust Accuracy. The main contributions are the following:\n\u2022 An empirical comparison of 1-Lipschitz layers based on six different metrics, and three different datasets on four architecture sizes with three time constraints. \u2022 A theoretical comparison of the runtime complexity and the memory usage of existing methods. \u2022 A review of the most recent methods in the literature, including implementations with a revised code that we will release publicly for other researchers to build on.", "publication_ref": ["b2", "b32", "b20", "b14", "b37", "b36", "b8", "b5", "b4", "b7", "b18", "b21", "b23", "b27", "b33", "b0", "b21", "b29", "b33", "b27"], "figure_ref": [], "table_ref": []}, {"heading": "Existing Works and Background", "text": "In recent years, various methods have been proposed for creating artificial neural networks with a bounded Lipschitz constant. The Lipschitz constant of a function f : R n \u2192 R m with respect to the l 2 norm is the smallest\nL such that for all x, y \u2208 R n \u2225f (x) -f (y)\u2225 2 \u2264 L\u2225x -y\u2225 2 .(1)\nWe also extend this definition to networks and layers, by considering the l 2 norms of the flattened input and output tensors in Equation (1). A layer is called 1-Lipschitz if its Lipschitz constant is at most 1. For linear layers, the Lipschitz constant is equal to the spectral norm of the weight matrix that is given as\n\u2225M \u2225 2 = sup v\u0338 =0 \u2225M v\u2225 2 \u2225v\u2225 2 .(2)\nA particular class of linear 1-Lipschitz layers are ones with an orthogonal Jacobian matrix. The Jacobian matrix of a layer is the matrix of partial derivatives of the flattened outputs with respect to the flattened inputs. A matrix M is orthogonal if M M \u22a4 = I, where I is the identity matrix.\nFor layers with an orthogonal Jacobian, Equation (1) always holds with equality and, because of this, a lot of methods aim at constructing such 1-Lipschitz layers.\nAll the neural networks analyzed in this paper consist of 1-Lipschitz parameterized layers and 1-Lipschitz activation functions, with no skip connections and no batch normalization. Even though the commonly used ReLU activation function is 1-Lipschitz, Anil et al. [1] showed that it reduces the expressive capability of the model. Hence, we adopt the MaxMin activation proposed by the authors and commonly used in 1-Lipschitz models. Concatenations of 1-Lipschitz functions are 1-Lipschitz, so the networks analyzed are 1-Lipschitz by construction.", "publication_ref": ["b0", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Parameterized 1-Lipschitz Layers", "text": "This section provides an overview of the existing methods for providing 1-Lipschitz layers. We discuss fundamental methods and for estimating the spectral norms of linear and convolutional layers, i.e. Power Method [26] and Fantis-tic4 [29], and for crafting orthogonal matrices, i.e. Bjorck & Bowie [4], in Appendix A. The rest of this section describes 7 methods from the literature that construct 1-Lipschitz convolutions: BCOP, Cayley, SOC, AOL, LOT, CPL, and SLL. Further 1-Lipschitz methods, [14,36,41], and the reasons why they were not included in our main comparison can be found in Appendix B.\nBCOP Block Orthogonal Convolution Parameterization (BCOP) was introduced by Li et al. in [22] to extend a previous work by Xiao et al. [39] that focused on the importance of orthogonal initialization of the weights. For a k \u00d7 k convolution, BCOP uses a set of (2k -1) parameter matrices. Each of these matrices is orthogonalized using the algorithm by Bjorck & Bowie [4] (see also Appendix A). Then, a k \u00d7 k kernel is constructed from those matrices to guarantee that the resulting layer is orthogonal.\nCayley Another family of orthogonal convolutional and fully connected layers has been proposed by Trockman and Kolter [34] by leveraging the Cayley Transform [7], which maps a skew-symmetric matrix A into an orthogonal matrix Q using the relation\nQ = (I -A)(I + A) -1 .\n(\nThe transformation can be used to parameterize orthogonal weight matrices for linear layers in a straightforward way. For convolutions, the authors make use of the fact that circular padded convolutions are vector-matrix products in the Fourier domain. As long as all those vector-matrix products have orthogonal matrices, the full convolution will have an orthogonal Jacobian. For Cayley Convolutions, those matrices are orthogonalized using the Cayley transform.\nSOC Skew Orthogonal Convolution is an orthogonal convolutional layer presented by Singla et al. [30], obtained by leveraging the exponential convolution [13]. Analogously to the matrix case, given a kernel L \u2208 R c\u00d7c\u00d7k\u00d7k , the exponential convolution can be defined as\nexp(L)(x) := x + L \u22c6 x 1 + L \u22c6 2 x 2! + \u2022 \u2022 \u2022 + L \u22c6 k x k! + \u2022 \u2022 \u2022 ,(4)\nwhere \u22c6 k denotes a convolution applied k-times. The authors proved that any exponential convolution has an orthogonal Jacobian matrix as long as L is skew-symmetric, providing a way of parameterizing 1-Lipschitz layers. In their work, the sum of the infinite series is approximated by computing only the first 5 terms during training and the first 12 terms during the inference, and L is normalized to have unitary spectral norm following the method presented in [29] (see Appendix A).\nAOL Prach and Lampert [28] introduced Almost Orthogonal Lipschitz (AOL) layers. For any matrix P , they defined a diagonal rescaling matrix D with (5) and proved that the spectral norm of P D is bounded by 1. This result was used to show that the linear layer given by l(x) = P Dx + b (where P is the learnable matrix and D is given by Eq. ( 5)) is 1-Lipschitz. Furthermore, the authors extended the idea so that it can also be efficiently applied to convolutions. This is done by calculating the rescaling in Equation ( 5) with the Jacobian J of a convolution instead of P . In order to evaluate it efficiently the authors express the elements of J \u22a4 J explicitly in terms of the kernel values.\nD ii = j P \u22a4 P ij -1/2\nLOT The layer presented by Xu et al. [40] extends the idea of [14] to use the Inverse Square Root of a matrix in order to orthogonalize it. Indeed, for any matrix V , the matrix\nQ = V (V T V ) -1\n2 is orthogonal. Similarly to the Cayley method, for the layer-wise orthogonal training (LOT) the convolution is applied in the Fourier frequency domain. To find the inverse square root, the authors relay on an iterative Newton Method. In details, defining Y 0 = V T V , Z 0 = I, and\nY i+1 = 1 2 Y i (3I -Z i Y i ) , Z i+1 = 1 2 (3I -Z i Y i ) Z i ,(6)\nit can be shown that Y i converges to (V T V ) -1 2 . In their proposed layer, the authors apply 10 iterations of the method for both training and evaluation.\nCPL Meunier et al. [25] proposed the Convex Potential Layer. Given a non-decreasing 1-Lipschitz function \u03c3 (usually ReLU), the layer is constructed as\nl(x) = x - 2 \u2225W \u2225 2 2 W \u22a4 \u03c3(W x + b),(7)\nwhich is 1-Lipschitz by design. The spectral norm required to calculate l(x) is approximated using the power method (see Appendix A).", "publication_ref": ["b25", "b28", "b3", "b13", "b35", "b40", "b21", "b38", "b3", "b33", "b29", "b12", "b28", "b27", "b4", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "SLL", "text": "The SDP-based Lipschitz Layers (SLL) proposed by Araujo et al. [2] combine the CPL layer with the upper bound on the spectral norm from AOL. The layer can be written as\nl(x) = x -2W Q -2 D 2 \u03c3 W \u22a4 x + b ,(8)\nwhere Q is a learnable diagonal matrix with positive entries and D is deduced by applying Equation (5) to\nP = W Q -1 .\nRemark 1. Both CPL and SLL are non-linear by construction, so they can be used to construct a network without any further use of activation functions. However, carrying out some preliminary experiments, we empirically found that alternating CPL (and SLL) layers with MaxMin activation layers allows achieving a better performance.", "publication_ref": ["b1", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "Theoretical Comparison", "text": "As illustrated in the last section, various ideas and methods have been proposed to parameterize 1-Lipschitz layers. This causes the different methods to have very different properties and requirements. This section aims at highlighting the properties of the different algorithms, focusing on the algorithmic complexity and the required memory.\nTable 1 provides an overview of the computational complexity and memory requirements for the different layers considered in the previous section. For the sake of clarity, the analysis is performed by considering separately the transformations applied to the input of the layers and those applied to the weights to ensure the 1-Lipschitz constraint. Each of the two sides of the table contains three columns: i) Operations contains the most costly transformations applied to the input as well as to the parameters of different layers; ii) MACS reports the computational complexity expressed in multiply-accumulate operations (MACS) involved in the transformations (only leading terms are presented); iii) Memory reports the memory required by the transformation during the training phase.\nAt training time, both input and weight transformations are required, thus the training complexity of the forward pass can be computed as the sum of the two corresponding MACS columns of the table. Similarly, the training memory requirements can be computed as the sum of the two corresponding Memory columns of the table. For the considered operations, the cost of the backward pass during training has the same computational complexity as the forward pass, and therefore increases the overall complexity by a constant factor. At inference time, all the parameter transformations can be computed just once and cached afterward. Therefore, the inference complexity is equal to the complexity due to the input transformation (column 3 in the table). The memory requirements at inference time are much lower than those needed at the training time since intermediate activation values do not need to be stored in memory, hence we do not report them in Table 1.\nNote that all the terms reported in Table 1 depend on the batch size b, the input size s \u00d7 s \u00d7 c, the number of inner iterations of a method t, and the kernel size k \u00d7 k. (Often, t is different at training and inference time.) For the sake of clarity, the MACS of a naive convolution implementation is denoted by C (C = bs 2 c 2 k 2 ), the number of inputs of a layer is denoted by M (M = bs 2 c), and the size of the kernel of a standard convolution is denoted by P (P = c 2 k 2 ). Only the leading terms of the computations are reported in Table 1. In order to simplify some terms, we assume that c > log 2 (s) and that rescaling a tensor (by a scalar) as well as adding two tensors does not require any memory in order to do backpropagation. We also assume that each additional activation does require extra memory. All these assumptions have been verified to hold within PyTorch, [27]. Also, when the algorithm described in the paper and the version provided in the supplied code differed, we considered the algorithm implemented in the code.\nThe transformations reported in the table are convolutions (CONV), Fast Fourier Transformations (FFT), matrixvector multiplications (MV), matrix-matrix multiplications (MM), matrix inversions (INV), as well as applications of an activation function (ACT). The application of algorithms such as Bjorck & Bowie (BnB), power method, and Fantastic 4 (F4) is also reported (see Appendix A for descriptions).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Analysis of the computational complexity", "text": "It is worth noting that the complexity of the input transformations (in Table 1) is similar for all methods. This implies that a similar scaling behaviour is expected at inference time for the models. Cayley and LOT apply an FFT-based convolution and have computational complexity independent of the kernel size. CPL and SLL require two convolutions, which make them slightly more expensive at inference time. Notably, SOC requires multiple convolutions, making this method more expensive at inference time.\nAt training time, parameter transformations need to be applied in addition to the input transformations during every forward pass. For SOC and CPL, the input transformations always dominate the parameter transformations in terms of computational complexity. This means the complexity scales like c 2 , just like a regular convolution, with a further factor of 2 and 5 respectively. All other methods require parameter transformations that scale like c 3 , making them more expensive for larger architectures. In particular, we do expect Cayley and LOT to require long training times for larger models, since the complexity of their parameter transformations further depends on the input size.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Analysis of the training memory requirements", "text": "The memory requirements of the different layers are important, since they determine the maximum batch size and the type of models we can train on a particular infrastructure. At training time, typically all intermediate results are kept in memory to perform backpropagation. This includes intermediate results for both input and parameter transformations. The input transformation usually preserves the size, and therefore the memory required is usually of O(M ).\nTable 1. Computational complexity and memory requirements of different methods. We report multiply-accumulate operations (MACS) as well as memory requirements (per layer) for batch size b, image size s \u00d7 s \u00d7 c, kernel size k \u00d7 k and number of inner iterations t. We use C = bs 2 c 2 k 2 , M = bs 2 c and P = c 2 k 2 . For a detailed explanation on what is reported see Section 3. For some explanation on how the entries of this table were derived, see Appendix C.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Method Input Transformations Parameter Transformations", "text": "Operations MACS O(\u2022) Memory Operations MACS O(\u2022) Memory O(\u2022) Standard CONV C M - - P AOL CONV C M CONV c 3 k 4 5P BCOP CONV C M BnB & MMs c 3 kt + c 3 k 3 c 2 kt + c 2 k 3 Cayley FFTs & MVs bs 2 c 2 5 2 M FFTs & INVs s 2 c 3 3 2 s 2 c 2 CPL CONVs & ACT 2C 3M power method s 2 c 2 k 2 P + s 2 c LOT FFTs & MVs bs 2 c 2 3M FFTs & MMs 4s 2 c 3 t 4s 2 c 2 t SLL CONVs & ACT 2C 3M CONVs c 3 k 4 5P SOC CONVs Ct 1 M t 1 F4 c 2 k 2 t 2 P\nTherefore, for the input transformations, all methods require memory not more than a constant factor worse than standard convolutions, with the worst method being SOC, with a constant t 1 , typically equal to 5.\nIn addition to the input transformation, we also need to store intermediate results of the parameter transformations in memory in order to evaluate the gradients. Again, most methods approximately preserve the sizes during the parameter transformations, and therefore the memory required is usually of order O(P ). Exceptions to this rule are Cayley and LOT, which contain a much larger O(s 2 c 2 ) term, as well as BCOP.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Setup", "text": "This section presents an experimental study aimed at comparing the performance of the considered layers with respect to different metrics. Before presenting the results, we first summarize the setup used in our experiments. For a detailed description see Appendix E. To have a fair and meaningful comparison among the various models, all the proposed layers have been evaluated using the same architecture, loss function, and optimizer. Since, according to the data reported in Table 1, different layers may have different throughput, to have a fair comparison with respect to the tested metrics, we limited the total training time instead of fixing the number of training epochs. Results are reported for training times of 2h, 10h, and 24h on one A100 GPU.\nOur architecture is a standard convolutional network that doubles the number of channels whenever the resolution is reduced [5,34]. For each method, we tested architectures of different sizes. We denoted them as XS, S, M and L, depending on the number of parameters, according to the criteria in Table 7, ranging from 1.5M to 100M parameters.\nSince different methods benefit from different learning rates and weight decay, for each setting (model size, method and dataset), we used the best values resulting from a random search performed on multiple training runs on a validation set composed of 10% of the original training set. More specifically, 16 runs were performed for each configuration of randomly sampled hyperparameters, and we selected the configuration maximizing the certified robust accuracy w.r.t. \u03f5 = 36/255 (see Appendix E.5 for details).\nThe evaluation was carried out using three different datasets: CIFAR-10, CIFAR-100 [16], and Tiny ImageNet [18]. Augmentation was used during the training (Random crops and flips on CIFAR-10 and CIFAR-100, and Ran-dAugment [10] on Tiny ImageNet). We use the loss function proposed by [28], with the margin set to 2 \u221a 2\u03f5, and temperature 0.25.", "publication_ref": ["b4", "b33", "b15", "b17", "b9", "b27"], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "Metrics", "text": "All the considered models were evaluated based on three main metrics: the throughput, the required memory, and the certified robust accuracy.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Throughput and epoch time", "text": "The throughput of a model is the average number of examples that the model can process per second. It determines how many epochs are processed in a given time frame. The evaluation of the throughput was performed on an 80GB-A100-GPU based on the average time of 100 mini-batches. We measured the inference throughput with cached parameter transformations.\nMemory required Layers that require less memory allow for larger batch size, and the memory requirements also determine the type of hardware we can train a model on. For each model, we measured and reported the maximal GPU memory occupied by tensors using the function torch.cuda.max memory allocated() provided by the PyTorch framework. This is not exactly equal to the overall GPU memory requirement but gives a fairly good approximation of it. Note that the model memory measured in this way also includes additional memory required by the optimizer (e.g. to store the momentum term) as well as by the activation layers in the forward pass. However, this additional memory should be at most of order O(M +P ). As for the throughput, we evaluated and cached all calculations independent of the input at inference time.\nCertified robust accuracy In order to evaluate the performance of a 1-Lipschitz network, the standard metric is the certified robust accuracy. An input is classified certifiably robustly with radius \u03f5 by a model, if no perturbations of the input with norm bounded by \u03f5 can change the prediction of the model. Certified robust accuracy measures the proportion of examples that are classified correctly as well as certifiably robustly. For 1-Lipschitz models, a lower bound of the certified \u03f5-robust accuracy is the ratio of correctly classified inputs such that\nM f (x i , l i ) > \u03f5 \u221a 2 where the margin M f (x, l) of a model f at input x with label l, given as M f (x, l) = f (x) l -max j\u0338 =l f j (x)\n, is the difference between target class score and the highest score of a different class. For details, see [35].", "publication_ref": ["b34"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Results", "text": "This section presents the results of the comparison performed by applying the methodology discussed in Section 4. The results related to the different metrics are discussed in dedicated subsections and the key takeaways are summarized in the radar-plot illustrated in Figure 1. corresponding inference throughput for the various sizes as described in Section 4. As described in Table 5, the model base width, referred to as w, is doubled from one model size to the next. We expect the training and inference time to scale with w similarly to how individual layers scale with their number of channels, c (in Table 1). This is because the width of each of the 5 blocks of our architecture is a constant multiple of the base width, w.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": ["tab_5"]}, {"heading": "Training and inference times", "text": "The training time increases (at most) about linearly with w for standard convolutions, whereas the computational complexity of each single convolution scales like c 2 . This suggests that parallelism on the GPU and the overhead from other operations (activations, parameter updates, etc.) are important factors determining the training time. This also explains why CPL (doing two convolutions, with identical kernel parameters) is only slightly slower than a standard convolution, and SOC (doing 5 convolutions) is only about 3 times slower than the standard convolution. The AOL and SLL methods also require times comparable to a standard convolution for small models, although eventually, the c 3 term in the computation of the rescaling makes them slower for larger models. Finally, Cayley, LOT, and BCOP methods take much longer training times per epoch. For Cayley and LOT this behavior was expected, as they have a large O(s 2 c 3 ) term in their computational complexity. See Table 1 for further details.\nAt inference time transformations of the weights are cached, therefore some methods (AOL, BCOP) do not have any overhead compared to a standard convolution. As expected, other methods (CPL, SLL, and SOC) that apply additional convolutions to the input suffer from a corresponding overhead. Finally, Cayley and LOT have a slightly different throughput due to their FFT-based convolution. Among them, Cayley is about twice as fast because it involves a real-valued FFT rather than a complex-valued one.   From Figure 3, it can be noted that cached Cayley and CPL have the same inference time, even though CPL uses twice the number of convolutions. We believe this is due to the fact that the conventional FFT-based convolution is quite efficient for large filters, but PyTorch implements a faster algorithm, i.e., Winograd, [17], that can be up to 2.5 times faster.", "publication_ref": ["b16"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "XS", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Training memory requirements", "text": "The training and inference memory requirements of the various models (measured as described in Section 4.1) are reported in Figure 4 as a function of the model size. The results of the theoretical analysis reported in Table 1 suggest that the training memory requirements always have a term linear in the number of channels, c (usually the activations from the forward pass), as well as a term quadratic in c (usually the weights and all transformations applied to the weights during the forward pass). This behavior can also be observed from Figure 4. For some of the models, the memory required approximately doubles from one model size to the next one, just like the width. This means that the linear term dominates (for those sizes), which makes those models relatively cheap to scale up. For the BCOP, LOT, and Cayley methods, the larger coefficients in the c 2 term (for LOT and Cayley the coefficient is even dependent on the input size, s 2 ) cause this term to dominate. This makes it much harder to scale those methods to more parameters. Method LOT requires huge amounts of memory, in particular LOT- L is too large to fit in 80GB GPU memory.\nNote that at test time, the memory requirements are much lower, because the intermediate activation values do not need to be stored, as there is no backward pass. Therefore, at inference time, most methods require a very similar amount of memory as a standard convolution. The Cayley and LOT methods require more memory since perform the calculation in the Fourier space, as they create an intermediate representation of the weight matrices of size O(s 2 c 2 ).", "publication_ref": [], "figure_ref": ["fig_3", "fig_3"], "table_ref": []}, {"heading": "Certified robust accuracy", "text": "The results related to the accuracy and the certified robust accuracy for the different methods, model sizes, and datasets measured on a 24h training budget are summarized in Table 2. The differences among the various model sizes are also highlighted in Figure 5  pendix D. However, it is worth noting that, to reach state-ofthe-art performance, authors often carry out experiments using large model sizes and long training times, which makes it hard to compare the methods themselves. On the other hand, the evaluation proposed in this paper allows a fairer comparison among the different methods, since it also considers timing and memory aspects. This restriction based on time, rather than the number of epochs, ensures that merely enlarging the model size does not lead to improved performance, as bigger models typically process fewer epochs of data. Indeed, in our results in Figure 5 it is usually the M (and not the L) model that performs best. To assign a score that combines the performance of the methods over all the three datasets, we sum the number of times that each method is ranked in the first position, in the top-3, and top-10 positions. In this way, top-1 methods are counted three times, and top-3 methods are counted twice. The scores in the radar-plot shown in Figure 1 are based on those values.\nAmong all methods, SOC achieved a top-1 robust accuracy twice and a top-3 one 6 times, outperforming all the other methods. CPL ranks twice in the top-3 and 9 times in the top-10 positions, showing that it generally has a more stable performance compared with other methods. LOT achieved the best certified robust accuracy on Tiny Ima-geNet, appearing further 5 times in the top-10. AOL did not perform very well on CIFAR-10, but reached more competitive results on Tiny ImageNet, ending up in the top-10 a total of 5. An opposite effect can be observed for SLL, which performed reasonably well on CIFAR-10, but not so well on the two datasets with more classes, placing in the top-10 only once. This result is tied with BCOP, which also has only one model in the top-10. Finally, Cayley is consistently outperformed by the other methods. The very same analysis can be applied to the clean accuracy, whose sorted bar-plots are reported in Appendix G, where the main difference is that Cayley performs slightly better for that metric. Furthermore, it is worth highlighting that CPL is sensitive to weight initialization. We faced numerical errors during the 10h and 24h training of the small model on CIFAR-100.", "publication_ref": [], "figure_ref": ["fig_4", "fig_4", "fig_0"], "table_ref": ["tab_1"]}, {"heading": "Conclusions and Guidelines", "text": "This work presented a comparative study of state-of-the-art 1-Lipschitz layers under the lens of different metrics, such as time and memory requirements, accuracy, and certified robust accuracy, all evaluated at training and inference time. A theoretical comparison of the methods in terms of time and memory complexity was also presented and validated by experiments.\nTaking all metrics into account (summarized in Figure 1), the results are in favor of CPL, due to its highest performance and lower consumption of computational resources. When large computational resources are available and the application does not impose stringent timing constraints during inference and training, the SOC layer could be used, due to its slightly better performance. Finally, those applications in which the inference time is crucial may take advantage of AOL or BCOP, which do not introduce additional runtime overhead (during inference) compared to a standard convolution.\nTechnical Appendix of \"1-Lipschitz Layers Compared: Memory, Speed, and Certifiable Robustness\"", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "A. Spectral norm and orthogonalization", "text": "A lot of recently proposed methods do rely on a way of parameterizing orthogonal matrices or parameterizing matrices with bounded spectral norm. We present methods that are frequently used below:\nBjorck & Bowie [4] introduced an iterative algorithm that finds the closest orthogonal matrix to the given input matrix. In the commonly used form, this is achieved by computing a sequence of matrices using\nA k+1 = A k I + 1 2 Q k , for Q k = I -A \u22a4 k A k(9)\nwhere A 0 = A, is the input matrix. The algorithm is usually truncated after a fixed number of steps, during training often 3 iterations are enough, and for inference more (e.g. 15) iterations are used to ensure a good approximation. Since the algorithm is differentiable, it can be applied to construct 1-Lipschitz networks as proposed initially in [1] or also as an auxiliary method for more complex strategies [22].\nPower Method The power method was used in [26], [19] and [25] in order to bound the spectral norm of matrices. It starts with a random initialized vector u 0 , and iteratively applies the following:\nv k+1 = W \u22a4 u k \u2225W \u22a4 u k \u2225 2 , u k+1 = W v k+1 \u2225W v k+1 \u2225 2 . (10\n)\nThen the sequence \u03c3 k converges to the spectral norm of W , for \u03c3 k given by\n\u03c3 k = u \u22a4 k W v k .(11)\nThis procedure allows us to obtain the spectral norm of matrices, but it can also be efficiently extended to find the spectral norm of the Jacobian of convolutional layers. This was done for example by [12,19], using the fact that the transpose of a convolution operation (required to calculate Equation ( 10)) is a convolution as well, with a kernel that can be constructed from the original one by transposing the channel dimensions and flipping the spatial dimensions of the kernel. When the power method is used on a parameter matrix of a layer, we can make it even more efficient with a simple trick. We usually expect the parameter matrix to change only slightly during each training step, so we can store the result u k during each training step, and start the power method with this vector as u 0 during the following training step. With this trick it is enough to do a single iteration of the power method at each training step. The power method is usually not differentiated through.\nFantasic Four proposed, in [29], allows upper bounding the Lipschitz constant of a convolution. The given bound is generally not tight, so using the method directly does not give good results. Nevertheless, since various methods require a way of bounding the spectral norm to have convergence guarantees, Fantastic Four is often used.", "publication_ref": ["b3", "b0", "b21", "b25", "b18", "b11", "b18", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "B. Algorithms omitted in the main paper", "text": "Observe that the strategies presented in [8,14,20,23,26,36,41] have intentionally not been compared for different reasons. In the works presented in [8,23], the Lipschitz constraint was solely used during training and no guarantees were provided that the resulting layers are 1-Lipschitz. The method proposed in [26] has been extended by Fantastic 4 [29] and, indeed, can only be used as an auxiliary method to upper-bound the Lispchitz constant. The method proposed in [20] only works for linear layers and can be thought of as a special case of SOC (described in Section 2). We will give detailed reasons for the other methods below.\nONI The method ONI [14] proposed the orthogonalization used in LOT. They parameterize orthogonal matrices as (V V \u22a4 ) -1 2 V , and calculate the inverse square root using Newton's iterations. They use this methods to define 1-Lipschitz linear layers. However, the extension to convolutions only uses a simple unrolling, and does not provide a tight bound in general. Therefore, we did not include the method in the paper.\nECO Explicitly constructed orthogonal (ECO) convolutions [41] also do use properties of the Fourier domain in order to parameterize a convolution. However, they do not actually calculate the convolution in the Fourier domain, but instead parameterize a layer in the Fourier domain, and then use an inverse Fourier transformation to obtain a kernel from this parameterization. We noticed, however, that the implementation provided by the authors does not produce 1-Lipschitz layers (at least with our architecture), as can be seen in Figure 6. There, we report the batch activation variance (defined in Appendix F) as well as the spectral norm of each layer. The batch activation variance should be non-increasing for 1-Lipschitz layers (also see Appendix F), however, for ECO this is not the case. Also, power iteration shows that the Lipschitz constant of individual layers is not 1. Therefore we do not report this method in the main paper.  Sandwich The authors of [36] introduced the Sandwich layer. It considers a layer of the form\n0\nl(x) = \u221a 2A T \u03a8ReLU \u221a 2\u03a8 -1 Bx + b ,(12)\nfor \u03c3 typically the ReLU activation. The authors propose a (simultaneous) parameterization of A and B, based on the Cayley Transform, that guarantees the whole layer to be 1-Lipschitz. They also extend the idea to convolutions. However, for this they require to apply two Fourier transformations as well as two inverse ones. During the training of the models within the Sandwich layers, a severe vanishing gradient phenomena happens. We summarize the Frobenious norm of the gradient, obtained by inspecting the inner blocks during the training, in Table 3. For this reason we did not report the results in the main paper.", "publication_ref": ["b7", "b13", "b19", "b22", "b25", "b35", "b40", "b7", "b22", "b25", "b28", "b19", "b13", "b40", "b35"], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "C. Computation Complexity and Memory Requirement", "text": "In this section we give some intuition of the values in Table 1.\nRecall that we consider a layer with input size s \u00d7 s \u00d7 c, and kernel size k \u00d7 k, batch size b, and (for some layers) we will denote the number of inner iterations by t.\nWe also use C = bs 2 c 2 k 2 and M = bs 2 c and P = c 2 k 2 .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "AOL:", "text": "In order to compute the rescaling matrix for AOL, we need to convolve the kernel with itself. This operation has complexity O(c 3 k 4 ). It outputs a tensor of size c \u00d7 c \u00d7 (2k -1) \u00d7 (2k -1), so in total we require memory of about 5P for the parameter as well as the transformation.\nBCOP: For BCOP we only require a single convolution as long as we know the kernel. However, we do require a lot of computation to create the kernel for this convolution. In particular, we require 2k -1 matrix orthogonalizations (usually done with Bjorck & Bowie), as well as O(k 3 ) matrix multiplications for building up the kernel. These require about c 3 kt + c 3 k 3 MACS as well as c 2 kt + c 2 k 3 memory.\nCayley: Cayley Convolutions make use of the fact that circular padded convolutions are vector-matrix products in the Fourier domain. Applying the fast Fourier transform to inputs and weights has complexity of O(bcs 2 log(s 2 )) and O(c 2 s 2 log(s 2 )). Then, we need to orthogonalize 1 2 s 2 matrices. Note that the factor of 1 2 appears due to the fact that the Fourier transform of a real matrix has certain symmetry properties, and we can use that fact to skip half the computations. Doing the matrix orthogonalization with the Cayley Transform requires taking the inverse of a matrix, as well as matrix multiplication, the whole process has a complexity of about s 2 c 3 . The final steps consists of doing 1  2 bs 2 matrix-vector products, requiring 1  2 bs 2 c 2 MACS, as well as another fast Fourier transform . Note that under our assumption that c > log(s 2 ), the fast Fourier transform operation is dominated by other operations. Cayley Convolutions require padding the kernel from a size of c \u00d7 c \u00d7 k \u00d7 k to a (usually much larger) size of c \u00d7 c \u00d7 s \u00d7 s requiring a lot of extra memory. In particular we need to keep the output of the (real) fast Fourier transform, the matrix inversion as well as the matrix multiplication in memory, requiring about 1 2 s 2 c 2 memory each.\nCPL: CPL applies two convolutions as well as an activation for each layer. They also use the Power Method (on the full convolution), however, its computational cost is dominated by the application of the convolutions.\nLOT: Similar to Cayley, LOT performs the convolution in Fourier space. However, instead of using the Cayley transform, they parameterize orthogonal matrices as V (V T V ) -1 2 . To find the inverse square root, authors relay on an iterative Newton Method. In details, let Y 0 = V T V and Z 0 = I, then Y i defined as\nY i+1 = 1 2 Y i (3I -Z i Y i ) , Z i+1 = 1 2 (3I -Z i Y i ) Z i ,(13)\nconverges to (V T V ) -1 2 . Executing this procedure, includes computing 4s 2 t matrix multiplications, requiring about 4s 2 c 3 t MACS as well as 4s 2 c 2 t memory.\nSLL: Similar to CPL, each SLL layer also requires evaluating two convolutions as well as one activation. However, SLL also needs to compute the AOL rescaling, resulting in total computational cost of 2C + O(c 3 k 4 ).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "SOC:", "text": "For each SOC layer we require applying t convolutions. Other required operations (application of Fantastic 4 for an initial bound, as well as parameterizing the kernel such that the Jacobian is skew-symmetric) are cheap in comparison. Table 3. Vanishing Gradient Phenomena of Sandwhich Layer. ConvNetXS model has been tested with a small batch size (32). Training of deeper layers (i.e., layers that are close to the input of the network) is tough due to the almost zero gradients.", "publication_ref": ["b31"], "figure_ref": [], "table_ref": []}, {"heading": "Layer name", "text": "Output Shape Gradient Norm  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Layer name", "text": "Output size\nInput 32 \u00d7 32 \u00d7 3 Zero Channel Padding 32 \u00d7 32 \u00d7 w Conv (1 \u00d7 1 kernel size) 32 \u00d7 32 \u00d7 w Activation 32 \u00d7 32 \u00d7 w Downsize Block(k) 16 \u00d7 16 \u00d7 2w Downsize Block(k) 8 \u00d7 8 \u00d7 4w Downsize Block(k) 4 \u00d7 4 \u00d7 8w Downsize Block(k) 2 \u00d7 2 \u00d7 16w Downsize Block(1) 1 \u00d7 1 \u00d7 32w Flatten 32w Linear 32w First Channels(c) c", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D. Comparison with SOTA", "text": "In this section, we report state-of-the-art results from the literature. In contrast to our comparison, the runs reported often use larger architectures and longer training times. Find results in Table Tab. 4.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E. Experimental Setup", "text": "In addition to theoretically analyzing different proposed layers, we also do an empirical comparison of those layers. In order to allow for a fair and meaningful comparison, we try to fix the architecture, loss function and optimizer, and evaluate all proposed layers with the same setting.\nFrom the data in Table 1 we know that different layers will have very different throughputs. In order to have a fair comparison despite of that, we limit the total training time instead of fixing a certain amount of training epochs. We report results of training for 2h, 10h as well as 24h.\nWe describe the chosen setting below.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E.1. Architecture", "text": "We show the architecture used for our experiments in Tables 5 and6. It is a standard convolutional architecture, that doubles the number of channels whenever the resolution is reduced. Note that we exclusively use convolutions with the same input and output size as an attempt to make the model less dependent on the initialization used by the convolutional layers. We  5 just appends channels with value 0 to the input, and the layer First Channels(c) outputs only the first c channels, and ignores the rest. Finally, the layer Pixel Unshuffle (implemented in PyTorch) takes each 2 \u00d7 2 \u00d7 c patches of an image and reshapes them into size 1 \u00d7 1 \u00d7 4c.\nFor each 1-Lipschitz layer, we also test architectures of different sizes. In particular, we define 4 categories of models based on the number of parameters. We call those categories XS, S, M and L. See Table 7 for the exact numbers. In this table we also report the width parameter w that ensures our architecture has the correct number of parameters.\nRemark 2. For most methods, the number of parameters per layer are about the same. There are two exceptions, BCOP and Sandwich. BCOP parameterizes the convolution kernel with c input channels and c output channels using a matrix of size c \u00d7 c and 2(k -1) matrices of size c \u00d7 c/2. Therefore, the number of parameters of a convolution using BCOP is kc 2 , less than the k 2 c 2 parameters of a plain convolution. The Sandwich layer has about twice as many parameters as the other layers for the same width, as it parameterizes two weight matrices, A and B in Equation ( 12), per layer.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_5", "tab_6", "tab_5", "tab_7"]}, {"heading": "E.2. Loss function", "text": "We use the loss function proposed by [28], with the temperature parameter set to the value used there (t = 1/4). Our goal metric is certified robust accuracy for perturbation of maximal size \u03f5 = 36/255. We aim at robustness of maximal size 2\u03f5 during training. In order to achieve that we set the margin parameter to 2 \u221a 2\u03f5.", "publication_ref": ["b27"], "figure_ref": [], "table_ref": []}, {"heading": "E.3. Optimizer", "text": "We use SGD with a momentum of 0.9 for all experiments. We also used a learning rate schedule. We choose to use OneCycleLR, as described by [32], with default values as in PyTorch. We set the batch size to 256 for all experiments.", "publication_ref": ["b31"], "figure_ref": [], "table_ref": []}, {"heading": "E.4. Training Time", "text": "On of our main goals is to evaluate what is the best model to use given a certain time budget. In order to do this, we measure the time per epoch as described in Section 4.1 on an A100 GPU with 80GB memory for different methods and different model sizes. Then we estimate the number of epochs we can do in our chosen time budget of either 2h, 10h or 24h, and use that many epochs to train our models. The amount of epochs corresponding to the given time budget is summarized in Table 8.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_8"]}, {"heading": "E.5. Hyperparameter Random Search", "text": "The learning rate and weight decay for each setting (model size, method and dataset) was tuned on the validation set. For each method we did hyperparameter search by training for 2h (corresponding number of epochs in Table 8). We did 16 runs with learning-rate of the form 10 x , where x is sampled uniformly in the interval [-4, -1], and with weight-decay of the form 10 x , where x is sampled uniformly in the interval [-5.5, -3.5]. Finally, we selected the learning rate and weight decay corresponding to the run with the highest validation certified robust accuracy for radius 36/255. We use these hyperparameters found also for the experiments with longer training time.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_8"]}, {"heading": "E.6. Datasets", "text": "We evaluate on three different datasets, CIFAR-10, CIFAR-100 [16] and Tiny ImageNet [18]. For CIFAR-10 and CIFAR-100 we use the architecture described in Table 5. Since the architectures are identical, so are time-and memory requirements, and therefore also the epoch budget. As preprocessing we subtract the dataset channel means from each image. As data augmentation at training time we apply random crops (4 pixels) and random flipping.\nIn order to assess the behavior on larger images, we replicate the evaluation on the Tiny ImageNet dataset [18]: a subset of 200 classes of the ImageNet [11] dataset, with images scaled to have size 64 \u00d7 64.\nIn order to allow for the larger input size of this dataset, we add one additional Downsize Block to our model. We also divide the width parameter (given in Table 7) by 2 to keep the amount of parameters similar. We again subtract the channel mean for each image. As data augmentation we we us RandAugment [10] with 2 transformations of magnitude 9 (out of 31).", "publication_ref": ["b15", "b17", "b17", "b10", "b9"], "figure_ref": [], "table_ref": ["tab_5", "tab_7"]}, {"heading": "E.7. Metrics", "text": "As described in Section 4.1 we evaluate the methods in terms of three main metrics. The throughput, the memory requirements as well and the certified robust accuracy a model can achieve in a fixed amount of time.\nThe evaluation of the throughput is performed on an NVIDIA A100 80GB PCIe GPU * . We measure it by averaging the time it takes to process 100 batches (including forward pass, backward pass and parameter update), and use this value to calculate the average number of examples a model can process per second. In order to estimate the inference throughput, we first evaluate and cache all calculations that do not depend on the input (such as power iterations on the weights). With this we measure the average time of the forward pass of 100 batches, and calculate the throughput from that value.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F. Batch Activation Variance", "text": "As one (simple to compute) sanity check that the models we train are actually 1-Lipschitz, we consider the batch activation variance. For layers that are 1-Lipschitz, we show below that the batch activation variance cannot increase from one layer to the next. This gives us a mechanism to detect (some) issues with trained models, including numerical ones, conceptual ones as well as problems in the implementation.\nTo compute the batch activation variance we consider a mini-batch of inputs, and for this mini-batch we consider the outputs of each layer. Denote the outputs of layer l as a \nBatchVar l = 1 b b i=1 \u2225a (l) i -\u00b5 (l) \u2225 2 2 ,(14)\nwhere the l 2 norm is calculated based on the flattened tensor. Denote layer l as f l . Then we have that\nBatchVar l+1 = 1 b b i=1 \u2225a (l+1) i -\u00b5 (l+1) \u2225 2 2 (16) \u2264 1 b b i=1 \u2225f l (a (l) i ) -f l (\u00b5 (l) )\u2225 2 2 (17) \u2264 1 b b i=1 \u2225a (l) i -\u00b5 (l) \u2225 2 2 (18) = BatchVar l .(19)\nHere, for the first inequality we use that (by definition) a\n(l+1) i = f l (a(l)\ni ) and that the term l+1) . The second inequality follows from the 1-Lipschitz property. The equation above shows that the batch activation variance can not increase from one layer to the next for 1-Lipschitz layers. Therefore, if we see an increase in experiments that shows that the layer is not actually 1-Lipschitz.\nn i=1 \u2225a (l+1) i -x\u2225 2 2 is minimal for x = \u00b5 (\nAs a further check that the layers are 1-Lipschitz we also apply (convolutional) power iteration to each linear layer after training.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "G. Further Experimental Results", "text": "In this section, further experiments -not presented in the main paper-can be found.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "G.1. Different training time budgets", "text": "In this section we report the experimental results for three different training budgets: 2h, 10h and 24h. See the results in Table 9 (CIFAR-10), Table 10 (CIFAR-100), and Table 11 (Tiny ImageNet). Each of those tables also reports the best learning rate and weight decay found by the random search for each setting. Furthermore, a different representation of the impact of the training time on the robust accuracy can be found in Figure 7.", "publication_ref": [], "figure_ref": ["fig_7"], "table_ref": []}, {"heading": "G.2. Time and Memory Requirements on Tiny ImageNet", "text": "See plot Fig. 8 for an evaluation of time and memory usage for Tiny ImageNet dataset. The models used on CIFAR-10 and the ones on Tiny ImageNet are identical up to one convolutional block, therefore also the results in Figure 8 are similar to the results on CIFAR-10 reported in the main paper.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "G.3. Kernel size 1 \u00d7 1", "text": "For CIFAR-10 dataset, we tested models where convolutional layers have a kernel size of 1 \u00d7 1, to evaluate if the quicker epoch time can compensate for the lower number of parameters. In almost all cases the answer was negative, the version with 3 \u00d7 3 kernel outperformed the one with the 1 \u00d7 1 kernel.We therefore do not recommend reducing the kernel size. See Table 12 for a detailed view of the accuracy and robust accuracy.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "G.4. Clean Accuracy", "text": "As a reference, we plotted the accuracy of different models in Figure 9. The rankings by accuracy are similar to the rankings by certified robust accuracy. One difference is that the Cayley method performs better relative to other methods when measured in terms of accuracy. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "H. Issues and observations", "text": "As already thoroughly analyzed in the dedicated section, some of the known methods in the literature have been omitted in the main paper since we faced serious concerns. Nevertheless, we also encountered some difficulties during the implementation of the methods that we did report in the main paper. The aim of this section is to highlight these difficulties, we hope this can open a constructive debate. 1. In the SLL [2] code, taken from the authors' repository, there is no attention to numerical errors that can easily happen from close-to-zero divisions during the parameter transformation. We solve the issue, by adopting the commonly used strategy, i.e. we included a factor of 1 \u2022 10 -6 while dividing for the AOL-rescaling of the weight matrix. Furthermore, the code provided in the SLL repository only works for a kernel size of 3, we fixed the issue in our implementation. 2. The CPL method [25] features a high sensitivity to the initialization of the weights. Long training, e.g. 24-hour training, can sometimes result in NaN during the update of the weights. In that case we re-ran the models with different seeds. 3. Furthermore, there was no initialization method stated in the CPL paper, and also no code was provided. Therefore, we used the initialization from the similar SLL method. 4. During the training of Sandwich we faced some numerical errors. To investigate such errors, we tested a lighter version of the method -without the learnable rescaling \u03a8 -for the reason described in Remark 3, which shows that the rescaling \u03a8 inside the layer can be embedded into the bias term and hence the product \u03a8\u03a8 -1 can be omitted. 5. Similarly, for SLL, the matrix Q in Equation (8) does not add additional degrees of freedom to the model. Instead of having parameters W , Q and b we could define and optimize over P = W Q -1 and b = Q -1 b. However, for our experiments we used the original parameterization. 6. The method Cayley [34], in the form proposed in the original paper, does not cache the -costly -transformation of the weight matrix whenever the layer is in inference mode. We fix this issue in our implementation. 7. The LOT method, [40], leverages 10 inner iterations for both training and inference in order to estimate the inverse of the square root with their proposed Newton-like method. Since the gradient is tracked for the whole procedure, the amount of memory required during the training is prohibitive for large models. Furthermore, since the memory is required for the parameter transformation, reducing the batch size does not solve this problem. In order to make the Large model (L) fit in the memory, we tested the LOT method with only 2 inner iterations. However, the performance in terms of accuracy and robust accuracy is not comparable to other strategies, hence we omitted it from our tables. \n(x) = \u221a 2A T \u03a8ReLU \u221a 2\u03a8 -1 Bx + b = \u221a 2A T \u03a8ReLU \u221a 2\u03a8 -1 \u221a 2Bx + \u03a8b = \u221a 2A T \u03a8\u03a8 -1 ReLU \u221a 2Bx + \u03a8b = \u221a 2A T ReLU \u221a 2Bx + \u03a8b .(21)\nConsidering b = \u03a8b concludes the proof.", "publication_ref": ["b1", "b7", "b33"], "figure_ref": [], "table_ref": []}, {"heading": "I. Code", "text": "We often build on code provided with the original papers. This includes ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Sorting out Lipschitz function approximation", "journal": "", "year": "2019", "authors": "Cem Anil; James Lucas; Roger Grosse"}, {"ref_id": "b1", "title": "A unified algebraic perspective on Lipschitz neural networks", "journal": "", "year": "2023", "authors": "Alexandre Araujo; Aaron J Havens; Blaise Delattre; Alexandre Allauzen; Bin Hu"}, {"ref_id": "b2", "title": "Evasion attacks against machine learning at test time", "journal": "", "year": "2013", "authors": "Battista Biggio; Igino Corona; Davide Maiorca; Blaine Nelson; Nedim \u0160rndi\u0107; Pavel Laskov; Giorgio Giacinto; Fabio Roli"}, {"ref_id": "b3", "title": "An iterative algorithm for computing the best estimate of an orthogonal matrix", "journal": "SIAM Journal on Numerical Analysis", "year": "1971", "authors": "\u00c5 Bj\u00f6rck; C Bowie"}, {"ref_id": "b4", "title": "Robust-by-design classification via unitarygradient neural networks", "journal": "", "year": "2023", "authors": "Fabio Brau; Giulio Rossolini; Alessandro Biondi; Giorgio Buttazzo"}, {"ref_id": "b5", "title": "Certified!!) adversarial robustness for free!", "journal": "", "year": "2023", "authors": "Nicholas Carlini; Florian Tramer; Dj Krishnamurthy; Leslie Dvijotham; Mingjie Rice; J Zico Sun;  Kolter"}, {"ref_id": "b6", "title": "About the algebraic structure of the orthogonal group and the other classical groups in a field of characteristic zero or a prime characteristic", "journal": "Journal f\u00fcr die reine und angewandte Mathematik", "year": "", "authors": "Arthur Cayley"}, {"ref_id": "b7", "title": "Parseval networks: Improving robustness to adversarial examples", "journal": "", "year": "2017", "authors": "Moustapha Cisse; Piotr Bojanowski; Edouard Grave; Yann Dauphin; Nicolas Usunier"}, {"ref_id": "b8", "title": "Certified adversarial robustness via randomized smoothing", "journal": "", "year": "2019", "authors": "Jeremy Cohen; Elan Rosenfeld; Zico Kolter"}, {"ref_id": "b9", "title": "Randaugment: Practical automated data augmentation with a reduced search space", "journal": "", "year": "2020", "authors": "Barret Ekin D Cubuk; Jonathon Zoph; Quoc V Shlens;  Le"}, {"ref_id": "b10", "title": "Imagenet: A large-scale hierarchical image database", "journal": "", "year": "2009", "authors": "Jia Deng; Wei Dong; Richard Socher; Li-Jia Li; Kai Li; Li Fei-Fei"}, {"ref_id": "b11", "title": "Generalizable adversarial training via spectral normalization", "journal": "", "year": "2018", "authors": "Farzan Farnia; Jesse Zhang; David Tse"}, {"ref_id": "b12", "title": "The convolution exponential and generalized Sylvester flows", "journal": "", "year": "2020", "authors": "Emiel Hoogeboom; Victor Garcia Satorras; Jakub Tomczak; Max Welling"}, {"ref_id": "b13", "title": "Controllable orthogonalization in training DNNs", "journal": "", "year": "2020", "authors": "Lei Huang; Li Liu; Fan Zhu; Diwen Wan; Zehuan Yuan; Bo Li; Ling Shao"}, {"ref_id": "b14", "title": "Reluplex: An efficient SMT solver for verifying deep neural networks", "journal": "", "year": "2017", "authors": "Guy Katz; Clark Barrett; David L Dill; Kyle Julian;  Mykel;  Kochenderfer"}, {"ref_id": "b15", "title": "Learning multiple layers of features from tiny images", "journal": "", "year": "2009", "authors": "Alex Krizhevsky"}, {"ref_id": "b16", "title": "Fast algorithms for convolutional neural networks", "journal": "", "year": "2016", "authors": "Andrew Lavin; Scott Gray"}, {"ref_id": "b17", "title": "Tiny imagenet visual recognition challenge", "journal": "CS", "year": "2015", "authors": "Ya Le; Xuan Yang"}, {"ref_id": "b18", "title": "Globallyrobust neural networks", "journal": "", "year": "2021", "authors": "Klas Leino; Zifan Wang; Matt Fredrikson"}, {"ref_id": "b19", "title": "Cheap orthogonal constraints in neural networks: A simple parametrization of the orthogonal and unitary group", "journal": "", "year": "2019", "authors": "Mario Lezcano; -Casado ; David Mart\u00ednez-Rubio"}, {"ref_id": "b20", "title": "Sok: Certified robustness for deep neural networks", "journal": "", "year": "2023", "authors": "Linyi Li; Tao Xie; Bo Li"}, {"ref_id": "b21", "title": "Preventing gradient attenuation in Lipschitz constrained convolutional networks", "journal": "", "year": "2019", "authors": "Qiyang Li; Saminul Haque; Cem Anil; James Lucas; Roger B Grosse; Joern-Henrik Jacobsen"}, {"ref_id": "b22", "title": "Orthogonal deep neural networks", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2021", "authors": "Shuai Li; Kui Jia; Yuxin Wen; Tongliang Liu; Dacheng Tao"}, {"ref_id": "b23", "title": "Certified robust models with slack control and large Lipschitz constants", "journal": "", "year": "2023", "authors": "Max Losch; David Stutz; Bernt Schiele; Mario Fritz"}, {"ref_id": "b24", "title": "A dynamical system perspective for Lipschitz neural networks", "journal": "", "year": "2022", "authors": "Laurent Meunier; J Blaise; Alexandre Delattre; Alexandre Araujo;  Allauzen"}, {"ref_id": "b25", "title": "Spectral normalization for generative adversarial networks", "journal": "", "year": "2018", "authors": "Takeru Miyato; Toshiki Kataoka; Masanori Koyama; Yuichi Yoshida"}, {"ref_id": "b26", "title": "Pytorch: An imperative style, high-performance deep learning library", "journal": "", "year": "2019", "authors": "Adam Paszke; Sam Gross; Francisco Massa; Adam Lerer; James Bradbury; Gregory Chanan; Trevor Killeen; Zeming Lin; Natalia Gimelshein; Luca Antiga; Alban Desmaison; Andreas Kopf; Edward Yang; Zachary Devito; Martin Raison; Alykhan Tejani; Sasank Chilamkurthy; Benoit Steiner; Lu Fang; Junjie Bai; Soumith Chintala"}, {"ref_id": "b27", "title": "Almost-orthogonal layers for efficient general-purpose Lipschitz networks", "journal": "", "year": "2022", "authors": "Bernd Prach; Christoph H Lampert"}, {"ref_id": "b28", "title": "Fantastic four: Differentiable bounds on singular values of convolution layers", "journal": "", "year": "2021", "authors": "S Singla;  Feizi"}, {"ref_id": "b29", "title": "Skew orthogonal convolutions", "journal": "", "year": "2021", "authors": "Sahil Singla; Soheil Feizi"}, {"ref_id": "b30", "title": "Improved techniques for de-terministic l2 robustness", "journal": "", "year": "2022", "authors": "Sahil Singla; Soheil Feizi"}, {"ref_id": "b31", "title": "Super-convergence: Very fast training of neural networks using large learning rates", "journal": "", "year": "2019", "authors": "N Leslie; Nicholay Smith;  Topin"}, {"ref_id": "b32", "title": "Intriguing properties of neural networks", "journal": "", "year": "2014", "authors": "Christian Szegedy; Wojciech Zaremba; Ilya Sutskever; Joan Bruna; Dumitru Erhan; Ian Goodfellow; Rob Fergus"}, {"ref_id": "b33", "title": "Orthogonalizing convolutional layers with the Cayley transform", "journal": "", "year": "2021", "authors": "Asher Trockman; J Zico; Kolter "}, {"ref_id": "b34", "title": "Lipschitz-margin training: Scalable certification of perturbation invariance for deep neural networks", "journal": "", "year": "2018", "authors": "Yusuke Tsuzuku; Issei Sato; Masashi Sugiyama"}, {"ref_id": "b35", "title": "Direct parameterization of Lipschitz-bounded deep networks", "journal": "", "year": "2023", "authors": "Ruigang Wang; Ian Manchester"}, {"ref_id": "b36", "title": "Towards fast computation of certified robustness for relu networks", "journal": "", "year": "2018", "authors": "Lily Weng; Huan Zhang; Hongge Chen; Zhao Song; Cho-Jui Hsieh; Luca Daniel; Duane Boning; Inderjit Dhillon"}, {"ref_id": "b37", "title": "Provable defenses against adversarial examples via the convex outer adversarial polytope", "journal": "", "year": "2018", "authors": "Eric Wong; Zico Kolter"}, {"ref_id": "b38", "title": "Dynamical isometry and a mean field theory of CNNs: How to train 10,000-layer vanilla convolutional neural networks", "journal": "", "year": "2018", "authors": "Lechao Xiao; Yasaman Bahri; Jascha Sohl-Dickstein; Samuel Schoenholz; Jeffrey Pennington"}, {"ref_id": "b39", "title": "Lot: Layer-wise orthogonal training on improving l2 certified robustness", "journal": "", "year": "2022", "authors": "Xiaojun Xu; Linyi Li; Bo Li"}, {"ref_id": "b40", "title": "Constructing orthogonal convolutions in an explicit manner", "journal": "", "year": "2021", "authors": "Tan Yu; Jun Li; Yunfeng Cai; Ping Li"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 .1Figure 1. Evaluation of 1-Lipschitz methods on different metrics. Scores are assigned from 1 (worst) to 5 (best) to every method based on the results reported in Sections 3 and 5.", "figure_data": ""}, {"figure_label": "22", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 Figure 2 .22Figure 2 plots the training time per epoch of the different models as a function of their size, while Figure 3 plots the", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 .3Figure 3. Inference throughput for different methods as a function of their size for CIFAR-10 sizes input images. All parameter transformations have been evaluated and cached beforehand", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 .4Figure 4. Memory required at training and inference time for input size 32 \u00d7 32.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 .5Figure 5. Certified robust accuracy by decreasing order. Note that the axes do not start at 0. For CIFAR-100 and Tiny ImageNet only the 10 best performing models are shown.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 6 .6Figure 6. Left: Variance of a validation batch over the batch dimension. For 1-Lipschitz layers, this property should be non-increasing, as proven in Appendix F. Right: Upper bound on the Lipschitz Constant applying the power method to every linear layer, and multiplying the results. Plot for ECO on CIFAR-10, with model S.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "b, where b is the batch size. Then we set \u00b5(l) ", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 7 .7Figure 7. Line plots of the robust accuracy for various methods where models are training with different time budgets", "figure_data": ""}, {"figure_label": "89", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 8 .Figure 9 .89Figure 8. Measured time and memory requirements on Tiny ImageNet.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Remark 3 .3The learnable parameter \u03a8 of the sandwich layer corresponds to a scaling of the bias. In details, for each parameters A, B, b and \u03a8 = diag e di there exists a rescaling of the bias b such thatl(x) = \u221a 2A T \u03a8ReLU \u221a 2\u03a8 -1 Bx + b = \u221a 2A T ReLU \u221a 2Bx + b(20)Proof. Observing that for each \u03b1 > 0 and x \u2208 R, ReLU (\u03b1x) = \u03b1ReLU (x), and that \u2200x \u2208 R n , \u03a8 -1 x =", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Certified robust accuracy for radius \u03f5 = 36/255 on the evaluated datasets. Training is performed for 24 hours. BCOP 22.4 26.2 27.6 27.0 13.8 16.9 17.2 16.8 CPL 28.3 29.3 29.8 30.3 18.9 19.7 20.3 20.1 Cayley 27.8 29.6 30.1 27.2 17.9 19.5 19.3 16.7", "figure_data": "Accuracy [%]Robust Accuracy [%]Methods XSSMLXSSMLCIFAR-10AOL71.7 73.6 73.4 73.7 59.1 60.8 61.0 61.5BCOP71.7 73.1 74.0 74.6 58.5 59.3 60.5 61.5CPL74.9 76.1 76.6 76.8 62.5 64.2 65.1 65.2Cayley 73.1 74.2 74.4 73.6 59.5 61.1 61.0 60.1LOT75.5 76.6 72.0-63.4 64.6 58.7-SLL73.7 74.2 75.3 74.3 61.0 62.0 62.8 62.3SOC74.1 75.0 76.9 76.9 61.3 62.9 66.3 65.4CIFAR-100AOL40.3 43.4 44.3 41.9 27.9 31.0 31.4 29.7BCOP41.4 42.8 43.7 42.2 28.4 30.1 31.2 29.2CPL42.3-45.2 44.3 30.1-33.2 32.1Cayley 42.3 43.9 43.5 42.9 29.2 30.5 30.5 29.5LOT43.5 45.2 42.8-30.8 32.5 29.6-SLL41.4 42.8 42.4 42.1 28.9 30.5 29.9 29.6SOC43.1 45.2 47.3 46.2 30.6 32.6 34.9 33.5Tiny ImageNetAOL26.6 29.3 30.3 30.0 18.1 19.7 21.0 20.6LOT30.7 32.5 28.8-20.8 21.9 18.1-SLL25.1 27.0 26.5 27.9 16.6 18.4 17.7 18.8SOC28.9 28.8 32.1 32.1 18.9 18.8 21.2 21.1"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "SOTA from the literature on CIFAR-10 sorted by publication date (from older to newer). Readers can note that there is a clear trend of increasing the model dimension to achieve higher robust accuracy.", "figure_data": "Certifiable Accuracy [%]Number of"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Architecture. It depends on width parameter w, kernel size k (k \u2208 {1, 3}) and the number of classes c. For details of the Downsize Block see Tab. 6.", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Downsize Block(k) with input size s \u00d7 s \u00d7 t:", "figure_data": "Layer nameKernel size Output size5 \u00d7Conv Activationk \u00d7 k -s \u00d7 s \u00d7 t s \u00d7 s \u00d7 tFirst Channels -s \u00d7 s \u00d7 t/2Pixel Unshuffle -s/2 \u00d7 s/2 \u00d7 2t"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Number of parameters for different model sizes, as well as the width parameter w such that the architecture in Tab. 5 has the correct size. in all our main experiments. The layer Zero Channel Padding in Table", "figure_data": "Size Parameters (millions) wXS1 < p < 216S4 < p < 832M16 < p < 3264L64 < p < 128128use kernel size 3"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Budget of training epochs for different model sizes, layer types and datasets. Batch size and training time are set to be 256 and 2h respectively for all the architectures.", "figure_data": "CIFARTinyImageNetXS SMLXS SMLAOL837 763 36783 223 213 12334BCOP127 125 942450 50 3911CPL836 797 522 194 240 194 14863Cayley356 214 7017 138 86 308ECO399 387 290 162 142 131 9554LOT222 68 11-83 295-SLL735 703 35379 242 194 11832SOC371 336 20177 122 87 6327Param.s (M)  \u2020 1.57 6.28 25.12 100.46 1.58 6.29 25.16 100.63\u2020 BCOP has less parameters overall, see Remark 2."}], "formulas": [{"formula_id": "formula_0", "formula_text": "L such that for all x, y \u2208 R n \u2225f (x) -f (y)\u2225 2 \u2264 L\u2225x -y\u2225 2 .(1)", "formula_coordinates": [2.0, 308.86, 284.47, 236.25, 41.28]}, {"formula_id": "formula_1", "formula_text": "\u2225M \u2225 2 = sup v\u0338 =0 \u2225M v\u2225 2 \u2225v\u2225 2 .(2)", "formula_coordinates": [2.0, 382.0, 413.84, 163.11, 23.51]}, {"formula_id": "formula_2", "formula_text": "Q = (I -A)(I + A) -1 .", "formula_coordinates": [3.0, 118.37, 381.97, 99.74, 10.81]}, {"formula_id": "formula_4", "formula_text": "exp(L)(x) := x + L \u22c6 x 1 + L \u22c6 2 x 2! + \u2022 \u2022 \u2022 + L \u22c6 k x k! + \u2022 \u2022 \u2022 ,(4)", "formula_coordinates": [3.0, 50.11, 573.66, 236.25, 31.89]}, {"formula_id": "formula_5", "formula_text": "D ii = j P \u22a4 P ij -1/2", "formula_coordinates": [3.0, 370.17, 119.78, 113.13, 26.86]}, {"formula_id": "formula_6", "formula_text": "Q = V (V T V ) -1", "formula_coordinates": [3.0, 325.5, 314.97, 71.22, 11.61]}, {"formula_id": "formula_7", "formula_text": "Y i+1 = 1 2 Y i (3I -Z i Y i ) , Z i+1 = 1 2 (3I -Z i Y i ) Z i ,(6)", "formula_coordinates": [3.0, 314.18, 396.82, 230.93, 22.31]}, {"formula_id": "formula_8", "formula_text": "l(x) = x - 2 \u2225W \u2225 2 2 W \u22a4 \u03c3(W x + b),(7)", "formula_coordinates": [3.0, 355.79, 526.27, 189.33, 24.39]}, {"formula_id": "formula_9", "formula_text": "l(x) = x -2W Q -2 D 2 \u03c3 W \u22a4 x + b ,(8)", "formula_coordinates": [3.0, 348.97, 667.93, 196.15, 11.03]}, {"formula_id": "formula_10", "formula_text": "P = W Q -1 .", "formula_coordinates": [3.0, 492.19, 702.62, 52.92, 10.53]}, {"formula_id": "formula_11", "formula_text": "Operations MACS O(\u2022) Memory Operations MACS O(\u2022) Memory O(\u2022) Standard CONV C M - - P AOL CONV C M CONV c 3 k 4 5P BCOP CONV C M BnB & MMs c 3 kt + c 3 k 3 c 2 kt + c 2 k 3 Cayley FFTs & MVs bs 2 c 2 5 2 M FFTs & INVs s 2 c 3 3 2 s 2 c 2 CPL CONVs & ACT 2C 3M power method s 2 c 2 k 2 P + s 2 c LOT FFTs & MVs bs 2 c 2 3M FFTs & MMs 4s 2 c 3 t 4s 2 c 2 t SLL CONVs & ACT 2C 3M CONVs c 3 k 4 5P SOC CONVs Ct 1 M t 1 F4 c 2 k 2 t 2 P", "formula_coordinates": [5.0, 88.98, 142.7, 417.27, 111.72]}, {"formula_id": "formula_12", "formula_text": "M f (x i , l i ) > \u03f5 \u221a 2 where the margin M f (x, l) of a model f at input x with label l, given as M f (x, l) = f (x) l -max j\u0338 =l f j (x)", "formula_coordinates": [6.0, 50.11, 332.5, 236.25, 41.8]}, {"formula_id": "formula_13", "formula_text": "A k+1 = A k I + 1 2 Q k , for Q k = I -A \u22a4 k A k(9)", "formula_coordinates": [11.0, 196.58, 235.04, 348.53, 22.31]}, {"formula_id": "formula_14", "formula_text": "v k+1 = W \u22a4 u k \u2225W \u22a4 u k \u2225 2 , u k+1 = W v k+1 \u2225W v k+1 \u2225 2 . (10", "formula_coordinates": [11.0, 209.32, 368.38, 331.65, 24.8]}, {"formula_id": "formula_15", "formula_text": ")", "formula_coordinates": [11.0, 540.96, 377.01, 4.15, 8.64]}, {"formula_id": "formula_16", "formula_text": "\u03c3 k = u \u22a4 k W v k .(11)", "formula_coordinates": [11.0, 267.82, 425.54, 277.29, 12.69]}, {"formula_id": "formula_17", "formula_text": "0", "formula_coordinates": [12.0, 104.93, 337.24, 4.33, 12.87]}, {"formula_id": "formula_18", "formula_text": "l(x) = \u221a 2A T \u03a8ReLU \u221a 2\u03a8 -1 Bx + b ,(12)", "formula_coordinates": [12.0, 212.59, 463.74, 332.53, 17.68]}, {"formula_id": "formula_19", "formula_text": "Y i+1 = 1 2 Y i (3I -Z i Y i ) , Z i+1 = 1 2 (3I -Z i Y i ) Z i ,(13)", "formula_coordinates": [13.0, 186.47, 367.52, 358.65, 22.31]}, {"formula_id": "formula_20", "formula_text": "Input 32 \u00d7 32 \u00d7 3 Zero Channel Padding 32 \u00d7 32 \u00d7 w Conv (1 \u00d7 1 kernel size) 32 \u00d7 32 \u00d7 w Activation 32 \u00d7 32 \u00d7 w Downsize Block(k) 16 \u00d7 16 \u00d7 2w Downsize Block(k) 8 \u00d7 8 \u00d7 4w Downsize Block(k) 4 \u00d7 4 \u00d7 8w Downsize Block(k) 2 \u00d7 2 \u00d7 16w Downsize Block(1) 1 \u00d7 1 \u00d7 32w Flatten 32w Linear 32w First Channels(c) c", "formula_coordinates": [14.0, 220.22, 315.18, 162.49, 140.46]}, {"formula_id": "formula_21", "formula_text": "BatchVar l = 1 b b i=1 \u2225a (l) i -\u00b5 (l) \u2225 2 2 ,(14)", "formula_coordinates": [17.0, 227.56, 119.18, 317.55, 54.62]}, {"formula_id": "formula_23", "formula_text": "BatchVar l+1 = 1 b b i=1 \u2225a (l+1) i -\u00b5 (l+1) \u2225 2 2 (16) \u2264 1 b b i=1 \u2225f l (a (l) i ) -f l (\u00b5 (l) )\u2225 2 2 (17) \u2264 1 b b i=1 \u2225a (l) i -\u00b5 (l) \u2225 2 2 (18) = BatchVar l .(19)", "formula_coordinates": [17.0, 207.39, 201.28, 337.73, 115.23]}, {"formula_id": "formula_24", "formula_text": "(l+1) i = f l (a(l)", "formula_coordinates": [17.0, 273.04, 326.56, 58.52, 14.07]}, {"formula_id": "formula_25", "formula_text": "n i=1 \u2225a (l+1) i -x\u2225 2 2 is minimal for x = \u00b5 (", "formula_coordinates": [17.0, 50.11, 326.56, 495.0, 23.83]}, {"formula_id": "formula_26", "formula_text": "(x) = \u221a 2A T \u03a8ReLU \u221a 2\u03a8 -1 Bx + b = \u221a 2A T \u03a8ReLU \u221a 2\u03a8 -1 \u221a 2Bx + \u03a8b = \u221a 2A T \u03a8\u03a8 -1 ReLU \u221a 2Bx + \u03a8b = \u221a 2A T ReLU \u221a 2Bx + \u03a8b .(21)", "formula_coordinates": [20.0, 201.65, 205.68, 343.46, 83.22]}], "doi": ""}
