{"title": "Ae 2 I: A Double Autoencoder for Imputation of Missing Values", "authors": "Fuchang Gao", "pub_date": "2023-01-16", "abstract": "The most common strategy of imputing missing values in a table is to study either the column-column relationship or the row-row relationship of the data table, then use the relationship to impute the missing values based on the non-missing values from other columns of the same row, or from the other rows of the same column. This paper introduces a double autoencoder for imputation (Ae 2 I) that simultaneously and collaboratively uses both row-row relationship and column-column relationship to impute the missing values. Empirical tests on Movielens 1M dataset demonstrated that Ae 2 I outperforms the current state-of-the-art models for recommender systems by a significant margin.", "sections": [{"heading": "Introduction", "text": "From medical research to recommender systems, missing data are ubiquitous. In many cases, the missing values need to be imputed before the dataset can be used. Indeed, imputation of missing values has been studied for decades [26], and many sophisticated methods of imputation have been developed using statistical or machine learning approaches [28], [7]. One basic strategy is to impute missing values with a weighted average of some selected non-missing values. The simplest is Mean Imputation, which imputes a missing value by the simple average of all non-missing values from the same row or same column. This simple imputation decreases the variance and alters the covariance structure of the original data. An improvement is to use the average of selective entries only. Along this line, the hot deck methods use some specific assumptions on missing patterns to determine a set of auxiliary variables based on which to match donors for each missing entry [34]. The k-nearest neighbor (KNN) imputation [3] method uses a metric to define the similarity between two rows or columns. For each missing entry, the k most similar rows or columns with non-missing values in the corresponding entry are selected. Because there are many different ways to introduce the similarity metric, KNN enables more sophisticated ways than hot deck. The selection of the similarity metric is of key importance to the performance of the model. Various modifications and refinements have led to many variants of KNN, such as Sequential KNN [19] and Iterative KNN [39].\nThe basic assumption behind these selection methods is that each missing value can be viewed as a function of some non-missing values in the same row or same column. One specific example is to assume that column (or row) variables satisfy some multivariate relationship. For instance, the Maximum Likelihood methods [1] assume a joint probability distribution of these variables, say a multi-normal distribution, then use non-missing values to find the best parameters for that joint distribution. Regression methods [20] assume a conditional probability distribution of one variable given a sequence of variables with complete data, then conduct univariate imputations sequentially until convergence. To avoid noise introduced by imputation, Multiple Imputation ( [27] [36]) techniques were introduced. A popular example is Multiple Imputation by Chained Equations (MICE) (c.f. [15]).\nTo assume a specific joint distribution of all the variables is very strict. A generalization is to assume that these variables can be expressed as a function of some smaller set of latent variables. The latter assumption can be justified by low-rank approximation. Indeed, if we view the data table as a rectangular matrix X, and factorize the matrix using singular value decomposition into X = U \u03a3V T , where \u03a3 is a diagonal matrix with singular values of X in its diagonal. By replacing the small singular values in \u03a3 with 0 to create a new diagonal matrix \u03a3 with k non-zero diagonal entries, we can approximate X by a low-rank matrix X := U \u03a3V T . If we denote by H the first k-column of the matrix U \u221a \u03a3, and W the first k rows of the matrix \u221a \u03a3V T , we can approximate X \u2248 HW . If we view the k row vectors of W as latent variables, then each column variables of X can be approximately expressed as a linear combination of these k latent variables. Matrix Factorization (MF) (e.g. [32] [16] [21]) methods have been developed from this idea.\nNote that the latent variables in MF are linear functions of the original variables. Extending from linear to non-linear functions is the main contribution of Collaborative Filtering (CF) (e.g. [6]). Closely related to CF are autoencoders. Unlike CF that starts with random low-rank matrices H and W to recover X, autoencoders start with X with missing values filled with 0, and generate low-rank matrices, and then use them to recover X. Starting from X takes more computations than directly starting with two random lowrank matrices, but being paid off with a more targeted generation of low-rank matrices. Because the idea of autoencoder is directly related to the current work, let us take a closer look.\nAn autoencoder is a fully-connected neural network consisting of two parts. The first part is called an encoder, which transforms d-dimensional input vectors to k d dimensional vectors through one or several layers. The second part is called a decoder, which transforms the k-dimensional vectors back to d-dimensional vectors. If the input vector x is a column vector, then the encoder transformation can be expressed as\nx = W r \u03c3 (W r-1 (\u2022 \u2022 \u2022 \u03c3 (W 1 x + b 1 ) \u2022 \u2022 \u2022 ) + b r-1 ) + b r ,\nwhere W i , b i are the weights and bias in the i-th layer. The encoder transforms the input d variables into k latent variables. The decoder transformation can be expressed as\nx = W s \u03c3 (W s-1 (\u2022 \u2022 \u2022 \u03c3 (W r+1 x + b r+1 ) \u2022 \u2022 \u2022 ) + b s-1 ) + b s ,\nwhich decodes the k latent variables back to the original d variables. Thus, an autoencoder can be viewed as non-linear matrix factorization.\nMany variants of autoencoder models have been developed. The most popular models include Autoencoder Recommender (AutoRec) [29], Denoising Autoencoder (DAE) [33], Multiple Imputation Denoising Autoencoder [11] [17], Variational Autoencoder [23] [24], and etc. For example, DAEs have put up strong performances in imputing a wide range of datasets, from electronic health records to scRNA-seq data and traffic data. While AutoRec has great performance on many benchmark datasets for collaborative filtering, including Movielens-1M, Movielens-10M [13].\nIn spite of their great performance, there is an obvious drawback in current autoencoder models for imputation, that is, they can only use column-column relationship or row-row relationship, not both. Observed the importance of using both row-row relationship and column-column relationships, Joint Neural Collaborative Filtering (J-NCF) was proposed in [6], in which column-column relationship was used to build a user-based autoencoder and row-row relationship was used to build an item-based autoencoder. The two autoencoders were trained separately, and the average of the two predictions was used for the final prediction. This idea was also used in Badsha et al. [2] on single cell gene expression research. In their \"LATE combined\" model, they alternately apply an autoencoder on row-row relationships, and an autoencoder on column-column relationships. The result was better than each individual autoencoder. However, because one relationship was significantly stronger than the other, the alternating training converged slowly, and the improvement was marginal.\nThere are also many other imputation methods such as the ones based on Generative Adversarial Networks (e.g. [30], [35], [18]). Because they are not directly related to the current work, we will not discuss them here.\nIn this paper, we introduce a double autoencoder with two branches, one of which uses row-row relationship and the other column-column relationship. Unlike J-NCF ( [6]) and \"LATE combined\" ( [2], in which two autoencoders were trained separately, in our model, the two branches are connected in the last layers. By penalizing the discrepancy on the cross intersections of the two outputs, information gathered from one branch is passed to the other, enabling both branches to make better predictions.", "publication_ref": ["b25", "b28", "b6", "b34", "b2", "b18", "b39", "b0", "b19", "b14", "b32", "b20", "b5", "b29", "b33", "b10", "b22", "b12", "b5", "b1", "b30", "b35", "b17", "b5", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Methods", "text": "To better introduce our methods, we represent the data as an m \u00d7 n real-values matrix with missing valued entered as 0. The m rows represents different observers, and the n column represents different items (variables).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Architecture", "text": "Our model consists of two connected autoencoders (or branches), the row autoencoder and the column autoencoder. The row autoencoder is trained on row vectors of the matrix. By a nested sequence of column operations and non-linear transforms, the row autoencoder outputs an m \u00d7 n matrix X with the missing entries filled using the learnt column-column relationship. The column autoencoder is trained on column vectors of the matrix. By a nested sequence of row operations and non-linear transforms, the column autoencoder also outputs an m \u00d7 n matrix with the missing entries filled in using the learnt row-row relationship. The two autoencoders are connected by the output layers. Figure 1 illustrates the architecture of the double autoencoder. The two antoencoders are trained simultaneously, and collaboratively. During each iteration,\n\u00d7 \ud835\udc4a1 \u0de9 \ud835\udc4a1 \u00d7 \u00d7 \ud835\udc4a3 \u00d7 \ud835\udc4a4 \u00d7 \ud835\udc4a2 \u0de9 \ud835\udc4a2 \u00d7 \u0de9 \ud835\udc4a3 \u00d7 \u0de9 \ud835\udc4a4 \u00d7 \ud835\udf0e \u2218 \ud835\udf0e \u2218 \ud835\udf0e \u2218 \ud835\udf0e \u2218 \ud835\udf0e \u2218 \ud835\udf0e \u2218", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Row autoencoder", "text": "Column autoencoder Fig. 1. The architecture of a double autoencoder.\na random list I of row vectors of X are fed to the row autoencoders, and a random list J of column vectors of X are fed to the column autoencoder. Let xij , 1 \u2264 j \u2264 n be the output of the row autoencoder when i-th row of X is fed into it, we define the loss function\nLoss row = i\u2208I n j=1 (x ij -x ij ) 2 * 1(x ij is not missing) i\u2208I n j=1 1(x ij is not missing) .\nSimilarly, let xij , 1 \u2264 i \u2264 m be the output of the column autoencoder when j-th column of X is fed into it, we define the loss function\nLoss col = j\u2208J m i=1 (x ij -x ij ) 2 * 1(x ij is not missing) j\u2208J m i=1 1(x ij is not missing) .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Connecting the Two Autoencoders", "text": "The key component of the model is the added penalty function that ties the two autoencoders together.\nBecause the two autoencoders are meant to impute the same matrix, at each iteration of training, if I row vectors are fed to the row autoencoder, and J column vectors are fed in to column autoencoder, the values of the outputs of the two autoenconders are meant to be equal at the cross interactions of these I rows and J columns. This motivates us to introduce the cross loss as\nLoss cross = i\u2208I j\u2208J (x ij -xij ) 2 |I| \u2022 |J| ,\nwhere |I| and |J| mean the cardinality of I and J respectively.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Consecutive Weight Regularization", "text": "Weight regularization is important to reduce overfitting of neural networks. The most common weight regularization is probably the L 2 -regularization. Here we use the consecutive weight regularization technique that has been recently developed in [9]. If we denote by W k (i, j) the i-th row j-th column entry of the weight matrix W k in k-th layer, then the consecutive weight regularization can be defined by\nLoss weight = s k=1 h i W k+1 (i, h) 2 - j W k (h, j) 2 ,\nAfter taking this weight regularization into consideration, the total loss function of the double autoencoder is L = Loss row + Loss col + \u03bbLoss cross + \u00b5Loss weight ,\nwhere \u03bb is a penalty constant and \u00b5 is the consecutive weight regularization constant.", "publication_ref": ["b8"], "figure_ref": [], "table_ref": []}, {"heading": "Customizing Activation Functions", "text": "It is known that almost all non-linear function can be used as an activation functions. The performances of activation functions, however, can be very different, and often depend the datasets and the model hyperparameters. There are several dozens of activation functions that are commonly used. In image classification, ReLU and Leaky ReLU are popular choices. For autoencoders for recommender systems, AutoRec [29] reported that sigmoid function performed better than ReLU. In recurrent neural networks, hyperbolic tangent is more commonly used.\nFor almost all the existing activation functions, when x \u2192 \u221e, the functions are either bounded (such as in Sigmoid, Hyperbolic tangent, Binary Step), or grow linearly (such as in ReLU, Leaky ReLU, PReLU, ELU, GELU, SELU, SiLU, Softplus, Mish). To take into consideration of the need for activation functions with different squashness at x \u2192 \u221e, and motivated by the even Seagull activation function constructed in [10], we introduce a family of activation functions: \u03c3 \u03b1 (x) = log(1 + |x| \u03b1 ) for \u03b1 > 0, \u03c3 \u00b1 \u03b1 = sign(x) log(1 + |x| \u03b1 ) and \u03c3 + \u03b1 = sign(x) log(1 + (max{x, 0}) \u03b1 ). Because these functions behavior like \u03b1 log x for large x, we call this family of activation functions as logarithmic growth family. These activation functions fill the gap between bounded activation functions and linear growth activation functions. The function log(1 + x 2 ) was called Seagull in [10], as its graph looks like a flying seagull. Here we call the function sign(x) log(1 + |x|) as linear logarithmic unit (LLU). Figure 2 illustrates the graph of these two functions. 3 Empirical Testing", "publication_ref": ["b29", "b9", "b9"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Data Set", "text": "To test the effectiveness of our model, we used the Movielens 100K (ML-100K), and Movielens 1M (ML-1M) datasets [13], which are popular stable benchmark datasets for recommendation systems. ML-100K contains about 100K anonymous ratings of 1682 movies by about 943 users. The data sparsity of 6.0%. ML-1M contains about 1 million anonymous ratings of 3706 movies by about 6040 users. The sparsity of the matrix is 4.47%. For ML-100K, we use the canonical u1.base for training, and u1.test for testing. For ML-1M, we use random 90:10 split for train/test.", "publication_ref": ["b12"], "figure_ref": [], "table_ref": []}, {"heading": "Baseline Models", "text": "We compared our model with 12 models which are either in the top 8 scorers for ML-1M, or in the top 6 scorer for ML-100K without using side information. These 12 models include: GLocal-K [12], Sparse FC [22], CF-NADE [38], I-AutoRec [29], GC-MC [4], I-CFN [31], BST [5], NNMF [8], GraphRec [25], IGMC [37], Self-Supervised Exchangeable Model and Factorized EAE [14]. GLocal-K is a Global-Local Kernel-based matrix completion framework that generalizes and represents a high-dimensional sparse user-item matrix entry into a low dimensional space with a small number of important features. Sparse FC uses a neural network architecture in which weight matrices are re-parametrized in terms of low-dimensional vectors, interacting through kernel functions. CF-NADE is a neural autoregressive architecture for CF tasks, which is inspired by the Restricted Boltzmann Machine (RBM) based CF model and the Neural Autoregressive Distribution Estimator (NADE). I-AutoRec is an autoencoder based model that uses item embeddings in the encoder. GC-MC is a graph auto-encoder framework based on differentiable message passing on the bipartite interaction graph. I-CFN is a hybrid model that enhances an autoencoder by using a loss function adapted to input data with missing values, and by incorporating side information. BST uses a transformer model to capture the sequential signals underlying users' behavior sequences followed by MLP. NNMF is a neural network matrix factorization model that replaces inner products in matrix factorization by a neural network. GraphRec uses a bipartite graph and constructs generic user and item attributes via the Laplacian of the user-item co-occurrence graph. IGMC is an inductive matrix completion model based on graph neural networks. Self-Supervised Exchangeable Model and Factorized EAE are deep learning models that use a parameter-sharing scheme.", "publication_ref": ["b11", "b21", "b38", "b29", "b3", "b31", "b4", "b7", "b24", "b37", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Implementation", "text": "We experimented the model with the sizes of the hidden layers to be of the form [m, k, m], for m \u2208 {300, 500, 700} and k \u2208 {50, 70, 200, 250} in both autoencoders. For ML-1M, which contains more training data, a larger k gave a slightly better result, and we chose [500, 250, 500]. For ML-100K which has relatively less training data, a smaller k performed slightly better. We chose [700, 70, 700]. We also experimented with 11 activation functions (Sigmoid, Tanh, ReLU, PReLU, ELU, GELU, SELU, SiLU, Softplus, Seagull, LLU). Sigmoid and Tanh are from the bounded family, ReLU and Softplus are from the linear growth family, and Seagull (i.e. log(1 + x 2 )) and LLU (i.e. sign(x) log(1 + |x|)) are from the logarithmic growth family. The functions from the same family had similar performance for both datasets. But activation functions from the different families reveal more noticeable difference. For ML-100K, the ones from linear growth family performed better, and we chose Softplus. For the ML-1M, the ones from the logarithmic growth family perform the best, and we chose Seagull.\nADAM optimizer was used to minimize the loss function (1) starting with learning rate 0.001. The cross intersection penalty constant \u03bb was chosen as 0.001 for ML-100K and 0.004 for ML-1M. The consecutiveweight regularization constant \u00b5 was chosen as 0.001 for ML-100K and 0.002 for ML-1M. Batch sizes for the row autoencoder and the column autoencoder were chosen so that the entire dataset was divided into 60 equal sized batches for ML-100K and 50 equal sized batches for ML-1M.\nTo tune the hyperparameters, we used k-folder cross validation with k = 5, 10, 20, 30 folders. Bigger k has a slightly better performance. However, after k = 30, the gain diminished. Each folder was training 50 epochs with early stop with tolerance 5 determined by the weighted RMSE on the validation set, that was defined by\n\u0176avg = MSE col \u0176row + MSE row \u0176col MSE row + MSE col ,\nwhere MES row and MSE col are the mean square errors of the row and column autoencoders on the validation set. After an early stop, the learning rate, weight regularization strength, and cross-intersection discrepancy penalty are multiplied by a factor of 1/6, 1/3, 2 respectively for ML-100K, and 1/2, 1/3, 1/2 respectively for ML-1M. The training was resumed from the best saved model. This process was repeated twice for ML-100K and 3 times for ML-1M. After the training of each folder completed, a prediction on the test set was made and saved. The average prediction of all k folders was used as the final prediction.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "We compared our model with 12 baseline models that are either the top 8 scorers for ML-1M or the top 6 scorers for ML-100K without using side information. The results are in Table 1. ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Model", "text": "Movielens 100K Movielens 1M GLocal-K [12] 0.890 0.822 Sparse FC [22] -0.824 CF-NADE [38] -0.829 I-AutoRec [29] -0.831 GC-MC [4] 0.910 0.832 I-CFN [31] -0.8321 BST [5] -0.8401 NNMF [8] -0.843 GraphRec [25] 0.901 -IGMC [37] 0.905 -Self-Supervised Exchangeable [14] 0.910 -Factorized EAE [14] 0.920 -Ae 2 I (current) 0.887 0.817\nOur testing on the ML-100K (see Figure 3-left) and ML-1M datasets (data not shown) revealed that consecutive weight regularization helped to reduce the RMSE for the test error. The effect of activations functions on the model performance depends on the dataset and model hyperparameters. For both ML-100K and ML-1M datasets, we observed that the activation functions with the same growth family tends to behave similarly, while activation functions from different growth families may have more noticeable difference. For ML-100K dataset, Softplus (from the linear growth family) performed the best. While for the ML-1M dataset, the logarithmic growth family (represented by Seagull and LLU) performed the best, see Figure 3-right.", "publication_ref": ["b11", "b21", "b38", "b29", "b3", "b31", "b4", "b7", "b24", "b37", "b13", "b13"], "figure_ref": ["fig_1", "fig_1"], "table_ref": []}, {"heading": "Discussion", "text": "In this paper, besides the double autoencoder, we used consecutive weight regularization, and demonstrated that it could be more effective than the usual L 2 regularization. We also noticed that under the new regularization, the training is more sensitive to learning rate and choice of activation function. Thus, an appropriate learning rate adjustment and customization of activation function appeared to be necessary. This may be due to the increased number of constraints on the weights posted by the consecutive weight regularization, which seems to have changed the distribution of the magnitudes of the weights, and thus, using a customized activation functions other than the commonly used linear growth family may take some advantages, as demonstrated by Figure 3-right.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Maximum-likelihood estimation of the parameters of a multivariate normal distribution", "journal": "Linear Algebra and its Applications", "year": "1985", "authors": "T Anderson; I Olkin"}, {"ref_id": "b1", "title": "Imputation of single-cell gene expression with an autoencoder neural network", "journal": "Quantitative Biology", "year": "2020", "authors": "D M Badsha; R Li; B Liu; Y Li; M Xian; N Banovich; A Fu"}, {"ref_id": "b2", "title": "Nearest neighbor imputation algorithms: A critical evaluation", "journal": "BMC Medical Informatics and Decision Making", "year": "2016", "authors": "L Beretta; A Santaniello"}, {"ref_id": "b3", "title": "Graph convolutional matrix completion", "journal": "", "year": "2017", "authors": "R Van Den Berg; T Kipf; M Welling"}, {"ref_id": "b4", "title": "Behavior sequence transformer for e-commerce recommendation in alibaba", "journal": "", "year": "2019", "authors": "Q Chen; H Zhao; W Li; P Huang; W Ou"}, {"ref_id": "b5", "title": "Joint neural collaborative filtering for recommender systems", "journal": "ACM Trans. Inf. Syst", "year": "2019", "authors": "W Chen; F Cai; H Chen; M D Rijke"}, {"ref_id": "b6", "title": "Review: A gentle introduction to imputation of missing values", "journal": "Journal of Clinical Epidemiology", "year": "2006", "authors": "A R T Donders; G J Van Der Heijden; T Stijnen; K G Moons"}, {"ref_id": "b7", "title": "Neural network matrix factorization", "journal": "", "year": "2019", "authors": "G K Dziugaite; D M Roy"}, {"ref_id": "b8", "title": "Consecutive weight regularization in neural networks", "journal": "", "year": "2022", "authors": "F Gao"}, {"ref_id": "b9", "title": "Data-aware customization of activation functions reduces neural network error", "journal": "", "year": "2021", "authors": "F Gao; B Zhang"}, {"ref_id": "b10", "title": "MIDA: Multiple Imputation Using Denoising Autoencoders", "journal": "", "year": "2018", "authors": "L Gondara; K Wang"}, {"ref_id": "b11", "title": "Glocal-k: Global and local kernels for recommender systems", "journal": "ACM", "year": "2021", "authors": "S C Han; T Lim; S Long; B Burgstaller; J Poon"}, {"ref_id": "b12", "title": "The movielens datasets: History and context", "journal": "ACM Transactions on Interactive Intelligent Systems", "year": "2015", "authors": "F Harper; J Konstan"}, {"ref_id": "b13", "title": "Deep models of interactions across sets", "journal": "PMLR", "year": "2018", "authors": "J Hartford; D Graham; K Leyton-Brown; S Ravanbakhsh"}, {"ref_id": "b14", "title": "Multiple imputation of unordered categorical missing data: a comparison of the multivariate normal imputation and multiple imputation by chained equations", "journal": "Braz. J. Probab. Stat", "year": "2016", "authors": "I Karangwa; D Kotze; R Blignaut"}, {"ref_id": "b15", "title": "Matrix factorization techniques for recommender systems", "journal": "Computer", "year": "2009", "authors": "Y Koren; R Bell; C Volinsky"}, {"ref_id": "b16", "title": "The midas touch: Accurate and scalable missing-data imputation with deep learning", "journal": "Political Analysis", "year": "2021", "authors": "R Lall; T Robinson"}, {"ref_id": "b17", "title": "Learning from incomplete data with generative adversarial networks", "journal": "", "year": "2019", "authors": "S C X Li; B Jiang; B Marlin"}, {"ref_id": "b18", "title": "A study on sequential k-nearest neighbor (sknn) imputation for treating missing rainfall data", "journal": "", "year": "2019", "authors": "D Ling; W Lai; K Kuok; S Gato-Trinidad"}, {"ref_id": "b19", "title": "Statistical analysis with missing data", "journal": "John Wiley & Sons", "year": "2002", "authors": "R J A Little; D B Rubin"}, {"ref_id": "b20", "title": "Spectral regularization algorithms for learning large incomplete matrices", "journal": "J. Mach. Learn. Res", "year": "2010", "authors": "R Mazumder; T Hastie; R Tibshirani"}, {"ref_id": "b21", "title": "Kernelized synaptic weight matrices", "journal": "PMLR", "year": "2018", "authors": "L Muller; J Martel; G Indiveri"}, {"ref_id": "b22", "title": "Handling incomplete heterogeneous data using vaes", "journal": "Pattern Recognition", "year": "2020", "authors": "A Naz\u00e1bal; P M Olmos; Z Ghahramani; I Valera"}, {"ref_id": "b23", "title": "Genomic data imputation with variational auto-encoders", "journal": "GigaScience", "year": "2020", "authors": "Y L Qiu; H Zheng; O Gevaert"}, {"ref_id": "b24", "title": "Attribute-aware non-linear co-embeddings of graph features", "journal": "Association for Computing Machinery", "year": "2019", "authors": "A Rashed; J Grabocka; L Schmidt-Thieme"}, {"ref_id": "b25", "title": "Inference and missing data", "journal": "Biometrika", "year": "1976", "authors": "D B Rubin"}, {"ref_id": "b26", "title": "Multiple imputation for nonresponse in surveys", "journal": "Wiley-Interscience", "year": "", "authors": "D B Rubin"}, {"ref_id": "b27", "title": "", "journal": "John Wiley & Sons, Inc", "year": "2004", "authors": "John Wiley; & Sons"}, {"ref_id": "b28", "title": "Multiple imputation: a primer", "journal": "Statistical methods in medical research", "year": "1999", "authors": "J L Schafer"}, {"ref_id": "b29", "title": "Autorec: Autoencoders meet collaborative filtering", "journal": "Association for Computing Machinery", "year": "2015", "authors": "S Sedhain; A K Menon; S Sanner; L Xie"}, {"ref_id": "b30", "title": "Vigan: Missing view imputation with generative adversarial networks", "journal": "", "year": "2017", "authors": "C Shang; A Palmer; J Sun; K S Chen; J Lu; J Bi"}, {"ref_id": "b31", "title": "Hybrid recommender system based on autoencoders", "journal": "Association for Computing Machinery", "year": "2016", "authors": "F Strub; R Gaudel; J Mary"}, {"ref_id": "b32", "title": "Missing value estimation methods for DNA microarrays", "journal": "Bioinformatics", "year": "2001", "authors": "O Troyanskaya; M Cantor; G Sherlock; P Brown; T Hastie; R Tibshirani; D Botstein; R B Altman"}, {"ref_id": "b33", "title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "journal": "J. Mach. Learn. Res", "year": "2010", "authors": "P Vincent; H Larochelle; I Lajoie; Y Bengio; P A Manzagol"}, {"ref_id": "b34", "title": "Empirical likelihood-based hot deck imputation methods", "journal": "J. Nonparametr. Stat", "year": "2012", "authors": "Y Xue; N A Lazar"}, {"ref_id": "b35", "title": "GAIN: Missing data imputation using generative adversarial nets", "journal": "PMLR", "year": "2018", "authors": "J Yoon; J Jordon; M Van Der Schaar"}, {"ref_id": "b36", "title": "Multiple imputation for missing data: Concepts and new development", "journal": "", "year": "2005", "authors": "Y Yuan"}, {"ref_id": "b37", "title": "Inductive matrix completion based on graph neural networks", "journal": "", "year": "2019", "authors": "M Zhang; Y Chen"}, {"ref_id": "b38", "title": "A neural autoregressive approach to collaborative filtering", "journal": "", "year": "2016", "authors": "Y Zheng; B Tang; W Ding; H Zhou"}, {"ref_id": "b39", "title": "Iterative knn imputation based on gra for missing values in tplms", "journal": "", "year": "2015", "authors": "M Zhu; X Cheng"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Fig. 2 .2Fig. 2. The graphs of activation functions with logarithmic growth", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Fig. 3 .3Fig. 3. Effect of Regularization and Activation on RMSE", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Comparison of RMSE test results. The best results are highlighted in bold. All RMSE results of the baseline models are from the respective papers cited in the first column.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "x = W r \u03c3 (W r-1 (\u2022 \u2022 \u2022 \u03c3 (W 1 x + b 1 ) \u2022 \u2022 \u2022 ) + b r-1 ) + b r ,", "formula_coordinates": [2.0, 196.35, 260.97, 219.93, 9.65]}, {"formula_id": "formula_1", "formula_text": "x = W s \u03c3 (W s-1 (\u2022 \u2022 \u2022 \u03c3 (W r+1 x + b r+1 ) \u2022 \u2022 \u2022 ) + b s-1 ) + b s ,", "formula_coordinates": [2.0, 186.63, 309.79, 239.37, 9.65]}, {"formula_id": "formula_2", "formula_text": "\u00d7 \ud835\udc4a1 \u0de9 \ud835\udc4a1 \u00d7 \u00d7 \ud835\udc4a3 \u00d7 \ud835\udc4a4 \u00d7 \ud835\udc4a2 \u0de9 \ud835\udc4a2 \u00d7 \u0de9 \ud835\udc4a3 \u00d7 \u0de9 \ud835\udc4a4 \u00d7 \ud835\udf0e \u2218 \ud835\udf0e \u2218 \ud835\udf0e \u2218 \ud835\udf0e \u2218 \ud835\udf0e \u2218 \ud835\udf0e \u2218", "formula_coordinates": [3.0, 191.61, 244.84, 229.27, 11.42]}, {"formula_id": "formula_3", "formula_text": "Loss row = i\u2208I n j=1 (x ij -x ij ) 2 * 1(x ij is not missing) i\u2208I n j=1 1(x ij is not missing) .", "formula_coordinates": [3.0, 181.28, 394.12, 249.44, 29.26]}, {"formula_id": "formula_4", "formula_text": "Loss col = j\u2208J m i=1 (x ij -x ij ) 2 * 1(x ij is not missing) j\u2208J m i=1 1(x ij is not missing) .", "formula_coordinates": [3.0, 182.23, 465.07, 247.54, 29.26]}, {"formula_id": "formula_5", "formula_text": "Loss cross = i\u2208I j\u2208J (x ij -xij ) 2 |I| \u2022 |J| ,", "formula_coordinates": [3.0, 228.58, 609.55, 154.85, 25.38]}, {"formula_id": "formula_6", "formula_text": "Loss weight = s k=1 h i W k+1 (i, h) 2 - j W k (h, j) 2 ,", "formula_coordinates": [4.0, 188.74, 106.44, 234.53, 30.55]}, {"formula_id": "formula_8", "formula_text": "\u0176avg = MSE col \u0176row + MSE row \u0176col MSE row + MSE col ,", "formula_coordinates": [5.0, 232.39, 669.05, 148.73, 25.74]}], "doi": "10.1016/0024-3795(85)90049-7"}
