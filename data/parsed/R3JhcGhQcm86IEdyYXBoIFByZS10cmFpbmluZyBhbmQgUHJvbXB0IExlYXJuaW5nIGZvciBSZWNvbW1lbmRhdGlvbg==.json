{
  "GraphPro: Graph Pre-training and Prompt Learning for Recommendation": "Yuhao Yang University of Hong Kong yuhao-yang@outlook.com Lianghao Xia University of Hong Kong aka_xia@foxmail.com Da Luo Wechat, Tencent lodaluo@tencent.com Kangyi Lin Wechat, Tencent plancklin@tencent.com",
  "ABSTRACT": "GNN-based recommendation systems have excelled at capturing complex user-item interactions through multi-hop message passing. Nevertheless, these methods often fail to account for the dynamic nature of user-item interactions, leading to challenges in adapting to changes in user preferences and the distribution of new data. Consequently, their scalability and performance in real-world dynamic settings are constrained. In our study, we introduce GraphPro, a framework that merges dynamic graph pre-training with prompt learning in a parameter-efficient manner. This innovative blend enables GNNs to adeptly grasp both enduring user preferences and transient behavior changes, thereby providing precise and up-todate recommendations. GraphPro tackles the issue of changing user preferences through the integration of a temporal prompt mechanism and a graph-structural prompt learning mechanism into the pre-trained GNN architecture. The temporal prompt mechanism imprints time-related information onto user-item interactions, equipping the model to inherently assimilate temporal dynamics, while the graph-structural prompt learning mechanism allows for the application of pre-trained insights to new behavior dynamics without continuous retraining. We also introduce a dynamic evaluation framework for recommendations to better reflect real-world situations and narrow the offline-online discrepancy. Our comprehensive experiments, including deployment in a large-scale industrial context, demonstrate the effortless plug-in scalability of GraphPro alongside various leading recommenders, underscoring the superiority of GraphPro in effectiveness, robustness, and efficiency. The implementation details and source code of our GraphPro are available in the repository at https://github.com/HKUDS/GraphPro.",
  "ACMReference Format:": "Yuhao Yang, Lianghao Xia, Da Luo, Kangyi Lin, and Chao Huang. 2024. GraphPro: Graph Pre-training and Prompt Learning for Recommendation. In Proceedings of ACM Conference (Conference'17). ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn ‚àó Chao Huang is the corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference'17, July 2017, Washington, DC, USA ¬© 2024 Association for Computing Machinery. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn",
  "Chao Huang ‚àó": "University of Hong Kong chaohuang75@gmail.com",
  "1 INTRODUCTION": "Recommender systems are integral to numerous Web platforms, assisting users in navigating through the overwhelming amount of information by suggesting relevant items. In recent years, graph neural networks (GNNs) have emerged as powerful tools for modeling user-item interactions in recommendation tasks, enabling effective representation learning on graph-structured data. By treating users and items as nodes and their interactions as edges, GNNs can capture intricate multi-hop relationships between users and items, facilitating the generation of personalized recommendation results. Previous studies in GNN-augmented recommendation have centered on crafting effective message passing strategies to delineate the collaborative relations between users and items [1, 45, 53]. These efforts seek to harness GCN's potential to unravel high-order connections within the user-item interaction graph. Follow-up investigations have delved into streamlining the message-passing approach [5, 14], diminishing GNN model intricacies [29, 36], and advancing sampling method quality [20]. Lately, the field has progressed with the integration of self-supervised learning (SSL) into GNN frameworks for recommendation [33]. These cutting-edge methodologies [2, 47, 56] predominantly utilize the InfoNCE loss function [30] to align contrastive perspectives, thus enhancing the performance of the LightGCN architecture [14]. Although these methods have demonstrated impressive performance, they have primarily focused on static scenarios (Figure 1 upper), disregarding the dynamic nature of recommendation. In real-world scenarios (Figure 1 lower), the recommendation system operates in a dynamic setting where the model continuously learns from newly arriving data and predicts for the current time [49, 52]. However, existing methods are mainly designed for single-graph training and evaluation, leading to a degradation in recommendation dynamics and widening the offline-online gap [21]. Furthermore, the arrival of new data may exhibit distribution shifts, further complicating the task of making accurate recommendations for graph-based recommendation models without incorporating useful contextual information for the newly arrived data. These challenges significantly limit the scalability of existing models and hinder their ability to adapt to evolving user preferences in a timely manner. This is critical for providing up-to-date and precise recommendations in dynamic environments. To address these challenges, we present a simple and effective framework called GraphPro, which combines parameter-efficient and dynamic Graph Pre-Training with Prompt Learning for recommendation. Our method begins with pre-training Graph Neural Conference'17, July 2017, Washington, DC, USA Yuhao Yang et al. Figure 1: Our dynamic recommendation setting compared to the vanilla single-graph training in existing methods. Existing Methods Real-world Dynamic Scenarios (Ours) Learning and Predicting Paradigms ùëá ! Predict Learn ùëá ! Predict Learn ùëá \" Predict Learn ùëá # Predict Learn ‚Ä¶ ‚Ä¶ Update Update New Data New Data Single Graph Networks (GNNs) on extensive historical interaction data, enabling them to capture long-term user preferences and item relevance. This pre-training phase assimilates knowledge from a substantial amount of historical interactions. Subsequently, during the finetuning phase on more recent target data, our model swiftly adapts to evolving user preferences and captures short-term behavior dynamics. This is achieved through a prompt learning schema, which facilitates effective knowledge transfer. To ensure the effective handling of evolving user preferences by the pre-trained GNN, our GraphPro framework seamlessly integrates a temporal prompt mechanism and a graph-structural prompt learning mechanism. This integration allows for the injection of time-aware context from new data, enabling the model to adapt to changing user preferences. Inspired by advancements in relative positional encoding techniques [32, 39], we meticulously design a dedicated temporal prompt mechanism that aligns with the message aggregation layer of GNNs. Within this prompt mechanism, we encode time information on interaction edges as part of the normalization term for aggregation, all in a parameter-free manner. This innate capability allows the model to naturally incorporate temporal information without the need for additional fine-tuning. By incorporating temporal awareness into the pre-trained graph model, we empower the model to effectively capture vital signals that are highly relevant to the evolving user preferences. Moreover, our graph-structural prompt learning mechanism facilitates the seamless transfer of knowledge from the pre-trained model to downstream recent recommendation tasks. This framework eliminates the need for continuous incremental learning of the pre-trained model, enabling the transfer of pre-trained knowledge to any future time period to effectively adapt to behavior dynamics. In this mechanism, we include the newly generated interaction edges between the fine-tuning time and the pre-training time as prompt edges, providing the pre-trained model with essential contextual information for the fine-tuning process. Rather than undergoing extensive training, we perform a single non-training forward pass on the prompt edges, prompting the pre-trained model to adapt to the distribution shift of node representations and effectively adjust its predictions. It's worth mentioning that our GraphPro is model-agnostic and parameter-efficient, making it easy to integrate into existing GNN recommenders as a plug-in enhancement. In summary, the main contributions of our work are as follows: ¬∑ We emphasize the importance of effectively and scalably pretraining and fine-tuning graph-based recommenders to accommodate time-evolving user preferences, thus facilitating up-to-date and accurate recommendations in dynamic environments. ¬∑ We introduce GraphPro, which effectively handles evolving user preferences through the pre-training and fine-tuning of GNNs. The proposed prompt learning paradigm enables the transfer of relevant knowledge from the pre-trained model to downstream recommendation tasks in both temporal and structural ways. ¬∑ Furthermore, we introduce a snapshot-based dynamic setting for recommender system evaluation, which offers a more realistic approximation to real-world recommendation scenarios compared to the traditional single-time testing approach. ¬∑ We conduct experiments on diverse datasets to showcase the robustness, efficiency, and performance advantages of GraphPro. To further validate the effectiveness of our framework, we also present an industry deployment on a large-scale online platform.",
  "2 PRELIMINARIES": "We define the task of pre-training and fine-tuning GNNs for recommendation. We denote the user set as U and the item set as I . In the context of collaborative filtering, a typical graph structure, constructed using existing methods [14], can be represented as G = (V , E) , where V = U ‚à™ I represents the set of all nodes in user-item interaction graph G . The edges in E correspond to interactions between users and items, with a value of ùë¶ ùë¢,ùëñ = 1. In order to provide recommendations at time slot ùëá 1, we gather historical user-item interactions to construct a graph G 1 = (V , E 1 ) , where E 1 represents the user-item interactions collected before ùëá 1. Existing stationary graph collaborative filtering models typically train the model from scratch using the complete dataset G 1. The objective is to optimize time-specific model parameters Œò 1 by maximizing the likelihood of generating accurate recommendations:  Dynamic Learning in Recommender Systems . In real-world applications, the evaluation of recommenders goes beyond the simplistic static setting commonly used in existing collaborative filtering method [14, 45]. In practice, researchers assess the long-term performance of models by deploying them in a live-update environment, as discussed in [54], where new user-item interactions are continuously generated over time. The model is specifically designed to make ongoing predictions for future user-item interactions based on this evolving data. Formally, the model should have initial weights Œò ùëõ -1 corresponding to different time intervals ùëá ùëõ , which are then updated through learning on new interactions G ùëõ to enhance the accuracy of up-to-date predictions.  In this study, we draw inspiration from the work [31] and employ a series of graph snapshots to simulate practical dynamic recommendation scenarios. These graph snapshots are represented as [G 1 , G 2 , ..., G ùëÅ ] , where we have a total of ùëÅ snapshots corresponding to different time intervals. Each graph snapshot consists of subsets of users and items from a global set, and the interactions between them evolve over time. Specifically, G ùëõ = (V ùëõ ‚äÇ V , E ùëõ ) , where V ùëõ represents the nodes in the snapshot and E ùëõ denotes the time slot-specific interaction edges. It is important to emphasize that snapshots G ùëõ are collected within the time period between two consecutive snapshots, namely [ ùëá ùëõ -1 , ùëá ùëõ ] . GraphPro: Graph Pre-training and Prompt Learning for Recommendation Conference'17, July 2017, Washington, DC, USA Figure 2: Overall framework of GraphPro. ùë° !\" ! ùë° ! ! \" \" ùë¢ # ùë¢ $ ùë¢ % ùë£ # ùë£ $ ùë° ! ! \" ! ùë° ! # \" \" ùë° ! \" \" \" min-max norm {ùë° !\" } [0, 1] = exp ‚àë \" ! exp ‚äï Graph Pre-training with Temporal Prompt ùê∫ ! ùê∫ \"#! Fine-Tuning with Graph-Structural Prompt ! ùê∫ $%&'$( ùê∫ $ +‚àëŒ¶ ) ‚ãÖ ùê∫ ) Forward Pass ùëø \" * ‚Ä¶‚Ä¶ {ùõº !\" } ùê∫ \" {ùõº !\" } ùëø \" Bi-Norm ùë¢ # ùõº !\" ùë° !\" ùë° !\" ! newly arriving Historical User-Item Graph Snapshots Graph Fine-tuning User-Item Graph with Timestamps no gradient time span extended time span extended Pre-trained GNN",
  "3 METHODOLOGY": "In this section, we provide the technical details of our proposed GraphPro framework, depicted in Figure 2, which illustrates its architecture. We introduce two crucial components: graph pretraining with a temporal prompt mechanism and a graph-structural prompt-enhanced fine-tuning mechanism. These components are specifically designed to enhance the performance and scalability of GNN-based recommenders in dynamic recommendation scenarios.",
  "3.1 Graph Pre-training with Temporal Prompt Mechanism": "In practical recommendation scenarios, user-item interaction data continues to accumulate over time. Online platforms like Amazon and TikTok constantly receive new user purchases and video watch logs, respectively, on a daily basis. In such dynamic environments, the availability of fresh user-item interactions provides valuable information that can be leveraged to guide pre-trained models in adapting to time-evolving user preferences and providing continuously up-to-date recommendations in dynamic settings. 3.1.1 Temporal Prompt Mechanism. In our framework, we introduce a temporal prompt mechanism to incorporate time-aware contextual information from the latest user preferences and behaviors. This mechanism allows for personalized and timely recommendations by considering the temporal dynamics of user-item interactions. To capture the temporal sequence of user-item interactions, we propose a relative time encoding scheme, which enables us to incorporate temporal information into graph convolutions. By encoding the relative time between interactions, the model can explicitly capture the temporal dependencies and changes in user preferences that are reflected in the newly arrived data. Our temporal prompt mechanism offers two significant advantages over existing time encoding techniques when it comes to capturing user behavior dependencies across different time slots. ¬∑ Generalization . Unlike the use of absolute positional embeddings in models like BERT [8], our mechanism takes inspiration from recent advancements in sequence modeling in NLP and leverages a relative positional encoding. Absolute positional embeddings have limited generalization capabilities across continuous time steps, which is problematic in our dynamic recommendation setting. These embeddings are trained on sequences with varying lengths but struggle to handle sequences beyond the trained lengths during fine-tuning and prediction. This leads to a distribution gap between the pretraining phase, where the model learns from fixed-length data, and the fine-tuning and prediction phase, where longer future time steps are encountered. ¬∑ Scalability . Our temporal prompt design avoids the need to add a fixed-length positional embedding to node representations. Instead, it generates relative temporal-aware weights that can be seamlessly integrated with message passing. This design allows our pretrained GNN to be easily applied to longer-range graph structures during fine-tuning and testing, greatly improving scalability for dynamic recommendation tasks. 3.1.2 Temporal Prompt-enhanced Graph Convolutions. In order to effectively capture the temporal dynamics of user-item interactions in our model, we have implemented a temporal prompt and incorporated relative time encoding into our graph convolutions. This enables our model to consider the most recent contextual signals from the new data, and adapt to evolving user preferences over time. Within the context of our user-item interaction graph G , the edge attributes consist of Unix timestamps denoted as ùíï unix . These timestamps represent the exact moments when users ùë¢ interacted with items ùë£ . To prepare these timestamps for encoding in our model, we convert them into relative time steps by dividing them by a fixed time interval ùúè . This time interval, which is a hyperparameter, can be defined with a resolution of either hour, day, or week. As a result, for any given edge ùëí ùë¢,ùë£ in the graph, its corresponding timestep attribute can be computed as follows:  Here, ùë° unix ùë¢,ùë£ is the Unix timestamp assigned to the edge ùëí ùë¢,ùë£ , and the ‚åä‚àó‚åã notation denotes the floor operation. To avoid the influence of specific numerical scales and ensure uniformity, we normalize these time attributes ùíï = ùë° ùë¢,ùë£ | ùëí ùë¢,ùë£ = 1 to the range of [0, 1].  To consider the temporal information among the neighbors during message aggregation in our GNNs, we apply the softmax function to the time attributes ùë° ùë¢,ùë£ of the first-order neighbors on the graph.  To enable dynamic time-aware graph neural network (GNN) for recommendation pretraining, we introduce an additional normalization term, ùõº ùë¢,ùë£ , into the message passing step of LightGCN. In this case, N ùë¢ represents the neighbors of node ùë¢ , and ùõº ùë¢,ùë£ encodes Conference'17, July 2017, Washington, DC, USA Yuhao Yang et al. the weight for aggregating information from node ùë£ to node ùë¢ .  We introduce a normalization term and apply mean-pooling to incorporate time-aware normalization into the original bidirectional graph normalization while preserving embedding magnitude. Adaptability and Efficiency . The incorporation of the time-aware normalization term ùõº ùë¢,ùë£ into the message passing of LightGCN enhances the GNN's adaptability to evolving user-item interactions over time. By giving more weight to interactions that are closer in time during neighbor aggregation, the model becomes more attentive to the dynamic nature of user-item interactions and assigns higher importance to recent interactions. This alignment with the objective of recommendation tasks ensures that the model captures timely and relevant user preferences, leading to more accurate and personalized recommendations. Importantly, our time-aware regularization approach does not introduce additional embedding encoding. Instead, it dynamically generates graph regularization terms based on the relative time order, making it a lightweight and efficient solution. This design allows the model to handle varying absolute time lengths effectively, showcasing excellent generalization capabilities and requiring minimal computational overhead.",
  "3.2 Fine-Tuning with Graph Prompt Mechanism": "In this section, we will discuss how we effectively transfer knowledge from a pre-trained Graph Neural Network (GNN) model for fine-tuning with future user-item interactions. To begin the finetuning process at the target time ùëá ùëõ , which occurs after the pretraining time ùëá ùëù , the intuitive way is to update the model parameters incrementally by simply fine-tuning. That is, we iteratively provide the model with data from the updated time intervals to fine-tune the node representations that were previously updated in the preceding time intervals. Therefore, the initial embeddings for fine-tuning at ùëá ùëõ are derived from the forward pass after the last fine-tuning step.  where forward (‚àó) represents the complete forward pass of the model, utilizing the last fine-tuned embeddings X ùëõ -1 and the graph structure G ùëõ -1. This method has the advantage of directly capturing users' continuous interest changes within a specific time span. However, the incremental fine-tuning mechanism has two significant drawbacks. First, iteratively updating model parameters based on small-range interactions may lead the model to converge to a local optimum specific to that time period, limiting the potential for continuous fine-tuning on the updated representations in the future. Secondly, persistently updating the parameters of the pre-trained model can result in a significant computational burden. 3.2.1 Graph-Structural Prompt Mechanism. In our approach, weaddress the mentioned issues by leveraging the interaction edges between the pre-training time ùëá ùëù and the current time ùëá ùëõ as prompt edges. This allows the pretrained model to directly fine-tune on future time periods without the need for iterative updates. Inspired by discrete prompt tuning in large language models [35, 37], we treat the edges of the graph during a specific time period as discrete prompts that guide the propagation of pretrained embeddings. This captures the representation shift between the pre-training and fine-tuning time points and provides better temporal-aware initial embeddings for fine-tuning. To generate prompt structures, we concatenate the pre-training graph structure with the sampled future edges between the pre-training and current fine-tuning time. This combination enables the model to capture the temporal dynamics and improve the effectiveness of fine-tuning:  where \" ‚äï \" denotes graph concatenation and \" ‚äô \" denotes graph sampling. Here, a hyper-parameter ùúô is introduced as the sampling decay for prompt structures, where a positive ùúô suggests that we include more early structures and less recent ones, and vice versa. After generating the prompt structures, we proceed with a forward pass using the pretrained embeddings X ùëù on the prompt graph to generate embeddings for fine-tuning. To mitigate the overfitting effect and improve generalization for more robust fine-tuning in our GraphPro framework, we introduce a random gating [3] mechanism that slightly perturbs the pre-trained embeddings.   The non-learnable random gating weights, e W ‚àà R ùëë √ó ùëë and e b ‚àà R ùëë , are generated from a Gaussian distribution. It's important to note that the relative time encoding also plays a vital role in facilitating the model's ability to sense relative temporal connections during the prompt propagation process. By propagating the embeddings learned from extensive pretraining over a large time period on the prompt edges, which include interactions from subsequent time periods, we achieve two objectives. Firstly, we enable the obtained embeddings to maintain stable user interests. Secondly, we swiftly capture changes in user interests within the subsequent time span. By refraining from directly training the embeddings on the short-term graph, we mitigate the risk of the model parameters becoming trapped in local optima. This approach grants us greater flexibility for subsequent fine-tuning and enables the model to more effectively adapt to users' evolving interests over time.",
  "3.2.2 Prompt Learning with Adaptive Gating Mechanism.": "To address the distribution shift in node representations between the time-aware graph snapshots G ùëõ -1 and G ùëõ , we introduce a learnable gating mechanism that adaptively transforms the input embeddings X 0 ùëõ . This gating mechanism allows for modeling the changes in user/item representations over time, effectively preserving the informative signals necessary for making accurate future recommendations. We employ gradient truncation on X 0 ùëõ to prevent direct optimization of the large-scale pre-trained model. Instead, we fine-tune X 0 ùëõ using newly interaction structual contexts G ùëõ to improve the accuracy of predictions at the target time interval ùëá ùëõ :   GraphPro: Graph Pre-training and Prompt Learning for Recommendation Conference'17, July 2017, Washington, DC, USA At this stage, we have derived the user and item representations X ùëõ for making predictions starting from time ùëá ùëõ . In this process, the learnable weights W ùëô and b ùëô , which have the same size as the random gating, are introduced. To estimate the probability of user ùë¢ interacting with item ùëñ , we calculate the dot product between the user and item representations x ùë¢ ùëõ , x ùëñ ùëõ , denoted as ÀÜ ùë¶ ùë¢,ùëñ = x ùë¢ ùëõ T ¬∑ x ùëñ ùëõ .",
  "3.3 Model Learning and Discussion": "3.3.1 Optimized Objective. In both the pre-training and finetuning stages, we define our training objectives based on optimizing the BPR loss. The BPR loss ensures that the predicted score for an observed interaction is higher than that of its unobserved negative samples. This loss function is commonly used in recommendation systems to model the preference ranking between items for individual users. By optimizing this loss, we aim to improve the model's ability to accurately rank and predict user-item interactions.  In our training strategy, we utilize a dataset ùê∑ that includes negative items ùëó sampled at each training mini-batch. Our approach follows a two-stage process. In the first stage, we pre-train a GNN-based recommender on a large-time-scale graph until convergence. This involves training the model on a comprehensive set of historical data, allowing it to learn long-term patterns and user preferences. In the second stage, we fine-tune the pre-trained model on small-timescale graph snapshots that include interactions from a more recent time period. This fine-tuning process helps the model adapt and capture short-term changes in user interests and item dynamics. 3.3.2 Interplotive Parameter Update. To ensure that the model parameters are learned in synchronization with the evolving user and item representations, it is important to update the pre-trained node embeddings over time steps. Inspired by the investigation in [31, 54], we propose an interpolative approach for updating the pre-trained user and item embeddings. Specifically, to estimate the best initial state for training at the next time step ùëá ùëõ , we combine the pre-trained embeddings with the embeddings learned within a sliding window [ ùëá ùëõ -ùúî , ùëá ùëõ -1 ] using interpolation. This allows us to leverage both the long-term knowledge captured during pretraining and the recent changes observed within the sliding window, enabling the model to effectively adapt to the evolving dynamics of user-item interactions:  X represents the model parameters, which correspond to the user and item embeddings. The left term of the equation calculates a weighted normalization of the weights [ X ùëõ -1 , ..., X ùëõ -ùúî ] , where the more recent fine-tuned representations are given less weight. This weighting helps to mitigate the local optima effect, where the model may get stuck in suboptimal solutions based on recent but noisy information. The hyperparameter ùúî controls the size of the sliding window, which determines the number of previous time steps considered for fine-tuning. As ùúî becomes smaller, the model updates its evolved representations more frequently, allowing it to capture recent changes. However, this can increase the risk of Table 1: Statistics of the experimental datasets. getting trapped in local optima due to the limited historical information considered. On the other hand, if ùúî is larger, the model can incorporate longer-term information, but may have reduced sensitivity to recent fine-tuned weights.",
  "4 EVALUATION": "In this section, we compare our proposed GraphPro with state-ofthe-art methods to address the following research questions. ¬∑ RQ1 : Can GraphPro outperform other time-aware graph learning models and pre-trained GNNs in dynamic recommendations? ¬∑ RQ2 : How does GraphPro perform when integrated as a modelagnostic plug-in component with state-of-the-art recommenders? ¬∑ RQ3 : Can GraphPro perform equally well or even better than the vanilla full-data training paradigm? ¬∑ RQ4 : How does the performance of GraphPro change under different ablation settings of key components and hyper-parameters? ¬∑ RQ5 : How effective is GraphPro in tackling the cold-start issue? ¬∑ RQ6 : How does the potential scalability of GraphPro facilitate efficient model convergence with our prompt learning paradigm? ¬∑ RQ7 : Can GraphPro effectively empower real-world recommendation systems when deployed in industrial applications?",
  "4.1 Experimental Settings": "4.1.1 Datasets. We utilize three public datasets that cover diverse real-world scenarios in dynamic recommendation. The Taobao dataset captures implicit feedback from Taobao.com, a Chinese ecommerce platform, over a period of 10 days. The Koubei dataset, provided for the IJCAI'16 contest, records 9 weeks of user interactions with nearby stores on Koubei in Alipay. The Amazon dataset consists of a 13-week collection of product reviews sourced from Amazon. More details about these datasets can be found in Table 1. 4.1.2 Baseline Models. We include the recent dynamic GNNs and graph prompt approaches as our baselines. Specifically, three most relevant research lines are included for comparison: Dynamic Recommendation Methods . We include DGCN [24], which explores categorizes edges as past and current and designing a GNN to propagate the information from past edges to current ones. However, it does not explicitly consider a dynamic setting with snapshots and focuses on a single graph. Graph Prompt Methods . This line aims to unify the pre-training and downstream tasks using a common template while leveraging prompts for task-specific knowledge retrieval. ¬∑ GraphPrompt [27]. It introduces an approach to pretraining and prompting in the context of graphs. It utilizes a learnable prompt Conference'17, July 2017, Washington, DC, USA Yuhao Yang et al. to guide downstream tasks, enabling them to access relevant knowledge from pretrained models using a shared template. ¬∑ GPF [10]. This method introduces prompts within the feature space of the graph, thereby establishing a general approach for tuning prompts in any pre-trained graph neural networks. Dynamic Graph Neural Networks . These networks are tailored to dynamic graphs, updating embeddings with time sensitivity to reflect graph changes. We benchmark our approach against notable models EvolveGCN [31] and ROLAND [54]: ¬∑ EvolveGCN [31]. It adapts to graph evolution using recurrent neural networks to modify GCN parameters over time, available as hidden state (-H) or recurrent input (-O) variants. ¬∑ ROLAND [54]. A dynamic graph method that employs metalearning to refresh embeddings for subsequent re-initialization, integrating these with GNN's layer-wise hidden states. 4.1.3 Integration with GNN Recommenders. GraphPro is a versatile architecture that seamlessly integrates as a plug-in component with any GNN-based recommender, highlighting its flexibility. In our evaluation, we implement GraphPro using the efficient LightGCN [14] model. Additionally, we extend GraphPro's applicability by integrating it with self-supervised learning-based recommenders like SGL [47], MixGCF [20], and SimGCL [56]. This integration provides empirical evidence of GraphPro's effectiveness in enhancing dynamic and adaptable recommendations. 4.1.4 Evaluation Protocols. In our evaluation, we model realworld dynamics using graph snapshots at varying intervals, such as daily or weekly. A two-step sliding window technique is applied to learn from current data and predict subsequent changes. Under the Pre-train and Fine-tune framework, we pre-train with a substantial dataset fraction, fine-tune, and test on the latter snapshots (refer to Table 1). Baselines adopt the same approach to maintain consistency. Dynamic GNNs start fine-tuning with weights from the pre-training phase. We average our results over all future temporal snapshots, using standard metrics such as Recall@k and nDCG@k at k=20, in line with existing methodologies [14, 15, 47].",
  "4.2 Performance Comparison (RQ1-RQ3)": "4.2.1 Comparison with Baselines. We showcase the performance of GraphPro alongside other methods, such as graph prompt techniques and dynamic GNNs, using LightGCN as the baseline, detailed in Table 2. Key observations from the analysis include: ¬∑ Our GraphPro surpasses both graph prompt and dynamic graph learning methods, highlighting the efficacy of our pre-training and prompt learning strategy. This performance can be credited to: 1) Our temporal prompt mechanism that adeptly captures the evolving user-item interactions during pre-training and finetuning, and 2) Our graph prompt design that ensures seamless knowledge transfer from the pre-trained model and mitigates distribution shifts across temporal snapshots. ¬∑ The varying performance of baseline methods underscores the complexity of dynamic recommendations. EvolveGCN excels with the Taobao dataset but not universally, possibly due to overfitting short-term user behaviors with its complex architecture. In contrast, GraphPro employs streamlined prompt mechanisms Table 2: When compared to various baselines utilizing different backbone architectures, GraphPro consistently exhibits strong overall performance across different types of datasets. The script ‚àó denotes the statistically significant results compared to the second best at ùëù < 0 . 01 level. that adeptly seize long-term user interests and assimilate fresh behavioral data, offering a more consistent and effective approach. ¬∑ Despite being regarded as the meticulously-designed dynamic GNN, ROLAND does not demonstrate superior performance. This limitation may be attributed to its intricate model parameter update schemes, which introduce larger perturbations to embeddings. Consequently, the representation learning for users and items is disrupted, rendering it less effective in capturing the time-evolving user preferences in recommendation tasks. 4.2.2 Integration with SOTA Methods. Wealso evaluate GraphPro's versatility with various base recommenders-MixGCF, SGL, and SimGCL-reimplementing all methods under uniform evaluation criteria. The averaged results across time slots can be found in Table 2. Our summarized observations are as follows: ¬∑ GraphPro consistently excels alongside advanced recommenders, showcasing our method's flexibility and capability to elevate performance across scenarios. Baselines, however, show varied performance depending on the base recommender and dataset used, with no single method dominating across all scenarios. While EvolveGCN-O often ranks second on Taobao, GPF stands out on Amazon regardless of the base recommender. Such variability GraphPro: Graph Pre-training and Prompt Learning for Recommendation Conference'17, July 2017, Washington, DC, USA Original(-)PT (-)GT (-)IU 0.0100 0.0125 0.0150 0.0175 0.0200 0.0225 0.0250 0.0275 0.0300 Metrics Recall nDCG 0 50 100 150 200 250 300 350 400 Epochs (a) Taobao Original(-)PT (-)GT (-)IU 0.020 0.025 0.030 0.035 0.040 Recall nDCG 0 50 100 150 200 250 300 350 400 Epochs (b) Koubei Metrics Original(-)PT (-)GT (-)IU 0.0050 0.0075 0.0100 0.0125 0.0150 0.0175 0.0200 0.0225 Recall nDCG 0 25 50 75 100 125 150 175 200 Epochs (c) Amazon Metrics Figure 3: Key component ablation study for fine-tuning stage. Y-axis denotes performance metrics on the left and epochs (displayed as ‚ñΩ ) for convergence on the right. in response to dataset specifics and underlying models prevents baselines from achieving consistent, substantial outcomes. ¬∑ Superior base model representations typically yield better results. GraphPro sees a 4.5% boost when comparing SGL to SimGCL on Taobao, suggesting it capitalizes well on the base model's improved representations in dynamic contexts. Conversely, methods like EvolveGCN-O fail to show gains with better base models, hinting at potential generalization weaknesses in their design. 4.2.3 Comparison with Full-Data Training. We place the results and discussion in Appendix A.2 due to space limitation.",
  "4.3 Ablation Study (RQ4)": "4.3.1 Key Components in Fine-Tuning. We perform a detailed ablation study to assess the impact of GraphPro's crucial elements during pre-training and fine-tuning. For comparative analysis against the original design, we introduce three GraphPro variants, each omitting a specific key component: ¬∑ (-)PT: This variant removes the P rompt T uning component that leverages prompt edges from past interactions, opting to finetune the model directly with new interactions. ¬∑ (-)GT: Adaptive G a T ing mechanism, which facilitates dynamic knowledge transformation in the fine-tuning phase, is omitted. ¬∑ (-)IU: The I nterplotive U pdate module is excluded, maintaining static pre-trained weights during fine-tuning. From the data in Figure 3, we discern that: 1) Each of the three integral elements of GraphPro plays a vital role. Omitting any one of them not only diminishes recommendation precision but may also prolong the time to convergence, underscoring their collective efficacy. 2) The structural prompt and interpolative update features are instrumental in boosting accuracy and avoiding local optima during dynamic learning. Their absence can lead to quicker convergence but at the cost of significantly reduced accuracy. 3) The adaptive gating feature is key to expedited convergence and enhanced accuracy through improved gradient flow. Without it, we observe notably extended convergence times and diminished accuracy, highlighting its role in bridging the distribution shifts encountered during fine-tuning across snapshots. 4.3.2 Effect of Pre-trained Model. We assess how different pretraining model architectures influence fine-tuning outcomes by examining four variants, focusing on the use of relative time encoding and the intrinsic representational strength of the models. The baseline model \"LGN(+)TE\" incorporates time encoding (TE) with LightGCN. The \"LGN(-)TE\" variant excludes TE during pretraining. The other two, \"MixGCF(+)TE\" and \"SimGCL(+)TE,\" employ more robust models for pretraining while maintaining LightGCN Epochs Pretrain Performance Test Performance 0.015 0.020 0.025 0.030 0.035 0.040 Metrics -6.36% -2.40% +2.40% +1.20% LGN(+)TE LGN(-)TE MixGCF(+)TE SimGCL(+)TE 0 50 100 150 200 250 300 350 (a) Taobao Pretrain Performance Test Performance 0.02 0.03 0.04 0.05 0.06 0.07 Metrics -6.16% -2.21% +4.70% +3.31% LGN(+)TE LGN(-)TE MixGCF(+)TE SimGCL(+)TE 0 25 50 75 100 125 150 Epochs (b) Koubei Figure 4: Ablation study for pretrained models. t1 t2 t3 t4 t5 t6 t7 t8 20% 40% 60% 80% 100% Relative Recall 0.0683 0.0458 0.0213 0.0139 0.0109 0.0081 0.0078 0.0052 Ours GPF (a) Tuned Users t1 t2 t3 t4 t5 t6 t7 t8 20% 40% 60% 80% 100% 0.0234 0.0194 0.0179 0.0203 0.018 0.0207 0.0192 0.021 Ours GPF (b) Untuned Users Relative Recall Figure 5: Evaluation performance for tuned and untuned users on Amazon compared with the best baseline, GPF. for fine-tuning. Performance contrasts between pretraining and fine-tuning for these variants are depicted in Figure 4. ¬∑ The temporal prompt mechanism enhances both the speed of convergence and the accuracy of predictions during pretraining and fine-tuning. It directs the model to capitalize on crucial temporal cues in the message-passing process. ¬∑ The use of more robust pretrained models correlates with improved performance, echoing trends seen in pre-trained language models like BERT [8] and vision models like ViT [9]. Our framework's scalability and adaptability are evident, as it allows the use of powerful pretrained models to excel in recommendation. This underscores the strategy of refining large pretrained models with more streamlined models downstream for enhanced results.",
  "4.4 Learning Impact Analysis (RQ5 & RQ6)": "4.4.1 Fine-tuned v.s. Untuned (Cold-start) Nodes. This section delves into the advantages of our fine-tuning approach on node representation learning for recommendations. We divide users into two categories according to whether they are subject to fine-tuning at each time step. Users who do not undergo fine-tuning are considered as cold-start users for that period. Using the Amazon dataset, we assess these two groups independently at every time step and present the findings in Figure 5. ¬∑ GraphPro effectively enhances representations for both tuned and cold-start users, consistently outperforming in most scenarios. This success is largely due to the structural prompt design, which leverages past interactions to enrich the representations. ¬∑ GraphPro showcases enhanced performance over the baseline with sustained long-term improvement. After an initial lag in the first snapshot, GraphPro overtakes the top baseline from ùëá 2 through ùëá 8, underscoring its effectiveness at navigating local optima for superior long-term results in dynamic settings. 4.4.2 Efficiency in Learning. This section examines the learning efficiency of our GraphPro. As a parameter-efficient approach, GraphPro minimizes learnable weights during both pretraining Conference'17, July 2017, Washington, DC, USA Yuhao Yang et al. Figure 6: The training curves for GraphPro and baselines on the Taobao and Koubei are presented, with scatters marking performance over various stages (pre-training and finetuning) against epochs. A star marker denotes the final convergence, and the right y-axis shows the total time consumed. 0 50 100 150 200 250 300 Epochs 0.010 0.015 0.020 0.025 0.030 0.035 Recall 100 200 300 400 500 Training Time (min) Ours EvoleGCN-O Pretraining Fine-tuning Time Cost (a) Taobao 0 50 100 150 200 250 Epochs 0.02 0.03 0.04 0.05 0.06 0.07 Recall 0 100 200 300 400 Training Time (min) Ours GPF Pretraining Fine-tuning Time Cost (b) Koubei Table 3: Online A/B test results spanning 5 days. HPC: highlypersonalized content. CC: click count. VCC: video click count. and fine-tuning, unlike incremental training methods, thereby reducing training time and computational costs. Our comparisons of training curves, shown in Figure 6, reveal that GraphPro not only outperforms EvolveGCN-O and GPF on the Taobao and Koubei datasets but also achieves this with fewer epochs. For example, GraphPro reaches convergence after just four fine-tuning stages, taking roughly half the epochs and time compared to the baselines, underscoring the efficiency improvements GraphPro provides.",
  "4.5 Online Deployment (RQ7)": "GraphPro is implemented on a major online content platform (name omitted for anonymity), serving millions of users, to assess its effectiveness in personalizing content recommendations. Integrating GraphPro into the core CTR prediction system, it utilizes unsupervised DGI-trained user embeddings as pretraining weights. Historical item-to-user interactions shape prompt edges, and these embeddings are fine-tuned with a 1-layer GNN to inform item representations for the primary model, with updates occurring every ten minutes. During an online A/B test, GraphPro and the current online model each engage approximately 2 million users. Performance is evaluated over a 5-day period using four metrics related to CTR and click count, as presented in Table 3. Results show that GraphPro significantly enhances the real-world recommender system by effectively modeling evolving user and item representations and leveraging deep user interests through pretraining and fine-tuning, all while being easy and cost-effective to deploy.",
  "5 RELATED WORKS": "GNNs for Recommendation. GNNs have become key in recommendation systems for capturing multi-hop collaborative signals [11]. Models like NGCF [45] and PinSage [53] recursively enhance user and item embeddings through message-passing. GCCF [6] adds a residual structure, whereas LightGCN [14] streamlines by discarding non-linearities. Further innovations include hypergraph learning [25, 51] and intent disentanglement [34, 46] to parse complex collaborative patterns. To tackle sparsity in graph recommenders, self-supervised techniques such as contrastive [23, 38] and generative approaches [48] have been introduced. Pre-training and Fine-tuning on GNNs. Drawing from NLP's pre-training and fine-tuning success, researchers have applied similar concepts to GNNs. Techniques like contrastive learning and infomax-based pre-training have been designed for advanced representation learning [18, 41, 44, 55]. Innovations in pre-training, such as link prediction, feature generation, and prompt-based finetuning, have emerged [10, 16, 19, 27, 42, 43]. Yet, these methods haven't fully overcome the intricacies of dynamic graph learning, often resulting in less-than-ideal performance in tasks with temporal elements. Our GraphPro stands out for its proficiency in handling dynamic graphs within the pre-training and fine-tuning. In the realm of recommendation tasks, [13, 26] suggest pretraining models tailored for user and item modeling. [13] pre-trains a GNN to mimic cold-start conditions, while [26] develops a pretraining strategy using side information. Nevertheless, these approaches concentrate on static recommendation settings and fail to account for the dynamic evolution of user preferences, thus limiting their effectiveness as time-sensitive recommenders. Dynamic Graph Learning. Research on learning with temporally evolving graphs is gaining traction. Techniques such as EvolveGCN [31], Dyngraph2vec [12], DGNN [28], ROLAND [54], and WinGNN [58] leverage RNNs and recurrent layers to grasp graph changes. Yet, they miss a dedicated pre-training and fine-tuning strategy specific to dynamic graphs and can introduce noise into user and item representations. DGCN [24] does consider dynamics in graph recommendation learning, but it falls short of handling graphs with evolving snapshots, limiting its assessment to a static graph. Sequential Recommendation. Sequential recommendation, a field attuned to time-sensitive recommendation scenarios, showcases works like attention-based approaches: SASRec [22] and BERT4Rec [40], GNN-based methods: SURGE [4] and Retagnn [17], and SSL models such as S3-rec [57], ICL [7], and DCRec [50]. Our approach, while similarly embracing temporal dynamics, diverges from these next-item predictors. Sequential models, with their autoregressive encoders, face limitations in next-item prediction and differ from graph-based methods that excel in top-K item retrieval. Furthermore, they don't incorporate explicit time intervals like days or weeks, but rather rely on fixed historical sequences.",
  "6 CONCLUSION": "This study introduces GraphPro, a new framework combining dynamic graph pre-training with prompt learning, aiming to substantially enhance the adaptability and scalability of recommender systems that are sensitive to temporal dynamics. GraphPro uses a temporal prompt mechanism to transfer knowledge from pretrained models to recommendation tasks on fresh data. Its graphstructured prompt learning features an adaptive gating mechanism that includes vital contextual information, easing fine-tuning and responsiveness to behavioral shifts. Through an extensive array of experiments conducted on a range of real-world datasets, we have empirically validated that GraphPro consistently outpaces contemporary state-of-the-art baselines, delivering exceptionally accurate dynamic recommendations across various temporal contexts. Looking ahead, our research will delve into the interpretability aspect of GraphPro, particularly focusing on the prompt graph edges. GraphPro: Graph Pre-training and Prompt Learning for Recommendation Conference'17, July 2017, Washington, DC, USA",
  "REFERENCES": "[1] Rianne van den Berg, Thomas N Kipf, and Max Welling. 2017. Graph convolutional matrix completion. arXiv preprint arXiv:1706.02263 (2017). [2] Xuheng Cai, Chao Huang, Lianghao Xia, and Xubin Ren. 2023. LightGCL: Simple Yet Effective Graph Contrastive Learning for Recommendation. arXiv preprint arXiv:2302.08191 (2023). [3] Yi Cai, Xuefei Ning, Huazhong Yang, and Yu Wang. 2023. Ensemble-in-One: Ensemble Learning within Random Gated Networks for Enhanced Adversarial Robustness. In AAAI , Vol. 37. 14738-14747. [4] Jianxin Chang, Chen Gao, Yu Zheng, Yiqun Hui, Yanan Niu, Yang Song, Depeng Jin, and Yong Li. 2021. Sequential recommendation with graph neural networks. In SIGIR . 378-387. [5] Lei Chen, Le Wu, Richang Hong, Kun Zhang, and Meng Wang. 2020. Revisiting graph based collaborative filtering: A linear residual graph convolutional network approach. In AAAI , Vol. 34. 27-34. [6] Lei Chen, Le Wu, Richang Hong, Kun Zhang, and Meng Wang. 2020. Revisiting graph based collaborative filtering: A linear residual graph convolutional network approach. In AAAI , Vol. 34. 27-34. [7] Yongjun Chen, Zhiwei Liu, Jia Li, Julian McAuley, and Caiming Xiong. 2022. Intent contrastive learning for sequential recommendation. In WWW . 2172-2182. [8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018). [9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020). [10] Taoran Fang, Yunchao Zhang, Yang Yang, Chunping Wang, and Lei Chen. 2022. Universal Prompt Tuning for Graph Neural Networks. arXiv preprint arXiv:2209.15240 (2022). [11] Chen Gao, Yu Zheng, Nian Li, Yinfeng Li, Yingrong Qin, Jinghua Piao, Yuhan Quan, Jianxin Chang, Depeng Jin, Xiangnan He, et al. 2023. A survey of graph neural networks for recommender systems: Challenges, methods, and directions. Transactions on Recommender Systems 1, 1 (2023), 1-51. [12] Palash Goyal, Sujit Rokka Chhetri, and Arquimedes Canedo. 2020. dyngraph2vec: Capturing network dynamics using dynamic graph representation learning. Knowledge-Based Systems 187 (2020), 104816. [13] Bowen Hao, Jing Zhang, Hongzhi Yin, et al. 2021. Pre-training graph neural networks for cold-start users and items representation. In WSDM . 265-273. [14] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for recommendation. In SIGIR . 639-648. [15] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural collaborative filtering. In WWW . 173-182. [16] Zhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, et al. 2022. Graphmae: Self-supervised masked graph autoencoders. In KDD . 594-604. [17] Cheng Hsu and Cheng-Te Li. 2021. Retagnn: Relational temporal attentive graph neural networks for holistic sequential recommendation. In WWW . 2968-2979. [18] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec. 2019. Strategies for pre-training graph neural networks. arXiv preprint arXiv:1905.12265 (2019). [19] Ziniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, and Yizhou Sun. 2020. Gpt-gnn: Generative pre-training of graph neural networks. In KDD . 1857-1867. [20] Tinglin Huang, Yuxiao Dong, Ming Ding, Zhen Yang, Wenzheng Feng, Xinyu Wang, and Jie Tang. 2021. Mixgcf: An improved training method for graph neural network-based recommender systems. In KDD . 665-674. [21] Yitong Ji, Aixin Sun, Jie Zhang, and Chenliang Li. 2023. A critical study on data leakage in recommender system offline evaluation. Transactions on Information Systems 41, 3 (2023), 1-27. [22] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recommendation. In ICDM . IEEE, 197-206. [23] Kang Li, Chang-Dong Wang, et al. 2023. Self-Supervised Group Graph Collaborative Filtering for Group Recommendation. In WSDM . 69-77. [24] Xiaohan Li, Mengqi Zhang, Shu Wu, Zheng Liu, Liang Wang, and S Yu Philip. 2020. Dynamic graph collaborative filtering. In ICDM . IEEE, 322-331. [25] Yinfeng Li, Chen Gao, Hengliang Luo, Depeng Jin, and Yong Li. 2022. Enhancing hypergraph neural networks with intent disentanglement for session-based recommendation. In SIGIR . 1997-2002. [26] Siwei Liu, Zaiqiao Meng, Craig Macdonald, and Iadh Ounis. 2023. Graph neural pre-training for recommendation with side information. Transactions on Information Systems 41, 3 (2023), 1-28. [27] Zemin Liu, Xingtong Yu, Yuan Fang, and Xinming Zhang. 2023. Graphprompt: Unifying pre-training and downstream tasks for graph neural networks. In WWW . 417-428. [28] Franco Manessi, Alessandro Rozza, and Mario Manzo. 2020. Dynamic graph convolutional networks. Pattern Recognition 97 (2020), 107000. [29] Kelong Mao, Jieming Zhu, Xi Xiao, Biao Lu, Zhaowei Wang, and Xiuqiang He. 2021. UltraGCN: ultra simplification of graph convolutional networks for recommendation. In CIKM . 1253-1262. [30] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018). [31] Aldo Pareja, Giacomo Domeniconi, Jie Chen, Tengfei Ma, Toyotaro Suzumura, Hiroki Kanezashi, Tim Kaler, Tao Schardl, et al. 2020. Evolvegcn: Evolving graph convolutional networks for dynamic graphs. In AAAI , Vol. 34. 5363-5370. [32] Ofir Press, Noah Smith, and Mike Lewis. 2021. Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation. In ICLR . [33] Xubin Ren, Lianghao Xia, Yuhao Yang, Wei Wei, Tianle Wang, Xuheng Cai, and Chao Huang. 2023. SSLRec: A Self-Supervised Learning Library for Recommendation. arXiv preprint arXiv:2308.05697 (2023). [34] Xubin Ren, Lianghao Xia, Jiashu Zhao, Dawei Yin, and Chao Huang. 2023. Disentangled Contrastive Collaborative Filtering. arXiv preprint arXiv:2305.02759 (2023). [35] Timo Schick and Hinrich Sch√ºtze. 2020. Exploiting cloze questions for few shot text classification and natural language inference. arXiv preprint arXiv:2001.07676 (2020). [36] Yifei Shen, Yongji Wu, Yao Zhang, Caihua Shan, Jun Zhang, et al. 2021. How powerful is graph convolution for recommendation?. In CIKM . 1619-1629. [37] Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. 2020. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980 (2020). [38] Jie Shuai, Kun Zhang, Le Wu, Peijie Sun, Richang Hong, Meng Wang, and Yong Li. 2022. A review-aware graph contrastive learning framework for recommendation. In SIGIR . 1283-1293. [39] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. 2021. RoFormer: Enhanced Transformer with Rotary Position Embedding. arXiv:2104.09864 [cs.CL] [40] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer. In CIKM . 1441-1450. [41] Fan-Yun Sun, Jordan Hoffman, Vikas Verma, and Jian Tang. 2019. InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization. In ICLR . [42] Mingchen Sun, Kaixiong Zhou, Xin He, et al. 2022. Gppt: Graph pre-training and prompt tuning to generalize graph neural networks. In KDD . 1717-1727. [43] Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, and Jihong Guan. 2023. All in One: Multi-Task Prompting for Graph Neural Networks. (2023). [44] Petar Veliƒçkoviƒá, William Fedus, William L Hamilton, Pietro Li√≤, Yoshua Bengio, et al. 2018. Deep graph infomax. arXiv preprint arXiv:1809.10341 (2018). [45] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019. Neural graph collaborative filtering. In SIGIR . 165-174. [46] Jiahao Wu, Wenqi Fan, Jingfan Chen, Shengcai Liu, et al. 2022. Disentangled contrastive learning for social recommendation. In CIKM . 4570-4574. [47] Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian, et al. 2021. Self-supervised graph learning for recommendation. In SIGIR . 726-735. [48] Lianghao Xia, Chao Huang, Chunzhen Huang, Kangyi Lin, Tao Yu, and Ben Kao. 2023. Automated Self-Supervised Learning for Recommendation. In WWW . 992-1002. [49] Teng Xiao, Zhengyu Chen, and Suhang Wang. 2023. Reconsidering Learning Objectives in Unbiased Recommendation: A Distribution Shift Perspective. In KDD . 2764-2775. [50] Yuhao Yang, Chao Huang, Lianghao Xia, Chunzhen Huang, Da Luo, and Kangyi Lin. 2023. Debiased Contrastive Learning for Sequential Recommendation. In WWW . 1063-1073. [51] Yuhao Yang, Chao Huang, Lianghao Xia, Yuxuan Liang, Yanwei Yu, and Chenliang Li. 2022. Multi-behavior hypergraph-enhanced transformer for sequential recommendation. In KDD . 2263-2274. [52] Zhengyi Yang, Xiangnan He, Jizhi Zhang, Jiancan Wu, Xin Xin, Jiawei Chen, and Xiang Wang. 2023. A Generic Learning Framework for Sequential Recommendation with Distribution Shifts. In SIGIR . [53] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec. 2018. Graph convolutional neural networks for web-scale recommender systems. In KDD . 974-983. [54] Jiaxuan You, Tianyu Du, and Jure Leskovec. 2022. ROLAND: graph learning framework for dynamic graphs. In KDD . 2358-2366. [55] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, et al. 2020. Graph contrastive learning with augmentations. NeurIPS 33 (2020), 5812-5823. [56] Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Lizhen Cui, and Quoc Viet Hung Nguyen. 2022. Are graph augmentations necessary? simple graph contrastive learning for recommendation. In SIGIR . 1294-1303. [57] Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, et al. 2020. S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization. In CIKM . 1893-1902. [58] Yifan Zhu, Fangpeng Cong, Dan Zhang, Wenwen Gong, Qika Lin, Wenzheng Feng, Yuxiao Dong, and Jie Tang. 2023. WinGNN: Dynamic Graph Neural Networks with Random Gradient Aggregation Window. In KDD . 3650-3662. Conference'17, July 2017, Washington, DC, USA Yuhao Yang et al.",
  "A APPENDIX": "",
  "A.1 Hyper-parameter Sensitivity": "Our research focuses on investigating the response of GraphPro to hyper-parameter adjustments, using three distinct datasets. We conduct a meticulous analysis of the primary hyperparameters that play a crucial role in the operation of GraphPro. These hyperparameters include the temporal prompt interval ùúè , the updating window ùúî , which determines the intervals for model updates, and the sampling decay ùúô , which influences the emphasis on past interactions. To ensure a comprehensive evaluation, we carefully select ranges of values for each hyperparameter that are tailored to the unique characteristics of each experimental dataset utilized. ¬∑ ùúè : Taobao-[ 0 . 5 , 1 , 4 , 12 ] ; Koubei and Amazon-[ 24 , 48 , 72 , 96 ] ¬∑ ùúî : Taobao and Koubei-[ 1 , 2 , 3 ] ; Amazon-[ 2 , 4 , 6 ] ¬∑ ùúô : [ 0 . 05 , 0 . 1 , -0 . 05 , -0 . 1 ] Our analysis of hyper-parameter sensitivity in GraphPro is presented in Figure 8, from which we derive several observations: 1) GraphPro exhibits a greater degree of sensitivity to hyperparameter changes when applied to the Amazon data in comparison to those of Taobao and Koubei. This heightened sensitivity on Amazon is likely due to its considerably longer fine-tuning and prediction period, which spans an entire 9 weeks. This duration is substantial when contrasted with the other datasets and suggests a correlation between the span of fine-tuning and the impact of hyperparameters on the model's long-term performance. For enhanced long-term accuracy, we need to be careful when picking these settings. 2) When examining the details, it becomes evident that GraphPro is notably more affected by the updating window ùúî compared to other hyperparameters in its configuration. The size of the updating window requires careful consideration, as it is crucial to strike a delicate balance. If the window is too narrow, the model may fail to incorporate important updates that contribute to the evolving representations of users and items. On the other hand, if the window is too wide, the model may become burdened with irrelevant noise, thus obscuring the current representation learning process. 3) Moreover, our findings reveal a reassuring aspect of GraphPro's design: its resilience. Minor modifications in the hyper-parameter settings do not precipitate substantial declines in the model's performance. We emphasize the significance of customizing hyperparameter choices to align with the intrinsic characteristics and unique aspects of the dataset being utilized. hp1 hp2 hp3 hp4 98.6% 98.8% 99.0% 99.2% 99.4% 99.6% 99.8% 100.0% Change in Recall taobao koubei amazon hp1 hp2 hp3 88.0% 90.0% 92.0% 94.0% 96.0% 98.0% 100.0% taobao koubei amazon Change in Recall (a) Time Interval ùúè hp1 hp2 hp3 hp4 99.0% 99.2% 99.4% 99.6% 99.8% 100.0% Change in Recall taobao koubei amazon (b) Updating Window ùúî (c) Sampling Decay ùúô Figure 8: Performance change w.r.t. key hyperparameters. 0 1 2 3 Snapshot 0.020 0.025 0.030 0.035 0.040 Recall 10 2 10 3 Epoch Time (s) 1200s 20s 360s 15s taobao koubei full Ours (a) Taobao & Koubei (b) Amazon 0 1 2 3 4 5 6 7 Snapshot 0.010 0.015 0.020 0.025 0.030 0.035 Recall 10 0 10 1 Epoch Time (s) 22s 0.27s full Ours Figure 7: GraphPro v.s. full-data training using LightGCN. The results with the average training time for a single epoch are being depicted by a horizontal line on the right Y-axis.",
  "A.2 Comparsion with Full-Data Training": "In this section, our primary objective is to conduct a comprehensive performance comparison between our GraphPro and the traditional full-data training approach (FULL). Our aim is to determine if GraphPro can match or exceed the performance of FULL. To achieve this, we conduct a meticulous comparison using three distinct datasets, with a keen focus on Recall and average epoch time. The results of our analysis are visually represented in Figure 7. Our findings can be summarized into two key observations: 1) In our study, GraphPro stands out on both the Taobao and Koubei datasets, surpassing FULL's performance at every testing point. This underscores GraphPro's robust capability in processing time-sensitive data to yield more precise recommendations. With the Amazon dataset, the two approaches are closely matched, but GraphPro shows a distinct advantage in the initial phases. This suggests that GraphPro's dynamic method can be immediately beneficial, improving performance from the start without losing ground to FULL as the evaluation progresses over time. 2) Furthermore, a significant aspect of GraphPro's appeal lies in its exceptional efficiency. When compared to the FULL approach, GraphPro cuts down the average training time dramatically, achieving astonishing efficiency gains. Specifically, we observe an impressive 60-fold increase in speed on the Taobao dataset, a 24-fold increase on the Koubei dataset, and an unparalleled 81-fold increase on the Amazon dataset. These improvements in training efficiency can be attributed to the innovative architecture of GraphPro, which seamlessly integrates pre-training and prompt learning techniques. This strategic integration significantly accelerates the learning process, enabling GraphPro to rapidly assimilate and adeptly apply new data to generate accurate and timely recommendations.",
  "keywords_parsed": [
    "None"
  ]
}