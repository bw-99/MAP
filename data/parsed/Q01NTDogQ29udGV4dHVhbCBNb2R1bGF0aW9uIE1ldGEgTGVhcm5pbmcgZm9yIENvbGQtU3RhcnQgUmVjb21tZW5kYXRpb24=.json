{"title": "CMML: Contextual Modulation Meta Learning for Cold-Start Recommendation", "authors": "Xidong Feng; Chen Chen; Dong Li; Mengchen Zhao; Jianye Hao; Jun Wang", "pub_date": "2021-10-29", "abstract": "Practical recommender systems experience a cold-start problem when observed user-item interactions in the history are insufficient. Meta learning, especially gradient based one, can be adopted to tackle this problem by learning initial parameters of the model and thus allowing fast adaptation to a specific task from limited data examples. Though with significant performance improvement, it commonly suffers from two critical issues: the non-compatibility with mainstream industrial deployment and the heavy computational burdens, both due to the inner-loop gradient operation. These two issues make them hard to be applied in practical recommender systems. To enjoy the benefits of meta learning framework and mitigate these problems, we propose a recommendation framework called Contextual Modulation Meta Learning (CMML). CMML is composed of fully feed-forward operations so it is computationally efficient and completely compatible with the mainstream industrial deployment. It consists of, a context encoder that can generate context embedding to represent a specific task, a hybrid context generator that aggregates specific user-item features with task-level context, and a contextual modulation network, which can modulate the recommendation model to adapt effectively. We validate our approach on both scenario-specific and user-specific cold-start setting on various real-world datasets, showing CMML can achieve comparable or even better performance with gradient based methods yet with higher computational efficiency and better interpretability.", "sections": [{"heading": "INTRODUCTION", "text": "Personalized recommendation has been rapidly developed and thoroughly influenced various fields including web services, Ecommerce, and social media. Based on user's feature and past interaction history with items, the personalized recommender system can generate reasonable recommendation results for user's preferences. Among the personalized recommendation approaches, the data-driven deep learning based recommendation algorithms have gained increasing attention because of their superior performance compared with traditional recommendation algorithms. However, both traditional and data-driven machine learning based recommendation algorithms struggle to tackle the cold-start problem, since the recommender system can only get access to very limited user's interaction history. To address cold-start recommendation problems, some works directly utilize content information such as user's profiles [2,30] or incorporate it into traditional collaborative filtering [5,28,34], but they still often fail to generalize to users with only a few interactions. Some other works [14,22] try to solve the cold-start problem by transferring domain knowledge from cross-domain datasets, but they still cannot get rid of the need for shared examples from different domains.\nMeta learning, also known as leaning to learn, is a prominent machine learning paradigm aiming at learning meta knowledge among tasks to achieve fast adaptation with limited data examples when facing a new task. Inspired by this, some works [7,8,19,35] leverage the latest progress on meta learning to model and solve the cold-start recommendation problem. They treat different entities (like users, scenarios) in recommender system as tasks, so the coldstart recommendation problem can be transformed into a newtask adaptation problem in meta learning. They commonly adopt MAML [9], which is a representative gradient based meta learning framework to solve the problem. Specifically, they try to optimize the recommendation model's initial parameters, to make it capable of fast adaptation from limited data examples through taking only a few gradient steps. Practically, they have achieved significant improvements over previous deep learning based recommendation algorithms, which validates the effectiveness of introducing meta learning framework into the cold-start problem.\nHowever, they have also brought some new issues which can be basically summarized into two parts. (1) The additional innerloop gradient adaptation will lead to much lower computational efficiency with respect to both the training phase and inference phase. (2) The inner-loop gradient operator is incompatible with current industrial recommender system's framework, where most recommendation algorithms only consist of feed-forward network like Multilayer perception (MLP). These two problems prevent the broad use of these gradient based approach on cold-start problems.\nIn order to both enjoy the benefits from meta learning and mitigate the computational or deployment problems, we are trying to answer the following question: from limited data examples, how can we conduct meta learning model adaptation based on feed-forward operations, rather than backward propagation as done in the gradient based approach? An intuitive idea is to directly map limited samples to continuous representation for guiding the model adaptation. Specifically, we need a powerful embedding network for information extraction from samples and a flexible modulation network for model's adaptation. These modules can be trained via the meta learning framework and thus equip the model with fast adaptation ability even given limited data.\nBased on these motivations, we propose a Contextual Modulation Meta Learning (CMML) recommendation framework for cold-start problems. Note that the term contextual here does not represent any specific context information, such as time, user's features. Here we treat the interaction history as contextual information to reveal entities' (users/scenarios) implicit features. In our framework, instead of utilizing inner-loop gradient adaptation, we make use of modulation techniques for model adaptation when a new task comes. Specifically, our model consists of three parts: (1) A context encoder that maps the limited data examples to effective context aggregation. (2) A hybrid context generator, which combines the above encoded task-level context (scenario/user-specific information) and instance-level feature (user-item features) together. By modelling the interaction between task-level context and local useritem pair's features, flexible hybrid context can largely enhance the model's capacity. (3) A context based modulation network, which can modulate the neural network for fast adaptation based on context generated by two parts above. Compared with previous gradient based meta recommendation algorithms, one of the most appealing characteristics for our method is that it is fully composed of feed-forward neural network and successfully gets rid of the gradient adaptation. It can significantly improves the computational efficiency and is compatible with industrial recommender systems.\nThe remainder of this paper is organized as follows: in Section 2, we introduce the problem formulation for meta learning based cold-start recommendation and the setting for two representative types of important cold-start problems. Section 3 depicts the whole framework of the proposed CMML approach and also provides computational efficiency analysis. Experimental results about detailed experimental settings, comparison, visualization, ablation study are shown in Section 4. We review related work in Section 5. Finally, we conclude our paper in Section 6.", "publication_ref": ["b2", "b30", "b5", "b28", "b34", "b14", "b22", "b7", "b8", "b19", "b35", "b9", "b1", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "PRELIMINARY", "text": "In this section, we formally define the problem formulation for meta learning based recommendation and detail the settings for scenario-specific and user-specific meta recommendation problems.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Problem Formulation", "text": "Traditional recommendation is composed of two sets, user set U, item set I, which includes user's features and item's features, respectively. It conducts the mapping function: U \u00d7 I \u2192 R, which means they will generate prediction results based on users' and items' features. For context-aware recommendation problem, the recommender system is required to conduct the mapping function: U \u00d7 I \u00d7 C \u2192 R, where \ud835\udc36 represents contextual information set. In this paper, we are trying to solve cold-start problem by meta learning based recommendation, where model can conduct task adaptation based on limited data from specific entities (users/scenarios). For meta learning based recommendation, the framework needs to transform interaction history into implicit contextual information and conduct rapid adaptation to predict results. This mapping function corresponds to U \u00d7 I \u00d7 {U, I} \u210e \u2192 U \u00d7 I \u00d7 C \u2192 R, where {U, I} \u210e represents interaction history. The mapping function {U, I} \u210e \u2192 C can be gradient descent operator like [19], [8], or the contextual modulation framework introduced in this paper.\nFor meta learning problems, there are some important terms that need to clarify for clear comprehension. The terms meta-training and meta-testing: these two terms correspond to the standard definition of training set and testing set in traditional machine learning. The major difference is that data examples in meta learning are tasks. Support-set and query-set: in meta learning, it commonly assumes that there exist limited labeled data examples from which the model can take rapid adaptation. Such labeled samples are denoted as support-set. The samples which need to be predicted are denoted as query-set. Note that in the meta-training phase, we will get labels for both support-set and query-set. The labeled support-set is used for inner-loop adaptation, while the labeled query-set is used for outer-loop optimization. In the meta-testing phase, we will only get labeled data examples from support-set, and leaving the adapted model evaluated on the unlabeled query-set.\nWith above definitions, the meta recommendation problems can be formulated as the following machine learning problem. It assumes that the recommendation tasks \ud835\udc47 subjects to a fixed task distribution \ud835\udc43 (\ud835\udf0f). The final objective of model \ud835\udc53 \u0398 is to minimize the expectation loss on the prediction results for query-set samples \ud835\udc37 query \ud835\udc47 (\ud835\udc37 \ud835\udc5e \ud835\udc47 ) after conducting adaptation from support-set samples \ud835\udc37 support \ud835\udc47 (\ud835\udc37 \ud835\udc60 \ud835\udc47 ). Thus, the objective function can be written as follows:\nmin\n\u0398 E \ud835\udc47 \u223c\ud835\udc43 (\ud835\udf0f) L \u0398 \ud835\udc37 query \ud835\udc47 | \ud835\udc37 support \ud835\udc47 ,(1)\nwhere we denote all parameters and loss function with \u0398 and L respectively. For meta recommendation problem, the \ud835\udc37 support \ud835\udc47 here represents interaction history for recommended entities and \ud835\udc37 query \ud835\udc47 denotes the user-item pairs the adapted model needs to predict.\nFor our proposed CMML framework, we split the parameters \u0398 into two parts: backbone network's parameters \u03a6 and meta model parameters \u0398 \ud835\udc40 . Practically, we split the tasks of the dataset into meta-training tasks \ud835\udc37 \ud835\udc5a\ud835\udc61\ud835\udc5f and meta-testing tasks \ud835\udc37 \ud835\udc5a\ud835\udc61\ud835\udc52 and try to minimize the empirical risks on \ud835\udc37 \ud835\udc5a\ud835\udc61\ud835\udc5f . In all, the loss function can be represented as Equation ( 2) and (3). min\n\u0398 \ud835\udc40 ,\u03a6 E \ud835\udc47 \u223c\ud835\udc43 (\ud835\udc37 \ud835\udc5a\ud835\udc61\ud835\udc5f ) L \u0398 \ud835\udc40 ,\u03a6 \ud835\udc37 query \ud835\udc47 | \ud835\udc37 support \ud835\udc47 ,(2)\nL \u0398 \ud835\udc40 ,\u03a6 \ud835\udc37 q \ud835\udc47 | \ud835\udc37 s \ud835\udc47 = \u2211\ufe01 (\ud835\udc62 \ud835\udc58 ,\ud835\udc56 \ud835\udc58 ) \u2208\ud835\udc37 q \ud835\udc47 \u2113 \ud835\udc53 \u0398 \ud835\udc40 ,\u03a6 (\ud835\udc62 \ud835\udc58 , \ud835\udc56 \ud835\udc58 ; {\ud835\udc62, \ud835\udc56} \u210e ), \ud835\udc66 \ud835\udc58 \ud835\udc37 q \ud835\udc47 ,(3)\nwhere \ud835\udc53 \u0398 \ud835\udc40 ,\u03a6 represents the whole model, (\ud835\udc62 \ud835\udc58 , \ud835\udc56 \ud835\udc58 ), \ud835\udc66 \ud835\udc58 denotes the \ud835\udc58-th user-item pair and corresponding label respectively. {\ud835\udc62, \ud835\udc56} \u210e is the user-item interaction history in support-set. Note that we neglect the symbol of real labels for {\ud835\udc62, \ud835\udc56} \u210e in support-set in the whole paper for notation simplicity.", "publication_ref": ["b19", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Scenario/User-specific Settings", "text": "In this paper, we choose two cold-start recommendation settings, scenario-specific [8] and user-specific settings [19], to show the effectiveness and generalization ability of our proposed framework. Note that our framework is not limited to these two settings and can also be applied into broader industrial recommendation problems. The scenario-specific recommendation utilizes scenario characteristics to enhance the recommendation performance. For instance, for sports related scenarios, items related with outdoor activities should gain more popularity. In meta learning, our framework treats different scenarios as different tasks. Without explicit information, the algorithm needs to extract implicit scenario characteristics from limited support-set, and conducts scenario-specific adpatation. Following [8], we model the learning objective as a click-throughrate(CTR) prediction problem and utilize hinge loss on query-set as our meta objective function. We denote \ud835\udc56 + and \ud835\udc56 -as positive items and negative items for CTR. Following Equation (3), the loss function can be written as follows:\n\u2113 = max 0, 1 -\ud835\udc53 \u03a6,\u0398 \ud835\udc40 \ud835\udc62, \ud835\udc56 + ; {\ud835\udc62, \ud835\udc56} \u210e + \ud835\udc53 \u03a6,\u0398 \ud835\udc40 (\ud835\udc62, \ud835\udc56 -; {\ud835\udc62, \ud835\udc56} \u210e ) . (4)\nFor user-specific meta recommendation setting, the tasks in meta learning framework corresponding to users \ud835\udc48 in recommendation. The interaction history of {\ud835\udc62, \ud835\udc56} \u210e will be treated as the supportset, which can implicitly reveal the user's profiles and preferences. We consider a score regression problem [19] in this setting. The objective of meta model is to minimize the mean square error on new user-item pairs' prediction scores. Following Equation (3), the loss function can be written as follows:\n\u2113 = (\ud835\udc66 \ud835\udc62,\ud835\udc56 -\ud835\udc53 \u03a6,\u0398 \ud835\udc40 (\ud835\udc62, \ud835\udc56; {\ud835\udc62, \ud835\udc56} \u210e )) 2 ,(5)\nwhere \ud835\udc66 \ud835\udc62,\ud835\udc56 is the real score for user-item pair (\ud835\udc62, \ud835\udc56).", "publication_ref": ["b8", "b19", "b8", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "CMML", "text": "In this part, we illustrate the backbone network and detailed structure for our proposed CMML framework. In all, the framework consists of three components: a context encoder that can aggregate the interaction history in the support-set for implicit contextual information extraction and effective task representation, a hybrid context generator that aggregates task-level information and instance-level user-item features, and modulation network, which modulates the backbone network for fast adaptation on new scenarios/users. The detailed procedure is summarized as follows. First, the context encoder will map interaction history for task-level contextual information \ud835\udc36. Second, the embedded context will be combined with specific user-item feature embedding, generating hybrid context. Conditioned on hybrid context, the backbone network will be modulated for new scenario/user adaptation. The detailed algorithm chart is shown in Algorithm 1. We also provide Figure 1 for better illustration of the overall framework and specific modules.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Backbone Network Structure", "text": "The recommender backbone network maps user-item features to prediction results, and will be modulated by the modulation modules introduced in Section 3.4. We adopt a common feed-forward neural network as the backbone network structure for simplicity. \n\u0398 \ud835\udc5a\ud835\udc52\ud835\udc61\ud835\udc4e \u2190 \u0398 \ud835\udc40 -\ud835\udefc\u2207 \u0398 \ud835\udc40 L \ud835\udc5a\ud835\udc52\ud835\udc61\ud835\udc4e 12:\n\u03a6 \u2190 \u03a6 -\ud835\udefc\u2207 \u03a6 L \ud835\udc5a\ud835\udc52\ud835\udc61\ud835\udc4e 13: end for But note that our framework can also be plugged into many sophisticated backbone networks. The whole backbone network consists of three components: embedding layers, hidden layers, and output layer. All parameters mentioned here belong to \u03a6.\nThe embedding layer consists of user embedding matrices \ud835\udc40 \ud835\udc62 and item embedding matrices \ud835\udc40 \ud835\udc56 . The high-dimensional user features/ids \ud835\udc65 \ud835\udc62 and item features/ids \ud835\udc65 \ud835\udc56 can be transformed into low dimensional representations \ud835\udc52 \ud835\udc62 , \ud835\udc52 \ud835\udc56 by \ud835\udc52 \ud835\udc62 = \ud835\udc40 \ud835\udc62 \ud835\udc65 \ud835\udc62 and \ud835\udc52 \ud835\udc56 = \ud835\udc40 \ud835\udc56 \ud835\udc65 \ud835\udc56 . Here \ud835\udc40 \ud835\udc62 , \ud835\udc40 \ud835\udc56 can be learned through the end to end training process [19] or be pre-generated [8] by collaborative filtering [17]. The hidden layers are ReLU activated MLP which map the concatenated user-item feature embedding (\ud835\udc52 \ud835\udc62 , \ud835\udc52 \ud835\udc56 ) to the continuous representation \u210e \ud835\udc62\ud835\udc56 . The final output is a linear layer \ud835\udc53 \ud835\udc5c , which maps the hidden layer's output to final score \ud835\udc5c \ud835\udc62\ud835\udc56 for specific user-item pair. The transformation corresponds to \ud835\udc5c \ud835\udc62\ud835\udc56 = \ud835\udc64 \ud835\udc47 \u210e \ud835\udc62\ud835\udc56 + \ud835\udc4f, where \ud835\udc64, \ud835\udc4f are weights and bias for the last layer respectively.", "publication_ref": ["b19", "b8", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Context Encoder", "text": "Context encoder can be regarded as the embedding network that maps interaction history of user-item pairs to implicit contextual information: {U, I} \u210e \u2192 C. Here we offer two alternatives: pooling aggregated encoder (PE) and sequential aggregated encoder (SE).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Pooling aggregated encoder.", "text": "For pooling aggregated encoder, it maps \ud835\udc5b user-item pairs {\ud835\udc62 \ud835\udc58 , \ud835\udc56 \ud835\udc58 } \ud835\udc5b \ud835\udc58=1 \u2208 \ud835\udc37 \ud835\udc60 \ud835\udc47 to user/scenario context. Each user-item pair (\ud835\udc62, \ud835\udc56) will be firstly mapped to the corresponding embedding \ud835\udc52 \ud835\udc62 , \ud835\udc52 \ud835\udc56 by embedding layers of the backbone network. Then the embedding is fed into ReLU activated MLP \ud835\udc53 \ud835\udf03 \ud835\udc52 , which is shared among all user-item pairs. At last, mean/max pooling operation will be conducted to aggregate user-item pairs to single user/scenario context \ud835\udc36. The above process can be formulated as Equation (6): Pooling aggregated encoder is a fairly simple model for implementation and it has the property of permutational invariance, which means the output of neural network will not be affected for different permutations of inputs. Since we usually get no access to explicit sequential data in our setting, the permutation for user-item pairs contains no useful information for identifying task-specific features. The permutational invariance property enables the model to neglect the sequential order for items in support-set.\n\ud835\udc36 \ud835\udc58 = \ud835\udc53 \ud835\udf03 \ud835\udc52 ({\ud835\udc40 \ud835\udc62 \ud835\udc65 \ud835\udc62 , \ud835\udc40 \ud835\udc56 \ud835\udc65 \ud835\udc56 } \ud835\udc58 ), \ud835\udc36 = Mean/Max Pooling({\ud835\udc36 \ud835\udc58 } \ud835\udc5b ),(6)", "publication_ref": ["b6"], "figure_ref": [], "table_ref": []}, {"heading": "Sequential aggregated encoder.", "text": "To further enhance the information aggregation ability of the context encoder, we also provide sequential aggregated encoder (SE). We treat all user-item pairs {\ud835\udc62 \ud835\udc58 , \ud835\udc56 \ud835\udc58 } \ud835\udc5b \ud835\udc58=1 \u2208 \ud835\udc37 \ud835\udc47 \ud835\udc60 as a sequence of information and leverage sequential model like Gated Recurrent Unit(GRU) [6] to handle the representation of the support-set. The user-item pairs are firstly fed into GRU network sequentially, and the embedded sequence representation from it will be mapped into the global context by ReLU activated MLP. The process can be formulated as Equation (7):\n{\u210e \ud835\udc58 } \ud835\udc5b \ud835\udc58=1 , {\ud835\udc5c \ud835\udc58 } \ud835\udc5b \ud835\udc58=1 = GRU \ud835\udf03 1 \ud835\udc52 ({\ud835\udc40 \ud835\udc62 \ud835\udc65 \ud835\udc58 \ud835\udc62 , \ud835\udc40 \ud835\udc56 \ud835\udc65 \ud835\udc58 \ud835\udc56 } \ud835\udc5b \ud835\udc58=1 ), \ud835\udc36 = \ud835\udc53 \ud835\udf03 2 \ud835\udc52 (\ud835\udc5c \ud835\udc5b ),(7)\nwhere GRU \ud835\udf03 It deserves to be pointed that since data examples in support-set usually contain no sequential information, we choose sequential model because of its better context aggregation ability compared with mean-pooling operation in PE rather than its sequential property. Note that though in principle, the sequential aggregated encoder does not hold the property of permutational invariance, we can still randomly permutate the support-set at each iteration in practice so the sequential aggregated encoder can learn to be agnostic to the order information. This is also the technique utilized in [12].", "publication_ref": ["b6", "b7", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Hybrid Context Generator", "text": "Note that the context encoder in previous section only extracts user/scenario information for task-level context from support-set \ud835\udc37 support \ud835\udc47 . However, the task-level context will be the same for different user-item pairs in the query-set. The interaction between task-level context and specific instance-level user-item feature is neglected. Thus, in this section, we provide the module of hybrid context generator to combine both information effectively.\nThere are many classical works in recommender system's literature like [10,33] about how to conduct low-order and high-order feature interactions. Usually, low-order interaction is obtained by taking low-order computation like dot-product among features while high-order interaction is obtained by deep neural network. We borrow the idea of this to formulate the hybrid context generator. By taking dot-product or MLP to model the relationship of these features, we provide two ways to combine information in Equation (8).\n\ud835\udc36 \u210e \ud835\udc62 \ud835\udc58 ,\ud835\udc56 \ud835\udc58 = \ud835\udc36 \u2299 {\ud835\udc40 \ud835\udc62 \ud835\udc65 \ud835\udc62 \ud835\udc58 , \ud835\udc40 \ud835\udc56 \ud835\udc65 \ud835\udc56 \ud835\udc58 }, \ud835\udc36 \u210e \ud835\udc62 \ud835\udc58 ,\ud835\udc56 \ud835\udc58 = MLP \ud835\udf03 \ud835\udc51 (\ud835\udc36, {\ud835\udc40 \ud835\udc62 \ud835\udc65 \ud835\udc62 \ud835\udc58 , \ud835\udc40 \ud835\udc56 \ud835\udc65 \ud835\udc56 \ud835\udc58 }),(8)\nwhere \ud835\udc36 \u210e \ud835\udc62 \ud835\udc58 ,\ud835\udc56 \ud835\udc58 denotes the specific hybrid context for specific useritem pair {\ud835\udc62 \ud835\udc58 , \ud835\udc56 \ud835\udc58 }, \ud835\udf03 \ud835\udc51 \u2208 \u0398 \ud835\udc40 represents the meta parameters of hybrid context generator.", "publication_ref": ["b10", "b33", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Context based Modulation Network", "text": "After obtaining hybrid context, the next step for our framework is to effectively adapt for task-specific model. Specifically, we propose context based modulation network which modulates the backbone network \u03a6 for fast adaptation. In this section, we provide three ways to conduct network modulation: weight modulation, layer modulation and soft modularization. And we will use \ud835\udc36 \u210e to denote the specific hybrid context \ud835\udc36 \u210e \ud835\udc62 \ud835\udc58 ,\ud835\udc56 \ud835\udc58 in Equation ( 8) for simplicity in the following.\n3.4.1 Weight Modulation. The directest way of modulating network is to generate weights by hyper-network [11] for the backbone network based on hybrid context. However, the backbone network usually has thousands of parameters, making it difficult to generate such high-dimension output. Even we can reduce the parameters dimension, it is still unstable for training if all the parameters are generated via hyper-network. Thus, we choose to only generate weights and bias by hyper-network \ud835\udc53 \ud835\udf03 \u210e for the final linear layer. In fact, Vartak et al. [31] also try to generate weights for a linear model. But they only conduct modulation on a single linear model rather than the last layer of MLP, which limits the representation ability of the modulated network. The detailed equation is shown as follows.\n\ud835\udc64 \u210e , \ud835\udc4f \u210e = \ud835\udc53 \ud835\udf03 \u210e (\ud835\udc36 \u210e ),\n\ud835\udc5c \ud835\udc62\ud835\udc56 = \ud835\udc64 \ud835\udc47 \u210e \u210e \ud835\udc62\ud835\udc56 + \ud835\udc4f \u210e ,(9)\nwhere we denote the hybrid context as \ud835\udc36 \u210e , hyper-network's parameter as \ud835\udf03 \u210e \u2208 \u0398 \ud835\udc40 , the generated modulation parameters as \ud835\udc64 \u210e , \ud835\udc4f \u210e , the output of final hidden layer as \u210e \ud835\udc62\ud835\udc56 and the final output as \ud835\udc5c \ud835\udc62\ud835\udc56 .\nThe strength for weight modulation is its simplicity, which makes it quite easy to implement and suitable for some simple tasks. When tasks are quite different, this modulation may not have enough representation ability and capacity for rapid adaptation. We offer the second way, layer modularion, to solve the problem.", "publication_ref": ["b11", "b31"], "figure_ref": [], "table_ref": []}, {"heading": "Layer Modulation.", "text": "As mentioned above, it is hard to directly generate weights for all hidden layer's parameters because of its high dimensionality. Therefore, we adopt layer modulation rather than layer weights' modulation, where we modulate layers ouput by hyper-network. It can effectively lower the dimensionality of parameter space from \ud835\udc5b 2 to \ud835\udc5b, where \ud835\udc5b refers to the amount of nodes in one layer. Here we show two ways of layer modulation. The first one is that hyper-network generates weights activated by Sigmoid function and the modulated output of each layer is the dot-product of the original output and the generated weights. The second way follows [23](FiLM) by utilizing feature-wise Linear Modulation on backbone network. The hyper-network generates weights and bias for linear modulation on layers' output. For layer \ud835\udc56's output \ud835\udc59 \ud835\udc56 , the sigmoid-dot modulation and FiLM modulation can be written by Equations ( 10) and ( 11) respectively.\n\ud835\udc64 \u210e = Sigmoid(\ud835\udc53 \ud835\udf03 \u210e (\ud835\udc36 \u210e )), \ud835\udc5c \ud835\udc56 = \ud835\udc64 \u210e \u2299 \ud835\udc59 \ud835\udc56 ,(10)\n\ud835\udc64 \u210e , \ud835\udc4f \u210e = \ud835\udc53 \ud835\udf03 \u210e (\ud835\udc36 \u210e ), \ud835\udc5c \ud835\udc56 = \ud835\udc64 \u210e \u2299 \ud835\udc59 \ud835\udc56 + \ud835\udc4f \u210e ,(11)\nwhere we denote the hyper-network's parameter as \ud835\udf03 \u210e \u2208 \u0398 \ud835\udc40 , the generated modulation parameters as \ud835\udc64 \u210e , \ud835\udc4f \u210e and the modulated output of layer \ud835\udc56 as \ud835\udc5c \ud835\udc56 .\nLayer modulation increases its representation ability for task adaptation compared with the weight modulation. The weakness is that the modulation weights are still high dimensional and even though we can interpret it by visualizing the modulation weights' clusterings, it is still hard to interpret how the model works for specific tasks. That is why we provide the third modulation way called soft modularization.", "publication_ref": ["b23"], "figure_ref": [], "table_ref": []}, {"heading": "Soft Modularization.", "text": "In this section we introduce soft modularization method. The model modulation is conducted by controlling mixture of experts network. Some works like [21] discuss how mixture of experts with shared bottom layer can be used for handling multi-task recommendation problems. The multi-task setting has some similarity with meta learning setting since both need to adapt model when facing one specific task. For modulating the network in our setting, we adopt the similar way of soft modularization from [37] to generate route weights for sub-networks. A main difference is that our modulation is conditioned on continuous hybrid context which can easily generalize to completely new task while the task indicator used in [37] can only handle multitask problem. The soft modularization consists of two parts: base network and route network. In base network, there exists \ud835\udc58 layers and each layer has several sub-networks (modules). For instance, there exists four 32 \u00d7 32 fully connected sub-networks at each layer, which has equivalent amount of parameters with a 64 \u00d7 64 fully connected network. Conditioned on the hybrid context, the route network will generate dynamic routing for base network, which consists of \ud835\udc58 probability distributions. Each distribution is used to aggregate the output of sub-networ k.\nThe detailed procedures are as follows: when a hybrid context is fed into the route network, it will be used to generate a probability distribution of module's route weights by Softmax activation function for each base network layer. For a \ud835\udc58-layer base network with \ud835\udc5a modules each layer, the route network with the same depth \ud835\udc58 will generate \ud835\udc58 R \ud835\udc5a\u00d7\ud835\udc5a matrices for all layers. In each matrix, the \ud835\udc56-th (\ud835\udc56 \u2208 1, 2...\ud835\udc5a) column vector sums up to 1 and corresponds to the weights between \ud835\udc56-th module at layer \ud835\udc59 to all \ud835\udc5a modules at layer \ud835\udc59 + 1. We denote the route vector logits of layer \ud835\udc59 as \ud835\udf0e \ud835\udc59 \u2208 R \ud835\udc5a\u00d7\ud835\udc5a , the probability scalar from \ud835\udc56-th module in layer \ud835\udc59 to module j in layer \ud835\udc59 + 1 as \ud835\udc5d \ud835\udc59 \ud835\udc56,\ud835\udc57 \u2208 R, the output of \ud835\udc59-th hidden layer as \u210e \ud835\udc59 \ud835\udc5f , the meta model parameters as \ud835\udf03 \u210e \u2208 \u0398 \ud835\udc40 , the modulated output of \ud835\udc56-th modules at layer \ud835\udc59 as \ud835\udc5c\ud835\udc4f \ud835\udc59 \ud835\udc56 . Equations ( 12) and ( 13) depict how the route function and aggregated base network work in layer \ud835\udc59.\n\ud835\udf0e \ud835\udc59 = MLP \ud835\udf03 \u210e (\u210e \ud835\udc59-1 \ud835\udc5f ), \u210e 0 \ud835\udc5f = \ud835\udc36 \u210e , \ud835\udc5d \ud835\udc59 \ud835\udc56,\ud835\udc57 = exp(\ud835\udf0e \ud835\udc59 \ud835\udc56,\ud835\udc57 ) \ud835\udc5a-1 \ud835\udc57=0 exp(\ud835\udf0e \ud835\udc59 \ud835\udc56,\ud835\udc57 ) , (12\n)\n\ud835\udc5c\ud835\udc4f \ud835\udc59 \ud835\udc56 = \ud835\udc61 \u2211\ufe01 \ud835\udc57=0 \ud835\udc5d \ud835\udc59-1 \ud835\udc56,\ud835\udc57 MLP \u03a6 (\ud835\udc5c\ud835\udc4f \ud835\udc59-1 \ud835\udc57 ),(13)\nSoft modularization can somehow be treated as shared layer modulation, since the route weight is shared among all nodes in one module. The representation ability of soft modularization will be discounted compared with layer modulation. However, the route weights generated by route network are now low-dimensional and we can easily get access to the weight distribution to know how ", "publication_ref": ["b21", "b37", "b37"], "figure_ref": [], "table_ref": []}, {"heading": "Computational Efficiency Analysis", "text": "Though the gradient based meta learning approach brings a notably performance improvement over previous traditional recommendation algorithms, it imposes considerable computational and memory burdens due to the the inner-loop gradient operator. In order to show the strengths of CMML framework with respect to the computational efficiency, we briefly analyze the time and space complexity for MAML-like algorithms and CMML in training and inference phase. Note that the analysis here only considers the computational complexity for basic MAML based algorithms like [19]. [8] and [7] will have larger computational complexity. In the following analysis, we denote \ud835\udc53 \ud835\udc61 , \ud835\udc53 \ud835\udc60 , \ud835\udc4f \ud835\udc61 , \ud835\udc4f \ud835\udc60 as the time and space complexity for forward process and backward process respectively. Assume we have \ud835\udc5a data points to be fed into the neural network and we take k gradient steps in the inner-loop.\nIn the inference phase, the MAML-like algorithms need to perform feed-forward operation and backward operation for m data points each step, so the final time and space complexity for MAMLlike algorithms is \ud835\udc42 (\ud835\udc5a\ud835\udc58 (\ud835\udc53 \ud835\udc61 + \ud835\udc4f \ud835\udc61 )) and \ud835\udc42 (\ud835\udc5a\ud835\udc58 (\ud835\udc53 \ud835\udc60 + \ud835\udc4f \ud835\udc60 )). For CMML, it only needs to take feed-forward operation, resulting in the final time and space complexity: \ud835\udc42 (\ud835\udc5a\ud835\udc53 \ud835\udc61 ), \ud835\udc42 (\ud835\udc5a\ud835\udc53 \ud835\udc60 ). Note that in most cases, \ud835\udc4f \ud835\udc61 > \ud835\udc53 \ud835\udc61 and \ud835\udc4f \ud835\udc60 > \ud835\udc53 \ud835\udc60 holds. So the time and space complexity of MAML-like algorithms is 2k times larger than that of CMML in the inference phase.\nIn the training phase, the efficiency strength of CMML can be more clear since the gradient based method needs to differentiate through the whole gradient paths in the inner-loop, which requires the computation of the hessian-vector product. In this part, we neglect the complexity analysis for forward process. [24] and [27] show that in the reverse mode of gradient calculation, the time and space complexity of hessian-vector product are typically no more than a constant over that of first-order gradient calculation. Usually, the constant is 5 and 2 for time and space complexity respectively. Thus, for MAML-like algorithms, the time and space complexity is \ud835\udc42 (5\ud835\udc58\ud835\udc5a\ud835\udc4f \ud835\udc61 ) and \ud835\udc42 (2\ud835\udc58\ud835\udc5a\ud835\udc4f \ud835\udc60 ), while for CMML, the time and space complexity is \ud835\udc42 (\ud835\udc5a\ud835\udc4f \ud835\udc61 ) and \ud835\udc42 (\ud835\udc5a\ud835\udc4f \ud835\udc60 ).\nThe analysis is conducted given fixed gradient steps k. Combining our analysis and the Prop 3.1 of [27], we can also show that when we are trying to get a \ud835\udeff -accurate optimal solution in the inner-loop (which means the distance between optimized parameters and optimal parameters of the inner-loop problem is bounded by \ud835\udeff), the time and space complexity is \ud835\udc42 (5\ud835\udc5a\ud835\udc4f \ud835\udc61 \ud835\udf05 log \ud835\udc37 \ud835\udeff ) and \ud835\udc42 (2\ud835\udc5a\ud835\udc4f \ud835\udc60 \ud835\udf05 log \ud835\udc37 \ud835\udeff ), where \ud835\udf05 is the condition number for innerloop optimization and \ud835\udc37 is the diameter of parameters' search space. We summarize the analysis results in Table 1.", "publication_ref": ["b19", "b8", "b7", "b24", "b27", "b27"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "EXPERIMENT", "text": "In this section, we detail the experimental settings and results to validate the effectiveness of CMML. This section includes the experimental settings like dataset configuration, evaluation metrics and baseline algorithms. The performance comparison section shows that CMML can achieve both outstanding performance and high computational efficiency. Visualization of the learned embedding and route is shown for demonstrating the interpretability of CMML. And finally, the ablation study is conducted to analyze how specific structure influences the performance of CMML.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Settings", "text": "4.1.1 Dataset. For scenario-specific recommendation setting, we evaluate our algorithm on two public datasets from MovieLens-20M [13] and Taobao [8]. MovieLens-20M is a large movie-rating dataset from movie recommendation service MovieLens, and it consists of 138,493 users, 27,279 movies, and 20,000,263 rating records. Following similar CTR problem formulation of [8], we transform the MovieLens-20M dataset from explicit rating into implicit feedback. All movies rated by the user will be regarded as positive samples with the rest as negative samples. 306 different genres of movie are treated as scenarios for scenario-specific setting. The second Taobao datasetfoot_0 is from the cloud theme click log on Taobao recommender system. This dataset consists of users' purchase history in 355 different themes such as \"things to prepare when a baby is coming\", which are treated as different scenarios. There are 700k users, 1400k items, and 5717k purchase history in Taobao. We also build a synthetic dataset called hybrid Movie-Taobao which concatenates scenarios in both datasets to test our algorithms on multi-domain cold-start problems. The meta-training and meta-testing dataset of all three datasets are split based on different scenarios.\nFor user-specific recommendation setting, we use the MovieLens-1M [13] dataset following [19]. MovieLens-1M is an older and smaller version of MovieLens dataset compared with MovieLens-20M. It consists of 6,040 users, 3,706 movies, and 1,000k rating records. Different from the CTR problem in scenario-specific setting, we need to solve the cold-start regression problem for user-item score prediction. The meta-training and meta-testing dataset of it are split based on different users, denoted as \ud835\udc4a -\ud835\udc4a and \ud835\udc36 -\ud835\udc4a (which is short for two settings of training on warm users -testing on warm users and training on cold users -testing on warm users respectively). Note that the split method is different from that in [19] because cold users are defined as earlier users than warm users and meta learning problem is motivated to the goal of generalizing towards unseen or new tasks (users).\nHere we also detail the data preprocessing procedures. In scenariospecific setting, we only select scenarios with less than 1000 but more than 100 items in each dataset to make sure it can guarantee the cold-start property. There exists 232 scenarios for meta-training set with the rest as meta-testing set. In the training process, we will randomly sample 64 user-item positive pairs as support-set and 128 user-item pairs as query-set from each scenario. Note that since the amount of negative samples is much larger than that of positive samples, we will also sample the same amount of negative pairs in support and query-set. For user-specific setting, we trim the dataset and only choose 50 rating records for each user to match the cold-start setting. There exists 4832 users in meta-trainng set with the rest as meta-testing set. For each user, we randomly sample 10 items for query-set with the rest as support-set.", "publication_ref": ["b13", "b8", "b8", "b13", "b19", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation Metrics.", "text": "For scenario-specific setting, since it is modeled as a CTR prediction problem in [8], we adopt the same metric and utilize top-N recall for performance evaluation. And for user-specific setting, which is modeled as a score regression problem in [19], we utilize both mean absolute error (MAE) and normalized discounted cumulative gain (NDCG [15]) for evaluation as used in [19], and the higher the better in NDCG, the lower the better in MAE. The experimental results also reveal that our method generally works for different loss functions.", "publication_ref": ["b8", "b19", "b15", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Baseline algorithms.", "text": "We select many state-of-the-art baselines of deep learning or meta learning based recommendation algorithms for comparison. For scenario-specific setting,\n\u2022 Deep Cross Network(DCN): Deep cross network [33] is one of the most classical item recommendation algorithms and it has been widely applied in industrial recommendation system. It proposes cross layer which successfully combines benefits of FM and DNN for item recommendation. We train our DCN model on all scenarios in the meta-training set. \u2022 Deep cross network with fine-tuning(DCN-F). We also provide a baseline which finetunes the pretrained DCN model on specific scenario based on data in the support-set. \u2022 ItemPop: ItemPop is a heuristic algorithms used in [26] which ranks items according to its popularity, measured by the amount of corresponding interactions in the support-set. \u2022 CoNet: CoNet is a cross-domain recommendation algorithms for tackling cold-start problem. It utilizes cross connection to enable dual knowledge from source domain to target domain. We reimplement the algorithm on MovieLens-20M by regarding movies without labeled genre as the source domain. For Taobao datast and hybrid dataset, the authors did not offer the source domain data used in [8]. Thus here we can only show the reported result in [8] in Taobao dataset. \u2022 \ud835\udc60 2 Meta [8]: The state-of-the-art meta learning approach for scenario-specific cold-start problem. It proposes update and stop controllers for better inner-loop optimization.\nFor user-specific setting,\n\u2022 DCN and DCN-F: Same with previous baselines and we modify the model into a regression model. \u2022 MetaCS-DNN [4]: A meta-learning baseline that tries to tackle the cold-start user recommendation based on MAMLlike algorithms. For a specific user, the whole model will take a few gradient steps for adaptation. \u2022 MELU [19]: MELU basically shares similar idea with MetaCS-DNN by utilizing MAML-like algorithm. The only difference is the inner-loop optimization will only happen in fully connected layers rather than the whole network.", "publication_ref": ["b33", "b26", "b8", "b8", "b8", "b4", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Features and Hyper-parameters.", "text": "For MovieLens-1M dataset, we select the same features as used in [19], and for MovieLens-20M dataset and Taobao dataset, we seclect the same features as adopted in [8]. The network architectures involved are designed to be comparable with the baselines for fairness. The base network is constructed as a ReLU activated MLP with hidden units (64, 64, 64), the context encoder includes a GRU and a ReLU activated MLP with hidden units (128, 128). For CMML-FiLM, we also utilize a ReLU activated MLP with hidden units (64, 64, 64) for layer modularization. For CMML-Soft-M, it contains \ud835\udc58 = 3 module layers, \ud835\udc5a = 4 modules per layer and each module outputs a \ud835\udc51 = 32 representation.\nThe user embeddings and item embeddings are pre-generated [8] by collaborative filtering [17]. We optimize our model with Adam [16] optimizer with learning rate 1e-4. These hyper-parameters are configured by grid search. Clarification: During our preparation for code, we find out in the scenario-based experiments, the random seeds have some influences on our final experimental results. So we reconduct experiments in the scenario-based setting for 5 seeds and report the results in the appendix.", "publication_ref": ["b19", "b8", "b8", "b17", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Performance Comparison", "text": "In this part we summarize the performance comparison for CMML (based on FiLM layer modulation and soft modularization) and other baseline algorithms in scenario/user-specific setting, respectively.\nFor scenario-specific setting, Table 2 shows that CMML generally achieves comparable or better results among all three datasets in terms of top-N recall. For Taobao dataset, we notice that DCN-F and other scenario-specific algorithms except \ud835\udc60 2 Meta perform worse than DCN. The counterintuitive phenomenon reveals that scenario-specific support-set in Taobao may contain noisy and irrelevant information, which might be detrimental since adaptation may easily cause over-fitting on limited noisy support-set samples. Under this condition, CMML still achieves improvement comparable to the best baseline \ud835\udc60 2 Meta, which validataes the effectiveness of CMML. The results also reveal that CMML can work for different scales of datasets. Hybrid MovieLens-Taobao evaluates the algorithms' performance on datasets composed of multi-modal data. CMML achieves much improvement over the best baseline \ud835\udc60 2 Meta. The reason might be that gradient based algorithms can only lean one group of initial parameters, which can work pretty well dealing with single-domain dataset, yet not suitable for multi-domain dataset. CMML works better here due to its flexible representation and modulation ability for multi-domain datasets.\nFor user-specific setting, we basically observe similar results. The fine-tuning of DCN-F helps for identifying users preferences and increasing the performance. Three meta learning based algorithms achieve comparable results and are better than DCN baselines.\nWe remark that CMML does not achieve remarkably better results on single modal datasets compared with gradient based method. That is because the gradient descent is a strong baseline for adaptation and can guarantee performance improvement even without any meta-training. We leave further investigation of combining CMML with gradient based methods for future work.  We also provide the comparison results of time and memory efficiency of \ud835\udc60 2 Meta, MELU and CMML in Figure 2. We use sequential encoder, dot hybrid context generator and FiLM based modulation network for CMML, and the inner gradient steps of\n\ud835\udc46 2\nMeta and MELU are both set with 1, 5, 10, 20. It can be shown that the time and memory consumption increases with more gradient steps. For scenario specific setting, our algorithm achieves remarkable computational efficiency compared with \ud835\udc46 2 Meta. It consumes less memory and time compared with even one-step \ud835\udc46 2 Meta while it usually conducts 20-step gradient descent in the implementation. For user-specific setting, our algorithm achieves comparable time and memory demands with one-step MELU, while MELU needs ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Embedding and Route Visualization", "text": "In this section, we provide the visualization of learned context embedding and the activated route of soft modularization to show the interpretability of our proposed CMML in Figure 3 and 4 respectively. In Figure 3, we show the visualization for task-level context on MovieLens-20M dataset and hybrid dataset. For the clusterings on MovieLens, we sample the support-set for 8 different genres(tasks) and the corresponding context embedding can be successfully clustered. It demonstrates that the context encoder is capable of distinguishing movies from different genres. In addition, we investigate the clustering of semantic groups by classifying these 8 genres into 4 broader types: horror, suspense, romance and war. We can find that the tasks with semantically similar genres (shown with the same color) are closer, revealing that context encoder can learn some implicit semantic information from the dataset. The second figure shows the context embedding clustering within the hybrid dataset. All task context embeddings are divided into two clusters which exactly correspond to two datasets -MovieLens20M and Taobao. In Figure 4, we display the activated route visualization of two different tasks for soft modularization, where darker color represents higher probability. Due to the low-dimension property of soft modulairzation, we can easily know what modules are activated for a specific task.\nThe task-level context and route visualization can successfully validate the appealing property of better interpretability of CMML. ", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Ablation Study", "text": "In this section, we conduct ablation study to illustrate the effectiveness of the context encoder, hybrid context generator, and context modulation network.\nIn Table 9, we show the ablation results for different context encoder and hybrid context generator on Moviesen20M dataset. All results here are based on final weight modulation. Firstly, in order to validate the importance of useful interaction history, we conduct one setting called 'Non-negative-PE', which means we remove the negative user-item pairs in the support-set and utilize pooling aggregated encoder. The comparison between 'Non-negative-PE' and 'PE' shows without useful interaction history, it will be hard to generate scenario-specific context and further modulate the network. The comparison between pooling aggregated encoder(PE) and sequential aggregated encoder(SE) shows context extraction requires high representation ability for context encoder. The comparison between SE and SE plus hybrid MLP generator shows the importance of building hybrid context for specific instance example. And between hybrid MLP generator and hybrid dot generator, the latter one is not only free of additional parameters, but also achieves better performance. Low-order interaction is enough to build the relationship between task-level context and instance's features. Thus, we set sequential aggregated encoder plus hybrid dot generator as the default setting for CMML.\nTable 5 shows the ablation study on different approaches for modulating the model on MovieLens-20M dataset and Taobao dataset. The method linear correpsonds linear weight modulation. S-LM and F-LM denote Sigmoid based and FiLM based layer modulation respectively. And Soft-M represents the soft modulation approach. In MovieLens-20M dataset, all modulation ways except S-LM achieves comparable results. In Taobao dataset, F-LM and Soft-M achieve better results compared with rest two approaches. The complexity of dataset will influence the need for modulation network's capacity. For relatively simple Movielen dataset, the ability of linear weight modulation is enough to handle the model adaptation, while in Taobao dataset, stronger network capacity for the model adaptation is required, and that' why F-LM and Soft-M achieve better results.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_9", "tab_5"]}, {"heading": "RELATED WORK 5.1 Contextual Recommendation", "text": "There are many works [1-3, 29, 30, 36] showing that by leveraging extra explicit contextual information like time, location, or users' profile, the recommendation algorithms can improve performance. Compared with traditional algorithms that map user-item pairs into some scores, context aware recommendation framework will take additional context information as input. The difference between our framework and context-aware recommendation is that we do not have access to any explicit contextual information. We believe the historical interaction information itself is already a great source of contextual information which can reveal characteristic of the recommendation entities. Our solution is also somewhat conceptually related with the domain-adaptation problem, where they focus on domain transfer from source domain to target domain, and invariant contextual invariant information is extracted to achieve domain transfer [18], [20]. CMML focuses more on the few shot cold-start recommendation problem.", "publication_ref": ["b18", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "Meta Learning for Recommendation", "text": "Meta Learning, also known as learning to learn, is a new rising machine learning paradigm for learning meta knowledge. Specifically, the meta learning framework aims at learning useful inductive bias, which is helpful when the available data is limited. There are many applications of meta learning, like [32] for one-shot classification in computer vision, and [25] for few-shot meta reinforcement learning problem. One of the most classical algorithms is MAML [9], which aims at learning model's initial parameters which are capable of adapting to new tasks with only a few gradient steps.\nThere exists work [31] that incorporates the meta learning into recommendation problem. Many recent works introduce the framework of MAML into the cold-start recommendation problems. MELU [19] introduces the MAML framework into user-specific cold-start recommendation problems, in which it transforms the cold start recommendation problem for new coming users/items as new coming tasks in the setting of MAML. \ud835\udc46 2 Meta [8] tries to tackle the coldstart problem of scenario setting by learning proper initial MLP parameters, better gradient controller, and an RL stop-controller for better inner-loop adaptation. MAMO [7] introduces task-specific memories and feature-specific memories to get rid of problems brought by global parameter sharing in MELU. [35] equips MAML with dynamic subgraph sampling, which can solve the problem for dynamic arrival of new users. [38] models the training process of recommender system as a meta learning problem and propose a framework for meta learning model retraining mechanism. [39] generates sequential recommendation results by similarity metric between query-set and support-set.", "publication_ref": ["b32", "b25", "b9", "b31", "b19", "b8", "b7", "b35", "b38", "b39"], "figure_ref": [], "table_ref": []}, {"heading": "CONCLUSION", "text": "In this paper, we propose a contextual modulation meta learning framework for solving cold-start recommendation problems with several algorithmic alternatives. We formulate the cold-start recommendation problem as a meta learning problem. By context encoder, hybrid context generator and modulation network, our framework can easily adapt to new tasks even with limited interaction examples. Extensive experiments on real-world datasets successfully validate that effectiveness of CMML with much higher computational efficiency and interpretability. Also, the whole framework is completely compatible with the current practical industrial framework for broader application.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "APPENDIX 7.1 Multi-seed results", "text": "For reproducibility and more reliable results, we conduct experiment on 5 seeds for the scenario-based setting and report the average and the confidence interval (calculated by std * 1.96/ \u221a 5). Firstly, from the new multi-seed results in Table 6, 7, 8, we can find out that one of CMML variant CMML-FiLM is sensitive to different random seeds so we can observe performance decrease on both MovieLens-20M and Taobao dataset, while CMML-Soft-M achieves really stable results and is basically consistently with the results presented in the main paper. The performance of the strongest baseline \ud835\udc60 2 Meta is stable on MovieLens-20M but decreases a lot on Taobao. The main conclusion stays the same that we can achieve comparable results on MovieLens-20M dataset and better performance on Taobao (Especially on Recall@50 and Recall 100) and hybrid dataset (in all metrics).\nIn addition, we also conduct full multi-seed ablation experiments on two variants: CMML-Sigmoid and CMML-Linear. We observe similar results with the main paper on CMML-Linear. It can perform well on MovieLens-20M but not on Taobao dataset. That validates our conclusion that Taobao dataset requires stronger network capacity. The CMML-sigmoid results are different from the main paper, we find out it is more stable than CMML-FiLM variant for multi-seed experients and can achieve comparable results with CMML-Soft-M.\nWe believe the main reason that CMML-FiLM is sensitive to different seeds is that its modulation is the most general and powerful one without any constraints. In contrast, CMML-Sigmoid has constriants on modulation weights by Sigmoid activation function and CMML-Soft-M modulates the network with shared weight modulation. During the training, the modulation way of CMML-FiLM might be sensitive to different network initialization and training data brought by different random seeds. We also conduct multi-seed experiments on context encoder and hybrid context generator ablation study. Basically the conclusion stays the same with the main paper.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_6"]}, {"heading": "Implementation Details", "text": "For better reproducibility, in this subsection we detail how we implement our method and baseline algorithms. You can email xidong.feng.20@ucl.ac.uk if you need codebase for this paper. 7.2.1 scenario-specific setting. For scenario-specific setting, we follow [8] hyper-parameter setting for ItemPoP and CoNet baseline. For DCN baseline we use 3-layer cross network and deep network with [64,64] hidden size and ReLU activation function. We use Adam optimizer with lr=4e-7, weight decay=1e-5 and batch size=128. For DCN-F baseline we conduct hyper-parameter search over fine-tuning learning rate and number of step and use (lr=0.001, step=10) for MovieLens-20M, (lr=0.001, step=5) for Taobao and (lr=0.001, step=10) for MovieLens-Taobao hybrid dataset.\nFor the strongest baseline \ud835\udc60 2 \ud835\udc40\ud835\udc52\ud835\udc61\ud835\udc4e, we basically adopt the hyperparameter used in their released codebase 2 . To basically remove the influence brought by the backbone network, we choose the following backbone network. MovieLens20M: interaction model with [64, 32,16] hidden size and ReLU activation function. Taobao: interaction model with [64,64,64] hidden size and ReLU activation function. MovieLen-Taobao: interaction model with [64,64,64] hidden size and ReLU activation function. Note that the interaction model is a model which directly use user-item feature concatenation as input. Other training hyper-parameters are the same with the released code.\nNote that it is intractable to conduct completely fair comparison between CMML and \ud835\udc60 2 \ud835\udc40\ud835\udc52\ud835\udc61\ud835\udc4e since they follow completely different inner adaptation strategy. The whole CMML model inherently requires more parameters compared with \ud835\udc60 2 \ud835\udc40\ud835\udc52\ud835\udc61\ud835\udc4e because it is not allowed to conduct gradient descent like \ud835\udc60 2 \ud835\udc40\ud835\udc52\ud835\udc61\ud835\udc4e. It has to use additional parameters for context encoder and modulation network to conduct inner-loop adaptation. Even though CMML has more parameters, it still can achieve much higher acceleration compared with \ud835\udc60 2 \ud835\udc40\ud835\udc52\ud835\udc61\ud835\udc4e, which can validate the low computational efficiency of gradient based Meta Learning.\nFor a relatively fair comparison, we ensure the amount of parameters in CMML backbone network is on the same scale compared with that in \ud835\udc60 2 \ud835\udc40\ud835\udc52\ud835\udc61\ud835\udc4e. For LSTM context encoder, we use GRU with hidden size 256 and the output of GRU will also be fed into a MLP with hidden size [256] and ReLU activation function. The output of context encoder has the same dimension with use-item feature concatenation.\nFor the modulation network, we use a MLP with ReLU activation function. For example, with input size 128 and hidden size 64, CMML-Linear only generates weights for the final weight so it consists of one [128*64, 64*64, 64*output] network. For CMML-Sigmoid/Film/Soft-M, the overall network is [128*64, 64*64, 64*output], with each layer's output extracted to generate the corresponding layer in the backbone network (the output of first 128 * 64 is ReLU activated then goes through one linear layer and get the weight modulation on the first hidden layer of backbone, etc). The hidden size used in our experiment is 128 for Taobao and MovieLens-Taobao and 64 for MovieLens-20M.\nAnd sometimes we find out the bias initialization with small value can increase the stability of the results for CMML-Sigmoid and CMML-FiLM. For these two methods we initialize all MLP's bias with 0.1 in the MovieLens-20M and 0 in Taobao experiment. The other training setting is basically the same with \ud835\udc60 2 \ud835\udc40\ud835\udc52\ud835\udc61\ud835\udc4e. 7.2.2 User-specific setting. For basaeline DCN and DCN-F, the hyper-parameter setting is the same with that in scenario setting 2 https://github.com/THUDM/ScenarioMeta except that learning rate = 1e-3. and we utilise fine-tuning learning rate = 2e-4 and step = 5. And for MELU and MetaDNN baseline, we basically follow the hyperparameter setting of original MELU paper. we modify the learning rate as 3e-4 and it can achieve better result compared with original implementation. For CMML-Film, We use the mean encoder with hidden size [64] and ReLU activation function and backbone network with [64, 64] and Leaky ReLU activation funciton. The modulation network is similar with that of scenario-specific setting except the number of layer becomes 2 and the hidden size is 256. The learning rate is 1e-4 and weight decay is 1e-5.\nOur dataset split is different from the original MELU paper because we think the original dataset split violates the basic assumption of Meta Learning -the meta-training distribution and metatesting distribution should have the same distribution and the split based on time in original MELU paper clearly results in out of distribution problem. Thus we obtain the dataset by only splitting the user to get warm-warm scenario and warm-cold scenario and retest all baseline on the new dataset. To construct a few-shot setting, we clip the amount of ratings to 100 for users who have more than 100 ratings. The following setting like support-query split is the same with original MELU.", "publication_ref": ["b8", "b32", "b16"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Top-N recall in 3 datasets MovieLens-20M Taobao Hybrid Movie-Taobao Method Recall@10 Recall@20 Recall@50 Recall@20 Recall@50 Recall@100 Recall@20 Recall@50 Recall@100", "journal": "CMML", "year": "", "authors": ""}, {"ref_id": "b1", "title": "Incorporating contextual information in recommender systems using a multidimensional approach", "journal": "ACM Transactions on Information Systems (TOIS)", "year": "2005", "authors": "Gediminas Adomavicius; Ramesh Sankaranarayanan; Shahana Sen; Alexander Tuzhilin"}, {"ref_id": "b2", "title": "Context-aware recommender systems", "journal": "Springer", "year": "2011", "authors": "Gediminas Adomavicius; Alexander Tuzhilin"}, {"ref_id": "b3", "title": "Modeling the impact of short-and long-term behavior on search personalization", "journal": "", "year": "2012", "authors": "Ryen W Paul N Bennett; Wei White; Susan T Chu; Peter Dumais; Fedor Bailey; Xiaoyuan Borisyuk;  Cui"}, {"ref_id": "b4", "title": "Meta-learning for user cold-start recommendation", "journal": "", "year": "2019", "authors": "Homanga Bharadhwaj"}, {"ref_id": "b5", "title": "Wide & deep learning for recommender systems", "journal": "", "year": "2016", "authors": "Heng-Tze Cheng; Levent Koc; Jeremiah Harmsen; Tal Shaked; Tushar Chandra; Hrishi Aradhye; Glen Anderson; Greg Corrado; Wei Chai; Mustafa Ispir"}, {"ref_id": "b6", "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "journal": "", "year": "2014-10-25", "authors": "Kyunghyun Cho; Bart Van Merrienboer; \u00c7aglar G\u00fcl\u00e7ehre; Dzmitry Bahdanau; Fethi Bougares; Holger Schwenk; Yoshua Bengio"}, {"ref_id": "b7", "title": "MAMO: Memory-Augmented Meta-Optimization for Cold-start Recommendation", "journal": "", "year": "2020", "authors": "Manqing Dong; Feng Yuan; Lina Yao; Xiwei Xu; Liming Zhu"}, {"ref_id": "b8", "title": "Sequential scenario-specific meta learner for online recommendation", "journal": "", "year": "2019", "authors": "Zhengxiao Du; Xiaowei Wang; Hongxia Yang; Jingren Zhou; Jie Tang"}, {"ref_id": "b9", "title": "Model-agnostic metalearning for fast adaptation of deep networks", "journal": "PMLR", "year": "2017", "authors": "Chelsea Finn; Pieter Abbeel; Sergey Levine"}, {"ref_id": "b10", "title": "DeepFM: A Factorization-Machine based Neural Network for CTR Prediction", "journal": "", "year": "2017-08-19", "authors": "Huifeng Guo; Ruiming Tang; Yunming Ye; Zhenguo Li; Xiuqiang He"}, {"ref_id": "b11", "title": "HyperNetworks. In 5th International Conference on Learning Representations, ICLR 2017", "journal": "", "year": "2017-04-24", "authors": "David Ha; Andrew M Dai; Quoc V Le"}, {"ref_id": "b12", "title": "Inductive Representation Learning on Large Graphs", "journal": "", "year": "2017-09", "authors": "William L Hamilton; Zhitao Ying; Jure Leskovec"}, {"ref_id": "b13", "title": "The movielens datasets: History and context", "journal": "Acm transactions on interactive intelligent systems (tiis)", "year": "2015", "authors": "Maxwell Harper; Joseph A Konstan"}, {"ref_id": "b14", "title": "Conet: Collaborative cross networks for cross-domain recommendation", "journal": "", "year": "2018", "authors": "Guangneng Hu; Yu Zhang; Qiang Yang"}, {"ref_id": "b15", "title": "IR evaluation methods for retrieving highly relevant documents", "journal": "ACM", "year": "2017", "authors": "Kalervo J\u00e4rvelin; Jaana Kek\u00e4l\u00e4inen"}, {"ref_id": "b16", "title": "Adam: A Method for Stochastic Optimization", "journal": "", "year": "2015-05-07", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"ref_id": "b17", "title": "Matrix factorization techniques for recommender systems", "journal": "Computer", "year": "2009", "authors": "Yehuda Koren; Robert Bell; Chris Volinsky"}, {"ref_id": "b18", "title": "Transfer Learning via Contextual Invariants for One-to-Many Cross-Domain Recommendation", "journal": "ACM", "year": "2020-07-25", "authors": "Adit Krishnan; Mahashweta Das; Mangesh Bendre; Hao Yang; Hari Sundaram"}, {"ref_id": "b19", "title": "MeLU: meta-learned user preference estimator for cold-start recommendation", "journal": "", "year": "2019", "authors": "Hoyeop Lee; Jinbae Im; Seongwon Jang; Hyunsouk Cho; Sehee Chung"}, {"ref_id": "b20", "title": "DDTCDR: Deep Dual Transfer Cross Domain Recommendation", "journal": "ACM", "year": "2020-02-03", "authors": "Pan Li; Alexander Tuzhilin"}, {"ref_id": "b21", "title": "Modeling task relationships in multi-task learning with multi-gate mixture-ofexperts", "journal": "", "year": "1930", "authors": "Jiaqi Ma; Zhe Zhao; Xinyang Yi; Jilin Chen; Lichan Hong; Ed H Chi"}, {"ref_id": "b22", "title": "Cross-Domain Recommendation: An Embedding and Mapping Approach", "journal": "", "year": "2017", "authors": "Huawei Tong Man; Xiaolong Shen; Xueqi Jin;  Cheng"}, {"ref_id": "b23", "title": "Film: Visual reasoning with a general conditioning layer", "journal": "", "year": "2018", "authors": "Ethan Perez; Florian Strub; Harm De Vries; Vincent Dumoulin; Aaron Courville"}, {"ref_id": "b24", "title": "Meta-Learning with Implicit Gradients", "journal": "", "year": "2019-12-08", "authors": "Aravind Rajeswaran; Chelsea Finn; M Sham; Sergey Kakade;  Levine"}, {"ref_id": "b25", "title": "Efficient off-policy meta-reinforcement learning via probabilistic context variables", "journal": "PMLR", "year": "2019", "authors": "Kate Rakelly; Aurick Zhou; Chelsea Finn; Sergey Levine; Deirdre Quillen"}, {"ref_id": "b26", "title": "BPR: Bayesian Personalized Ranking from Implicit Feedback", "journal": "AUAI Press", "year": "2009-06-18", "authors": "Steffen Rendle; Christoph Freudenthaler; Zeno Gantner; Lars Schmidt-Thieme"}, {"ref_id": "b27", "title": "Truncated back-propagation for bilevel optimization", "journal": "PMLR", "year": "2019", "authors": "Amirreza Shaban; Ching-An Cheng; Nathan Hatch; Byron Boots"}, {"ref_id": "b28", "title": "Hybrid recommender system based on autoencoders", "journal": "", "year": "2016", "authors": "Florian Strub; Romaric Gaudel; J\u00e9r\u00e9mie Mary"}, {"ref_id": "b29", "title": "Adaptive web search based on user profile constructed without any effort from users", "journal": "", "year": "2004", "authors": "Kazunari Sugiyama; Kenji Hatano; Masatoshi Yoshikawa"}, {"ref_id": "b30", "title": "Deep content-based music recommendation", "journal": "NIPS", "year": "2013", "authors": "A\u00e4ron Van Den; Sander Oord; Benjamin Dieleman;  Schrauwen"}, {"ref_id": "b31", "title": "A meta-learning perspective on cold-start recommendations for items", "journal": "", "year": "2017", "authors": "Manasi Vartak; Arvind Thiagarajan; Conrado Miranda; Jeshua Bratman; Hugo Larochelle"}, {"ref_id": "b32", "title": "Matching Networks for One Shot Learning", "journal": "", "year": "2016-12-05", "authors": "Oriol Vinyals; Charles Blundell; Tim Lillicrap; Koray Kavukcuoglu; Daan Wierstra"}, {"ref_id": "b33", "title": "Deep & cross network for ad click predictions", "journal": "", "year": "2017", "authors": "Ruoxi Wang; Bin Fu; Gang Fu; Mingliang Wang"}, {"ref_id": "b34", "title": "Collaborative filtering and deep learning based recommendation system for cold start items", "journal": "Expert Systems with Applications", "year": "2017", "authors": "Jian Wei; Jianhua He; Kai Chen; Yi Zhou; Zuoyin Tang"}, {"ref_id": "b35", "title": "Fast Adaptation for Cold-start Collaborative Filtering with Meta-learning", "journal": "", "year": "2020", "authors": "Tianxin Wei; Ziwei Wu; Ruirui Li; Ziniu Hu; Fuli Feng; Xiangnan He; Yizhou Sun; Wei Wang"}, {"ref_id": "b36", "title": "Predicting short-term interests using activity-based search context", "journal": "", "year": "2010", "authors": "Paul N Ryen W White; Susan T Bennett;  Dumais"}, {"ref_id": "b37", "title": "Multi-Task Reinforcement Learning with Soft Modularization", "journal": "", "year": "2020-12-06", "authors": "Ruihan Yang; Huazhe Xu; Yi Wu; Xiaolong Wang"}, {"ref_id": "b38", "title": "How to retrain recommender system? A sequential metalearning method", "journal": "", "year": "2020", "authors": "Yang Zhang; Fuli Feng; Chenxu Wang; Xiangnan He; Meng Wang; Yan Li; Yongdong Zhang"}, {"ref_id": "b39", "title": "Cold-start Sequential Recommendation via Meta Learner", "journal": "", "year": "2020", "authors": "Yujia Zheng; Siyi Liu; Zekun Li; Shu Wu"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: CMML framework", "figure_data": ""}, {"figure_label": "23", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :Figure 3 :23Figure 2: Memory and time", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: Activated routes for two different tasks", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "For CMML-Linear, CMML-Sigmoid, and CMML-Film, we use deep network with hidden size [64,64,64] and Leaky ReLU activation function for the backbone network in all three datasets. For CMML-Soft-M, we use 3 layers network and each layer consists of 4 modules. The hidden size of each module is 32. For Soft-M on MovieLens and Taobao, with input size x, the Soft-M has [x*32*4, 32*32*4, 32*64*4] hidden layer and the final [32*1] layer. For Soft-M on MovieLens-Taobao, with input size x, the Soft-M has [x*32*4, 32*32*4, 32*32*4] hidden layer and the final [32*1] layer.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "1 \ud835\udc52 is the GRU based context encoder parameterized by meta parameters \ud835\udf03 1 \ud835\udc52 \u2208 \u0398 \ud835\udc40 , {\u210e \ud835\udc58 } \ud835\udc5b \ud835\udc58=1 , {\ud835\udc5c \ud835\udc58 } \ud835\udc5b \ud835\udc58=1 denote the output sequence of the GRU model and \ud835\udc53 \ud835\udf03 2 \ud835\udc52 represents the MLP parameterized by meta parameters \ud835\udf03 2 \ud835\udc52 \u2208 \u0398 \ud835\udc40 that maps sequence embedding \ud835\udc5c \ud835\udc5b into context \ud835\udc36.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Time and Space complexity for gradient based and context-based Meta Learning algorithms", "figure_data": "AlogrithmTimeSpaceProcessMAML-k\ud835\udc42 (\ud835\udc5a\ud835\udc58 (\ud835\udc53 \ud835\udc61 + \ud835\udc4f \ud835\udc61 ))\ud835\udc42 (\ud835\udc5a\ud835\udc58 (\ud835\udc53 \ud835\udc60 + \ud835\udc4f \ud835\udc60 ))InferMAML-k\ud835\udc42 (5\ud835\udc58\ud835\udc5a\ud835\udc4f \ud835\udc61 )\ud835\udc42 (2\ud835\udc58\ud835\udc5a\ud835\udc4f \ud835\udc60 )TrainMAML-\ud835\udeff \ud835\udc42 (5\ud835\udc5a\ud835\udc4f \ud835\udc61 \ud835\udf05 log \ud835\udc37 \ud835\udeff ) \ud835\udc42 (2\ud835\udc5a\ud835\udc4f \ud835\udc60 \ud835\udf05 log \ud835\udc37 \ud835\udeff ) TrainCMML\ud835\udc42 (\ud835\udc5a\ud835\udc53 \ud835\udc61 )\ud835\udc42 (\ud835\udc5a\ud835\udc53 \ud835\udc60 )InferCMML\ud835\udc42 (\ud835\udc5a\ud835\udc4f \ud835\udc61 )\ud835\udc42 (\ud835\udc5a\ud835\udc4f \ud835\udc60 )Trainthe sub-modules are activated for a specific task, which stronglyincreases the interpretability."}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Performance in MovieLens-1M", "figure_data": "W-WC-WMethodMAE NDCG@3 MAE NDCG@3DCN0.8690.6930.9070.665DCN-F0.8310.6830.8470.679MetaDNN0.7260.7720.7330.765MELU0.7250.7710.7340.763CMML-FiLM 0.7270.7710.7450.760"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Context encoder and hybrid context generator", "figure_data": "MethodRecall@10 Recall@20 Recall@50Non-negative31.6752.6986.20PE40.0561.2487.78SE44.3063.8288.10SE-MLP46.3765.1888.61SE-dot47.2965.5088.62"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Results for different Modulation Network", "figure_data": "MovieLens-20MTaobaoMethod Recall@10 Recall@20 Recall@20 Recall@50Linear47.2965.5024.6341.30S-LM47.2165.3924.6841.39F-LM47.3665.3225.7742.30Soft-M47.3265.6225.5642.19"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "MovieLens-20M with 5 seeds Soft-M 47.31 \u00b1 0.05 65.65 \u00b1 0.04 88.75 \u00b1 0.02 CMML-Linear 47.20 \u00b1 0.07 65.52 \u00b1 0.05 88.66 \u00b1 0.02 CMML-Sigmoid 47.16 \u00b1 0.05 65.49 \u00b1 0.08 88.55 \u00b1 0.03", "figure_data": "MethodRecall@10Recall@20Recall@50DCN31.25 \u00b1 0.12 52.92 \u00b1 0.18 86.19 \u00b1 0.08DCN-F39.18 \u00b1 0.19 59.39 \u00b1 0.24 86.32 \u00b1 0.08ItemPop39.65 \u00b1 0.00 54.33 \u00b1 0.00 78.12 \u00b1 0.00CoNet46.37 \u00b1 0.08 63.93 \u00b1 0.12 87.29 \u00b1 0.05s 2 Meta47.56 \u00b1 0.02 66.09 \u00b1 0.04 89.08 \u00b1 0.02CMML-FiLM47.06 \u00b1 0.07 65.14 \u00b1 0.04 88.48 \u00b1 0.02CMML-"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Taobao with 5 seeds Sigmoid 25.29 \u00b1 0.14 42.05 \u00b1 0.13 58.38 \u00b1 0.11", "figure_data": "MethodRecall@20Recall@50Recall@100DCN22.58 \u00b1 0.14 38.88 \u00b1 0.13 55.05 \u00b1 0.20DCN-F22.57 \u00b1 0.23 37.67 \u00b1 0.14 53.77 \u00b1 0.24ItemPop21.94 \u00b1 0.00 36.11 \u00b1 0.00 38.19 \u00b1 0.00CoNet20.2731.4844.53s 2 Meta24.99 \u00b1 0.22 40.98 \u00b1 0.16 57.30 \u00b1 0.14CMML-FiLM25.24 \u00b1 0.13 41.84 \u00b1 0.14 58.42 \u00b1 0.17CMML-Soft-M 25.26 \u00b1 0.08 42.07 \u00b1 0.04 58.66 \u00b1 0.09CMML-Linear24.77 \u00b1 0.15 41.49 \u00b1 0.18 58.09 \u00b1 0.14CMML-"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "MovieLens-Taobao with 5 seeds Meta 39.35 \u00b1 0.30 58.01 \u00b1 0.17 70.88 \u00b1 0.18 CMML-FiLM 40.52 \u00b1 0.25 59.94 \u00b1 0.08 73.77 \u00b1 0.10 CMML-Soft-M 40.75 \u00b1 0.08 60.02 \u00b1 0.10 73.89 \u00b1 0.09", "figure_data": "MethodRecall@20Recall@50Recall@100DCN34.06 \u00b1 0.67 56.80 \u00b1 0.40 71.77 \u00b1 0.32DCN-F36.60 \u00b1 0.50 55.66 \u00b1 0.27 69.56 \u00b1 0.36ItemPop35.00 \u00b1 0.00 52.76 \u00b1 0.00 61.02 \u00b1 0.00s 2"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Context encoder and hybrid context generator", "figure_data": "MethodRecall@10Recall@20Recall@50Non-negative 32.83 \u00b1 0.98 54.35 \u00b1 0.90 86.49 \u00b1 0.16PE37.35 \u00b1 0.57 58.85 \u00b1 0.51 87.34 \u00b1 0.09SE41.24 \u00b1 0.66 61.94 \u00b1 0.43 87.95 \u00b1 0.08SE-MLP46.62 \u00b1 0.07 65.25 \u00b1 0.07 88.68 \u00b1 0.04SE-dot47.20 \u00b1 0.07 65.52 \u00b1 0.05 88.66 \u00b1 0.02"}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u0398 E \ud835\udc47 \u223c\ud835\udc43 (\ud835\udf0f) L \u0398 \ud835\udc37 query \ud835\udc47 | \ud835\udc37 support \ud835\udc47 ,(1)", "formula_coordinates": [2.0, 371.12, 448.11, 187.08, 18.42]}, {"formula_id": "formula_1", "formula_text": "\u0398 \ud835\udc40 ,\u03a6 E \ud835\udc47 \u223c\ud835\udc43 (\ud835\udc37 \ud835\udc5a\ud835\udc61\ud835\udc5f ) L \u0398 \ud835\udc40 ,\u03a6 \ud835\udc37 query \ud835\udc47 | \ud835\udc37 support \ud835\udc47 ,(2)", "formula_coordinates": [2.0, 352.1, 591.54, 206.11, 18.42]}, {"formula_id": "formula_2", "formula_text": "L \u0398 \ud835\udc40 ,\u03a6 \ud835\udc37 q \ud835\udc47 | \ud835\udc37 s \ud835\udc47 = \u2211\ufe01 (\ud835\udc62 \ud835\udc58 ,\ud835\udc56 \ud835\udc58 ) \u2208\ud835\udc37 q \ud835\udc47 \u2113 \ud835\udc53 \u0398 \ud835\udc40 ,\u03a6 (\ud835\udc62 \ud835\udc58 , \ud835\udc56 \ud835\udc58 ; {\ud835\udc62, \ud835\udc56} \u210e ), \ud835\udc66 \ud835\udc58 \ud835\udc37 q \ud835\udc47 ,(3)", "formula_coordinates": [2.0, 324.7, 621.71, 233.5, 28.62]}, {"formula_id": "formula_3", "formula_text": "\u2113 = max 0, 1 -\ud835\udc53 \u03a6,\u0398 \ud835\udc40 \ud835\udc62, \ud835\udc56 + ; {\ud835\udc62, \ud835\udc56} \u210e + \ud835\udc53 \u03a6,\u0398 \ud835\udc40 (\ud835\udc62, \ud835\udc56 -; {\ud835\udc62, \ud835\udc56} \u210e ) . (4)", "formula_coordinates": [3.0, 60.46, 290.79, 233.59, 12.24]}, {"formula_id": "formula_4", "formula_text": "\u2113 = (\ud835\udc66 \ud835\udc62,\ud835\udc56 -\ud835\udc53 \u03a6,\u0398 \ud835\udc40 (\ud835\udc62, \ud835\udc56; {\ud835\udc62, \ud835\udc56} \u210e )) 2 ,(5)", "formula_coordinates": [3.0, 115.21, 401.06, 178.84, 12.56]}, {"formula_id": "formula_5", "formula_text": "\u0398 \ud835\udc5a\ud835\udc52\ud835\udc61\ud835\udc4e \u2190 \u0398 \ud835\udc40 -\ud835\udefc\u2207 \u0398 \ud835\udc40 L \ud835\udc5a\ud835\udc52\ud835\udc61\ud835\udc4e 12:", "formula_coordinates": [3.0, 320.58, 257.21, 128.59, 19.43]}, {"formula_id": "formula_6", "formula_text": "\ud835\udc36 \ud835\udc58 = \ud835\udc53 \ud835\udf03 \ud835\udc52 ({\ud835\udc40 \ud835\udc62 \ud835\udc65 \ud835\udc62 , \ud835\udc40 \ud835\udc56 \ud835\udc65 \ud835\udc56 } \ud835\udc58 ), \ud835\udc36 = Mean/Max Pooling({\ud835\udc36 \ud835\udc58 } \ud835\udc5b ),(6)", "formula_coordinates": [3.0, 376.72, 687.34, 181.48, 22.45]}, {"formula_id": "formula_7", "formula_text": "{\u210e \ud835\udc58 } \ud835\udc5b \ud835\udc58=1 , {\ud835\udc5c \ud835\udc58 } \ud835\udc5b \ud835\udc58=1 = GRU \ud835\udf03 1 \ud835\udc52 ({\ud835\udc40 \ud835\udc62 \ud835\udc65 \ud835\udc58 \ud835\udc62 , \ud835\udc40 \ud835\udc56 \ud835\udc65 \ud835\udc58 \ud835\udc56 } \ud835\udc5b \ud835\udc58=1 ), \ud835\udc36 = \ud835\udc53 \ud835\udf03 2 \ud835\udc52 (\ud835\udc5c \ud835\udc5b ),(7)", "formula_coordinates": [4.0, 88.71, 591.49, 205.34, 25.56]}, {"formula_id": "formula_8", "formula_text": "\ud835\udc36 \u210e \ud835\udc62 \ud835\udc58 ,\ud835\udc56 \ud835\udc58 = \ud835\udc36 \u2299 {\ud835\udc40 \ud835\udc62 \ud835\udc65 \ud835\udc62 \ud835\udc58 , \ud835\udc40 \ud835\udc56 \ud835\udc65 \ud835\udc56 \ud835\udc58 }, \ud835\udc36 \u210e \ud835\udc62 \ud835\udc58 ,\ud835\udc56 \ud835\udc58 = MLP \ud835\udf03 \ud835\udc51 (\ud835\udc36, {\ud835\udc40 \ud835\udc62 \ud835\udc65 \ud835\udc62 \ud835\udc58 , \ud835\udc40 \ud835\udc56 \ud835\udc65 \ud835\udc56 \ud835\udc58 }),(8)", "formula_coordinates": [4.0, 371.24, 640.42, 186.96, 27.51]}, {"formula_id": "formula_9", "formula_text": "\ud835\udc5c \ud835\udc62\ud835\udc56 = \ud835\udc64 \ud835\udc47 \u210e \u210e \ud835\udc62\ud835\udc56 + \ud835\udc4f \u210e ,(9)", "formula_coordinates": [5.0, 147.03, 342.95, 147.02, 17.25]}, {"formula_id": "formula_10", "formula_text": "\ud835\udc64 \u210e = Sigmoid(\ud835\udc53 \ud835\udf03 \u210e (\ud835\udc36 \u210e )), \ud835\udc5c \ud835\udc56 = \ud835\udc64 \u210e \u2299 \ud835\udc59 \ud835\udc56 ,(10)", "formula_coordinates": [5.0, 129.25, 622.83, 164.8, 22.61]}, {"formula_id": "formula_11", "formula_text": "\ud835\udc64 \u210e , \ud835\udc4f \u210e = \ud835\udc53 \ud835\udf03 \u210e (\ud835\udc36 \u210e ), \ud835\udc5c \ud835\udc56 = \ud835\udc64 \u210e \u2299 \ud835\udc59 \ud835\udc56 + \ud835\udc4f \u210e ,(11)", "formula_coordinates": [5.0, 134.02, 652.16, 160.02, 22.23]}, {"formula_id": "formula_12", "formula_text": "\ud835\udf0e \ud835\udc59 = MLP \ud835\udf03 \u210e (\u210e \ud835\udc59-1 \ud835\udc5f ), \u210e 0 \ud835\udc5f = \ud835\udc36 \u210e , \ud835\udc5d \ud835\udc59 \ud835\udc56,\ud835\udc57 = exp(\ud835\udf0e \ud835\udc59 \ud835\udc56,\ud835\udc57 ) \ud835\udc5a-1 \ud835\udc57=0 exp(\ud835\udf0e \ud835\udc59 \ud835\udc56,\ud835\udc57 ) , (12", "formula_coordinates": [5.0, 384.47, 559.34, 170.32, 44.97]}, {"formula_id": "formula_13", "formula_text": ")", "formula_coordinates": [5.0, 554.78, 578.24, 3.42, 8.97]}, {"formula_id": "formula_14", "formula_text": "\ud835\udc5c\ud835\udc4f \ud835\udc59 \ud835\udc56 = \ud835\udc61 \u2211\ufe01 \ud835\udc57=0 \ud835\udc5d \ud835\udc59-1 \ud835\udc56,\ud835\udc57 MLP \u03a6 (\ud835\udc5c\ud835\udc4f \ud835\udc59-1 \ud835\udc57 ),(13)", "formula_coordinates": [5.0, 386.66, 615.41, 171.54, 24.75]}, {"formula_id": "formula_15", "formula_text": "\ud835\udc46 2", "formula_coordinates": [8.0, 53.53, 621.93, 8.17, 8.81]}], "doi": "10.1145/3459637.3482241"}
