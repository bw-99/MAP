{"A Unified Search and Recommendation Framework Based on Multi-Scenario Learning for Ranking in E-commerce": "Jinhan Liu \u2217 JD.com Beijing, China liujinhan1@jd.com Qiyu Chen JD.com Beijing, China chenqiyu@jd.com Junjie Xu JD.com Beijing, China xujunjie33@jd.com Junjie Li JD.com Beijing, China lijunjie72@jd.com Baoli Li JD.com Beijing, China libaoli5@jd.com", "ABSTRACT": "Sulong Xu JD.com Beijing, China xusulong@jd.com", "ACMReference Format:": "Search and recommendation ( S&R ) are the two most important scenarios in e-commerce. The majority of users typically interact with products in S&R scenarios, indicating the need and potential for joint modeling. Traditional multi-scenario models use shared parameters to learn the similarity of multiple tasks, and task-specific parameters to learn the divergence of individual tasks. This coarsegrained modeling approach does not effectively capture the differences between S&R scenarios. Furthermore, this approach does not sufficiently exploit the information across the global label space. These issues can result in the suboptimal performance of multiscenario models in handling both S&R scenarios. To address these issues, we propose an effective and universal framework for U nified S earch and R ecommendation (USR), designed with S&R Views User Interest Extractor Layer (IE) and S&R Views Feature Generator Layer (FG) to separately generate user interests and scenario-agnostic feature representations for S&R. Next, we introduce a Global Label Space Multi-Task Layer (GLMT) that uses global labels as supervised signals of auxiliary tasks and jointly models the main task and auxiliary tasks using conditional probability. Extensive experimental evaluations on real-world industrial datasets show that USR can be applied to various multi-scenario models and significantly improve their performance. Online A/B testing also indicates substantial performance gains across multiple metrics. Currently, USR has been successfully deployed in the 7Fresh App.", "CCS CONCEPTS": "\u00b7 Information systems \u2192 Recommender systems .", "KEYWORDS": "search and recommendation, multi-scenario, global label space \u2217 Corresponding author Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. https://doi.org/10.1145/3626772.3661356 Jinhan Liu, Qiyu Chen, Junjie Xu, Junjie Li, Baoli Li, and Sulong Xu. 2024. A Unified Search and Recommendation Framework Based on Multi-Scenario Learning for Ranking in E-commerce. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '24), July 14-18, 2024, Washington, DC, USA. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3626772.3661356", "1 INTRODUCTION": "Search and recommendation functions have become essential services for online applications, with many researchers working to advance the outcomes of these services [2, 6, 8, 10, 12, 26, 27]. In addition, many studies [15, 20-22, 25] have validated that the S&R scenarios complement each other in terms of their functions and information. Integrating S&R into a joint modeling approach is a critical and valuable problem to solve. In fact, some state-ofthe-art work has garnered impressive results in Multi-Scenario Learning (MSL). Based on the different architectures, the existing Multi-Scenario Models (MSM) can be divided into two categories: Tower-based models [9, 13, 16, 28, 30] and Dynamic Weight (DW) models [3, 18, 19]. These studies typically focus on modeling scenarios that are similar, such as different recommendation locations. However, when applied to S&R scenarios, the inherent limitations of traditional multi-scenario modeling methods may lead to less than optimal outcomes. Traditional MSM are unable to extract scenariospecific representations from the combined behavior sequences collected from both S&R scenarios. For domain-agnostic features, MSM do not differentiate their importance across the respective scenarios, treating them indiscriminately. Furthermore, traditional MSM do not utilize cross-domain label information, which could serve as data augmentation for positive samples in each scenario. Therefore, these potential gains have been neglected. To this end, in this paper, we propose a Unified Search and Recommendation (USR) framework based on MSL. In the bottom of USR, we design User Interest Extractor Layer (IE) and Feature Generator Layer (FG) to obtain fine-grained representations for user interests and scenario-agnostic features in S&R views. Specifically, we define a scenario trigger that introduces prior knowledge of the scenario. After that, we extract user interests and scale domain-agnostic features across S&R views using dynamic weight technique and gating mechanism with the S&R trigger. Moreover, we apply contrastive learning to these representations to further capture the differences between S&R scenarios. In the Global Label Space Multi-Task Layer S&R Views Fcaturc Gencrator Layer S&R Views Interest Fxtractor Layer Vs&r Global Space Label Muti-Task Layer scenario router multi-scenario Vk scenario indicator model Ncural Laycr(sigmoid) Vs&r MLP Cmain S&R Views Interest Extractor Layer S&R View Feature Generator Layer intrests and feature contrast Pulling Closer Pushing Away CCL Tanstormer Layer Scenario Encoding] Input Network Element-wise product gadient Concatenate operation Embedding \u00dbg p(globalinteraction _ erposure p(scenraiointeraction globalinteraction ) S&R User Target Scenario-agnostic Scenario- p(scenraiointeraction exposure Query Behavitor Sequences Item features features Trigger Siop Laycr specific (GLMT), we propose a novel method that jointly models scenariospecific CTR/CTCVR and global CTR/CTCVR, based on conditional probability. The purpose is to let the model perceive global label information during learning. To summarize, the contributions of this paper are as follows: \u00b7 To the best of our knowledge, this is the first work that improves upon multi-scenario learning for the joint modeling of search and recommendation. \u00b7 We propose a USR framework based on MSL to enhance search and recommendation. Through the IE and FG, we obtain fine-grained representations of user interests and scenario-agnostic features within S&R views. Additionally, we design the GLMT which employs global labels as supervised signals for auxiliary tasks and jointly models the main and auxiliary tasks using conditional probability. \u00b7 Both offline and online experiments demonstrate the superiority of our proposed USR. Currently, USR has been successfully deployed in the 7Fresh 1 App.", "2 PROBLEM FORMULATION": "Considering a training dataset D = nGLYPH<16> \ud835\udc65 \ud835\udc56 , \ud835\udc66 \ud835\udc57 \ud835\udc56 GLYPH<17>o | D| \ud835\udc56 = 1 , where \ud835\udc65 \ud835\udc56 rep- resents the feature set and \ud835\udc66 \ud835\udc57 \ud835\udc56 \u2208 { 0 , 1 } represents the class label indicating a click for click-through rate (CTR) prediction or a purchase for click-through & conversion rate (CTCVR) prediction of the \ud835\udc56 \ud835\udc61\u210e sample. Note that the original input features include: S&R user behavior sequence \ud835\udc35 \ud835\udc46 & \ud835\udc45 , query \ud835\udc5e , target item \ud835\udc56 , scenario indicator \ud835\udc58 and other features \ud835\udc65 . In addition, we divide the other features into two parts based on whether they are scenario-aware. We need to train a unified model, to predict multiple tasks in S&R scenarios, which can be formulated as: \ud835\udc53 \u0398 (\u00b7) denotes the underlying unified model with parameters \u0398 , \ud835\udc65 \ud835\udc60 is the scenario-specific feature set, \ud835\udc65 \ud835\udc4e is the scenario-agnostic feature set and \u02c6 \ud835\udc66 \ud835\udc57 is the model's prediction of the task\ud835\udc57 .", "3 METHOD": "The network architecture of USR is illustrated in Fig. 1. In this section, we present the proposed USR approach in detail.", "3.1 The Input Network": "The embedding layer is shared across S&R scenarios and learns the representation of the inputs. After transforming all features into embeddings, we denote the representations of S&R user behavior sequence, target item, query, scenario-specific, and scenarioagnostic features as \ud835\udc83 \ud835\udc46 & \ud835\udc45 , \ud835\udc86 \ud835\udc61 , \ud835\udc86 \ud835\udc5e , \ud835\udc86 \ud835\udc60 and \ud835\udc86 \ud835\udc4e , respectively. Next, in the sequence encoding, we follow BST [4] to utilize \ud835\udc83 \ud835\udc46 & \ud835\udc45 and \ud835\udc86 \ud835\udc61 to extract the global user interests, denoted as \ud835\udc8a \ud835\udc54 . In the scenario encoding, inspired by [24], we embed the S&R scenario in the same space. Note that the scenario representation we extract is global rather than local. For each instance, there will be both S&R views representation simultaneously. Let \ud835\udc86 \ud835\udc60 \ud835\udc63 and \ud835\udc86 \ud835\udc5f \ud835\udc63 denote S&R views representations, respectively. Each of the embeddings previously mentioned has a dimension of \ud835\udc85 . Additionally, we define a scenario trigger that introduces prior knowledge of scenario and influence the weight of feature information. The trigger \ud835\udc95 can be formulated as follows:", "3.2 S&R Views User Interest Extractor Layer": "The global user interest representations obtained merely through sequence encoding do not effectively distinguish between user interest representations in S&R scenarios. To address this issue, inspired by the Dynamic Weight (DW) technique [18], we propose a layer to extract user interests for S&R views based on the re-parameterization method to adaptively generate parameters depending on the given condition. Specifically, we concatenate the global user interests \ud835\udc8a \ud835\udc54 and the scenario trigger \ud835\udc95 \ud835\udc58 as the condition, with \ud835\udc8a \ud835\udc54 as the input for the DW network. The network weights are adaptively generated under this condition, using Fully Connected (FC) layers and a Reshape function. For each sample, given the scenario trigger \ud835\udc95 \ud835\udc58 , the user interest representations for S&R views can be represented as: where \u2295 indicates concatenation, \u2298 means stop gradient, \ud835\udc8a \ud835\udc60 , \ud835\udc8a \ud835\udc5f \u2208 R \ud835\udc51 denote the user interest representations for S&R views, respectively. To better capture the differences in user interests between S&R scenarios, we draw inspiration from classic contrastive learning [5, 7, 29], and design a novel contrastive loss, which is formulated as follows: where \ud835\udc56 + \ud835\udc5f and \ud835\udc56 + \ud835\udc60 denote the positive samples, which are randomly sampled from \ud835\udc8a \ud835\udc5f and \ud835\udc8a \ud835\udc60 within a batch, \ud835\udc56 \ud835\udc58 \ud835\udc5f denotes the negative sample, \ud835\udc41 is the batch size, and \ud835\udf0f is a temperature coefficient.", "3.3 S&R Views Feature Generator Layer": "Considering the heterogeneity of S&R, it is apparent that scenarioagnostic features have different discrimination in S&R. For example, product click-related features generally play a more significant role in the recommendation scenario than in the search scenario. Thus, FG is designed to scale each feature according to the S&R trigger. Specifically, for each sample, we concatenate the scenario trigger \ud835\udc95 \ud835\udc58 and the scenario-agnostic features \ud835\udc86 \ud835\udc4e as the prior information to scale \ud835\udc86 \ud835\udc4e . Then, the dimension of the hidden vector is projected to \ud835\udc51 dimensions through a feed-forward layer. Formally, we have the following equation: where \ud835\udc87 \ud835\udc60 , \ud835\udc87 \ud835\udc5f \u2208 R \ud835\udc51 denote the feature representations for the S&R views, respectively, \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5a\ud835\udc5c\ud835\udc56\ud835\udc51 is the sigmoid activation function. Just as we learned the representations for \ud835\udc8a \ud835\udc5f and \ud835\udc8a \ud835\udc60 , we also apply the contrastive loss to \ud835\udc87 \ud835\udc5f and \ud835\udc87 \ud835\udc60 , which is formulated as follows: where \ud835\udc53 + \ud835\udc5f and \ud835\udc53 + \ud835\udc60 denote the positive samples, which are randomly sampled from \ud835\udc87 \ud835\udc5f and \ud835\udc87 \ud835\udc60 within a batch, \ud835\udc53 \ud835\udc58 \ud835\udc5f denotes the negative sample, \ud835\udc41 is the batch size, and \ud835\udf0f is a temperature coefficient.", "3.4 Global Label Space Multi-Task Layer": "In the multi-task layer, to fully utilize label information in S&R, we propose a novel method that jointly models the main task and auxiliary tasks using conditional probability. The main task and auxiliary tasks are scenario-CTR/CTCVR prediction and globalCTR/CTCVR prediction, respectively. We construct a global label denoted as \ud835\udc66 \ud835\udc54 , for a given user \ud835\udc62 and product \ud835\udc5d , which is formulated as follows: where \ud835\udc66 \ud835\udc60 , \ud835\udc66 \ud835\udc5f are the labels of S&R scenario, respectively. In Fig. 1, we have defined the probability of global action (i.e., click and order) as \u02c6 \ud835\udc9a \ud835\udc54 : \ud835\udc5d ( \ud835\udc54\ud835\udc59\ud835\udc5c\ud835\udc4f\ud835\udc4e\ud835\udc59 \ud835\udc4e\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b | \ud835\udc52\ud835\udc65\ud835\udc5d\ud835\udc5c\ud835\udc60\ud835\udc62\ud835\udc5f\ud835\udc52 ) , the probability of action in current scenario given global action as \u02c6 \ud835\udc9a | \ud835\udc54 : \ud835\udc5d ( \ud835\udc60\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc4e\ud835\udc5f\ud835\udc56\ud835\udc5c \ud835\udc4e\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b | \ud835\udc54\ud835\udc59\ud835\udc5c\ud835\udc4f\ud835\udc4e\ud835\udc59 \ud835\udc4e\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b ) , and the probability of action in current scenario as \u02c6 \ud835\udc9a : \ud835\udc5d ( \ud835\udc60\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc4e\ud835\udc5f\ud835\udc56\ud835\udc5c \ud835\udc4e\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b | \ud835\udc52\ud835\udc65\ud835\udc5d\ud835\udc5c\ud835\udc60\ud835\udc62\ud835\udc5f\ud835\udc52 ) . It is obvious that \u02c6 \ud835\udc9a = \u02c6 \ud835\udc9a | \ud835\udc54 \u2217 \u02c6 \ud835\udc9a \ud835\udc54 . The inputs are differentiated as \ud835\udc7d \ud835\udc58 for the main task and \ud835\udc7d \ud835\udc46 & \ud835\udc45 for the auxiliary task, respectively. Both \ud835\udc7d \ud835\udc58 and \ud835\udc7d \ud835\udc46 & \ud835\udc45 include representations of global user interests and scenario-specific features. The difference is that \ud835\udc7d \ud835\udc58 only contains representations of user interest and domain-agnostic features from the current scenario view. However, \ud835\udc7d \ud835\udc46 & \ud835\udc45 includes both S&R views representations, aiming to better predict the global label: where \ud835\udc58 is the scenario indicator. We employ Multi-Layer Perception (MLP) as the main task network and Multi-Scenario Model (MSM) as the auxiliary task network. Then \u02c6 \ud835\udc9a | \ud835\udc54 and \u02c6 \ud835\udc9a \ud835\udc54 can be formulated as follows:", "3.5 Model Optimization": "The objective functions applied in main task and auxiliary task is the cross entropy loss function, defined as: where S is the set of search samples, R is the set of recommendation samples, \u2113 (\u00b7) is the cross-entropy loss function. For sample x in task\ud835\udc57 , \u02c6 \ud835\udc66 \ud835\udc57 x and \u02c6 \ud835\udc66 \ud835\udc57 \ud835\udc54 x are the predictions for the main and auxiliary tasks, \ud835\udc66 \ud835\udc57 x and \ud835\udc66 \ud835\udc57 \ud835\udc54 x are the label and global label, respectively. Then the final objective function of our proposed USR is: where \ud835\udf06 \ud835\udc56 is a hyper-parameter to control the weight of loss and \u02dd 4 \ud835\udc56 = 1 \ud835\udf06 \ud835\udc56 = 1, L \ud835\udc56 and L \ud835\udc53 are the contrastive loss by from Equation 5 and 8.", "4 EXPERIMENT": "", "4.1 Experimental Setup": "4.1.1 Dataset. We collected an industrial dataset from the traffic logs of the 7Fresh S&R system, spanning from Oct. 1 to Dec. 1, 2023. The training dataset includes over 5,304,568 search sessions and 540,471 recommendation sessions. For each session, items that were purchased or clicked were labeled as positive examples for the CTCVR or CTR tasks, and all items that were shown but not purchased or clicked were labeled as negative examples. 4.1.2 Baselines. To show the effectiveness of the proposed USR framework, we apply it to the following Multi-Scenario Learning (MSL) methods: 1) SharedBottom [1], which adopts the multitask learning framework that shares the parameters of the bottom layer and designs scenario-specific parameters for each scenario. 2) MMoE [13], which designs the shared multi-gate mixture experts structure to model task relationships and capture common representations from multiple scenarios. 3) PLE [16], which uses a progressive layered extraction for multi-task learning. Similar to MMoE, we apply scenario-specific experts and towers for each scenario. 4) STAR [14], a novel model with star topology that can cater to multiple domains using a unified model. 5) M2M [23], which designs a meta unit to generate specific parameters to capture scenario-specific knowledge. 6) APG [18], which generates domain-specific parameters using additional networks and uses low-rank decomposition to reduce computation cost. 4.1.3 Evaluation metrics and hyper-parameters. To evaluate the performance of all methods, we adopt the commonly-used AUC (Area Under ROC) as the metric. For a fair comparison, all methods use the same feature embedding size \ud835\udc51 of 32, and employ the AdamW [11] optimizer with an initial learning rate of 1E-4 and a batch size of 1024. The hyper-parameters in USR are set as follows: \ud835\udf0f = 1, \ud835\udf06 1 = 0 . 9, \ud835\udf06 2 = 0 . 08, \ud835\udf06 3 = 0 . 01, \ud835\udf06 4 = 0 . 01. The output sizes of hidden layers in the auxiliary task are set to {128, 64, 32}. 10 5 0 5 15 10 5 0 5 10 15 15 10 5 0 5 10 15 i s & i r without CL 10 5 0 5 10 15 10 5 0 5 10 15 15 10 5 0 5 10 15 i s & i r with CL 20 10 0 10 20 20 10 0 10 20 20 10 0 10 20 30 f s & f r without CL 15 10 5 0 5 10 20 10 0 10 20 20 10 0 10 f s & f r with CL", "4.2 Results and Discussion": "4.2.1 Overall performance . We show experiment results in Table 1, where we find that with the assistance of USR, all methods achieve significant improvements in the CTR and CTCVR tasks within the S&R scenarios. It demonstrates (1) the effectiveness of fine-grained representations for user interests and scenarioagnostic features in S&R views; (2) our proposed method GLMT which exploits global labels in the multi-task layer can be adequately trained using the entire set of S&R label data; (3) the proposed USR is a universal framework that can enhance the performance of many MSL methods in S&R. This promising property encourages the application of USR to various methods within S&R. 4.2.2 Ablation study . We perform an ablation study to demonstrate the effectiveness of the different key components. We use STAR [14] as the backbone network for the multi-scenario model. As shown in Fig. 2 (b), w/o S&R Views IE means removing the S&R Views User Interest Extractor Layer. We can observe that it leads to a significant performance drop, which verifies the importance of extracting and differentiating user interests for S&R views from global user interests. And w/o S&R Views FG means that we remove the S&R Views Feature Generator Layer, and the performance also significantly decreases a lot, indicating the effectiveness of the representations of domain-agnostic features for S&R views. Finally, w/o GLMT means removing the Global Label Space Multi-Task Layer, which brings the most drop in the performance. This shows that using the entire set of S&R label data can significantly benefit the joint modeling of S&R. 4.2.3 In-depth analyses . To better understand the benefits of the contrastive loss, we visualize the learned representations of user interests and scenario-agnostic features from S&R views. Specifically, we use T-SNE [17] to reduce the dimension of the representations to the three-dimensional space and then plot them. The results are shown in Fig. 2 (a), different colors represent different scenarios. Search CTCVR Search CTR Rec CTCVR Rec CTR 0.824 0.826 0.828 0.830 0.832 0.834 0.836 0.838 0.840 AUC (Search) (b) Ablation study of USR w/o S&R Views IE w/o S&R Views FG w/o GLMT our STAR 32 64 96 128 0.826 0.828 0.830 0.832 0.834 0.836 AUC (Search) CTCVR CTR 32 64 96 128 0.66 0.67 0.68 0.69 AUC (Rec) (c) Effect of Dimension d CTCVR CTR 0.64 0.65 0.66 0.67 0.68 0.69 0.70 0.71 AUC (Rec) These results indicate that CL can cause representations from the same scenario to cluster closely together, while pushing those from dissimilar scenarios further apart. Additionally, we analyzed the performance impact of varying embedding dimensions \ud835\udc51 in Fig. 2 (c), and the results show that \ud835\udc51 = 32 yields the best effects for both CTCVR and CTR tasks in S&R scenarios.", "4.3 Online A/B Test": "We conducted a one-week online A/B test on the 7Fresh online fresh retail platform. USR contributed to 2.85% in increase user conversion rate (UCVR) and 1.05% increase in user click through rate (UCTR) in the search scenario, 3.35% in increase UCVR and 1.17% increase in UCTR in the recommendation scenario. The significant improvements achieved by USR confirm its industrial effectiveness. As a result, the proposed USR ranking model has been deployed in the 7Fresh App.", "5 CONCLUSION": "In this work, we propose an effective and universal S&R approach to improve Multi-Scenario Learning within S&R scenarios. We designed two layers to obtain representations of user interests and scenario-agnostic features for S&R views, which precisely capture the differences within S&R scenarios. Additionally, we introduce a global label model method in the multi-task layer to fully utilize the entire set of S&R label data. Experimental results show that with the help of USR, all of the existing MSL models significantly improve, which further promotes the broad applicability of USR. Currently, USR has been fully deployed in the 7Fresh App.", "REFERENCES": "[1] Rich Caruana. 1998. Multitask Learning. In In Learning to Learn . 95-133. [2] Jianxin Chang, Chenbin Zhang, Zhiyi Fu, Xiaoxue Zang, Lin Guan, Jing Lu, Yiqun Hui, Dewei Leng, Yanan Niu, Yang Song, and Kun Gai. 2023. TWIN: TWo-stage Interest Network for Lifelong User Behavior Modeling in CTR Prediction at Kuaishou. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 3785-3794. [3] Jianxin Chang, Chenbin Zhang, Yiqun Hui, Dewei Leng, Yanan Niu, Yang Song, and Kun Gai. 2023. Pepnet: Parameter and embedding personalized network for infusing with personalized prior information. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 3795-3804. [4] Qiwei Chen, Huan Zhao, Wei Li, Pipei Huang, and Wenwu Ou. 2019. Behavior sequence transformer for e-commerce recommendation in alibaba. In Proceedings of the 1st international workshop on deep learning practice for high-dimensional sparse data . 1-4. [5] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A simple framework for contrastive learning of visual representations. In Proceedings of the 37th International Conference on Machine Learning . 1597-1607. [6] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and Hemal Shah. 2016. Wide & Deep Learning for Recommender Systems. In Proceedings of the 1st Workshop on Deep Learning for Recommender Systems . 7-10. [7] John Giorgi, Osvald Nitski, Bo Wang, and Gary Bader. 2021. DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) . 879-895. [8] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. In Proceedings of the 26th International Joint Conference on Artificial Intelligence . 1725-1731. [9] Pengcheng Li, Runze Li, Qing Da, An-Xiang Zeng, and Lijun Zhang. 2020. Improving multi-scenario learning to rank in e-commerce by exploiting task relationships in the label space. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management . 2605-2612. [10] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong Sun. 2018. xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 1754-1763. [11] Ilya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . [12] Ze Lyu, Yu Dong, Chengfu Huo, and Weijun Ren. 2020. Deep match to rank model for personalized click-through rate prediction. In Proceedings of the AAAI Conference on Artificial Intelligence . 156-163. [13] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H. Chi. 2018. Modeling Task Relationships in Multi-task Learning with Multi-gate Mixtureof-Experts. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 1930-1939. [14] Xiang-Rong Sheng, Liqin Zhao, Guorui Zhou, Xinyao Ding, Binding Dai, Qiang Luo, Siran Yang, Jingshan Lv, Chi Zhang, Hongbo Deng, and Xiaoqiang Zhu. 2021. One Model to Serve All: Star Topology Adaptive Recommender for MultiDomain CTR Prediction. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management . 4104-4113. [15] Zihua Si, Zhongxiang Sun, Xiao Zhang, Jun Xu, Xiaoxue Zang, Yang Song, Kun Gai, and Ji-Rong Wen. 2023. When Search Meets Recommendation: Learning Disentangled Search Representation for Recommendation. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1313-1323. [16] Hongyan Tang, Junning Liu, Ming Zhao, and Xudong Gong. 2020. Progressive Layered Extraction (PLE): A Novel Multi-Task Learning (MTL) Model for Personalized Recommendations. In Proceedings of the 14th ACM Conference on Recommender Systems . 269-278. [17] Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing Data using t-SNE. In Journal of Machine Learning Research (2008) . 2579-2605. [18] Bencheng Yan, Pengjie Wang, Kai Zhang, Feng Li, Hongbo Deng, Jian Xu, and Bo Zheng. 2022. APG: Adaptive Parameter Generation Network for ClickThrough Rate Prediction. In Advances in Neural Information Processing Systems . 24740-24752. [19] Xuanhua Yang, Xiaoyu Peng, Penghui Wei, Shaoguo Liu, Liang Wang, and Bo Zheng. 2022. AdaSparse: Learning Adaptively Sparse Structures for Multi-Domain Click-Through Rate Prediction. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management . 4635-4639. [20] Jing Yao, Zhicheng Dou, Ruobing Xie, Yanxiong Lu, Zhiping Wang, and Ji-Rong Wen. 2021. USER: A Unified Information Search and Recommendation Model based on Integrated Behavior Sequence. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management . 2373-2382. [21] Hamed Zamani and W. Bruce Croft. 2018. Joint Modeling and Optimization of Search and Recommendation. In DESIRES'18 . 36-41. [22] Hamed Zamani and W. Bruce Croft. 2020. Learning a Joint Search and Recommendation Model from User-Item Interactions. In Proceedings of the 13th International Conference on Web Search and Data Mining . 717-725. [23] Qianqian Zhang, Xinru Liao, Quan Liu, Jian Xu, and Bo Zheng. 2022. Leaving No One Behind: A Multi-Scenario Multi-Task Meta Learning Approach for Advertiser Modeling. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining . 1368-1376. [24] Jiejie Zhao, Bowen Du, Leilei Sun, Fuzhen Zhuang, Weifeng Lv, and Hui Xiong. 2019. Multiple Relational Attention Network for Multi-task Learning. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 1123-1131. [25] Kai Zhao, Yukun Zheng, Tao Zhuang, Xiang Li, and Xiaoyi Zeng. 2022. Joint Learning of E-commerce Search and Recommendation with a Unified Graph Neural Network. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining . 1461-1469. [26] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Deep interest evolution network for click-through rate prediction. In In Proceedings of the AAAI conference on artificial intelligence . 5941-5948. [27] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep Interest Network for ClickThrough Rate Prediction. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 1059-1068. [28] Jie Zhou, Xianshuai Cao, Wenhao Li, Lin Bo, Kun Zhang, Chuan Luo, and Qian Yu. 2023. Hinet: Novel multi-scenario & multi-task learning with hierarchical information extraction. In 2023 IEEE 39th International Conference on Data Engineering (ICDE) . 2969-2975. [29] Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, and Ji-Rong Wen. 2020. S3-Rec: Self-Supervised Learning for Sequential Recommendation with Mutual Information Maximization. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management . 1893-1902. [30] Xinyu Zou, Zhi Hu, Yiming Zhao, Xuchu Ding, Zhongyi Liu, Chenliang Li, and Aixin Sun. 2022. Automatic expert selection for multi-scenario and multi-task search. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1535-1544."}
