{"CL4CTR: A Contrastive Learning Framework for CTR Prediction": "Fangye Wang \u2217 School of Computer Science Fudan University, Shanghai, China fywang18@fudan.edu.cn", "Yingxu Wang \u2217": "School of Computer Science Fudan University, Shanghai, China yingxuwang20@fudan.edu.cn Dongsheng Li Microsoft Research Asia Shanghai, China dongsli@microsoft.com Hansu Gu \u2020 Seattle, United States hansug@acm.org Tun Lu \u2217\u2020 School of Computer Science Fudan University, Shanghai, China lutun@fudan.edu.cn Ning Gu \u2217 School of Computer Science Fudan University, Shanghai, China ninggu@fudan.edu.cn", "ABSTRACT": "", "KEYWORDS": "Many Click-Through Rate (CTR) prediction works focused on designing advanced architectures to model complex feature interactions but neglected the importance of feature representation learning, e.g., adopting a plain embedding layer for each feature, which results in sub-optimal feature representations and thus inferior CTR prediction performance. For instance, low frequency features, which account for the majority of features in many CTR tasks, are less considered in standard supervised learning settings, leading to sub-optimal feature representations. In this paper, we introduce self-supervised learning to produce high-quality feature representations directly and propose a model-agnostic Contrastive Learning for CTR (CL4CTR) framework consisting of three selfsupervised learning signals to regularize the feature representation learning: contrastive loss, feature alignment, and field uniformity. The contrastive module first constructs positive feature pairs by data augmentation and then minimizes the distance between the representations of each positive feature pair by the contrastive loss. The feature alignment constraint forces the representations of features from the same field to be close, and the field uniformity constraint forces the representations of features from different fields to be distant. Extensive experiments verify that CL4CTR achieves the best performance on four datasets and has excellent effectiveness and compatibility with various representative baselines.", "CCS CONCEPTS": "", "\u00b7 Information systems \u2192 Recommender systems .": "\u2217 Also Shanghai Key Laboratory of Data Science, Fudan University, Shanghai, China. \u2020 Corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WSDM '23, February 27-March 3, 2023, Singapore, Singapore. ACM ISBN 978-1-4503-9407-9/23/02...$15.00 \u00a9 2023 Association for Computing Machinery. https://doi.org/10.1145/3539597.3570372 Contrastive Learning, Representation Learning, CTR Prediction.", "ACMReference Format:": "Fangye Wang, Yingxu Wang, Dongsheng Li, Hansu Gu, Tun Lu, Peng Zhang, and Ning Gu. 2023. CL4CTR: A Contrastive Learning Framework for CTR Prediction. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining (WSDM '23), February 27-March 3, 2023, Singapore, Singapore. ACM, New York, NY, USA, 9 pages. https://doi.org/10. 1145/3539597.3570372", "1 INTRODUCTION": "CTR prediction [7, 43], aiming to predict the probability of a given item being clicked, has been widely used in many applications, e.g., recommender systems [4] and computational advertising [18]. Recently, many methods [1, 33] achieved huge success by modeling complex feature interactions (FI). Following recent works [8, 13, 33], we categorize CTR prediction methods into two types: (1) traditional methods, such as logistic regression (LR) [27] and FM-based models [10, 26], can only model low-order feature interactions; and (2) deep-learning based methods, such as xDeepFM [17] and DCN-V2 [35], can further enhance the accuracy of CTR prediction by capturing high-order FI. In addition, many novel architectures (e.g., self-attention [16, 28], CIN [17], PIN [25]) have been proposed and widely deployed to capture sophisticated arbitrary-order FI. Several prior works [19, 42] have also realized the importance of feature representation learning and proposed to deploy a weight Although successful in performance, many existing CTR prediction methods suffer from an inherent problem: high frequency features have higher chances to be trained than low frequency features , causing the representations of low frequency features to be sub-optimal. In Figure 1, we present the feature cumulative distributions of Frappe and ML-tag datasets. We can observe a clear 'long tail' distribution of feature frequencies, e.g., bottom 80% of features appeared only 38 times or less in the ML-tag dataset. Since most CTR prediction models learn feature representations by the backpropagation [43], the low frequency features cannot be sufficiently trained due to less appearance, resulting in sub-optimal feature representations and thus sub-optimal CTR prediction performance. Peng Zhang \u2217 School of Computer Science Fudan University, Shanghai, China zhangpeng_@fudan.edu.cn 0 200 400 600 800 1000 Feature frequency 0.0 0.2 0.4 0.6 0.8 1.0 Proportion of all features (76,80.12%) (a) Frappe 0 200 400 600 800 1000 Feature frequency 0.0 0.2 0.4 0.6 0.8 1.0 Proportion of all features (38,80.07%) (b) ML-tag learning module (i.e., FEN [42], Dual-FEN [19]) after the embedding layer which assigns weights for each feature to enhance their representations. However, the additional weighting modules will increase the model parameters and inference time. In addition, similar to FI-based methods, these methods only use the supervised learning signals to optimize feature representations from the plain embedding layer, which is not strong enough to produce accurate feature representations. Therefore, in this paper, we focus on directly learning accurate feature representations from the embedding layer without introducing additional weighting mechanisms, which is model-agnostic and has not been extensively studied. Our major contributions are summarized as follows: In this paper, we seek to utilize self-supervised learning (SSL) to address the above issue, in which we design self-supervised learning signals as constraints to regularize the learned feature representations during the training process. As shown in Figure 2, we propose a novel framework called Contrastive Learning for Click Through Rate Prediction ( CL4CTR ), which consists of three key modules: CTR prediction model, contrastive module, and alignment&uniformity constraints. In detail, the CTR prediction model aims to predict the probability of items being clicked by a user, which can be replaced with most existing CTR models in the CL4CTR framework. In the contrastive module, we design three key components: (1) a data augmentation unit aiming to generate two different views for the output embedding as positive training pairs, which includes three different permutation approaches: random mask, feature mask, and dimension mask; (2) a feature interaction encoder aiming to learn compact FI representations based on the perturbed embeddings from the data augmentation unit; and (3) a task-oriented contrastive loss, which is designed to minimize the distance between the positive training pairs. In addition, we introduce two constraints: feature alignment and field uniformity, to facilitate contrastive learning. Feature alignment forces the representations of features from the same field to be as close as possible, and field uniformity forces the representations of features from different fields to be as distant as possible. \u00b7 We propose a model-agnostic contrastive learning framework - CL4CTR, which can directly improve the quality of feature representations in an end-to-end manner. \u00b7 Considering the unique characteristics of CTR prediction tasks, we design three self-supervised learning signals: contrastive loss, feature alignment constraint and field uniformity constraint to improve contrastive learning performance. \u00b7 Extensive experiments on four datasets demonstrate that simply applying CL4CTR into FM [26] can outperform stateof-the-art methods. More importantly, CL4CTR shows high compatibility with existing methods, i.e., it can generally improve the performance of many representative baselines.", "2 RELATED WORK": "", "2.1 Deep CTR Prediction": "According to the main focuses, recent CTR prediction works can be divided into two categories: FI-based methods [35, 43] and user interests modeling based methods [24]. Since our CL4CTR framework can be generally applied in FI-based models, we briefly summarize the FI-based works in this section. Most FI-based CTR prediction methods follow the common design paradigm: embedding layer, FI layer, and prediction layer. Some classical methods can only model fixed-order or low-order feature interactions. For instance, FM-based methods [14, 19, 26] model all pairwise interactions by using factorized parameters. Due to the importance of FI in the CTR prediction, many works focus on how to design novel structures for the FI layer to capture more informative and complicated feature interactions. Wide&Deep (WDL) [4] jointly trains the wide linear unit and Deep Neural Network (DNN) to combine memorization and generalization. DeepFM [7] comprises DNN and FM, and xDeepFM [17] additional proposes Compressed Interaction Network (CIN) based DeepFM to model high-order feature interaction explicitly. DCN [34] and DCN-V2 [35] explicitly and automatically use a cross-vector/cross-matrix network to improve the accuracy and efficiency of the DNN model. Furthermore, attention mechanism is one of the most effective structure to improve the performance and has been widely adopted for different purposes, e.g., AFM [39], Autoint [28], InterCTR [16], DCAP [3]. Notably, some works [13, 19, 42] attempt to improve the performance of CTR prediction by assigning different weights for features, in which they deploy a weight learning module to adjust the importance of feature representations after the embedding layer. However, these additional weighting modules may increase the model parameters and inference time. More importantly, these works only learn feature representations from a plain embedding layer, which is not strong enough to produce accurate feature representations as demonstrated in our experiments. The proposed CL4CTR can directly improve the quality of feature representations without requiring any additional modules after the embedding layer.", "2.2 Self-supervised Learning": "Recently, self-supervised learning has achieved remarkable success in learning powerful representations in many machine learning tasks [2, 6, 9, 15, 31, 40]. Contrastive Learning is one of the mainstream methods in SSL, which learns representations by attracting the positive sample pairs and repulsing the negative sample pairs [8]. Wang and Isola [36] identify two key properties related to the success of contrastive learning, i.e., alignment and uniformity. Alignment favors encoders that assign similar features to similar samples. Uniformity prefers a feature distribution that preserves maximal information. In CTR prediction tasks, contrastive learning has not been extensively studied. Guo et al. [8] focus on sequential-based CTR tasks, which apply interest-level contrastive losses to enhance feature embeddings. Pan et al. [23] propose an auxiliary AQCL loss that automatically leverages instance-instance similarity and instancecluster similarity to regularize feature representations under the cold-start scenarios. Unlike them, our CL4CTR focuses on FI-based CTR prediction models, which can enhance the quality of feature representations by designing three SSL signals: contrastive loss, feature alignment constraint, and field uniformity constraint.", "3 THE CL4CTR FRAMEWORK": "", "3.1 CTR Prediction": "CTR prediction is a binary classification task [23, 33]. Suppose a dataset for training CTR prediction model contains \ud835\udc41 instances ( x , \ud835\udc66 ) , where \ud835\udc66 \u2208 { 0 , 1 } (click or not) is the true label indicating user's click behaviors. Input instance x is usually multi-field tabular data record [8, 20], which contains \ud835\udc39 different fields and \ud835\udc40 features, as shown in Table 1. Recently, as shown in Figure 2(a), many CTR prediction models follow the common design paradigm [33, 37]: embedding layer, FI layer, and prediction layer. Feature interaction layer. The FI layer usually contains various types of interaction operations to capture arbitrary-order feature interactions, such as MLP [4, 7], Cross Network [34], Cross Network2 [35] and transformer layer [16, 28], etc. We refer to these structures as feature interaction encoders, represented by \ud835\udc39\ud835\udc3c (\u00b7) . \ud835\udc39\ud835\udc3c (\u00b7) can generate a compact feature interaction representation h \ud835\udc56 based on embedding matrix E . Embedding layer. Generally, each input instance x \ud835\udc56 is a sparse high-dimensional vector represented by a one-hot vector [18, 33]. And embedding layer transforms the sparse high-dimensional features x \ud835\udc56 into a dense low-dimensional embedding matrix E = [ e 1 ; e 2 ; ... ; e \ud835\udc39 ] \u2208 R \ud835\udc39 \u00d7 \ud835\udc37 , where \ud835\udc37 is the dimension size. Additionally, we use E = [ E 1 , E 2 , ..., E \ud835\udc39 ] \u2208 R \ud835\udc40 \u00d7 \ud835\udc37 to represent all feature representations, where E \ud835\udc53 is the subset representation of the \ud835\udc53 -th field \ud835\udc53 \u2208 { 1 , 2 , ..., \ud835\udc39 } . | E \ud835\udc53 | is the number of features belonging to field \ud835\udc53 , and \ud835\udc40 = \u02dd \ud835\udc39 \ud835\udc53 = 1 | E \ud835\udc53 | . Prediction layer. Finally, a prediction layer (usually a linear regression or MLP module) produces the final prediction probability \ud835\udf0e ( ^ \ud835\udc66 \ud835\udc56 ) \u2208 [ 0 , 1 ] based on the compact representations h \ud835\udc56 from the FI layer, where \ud835\udf0e ( \ud835\udc65 ) = 1 /( 1 + exp (-\ud835\udc65 )) is the sigmoid function. Finally, with the predicted label ^ \ud835\udc66 \ud835\udc56 and the true label \ud835\udc66 \ud835\udc56 , the commonly adopted loss function of CTR models is as follows: Contrastive learning. As shown in Figure 2, in addition to the above components, we propose three contrastive learning signals: contrastive loss, feature alignment constraint and field uniformity constraint on top of the embedding layer to regularize the representation learning. Since these signals are not necessary during model inference, our method will not increase the inference time and the parameters of the underlying CTR prediction models.", "3.2 Contrastive Module": "Inspired by the success of SSL, we seek to deploy contrastive learning in the CTR prediction tasks to generate high-quality feature representations. As illustrated in Figure 2(b), the contrastive module consists of three major components: a data augmentation unit, a FI encoder, and a contrastive loss function. In the data augmentation unit, we propose three different task-oriented posterior embedding augmentation techniques to generate positive training pairs, i.e., two different views of each feature embedding. Then we feed the two perturbed embeddings to the same FI encoder to generate two compressed feature representations. Finally, the contrastive loss is applied to minimize the distance between the two compressed feature representations. 3.2.1 Data Augmentation via Output Perturbation. Data augmentation has shown great potential in improving the performance of feature representations in SSL [41]. Different and well-designed augmentation approaches have been proposed and used to construct different views of the same input instance. For example, in the scenarios of sequential recommendation, three widely used augmentation methods are item masking, reordering, and cropping [41]. However, these methods are designed to augment behavior sequences and are not appropriately deployed in FI-based CTR prediction models. Hence, we firstly propose three task-oriented augmentation approaches, which aim to perturb feature embeddings for FI-based models. As shown in Figure 2(d), we use the function ^ E = g ( E ) to represent data augmentation process. Random Mask. Firstly, we introduce the random mask method, which is analogous to Dropout [11]. This method randomly masks some elements in initial embedding E with a certain probability p . The random mask is generated as follows: Bernoulli (\u00b7) is the Bernoulli distribution, and I is a matrix of Bernoulli random variables each of which has probability \ud835\udc5d of being 1. Feature Mask. Motivated by prior works [18, 28], we propose to mask the feature information in the initial embedding, where the feature mask can be generated as follows: where we set a proportion \ud835\udc5d of features T = ( \ud835\udc61 1 , \ud835\udc61 2 , ..., \ud835\udc61 \ud835\udc3f \ud835\udc39 ) with the length \ud835\udc3f \ud835\udc39 = \u230a \ud835\udc5d \u2217 \ud835\udc39 \u230b . \ud835\udc61 \ud835\udc53 is the index of feature in E . If one feature is masked, then the representation of this feature will be replaced with [mask], which is a zero vector. DimensionMask. Thedimensions of feature representations affect the effectiveness of deep learning models. Inspired by FED [44], which attempts to improve prediction performance by capturing dimension relations, we propose to perturb the initial embedding Perturbation g 1 (\u00b7) FI Encoder FI !\" (\u00b7) FI Encoder FI !\" (\u00b7) shared Projector \ud835\udc5d # (\u00b7) Projector \ud835\udc5d $ (\u00b7) Contrastive loss Perturbation g 2 (\u00b7) Embedding Layer FI Layer FI (\u00b7) Prediction Layer CTR Loss (a) Basic CTR Model (b) Contrastive Module (d) Embedding Perturbation Output Embedding Dimension Mask Feature Mask Random Mask Alignment& Uniformity loss Feature alignment Field uniformity (c) Alignment&Uniformity Constraints by replacing specific proportions of dimensional information of feature representations, which can be described as follows:", "3.3 Feature Alignment and Field Uniformity": "where \ud835\udc51 is a vector of Bernoulli random variables, each of which has a probability \ud835\udc5d of being 1. During the training process, we select one of the above mask methods to generate two perturbed embedding ^ E 1 and ^ E 2 , where ^ E 1 = g ( E ) and ^ E 2 = g ( E ) in Figure 2(b). More analyses about the effectiveness of different mask methods are showed in Section 4.3. 3.2.2 Feature Interaction Encoder. We utilize a shared FI encoder to extract feature interaction information from the two perturbed embeddings ^ E 1 and ^ E 2 as follows: \ud835\udc39\ud835\udc3c \ud835\udc50\ud835\udc59 (\u00b7) represents FI encoder function, and \u210e 1 , \u210e 2 are two compressed representations generated from two perturbed embeddings. Notably, any FI encoder can be deployed in our CL4CTR, such as cross-network [35], self-attention [28], and bi-interaction [10], as described in Section 3.1. Specifically, we select the Transformer layer [30] as our primary FI encoder, which is widely used to extract vector-level relationships between features [3, 28, 40]. Additionally, we find that the dimensions of compressed representations ( \u210e 1 , \u210e 2 ) generated by some FI encoders (e.g., cross network [35], PIN [25]) could be huge, e.g., over thousands when field F is large, which produce adversely impacts on the training stability. Hence, we utilize a projection function to reduce the dimensions of representations from FI encoder to D as follows: The projection function \ud835\udc5d (\u00b7) is a single layer MLP. 3.2.3 Contrastive Loss Function. Finally, a contrastive loss function is applied to minimize the expected distance between the above two perturbed representations as follows: B is the batch size and | | \u00b7 | | 2 2 denotes the \u2113 2 distance. To ensure low frequency features and high-frequency features be trained equally, a naive way is to increase the frequency of low frequency features or reduce the frequency of high-frequency features during training. Inspired by previous works [32, 36] in other areas (CV, NLP), which can achieve similar goal by introducing two critical properties, named the alignment and uniformity constraints, but they need to construct positive and negative sample pairs to optimize the two constraints. In CTR prediction tasks, we find that features in the same field are analogous to positive sample pairs, and features of different fields are analogous to negative sample pairs. Thus, we propose two new properties for contrastive learning in CTR prediction, named feature alignment and field uniformity, which can regularize feature representations during training process. Specifically, feature alignment pulls the representations of features from the same field to be as close as possible. In contrast, field uniformity pushes representations of features from different fields to be as distant as possible. 3.3.1 Feature Alignment. Firstly, we introduce the feature alignment constraint, which aims to minimize the distance between features from the same field. Intuitively, by adding a feature alignment constraint, the representations of features in the same field should be more closely distributed in the low-dimensional space. Formally, the loss function of feature alignment is as follows: where e i and e j are two features from the same field, and E \ud835\udc53 is the subset features of field \ud835\udc53 . 3.3.2 Field Uniformity. The relationships among different fields have not been extensively studied in existing CTR prediction methods. For instance, FFM [14] learns field-aware representation for each feature, and NON [20] extracts intra-field information, but their techniques cannot be directly applied in contrastive learning. Differently, we introduce field uniformity to optimize feature representation directly, which minimizes the similarity between features belonging to different fields. The loss function of field uniformity is formally defined as follows: Similar to the other approaches [41, 46], we use cosine similarity to regularize negative sample pairs, i.e., \ud835\udc60\ud835\udc56\ud835\udc5a ( e i , e j ) = e i \ud835\udc47 e j /\u2225 e i \u2225\u2225 e j \u2225 . Other similarity functions can also be used here. E -E \ud835\udc53 contains all features except those from field \ud835\udc53 . In both feature alignment and field uniformity constraints, we find that low frequency features and high frequency features have equal chances to be considered. Therefore, the suboptimal representation issue for low frequency features can be largely alleviated when the two constraints are introduced during training.", "3.4 Multi-task Training": "To integrate the CL4CTR framework into the scenarios of CTR prediction, we adopt a multi-task training strategy to jointly optimize these three auxiliary SSL losses and the original CTR prediction loss in an end-to-end manner. Thus the final objective function can be formulated as follows: where \ud835\udefc and \ud835\udefd are the hyper-parameters to control the strengths of contrastive loss and feature alignment and field uniformity loss.", "4 EXPERIMENTS": "", "4.1 Experimental Setup": "4.1.1 Datasets. We evaluate CL4CTR on four popular datasets: Frappe 1 [10], ML-tag 2 [10], SafeDriver 3 [12] and ML-1M 4 [3]. The statistics of the four datasets are presented in Table 2. NFM [10] and AFM [39] have strictly split Frappe and ML-tag to training, validation, and testing by 7:2:1, and we directly follow their settings. For SafeDriver and ML-1M, following [12] and [3], we randomly split the instances by 8:1:1. Detailed descriptions of those datasets can be found in the links or references. 4.1.2 ComparedMethods. Toevaluate the proposed CL4CTR framework, we compare its performance with four classes of representative CTR methods [21]. 1) First-order method that is a weighted sum of raw features, including LR ; 2) FM-based methods that consider second-order FI, including FM [26], FwFM [22], IFM [42], and FmFM [29]; 3) Approaches that model higher-order FI, including CrossNet [34], IPNN [25], OPNN [25], FINT [45], and DCAP [3]; 4) Ensemble methods or multi-tower structures, including WDL [4], DCN [34], DeepFM [7], xDeepFM [17], FiBiNET [13], AutoInt+ [28], AFN+ [5], TFNET [38], FED [44], and DCN-V2 [35]. The proposed CL4CTR framework is model-agnostic. For simplicity, a base model M equipped with CL4CTR is represented as \ud835\udc36\ud835\udc3f 4 \ud835\udc36\ud835\udc47\ud835\udc45 M . We choose FM [26] as the basic model to verify the effectiveness of CL4CTR, which only models second-order FI and has no additional parameters except for feature representations. Therefore, the performance boost of \ud835\udc36\ud835\udc3f 4 \ud835\udc36\ud835\udc47\ud835\udc45 \ud835\udc39\ud835\udc40 directly reflects the quality of the feature representations. CL4CTR only helps the base CTR models training and does not add any operation or parameter to the inference process. 4.1.3 Evaluation Metrics. To evaluate the performance of all methods, we adopt the commonly-used AUC (Area Under ROC) and Logloss (cross entropy) as the metrics. Notably, a slightly higher AUC or a lower Logloss at 0.001 -level can be considered significant for CTR prediction [1, 13, 17, 20, 35]. 4.1.4 Implementation Details. For fair comparisons, we implement all the models with Pytorch 5 and optimize all models with Adam. The embedding size is set to 64 for Frappe and ML-tag and 20 for ML-1M and SafeDriver, respectively. Meanwhile, the batch size is fixed to 1024. the learning rate is 0.01 for SafeDriver and 0.001 for other datasets. As for the models including DNN in the prediction layer, we adopt the same structure { 400,400,400,1 } . All the activation functions are ReLU, and the dropout rate is 0.5. We perform early stopping according to AUC on the validation set to avoid overfitting. We also implement the Reduce-LR-On-Plateau scheduler to reduce the learning rate by a factor of 10 when the given metric stops improving within four continuous epochs. Each experiment is repeated 5 times, and the average results are reported. In CL4CTR, \ud835\udc39\ud835\udc3c \ud835\udc50\ud835\udc59 (\u00b7) adopts three transformer layers. And we use hyper-parameters: \ud835\udefc =1, \ud835\udefd =0.01 in the final loss function.", "4.2 Overall Comparison": "In this section, we compare the performances of \ud835\udc36\ud835\udc3f 4 \ud835\udc36\ud835\udc47\ud835\udc45 \ud835\udc39\ud835\udc40 with the state-of-the-art (SOTA) CTR prediction models. Table 3 shows the experimental results of all compared models over four datasets. \ud835\udc36\ud835\udc3f 4 \ud835\udc36\ud835\udc47\ud835\udc45 \ud835\udc39\ud835\udc40 consistently performs better than all baselines on all datasets. Furthermore, \ud835\udc36\ud835\udc3f 4 \ud835\udc36\ud835\udc47\ud835\udc45 \ud835\udc39\ud835\udc40 significantly outperforms the strongest baseline DCN-V2 [35] by 0.13%, 0.11%, 0.39% and 0.67% in terms of AUC (16.10%, 8.41%, 0.64% and 1.46% in terms of Logloss) on Frappe, ML-tag, ML-1M, and SafeDriver respectively. Additionally, we find that the improvements on Logloss are more remarkable than those on AUC, indicating that CL4CTR enables us to predict the true click probability effectively. Meanwhile, \ud835\udc36\ud835\udc3f 4 \ud835\udc36\ud835\udc47\ud835\udc45 \ud835\udc39\ud835\udc40 shows It can be observed that LR and FM achieve the worst performance compared with other baselines, which indicates that shallow models are insufficient for CTR prediction. Other FM-based models improve FM by introducing field importance mechanism (e.g., FwFM [22] and IFM [42]) or deploying a novel field-pair matrix approach (e.g., FmFM [29]). Generally, deep-learning based models (e.g., DeepFM [7], DCN [34], DCN-V2 [35]), which combine highorder feature interactions with well-designed feature interaction structures, achieve better performance than FM. strong generalization ability on all datasets, where Table 3 shows the averaged performance boost ( \u0394 AUC and \u0394 Logloss). Notably, most SOTA CTR prediction models design complex networks to produce advanced feature representations and useful feature interactions to improve the performance. However, our CL4CTR only helps FM to learn accurate feature representations from the embedding layer with contrastive learning instead of introducing extra modules. The improvement of our CL4CTR verifies the necessity of learning accurate feature representations in CTR prediction tasks.", "4.3 Ablation Study": "4.3.1 Compatibility Analysis. Toverify the compatibility of CL4CTR, we deploy it into other SOTA models, such as DeepFM [7], Autoint+ [28], and DCN-V2 [35]. The results are shown in Table 4. Firstly, learning feature representation with CL4CTR can significantly improve the performance of CTR prediction. Applied with CL4CTR, the performance of base models is remarkably boosted, which confirms our hypothesis of improving the performance of CTR prediction models by improving the quality of the feature representations and demonstrates the effectiveness of CL4CTR. In addition, the experimental results show that learning high-quality feature representations is at least as important as designing advanced FI techniques. Modeling complex feature interactions can improve the performance of CTR models when these models leverage supervised signal for training, which explains why FI-based models outperform FM. However, after introducing self-supervised signals into CTR models for learning high-quality feature representations, FM can achieve the best performance compared with other models deployed with CL4CTR. The possible reason is that both supervised and self-supervised learning are directly and only optimizing the parameters in feature representations in FM without disturbing by other parameters. 4.3.2 Data Augmentation Approaches. To verify the effectiveness of our proposed data augmentation methods, we change the augmentation methods in the contrastive module and fix other settings for a fair comparison. Furthermore, we select different baseline models and deploy CL4CTR into them to compare their performance under this setting. Table 5 shows the experimental results. The random mask method achieves the best performance in most cases. We think the random mask is more moderated than feature mask and dimension mask because it omits element information. Additionally, the FwFM model achieves the best performance with the feature mask method on Frappe; in contrast, it achieves the best performance with the dimension mask method on SafeDriver, demonstrating that our proposed augmentation methods are effective and can be used in different baseline models and datasets. 4.3.3 FI Encoder. In the contrastive module, the FI encoder also affects the performance of CL4CTR, as different structures of the FI encoder extract different information. For instance, transformer layer can model high-order feature interactions in feature-level [3, 28] explicitly, however, CrossNet2 [35] focuses on modelling boundeddegree feature interactions in element-level explicitly. DNN is a common and widely used structure in most CTR models for modeling bit-level feature relationships implicitly. We select the above three representative structures as FI encoders and verify their performance. Notably, we adopt three layers structure as reported in their paper. Table 6 shows the experimental results. It can be observed that CL4CTR can consistently improve the performance of these baseline models with different FI encoders. In addition, since different FI encoders extract different information based on specific base models and datasets, the performance of these models is different. However, CL4CTR with transformer layers 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 (0,5) [5,10) [10,15) [15,20) [20,25) [25,30) [30,35) [35,40) [40,\u221e) \u0394Logloss Logloss Frequency threshold FM AFN+ DeepFM DCN-V2 CL4CTR_FM \u0394Logloss achieves the best performance in most cases since the transformer layer is a more effective solution than others. 4.3.4 Loss Function. In this section, we evaluate the effectiveness of self-supervised learning signals ( i.e., L \ud835\udc50\ud835\udc59 , L \ud835\udc4e , L \ud835\udc62 ) by eliminating them from CL4CTR respectively. We still regard L \ud835\udc4e and L \ud835\udc62 as a whole part. The experimental results are shown in Table 7. Compared with individual SSL signals, we find that training with all of them can always achieve the best performance. Furthermore, adopting L \ud835\udc4e and L \ud835\udc62 in FM consistently outperforms adopting L \ud835\udc50\ud835\udc59 in FM on two datasets evaluated by Logloss, which indicates that inducing feature alignment and field uniformity into CTR prediction models enables us to predict probabilities closer to the true label. Firstly, we can find that each self-supervised learning signal deployed in baseline models can improve their performance. In addition, by comparing the contrastive loss and alignment&uniformity constraints individually, we conclude that they play different roles in different datasets and baseline models. Specifically, FM with L \ud835\udc4e and L \ud835\udc62 perform better than FM with L \ud835\udc50\ud835\udc59 ; in contrast, DCN with L \ud835\udc4e and L \ud835\udc62 perform worse than DCN with L \ud835\udc50\ud835\udc59 , which verifies our hypothesis. Furthermore, all experiments achieve the best performance when L \ud835\udc50\ud835\udc59 , L \ud835\udc4e , and L \ud835\udc62 are deployed simultaneously.", "4.4 Feature Frequency Analysis": "To verify the effects of feature frequency on different models, we divide the test set of ML-tag according to feature frequency and calculate the corresponding Logloss, where \u0394 \ud835\udc3f\ud835\udc5c\ud835\udc54\ud835\udc59\ud835\udc5c\ud835\udc60\ud835\udc60 represents the improvement over the base FM model after applying CL4CTR. Figure 3 shows the experimental results. 1 1e-1 1e-2 1e-3 1e-4 1e-5 0 \u03b1 (for \ue238 cl ) 1 1e-1 1e-2 1e-3 1e-4 1e-5 0 \u03b2 (for \ue238 a + \ue238 u ) 0.9341 0.9336 0.9329 0.9326 0.9325 0.9328 0.9328 0.9716 0.9702 0.9696 0.9698 0.9700 0.9695 0.9703 0.9822 0.9805 0.9800 0.9795 0.9795 0.9796 0.9794 0.9813 0.9806 0.9784 0.9777 0.9771 0.9764 0.9785 0.9812 0.9802 0.9771 0.9752 0.9752 0.9759 0.9754 0.9813 0.9804 0.9761 0.9751 0.9755 0.9757 0.9749 0.9812 0.9796 0.9768 0.9762 0.9742 0.9745 0.9746 0.94 0.95 0.96 0.97 0.98 (a) The AUC of Frappe 1 1e-1 1e-2 1e-3 1e-4 1e-5 0 \u03b1 (for \ue238 cl ) 1 1e-1 1e-2 1e-3 1e-4 1e-5 0 \u03b2 (for \ue238 a + \ue238 u ) 0.7927 0.7911 0.7895 0.7914 0.7907 0.7902 0.7907 0.7924 0.7919 0.7933 0.7930 0.7945 0.7915 0.7921 0.8164 0.8152 0.8146 0.8140 0.8141 0.8129 0.8139 0.8105 0.8066 0.8064 0.8067 0.8047 0.8044 0.8043 0.8101 0.8057 0.8043 0.8038 0.8020 0.8032 0.8026 0.8099 0.8049 0.8030 0.8033 0.8028 0.8037 0.8031 0.8102 0.8061 0.8046 0.8048 0.8034 0.8025 0.8023 0.790 0.795 0.800 0.805 0.810 0.815 (b) The AUC of ML-1M Firstly, the low frequency features adversely affect the accuracy of the base model. Specifically, we show the performance of FM, three SOTA models (AFN+, DeepFM, DCN-V2), and \ud835\udc36\ud835\udc3f 4 \ud835\udc36\ud835\udc47\ud835\udc45 \ud835\udc39\ud835\udc40 in different frequency ranges. It can be observed that all models perform the worst when the input subset contains low frequency features. With the increasing of feature frequency, the performance of all models improves consistently. When the feature frequency is over 20, the performance of all models becomes stable. Figure 3 confirms our hypothesis that only using the back-propagation to learn the representations of low frequency features with a single supervised signal cannot achieve optimal performance. Secondly, CL4CTR can effectively alleviate the negative effects caused by low frequency features and keep achieving the best performance among different feature frequency ranges. By applying the alignment&uniformity constraints, we ensure the low frequency features can be optimized in each back-propagation process with equal chances to high frequency features. Additionally, the contrastive module can also improve the quality of the representations of all features, including both low and high frequency features.", "4.5 Hyper-parameter Analysis": "4.5.1 Impact of the Weights in the Loss Function. We further investigate the impact of different weights ( \ud835\udefc and \ud835\udefd in Equation 10). We tune both \ud835\udefc and \ud835\udefd from { 1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 0 } . We keep other settings fixed for fair comparison. Figure 4 shows the experimental results. Additionally, the trend of Logloss is consistent with AUC on those two datasets. Overall, CL4CTR achieves the best performance when \ud835\udefc is 1, and \ud835\udefd is 1e-2 for Frappe and ML-1M datasets. Specifically, the performance of CL4CTR deteriorates when \ud835\udefc is less than 1. In addition, when \ud835\udefd is over 1e-2 (i.e., 1 or 1e-1), the performance of CL4CTR is significantly reduced. Meanwhile, CL4CTR performs worse with lower \ud835\udefc and \ud835\udefd (lower right corners). 4.5.2 Mask Proportion. We change the mask proportion \ud835\udc5d within the range (0, 1) with a step size of 0.1. Note that the mask proportion is only applied in L \ud835\udc50\ud835\udc59 . Figure 5 shows the results. For models with L \ud835\udc50\ud835\udc59 , their performance shows similar trends on Frappe and ML-1M. When the mask proportion is around 0.4 or 0.5, \ud835\udc36\ud835\udc3f 4 \ud835\udc36\ud835\udc47\ud835\udc45 \ud835\udc39\ud835\udc40 can achieve the best performance. Specifically, the model performance decreases slightly when smaller mask proportions (i.e., 0.1 to 0.3) are chosen. When mask proportion is over 0.5, the model performance decreases consistently, which is because the FI encoders only use a small percentage of information to produce valid interaction representations for calculating contrastive loss with higher mask proportions. 0.5100 0.5130 0.5160 0.5190 0.5220 0.5250 0.5280 0.8050 0.8070 0.8090 0.8110 0.8130 0.8150 0.8170 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Logloss AUC Mask proportion 0.1250 0.1300 0.1350 0.1400 0.1450 0.1500 0.1550 0.9785 0.9795 0.9805 0.9815 0.9825 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Logloss AUC Mask proportion (a) Frappe (b) ML-1M 0.100 0.130 0.160 0.190 0.220 0.970 0.973 0.976 0.979 0.982 16 32 48 64 Logloss AUC Embedding Size (Frappe) 0.200 0.220 0.240 0.260 0.280 0.300 0.930 0.940 0.950 0.960 0.970 16 32 48 64 Logloss AUC Embedding Size (ML-tag) FMAUC CL4CTR FM AUC FM Logloss CL4CTR FM Logloss 4.5.3 Embedding Size. We change the embedding size from 16 to 64 with a step of 16 in the embedding layer and show the experimental results in Figure 6. It can be observed that the performance of \ud835\udc36\ud835\udc3f 4 \ud835\udc36\ud835\udc47\ud835\udc45 \ud835\udc39\ud835\udc40 is improved substantially with the embedding sizes increasing. Meanwhile, CL4CTR can improve the performance of FM with all embedding sizes. Furthermore, compared with the embedding size of 64 in FM, \ud835\udc36\ud835\udc3f 4 \ud835\udc36\ud835\udc47\ud835\udc45 \ud835\udc39\ud835\udc40 achieves better performance with a small size of 16. This means we can reduce the parameters while achieving better results by applying CL4CTR on FM.", "5 CONCLUSION": "In this paper, we propose a novel framework named Contrastive Learning for CTR prediction (CL4CTR), which directly improves the quality of feature representations, especially for low frequency features. In CL4CTR, we introduce a contrastive module to improve the quality and generalizability of the feature representations by fully utilizing the self-supervised signals from the features. Furthermore, considering the unique characteristics of CTR prediction tasks, we propose two constraints in contrastive learning: feature alignment and feature uniformity, which are used to regularize feature representations. The extensive experimental results demonstrate the excellent effectiveness and compatibility of our proposed CL4CTR on four public datasets.", "ACKNOWLEDGMENTS": "This work was supported by the National Natural Science Foundation of China (NSFC) under Grants 62172106 and 61932007.", "REFERENCES": "[1] Bo Chen, Yichao Wang, Zhirong Liu, Ruiming Tang, Wei Guo, Hongkun Zheng, Weiwei Yao, Muyu Zhang, and Xiuqiang He. 2021. Enhancing explicit and implicit feature interactions via information sharing for parallel deep CTR models. In CIKM . 3757-3766. [3] Zekai Chen, Fangtian Zhong, Zhumin Chen, Xiao Zhang, Robert Pless, and Xiuzhen Cheng. 2021. DCAP: Deep Cross Attentional Product Network for User Response Prediction. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management . 221-230. [2] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A simple framework for contrastive learning of visual representations. In International conference on machine learning . PMLR, 1597-1607. [4] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems . 7-10. [6] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple Contrastive Learning of Sentence Embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing . 6894-6910. [5] Weiyu Cheng, Yanyan Shen, and Linpeng Huang. 2020. Adaptive factorization network: Learning adaptive-order feature interactions. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 34. 3609-3616. [7] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. In Proceedings of the 26th International Joint Conference on Artificial Intelligence . [9] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition . 9729-9738. [8] Wei Guo, Can Zhang, Zhicheng He, Jiarui Qin, Huifeng Guo, Bo Chen, Ruiming Tang, Xiuqiang He, and Rui Zhang. 2022. Miss: Multi-interest self-supervised learning framework for click-through rate prediction. In 2022 IEEE 38th International Conference on Data Engineering (ICDE) . IEEE, 727-740. [10] Xiangnan He and Tat-Seng Chua. 2017. Neural factorization machines for sparse predictive analytics. In Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval . 355-364. [12] Tongwen Huang, Qingyun She, Zhiqiang Wang, and Junlin Zhang. 2020. GateNet: Gating-Enhanced Deep Network for Click-Through Rate Prediction. arXiv preprint arXiv:2007.03519 (2020). [11] Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. 2012. Improving neural networks by preventing coadaptation of feature detectors. arXiv preprint arXiv:1207.0580 (2012). [13] Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. 2019. FiBiNET: combining feature importance and bilinear feature interaction for click-through rate prediction. In Proceedings of the 13th ACM Conference on Recommender Systems . 169-177. [15] Dongha Lee, SeongKu Kang, Hyunjun Ju, Chanyoung Park, and Hwanjo Yu. 2021. Bootstrapping user and item representations for one-class collaborative filtering. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval . 317-326. [14] Yuchin Juan, Yong Zhuang, Wei-Sheng Chin, and Chih-Jen Lin. 2016. Fieldaware factorization machines for CTR prediction. In Proceedings of the 10th ACM Conference on Recommender Systems . 43-50. [16] Zeyu Li, Wei Cheng, Yang Chen, Haifeng Chen, and Wei Wang. 2020. Interpretable click-through rate prediction through hierarchical attention. In Proceedings of the 13th International Conference on Web Search and Data Mining . 313-321. [18] Bin Liu, Ruiming Tang, Yingzhi Chen, Jinkai Yu, Huifeng Guo, and Yuzhou Zhang. 2019. Feature generation by convolutional neural network for click-through rate prediction. In The World Wide Web Conference . 1119-1129. [17] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong Sun. 2018. xdeepfm: Combining explicit and implicit feature interactions for recommender systems. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 1754-1763. [19] Wantong Lu, Yantao Yu, Yongzhe Chang, Zhen Wang, Chenhui Li, and Bo Yuan. 2020. A Dual Input-aware Factorization Machine for CTR Prediction. In IJCAI . 3139-3145. [21] Ze Meng, Jinnian Zhang, Yumeng Li, Jiancheng Li, Tanchao Zhu, and Lifeng Sun. 2021. A general method for automatic discovery of powerful interactions in click-through rate prediction. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1298-1307. [20] Yuanfei Luo, Hao Zhou, Wei-Wei Tu, Yuqiang Chen, Wenyuan Dai, and Qiang Yang. 2020. Network on network for tabular data classification in real-world applications. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . 2317-2326. [22] Junwei Pan, Jian Xu, Alfonso Lobos Ruiz, Wenliang Zhao, Shengjun Pan, Yu Sun, and Quan Lu. 2018. Field-weighted factorization machines for click-through rate prediction in display advertising. In Proceedings of the 2018 World Wide Web Conference . 1349-1357. [23] Yujie Pan, Jiangchao Yao, Bo Han, Kunyang Jia, Ya Zhang, and Hongxia Yang. 2021. Click-through Rate Prediction with Auto-Quantized Contrastive Learning. arXiv preprint arXiv:2109.13921 (2021). [25] Yanru Qu, Bohui Fang, Weinan Zhang, Ruiming Tang, Minzhe Niu, Huifeng Guo, Yong Yu, and Xiuqiang He. 2018. Product-based neural networks for user response prediction over multi-field categorical data. ACM Transactions on Information Systems (TOIS) 37, 1 (2018), 1-35. [24] Jiarui Qin, Weinan Zhang, Xin Wu, Jiarui Jin, Yuchen Fang, and Yong Yu. 2020. User behavior retrieval for click-through rate prediction. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . 2347-2356. [26] Steffen Rendle. 2012. Factorization machines with libfm. ACM Transactions on Intelligent Systems and Technology (TIST) 3, 3 (2012), 1-22. [28] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, and Jian Tang. 2019. Autoint: Automatic feature interaction learning via selfattentive neural networks. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management . 1161-1170. [27] Matthew Richardson, Ewa Dominowska, and Robert Ragno. 2007. Predicting clicks: estimating the click-through rate for new ads. In Proceedings of the 16th international conference on World Wide Web . 521-530. [29] Yang Sun, Junwei Pan, Alex Zhang, and Aaron Flores. 2021. Fm2: Field-matrixed factorization machines for recommender systems. In Proceedings of the Web Conference 2021 . 2828-2837. [31] Vikas Verma, Thang Luong, Kenji Kawaguchi, Hieu Pham, and Quoc Le. 2021. Towards domain-agnostic contrastive learning. In International Conference on Machine Learning . PMLR, 10530-10541. [30] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems . 5998-6008. [32] Feng Wang and Huaping Liu. 2021. Understanding the behaviour of contrastive loss. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition . 2495-2504. [34] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network for ad click predictions. In Proceedings of the ADKDD'17 . 1-7. [33] Fangye Wang, Yingxu Wang, Dongsheng Li, Hansu Gu, Tun Lu, Peng Zhang, and Ning Gu. 2022. Enhancing CTR Prediction with Context-Aware Feature Representation Learning. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval . [35] Ruoxi Wang, Rakesh Shivanna, Derek Z Cheng, Sagar Jain, Dong Lin, Lichan Hong, and Ed H Chi. 2020. DCN-M: Improved Deep & Cross Network for Feature Cross Learning in Web-scale Learning to Rank Systems. arXiv preprint arXiv:2008.13535 (2020). [37] Zhiqiang Wang, Qingyun She, and Junlin Zhang. 2021. MaskNet: introducing feature-wise multiplication to CTR ranking models by instance-guided mask. arXiv preprint arXiv:2102.07619 (2021). [36] Tongzhou Wang and Phillip Isola. 2020. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International Conference on Machine Learning . PMLR, 9929-9939. [38] Shu Wu, Feng Yu, Xueli Yu, Qiang Liu, Liang Wang, Tieniu Tan, Jie Shao, and Fan Huang. 2020. TFNet: Multi-Semantic Feature Interaction for CTR Prediction. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . 1885-1888. [40] Xu Xie, Fei Sun, Zhaoyang Liu, Shiwen Wu, Jinyang Gao, Jiandong Zhang, Bolin Ding, and Bin Cui. 2022. Contrastive learning for sequential recommendation. In 2022 IEEE 38th International Conference on Data Engineering (ICDE) . IEEE. [39] Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua. 2017. Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks. In IJCAI . [41] Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Jundong Li, and Zi Huang. 2022. Self-Supervised Learning for Recommender Systems: A Survey. arXiv preprint arXiv:2203.15876 (2022). [43] Weinan Zhang, Jiarui Qin, Wei Guo, Ruiming Tang, and Xiuqiang He. 2021. Deep learning for click-through rate estimation. In IJCAI . [42] Yantao Yu, Zhen Wang, and Bo Yuan. 2019. An Input-aware Factorization Machine for Sparse Prediction. In IJCAI . 1466-1472. [44] Zihao Zhao, Zhiwei Fang, Yong Li, Changping Peng, Yongjun Bao, and Weipeng Yan. 2020. Dimension Relation Modeling for Click-Through Rate Prediction. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management . 2333-2336. [46] Xin Zhou, Aixin Sun, Yong Liu, Jie Zhang, and Chunyan Miao. 2021. SelfCF: A Simple Framework for Self-supervised Collaborative Filtering. arXiv preprint arXiv:2107.03019 (2021). [45] Zhishan Zhao, Sen Yang, Guohui Liu, Dawei Feng, and Kele Xu. 2022. FINT: Field-Aware Interaction Neural Network for Click-Through Rate Prediction. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) . IEEE, 3913-3917."}
