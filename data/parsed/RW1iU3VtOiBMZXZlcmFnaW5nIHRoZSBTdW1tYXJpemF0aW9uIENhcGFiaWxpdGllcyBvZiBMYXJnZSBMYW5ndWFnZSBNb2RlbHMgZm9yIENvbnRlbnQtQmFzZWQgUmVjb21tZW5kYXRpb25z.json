{
  "EmbSum: Leveraging the Summarization Capabilities of Large Language Models for Content-Based Recommendations": "",
  "Abstract": "Content-based recommendation systems play a crucial role in delivering personalized content to users in the digital world. In this work, we introduce EmbSum, a novel framework that enables offline pre-computations of users and candidate items while capturing the interactions within the user engagement history. By utilizing the pretrained encoder-decoder model and poly-attention layers, EmbSum derives User Poly-Embedding (UPE) and Content PolyEmbedding (CPE) to calculate relevance scores between users and candidate items. EmbSum actively learns the long user engagement histories by generating user-interest summary with supervision from large language model (LLM). The effectiveness of EmbSum is validated on two datasets from different domains, surpassing state-of-the-art (SoTA) methods with higher accuracy and fewer parameters. Additionally, the model's ability to generate summaries of user interests serves as a valuable by-product, enhancing its usefulness for personalized content recommendations.",
  "CCS Concepts": "Â· Information systems â†’ Content ranking ; Â· Computing methodologies â†’ Natural language generation .",
  "Keywords": "Recommendation System, User Interest Summarization, Large Language Model",
  "ACMReference Format:": "Chiyu Zhang â€  , Yifei Sun, Minghao Wu, Jun Chen, Jie Lei, Muhammad AbdulMageed, Rong Jin, Angli Liu, Ji Zhu, Sem Park, Ning Yao, and Bo Long. 2024. âˆ— Corresponding Authors: chiyuzh@mail.ubc.ca; sunyifei@meta.com Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. RecSys '24, October 14-18, 2024, Bari, Italy Â© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/18/06 https://doi.org/10.1145/nnnnnnn.nnnnnnn EmbSum: Leveraging the Summarization Capabilities of Large Language Models for Content-Based Recommendations. In 18th ACM Conference on Recommender Systems (RecSys '24), October 14-18, 2024, Bari, Italy. ACM, New York, NY, USA, 8 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn",
  "1 Introduction": "In the thriving digital world, billions of users interact daily with a diverse range of digital content, encompassing news, social media updates, e-books, etc. Content-based recommendation systems, as discussed in various studies [4, 16-18], leverage the textual content, such as news articles and books, and the sequence of a user's interaction history. It facilitates the delivery of content recommendations that are more precise, relevant, and customized to each user's preferences. Recent studies have successfully incorporated Pretrained Language Models (PLMs) into recommendation systems for processing textual inputs [13, 14, 30]. This integration has significantly improved the efficiency of content-based recommendations. Due to the well-known memory limitation of the attention mechanism, previous studies [11, 33] typically encode each piece of user historical content separately and then aggregate them. This approach, however, falls short in modeling the interactions among the user's historical contents. Addressing this, Mao et al. [17] introduce local and global attention mechanisms to encode user histories hierarchically. However, this method needs to truncate the history sequence to 1K tokens due to the limitation of PLMs, thus diminishing the benefits of leveraging extensive engagement history to capture users' comprehensive interests. In another vein, to improve the alignment between user and candidate content, many studies have directly integrated candidate items into user modeling [11, 20, 32]. This online real-time strategy prevents recommendation systems from performing offline pre-computations for efficient inference, which restricts their real-world applications. To tackle the aforementioned challenges, we introduce a new framework, EmbSum, which enables offline pre-computations of â€  Work done during Meta internship. RecSys '24, October 14-18, 2024, Bari, Italy Zhang et al. Figure 1: Overview of our EmbSum framework. Note that the user summaries generated by LLMs are only used in training. NCE Loss User-Item Attention Content Poly-Embedding User Poly-Embedding Auto-Regressive LM Loss Attention bo Attention LM Head Attention Attention Q0 Transformer Transformer Decoder Decoder Transformer Transformer Transformer Transformer Encoder <S05> <505> Encoder Encoder Encoder Candidate Content MISTRAL Content Pnt 2 Content Content Bnt 8 User Interest Summary Mixtral-Instruct Session Session 2 Session 3 embeddings of users and candidate items while capturing the interactions within the user's long engagement history. As Figure 1 shows, we utilize poly-attention [5] layers to derive multiple embeddings for the detailed features of both users and candidate items, referred to as User Poly-Embedding (UPE) and Content PolyEmbedding (CPE), respectively. These embeddings are then used to calculate the relevance scores between users and candidate items. More specifically, we use the pretrained T5 encoder [21] to encode user engagement sessions independently. We hypothesize that merely concatenating the embeddings of history sequences does not effectively model the interactions between user engagement sessions. To address this, we use the T5 decoder to fuse sessionbased encoded sequences by training it to generate user-interest summaries, supervised by the large language models (LLMs) [9] generated summaries of user's holistic interests when modeling user representations. Our contributions in this work can be summarized as follows: (1) We present a new framework, EmbSum , for embedding and summarizing user interests in content-based recommendation systems. This framework employs an encoder-decoder architecture to encode extensive user engagement histories and produce summaries of user interests. (2) We validate the effectiveness of EmbSum by testing it on two popular datasets from different domains. Our approach surpasses SoTA methods, delivering higher accuracy with fewer parameters. (3) Our model can generate summaries of user interests, which serves as a beneficial by-product, thereby enhancing its usefulness for personalized content recommendations.",
  "2 Methodology": "In this section, we first describe the problem formulation of our work (Section 2.1). Then, we provide an overview of our EmbSum framework (Section 2.2). Next, we introduce the details of modeling user engagements (Section 2.3), candidate contents (Section 2.4), and the click-through predictor and training objectives (Section 2.5).",
  "2.1 Problem Formulation": "Given a user ğ‘¢ ğ‘– and a candidate content item Â¯ ğ‘’ ğ‘— (such as news articles or books), the objective is to derive a relevance score ğ‘  ğ‘– ğ‘— , which indicates the likelihood of user ğ‘¢ ğ‘– engaging with (e.g., clicking on) the content item Â¯ ğ‘’ ğ‘— . Considering a set of candidate contents ğ¶ = { Â¯ ğ‘’ 1 , Â¯ ğ‘’ 2 , . . . , Â¯ ğ‘’ ğ‘— } , these contents are ranked based on their relevance scores { ğ‘  ğ‘– 1 , ğ‘  ğ‘– 2 , . . . , ğ‘  ğ‘– ğ‘— } for user ğ‘¢ ğ‘– . It is crucial to effectively extract user interests from their engagement history. User ğ‘¢ ğ‘– is characterized by a sequence of ğ‘˜ historically engaged contents ğ¸ ğ‘¢ ğ‘– (such as browsed news articles or positively rated books), sorted in descending order by engagement time.",
  "2.2 Overview of EmbSum": "Figure 1 presents an overview of our proposed model, EmbSum. Both the user engagements and the candidate contents are encoded using a pretrained encoder-decoder Transformer model. The clickthrough rate (CTR) predictions between the users and the candidate content are modeled using noisy contrastive estimation (NCE) loss. Moreover, we also introduce a user-interest summarization objective that is supervised by LLM generations. By leveraging both poly-embedding of user engagement history and candidate content, EmbSum can identify patterns and preferences from user interactions and match them with the most suitable content.",
  "2.3 User Engagement Modeling": "Session Encoding . The user engagement history, denoted as ğ¸ ğ‘¢ ğ‘– , comprises a sequence of ğ‘˜ content items that a user has previously engaged with. To address the high memory demands of processing long sequences with attention mechanisms, we partition these ğ‘˜ content items into ğ‘” distinct sessions, represented as ğ¸ ğ‘¢ ğ‘– = { ğœ‚ 1 , ğœ‚ 2 , . . . , ğœ‚ ğ‘” } . Each session ğœ‚ ğ‘– encapsulates ğ‘™ tokens from ğ‘ content items, expressed as ğœ‚ ğ‘– = { ğ‘’ 1 , ğ‘’ 2 , . . . , ğ‘’ ğ‘ } . This structure is designed to reflect the user's interests over specific time periods and to improve the interactions within each session. We encode each session using a T5 encoder [21] independently. The representation EmbSum: Leveraging the Summarization Capabilities of Large Language Models for Content-Based Recommendations RecSys '24, October 14-18, 2024, Bari, Italy of each piece of content is then derived from the hidden state output corresponding to its first token, which is the start-of-sentence symbol [SOS] . This process yields ğ‘˜ representation vectors. User Engagement Summarization . We posit that merely utilizing these ğ‘˜ representations falls short in capturing the subtleties of user interests and the dynamics among long-range engaged contents. To address this, we exploit the capabilities of SoTA LLMs to synthesize a distilled summary of user interests, given that recent studies [1, 19] have demonstrated LLMs' capacity for summarizing long sequences. This summary encapsulates a rich and comprehensive perspective of user preferences. We utilize Mixtral-8x22B-Instruct [9] to generate these interest summaries from the engagement histories. The resulting summaries are then incorporated into the T5 decoder. Drawing inspiration from the fusion-in-decoder concept [7], we concatenate the hidden states of all tokens from all subsequences encoded in a session-based manner and input this combined sequence into the T5 decoder. We then train the model to produce a summary of the user's interests, employing the following loss:  where ğ‘¦ ğ‘¢ ğ‘– ğ‘— represents the summary generated for user ğ‘¢ ğ‘– , and | ğ‘¦ ğ‘¢ ğ‘– ğ‘— | is the length of the user-interest summary. User Poly-Embedding . Given that each session is encoded independently, a global representation for all engaged contents is also necessary. We hence acquire a global representation that is derived from the last token of the decoder output (i.e., [EOS] token), representing all user engagements collectively. We then concatenate it with the ğ‘˜ representation vectors from session encoding to form a matrix ğ‘ âˆˆ R ( ğ‘˜ + 1 ) Ã— ğ‘‘ . With these representations of user interaction history, we employ a poly-attention layer [5] to extract the user's nuanced interests into multiple representations. The computation of each user-interest vector ğ›¼ ğ‘ is as follows:  where ğ‘ ğ‘ âˆˆ R 1 Ã— ğ‘ and ğ‘Š ğ‘“ âˆˆ R ğ‘‘ Ã— ğ‘ are the trainable parameters. We then concatenate ğ‘š user-interest vectors into a matrix ğ´ âˆˆ R ğ‘š Ã— ğ‘‘ , which serves as the user representation, referring to as the User Poly-Embedding (UPE) in our framework.",
  "2.4 Candidate Content Modeling": "Unlike the conventional practice of using only the first token of a sequence for representation, we introduce the novel Content PolyEmbedding (CPE). Similar to UPE, this method employs a set of context codes, denoted as { ğ‘ 1 , ğ‘ 2 , . . . , ğ‘ ğ‘› } , to create multiple embeddings for a piece of candidate content. For each piece of candidate content, we generate a corresponding vector ğ›½ ğ‘ through a polyattention layer defined by the equation referenced as Equation 2, which includes a trainable parameter ğ‘Š ğ‘œ . The resulting ğ‘› vectors for candidate content are then aggregated into a matrix ğµ âˆˆ R ğ‘› Ã— ğ‘‘ , providing a more nuanced representation that is expected to improve the performance of relevance scoring in the user-candidate matching predictor.",
  "2.5 CTR Prediction and Training": "CTR Prediction . To compute the relevance score ğ‘  ğ‘– ğ‘— , we first establish the matching scores between the user representation embedding ğ´ ğ‘– and the candidate content representation embedding ğµ ğ‘— . This computation is performed using the inner product, followed by flattening the resultant matrix:  where ğ¾ ğ‘– ğ‘— âˆˆ ğ‘… ğ‘šğ‘› is the flattened attention matrix. Subsequently, an attention mechanism is employed to aggregate the matching scores represented by this flattened vector:  where ğ‘Š ğ‘  âˆˆ ğ‘… ğ‘‘ Ã— ğ‘‘ signifies a trainable parameter matrix, ğ‘Š ğ‘ âˆˆ ğ‘… ğ‘šğ‘› represents the attention weights obtained after flattening and the softmax function, and ğ‘  ğ‘– ğ‘— is the scaled relevance score. Training . We follow the method of training the end-to-end recommendation models using the NCE loss [13, 28]:  where ğ‘  ğ‘– + represents the score of the positive sample with which the user engaged, and ğ‘  ğ‘– -represents the scores of negative samples. Therefore, the overall loss function is defined as:  where ğœ† is a scaling factor determined to be 0.05 based on the performance on the validation set.",
  "3 Experiments": "Baselines . We evaluate our EmbSum against a range of commonly used and SoTA neural network-based content recommendation approaches. These include methods that train text encoders from scratch, such as (1) NAML [26], (2) NRMS [27], (3) Fastformer [29], (4) CAUM [20], and (5) MINS [24]. We also consider systems that utilize PLMs, including (6) NAML-PLM, (7) UNBERT [33], (8) MINER [11], and (9) UniTRec [17]. More Details on these baselines and our implementation are provided in Appendix A. Dataset . We employ two publicly available benchmark datasets for content-based recommendation. The first dataset, MIND [31], consists of user engagement logs from Microsoft News, incorporating both positive and negative labels determined by user clicks on news articles. We use the smaller version of this dataset, which includes 94K users and 65K news articles. The second dataset is obtained from Goodreads [23], which focuses on book recommendations derived from user ratings. In this dataset, ratings above 3 are considered positive labels, while ratings below 3 are regarded as negative labels. The Goodreads dataset comprises 50K users and 330K books. More details on dataset statistics are at Appendix B. Evaluation . We utilize a variety of metrics to assess the performance of content-based recommendation systems. These metrics include the classification-based metric AUC [3], and ranking-based metrics such as MRR [22] and nDCG@top ğ‘ (with top ğ‘ = 5, 10) [8]. RecSys '24, October 14-18, 2024, Bari, Italy Zhang et al. Input [INST] You uscrs, YOUF task is engagement history. You interest basedon his her browsed news list, Fach browsed new s line and contains Input solve the puzzle. Pat: Blair Davis' loveless manriage? On Monday. alivethanks impressive starting pitching and Task Requircment Now, please provide You do not nced Summary of uscr interest: MISTRAL Mixtral-Instruct 22B AI _ Output Based on the user's browsed news history; it can be summarized that the uscr is interested in varicty of topics including entertainment , sports, rclated to tclevision shows- IssuCS and political figurcs. Additionally; the user has also shown interest in news rclated to aviation and international travel. Overall, the Intcrests appcar to be diverse and encompass wide range of subjects. hunstory Figure 2: Illustration of using an LLM for user interest profiling. The input provided to the LLM is enclosed in a red box, and the output generated by the LLM is shown in a green box. The segment marked in orange within the input specifies the instruction for the task, whereas the portion in blue highlights the history of news browsed by the user. Metric calculations are performed using the Python library TorchMetrics [2]. We determine the best model on the Dev set using AUC and report the corresponding Test performance. Content Formatting . As an example, for each content in MIND dataset, we combine its fields into a single text sequence using the following template: 'News Title: âŸ¨ ğ‘¡ğ‘–ğ‘¡ğ‘™ğ‘’ âŸ© ; News Abstract: âŸ¨ ğ‘ğ‘ğ‘ ğ‘¡ğ‘Ÿğ‘ğ‘ğ‘¡ âŸ© ; News Category: âŸ¨ ğ‘ğ‘ğ‘¡ğ‘’ğ‘”ğ‘œğ‘Ÿğ‘¦ âŸ© '. LLM Based User-Interest Summary . We leverage the opensource Mixtral-8x22B-Instruct [9] to create summaries that reflect users' interests based on their engagement history. Figure 2 illustrates an example of the input provided and the corresponding summary generated for the MIND dataset. The process starts with an instruction to frame the task, followed by a list of news items the user has viewed, ordered from the most recent to the oldest. Each item includes its title, abstract, and category. The input is limited to 60 engagement items, with extended news abstracts or book descriptions being condensed to 100 words. The instruction concludes with a request for the model to condense the user's interests into three sentences. The latter part of Figure 2 shows a sample output from the LLM. Additionally, we evaluate the LLM-generated userinterest summaries using GPT-4 (i.e., gpt-4o API) as a judge. The results indicate that most of the generated summaries accurately capture the user's interests. Further details of this experiment are provided in Section C of Appendix. As shown in Table 4 in Appendix, the average length of summaries generated by this process is 76 tokens for the MIND dataset and 115 tokens for the Goodreads dataset. Implementation . Weutilize the pretrained T5-small model [21], which comprises 61M parameters. After hyperparameter tuning, we determined that the optimal codebook sizes for the UPE and Table 1: Results on MIND-small and Goodreads. The best results are highlighted in bold. The second-best results are highlighted in underscore. CPE layers are 32 and 4, respectively, for both datasets. The model is trained with a learning rate of 5 ğ‘’ -4 and a batch size of 128 over 10 epochs. In all experiments, we cosnsider the latest 60 interactions as the user's engagement history. For the MIND dataset, we apply a negative sampling ratio of 4, limit news titles to 32 tokens, and restrict news abstracts to 72 tokens. Including approximately 20 additional tokens for the news category and template, each user's engaged history in MIND can total up to 7,440 tokens. In the Goodreads dataset, we set the negative sampling ratio to 2, limit book titles to 24 tokens, and constrain book descriptions to 85 tokens. Consequently, a user in the Goodreads dataset can have up to 7,740 tokens for their engagement history.",
  "4 Results": "Main Results . Table 1 shows the test results of nine baselines and our EmbSum. We can find that models initialized with PLMs obtained much better performance than baselines trained from scratch. We observe that EmbSum outperforms previous SoTA AUC scores given by UNBERT [33]. Compared to UNBERT, EmbSum achieves an improvement of 0.22 and 0.24 AUC on the MIND and Goodreads datasets, respectively. Notably, EmbSum uses only T5-small as the backbone, which has 61M parameters, significantly fewer than the 125M parameters of BERT-based methods (e.g., UNBERT and MINER). On other ranking-based metrics, EmbSum achieves the EmbSum: Leveraging the Summarization Capabilities of Large Language Models for Content-Based Recommendations RecSys '24, October 14-18, 2024, Bari, Italy",
  "Goodreads": "Table 2: Result of ablation study for EmbSum. The best results are highlighted in bold. best MRR and nDCG@10 scores on both datasets. EmbSum outperforms UniTRec which also uses an encoder-decoder architecture but cannot produce standalone user and candidate embeddings. Ablation Studies . To better understand the effectiveness of our framework, we conduct ablation studies on both datasets, the results of which are presented in Table 2. We first remove the CPE for the candidate item and use only the encoder hidden state of the [SOS] token to represent the candidate content. This alteration consistently results in the largest performance drop on both datasets, decreasing the AUC scores by 3.78 and 0.67 on the MIND and Goodreads datasets, respectively. These results suggest the effectiveness of utilizing multiple embeddings to represent candidate content and enhance user-item interactions. Removing sessionbased grouping and encoding each content separately leads to AUC drops of 0.61 and 0.25 on the MIND and Goodreads datasets, respectively. When we reduce the codebook size of the UPE layer to 1, meaning each user is represented by a single vector, it results in a performance decrease of 0.54 and 0.29 AUC on the MIND and Goodreads datasets, respectively. This also justifies the efficacy of using multiple embeddings in EmbSum. Furthermore, we investigate the efficacy of using LLM-generated user-interest summaries by removing L ğ‘ ğ‘¢ğ‘š . Without L ğ‘ ğ‘¢ğ‘š , we only provide the [SOS] token as the decoder input and take the hidden state of the [SOS] token as the global representation. As Table 2 shows, this change results in a performance drop on both datasets (0.52 on MIND and 0.14 on Goodreads). Influence of Hyperparameters. We investigate the model sensitivity to the weight for summarization loss ( ğœ† ) and the size of CPE and UPE. We randomly sampled 20% Train data of MIND dataset for training and validated performance on the Dev set. As Figure 3a shows, the ( ğœ† ) values of 0.05, 0.1, and 0.3 yielded Dev performances of 70.76, 70.55, and 70.47, respectively. The CPE size values of 2, 4, and 8 resulted in Dev performances of 70.43, 70.76, and 70.56, respectively. The UPE size values of 16, 32, and 48 achieved Dev performances of 70.46, 70.76, and 70.58, respectively. These results Figure 3: Influence of different hyperparameters. 70,76 70,76 70.75 70.65 70.60 70.58 70,56 70,55 70.55 70,45 70,43 UPE Size CPE Size Table 3: Evaluation on EmbSum-generated user summary. indicate that our EmbSum model is not highly sensitive to hyperparameters. For CPE size, our findings suggest that increasing the number of context codes on the candidate item side does not improve model performance. We believe that excessively numerous codebooks for a single item might introduce superfluous parameters, thereby negatively impacting performance. EmbSum-Generated Summary . We also evaluate the quality of the summaries generated by our model and report the ROUGE scores in Table 3. Since CTR is our main task, we generate summaries using the checkpoint that achieves the best AUC score on the Dev set. We select the test users who are not included in the training set and then generate their interest summaries based on their engagement history. We use the summaries generated by Mixtral-8x22B-Instruct as references for calculating ROUGE scores. For the MIND and Goodreads datasets, our model achieves ROUGE-L scores of 39.12 and 28.16, respectively. More concrete examples of the EmbSum-generated summaries can be found at Appendix D, demonstrating that EmbSum is capable of accurately capturing the user's diverse interests.",
  "5 Conclusion": "We present a novel framework EmbSum for content-based recommendation. EmbSum utilize encoder-decoder architecture and poly-attention modules to learn independent user and candidate content embeddings as well as generate user-interest summaries based on long user engagement histories. Through our experiments on two benchmark datasets, we have demonstrated that our framework achieves SoTA performance while using fewer parameters, and being able to generate a user-interest summary which can be used for recommendation explainability/transparency.",
  "References": "[1] Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy P. Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul Ronald Barham, Tom RecSys '24, October 14-18, 2024, Bari, Italy Zhang et al. Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, AnaÃ¯s White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, and et al. 2023. Gemini: A Family of Highly Capable Multimodal Models. ArXiv preprint abs/2312.11805 (2023). [2] Nicki Skafte Detlefsen, JirÃ­ Borovec, Justus Schock, Ananya Harsh Jha, Teddy Koker, Luca Di Liello, Daniel Stancl, Changsheng Quan, Maxim Grechkin, and William Falcon. 2022. TorchMetrics - Measuring Reproducibility in PyTorch. J. Open Source Softw. 7, 69 (2022), 4101. https://doi.org/10.21105/JOSS.04101 [3] Tom Fawcett. 2006. An introduction to ROC analysis. Pattern Recognit. Lett. 27, 8 (2006), 861-874. https://doi.org/10.1016/J.PATREC.2005.10.010 [4] Youyang Gu, Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2016. Learning to refine text based recommendations. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, Austin, Texas, 2103-2108. https://doi.org/10.18653/v1/D16-1227 [5] Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. 2019. Poly-encoders: Transformer Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring. [6] Andreea Iana, Goran GlavaÅ¡, and Heiko Paulheim. 2023. NewsRecLib: A PyTorchLightning Library for Neural News Recommendation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations , Yansong Feng and Els Lefever (Eds.). Association for Computational Linguistics, Singapore, 296-310. [7] Gautier Izacard and Edouard Grave. 2021. Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume . Association for Computational Linguistics, Online, 874-880. https://doi.org/10.18653/v1/2021.eacl-main.74 [8] Kalervo JÃ¤rvelin and Jaana KekÃ¤lÃ¤inen. 2002. Cumulated gain-based evaluation of IR techniques. ACM Trans. Inf. Syst. 20, 4 (2002), 422-446. https://doi.org/10. 1145/582415.582418 [9] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, and William El Sayed. 2024. Mixtral of Experts. ArXiv preprint abs/2401.04088 (2024). [10] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics, Online, 7871-7880. https://doi.org/10.18653/v1/2020.acl-main.703 [11] Jian Li, Jieming Zhu, Qiwei Bi, Guohao Cai, Lifeng Shang, Zhenhua Dong, Xin Jiang, and Qun Liu. 2022. MINER: Multi-Interest Matching Network for News Recommendation. In Findings of the Association for Computational Linguistics: ACL 2022 . Association for Computational Linguistics, Dublin, Ireland, 343-352. https://doi.org/10.18653/v1/2022.findings-acl.29 [12] Qijiong Liu. 2023. Legommenders: A Modular Framework for Recommender Systems. [13] Qijiong Liu, Nuo Chen, Tetsuya Sakai, and Xiao-Ming Wu. 2023. ONCE: Boosting Content-based Recommendation with Both Open- and Closed-source Large Language Models. [14] Rui Liu, Bin Yin, Ziyi Cao, Qianchen Xia, Yong Chen, and Dell Zhang. 2023. PerCoNet: News Recommendation with Explicit Persona and Contrastive Learning. [15] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. ArXiv preprint abs/1907.11692 (2019). [16] Itzik Malkiel, Oren Barkan, Avi Caciularu, Noam Razin, Ori Katz, and Noam Koenigstein. 2020. RecoBERT: A Catalog Language Model for Text-Based Recommendations. In Findings of the Association for Computational Linguistics: EMNLP 2020 . Association for Computational Linguistics, Online, 1704-1714. https://doi.org/10.18653/v1/2020.findings-emnlp.154 [17] Zhiming Mao, Huimin Wang, Yiming Du, and Kam-Fai Wong. 2023. UniTRec: A Unified Text-to-Text Transformer and Joint Contrastive Learning Framework for Text-based Recommendation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) , Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 1160-1170. https://doi.org/10.18653/v1/2023.aclshort.100 [18] Shumpei Okura, Yukihiro Tagami, Shingo Ono, and Akira Tajima. 2017. Embedding-based News Recommendation for Millions of Users. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, NS, Canada, August 13 - 17, 2017 . ACM, 1933-1942. https://doi.org/10.1145/3097983.3098108 [19] OpenAI. 2023. GPT-4 Technical Report. ArXiv preprint abs/2303.08774 (2023). [20] Tao Qi, Fangzhao Wu, Chuhan Wu, and Yongfeng Huang. 2022. News Recommendation with Candidate-aware User Modeling. In SIGIR '22: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022 , Enrique AmigÃ³, Pablo Castells, Julio Gonzalo, Ben Carterette, J. Shane Culpepper, and Gabriella Kazai (Eds.). ACM, 1917-1921. https://doi.org/10.1145/3477495.3531778 [21] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. J. Mach. Learn. Res. 21 (2020), 140:1-140:67. [22] Ellen M. Voorhees. 1999. The TREC-8 Question Answering Track Report. In Proceedings of The Eighth Text REtrieval Conference, TREC 1999, Gaithersburg, Maryland, USA, November 17-19, 1999 (NIST Special Publication, Vol. 500-246) , Ellen M. Voorhees and Donna K. Harman (Eds.). National Institute of Standards and Technology (NIST). [23] Mengting Wan and Julian J. McAuley. 2018. Item recommendation on monotonic behavior chains. In Proceedings of the 12th ACM Conference on Recommender Systems, RecSys 2018, Vancouver, BC, Canada, October 2-7, 2018 , Sole Pera, Michael D. Ekstrand, Xavier Amatriain, and John O'Donovan (Eds.). ACM, 86-94. https://doi.org/10.1145/3240323.3240369 [24] Rongyao Wang, Shoujin Wang, Wenpeng Lu, and Xueping Peng. 2022. News Recommendation Via Multi-Interest News Sequence Modelling. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2022, Virtual and Singapore, 23-27 May 2022 . IEEE, 7942-7946. https://doi.org/10.1109/ICASSP43922. 2022.9747149 [25] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. Self-Instruct: Aligning Language Models with Self-Generated Instructions. In Proceedings of ACL 2023 (Volume 1: Long Papers) . ACL, 13484-13508. https://doi.org/10.18653/V1/2023.ACL-LONG. 754 [26] Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang, and Xing Xie. 2019. Neural News Recommendation with Attentive Multi-View Learning. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019 , Sarit Kraus (Ed.). ijcai.org, 3863-3869. https://doi.org/10.24963/ijcai.2019/536 [27] Chuhan Wu, Fangzhao Wu, Suyu Ge, Tao Qi, Yongfeng Huang, and Xing Xie. 2019. Neural News Recommendation with Multi-Head Self-Attention. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP) . Association for Computational Linguistics, Hong Kong, China, 63896394. https://doi.org/10.18653/v1/D19-1671 [28] Chuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng Huang. 2021. Empowering News Recommendation with Pre-trained Language Models. [29] Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang, and Xing Xie. 2021. Fastformer: Additive Attention Can Be All You Need. [30] Chuhan Wu, Fangzhao Wu, Yang Yu, Tao Qi, Yongfeng Huang, and Qi Liu. 2021. NewsBERT: Distilling Pre-trained Language Model for Intelligent News Application. In Findings of the Association for Computational Linguistics: EMNLP 2021 . Association for Computational Linguistics, Punta Cana, Dominican Republic, 3285-3295. https://doi.org/10.18653/v1/2021.findings-emnlp.280 [31] Fangzhao Wu, Ying Qiao, Jiun-Hung Chen, Chuhan Wu, Tao Qi, Jianxun Lian, Danyang Liu, Xing Xie, Jianfeng Gao, Winnie Wu, and Ming Zhou. 2020. MIND: A Large-scale Dataset for News Recommendation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics, Online, 3597-3606. https://doi.org/10.18653/v1/2020. acl-main.331 [32] Liancheng Xu, Xiaoxiang Wang, Lei Guo, Jinyu Zhang, Xiaoqi Wu, and Xinhua Wang. 2023. Candidate-Aware Dynamic Representation for News Recommendation. In Artificial Neural Networks and Machine Learning - ICANN 2023 - 32nd International Conference on Artificial Neural Networks, Heraklion, Crete, Greece, September 26-29, 2023, Proceedings, Part VII (Lecture Notes in Computer Science, Vol. 14260) , Lazaros Iliadis, Antonios Papaleonidas, Plamen P. Angelov, and Chrisina Jayne (Eds.). Springer, 272-284. https://doi.org/10.1007/978-3-031-44195-0_23 [33] Qi Zhang, Jingjie Li, Qinglin Jia, Chuyuan Wang, Jieming Zhu, Zhaowei Wang, and Xiuqiang He. 2021. UNBERT: User-News Matching BERT for News Recommendation. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence . International Joint Conferences on Artificial Intelligence Organization, Montreal, Canada, 3356-3362. https://doi.org/10.24963/ijcai.2021/462 EmbSum: Leveraging the Summarization Capabilities of Large Language Models for Content-Based Recommendations RecSys '24, October 14-18, 2024, Bari, Italy",
  "Appendices": "",
  "A Baselines": "Our model is evaluated in comparison with the prior state-of-the-art neural network-based methods for content-based recommendation: (1) NAML [26] employs a content representation approach that integrates CNNs, additive attention, and pretrained word embeddings. It further uses additive attention to create a user representation based on their engagement history. (2) NRMS [27] combines pretrained word embeddings with multi-head self-attention and additive attention mechanisms to develop representations for user preferences and candidate content. (3) Fastformer [29] presents an efficient Transformer model that consists of additive attention. (4) CAUM [20] extends NRMS by incorporating content title entities into embeddings and utilizing candidate-aware selfattention for generating user embeddings. (5) MINS [24] improves upon NRMS with a multi-channel GRUbased network designed to better capture the sequential dynamics in user engagement history. (6) NAML-PLM utilizes a PLM as the content encoder, leveraging a pretrained model rather than training from scratch. RoBERTa-base [15] (with 125M parameters) is used as the text encoder. (7) UNBERT [33] employs a PLM for content encoding, deriving user-content matching indicators at both item and word levels, with RoBERTa-base [15] (with 125M parameters) as the backbone model. (8) MINER [11] uses a PLM for text encoding and introduces a poly-attention mechanism to extract diverse user interest vectors for enhanced user representation, standing out as a leading model on the MIND dataset leaderboard. 1 RoBERTabase [15] (with 125M parameters) is the text encoder for MINER. (9) UniTRec [17] applies an encoder-decoder structure (i.e., BART) to separately encode user history and candidate content, assessing candidate content using perplexity scores from the decoder and a discriminative scoring head. 2 BART-base [10] (with 139M parameters) is the backbone in our implementation. We utilize the optimal hyperparameters recommended for these baselines and perform training and evaluations on our dataset splits. For NAML, NRMS, Fastformer, and NAML-PLM, we employ the implementations provided by Liu [12]. The CAUM and MINS implementations are obtained from Iana et al. [6]. For UNBERT, MINER, and UniTRec, we use the original scripts released by their respective authors. 3 1 https://msnews.github.io/ 2 Metrics are calculated using predictions from the discriminative scoring head. 3 UNBERT: https://github.com/reczoo/RecZoo/tree/main/pretraining/news/UNBERT, MINER: https://github.com/duynguyen-0203/miner, UniTRec: https://github.com/ Veason-silverbullet/UniTRec.",
  "B Dataset Statistics": "In this work, we evaluate our model EmbSum on two public benchmark datasets for content-based recommendation. The dataset statistics is presented in Table 4.",
  "C Quality of LLM Generated User-Interest Summary": "Due to the lack of gold labels for evaluating generated user-interest summarization and the difficulty of human evaluation, we follow recent works that use LLM as a judge to evaluate AI response. We adapt the rubric from [25], 4 which scores the AI response in four levels, A (accurate and comprehensive), B (acceptable but with few minor errors), C (related but has significant errors), and D (invalid and unrelated response). We prompt the SoTA LLM GPT-4 (i.e., gpt-4o API) to evaluate the generated user-interest summaries. To be budget-friendly, we randomly sample 500 users for this evaluation. We provide the original user engagement history and our generated user-interest summary. The GPT-4 judge classifies 187 summaries to A, 305 to B, 8 to C, and 0 to D. We also conducted this evaluation on the generated user-interest summaries of Goodreads users. The GPT-4 judge classifies 156, 331, 13, and 0 summaries to A, B, C and D, respectively. This result indicates that most of our generated summaries are able to capture the user's interests.",
  "D EmbSum-Generated Summary Examples": "We present the examples of EmbSum-generated summaries in Table 5. 4 https://github.com/yizhongw/self-instruct RecSys '24, October 14-18, 2024, Bari, Italy Zhang et al. Table 4: Dataset Statistics. '# new users' indicates the number of users not included in the Train set. '# tokens/user summary' represents the average length of user interest summaries generated by LLM. The number of tokens are calculated using the RoBERTa-base model's vocabulary. EmbSum-Generated Summary Based on the user's browsed news history, their interests seem to be focused on entertainment, lifestyle, and current events. They appear to enjoy reading about celebrities, royal families, and unusual or quirky stories. Additionally, they seem to have an interest in health and wellness, specifically for weight loss and fitness. Table 5: Example of EmbSum-generated summary.",
  "keywords_parsed": [
    "Recommendation System",
    "User Interest Summarization",
    "Large Language Model"
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "Gemini: A Family of Highly Capable Multimodal Models"
    },
    {
      "ref_id": "b2",
      "title": "TorchMetrics - Measuring Reproducibility in PyTorch"
    },
    {
      "ref_id": "b3",
      "title": "An introduction to ROC analysis"
    },
    {
      "ref_id": "b4",
      "title": "Learning to refine text based recommendations"
    },
    {
      "ref_id": "b5",
      "title": "Poly-encoders: Transformer Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring"
    },
    {
      "ref_id": "b6",
      "title": "NewsRecLib: A PyTorchLightning Library for Neural News Recommendation"
    },
    {
      "ref_id": "b7",
      "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering"
    },
    {
      "ref_id": "b8",
      "title": "Cumulated gain-based evaluation of IR techniques"
    },
    {
      "ref_id": "b9",
      "title": "Mixtral of Experts"
    },
    {
      "ref_id": "b10",
      "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"
    },
    {
      "ref_id": "b11",
      "title": "MINER: Multi-Interest Matching Network for News Recommendation"
    },
    {
      "ref_id": "b12",
      "title": "Legommenders: A Modular Framework for Recommender Systems"
    },
    {
      "ref_id": "b13",
      "title": "ONCE: Boosting Content-based Recommendation with Both Open- and Closed-source Large Language Models"
    },
    {
      "ref_id": "b14",
      "title": "PerCoNet: News Recommendation with Explicit Persona and Contrastive Learning"
    },
    {
      "ref_id": "b15",
      "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
    },
    {
      "ref_id": "b16",
      "title": "RecoBERT: A Catalog Language Model for Text-Based Recommendations"
    },
    {
      "ref_id": "b17",
      "title": "UniTRec: A Unified Text-to-Text Transformer and Joint Contrastive Learning Framework for Text-based Recommendation"
    },
    {
      "ref_id": "b18",
      "title": "Embedding-based News Recommendation for Millions of Users"
    },
    {
      "ref_id": "b19",
      "title": "GPT-4 Technical Report"
    },
    {
      "ref_id": "b20",
      "title": "News Recommendation with Candidate-aware User Modeling"
    },
    {
      "ref_id": "b21",
      "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
    },
    {
      "ref_id": "b22",
      "title": "The TREC-8 Question Answering Track Report"
    },
    {
      "ref_id": "b23",
      "title": "Item recommendation on monotonic behavior chains"
    },
    {
      "ref_id": "b24",
      "title": "News Recommendation Via Multi-Interest News Sequence Modelling"
    },
    {
      "ref_id": "b25",
      "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions"
    },
    {
      "ref_id": "b26",
      "title": "Neural News Recommendation with Attentive Multi-View Learning"
    },
    {
      "ref_id": "b27",
      "title": "Neural News Recommendation with Multi-Head Self-Attention"
    },
    {
      "ref_id": "b28",
      "title": "Empowering News Recommendation with Pre-trained Language Models"
    },
    {
      "ref_id": "b29",
      "title": "Fastformer: Additive Attention Can Be All You Need"
    },
    {
      "ref_id": "b30",
      "title": "NewsBERT: Distilling Pre-trained Language Model for Intelligent News Application"
    },
    {
      "ref_id": "b31",
      "title": "MIND: A Large-scale Dataset for News Recommendation"
    },
    {
      "ref_id": "b32",
      "title": "Candidate-Aware Dynamic Representation for News Recommendation"
    },
    {
      "ref_id": "b33",
      "title": "UNBERT: User-News Matching BERT for News Recommendation"
    },
    {
      "ref_id": "b34",
      "title": "EmbSum: Leveraging the Summarization Capabilities of Large Language Models for Content-Based Recommendations"
    }
  ]
}