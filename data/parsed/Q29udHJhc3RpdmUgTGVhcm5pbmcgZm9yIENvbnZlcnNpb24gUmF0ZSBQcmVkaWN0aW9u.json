{"Contrastive Learning for Conversion Rate Prediction": "Wentao Ouyang Alibaba Group maiwei.oywt@alibaba-inc.com Rui Dong Alibaba Group kailu.dr@alibaba-inc.com Xiuwu Zhang Alibaba Group xiuwu.zxw@alibaba-inc.com Chaofeng Guo Alibaba Group chaofeng.gcf@alibaba-inc.com Jinmei Luo Alibaba Group cathy.jm@alibaba-inc.com Xiangzheng Liu Alibaba Group xiangzheng.lxz@alibaba-inc.com Yanlong Du Alibaba Group yanlong.dyl@alibaba-inc.com", "ABSTRACT": "", "ACMReference Format:": "Conversion rate (CVR) prediction plays an important role in advertising systems. Recently, supervised deep neural network-based models have shown promising performance in CVR prediction. However, they are data hungry and require an enormous amount of training data. In online advertising systems, although there are millions to billions of ads, users tend to click only a small set of them and to convert on an even smaller set. This data sparsity issue restricts the power of these deep models. In this paper, we propose the Contrastive Learning for CVR prediction (CL4CVR) framework. It associates the supervised CVR prediction task with a contrastive learning task, which can learn better data representations exploiting abundant unlabeled data and improve the CVR prediction performance. To tailor the contrastive learning task to the CVR prediction problem, we propose embedding masking (EM), rather than feature masking, to create two views of augmented samples. We also propose a false negative elimination (FNE) component to eliminate samples with the same feature as the anchor sample, to account for the natural property in user behavior data. We further propose a supervised positive inclusion (SPI) component to include additional positive samples for each anchor sample, in order to make full use of sparse but precious user conversion events. Experimental results on two real-world conversion datasets demonstrate the superior performance of CL4CVR. The source code is available at https://github.com/DongRuiHust/CL4CVR.", "CCS CONCEPTS": "\u00b7 Information systems \u2192 Online advertising .", "KEYWORDS": "Online advertising; Conversion rate (CVR) prediction; Contrastive learning Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '23, July 23-27, 2023, Taipei, Taiwan \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-9408-6/23/07...$15.00 https://doi.org/10.1145/3539618.3591968 Wentao Ouyang, Rui Dong, Xiuwu Zhang, Chaofeng Guo, Jinmei Luo, Xiangzheng Liu, and Yanlong Du. 2023. Contrastive Learning for Conversion Rate Prediction. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '23), July 23-27, 2023, Taipei, Taiwan. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3539618.3591968", "1 INTRODUCTION": "Conversion rate (CVR) prediction [4, 9, 11, 12] is an essential task in online advertising systems. The predicted CVR impacts both the ad ranking strategy and the ad charging model [13, 15, 16, 28]. Recently, deep neural network-based models have achieved promising performance in CVR prediction [9, 13, 20, 24]. However, deep models are data hungry and require an enormous amount of training data. In online advertising systems, although there may be millions to billions of ads, users tend to click only a small set of them and to convert on an even smaller set. This data sparsity issue restricts the prediction power of these deep models. Contrastive learning (CL) [6, 26] offers a new way to conquer the data sparsity issue via unlabeled data. The idea is to impose different transformations on the original data and to obtain two augmented views for each sample. It then pulls views of the same sample close in the latent space and pushes views of different samples apart in order to learn discriminative and generalizable representations. In this paper, we propose the Contrastive Learning for CVR prediction (CL4CVR) framework, which associates the supervised CVR prediction task with a CL task. The CL task can learn better data representations and improve the CVR prediction performance. The way to create different data augmentations highly impacts the performance of CL. In recommender systems, most data augmentation methods are ID-based sequence or graph approaches [21, 23, 26], which do not apply to CVR prediction which is a feature-rich problem. The most relevant work is [25], which proposes feature masking for item recommendation. The aim is that two differently masked views, each containing part of item features, can still well represent the same item. However, feature masking does not work well for CVR prediction, because the input features are diverse, which relate to the user, the item, the context and the interaction, rather than only the item. We cannot make a good CVR prediction if we only know the target user but not the target item (which is masked). Emb. layer feat_1, feat_2, \u2026 Emb. vector e Prediction network Encoder Encoder Mask i Mask j Emb. vector e i Emb. vector e j Rep. vector h i Rep. vector h j Supervised loss Contrastive loss prediction(s) FNE SPI FNE: False Negative Elimination SPI: Supervised Positive Inclusion shared (a) Structure of CL4CVR FC layer FC layer FC layer FC layer FC layer FC layer Emb. vector CTR tower CVR tower Supervised loss pCTR pCVR pCTCVR FC layer FC layer FC layer Emb. vector e i Rep. vector h i (b) ESMM as the supervised model (c) MLP as the encoder Encoder Prediction Network x ~ ~ ~ To tailor the CL task to the CVR prediction problem, we propose embedding masking (EM), rather than feature masking, to generate two views of augmented samples. In this way, each augmented view contains all the features, except that some embedding dimensions are masked. The CL loss will force the learned embeddings to be more representative. We also propose a false negative elimination (FNE) component to account for the natural property in user behavior data. We further propose a supervised positive inclusion (SPI) component to make full use of sparse but precious user conversion events. Experimental results show that the proposed EM, FNE and SPI strategies all improve the CVR prediction performance. In summary, the main contributions of this paper are \u00b7 We propose the CL4CVR framework, which leverages a contrastive learning task to learn better data representations and to improve the CVR prediction performance. \u00b7 We propose embedding masking for data augmentation that is tailored to feature-rich CVR prediction. \u00b7 We propose a false negative elimination component and a supervised positive inclusion component to further improve the contrastive learning performance.", "2 MODEL DESIGN": "We propose the CL4CVR framework that combines contrastive learning (CL) with supervised learning (SL) to improve the performance of CVR prediction. The structure is shown in Fig. 1(a).", "2.1 Problem Formulation": "In typical advertising systems, user actions follow an impression \u2192 click \u2192 conversion path. Denote the input feature vector as x , which contains multiple fields such as user ID, gender, age group, ad ID, ad title, ad industry, city, OS, etc. If a click event occurs, the click label is \ud835\udc66 = 1, otherwise, \ud835\udc66 = 0. If a conversion event occurs, the conversion label is \ud835\udc67 = 1, otherwise, \ud835\udc67 = 0. The (post-click) CVR prediction problem is to estimate the probability \u02c6 \ud835\udc67 = \ud835\udc5d ( \ud835\udc67 = 1 | \ud835\udc66 = 1 , x ) . feat_1, feat_2, feat_3, feat_4 Emb. layer Feature mask 1 Feature mask 2 Emb. vector 1 Emb. vector 2 feat_1, feat_2, feat_3, feat_4 Emb. layer Emb. mask 1 Emb. mask 2 Emb. vector 1 Emb. vector 2 (a) Feature masking (b) Embedding masking Concate. emb. vector e Raw features x Raw features x feat_1, feat_2, mask, mask mask, mask, feat_3, feat_4", "2.2 Supervised Prediction Model": "Our focus in this paper is on the design of the CL task, and we use existing CVR prediction model as the SL task. In particular, we use ESMM [13] as the supervised prediction model because of its popularity and versatility. More sophisticated models such as ESM 2 [20], GMCM [3] and HM 3 [19] require additional post-click behaviors (e.g., favorite, add to cart and read reviews), which are not always available in different advertising systems. Fig. 1(b) shows the structure of ESMM. It has a shared embedding layer, a CTR tower and a CVR tower. Assume there are \ud835\udc41 samples in a mini-batch. Denote the predicted CTR as \u02c6 \ud835\udc66 and the predicted CVR as \u02c6 \ud835\udc67 , the supervised loss is defined as", "2.3 Embedding Masking": "We now turn our attention to the CL task. Data augmentation is an important step that highly impacts the CL performance. In [25], the authors propose to create two views of each original sample by feature masking for item recommendation (Fig. 2(a)). The aim is that two differently masked views, each containing part of item features, can still well represent the same item. However, feature masking does not work well in CVR prediction, because the input features are diverse, rather than only about the item. We cannot decide whether a user would like to convert on an ad if the ad features are masked. Therefore, we propose embedding masking (EM) in this paper, which is illustrated in Fig. 2(b). In EM, we apply two different element-wise masks on the concatenated long embedding vector e rather than on the raw features x . Assume there are \ud835\udc39 features and the embedding dimension for each feature is \ud835\udc3e . Then a feature mask has dimension \ud835\udc39 , but an embedding mask has dimension \ud835\udc39\ud835\udc3e . By EM, each masked view contains all (rather than part of) the features, except that some random embedding dimensions are masked. The aim is that the remaining embedding dimensions can still well represent the whole sample and the CL loss will force the learned embeddings to be more representative. We denote the two augmented embedding vectors of the same sample as \u02dc e \ud835\udc56 and \u02dc e \ud835\udc57 , which form a positive pair.", "2.4 Encoder and Traditional Contrastive Loss": "We map the two views \u02dc e \ud835\udc56 and \u02dc e \ud835\udc57 of the same sample to two highlevel representation vectors h \ud835\udc56 and h \ud835\udc57 through the same encoder \ud835\udc53 . That is, h \ud835\udc56 = \ud835\udc53 ( \u02dc e \ud835\udc56 ) and h \ud835\udc57 = \ud835\udc53 ( \u02dc e \ud835\udc57 ) . For simplicity, we use an MLP as the encoder, which contains several fully connected (FC) layers with the ReLU activation [14] except the last layer (Fig. 1(c)). Given \ud835\udc41 original samples in a mini-batch, there are 2 \ud835\udc41 augmented samples. Given an anchor sample \u02dc e \ud835\udc56 , the authors in [6] treat the other augmented sample \u02dc e \ud835\udc57 of the same original sample as the positive and treat other augmented samples as negatives. We illustrate it in Fig. 3(a). The traditional contrastive loss [6] is where \ud835\udc60 ( h \ud835\udc56 , h \ud835\udc57 ) is the cosine similarity function and \ud835\udf0f is a tunable temperature hyper-parameter. This loss function aims to learn robust data representations such that similar samples are close to each other and random samples are pushed away in the latent space.", "2.5 False Negative Elimination": "In advertising systems, it is common that an ad is shown to a user multiple times at different time epochs. Because user behaviors naturally contain uncertainty, it is possible that the user clicks the ad \ud835\udc4e 1 times and converts \ud835\udc4e 2 times ( \ud835\udc4e 2 \u2264 \ud835\udc4e 1). This results in \ud835\udc4e 1 click samples with the same features but possibly different conversion labels. When such samples are included for CL, contradiction happens. It is because augmented samples corresponding to original samples with different indices will be treated as negatives. However, their original samples actually have the same features. Therefore, we propose a false negative elimination (FNE) component. It generates a set M( \ud835\udc56 ) for an anchor sample index \ud835\udc56 (Fig. 3(b)). Note that, FNE only impacts the CL task and the supervised prediction model still uses all the original samples for training, as otherwise, the learned conversion probabilities are incorrect. We use \ud835\udc5c ( \u02dc e \ud835\udc56 ) to denote the original sample of \u02dc e \ud835\udc56 . We introduce a duplication indicator where \ud835\udc3c ( \ud835\udc5c ( \u02dc e \ud835\udc56 ) , \ud835\udc5c ( \u02dc e \ud835\udc58 )) = 1 indicates that \ud835\udc5c ( \u02dc e \ud835\udc56 ) and \ud835\udc5c ( \u02dc e \ud835\udc58 ) have the same features and it is 0 otherwise. Given an anchor sample index \ud835\udc56 , we define the set M( \ud835\udc56 ) contains the indices of samples that should be included in the denominator of the CL loss function.", "2.6 Supervised Positive Inclusion": "As the conversion label is sparse but also precious, we further propose a supervised positive inclusion (SPI) component to effectively leverage label information. It generates a set S( \ud835\udc56 ) with supervised positive included for an anchor sample index \ud835\udc56 . Inspired by supervised contrastive learning [10], we include additional positive samples for an anchor sample when its conversion label is 1 (Fig. 3(c)). Note that in traditional contrastive learning [6], an anchor sample \u02dc e \ud835\udc56 has a single positive sample \u02dc e \ud835\udc57 . Given an anchor sample index \ud835\udc56 , we define the set where \ud835\udc67 ( \u02dc e \ud835\udc56 ) denotes the label of \u02dc e \ud835\udc56 , which is the same as the original sample. In other words, S( \ud835\udc56 ) = { \ud835\udc57 } if \ud835\udc67 ( \u02dc e \ud835\udc56 ) = 0 (i.e., the anchor sample has label 0) and S( \ud835\udc56 ) may contain more positive samples if \ud835\udc67 ( \u02dc e \ud835\udc56 ) = 1. We do not include supervised positive samples when \ud835\udc67 ( \u02dc e \ud835\udc56 ) = 0 because of the data sparsity issue. It is possible that all the samples in a mini-batch has \ud835\udc67 = 0, which makes all the samples supervised positives and there is no negative and no contrast at all.", "2.7 Contrastive Loss and Overall Loss": "M( \ud835\udc56 ) generated by FNE and S( \ud835\udc56 ) generated by SPI impact the contrastive loss. In particular, we define the contrastive loss used in this paper as where Q( \ud835\udc56 ) = S( \ud835\udc56 )\u2229M( \ud835\udc56 ) . For each anchor sample, we average over all its positives. The overall loss is the combination of the supervised CVR prediction loss and the contrastive loss as \ud835\udc3f = \ud835\udc3f \ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc51 + \ud835\udefc\ud835\udc3f \ud835\udc50\ud835\udc59 , where \ud835\udefc is a tunable balancing hyper-parameter.", "3 EXPERIMENTS": "", "3.1 Datasets": "The statistics of the datasets are listed in Table 1. Both datasets contain samples from advertising systems with rich features, and are tagged with click and conversion labels. 1) Industrial dataset : This dataset contains a random sample of user behavior logs from an industrial news feed advertising system in 2022. 2) Public dataset : This dataset is gathered from the traffic logs in Taobao 1 .", "3.2 Compared Methods": "We compare the following methods for CVR prediction. Base is the supervised prediction model. Other methods associate the same base model with different data regularization or CL algorithms. \u00b7 Base . The supervised CVR prediction model. In this paper, we use ESMM [13] as the base. \u00b7 FD . Base model with random Feature Dropout [18] in the supervised task. Table 2: Test AUCs on experimental datasets. The best result is in bold font. A small improvement in AUC (e.g., 0.0020) can lead to a significant increase in online CVR (e.g., 3%). * indicates the statistical significance for \ud835\udc5d \u2264 0 . 01 compared with the best baseline over paired t-test. \u00b7 SO . Base model with Spread-Out regularization [27] on original examples. \u00b7 RFM . Random Feature Masking [25]. Base model with a CL task. It randomly splits features into two disjoint sets. \u00b7 CFM . Correlated Feature Masking [25]. Base model with a CL task. It splits features according to feature correlation. \u00b7 CL4CVR . The framework proposed in this paper.", "3.3 Settings": "Parameter Settings. We set the dimensions of fully connected layers in prediction towers and those in the CL encoder as {512, 256, 128}. The training batch size is set to 64. All the methods are implemented in Tensorflow [1] and optimized by Adagrad [7]. We run each method 3 times and report the average results. Evaluation Metric. The Area Under the ROC Curve (AUC) is a widely used metric for CVR prediction. It reflects the probability that a model ranks a randomly chosen positive sample higher than a randomly chosen negative sample. The larger the better.", "3.4 Experimental Results": "3.4.1 Effectiveness . Table 2 shows the AUCs of different methods. It is observed that FD performs worst because it operates on the supervised task. SO performs better than RFM and CFM on the industrial dataset, but they have comparable performance on the public dataset. CFM performs better than RFM because it further considers feature correlation. CL4CVR performs best on both datasets, showing its effectiveness to cope with the data sparsity issue and to improve the CVR prediction performance. 3.4.2 Ablation Study . Table 3 lists the AUCs of three components in CL4CVR. It is observed that EM itself outperforms RFM 0.05 0.1 0.5 1 2 5 8 10 12 15 \u03c4 0.820 0.845 0.870 AUC CVR (a) Industrial dataset 0.05 0.1 0.5 1 2 5 8 10 12 15 \u03c4 0.645 0.650 0.655 0.660 AUC CVR (b) Public dataset Figure 4: Impact of the temperature \ud835\udf0f . 0 1e-3 5e-3 0.01 0.05 0.1 0.5 1 \u03b1 0.845 0.855 0.865 AUC CVR (a) Industrial dataset 0 1e-3 5e-3 0.01 0.05 0.1 0.5 1 \u03b1 0.652 0.656 0.660 AUC CVR (b) Public dataset and CFM, showing that embedding masking is more suitable than feature masking for CVR prediction. The incorporation of the FNE component or the SPI component leads to further improvement. CL4CVR that uses all the three components perform best, showing that these components complement each other and improve the prediction performance from different perspectives. 3.4.3 Impact of the Temperature and the CL Loss Weight . Fig. 4 plots the impact of the temperature \ud835\udf0f . It is observed that generally a large \ud835\udf0f works well on the two datasets. Fig. 5 plots the impact of the CL loss weight \ud835\udefc , where 0 denotes the supervised base model. It is observed that when \ud835\udefc increases initially, performance improvement is observed. But when \ud835\udefc is too large, too much emphasis on the CL task will degrade the performance.", "4 RELATED WORK": "CVR prediction. The task of CVR prediction [4, 11, 12] in online advertising is to estimate the probability of a user makes a conversion event on a specific ad. [11] estimates CVR based on past performance observations along data hierarchies. [5] proposes an LR model and [2] proposes a log-linear model for CVR prediction. [17] proposes a model in non-guaranteed delivery advertising. [13] proposes ESMM to exploit click and conversion data in the entire sample space. ESM 2 [20], GMCM [3] and HM 3 [19] exploit additional purchase-related behaviors after click (e.g., favorite, add to cart and read reviews) for CVR prediction. Contrastive learning. Contrastive learning [6, 8, 26] offers a new way to conquer the data sparsity issue via unlabeled data. It is able to learn more discriminative and generalizable representations. Contrastive learning has been applied to a wide range of domains such as computer vision [6], natural language processing [8] and recommendation [26]. In the recommendation domain, most data augmentation methods are ID-based sequence or graph approaches [21-23, 26], which do not apply to CVR prediction which is a featurerich problem. The most relevant work is [25] which proposes feature masking for item recommendation.", "5 CONCLUSION": "In this paper, we propose the Contrastive Learning for CVR prediction (CL4CVR) framework. It associates the supervised CVR prediction task with a contrastive learning task, which can learn better data representations exploiting abundant unlabeled data and improve the CVR prediction performance. To tailor the contrastive learning task to the CVR prediction problem, we propose embedding masking, false negative elimination and supervised positive inclusion strategies. Experimental results on two real-world conversion datasets demonstrate the superior performance of CL4CVR.", "REFERENCES": "[1] Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. 2016. Tensorflow: A system for large-scale machine learning. In 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI) . USENIX, 265-283. [2] Deepak Agarwal, Rahul Agrawal, Rajiv Khanna, and Nagaraj Kota. 2010. Estimating rates of rare events with multiple hierarchies through scalable log-linear models. In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD) . 213-222. [3] Wentian Bao, Hong Wen, Sha Li, Xiao-Yang Liu, Quan Lin, and Keping Yang. 2020. GMCM:Graph-based micro-behavior conversion model for post-click conversion rate estimation. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR) . 2201-2210. [4] Olivier Chapelle. 2014. Modeling delayed feedback in display advertising. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD) . 1097-1105. [5] Olivier Chapelle, Eren Manavoglu, and Romer Rosales. 2014. Simple and scalable response prediction for display advertising. ACM Transactions on Intelligent Systems and Technology (TIST) 5, 4 (2014), 1-34. [6] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A simple framework for contrastive learning of visual representations. In International Conference on Machine Learning (ICML) . PMLR, 1597-1607. [7] John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research 12, Jul (2011), 2121-2159. [8] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple Contrastive Learning of Sentence Embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP) . 6894-6910. [9] Siyuan Guo, Lixin Zou, Yiding Liu, Wenwen Ye, Suqi Cheng, Shuaiqiang Wang, Hechang Chen, Dawei Yin, and Yi Chang. 2021. Enhanced doubly robust learning for debiasing post-click conversion rate estimation. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR) . 275-284. [10] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. 2020. Supervised contrastive learning. Advances in Neural Information Processing Systems 33 (2020), 1866118673. [11] Kuang-chih Lee, Burkay Orten, Ali Dasdan, and Wentong Li. 2012. Estimating conversion rate in display advertising from past erformance data. In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD) . 768-776. [12] Quan Lu, Shengjun Pan, Liang Wang, Junwei Pan, Fengdan Wan, and Hongxia Yang. 2017. A practical framework of conversion rate prediction for online display advertising. In Proceedings of the International Workshop on Data Mining for Online Advertising (ADKDD) . 1-9. [13] Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu, and Kun Gai. 2018. Entire space multi-task model: An effective approach for estimating post-click conversion rate. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval (SIGIR) . ACM, 1137-1140. [14] Vinod Nair and Geoffrey E Hinton. 2010. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning (ICML) . 807-814. [15] Junwei Pan, Yizhi Mao, Alfonso Lobos Ruiz, Yu Sun, and Aaron Flores. 2019. Predicting different types of conversions with multi-task learning in online"}
