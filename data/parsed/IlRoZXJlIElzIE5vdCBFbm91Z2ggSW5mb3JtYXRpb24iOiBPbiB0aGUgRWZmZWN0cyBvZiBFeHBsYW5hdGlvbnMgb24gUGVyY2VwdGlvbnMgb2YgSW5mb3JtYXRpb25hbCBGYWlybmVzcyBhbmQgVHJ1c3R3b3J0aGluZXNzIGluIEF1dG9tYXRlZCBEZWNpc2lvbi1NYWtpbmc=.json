{"title": "\"There Is Not Enough Information\": On the Effects of Explanations on Perceptions of Informational Fairness and Trustworthiness in Automated Decision-Making", "authors": "Jakob Schoeffer; Niklas Kuehl; Yvette 2022 Machowski", "pub_date": "2022-05-11", "abstract": "Automated decision systems (ADS) are increasingly used for consequential decision-making. These systems often rely on sophisticated yet opaque machine learning models, which do not allow for understanding how a given decision was arrived at. In this work, we conduct a human subject study to assess people's perceptions of informational fairness (i.e., whether people think they are given adequate information on and explanation of the process and its outcomes) and trustworthiness of an underlying ADS when provided with varying types of information about the system. More specifically, we instantiate an ADS in the area of automated loan approval and generate different explanations that are commonly used in the literature. We randomize the amount of information that study participants get to see by providing certain groups of people with the same explanations as others plus additional explanations. From our quantitative analyses, we observe that different amounts of information as well as people's (self-assessed) AI literacy significantly influence the perceived informational fairness, which, in turn, positively relates to perceived trustworthiness of the ADS. A comprehensive analysis of qualitative feedback sheds light on people's desiderata for explanations, among which are (i) consistency (both with people's expectations and across different explanations), (ii) disclosure of monotonic relationships between features and outcome, and (iii) actionability of recommendations.\u2022", "sections": [{"heading": "INTRODUCTION", "text": "Automated decision-making has become ubiquitous in many highstakes domains such as hiring [72], bank lending [118], grading [105], and policing [54], among others. The underlying motives of adopting automated decision systems (ADS) 1 are manifold: they range from cost-cutting to improving performance and enabling more robust and objective decisions [53,72,93]. Hopes are also that, if properly designed, ADS can be a valuable tool for breaking out of vicious patterns of human stereotyping and contributing to social equity, e.g., in the realms of recruitment [21,67], health care [50,119], or financial inclusion [81]. However, ADS are typically based on ML techniques, which, in turn, rely on historical data. If, e.g., this underlying data is biased (e.g., because certain sociodemographic groups were favored in a disproportionate way), an ADS will learn from and perpetuate existing patterns of unfairness [40]. Prominent examples of such behavior from the recent past are race and gender stereotyping in job ad delivery [58], as well as the discrimination of Latinx and African-American borrowers in algorithmic mortgage loan pricing [8]. These and other cases have put ADS under enhanced scrutiny, justifiably jeopardizing trust in these systems [36].\nIn recent years, a growing body of AI and ML research has been devoted to detecting, quantifying, and mitigating unfairness in ADS [89]. A significant share of this work has focused on formalizing different concepts of fairness through statistical equity constraints, many of which are at odds with each other [27,65]. As a consequence, there cannot be a one-size-fits-all technical fairness criterion. Moreover, in many cases, these techno-centric works do not explicitly take into account the opinions of people that are (potentially) affected by such automated decisions. While the FAccT community has made a plethora of impactful contributions over the past years, it is still crucial to better understand people's perceptions and attitudes towards ADS-in addition to how researchers may define those systems' fairness in technical terms.\nA related issue revolves around explaining automated decisions to affected individuals. As ADS employ ever more sophisticated and \"black-box\" ML models, several problems arise; one of which is the hampered detectability of adverse behavior of such systems. In order to safeguard transparency and accountability of automated decisions, several laws and regulations demand a \"right to explanation\". The EU General Data Protection Regulation (GDPR), e.g., requires the disclosure of \"the existence of automated decisionmaking, including [. . . ] meaningful information about the logic involved [. . . ]\" [39] to data subjects. In fact, it has been shown, among others, that explanations can enhance people's understanding of certain automated decisions [83]. For most real-world cases, however, those regulations generally remain (too) vague and little actionable-which often results in deficient adoption, as noticed in the context of bank lending [116]. Moreover, research on explainable AI (XAI) suggests that there exists no one-size-fits-all approach to explaining ADS either [5,74].\nIn this work, we conduct a human subject study to examine the effects of explanations on people's perceptions towards an automated loan approval system, where we randomize the type and amount of information that study participants get to see. The primary dependent variables that we are interested in are perceptions of informational fairness of the system (i.e., whether people think they are given adequate information on and explanation of the decision-making process and its outcomes) as well as perceived trustworthiness, and the relationship between both. We also assess the influence of people's (self-assessed) AI literacy on the outcomes. Finally, we ask multiple open-ended questions w.r.t. people's ability to assess the given system's fairness, as well as regarding the appropriateness of explanations' content.", "publication_ref": ["b71", "b104", "b53", "b52", "b71", "b92", "b20", "b66", "b49", "b118", "b80", "b39", "b57", "b7", "b35", "b88", "b26", "b64", "b38", "b82", "b115", "b4", "b73"], "figure_ref": [], "table_ref": []}, {"heading": "BACKGROUND AND RELATED WORK", "text": "Topics of fairness and trustworthiness have become important pillars of AI and HCI research in recent years. In this section, we provide an overview of relevant literature and highlight our contributions. For brevity, we do not explicitly cover the vast technical literature on algorithmic fairness. While we assume that the FAccT community is familiar with seminal work in this field, we refer interested readers from other disciplines to relevant survey literature: [7,20,89].\nIt is-albeit unsurprisingly-important to note that a \"fair\" (according to some technical fairness notion) system does not imply that people perceive it as such; either because their personal fairness concepts differ from the employed technical notion or because they are not enabled to assess the system's (un)fairness, to begin with. In fact, it must be questioned whether an ADS that satisfies given statistical notions of fairness (e.g., equitable distribution of outcomes) can ever be truly considered fair when at the same time decision-subjects are left in the dark w.r.t. the inner workings of the system. Instead, fairness (of ADS) is likely a multi-faceted construct that encompasses different dimensions, similar to dimensions of (organizational) justice [29,31], which are commonly made up of distributive, procedural, interpersonal, and informational justice [31]. While distributive and procedural aspects have been considered in the context of ADS (e.g., in [49,78,86]), work on informational fairness of ADS is lacking.\nBorrowing from [22], we call a system informationally fair if it conveys adequate information on and explanation of the decisionmaking process and its outcomes; and we define adequate information (similar to [31]) as information being thorough, reasonable, tailored (to individual needs), as well as helping people understand the decision-making process, and enabling them to judge whether this process is fair or unfair. We refer to \u00a7 B for an overview of our measurement items. Trustworthiness is a well-established construct that, according to [17], is defined as \"the perception of confidence in the [. . . ] reliability and integrity [of an ADS]. \" We refer the reader to [60,75,123] for survey literature on trust and trustworthiness.", "publication_ref": ["b6", "b19", "b88", "b28", "b30", "b30", "b48", "b77", "b85", "b21", "b30", "b16", "b59", "b74", "b122"], "figure_ref": [], "table_ref": []}, {"heading": "Related work", "text": "Automated decision systems. Harris and Davenport [53] define automated decision systems (ADS) as systems that aim to minimize human involvement in decision-making processes. In this work, we assume ADS to be supervised ML models. In many cases, ADS have the potential to make more consistent decisions than humans. Such systems are popular in many industries, such as banking [53,118] or hiring [18,21,67,72]-and they are emerging in new areas as well, e.g., in health care [50,119]. With their increasing adoption in different consequential areas, it is important to ensure that ADS reach fair decisions that are transparent, primarily, to affected individuals or auditors. However, there have been multiple cases in the recent past where algorithms made biased decisions that discriminated against certain groups, e.g., based on gender or race [3,15,54]. In other instances, ADS have been operating in an opaque (\"blackbox\") fashion, making it, among others, difficult (i) for affected individuals to grasp the rationale behind certain decisions, and (ii) for regulatory agencies and other responsible stakeholders to vet such systems appropriately [96]. On that account, fairness and transparency of ADS have become important topics of interest for the research community. Interestingly, despite known weaknesses of ADS, some prior work has found that human-made decisions are not generally perceived as fairer or more trustworthy than automated decisions; primarily for reasons of (alleged) consistency in automated decision-making [108,111].\nExplainable AI. Despite being a popular topic of current research, XAI is a natural consequence of designing ADS and, as such, has been around at least since the 1980s [82]. Its importance, however, keeps rising as increasingly sophisticated (and opaque) AI techniques are used to inform ever more consequential decisions. XAI is not only required by law (e.g., GDPR, ECOA); Eslami et al. [38], e.g., have shown that users' attitudes towards algorithms change when transparency is increased. In general, both quantity and quality of explanations matter: Kulesza et al. [71] explored the effects of soundness and completeness of explanations on end users' mental models and suggest, among others, that oversimplification is problematic. Recent findings from Langer et al. [73], on the other hand, suggest that in the case of automated job interviews it might make sense to withhold certain pieces of information from applicants in order to not evoke negative reactions.\nEven in the presence of explanations, people sometimes rely too heavily on system suggestions [16], a phenomenon commonly referred to as automation bias [33,44]. Ehsan and Riedl [37] have also used the term \"explainability pitfalls\" for any such unanticipated negative effects of explanations (e.g., unwarranted trust [107]). Eventually, Chromik et al. [28] (inspired by seminal work related to UX design [46]) warn that explanations can be exploited to purposefully deceive users for the benefit of other stakeholders. Hence, explanations are by no means the \"silver bullet\" when it comes to solving problems of opaque AI systems [9]. A comprehensive overview of XAI stakeholders and their distinct desiderata is given by Langer et al. [74]. For instance, people affected by automated decisions may be particularly interested in explanations that enable them to evaluate the fairness and trustworthiness of the underlying systems [74,109]. This desideratum is closely linked to informational fairness of ADS [29], as introduced earlier. We refer the interested reader to, among others, [2,4,5,45,51,74,90,91] for more in-depth literature on different XAI techniques and their inner workings. Regarding the effectiveness of explanations, generally speaking, prior research has primarily focused on comparing individual explanation styles head-to-head (e.g., [11,35]), while little work has been done on evaluating the interplay of different styles, including potential complementarity. Langer et al. [74] emphasize the sparsity of empirical work w.r.t. the effectiveness of explanations overall.", "publication_ref": ["b52", "b52", "b17", "b20", "b66", "b71", "b49", "b118", "b2", "b14", "b53", "b95", "b107", "b110", "b81", "b37", "b70", "b72", "b15", "b32", "b43", "b36", "b106", "b27", "b45", "b8", "b73", "b73", "b108", "b28", "b1", "b3", "b4", "b44", "b50", "b73", "b89", "b90", "b10", "b34", "b73"], "figure_ref": [], "table_ref": []}, {"heading": "Perceptions towards ADS. A relatively new line of research in AI", "text": "and HCI has started focusing on perceptions of fairness and trustworthiness in automated decision-making. For instance, Binns et al. [11] and Dodge et al. [35] compare fairness perceptions in ADS for distinct explanation styles. Their works suggest differences in effectiveness of individual explanation styles-however, they also note that there does not seem to be a single best approach to explaining automated decisions. A different line of research has examined people's moral judgments w.r.t. the use of specific features in ADS [47,49], also with mixed empirical findings. Lee [76] compares perceptions of fairness and trustworthiness depending on whether the decision maker is a person or an algorithm in the context of managerial decisions. Their findings suggest that, among others, people perceive automated decisions as less fair and trustworthy for tasks that require typical human skills. Lee and Baykal [77] explore how algorithmic decisions are perceived in comparison to group-made decisions. Wang et al. [125] combine a number of manipulations, such as favorable and unfavorable outcomes, to gain an overview of fairness perceptions. An interesting finding by Lee et al. [78] suggests that fairness perceptions decline for some people when gaining an understanding of an algorithm if their personal fairness concepts differ from those of the algorithm. Woodruff et al. [128] conducted workshops with people from traditionally marginalized backgrounds, inferring that awareness of unfairness in ADS can substantially affect trust in companies or products. Some work has also assessed the impact of people's demographics (including gender [98]), as well as political views and task experience [48] on their perceptions. Saxena et al. [106] examined lay people's perceptions of different technical fairness notions for ADS, suggesting that people prefer notions related to meritocratic fairness [61,85]. Regarding trustworthiness, Kizilcec [64], e.g., concludes that it is important to provide the right amount of transparency for optimal trust effects, as both too much and too little transparency can have undesirable effects. K\u00e4stner et al. [62] also examined the relationship between explainability and trust(worthiness), urging system designers to engineer for trustworthiness (as opposed to trust), and indicating that explanations can be a crucial toolbox towards that goal. Regarding perceptions of different social groups, Lee and Rich [79] point out that prior studies have mostly recruited respondents from Amazon Mechanical Turk [95], which has predominantly white participants [55]-because of this, among other reasons [99] we have recruited our study participants through Prolificfoot_1 [94].", "publication_ref": ["b10", "b34", "b46", "b48", "b75", "b76", "b124", "b77", "b127", "b97", "b47", "b105", "b60", "b84", "b63", "b61", "b78", "b94", "b54", "b98", "b93"], "figure_ref": [], "table_ref": []}, {"heading": "Research gaps and our contributions", "text": "We aim to complement prior work to better understand how much of which information should be provided so that people are optimally enabled to understand the inner workings and appropriately assess the fairness and trustworthiness of ADS. To that end, we conducted a randomized experiment to examine people's perceptions of informational fairness and trustworthiness towards an automated loan approval system, given different combinations of common explanations (relevant factors, factor importance, and counterfactual explanations). While there exists prior work on trustworthiness perceptions for individual explanation styles, we see a significant gap w.r.t. assessing combinations of different explanations. We argue that this is an important gap to fill because different explanations convey different information and will likely have to be leveraged complementarily (i.e., not in isolation) in practice. On a related note, we also set about examining the marginal effects of providing certain explanations on top of others-which, to the best of our knowledge, has not been analyzed in depth before. As a consequence, we alter the amount of information that different groups of people get to see. We do by no means claim to examine these aspects exhaustively, but we hope that our work will be a stepping stone for further research.\nFinally, and perhaps most importantly, we shift focus from examining distributive and procedural fairness perceptions to informational fairness. In other words, we do not ask people whether they find particular ADS outcomes or procedures fair or not, butbroadly speaking-whether they feel they received sufficient information to assess a given system. This is an important distinction. Only very few works have considered the informational fairness dimension when experimentally evaluating effectiveness of ADS explanations: Binns et al. [11] only measure the understandability aspect of informational fairness for individual explanation styles; Schlicker et al. [108] and Schoeffer et al. [111] assess informational fairness perceptions, but with a focus on comparing human with automated decision makers. Uhde et al. [120] and Brown et al. [13] conducted interviews [120] and workshops [13] to infer qualitative statements related to informational fairness; whereby Brown et al. [13] explicitly state that \"more research is needed to understand how different elements of algorithmic systems affect perceptions of [. . . ] informational justice.\" Empirical work on the interplay of informational fairness and trustworthiness perceptions for ADS is, to our knowledge, entirely novel. Finally, we also analyze the relationship between study participants' (self-assessed) AI literacy and their perceptions, and we qualitatively examine their answers to open-ended question regarding (in)appropriateness of explanations as well as what information they feel is missing (if any) to properly vet the given ADS.", "publication_ref": ["b10", "b107", "b110", "b119", "b12", "b119", "b12", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "RESEARCH HYPOTHESES", "text": "The conditions of our experiment comprise different amounts of information that study participants get to see w.r.t. an ADS in the realm of automated loan decisioning. Regarding the potential effects of varying amounts of information on our dependent variables of perceived informational fairness and trustworthiness, we formulate two research hypotheses based on preliminary qualitative insights w.r.t. people's desire for transparency and information [13,120] as well as prior findings from the psychology literature [30,31,57,84,117,121]. First, assuming that explanations are not entirely lacking in content, we conjecture (similar to [13,120]) that more provided information leads to higher informational fairness perceptions. Regarding effects on trustworthiness perceptions, we note that several factors contribute to a system's fairness [31,78]; among these are consistency (of decision-making procedures) as well as process and outcome control on behalf of decision-subjects [34,78]. Process control means that decision-subjects have the \"ability to influence what [. . . ] data is considered by the decision maker\" [78], and outcome control, borrowing from [57], refers \"to the ability to appeal or modify the outcome [. . . ] once it has been made\" [78]. While we do not anticipate our employed explanations to readily increase perceptions of outcome control, we conjecture that certain information may enhance assumed process control, which, in turn, affects procedural fairness perceptions [31,78] and, ultimately, trust [121].\nH1 As the amount of information provided increases, perceptions of informational fairness towards the ADS increase. H2 As the amount of information provided increases, perceptions of trustworthiness towards the ADS increase.\nWhile investigating these relationships, we are not only interested in the effects of our conditions on informational fairness and trustworthiness but also in the relationship between the latter two. Some prior work has examined the relationship between informational fairness/justice and trust/trustworthiness (e.g., [30,43,129]) in other contexts. Frazier et al. [43] identified a significant positive effect of informational justice on different facets of trustworthiness perceptions in one of their two examined settings in the realm of organizational justice. Similarly, Zhu and Chen [129], in the context of customer satisfaction in internet banking, found that informational fairness (as a component of overall systemic fairness) has a positive effect on trust. Finally, Colquitt and Rodell [30] affirm that \"conventional wisdom on the justice-trust connection\" implies a causal path from (informational) justice to trust, and not the other way round. While these works address different use cases, we conjecture a positive relationship between informational fairness and trustworthiness perceptions for our ADS setting as well:\nH3 Perceptions of informational fairness relate positively to perceptions of trustworthiness.\nExperts may have a different attitude towards procedures or phenomena that touch on their area of expertise than non-experts. Slovic et al. [114,115], e.g., found differences in risk perceptions between experts and lay people. Regarding innovative (food) technologies, Siegrist [112] notes that lay people may neither be able to assess risks nor benefits appropriately. For the specific case of ADS, Wang et al. [125] found a significant effect of computer literacy on a mix of procedural and distributive fairness perceptions; specifically, their findings suggest that fairness perceptions are lower for people with lower computer literacy. Pierson [98], along the same lines, found that students' views on algorithmic fairness changed by increasing algorithmic literacy through lecture and discussion: students \"became more likely to emphasize transparency, [and] more open to using algorithms rather than using judges.\" [98] Finally, intuition tells us that AI-literate people may \"extract\" more information and understanding out of ADS explanations (e.g., because they know how supervised ML in general works). H4 People with higher AI literacy perceive an automated decision system to be more informationally fair than people with little or no knowledge in the field. H5 People with higher AI literacy perceive an automated decision system to be more trustworthy than people with little or no knowledge in the field.", "publication_ref": ["b12", "b119", "b29", "b30", "b56", "b83", "b116", "b120", "b12", "b119", "b30", "b77", "b33", "b77", "b77", "b56", "b77", "b30", "b77", "b120", "b29", "b42", "b128", "b42", "b128", "b29", "b113", "b114", "b111", "b124", "b97", "b97"], "figure_ref": [], "table_ref": []}, {"heading": "METHODOLOGY", "text": "We examine our hypotheses in the context of algorithmic lending. We argue that this is a common context that affects many people at some point in life. It is, furthermore, an area where ADS are typically already utilized within productive settings [1,59]. Specifically, we confront study participants (SPs) with situations where a person was denied a loan. Similar to [11], we argue that, in practice, explanations are much more likely to be requested by decisionsubjects in response to negative outcomes; or, in other words: if someone gets the loan, interest in how and why exactly the decision was arrived at will likely drop. However, we do by no means imply that reactions to positive outcomes are unworthy of being examined-given budget constraints, we defer them to future work.", "publication_ref": ["b0", "b58", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Study design", "text": "We choose a between-subject design with the following conditions: first, we reveal to SPs some basic information about the lending company. We then explain that a given individual's loan application was rejected by the company, as well as that this decision was communicated to the applying individual electronically and in a timely fashion (see Fig. 1 for the exact wording in our questionnaires).\nAfterwards, we provide one of four explanations (i.e., conditions) to each SP. Eventually, we measure the effects of assigning different conditions-and by design of the conditions, different amounts of information (AMTIN)-on two dependent variables: perceived informational fairness (INFF) and perceived trustworthiness (TRST) regarding the ADS. (Recall that informational fairness perceptions do not involve an actual assessment of the system's fairness w.r.t. its processes or outcomes.) Additionally, we measure the (self-assessed) AI literacy (AILIT) of SPs. We analyze whether differences in SPs' AI literacy affect their perceptions. All measurement items are summarized in \u00a7 B. Note that for each construct, we measure multiple items; mostly drawn (and partially adapted) from prior work.\nADS Setup. The ADS for our study consists of a random forest classifier which predicts loan approval on unseen data and is able to output different explanations. For training our model, we utilize a publicly available dataset on home loan application decisions [24], which has been used in multiple data science competitions on Kaggle. Note that comparable data-reflecting a given finance A finance company offers loans on real estate in urban, semi-urban, and rural areas. A potential customer first applies online for a specific loan, and afterwards, the company assesses the customer's eligibility for that loan. An individual applied online for a loan at this company. The company denied the loan application. The decision to deny the loan was communicated to the applying individual electronically and in a timely fashion. company's individual circumstances and approval criteria-might in practice be used to train ADS [59]. The dataset at hand consists of 614 labeled (loan Y/N) observations and includes the following features: applicant income, co-applicant income, credit history, dependents, education, gender, loan amount, loan amount term, marital status, property area, self-employment. After removing data points with missing values, 480 observations remain, 332 of which (69.2%) involve the positive label (Y) and 148 (30.8%) the negative label (N). We used 70% of the dataset to train our ADS and use the remaining 30% as a holdout set for the experiment. After encoding and scaling the features, we trained a random forest classifier with bootstrapping [12], which achieves an out-of-bag accuracy estimate of 80.1% on the held-out data. We use this classifier's predictions on the holdout set as a basis for the upcoming conditions/explanations that the SPs are confronted with. Since we are not asking to assess the actual (procedural or distributive) fairness of the ADS, it is not critical to quantify how fair the system really is-any such effort would be highly contestable anyhow, for reasons of incompatible fairness notions [27,65,92]. The authors still (informally but independently) checked training data as well as output quality for any salient problems that may bias SPs' responses w.r.t. the dependent variables.\nExplanations. We impose several requirements on the explanations that we provide to SPs: overall, we employ only modelagnostic explanations [2] in a way that they could plausibly be provided to loan applicants (i.e., lay people) in real-world scenarios. While explanations can be communicated in a wide variety of ways (see, e.g., [2,4,51,90]), we confine ourselves to textual explanations (esp. no visuals) to control for differences in conveyance. We also pick explanations that are immediately understandable semantically-this is important so as to collect meaningful responses. On a related note, we ensure that explanations are not too long, in order to account for known issues around information overload [10]. Finally, and similar to [11], we pick explanations that can plausibly provide insights about a system's \"logic involved,\" as required, e.g., by the GDPR. Based on these preliminaries, we assign SPs to one of four conditions that involve combinations of explanations w.r.t. (i) factors considered by the ADS, (ii) relative importance of these factors, and (iii) counterfactual scenarios where a rejected applicant would have been granted the loan. We acknowledge that additional explanation styles would be equally interesting to consider; however, in order to keep the experiment size manageable, we must defer them to future work.\nOur first condition, (Base), only reveals to the SPs that the loan decision was communicated to the applying individual electronically and in a timely fashion (as in Fig. 1). Apart from the (Base) condition-which might be regarded as a black-box system-all other conditions include the additional information that the loan decision was made by an ADS (i.e., automated). The second condition, (F), consists of disclosing the factors, including corresponding values for an observation (i.e., an applicant) from the holdout set whom our model denied the loan. We refer to such an observation as a setting. In our study, we employ two different settings in each questionnaire, where settings are chosen at random from the pool of rejected applicants. The authors, again, checked informally that no highly unusual (e.g., extreme outliers) settings were displayed that might distract SPs' perceptions and bias recorded responses. Please refer to \u00a7 C for an exemplary setting (introduction of use case plus conditions). Next, we computed permutation feature importance [12] from our model and obtained the following hierarchy, using \"\u227b\" as a shorthand for \"is more important than\": credit history \u227b loan amount \u227b applicant income \u227b co-applicant income \u227b property area \u227b marital status \u227b dependents \u227b education \u227b loan amount term \u227b self-employment \u227b gender. Revealing this ordered list in conjunction with (F) makes up our third condition, (FFI). To construct our fourth condition, we conducted an online survey with 20 quantitative and qualitative researchers to ascertain which of the aforementioned factors are actionable-in a sense that people can (hypothetically) act on them in order to increase their chances of being granted a loan. According to this survey, the top-5 actionable factors are loan amount, loan amount term, property area, applicant income, co-applicant income. Our fourth condition (FFICF) is then-in conjunction with (F) and (FFI)-the provision of three counterfactual scenarios where one actionable factor each is (minimally) altered such that our model predicts a loan approval instead of a rejection. Our four conditions are summarized as follows: Note that the order of provided explanations ((\ud835\udc35\ud835\udc4e\ud835\udc60\ud835\udc52) \u2192 (\ud835\udc39 ) \u2192 (\ud835\udc39 \ud835\udc39 \ud835\udc3c ) \u2192 (\ud835\udc39 \ud835\udc39 \ud835\udc3c\ud835\udc36\ud835\udc39 )) is not arbitrary: each subsequent condition provides the exact same information as the previous one and more. Since, e.g., factor importances implicitly reveal which factors the ADS considers, this would not necessarily hold true for, e.g., (\ud835\udc39 \ud835\udc3c ) \u2192 (\ud835\udc39 \ud835\udc3c \ud835\udc39 ).", "publication_ref": ["b23", "b58", "b11", "b26", "b64", "b91", "b1", "b1", "b3", "b50", "b89", "b9", "b10", "b11"], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "Data collection", "text": "Study participants (SPs) for our online study were (voluntarily) recruited via Prolific [94] and asked to rate their agreement with multiple statements w.r.t. our dependent variables as well as their AI literacy on 5-point Likert scales-where 1 corresponds to \"strongly disagree\" and 5 denotes \"strongly agree\". Additionally, we included multiple open-ended questions in the questionnaires to be able to better understand the reasoning behind SPs' quantitative responses. The SPs were randomly and in equal proportions assigned to one of the four conditions, and each SP was provided with two consecutive questionnaires associated with two different settings. We collected 401 responses, of which 4 had to be eliminated due to failure to pass one or more attention checks. Thus, we obtained 397 analyzable responses. Among the SPs, 60% indicated to be male, 39% female, and the remaining SPs either responded with \"non-binary\" or chose not to disclose their gender; 46% were students, 27% employed fulltime, 8% employed part-time, 7% self-employed, 11% unemployed, less than 1% retired, and 1% chose not to disclose their profession.\nThe reported average age of SPs was 25.7. SPs were monetarily compensated above the recommended min. pay of $6.50 per hour.", "publication_ref": ["b93"], "figure_ref": [], "table_ref": []}, {"heading": "QUANTITATIVE ANALYSES AND RESULTS", "text": "We now examine the effects of our conditions and people's (selfassessed) AI literacy on perceived informational fairness and trustworthiness of our ADS. For our measurement model, describing a confirmatory factor analysis and reporting correlations and factor loadings, we refer the reader to \u00a7 D. In this section, we first present the results of group difference analyses for our conditions with tests for pairwise comparison. After that, we report our findings on the validation of our hypotheses H1 to H5 with a full structural equation model.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Analysis of group differences", "text": "Since we cannot confirm the assumption of normality for all variables, we conduct multiple non-parametric Kruskal-Wallis H tests for multiple group comparisons [68]. Afterwards, we carry out pairwise comparisons using Bonferroni-corrected Mann-Whitney U tests [88]. With these tests, we initially assess the effects of our four conditions revealing different amount of information (AMTIN) on the constructs of informational fairness (INFF) and trustworthiness (TRST). Overall, we find a significant effect between different conditions on perceptions of informational fairness (\ud835\udc5d < 0.001) as well as on perceptions of trustworthiness (\ud835\udc5d < 0.001). A Mann-Whitney U test for pairwise comparisons shows that the effect for informational fairness is significant (\ud835\udc5d < 0.05) between all conditions except (Base) and (F). The effect for trustworthiness is significant between (Base) and (FFI), (Base) and (FFICF), as well as (F) and (FFICF), and marginally significant between (F) and (FFI) (\ud835\udc5d = 0.052). Looking at the mean response values for (INFF) and (TRST) by condition (see Tab. 1), we note that they are increasing as more information is shown to SPs. Please refer to Fig. 2 for the distribution of responses ", "publication_ref": ["b67", "b87"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Hypotheses testing", "text": "We estimate a full structural equation model (SEM), the results of which are depicted in Fig. 3. We also report more exhaustive information, including standard errors, z-values, p-values, and standardized path estimates in Tab. 6 in \u00a7 E. Consistent with using Kruskal-Wallis H tests for group comparisons, we estimate our SEM using unweighted least squares (ULS) because this estimator makes no distributional assumptions. We assess the fit of our model with multiple common measures: the comparative fit index (CFI) as well as Tucker-Lewis index (TLI) should be above 0.9 [66], root mean square error of approximation (RMSEA) below 0.05 [14], and standardized root mean squared residual (SRMR) below 0.08 [52] to indicate good model fit. Our model's values are \ud835\udc36\ud835\udc39 \ud835\udc3c = 0.997; \ud835\udc47 \ud835\udc3f\ud835\udc3c = 0.997; \ud835\udc45\ud835\udc40\ud835\udc46\ud835\udc38\ud835\udc34 = 0.024; \ud835\udc46\ud835\udc45\ud835\udc40\ud835\udc45 = 0.051.\nHence, all considered fit measures meet the required thresholds. Note that the chi-square test is not a meaningful measure of model fit in our case because variables are not normally distributed, and because we apply the ULS method to estimate our model [63].\nIn the following, we use a shorthand for our variables: AMTIN, AILIT, INFF, TRST (as introduced in \u00a7 4.1 and summarized in Tab. 3 of \u00a7 A). To investigate our hypotheses, we first examine the effect of AMTIN on INFF. As expected, and previously supported by the Kruskal-Wallis H test as well as the comparison of means between different conditions, increasing AMTIN has a significant positive effect on INFF (0.37***). Hence, H1 is supported. Next, we examine the influence of AMTIN on TRST. The results of the Kruskal-Wallis H test from \u00a7 5.1 indicate that there is a significant positive relationship between AMTIN and TRST. However, a mediation analysis within the SEM reveals that this effect is mediated by INFF. When assessing this mediating effect more closely in the context of our SEM, a small direct effect of AMTIN on TRST persists. Interestingly, in the context of the model, the stronger effect of AMTIN on TRST through INFF is positive, while the smaller but significant remaining direct effect is negative (-0.09*). We discuss this in more detail in \u00a7 7. Overall, H2, which conjectures a positive total (i.e., direct plus indirect) effect of AMTIN on TRST, is supported in our study.\nThe SEM's path coefficient concerning H3 (0.78***) confirms that there is a statistically significant positive relationship between INFF and TRST-which confirms H3. This result provides a crucial individual piece of information in the context of the analysis of INFF as a mediator between AMTIN and TRST. As presumed in H4, the path coefficient between AILIT and INFF (0.59***) confirms the conjecture of a significant positive relationship between these two variables-therefore, H4 is supported by our results. Similar to our findings w.r.t. the effect of AMTIN on TRST, the relationship between AILIT and TRST is also mediated by INFF. The analysis of effects within the full SEM confirms a strong indirect effect of AILIT on TRST through INFF, but the remaining direct effect of AILIT on TRST is not significant. Hence, the effect of AILIT on TRST is completely mediated by INFF. In conclusion, H5, which assumes a positive relationship between AILIT and TRST, is supported.", "publication_ref": ["b65", "b13", "b51", "b62"], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "QUALITATIVE ANALYSIS", "text": "In this section, we aim to understand people's perceptions in more detail. To that end, we collected responses to open-ended questions regarding (i) what information SPs think they are missing (if any) to be able to judge whether the system behaves fairly, and (ii) SPs' perceptions of (in)appropriateness of the given explanations. These questions were part of each condition. The first and second author jointly coded the qualitative data according to grounded theory [23], i.e., codes evolved as we analyzed the data. In total, 982 text passages were coded over five coding sessions with MAXQDA [69]. The emerging themes from the collected responses are summarized in the following subsections. Every direct quote is provided with a unique identifier, introduced with the \"#\" symbol. Some responses contain statements w.r.t. multiple themes; hence, percentages do not always add up to 100%.", "publication_ref": ["b22", "b68"], "figure_ref": [], "table_ref": []}, {"heading": "What information is missing?", "text": "For this question, we coded 421 text passages from SPs' responses to the open-ended question: If you don't feel you received sufficient information to judge whether the decision-making procedures are fair or unfair, what information is missing? We distinguish responses by condition and examine how many SPs felt that they received sufficient information (either by saying so explicitly or by not answering this question altogether). The latter is visually summarized in Figure 4.\n(Base). Most SPs (79%) assigned to this condition felt that they did not receive sufficient information; 17% did not answer the question, and 4% explicitly stated that they are not missing any information. Little surprisingly, when asked which information they are missing, SPs were interested in knowing why the system made particular decisions; 37% of all responses contained statements substantially similar to this: \"All I know is that the loan was denied and not the reason why\" (#1315). Similarly, 30% of responses inquired about decision criteria that underlie the rejected loans: \"I have no way to know what references the company may or may not use to consolidate a decision about the eligibility of an individual for a particular loan, and therefore I might or might not find the procedures to be truly fair\" (#1260). 16% of responses also thought that decision-making procedures in general must be explained more thoroughly, arguing that \"everything to do with how they made their decision of whether to accept the loan or not [is missing]\" (#1234). Some SPs were more specific as to what explanations they need: 18% indicated that relevant factors of applicants would be helpful to know (#1259: \"To decide whether the decision-making procedures are fair or unfair, I probably would need to know how the client was economically and other factors such as criminal records\"); and 6% of responses requested counterfactual-type insights related to recourse, e.g., \"what he can do to try again\" (#1265).\n(F). In the (Factors) condition, already 54% of SPs indicated that they received sufficient information. Of those who indicated that more information is needed, 15% are still interested in the \"why\" behind the rejections (#587: \"I think clearly spelled reason is missing instead of numbers\"). 15% still thought that more information w.r.t. decision criteria is needed. Interestingly, knowing what factors are used by the ADS raises further, more specific, questions as to why (i) these given factors are considered (#731: \"There needs to be more in depth explanations given as to why these factors are taken into consideration\"), and (ii) not others, e.g., \"how many loans have they taken out in the past, what is the money going to be spent on etc\" (#663). Overall, 23% of SPs requested these justifications. Another 10% of responses indicated that it would be necessary to know how each factor impacts the final decision-both in terms of weighting (#474: \"What kind of value does each factor hold?\") and monotonic relationships with the outcome (#602: \"The factors are told, but not which ones influenced the response positively of negatively. \") Finally, 3% are interested in counterfactual explanations, e.g., \"how the factors should differ for the application to be approved\" (#474).\n(FFI). In this condition, only 37% of SPs requested further information. Among these, 15% still requested more information w.r.t. reasons why the ADS rejected the applications; and 17% felt that they still had not received sufficient information regarding decisioncriteria (#677: \"There is not enough information about what thresholds have to be met to qualify for a loan.\") On a related note, 6% of SPs wanted to see more explanation as to why \"the [factor importance] ranking is the way it is\" (#764). Similar to the (F) condition, some SPs (10%) wanted to know why certain factors of the applicants are not being considered by the ADS. 3% of SPs still needed to know how exactly specific factors impact the final decision (#684: \" I don't know the significance level/weight assigned to [the factors]\"); and another 3% specifically requested counterfactual-type explanations. A newly occurring theme is w.r.t. communication of the explanations, as 3% requested \"less formal descriptions\" (#714).\n(FFICF). In our condition with the highest amount of provided information, only 22% requested additional information. Generally speaking, responses are more dispersed compared to other conditions. Some SPs still alluded to missing justification w.r.t. the given selection and importance of relevant factors (overall 14%), and others (7%) still asked for more information on the relationship between certain input factors and the outcome (#796: \"Since I think gender being a factor is unfair, not knowing the degree to which it affects the outcome seems to be a deficiency.\") 6% of SPs were interested in the rationale behind providing given counterfactuals: \"The factors that could have changed the outcome [are revealed], but not the reason why those [. . . ] factors would be needed. Ex.: Why would a rural area be more easily accepted?\" (#856) Interestingly, no SP requested additional information as to why the ADS rejected the applicants-as opposed to the other conditions. Yet, 11% still requested more information w.r.t. decision criteria, e.g., \"the thresholds that are required for a loan to be accepted\" (#800). 6% stated that processes were generally still not fully clear; however, some acknowledged that this might not necessarily be expedient, to begin with (#863: \"It's not clear how practically the priority system works, but I can understand it would be too hard to explain, and probably most of the people wouldn't understand it anyway. \") Figure 4: Percentage of responses indicating that study participants received sufficient information to judge whether the system's procedures are fair or unfair; either indicated explicitly in their responses, or implicitly by not answering the respective question.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "(In)Appropriateness of individual explanations", "text": "We also asked SPs about their feelings of (in)appropriateness of isolated explanations, specific to the condition they were assigned to: Why do you think {some factors, the order of factor importance, some counterfactual scenarios} are appropriate or inappropriate? For that, we coded 561 text passages and summarized the main themes for each type of explanation. Factors. Only 14% of responses explicitly stated that (at least a subset of) the factors considered by the ADS were appropriatemostly those related to an applicant's financial situation (#602: \"Economic factors seem apropriate [sic] to me. Self employment sometimes involves risks and it is a relevant factor also. \") We also asked SPs to check specific factors they deem inappropriate-this is visualized (by condition) in Figure 5. Among responses w.r.t. inappropriate factors, two general themes emerged: 72% indicated that some factors are (causally) irrelevant for deciding on creditworthiness (#632: \"Some of the more social-oriented factors (ie education, gender, dependents) aren't necessarily indicative of someone's ability to pay back a loan\"), and 28% found the usage of certain factors (primarily gender, education, and married) morally wrong (#561: \"In the world we live, i dont [sic] think gender is something to even be at question, neither marriage. \") Interestingly, SPs often assumed that the sheer presence of a factor like gender means that it is being used with malicious intent: \"Gender can be somewhat problematic because all people deserve to have the right to the loan and not only men\" (#637), or, \"some factors like gender are plain racist to make a financial decision\" (#647).\nFactor importance. Generally speaking, most SPs found the order of factor importance reasonably appropriate. Many responses resembled this: \"I may not agree with the placement of every single factor, but overall i think they are ranked appropriately\" (#695). Yet, 35% still suggested concrete changes w.r.t. the order of importance; particularly around assigning less weight to education and marital status. 14% were still entirely put off by the fact that gender or marital status were used in the decision-making process. However, learning that gender is the least important factor made many SPs feel better w.r.t. appropriateness of procedures (#510: \"It is appropriate. Gender should be considered the least and credit history is most important. \") One SP even suggested that \"gender could play a part in the decision making, but not a big one so it's good as it is\" (#751). (Recall that gender was ranked last in our explanation (see \u00a7 4.1).)\nCounterfactual scenarios. 47% of coded responses indicated that the provided counterfactual scenarios are appropriate, e.g., endorsing that they \"are all financial and based on the ability of the loan to be paid back\" (#448). However, 20% questioned the effectiveness of adhering to some of the counterfactual recommendations; especially regarding suggested changes to co-applicant income or property area: \"These factors do not change the fact that an applicant can or can not pay his/her debt\" (#454). Actionability of counterfactual scenarios was another important theme: 9% overall addressed this, being appreciative that some counterfactual scenarios are explicitly actionable (#836: \"Changing the loan term is possible immediately\") and disenchanted when not (#462: \"Some hardly achivable [sic] scenarios must be met to ensure the bank [will] be repayed [sic]. \") Some themes were addressed by fewer SPs but are highly interesting: one SP was, e.g., confused by the \"direction\" of suggested changes: \"Instead of a short loan amount term, it could be a bit longer\" (#778). Others were seemingly distracted by suggested changes that are (too) small: \"The incomes are so close to the required that it shouldn't matter\" (#447). Finally, some SPs hinted at potential inconsistencies between individual explanations: \"It seems odd that loan amount term is placed so low when it was one of the areas the individual could change to obtain the loan\" (#435).", "publication_ref": [], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "DISCUSSION AND IMPLICATIONS", "text": "In this section, we link our quantitative results to qualitative insights to get a better understanding as to why certain effects were observed, and we analyze and discuss in more detail the findings from the fitted SEM. Finally, we allude to several implications of our work.\nConnecting quantitative and qualitative findings. As observed in Tab. 1 ( \u00a7 5), both perceptions of informational fairness and trustworthiness increase as more explanations are provided to SPs-however, INFF at a much higher rate than TRST. Interestingly, many SPs in the (Base) condition, who do not receive any further explanations w.r.t. the inner workings of the ADS, do not find this \"black-box\" system to be overly problematic w.r.t. informational fairness: as can be seen in Fig. 2 ( \u00a7 5), SPs' responses for INFF are approx. equally distributed across ratings 1-4. This might be due to people's expectations; one SP simply stated that this \"seems to be standard practice\" (#1212) in terms of explaining ADS. From Tab. 2 ( \u00a7 5) we infer that providing relevant factors (F) to SPs does not significantly increase INFF. A likely reason for this observation is that SPs asked for significant follow-up information w.r.t. how the factors are used for decision-making. Both the differences for (\ud835\udc39 ) \u2192 (\ud835\udc39 \ud835\udc39 \ud835\udc3c ) and (\ud835\udc39 \ud835\udc39 \ud835\udc3c ) \u2192 (\ud835\udc39 \ud835\udc39 \ud835\udc3c\ud835\udc36\ud835\udc39 ) are significant for INFF. Considering the qualitative findings ( \u00a7 6.1), this seems little surprising as the complementary explanations (e.g., factor importance in (FFI) over (F)) were specifically requested by SPs.\nWhile some explanations clearly helped SPs understand the given ADS better, they also reveal certain aspects that might be detrimental to people's trust. Similar to INFF, one might have expected to see lower ratings for TRST in the (Base) condition. Instead, SPs' responses for TRST are symmetrically distributed around the mean of 3 (see Fig. 2,\u00a7 5). Regarding marginal effects of explanations on TRST, we note that none of (\ud835\udc35\ud835\udc4e\ud835\udc60\ud835\udc52) \u2192 (\ud835\udc39 ), (\ud835\udc39 ) \u2192 (\ud835\udc39 \ud835\udc39 \ud835\udc3c ), or (\ud835\udc39 \ud835\udc39 \ud835\udc3c ) \u2192 (\ud835\udc39 \ud835\udc39 \ud835\udc3c\ud835\udc36\ud835\udc39 ) lead to statistically significant changes in SPs' perceptions. As for (\ud835\udc35\ud835\udc4e\ud835\udc60\ud835\udc52) \u2192 (\ud835\udc39 ), SPs' trust appears to be hampered by the experience that certain (presumably) inappropriate factors (e.g., gender) are being considered by the ADS. While the change (\ud835\udc39 ) \u2192 (\ud835\udc39 \ud835\udc39 \ud835\udc3c ) is marginally significant (\ud835\udc5d = 0.052) for TRST, we still suspect a certain attenuation due to SPs' disagreement with the relative importance ranking of certain factors like education and married. On the other hand, from analyzing the qualitative statements, we might assume gender playing the least important role in the decision-making process had a positive effect on SPs' trust. As for (\ud835\udc39 \ud835\udc39 \ud835\udc3c ) \u2192 (\ud835\udc39 \ud835\udc39 \ud835\udc3c\ud835\udc36\ud835\udc39 ), we suspect that a potential positive effect of counterfactual explanations on perceived outcome control [57] might have been overshadowed by the fact that several SPs found some of the provided scenarios incomprehensible, ineffective, or unactionable.\nInterpreting SEM results. In addition to confirming significant total effects (see Fig. 3,\u00a7 5) of the amount of information (AMTIN) on INFF (0.37***) and TRST (0.37 \u2022 0.78 -0.09 = 0.20***), we also learn that SPs' (self-assessed) AI literacy (AILIT) is strongly related to INFF (0.59***) and TRST (0.44***), implying that we observe higher INFF and TRST ratings for higher AI-literacy people-given our study setup. Additionally, we see a strong positive relationship between INFF and TRST (0.78***). The SEM also lets us decompose total effects of AMTIN and AILIT on TRST into direct and indirect (through the mediator INFF) effects (see Tab. 7, \u00a7 E). We see, e.g., that the direct effect of AILIT on TRST (-0.02) is not significantly different from zero when INFF is acting as a mediator. Since the indirect effect AILIT\u2192INFF\u2192TRST is significantly positive (0.46***), we observe a complete mediation of the effect of AILIT on TRST through INFF. A similar observation can be made for the effect of AMTIN on TRST: the total effect consists of a significantly positive indirect effect through INFF (0.29***) as well as a small negative direct effect (-0.09*). Hence, we conclude that increasing AMTIN does not directly increase TRST, but that the positive total effect stems from the strong indirect effect through INFF. This phenomenon is sometimes also referred to as inconsistent mediation [63,87]. Future work should further investigate the link between INFF and TRST for other scenarios.\nImplications. Our work has several implications for the design of automated decision systems and explanations thereof. Revealing to (potential) decision-subjects what information about them is used and how exactly individual factors affect the outcome is something that appears to go a long way towards facilitating informational fairness. We have also seen that many people require an understanding of (assumed) monotonic relationships between individual features and outcome (#856: \"We don't know if being married is a good or bad thing in this case. \") However, these types of global monotonic relationships cannot generally be derived from nonlinear ML models-something that has been discussed, e.g., in [104,110,126]. Employing inherently interpretable (e.g., linear) models might be a potential remedy.\nWe made a similar observation w.r.t. monotonicity for counterfactual explanations: people are put off when the \"direction\" of suggested change(s) contradicts commonly-held assumptions (e.g., if a decrease in income were suggested in order to get the loan). System designers must therefore pay close attention that counterfactual scenarios or general recommendations on recourse are intuitive, meaningful, and actionable. Regarding the latter, we have observed that certain factors are deemed actionable by some SPs and immutable by others. This poses further challenges w.r.t. individualizing explanations [70]; this is also relevant for people with different AI backgrounds as their perceptions differ. In general, however, counterfactual explanations appear to be effective in a way that they help people understand \"where [an] applicant fell short\" (#731). From the analysis of qualitative data (also confirmed quantitatively), we learned that SPs in the (Base) condition specifically requested explanations related to both factor importance and recourse / why the ADS decided negatively. This suggests the employment of both explanation types in a complementary fashion. Designers will have to ensure, however, that they are consistent with one another. For instance, people seem to expect that recommendations for recourse (e.g., that income should be increased) apply to the factors that are most important in the decision-making process. Since individual explanations are often automatically and independently generated, this poses a significant technical challenge. Our findings also suggest that informational fairness might be further increased by providing rejected loan applicants with a crisp statement in lay people's terms as to why they were denied. Finally, regarding the usage of sensitive information like gender, it should be clearly justified why and how (if at all) this information is used, and that this is not automatically to the disadvantage of marginalized groups; e.g., in the case of affirmative action [56].", "publication_ref": ["b56", "b62", "b86", "b103", "b109", "b125", "b69", "b55"], "figure_ref": ["fig_4", "fig_2", "fig_4", "fig_3", "fig_4"], "table_ref": []}, {"heading": "LIMITATIONS AND OUTLOOK", "text": "We acknowledge limitations of our work that open up avenues for future studies. Firstly, we investigated only one setting where ADS are currently used to inform consequential decisions: lending. Our study design should be replicated and the results should be compared in different settings, e.g., hiring or university admissions, where the relevant factors will be significantly different. It would also be interesting to work with domain experts, as opposed to crowdworkers. Future work should further examine the complementarity and interplay of other explanation styles (e.g., case-based or demographic explanations [11]). Furthermore, our quantitative results (including SEM) are contingent upon the concrete instantiation of our ADS including the employed explanations, which limits our ability to generalize findings.\nWhile we informally checked the model as well as the underlying data and all derived explanations so as to ensure behavior that might be representative of many real-world applications, it would be insightful to randomize different aspects about the model's quality and compare the results. More specifically, if we managed to construct-broadly speaking-a trustworthy ADS and an untrustworthy ADS, we would be able to contrast people's perceptions for either system. This would allow to derive insights w.r.t. (un)warranted perceptions, i.e., (i) are people actually able to spot problematic behavior of ADS, and (ii) do they trust the system if and only if the system is trustworthy? In fact, for an untrustworthy ADS, we would ideally expect that more explanations lead to higher informational fairness perceptions but to lower trust. If perceptions of trustworthiness increase regardless of the actual trustworthiness of the ADS, this would indicate serious issues around over-reliance [113] or automation bias [33,44], and must be avoided by system designers at all costs.\nWe also acknowledge that our work does not explicitly take into account potential issues around information overload [10]: while we specifically examine situations where selected explanations convey complementary information, unsystematic provision of more and more explanations will likely have undesirable effects.\nThe authors suggest by no means that more information is always better. Finally, we hope that this work can serve as a stepping stone for further empirical research on the complementarity and interplay of different explanations and their effects on people's perceptions towards ADS.\n\u2022 Applicant Income: $3,069 per month A finance company offers loans on real estate in urban, semi-urban and rural areas. A potential customer first applies online for a specific loan, and afterwards the company assesses the customer's eligibility for that loan. An individual applied online for a loan at this company. The company denied the loan application. The decision to deny the loan was made by an automated decision system and communicated to the applying individual electronically and in a timely fashion. The automated decision system explains . . .\n\u2022 . . . that the following factors (in alphabetical order) on the individual were taken into account when making the loan application decision:\n-Applicant Income: $3,069 per month -Co-Applicant Income: $0 per month -Credit History: Good -Dependents: 0 -Education: Graduate -Gender: Male -Loan Amount: $71,000 -Loan Amount Term: 480 months -Married: No -Property Area: Urban -Self-Employed: No \u2022 . . . that different factors are of different importance in the decision. The following list shows the order of factor importance, from most important to least important: Credit History \u227b Loan Amount \u227b Applicant Income \u227b Co-Applicant Income \u227b Property Area \u227b Married \u227b Dependents \u227b Education \u227b Loan Amount Term \u227b Self-Employed \u227b Gender", "publication_ref": ["b10", "b112", "b32", "b43", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Condition (FFICF)", "text": "A finance company offers loans on real estate in urban, semi-urban and rural areas. A potential customer first applies online for a specific loan, and afterwards the company assesses the customer's eligibility for that loan. An individual applied online for a loan at this company. The company denied the loan application. The decision to deny the loan was made by an automated decision system and communicated to the applying individual electronically and in a timely fashion.\nThe automated decision system explains . . . \u2022 . . . that the following factors (in alphabetical order) on the individual were taken into account when making the loan application decision:\n-Applicant Income: $3,069 per month -Co-Applicant Income: $0 per month -Credit History: Good -Dependents: 0 -Education: Graduate -Gender: Male -Loan Amount: $71,000 -Loan Amount Term: 480 months -Married: No -Property Area: Urban -Self-Employed: No \u2022 . . . that different factors are of different importance in the decision. The following list shows the order of factor importance, from most important to least important: Credit History \u227b Loan Amount \u227b Applicant Income \u227b Co-Applicant Income \u227b Property Area \u227b Married \u227b Dependents \u227b Education \u227b Loan Amount Term \u227b Self-Employed \u227b Gender \u2022 . . . that the individual would have been granted the loan if-everything else unchanged-one of the following hypothetical scenarios had been true:\n-The Co-Applicant Income had been at least $800 per month -The Loan Amount Term had been 408 months or less -The Property Area had been Rural", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D MEASUREMENT MODEL", "text": "In order to assess the validity and the reliability of our constructs, we conduct a confirmatory factor analysis and assess the results w.r.t. multiple measures. As measures for convergent reliability, we examine average variance extracted (AVE) and composite reliability (CR). For the constructs of informational fairness and trustworthiness, AVE is above the recommended threshold of 0.5, whereas the AVE of AI literacy is 0.41. According to Fornell and Larcker [41], if AVE is low, convergent validity of a construct can still be sufficient if composite reliability (CR) is above 0.6, which is the case for all three constructs, including AI literacy (see Tab. 4). In fact, the CR of our three main constructs, informational fairness (0.88), trustworthiness (0.94), and AI literacy (0.72) is above the recommended threshold of 0.7 [6], indicating that our convergent validity is adequate for AI literacy as well, despite the lower AVE measure. Cronbach's alpha (CA) values for our constructs are larger than the recommended threshold of 0.7, thus showing good reliability for all constructs [32]. Validity and reliability measures are summarized in Tab. 4. Our matrix of factor loadings, demonstrated in Tab. 5, shows that all items load highly (>0.5) on one factor each with low cross-loadings, and the correlations between factors are all below 0.7 (see Tab. 4). Furthermore, the AVE value of each of our constructs is larger than the squared correlation of that construct with every other construct, which is a discriminant validity measure suggested by Chin [25] and Fornell and Larcker [41]. Therefore, convergent validity and discriminant validity are sufficiently satisfied. We test for multicollinearity by determining the variance inflation factors (VIF). According to a rule of thumb, the VIF has to be lower than 10, otherwise, multicollinearity might be a serious problem [124]. All VIFs in our model are less than 2, which indicates that there are no issues of multicollinearity. ", "publication_ref": ["b40", "b5", "b31", "b24", "b40", "b123"], "figure_ref": [], "table_ref": []}, {"heading": "F SOFTWARE AND TOOLS", "text": "Tab. 8 contains all employed software and tools.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank our study participants as well as our anonymous reviewers, who helped improve this manuscript.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A ABBREVIATIONS", "text": "Tab. 3 contains our most commonly used abbreviation. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B CONSTRUCTS AND MEASUREMENT ITEMS", "text": "All items within the following constructs were measured on a 5-point Likert scale and mostly drawn (and adapted) from previous studies.\n(1) Informational Fairness (INFF)\n\u2022 The automated decision system explains decision-making procedures thoroughly. [31] \u2022 The automated decision system's explanations regarding procedures are reasonable. [31] \u2022 The automated decision system tailors communications to meet the applying individual's needs. [31] \u2022 I understand the process by which the decision was made. [11] \u2022 I received sufficient information to judge whether the decision-making procedures are fair or unfair. (2) Trustworthiness (TRST)\n\u2022 Given the provided explanations, I trust that the automated decision system makes good-quality decisions. [76] \u2022 Based on my understanding of the decision-making procedures, I know the automated decision system is not opportunistic. [26] \u2022 Based on my understanding of the decision-making procedures, I know the automated decision system is trustworthy. [26] \u2022 I think I can trust the automated decision system. [19] \u2022 The automated decision system can be trusted to carry out the loan application decision faithfully. [19] \u2022 In my opinion, the automated decision system is trustworthy. [19] (3) (Self-Assessed) AI Literacy (AILIT)\n\u2022 How would you describe your knowledge in the field of artificial intelligence?\n\u2022 Does your current employment include working with artificial intelligence?\n\u2022 I am confident interacting with artificial intelligence. [127] \u2022 I understand what the term artificial intelligence means.", "publication_ref": ["b30", "b30", "b30", "b10", "b75", "b25", "b25", "b18", "b18", "b18", "b126"], "figure_ref": [], "table_ref": []}, {"heading": "C EXPLANATION STYLES FOR ONE EXEMPLARY SETTING Condition (F)", "text": "A finance company offers loans on real estate in urban, semi-urban and rural areas. A potential customer first applies online for a specific loan, and afterwards the company assesses the customer's eligibility for that loan. An individual applied online for a loan at this company. The company denied the loan application. The decision to deny the loan was made by an automated decision system and communicated to the applying individual electronically and in a timely fashion.\nThe automated decision system explains that the following factors (in alphabetical order) on the individual were taken into account when making the loan application decision: ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Automated credit decisioning for enhanced efficiency", "journal": "", "year": "2021", "authors": " Actico"}, {"ref_id": "b1", "title": "Peeking inside the black-box: A survey on explainable artificial intelligence (XAI)", "journal": "IEEE Access", "year": "2018", "authors": "Amina Adadi; Mohammed Berrada"}, {"ref_id": "b2", "title": "Machine bias", "journal": "ProPublica", "year": "2016", "authors": "Julia Angwin; Jeff Larson; Surya Mattu; Lauren Kirchner"}, {"ref_id": "b3", "title": "Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI", "journal": "Information Fusion", "year": "2020", "authors": "Alejandro Barredo Arrieta; Natalia D\u00edaz-Rodr\u00edguez; Javier Del Ser; Adrien Bennetot; Siham Tabik; Alberto Barbado; Salvador Garcia; Sergio Gil-Lopez; Daniel Molina;  Benjamins"}, {"ref_id": "b4", "title": "One explanation does not fit all: A toolkit and taxonomy of AI explainability techniques", "journal": "", "year": "2019", "authors": "Vijay Arya; K E Rachel; Pin-Yu Bellamy; Amit Chen; Michael Dhurandhar;  Hind; Stephanie Samuel C Hoffman; Q Houde; Ronny Vera Liao; Aleksandra Luss;  Mojsilovi\u0107"}, {"ref_id": "b5", "title": "The Partial Least Squares (PLS) Approach to Causal Modeling: Personal Computer Adoption and Use as an Illustration", "journal": "", "year": "1995", "authors": "Donald Barclay; Christopher Higgins; Ronald Thompson"}, {"ref_id": "b6", "title": "Fairness and machine learning", "journal": "", "year": "2018", "authors": "Solon Barocas; Moritz Hardt; Arvind Narayanan"}, {"ref_id": "b7", "title": "Consumer-lending discrimination in the FinTech era", "journal": "Journal of Financial Economics", "year": "2021", "authors": "Robert Bartlett; Adair Morse; Richard Stanton; Nancy Wallace"}, {"ref_id": "b8", "title": "Expl(AI)ned: The impact of explainable artificial intelligence on cognitive processes", "journal": "", "year": "2021", "authors": "Kevin Bauer; Oliver Hinz;  Moritz Von Zahn"}, {"ref_id": "b9", "title": "The dark side of information: Overload, anxiety and other paradoxes and pathologies", "journal": "Journal of Information Science", "year": "2009", "authors": "David Bawden; Lyn Robinson"}, {"ref_id": "b10", "title": "It's reducing a human being to a percentage' -Perceptions of justice in algorithmic decisions", "journal": "", "year": "2018", "authors": "Reuben Binns; Max Van Kleek; Michael Veale; Ulrik Lyngs; Jun Zhao; Nigel Shadbolt"}, {"ref_id": "b11", "title": "Random forests", "journal": "", "year": "2001", "authors": "Leo Breiman"}, {"ref_id": "b12", "title": "Toward algorithmic accountability in public services: A qualitative study of affected community perspectives on algorithmic decision-making in child welfare services", "journal": "", "year": "2019", "authors": "Anna Brown; Alexandra Chouldechova; Emily Putnam-Hornstein; Andrew Tobin; Rhema Vaithianathan"}, {"ref_id": "b13", "title": "Alternative ways of assessing model fit", "journal": "Sociological Methods & Research", "year": "1992", "authors": "W Michael; Robert Browne;  Cudeck"}, {"ref_id": "b14", "title": "Gender shades: Intersectional accuracy disparities in commercial gender classification", "journal": "PMLR", "year": "2018", "authors": "Joy Buolamwini; Timnit Gebru"}, {"ref_id": "b15", "title": "The role of explanations on trust and reliance in clinical decision support systems", "journal": "IEEE", "year": "2015", "authors": "Adrian Bussone; Simone Stumpf; O' Dympna;  Sullivan"}, {"ref_id": "b16", "title": "Trustworthiness in electronic commerce: The role of privacy, security, and site attributes", "journal": "The Journal of Strategic Information Systems", "year": "2002", "authors": "France B\u00e9langer; Janine S Hiller; Wanda J Smith"}, {"ref_id": "b17", "title": "How companies are using simulations, competitions, and analytics to hire", "journal": "Harvard Business Review", "year": "2016", "authors": "Dennis Carey; Matt Smith"}, {"ref_id": "b18", "title": "The utilization of e-government services: Citizen trust, innovation and acceptance factors", "journal": "Information Systems Journal", "year": "2005", "authors": "Lemuria Carter; France B\u00e9langer"}, {"ref_id": "b19", "title": "Fairness in machine learning: A survey", "journal": "", "year": "2020", "authors": "Simon Caton; Christian Haas"}, {"ref_id": "b20", "title": "Productivity and selection of human capital with machine learning", "journal": "American Economic Review", "year": "2016", "authors": "Aaron Chalfin; Oren Danieli; Andrew Hillis; Zubin Jelveh; Michael Luca; Jens Ludwig; Sendhil Mullainathan"}, {"ref_id": "b21", "title": "Perceptions of fairness", "journal": "", "year": "2011", "authors": "David Chan"}, {"ref_id": "b22", "title": "Grounded theory", "journal": "Qualitative Psychology: A Practical Guide to Research Methods", "year": "2003", "authors": "Kathy Charmaz;  Smith"}, {"ref_id": "b23", "title": "Loan Prediction Problem Dataset", "journal": "", "year": "2019", "authors": "Debdatta Chatterjee"}, {"ref_id": "b24", "title": "The partial least squares approach to structural equation modeling", "journal": "Modern Methods for Business Research", "year": "1998", "authors": " Wynne W Chin"}, {"ref_id": "b25", "title": "Understanding customers' loyalty intentions towards online shopping: An integration of technology acceptance model and fairness theory", "journal": "Behaviour & Information Technology", "year": "2009", "authors": " Chao-Min; Hua-Yang Chiu; Szu-Yuan Lin; Meng-Hsiang Sun;  Hsu"}, {"ref_id": "b26", "title": "Fair prediction with disparate impact: A study of bias in recidivism prediction instruments", "journal": "Big Data", "year": "2017", "authors": "Alexandra Chouldechova"}, {"ref_id": "b27", "title": "Dark patterns of explainability, transparency, and user control for intelligent systems", "journal": "", "year": "2019", "authors": "Michael Chromik; Malin Eiband; Sarah Theres V\u00f6lkel; Daniel Buschek"}, {"ref_id": "b28", "title": "Justice at the millennium: A meta-analytic review of 25 years of organizational justice research", "journal": "Journal of Applied Psychology", "year": "2001", "authors": "Jason A Colquitt; Donald E Conlon; Michael J Wesson; O L H Christopher; Yee Porter;  Ng"}, {"ref_id": "b29", "title": "Justice, trust, and trustworthiness: A longitudinal analysis integrating three theoretical perspectives", "journal": "Academy of Management Journal", "year": "2011", "authors": "Jason A Colquitt; Jessica B Rodell"}, {"ref_id": "b30", "title": "Measuring justice and fairness", "journal": "", "year": "2015", "authors": "Jason A Colquitt; Jessica B Rodell"}, {"ref_id": "b31", "title": "What is coefficient alpha? An examination of theory and applications", "journal": "Journal of Applied Psychology", "year": "1993", "authors": "M Jose;  Cortina"}, {"ref_id": "b32", "title": "A case for humans-in-the-loop: Decisions in the presence of erroneous algorithmic scores", "journal": "", "year": "2020", "authors": "Maria De-Arteaga; Riccardo Fogliato; Alexandra Chouldechova"}, {"ref_id": "b33", "title": "Overcoming algorithm aversion: People will use imperfect algorithms if they can (even slightly) modify them", "journal": "Management Science", "year": "2018", "authors": "J Berkeley; Joseph P Dietvorst; Cade Simmons;  Massey"}, {"ref_id": "b34", "title": "Explaining models: An empirical study of how explanations impact fairness judgment", "journal": "", "year": "2019", "authors": "Jonathan Dodge; Q Vera Liao; Yunfeng Zhang; Rachel Ke Bellamy; Casey Dugan"}, {"ref_id": "b35", "title": "", "journal": "", "year": "2021", "authors": " Edelman"}, {"ref_id": "b36", "title": "Explainability pitfalls: Beyond dark patterns in explainable AI", "journal": "", "year": "2021", "authors": "Upol Ehsan; Mark O Riedl"}, {"ref_id": "b37", "title": "User attitudes towards algorithmic opacity and transparency in online reviewing platforms", "journal": "", "year": "2019", "authors": "Motahhare Eslami; Kristen Vaccaro; Min ; Kyung Lee; Amit Elazari Bar; Eric On; Karrie Gilbert;  Karahalios"}, {"ref_id": "b38", "title": "", "journal": "General Data Protection Regulation", "year": "2016", "authors": ""}, {"ref_id": "b39", "title": "Fair AI: Challenges and opportunities", "journal": "Business & Information Systems Engineering", "year": "2020", "authors": "Stefan Feuerriegel; Mateusz Dolata; Gerhard Schwabe"}, {"ref_id": "b40", "title": "Evaluating structural equation models with unobservable variables and measurement error", "journal": "Journal of Marketing Research", "year": "1981", "authors": "Claes Fornell; David F Larcker"}, {"ref_id": "b41", "title": "An R Companion to Applied Regression", "journal": "Sage", "year": "2019", "authors": "John Fox; Sanford Weisberg"}, {"ref_id": "b42", "title": "Organizational justice, trustworthiness, and trust: A multifoci examination", "journal": "Group & Organization Management", "year": "2010", "authors": "Lance Frazier; Paul D Johnson; Mark Gavin; Janaki Gooty; Bradley Snow"}, {"ref_id": "b43", "title": "Automation bias: Empirical results assessing influencing factors", "journal": "International Journal of Medical Informatics", "year": "2014", "authors": "Kate Goddard; Abdul Roudsari; Jeremy C Wyatt"}, {"ref_id": "b44", "title": "Explainable AI: The new 42?", "journal": "Springer", "year": "2018", "authors": "Randy Goebel; Ajay Chander; Katharina Holzinger; Freddy Lecue; Zeynep Akata; Simone Stumpf; Peter Kieseberg; Andreas Holzinger"}, {"ref_id": "b45", "title": "The dark (patterns) side of UX design", "journal": "", "year": "2018", "authors": "Colin M Gray; Yubo Kou; Bryan Battles; Joseph Hoggatt; Austin L Toombs"}, {"ref_id": "b46", "title": "Human perceptions of fairness in algorithmic decision making: A case study of criminal risk prediction", "journal": "", "year": "2018", "authors": "Nina Grgi\u0107-Hla\u010da; Elissa M Redmiles; Krishna P Gummadi; Adrian Weller"}, {"ref_id": "b47", "title": "Dimensions of diversity in human perceptions of algorithmic fairness", "journal": "", "year": "2020", "authors": "Nina Grgi\u0107-Hla\u010da; Adrian Weller; Elissa M Redmiles"}, {"ref_id": "b48", "title": "Beyond distributive fairness in algorithmic decision making: Feature selection for procedurally fair learning", "journal": "", "year": "2018", "authors": "Nina Grgi\u0107-Hla\u010da; Muhammad Bilal Zafar; Krishna P Gummadi; Adrian Weller"}, {"ref_id": "b49", "title": "On the ethics of algorithmic decisionmaking in healthcare", "journal": "Journal of Medical Ethics", "year": "2020", "authors": "Thomas Grote; Philipp Berens"}, {"ref_id": "b50", "title": "A survey of methods for explaining black box models", "journal": "ACM Computing Surveys (CSUR)", "year": "2018", "authors": "Riccardo Guidotti; Anna Monreale; Salvatore Ruggieri; Franco Turini; Fosca Giannotti; Dino Pedreschi"}, {"ref_id": "b51", "title": "A Primer on Partial Least Squares Structural Equation Modeling (PLS-SEM)", "journal": "Sage Publications", "year": "2016", "authors": "Joseph F Hair Jr; G Tomas; M Hult; Christian Ringle; Marko Sarstedt"}, {"ref_id": "b52", "title": "Automated decision making comes of age", "journal": "MIT Sloan Management Review", "year": "2005", "authors": "G Jeanne; Thomas H Harris;  Davenport"}, {"ref_id": "b53", "title": "Predictive policing algorithms are racist. They need to be dismantled", "journal": "MIT Technology Review", "year": "2020", "authors": "Will Douglas Heaven"}, {"ref_id": "b54", "title": "Research in the crowdsourcing age: A case study", "journal": "", "year": "2016", "authors": "Paul Hitlin"}, {"ref_id": "b55", "title": "Assessing affirmative action", "journal": "Journal of Economic literature", "year": "2000", "authors": "Harry Holzer; David Neumark"}, {"ref_id": "b56", "title": "Preference for modes of dispute resolution as a function of process and decision control", "journal": "Journal of Experimental Social Psychology", "year": "1978", "authors": "Pauline Houlden; Stephen Latour; Laurens Walker; John Thibaut"}, {"ref_id": "b57", "title": "Auditing for discrimination in algorithms delivering job ads", "journal": "", "year": "2021", "authors": "Basileal Imana; Aleksandra Korolova; John Heidemann"}, {"ref_id": "b58", "title": "How FinTechs can enable better support to FIs' credit decisioning?", "journal": "Infosys", "year": "2019", "authors": ""}, {"ref_id": "b59", "title": "Formalizing trust in artificial intelligence: Prerequisites, causes and goals of human trust in AI", "journal": "", "year": "2021", "authors": "Alon Jacovi; Ana Marasovi\u0107; Tim Miller; Yoav Goldberg"}, {"ref_id": "b60", "title": "Fairness in learning: Classic and contextual bandits", "journal": "", "year": "2016", "authors": "Matthew Joseph; Michael Kearns; Jamie Morgenstern; Aaron Roth"}, {"ref_id": "b61", "title": "On the relation of trust and explainability: Why to engineer for trustworthiness", "journal": "", "year": "2021", "authors": "Lena K\u00e4stner; Markus Langer; Veronika Lazar; Astrid Schom\u00e4cker; Timo Speith; Sarah Sterz"}, {"ref_id": "b62", "title": "Measuring model fit", "journal": "", "year": "2015", "authors": "A David;  Kenny"}, {"ref_id": "b63", "title": "How much information? Effects of transparency on trust in an algorithmic interface", "journal": "", "year": "2016", "authors": "F Ren\u00e9;  Kizilcec"}, {"ref_id": "b64", "title": "Inherent trade-offs in the fair determination of risk scores", "journal": "", "year": "2016", "authors": "Jon Kleinberg; Sendhil Mullainathan; Manish Raghavan"}, {"ref_id": "b65", "title": "Principles and Practice of Structural Equation Modeling", "journal": "Guilford Publications", "year": "2015", "authors": "Rex B Kline"}, {"ref_id": "b66", "title": "Understanding decision-making in recruitment: Opportunities and challenges for information technology", "journal": "GROUP", "year": "2019", "authors": "Thomas Sami Koivunen; Ekaterina Olsson; Aki Olshannikova;  Lindberg"}, {"ref_id": "b67", "title": "Use of ranks in one-criterion variance analysis", "journal": "Journal of the American statistical Association", "year": "1952", "authors": "H William; W Kruskal; Wallis Allen"}, {"ref_id": "b68", "title": "Analyzing Qualitative Data with MAXQDA", "journal": "Springer", "year": "2019", "authors": "Udo Kuckartz; Stefan R\u00e4diker"}, {"ref_id": "b69", "title": "Do you comply with AI? -Personalized explanations of learning algorithms and their impact on employees' compliance behavior", "journal": "", "year": "2020", "authors": "Niklas Kuehl; Jodie Lobana; Christian Meske"}, {"ref_id": "b70", "title": "Too much, too little, or just right? Ways explanations impact end users' mental models", "journal": "IEEE", "year": "2013", "authors": "Todd Kulesza; Simone Stumpf; Margaret Burnett; Sherry Yang; Irwin Kwan; Weng-Keen Wong"}, {"ref_id": "b71", "title": "In hiring, algorithms beat instinct", "journal": "Harvard Business Review", "year": "2014", "authors": "David M Nathan R Kuncel; Deniz S Klieger;  Ones"}, {"ref_id": "b72", "title": "Spare me the details: How the type of information about automated interviews influences applicant reactions", "journal": "International Journal of Selection and Assessment", "year": "2021", "authors": "Markus Langer; Kevin Baum; Cornelius J K\u00f6nig; Viviane H\u00e4hne; Daniel Oster; Timo Speith"}, {"ref_id": "b73", "title": "What do we want from explainable artificial intelligence (XAI)? -A stakeholder perspective on XAI and a conceptual model guiding interdisciplinary XAI research", "journal": "Artificial Intelligence", "year": "2021", "authors": "Markus Langer; Daniel Oster; Timo Speith; Holger Hermanns; Lena K\u00e4stner; Eva Schmidt; Andreas Sesing; Kevin Baum"}, {"ref_id": "b74", "title": "Trust in automation: Designing for appropriate reliance", "journal": "Human Factors", "year": "2004", "authors": "D John; Katrina A Lee;  See"}, {"ref_id": "b75", "title": "Understanding perception of algorithmic decisions: Fairness, trust, and emotion in response to algorithmic management", "journal": "Big Data & Society", "year": "2018", "authors": "Min Kyung; Lee "}, {"ref_id": "b76", "title": "Algorithmic mediation in group decisions: Fairness perceptions of algorithmically mediated vs. discussion-based social division", "journal": "", "year": "2017", "authors": "Min Kyung; Lee ; Su Baykal"}, {"ref_id": "b77", "title": "Procedural justice in algorithmic fairness: Leveraging transparency and outcome control for fair algorithmic mediation", "journal": "CSCW", "year": "2019", "authors": "Min Kyung; Lee ; Anuraag Jain; Jin Hea; Shashank Cha; Daniel Ojha;  Kusbit"}, {"ref_id": "b78", "title": "Who is included in human perceptions of AI? Trust and perceived fairness around healthcare AI and cultural mistrust", "journal": "", "year": "2021", "authors": "Min Kyung; Lee ; Katherine Rich"}, {"ref_id": "b79", "title": "SoSci Survey", "journal": "SoSci Survey GmbH", "year": "2019", "authors": "D J Leiner"}, {"ref_id": "b80", "title": "The tyranny of data? The bright and dark sides of data-driven decision-making for social good", "journal": "Springer", "year": "2017", "authors": "Bruno Lepri; Jacopo Staiano; David Sangokoya; Emmanuel Letouz\u00e9; Nuria Oliver"}, {"ref_id": "b81", "title": "The role of abduction in learning to use a computer system", "journal": "", "year": "1982", "authors": "Clayton Lewis; Robert Mack"}, {"ref_id": "b82", "title": "Why and why not explanations improve the intelligibility of context-aware intelligent systems", "journal": "", "year": "2009", "authors": "Brian Y Lim; Anind K Dey; Daniel Avrahami"}, {"ref_id": "b83", "title": "Decision control and process control effects on procedural fairness judgments", "journal": "Journal of Applied Social Psychology", "year": "1983", "authors": "Allan Lind; Robin I Lissak; Donald E Conlon"}, {"ref_id": "b84", "title": "Calibrated fairness in bandits", "journal": "", "year": "2017", "authors": "Yang Liu; Goran Radanovic; Christos Dimitrakakis; Debmalya Mandal; David C Parkes"}, {"ref_id": "b85", "title": "Fairness in machine learning: Against false positive rate equality as a measure of fairness", "journal": "Journal of Moral Philosophy", "year": "2021", "authors": "Robert Long"}, {"ref_id": "b86", "title": "Mediation analysis", "journal": "Annu. Rev. Psychol", "year": "2007", "authors": "Amanda J David P Mackinnon; Matthew S Fairchild;  Fritz"}, {"ref_id": "b87", "title": "On a test of whether one of two random variables is stochastically larger than the other", "journal": "The Annals of Mathematical Statistics", "year": "1947", "authors": "B Henry; Donald R Mann;  Whitney"}, {"ref_id": "b88", "title": "A survey on bias and fairness in machine learning", "journal": "", "year": "2019", "authors": "Ninareh Mehrabi; Fred Morstatter; Nripsuta Saxena; Kristina Lerman; Aram Galstyan"}, {"ref_id": "b89", "title": "Explanation in artificial intelligence: Insights from the social sciences", "journal": "Artificial intelligence", "year": "2019", "authors": "Tim Miller"}, {"ref_id": "b90", "title": "Interpretable machine learning", "journal": "", "year": "2020", "authors": "Christoph Molnar"}, {"ref_id": "b91", "title": "This thing called fairness: Disciplinary confusion realizing a value in technology", "journal": "CSCW", "year": "2019", "authors": "Joshua A Deirdre K Mulligan; Nitin Kroll; Richmond Y Kohli;  Wong"}, {"ref_id": "b92", "title": "Strategic opportunities (and challenges) of algorithmic decision-making: A call for action on the long-term societal effects of 'datification", "journal": "The Journal of Strategic Information Systems", "year": "2015", "authors": "Sue Newell; Marco Marabelli"}, {"ref_id": "b93", "title": "Prolific.ac-A subject pool for online experiments", "journal": "Journal of Behavioral and Experimental Finance", "year": "2018", "authors": "Stefan Palan; Christian Schitter"}, {"ref_id": "b94", "title": "Running experiments on Amazon Mechanical Turk", "journal": "Judgment and Decision making", "year": "2010", "authors": "Gabriele Paolacci; Jesse Chandler; Panagiotis G Ipeirotis"}, {"ref_id": "b95", "title": "The Black Box Society", "journal": "Harvard University Press", "year": "2015", "authors": "Frank Pasquale"}, {"ref_id": "b96", "title": "Scikit-learn: Machine learning in Python", "journal": "The Journal of Machine Learning Research", "year": "2011", "authors": "Fabian Pedregosa; Ga\u00ebl Varoquaux; Alexandre Gramfort; Vincent Michel; Bertrand Thirion; Olivier Grisel; Mathieu Blondel; Peter Prettenhofer; Ron Weiss; Vincent Dubourg"}, {"ref_id": "b97", "title": "Demographics and discussion influence views on algorithmic fairness", "journal": "", "year": "2017", "authors": "Emma Pierson"}, {"ref_id": "b98", "title": "Prolific vs", "journal": "MTurk", "year": "2022", "authors": " Prolific"}, {"ref_id": "b99", "title": "R: A Language and Environment for Statistical Computing", "journal": "", "year": "2017", "authors": "Team Core"}, {"ref_id": "b100", "title": "cSEM: Composite-Based Structural Equation Modeling", "journal": "", "year": "2020", "authors": "E Manuel; Florian Rademaker;  Schuberth"}, {"ref_id": "b101", "title": "psych: Procedures for Psychological, Psychometric, and Personality Research", "journal": "", "year": "2020", "authors": "William Revelle"}, {"ref_id": "b102", "title": "lavaan: An R package for structural equation modeling", "journal": "Journal of Statistical Software", "year": "2012", "authors": "Yves Rosseel"}, {"ref_id": "b103", "title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead", "journal": "Nature Machine Intelligence", "year": "2019", "authors": "Cynthia Rudin"}, {"ref_id": "b104", "title": "British grading debacle shows pitfalls of automating government", "journal": "The New York Times", "year": "2020", "authors": "Adam Satariano"}, {"ref_id": "b105", "title": "How do fairness definitions fare? Examining public attitudes towards algorithmic definitions of fairness", "journal": "Ethics, and Society", "year": "2019", "authors": "Ani Nripsuta; Karen Saxena; Evan Huang; Goran Defilippis; David C Radanovic; Yang Parkes;  Liu"}, {"ref_id": "b106", "title": "Towards warranted trust: A model on the relation between actual and perceived system trustworthiness", "journal": "Mensch und Computer", "year": "2021", "authors": "Nadine Schlicker; Markus Langer"}, {"ref_id": "b107", "title": "What to expect from opening up 'black boxes'? Comparing perceptions of justice between human and automated agents", "journal": "Computers in Human Behavior", "year": "2021", "authors": "Nadine Schlicker; Markus Langer; Sonja \u00d6tting; Kevin Baum; Cornelius J K\u00f6nig; Dieter Wallach"}, {"ref_id": "b108", "title": "Appropriate fairness perceptions? On the effectiveness of explanations in enabling people to assess the fairness of automated decision systems", "journal": "", "year": "2021", "authors": "Jakob Schoeffer; Niklas Kuehl"}, {"ref_id": "b109", "title": "A ranking approach to fair classification", "journal": "", "year": "2021", "authors": "Jakob Schoeffer; Niklas Kuehl; Isabel Valera"}, {"ref_id": "b110", "title": "Perceptions of fairness and trustworthiness based on explanations in human vs. automated decision-making", "journal": "", "year": "2021", "authors": "Jakob Schoeffer; Yvette Machowski; Niklas Kuehl"}, {"ref_id": "b111", "title": "Factors influencing public acceptance of innovative food technologies and products", "journal": "Trends in Food Science & Technology", "year": "2008", "authors": "Michael Siegrist"}, {"ref_id": "b112", "title": "Accountability and automation bias", "journal": "International Journal of Human Computer Studies", "year": "2000", "authors": "Linda J Skitka; Kathleen Mosier; Mark D Burdick"}, {"ref_id": "b113", "title": "Perception of risk", "journal": "Science", "year": "1987", "authors": "Paul Slovic"}, {"ref_id": "b114", "title": "Perceived risk: Psychological factors and social implications", "journal": "Proceedings of the Royal Society of London. A. Mathematical and Physical Sciences", "year": "1981", "authors": "Paul Slovic; Baruch Fischhoff; Sarah Lichtenstein"}, {"ref_id": "b115", "title": "In Poland, a law made loan algorithms transparent", "journal": "", "year": "2022", "authors": "Konrad "}, {"ref_id": "b116", "title": "Procedural Justice: A Psychological Analysis", "journal": "L. Erlbaum Associates", "year": "1975", "authors": "John W ; Thibaut ; Laurens Walker"}, {"ref_id": "b117", "title": "AI can make bank loans more fair", "journal": "Harvard Business Review", "year": "2020", "authors": "Sian Townson"}, {"ref_id": "b118", "title": "A \"third wheel\" effect in health decision making involving artificial entities: A psychological perspective", "journal": "Frontiers in Public Health", "year": "2020", "authors": "Stefano Triberti; Ilaria Durosini; Gabriella Pravettoni"}, {"ref_id": "b119", "title": "Fairness and decision-making in collaborative shift scheduling systems", "journal": "", "year": "2020", "authors": "Alarith Uhde; Nadine Schlicker; Dieter P Wallach; Marc Hassenzahl"}, {"ref_id": "b120", "title": "When do we need procedural fairness? The role of trust in authority", "journal": "Journal of Personality and Social Psychology", "year": "1998", "authors": "Kees Van Den Bos; A M Henk; E Wilke; Lind Allan"}, {"ref_id": "b121", "title": "Python Tutorial", "journal": "", "year": "1995", "authors": "Guido Van Rossum; Fred L Drake"}, {"ref_id": "b122", "title": "How to evaluate trust in AI-assisted decision making? A survey of empirical methodologies", "journal": "", "year": "2021", "authors": "Oleksandra Vereschak; Gilles Bailly; Baptiste Caramiaux"}, {"ref_id": "b123", "title": "Regression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models", "journal": "Springer Science & Business Media", "year": "2011", "authors": "Eric Vittinghoff; Stephen C David V Glidden; Charles E Shiboski;  Mcculloch"}, {"ref_id": "b124", "title": "Factors influencing perceived fairness in algorithmic decision-making: Algorithm outcomes, development procedures, and individual differences", "journal": "", "year": "2020", "authors": "Ruotong Wang; Maxwell Harper; Haiyi Zhu"}, {"ref_id": "b125", "title": "Deontological ethics by monotonicity shape constraints", "journal": "PMLR", "year": "2020", "authors": "Serena Wang; Maya Gupta"}, {"ref_id": "b126", "title": "Construction of an instrument to measure student information and communication technology skills, experience and attitudes to e-learning", "journal": "Computers in Human Behavior", "year": "2010", "authors": "Ann Wilkinson; Julia Roberts; Alison E While"}, {"ref_id": "b127", "title": "A qualitative exploration of perceptions of algorithmic fairness", "journal": "", "year": "2018", "authors": "Allison Woodruff; Sarah E Fox; Steven Rousso-Schindler; Jeffrey Warshaw"}, {"ref_id": "b128", "title": "Service fairness and customer satisfaction in internet banking: Exploring the mediating effects of trust and customer value", "journal": "Internet Research", "year": "2012", "authors": "Yu-Qian Zhu; Houn-Gee Chen"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Introduction of use case in questionnaires.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Baseline without further explanations. (F) Disclosure of factors. (FFI) Disclosure of factors and factor importance. (FFICF) Disclosure of factors, factor importance, and counterfactual scenarios.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: Distributions of responses for informational fairness (INFF) and trustworthiness (TRST) per condition.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: Full structural equation model (SEM) including measurement model; *\ud835\udc5d < 0.05, **\ud835\udc5d < 0.01, ***\ud835\udc5d < 0.001.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 :5Figure 5: Inappropriate factors according to responses from study participants, broken down by condition.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Means and standard deviations of response values for informational fairness (INFF) and trustworthiness (TRST) by condition. All items were measured on 5-point Likert scales.", "figure_data": "Condition M(INFF) SD(INFF) M(TRST) SD(TRST)(Base)2.711.163.010.89(F)2.931.163.101.12(FFI)3.301.053.430.99(FFICF)3.680.943.510.99"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Pairwise differences in perceptions of informational fairness (INFF) and trustworthiness (TRST) between conditions.", "figure_data": "INFFTRSTCondition 1 Condition 2 Difference Condition 1 Condition 2 Difference(Base)(F)n/s(Base)(F)n/s(Base)(FFI)***(Base)(FFI)***(Base)(FFICF)***(Base)(FFICF)***(F)(FFI)*(F)(FFI)n/s(F)(FFICF)***(F)(FFICF)**(FFI)(FFICF)**(FFI)(FFICF)n/sNotes: *\ud835\udc5d < 0.05; **\ud835\udc5d < 0.01; ***\ud835\udc5d < 0.001; n/s: not significant"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Correlations and measurement information for latent factors.Detailed information on the results of the SEM model estimation, including path estimates, standard errors (SE), z-values, p-values, and standardized estimates (Std.lv) are reported in Tab. 6. A breakdown of direct and indirect effects of independent variables on trustworthiness (TRST) is given in Tab. 7.", "figure_data": "Factor MSD CA CR AVE INFF TRST AILITINFF3.15 0.87 0.87 0.88 0.601.00TRST 3.26 0.84 0.94 0.94 0.730.671.00AILIT 2.87 0.61 0.71 0.72 0.410.250.181.00Notes: M = Mean; SD = Standard deviationE SEM MODEL: RESULTS OF MODEL ESTIMATION"}], "formulas": [], "doi": "10.1145/3531146.3533218"}
