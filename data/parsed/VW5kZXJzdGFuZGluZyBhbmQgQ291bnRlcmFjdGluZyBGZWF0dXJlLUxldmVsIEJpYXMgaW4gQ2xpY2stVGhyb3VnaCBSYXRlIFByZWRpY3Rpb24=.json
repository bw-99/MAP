{"Understanding and Counteracting Feature-Level Bias in Click-Through Rate Prediction": "Jinqiu Jin 1 , Sihao Ding 1 , Wenjie Wang 2 , Fuli Feng 1 \u2020 1 University of Science and Technology of China, 2 National University of Singapore {jjq20021015,dsihao}@mail.ustc.edu.cn,{wenjiewang96,fulifeng93}@gmail.com", "ABSTRACT": "Common click-through rate (CTR) prediction recommender models tend to exhibit feature-level bias, which leads to unfair recommendations among item groups and inaccurate recommendations for users. While existing methods address this issue by adjusting the learning of CTR models, such as through additional optimization objectives, they fail to consider how the bias is caused within these models. To address this research gap, our study performs a topdown analysis on representative CTR models. Through blocking different components of a trained CTR model one by one, we identify the key contribution of the linear component to feature-level bias. We conduct a theoretical analysis of the learning process for the weights in the linear component, revealing how group-wise properties of training data influence them. Our experimental and statistical analyses demonstrate a strong correlation between imbalanced positive sample ratios across item groups and feature-level bias. Based on this understanding, we propose a minimally invasive yet effective strategy to counteract feature-level bias in CTR models by removing the biased linear weights from trained models. Additionally, we present a linear weight adjusting strategy that requires fewer random exposure records than relevant debiasing methods. The superiority of our proposed strategies are validated through extensive experiments on three real-world datasets.", "CCS CONCEPTS": "\u00b7 Information systems \u2192 Recommender systems .", "KEYWORDS": "Recommender Systems, Bias & Fairness in Recommendation, CTR Prediction", "ACMReference Format:": "Jinqiu Jin, Sihao Ding, Wenjie Wang, Fuli Feng. 2018. Understanding and Counteracting Feature-Level Bias in Click-Through Rate Prediction. In Proceedings of Make sure to enter the correct conference title from your rights confirmation emai (Conference acronym 'XX). ACM, New York, NY, USA, 11 pages. https://doi.org/XXXXXXX.XXXXXXX \u2020 Corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference acronym 'XX, June 03-05, 2018, Woodstock, NY ACM ISBN 978-1-4503-XXXX-X/18/06...$15.00 \u00a9 2018 Association for Computing Machinery. https://doi.org/XXXXXXX.XXXXXXX Figure 1: A toy example to illustrate feature-level bias. (a) Ideal and Actual denote the expected number of recommendation by ground truth and by CTR model, respectively. (b) Item-side unfairness, group A and group B are over-recommended and under-recommended, respectively. (c) Hurting user experiences, the model recommends 60% A and 40% B to a user ( ID=23 ), whereas his true preference consists of 40% A and 60% B. (a) Illustration of Feature-level Bias causes\u2026 (b) Impact on Item-Side (Unfairness) A B More opportunities of recommending to users Less opportunities and at disadvantage (c) Impact on User-Side (Hurt Experiences) User True Preference: Actual Recommendation: 40% 60% 40% 60% ID:23", "1 INTRODUCTION": "Recommender systems are commonly used to address information overload on various web platforms by providing personalized information retrieval [17, 21]. Click-through-rate prediction is a critical task in recommender systems that leverages user-item features to rank candidate items for better personalization [70]. However, CTR models are susceptible to feature-level bias , which leads to over-recommendation or under-recommendation of different item groups based on a feature field (e.g., genre). This bias can cause unfairness among item groups and negatively impact user experiences. Figure 1 illustrates this issue through a toy example where group A is over-recommended while group B is under-recommended, leading to deviation from the true preference distribution of some users. It is thus essential to counteract the feature-level bias of CTR models. Considering the wide usage of Factorization Machine [45] (FM) and its extensions [17, 20, 62], we first study the role of linear Existing methods mainly deal with the feature-level bias by adjusting the learning of CTR models. For instance, a) regularization [16], which adjusts the learning objective by applying fairness or bias oriented terms to regulate the outputs of CTR models; b) adversarial training [73], which adjusts the learning procedure by adding an adversary to control the discrimination between groups at the level of item representations or model outputs; c) causal inference [54], which adjusts the learning target as the causal effect of exposure through the potential outcome framework [48] or structural causal models [43]. Nevertheless, there remains an open problem about the mechanism of leading to the feature-level bias in CTR models, i.e., how the bias generates from raw training data to biased model outputs, which is important to the development of trustworthy CTR models and the regulation of CTR models [13]. This calls for answering the following question: Which component of the CTR model mostly explains the feature-level bias? feature projection and feature interaction in misleading the recommendation. Specifically, we compare the recommendations of well-trained FM and NFM [20] when their linear component or high-order component is blocked. Results on benchmark datasets reveal the dominant role of the linear component in differentiating item groups, i.e., generating feature-level bias. Subsequently, we conduct theoretical analysis on the learning process of the linear weights related to the bias field, discovering their connection to the statistics of positive and negative samples in the training data. Lastly, results of statistical analysis further validate that the uneven distribution of positive sample ratio over item groups with different feature values is the cause of feature-level bias. Taking one step further, according to the analysis, we pursue minimally invasive strategies for counteracting the feature-level bias to avoid the overhead of adjusting the learning of CTR models. In particular, we design two debiasing strategies that perform minimally invasive treatments on the well-trained CTR model by adjusting the linear weights. To block the impact of the featurelevel bias, we first propose a linear weight reduction strategy, which controls the contribution of linear weights with a linear coefficient. Furthermore, we pursue better estimation of user preference by adjusting the linear weights according to limited random exposure data with a linear weight reconstruction strategy. We conduct extensive experiments on three benchmark datasets. The results show the effectiveness of the strategies, which can remarkably improve itemside fairness and recommendation performance. The code and data are available at https://github.com/mitao-cat/feature-level_bias.git. In a nutshell, the contributions of this work are three folds: \u00b7 We uncover the mechanism of generating feature-level bias in CTR models, which reveals the key role of linear weights and the positive sample ratio. \u00b7 Weconductextensive experiments on three benchmark datasets, validating the rationality and effectiveness of our methods. \u00b7 We design two simple-yet-effective methods for counteracting feature-level bias, which is applicable to widely-used FM-based CTR models with limited overhead.", "2 PRELIMINARIES": "In this section, we briefly introduce the basic concepts of CTR prediction and feature-level bias.", "2.1 CTR Prediction": "CTR prediction is an important task in recommender systems, which aims at using abundant features to estimate the probability of user clicks over items [70]. The key of the CTR task lies in the comprehensive modeling of the rich user and item features. Specifically, we introduce the CTR prediction task as follows. Input. We denote the training samples for CTR models as D = {( \ud835\udc99 , \ud835\udc66 )} , where \ud835\udc99 \u2208 R \ud835\udc5b is a sparse vector and \ud835\udc66 \u2208 { 0 , 1 } is the label ( i.e., click or not). The label is typically collected from previous exposure strategies. The feature vector describes a pair of user and item with \ud835\udc5b features. Following the previous work [24], we suppose that the feature vector \ud835\udc99 contains different fields , where a field contains different features corresponding to different categories of the field. For example, we represent the click of a male user on an action and sci-fi movie as:  where Genre , MovieID and Gender are fields with values {Action,SciFi} 1 , Male and 1024 , respectively. Prediction. The CTR model typically estimates the click probability \ud835\udc43 ( \ud835\udc66 = 1 | \ud835\udc99 ) with consideration of both raw features and feature-interactions. We focus on the classical FM [45] and its following extensions which are widely used in practical applications. These FM-based CTR models can be abstracted as:  where \ud835\udc64 0 is the global bias 2 ; \u02dd \ud835\udc5b \ud835\udc56 = 1 \ud835\udc64 \ud835\udc56 \ud835\udc65 \ud835\udc56 is a linear component where \ud835\udc64 \ud835\udc56 models the direct contribution of the \ud835\udc56 -th feature to the prediction of clicks; and \ud835\udc53 \ud835\udf03 ( \ud835\udc99 ) with parameters \ud835\udf03 models feature interactions which has model-specific design, e.g., FM [45] uses feature embeddings to model second-order interactions, NFM [20] uses deep neural networks to model higher-order interactions. Optimization. The parameters of a CTR model is optimized over the historical clicks D by minimizing a recommendation loss, such as the commonly used binary cross-entropy (BCE) loss:  where |D| denotes the number of training samples and \ud835\udf0e (\u00b7) denotes the sigmoid function to normalize the prediction. For briefness, we omit the regularization term such as the \ud835\udc3f 2-norm, which is widely used to prevent overfitting.", "2.2 Feature-level Bias": "Feature-level bias in CTR prediction refers that CTR models may over/under-recommend different item groups w.r.t. a given feature field. In other words, the recommendation is biased towards some item groups which are given more recommendation opportunities than deserved [18], i.e., some items are over-scored due to their bias feature. Given a CTR model, we study the feature-level bias of its recommendations (one ranking list per user) over a set of testing samples D \ud835\udc61 . As to user \ud835\udc62 , let \ud835\udc58 + \ud835\udc62 and \ud835\udc58 -\ud835\udc62 denote the number of positive and negative samples in the testing set D \ud835\udc61 given by the user. Accordingly, we take the top\ud835\udc58 + \ud835\udc62 ranked items in the ranking list of user \ud835\udc62 as the exposures, which is denoted as P \ud835\udc62 . Without loss of generality, we assume the bias field has \ud835\udc58 features, e.g., there are \ud835\udc58 different video tags, leading to \ud835\udc58 item groups. Furthermore, we assume the bias field is placed at the beginning of the feature vector \ud835\udc99 , i.e., { \ud835\udc65 \ud835\udc57 | \ud835\udc57 \u2208 [ 1 , \ud835\udc58 ]} are the bias features. In particular, we adopt the following metric to measure the feature-level bias: \u00b7 Exposure-to-Hit Ratio ( \ud835\udc38\ud835\udc3b\ud835\udc45 ). Given a feature \ud835\udc57 in the bias field, the Exposure-to-Hit Ratio can be defined as:  1 Note that some fields may take multiple feature values, i.e., a user or item could belong to different groups at the same time. 2 Note that \ud835\udc64 0 is omitted in the following analysis since it is constant to items with different feature values. Figure 2: An example to illustrate the calculation of EHR. Circle, diamond and triangle denote three different item groups. GT denotes the ground-truth of user interactions. Rec denotes the ranked user list given by the CTR model. GT Rec GT Rec GT Rec \ud835\udc38\ud835\udc3b\ud835\udc45 = 3 + 2 + 2 2 + 1 + 1 = 1.75 > 1 (over - recommended) \ud835\udc38\ud835\udc3b\ud835\udc45 = 0 + 0 + 1 1 + 1 + 2 = 0.25 < 1 (under - recommended) \ud835\udc38\ud835\udc3b\ud835\udc45 = 2 + 2 + 0 2 + 2 + 0 = 1 (absolutely fair) : positive samples (y=1) : negative samples (y=0) : system exposure (top- \ud835\udc58\ud835\udc62 + ranked items) : not exposed \ud835\udc3c ( \ud835\udc99 , \ud835\udc57 ) indicates whether the sample has the bias feature, i.e., the related item belongs to the \ud835\udc57 -th group. \ud835\udc3c ( \ud835\udc99 , \ud835\udc57 )\u2227 \ud835\udc66 = 1 means a user click on the \ud835\udc57 -th group ( i.e., \ud835\udc3c ( \ud835\udc99 , \ud835\udc57 ) = 1), \ud835\udc3c ( \ud835\udc99 , \ud835\udc57 ) = 0 otherwise. \ud835\udc38\ud835\udc3b\ud835\udc45 ( \ud835\udc57 ) calculates the ratio between the number of exposures ( i.e., ranked in user's top\ud835\udc58 + \ud835\udc62 list) belonging to the \ud835\udc57 -th group and the number of clicks ( i.e., positive sample in user list) on the \ud835\udc57 -th group within the testing set D \ud835\udc61 . Figure 2 gives an example about the calculation process of EHR. For the triangle group, the model correctly ranks all positive samples in each user's top\ud835\udc58 + \ud835\udc62 list, without falsely putting forward any negative samples. Accordingly, \ud835\udc38\ud835\udc3b\ud835\udc45 (\u25b3) = 1, which means the CTR model exactly gives the triangle group recommendation opportunities it deserves. In contrast, the CTR model shows strong bias towards the circle group as compared to the diamond group as the model tends to rank negative circle samples before the positive diamond samples. As a result, \ud835\udc38\ud835\udc3b\ud835\udc45 (\u25e6) > 1 ( i.e., over-recommended) and \ud835\udc38\ud835\udc3b\ud835\udc45 (\u22c4) < 1 ( i.e., under-recommended), indicating their unequal recommendation opportunities [16].", "2.3 Further Discussion": "2.3.1 Connection between EHR and Fairness Notions. Feature-level bias has strong connection with item-side group fairness, which requires that different item groups should be treated similarly by the model [5]. Among the fairness notions, Equal Opportunity (EO) [18] is the most related to Exposure-to-Hit Ratio. EO requires that the True Positive Rate (TPR) across groups are the same [16]. Formally,  where \ud835\udc50 \u2208 [ 0 , 1 ] denotes a constant value. However, as TPR takes value between 0 and 1, it's hard to judge whether the item group is over-recommended ( i.e., at advantage) or under-recommended ( i.e., at disadvantage) by the CTR model, which is the key of analyzing feature-level bias. In contrast, the Exposure-to-Hit Ratio reflects the recommendation opportunity of an item group (see Figure 2). Comparing the EHR value to 1 allows for easy determination of whether a group is over-recommended or under-recommended. Furthermore, in the ideal case without feature-level bias ( i.e., \u2200 \ud835\udc57 \u2208 [ 1 , \ud835\udc58 ] , \ud835\udc38\ud835\udc3b\ud835\udc45 ( \ud835\udc57 ) = 1), recommendation probabilities for different groups would be relatively similar and approach the EO standard. In this paper, we leverage Exposure-to-Hit Ratio to study the featurelevel bias, and use conventional fairness notions ( e.g., EO) to measure its impact on item-side fairness. 2.3.2 Connection between Feature-level Bias and Recommendation Qualities. Recent literatures [7, 30] reveal that biases could produce Figure 3: (a) Illustration of item-side unfairness w.r.t. \ud835\udc43 ( \ud835\udc57, \ud835\udc3e = 5 ) . (b) An example of the gap between History and True Preferences and the amplification effect of the CTR model. History denotes the ratio of positive samples in a group to the total positive samples collected from previous recommendations. CTR Model denotes the ratio of predicted positive samples in a group to the total positive samples collected from previous recommendations. True Preferences denotes the ratio of positive samples in a group to the total positive samples collected from the random exposure. 18 8 5 9 13 6 171514 7 3 1 4 16 2 121011 Genre_ID 0.00 0.05 0.10 0.15 0.20 0.25 P ( j , K =5) (a) ML-1M Tag_3 Tag_18 0.04 0.05 0.06 0.07 0.08 0.09 0.10 0.11 0.12 Ratios 8.4% 6.9% 10.6% 6.2% 7.8% 9.0% History CTR Model True Preferences (b) KuaiRand unfair recommendation results and hurt user experiences. As for item-side, the items ranked in top places usually catch more user's attention [39]. If we regard the clicked items that are ranked in top\ud835\udc3e list of user \ud835\udc62 (denoted as P \ud835\udc3e \ud835\udc62 ) as successful recommendations, by substituting P \ud835\udc3e \ud835\udc62 for P \ud835\udc62 in Equation (5), we obtain the new TPR values (denoted as \ud835\udc43 ( \ud835\udc57, \ud835\udc3e ) ) under the top\ud835\udc3e 's consideration. Figure 3(a) illustrates the item-side unfairness issue with an example in movie recommendation, where \ud835\udc43 ( \ud835\udc57, \ud835\udc3e ) values vary greatly across item groups w.r.t. different movie genres, indicating the unequal opportunity of being recommended. As for user-side, the over-recommendation of some groups will amplify the impact of the gap between the user preference observed from the historical interactions and the true user preference [37] as shown in Figure 3(b), which might hurt the user experience.", "3 MODEL ANALYSIS": "This section presents a top-down analysis of feature-level bias generation from the perspective of CTR models. We focus on two widely-used models, Factorization Machine (FM) [45] and Neural Factorization Machine (NFM) [20], as they are commonly used in both industry and academia. The analysis of more complex CTR models ( e.g., DIN [69]) is left for future work.", "3.1 Bias Analysis from Model View": "Firstly, we identify which component of the CTR model results in the feature-level bias in the recommendations. Functionally speaking, the core operations of CTR models are feature projection and feature interaction modeling. Accordingly, CTR models mainly consist of two parts: a) the high-order part \ud835\udc53 \ud835\udf03 ( \ud835\udc99 ) in Equation (2) for feature interaction, and b) the linear part \u02dd \ud835\udc5b \ud835\udc56 = 1 \ud835\udc64 \ud835\udc56 \ud835\udc65 \ud835\udc56 in Equation (2) for feature projection. Thereafter, we aim to answer the question: to what extent the high-order and linear parts contribute to the featurelevel bias in the output of CTR models? To answer the question, the key lies in studying how the two parts affect the biased distribution of prediction scores across item groups. To this end, we calculate the average prediction scores of Table 1: Variance of the linear and high-order parts, where a larger variance implies a larger influence on the feature-level bias. \"Positive\" and \"Negative\" denotes that the variance is calculated over all positive or negative samples in the testing set, respectively. the linear and high-order parts in each group, respectively; and then obtain the variance of scores across groups. The variance reflects the strength of two parts biasing the prediction scores across groups, where a larger variance indicates the stronger effect of injecting feature-level bias into prediction scores. In other words, the variance reflects to what extent the high-order or linear parts treat item groups differently. Since the testing data contains positive and negative samples, we need to calculate the variance separately to block the influence of sample labels. Formally, we compute the variance of the high-order part on positive samples as follows:  To compare the contributions of the linear and high-order parts to biased prediction scores, we train two representative CTR models, FMand NFM, on a popular CTR dataset ML-1M [19]. Thereafter, we calculate the variance of the two parts via Equation (6). The results are presented in Table 1, from which we find that the variance of the linear part is significantly higher than that of the high-order part. This indicates that the linear part is a critical reason for the biased prediction scores across item groups , further leading to the feature-level bias in the recommendations. This is consistent with the function of two parts: the linear part models the strength of each feature through linear weights [45], thus the logit value may be biased towards certain feature groups; by contrast, as the high-order part models the complex interaction between multiple features, it should be less biased towards single feature groups. where \ud835\udc5a \ud835\udc57 is the average prediction score of the positive samples ( i.e., \ud835\udc66 = 1) in the \ud835\udc57 -th group with feature \ud835\udc57 ( i.e., \ud835\udc65 \ud835\udc57 > 0); M(\u00b7) and V(\u00b7) calculates the mean and variance, respectively. Similarly, to calculate the variance of the linear part w.r.t. positive samples, we only need to substitute \u02dd \ud835\udc5b \ud835\udc56 = 1 \ud835\udc64 \ud835\udc56 \ud835\udc65 \ud835\udc56 for \ud835\udc53 ( \ud835\udc99 ) . Besides, calculating the variance on negative samples only needs to change \ud835\udc66 = 1 with \ud835\udc66 = 0 in Equation (6).", "3.2 Bias Analysis from Data View": "Next, we further explore what causes the bias in the linear part from the data view. In the linear part, the linear weights { \ud835\udc64 \ud835\udc57 | \ud835\udc57 \u2208 { 1 , \u00b7 \u00b7 \u00b7 , \ud835\udc58 }} in Equation (2) are the only learnable parameters. A higher value of the linear weight \ud835\udc64 \ud835\udc57 is more likely to increase the final prediction scores of the samples with feature \ud835\udc57 , leading to the feature-level bias. As model weights are typically initialized with random values and learned in a data-driven manner, we postulate that the scale of the linear weight is related to the property of the training data. To discover how the data affect the linear weights during training, we analyze the relations between \ud835\udc64 \ud835\udc57 and a training Table 2: Correlation coefficients between model's linear weights and the three group-level dataset statistics Np , Np -Nn , and Np /( Np + Nn ) . \ud835\udc46\ud835\udc43 and \ud835\udc43\ud835\udc46 denote the Spearman and Pearson coefficients, respectively, attached with \ud835\udc5d -values in the bracket. A larger absolute value of either SP or PS implies stronger correlation. 0 4000 8000 12000 16000 1.5 1.0 0.5 0.0 0.5 1.0 1.5 NFM's Linear Weights Np Figure 4: Relations between linear weights and three statistics. Each point denotes an item feature group in ML-1M and the dashed lines are obtained by linear regression. 1200 0 1200 3600 6000 Np Nn 1.5 1.0 0.5 0.0 0.5 1.0 1.5 NFM's Linear Weights 0.36 0.44 0.52 0.60 0.68 0.76 Np /( Np + Nn ) 1.5 1.0 0.5 0.0 0.5 1.0 1.5 NFM's Linear Weights sample ( \ud835\udc99 , \ud835\udc66 ) by considering the gradient update. As the BCE loss for sample ( \ud835\udc99 , \ud835\udc66 ) can be written as  We can take the partial derivative of \ud835\udc59 w.r.t. \ud835\udc64 \ud835\udc57 as follows:  By backward propagation with the learning rate \ud835\udf16 , the updated weight \ud835\udc64 \u2032 \ud835\udc57 should be:  where \ud835\udf0e ( \u02c6 \ud835\udc66 ) \u2208 ( 0 , 1 ) is the prediction score for \ud835\udc99 . From Equation (9), wededuce that 1) a negative sample with the \ud835\udc57 -th feature ( i.e., \ud835\udc65 \ud835\udc57 > 0 and \ud835\udc66 = 0) will decrease \ud835\udc64 \ud835\udc57 due to \ud835\udc66 -\ud835\udf0e ( \u02c6 \ud835\udc66 ) < 0; and 2) a positive sample with the \ud835\udc57 -th feature will increase \ud835\udc64 \ud835\udc57 in contrary since \ud835\udc66 -\ud835\udf0e ( \u02c6 \ud835\udc66 ) > 0. As such, a reasonable judgement is that the linear weights should be correlated to the number of positive and negative samples in the training set , establishing the relations between the biased CTR model and the training data [7].", "3.3 Statistical Analysis on Bias Cause": "To validate our conjecture, we investigate the relations between the linear weights of the bias field and three representative grouplevel statistical characteristics w.r.t. the bias field in the training set, including 1) the number of positive samples in each item group ( \ud835\udc41 \ud835\udc5d \u2208 R \ud835\udc58 ), 2) the difference between positive and negative samples in each item group ( \ud835\udc41 \ud835\udc5d -\ud835\udc41 \ud835\udc5b \u2208 R \ud835\udc58 ), and 3) the positive sample ratio in each item group ( \ud835\udc41 \ud835\udc5d /( \ud835\udc41 \ud835\udc5d + \ud835\udc41 \ud835\udc5b ) \u2208 R \ud835\udc58 ) 3 . We take Pearson [2] and Spearman [65] tests to measure the monotonic and linear relationship between the linear weights and these statistics, respectively. 3 Note that \ud835\udc41 \ud835\udc5d /( \ud835\udc41 \ud835\udc5d + \ud835\udc41 \ud835\udc5b ) is a vector where the \ud835\udc57 -th entry denotes the positive ratio of samples with feature \ud835\udc65 \ud835\udc57 . 185147151 6 4169 8 217123101311 Genre_ID 0.3 0.4 0.5 0.6 0.7 0.8 Positive Sample Ratio (a) Biased positive sample ratio 0.36 0.44 0.52 0.60 0.68 0.76 Positive Sample Ratio 1.5 1.0 0.5 0.0 0.5 1.0 1.5 NFM's Linear Weights PS =0.8584 (b) Biased linear weights 0.36 0.44 0.52 0.60 0.68 0.76 Positive Sample Ratio 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 Average Model Prediction PS =0.9316 PS =0.9431 pos neg (c) Biased prediction scores 185147151 6 4169 8 217123101311 Genre_ID 0.6 0.7 0.8 0.9 1.0 1.1 1.2 Exposure-to-Hit Ratio SP=0.8514 p-value=7.422e-06 (d) Biased recommendations Figure 5: Illustration of the generation path of feature-level bias in a NFM model. In (a), each item group's positive sample ratios are sorted in ascending order. In (b)(c), we calculate the Pearson [2] correlation coefficients (denoted as \ud835\udc43\ud835\udc46 ) between the value of interested ( \ud835\udc66 -axis) and the positive sample ratio to verify their linear relationship. In (c), \"pos\" and \"neg\" denote the average prediction scores computed over all positive or negative testing samples, respectively. In (d), \ud835\udc46\ud835\udc43 and p-value are results of Spearman test [65] between each item group's \ud835\udc38\ud835\udc3b\ud835\udc45 in Equation (4) and positive sample ratio, showing the monotonic relationship between them. The correlation coefficients with \ud835\udc5d -values are shown in Table 2 and the linear weights of NFM are visualized in Figure 4. From Figure 4 and Table 2, we have the following three main observations.", "4 METHODOLOGY": "\u00b7 The small correlation coefficients w.r.t. \ud835\udc41 \ud835\udc5d show that the linear weights are not highly correlated with \ud835\udc41 \ud835\udc5d . We thus postulate that the difference across item groups w.r.t. the number of positive sample is insufficient to explain the feature-level bias in the linear weights as the negative samples will decrease the weight. \u00b7 The weights and positive sample ratios show strong linear correlations from Figure 4(c), which are further verified by the large Spearman and Pearson coefficients in Table 2 (SP > 0 . 87, PS > 0 . 80 with \ud835\udc5d -value < 1 \ud835\udc52 -4). The strong correlations demonstrate that the biased positive sample ratios are more likely to affect the linear weights in CTR models, further leading to the biased prediction scores and biased recommendations. \u00b7 In Figure 4, the feature groups with higher \ud835\udc41 \ud835\udc5d -\ud835\udc41 \ud835\udc5b elements usually obtain higher linear weights, while the correlations break on some outliers. This is reasonable since not only the relative difference but also the absolute quantity of positive and negative samples affect the final scale of linear weights. \u00b7 Summary. To summarize, we have discovered a generation path of feature-level bias: biased positive sample ratios \u2192 biased linear weights in CTR model \u2192 biased prediction scores \u2192 biased recommendations. Specifically, 1) as shown in Figure 5(a), there exists an imbalanced distribution of the positive sample ratios across item groups. Such data bias causes feature-level bias originally 4 . 2) The CTR model will learn such bias via the linear weights of the bias field. As illustrated in Figure 5(b), the linear weights are highly correlate with the positive sample ratios. 3) Subsequently, the prediction score of CTR models are affected by the linear weights, causing the correlations between the average scores with the positive sample ratios as shown in Figure 5(c). 4) Lastly, the feature groups with higher positive sample ratios are more likely to be over-recommended ( cf. Figure 5(d)) and vice versa. 4 As an initial attempt, we focus on one iteration of the recommender system, leaving the consideration of feedback loop to future work. According to the above analysis, the key to counteracting the feature-level bias lies in cutting off the path from the uneven distribution of positive sample ratio to the biased model predictions. Existing methods achieve the target through regularization [72], adversarial training [73], or causal inference [54], which changes the learning of CTR models and inevitably brings significant overhead such as updating the system architecture to be compatible with such changes. In this light, we pursue strategies for counteracting the feature-level bias with minimally invasive changes to the recommender system. Generally, there are two options to achieve the target at the pre-training stage and the post-training stage. \u00b7 Pre-training Adjustment. As the uneven distribution of positive sample ratio is the start of generation path, it seems to be a promising strategy to counteract the feature-level bias by adjusting the distribution of training data. This can be easily implemented into the existing data sampler for CTR model training. \u00b7 Post-training Adjustment. As the linear weights corresponding to the bias field, i.e., { \ud835\udc64 \ud835\udc57 | \ud835\udc57 \u2208 [ 1 , \ud835\udc58 ]} , largely explain the biased predictions of CTR models, we also consider adjusting the weights after the training of CTR models to counteract the feature-level bias. This is also minimally invasive as only changing \ud835\udc58 parameters. Note that \ud835\udc58 \u226a \ud835\udc5b and the high-order component typically contains much more parameters than the linear part. Undoubtedly, adjusting the data will influence the whole learning procedure, which is complex due to non-convex property of most CTR models. The effect of pre-training adjustment is thus hard to control. As such, we mainly investigate how to perform the post-training adjustment, which is formulated as:  where \ud835\udc64 \u2032 \ud835\udc57 denotes the expected weight for the \ud835\udc57 -th feature. Accordingly, we can obtain the adjusted weights through the following two ways: linear weight reduction and linear weight reconstruction .", "4.1 Linear Weight Reduction": "To counteract the impact of biased linear weights, an intuitive operation is removing them from the biased CTR model, i.e.,  Figure 6: The distribution gap w.r.t. positive sample ratio between the biased data collected from the regular exposure and unbiased data collected from random exposure in the KuaiRand [14] dataset. 0 5 10 15 20 25 30 35 40 45 Tag_ID 0.0 0.1 0.2 0.3 0.4 0.5 0.6 Positive Sample Ratio Distribution Gap in KuaiRand biased_train unbiased_test To enhance flexibility, we seek a general form of such adjustment, which reduces linear weights with controllable strength. Formally,  where \ud835\udefc \u2208 [ 0 , 1 ] is a specified coefficient. In particular, \ud835\udefc = 1 indicates no adjustment and \ud835\udefc = 0 means ignoring the linear weights in the bias field entirely. In practice, we set \ud835\udefc = 0 for default value, which blocks the path of bias generation and reduces the featurelevel bias to a large extent. This adjustment applies very limited changes to the well trained CTR model, which should largely preserve the preference estimation since the linear weights in other fields ( e.g., ItemID) and the high-order part also provides sufficient information for user preference estimation. Empirical results in Table 4 validate this hypothesis, where the adjusted model achieves comparable performance as the original model.", "4.2 Linear Weight Reconstruction": "Recall that there is a gap between the training samples collected from previous exposures ( i.e., non-random exposure) and the data from random exposure which reflects the true user preference. To further illustrate this phenomenon, we compute the positive sample ratios of different video tags in KuaiRand [14] and visualize the distributions over both biased data from non-random exposure and unbiased data from random exposure. As shown in Figure 6, the biased and unbiased data exhibit huge gaps regarding the distribution of positive sample ratio. Furthermore, the positive sample ratio of the unbiased data is not uniformly distributed over bias features. As unbiased data is the golden standard for testing CTR models, we further set the target of post-training adjustment as approaching the expected linear weights as learning CTR models from the unbiased data. In this case, we assume very limited unbiased data are available as most existing works for unbiased recommendation [6]. Nevertheless, biased data is still the default choice for training CTR models due to the extreme sparsity of unbiased data. Let D \ud835\udc62 denote the unbiased data, we obtain the expected weights through linear weight reconstruction according to D \ud835\udc62 . As the target is to adjust the linear weights learned from biased data to approach those coherent with unbiased data, there are two key steps to remove the influence of positive sample ratio of the biased data and inject the influence of positive sample ratio of the unbiased data. In particular, Step 1: Removing the biased positive sample ratio. As the linear weights of CTR models are highly correlated with the positive sample ratio of the biased training data, we conduct linear Algorithm 1: Linear Weight Reduction & Reconstruction Input: A well trained CTR model \ud835\udc39\ud835\udc40 (\u00b7) with biased linear weights \ud835\udc64 1 , \u00b7 \u00b7 \u00b7 , \ud835\udc64 \ud835\udc58 ; training data D ; unbiased data D \ud835\udc62 ; hyper-parameters \ud835\udefc, \ud835\udefd,\ud835\udefe Output: A debiased CTR model 1 if D \ud835\udc62 = \u2205 then 2 /* Linear Weight Reduction */ 3 Calculate \ud835\udc64 \u2032 \ud835\udc57 with Equation (12); 4 else 5 /* Linear Weight Reconstruction */ 6 Calculate \ud835\udc5f \ud835\udc57 with Equation (13); 7 Estimate \ud835\udc60 \ud835\udc57 with D \ud835\udc62 ; 8 Calculate \ud835\udc64 \u2032 \ud835\udc57 with Equation (14); 9 end 10 Replace linear weights with \ud835\udc64 \ud835\udc57 = \ud835\udc64 \u2032 \ud835\udc57 . regression on the (ratio, weight) pairs and regress the estimated weights out to get the residuals as follows:  where \u02c6 \ud835\udc64 \ud835\udc57 denotes the estimation from linear regression. The residuals are no longer correlated with the positive sample ratio of the training data, thus independent from the data cause of the featurelevel bias. Step 2: Leveraging the unbiased positive sample ratio. We use D \ud835\udc62 to estimate the ideal positive sample ratio under the random exposure, i.e., the true user preference. We denote the estimated ratios as \ud835\udc60 1 , \u00b7 \u00b7 \u00b7 , \ud835\udc60 \ud835\udc58 , and combine the ratios with the residuals linearly to construct new linear weights for the bias field:  where \ud835\udefd > 0 , \ud835\udefe > 0 are hyper-parameters. We use the random exposure data D \ud835\udc62 to search for optimal value of \ud835\udefd and \ud835\udefe in practice. As such, we only estimate \ud835\udc58 + 2 values from D \ud835\udc62 , which means that the amount of required random exposure data is limited. The overall procedures for linear weights reduction and reconstruction are shown in Algorithm 1. It is worth mentioning that both methods do minimally invasive adjustments on the pre-trained CTR model, which are fast and energy-saving, showing promising utility in real-world scenarios.", "5 RELATED WORK": "In this section, we review recent works about bias in recommendation , fairness in recommendation , and CTR prediction . Bias in Recommendation. Recommender systems utilize useritem interactions for model training. However, since the training data is observational rather than experimental [7], the model suffer from various biases, such as selection bias [35], position bias [12], exposure bias [34] and popularity bias [36, 64, 66]. Existing works address the bias issue mainly by regularization [1, 46, 72], representation learning [27, 64, 68] and causal inference [49, 55, 56, 66]. For example, [72] adopts the Pearson correlation coefficient as regularization to discourage the effect of popularity. [68] disentangles user representation into the interest and conformity part, then uses the well-trained interest embedding for debiased recommendation. [66] uses backdoor adjustment to block the effect of popularity on item representation and leverages popularity for better recommendation. Despite their success, they mainly focus on debiasing for collaborative filtering models [26] that optimized with only interaction data. In addition to interaction data, real-world CTR models [17, 20] also rely on abundant side-information ( e.g., user features) for model training, making bias analysis more challenging. To tackle this challenge, our work formulates and analyzes the feature-level bias problem in CTR prediction task, and proposes simple-yet-effective strategies to counteract feature-level bias for FM-based CTR models. Fairness in Recommendation. Recently, many impressive works focus on improving fairness for recommender system [51, 53, 63]. They usually eliminate the unfairness from different perspectives ( i.e., user-side, item-side and multi-side). For example, [8, 22, 29, 31, 44] try to eliminate the unfairness between different users, [3, 9, 28, 38, 60] dedicate to improve the fairness from item-side, while [36, 40, 42, 57, 58, 61] focus on improve fairness from both sides. This work is most related to item-side fairness and equal opportunity [18] notion, which encourages the recommendation opportunities of different item groups to be identical. Existing methods to improve item-side fairness include adversarial training [28, 73], regularization [3, 16] and reinforcement learning [15]. Consistent with previous views [30], we believe that bias could be the main cause of unfairness. So different from the existing works that directly optimize the various fairness metrics, we analyze the cause of feature-level bias which is responsible for unfair recommendation in CTR prediction, and improve the fairness by combating the bias. CTR Prediction. CTR prediction is one of the most important tasks in recommendation, which aims at ranking a small number of candidate items using abundant features for better personalization [70]. The widely-used CTR models [71] include shallow models such as logistic regression (LR [47]) and factorization machines ( e.g., FM [45], FFM [24], HOFM [4]), as well as deep models which leverages DNN ( e.g., Wide&Deep [10], NFM [20], DeepFM [17], xDeepFM[33]) or attention mechanisms ( e.g., AFM[62], AutoInt [52], InterHAt [32]) to model high-order feature interactions. Despite their great progress in improving recommendation accuracy, alleviating bias in CTR prediction receives little scrutiny. This work bridges the research gap by studying the feature-level bias in CTR prediction, and identifies the importance of linear part in introducing the feature-level bias.", "6 EXPERIMENTS": "In this section, we conduct extensive experiments to answer the following three questions: \u00b7 RQ1: Can our proposed methods improve item-side fairness by alleviating feature-level bias? \u00b7 RQ3: How do the designs in our methods ( e.g., leveraging unbiased positive ratios) affect the performance? \u00b7 RQ2: How do our methods perform when predicting user preference on unbiased random exposure data?", "6.1 Experimental Settings": "6.1.1 Evaluation protocol. In this work, we adopt the following two offline tests [11] to evaluate the feature-level bias. Table 3: Statistics of the datasets. #G is short for the number of groups. nbt and dt denote that the samples are used as normal biased and debiased validation/test data, respectively. \u00b7 Normal biased test. It aims at improving the click or conversion rate on user clicks collected from regular exposure. We use the linear weights reduction algorithm to improve item-side fairness with negligible sacrifice of accuracy [30]. Accordingly, we evaluate both recommendation performance and item-side fairness under the normal biased test. \u00b7 Debiased test. It evaluates recommendation performance on the unbiased random exposure data which better reflect true user preferences. As unbiased data is typically viewed as the golden standard, we focus on the recommendation performance under the debiased test. Evaluation metrics. We adopt UAUC and NDCG@K [23] to evaluate the recommendation accuracy. Following [6], we calculate AUC and NDCG@K for each user and then take the average score of all users to obtain UAUC and NDCG@K. As to the item-side fairness, we follow the previous work [73] to compute the relative standard deviation of successful recommendation probabilities regarding users' top\ud835\udc3e list ( i.e., \ud835\udc43 ( \ud835\udc57, \ud835\udc3e ) ) over item groups. Formally,  where S(\u00b7) calculates standard deviation. Apparently, the value of \ud835\udc45\ud835\udc38\ud835\udc42 @ \ud835\udc3e is in the range of [ 0 , +\u221e) , and smaller value of \ud835\udc45\ud835\udc38\ud835\udc42 @ \ud835\udc3e indicates less feature-level bias and better item-side fairness. \ud835\udc45\ud835\udc38\ud835\udc42 @ \ud835\udc3e = 0 means the absolute equal opportunity [18] is achieved. In the experiment we take \ud835\udc3e = 5 as default. 6.1.2 Datasets. We conduct experiments on the following three datasets: (1) ML-1M 5 is a popular movie recommendation dataset which involves abundant features ( e.g., user gender, age, occupation and movie genre). (2) Book is one of the Amazon product datasets 6 with book categories. (3) KuaiRand [14] is a short-video recommendation dataset on the video-sharing platform Kuaishou 7 which contains interactions from both normal and random exposure. We sort and split the interactions into training, validation, and test set chronologically. For ML-1M and Book, interactions with rating value larger than 3 are treated as positive samples otherwise negative. For KuaiRand, we treat the interaction with \ud835\udc56\ud835\udc60 _ \ud835\udc50\ud835\udc59\ud835\udc56\ud835\udc50\ud835\udc58 = 1 as positive samples otherwise negative. Considering that random exposure data is usually at the high expense of hurting user experience [6, 50], we randomly sample 10 , 000 interactions from the unbiased validation data. For each dataset, we adopt the 20-core filtering setting and split the items into groups based on movie genre, book category, and video tag respectively. The statistics of these datasets are summarized in Table 3. 5 https://grouplens.org/datasets/movielens/1m/. 7 https://www.kuaishou.com. 6 https://jmcauley.ucsd.edu/data/amazon/. Table 4: Performance comparison on the normal biased tests. The best and second best results are highlighted in bold and underlined, respectively. %improve denotes the relative performance improvements of our method compared to the basemodel. 6.1.3 Baselines. Weselect two representative CTR models, FM [45] and NFM [20], as our backbone models. On normal biased tests , we select the following heuristic, debias, and fairness methods which may alleviate the bias amplification in CTR prediction tasks: For the linear weight reconstruction algorithm, we search both \ud835\udefd and \ud835\udefe in { 1 , 2 , 4 , 8 , 12 , 16 , 20 } . \u00b7 Unawareness removes the bias field ( e.g., movie genres in ML1M) from the data and leverages the remaining fields for training. \u00b7 w/o b/l/w removes linear weights in FM/NFM about the bias field and keeps the remaining parts for training. In contrast, our method trains a whole CTR model and adjust the linear weights in the inference stage. \u00b7 w/o l/p is a variant of FM/NFM which only keeps the high-order part ( i.e., \ud835\udc53 \ud835\udf03 ( \ud835\udc99 ) in Equation (2)) for training. \u00b7 IPW [49] is the inverse propensity weighting method in causal debiasing. We use each group's positive sample ratio as propensity scores to down-weight the items in advantaged groups. \u00b7 FairGo [59] is a method that pursues fair recommendation by removing the information of sensitive features through adversarial training. We implement FairGo on FM and NFM by regarding the features in the bias field as sensitive features. We tune the bias control coefficient \ud835\udf06 in [1e-3,10] at 5x multiplicative ratio. \u00b7 DecRS [54] uses causal inference technique to alleviate the bias amplification issue for CTR models. We implement the DecRS on FM and NFM backbones. On debiased tests , as the linear weight reconstruction algorithm leverages a few random exposure records, we further compare two baselines which also use random exposure data: \u00b7 Finetune is a heuristic method that finetunes the pre-trained FM/NFM model on unbiased validation data. \u00b7 InterD [11] is a multi-teacher distillation method that aims to achieve better performances on both tests. We adopt the pretrained biased model as the biased-teacher and the model finetuned on unbiased validation data as the unbiased-teacher. The student model is another NFM/FM which has the same settings as both teachers. 6.1.4 Hyper-parameter settings. We take the same experimental setups for all the baselines to compare them fairly. The FM and NFMareimplemented with the popular CTR benchmark DeepCTR 8 . All the baselines are optimized by the Adam [25] optimizer with batch size 256 and BCE loss. For FM, we tune the learning rate in { 5 \ud835\udc52 -4 , 1 \ud835\udc52 -3 , 5 \ud835\udc52 -3 } . For NFM, we additionally tune the dropout rate of DNN and bi-interaction module in { 0 . 1 , 0 . 2 , 0 . 5 } . We also tune the regularization coefficients in { 1 \ud835\udc52 -6 , 5 \ud835\udc52 -6 , 1 \ud835\udc52 -5 , 1 \ud835\udc52 -4 , 1 \ud835\udc52 -3 } . For the linear weight reduction algorithm, we set \ud835\udefc = 0 by default. 8 https://github.com/shenweichen/DeepCTR-Torch.", "6.2 Performance Comparison": "6.2.1 Improving item-side fairness on normal biased tests (RQ1). Table 4 reports the recommendation accuracy and fairness metrics on normal biased tests of three datasets. From the two tables, we have the following main observations: \u00b7 On three datasets, our method ( i.e., linear weight reduction) remarkably reduces REO@5 with only little sacrifice of accuracy. Specifically, the unfairness decrease of our method over the basemodel w.r.t. REO@5is from 14.35% to 47.00% on different datasets, which shows that our method significantly improves item-side fairness. Meanwhile, the average performance drop is negligible w.r.t. UAUC and NDCG@5 (-1.06% on average across all cases). \u00b7 As two variants of linear weight reduction algorithm, w/o l/p and w/o b/l/w encounter performance degradation in most cases; meanwhile, they cannot remarkably improve item-side fairness. This indicates that isolating the linear part in the training stage cannot solve the feature-level bias effectively. As such, this result further validate the rationality of post-training adjustment, i.e., adjusting the linear weights of trained CTR models. \u00b7 IPWandUnawareness can reduce REO@5, but their effectiveness is sensitive to datasets ( e.g., IPW fails on Book). IPW assigns a propensity weight for each training sample, affecting both linear and high-order weights. Our method directly post-adjusts linear weights, which is thus more robust and can alleviate bias consistently. This is the benefit of recognizing the role of CTR model components w.r.t. the feature-level bias. As for Unawareness, we postulate that CTR models can implicitly infer information in the bias field [67] from the remaining features and historical interactions. Consequently, the linear part is still highly correlated to the biased positive ratios. \u00b7 Although DecRS and FairGo sacrifice minor accuracy in some cases, they cannot reduce REO@5 effectively in most cases. The main reason is that they alleviate biases by improving high-order embedding (DecRS introduces confounded embedding for users and tags and FairGo removes the sensitive information from embeddings), whereas the linear part is more important in introducing feature-level bias. 6.2.2 Improving accuracy on debiased tests (RQ2). We present the performance on the debiased test of KuaiRand in Table 5, where we have the following findings: Table 5: Performance on the debiased test of KuaiRand. The best and second best results are highlighted in bold and underlined, respectively. %improve denotes the relative performance improvements of our method compared to the basemodel. Debiased tests are unavailable in ML-1M and Book. 0.0 0.2 0.4 0.6 0.8 1.0 Values of 0.712 0.716 0.720 0.724 0.728 UAUC UAUC REO@5 0.30 0.36 0.42 0.48 0.54 REO@5 ML-1M (a) ML-1M 0.0 0.2 0.4 0.6 0.8 1.0 Values of 0.616 0.617 0.618 0.619 0.620 0.621 0.622 UAUC UAUC REO@5 0.13 0.14 0.15 0.16 0.17 0.18 0.19 REO@5 Book (b) Book 0.0 0.2 0.4 0.6 0.8 1.0 Values of 0.622 0.626 0.630 0.634 0.638 UAUC REO@5 0.30 0.34 0.38 0.42 0.46 REO@5 KuaiRand (c) KuaiRand UAUC Figure 7: Fairness-accuracy trade-off of NFM as \ud835\udefc changes in Equation (12) on the normal biased tests. \u00b7 Our method ( i.e., linear weight reconstruction) achieves the best performance among all baselines, validating the superiority of our method. Specifically, the average relative improvements over the basemodel w.r.t. UAUC and NDCG@5 are 4.52% and 7.71%, respectively. We attribute this superior performance to using estimated positive sample ratios to construct new linear weights, which can better reflect the true user preference. \u00b7 All the baselines without leveraging random exposure data (Unawareness, w/o l/p, w/o b/l/w, IPW, DecRS, FairGo) show inferior results than Finetune, InterD and our method on the debiased test . This demonstrates the effectiveness of using random exposure data for estimating the actual user preference. Nevertheless, our method still surpasses Finetune and InterD, since the linear weight reconstruction avoids the impact of data sparsity.", "6.3 In-depth Analysis (RQ3)": "To further explore what influences the effectiveness of our methods, we conduct the following parameter analysis and ablation studies. 6.3.1 Study of \ud835\udefc in Equation (12) . In previous experiments to improve item-side fairness on normal biased tests, we set 0 as the default value of \ud835\udefc in Equation (12). In practice, the platform-side can set \ud835\udefc \u2208 [ 0 , 1 ] to control the fairness-accuracy trade-off as shown in Figure 7. There are several findings: (1) As \ud835\udefc increases, the UAUC of different datasets keep increasing, demonstrating that the linear weights of the bias field do contribute to recommendation accuracy. The more linear weights keep, the less accuracy is sacrificed. (2) On Book and KuaiRand, REO@5 keeps increasing with an increasing value of \ud835\udefc , which demonstrates the fairness-accuracy trade-off phenomenon. However, on ML-1M, REO@5 firstly decreases and then increases, which suggests that when \ud835\udefc is small, there exists a win-win solution in improving both fairness and accuracy. This Table 6: Performance comparison between different variants of our method on the debiased test. The best and second best results are highlighted in bold and underlined, respectively. sheds light on the choice of \ud835\udefc . Specifically, we recommend to select small but nonzero values ( e.g., \ud835\udefc = 0 . 1 , 0 . 2), which can always alleviate item-side unfairness greatly, and may further approach pareto optimality [41] in some lucky cases. 6.3.2 Ablation study. In our proposed linear weight reconstruction algorithm which aims at improving performances on debiased tests, there are mainly two parts-residual part \ud835\udc5f \ud835\udc56 and positive ratio part \ud835\udc60 \ud835\udc56 . To check their effectiveness, we compare our method with the following three variants. w/ finetuning LW denotes initializing the FM/NFM with the pre-trained biased model and only finetuning linear weights of the bias field with the unbiased validation data. w/o ratio denotes that only using \ud835\udc5f \ud835\udc57 to construct weights ( i.e., \ud835\udefd = 0 in Equation (14)), while w/o residual only uses \ud835\udc60 \ud835\udc57 for construction ( i.e., \ud835\udefe = 0 in Equation (14)). From results in Table 6, there are three key observations. (1) The vanilla method consistently outperforms w/o ratio and w/o residual . In addition, both variants still outperform the basemodel. This demonstrates the effectiveness of each part of our method. (2) It is worth noticing that although w/o ratio doesn't require random exposure data, it outperforms the basemodel, showing potential use in cases without unbiased data. (3) Although w/ finetuning LW can also improve performance, it doesn't outperform our construction method, which shows the limitation of simply finetuning linear weights of the bias field.", "7 CONCLUSION AND FUTURE WORK": "In this work, we studied the feature-level bias issue in CTR prediction task, and discussed its negative impacts on item-side fairness and user experiences. We then conducted top-down analysis on CTR models, recognized the key influence of the model's linear part on feature-level bias, where positive sample ratios of the training data significantly correlates with linear weights. Upon this understanding, we propose two simple-yet-effective strategies to counteract the feature-level bias, whose key idea lies in adjusting biased linear weights to cut off the bias generation path. Our method is minimally invasive to CTR models, whose effectiveness is validated by extensive experiments on both biased and debiased tests of real-world datasets. In the future, we would like to explore how high-order bias ( i.e., biases related to multi-features) is generated inside the model and the approach to addressing the issue without retraining. Lastly, our work is limited to debiasing classical FM-based CTR models with linear part, thus how to extend our analysis and method to more general and complicated CTR models ( e.g., DIN [69]) is an interesting topic. Besides, it is valuable to explore the transferability of linear weights across CTR models ( e.g., whether transfering unbiased linear weights from a debiased model to another biased CTR model will help alleviate feature-level bias or not).", "REFERENCES": "[1] Himan Abdollahpouri, Robin Burke, and Bamshad Mobasher. 2017. Controlling Popularity Bias in Learning-to-Rank Recommendation. In Proceedings of the 11th ACM Conference on Recommender Systems . ACM, 42-46. [3] Alex Beutel, Jilin Chen, Tulsee Doshi, Hai Qian, Li Wei, Yi Wu, Lukasz Heldt, Zhe Zhao, Lichan Hong, Ed H Chi, et al. 2019. Fairness in Recommendation Ranking through Pairwise Comparisons. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining . ACM, 2212-2220. [2] Jacob Benesty, Jingdong Chen, Yiteng Huang, and Israel Cohen. 2009. Pearson Correlation Coefficient. In Noise reduction in speech processing . Springer, 1-4. [4] Mathieu Blondel, Akinori Fujino, Naonori Ueda, and Masakazu Ishihata. 2016. Higher-Order Factorization Machines. Advances in Neural Information Processing Systems 29 (2016). [6] Jiawei Chen, Hande Dong, Yang Qiu, Xiangnan He, Xin Xin, Liang Chen, Guli Lin, and Keping Yang. 2021. AutoDebias: Learning to Debias for Recommendation. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval . ACM, 21-30. [5] Simon Caton and Christian Haas. 2020. Fairness in Machine Learning: A Survey. arXiv preprint arXiv:2010.04053 (2020). [7] Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan He. 2023. Bias and Debias in Recommender System: A Survey and Future Directions. ACM Transactions on Information Systems 41, 3 (2023), 1-39. [9] Xiao Chen, Wenqi Fan, Jingfan Chen, Haochen Liu, Zitao Liu, Zhaoxiang Zhang, and Qing Li. 2023. Fairly adaptive negative sampling for recommendations. In Proceedings of the ACM Web Conference 2023 . ACM, 3723-3733. [8] Lei Chen, Le Wu, Kun Zhang, Richang Hong, Defu Lian, Zhiqiang Zhang, Jun Zhou, and Meng Wang. 2023. Improving Recommendation Fairness via Data Augmentation. In Proceedings of the ACM Web Conference 2023 . ACM, 1012-1020. [10] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & Deep Learning for Recommender Systems. In Proceedings of the 1st Workshop on Deep Learning for Recommender Systems . 7-10. [12] Yaron Fairstein, Elad Haramaty, Arnon Lazerson, and Liane Lewin-Eytan. 2022. External Evaluation of Ranking Models under Extreme Position-Bias. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining . ACM, 252-261. [11] Sihao Ding, Fuli Feng, Xiangnan He, Jinqiu Jin, Wenjie Wang, Yong Liao, and Yongdong Zhang. 2022. Interpolative Distillation for Unifying Biased and Debiased Recommendation. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval . ACM, 40-49. [13] Wenqi Fan, Xiangyu Zhao, Xiao Chen, Jingran Su, Jingtong Gao, Lin Wang, Qidong Liu, Yiqi Wang, Han Xu, Lei Chen, et al. 2022. A Comprehensive Survey on Trustworthy Recommender Systems. (2022). [15] Yingqiang Ge, Shuchang Liu, Ruoyuan Gao, Yikun Xian, Yunqi Li, Xiangyu Zhao, Changhua Pei, Fei Sun, Junfeng Ge, Wenwu Ou, et al. 2021. Towards LongTerm Fairness in Recommendation. In Proceedings of the 14th ACM International Conference on Web Search and Data Mining . ACM, 445-453. [14] Chongming Gao, Shijun Li, Yuan Zhang, Jiawei Chen, Biao Li, Wenqiang Lei, Peng Jiang, and Xiangnan He. 2022. KuaiRand: An Unbiased Sequential Recommendation Dataset with Randomly Exposed Videos. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management . ACM, 3953-3957. [16] Sahin Cem Geyik, Stuart Ambler, and Krishnaram Kenthapadi. 2019. FairnessAware Ranking in Search & Recommendation Systems with Application to LinkedIn Talent Search. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . ACM, 2221-2231. [18] Moritz Hardt, Eric Price, and Nati Srebro. 2016. Equality of Opportunity in Supervised Learning. Advances in Neural Information Processing Systems 29 (2016). [17] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: A Factorization-Machine Based Neural Network for CTR Prediction. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence . 1725-1731. [19] F Maxwell Harper and Joseph A Konstan. 2015. The Movielens Datasets: History and Context. Acm Transactions on Interactive Intelligent Systems (TIIS) 5, 4 (2015), 1-19. [21] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural Collaborative Filtering. In Proceedings of the 26th International Conference on World Wide Web . ACM, 173-182. [20] Xiangnan He and Tat-Seng Chua. 2017. Neural Factorization Machines for Sparse Predictive Analytics. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval . ACM, 355-364. [22] Rashidul Islam, Kamrun Naher Keya, Ziqian Zeng, Shimei Pan, and James Foulds. 2021. Debiasing Career Recommendations with Neural Fair Collaborative Filtering. In Proceedings of the Web Conference 2021 . ACM / IW3C2, 3779-3790. [23] Kalervo J\u00e4rvelin and Jaana Kek\u00e4l\u00e4inen. 2002. Cumulated Gain-Based Evaluation of IR Techniques. ACM Transactions on Information Systems (TOIS) 20, 4 (2002), [47] Matthew Richardson, Ewa Dominowska, and Robert Ragno. 2007. Predicting Clicks: Estimating the Click-Through Rate for New Ads. In Proceedings of the 16th International Conference on World Wide Web . ACM, 521-530. [49] Yuta Saito, Suguru Yaginuma, Yuta Nishino, Hayato Sakata, and Kazuhide Nakata. 2020. Unbiased Recommender Learning from Missing-Not-at-Random Implicit Feedback. In Proceedings of the 13th International Conference on Web Search and Data Mining . ACM, 501-509. [48] Donald B Rubin. 2005. Causal Inference Using Potential Outcomes: Design, Modeling, Decisions. J. Amer. Statist. Assoc. 100, 469 (2005), 322-331. [50] Tobias Schnabel, Adith Swaminathan, Ashudeep Singh, Navin Chandak, and Thorsten Joachims. 2016. Recommendations as Treatments: Debiasing Learning and Evaluation. In Proceedings of the 33nd International Conference on Machine Learning . PMLR, 1670-1679. [52] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, and Jian Tang. 2019. Autoint: Automatic Feature Interaction Learning via SelfAttentive Neural Networks. In Proceedings of the 28th ACM International Conference on Information & Knowledge Management . ACM, 1161-1170. [51] Jessie J Smith, Lex Beattie, and Henriette Cramer. 2023. Scoping Fairness Objectives and Identifying Fairness Metrics for Recommender Systems: The Practitioners' Perspective. In Proceedings of the ACM Web Conference 2023 . ACM, 3648-3659. [53] Lequn Wang and Thorsten Joachims. 2023. Uncertainty Quantification for Fairness in Two-Stage Recommender Systems. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining . ACM, 940-948. [55] Xiaojie Wang, Rui Zhang, Yu Sun, and Jianzhong Qi. 2019. Doubly Robust Joint Learning for Recommendation on Data Missing Not at Random. In International Conference on Machine Learning . PMLR, 6638-6647. [54] Wenjie Wang, Fuli Feng, Xiangnan He, Xiang Wang, and Tat-Seng Chua. 2021. Deconfounded Recommendation for Alleviating Bias Amplification. In The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . ACM, 17171725. [56] Tianxin Wei, Fuli Feng, Jiawei Chen, Ziwei Wu, Jinfeng Yi, and Xiangnan He. 2021. Model-Agnostic Counterfactual Reasoning for Eliminating Popularity Bias in Recommender System. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining . ACM, 1791-1800. [58] Haolun Wu, Bhaskar Mitra, Chen Ma, Fernando Diaz, and Xue Liu. 2022. Joint Multisided Exposure Fairness for Recommendation. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval . ACM, 703-714. [57] Haolun Wu, Chen Ma, Bhaskar Mitra, Fernando Diaz, and Xue Liu. 2022. A Multi-Objective Optimization Framework for Multi-Stakeholder Fairness-Aware Recommendation. ACM Transactions on Information Systems 41, 2 (2022), 1-29. [59] Le Wu, Lei Chen, Pengyang Shao, Richang Hong, Xiting Wang, and Meng Wang. 2021. Learning Fair Representations for Recommendation: A Graph-Based Perspective. In Proceedings of the Web Conference 2021 . ACM / IW3C2, 2198-2208. [61] Yao Wu, Jian Cao, Guandong Xu, and Yudong Tan. 2021. TFROM: A Two-Sided Fairness-Aware Recommendation Model for Both Customers and Providers. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval . ACM, 1013-1022. [60] Yao Wu, Jian Cao, and Guandong Xu. 2023. FASTER: A Dynamic Fairnessassurance Strategy for Session-based Recommender Systems. ACM Transactions on Information Systems (2023). [62] Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua. 2017. Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks. In Proceedings of the 26th International Joint Conference on Artificial Intelligence . AAAI Press, 3119-3125. [64] Guipeng Xv, Chen Lin, Hui Li, Jinsong Su, Weiyao Ye, and Yewang Chen. 2022. Neutralizing Popularity Bias in Recommendation Models. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval . ACM, 2623-2628. [63] ChenXu,Sirui Chen, Jun Xu, Weiran Shen, Xiao Zhang, Gang Wang, and Zhenhua Dong. 2023. P-MMF: Provider Max-min Fairness Re-ranking in Recommender System. In Proceedings of the ACM Web Conference 2023 . ACM, 3701-3711. [65] Jerrold H Zar. 1972. Significance Testing of the Spearman Rank Correlation Coefficient. J. Amer. Statist. Assoc. 67, 339 (1972), 578-580. [67] Tianxiang Zhao, Enyan Dai, Kai Shu, and Suhang Wang. 2022. Towards Fair Classifiers Without Sensitive Attributes: Exploring Biases in Related Features. In Proceedings of the 15th International Conference on Web Search and Data Mining . ACM, 1433-1442. [66] Yang Zhang, Fuli Feng, Xiangnan He, Tianxin Wei, Chonggang Song, Guohui Ling, and Yongdong Zhang. 2021. Causal Intervention for Leveraging Popularity Bias in Recommendation. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval . ACM, 11-20. [68] Yu Zheng, Chen Gao, Xiang Li, Xiangnan He, Yong Li, and Depeng Jin. 2021. Disentangling User Interest and Conformity for Recommendation with Causal Embedding. In Proceedings of the Web Conference 2021 . ACM / IW3C2, 2980-2991. [69] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep Interest Network for ClickThrough Rate Prediction. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . ACM, 1059-1068. [70] Jieming Zhu, Quanyu Dai, Liangcai Su, Rong Ma, Jinyang Liu, Guohao Cai, Xi Xiao, and Rui Zhang. 2022. BARS: Towards Open Benchmarking for Recommender Systems. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval . ACM, 2912-2923. [72] Ziwei Zhu, Yun He, Xing Zhao, Yin Zhang, Jianling Wang, and James Caverlee. 2021. Popularity-Opportunity Bias in Collaborative Filtering. In Proceedings of the 14th ACM International Conference on Web Search and Data Mining . ACM, 85-93. [71] Jieming Zhu, Jinyang Liu, Shuai Yang, Qi Zhang, and Xiuqiang He. 2021. Open Benchmarking for Click-Through Rate Prediction. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management . ACM, 2759-2769. [73] Ziwei Zhu, Jianling Wang, and James Caverlee. 2020. Measuring and Mitigating Item Under-Recommendation Bias in Personalized Ranking Systems. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . ACM, 449-458."}
