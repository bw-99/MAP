{
  "Minimizing Live Experiments in Recommender Systems: User Simulation to Evaluate Preference Elicitation Policies": "CHIH-WEI HSU, Google Research, USA MARTIN MLADENOV, OFER MESHI, Google Research, USA JAMES PINE, Google Research, USA HUBERT PHAM, Google Research, USA SHANE LI, Google, USA XUJIAN LIANG, Google, USA ANTON POLISHKO, Google, USA LI YANG, Google Research, USA BEN SCHEETZ, YouTube, USA Google Research, USA CRAIG BOUTILIER, Google Research, USA Evaluation of policies in recommender systems typically involves A/B testing using live experiments on real users to assess a new policy's impact on relevant metrics. This 'gold standard' comes at a high cost, however, in terms of cycle time, user cost, and potential user retention. In developing policies for onboarding new users, these costs can be especially problematic, since on-boarding occurs only once. In this work, we describe a simulation methodology used to augment (and reduce) the use of live experiments. We illustrate its deployment for the evaluation of preference elicitation algorithms used to onboard new users of the YouTube Music platform. By developing counterfactually robust user behavior models, and a simulation service that couples such models with production infrastructure, we are able to test new algorithms in a way that reliably predicts their performance on key metrics when deployed live. We describe our domain, our simulation models and platform, results of experiments and deployment, and suggest future steps needed to further realistic simulation as a powerful complement to live experiments. Additional Key Words and Phrases: User modeling, Preference elicitation, Interactive recommender systems, Simulation",
  "1 INTRODUCTION": "Recommender systems (RSs) play a crucial role in making digital content accessible to users in domains such as ecommerce, news, video, among others [1, 13, 17, 29, 37]. RSs typically leverage past user interactions to learn about a user's preferences to improve their recommendations. However, when information about the user's preferences is minimal, or even non-existent in the case of new users, preference prediction is especially challenging-this is often dubbed the cold-start problem [8, 27]. Cold-start can be addressed in various ways [21], but for new users it is often natural to ask some preliminary questions about their preferences during an onboarding process , using some form of explicit preference elicitation (PE) [24, 42, 45]. In this work, we address the problem of onboarding new users of the Authors' addresses: Chih-Wei Hsu, Google Research, 1600 Amphitheatre Parkway, Mountain View, USA, 94043, cwhsu@google.com; Martin Mladenov, Google Research, 1600 Amphitheatre Parkway, Mountain View, USA, 94043, mmladenov@google.com; Ofer Meshi, Google Research, 1600 Amphitheatre Parkway, Mountain View, USA, 94043, meshi@google.com; James Pine, Google Research, 1600 Amphitheatre Parkway, Mountain View, USA, 94043, rubicon@google.com; Hubert Pham, Google Research, 1600 Amphitheatre Parkway, Mountain View, USA, 94043, huberpham@google.com; Shane Li, Google, Mountain View, USA, lishane@google.com; Xujian Liang, Google, Mountain View, USA, xujianliang@google.com; Anton Polishko, Google, Mountain View, USA, antp@google.com; Li Yang, Google Research, 1600 Amphitheatre Parkway, Mountain View, USA, 94043, lyliyang@google.com; Ben Scheetz, YouTube, New York, USA, bscheetz@google.com; Craig Boutilier, Google Research, 1600 Amphitheatre Parkway, Mountain View, USA, cboutilier@google.com. 1 2 Chih-Wei Hsu et al. YouTube Music platform using various PE methods that adaptively, in a personalized fashion, ask a new user about their preferences for specific musical artists (see Fig. 1 and Sec. 2 for further details). Developing new algorithms for any user interaction in an RS is challenging. Offline data generated by past interactions with users is commonly used to build predictive models of user responses to an RS's actions or recommendations (e.g., click-through, purchase, or engagement models) [14, 47], and use these to devise new policies. However, because new policies often influence usage, user state, and other factors-effectively, they change the data distribution relative to that on which models were originally trained-rarely is a new policy deployed without first running controlled A/B tests in live experiments (LEs) with real users to assess its impact on key long-term RS metrics [25, 46]. Indeed, well-designed A/B tests are considered to be the gold standard for such evaluation. 1 Of course, the use of LEs for A/B testing has well-known costs [46]. Online evaluation can impose reputational and retention costs by degrading the experience of users exposed to poorly performing (experimental) policies. Statistically significant results-which place strict demands on required sample sizes-may require non-trivial amounts of time to emerge, which can slow the algorithm development cycle. Finally, opportunity cost is especially acute when testing onboarding methods: since users onboard only once (in contrast to, say, repeated interaction with content recommendations), a poor user experience induced by an experimental algorithm leaves no opportunity for recovery. To alleviate these concerns, simulation has been used increasingly for RS research and algorithm development [2, 23, 36, 51, 53]. Simulated user models allow developers to 'observe' how (stochastic) user responses unfold in reaction to sequences of RS actions or recommendations, nominally circumventing the out-of-distribution issues of static data sets, allowing effective assessment of new algorithms, and supporting quicker iteration without imposing the costs of LEs on users. That said, simulation is largely used for algorithm development rather than to replace or reduce LEs since simulation models tend to be relatively stylized, are often designed to predict short-term effects rather than the long-term metrics typically assessed by LEs, and are rarely connected to production infrastructure (which links other RS components, beyond the algorithm in question, that influence user behavior). In this work, we propose a practical methodology for the use of simulation not only to help develop new RS algorithms, but to move a step closer to using simulation to reduce (and even partially replace) LEs for evaluation. We illustrate this methodology using our experience with the development, evaluation and deployment of novel onboarding algorithms for new users of YouTube Music. Along the way we outline: (a) the development of user models-in our case recurrent neural networks [19] and transformers [48]-that capture the distribution of (relevant properties of) real users expected to be seen under product usage; (b) a process that ensures these models are reasonably counterfactually robust so performance predictions for new policies can be used to guide deployment decisions [23]; and (c) the framework used to run our simulated users against production infrastructure [2] to ensure simulated metrics reflect all product intricacies. We conclude with a discussion of some open challenges that need to be addressed to make our methodology universally applicable.",
  "2 ONBOARDING IN A MUSIC RS": "We begin by outlining the general onboarding interface and experience for new users of YouTube Music, and the types of algorithms-namely, preference elicitation (PE) methods-we have developed to drive actual user interaction. The effective development and evaluation of these PE methods motivates our simulation framework and methodology. We refer to Meshi et al. [34] for a more detailed description of the PE methods themselves. 1 User studies can complement such tests [4, 5, 41], but we do not address this here. User Simulation to Evaluate Preference Elicitation Policies 3 We assume an RS designed to recommend items from a large content corpus. We use music recommendation as our focus, where items are music tracks. Behavior-based collaborative filtering (CF) is a dominant approach to RS design in content domains, where methods such as matrix factorization [44] or neural CF [7, 52] are used to generate user and item embeddings in some latent space X âŠ† R ğ‘‘ . A user's general affinity for an item is then given by the dot product or cosine similarity between their embeddings. We assume access to stable item embeddings ğœ™ ( ğ‘– ) âˆˆ X . CF methods require sufficient behavioral data (e.g., listens, item ratings) to generate a usable embedding for a user; hence, valuable (non-generic) recommendations are not viable for new users with no interaction history. One way to address this cold start problem [8, 27] is to use PE [9, 10, 24, 45]. For new users, PE may be used as part of an onboarding process designed to make the service quickly usable and useful. PE methods require some class of queries designed to elicit information about a user's preferences, a specific semantics for user responses, and a procedure for selecting queries (which often adapts to a user's previous responses). For example, PE might ask users for individual item preferences (e.g., 'do you like track X?'), to compare two items (e.g., 'which track do you prefer, X or Y?'), or for attribute preferences (e.g., 'do you like (tracks by) artist A?' or 'do you prefer genre G to genre H?'). Item-based elicitation is often effective with small item corpora, or when making fine-grained distinctions when the RS already has a good understanding of a user's preferences. However, it is unsuitable for onboarding with vast corpora like those in music recommendation: the granularity of item-based PE allows precision, but renders sufficient coverage of the music domain infeasible during a time-constrained onboarding process. Thus, we focus on attribute-based elicitation, specifically, asking a user for their preferences w.r.t. one or more music artists . We assume that attributes (artists) are embedded in the same latent space as tracks so a user's artist preferences informs the RS's assessment of their track preferences. The embedding of query (i.e., artist) ğ‘ is denoted by ğœ™ ( ğ‘ ) âˆˆ X . The interface used to elicit artist preferences is shown in Figure 1. The user can scroll to select as many artists as desired and choose 'Done' to terminate the onboarding process. It is important that onboarding quickly and effectively discover user preferences that support downstream recommendations, since it delays actual music consumption, and a user can terminate the process at any point. As such, the PE strategy plays a critical role in the user experience: it impacts how much and what type of preference information the RS obtains, hence downstream recommendation quality and even overall RS usage. For example, if the PE algorithm shows many similar artists to the user, or artists they are not familiar with, the user might decide that onboarding is not worth their time, and abandon before providing much (or any) preference information. A strategy that does not cover the diversity of a user's musical tastes may lead to less-than-ideal initial RS performance, as may a method that does not delve deeply enough into parts of the music space important to the user. Finally, patience or inclination toward various modes of interaction varies across users. These considerations mean that developing a good PE algorithm for onboarding is highly non-trivial, involving tradeoffs between familiarity and novelty, coverage and 'deep dives,' and much more. We do not provide detailed algorithm descriptions here other than to point out that our PE methods are both personalized to user context (see below) and dynamic (i.e., artists are chosen adaptively based on a user's previous selections/skips during the onboarding session). For our purposes, it is sufficient to treat various algorithms and their parameterizations as black boxes to be evaluated or compared. 2 2 The actual PE methods deployed here involve different methods of personalization , the use of information gain/value to select artists, and tradeoffs between artist coverage/diversity and depth of preference understanding. We refer to [34] for details. For further discussion of onboarding techniques for RSs, see [33], and PE, see [9, 10, 50]. 4 Chih-Wei Hsu et al. Fig. 1. The artist selection interface. Users select artists they like, and skip those they don't. The scrollable interface allows for selection of as many artists as desired, with 'Done' confirming the end of the onboarding session. The artists displayed as the user scrolls are selected dynamically given earlier selections and non-selections (or skips ). Music",
  "Pick 5 artists you like": "Your experience will improve the more you listen Search artists Marshmello Frank Sinatra Elton John blackbear Animals DJ Snake Done Oacnar Glass",
  "3 EVALUATING PREFERENCE ELICITATION POLICIES WITH SIMULATION": "Our goal is to effectively evaluate new PE policies for onboarding. While A/B testing of a new policy versus an incumbent using live experiments (LEs) is the usual approach, it comes with various costs, including the potential imposition on users, and the elongation of the development/testing/refinement cycle for new algorithms. Both of these costs are magnified in our onboarding setting: the first due to the one-off nature of onboarding; and the second due to the novelty of the approaches being tested relative to incumbent policies. We consider the use of simulation to mitigate both of these issues. Simulation has been proposed in recent years as an important tool for RSs. It has primarily been used to reduce the second cost above, namely, algorithm-development cycle time [23, 43, 51, 53], or analyze user dynamics [2]. These can be especially important when testing sequential interaction involving (say) reinforcement learning [12, 22, 55], or multiagent interactions [35]. Effective simulation for algorithm development requires one to develop synthetic user models that capture both the distribution of relevant user characteristics, and user behavioral responses to recommender actions (e.g., recommendations, questions). These characteristic and response models need not reflect every aspect of users; but when tested in a User Simulation to Evaluate Preference Elicitation Policies 5 population of simulated users, they should be faithful enough to assess whether one algorithm is expected to perform better than another when deployed. Many of the methods cited above adopt this perspective. In our case, user models reflect (conditional) user responses to questions about artist preferences, their chance of exiting the onboarding process at any stage, and the distribution of such behaviors in the new-user population. Developing such user models is itself quite challenging. Typically, an existing RS policy has already been deployed whose data can be used to train predictive user models. Unfortunately, this static offline approach is often unable to predict behaviors when users are presented with new policies , since new policies generate data and behaviors out-of-distribution w.r.t. the production policy. Without effective techniques for off-policy evaluation [16], these models are unlikely to be counterfactually robust . We approach this issue by building models using data collected from several policies that we believe capture a suitable range of user behaviors. We discuss this issue further in Sec. 4 where we describe our user models, Sec. 6 where we validate our approach empirically, and Sec. 7 where we outline additional challenges for future research. Apart from reducing cycle time, we also want to use simulation to predict how actual deployment of a new algorithm in production will impact key performance metrics, including user satisfaction/value. This raises another complication: typical production RSs consist of many interacting components, all of which influence what a user sees/experiences, of which the new algorithm is just one part. A true evaluation of an algorithm's impact requires integrating it into the production stack-something that happens naturally in both LEs and deployment-and running the production RS against simulated users. RS simulation work rarely considers the integration of synthetic or simulated users into production infrastructure-one exception is Meta's WW model [2], used to simulate, for example, scam and spam attacks on users. This integration is critical if simulation is to replace, or at least reduce the use of, LEs. We describe our framework for production-level simulation in Sec 5.",
  "4 USER MODELS": "We now describe our user models, designed to interact with the onboarding PE module of the RS. Our models have two requirements: generating a synthetic user population whose distribution of relevant characteristics reflects those of the user base; and providing a user response model that accurately reflects user behavior with our PE methods conditioned on these characteristics. We make two key assumptions that support our modeling approach. First, some version of the onboarding process (OP) (Sec. 2) has been deployed prior to the development or testing of the new methods that are the target of simulation. Often simple methods are deployed prior to developing more sophisticated algorithms. Second, most users who engaged with this original OP moved on to use the RS-in this case YouTube Music-for an extended period. Together, this provides access to a set of users U , each of whom has engaged with the existing (or prior) OP to generate OP session data , and, following onboarding, used the RS to engage with content giving us post-onboarding usage . Specifically, each ğ‘¢ âˆˆ U is associated with a single OP session ğ‘† ğ‘¢ and static user state (or context ) ğ¶ ğ‘¢ . We elaborate on the form of this data below. The new OP that we wish to test is driven by one of several PE algorithms being considered for evaluation. These algorithms adapt their artist selection to the previous selections/skips of the user. 3 When presented with an artist, a user makes two distinct binary choices: (i) select or skip that artist; and (ii) terminate or continue with the OP. Simulating a user engaging with the OP requires generating these two actions at each step. Action sampling is conditioned on (a) the 3 Our PE methods are also personalized given prior music engagement by the user, but we set this aside in our treatment here. 6 Chih-Wei Hsu et al. Fig. 2. Histograms of cosine similarity between inferred user interests and selected/unselected artists (left). Artist click-through rate with respect to cosine similarity (right). Click No click 0.0 0 4 0.6 0.8 10 Cosine 00 0 2 0 4 0.6 Cosine static (or prior) user state ; and (b) the user's OP session history so far. The interaction of a synthetic user ğ‘¢ with the OP proceeds through a sequence of ğ‘¡ > 0 turns as follows: (1) RS (PE module) queries ğ‘¢ with an artist. (2) ğ‘¢ responds (selects/skips, terminates/continues). (3) ğ‘¢ 's context (latent state) is updated. (4) RS (PE module) updates its beliefs about the user given the response (note: we treat this as a black box). This repeats until ğ‘¢ terminates the session.",
  "4.1 User State Generator": "The first component of our generative user model comprises a distribution ğ‘ƒ ( ğ¶ ğ‘¢ ) of static user states , those user characteristics that drive a user's behavior when engaging with the OP. Our user state generator divides this user state into observable context ğ¶ ğ‘¢ ğ‘œ and latent state ğ¶ ğ‘¢ â„“ . Observable context consists of factors that are directly observable and (potentially) influence a user's musical tastes and OP interactions-these might include geography, device type (e.g., mobile vs. desktop, OS type), and other such features. 4 The distribution of these features is readily available in data (see below), and the OP's PE algorithm has access to these features. Perhaps more critical is the user's latent state, the most important element of which is the user's true underlying musical preferences. These are, of course, not observable; moreover, for new users, there is no data to support its estimation. However, by assumption, we have a large number of users who have not only used the original OP, but have subsequently engaged with the RS. It is this subsequent engagement that allows us to estimate a user's true preferences, retrospectively correlate them with behavior in the OP, and generate realistic preference distributions. Together with the observable state, this comprises our user state generator . Specifically, we augment OP session data with an ordered list { ğ‘ ğ‘¢ 1 , ğ‘ ğ‘¢ 2 , . . . , ğ‘ ğ‘¢ ğ¾ } comprising the topğ¾ artists w.r.t. ğ‘¢ 's post-onboarding music consumption time during the first ğ‘Š weeks of YouTube Music usage. 5 We use this data below to train latent user-preference models. Figure 2 (left) shows histograms of (embedding) similarity scores between users' topğ¾ post-onboarding artists (their 'true' interests) and their selected (Click) or skipped (No click) artists during the 4 Geography is strongly correlated with musical preferences, while device type influences the UI layout, hence OP interactions. 5 Initial RS recommendations will of course be correlated with onboarding selections and affect post-onboarding consumption. However, user searches and RS exploration will make this effect smaller as the horizon ğ‘Š increases. User Simulation to Evaluate Preference Elicitation Policies 7 OP, showing the greater correlation/similarity of actual music consumption with OP selections. Figure 2 (right) shows the click-through rate (CTR) during the OP as a function of the similarity of the query artist with post-onboarding artists: query artists that are more similar to post-OP artists are more likely to be selected during the OP. To train the user state generator ğ‘ƒ ( ğ¶ ğ‘¢ ) , we can easily learn a categorical (joint) distribution over relevant observable state variables ğ¶ ğ‘¢ ğ‘œ (e.g., geo, device) from our data. However, directly modeling the joint distribution ğ‘ƒ ( ğ¶ ğ‘¢ â„“ ) of latent preferences (i.e., all combinations of inferred user interests) is intractable. Hence, we formulate our latent state model as a sequence generation or next-item prediction problem [54] conditioned on the observable state ğ‘ ğ‘¢ . Specifically, given a user's (post-OP) music consumption, we predict the next item ğ‘ ğ‘¢ ğ‘˜ given a user ğ‘¢ 's past interaction history: ğ‘ƒ ( ğ‘ ğ‘¢ ğ‘˜ | ğ‘ ğ‘¢ 1 , . . . , ğ‘ ğ‘¢ ğ‘˜ -1 ) . We factorize the joint distribution ğ‘ƒ ( ğ¶ ğ‘¢ ) as:  Recurrent neural networks (RNNs) [20] have proven effective for next-item prediction [18, 31, 40]. Thus, we use an RNN to model the temporal dependencies needed to generate inferred user interests ğ‘ ğ‘¢ 1 , . . . , ğ‘ ğ‘¢ ğ¾ . We apply a trainable multilayer perceptron (MLP) adapter to the one-hot encoding of ğ‘ ğ‘¢ and seed the initial state of the RNN â„ ğ‘¢ 0 with the output. The RNN updates its hidden state â„ ğ‘¢ ğ‘˜ and generates output ğ‘œ ğ‘¢ ğ‘˜ as follows:  where ğœ™ ( ğ‘ ğ‘¢ ğ‘˜ ) is the embedding of artist ğ‘ ğ‘¢ ğ‘˜ and ğ›¿ ğ‘˜ is a one-hot encoding of the number of artists generated so far. We do not train the artist embedding layer, but instead reuse embeddings from the PE module. The next artist ğ‘ ğ‘¢ ğ‘˜ + 1 is then sampled (without replacement) using a standard multinomial logit model [11, 32] given ğ‘œ ğ‘¢ ğ‘˜ :  To generate variable-length sequences, we introduce a fixed null artist embedding ğœ™ âˆ… . We do not increase ğ‘˜ and change the input vector whenever the null artist is sampled. Including ğ›¿ ğ‘˜ as an RNN input helps match the distribution of 'lengths' of the number of artists in users' inferred interests to the observed data. Our latent state generator is implemented with a long short-term memory (LSTM) architecture [19]. By modeling generation of user interests ğ‘ƒ ( ğ‘ ğ‘¢ ğ‘˜ + 1 | ğ¶ ğ‘¢ , ğ‘ ğ‘¢ 1 , . . . , ğ‘ ğ‘¢ ğ‘˜ ) = ğ‘ƒ ( ğ‘ ğ‘¢ ğ‘˜ + 1 | ğ‘œ ğ‘¢ ğ‘˜ ) with an RNN and ğ‘ƒ ( ğ¶ ğ‘¢ ) as a categorical distribution, we can optimize parameters ğœƒ of the entire generator by maximum likelihood estimation (MLE):",
  "4.2 User Session Generator": "The second component of our model is a user session generator that samples a user's action choice when presented with an artist, conditioned on their (static) state and their action choices thus far. User's attitudes evolve during the OP (e.g., degree of engagement/frustration with the OP; satisfaction with the information provided; try to steer the OPs future 'queries'). These are part of the user's latent, dynamic state : our model captures such dynamics implicitly to the extent they help generate realistic session behaviors. To train the session generator, we assume a set of artists Q = { ğ‘ 1 , . . . , ğ‘ ğ‘š } eligible to be shown during PE. A session ğ‘† ğ‘¢ for ğ‘¢ âˆˆ U comprises a sequence of artists/queries ğ‘ ğ‘¢ ğ‘¡ and corresponding responses ğ‘Ÿ ğ‘¢ ğ‘¡ : ğ‘† ğ‘¢ = {( ğ‘ ğ‘¢ 1 , ğ‘Ÿ ğ‘¢ 1 ) , . . . , ( ğ‘ ğ‘¢ | ğ‘† ğ‘¢ | , ğ‘Ÿ ğ‘¢ | ğ‘† ğ‘¢ | )} , 8 Chih-Wei Hsu et al. where ğ‘Ÿ ğ‘¢ ğ‘¡ âˆˆ { 0 , 1 } is the user response (skip or select) to ğ‘ ğ‘¢ ğ‘¡ at turn ğ‘¡ . User termination occurs only at turn ğ‘¡ = | ğ‘† ğ‘¢ | . Define ğ‘† ğ‘¢ < ğ‘¡ = {( ğ‘ ğ‘¢ 1 , ğ‘Ÿ ğ‘¢ 1 ) , . . . , ( ğ‘ ğ‘¢ ğ‘¡ -1 , ğ‘Ÿ ğ‘¢ ğ‘¡ -1 )} to be ğ‘¢ 's sub-session prior to ğ‘¡ . We note that this data format makes certain simplifying assumptions about the linear nature in which users inspect artists, which may not be precisely valid depending on the UI, since multiple artists are visible at once (slate size varies with device type). 6 We truncate each session ğ‘† ğ‘¢ at 300 turns. We formulate the user response model ğ‘ƒ ( ğ‘Ÿ ğ‘¢ ğ‘¡ | ğ¶ ğ‘¢ , ğ‘† ğ‘¢ < ğ‘¡ , ğ‘ ğ‘¢ ğ‘¡ ) as contextual sequence generation, using an RNN to encode (in-session) dynamic user state and its dynamics. We first embed inferred user interests in ğ¶ ğ‘¢ via function ğœ™ , to which we apply a trainable MLP to obtain encoded context ğ¸ ( ğ¶ ğ‘¢ ) to personalize session state, dynamics and output (user response) for any synthetic user ğ‘¢ . The input vector ğ‘¥ ğ‘¢ ğ‘¡ to the RNN at turn ğ‘¡ includes ğ¸ ( ğ¶ ğ‘¢ ) , the embedding ğœ™ ( ğ‘ ğ‘¢ ğ‘¡ ) of the artist/query, the embedding of selection response ğœ™ ğ‘Ÿ ( ğ‘Ÿ ğ‘¢ ğ‘¡ ) , the number ğ‘¦ ğ‘¡ of selections so far, and ğ‘¡ . The latter two are critical for reproducing realistic distributions of the number of artist selections and session length. The response embedding ğœ™ ğ‘Ÿ is trainable but, as above, we reuse artist embeddings { ğœ™ ( ğ‘ )} ğ‘ âˆˆQ from PE. The RNN hidden state â„ ğ‘¢ ğ‘¡ at turn ğ‘¡ is updated as:  where â„ ğ‘¢ 0 = ğ¸ ( ğ¶ ğ‘¢ ) and ğ‘œ ğ‘¢ ğ‘¡ is the output of the RNN at turn ğ‘¡ . OP sessions ğ‘† ğ‘¢ can vary in length | ğ‘† ğ‘¢ | since ğ‘¢ determines when to terminate based on their latent state and the current artist; thus, the response model must also predict session continuation. We use an MLP with two heads and a sigmoid activation function ğœ to implement selection response ğ‘Ÿ ğ‘¢ ğ‘¡ and session continuation ğ‘  ğ‘¢ ğ‘¡ \"   We also pass Â¯ ğ‘¥ ğ‘¢ ğ‘¡ -1 = ( ğ¸ ( ğ¶ ğ‘¢ ) , ğ‘¦ ğ‘¡ -1 , ğ‘¡ -1 , ğœ™ ( ğ‘ ğ‘¢ ğ‘¡ )) to the MLP. The response model does not predict a selection if the model predicts termination. As above, the session generator is implemented using an LSTM, and we optimize its parameters by MLE.",
  "4.3 Transformer-based User Models": "In addition to RNNs, we experimented with transformers [49] to generate user context and sessions. Transformers perform similarly to RNNs, so we focus on RNNs in the following sections; but we describe the transformer for completeness. Generating user context with a transformer is similar to the RNN-based approach. The input is the same sequence of vectors as used by the RNN. Each input also includes a position embedding. We sample the ğ‘˜ + 1st artist using a multinomial logit (Eq. 3), where the ğ‘œ ğ‘¢ ğ‘˜ is the transformer's output given the ğ‘˜ th input. The sampled artist is appended to the input, and the process repeats to generate a user's preferences. Generating a user session with a transformer is also similar to the RNN, though its input differs more; the input sequence comprises: the sequence of inferred artists ( ğœ™ ( ğ‘ ğ‘¢ 1 ) , ..., ğœ™ ( ğ‘ ğ‘¢ ğ¾ )) is (static) state ğ¶ ğ‘¢ , each concatenated with a learned embedding denoting user context; the artists queried so far, ( ğœ™ ( ğ‘ ğ‘¢ 1 ) , ..., ğœ™ ( ğ‘ ğ‘¢ ğ‘¡ -1 )) , each concatenated with its response ğœ™ ğ‘Ÿ ( ğ‘Ÿ ğ‘¢ ) embedding; and the current query/artist embedding ğœ™ ( ğ‘ ğ‘¢ ğ‘¡ ) . We sample a response and session continuation with Eqs. 6 and 7, using the transformer output ğ‘œ ğ‘¢ ğ‘¡ given to the current artist/query input ğœ™ ( ğ‘ ğ‘¢ ğ‘¡ ) , along with Â¯ ğ‘¥ ğ‘¢ ğ‘¡ as in the RNN. 6 We discuss our assumptions further in Appendix A Since the linear inspection assumption more closely approximates usage on mobile devices (vs., say, desktop), we confine our models and experiments to mobile platforms. User Simulation to Evaluate Preference Elicitation Policies 9",
  "5 THE SIMULATOR": "We now describe our production-level simulator, which integrates our synthetic user models into the production stack to simulate users engaging in the real OP. As discussed in Sec. 3, a controlled offline, synthetic user experiment requires running the production RS with simulated users (vs. a mock RS). We create a simulation service that mocks user interactions, while preserving production logging/serving. This poses challenges, including maintaining separation between the simulated and the live system. To do so, we must ensure: (a) The simulator runs against the full contentserving stack; (b) Simulation does not interfere with live system function; (c) Simulated-user data is separated from real-user data; and (d) The production RS cannot distinguish simulated- and real-user data. Separation of simulated and production data does not imply separation of the infrastructure accessing this data. This too poses challenges since production infrastructure may not be designed to handle the usage patterns generated by a simulator (e.g., O(1B) existing users consuming content might exercise the infrastructure differently than O(100K) new users onboarding at once). Simulating Music Onboarding. The user model interacts with the simulator through an API. This API provides the following capabilities: (1) Navigation controls for 'looking' at different artists in the slate; (2) Pre-seeding the (synthetic) user with music preferences to be reflected in the list of artists; (3) Artist selection from the slate; (4) Dynamic updating of the artist list presented to the synthetic user given any artist selection; (5) Submission of all artist selections to be saved in the user profile to be consumed by the downstream RS; and (6) Navigation to the home page after submitting selections to see homepage results after users make their selections. To ensure feature parity between the simulator and production RS, the simulator queries a front-end service which returns all elements to be rendered on the YouTube Music app. The front-end communicates with the RS back-end using the same content corpus and control flow as the live system. For each synthetic user, the simulator creates a new test account and requests the user-state generator to initialize simulated user data. The front end includes the list of artists to display upon-load, taking into account the generated user state and additional artists to be dynamically loaded when the user selects an artist. The simulator parses this response and organizes artists linearly (see Sec. 4) to present to the user. The user session generator selects an artist once the recommendations are served. The simulator responds to the front-end when an artist is selected, and the front-end adjusts the artist list accordingly. Once the user session generator has finished selecting artists, it calls the API to submit the artists, which triggers a call to store these selections (i.e., our simulated user data) so as to affect downstream recommendations when the synthetic user is redirected to the YouTube Music homepage. The interface to most current large-scale production infrastructure is request-based, where a request is typically associated with a single user. While underlying systems might batch or aggregate individual requests to exploit vectorized operations during model inference, not all systems support interaction in a batched manner. Thus, middleware may be necessary to bridge the two, and provide the scale needed to simulate many synthetic users. Handling Simulated User Data. Storing artist selections by synthetic users must not change RS behavior for real users. For instance, suppose the number of artist selections during the OP across some user segment changes which artists are recommended. Then synthetic artist selections must not increment those counts for real users. Ensuring consistency and proper access control of (real and synthetic) user data for dependent systems happens using a single user data serving service (UDSS) . In addition, we use a data overlay service (DOS) which allows on-demand rewriting of synthetic data for the RS (not just the OP). In a live system, real user data is recorded by a production service and 10 Chih-Wei Hsu et al. written to the UDSS. However, the simulator makes no such call; instead it calls the DOS to write the synthetic user history. Because the simulator runs in a controlled environment, we directly control synthetic user data. There are two paths to the DOS, a write path and a read path. On the write path, DOS accepts a user specification as input, from which it creates simulated user data, which is stored in its own storage system under each test account. The simulated user data generated by DOS fulfills any requirements needed by the RS consuming the data. On the read path, when UDSS receives a request, it recognizes whether is comes from our simulator. For such requests, UDSS makes an additional call to DOS to obtain its simulated user data, and either replaces the production data or merges with production data when generating a response. Adding the DOS overlay only when reading user data ensures no changes need to be made to upstream services. Note that separation of simulated and production data may not imply a separation of infrastructure accessing simulated data. To provide a single point of access to upstream systems, DOS may need to interact with production infrastructure at a lower level than a typical 'client' and therefore have to implement its own caching, batching, throttling, etc. to prevent disruption of production infrastructure.",
  "6 EXPERIMENTAL RESULTS": "We train RNN user models, using logged data of YouTube Music OP sessions, using the user-simulation development platform RecSim NG [36]. We deploy a simulation service for the OP, as described in Sec. 5, that allows our simulated user models to interact with production infrastructure. We describe simulation results generated by our user models when interacting with both existing and previously unseen PE policies, focusing on validation of counterfactual robustness and policy optimization via offline simulation.",
  "6.1 Simulation with the Original Data Policy": "Our first experiment exploits a deployed heuristic, well-performing OP policy, which serves as the behavior (i.e., datagenerating) policy. Our user models are trained on this data, and we evaluate simulation quality by assessing whether it reproduces the distribution of key statistics observed in live (ground-truth) data. The user (static) state generator does not depend on the RS policy, so we simply compare the distribution of generated contexts with the ground-truth distribution. Figure 3 shows convergence of user state generator training. The histogram of log-probabilities (left) shows model fitness improves with training steps. The right plot shows the Wasserstein distance (over 4096 randomly sampled users) between the ground-truth and generated distributions of the number artists in each user's preferences. We see that the model quickly reproduces a suitable distribution of user interest 'sizes.' Precision and recall metrics in Figure 4 demonstrate the accuracy of generated- w.r.t. ground-truth preferences. For user session generation, we assess the distribution of (i) the number of artist selections per session and (ii) number of artist impressions before termination (OP session length). Fig. 5 shows that the session generator reproduces the session-length distribution well: the cumulative distribution function (CDF) of generated lengths (red line) matches the observed CDF (black line) well. It also reproduces the artist-selection count distribution relatively well (generated in red, observed/ground-truth in black).",
  "6.2 Simulation of New Policies": "Our end-to-end simulation results above with 'on-policy' models (i.e., models interacting with behavior policies that generated their training data) show that the synthetic trajectories closely resemble those observed in logs. However, our goal of using simulation is to test (and ultimately optimize) new/unseen OP PE policies requires that our user models User Simulation to Evaluate Preference Elicitation Policies 11 Fig. 3. Convergence of training the user context generator: histogram of log-probabilities over number of training steps (left) and Wasserstein distance between the ground truth distribution of the number of artists in inferred user interests and the generated one (right). Wasserstein Distance Probs -80 -120 -160 5k 10k 15k 2Ok 25k 30k 35k 4Ok 10k 30k 40k Training Steps Training Steps Log Fig. 4. Precision (left) and recall (right) when comparing generated user interests with ground-truth user interests. Rccal 0.33 0.35 0.31 0.33 0.29 0.31 0.27 0.25 0.27 5k 10k 15k 2Ok 25k 30k 35k 4Ok 5k 15k 2Ok 25k 30k 35k 4Ok Training Sleps Trannasteps exhibit a degree of counterfactual robustness. In other words, can they be used for off-policy evaluation to provide a reasonable assessment of (at least some) previously unseen policies. We now turn to this assessment. A critical user response during the OP is whether to select a presented query/artist (Eq. 6). To this end, a new pCTR policy was deployed in production: it exploits a model trained to predict the artist click-through rate (CTR) (probability of selection) given the user's OP history so far, and heuristically prioritizes artists with higher predicted CTR (pCTR) to increase onboarding selections. To validate the counterfactual robustness of our state and session generators-trained only on the previous behavior policy's data-we simulate 200 000 synthetic users against the new pCTR policy to forecast changes in both impressions and selections. We also ran LEs to gather the same online metrics with Ëœ 134 000 real users. Table 1 provides the results of the new pCTR policy vs. the incumbent production policy (percentage change) with 95% confidence intervals (CIs) on a (relatively large) traffic slice (one geo, one device type). Employing such traffic slicing imposes difficulties for LEs, since the small test population (even for the largest slices) induces much wider CIs than simulated metrics-and shrinking the CI risks exposing significantly more users to a potentially poorly performing experimental policy. Thus, the ability to produce tighter CIs are a significant advantage of simulation. This leads to consistent overlap between online and simulated CIs. The LE and simulation both serve the common purpose of 12 Chih-Wei Hsu et al. Fig. 5. Cumulative distribution functions (CDFs) for both session lengths (left) and number of selections per session (right) by interacting with the data-generating policy where x-axis represents the rank in a batch of 4,096 sessions. Generated CDFs are red and observed CDFs in the log are black. Session Length Selection Count 250 1000 7000 3000 4000 Session Length Rank Selection Count Rank predicting the impact of the new policy if/when launched. In this case, the policy was launched, allowing us to compare the simulated metrics to the actual launch metrics-in Table 1, we compute post-launch metrics by sampling logs of 500 000 real OP users (this size provides much narrower CIs). 7 Moreover, while the LE is less helpful in predicting launch impact due to its wide CIs, our simulated predictions are more closely aligned (informally, not statistically due to the wide LE CIs) with the real-world impact of the pCTR policy w.r.t actual post-launch metrics obtained from 500,000 real users. Our simulation results also closely match the observed session length and artist-selection count distributions in post-launch logs, and faithfully reproduce the artist-selection count distribution, as seen in Figure 6 (generated in red, observed in black). Additionally, we evaluated AUCLoss of the selection prediction (Eq. 6), which demonstrates that the user model has similar accuracy on both post-launch logs (0.760) and training data (0.763). Naturally, this approach cannot, in general, guarantee strong off-policy results. However, to the extent that (a) the user model is trained on a reasonable variety of data/trajectories (due to policy variability and/or inherent stochasticity in user behaviors and contexts), and (b) the new policy is not radically different from the deployed policies used to generate training, a user simulator for new-policy evaluation can offer significant insights. Both of these conditions are met in our set up, which accounts for the strong performance. We next exploit this counterfactual robustness further to 'generate' new policies.",
  "6.3 Offline Optimization": "Once validating model quality (Sec. 6.1) and partial counterfactual robustness (Sec. 6.2), we test whether we can use our user models to improve the RS's policy purely offline using simulation with synthetic users. The ability to do so reliably can significantly reduce both algorithm-development cycle time and the user-cost of running LEs to compare 7 We assume metrics and user behaviors are stable over time in this pre-post analysis. User Simulation to Evaluate Preference Elicitation Policies 13 Fig. 6. Cumulative distribution functions (CDFs) for both session length (left) and selection counts per session (right) by interacting with a new pCTR policy where x-axis represents the percentile rank. Generated CDFs are red and observed CDFs in post-launch log are black.",
  "Session Length": "",
  "Selection Count": "300 300 200 200 100 100 0% 20% 40% 60% 80% 100% 0% 40% 60% 80% 100% Rank Rank Table 1. Comparison of Live Experiment, Simulated, and Post-launch Metrics of New pCTR Policy on a Traffic Slice. candidate algorithms. In its simplest form, we use simulation to compare policies at a scale that is infeasible to test using LEs, and select the most promising policies for LE. In the first test of this approach, we simulated three new pCTR models -these vary in the features used to predict an artist's selection probability and in their training data filters-and identified the best candidate: it has a simulated CTR gain of + 1 . 15% [-1 . 71% , 4 . 01% ] over the original pCTR model. This candidate was run in an LE with an observed CTR gain of + 2 . 06% [ 0 . 20% , 3 . 92% ] ; and it was subsequently launched (put into production). We next conduct simulation-based experimentation to test another class of PE/OP policies for product launch. We compare versions of a new PE policy which vary a single parameter controlling the trade-off between an artist's pCTR and its coverage of artist space. Specifically, each candidate query/artist ğ‘ is scored using Score ( ğ‘ ) = pCTR ( ğ‘ ) + ğœ† Coverage ( ğ‘ ) : the first term is ğ‘ 's pCTR and the second, ğ‘ 's gain in coverage (see Meshi et al. [34]). A strong-performing production policy with ğœ† = 0 . 1 was launched. We subsequently used simulation to tune the trade-off parameter ğœ† , by enumerating nine values of ğœ† and testing if any generate better metrics than the production policy with ğœ† = 0 . 1. Table 2 presents our offline simulation (Sim.) results on all mobile traffic, comparing simulation results of our nine candidates with the simulated results of the original. Policies with ğœ† = 0 . 2 and ğœ† = 0 . 05 provide the greatest selection gains, with only modest impression increases, but have wide CIs. These results led to running LEs with policies using only these two 14 Chih-Wei Hsu et al. Table 2. Simulated Results (with 95% CIs) when Tuning the Trade-off between pCTR and Coverage. Table 3. Selection Change (95% CIs) for 3 Policies, and Induced Policy Ordering: LEs vs. 'Launch' vs. Simulation. values of ğœ† to test their actual improvements. To test if simulation can rule out poor policies, we also run ğœ† = 0 . 001 in LE (it was effectively neutral w.r.t. selections). To test the hypothesis that large-scale simulation can more accurately order policies w.r.t. their launch metrics than small LEs (whose size may be constrained for 'cost' reasons), we run one large 'launch LE' over a specific period with the three policies ( ğœ† âˆˆ { 0 . 001 , 0 . 05 , 0 . 2 } ) to measure their impact on the number of artist selections in an OP session. 8 We then partition the 'launch' data by looking at three subperiods, treating each as if it were a smaller scale 'true LE.' We assess the policy ordering w.r.t. each 'true LE' to assess consistency with the launch data; we evaluate our simulation similarly. 9 Our results in Table 3 show the variation across the three small 'true LEs,' none of which order the policies in the same way, and only one of which conforms to the 'launch;' while the simulation ordering predicts (informally) that of the launch. While this test is imperfect (since we do not want to incur the cost of 'launch-scale' LEs on multiple policies), it suggests that simulation can be used to effectively order policies offline.",
  "7 SIMULATION-BASED RS DEVELOPMENT": "We have demonstrated above the use of user simulation for some of the evaluation needed during RS development typically handled using costly LEs. This idea, taken to the extreme, paints a picture of a development and testing workflow largely free of LEs, where most evaluation takes place in silico . However, deploying such a workflow across a broad selection of RS tasks, beyond onboarding, faces many methodological challenges before it can be deployed as a production-ready technology, some of which we outline below (we also speculate on potential solutions). Counterfactual robustness. Our methodology relies on the counterfactual robustness of the user model one develops; that is, it exploits a user model, trained on user behaviors induced by a (small) set of deployed behavior 8 We focus on artist selections here, but in Appendix B we show similar results for impressions (which are somewhat noisier). 9 We note that the 'LEs' and 'Launch' metrics will be more correlated in our design than they would be in practice, since each LE is in fact a one-third sample of the Launch traffic. User Simulation to Evaluate Preference Elicitation Policies 15 policies , to evaluate the performance of a new policy (w.r.t. relevant metrics) not yet encountered by users. Without special accommodation, a model so-trained may overfit to the behavior policies. Promising solutions to this may be found in causal modeling [38, 39], though approaches that enforce constraints on model structure (e.g., mechanistic plausibility) might limit the use of high-capacity models (e.g., transformers). Notions like ğ‘šğ‘§ -transportability [6] may prove useful, by forcing a model to learn patterns common to multiple behavior policies, so that what is learned is (ideally) independent of the RS policy. Given that production RSs deploy numerous policies (and LEs) over time, the natural variation in behavior policies and data distributions can be harnessed for effective model training. How to select a minimal useful set remains an open question. Exploration and exploitation. From the point of view of the RS lifecycle, simulated A/B testing is effectively a bandit problem [3, 26, 28]: policies are arms whose value/metrics are predicted by the simulator. Since all models are imperfect, the value of a policy may be either overestimated or underestimated. Overestimation may be less problematic than underestimation in the long-run-if a policy's value is overestimated, a policy will 'graduate' to an A/B test in an LE, where its true value will be revealed (at a cost). Ideally, data generated from such an LE is used to improve the simulation model as well, so that model deficiencies inducing overestimation can be rectified. Underestimation may be more troublesome, since good policies may be discarded prematurely, leaving value/progress 'on the table' and biasing the development process toward what the simulator is able to accurately predict. Avoiding this requires live A/B testing of apparently suboptimal policies, at a rate ideally determined by calibrated uncertainty estimates (also taking environment non-stationarity into account). From a Bayesian perspective, we can think of the uncertainty in the predicted metrics as the product of the uncertainty in the model parameters and the uncertainty of the prediction conditioned on model parameters. The latter can be easy to narrow down by simply increasing the synthetic sample size. However, this does not help with the former uncertainty, which needs to be addressed with proper uncertainty quantification methods. Reliability. If the aforementioned uncertainty estimates are available, a reliable measure of model risk can be used to judge whether simulation results can be trusted. Since this can be challenging, as noted above, there is significant value in developing proxy heuristics that indicate whether simulation results are trustworthy before an LE is carried out. One proxy is to measure how different a test policy is from the behavior policies used to train the model. A smoothness argument suggests a model prediction is more reliable if the test policy is closer to the training policies. Closeness should be measured, however, in the latent space of the model, which may lead to non-trivial estimation problems. Another potential heuristic, applicable to models with latent state, is to measure the predictiveness of its latent representation: if two partial sequences of interactions are compressed into the same latent representation, then the distribution of sequence continuations conditioned on one should be similar to that conditioned on the other. Predictiveness is a sufficient (but not necessary) condition for reliability. Ideas for measuring predictiveness can be found in the literature on predictive state representations [30].",
  "8 CONCLUSION": "In this paper we demonstrated that simulation can play a key role in the evaluation and optimization of RS policies by reducing the need for costly live experiments. We focused on the new user onboarding process in YouTube Music and showed how realistic user models can be learned from logged onboarding sessions and post-onboarding consumption. We described a simulation platform that allows the integration of our synthetic user models will full production infrastructure with suitable separation of live and simulated data, and demonstrated the potential of our approach with 16 Chih-Wei Hsu et al. empirical studies of YouTube Music onboarding. Finally, we outlined some avenues for future research in the promising area and wider adoption of simulation for RS algorithm development and evaluation.",
  "REFERENCES": "[1] Fabian Abel, Qi Gao, Geert-Jan Houben, and Ke Tao. 2011. Analyzing user modeling on Twitter for personalized news recommendations. In User Modeling, Adaption and Personalization . [2] John Ahlgren, Kinga Bojarczuk, Sophia Drossopoulou, Inna Dvortsova, Johann George, Natalija Gucevska, Mark Harman, Maria Lomeli, Simon Mark Lucas, Erik Meijer, Steve Omohundro, Rubmary Rojas, Silvia Sapora, Jie M. Zhang, and Norm Zhou. 2021. Facebook's Cyber-Cyber and CyberPhysical Digital Twins. In Proceedings of the International Conference on Evaluation and Assessment in Software Engineering (EASE-21) . 1-9. [3] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. 2002. Finite-time Analysis of the Multiarmed Bandit Problem. Machine Learning 47 (2002), 235-256. [4] Krisztian Balog and Filip Radlinski. 2020. Measuring Recommendation Explanation Quality: The Conflicting Goals of Explanations. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 . 329-338. [5] Gagan Bansal, Besmira Nushi, Ece Kamar, Walter S. Lasecki, Daniel S. Weld, and Eric Horvitz. 2019. Beyond Accuracy: The Role of Mental Models in Human-AI Team Performance. In Proceedings of the Seventh AAAI Conference on Human Computation and Crowdsourcing, HCOMP 2019, Stevenson, WA, USA, October 28-30, 2019 . 2-11. [6] Elias Bareinboim and Judea Pearl. 2014. Transportability from Multiple Environments with Limited Experiments: Completeness Results. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada . 280-288. [7] Alex Beutel, Paul Covington, Sagar Jain, Can Xu, Jia Li, Vince Gatto, and Ed H. Chi. 2018. Latent Cross: Making Use of Context in Recurrent Recommender Systems. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining (WSDM-18) . Marina Del Rey, CA, 46-54. [8] JesÃºS Bobadilla, Fernando Ortega, Antonio Hernando, and JesÃºs Bernal. 2012. A collaborative filtering approach to mitigate the new user cold start problem. Knowledge-based systems 26 (2012), 225-238. [9] Craig Boutilier. 2002. A POMDP Formulation of Preference Elicitation Problems. In Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI-02) . Edmonton, 239-246. [10] Urszula Chajewska, Daphne Koller, and Ronald Parr. 2000. Making Rational Decisions Using Adaptive Utility Elicitation. In Proceedings of the Seventeenth National Conference on Artificial Intelligence (AAAI-00) . Austin, TX, 363-369. [11] Bassam H. Chaptini. 2005. Use of Discrete Choice Models with Recommender Systems . Ph. D. Dissertation. Massachusetts Institute of Technology, Cambridge, MA. [12] Minmin Chen, Alex Beutel, Paul Covington, Sagar Jain, Francois Belletti, and Ed Chi. 2018. Top-K Off-Policy Correction for a REINFORCE Recommender System. In 12th ACM International Conference on Web Search and Data Mining (WSDM-19) . Melbourne, Australia, 456-464. [13] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks for YouTube recommendations. In Proceedings of the 10th ACM Conference on Recommender Systems . [14] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep Neural Networks for YouTube Recommendations. In Proceedings of the 10th ACM Conference on Recommender Systems, Boston, MA, USA, September 15-19, 2016 . 191-198. [15] Nick Craswell, Onno Zoeter, Michael J. Taylor, and Bill Ramsey. 2008. An experimental comparison of click position-bias models. In Proceedings of the International Conference on Web Search and Web Data Mining, WSDM 2008, Palo Alto, California, USA, February 11-12, 2008 . 87-94. [16] Miroslav DudÃ­k, John Langford, and Lihong Li. 2011. Doubly Robust Policy Evaluation and Learning. In Proceedings of the Twenty-eighth International Conference on Machine Learning (ICML-11) . Bellevue, WA, 1097-1104. [17] Blake Hallinan and Ted Striphas. 2016. Recommended for you: The Netflix Prize and the production of algorithmic culture. New Media & Society 18 (2016), 117-137. [18] BalÃ¡zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2016. Session-based Recommendations with Recurrent Neural Networks. In Proceedings of the Fourth International Conference on Learning Representations (ICLR-16) . San Juan, Puerto Rico. [19] Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long Short-Term Memory. Neural Computation 9, 8 (1997), 1735-1780. [20] J. J. Hopfield. 1982. Neural Networks and Physical Systems with Emergent Collective Computational Abilities. Proceedings of the National Academy of Sciences 79, 8 (1982), 2554-2558. [21] Zan Huang, Hsinchun Chen, and Daniel Dajun Zeng. 2004. Applying associative retrieval techniques to alleviate the sparsity problem in collaborative filtering. ACM Transactions on Information Systems 22, 1 (2004), 116-142. [22] Eugene Ie, Vihan Jain, Jing Wang, Sanmit Narvekar, Ritesh Agarwal, Rui Wu, Heng-Tze Cheng, Tushar Chandra, and Craig Boutilier. 2019. SlateQ: A Tractable Decomposition for Reinforcement Learning with Recommendation Sets. In Proceedings of the Twenty-eighth International Joint Conference on Artificial Intelligence (IJCAI-19) . Macau, 2592-2599. User Simulation to Evaluate Preference Elicitation Policies 17 [23] Eugene Ie, Chih wei Hsu, Martin Mladenov, Vihan Jain, Sanmit Narvekar, Jing Wang, Rui Wu, and Craig Boutilier. 2019. RecSim: A Configurable Simulation Platform for Recommender Systems. (2019). arXiv:1909.04847 . [24] Ralph L Keeney and Howard Raiffa. 1993. Decisions with multiple objectives: preferences and value trade-offs . Cambridge university press. [25] Ron Kohavi, Roger Longbotham, Dan Sommerfield, and Randal M. Henne. 2009. Controlled Experiments on the Web: Survey and Practical Guide. Data Mining and Knowledge Discovery 18 (2009), 140-181. [26] T. L. Lai and Herbert Robbins. 1985. Asymptotically Efficient Adaptive Allocation Rules. Advances in Applied Mathematics 6, 1 (1985), 4-22. [27] Xuan Nhat Lam, Thuc Vu, Trong Duc Le, and Anh Duc Duong. 2008. Addressing cold-start problem in recommendation systems. In Proceedings of the 2nd International Conference on Ubiquitous Information Management and Communication . [28] Tor Lattimore and Csaba Szepesvari. 2020. Bandit Algorithms . [29] Greg Linden, Brent Smith, and Jeremy York. 2003. Amazon.com recommendations: Item-to-item collaborative filtering. IEEE Distributed Systems Online 4 (2003). [30] Michael L. Littman, Richard S. Sutton, and Satinder Singh. 2001. Predictive Representations of State. In Advances in Neural Information Processing Systems 14 [Neural Information Processing Systems: Natural and Synthetic, NIPS 2001, December 3-8, 2001, Vancouver, British Columbia, Canada] . MIT Press, 1555-1561. [31] Q. Liu, S. Wu, D. Wang, Z. Li, and L. Wang. 2016. Context-Aware Sequential Recommendation. In Proceedings of the IEEE International Conference on Data Mining (ICDM-16) . Barcelona, 1053-1058. [32] Jordan J. Louviere, David A. Hensher, and Joffre D. Swait. 2000. Stated Choice Methods: Analysis and Application . Cambridge University Press, Cambridge. [33] Sean M. McNee, Shyong K. Lam, Joseph A. Konstan, and John Riedl. 2003. Interfaces for Eliciting New User Preferences in Recommender Systems. In Proceedings of the 9th International Conference on User Modeling (UM-03) . Johnstown, PA, 178-187. [34] Ofer Meshi, Jon Feldman, Li Yang, Ben Scheetz, Yanli Cai, Mohammadhossein Bateni, Corbyn Salisbury, Vikram Aggarwal, and Craig Boutilier. 2023. Preference Elicitation for Music Recommendations. In ICML 2023 Workshop The Many Facets of Preference-Based Learning . [35] Martin Mladenov, Elliot Creager, Kevin Swerksy, Omer Ben-Porat, Richard S. Zemel, and Craig Boutilier. 2020. Optimizing Long-term Social Welfare in Recommender Systems: A Constrained Matching Approach. In Proceedings of the Thirty-seventh International Conference on Machine Learning (ICML-20) . Vienna, 6987-6998. [36] Martin Mladenov, Chih-wei Hsu, Vihan Jain, Eugene Ie, Chris Colby, Nic Mayoraz, Hubert Pham, Dustin Tran, Ivan Vendrov, and Craig Boutilier. 2021. RecSim NG: Toward Principled Uncertainty Modeling for Recommender Ecosystems. (2021). arXiv:2103.08057 . [37] Aditya Pal, Chantat Eksombatchai, Yitong Zhou, Bo Zhao, Charles Rosenberg, and Jure Leskovec. 2020. PinnerSage: Multi-modal user embedding framework for recommendations at Pinterest. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . [38] Judea Pearl. 2009. Causality: Models, Reasoning and Inference (2nd ed.). Cambridge University Press. [39] Jonas Peters, Dominik Janzing, and Bernhard Schlkopf. 2017. Elements of Causal Inference: Foundations and Learning Algorithms . The MIT Press. [40] Massimo Quadrana, Alexandros Karatzoglou, BalÃ¡zs Hidasi, and Paolo Cremonesi. 2017. Personalizing Session-based Recommendations with Hierarchical Recurrent Neural Networks. In Proceedings of the Eleventh ACM Conference on Recommender Systems (RecSys-17) . 130-137. [41] Marissa Radensky, Julie Anne SÃ©guin, Jang Soo Lim, Kristen Olson, and Robert Geiger. 2023. \"I Think You Might Like This\": Exploring Effects of Confidence Signal Patterns on Trust in and Reliance on Conversational Recommender Systems. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, FAccT 2023, Chicago, IL, USA, June 12-15, 2023 . 792-804. [42] Al Mamunur Rashid, George Karypis, and John Riedl. 2008. Learning preferences of new users in recommender systems: An information theoretic approach. SIGKDD Explor. Newsl. 10 (2008), 90-100. [43] David Rohde, Stephen Bonner, Travis Dunlop, Flavian Vasile, and Alexandros Karatzoglou. 2018. RecoGym: A Reinforcement Learning Environment for the problem of Product Recommendation in Online Advertising. (2018). arXiv:1808.00720 [cs.IR] . [44] Ruslan Salakhutdinov and Andriy Mnih. 2007. Probabilistic Matrix Factorization. In Advances in Neural Information Processing Systems 20 (NIPS-07) . Vancouver, 1257-1264. [45] Ahti A Salo and Raimo P Hamalainen. 2001. Preference ratios in multiattribute evaluation (PRIME)-elicitation and decision procedures under incomplete information. IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans 31 (2001), 533-545. [46] Guy Shani and Asela Gunawardana. 2011. Evaluating Recommendation Systems. In Recommender Systems Handbook , F. Ricci, L. Rokach, B. Shapira, and P. Kantor (Eds.). Springer, Boston, 257-297. [47] AÃ¤ron van den Oord, Sander Dieleman, and Benjamin Schrauwen. 2013. Deep content-based music recommendation. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States . 2643-2651. [48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017). [49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Å ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems , I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.), Vol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aaPaper.pdf 18 Chih-Wei Hsu et al. [50] Paolo Viappiani and Craig Boutilier. 2010. Optimal Bayesian Recommendation Sets and Myopically Optimal Choice Query Sets. In Advances in Neural Information Processing Systems 23 (NIPS) . Vancouver, 2352-2360. [51] Yueqi Wang, Yoni Halpern, Shuo Chang, Jingchen Feng, Elaine Ya Le, Longfei Li, Xujian Liang, Min-Cheng Huang, Shane Li, Alex Beutel, Yaping Zhang, and Shuchao Bi. 2023. Learning from Negative User Feedback and Measuring Responsiveness for Sequential Recommenders. In Proceedings of the 17th ACM Conference on Recommender Systems (RecSys-23) . Singapore, 1049-1053. [52] Ji Yang, Xinyang Yi, Derek Zhiyuan Cheng, Lichan Hong, Yang Li, Simon Xiaoming Wang, Taibai Xu, and Ed H. Chi. 2020. Mixed Negative Sampling for Learning Two-tower Neural Networks in Recommendations. In Proceedings of the Web Conference (WWW-20) . Taipei, 441-447. [53] Siriu Yao, Yoni Halpern, Nithum Thain, Xuezhi Wang, Kang Lee, Flavien Prost, Ed H. Chi, Jilin Chen, and Alex Beutel. 2020. Measuring Recommender System Effects with Simulated Users. In 2nd Workshop on Fairness, Accountability, Transparency, Ethics and Society on the Web (FATES-20) . Taipei. [54] Fajie Yuan, Alexandros Karatzoglou, Ioannis Arapakis, Joemon M. Jose, and Xiangnan He. 2019. A Simple Convolutional Generative Network for Next Item Recommendation. In Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining (WSDM-19), WSDM 2019, Melbourne, VIC, Australia, February 11-15, 2019 . Melbourne, 582-590. [55] Xiangyu Zhao, Long Xia, Liang Zhang, Zhuoye Ding, Dawei Yin, and Jiliang Tang. 2018. Deep Reinforcement Learning for Page-wise Recommendations. In Proceedings of the 12th ACM Conference on Recommender Systems (RecSys-18) . Vancouver, 95-103. User Simulation to Evaluate Preference Elicitation Policies 19 Table 4. Impression Change (95% CIs) for 3 Policies, and Induced Policy Ordering: LEs vs. 'Launch' vs. Simulation.",
  "A ASSUMPTIONS FOR SESSION LINEARIZATION": "We should note that the data format for training user session generator in Section 4.1 makes certain simplifying assumptions, since the onboarding UI does not in fact ask about a single artist at a time, but rather displays multiple artists/queries at the same time (with a slate size that varies with device). See Fig. 1. We make a simplifying assumption about user interactions to allow training of more tractable sequence models of user behavior, specifically, adopting something akin to a cascade model [15]: Â· The user examines artists in a linear fashion when multiple artists are displayed, and scrolls to the next screen when all displayed artists are examined (assuming non-termination); the user never scrolls back to reexamine artists. Â· Once examined, the user decides whether to select or skip that artist. The user never reverts their decisions (e.g., by deselecting an artist). We also assume the user must examine all artists displayed even if they left the OP immediately. Â· New artists can be displayed 'on-the-fly' (insertion) given any previous selection. We assume the user examines these new artists immediately after the selection that generated them, even if other artists were displayed with the selected artist. The linearization assumption is less realistic with the YouTube Music desktop app (which shows more artists per screen) than the mobile app; so we confine our attention in this work to mobile users.",
  "B ADDITIONAL RESULTS": "Table 3 shows the policy ordering induced by the three small 'true LEs', the 'Launch', and the simulation. For completeness, Table 4 shows similar results for artist impressions. Notice that none of the three 'true LEs' order the policies in the same way. Indeed, none of the 'true LEs,' nor the simulation test, predict the policy ordering of the 'Launch.' The small LEs are slightly better aligned with the Launch than Simulation for ğœ† values of 0 . 001 and 0 . 05, in part, due to the variance in simulation results across all device and geo types. We note, however, the the LE and Launch are more correlated in our design than they would be in practice, since each LE is in fact a sample of 1/3 of the Launch traffic.",
  "keywords_parsed": [
    "None"
  ]
}