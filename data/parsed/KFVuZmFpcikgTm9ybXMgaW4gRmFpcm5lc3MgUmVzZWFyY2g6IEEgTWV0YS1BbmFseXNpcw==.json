{"title": "(Unfair) Norms in Fairness Research: A Meta-Analysis", "authors": "Jennifer Chien; A Stevie Bergman; Kevin R Mckee; Nenad Tomasev; Vinodkumar Prabhakaran; Rida Qadri; Nahema Marchal; William Isaac", "pub_date": "2024-06-17", "abstract": "Algorithmic fairness has emerged as a critical concern in artificial intelligence (AI) research. However, the development of fair AI systems is not an objective process. Fairness is an inherently subjective concept, shaped by the values, experiences, and identities of those involved in research and development. To better understand the norms and values embedded in current fairness research, we conduct a metaanalysis of algorithmic fairness papers from two leading conferences on AI fairness and ethics, AIES and FAccT, covering a final sample of 139 papers over the period from 2018 to 2022. Our investigation reveals two concerning trends: first, a US-centric perspective dominates throughout fairness research; and second, fairness studies exhibit a widespread reliance on binary codifications of human identity (e.g., \"Black/White\", \"male/female\"). These findings highlight how current research often overlooks the complexities of identity and lived experiences, ultimately failing to represent diverse global contexts when defining algorithmic bias and fairness. We discuss the limitations of these research design choices and offer recommendations for fostering more inclusive and representative approaches to fairness in AI systems, urging a paradigm shift that embraces nuanced, global understandings of human identity and values.", "sections": [{"heading": "Introduction", "text": "The widespread adoption of artificial intelligence (AI) brings with it the potential for substantial harm. AI systems frequently encode and amplify historical biases (Buolamwini and Gebru, 2018;D'ignazio and Klein, 2020;Eubanks, 2018;Lum and Isaac, 2016;Zou and Khern-am nuai, 2023), thus exacerbating and perpetuating discrimination against marginalized communities (Benjamin, 2020;Eubanks, 2018;Irani et al., 2010). As a consequence, algorithmic fairness has emerged as a growing priority for AI developers, ethicists, policymakers, and regulators (Kleanthous et al., 2022;Lepri et al., 2018;Pfeiffer et al., 2023;Zarsky, 2016).\nThere is no single definition of fairness (Barocas et al., 2023;Mehrabi et al., 2021). Accordingly, approaches to algorithmic fairness vary widely, encompassing individual and group perspectives, parity and equity considerations, and beyond (Binns, 2020;Chien and Danks, 2023;Dwork et al., 2012). While this pluralism has allowed researchers to generate a diverse toolkit of metrics, techniques, and frameworks for mitigating bias, it also introduces a crucial challenge: the inherent subjectivity of fairness itself.\nFairness is not a universal, abstract concept. It is deeply intertwined with the values, experiences, and identities of those involved in the research process (Bunge, 2018;Fook, 1999;Guillemin and Gillam, 2004;Iliadis and Russo, 2016;McCabe and Holmes, 2009). Thus, fairness research is sensitive to contextual questions of who is doing the work, what they are studying, and how they are studying it. A comprehensive understanding of fairness necessitates a conscious effort to acknowledge and understand the values embedded in the research process itself.", "publication_ref": ["b14", "b26", "b31", "b62", "b90", "b6", "b31", "b49", "b54", "b59", "b71", "b89", "b5", "b67", "b8", "b19", "b29", "b13", "b33", "b39", "b48", "b66"], "figure_ref": [], "table_ref": []}, {"heading": "Methods", "text": "This study aims to examine the practices of problem selection and formulation within algorithmic fairness research. What values do researchers explicitly and implicitly elevate? How does each research project approach its object(s) of study? How does each project interpret and present its results?\nIn answering these questions, we aim to reflexively surface and record common practices within fairness research-and subsequently to inform future best practice. We employed a meta-analysis methodology for our study, given its ability to synthesize and critically analyze existing research (Borenstein et al., 2021).", "publication_ref": ["b12"], "figure_ref": [], "table_ref": []}, {"heading": "Research Questions", "text": "To contribute to ongoing efforts to identify the normative choices and biases within fairness research, we investigate the following research questions:\nRQ1: What geographic and cultural biases are present in algorithmic fairness research? RQ2: What are the implicit norms embedded in the research process for algorithmic fairness, especially those affecting the diversity and representativeness of the research?", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data and Analysis", "text": "Our review investigated papers from two flagship conferences on algorithmic fairness and AI ethics: the AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society (AIES) and the ACM Conference of Fairness, Accountability, and Transparency (FAccT). Both have grown substantially since their establishment in 2018, with interdisciplinary proceedings (including computer science, law and policy, social sciences, ethics and philosophy) and attendees (including researchers, policymakers, and practitioners). Together, the conferences represent principal sites of the research discourse on algorithmic fairness (Acuna and Liang, 2021;Birhane et al., 2022b). We collected all papers included in the conferences' proceedings from 2018 (their inception) through 2022 (the most recent year available during our data collection period), accessing the papers through the conference websites and the ACM Digital Library. This initial sample comprised 265 and 416 papers from AIES and FAccT, respectively. We subsequently removed non-archival and abstract-only papers, and then filtered our sample to papers that specify at least one formal fairness definition and one model-based decision-making process. This final sample included 139 papers (52 from AIES and 87 from FAccT, respectively).\nWe focus our annotation and analysis on several crucial aspects of the sampled research, including dataset characteristics, author information, and modeling choices. To ensure consistency in our data collection, we collaboratively developed and defined our coding scheme. During this process, we independently coded two papers and compared results to clarify any points of confusion or disagreement.\nWe begin by examining the datasets employed in each paper, recording the dataset name, type (e.g., tabular, text), and topic domain. We further document the sensitive attributes studied, noting the specific categorization scheme applied to attribute labels, label definitions, and collection methods. We pay particular attention to whether papers employed proxies to study sensitive attributes, whether the papers discussed intersectionality, and the extent to which papers acknowledged and mitigated limitations related to these design choices. For simplicity, we categorize the degree of these practices on a three-point scale: (no acknowledgement, acknowledgement, and acknowledgement and mitigation).", "publication_ref": ["b1"], "figure_ref": [], "table_ref": []}, {"heading": "Collected Label Definition Paper Title", "text": "As provided Author Country Affiliation IBAN two-letter country codes for each author, comma separated Year Published [2018;2019;2020;2021;or  We pre-generated initial levels for variables such as \"Data Type\" and expanded levels for instances that did not fit the existing scheme.\nAdditionally, we note any available information on dataset provenance, allowing for identification of potential biases towards specific regions or populations and for comparisons with authors' country affiliations.\nWe next catalogue author information. We document the countries noted in author affiliations and record whether each paper incorporated participatory feedback throughout the development process. We also assess the extent to which authors acknowledged any interaction effects of human-in-the-loop processes in the decision-making process, employing the same three-point scale as for acknowledging dataset limitations.\nWe subsequently examine model design decisions. We note whether each model explicitly received sensitive attributes as inputs, record the name assigned to the model's fairness metric, and classify the metric within contrastive, counterfactual, group, or individual fairness. We also record any mitigation methods used, categorizing each as a constraint or objective.\nAfter coding the full sample, we quantify temporal and aggregate trends in these metrics. Guided by our research questions, we pay particular attention to author country affiliation, dataset country affiliation, sensitive attribute labels (number and instantiation), and label definition.\nFinally, we enrich our empirical findings through a sociotechnical analysis. Sociotechnical approaches emphasize the inherently intertwined nature of social and technical systems, allowing them to explore the ways in which social dynamics shape technological development (Cherns, 1976;Dolata et al., 2022). By applying this framework, we gain a more holistic understanding of the complex interplay between technological systems, human actors, and their broader societal context within algorithmic fairness research. We also adopt a reflexive stance, acknowledging that our own positions as researchers inevitably shape our perspective and analysis (Collins, 1992;Haraway, 1988;Harding, 1991). This self-awareness helps us to identify implicit norms and assumptions embedded within the field, leading to a more nuanced understanding of the practice of algorithmic fairness research.", "publication_ref": ["b74", "b18", "b28", "b21", "b43", "b44"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Quantitative Findings", "text": "Overall, we examine a total of 139 papers, with 52 and 87 from AIES and FAccT, respectively (Figure 1a). These papers map to a total of 124 unique datasets. Retrospective studies (research empirically examining fairness within pre-existing datasets) represented the majority of papers, followed by theory (research contributing theoretical guarantees of fairness definitions). The prevalence of prospective studies (research collecting and empirically examining novel data) remained relatively low across most years, increasing the most in 2022 (Figure 1b). Dataset domains spanned a wide range of topics, including finance, criminal justice, health, and politics. Finance emerged as the majority domain within most years (Figure 1c). Appendix: Study Design and Appendix: Fairness Definitions provide additional results concerning fairness formulations.", "publication_ref": [], "figure_ref": ["fig_1", "fig_1", "fig_1"], "table_ref": []}, {"heading": "Data and Authorship Provenance", "text": "Figure 2 visualizes geographic patterns in authorship provenance and dataset provenance through cartograms. These depict country size proportional to their representation. In terms of author affiliation, the US held the greatest count and proportion of affiliations (412, 78.0%), followed by Germany (19, 3.5%), Canada (14, 2.7%), the United Kingdom (13, 2.5%), Australia (12, 2.3%), Italy (11, 2.0%), Switzerland (10, 1.9%), and India (10, 1.9%). When broken down by year, we find that 80.6% of the papers published each year feature at least one author with ties to the United  States (see e.g. Appendix: Data and Authorship Provenance). For dataset provenance, the US again emerged as the most common country of origin (77, 72.6%), followed by Germany (9, 8.5%) and then Colombia, Rwanda, Australia, Burundi, Costa Rica, Finland, Hungary, Iceland, India, Kenya, Malawi, Mexico, Mozambique, New Zealand, Portugal, Senegal, South Africa, Sweden, Switzerland, Tanzania, Thailand, Uganda, the United Kingdom, Zambia, and Zimbabwe, all with fewer than three datasets (\u22642.8% each, 18.9% collectively). In our sample, 15.9% of papers acknowledge at least one dataset limitation. In addition, 1.4% employ some form of mitigation, such as introducing additional data (e.g., Dixon et al., 2018) or re-annotating the data with an expert (e.g., Buolamwini and Gebru, 2018).", "publication_ref": ["b27", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Sensitive Attributes", "text": "To account for the wide range of variables studied by the algorithmic fairness community, we define \"sensitive attributes\" as the features across which a study aims to measure or ensure fairness. This broad definition includes features protected by law (e.g., \"protected attributes\" such as race or gender) as well as those with potential for social or economic discrimination (e.g., income). Papers in our sample examined a total of 49 sensitive attributes, in which gender, race, and age emerged as the top three categories of sensitive attributes in our sample (Figure 3).  Our analysis revealed substantial variation in the labels assigned to each sensitive attribute (that is, variation in the category options for sensitive attributes, such as \"female\" or \"Black\"), even when papers relied on the same dataset (see Figures 4 &5). Race labels included \"Black/African American\", \"White/Caucasian\", \"Asian/South Asian/East Asian/Pacific Islander\", \"Hispanic/Latino/Mexican-American\", \"Native American/American Indian/Eskimo\", and \"unknown/other\". Age labels exhibited similar inconsistencies, employing various cutoff points such as 0-17/18+, 0-24/25+, 0-64/65+, as well as broader categorizations like \"young/old\" and systems using intervals of 5, 10, or 15 years. Gender displayed the least variation, with most papers employing \"female/male\". Although some datasets included third categories such as \"NA/other/not sure\" (Ekstrand et al., 2018;Usunier et al., 2022;Yang et al., 2022), \"non-binary or choose not to disclose \" (Schoeffer et al., 2022), or \"two spirit\" (Suresh et al., 2022), papers generally excluded these categories from analyses and empirical demonstrations.\nMost papers (89.7%) failed to provide any definition for sensitive attribute labels. This issue was particularly pronounced for age, with 20.6% of studies investigating age neglecting to define the age ranges that they used. Beyond simply defining the labels, most papers also omitted information regarding the source of their labels; researchers rarely clarified whether the labels originated from third-party observation or self-disclosure. Similarly, papers often did not provide any additional data or criteria that might have been used to determine the labels. Papers in our sample, for instance, frequently aggregated multiple categories into broader labels-such as \"young\" and \"old\" or \"African-American\" and \"non-African American\"-without providing any explanation.", "publication_ref": ["b30", "b86", "b88", "b76", "b81"], "figure_ref": ["fig_3", "fig_5", "fig_6"], "table_ref": []}, {"heading": "Sociotechnical Analysis", "text": "To further contextualize our quantitative findings, we conduct a sociotechnical analysis to explore and make explicit the unstated influences shaping algorithmic fairness research. This analysis focuses on identifying potential geographical, cultural, and implicit norms within the fairness research community and exploring how these biases might affect the diversity and representativeness of conducted studies. Given the interconnected nature of social and technical systems (Cherns, 1976;Dolata et al., 2022), these social norms and biases will ultimately influence the technological outcomes of algorithmic fairness research. Consequently, we consider whether these norms and biases affected the scope of research, the disclosure of limitations, and assumptions about global fungibility. For this analysis, we drew on our own experiences as scientists studying algorithmic fairness and as members of marginalized communities affected by deployed systems (Collins, 1992;Haraway, 1988).\nDespite the existing discourse on bias and limitations in fairness research (see Related Work), we observed two striking patterns across the empirical proceedings at AIES and FAccT: a disproportionate bias toward the US in both author affiliations and dataset origin, and a pervasive tendency towards binary formulations of sensitive attributes, particularly for gender, race, and age.", "publication_ref": ["b18", "b28", "b21", "b43"], "figure_ref": [], "table_ref": []}, {"heading": "US-Centrism and the Limits of Global Fungibility", "text": "Our meta-analysis reveals a clear and substantial bias toward the US in algorithmic fairness research. We find that 80.6% of papers published each year feature at least one US-based author, and that the US also dominates dataset provenance (72.6%; see Figure 2 & Appendix: Data and Authorship Provenance). In our sample, papers rarely acknowledged or addressed potential biases arising from the predominance of the US in author affiliations and dataset origins.  Each colored band represents, on the left, one of the three most frequently studied sensitive attributes in our sample, and on the right, the number of studies utilizing a particular formulation. The grey bars in the middle indicate the number of unique categories employed within each formulation. Studies predominantly formulate gender as a \"female/male\" binary. Across race, the top two formulations are \"Black/White\" and \"African-American/non-African-American\". Finally, age shows some increased diversity in the number of categories considered, though papers still exhibit a strong tendency towards binary formulations (e.g., 0-24 and 25+ or 0-64 and 65+). The very labels that researchers use to define sensitive attributes reveal a substantial bias toward American norms and values. For instance, papers in our sample predominantly frame race as a Black/White binary (e.g., \"White/Non-White\", \"Black/Non-Black\"). This simplification not only obscures nuances within Black identity (e.g., distinguishing between African and African-American experiences; Agyemang et al., 2005), but also homogenizes a diverse array of backgrounds under the label \"White\" (e.g., Mexican, Caribbean, Puerto Rican, Middle Eastern, and North African identities; Awad et al., 2021;Denton and Massey, 1989;Landale and Oropesa, 2002;Overmyer-Vel\u00e1zquez, 2013). This binary framing reflects a cultural phenomenon specific to the US: in particular, the historical and ongoing salience of the Black/White racial dichotomy, within American culture (Goldstein, 2006;Jones, 2015). Similarly, a number of papers and datasets exploring fairness with respect to age use 25 and 65 years as key demarcation points (Figures 4 &5). This practice echoes US legal frameworks regarding adulthood, dependence status, and retirement (Agich, 2003;Boni-Saenz, 2022;Dailey and Rosenbury, 2018;Hamilton, 2016).\nThese specific demarcation points cohere with US societal structures, but do not universally translate to other countries with different legal and social norms surrounding adulthood, retirement, and other life stages (Bhabha, 2008;Malone and Rudner, 2011;OECD, 2009OECD, , 2021)). These choices coalesce into a clear pattern of US-centrism-the tendency to view the world through the political, economic, and social lens of American society (Chaturvedi and Painter, 2007;Shabbar and Roberts, 2017).\nThe US-centrism pervading algorithmic fairness research inevitably shapes the field's outputs. Research does not exist in a vacuum-it is profoundly influenced by the socio-cultural and legal environments in which it originates (Cole and Bertenthal, 2017;Hagerty and Rubinov, 2019;Turner and Wiber, 2023). The definition of demographic categories demands particular attention here. Though often framed as objective and neutral, demographic labels carry a long history of weaponization, particularly against marginalized groups. Notions of lineage, morality, aesthetics, sexuality, and gender have long been used to define and differentiate social groups within the US, often to the detriment of those deemed \"different\" (see e.g. DeCuir-Gunby, 2014;Gilman and Peters, 1999;Gooren and Gijs, 2015;Harris, 1993;Hollinger, 2005;Kamin and Omari, 1998;Kitano et al., 1984;Said, 1993;Tasca et al., 2012). By neglecting to explicitly define sensitive attributes and acknowledge this history of US-centric categorization, algorithmic fairness research obscures the power dynamics inherent in its label choices. The lack of precise definitions effectively shields these conceptual frameworks from the critical scrutiny needed to deconstruct and remake systems of power. As a result, algorithmic fairness research risks encoding US-centric inequalities and social hierarchies into the very systems intended to promote fairness.\nThe uncritical adoption of US-centric norms extends beyond label choices to additional aspects of research design. For instance, researchers in our sample rarely justified their choice of fairness metrics, evaluation datasets, or even problem framing. These design choices, often shaped by the US socio-political context, are presented as default or universally applicable without acknowledging their inherent limitations. This assumption of global fungibility obscures the need for localized benchmarks and diverse perspectives (cf. Castro Torres and Alburez-Gutierrez, 2022). The subsequent lack of effort to contextualize research further entrenches the predominance of US-specific norms and design choices. As a result, the assumed generalizability of these research findings to other contexts goes largely unquestioned, and the perceived preeminence of US-centric values in the field remains unchallenged (Birhane et al., 2022b).", "publication_ref": ["b3", "b4", "b25", "b57", "b70", "b37", "b50", "b2", "b11", "b23", "b42", "b7", "b63", "b17", "b80", "b20", "b41", "b84", "b24", "b36", "b38", "b45", "b47", "b51", "b53", "b73", "b82"], "figure_ref": ["fig_5", "fig_6"], "table_ref": []}, {"heading": "Binary Formulations and the Erasure of Intersectional Realities", "text": "Our meta-analysis reveals a second concerning trend in algorithmic fairness research: the pervasive use of binary formulations for sensitive attributes (see Figure 4). This binary framing imposes an overly simplistic structure that obscures the complexities of lived experiences and positions groups as isolated, opposing poles on a single axis of power and privilege. For instance, fairness studies often categorize age as \"young\" versus \"old\", despite the enormous range of communities that exist within these two age groups. This dualistic logic presumes homogeneity within each class, ignoring the diverse experiences and identities within those broad categories.\nThese assumptions carry profound implications for algorithmic fairness. Binary formulations minimize the unique challenges faced by members of understudied groups. For instance, the \"male/female\" binary erases individuals who identify outside this dichotomy. Similarly, studies that focus on \"Black/White\" comparisons in the US obscure the experiences of Hispanic, Asian, and Indigenous communities. Yet research efforts that adopt a binary approach often incorrectly assume that their findings concerning harms and mitigations will translate seamlessly across groups.\nIronically, by treating each of its categories in isolation, this framework equates distinct experiences of discrimination, assuming that-for instance-bias toward racial and gender minorities pose interchangeable challenges. In essence, the binary approach to fairness encourages a view of discrimination as a one-size-fits-all problem, neglecting the nuanced ways in which different forms of oppression intersect and interact. By overlooking the interconnected nature of social categories, binary formulations fail to address the compounding effects of intersectionality in the matrix of domination: axes of discrimination are neither separable nor additive (Collins, 1992;Crenshaw, 1989). This flawed logic can burden minority groups with testing the performance of putatively fair-but actually misaligned-AI systems. Unfortunately, the emphasis on binary comparisons in fairness research creates further issues by implicitly positioning groups against each other, rather than collaborators against a system of oppression (cf. the \"racial wedge\" in political discourse; Puri, 2016; and the trans-exclusionary movement in modern feminism; Caslin, 2024;Fahs, 2024;Serano, 2016). This combination of inequitable burdens and manufactured divisions ultimately undermines the very goals of algorithmic fairness, hindering the development of truly equitable and inclusive systems.\nThe binary approach to fairness not only oversimplifies complex social identities, but also tends to treat those identities as fixed and unchanging. Papers in our sample rarely discussed how or disclosed whether identity labels could change over time. By framing sensitive attributes as static and binary categories, algorithmic fairness research overlooks the dynamic and fluid nature of identity (Keyes, 2019;Lu et al., 2022;Tomasev et al., 2021). The fluidity and complexity of identity demands that algorithmic fairness research move beyond static and binary solutions, embracing approaches that can adapt to the intersectional and evolving nature of social inequalities.", "publication_ref": ["b21", "b22", "b72", "b15", "b32", "b79", "b52", "b61", "b83"], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "Discussion", "text": "Our meta-analysis surfaces two prominent trends in contemporary algorithmic fairness research: a disproportionate US-centric bias in author backgrounds, data origins, and design choices; and a widespread policy of casting sensitive attributes into binaries.\nDespite popular depictions of research as an objective pursuit of truth, we recognize research as social praxis-a process inherently shaped by contemporary norms and historical context (Hacking, 1983;Hochstein, 2019;Kuhn, 1961). Early in the development of a field, certain norms and design choices can help scope feasible research questions, providing an important starting point for researchers and scientists. However, over time, these initial frameworks become so deeply ingrained that they limit the scope of inquiry and exclude alternative perspectives.\nUS-centrism and binary logic represent two such frameworks. Current praxis advances a myth of algorithmic fairness as a fungible, abstract, and universal phenomena. However, this myth breaks apart when confronted with the dynamism of identity and the plurality of experience around the world. To move beyond this myth, the fairness research community must chart new paths forward, guided by the principles of inclusivity, representativeness, and cultural specificity.\nThe AIES and FAccT conferences reflect two important sites of dialogue for the algorithmic fairness community field. As a result, this meta-analysis offers a valuable snapshot of current trends in fairness research. In the future, expanding the scope of inquiry to other sites of study will help enrich and contextualize our understanding of contemporary norms and practices in fairness research. Promising sites include generalist conference venues (e.g., the Conference and Workshop on Neural Information Processing Systems), pre-print repositories (e.g., arXiv), and journals (e.g., Big Data & Society). Future work should also extend beyond the examination of academic discourse. An important step for this line of research will be to investigate how developers conceptualize and implement fairness within the systems they deploy to the real world. A critical examination of the social and structural factors shaping research practices across these sites will help identify biases and encourage a more inclusive and representative vision of algorithmic fairness.\nResearchers and ethicists have proposed several solutions in this direction. Many prioritize transparency and self-reflection, advocating for explicit disclosure of the values embedded in technical work and operational definitions (e.g. van Berkel et al., 2023). Others emphasize diversifying representation across datasets (e.g. Sambasivan, 2021), within research teams (e.g. Laufer et al., 2022), and through participatory initiatives (e.g. Gadiraju et al., 2023). A final set of solutions focuses on grounding research, calling for greater engagement with real-world issues (e.g. Birhane et al., 2022b) and rigorous ethnographic research (e.g. Hagerty and Rubinov, 2019;Marda and Narayan, 2021;Martin Jr et al., 2020).\nThe current focus on the US and on binary representations of sensitive attributes threatens to perpetuate a narrow and misleading perspective on algorithmic fairness. To overcome these limitations, the research community must expand its scope to encompass diverse social and cultural settings, particularly those outside the Global North. This expansion also requires moving beyond simplistic binary classifications and engaging with the involute, intersectional realities of bias, identity, and community (Selbst et al., 2019). Qualitative methods and participatory approaches (Birhane et al., 2022a;Martin Jr et al., 2020) will be crucial for developing datasets and fairness frameworks tailored to these conditions. Overall, researchers should seek to challenge the homogenizing effect of purely technical solutions and recenter the complex lived experiences of marginalized communities.", "publication_ref": ["b40", "b46", "b56", "b87", "b74", "b58", "b34", "b41", "b64", "b65", "b77", "b65"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "Our meta-analysis of algorithmic fairness papers reveals a field grappling with several fundamental tensions. While researchers strive to develop fair and unbiased systems, current research practices often default to US-centric perspectives and binary representations of sensitive attributes. As a result, current praxis not only can compromise the credibility of research insights, but also may lead to policy solutions that are poorly aligned with the needs of diverse communities. Our findings underscore the urgent need for fairness researchers to reflect on the norms embedded in their work, echoing and expanding calls for more inclusive, representative, and context-specific approaches to algorithmic fairness.\n(Unfair) Norms in Fairness Research: A Meta-Analysis", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Positionality Statement", "text": "This paper emerges from a collective effort by researchers situated at the intersection of technology and marginalized communities. Our research backgrounds include the fields of algorithmic fairness, human-computer interaction, and critical algorithmic studies, with prior experience studying communities outside the Global North as well as issues afflicting LGBTQ+ populations. These experiences motivate our critical perspective on the existing AI ethics landscape and its inherent biases towards the US and the Global North. The lead author, for example, is a US-based doctoral student holding multiple intersecting identities-some marginalized, and others affording greater privilege. Navigating these complexities shapes their perspective on power dynamics within AI ethics discourse. Thus, we draw on our own identities and experiences to understand and motivate our research. While this personal perspective provides valuable insights, we acknowledge that it also introduces potential biases and welcome constructive engagement and alternative perspectives to challenge and enrich out understanding.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Expanded Results", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data and Authorship Provenance", "text": "Figure 6 shows the geographic distribution of authorship affiliations over time. ", "publication_ref": [], "figure_ref": ["fig_7"], "table_ref": []}, {"heading": "Study Design", "text": "Fairness research papers can be categorized by the methods they apply to their proposed fairness definitions: empirical experiments that collect new data with a deployed intervention (prospective); empirical simulations with existing datasets (retrospective); or proofs using mathematical guarantees (theory). Examining our sample through the lens of these categories reveals a clear trend: papers most frequently rely on retrospective analyses to study algorithmic fairness (Figure 7a). This approach predominates across all years in our sample. In terms of dataset type, we observe a notable shift towards using synthetic data after 2018. The most common non-synthetic datasets (e.g., COMPAS, UCI Adult, and German Credit) disclose a focus on applications within financial and criminal justice contexts, though the exact pattern varies from year to year (Figure 7b).\nWe next analyze aspects of the algorithmic interventions proposed. Our results indicate a clear preference for in-processing interventions, which directly modify the algorithm during the learning process (Figure 8a). In-processing approaches tend to overshadow pre-processing interventions, which focus on data adjustments before training, and post-processing interventions, which address fairness concerns by modifying algorithmic outputs. Across the years we examine, we observe a growing ambiguity regarding the inherent trade-off between fairness and performance (Figure 8b).  ", "publication_ref": [], "figure_ref": ["fig_8", "fig_8", "fig_9", "fig_9"], "table_ref": []}, {"heading": "Fairness Definitions", "text": "Figures 9 and 10 illustrate the prevalence of different fairness metrics in our sample over time, presenting the top ten most frequently used metrics and the full list, respectively. A single study might explore multiple metrics. For instance, an individual paper might examine both demographic parity and equalized false positive rates across demographic groups.\nCount  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sampled Papers", "text": "Tables 2 and3 provide titles for the full sample of papers selected and reviewed for our meta-analysis.   ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "An empirical analysis of racial categories in the algorithmic fairness literature", "journal": "", "year": "2023", "authors": "A A Abdu; I V Pasquetto; A Z Jacobs"}, {"ref_id": "b1", "title": "Are ai ethics conferences different and more diverse compared to traditional computer science conferences", "journal": "", "year": "2021", "authors": "D E Acuna; L Liang"}, {"ref_id": "b2", "title": "Dependence and autonomy in old age: An ethical framework for long-term care", "journal": "Cambridge University Press", "year": "2003", "authors": "G Agich"}, {"ref_id": "b3", "title": "Negro, black, black african, african caribbean, african american or what? labelling african origin populations in the health arena in the 21st century", "journal": "Journal of Epidemiology & Community Health", "year": "2005", "authors": "C Agyemang; R Bhopal; M Bruijnzeels"}, {"ref_id": "b4", "title": "Identity and ethnic/racial self-labeling among americans of arab or middle eastern and north african descent", "journal": "Identity", "year": "2021", "authors": "G H Awad; H Hashem; H Nguyen"}, {"ref_id": "b5", "title": "Fairness and machine learning: Limitations and opportunities", "journal": "MIT Press", "year": "2023", "authors": "S Barocas; M Hardt; A Narayanan"}, {"ref_id": "b6", "title": "Race after technology: Abolitionist tools for the new jim code", "journal": "", "year": "2020", "authors": "R Benjamin"}, {"ref_id": "b7", "title": "Independent children, inconsistent adults: International child migration and the legal framework", "journal": "", "year": "2008", "authors": "J Bhabha"}, {"ref_id": "b8", "title": "On the apparent conflict between individual and group fairness", "journal": "", "year": "2020", "authors": "R Binns"}, {"ref_id": "b9", "title": "Power to the people? opportunities and challenges for participatory ai", "journal": "", "year": "2022", "authors": "A Birhane; W Isaac; V Prabhakaran; M Diaz; M C Elish; I Gabriel; S Mohamed"}, {"ref_id": "b10", "title": "The forgotten margins of ai ethics", "journal": "", "year": "2022", "authors": "A Birhane; E Ruane; T Laurent; M S Brown; J Flowers; A Ventresque; C L Dancy"}, {"ref_id": "b11", "title": "Legal age", "journal": "BCL Rev", "year": "2022", "authors": "A A Boni-Saenz"}, {"ref_id": "b12", "title": "Introduction to meta-analysis", "journal": "John Wiley & Sons", "year": "2021", "authors": "M Borenstein; L V Hedges; J P Higgins; H R Rothstein"}, {"ref_id": "b13", "title": "Critical approaches to science and philosophy", "journal": "Routledge", "year": "2018", "authors": "M Bunge"}, {"ref_id": "b14", "title": "Gender shades: Intersectional accuracy disparities in commercial gender classification", "journal": "PMLR", "year": "2018", "authors": "J Buolamwini; T Gebru"}, {"ref_id": "b15", "title": "Trans feminism and the women's liberation movement in britain, c. 1970-1980", "journal": "", "year": "2024", "authors": "S Caslin"}, {"ref_id": "b16", "title": "North and south: Naming practices and the hidden dimension of global disparities in knowledge production", "journal": "Proceedings of the National Academy of Sciences", "year": "2022", "authors": "A F Castro Torres; D Alburez-Gutierrez"}, {"ref_id": "b17", "title": "Whose world, whose order? spatiality, geopolitics and the limits of the world order concept", "journal": "Cooperation and Conflict", "year": "2007", "authors": "S Chaturvedi; J Painter"}, {"ref_id": "b18", "title": "The principles of sociotechnical design", "journal": "Human relations", "year": "1976", "authors": "A Cherns"}, {"ref_id": "b19", "title": "Fairness vs. personalization: Towards equity in epistemic utility", "journal": "", "year": "2023", "authors": "J Chien; D Danks"}, {"ref_id": "b20", "title": "Science, technology, society, and law", "journal": "Annual Review of Law and Social Science", "year": "2017", "authors": "S A Cole; A Bertenthal"}, {"ref_id": "b21", "title": "Black feminist thought: Knowledge, consciousness, and the politics of empowerment", "journal": "routledge", "year": "1992", "authors": "P H Collins"}, {"ref_id": "b22", "title": "Demarginalizing the intersection of race and sex: A black feminist critique of antidiscrimination doctrine, feminist theory and antiracist politics", "journal": "University of Chicago Legal Forum", "year": "1989", "authors": "K Crenshaw"}, {"ref_id": "b23", "title": "The new law of the child", "journal": "The Yale Law Journal", "year": "2018", "authors": "A C Dailey; L A Rosenbury"}, {"ref_id": "b24", "title": "proving your skin is white, you can have everything\": Race, racial identity, and property rights in whiteness in the supreme court case of josephine decuir", "journal": "Routledge", "year": "2014", "authors": "J T Decuir-Gunby"}, {"ref_id": "b25", "title": "Racial identity among caribbean hispanics: The effect of double minority status on residential segregation", "journal": "American Sociological Review", "year": "1989", "authors": "N A Denton; D S Massey"}, {"ref_id": "b26", "title": "Data feminism", "journal": "MIT press", "year": "2020", "authors": "C ; L F Klein"}, {"ref_id": "b27", "title": "Measuring and mitigating unintended bias in text classification", "journal": "", "year": "2018", "authors": "L Dixon; J Li; J Sorensen; N Thain; L Vasserman"}, {"ref_id": "b28", "title": "A sociotechnical view of algorithmic fairness", "journal": "Information Systems Journal", "year": "2022", "authors": "M Dolata; S Feuerriegel; G Schwabe"}, {"ref_id": "b29", "title": "Fairness through awareness", "journal": "", "year": "2012", "authors": "C Dwork; M Hardt; T Pitassi; O Reingold; R Zemel"}, {"ref_id": "b30", "title": "All the cool kids, how do they fit in?: Popularity and demographic biases in recommender evaluation and effectiveness", "journal": "PMLR", "year": "2018", "authors": "M D Ekstrand; M Tian; I M Azpiazu; J D Ekstrand; O Anuyah; D Mcneill; M S Pera"}, {"ref_id": "b31", "title": "Automating inequality: How high-tech tools profile, police, and punish the poor", "journal": "St. Martin's Press", "year": "2018", "authors": "V Eubanks"}, {"ref_id": "b32", "title": "The urgent need for radical feminism today", "journal": "Signs: Journal of Women in Culture and Society", "year": "2024", "authors": "B Fahs"}, {"ref_id": "b33", "title": "Reflexivity as method", "journal": "Annual Review of Health Social Science", "year": "1999", "authors": "J Fook"}, {"ref_id": "b34", "title": "Disability-centered perspectives on large language models", "journal": "", "year": "2023", "authors": "V Gadiraju; S Kane; S Dev; A Taylor; D Wang; E Denton; R Brewer"}, {"ref_id": "b35", "title": "Fast flow-based algorithm for creating density-equalizing map projections", "journal": "Proceedings of the National Academy of Sciences", "year": "2018", "authors": "M T Gastner; V Seguy; P More"}, {"ref_id": "b36", "title": "Making the body beautiful: a cultural history of aesthetic surgery", "journal": "Canadian Medical Association. Journal", "year": "1999", "authors": "S L Gilman; W Peters"}, {"ref_id": "b37", "title": "The price of whiteness: Jews, race, and American identity", "journal": "Princeton University Press", "year": "2006", "authors": "E L Goldstein"}, {"ref_id": "b38", "title": "Medicalization of homosexuality", "journal": "", "year": "2015", "authors": "L Gooren; L Gijs"}, {"ref_id": "b39", "title": "Ethics, reflexivity, and \"ethically important moments\" in research", "journal": "Qualitative inquiry", "year": "2004", "authors": "M Guillemin; L Gillam"}, {"ref_id": "b40", "title": "Representing and intervening: Introductory topics in the philosophy of natural science", "journal": "Cambridge university press", "year": "1983", "authors": "I Hacking"}, {"ref_id": "b41", "title": "Global ai ethics: a review of the social impacts and ethical implications of artificial intelligence", "journal": "", "year": "2019", "authors": "A Hagerty; I Rubinov"}, {"ref_id": "b42", "title": "Adulthood in law and culture", "journal": "Tul. L. Rev", "year": "2016", "authors": "V E Hamilton"}, {"ref_id": "b43", "title": "Situated knowledges: The science question in feminism and the privilege of partial perspective", "journal": "Feminist Studies", "year": "1988", "authors": "D Haraway"}, {"ref_id": "b44", "title": "Whose science? Whose knowledge?: Thinking from women's lives", "journal": "Cornell University Press", "year": "1991", "authors": "S Harding"}, {"ref_id": "b45", "title": "Whiteness as property", "journal": "Harvard law review", "year": "1993", "authors": "C I Harris"}, {"ref_id": "b46", "title": "How metaphysical commitments shape the study of psychological mechanisms", "journal": "Theory & Psychology", "year": "2019", "authors": "E Hochstein"}, {"ref_id": "b47", "title": "The one drop rule & the one hate rule", "journal": "Daedalus", "year": "2005", "authors": "D A Hollinger"}, {"ref_id": "b48", "title": "Critical data studies: An introduction", "journal": "Big Data & Society", "year": "2016", "authors": "A Iliadis; F Russo"}, {"ref_id": "b49", "title": "Postcolonial computing: a lens on design and development", "journal": "", "year": "2010", "authors": "L Irani; J Vertesi; P Dourish; K Philip; R E Grinter"}, {"ref_id": "b50", "title": "The black-white dichotomy of race: Influence of a predominantly white environment on multiracial identity", "journal": "Higher Education in Review", "year": "2015", "authors": "V Jones"}, {"ref_id": "b51", "title": "Race, head size, and intelligence", "journal": "South African Journal of Psychology", "year": "1998", "authors": "L J Kamin; S Omari"}, {"ref_id": "b52", "title": "Counting the countless: Why data science is a profound threat for queer people", "journal": "", "year": "2019", "authors": "O Keyes"}, {"ref_id": "b53", "title": "Asian-american interracial marriage", "journal": "Journal of Marriage and the Family", "year": "1984", "authors": "H H Kitano; W.-T Yeung; L Chai; H Hatanaka"}, {"ref_id": "b54", "title": "Perception of fairness in algorithmic decisions: future developers' perspective", "journal": "Patterns", "year": "2022", "authors": "S Kleanthous; M Kasinidou; P Barlas; J Otterbacher"}, {"ref_id": "b55", "title": "Reduced, reused and recycled: The life of a dataset in machine learning research", "journal": "", "year": "2021", "authors": "B Koch; E Denton; A Hanna; J G Foster"}, {"ref_id": "b56", "title": "The function of measurement in modern physical science", "journal": "Isis", "year": "1961", "authors": "T S Kuhn"}, {"ref_id": "b57", "title": "White, black, or puerto rican? racial self-identification among mainland and island puerto ricans", "journal": "Social Forces", "year": "2002", "authors": "N S Landale; R S Oropesa"}, {"ref_id": "b58", "title": "Four years of facct: A reflexive, mixedmethods analysis of research contributions, shortcomings, and future prospects", "journal": "", "year": "2022", "authors": "B Laufer; S Jain; A F Cooper; J Kleinberg; H Heidari"}, {"ref_id": "b59", "title": "Fair, transparent, and accountable algorithmic decision-making processes: The premise, the proposed solutions, and the open challenges", "journal": "Philosophy & Technology", "year": "2018", "authors": "B Lepri; N Oliver; E Letouz\u00e9; A Pentland; P Vinck"}, {"ref_id": "b60", "title": "Sex and gender: An introduction", "journal": "Waveland Press", "year": "2020", "authors": "H M Lips"}, {"ref_id": "b61", "title": "Subverting machines, fluctuating identities: Re-learning human categorization", "journal": "Association for Computing Machinery", "year": "2022", "authors": "C Lu; J Kay; K Mckee"}, {"ref_id": "b62", "title": "To predict and serve? Signif", "journal": "Oxf.)", "year": "2016-10", "authors": "K Lum; W Isaac"}, {"ref_id": "b63", "title": "Global perspectives on children's independent mobility: a socio-cultural comparison and theoretical discussion of children's lives in four countries in asia and africa", "journal": "Global Studies of Childhood", "year": "2011", "authors": "K Malone; J Rudner"}, {"ref_id": "b64", "title": "On the importance of ethnographic methods in ai research", "journal": "Nature Machine Intelligence", "year": "2021", "authors": "V Marda; S Narayan"}, {"ref_id": "b65", "title": "Extending the machine learning abstraction boundary: A complex systems approach to incorporate societal context", "journal": "", "year": "2020", "authors": "D Martin; V Prabhakaran; J Kuhlberg; A Smart; W S Isaac"}, {"ref_id": "b66", "title": "Reflexivity, critical qualitative research and emancipation: A foucauldian perspective", "journal": "Journal of advanced nursing", "year": "2009", "authors": "J L Mccabe; D Holmes"}, {"ref_id": "b67", "title": "A survey on bias and fairness in machine learning", "journal": "ACM Computing Surveys (CSUR)", "year": "2021", "authors": "N Mehrabi; F Morstatter; N Saxena; K Lerman; A Galstyan"}, {"ref_id": "b68", "title": "Social policy division: Directorate of employment, labour and social affairs", "journal": "OECD family database", "year": "2009", "authors": ""}, {"ref_id": "b69", "title": "Pensions at a glance 2021: OECD and G20 indicators", "journal": "Organisation for Economic Co-operation and Development OECD", "year": "2021", "authors": ""}, {"ref_id": "b70", "title": "Good neighbors and white mexicans: Constructing race and nation on the mexico-us border", "journal": "Journal of American Ethnic History", "year": "2013", "authors": "M Overmyer-Vel\u00e1zquez"}, {"ref_id": "b71", "title": "Algorithmic fairness in ai: An interdisciplinary view", "journal": "Business & Information Systems Engineering", "year": "2023", "authors": "J Pfeiffer; J Gutschow; C Haas; F M\u00f6slein; O Maspfuhl; F Borgers; S Alpsancar"}, {"ref_id": "b72", "title": "Sexual states", "journal": "Duke University Press", "year": "2016", "authors": "J Puri"}, {"ref_id": "b73", "title": "Culture and imperialism (london: Chatto and windus). Culture and Imperialism", "journal": "", "year": "1993", "authors": "E Said"}, {"ref_id": "b74", "title": "Seeing like a dataset from the global south", "journal": "Interactions", "year": "2021", "authors": "N Sambasivan"}, {"ref_id": "b75", "title": "Re-imagining algorithmic fairness in india and beyond", "journal": "", "year": "2021", "authors": "N Sambasivan; E Arnesen; B Hutchinson; T Doshi; V Prabhakaran"}, {"ref_id": "b76", "title": "On the effects of explanations on perceptions of informational fairness and trustworthiness in automated decisionmaking", "journal": "", "year": "2022", "authors": "J Schoeffer; N Kuehl; Y Machowski"}, {"ref_id": "b77", "title": "Fairness and abstraction in sociotechnical systems", "journal": "", "year": "2019", "authors": "A D Selbst; D Boyd; S A Friedler; S Venkatasubramanian; J Vertesi"}, {"ref_id": "b78", "title": "Weird faccts: How western, educated, industrialized, rich, and democratic is facct?", "journal": "", "year": "2023", "authors": "A A Septiandri; M Constantinides; M Tahaei; D Quercia"}, {"ref_id": "b79", "title": "Whipping girl: A transsexual woman on sexism and the scapegoating of femininity", "journal": "Hachette UK", "year": "2016", "authors": "J Serano"}, {"ref_id": "b80", "title": "Authoritative points of references as the grounding for innovation and progress", "journal": "International Institute for Islamic Thought", "year": "2017", "authors": "S Shabbar; N Roberts"}, {"ref_id": "b81", "title": "Towards intersectional feminist and participatory ml: A case study in supporting feminicide counterdata collection", "journal": "", "year": "2022", "authors": "H Suresh; R Movva; A L Dogan; R Bhargava; I Cruxen; \u00c1 M Cuba; G Taurino; W So; C D Ignazio"}, {"ref_id": "b82", "title": "Women and hysteria in the history of mental health. Clinical practice and epidemiology in mental health", "journal": "CP & EMH", "year": "2012", "authors": "C Tasca; M Rapetti; M G Carta; B Fadda"}, {"ref_id": "b83", "title": "Fairness for unobserved characteristics: Insights from technological impacts on queer communities", "journal": "", "year": "2021", "authors": "N Tomasev; K R Mckee; J Kay; S Mohamed"}, {"ref_id": "b84", "title": "Legal pluralism and science and technology studies: Exploring sources of the legal pluriverse", "journal": "Science, Technology, & Human Values", "year": "2023", "authors": "B Turner; M G Wiber"}, {"ref_id": "b85", "title": "Toward a redefinition of sex and gender", "journal": "American psychologist", "year": "1979", "authors": "R K Unger"}, {"ref_id": "b86", "title": "Fast online ranking with fairness of exposure", "journal": "", "year": "2022", "authors": "N Usunier; V Do; E Dohmatob"}, {"ref_id": "b87", "title": "The methodology of studying fairness perceptions in artificial intelligence: Contrasting chi and facct", "journal": "International Journal of Human-Computer Studies", "year": "2023", "authors": "N Van Berkel; Z Sarsenbayeva; J Goncalves"}, {"ref_id": "b88", "title": "Enhancing fairness in face detection in computer vision systems by demographic bias mitigation", "journal": "", "year": "2022", "authors": "Y Yang; A Gupta; J Feng; P Singhal; V Yadav; Y Wu; P Natarajan; V Hedau; J Joo"}, {"ref_id": "b89", "title": "The trouble with algorithmic decisions: An analytic road map to examine efficiency and fairness in automated and opaque decision making", "journal": "Science, Technology, & Human Values", "year": "2016", "authors": "T Zarsky"}, {"ref_id": "b90", "title": "Ai and housing discrimination: the case of mortgage applications", "journal": "AI and Ethics", "year": "2023", "authors": "L Zou; W Khern-Am Nuai"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 |1Figure 1 | Conference, Study Type, and Dataset Topic Domains Over Time. (a) The number of papers published by each conference trends upward over time. (b) Over all years, a majority of papers are retrospective (conducting empirical analyses on pre-existing datasets). (c) Papers examine an increasing variety of topic domains over time. Overall, finance generally prevails as the most popular dataset domain application, followed by criminal justice.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure2| Distribution of Author Country-Affiliation and Dataset Country-Affiliation. These cartograms represent country sizes proportional to the count of (a) author affiliations and (b) datasets attributed to each country. The US emerges as the most highly represented country for both authorship origin and data provenance. Graphics created withGastner et al. (2018).", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 |3Figure 3 | Sensitive Attributes Studied Over Time. Count reflects the number of times papers in our sample analyzed each sensitive attribute category in a given year.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "2  ", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 4 |4Figure4| Distribution of Label Formulations for Sensitive Attributes Across Fairness Studies. This alluvial diagram illustrates the range of formulations that fairness studies use to label sensitive attributes. Each colored band represents, on the left, one of the three most frequently studied sensitive attributes in our sample, and on the right, the number of studies utilizing a particular formulation. The grey bars in the middle indicate the number of unique categories employed within each formulation. Studies predominantly formulate gender as a \"female/male\" binary. Across race, the top two formulations are \"Black/White\" and \"African-American/non-African-American\". Finally, age shows some increased diversity in the number of categories considered, though papers still exhibit a strong tendency towards binary formulations (e.g., 0-24 and 25+ or 0-64 and 65+).", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 5 |5Figure5| Binary Formulations for Sensitive Attributes. This alluvial diagram highlights the binary label formulations used for the three most studied sensitive attributes (gender, race, and age) within fairness studies. Each colored band represents, on the left, one of the three most frequently studied sensitive attributes in our sample, and on the right, the number of studies utilizing a particular formulation. The grey bars in the middle indicate the number of unique categories employed within each formulation.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 6 |6Figure 6 | Geographic Representation in Authorship Affiliations Over Time. The US consistently contributes the largest share of affiliations over all years included in our sample.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 7 |7Figure 7 | Prevalence of Study Types and Datasets Over Time. (a) The majority of fairness papers engage in retrospective analysis of existing datasets and algorithms. (b) Studies most frequently leverage synthetic datasets. The most studied individual dataset is COMPAS, with other non-synthetic datasets like the German Credit and UCI Adult datasets also seeing considerable use.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 8 |8Figure 8 | Trends in Intervention Points and Acknowledgement of Performance Trade-offs Over Time. (a) In-processing consistently emerges as the most common point for implementing fairness interventions, with the exception of 2020. (b) Notably, we observe an upward trend in the number of papers that do not address whether improving fairness metrics comes at the cost of performance.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 9 |9Figure9| Popularity of Top Fairness Metrics Over Time. While a wide range of metrics exist for assessing fairness, a small subset sees disproportionate use within the literature. This barplot depicts the ten most common fairness metrics from our sample. Demographic parity demonstrates consistent popularity over the years.", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure 10 |10Figure 10 | Prevalence of All Fairness Metrics Over Time. This barplot depicts the use of fairness metrics across papers in our sample.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Table of Collected Labels & Definitions.", "figure_data": "2022]"}], "formulas": [], "doi": "10.1145/3531146.3533161"}
