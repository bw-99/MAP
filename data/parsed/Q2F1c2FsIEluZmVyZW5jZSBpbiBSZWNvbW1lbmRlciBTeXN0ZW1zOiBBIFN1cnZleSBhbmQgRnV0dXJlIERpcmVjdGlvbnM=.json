{"Causal Inference in Recommender Systems: A Survey and Future Directions": "CHENGAO, Beijing National Research Center for Information Science and Technology, Tsinghua University, China YU ZHENG, Department of Electronic Engineering, Tsinghua University, China WENJIE WANG, School of Computing, National University of Singapore, Singapore FULI FENG, School of Information Science and Technology, University of Science and Technology of China, China XIANGNAN HE, School of Information Science and Technology, University of Science and Technology of China, China YONG LI, Department of Electronic Engineering, Tsinghua University, China Recommender systems have become crucial in information filtering nowadays. Existing recommender systems extract user preferences based on the correlation in data, such as behavioral correlation in collaborative filtering, feature-feature, or feature-behavior correlation in click-through rate prediction. However, unfortunately, the real world is driven by causality , not just correlation, and correlation does not imply causation. For instance, recommender systems might recommend a battery charger to a user after buying a phone, where the latter can serve as the cause of the former; such a causal relation cannot be reversed. Recently, to address this, researchers in recommender systems have begun utilizing causal inference to extract causality, thereby enhancing the recommender system. In this survey, we offer a comprehensive review of the literature on causal inferencebased recommendation. Initially, we introduce the fundamental concepts of both recommender system and causal inference as the foundation for subsequent content. We then highlight the typical issues faced by non-causality recommender system. Following that, we thoroughly review the existing work on causal inference-based recommender systems, based on a taxonomy of three-aspect challenges that causal inference can address. Finally, we discuss the open problems in this critical research area and suggest important potential future works. Additional Key Words and Phrases: Recommender Systems; Causal Inference; Information Retrieval", "ACMReference Format:": "Chen Gao, Yu Zheng, Wenjie Wang, Fuli Feng, Xiangnan He, and Yong Li. 2023. Causal Inference in Recommender Systems: A Survey and Future Directions. ACM Trans. Inf. Syst. 1, 1 (December 2023), 32 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn Authors' addresses: Chen Gao, Beijing National Research Center for Information Science and Technology, Tsinghua University, China; Yu Zheng, Department of Electronic Engineering, Tsinghua University, China; Wenjie Wang, School of Computing, National University of Singapore, Singapore, chgao96@gmail.com,y-zheng19@mails.tsinghua.edu.cn,wenjiewang96@ gmail.com,fulifeng93@gmail.com,xiangnanhe@gmail.com,liyong07@tsinghua.edu.cn; Fuli Feng, School of Information Science and Technology, University of Science and Technology of China, China; Xiangnan He, School of Information Science and Technology, University of Science and Technology of China, China; Yong Li, Department of Electronic Engineering, Tsinghua University, China. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \u00a9 2023 Association for Computing Machinery. 1046-8188/2023/12-ART $15.00", "1 INTRODUCTION": "In the era of information overload, recommender systems (RecSys) have emerged as the fundamental service for facilitating users' information access. From the early shallow models [47, 75] to recent advances of deep learning-based ones [15, 31] and the most recent graph neural network-based models [29, 131], the techniques and models of recommender systems are developing rapidly. In general, recommender systems aim to learn user preferences by fitting historical behaviors, along with collected user profiles, item attributes, or other contextual information. Here, the interaction is mainly induced by the previous recommender system and is largely affected by the recommendation policy. Then, recommender systems filter from the item-candidate pools and select items that match users' personalized preferences and demands. Once deployed, the system collects new interactions to update the model, where the whole framework thus forms a feedback loop. Generally, recommender systems can be divided into two categories: collaborative filtering (CF) and content-based recommendation ( a.k.a. , click-through rate (CTR) prediction, shortened as CTR prediction). Collaborative filtering focuses on users' historical behaviors, such as clicking, purchasing, etc. The basic assumption of collaborative filtering is that users with similar historical behaviors tend to have similar future behaviors. For example, the most representative matrix factorization model (MF) uses vectors to represent users and items, and then it uses the inner product to calculate the relevance scores between users and items. To improve the model capacity, recent work [15, 31] takes advantage of deep neural networks for matching users with items, such as neural collaborative filtering [31], which leverages multi-layer perceptrons to replace the inner product in the MF model. Furthermore, a broad view of collaborative filtering models the relevance with consideration of additional information, such as the timestamp of each behavior in sequential recommendation [12, 132], user social network in social recommendation [17, 114], and multi-type behaviors in multi-behavior recommendation [21, 117], etc. CTR prediction focuses on leveraging the rich attributes and features of users, items, or context to enhance recommendation. The mainstream CTR prediction task aims to learn high-order features with the proper featureinteraction module, such as the linear inner product in Factorization Machine (FM), multi-layer perceptrons in DeepFM [24], attention networks in AFM [119], stacked self-attention layers in AutoInt [91], etc. The basis of today's recommender systems is to model the correlation , such as behavioral correlation in collaborative filtering, feature-feature, or feature-behavior correlation in click-through rate prediction. However, the real world is driven by causality rather than correlation, while correlation does not imply causation. Two kinds of causality widely exist in recommender systems, user-aspect, and interaction-aspect. The user-aspect causality refers to the users' decision process being driven by causality. For example, a user may buy a battery charger after buying a phone, in which the latter can serve as the cause of the former, and such a causal relation cannot be reversed. The interaction-aspect causality refers to that the recommendation strategy largely affects users' interactions with the system. For example, the unobserved user-item interaction does not mean that the user does not like the item, which may only be caused by non-exposure. Formally speaking, causality can be defined as cause and effect in which the cause is partly responsible for the effect [128]. Causal inference is defined as the process of determining and further leveraging the causal relation based on experimental data or observational data [128]. Two popular and widely-used causal-inference frameworks are the potential outcome framework (Rubin Causal Model) [76], and the structural causal model (SCM) [69, 71]. Rubin's Framework aims to calculate the effect of certain treatments. The structural causal model establishes a causal graph and corresponding structural equations, comprising a set of variables and structural equations that depict the causal relationships between these variables. A Survey and Future Directions (a) Real preferences user item (b) Data bias item user (c) Data missing item user (d) Data noise item user Since following a correlation-driven paradigm, existing recommender systems still suffer from critical bottlenecks. Specifically, three main challenges limit the effectiveness of the current paradigm, for which causal inference can serve as a promising solution, as follows. \u00b7 Theissues of data bias . The collected data, such as the most important user-item interaction data, is observational (not experimental), resulting in biases including conformity bias, popularity bias, etc. [55] As for the non-causality recommender systems, not only the desired user preferences but also the data bias are learned by the model, which leads to inferior recommendation performance. \u00b7 The issues of data missing or even data noise. The collected data in recommender systems is limited by the collection procedure, which makes there is missing or noisy data. For example, despite the large-scale item pool, the users only interact with a tiny fraction of items, which means plenty of unobserved user-item feedback cannot be collected. Moreover, sometimes the observed implicit feedback is even noisy, not reflecting the actual satisfaction of users, such as those click behaviors that end with negative reviews on E-Commerce websites or some behaviors by mistake. \u00b7 Thebeyond-accuracy objectives are hard to achieve. Besides accuracy, recommender systems should also consider other objectives, such as fairness, explainability, transparency, etc. Improving these beyond-accuracy objectives may hurt the recommendation accuracy, resulting in a dilemma. For example, a model that considers the multiple driven causes under user behavior , based on assigning each cause with disentangled and interpretable embedding, can well provide both accurate and explainable recommendation. Another important objective is diversity but a highdiversity item recommendation list may not be able to well fit user interest. Here causal inference can help capture why users consume specific category of items, achieving both high accuracy and diversity. Recent research on recommender systems tackles these challenges with carefully-designed causality-driven methods. Over the last two years, there has been a surge of relevant papers, and there is a very high probability that causal inference will become predominant in the field of recommender systems. In this survey paper, we systematically review these pioneering research efforts, especially focusing on how they address the critical shortcomings with causal inference. First, recommendation methods incorporating causality can construct a causal graph. Within this framework, bias is typically viewed as a confounder, which can then be addressed using causal-inference techniques. Second, regarding the issue of missing data, causality-enhanced models can assist in constructing a counterfactual world. Thus, the missing data can be inferred through counterfactual reasoning. Third, causal inference naturally facilitates the development of interpretable and controllable models. As a result, the explainability of both the model itself and the recommendation outcomes can be enhanced. Moreover, other objectives, such as diversity and fairness, can also be realized since the model becomes more controllable. Specifically, the current works of causal inference in recommendation can be categorized as follows. \u00b7 Data debiasing with causal inference. For issues like popularity bias or exposure bias, the bias (arising from popularity-aware or exposure strategy-aware data collection) can often be seen as a form of confounder. Some existing work addresses this through backdoor adjustment. Conformity bias, on the other hand, can be conceptualized as a collider effect. \u00b7 Data augmentation and data denoising with causal inference. The dual challenge of data missing encompasses both limited user-data collection and the recommendation model's causal effect on the system. The extreme form of the first challenge can even lead to data noise. For the first challenge, counterfactual reasoning can be employed to generate the uncollected data as augmentation, thus addressing the data-missing problem. For the latter, causal models like IPW can be utilized to estimate the causal impact of recommendation models. \u00b7 Achieving explainability, diversity, and fairness via interpretable and controllable recommendation models using causal inference. Models crafted in alignment with the causal graph are intrinsically controllable. Some notable techniques in this regard encompass causal discovery and disentangled representations. Leveraging the interpretable model, high diversity can be realized by manipulating the model to sidestep the tradeoff, and fair recommendations can be secured by steering the model to ensure fairness across specific user demographics. It is worth mentioning that although there are surveys on either recommender systems [25, 113, 134] or causal inference [26, 63, 63, 129], there is no existing survey fully discussing this new and important area of causality-driven recommender systems. Note that there is a very short paper (8 page) [116] trying to survey existing work of causal-inspired recommendation methods, but it only discusses a few of representative papers due to its page limit. These surveys on recommender systems mainly introduce and discuss the basic concepts and various advances of recommender systems, with only a few discussions on causality-based recommendation. On the other hand, surveys of causal inference primarily introduce and discuss the basic concepts and fundamental methods of causal inference, lacking sufficient discussions on applications. There is a survey [10] about bias and debias in recommender system and we would discuss its relations with our survey as follows. First, the survey [10] concentrates on the bias issue in recommendations and describes how various works address these issues. Among these, causal inference-based methods represent just one segment, with numerous other methods available for tackling bias. Similarly, our survey underscores that while using causal inference to address data bias is a significant component, it is merely a portion of our broader theme: causal inference for recommender systems. Hence, even though some overlap exists between the two surveys, it is small due to the distinct focal topics. Second, when considering the shared part, the two surveys adopt different manners to discuss existing works. Our survey places greater emphasis on the causal inference technique itself, its ties to conventional causal inference methods, and its relevance to other challenges, such as data missing and data noise. In contrast, the bias survey [10] delves deeper into the intricacies of biases (types, origins, etc.) and elaborates on how causal inference-based methods differentiate themselves from other kinds of methods. We summarize the contribution of this survey as follows. \u00b7 To the best of our knowledge, we take the pioneering step to give a systematic survey of this new yet promising area. We categorize the existing work by answering the fundamental question of why the causal inference is needed and how causal inference enhances recommendation . A Survey and Future Directions X Z Y (a) Chain X Z Y (b) Fork X Z Y (c) Collider User features Preference Click Quality Price Preference Click Preference Popularity \u00b7 We first provide the necessary knowledge of recommender systems and causal inference. Subsequently, we introduce and explain the existing work of causal inference for recommendation, from the early attempts to the recently-published papers until 2023. \u00b7 We discuss important yet unresolved problems in this research area and propose promising directions, which we believe will be the mainstream research direction of the next few years.", "2 BACKGROUND": "As a survey of the interdisciplinary area of causal inference and recommender systems, we first introduce the background knowledge and fundamental concepts of these two topics.", "2.1 Causal Inference": "We introduce the fundamental concepts of causal inference to facilitate the readers' understanding. This involves two representative causal frameworks: SCMs (Structural Causal Models) proposed by Pearl et al. [71] and the potential outcome framework developed by Rubin et al. [76]. Considering the topic of this survey, we will elaborate on the core concepts using examples from recommender systems for clearer understanding. The basic concepts are shown in Fig. 2 and Fig. 3, which we will explain in detail in the following sections. 2.1.1 Structural Causal Models. Generally, SCMs abstract the causal relationships between variables into causal graphs, build structural functions and then conduct causal inference to estimate the effects of interactions or counterfactuals [71]. Causal Models. Causal models involve two essential concepts: causal graphs and structural functions. Specifically, a causal graph describes the causal relationships via a Directed Acyclic Graph (DAG), in which the nodes denote variables and the edges indicate causal relationships. According to a causal graph, structural functions are used to model the relationships. For each variable, one structural function calculates its value based on its parent nodes. Three Typical DAGs. As shown in Fig. 2, there are three classic structures in causal graphs: chain , fork , and collider , for each of which we give an example of recommender systems. In the chain structure, \ud835\udc4b affects \ud835\udc4c via the mediator \ud835\udc4d . For example, in Fig 2 (a), the user features affect the user preferences, and the user preferences affect the users' click behavior. Besides, in the fork structure, \ud835\udc4d is a confounder, affecting both \ud835\udc4b and \ud835\udc4c . For example, as shown in In Fig. 2(b), an item's quality can affect both its price and users' preferences towards it. In such a fork structure, \ud835\udc4d is defined as confounder variable . Roughly ignoring confounder \ud835\udc4d leads to spurious correlation between \ud835\udc4b and \ud835\udc4c . That is, products with higher prices may have larger sales on an e-commerce platform, which does not mean users prefer to spend much money. In Fig. 2(c), differently, \ud835\udc4d represents a collider, which is affected by \ud835\udc4b and \ud835\udc4d . For example, the users' click behavior is affected by user preference and item popularity. Conditioning on \ud835\udc4c will lead to correct correlation between \ud835\udc4b and \ud835\udc4d . That is, users' behaviors on two items with the same popularity level are only affected by their preferences. Intervention. Given the causal graph, a basic concept of intervention can be formally defined. Specifically, the intervention on a variable \ud835\udc4b is formulated with \ud835\udc51\ud835\udc5c -calculus, \ud835\udc51\ud835\udc5c ( \ud835\udc4b = \ud835\udc65 ) [71], which A counterfactual Frontdoor adjustment B D A B E C Backdoor adjustment A B C A B C' A' B C intervention association C blocks the effect of \ud835\udc4b 's parents and set the value of \ud835\udc4b as \ud835\udc65 . For example, \ud835\udc51\ud835\udc5c ( \ud835\udc4b = \ud835\udc65 ) in Fig. 2(b) will rule out the path \ud835\udc4d \u2192 \ud835\udc4b and force \ud835\udc4b to be \ud835\udc65 [72]. That is, in our above-mentioned example, we set the item prices to a specific value. Counterfactual. Another important concept is the counterfactual, which contrasts with the factual. It is used to address scenarios where the treatment variable's value settings do not occur in the real world. In other words, counterfactual inference estimates what the outcome would have been if the treatment variable had taken on a different value compared to its observed value in the real world [71]. For example, a bankrupted seller might wonder about potential sales in a counterfactual world where he/she had purchased advertisement services, setting the treatment variable \ud835\udc47 if_ads = 1. 2.1.2 Potential Outcome Framework. The potential outcome framework [76] is another widely-used causal inference framework besides the structural causal model [71]. It estimates the causal effect of a treatment variable on an outcome variable without the need for a causal graph. Potential Outcome [76]. Given the treatment variable \ud835\udc47 and the outcome variable \ud835\udc4c , the potential outcome \ud835\udc4c \ud835\udc56 \ud835\udc61 denotes the value of \ud835\udc4c under the treatment \ud835\udc47 = \ud835\udc61 for individual \ud835\udc56 . In the factual world, we can only observe the potential outcome of \ud835\udc4c under one treatment for each individual. Treatment Effect [76]. Given binary treatments \ud835\udc47 = 0 or 1, the Individual Treatment Effect (ITE) for an individual \ud835\udc56 is defined as \ud835\udc4c \ud835\udc56 1 -\ud835\udc4c \ud835\udc56 0 . However, ITE is impossible to calculate since we can only observe one potential outcome. Hence, ITE is extended to Average Treatment Effect (ATE) over a population. For a population \ud835\udc56 = { 1 , 2 , ..., \ud835\udc41 } , ATE is calculated by E \ud835\udc56 GLYPH<2> \ud835\udc4c \ud835\udc56 1 -\ud835\udc4c \ud835\udc56 0 GLYPH<3> = 1 \ud835\udc41 \u02dd \ud835\udc41 \ud835\udc56 = 1 GLYPH<0> \ud835\udc4c \ud835\udc56 1 -\ud835\udc4c \ud835\udc56 0 GLYPH<1> . Discussions about these two frameworks. We briefly summarize the similarities and differences between the two frameworks. As stated by Pearl [70], the two frameworks are logically equivalent. The theorem and assumptions in one framework can be equivalently translated into the language of the other framework. However, the key difference is that the potential outcome framework neither considers the causal graph to describe causal relationships nor conducts reasoning over the graph to estimate causal effects. 2.1.3 Causal Effect Estimation and Causal Discovery. For estimating the causal effect, one golden rule is to conduct randomized experiments. Since individuals are divided into the treatment group and the control group randomly, there are no unobserved confounders. Under randomized experiments, some favorable properties of causal inference are guaranteed, such as covariate balance and exchangeability. Meanwhile, the causal effect can be estimated directly by comparing the two groups. For example, online A/B testing can be regarded as a kind of randomized experiment that divides users randomly into several groups and can obtain trustworthy evaluation results of recommendation performance. However, randomized experiments can be expensive and sometimes impossible to conduct. For example, in recommender systems, experiments generating randomized recommendations can detrimentally affect user experiences and the platform's profitability. Therefore, estimating the causal effect solely from observational data becomes critical. In general, a causal estimand is first transformed into a statistical estimand with a causal model like SCM. Then the statistical estimand is estimated with observed data. In other words, with the defined causal model, we can discern causal effects and non-causal effects, such as confounding associations between treatment and outcome. Subsequently, the causal effect is extrapolated by estimation using observed data in alignment with the identified causal mechanisms. One classical method is backdoor adjustment [71]. We say a set of variables \ud835\udc4a satisfies backdoor criterion if \ud835\udc4a contain no descendant of T and \ud835\udc4a can block backdoor paths (which has arrow into \ud835\udc47 rather than from \ud835\udc47 ) between \ud835\udc47 and \ud835\udc4c . The causal effect of \ud835\udc47 on \ud835\udc4c then can be obtained with backdoor adjustment as follows, where \ud835\udc64 \u2208 \ud835\udc4a and the total causal effect is the weighted sum of the conditioned causal effect. The above backdoor adjustment can address observed confounders, but not unobserved confounders, where frontdoor adjustment [71] comes to help. We say a set of variables \ud835\udc40 satisfies frontdoor criterion if all the causal paths from treatment variable \ud835\udc47 to the outcome variable \ud835\udc4c are through \ud835\udc40 , and there is no unblocked backdoor path from \ud835\udc47 to \ud835\udc40 , as well as \ud835\udc40 to \ud835\udc4c when conditioned on \ud835\udc47 . The causal effect of \ud835\udc47 on \ud835\udc4c then can be obtained with frontdoor adjustment as follows, where possible unobserved confounders are addressed. With the sufficient adjustment set of variables \ud835\udc4a in the high dimension, it is difficult to directly estimate the causal effect as the positivity property is hard to satisfy. Instead of modeling the whole set \ud835\udc4a , we can turn to the propensity score as follow, which indicates the probability of receiving the treatment given \ud835\udc4a . Then the causal effect can be estimated by inverse propensity weighting ( IPW ) [35] on the treatment and control group as follows, All of the above causal effect estimations assume that we already have a causal graph. However, in the real world, we often lack prior knowledge about the causal relationships in collected data. This limitation gives rise to the problem of causal discovery, where the objective is to construct a causal graph from the existing data of a set of variables. Traditional approaches identify causal relations through conditional independence tests, bolstered by additional assumptions such as faithfulness [93]. Score-based algorithms [34, 87] have been also proposed to relax the strict assumptions for causal discovery. These methods utilize a score function to measure the quality of the discovered causal graph in comparison with observed data. Recently, various machine learning approaches have been developed to discover causal relations from large-scale data. For example, Zhu et al. [142] utilize reinforcement learning method to find an optimal DAG with respect to a scoring function and penalties on acyclicity. There is a survey [26] fully discusses different methods of causal discovery. To summarize it, we have introduced the fundamental knowledge of causal inference, including two basic frameworks and two important research topics, causal effect estimation and causal discovery.", "2.2 Recommender System": "2.2.1 Overview. As an approach to information filtering, the recommender system has been widely deployed on various platforms in recent decades, such as TikTok, YouTube, Twitter, etc. In general, the modeling of user preferences based on historical interactions is the key point for the recommendation algorithm, and users' future interactions are further predicted. In this way, the necessary data input of a recommendation task includes the records of user-item interactions, and the output is a model that can generate the interaction likelihood of a given user-item pair. This procedure can be formulated as, where U and I denotes the user set and item set, respectively. \ud835\udc66 \ud835\udc62 \u2032 ,\ud835\udc56 \u2032 = 1 if user \ud835\udc62 \u2032 \u2208 U has interacted with item \ud835\udc56 \u2032 \u2208 I ; if not, \ud835\udc66 \ud835\udc62 \u2032 ,\ud835\udc56 \u2032 = 0; here the function \ud835\udc53 (\u00b7 , \u00b7) denotes the recommendation model. Furthermore, with different input data, there are two primary families of models in recommendation, i.e. , collaborative filtering (CF) and click-through rate (CTR) prediction. Despite the vanilla CF which only considers user-item interaction data, some recommendation tasks enhance the behavioural data with auxiliary data, such as social network in social recommendation [18, 115], behavioral sequences in sequential recommendation [8, 143], multiple-type behaviors in multi-behavior recommendation [41, 136], multi-domain user behaviors in cross-domain recommendation [20, 37], etc. For CTR prediction problem, user and item features such as user profiles (occupation, age) and item attributes (category, brand) are also considered as input. The mainstream works of CTR prediction focus on extracting high-order cross-features with attention-based neural network [24], attention-based neural network [24], self-attentive layers [91], etc. 2.2.2 Recommendation Model Design. Here we present two folds of design of recommendation models, collaborative filtering and click-through rate prediction. Collaborative Filtering. Following the development process, existing CF models can be categorized into three types, including matrix factorization (MF)-based, neural network (NN)-based, and graph neural network (GNN)-based. The standard way of modeling is to represent users and items with latent vectors, i.e. , embeddings. With user embedding matrix P \u2208 R \ud835\udc51 \u00d7|U| and item embedding matrix Q \u2208 R \ud835\udc51 \u00d7|I| , in which \ud835\udc51 denotes embedding dimension, the interaction likelihood of ( \ud835\udc62, \ud835\udc56 ) will be the similarity of corresponding embeddings p \ud835\udc62 and q \ud835\udc56 . \u00b7 MF [47]. The similarity function is the inner product as follows, A Survey and Future Directions \u00b7 NCF [31]. In order to incorporate the capability of modeling non-linearity, NCF generalized the similarity function and introduced the multi-layer perceptron (MLP) as follows, where p \ud835\udc3a \ud835\udc62 , p \ud835\udc40 \ud835\udc62 ( q \ud835\udc3a \ud835\udc56 , q \ud835\udc40 \ud835\udc56 ) denotes the user (item) embedding for MF and MLP parts respectively, [\u00b7 , \u00b7] indicates the concatenation operation, \u2299 indicates the Hadamard product, h is the weight vector, and \ud835\udf19 (\u00b7) denotes MLP. \u00b7 NGCF [106]. This GNN-based recommendation model conducts multiple layers of message passing on the user-item bipartite graph. Formally, the similarity is calculated as follows, where p 0 \ud835\udc62 = p \ud835\udc62 , q 0 \ud835\udc56 = q \ud835\udc56 , and \ud835\udc59 indicates the propagation layer, N \ud835\udc62 refers to the set of interacted items of user \ud835\udc62 , and N \ud835\udc56 indicates the set of those users who have interacted with item \ud835\udc56 . Here Agg (\u00b7) is the aggregation function for collecting neighborhood information. In this way, high-order user-item connectivity is injected into the similarity measurement between nodes. Click-Through Rate Prediction. As introduced above, the unified procedure of CTR prediction is extracting high-order features. The input features are denoted as follows, where \ud835\udc40 denotes the number of feature fields. Furthermore, the raw features will be transformed into embeddings as follows, where V \ud835\udc58 \u2208 R \ud835\udc51 \ud835\udc58 \u00d7|F \ud835\udc58 | is the feature embedding matrix, F \ud835\udc58 is the set of optional features, \ud835\udc51 \ud835\udc58 is the dimension of embeddings, and \ud835\udc58 denotes the order of feature field. In general, there are two fields of users' and items' identity, supposed to be the first two ones, then V 1 = P and V 2 = Q . In terms of the mapping function, it can be represented as follows, The design of \ud835\udc54 (\u00b7) will introduce a module of feature interaction learning, via the inner product in FM [74], multi-layer perceptions in DeepFM [24], stacked self-attention layers in AutoInt [91], etc. 2.2.3 Objective Function. The primary objective functions for optimization utilized in recommendation models are in two categories, i.e. , point-wise and pair-wise. Specifically, the point-wise objective function focuses on the prediction of a user-item interaction of which the widely-used Logloss function is as follows, where \u02c6 \ud835\udc66 \ud835\udc62,\ud835\udc56 = \ud835\udc60 ( \ud835\udc62, \ud835\udc56 ) and O is the training set. In terms of the pair-wise objective function, it encourages a larger disparity between positive ( \ud835\udc66 \ud835\udc62,\ud835\udc56 = 1) and negative ( \ud835\udc66 \ud835\udc62,\ud835\udc57 = 0) samples, and the widely-used BPR loss function [75] is as data user bias user data missing RecSys RecSys Blackbox user Accuracy \u221a Explainability \u00d7 Fairness \u00d7 \u2026. \u00d7 Causal Inference debiasing causality-enhanced model architecture data augmentation causal effect estimation causal effect collection follows, where \ud835\udf0e (\u00b7) denotes the sigmoid function, and Q denotes the training set.", "3 WHYCAUSAL INFERENCE IS NEEDED FOR RECOMMENDER SYSTEMS": "In this section, we will discuss the essentiality and benefits of introducing causal inference into recommender systems from three aspects, illustrated in Fig. 4.", "3.1 The Issues of Data Bias in Recommender Systems": "3.1.1 Data bias in recommender systems. Data bias refers to the uneven distribution of recommendation data that does not faithfully reflect user preference. Generally, there are two main types of data bias in recommendation over interactions and attributes. Bias over interactions. Historical user-item interactions collected from previous recommendation strategies are typically treated as labels for recommender model training. Sometimes, historical interactions follow a highly skewed distribution over items ( a.k.a. long-tail distribution), resulting in models over-recommend popular items, i.e., popularity bias [111, 138]. Furthermore, the historical interactions of a user also exhibit uneven distributions over item categories. Consequently, recommender models will blindly assign high scores to items from the frequent category, ignoring the user preference over the remaining categories [102]. Worse still, such biases will be amplified in the feedback loop, leading to notorious issues like unfairness and the filter bubble. Conformity bias refers to the fact that users' behaviors are determined by not only user preferences but also conformity, making the collected data biased. It is a common issue in social-aware information systems, such as the user-post interaction behavior on Facebook 1 . Exposure bias is another widely-concerned bias, which refers to that the exposure algorithms will highly influence the data collection of user feedback. Bias over attributes. Item attributes that can directly result in interactions, especially clicks, can also mislead the estimation of user preference. Training over historical interactions will inevitably push the model to highlight such attributes, leading to shortcuts. Taking video recommendation as an example, videos with attractive titles or cover images are more likely to be clicked, while the user may not like the content [103]. Undeniably, the shortcuts of such item attributes will lead to recommendations failing to satisfy user preference. Worse still, they also make the recommender system vulnerable to relevant attacks, e.g., the item producer intentionally leverages such features. 3.1.2 The necessity of causal inference for data debiasing. Causal theory enables us to identify the root cause of data bias by scrutinizing the generation procedure of recommendation data and mitigating the impact of bias through causal recommendation modeling. Causal view of data bias. The main source of bias effect in recommendation is the backdoor path (Fig. 2(b)), where a confounder ( \ud835\udc4d ) simultaneously affects the inputs ( \ud835\udc4b ) and interactions ( \ud835\udc4c ). Due to the existence of the backdoor path, directly estimating the correlation between \ud835\udc4b and \ud835\udc4c will suffer from spurious correlations, leading to a recommendation score higher than \ud835\udc4b deserved. For instance, item popularity affects the exposure probability of an item in a previous recommendation strategy and interaction probability due to user conformity. Due to ignoring item popularity, CF methods will assign higher scores to items with higher exposure in previous recommendation strategies, leading to over-recommendation, i.e., popularity bias. In the causal terminology, this type of bias effect is termed as confounding bias . Beyond confounding bias, another source of bias in recommendation is the gap between the observed interactions and true user preference matching. Some item attributes directly affect the status of interactions. Causal recommendation modeling. The key to eliminating bias effects lies in modeling the causal effect of \ud835\udc4b on \ud835\udc4c instead of the correlation between them. In causal language, it means viewing \ud835\udc4b and \ud835\udc4c as treatment and outcome variables, respectively. The causal effect denotes to what extent \ud835\udc4c changes according to \ud835\udc4b , i.e., the changes of \ud835\udc4c when forcibly changing the value of \ud835\udc4b from a reference status to the observed value. To estimate such a causal effect, it is thus essential to incorporate conventional causal inference techniques into recommender models. Consider video recommendations on platforms like YouTube and Netflix. Here, \ud835\udc4b represents user preference while \ud835\udc4c symbolizes users' click behaviors. In an ideal setting, users' click behaviors should be directly influenced by their preferences. However, at times, external factors like an enticing video cover (represented by \ud835\udc4d ) might introduce bias.", "3.2 The Issues of Data Missing and Data Noise in Recommender Systems": "3.2.1 Data missing in recommender systems. The data utilized in recommender systems is typically limited, which cannot cover all possible user-item feedback. For example, a user has only rated a small ratio of clicked movies; or the user purchasing the camera is not recorded as having bought a camera lens and a roll film, which is intuitively reasonable. Therefore, the obtained data cannot fully represent the users' interest, leading to sub-optimal results for existing recommendation methods. First, the interaction data observed is constrained by the already-deployed recommendation policy of the recommender system [78]. Users can only interact with specific items if these items are exposed to them, which strongly correlates with the recommender system's intrinsic strategy. In addition, users may refuse to give feedback [107]. For example, on movie rating websites such as IMDB or Douban 2 , users may only rate a few of the movies they have watched. Under this condition, it becomes more challenging to model users' interests. Besides, features of users and items can also be missing in real-world recommender systems due to the high cost of feature collection. 3.2.2 The necessity of causal inference for data missing. Some earlier approaches [85, 94, 98] without causal inference were developed to address the data-missing problem. Steck [94] computes prediction errors for missing ratings. Schnabel et al. and Thomas et al. [85, 98] consider weights for each observed rating based on the probability of collecting that record. However, these methods are limited by low accuracy and poor generalization ability. Causal inference actually provides the causal descriptions of how data is generated, which can serve as prior knowledge to data-driven models. As a result, the negative impact of data-missing issues can be alleviated, improving accuracy and generalization ability. 3.2.3 Data noise in recommender systems. The recommender systems highly rely on the historical user-item interaction feedback to model users' preferences and predict the interaction probability between the user and the unseen item; thus, the reliability of collected data is the basis of the effectiveness of recommender systems. However, the data collected in the real world may be noisy, i.e., incorrect . It is hard to detect and eliminate noisy interactions in traditional recommendation methods. Mahony et al. [65] classified data noise into two categories: natural noises and malicious noises . Natural noise relates to the noise generated during the data-collection procedure by recommender systems, and malicious noise denotes the noise being deliberately inserted into the system. As for the natural noise, Li et al. [51] discussed various reasons that lead to the noisy data in recommender systems. The major reasons include the inaccurate impression of the users themself and the error in data collection. Jones et al. [43] points out that users can hardly accurately measure their preferences, thus leading to mismatch between their preferences and final ratings. Cosley et al. [14] found that noisy data arises when users map their opinions into discrete ratings. Zhang er al. [137] argued that in some streaming applications, the conversion events may be delayed to the time when data is collected. Thus the feedback of users may have not yet occurred, resulting in a large number of incompletely labeled instances and introducing noise to data. Some existing work [38, 60, 101, 112] also pointed out the difference between the implicit feedback and users' actual satisfaction because of noisy interactions. For example, in E-Commerce, many clicks do not lead to purchases, and a large portion of purchases finally get negative comments. Implicit interaction data widely used in recommender systems nowadays is easy to become noisy because of the inaccurate first impression of users. Since users are exposed to a flood of information in today's online services, users are very likely to have accidentally triggered feedback such as click-by-mistake. As for the malicious noise , it is produced by adversary attackers of recommender systems. For instance, on user-generated platforms such as TikTok 3 , some authors will create plenty of new accounts to rate their work with high scores, trying to earn over-exposure opportunities. In ecommerce websites such as Amazon, some adversary sellers may generate fake order records or positive comments on their products. 3.2.4 The necessity of causal inference for data denoising. Manyprevious works have experimentally demonstrated the severity of data noise and its negative effects on recommender systems. Cosley et al. [14] showed that only 60% of users will keep their rating to the same movie when they are asked to re-rate for it. Further experiments show that statistically significant MAE differences arise when exploiting CF models on the original rating data and new rating data. Amatriain et al. [3] showed that the recommendation performance will be significantly affected under noisy data compared to the noiseless data, with a difference of RMSE of about 40%. Wang et al. [101] found through experiments on two representative datasets the performance of recommender system trained by noisy data experienced a performance drop of 9.56%-21.81% w.r.t. Recall@20 and drop of 3.92%-8.81% w.r.t. NDCG@20, compared with the recommender system trained over cleaned data. Although existing work has confirmed the widespread existence of data noise, which reveals that we need to consider its impact during training recommendation models, existing solutions are a few. Data noise can arise from various sources, such as limitations in data collection (e.g., inaccurate values in users' questionnaire data) or during data preprocessing (e.g., crudely transforming feedback into simplified values, like converting continuous user watching durations into discrete positive/negative labels). Such noises present significant challenges in accurately discerning user preferences. Leveraging causal inference allows us to more effectively detect the presence of noise in interaction data or bridge the disparity between noisy training data and the expected clean testing data with the help of counterfactual learning and reasoning.", "3.3 Beyond-accuracy Concerns in Recommender Systems": "Traditional recommender systems are designed towards the major goal of achieving higher accuracy, i.e., click-through rate or conversion ratio, serving for the platform benefit. Nevertheless, as recommender systems have become fundamental information services in more and more aspects of daily life, these concerns are not just technical problems but also social challenges. 3.3.1 Explainability. The requirement of explainability for recommender systems refers to the need that we should understand why some items are recommended while others are not. It helps build a bridge between users and recommendation lists for better transparency and trustworthiness. Specifically, it can be divided into two categories, explainable recommendation model and explainable recommendation results. Some existing work [11, 100, 139] mainly took some item aspects to give explanations, which is concluded as the aspect-aware explainable recommendation. For example, Wang et al. [100] learned users' preferences on given aspects by factorization method to get the aspect-aware explanations. The necessity of causal inference. Despite their effectiveness to some extent, existing methods of explainable recommendation are still limited [96]. Specifically, the explanation is built on correlation. As mentioned above, roughly extracting correlations from the observed data without the support of causal inference may lead to wrong conclusions. Furthermore, the explanations of the recommendation model require building explicit causal relations between the components of the recommendation model and the prediction scores. Additionally, the explanation for recommendation results should fully consider how different decision-factors, i.e. , cause, lead to users' behaviors, i.e. , effect. Thus, achieving explainability is tightly connected to causal inference. 3.3.2 Diversity and Filter Bubble. Filter bubble describes the phenomenon where people tend to be isolated from diverse content and information by online personalization [66]. As a consequence, users are placed in a fixed environment where they can only encounter similar topics or information. Passe et al. [68] attribute this effect to homogenization, which means people's behavior and interest show consistency and convergence. The recommender system is one of the main causes of the filter bubble due to the principle of generating recommendation lists by learning the similarity between users or items [67], which inevitably leads to homogeneous recommendations. Gabriel Machado Lunardi et al. [61] empirically analyzed the filter-bubble formation based on popular CF methods and algorithms for diversified recommendation. In terms of human nature, researchers found that people tend to pursue a comfort zone and stay with the opinions they are interested in or agree with [6]. In the long term, the filter bubble will narrow people's views and radicalize their ideas. Thus, it is an urgent problem to break filter bubbles and improve recommendation heterogeneity. The necessity of causal inference. The biased feedback loop is one of the most critical challenges in addressing the filter bubble, as learning from biased data will exacerbate the homogeneity in recommendation exposure and further bias the collected data. Moreover, the accuracy-diversity dilemma is another challenge, which refers to the phenomenon where pursuing accuracy will lead to low diversity. Causal inference provides the opportunity to address these challenges. First, causal inference can alleviate the bias or missing data in collected data, supporting the exploration of unseen data. Second, the causal inference-enhanced model can utilize the causal relationships under user behaviors, understanding why users consume certain items. This can help recommend items outside the existing categories and meet user demands. 3.3.3 Fairness. Recently, the fairness of recommendations has gained significant attention. As we know, recommender systems operate as multi-stakeholder platforms, thus encompassing various aspects of fairness concerns, including both user-side and item-side [7]. The user-side fairness issue arises from the diverse fairness concerns among users. For instance, while some users may be predominantly worried about potential biases based on their gender, others might be more concerned about age-related biases [54]. To foster trust in the recommender system, it's essential to address these user-side fairness concerns in a personalized manner. While some approaches [28] have attempted to rectify these fairness challenges using association-based methods-aimed at eliminating statistical metric discrepancies between groups-research has shown these methods to be inadequate and lacking in certain areas [45, 48]. Notably, these associationbased techniques often overlook the intricate relationship between objective features and model outputs. Conversely, a few studies have explored fairness through a causal lens, offering insights into how output variables evolve with changes in input [1, 44]. Item-side fairness, on the other hand, evaluates the equity in treatment of each item during the recommendation process. Biases may emerge due to the oversight of particular items or their attributes. Several existing solutions [23, 86] have ventured into unbiased learning or heuristic ranking adjustments to rectify these biases. The necessity of causal inference. Tackling fairness issues is akin to hypothesizing in a counterfactual realm: Had a user not been part of a specific group, or had an item lacked a certain feature, would the recommendation outcomes remain unchanged? If not, what would these altered recommendations look like? This difference between the counterfactual and factual worlds forms the cornerstone of fairness evaluation in recommender systems. Hence, methods grounded in causal inference, particularly those employing counterfactual reasoning, offer a fresh and more comprehensive approach to enhancing recommendation fairness compared to their non-causal counterparts. In short, we have systematically discussed the limitations of existing recommender systems and why causal inference is essential to address these limitations. In the following, we will introduce how these challenges can be addressed (at least partially addressed) by presenting the recent advances in the causality-enhanced recommendation.", "4 TECHNICAL DETAILS OF EXISTING WORKS OF CAUSAL INFERENCE-BASED RECOMMENDER SYSTEMS": "The existing work of causal inference for recommendation is presented based on the three major issues of recommendation models with only correlation considered. The overall illustration is presented in Fig. 5, and the details are introduced one by one as follows. Data Bias Popularity Bias Others Exposure Bias Backdoor, Counterfactual IPW, Doubly-Robust Backdoor, Counterfactual Data Missing Data Collection Data Noise Causal Effect Counterfactual Counterfactual, IPW IPW Beyond-accuracy Optmization Goals Explainablity Fairness Diversity Causal Discovery, Counterfactual Disentangling Backdoor, Counterfactual Counterfactual Three major issues of RecSys Section 4.1 Section 4.2 Section 4.3 Different types of goals Different types of data bias Causal inference approaches Causal inference approaches Two kinds of data missing Causal inference approaches", "4.1 Causal Inference-based Recommendation for Addressing Data Bias": "Existing methods on causal debiasing are mainly in three categories : confounding effect, colliding effect, and counterfactual inference. 4.1.1 Confounding Effect. In most cases, biases are caused by confounders, which lead to confounding effect in correlations estimated from the observations. To eliminate the confounding effect, there are mainly two lines of research regarding the causal inference frameworks adopted. Structural Causal Model. Using SCM to eliminate confounding effect falls into two categories: backdoor and frontdoor adjustments. Backdoor adjustment is able to remove the correlations by blocking the effect of the observed confounders on the treatment variables. To address the data bias in recommender systems, the existing work usually inspects the causal relationships in the data generation procedure, identifies the confounders, and then utilizes backdoor adjustment to estimate causal effect instead of correlation. Specifically, backdoor adjustment blocks the effect of confounders on the treatment variables by intervention [71], which forcibly adjusts the distribution of treatment variables and cuts off the backdoor path from treatment variables to outcome variables via confounders. For example, Zhang et al. [138] ascribed popularity bias to the confounding of item popularity, which affects both the item exposure and observed interactions. They then introduced backdoor adjustment to remove the confounding popularity bias during model training, and incorporated an inference strategy to mitigate popularity bias. Besides, Wang et al. [102] explored the bias amplification issue of recommender systems, i.e., over-recommending some majority item categories in users' historical interactions. For instance, recommender systems tend to recommend more action movies to users if they have interacted with a large proportion of action movies before. To tackle this, Wang et al. [102] found that the imbalanced distribution of item categories is actually a confounder, affecting user representation and the interaction probability. Next, the authors proposed an approximation operator for backdoor adjustment, which can help alleviate the bias amplification. However, the assumption of observed confounders might be infeasible in recommendation scenarios. To tackle the unobserved confounders ( e.g., the temperature when users interact with items), frontdoor adjustment is a default choice [71]. Xu et al. [125] has made some initial attempts to address both global and personalized confounders via frontdoor adjustment. Zhu et al. [144] gave a more detailed analysis of the conditions to apply the frontdoor adjustment in recommendation. Liu et al. [58] approached the selection bias challenge and proposed counterfactual learning-based method. Specifically, the authors focus on policy learning approaches for top-K recommendations in extensive item spaces, identifying key challenges like importance weight explosion and observation scarcity. A novel framework is introduced for efficient policy learning that addresses these complexities. Ding et al. [16] emphasizes the challenge of unmeasured confounders in recommender systems which can influence the accuracy of feedback predictions. The authors proposed Robust Deconfounder (RD) to consider the effects of these unmeasured confounders on propensities, using a bounded effect approach. Potential Outcome Framework. From the perspective of the potential outcome framework, the target is formulated as an unbiased learning objective for estimating a recommender model. Let \ud835\udc42 \ud835\udc52 denote the exposure operation where \ud835\udc5c \ud835\udc62,\ud835\udc56 = 1 means item \ud835\udc56 is recommended to user \ud835\udc62 . The set O is defined as the exposure results under the given exposure strategy (with O \ud835\udc52 ). According to the definition of IPW [56], we can learn a recommender to estimate the causal effect of \ud835\udc4b on \ud835\udc4c by minimizing the following objective, where \ud835\udc59 (\u00b7) denotes a recommendation loss and \u02c6 \ud835\udc5d \ud835\udc62,\ud835\udc56 denotes the propensity, i.e., the probability of observing the user-item feedback \ud835\udc66 \ud835\udc62,\ud835\udc56 . As one of the initial attempts, Tobias et al. [86] adopted this objective to learn unbiased matrix factorization models where the propensity is estimated by a separately learned propensity model (logistic regression model). Beyond such shallow modeling of propensity [81], Zhang et al. integrated the learning of propensity model and recommendation model into a multi-task learning framework [135], which demonstrates advantages over the separately learned one. Wang et al. [109] took the pioneering step of considering the exposure bias in the sequential recommendation, by proposing an IPW-based method named USR for alleviating the confounder in sequential behaviors. Nevertheless, estimating the proper propensity score is non-trivial and typically suffers from high variance. To address these issues, a line of research [27, 79, 107] pursues a doubly-robust model estimator by augmenting Equation 14 with an error imputation model, which is formulated as: where \u02c6 \ud835\udc52 \ud835\udc62,\ud835\udc56 is the output of the imputation model with user-item features as inputs. To learn the parameter of the imputation model, a joint learning framework [107] optimizes: Undoubtedly, incorporating experimental data, i.e., interactions from randomized controlled trial (RCT) such as random exposure, can enhance the doubly-robust estimator. In this light, a line of research [9, 108] investigates data aggregation strategies, which largely focuses on tackling the sparsity issue of experimental data since RCT is costly. Recently, Li et al. [52] considered that the exposure bias is largely depends on the socially-connected users, and proposed IPS-based methods with the auxiliary social network data. Different with the existing works for specific kinds of bias, He et al. [32] studied how to address general feature biases. This work identified a challenge in recommender systems where some features, like video length, can bias user interaction data and misrepresent actual preferences. Approaching from a causal perspective, the study introduced the Deconfounding Causal Recommendation (DCR) framework to address this bias. The DCR used backdoor adjustment to counteract the effects of confounding features and combine it with the mixture-of-experts (MoE) model architecture. 4.1.2 Colliding Effect. We can discover many collider structures ( cf. Fig 2(c)) in the interaction generation process by inspecting the causal relationships. A representative case is that many different variables affect the observed interactions, such as user interests and conformity. Conditioning on the collected user interactions will lead to the correlation between user interests and conformity: an interaction caused by user conformity has a higher probability of being uninterested. To mitigate the conformity bias, an existing work [141] disentangles the interest and conformity representations by training over cause-specific data, which improves the robustness and interpretability of user representations. 4.1.3 Counterfactual Inference. Another SCM-based technique used for debiasing is counterfactual inference. In some SCMs of recommender systems, two causes (user and item features) lead to one effect (user behavior). If the user features or item features are significantly biased, this direct path (which we inaccurately referred to as a \"shortcut\" in our original version) can result in biased interaction learning, especially when other unbiased features take a more indirect route. The counterfactual inference is able to estimate the path-specific causal effect and eliminate the causal effect of partial user/item features. Specifically, it first imagines a counterfactual world without these features along specific paths and then compares the factual and counterfactual worlds to estimate the path-specific causal effect. For example, Wang et al. [103] conducted counterfactual inference to remove the effect of exposure features ( e.g., attractive titles) for mitigating clickbait issues. In addition, Wei et al. [111] reduced the direct causal effect from the item node to the ranking score to alleviate popularity bias. Furthermore, Xu et al. [123] proposed an adversarial component to capture the counterfactual exposure mechanism and optimized the candidate model over the worst-case scenario with a min-max game between two recommendation models.", "4.2 Causal Inference-based Recommendation for Addressing Data Missing and Noise": "Data collected from recommender systems are usually scarce due to limited user engagement compared with the whole item candidate pool. In addition, the data can also be unreliable and incorrect since the system may fail to collect the true reward within the tight time window for data collection. Meanwhile, the real causal effect of recommendation is largely unknown since the data of not recommending an item is unavailable. As a consequence, it is challenging for recommender systems to capture user preferences accurately since they are trained with missing and noisy data. Tools of causal inference can be leveraged to tackle the two problems by generating either counterfactual data to augment insufficient training samples or counterfactual rewards to adjust noisy data. Uplift modeling is utilized to measure the causal effect of recommendation. Table 2 provides a brief summary of recommender systems that utilize causal inference to address data missing and data noise problems. 4.2.1 Causal Inference for Data Missing. Interactions between users and items are the factual data, which expresses what really happens on the recommendation platforms and directly reflects user interest. However, factual data is usually scarce; thus, it is insufficient for recommender systems to accurately capture the user interest hidden in the data. The natural idea is to generate more samples that did not actually happen to augment the training data. Such data augmentation aims to answer a question in counterfactual world: 'what would ... if ...', which has been adopted in several research fields like computer vision [19], and natural language processing [145]. In terms Modeling Factual Input Factual Output Intervention Intervened Input Inference Intervened Input Simulated Output Original Data Augmented Data of recommendation, counterfactual data augmentation aims to generate more interactions under situations that are different from the real cases when the factual data is collected. Existing approaches answer counterfactual questions for the following recommendation scenarios, \u00b7 Collaborative Filtering (Top-N Recommendation). In this scenario, users are provided with a ranked list of items, and they will interact with several items in the list. Data augmentation generates the feedback of unseen recommendation lists; thus the counterfactual question is 'what would the given user's feedback be if the system had provided a different recommendation list?' [127]. \u00b7 Sequential Recommendation. In this scenario, recommendation is made according to the historical interaction sequences of users. In other words, interactions of the same user are regarded as a sequence ordered by the timestamp of each interaction. Augmented data are interaction sequences that do not exist in the real scenario. Therefore, the counterfactual question is 'what would users behave if their interaction sequences were different?'[110, 133]. \u00b7 Feature-based Recommendation. In this scenario, not only interactions but also features such as user profiles and item attributes are available for recommendation. In other words, user preference modeling can rely on the user/item features. The counterfactual question that data augmentation aims to answer is 'what would the given user's feedback be if his/her feature-level preference had been different?' [122]. For all the above three scenarios, counterfactual data augmentation follows a similar paradigm of three steps, modeling, intervention, and inference. Fig. 6 provides a brief illustration of counterfactual data augmentation, and we will now introduce these three steps separately. The modeling step captures the data generation process, which can be the recommendation model itself or another separate model. Specifically, it is usually a parametric model that is trained to fit factual data. In other words, given specific users and items that exist in factual data, the model serves as a simulator that is trained with the observed interactions and later generates unobserved interactions. For example, Yang et al. [127] first constructs a structural causal model to express the process of recommendation and then implements the SCM with an inner product between user and item embeddings. Xiong et al. [122] utilizes a multi-layer neural network that takes feature vectors of users and items as input, then uses merging operators such as element-wise product or attention to fuse user and item feature-level properties. Zhang et al. [133] and Wang et al. [110] propose model-agnostic counterfactual data augmentation thus the model can be off-the-shelf sequential recommendation models. The simulator is trained with existing factual data just as a normal recommendation task. After a well-trained simulator is obtained, the input is intervened to be different from factual cases, and the simulator is used to produce the counterfactual outcome. Gao [22] studied counterfactual interactive recommender system (CIRS), which combines offline reinforcement learning with causal inference. The authors used a causal user model derived from historical data to understand the overexposure effect on user satisfaction, with which model, the RL policy can be better planned. In the intervention step, the input is set as different values from the factual data. Specifically, this step generates the counterfactual cases either by heuristic or another learning-based model. Heuristic-based counterfactual intervention is usually achieved by randomization. In [110], a counterfactual interaction sequence can be generated by replacing an item at a random index with a random item. In [133], dispensable and indispensable items are replaced with random items to construct counterfactual positive and negative sequences, respectively. In contrast, the learningbased counterfactual intervention aims to construct more informative samples as data augmentation. In other words, it generates counterfactual data with higher importance for model optimization. For example, in [127], a counterfactual recommendation list is generated by selecting items with larger loss value i.e. the hard samples. In [110] and [122], items and feature-level preferences that are at the decision boundary are selected and then modified with minimal change to construct more effective counterfactual interaction sequences and input features, respectively. In the inference step, counterfactual outputs are generated with the above counterfactual inputs and simulator. This step which uses the simulator to simulate the output of the intervened input, is usually straightforward. In [127], the counterfactual clicked items of the intervened recommendation list are generated by inferring according to the constructed SCM. In [133] and [110], the intervened interaction sequences are fed into the sequential backbone model, and the obtained outputs can directly serve as the counterfactual user embeddings [133], or they can be used to derive counterfactual next items [110]. Wang et al. [105] further considered the problem of out-of-distribution recommendation, i.e. , the data in another distribution is missing. The authors proposed to use a variational auto-encoder to help learn the user representations in the counterfactual distribution. Mu et al. [64] proposed to use counterfactual generator to obtain user-item interaction data with the item's specific relation on the knowledge graph is changed. The counterfactual generator and recommender can be trained jointly to enhance each other. A recent work [33] approaches the issue of data-missing from another perspective, causal discovery. Specifically, this work delves into the vulnerability of current recommender systems to distribution shifts (the missing of IID data). The authors propose a novel causal preferencebased recommendation framework named CausPref, integrating a recommendation-specific DAG learner. With emphasizing causal learning of invariant user preference and anti-preference negative sampling, CausPref shows superiority and interpretability in varied OOD settings. 4.2.2 Causal Inference for Data Noise. Interactions can be noisy or incorrect due to the tight time window of data collection. For example, users' feedback can be delayed after the immediate interaction, such as purchasing an item a few days after adding it to the shopping cart. In realtime recommendation, these samples are used for model training before the complete reward is observed. Therefore, the reward at an early time is noisy, and whether the item will be purchased is unknown when it is added to the shopping cart. Zhang et al. [137] tackle the above problem of delayed feedback with the help of causal inference. Specifically, the authors utilize importance sampling [5, 130] to re-weight the original reward and obtain the modified reward in counterfactual world. In addition, noisy user feedback can be alleviated by incorporating reliable feedback ( e.g., ratings). However, reliable feedback is usually sparse, leading to insufficient training samples. To solve the sparsity issue, Wang et al. [101] contributed a colliding inference strategy, which leverages the colliding effect [71] of reliable feedback on the predictions to facilitate the users with sparse reliable feedback. 4.2.3 Causal Effect Estimation for Recommendation. The recommender systems impact data collection, resulting in the absence of real interaction data, as mentioned above. Existing recommendation approaches are primarily evaluated and trained using interaction data, where typically, more interactions with recommended items indicate a more successful recommendation. However, they overlook the fact that some items may be interacted with by users even without a recommendation. Take e-commerce recommendation as an example: users may have clear intentions and directly purchase the items they desire. On the contrary, some items are more effective in terms of recommendation, meaning that users will purchase these items if recommended but won't purchase them if not recommended. Consequently, recommender systems boost the purchase probability of these effective items, referred to as uplift . These items reflect a stronger causal effect of recommendation, emphasizing the importance of recommending more items with a larger uplift. Some studies [82-84, 121] in recent years investigated the causal effect of recommender systems from the perspective of uplift. Sato et al. [83] applied the potential outcome framework to obtain the average treatment effect (ATE) of recommendation. Specifically, all the interactions are divided into four categories according to the treatment (recommendation) and the effect (user feedback), and then a sampling approach named ULO is proposed to learn the uplift of each sample. IPW was adopted to achieve unbiased offline learning [84] and online evaluation [82] on the causal effect estimation of recommendation. Xie et al. [121] proposed to estimate the uplift with tensor factorization by regarding treatment as an extra embedding, and they use regression discontinuity design (RDD) analysis to simulate randomized experiments. Xiao el al. [120] proposed a doublyrobust estimator, along with which a deep variational information bottleneck method is proposed to aid the adjustment of causal effect estimation. Other studies view the causal effect of the recommendation algorithm as a problem related to off-policy evaluation. In reinforcement learning, the policy determines how the agent behaves (i.e., selecting the action) given the environmental context and the current states. In response, the environment provides the corresponding reward [50]. However, due to high costs and limitations in data collection, it is challenging to collect all possible rewards for every action. Consequently, researchers have proposed off-policy evaluation, aiming to estimate these rewards [2, 77]. In the context of recommendation, the items recommended can be viewed as the policy, and the offpolicy evaluation is understood as estimating the effect of the deployed algorithm. This is akin to uplift modeling but focuses more on a general framework that estimates rewards using historical data. To achieve off-policy evaluation, there are three major categories of estimators: model-based estimators (reward regression), model-free estimators like propensity score-based methods, and hybrid estimators using doubly robust methods [80]. Specifically, Swaminathan et al. [95] tackled the problem of slate recommendation, where an ordered set of items is recommended. They built on techniques from combinatorial bandits to estimate a policy's performance using logged data. Li et al. [53] addressed a similar issue, aiming to estimate the number of clicks for a recommendation list. They introduced click models to construct estimators that learn with statistical efficiency, and the results showed the superior performance of these constructed estimators. Mcinerney et al. [62] studied sequential recommendation and introduced a new counterfactual estimator that accounts for sequential interactions in the rewards, achieving lower variance. Specifically, they reweighted the rewards in the logging policy to approximate the expected sum of rewards under the target policy. Kiyohara et al. [46] based their work on the assumption that users interact with items sequentially, starting from the top position in a ranking, leading them to propose a Cascade Doubly Robust estimator.", "4.3 Beyond-accuracy RecSys with Causal Inference": "As mentioned in Section 3.3, non-causal recommender systems may find themselves focusing solely on improving accuracy, potentially overlooking other critical objectives such as explainability, fairness, diversity, and more. In this section, we elaborate on how existing work addresses this challenge by introducing causal inference into recommender systems. 4.3.1 Causal Inference for Explainable Recommendation. Causal inference naturally can improve the explainability of recommendation, since it captures how different factors (cause) leads to recommendation (effect) rather than only the correlations. To present the existing works, we divide them into three categories as follows. \u00b7 Counterfactual learning. Tan et al. [97] proposed CountER for explainable recommendation using counterfactual reasoning. CountER explained the recommendation by highlighting the distinctions between factual and counterfactual scenarios. Specifically, CountER included an optimization task with the goal of identifying an item that minimizes the difference to the original item, thereby reversing the recommendation outcome in the counterfactual world. CountER [97] also used causal discovery techniques to extract causal relations from historical interactions and the recommended items to enhance the explanation. \u00b7 Causal graph-guided representation learning. Zheng et al. [140] built a recommendation model based on the causal graph. The authors pre-define the causal relationships that how user behaviors (effect) are generated from users' two parts of preferences (causes), long-term preferences and short-term ones. Long-term preferences refer to those stable and intrinsic interests, while shortterm preferences refer to dynamic and temporary interests. The evolution manner is also defined for these two kinds of preferences. Based on the pre-defined causal relations, the authors proposed to assign two disentangled embeddings for two parts of preferences, and the extracted selfsupervised signals make the recommendation model explainable. Si et al. [90] proposed to improve the recommendation model's explainability by decomposing model parameters into two parts: causal part and non-causal part. Specifically, it built a model-agnostic framework by using users' search behaviors as an instrumental variable. \u00b7 Causal discovery. Xian et al. [118] proposed to make use of a knowledge graph for explainable recommendation, and the paths in the knowledge graph can be used for generating explanations. For example, the reason for purchasing AirPods may be that the user has purchased an iPhone before, and iPhone and AirPods are reachable in the knowledge graph via relation has_brand and node Apple Brand . Based on the knowledge graph and users' interaction history, the authors [118] proposed to extract causal relations by a reinforcement learning method. Specifically, the policy function of reinforcement learning is optimized to explicitly select items via paths in knowledge graph, ensuring high performance of both accuracy and explanation. Tran et al. [99] approached the problem of explanable job-skill recommendation. Specifically, it is essential to know which skill to learn to meet the requirements of the job. The authors first proposed causal-discovery methods based on different features with the employment-status label. Then the authors proposed a counterfactual reasoning method that finds the most important feature, of which the modification can lead to employment, which served as the explanations. 4.3.2 Causal Inference for Improving Diversity and Alleviating Filter Bubble. As mentioned earlier, focusing solely on accuracy gives rise to the issue of overly homogeneous content, resulting in the phenomenon known as the filter bubble. By leveraging causal inference, which aids in gaining a deeper understanding and explicitly modeling the causal effects of user-decision factors, recommendations with improved diversity and the reduction of the filter bubble can be achieved. \u00b7 Counterfactual learning. Wang et al. [104] proposed a causal inference framework to alleviate the filter bubble with the help of user control. Specifically, the framework allows users' active control commands with different granularity to seek out-of-bubble contents. Furthermore, the authors proposed a counterfactual learning method that generates new user embeddings in the counterfactual world to remove user representations of out-of-date features. By constructing counterfactual representations, the recommendation can keep both accurate and diverse. \u00b7 Backdoor Adjustment. Wang et al. [102] approached the problem of homogeneous recommendation, by regarding imbalanced item distribution as a confounder between user embedding and the prediction score. Specifically, the authors used the backdoor adjustment to block the effect of the imbalanced item-category distribution in training data, partly alleviating filter bubble. The proposed method is model agnostic and thus it can be adapted to different recommendation models, including both collaborative filtering and click-through rate prediction. Xu et al. [124] employed a causal graph with loops to represent the dynamic recommendation process which leads to the filter bubble. A Dynamic Causal Collaborative Filtering ( \ud835\udf15 CCF) model is proposed, which leverages back-door adjustment to estimate post-intervention user preferences and employs counterfactual reasoning to alleviate the echo chamber effect. Real-world dataset experiments validate the efficacy of the model in mitigating echo chambers, while maintaining strong recommendation performance. 4.3.3 Causal Inference for Fairness in Recommendation. The concept of achieving fairness naturally aligns with the counterfactual world in causal inference. For instance, when evaluating the fairness of a recommender system for a specific user profile, one can pose a counterfactual question: Would the recommendation results change if the user profile were altered? Li et al. [54] introduced the notion of counterfactual fairness in recommendation, where modifying the value of a given feature ensures that the distribution of recommendation probabilities remains unchanged. The authors address this issue by introducing personalized fairness criteria for users. The core idea is to acquire user embeddings that are independent of specific features. To accomplish this, they propose a filtering module positioned after the embedding layer, which eliminates information relevant to sensitive features and generates filtered embeddings. Subsequently, the authors introduce a prediction module that utilizes these filtered embeddings to predict sensitive features, employing an adversarial learning approach in conjunction with the primary recommendation loss functions.", "5 OPEN PROBLEMS AND FUTURE DIRECTIONS": "We discuss important yet not-well-explored research directions in causal inference-based recommender systems.", "5.1 Causal Discovery for Recommendation": "Wehave systematically reviewed numerous works that integrate causal inference into recommender systems. However, existing approaches relying on predefined causal graphs or structural causal models exhibit two significant limitations. First, the assumed causal relationships may be inaccurate. Although the recommendation tailored to the causal relations may improve the recommendation performance, hidden variables may exist that are the real causes. Second, these manually crafted causal graphs are often simplistic, typically involving only a few variables, such as the user conformity, user interest, and user behavior in DICE [141], the exposure feature, user/item/context features, and prediction score in CR [110]. Nevertheless, users' decision-making processes may involve many factors in real-world scenarios. For example, whether a user visits a restaurant depends on the location, cuisine, brand, price, etc. Therefore, it is essential to design causal discovery methods for learning causal relations from realworld data in recommender systems. Traditional methods for causal discovery can be categorized into the following types. Constraint-based (CB) algorithms, such as the PC algorithm [93] and the FCI algorithm [92], initially identify conditional independence relationships between pairs of variables and then construct a directed acyclic graph based on these relationships. GES methods [13, 73] extend CB algorithms by incorporating a scoring function to assess the suitability of a directed acyclic graph (DAG). However, these established methods still grapple with challenges like high computational costs and limited robustness when dealing with large-scale data [26]. Recently, novel approaches based on deep learning [42, 59, 88] and reinforcement learning [142] have emerged to infer causal relationships from extensive datasets. Therefore, it is a promising and crucial future direction for discovering causal relations and then leveraging the learned causal relations to enhance recommendation.", "5.2 Causality-aware Stable and Robust Recommendation": "Recommender systems are expected to be highly stable and robust, which can be explained in the following aspects. First, the utilized data is dynamically collected, such as newly-registered users, new products, etc. As a result, the data distribution may be fast-changing [105]. Secondly, there exist multiple recommendation scenarios, including different tabs within the same mobile app, diverse domains, and various objectives. This necessitates that the recommendation model be capable of maintaining robustness and stability across these scenarios. Last, there exists a disparity between offline evaluations and online experiments. A recommendation model that performs well in offline experiments should ideally deliver strong results in online environments. In pursuit of greater stability and robustness in machine learning models, prior research [40, 57] has underscored the potential of causality-aware models. These models demonstrate a promising ability to adapt to different domains and excel in out-of-distribution (OOD) generalization [105]. Therefore, harnessing causality for the enhancement of robust and stable recommendations holds significant importance.", "5.3 Causality-aware Graph Neural Network-based Recommendation": "In recent years, graph neural networks have been developing in recommendation at an unexpectedly fast speed. GNN-based models have achieved strong performance in various recommendation tasks, such as the significant performance improvement of LightGCN [30] against traditional neural network models [31] in collaborative filtering tasks. The success of graph neural networks is mainly due to the strong ability to extract structured information, especially for the high-order similarity on the graph. However, several critical challenges remain, awaiting solutions bolstered by causality. First, there is a pressing need to demystify the workings of GNNs in making precise and successful recommendations. The explainability of powerful GNN-based recommendation models, encompassing both the model itself and the rationale behind recommendation results, remains an area ripe for further research. Currently, these models often operate as black boxes. Second, while recent strides have been made in causality-aware recommendation models that incorporate GNN modules as integral components, the GNN module itself and the realm of causal inference remain somewhat separate. Explicitly intertwining the message-passing processes of GNNs with causal inference and reasoning for recommendation represents an open and uncharted research frontier.", "5.4 Causality-aware Simulator and Environment for Recommendation": "The recommender system is a kind of system that tries to estimate and recover how humans make decisions. With a longer-term and more rational objective, such systems should not merely predict current or next-step user interactions but also take into account sequences of interactions, with the aim of maximizing user engagement or aligning with platform requirements. Given the dynamic nature of user-system interactions, some prior works [39, 89] have introduced simulators for recommender systems. These works specifically employ reinforcement learning techniques, including imitation learning [36], to simulate how users select items within specific environments and contexts. However, these approaches are predominantly data-driven and often lack the underpinning of causality, potentially leading to inaccuracies in decision-making processes. Recently, causal reinforcement learning (CRL) methods have emerged to address the issue of missing data in reinforcement learning tasks. Bareinboim et al. [4] introduced the concept of leveraging causal interventions to aid in estimating rewards while accounting for unobserved confounders. Additional works[49, 126] have delved into causal bandit algorithms, offering theoretical bounds on performance improvements compared to non-causal bandits. Causally-aware reinforcement learning approaches exhibit substantial promise in handling data limitations when modeling dynamic and sequential user-system interactions. Consequently, they are poised to play an indispensable role in modeling both the simulator and the environment of recommender systems. To conclude, the future endeavors in the realm of causality-aware recommender systems should begin by addressing the constraints imposed by pre-defined causal graphs. Other promising avenues of research encompass enhancing robustness, which involves domain generalization, devising improved evaluation methods for long-term utility, bridging the gap between offline and online settings, exploring more effective integration with graph neural networks, and the development of causality-supported simulators for recommender systems.", "6 CONCLUSION": "In recent years, causal inference has emerged as a critically significant and transformative topic within the realm of recommender systems research. Its significance cannot be overstated, as it has fundamentally altered our understanding of recommendation models. This paper represents an initial stride towards presenting a comprehensive survey of existing literature in this domain. It meticulously and systematically delves into the rationale behind the applicability of causal inference and how it effectively mitigates the shortcomings inherent in non-causal recommendation models. Our primary aim is to serve as a source of motivation for researchers already active in this field and, equally importantly, to inspire those who are contemplating the initiation of research endeavors in this exciting and burgeoning area.", "ACKNOWLEDGMENT": "This work is supported in part by National Key Research and Development Program of China under 2020AAA0106000, and by National Natural Science Foundation of China under 62272262 and U23B2030. This work is also supported by grant from the Guoqiang Institute, Tsinghua University.", "REFERENCES": "[1] Junzhe Zhang and Elias Bareinboim . 2018. Fairness in Decision-Making - The Causal Explanation Formula. In AAAI 2018 . [2] Aman Agarwal, Soumya Basu, Tobias Schnabel, and Thorsten Joachims. 2017. Effective evaluation using logged bandit feedback from multiple loggers. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining . 687-696. [3] Xavier Amatriain, Josep M Pujol, and Nuria Oliver. 2009. I like it... i like it not: Evaluating user ratings noise in recommender systems. In International Conference on User Modeling, Adaptation, and Personalization . Springer, 247-258. [4] Elias Bareinboim, Andrew Forney, and Judea Pearl. 2015. Bandits with unobserved confounders: A causal approach. Advances in Neural Information Processing Systems 28 (2015), 1342-1350. [5] L\u00e9on Bottou, Jonas Peters, Joaquin Qui\u00f1onero-Candela, Denis X Charles, D Max Chickering, Elon Portugaly, Dipankar Ray, Patrice Simard, and Ed Snelson. 2013. Counterfactual Reasoning and Learning Systems: The Example of Computational Advertising. Journal of Machine Learning Research 14, 11 (2013). [6] Engin Bozdag, Qi Gao, Geert Jan Houben, and Martijn Warnier. 2014. Does Offline Political Segregation Affect the Filter Bubble? An Empirical Analysis of Information Diversity for Dutch and Turkish Twitter Users. Computers in Human Behavior 41, C (2014), 405-415. [7] Robin Burke. 2017. Multisided fairness for recommendation. arXiv preprint arXiv:1707.00093 (2017). [8] Jianxin Chang, Chen Gao, Yu Zheng, Yiqun Hui, Yanan Niu, Yang Song, Depeng Jin, and Yong Li. 2021. Sequential Recommendation with Graph Neural Networks. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval . 378-387. [9] Jiawei Chen, Hande Dong, Yang Qiu, Xiangnan He, Xin Xin, Liang Chen, Guli Lin, and Keping Yang. 2021. AutoDebias: Learning to Debias for Recommendation. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval . 21-30. [10] Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan He. 2023. Bias and debias in recommender system: A survey and future directions. ACM Transactions on Information Systems 41, 3 (2023), 1-39. [11] Tong Chen, Hongzhi Yin, Guanhua Ye, Zi Huang, Yang Wang, and Ming-Chieh Wang. 2020. Try This Instead: Personalized and Interpretable Substitute Recommendation. Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (2020). [12] Yongjun Chen, Zhiwei Liu, Jia Li, Julian McAuley, and Caiming Xiong. 2022. Intent Contrastive Learning for Sequential Recommendation. In Proceedings of the ACM Web Conference 2022 . 2172-2182. [13] David Maxwell Chickering. 2002. Optimal structure identification with greedy search. Journal of machine learning research 3, Nov (2002), 507-554. [14] Dan Cosley, Shyong K Lam, Istvan Albert, Joseph A Konstan, and John Riedl. 2003. Is seeing believing? How recommender system interfaces affect users' opinions. In Proceedings of the SIGCHI conference on Human factors in computing systems . 585-592. [15] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks for youtube recommendations. In RecSys . 191-198. [16] Sihao Ding, Peng Wu, Fuli Feng, Yitong Wang, Xiangnan He, Yong Liao, and Yongdong Zhang. 2022. Addressing unmeasured confounder for recommendation with sensitivity analysis. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 305-315. [17] Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. 2019. Graph neural networks for social recommendation. In The world wide web conference . 417-426. [18] Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. 2019. Graph neural networks for social recommendation. In The World Wide Web Conference . 417-426. [19] Tsu-Jui Fu, Xin Eric Wang, Matthew F Peterson, Scott T Grafton, Miguel P Eckstein, and William Yang Wang. 2020. Counterfactual vision-and-language navigation via adversarial path sampler. In European Conference on Computer Vision . Springer, 71-86. [20] Chen Gao, Xiangning Chen, Fuli Feng, Kai Zhao, Xiangnan He, Yong Li, and Depeng Jin. 2019. Cross-domain recommendation without sharing user-relevant data. In The world wide web conference . 491-502. [21] Chen Gao, Xiangnan He, Dahua Gan, Xiangning Chen, Fuli Feng, Yong Li, Tat-Seng Chua, and Depeng Jin. 2019. Neural multi-task recommendation from multi-behavior data. In 2019 IEEE 35th international conference on data engineering (ICDE) . IEEE, 1554-1557. [22] Chongming Gao, Shiqi Wang, Shijun Li, Jiawei Chen, Xiangnan He, Wenqiang Lei, Biao Li, Yuan Zhang, and Peng Jiang. 2023. CIRS: Bursting filter bubbles by counterfactual interactive recommender system. ACM Transactions on Information Systems 42, 1 (2023), 1-27. [23] Alois Gruson, Praveen Chandar, Christophe Charbuillet, James McInerney, Samantha Hansen, Damien Tardieu, and Ben Carterette. 2019. Offline Evaluation to Make Decisions About PlaylistRecommendation Algorithms. Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining (2019). [24] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. In Proceedings of the 26th International Joint Conference on Artificial Intelligence . 1725-1731. [25] Qingyu Guo, Fuzhen Zhuang, Chuan Qin, Hengshu Zhu, Xing Xie, Hui Xiong, and Qing He. 2020. A survey on knowledge graph-based recommender systems. IEEE Transactions on Knowledge and Data Engineering (2020). [26] Ruocheng Guo, Lu Cheng, Jundong Li, P Richard Hahn, and Huan Liu. 2020. A survey of learning causality with data: Problems and methods. ACM Computing Surveys (CSUR) 53, 4 (2020), 1-37. [27] Siyuan Guo, Lixin Zou, Yiding Liu, Wenwen Ye, Suqi Cheng, Shuaiqiang Wang, Hechang Chen, Dawei Yin, and Yi Chang. 2021. Enhanced Doubly Robust Learning for Debiasing Post-Click Conversion Rate Estimation. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval . 275-284. [28] Moritz Hardt, Eric Price, and Nathan Srebro. 2016. Equality of Opportunity in Supervised Learning. In NIPS . [29] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for recommendation. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval . 639-648. [30] Xiangnan He, Kuan Deng, Xiang Wang, Yaliang Li, Yongdong Zhang, and Meng Wang. 2020. LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation. Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (2020). [31] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural collaborative filtering. In WWW . 173-182. [32] Xiangnan He, Yang Zhang, Fuli Feng, Chonggang Song, Lingling Yi, Guohui Ling, and Yongdong Zhang. 2023. Addressing confounding feature issue for causal recommendation. ACM Transactions on Information Systems 41, 3 (2023), 1-23. [33] Yue He, Zimu Wang, Peng Cui, Hao Zou, Yafeng Zhang, Qiang Cui, and Yong Jiang. 2022. Causpref: Causal preference learning for out-of-distribution recommendation. In Proceedings of the ACM Web Conference 2022 . 410-421. [34] David Heckerman, Dan Geiger, and David M Chickering. 1995. Learning Bayesian networks: The combination of knowledge and statistical data. Machine learning 20, 3 (1995), 197-243. [35] Keisuke Hirano, Guido W Imbens, and Geert Ridder. 2003. Efficient estimation of average treatment effects using the estimated propensity score. Econometrica 71, 4 (2003), 1161-1189. [36] Jonathan Ho and Stefano Ermon. 2016. Generative adversarial imitation learning. Advances in neural information processing systems 29 (2016). [37] Guangneng Hu, Yu Zhang, and Qiang Yang. 2018. Conet: Collaborative cross networks for cross-domain recommendation. In Proceedings of the 27th ACM international conference on information and knowledge management . 667-676. [38] Yifan Hu, Yehuda Koren, and Chris Volinsky. 2008. Collaborative filtering for implicit feedback datasets. In ICDM . 263-272. [39] Eugene Ie, Chih-wei Hsu, Martin Mladenov, Vihan Jain, Sanmit Narvekar, Jing Wang, Rui Wu, and Craig Boutilier. 2019. Recsim: A configurable simulation platform for recommender systems. arXiv preprint arXiv:1909.04847 (2019). [40] Dominik Janzing. 2019. Causal Regularization. Advances in Neural Information Processing Systems 32 (2019), 1270412714. [41] Bowen Jin, Chen Gao, Xiangnan He, Depeng Jin, and Yong Li. 2020. Multi-behavior recommendation with graph convolutional networks. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . 659-668. [42] Fredrik Johansson, Uri Shalit, and David Sontag. 2016. Learning representations for counterfactual inference. In International conference on machine learning . PMLR, 3020-3029. [43] Nicolas Jones, Armelle Brun, and Anne Boyer. 2011. Comparisons instead of ratings: Towards more stable preferences. In 2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology , Vol. 1. IEEE, 451-456. [44] Junzhe Zhang and Elias Bareinboim. 2018. Equality of Opportunity in Classification: A Causal Approach. In NeurIPS . [45] Aria Khademi, Sanghack Lee, David Foley, and Vasant G Honavar. 2019. Fairness in Algorithmic Decision Making: An Excursion Through the Lens of Causality. The World Wide Web Conference (2019). [46] Haruka Kiyohara, Yuta Saito, Tatsuya Matsuhiro, Yusuke Narita, Nobuyuki Shimizu, and Yasuo Yamamoto. 2022. Doubly robust off-policy evaluation for ranking policies under the cascade behavior model. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining . 487-497. [47] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization techniques for recommender systems. Computer 42, 8 (2009). [48] Matt J. Kusner, Joshua R. Loftus, Chris Russell, and Ricardo Silva. 2017. Counterfactual Fairness. In NIPS . [49] Finnian Lattimore, Tor Lattimore, and Mark D Reid. 2016. Causal bandits: learning good interventions via causal inference. In Proceedings of the 30th International Conference on Neural Information Processing Systems . 1189-1197. [50] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. 2020. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643 (2020). [51] Dongsheng Li, Chao Chen, Zhilin Gong, Tun Lu, Stephen M Chu, and Ning Gu. 2019. Collaborative filtering with noisy ratings. In Proceedings of the 2019 SIAM International Conference on Data Mining . SIAM, 747-755. [52] Qian Li, Xiangmeng Wang, Zhichao Wang, and Guandong Xu. 2023. Be causal: De-biasing social network confounding in recommendation. ACM Transactions on Knowledge Discovery from Data 17, 1 (2023), 1-23. [53] Shuai Li, Yasin Abbasi-Yadkori, Branislav Kveton, Shan Muthukrishnan, Vishwa Vinay, and Zheng Wen. 2018. Offline evaluation of ranking policies with click models. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 1685-1694. [54] Yunqi Li, Hanxiong Chen, Shuyuan Xu, Yingqiang Ge, and Yongfeng Zhang. 2021. Towards Personalized Fairness Based on Causal Notion. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1054-1063. [55] Chen Lin, Xinyi Liu, Guipeng Xv, and Hui Li. 2021. Mitigating sentiment bias for recommender systems. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval . 31-40. [56] Roderick JA Little and Donald B Rubin. 2019. Statistical analysis with missing data . Vol. 793. John Wiley & Sons. [57] Chang Liu, Xinwei Sun, Jindong Wang, Haoyue Tang, Tao Li, Tao Qin, Wei Chen, and Tie-Yan Liu. 2020. Learning causal semantic representation for out-of-distribution prediction. arXiv preprint arXiv:2011.01681 (2020). [58] Yaxu Liu, Jui-Nan Yen, Bowen Yuan, Rundong Shi, Peng Yan, and Chih-Jen Lin. 2022. Practical counterfactual policy learning for Top-K recommendations. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 1141-1151. [59] Christos Louizos, Uri Shalit, Joris Mooij, David Sontag, Richard Zemel, and Max Welling. 2017. Causal effect inference with deep latent-variable models. In Proceedings of the 31st International Conference on Neural Information Processing Systems . 6449-6459. [60] Hongyu Lu, Min Zhang, and Shaoping Ma. 2018. Between clicks and satisfaction: Study on multi-phase user preferences and satisfaction for online news reading. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval . 435-444. [61] G. M. Lunardi, G. M. Machado, V. Maran, and JPMD Oliveira. 2020. A metric for Filter Bubble measurement in recommender algorithms considering the news domain. Applied Soft Computing 97, Part A (2020). [62] James McInerney, Brian Brost, Praveen Chandar, Rishabh Mehrotra, and Benjamin Carterette. 2020. Counterfactual evaluation of slate recommendations with sequential reward interactions. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 1779-1788. [63] Raha Moraffah, Mansooreh Karami, Ruocheng Guo, Adrienne Raglin, and Huan Liu. 2020. Causal interpretability for machine learning-problems, methods and evaluation. ACM SIGKDD Explorations Newsletter 22, 1 (2020), 18-33. [64] Shanlei Mu, Yaliang Li, Wayne Xin Zhao, Jingyuan Wang, Bolin Ding, and Ji-Rong Wen. 2022. Alleviating Spurious Correlations in Knowledge-aware Recommendations through Counterfactual Generator. In SIGIR . [65] Michael P. O'Mahony, Neil J. Hurley, and Gu\u00e9nol\u00e9 C.M. Silvestre. 2006. Detecting Noise in Recommender System Databases. In Proceedings of the 11th International Conference on Intelligent User Interfaces (IUI '06) . Association for Computing Machinery, New York, NY, USA, 109-115. [66] Eli Pariser. 2011. The filter bubble: What the Internet is hiding from you . penguin UK. [67] E. Pariser. 2011. The Filter Bubble: What the Internet Is Hiding from You . The Filter Bubble: What the Internet Is Hiding from You. [68] J. Passe, C. Drake, and L. Mayger. 2017. Homophily, echo chambers, & selective exposure in social networks: What should civic educators do? Journal of Social Studies Research (2017). [138] Yang Zhang, Fuli Feng, Xiangnan He, Tianxin Wei, Chonggang Song, Guohui Ling, and Yongdong Zhang. 2021. Causal Intervention for Leveraging Popularity Bias in Recommendation. In SIGIR '21: The 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, Virtual Event, Canada, July 11-15, 2021 . 11-20. [139] Yongfeng Zhang, Guokun Lai, Min Zhang, Yi Zhang, Yiqun Liu, and Shaoping Ma. 2014. Explicit factor models for explainable recommendation based on phrase-level sentiment analysis. Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval (2014). [140] Yu Zheng, Chen Gao, Jianxin Chang, Yanan Niu, Yang Song, Depeng Jin, and Yong Li. 2022. Disentangling Long and Short-Term Interests for Recommendation. In Proceedings of the ACM Web Conference 2022 . 2256-2267. [141] Yu Zheng, Chen Gao, Xiang Li, Xiangnan He, Yong Li, and Depeng Jin. 2021. Disentangling User Interest and Conformity for Recommendation with Causal Embedding. In WWW . ACM, 2980-2991. [142] Shengyu Zhu, Ignavier Ng, and Zhitang Chen. 2019. Causal discovery with reinforcement learning. arXiv preprint arXiv:1906.04477 (2019). [143] Tianyu Zhu, Leilei Sun, and Guoqing Chen. 2021. Graph-based Embedding Smoothing for Sequential Recommendation. IEEE Transactions on Knowledge and Data Engineering (2021). [144] Xinyuan Zhu, Yang Zhang, Fuli Feng, Xun Yang, Dingxian Wang, and Xiangnan He. 2022. Mitigating Hidden Confounding Effects for Causal Recommendation. arXiv preprint arXiv:2205.07499 (2022). [145] Ran Zmigrod, Sabrina J Mielke, Hanna Wallach, and Ryan Cotterell. 2019. Counterfactual data augmentation for mitigating gender stereotypes in languages with rich morphology. arXiv preprint arXiv:1906.04571 (2019)."}
