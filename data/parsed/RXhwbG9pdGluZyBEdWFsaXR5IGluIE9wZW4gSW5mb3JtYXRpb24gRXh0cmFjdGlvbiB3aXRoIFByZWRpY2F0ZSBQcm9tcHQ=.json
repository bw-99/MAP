{
  "Exploiting Duality in Open Information Extraction with Predicate Prompt": "",
  "Zhen Chen": "",
  "∗": "zhenchen21@m.fudan.edu.cn Fudan University, Shanghai Key Laboratory of Data Science Shanghai, China",
  "Yanghua Xiao": "",
  "Huimin Xu": "shawyh@fudan.edu.cn Fudan University, Shanghai Key Laboratory of Data Science Shanghai, China Jingping Liu jingpingliu@ecust.edu.cn East China University of Science and Technology Shanghai, China Deqing Yang yangdeqing@fudan.edu.cn Fudan University, Shanghai Key Laboratory of Data Science Shanghai, China",
  "Zongyu Wang": "xuhuimin04@meituan.com Meituan Shanghai, China",
  "Rui Xie": "wangzongyu02@meituan.com Meituan Shanghai, China",
  "Yunsen Xian": "rui.xie@meituan.com Meituan Shanghai, China",
  "ABSTRACT": "Open information extraction (OpenIE) aims to extract the schemafree triplets in the form of ( subject , predicate , object ) from a given sentence. Compared with general information extraction (IE), OpenIE poses more challenges for the IE models, especially when multiple complicated triplets exist in a sentence. To extract these complicated triplets more effectively, in this paper we propose a novel generative OpenIE model, namely DualOIE , which achieves a dual task at the same time as extracting some triplets from the sentence, i.e., converting the triplets into the sentence. Such dual task encourages the model to correctly recognize the structure of the given sentence and thus is helpful to extract all potential triplets from the sentence. Specifically, DualOIE extracts the triplets in two steps: 1) first extracting a sequence of all potential predicates, 2) then using the predicate sequence as a prompt to induce the generation of triplets. Our experiments on two benchmarks and our dataset constructed from Meituan demonstrate that DualOIE achieves the best performance among the state-of-the-art baselines. Furthermore, the online A/B test on Meituan platform shows that 0.93% improvement of QV-CTR and 0.56% improvement of UV-CTR have been obtained when the triplets extracted by DualOIE were leveraged in Meituan's search system. ∗ Corresponding Author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WSDM '24, March 4-8, 2024, Merida, Mexico © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0371-3/24/03...$15.00 https://doi.org/10.1145/3616855.3635799 xianyunsen@meituan.com Meituan Shanghai, China",
  "CCS CONCEPTS": "· Computing methodologies → Information extraction .",
  "KEYWORDS": "OpenIE, dual task, prompt, generative model.",
  "ACMReference Format:": "Zhen Chen, Jingping Liu, Deqing Yang ∗ , Yanghua Xiao, Huimin Xu, Zongyu Wang, Rui Xie, and Yunsen Xian. 2024. Exploiting Duality in Open Information Extraction with Predicate Prompt. In Proceedings of the 17th ACM International Conference on Web Search and Data Mining (WSDM '24), March 4-8, 2024, Merida, Mexico. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3616855.3635799",
  "1 INTRODUCTION": "Open information extraction (OpenIE) plays an important role in a variety of downstream tasks, such as knowledge base construction [8], question answering [26] and summarization [5]. OpenIE aims to extract schema-free triplets in the form of ( subject , predicate , object ) from unstructured natural language, where subjects and objects are both called arguments. In recent years, most OpenIE systems were devised based on deep neural networks (DNNs), which could be divided into two main categories: tagging-based and generative methods [29]. The tagging-based solutions [21] model OpenIE as a sequence labeling problem, where each token in the input is tagged as a subject, predicate or object. The generative solutions model OpenIE as a Seq2Seq problem. For example, IMoJIE [14] adopts an iterative generation mechanism to alleviate redundancy in extractions.Gen2OIE [15] leverages two mT5 models [25] to extract triplets by reconstructing multiple inputs. However, the previous models' inadequate understanding of diverse relations between arguments and the sentence's intricate structure hinder them to extract the complicated triplets WSDM'24, March 4-8, 2024, Merida, Mexico Zhen Chen et al.",
  "Table 1: A toy example of OpenIE where three triplets can be extracted from the input sentence.": "Sentence : Shea was born on September 5, 1900 in San Francisco, California.",
  "Triplets :": "1. (Shea, was born on, [September 5, 1900]) 2. (Shea, was born in, [San Francisco, California]) 3. (San Francisco, is in, California) effectively, which include the following three categories [27]. 1) Overlapping triplets refer to those triplets sharing the same element, as the first two triplets in Table 1 sharing the same subject 'Shea'; 2) Discontinuous triplet has the element composed by two separate spans, as the second triplet's predicate in Table 1 that composed of 'was born' and 'in' from the input; 3) Two triplets are regarded as nested triplet , if an element in one triplet contains other element or shares some words with the other triplet's element. For example, the predicates of the first two triplets in Table 1 share the same word 'was born'. In addition, the complicated triplets also include implicit triplets that are overlooked by current OpenIE research. The predicate in an implicit triplet is not explicitly mentioned in the sentence. As shown in Table 1, the predicate 'is in' of the third triplet is absent from the input. Implicit triplets often emerge in incomplete sentences of oral expression, such as user comments in real-world scenarios. Thus, the inefficiency in extracting implicit triplets would degrade the downstream task's performance severely. To address the challenges in OpenIE posed by these complicated triplets, in this paper we propose a novel generative OpenIE model DualOIE , which achieves enhanced OpenIE mainly through learning an auxiliary dual task of converting the triplets into a sentence. Since tagging model could not introduce new words, resulting in inefficiency in handling implicit triplets, our DualOIE adopts the paradigm of generation instead of tagging. Trained under the constraint of the dual task, our model is encouraged to capture the diverse relations between the arguments and recognize the overlapping elements in the overlapping triplet by figuring out how these elements fit into the context of the input sentence. In addition, the dual task also encourages the model to be more aware of word order and sentence structure, thus helping the model resolve the discontinuous triplet and nested triplet in the input sentence. For implicit triplets, transforming them into sentence enables the model to comprehend and learn those informal expressions, leading to more accurate identification of implicit triplets in the extraction task. In summary, our DualOIE is built with a dual framework with two task directions: the S (sentence) to T (triplet) direction corresponds to the primary extraction task, while the T (triplet) to S (sentence) direction corresponds to the dual task. The two directions share the same encoder, enabling the model to jointly learn from the two tasks. Specifically, in the S to T direction, given the sentence's complicated structure, directly using a Seq2Seq model to generate triplets from the input may result in incorrect output, such as repetition or omission. Therefore, we split the primary extraction process (S to T) into two steps: 1) first extracting a sequence of potential predicates from the input; 2) then using the predicate sequence as a prompt and concatenating it with the input to generate all triplets. Note that a subject of a triplet might be the object of another triplet, causing ambiguity in the second step, thus we first extract predicates as the prompt rather than extracting subjects or objects as the prompt. One important reason of previous OpenIE models' inefficiency in extracting implicit triplets is that, they are trained with the datasets only having less implicit triplets. According to our statistics, there are only 3% and 11% implicit triplets in the OpenIE benchmark CaRB and SAOKE, respectively. Besides, the two datasets only have less than 30 types of implicit predicates, which also limits the performance of the models trained with them. The sparsity of implicit triplets in these two benchmarks is due to that, they were collected from Wikipedia and Baidu Baike, where the expressions in sentences are more formalized and complete than oral expressions in various Web platforms such as Meituan, which is a famous Chinese review and search platform for daily life. In order to train and evaluate OpenIE models towards implicit triplet extraction, we constructed a dataset MTOIE (MeituanOpenIE) from the real-world scenario of Meituan. MTOIE was collected from the massive user reviews on Meituan, annotated following the philosophy of previous OpenIE benchmarks and crowdsourced by experienced workers. To sum up, MTOIE has 54,060 sentences and 87,971 triplets, in which there are about 33% implicit triplets and more than 2,000 types of implicit predicates. This paper's contributions are summarized as follows: 1. We propose a novel OpenIE model based on a joint dual framework with two task directions, which enables the model to learn under the mutual constraints of both tasks and further use predicate prompt to alleviate the omission and repetition in the S to T direction. To the best of our knowledge, this is the first work to introduce duality in OpenIE. 2. To evaluate OpenIE models on complicated triplet extraction, we also construct a high-quality dataset MTOIE that contains more than 29,000 implicit triplets and more than 2,000 types of implicit predicates, which exceeds current benchmarks by a significant margin in terms of both quantity and variety. 3. We conduct extensive experiments on two benchmark datasets and our MTOIE, to verify our model's advantage over all baselines, including ChatGPT. Furthermore, the online A/B test on the Meituan platform shows that the query view click-through rate (QV-CTR) and unique visitor click-through rate (UV-CTR) have increased by 0.93% and 0.56% respectively when our model's extraction results were deployed into the search system. The source code of DualOIE will be soon available at https://github.com/ ccczhen/DualOIE.",
  "2 OVERVIEW": "In this section, we first formalize the problem of OpenIE task addressed in this paper, and then present the framework of our proposed DualOIE.",
  "2.1 Problem Definition": "Given an input sentence 𝑥 = [ 𝑤 1 , ..., 𝑤 𝑛 ] , where 𝑤 𝑖 (1 ≤ 𝑖 ≤ 𝑛 ) is the 𝑖 -th word (token), our task's goal is to extract a collection of triplets 𝑌 = {( 𝑠 𝑖 , 𝑝 𝑖 , 𝑜 𝑖 )} | 𝑌 | 𝑖 = 1 where 𝑠 𝑖 , 𝑝 𝑖 , 𝑜 𝑖 denote the subject term, predicate Exploiting Duality in Open Information Extraction with Predicate Prompt WSDM'24, March 4-8, 2024, Merida, Mexico Concatenation Figure 1: The overall framework of our proposed DualOIE, including the structure of achieving two tasks of opposite directions, S → T and T → S. Triplet Decoder Predicate Decoder Text Decoder Sentence Input 𝒙𝒙 𝑷𝑷 Concatenated Input 𝒙𝒙 𝑻𝑻 Predicate Prompt 𝒚𝒚 𝑷𝑷 Triplets Output 𝒚𝒚 𝑻𝑻 Triplets Input 𝒙𝒙 𝑺𝑺 Sentence Output 𝒚𝒚 𝑺𝑺 + Transformer Block 1 Transformer Block L . . . . . . Shared Encoder 𝜶𝜶𝓛𝓛 𝑷𝑷 𝜷𝜷𝓛𝓛 𝑻𝑻 𝜸𝜸𝓛𝓛 𝑺𝑺 + 𝓛𝓛 𝑺𝑺→𝑻𝑻 𝓛𝓛 𝑺𝑺→𝑻𝑻 S → T Step 2 T → S Step 1 𝒙𝒙 𝑻𝑻 𝒚𝒚 𝒚𝒚 𝑷𝑷 𝑻𝑻 𝒚𝒚 𝑺𝑺 𝒙𝒙 𝑺𝑺 𝒙𝒙 𝑷𝑷 term, object term of the 𝑖 -th triplet, respectively. Generally, 𝑠 and 𝑜 might be a single word or a span extracted from 𝑥 , and 𝑝 might even be absent in 𝑥 . For example, given ' Joe Biden (November 20, 1942- ) is the U.S. president. \", a good model needs to output two triplets, i.e., ( Joe Biden , is , the U.S. president. ) and ( Joe Biden , was born on , [November 20, 1942] ). Note that the predicate of the second triplet is absent in the input sentence.",
  "2.2 Framework": "Given an input sentence 𝑥 , the primary objective of our OpenIE model is to maximize the following probability  where 𝑌 is the collection of fact triplets. It is in fact the task of S to T direction mentioned before, i.e., extracting triplets from the input sentence. This direction contains two steps: 1) a prompt 𝑧 = [ 𝑝 1 , ..., 𝑝 | 𝑌 | ] is first extracted from 𝑥 as a prompt, where 𝑝 𝑖 (1 ≤ 𝑖 ≤| 𝑌 | ) is the predicate of the 𝑖 -th triplet in 𝑌 ; 2) then 𝑥 is concatenated with 𝑧 as [ 𝑧 ; 𝑥 ] to generate 𝑌 . As the dual task of S to T , the T to S direction aims to convert the triplets into a sentence, of which the objective is to maximize  In our DualOIE, these two directions have their respective decoders but are combined with a shared encoder. During DualOIE's training, the overall loss is defined as:  where L 𝑆 → 𝑇 and L 𝑇 → 𝑆 are the loss of S to T and T to S (see Section 4.1 for details), respectively.",
  "3 METHODOLOGY": "In this section, we describe our DualOIE's details for the two task directions. The overall structure of DualOIE is depicted in Fig. 1.",
  "3.1 Sentence to Triplets": "As mentioned before, this direction is the primary task of our model which aims to generate triplets 𝑌 based on the input sentence 𝑥 . We introduce the two steps of this direction in turn as follows. Specifically, the triplet extraction is divided into two steps: 1) extract the predicate sequence from the input. 2) use the predicate sequence to induce the generation of triplets. 3.1.1 Predicate Extraction. This first step aims to extract a sequence of predicates 𝑦 𝑃 from the sentence 𝑥 . Predicate extraction is designed upon a Transformer encoder of 𝐿 blocks, along with a predicate decoder which is also a Transformer decoder. Formally, we first concatenate the input sentence 𝑥 with two special tokens to construct this step's input as:  Based on the encoder, we calculate the hidden state of 𝑥 𝑃 as:  Then, we use the predicate decoder to generate the predicate sequence 𝑦 𝑃 in an auto-regressive way:  where 𝑦 𝑖 is the 𝑖 -th token in 𝑦 𝑃 and h 𝑑 𝑖 is its decoder state. Please note that the extracted predicate sequence 𝑦 𝑃 might consist of multiple predicates, each of which is denoted as 𝑝 𝑖 and corresponds to one triplet. Thus, we use <rel> and </rel> to split them as below:  Finally, the loss function of predicate extraction is defined as:  where Ω 𝑃 is the training set of all sentence-predicates pairs, 𝜃 𝑒 and 𝜃 𝑑 𝑃 are the parameters of the encoder and the predicate decoder, respectively. 3.1.2 Triplet Extraction. This step aims to generate the triplets sequence 𝑌 based on 𝑥 𝑃 and 𝑦 𝑃 . Similar to the previous step, the triplet extraction also consists of an encoder and a triplet decoder. Note that the encoder is shared by the predicate extraction as well. Formally, given 𝑥 𝑃 and 𝑦 𝑃 from the former step, we take 𝑦 𝑃 as a prompt and concatenate it with 𝑥 𝑃 , to construct the input 𝑥 𝑇 of this step as:  𝐇𝐇 𝐇𝐇 𝑃𝑃 𝑑𝑑 𝑑𝑑 𝑇𝑇 𝐇𝐇 𝑑𝑑 𝑆𝑆 𝐇𝐇 𝐇𝐇 𝑃𝑃 𝑇𝑇 𝐇𝐇 𝑆𝑆 WSDM'24, March 4-8, 2024, Merida, Mexico Zhen Chen et al. Then, we use the shared encoder to get 𝑥 𝑇 's hidden state H 𝑇 , and decode the target sequence 𝑦 𝑇 based on H 𝑇 in the same way of Eq. (6). To split each extracted triplet, we utilize three pairs of special tokens to formulate a single triplet 𝑡 𝑖 in 𝑦 𝑇 as:  Similar to L 𝑃 defined in 8, the loss function of the triplet extraction L 𝑇 is defined as:  Note that the predicate prompt 𝑦 𝑃 enables DualOIE to avoid iterative generation and output all triplets in a single decoding process on the second step. In contrast, the number of decoding steps in IMoJIE and Gen2OIE depends on the number of triplets corresponding to the input. In addition, it prompts the model to generate triplets of the same number as the predicates.",
  "3.2 Triplets to Sentence": "This direction aims to reconstruct a sentence 𝑥 from the extracted triplets 𝑌 , which is composed of an encoder and a text decoder. Note that the encoder is shared by the S to T direction as well. We formulate the input of this direction as:  Next, we derive the hidden state H 𝑆 of 𝑥 𝑆 from the shared encoder. Then, we use the text decoder to decode the target sequence 𝑦 𝑆 in the same way of Eq. (6). And the output of this direction 𝑦 𝑆 is defined in the same way as 𝑥 𝑃 in Eq. (4). Finally, the loss function is defined as:  Note that the shared encoder would be optimized toward both directions, which enables DualOIE to benefit from the dual task simultaneously. In addition, the order of triplets in 𝑥 𝑆 is the same as 𝑌 , i.e., the appearance order of the predicate in the sentence.",
  "4 MODEL TRAINING AND INFERENCE": "In this section, we first introduce the training details of loss functions. Then, we describe the inference process of our model.",
  "4.1 Joint Training": "We employ a joint training way to S to T direction and T to S . The loss function of DualOIE is defined as:  where L 𝑆 → 𝑇 and L 𝑇 → 𝑆 are the loss of S to T and T to S . L 𝑃 , L 𝑇 and L 𝑆 are the loss of the predicate extraction, triplet extraction and sentence reconstruction, respectively. In addition, we also use hyper-parameters 𝛼 , 𝛽 and 𝛾 to balance different objectives. As a result, the overall loss of DualOIE is",
  "4.2 Model Inference": "When our model is trained well by the task of S to T and T to S , we only need to use S to T to solve the triplet extraction task inference phase. Finally, we split 𝑦 𝑇 into the triplets of 𝑌 by special tokens.",
  "5 EXPERIMENTS": "In this section, we display the results of our extensive experiments to verify DualOIE's advantage, based on which we also provide a detailed analysis.",
  "5.1 Datasets": "CaRB [4] is a crowdsourced English benchmark in OpenIE. However, it does not contain a training set due to the annotation cost, so we use the automatically annotated training data produced by IMoJIE [14]. SAOKE [23] is a large-scale human-annotated Chinese dataset. Note that 11% triplets in SAOKE are implicit. The statistics of two datasets are reported in Table 2.",
  "5.2 Baselines": "We compared our model against several recent neural OpenIE models, which could be divided into tagging models and generative models. 5.2.1 Tagging Models. RnnOIE [21] takes predicate head with the input together and outputs the tags indicating the token classes. SpanOIE [28] uses BiLSTM to derive the representation of a span, and then decoders tags from span representations. IGL-OIE [13] proposes an iterative grid labelling system to predict tag sequences. MacroIE [27] builds a fact graph based on token spans, and decodes the graph into fact triplets during the inference process. 5.2.2 Generative Models. NOIE [6] is built with an encoder-decoder structure based on stacked LSTM, where both the copy mechanism and attention mechanism are applied. We provided an advanced version of NOIE in our comparisons, of which the LSTM encoder is replaced with BERT. IMoJIE [14] is also composed of a BERT encoder and an LSTM decoder, and it leverages an iterative generation mechanism to alleviate redundant extraction. Gen2OIE [15] is an OpenIE system based on two mT5 models. Given that ChatGPT is a powerful large language model (LLM) capable of producing contextually appropriate and logically connected responses, We designed the prompts of 0-shot, 3-shot and Chain-Of-Thought (COT) [24] to instruct ChatGPT to achieve extraction task. Note that examples were randomly selected in the 3-shot setting.",
  "5.3 Evaluation Metrics": "F1(1-1) [4], is a token level scorer, which creates a label-prediction matching table and then computes precision and recall between Table 2: The statistics of CaRB and SAOKE. Exploiting Duality in Open Information Extraction with Predicate Prompt WSDM'24, March 4-8, 2024, Merida, Mexico Table 3: Main results on CaRB and SAOKE. Table 4: A toy example of the ablation study of DualOIE. Table 5: The results of DualOIE on the two datasets with different extraction order in the task of S to T. each pair. The overall precision and recall are computed through one-to-one mapping, where a gold triplet and a prediction could match each other only once. F1 [13] is a variant of F1(1-1), where the recall is computed through multi-to-one mapping.",
  "5.4 Implementation Detail": "During the training, we set the loss coefficient 𝛼 =0.4, 𝛽 =0.2, 𝛾 =0.6. For CaRB, we used T5 [17] encoder as the shared encoder, and T5 decoder as the predicate decoder, triplet decoder and text decoder, respectively. For SAOKE, we applied the Chinese T5-pegasus [22]. We trained our model using Pytorch on an NVIDIA Tesla V100 GPU with 32 GB dedicated memory. The network parameters were optimized by Adam [12] with a learning rate of 2e-5. The batch size was fixed to 32. The total training time was 5 hours. The final displayed results were reported as the average of the results of 5 random seeds. Figure 2: Performance on extraction of complicated triplets in SAOKE. Overlapping Discontinous Nested 50 55 60 F1 Score DuOIE w/o D DuOIE DuOIE w/o P",
  "5.5 Overall Comparison Results": "The comparison results between our DualOIE and the baselines are shown in Table 3. According to the results, we have the following conclusions. 1) DualOIE achieves the best extraction performance on the two datasets under each metric, fully justifying the effectiveness of both employing the dual task in joint training and the predicate prompt in extraction process. 2) On CaRB, there is always a gap of about 10% or more between the two metrics in the baselines, which is mainly caused by the low quality of automatically-derived training data. DualOIE decreases this gap to 4.8%, indicating that a triplet well-predicted by it is less mapped to several different gold triplets. In other words, the output of DualOIE is much more precise and robust in one-to-one mapping metric. 3) On SAOKE, DualOIE outperforms the best baseline by a large margin. In addition, note that generative models perform better overall than tagging models on SAOKE. This is because the implicit triplets in SAOKE were missed by the tagging models due to their inability to introduce new words. In addition, the gap between the two metrics is much smaller and more stable due to the high quality of SAOKE.",
  "5.6 In-depth Investigations": "5.6.1 Ablation Study. To evaluate the impact of each component in DualOIE, we compared it with two ablated variants: DualOIE w/o D whose dual task is removed, and DualOIE w/o P in which the predicate prompt is removed and the triplets are directly generated by the decoder. From the comparison results shown in Table 3, we find: 1) The two ablated variants still have an advantage over most baselines, justifying the effectiveness of incorporating either the predicate prompt or the dual task. 2) The removal of the dual task leads to an apparent performance drop on both datasets, i.e., (1.2%, 2.1%) on CaRB and (1.7%, 1.8%) on SAOKE. This proves the importance of the dual task for enhanced extraction. And the removal of the predicate prompt also decreases our model's performance. Fig. 2 shows the comparison results on complicated cases, which indicates that the introduction of duality could improve performance on these challenges. Since the syntactic knowledge endowed by the dual task is helpful to solve discontinuous and nested triplets, which arise from complicated sentence structure. In addition, the WSDM'24, March 4-8, 2024, Merida, Mexico Zhen Chen et al. Figure 3: The prompts we designed to instruct ChatGPT to perform the OpenIE task. You are an OpenIE extractor. Your objective is to extract fact triplets in the form of (subject, predicate, object) from the given sentence. I will give you an <Input sentence>, you should output triplets without numbering, i.e. <(s1,p1,o1);(s2,p2,o2)...> Examples:{examples} Optional Input sentence:{ input sentence } Output triplets:",
  "Few-shot Example": "Input sentence: Shea was born on September 5, 1900 in San Francisco, California. Output triplets: The explicit predicates of input are < was born on > and < was born in >, and the implicit predicate is < is >. Based on extracted predicates, the fact triplets are <(Shea, was born on, [September 5, 1900]); (Shea, was born in, [San Francisco, California]); (San Francisco, is in, California) >",
  "COT Example": "dual task encourages the model to capture diverse relations between arguments, which benefits the solving of overlapping. We also give a case example among ablated versions. As shown in Table 4, DualOIE correctly outputted both triplets. The removal of the duality prevented the model from effectively learning the structure of the sentence, resulting in confusion between the two triplets and outputting an incorrect triplet. Additionally, removing the predicate prompt caused the model to miss one of the triplets. 5.6.2 Analysis on Extraction Order. Wealso conducted experiments to investigate the effect of taking different terms in a triplet as the prompt. Table 5 details the performance of various prompt settings, where DualOIE-S(O) is the version where all subjects(objects) are extracted at first as the prompt, and then concatenated with the input to generate the triplets. From the table, we observe that the predicate prompt performs best followed by the subject prompt, and the object prompt performs worst. It is mainly due to the fact that a subject or object could appear in multiple triplets, and sometimes a subject in one triplet might even be an object in another triplet. Such complicated situations would confuse the model during triplets generation, while the predicate prompt could reduce this uncertainty since an extracted predicate corresponds to a certain triplet. 5.6.3 Impact of Loss Coefficient. As formalized in Eq. 15, the loss coefficient 𝛼 and 𝛽 indicate importance of the primary extraction task S to T , while 𝛾 indicates the importance of the dual task T to S . Thus, to analyze the importance of two task directions, we compared 𝛼 + 𝛽 with 𝛾 where 𝛾 𝛼 + 𝛽 was set to 0.5, 1 and 2, respectively. In addition, to analyze the importance of the two steps in S to T , we also compared 𝛼 with 𝛽 where the value ratio between them was set to 2, 3 and 4, respectively. From Table 6, we have the following conclusions. 1) The case of 𝛼 + 𝛽 = 𝛾 performs better, indicating that the dual task is as important as the primary task, but the balance between the two tasks should be kept. 2) the case of 𝛼 > 𝛽 shows its advantage, indicating that the predicate extraction is tougher and thus requires more concentrations. Besides, more concentrations on the predicate extraction could alleviate the cascading error in DualOIE's pipelinebased extraction framework. 5.6.4 Analysis on ChatGPT's Performance. We have examined the OpenIE performance of ChatGPT in three prompt settings: 0-shot, 3-shot and COT. Fig. 3 shows the main prompt and the COT example. Table 3's results indicate that ChatGPT is still competitive with fine-tuned baselines even in the 0-shot scenario. Compared with 3-shot prompt, COT prompt is more effective since it enables ChatGPT to learn the knack of extracting the predicate followed by extracting the subject and object, which is in fact an effective extraction manner adopted in our DualOIE. However, ChatGPT still shows less efficacy than DualOIE, since it sometimes fails to correctly comprehend the relations between arguments and tends to confuse the boundaries of arguments [11]. For example, given the input in Table 4, the output of ChatGPT(COT) is (He, idolized, receiving the name of 'God'), because it misjudges the predicate between arguments. In addition, we observe that ChatGPT sometimes generates triples that do not align with the input sentence, also possibly due to hallucination [2]. 5.6.5 Impact of Triplet Number. As illustrated in Fig. 4, it is harder for the extraction models to obtain satisfactory performance when the sentence's structure is getting more complicated, where the number of potential triplets increases. To further investigate the models' capabilities of handling the convoluted sentences with multiple triplets, we divided the samples (sentences) in SAOKE into 4 groups according to the triplet number in one sentence, and then evaluated all compared models in each group. Fig. 4 displays the relevant results where 𝑚 is the triplet number. It shows that DualOIE outperforms baselines on all groups by a large margin. Particularly, all baselines' performance degrades sharply when more triplets exist in a sentence. Comparatively, DualOIE's performance drop is slighter, justifying its stability of extracting triplets in complicated situations. 5.6.6 Correlation of the Two Tasks. We have also observed a phenomenon of mutual promotion between the dual task and the primary task (triplet extraction). We used BLEU [16] to evaluate the quality of the restored sentences generated by Tto S , and compared it with the F1 of S to T . The BLEU score varies between 0 and 1, showing how similar the generated text is to the gold text. The scatter and the regression line in Fig. 5 show the positive correlation between BLEU and F1, implying that two tasks could improve the performance of each other. Exploiting Duality in Open Information Extraction with Predicate Prompt WSDM'24, March 4-8, 2024, Merida, Mexico Table 6: Tuning results of loss coefficients on SAOKE. Figure 4: Performance comparisons on the groups with different triplet numbers ( 𝑚 ) of a sentence in SAOKE. 1 m 3 4 m 6 7 m 9 10 m 12 35 40 45 50 55 60 F1 Score DualOIE Gen2OIE IMoJIE NOIE+BERT \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 Table 7: Pred-F1 and Trip-F1 are defined identically to those in Sec.5.3. Gold refers to the situation where golden predicate prompt is provided. 5.6.7 Analysis on Cascading Error. Since we used a pipeline-style extraction structure in the T to S direction, it may cause the cascading error. Therefore, we designed an experiment to analyze the impact of the cascading error on final triplet extraction. As shown in Table 7, the F1 of the predicate extraction on two datasets are 65.5% and 65.7%. Given the prompt with golden predicates, there is 6.9% and 6.1% improvement in extraction results.",
  "6 CONSTRUCTION OF MTOIE": "MTOIE is a large-scale Chinese dataset and mainly focuses on noun-based attributes of POIs, which refer to the predicates in user comments, since the noun-based attributes often reflect customer attitudes to the POIs. We randomly collected sentences from user comments of POIs on the Meituan platform, covering domains like food, entertainment and accommodation. Then we sent these sentences to the annotators and asked them to annotate triples in the form of ( entity , attribute , attribute-value ). The annotation for Figure 5: Correlation analysis between the BLEU of T to S (X-axis) and the F1 of S to T (Y-axis). MTOIE is not easy for those inexperienced workers, especially for triples with implicit attributes. Thus, we employed 12 professional annotators served in Meituan. To obtain a high-quality dataset, we first trained them to annotate the triples with explicit attributes, and then evaluated their performance. Only the annotators achieving high evaluation scores were allowed to annotate the triplets with implicit attributes.",
  "6.1 Explicit Triplet Annotation": "The triplets with explicit attributes require the annotators to tag out the entity, attribute, and attribute-value in the sentence in order. To make the annotators quickly understand the definition of attributes, we leveraged a predefined attribute vocabulary 1 to match the attributes in the sentence and marked them to indicate possible attributes. Following the philosophy of previous OpenIE benchmarks [4, 23], we identified the following three guidelines for MTOIE annotation. Completeness Weaimto extract all triplets with marked attributes from the sentence. Firstly, the annotators must carefully examine all marked attributes, and annotate the entities and attribute-values corresponding to the attributes that were judged as correct. Then, the annotators checked again whether some attributes are missing from the marked attributes. Assertedness Theannotation for the triplets with explicit attributes follows the principle of 'literally honest', i.e., entity, attribute and 1 About 2,000 attributes accumulated by the Meituan company. WSDM'24, March 4-8, 2024, Merida, Mexico Zhen Chen et al. Algorithm 1 Iterative Annotation Framework. Require: 𝐷 𝑒𝑥 : annotated explicit samples; 𝐷 𝑢𝑛 : unannotated sentences. Ensure: 𝐷 𝑖𝑚 : annotated implicit samples; 1: Mask the attribute in the sentence of 𝐷 𝑒𝑥 to get the pseudo implicit samples 𝐷 𝑝𝑠 ; 2: for 𝑘 = 1 → 𝐾 do 3: Train OAE model 𝑀 𝑘 with 𝐷 𝑝𝑠 ; 4: Use model 𝑀 𝑘 to predict 𝐷 𝑢𝑛 and obtain semi-supervised samples; 5: Deliver semi-supervised samples to the annotators, get annotated implicit samples 𝐷 𝑖𝑚 ; 6: Update 𝐷 𝑝𝑠 with 𝐷 𝑖𝑚 ; 7: end for attribute-value must be asserted by the original sentence. Atomicity Each triplet is required to be an indivisible unit. The annotators must extract multiple atomic triplets from the sentence that has conjunctions. For example, given a sentence ' 面 包 有 两 种 口 味 ， 菠 萝 味和 芒 果 味 。 /Bread has two flavors, pineapple and mango.', the annotators should annotate ( 面 包 /Bread, 口 味 /flavors, 菠 萝 味 /pineapple) and ( 面 包 /Bread, 口 味 /flavors, 芒 果 味 /mango) instead of ( 面 包 /Bread, 口 味 /flavors, 菠 萝 味和 芒 果 味 /pineapple and mango).",
  "6.2 Implicit Triplet Annotation": "Implicit attributes are the most challenging part of the annotation. Manually annotating implicit attributes from the unstructured text tends to cause missing and incorrect annotations. Therefore, we proposed an iterative annotation framework to reduce the annotation difficulty, simplifying the 'manual inference' to 'model prediction along with by manual discrimination' (as shown in Algorithm 1). At first, we masked the attributes in the explicit samples 𝐷 𝑒𝑥 to obtain the pseudo implicit samples 𝐷 𝑝𝑠 . For example, given ' 这 家 游 泳池水 深 2 米 。 /The swimming pool is 2 meters deep', the original explicit triplet is ( 游 泳池 /The swimming pool, 水 深 /deep, 2 米 /2 meters), then we removed the attribute ' 水 深 /deep' from the sentence, and thus the triplet becomes a pseudo implicit one. Second, we trained the model 𝑀 based on 𝐷 𝑝𝑠 and then used 𝑀 to predict the unannotated sentences 𝐷 𝑢𝑛 and obtain the semisupervised samples. Then, the samples whose attributes do not appear in the sentences were delivered to the annotators. This method greatly reduces the annotation difficulty, and the annotators only need to judge whether the implicit triplets could be inferred from the sentence. Third, the annotated implicit samples were used to train the model 𝑀 continuously. After 𝐾 rounds of iteration, we obtained the final annotated implicit dataset. Following [7], we calculated the Cohen's Kappa to measure the agreements between annotators, and the Kappa score is 88.36%.",
  "6.3 Dataset Statistics": "In MTOIE, we have collected 54,060 Chinese sentences and 87,971 triplets in total. Among them, 15,137(28%) sentences contain implicit triplets, 29,290(33%) triplets are implicit, and there are 2109 types of implicit predicates. To estimate the overall quality of the dataset, two volunteers randomly picked 200 sentences from the dataset and evaluated them carefully. The result shows that the triple-level precision and recall are 94.6% and 85.5%, respectively.",
  "7 APPLICATION ON MEITUAN": "We have also evaluated our proposed DualOIE on our built MTOIE and verified its effectiveness for the real scenario, i.e., the search service on the Meituan Platform. Metric. Our concentration on POI attributes makes the triplets in MTOIE much shorter. Thus using the token-level metrics might overestimate model performance. Therefore, we adopted a more strict matching strategy, where each subject and object are evaluated by a full match. Besides, we used a semantic similarity model 2 to determine whether the predicted attribute and the gold have the same meaning. The threshold is set to 0.7. Comparison Results. We compared DualOIE's extraction performance on MTOIE with NOIE+BERT, IMoJIE and Gen2OIE. ChatGPT was not included as MTOIE has not been released yet. Using ChatGPT could lead to data leaks. The relevant results will be updated on GitHub once it is released in future. The results in Table 8 justify our DualOIE's advantage on both implicit triplets and all triplets under strict matching metrics. It shows that the relative performance improvement of DualOIE over the baselines on implicit triplets is more significant than that on all triplets, demonstrating DualOIE's stronger capability of extracting implicit information. We also compared all the models' efficiency through analyzing the number of sentences that the models can process per second, as shown in the last column of Table 8. As mentioned before, DualOIE shows an advantage over IMoJIE and Gen2OIE due to its decoding steps independent of the triplets number, while the two competitors spend more time on sentences with multiple triplets. Note that the single-turn decoding model NOIE+BERT is much faster than the other three generative methods, because it adopts beam search to output multiple triplets instead of a sequence of all triplets. Since the number of beams should be pre-decided, it fails to naturally adapt the extraction number to the input sentence. Furthermore, we display the extraction results of our DualOIE and two state-of-the-art baselines, i.e., IMoJIE and Gen2OIE, on some difficult cases. As shown in Table 9, for the first case, IMoJIE and Gen2OIE both miss a gold triplet. For the second case, IMoJIE predicts a wrong triplet and misses an implicit triplet, and Gen2OIE predicts the wrong subject of the implicit triplet. The two baselines' inferior results are due to their insufficient recognition of the input sentence's structure. By contrast, DualOIE generates more correct triplets with the help of the dual task. A/B Test. We also conducted an online A/B test to show how our model helps the Meituan platform improve search performance. Specifically, given a POI, we first used DualOIE to extract a collection of triplets from every user comment of it. Then, we used the semantic similarity model to normalize the collection as follows. 1) The similarity scores of the subject, attribute (predicate) and object were computed between two triplets, and they would be 2 https://github.com/shibing624/text2vec Exploiting Duality in Open Information Extraction with Predicate Prompt WSDM'24, March 4-8, 2024, Merida, Mexico Table 8: Triplet extraction comparisons of different models on MTOIE. Table 9: Extraction results for two cases where the incorrect terms are marked red. considered expressing the same fact if their scores are over (0.7, 0.7, 0.7). 2) The triplets with the same meaning were replaced by the most frequent one in the collection, and the reserved triplets were ranked by their frequencies. Finally, given a triplet such as ( 海 底 捞 /Haidilao ， 食 材 /food ， 新 鲜 /fresh) in the normalized collection, we concatenated the attribute ' 食 材 /food' with the object ' 新 鲜 /fresh' as the given POI's label ' 食 材 新 鲜 /food fresh'. For the online A/B test, we used two buckets where each bucket containing 25% randomly selected users. For one bucket, the system returned the search results using the labels to filter out POIs. For another bucket, the system directly returned the POIs by its default principle. We ran our A/B test on Meituan's searching system and compared daily average performance on query view click-through rate (QV-CTR) and unique visitor click-through rate (UV-CTR), which refer to the ratio of clicked queries to all queries and the ratio of clicked queries to all unique user-query pairs, respectively. After the running of 15 days, we noticed that QV-CTR and UV-CTR increased by 0.93% and 0.56%, respectively. These results indicate that the POI labels obtained through DualOIE indeed improve the performance of Meituan's search service.",
  "8 RELATED WORK": "Traditional OpenIE systems such as TextRunner [3], ReVerb [9], OLLIE [20], Stanford-IE [1], OpenIE-5 [19] and MinIE [10] are mainly based on rules and statistics, which are combined with some modules like POS taggers, SRL parsers and chunkers. In recent years, researchers mainly focus on neural solutions, which could be divided into two categories: tagging-based systems and generative systems. Tagging-based systems model OpenIE as a sequence labelling problem. For example, RnnOIE [21] introduces a custom BIO tagging scheme and takes the predicate head with the input together. SenseOIE [18] introduces the dependency tree, where the feature of a token is influenced by its parent and children. Besides, it also uses lexical and syntactic features such as POS embedding and syntactic role embedding. OpenIE6 [13] is built as an iterative grid labelling (IGL) system and achieves predictions in a 2D grid labelling way. SpanOIE [28] uses BiLSTM to derive the representation of a span, from which the tag of each token is predicted. In MacroIE [27], a fact graph is built based on the token spans, and decoded into fact triplets during the inference process. On the other hand, generative systems model OpenIE as a Seq2Seq problem, and thus are capable of introducing new words to deal with implicit triplets. NOIE [6] uses LSTM as both encoder and decoder, and introduces copy attention to address the out-of-vocabulary problem. In Logician [23], BiGRU is used as both encoder and decoder. As well, the coverage mechanism and gated dependency mechanism are both introduced. IMoJIE [14] uses BERT as encoder and LSTM as decoder. It leverages an iterative generation mechanism to alleviate the redundancy in triplets generation. Gen2OIE [15] is composed of two mT5 [25] models, which reconstructs multiple inputs based on the first model, and then uses the second model to generate triplets.",
  "9 CONCLUSION": "We propose a novel generative OpenIE model DualOIE based on an auxiliary dual task with predicate prompt. By exploiting duality, DualOIE performs better than the previous OpenIE models, since it can better understand the sentence structure and capture diverse relations between arguments. We also provide a high-quality OpenIE dataset MTOIE, which is constructed from the user comments on Meituan's platform. The experiments on public benchmarks and MTOIE justify DualOIE's superiority over the baselines. The online A/B test on Meituan platform also suggests the extracted triplets can promote the real-world search service.",
  "10 ACKNOWLEDGEMENT": "This paper was supported by Chinese NSF Major Research Plan No.92270121, NSF funding No.62306112, Shanghai Sailing Program No.23YF1409400, Shanghai Science and Technology Innovation Action Plan No.21511100401. WSDM'24, March 4-8, 2024, Merida, Mexico Zhen Chen et al.",
  "REFERENCES": "[1] Gabor Angeli, Melvin Jose Johnson Premkumar, and Christopher D Manning. 2015. Leveraging linguistic structure for open domain information extraction. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) . 344-354. [2] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. 2023. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023 (2023). [3] M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead, and O. Etzioni. 2007. Open information extraction from the web. Communications of the Acm (2007). [4] Sangnie Bhardwaj, Samarth Aggarwal, and Mausam Mausam. 2019. CaRB: A Crowdsourced Benchmark for Open IE. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) . Association for Computational Linguistics, Hong Kong, China, 6262-6267. https://doi.org/ 10.18653/v1/D19-1651 [5] Ziqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2018. Faithful to the original: Fact aware neural abstractive summarization. In thirty-second AAAI conference on artificial intelligence . [6] Lei Cui, Furu Wei, and Ming Zhou. 2018. Neural Open Information Extraction. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) . Association for Computational Linguistics, Melbourne, Australia, 407-413. https://doi.org/10.18653/v1/P18-2065 [7] Ning Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang, Xu Han, Pengjun Xie, Hai-Tao Zheng, and Zhiyuan Liu. 2021. Few-nerd: A few-shot named entity recognition dataset. arXiv preprint arXiv:2105.07464 (2021). [8] Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko Horn, Ni Lao, Kevin Murphy, Thomas Strohmann, Shaohua Sun, and Wei Zhang. 2014. Knowledge vault: Aweb-scale approach to probabilistic knowledge fusion. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining . 601-610. [9] Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information extraction. In Proceedings of the 2011 conference on empirical methods in natural language processing . 1535-1545. [10] Kiril Gashteovski, Rainer Gemulla, and Luciano del Corro. 2017. Minie: minimizing facts in open information extraction. Association for Computational Linguistics. [11] Ridong Han, Tao Peng, Chaohao Yang, Benyou Wang, Lu Liu, and Xiang Wan. 2023. Is Information Extraction Solved by ChatGPT? An Analysis of Performance, Evaluation Criteria, Robustness and Errors. arXiv preprint arXiv:2305.14450 (2023). [12] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014). [13] Keshav Kolluru, Vaibhav Adlakha, Samarth Aggarwal, Mausam, and Soumen Chakrabarti. 2020. OpenIE6: Iterative Grid Labeling and Coordination Analysis for Open Information Extraction. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . Association for Computational Linguistics, Online, 3748-3761. https://doi.org/10.18653/v1/2020.emnlpmain.306 [14] Keshav Kolluru, Samarth Aggarwal, Vipul Rathore, Mausam, and Soumen Chakrabarti. 2020. IMoJIE: Iterative Memory-Based Joint Open Information Extraction. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics, Online, 5871-5886. https://doi.org/10.18653/v1/2020.acl-main.521 [15] Keshav Kolluru, Muqeeth Mohammed, Shubham Mittal, Soumen Chakrabarti, et al. 2022. Alignment-Augmented Consistent Translation for Multilingual Open Information Extraction. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . 2502-2517. [16] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics . 311-318. [17] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res. 21, 140 (2020), 1-67. [18] Arpita Roy, Youngja Park, Taesung Lee, and Shimei Pan. 2019. Supervising unsupervised open information extraction models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) . 728-737. [19] Swarnadeep Saha, Harinder Pal, et al. 2017. Bootstrapping for numerical open ie. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) . 317-323. [20] Michael Schmitz, Stephen Soderland, Robert Bart, Oren Etzioni, et al. 2012. Open language learning for information extraction. In Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning . 523-534. [21] Gabriel Stanovsky, Julian Michael, Luke Zettlemoyer, and Ido Dagan. 2018. Supervised Open Information Extraction. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) . Association for Computational Linguistics, New Orleans, Louisiana, 885-895. https://doi.org/10.18653/v1/N181081 [22] Jianlin Su. 2021. T5 PEGASUS - ZhuiyiAI . Technical Report. https://github.com/ ZhuiyiTechnology/t5-pegasus [23] Mingming Sun, Xu Li, Xin Wang, Miao Fan, Yue Feng, and Ping Li. 2018. Logician: a unified end-to-end neural approach for open-domain information extraction. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining . 556-564. [24] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems 35 (2022), 24824-24837. [25] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2020. mT5: A massively multilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934 (2020). [26] Zhao Yan, Duyu Tang, Nan Duan, Shujie Liu, Wendi Wang, Daxin Jiang, Ming Zhou, and Zhoujun Li. 2018. Assertion-based QA with question-aware open information extraction. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 32. [27] Bowen Yu, Yucheng Wang, Tingwen Liu, Hongsong Zhu, Limin Sun, and Bin Wang. 2021. Maximal clique based non-autoregressive open information extraction. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing . 9696-9706. [28] Junlang Zhan and Hai Zhao. 2020. Span model for open information extraction on accurate corpus. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 34. 9523-9530. [29] Shaowen Zhou, Bowen Yu, Aixin Sun, Cheng Long, Jingyang Li, and Jian Sun. 2022. A Survey on Neural Open Information Extraction: Current Status and Future Directions. In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22 , Lud De Raedt (Ed.). International Joint Conferences on Artificial Intelligence Organization, 5694-5701. https: //doi.org/10.24963/ijcai.2022/793 Survey Track.",
  "keywords_parsed": [
    "OpenIE",
    "dual task",
    "prompt",
    "generative model"
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "Leveraging linguistic structure for open domain information extraction"
    },
    {
      "ref_id": "b2",
      "title": "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity"
    },
    {
      "ref_id": "b3",
      "title": "Open information extraction from the web"
    },
    {
      "ref_id": "b4",
      "title": "CaRB: A Crowdsourced Benchmark for Open IE"
    },
    {
      "ref_id": "b5",
      "title": "Faithful to the original: Fact aware neural abstractive summarization"
    },
    {
      "ref_id": "b6",
      "title": "Neural Open Information Extraction"
    },
    {
      "ref_id": "b7",
      "title": "Few-nerd: A few-shot named entity recognition dataset"
    },
    {
      "ref_id": "b8",
      "title": "Knowledge vault: Aweb-scale approach to probabilistic knowledge fusion"
    },
    {
      "ref_id": "b9",
      "title": "Identifying relations for open information extraction"
    },
    {
      "ref_id": "b10",
      "title": "Minie: minimizing facts in open information extraction"
    },
    {
      "ref_id": "b11",
      "title": "Is Information Extraction Solved by ChatGPT? An Analysis of Performance, Evaluation Criteria, Robustness and Errors"
    },
    {
      "ref_id": "b12",
      "title": "Adam: A method for stochastic optimization"
    },
    {
      "ref_id": "b13",
      "title": "OpenIE6: Iterative Grid Labeling and Coordination Analysis for Open Information Extraction"
    },
    {
      "ref_id": "b14",
      "title": "IMoJIE: Iterative Memory-Based Joint Open Information Extraction"
    },
    {
      "ref_id": "b15",
      "title": "Alignment-Augmented Consistent Translation for Multilingual Open Information Extraction"
    },
    {
      "ref_id": "b16",
      "title": "Bleu: a method for automatic evaluation of machine translation"
    },
    {
      "ref_id": "b17",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer"
    },
    {
      "ref_id": "b18",
      "title": "Supervising unsupervised open information extraction models"
    },
    {
      "ref_id": "b19",
      "title": "Bootstrapping for numerical open ie"
    },
    {
      "ref_id": "b20",
      "title": "Open language learning for information extraction"
    },
    {
      "ref_id": "b21",
      "title": "Supervised Open Information Extraction"
    },
    {
      "ref_id": "b22",
      "title": "T5 PEGASUS - ZhuiyiAI"
    },
    {
      "ref_id": "b23",
      "title": "Logician: a unified end-to-end neural approach for open-domain information extraction"
    },
    {
      "ref_id": "b24",
      "title": "Chain-of-thought prompting elicits reasoning in large language models"
    },
    {
      "ref_id": "b25",
      "title": "mT5: A massively multilingual pre-trained text-to-text transformer"
    },
    {
      "ref_id": "b26",
      "title": "Assertion-based QA with question-aware open information extraction"
    },
    {
      "ref_id": "b27",
      "title": "Maximal clique based non-autoregressive open information extraction"
    },
    {
      "ref_id": "b28",
      "title": "Span model for open information extraction on accurate corpus"
    },
    {
      "ref_id": "b29",
      "title": "A Survey on Neural Open Information Extraction: Current Status and Future Directions"
    }
  ]
}