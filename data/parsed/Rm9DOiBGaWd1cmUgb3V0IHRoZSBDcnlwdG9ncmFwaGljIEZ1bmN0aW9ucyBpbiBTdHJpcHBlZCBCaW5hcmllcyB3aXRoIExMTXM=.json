{
  "FoC: Figure out the Cryptographic Functions in Stripped Binaries with LLMs": "XIUWEI SHANG ∗ , University of Science and Technology of China, Hefei, China GUOQIANG CHEN ∗ , QI-ANXIN Technology Research Institute, Beijing, China SHAOYIN CHENG † , University of Science and Technology of China, Anhui Province Key Laboratory of Digital Security, Hefei, China SHIKAI GUO, Dalian Maritime University, The Dalian Key Laboratory of Artificial Intelligence, Dalian, China YANMING ZHANG, University of Science and Technology of China, Hefei, China WEIMING ZHANG, University of Science and Technology of China, Anhui Province Key Laboratory of Digital Security, Hefei, China NENGHAI YU, University of Science and Technology of China, Anhui Province Key Laboratory of Digital Security, Hefei, China Analyzing the behavior of cryptographic functions in stripped binaries is a challenging but essential task, which is crucial in software security fields such as malware analysis and legacy code inspection. However, the inherent high logical complexity of cryptographic algorithms makes their analysis more difficult than that of ordinary code, and the general absence of symbolic information in binaries exacerbates this challenge. Existing methods for cryptographic algorithm identification frequently rely on data or structural pattern matching, which limits their generality and effectiveness while requiring substantial manual effort. In response to these challenges, we present FoC ( F igure o ut the C ryptographic functions), a novel framework that leverages large language models (LLMs) to identify and analyze cryptographic functions in stripped binaries. In FoC, we first build an LLM-based generative model (FoC-BinLLM) to summarize the semantics of cryptographic functions in natural language form, which is intuitively readable to analysts. Subsequently, based on the semantic insights provided by FoC-BinLLM, we further develop a binary code similarity detection model (FoC-Sim), which allows analysts to effectively retrieve similar implementations of unknown cryptographic functions from a library of known cryptographic functions. The predictions of generative model like FoCBinLLM are inherently difficult to reflect minor alterations in binary code, such as those introduced by vulnerability patches. In contrast, the change-sensitive representations generated by FoC-Sim compensate for the shortcomings to some extent. To support the development and evaluation of these models, and to Both authors contributed equally to this research. ∗ † Corresponding author. Authors' addresses: Xiuwei Shang, shangxw@mail.ustc.edu.cn, University of Science and Technology of China, Hefei, China; Guoqiang Chen, guoqiangchen@qianxin.com, QI-ANXIN Technology Research Institute, Beijing, China; Shaoyin Cheng, sycheng@ustc.edu.cn, University of Science and Technology of China, Anhui Province Key Laboratory of Digital Security, Hefei, China; Shikai Guo, shikai.guo@dlmu.edu.cn, Dalian Maritime University, The Dalian Key Laboratory of Artificial Intelligence, Dalian, China; Yanming Zhang, azesinter@mail.ustc.edu.cn, University of Science and Technology of China, Hefei, China; Weiming Zhang, zhangwm@ustc.edu.cn, University of Science and Technology of China, Anhui Province Key Laboratory of Digital Security, Hefei, China; Nenghai Yu, ynh@ustc.edu.cn, University of Science and Technology of China, Anhui Province Key Laboratory of Digital Security, Hefei, China. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. © 2024 Association for Computing Machinery. 0004-5411/2024/7-ART $15.00 https://doi.org/10.1145/1122445.1122456 J. ACM, Vol. 37, No. 4, Article . Publication date: July 2024. 2 Shang et al. facilitate further research in this domain, we also construct a comprehensive cryptographic binary dataset and introduce an automatic method to create semantic labels for extensive binary functions. Our evaluation results are promising. FoC-BinLLM outperforms ChatGPT by 14.61% on the ROUGE-L score, demonstrating superior capability in summarizing the semantics of cryptographic functions. FoC-Sim also surpasses previous best methods with a 52% higher Recall@1 in retrieving similar cryptographic functions. Beyond these metrics, our method has proven its practical utility in real-world scenarios, including cryptographic-related virus analysis and 1-day vulnerability detection. CCS Concepts: · Software and its engineering → Software reverse engineering ; · Theory of computation → Program analysis ; · Computing methodologies → Artificial intelligence . Additional Key Words and Phrases: Binary Code Summarization, Cryptographic Algorithm Identification, Binary Code Similarity Detection, Large Language Models",
  "ACMReference Format:": "Xiuwei Shang, Guoqiang Chen, Shaoyin Cheng, Shikai Guo, Yanming Zhang, Weiming Zhang, and Nenghai Yu. 2024. FoC: Figure out the Cryptographic Functions in Stripped Binaries with LLMs. J. ACM 37, 4 (July 2024), 38 pages. https://doi.org/10.1145/1122445.1122456",
  "1 INTRODUCTION": "Cryptography algorithms play a crucial role in computer security. Analyzing the cryptographyrelated code in stripped binaries without access to their source code is common and critical in software reverse engineering. This analysis is essential for tasks such as examining viruses with encryption capabilities, checking for weak cryptographic implementations in legacy software, and verifying compliance with privacy encryption standards. Unfortunately, the difficulty of understanding their binary code is exacerbated by the complex logic of cryptographic algorithms and the absence of symbolic information in stripped binaries. Although modern decompilers (e.g., IDA Pro [1], Ghidra [2]) can heuristically convert binary code into C-like pseudo-code, this remains a challenging task as it still lacks sufficient human-readable semantic information. In this context, three existing technical routes show partial potential in addressing the challenges of analyzing cryptographic functions in stripped binaries. Cryptography-Oriented Heuristics Methods. There are some methods specifically designed to provide semantic information about cryptographic algorithms present in binary code. Current methods for cryptographic algorithm identification, which provide the primitive classes contained within binaries, utilize various approaches such as constants [3-5], statistics [6-9], structures [10-15], and others. These methods employ static or dynamic analysis to identify distinct patterns to detect cryptographic implementations within binaries. However, various factors affecting binary code (e.g., hidden constant features and compilation optimizations) can undermine the effectiveness of these methods. Additionally, these methods can usually only identify simple primitive classes, and lack a deep understanding of the complex binary code structure, providing very limited semantic information to human analysts. Binary Code Summarization. Just as creating documentation for source code enhances maintainability and comprehensibility [16], generating semantic summaries for binary code can significantly improve analysis efficiency. Compared with the primitive classes provided by cryptographyoriented heuristic methods, summaries carry more and deeper semantic information. Recently, several methods [17, 18] have utilized language models for binary code summarization, achieving preliminary successes. Moreover, Large Language Models (LLMs) in the source-code domain, such as CodeX [19], GPT-J [20], and GPT-NeoX [21] et.al., have demonstrated impressive code comprehension and interpretation capabilities [22]. Leveraging these capabilities to analyze binary code, particularly for cryptographic functions, holds promise in providing comprehensible semantic J. ACM, Vol. 37, No. 4, Article . Publication date: July 2024. FoC: Figure out the Cryptographic Functions in Stripped Binaries with LLMs 3 information to analysts. Unfortunately, there are currently no publicly available datasets designed specifically for cryptographic function analysis, which limits the foundation for training a binary large language model. Furthermore, creating high-quality natural language semantic labels for a large-scale binary code dataset is also a formidable challenge. Binary Code Similarity Detection. Binary Code Similarity Detection (BCSD) is also a potential technical route. These methods [23-29] typically generate embeddings of binary functions to capture code features, thereby measuring the similarity between two binary functions. This allows us to identify functions in a known cryptographic function library that are similar to an unknown cryptographic function. Additionally, in contrast, the prediction of generative language models inherently struggles to reflect minor changes in binary code. However, distinguishing between two similar code is an essential ability, especially to recognize patched and vulnerable cryptographic functions. The function embeddings generated by the BCSD method can highly sensitively reflect any code changes, thus, to some extent, compensating for the shortcomings of generative models. Although current BCSD methods have demonstrated promising results on general datasets, limited attention has been given to the domain of cryptographic binary. In addition, the effectiveness of the BCSD methods is limited by the scope and comprehensiveness of the known cryptographic function library, and performs poorly when dealing with unknown or variant binary functions. To address the challenge of analyzing cryptographic functions in stripped binaries, in this paper, we first construct a cryptographic binary dataset with popular libraries and employ automated methods to create semantic labels for large-scale binary code. To F igure o ut what the C ryptographic binary functions do, we propose our framework called FoC , which comprises two main components: (1) FoC-BinLLM, a generative model designed to summarize binary code semantics employing multi-task and frozen-decoder training strategies, and (2) FoC-Sim, a similarity model built upon the FoC-BinLLM, where we identify cryptographic features and use multi-feature fusion to train an advanced similarity model. In our experiments, FoC-BinLLM shows unprecedented performance and provides detailed semantics in natural language, which is beyond the reach of previous methods. FoC-Sim also achieves superior results in both cryptographic and general BCSD tasks. Furthermore, FoC exhibits promising results in analyzing cryptographic viruses and identifying vulnerable cryptographic implementations in real-world firmware. Our contributions can be summarized as follows: · Comprehensive Dataset . We construct a cryptographic binary dataset cross-compiled from popular open-source repositories written in C language, and we devise an automated method to create semantic labels for extensive binary functions. Our proposed discriminator guarantees a strong alignment between these labels and facts on cryptography-related semantics. · Innovative Methodology . We introduce FoC, an LLM-based framework designed for analyzing the cryptographic functions in stripped binaries. To our knowledge, FoC-BinLLM is the first generative model for cryptographic binary analysis, summarizing the code semantics of binary functions in natural language. Leveraging the semantic insights from FoC-BinLLM, combined with structural information and cryptographic features, we further build FoCSim to retrieve homologous functions in our cryptographic binary database for unknown functions. · Effective Experimental . Experiments demonstrate that FoC-BinLLM surpasses ChatGPT by 14.61% on the ROUGE-L score in accurately summarizing cryptographic binary functions. Additionally, FoC-Sim outperforms the previous best methods with a 52% higher Recall@1 in retrieving similar cryptographic functions. FoC also shows promising outcomes in analyzing J. ACM, Vol. 37, No. 4, Article . Publication date: July 2024. 4 Shang et al. cryptographic viruses and identifying vulnerable cryptographic implementations in realworld firmware. Paper Organization. The rest of this paper is organized as follows: Section 2 presents a discussion on the background and motivation underlying our work. Section 3 provides the process of dataset construction. Section 4 introduces an overview of FoC, and Section 5 details the implementation design. Section 6 and Section 7 explain the experimental setup and report the corresponding experimental results, respectively. The discussion is thoroughly studied in Section 8. Finally, Section 9 and Section 10 respectively summarize the related works and conclude this research. Artifact Availability. We release the code and the datasets we collected of FoC in the Github repository 1 to facilitate further research in this domain.",
  "2 BACKGROUND AND MOTIVATION": "In this section, we first formally define the research problem in this paper in Section 2.1. Then, in Section 2.2, we point out the challenges faced in solving this problem. We also describe existing technology and how it addresses the current challenges in Section 2.3. Finally, we briefly introduce large language models (LLMs) and illustrate our motivation in Section 2.4.",
  "2.1 Problem Definition": "To figure out the cryptographic function in stripped binaries, we expect to obtain its comprehensive natural language summary and an embedding representation. Formally, we aim to develop a method, denoted as 𝑓 , that can effectively analyze a cryptographic function F in a binary file B . Our primary objective is twofold: (1) first, to generate a summary in natural language denoted as E elucidating the behavior and purpose of the F for analysts, and (2) second, to create an embedding denoted as V , which serves as a vectorized representation of the function to achieve binary code similarity detection. This process can be formalized as:  To build the method 𝑓 , we are mainly facing the following four challenges.",
  "2.2 Challenges": "C1: Available and Diverse Cryptographic Binary Dataset. One of the fundamental challenges is the lack of publicly available comprehensive datasets tailored for the analysis of cryptographic functions in stripped binaries. There are dozens of cryptographic algorithms currently in the public domain, and a complete collection of their implementations is challenging. Moreover, since we focus on the binary domain, compiling the collected source code into binary code is an essential and labor-intensive task. In short, the absence of publicly available datasets hinders research on the current issue. In addition, mainstream cryptographic algorithms, such as ECC [30] and AES [31], serve as the foundation of computer security and must adhere to stringent specifications and standards to ensure both security and interoperability. However, practical implementations can differ significantly due to variations in the mathematical kernel and work mode employed. To advance research, it is crucial to collect as diverse implementations of cryptographic algorithms as possible. These implementations may be based on different standards and protocols, and designed for various platforms and purposes. Additionally, differences can arise even within the same algorithm specification due to developers' programming styles. For instance, developers might decompose a cryptographic algorithm with complex operations into multiple functions or, conversely, merge several simple operations into a 1 https://github.com/Ch3nYe/FoC J. ACM, Vol. 37, No. 4, Article . Publication date: July 2024. FoC: Figure out the Cryptographic Functions in Stripped Binaries with LLMs 5 Table 1. Comparison with related technologies and specific methods in addressing challenges. \" ✓ \" indicates that the method effectively addresses the corresponding challenge, \" ✗ \" indicates that it does not, and \" ✓ ✗ \" indicates that it partially addresses the challenge. single function. Such variations can undermine the effectiveness of certain control-flow or data-flow analysis methods. Furthermore, since we are dealing with binary code, it is essential to account for differences introduced by compilation environments, including optimizations (e.g., loop unrolling, function inlining), and target architectures, which can lead to vastly different binary representations from identical source code. C2: Well-built Semantic Labels. For effective training and evaluation, it is necessary to have high-quality semantic labels that accurately describe the functionality of cryptographic functions in binaries. However, the creation of these labels poses significant challenges. First, mapping the semantic information present in the source code to the binary code is a feasible solution, but the function names and primitive classes in the source code are too brief to carry enough semantic details. Source code comments can provide valuable insights in many cases, but in real-world projects, comments are often missing or incomplete. Additionally, for large datasets containing millions of binary functions, manually annotating them is time-consuming and requires a high degree of expertise, which is impractical. Therefore, an automated method to generate accurate semantic labels is crucial. C3: Cross-version Awareness. Given the central role of cryptographic libraries in computer security and their widespread use, any vulnerability in their implementation can lead to unacceptable damage, such as the Heartbleed vulnerability [32]. As a result, cryptographic algorithms are frequently updated to address vulnerabilities or improve performance, and such updates may be tiny code changes, such as applying a single-line patch to fix a critical vulnerability. For any analysis method, being sensitive to these tiny code changes is crucial, and achieving such cross-version awareness is particularly challenging in the context of purely generative models.",
  "2.3 Existing Techniques": "Existing techniques that may be used to perform analysis of cryptographic functions in stripped binaries can be broadly categorized into three areas: Cryptography-Oriented Heuristics Methods, Binary Code Summarization, and Binary Code Similarity Detection. As shown in Table 1, these techniques have not yet adequately addressed the challenges mentioned in Section 2.2. Cryptography-Oriented Heuristics Methods. Cryptographic algorithm identification methods have been a focal point of research for over two decades [36]. Numerous methods based on program structure have been proposed, especially the data-flow graph (DFG). For instance, Aligot [13] identifies the data-flow loops within the execution trace, while CryptoHunt [14] and Wherescrypto J. ACM, Vol. 37, No. 4, Article . Publication date: July 2024. 6 Shang et al. [15] construct DFGs with symbolic execution. These methods rely on manually designed graph patterns with known implementations, making them incapable of overcoming C1 . It means that these methods are not robust to any factors that make the binary code change. Certain cryptographic algorithms contain noticeable features, such as constants (e.g., S-box) and statistical attributes. FindCrypt2 [35], Signsrch [4], and findcrypt-yara [3] are three popular tools that identify the cryptographic algorithms present in binary files based on constant features. However, these methods will fail facing intentionally altered implementations or algorithms where constant values do not exist. Coarse-grained results at the file level (i.e., algorithm classes) fail on C2 . Advanced methods like CryptoKnight [33] and GENDA [34] use CNN and GNN, respectively, to learn function semantics to predict primitive classes. Nonetheless, CryptoKnight uses a dataset almost exclusively from OpenSSL [37], and GENDA's dataset is sourced from only four cryptographic algorithm libraries. These limitations in data diversity contribute to their inadequacies in addressing C1 . Binary Code Summarization. Binary code summarization enhances the understandability of binary analysis by generating human-readable descriptions of binary functions. It has only been proposed recently, and the progress of two existing works focusing on this issue, BinT5 [17] and HexT5 [18], have demonstrated the potential of summarizing binary codes using deep learning techniques. Specifically, BinT5 is built upon CodeT5 [38] to summarize decompiled code, while HexT5 is a unified pre-training model for binary code information inference tasks, including decompiled pseudo-code summarization. However, both of them are designed for general binary code rather than the cryptography domain. Therefore, we take them as very basic baselines. Besides, general LLMs could also be used to generate summaries for binary code with an appropriate prompt, and we also conduct a comparison with them. Binary Code Similarity Detection. BCSD methods aim to compare the degree of similarity between two binary code snippets, and have the potential to overcome C3 , i.e., cross-version awareness. BCSD plays a vital role in various security tasks, including malware detection, plagiarism detection, and patching analysis. By identifying similar or identical code in different binaries, the BCSD approach enables security analysts to track code reuse and identify potential vulnerabilities. Recently, several advanced BCSD methods have employed Transformer-based architectures as their backbones, and designed their own pre-training tasks. For instance, Trex[27] uses value prediction in micro-trace to learn the execution semantics. PalmTree[26] utilizes context window prediction and def-use prediction to learning assembly code from CFGs and DFGs. While jTrans[28] learns jump-aware semantics through jump target prediction pre-training. These methods have demonstrated impressive results on the general BCSD task, and we can employ them to detect cryptographic functions in binaries, although they are not specifically tailored for the cryptographic domain.",
  "2.4 Large Language Model and Our Motivation": "Recently, Large Language Models (LLMs) have captured significant attention from both academia and industry due to their remarkable capabilities. Typically, LLMs refer to language models that contain tens of billions or more parameters [39], trained on vast amounts of textual data using extensive computational resources. Notable examples include GPT-3 [40], PaLM [41], and LLaMA [42], which have shown impressive performance across a range of natural language processing tasks. Under this research boom, LLMs specifically focused on programming languages have also been proposed rapidly, including Codex [19], GPT-J [20], GPT-NeoX [21], CodeT5+ [43], PolyCoder [44], WizardCoder [45], CodeLlama [46], and so on. These models have demonstrated exceptional code comprehension capabilities. J. ACM, Vol. 37, No. 4, Article . Publication date: July 2024. FoC: Figure out the Cryptographic Functions in Stripped Binaries with LLMs 7 Fig. 1. (Left) Development of open-source cryptography repositories we investigated. The train data highlighted in blue, the test data highlighted in red. (Right) The code overlap between cryptography repositories, where the numbers represent the number of functions that are shared or unique among the repositories. OpenSSL LibreSSL TongSuo 95 2381 1121 32 4564 3084 5655 Qemu OpenSSL Linux Android BoringSSL 2014 OpenSSL_1.x LibreSSL 2014 OpenSSL_1.0.1g TongSuo 2020 OpenSSL_1.1.1 2023 OpenSSL_3.0.2 Nettle Libbcrypt LibTomCrypt Libgcrypt MbedTLS TEA Botan CryptoPP BeeCrypt scrypt WolfSSL GmSSL Libsodium Tiny-AES-c Additionally, for stripped binaries, semantic information such as types, variable names, function names, and even structural details like instruction and function boundaries are discarded during the compilation and stripping process. This makes semantic analysis of binary code extremely challenging, as it involves recovering missing information, which is essentially creating something from nothing. In this context, traditional analysis techniques and small-scale models are limited in their effectiveness. LLMs, however, present a viable solution. Applications such as text-to-image [47] and text-to-video [48] generation have shown that LLMs can compensate for and restore missing information, which is especially valuable for analyzing stripped binaries with significant information gaps. Although traditional analysis techniques have reached a bottleneck, LLMs are rapidly advancing, offering great potential for further improvement. In summary, given the demonstrated powerful understanding capabilities of LLMs in natural language (NL) and programming language (PL) tasks, it has become possible to devise an automated pipeline to create high-quality semantic descriptions for the cryptographic function in source code. This approach mitigates the challenge of creating semantic labels ( C2 ). Consequently, we can confidently gather a rich cryptographic binary dataset to tackle challenges C1 . Additionally, a recent evaluation [49] highlights that LLMs perform significantly worse on summarizing binary code compared to source code. Therefore, using the dataset, we propose developing a binary-specific LLM that generates comprehensive descriptions of cryptographic functions in stripped binaries, which potentially addresses C2 . To mitigate the generative model's limitations regarding C3 , we further incorporate a BCSD module based on our binary LLM to create the cross-version aware embedding representation.",
  "3 DATASET CONSTRUCTION": "In this section, we first introduce the collection and processing workflow of the cryptographic binary dataset in Section 3.1. Subsequently, we detail the automatic semantic creation method for extensive binary functions in Section 3.2. To ensure the correctness of the created labels, we present a keyword-based semantic discriminator in Section 3.3.",
  "3.1 Cryptographic Dataset Collection": "As discussed in C1 in Section 2.2, the lack of publicly available and implementation-diverse cryptographic binary datasets has prevented research in the current field. Therefore, we here collect a comprehensive cryptographic binary dataset for building our methods and stimulating further research. Our focus is on studying widely-used cryptographic algorithm libraries, popular J. ACM, Vol. 37, No. 4, Article . Publication date: July 2024. 8 Shang et al. Fig. 2. The workflow of building our cryptographic binary dataset. Source Code Cross-Compilation Binary W/ Debug Information Strip Stripped Binary Decompilation Binary Code DWARF Debug Information Offset Address Collector Build Prompt ChatGPT Discriminator Summary Metadata Create Semantic Labels = honoo PROMPT| cryptographic implementations, and cryptographic modules within large projects, all of which are written in the C programming language. Specifically, we first conduct a review of the development of existing cryptographic projects and their inter-dependencies. As shown in Figure 1 (Left), OpenSSL, one of the most popular cryptographic algorithm libraries, has influenced the development of other libraries (e.g., BoringSSL, LibreSSL, and TongSuo) and has been applied in many large projects (e.g., Linux, Android, and Qemu). Other cryptographic projects have their own unique development histories, and some of them are designed for specific scenarios. For example, WolfSSL and MbedTLS are friendly to embedded devices, while BoringSSL and TongSuo are forked from OpenSSL by enterprises and continue to evolve to meet business requirements. However, for those independently developed projects, it is challenging to ascertain whether they have been influenced by each other. Therefore, we conduct a statistical analysis of code overlap among them, focusing only on the code that would be compiled into their binary files. The results, as shown in Figure 1 (Right), indicate that only projects forked from OpenSSL share some similar function snippets. We avoid including these projects in both the training set and the test set simultaneously to prevent potential data leakage issues. As shown in Figure 2, for each project in the dataset, we employ cross-compilation to obtain their binaries under different compilation environments. Specifically, we employ two compilers, GCC-11.2.0 and Clang-13.0 , with four different optimization options, i.e., O0-O3 . These projects are compiled for six different target architectures, including x86_32, x86_64, arm_32, arm_64, mips_32, and mips_64 . Subsequently, we strip the binaries to make them consistent with release versions in real-world scenarios. IDA Pro [1] is used to decompile the binary files. We perform deduplication on all functions in our dataset, using the MD5 digest of binary functions, to avoid data redundancy. Our dataset contains considerable cryptographic algorithms and takes account of the complex compilation environments in the real world, which allows us to overcome C1 . The statistical information of the dataset is shown in Table 2.",
  "3.2 Automated Semantic Labels Creation": "As we described in C2 , creating NL semantic labels for extensive binary functions is a challenging issue. As shown in Figure 2, we can establish correspondences between binary code and source code using DWARF [50] debugging information and offset addresses in the binary file, but comprehending the source code itself is not straightforward. Wefirst note that function names provide a brief semantic overview, but usually cannot represent the complete behavior of the code. Compared to function names, code comments offer more comprehensive semantic summaries. Unfortunately, not all functions have developer-written J. ACM, Vol. 37, No. 4, Article . Publication date: July 2024. FoC: Figure out the Cryptographic Functions in Stripped Binaries with LLMs 9 Table 2. Statistics of our cryptographic binary dataset. 1 OpenSSL and MbedTLS have two versions in the dataset. 2 Functions-Uni means the number of unique functions after deduplication according to function hash. Table 3. Qualitative comparison of developer-written comments (defective) with model-generated summaries. 1 Defect Types of developer-written comments: the lack of functional descriptions, the indirect information given, and the redundant information. comments. We detect comments in less than 20% of the functions within our dataset. Worse yet, their formats are inconsistent, and the quality is often unsatisfactory. As shown in Table 3, we identify the three most common defects, i.e., lack of functional descriptions, indirect information, and redundant information. Therefore, using human-written comments as semantic labels is deemed unreliable. To mitigate this problem, inspired by recent research works [51, 52] that utilize LLMs to perform data annotation tasks with reasonable reliability, we leRverage ChatGPT [53], an advanced general LLM, to automatically generate summaries as comments, outlining the function's purpose and functionality. Specifically, we use the metadata extracted from the source code for each function to build the prompt shown in Figure 3. We prompt the LLM to generate only one-sentence summaries J. ACM, Vol. 37, No. 4, Article . Publication date: July 2024. 10 Shang et al. Fig. 3. Prompt template for generating function summaries. SYSTEM_PROMPT = '''  Imagine you are an experienced software developer. The user will provide a source code function and its basic information each time. Your task is to generate a comment to the function. Please follow the rules below: 1.  Comment should be accurate, precise, and helpful for code understanding. 2.  You can leverage the original comments in the source code, but you cannot directly copy the original comments. 3.  You need to write a brief comment in one sentence.  ''' USER_PROMPT = '''  Here is a source code function from {Path} file in the {Project} project: ``` C/C++ {Comment} {Code} ``` ''' Table 4. Categories and classes employed by the discriminator. to try to avoid including too much information that is not present in the pseudo-code, such as variable names and macro definitions, which potentially bias the model. On the other hand, a one-sentence summary is easier to read and understand.",
  "3.3 Keyword-based Discriminator": "It is essential to assess whether these summaries align with the facts. Therefore, we propose a keyword-based discriminator to judge the correctness of generated summaries on the crucial cryptographic semantics. Specifically, as shown in Table 4, we have defined three categories and specific classes. We use whole-word matching to retrieve class-related keywords. However, a class is often written in multiple forms in summary or source code. For example, '3des' might be written as 'triple-des' , 'triple des' , 'tripledes' , 'desede' , and 'des-ede' etc. To get the most accurate results possible, we perform extensive manual inspections to comprehensively summarize the different forms that a class may exist in, as shown in Table 5. The discriminator considers these different forms as the same class, and it passes the inspection only when it obtains the same class between the generated summary and the source code. The results indicate that more than 85% of the generated summary passed and they were retained in the end. We further evaluate the textual consistency between developer-written comments and model-generated summaries. The results show that they have a 43.55% ROUGE-L score, which means a high degree of consistency. And Table 3 shows a few generated labels. Overall, for each function in our dataset, we obtain its binary code, source code, and semantic labels, which highly align with the facts. In this way, we address the challenges presented in C2 . J. ACM, Vol. 37, No. 4, Article . Publication date: July 2024. FoC: Figure out the Cryptographic Functions in Stripped Binaries with LLMs 11 Table 5. Classes and their corresponding different forms.",
  "4 METHODOLOGY OVERVIEW": "In this section, we provide an overview of the FoC framework, as illustrated in Figure 4. The framework comprises two key components: (1) Binary Large Language Model (FoC-BinLLM), which is designed for understanding the pseudo-code of cryptographic functions that are lifted from stripped binaries. FoC-BinLLM predicts the stripped function names and generates concise summaries of the functions. (2) Binary Code Similarity Model (FoC-Sim), which leverages the semantic encoder from FoC-BinLLM and also incorporates graph structure information with Graph Convolutional Network (GCN), along with cryptographic features. These two components, though performing different tasks, are tightly integrated in the FoC framework to form an efficient collaborative working mechanism. During the training process, FoCBinLLM is first trained to perform seq-to-seq tasks using an encoder-decoder mode. Subsequently, FoC-Sim directly uses the encoder part trained by FoC-BinLLM to train the embedding generation task. Since the encoder has been fully trained and adapted to the domain knowledge of binary code and cryptographic functions, we freeze its parameters and only train the lightweight GCN and MLP, which greatly improves training efficiency and reduces computational overhead. During inference, FoC-BinLLM and FoC-Sim share the same encoder part, and after leaving the encoder, they perform parallel inference. For an input binary function, the FoC framework only needs to process the input once to simultaneously generate a semantic summary of the function (completed by FoC-BinLLM) J. ACM, Vol. 37, No. 4, Article . Publication date: July 2024. 12 Shang et al. Fig. 4. An overview of FoC framework. Part ① : Binary Large Language Model (FoC-BinLLM) Crypto Database Vulnerability Database Function Embedding Prompt: Pseudo Code Response: <Label> Initialization Formatted Summary: Semantic Encoder Graph Conv Network Crypto Features MLP Crypto Function Vulnerability Function Build Data <COMMENT> Encrypts or decrypts a given input using AES in CBC mode … </COMMENT> <FUNC_NAME> AES_cbc_encrypt </FUNC_NAME> Binary Code Part ② : Binary Code Similarity Model (FoC-Sim) Data Processing Extractor Pseudo Code Attributed Control-flow Graph Crypto Features Golden LLM Binary LLM Encoder Frozen Decoder Transfer Retrieve and retrieve homologous similar functions (completed by FoC-Sim), and the information they output is complementary to each other for human reverse engineers. The close synergy between the two models provides a more comprehensive understanding of binary cryptographic operations. These components are briefly described below. Build Binary Large Language Model (FoC-BinLLM). In this part, the primary goal is to train an LLM to accurately interpret the behavior of cryptographic function in stripped binaries. To this end, we employ three tasks and utilize a frozen-decoder training strategy for training our Binary Large Language Model (FoC-BinLLM) efficiently. We adopt the Transformer model following the encoder-decoder architecture, enabling flexible use in either encoder-only mode for semantic embedding generation, or in encoder-decoder mode for causal generation. As illustrated in Figure 4 ① , we initialize our model with the pre-trained weights from a gold implementation, allowing us to avoid heavy training from scratch. The base model is then fine-tuned on our cryptographic binary dataset, optimizing it specifically for binary code understanding. We train the base model on our cryptographic binary dataset to specialize it for understanding binary code. Our FoC-BinLLM processes the pseudo-code of an unknown function as input and generates a formatted summary for analysts, providing detailed semantic insights and mitigates challenge C2 . Additionally, we propose a keyword-based discriminator to determine the class of critical behavior of cryptographic functions based on model predictions. Build Binary Code Similarity Model (FoC-Sim). We further build a binary code similarity model (FoC-Sim) to identify functions in the database that are similar to an unknown function. It is built upon our binary LLM and enhances it with additional information extracted from binary functions. As shown in Figure 4 ② , FoC-Sim utilizes the pseudo-code, the attributed control-flow graph (ACFG), and the cryptographic features as inputs, which can be easily extracted from the binary function using a modern decompiler, such as IDA Pro [1]. The model then creates an embedding representation of the function. It is aware of any changes in the binary code, which compensates for the lack of sensitivity of our generative model and mitigates the challenge C3 . By leveraging FoCSim, we can access source-level information when analyzing cryptographic algorithms in binaries, J. ACM, Vol. 37, No. 4, Article . Publication date: July 2024. FoC: Figure out the Cryptographic Functions in Stripped Binaries with LLMs 13 Fig. 5. An overview of the training of FoC-BinLLM. Input Text Multi-Head Attention Initialize With Golden Causal Model Feed Foward Decoder Input Masked Multi- Head Attention Multi-Head Attention Task2: Binary Code Summarization Task1: Function Name Prediction Trainable Encoder Task3: Binary-Source Contrastive Learning Feed Foward Frozen Decoder provided that homologous implementations are present in the database. Additionally, FoC-Sim can be employed to inspect binaries for identifying vulnerable cryptographic implementations.",
  "5 DETAILED DESIGN": "In this section, we delve into the details of the components included in FoC, covering Section 5.1 to Section 5.2.",
  "5.1 Binary Large Language Model (FoC-BinLLM)": "5.1.1 Golden Model Initialization. Training an expert model from a pre-trained model, rather than starting from scratch, is intuitively more efficient, particularly in the context of LLMs. As illustrated in Figure 5, we initialize our binary LLM based on a golden implementation, specifically CodeT5+ [43], a recently released LLM designed for source code understanding and generation. CodeT5+ is initialized with weights from previous pre-trained LLMs (i.e., CodeGen-mono [54]), and is trained on two large-scale source code datasets: a multilingual dataset 2 containing 51.5 billion tokens, and the CodeSearchNet released by previous research [55]. Benefiting from the training on bimodal data consisting of both natural language (NL) and programming language (PL), CodeT5+ achieved state-of-the-art performance in various downstream tasks, such as code generation and code summarization, at the time of its release. From an architectural perspective, the encoder-decoder architecture of CodeT5+ provides it with extremely high flexibility in both understanding and generating tasks. It can flexibly switch between encoder-only, decoder-only, and encoder-decoder modes, perfectly adapting to our diverse needs in binary code analysis. The architectural flexibility of CodeT5+, as well as the pre-built understanding and generation capabilities in both NL and PL, form the foundation for training it to become an expert LLM for binary code tasks. 5.1.2 Multi-Task & Frozen-Decoder Training. We adopt multi-task training to build our cryptographic binary LLM. Figure 5 illustrates the three tasks: (Task1) Function Name Prediction, 2 https://huggingface.co/datasets/codeparrot/github-code J. ACM, Vol. 37, No. 4, Article . Publication date: July 2024. 14 Shang et al. (a) Function Name Prediction. Function: sub_126670 (DES_cfb_encrypt) Recovery function name: int *__fastcall sub_126670(__int64 a1, __int64 a2, int a3, unsigned __int64 a4, __int64 a5, int *a6, int a7) { int *result; // rax int v9;  // r12d ... result = (int *)(unsigned int)(a3 - 1); if ( (unsigned int)result <= 0x3F ) { v9 = (a3 + 7) >> 3; ... } else { v66 = v0 - 1LL; ... } return result; } ### Response: Output ： Input ： DES_cfb_encrypt Function: sub_126670 (DES_cfb_encrypt) Summarize in one sentence: int *__fastcall sub_126670(__int64 a1, __int64 a2, int a3, unsigned __int64 a4, __int64 a5, int *a6, int a7) { int *result; // rax int v9;  // r12d ... result = (int *)(unsigned int)(a3 - 1); if ( (unsigned int)result <= 0x3F ) { v9 = (a3 + 7) >> 3; ... } else { v66 = v0 - 1LL; ... } return result; } ### Response: Output ： Input ： This function takes an input data block and encrypts or decrypts it using the DES algorithm in Cipher Feedback (CFB) mode. (b) Binary Code Summarization. Fig. 6. An illustration shows two generation tasks that we employ to train FoC-BinLLM. (Task2) Binary Code Summarization, and (Task3) Binary-Source Contrastive Learning. Both Task1 and Task2 are causal generation tasks employed with encoder-decoder mode, which takes the pseudo-code of binary function as input, and predicts the corresponding NL text based on task prompts. Task3 optimizes only the semantic embeddings generated by the encoder, using contrastive learning loss to reduce the distance between source code and binary code in the embedding space, facilitating rapid domain adaptation for the base model. Descriptive function names serve as concise summaries of functionality and are helpful for program comprehension in binaries. Unfortunately, these names and other debugging information are generally stripped out for various reasons (e.g., copyright protection and size reduction). As shown in Figure 6 (a), we train FoC-BinLLM to reassign descriptive names for the stripped binary functions. For instance, from the name in the example, it is evident that the encryption algorithm performed by this function is DES and its block encryption mode is CFB . However, the learning target for this task only includes function names from the source code, which are often too brief to fully describe the function's behavior. For example, Figure 6 (b) shows a function DES_cfb_encrypt(..., int enc) from the project OpenSSL , and this function's name alone cannot indicate the details of the encryption or decryption it performs. Therefore, we utilize the binary code summarization task to generate more comprehensive summaries of function semantics. These clear and concise semantic descriptions avoid unnecessary information that could impede understanding, making Task1 more accessible for the model to learn without introducing biases. Figure 6 presents two examples for Task1 and Task2, where the input text is constructed with code and a prompt prefix, and the output is the function name or summary. Both of them are auto-regressive generation task, which predicts the next token based on the current token sequence. J. ACM, Vol. 37, No. 4, Article . Publication date: July 2024. FoC: Figure out the Cryptographic Functions in Stripped Binaries with LLMs 15 Fig. 7. An illustration shows the semantic encoder uses contrastive learning to shorten the distance between the source code embedding and the corresponding binary code embedding. __int64 __fastcall sub_0BA040(__int64 a1, __int64 a2, __int64 a3, __int64 a4, __int64 a5, int a6) { __int64 result; // rax if (a6) result = sub_1B6E10(a1,a2,a3,a4,a5,sub_0BA6E0); else result = sub_1B6F50(a1,a2,a3,a4,a5,sub_0BAAA0); return result; } void AES_cbc_encrypt(const unsigned char*in, unsigned char *out, size_t len, const AES_KEY* key, unsigned char *ivec, const int enc){ if (enc) CRYPTO_cbc128_encrypt(in, out,len, key, ivec, (block128_f)AES_encrypt); else CRYPTO_cbc128_decrypt(in, out, len, key, ivec, (block128_f)AES_decrypt); } Binary Code Embedding Binary-Source Contrastive Learning Binary Code Source Code Semantic Encoder Source Code Embedding The loss function used is cross-entropy, which can be formalized as:  where X is the output sequence, and P is the probability of predicting the 𝑖 -th token 𝑥 𝑖 base on the part of label ˆ X 0: 𝑖 -1. The model is trained to maximize P for each token in the labels. Additionally, since the base model is trained on source code rather than binary code, as illustrated in Figure 7, we employ Binary-Source Contrastive Learning (Task3) to reduce the distance between source code and binary code in the embedding space, which facilitates rapid domain adaptation for the base model. Specifically, we apply mean pooling to the hidden states of each token in the last layer of the encoder, which is to obtain an embedding representing the input function. We then use cosine distance to measure the similarity between these embeddings. As shown in Equation 3, the cosine-similarity loss is used to optimize the model parameters, where the 𝑉 𝑠𝑜𝑢𝑟𝑐𝑒 and 𝑉 𝑏𝑖𝑛𝑎𝑟𝑦 represent the embeddings of the same function in source code and binary code, respectively:  This approach is based on insight from previous work [43, 56], which indicates that the decoder is critical for complex causal-generation tasks and thus requires more careful training. Consequently, we configure the decoder with more layers than the encoder, i.e., 𝑁 𝑑𝑒𝑐 > 𝑁 𝑒𝑛𝑐 , and the encoder has a smaller proportion of parameters in the overall model. Instead of training the entire large model, as depicted in Figure 5, we freeze the decoder and set only the encoder and the cross-attention layer as trainable, which significantly reduces a large number of trainable parameters for more efficient training.",
  "5.2 Binary Code Similarity Model (FoC-Sim)": "5.2.1 Code Semantic & Control Structure Encoding. We have built FoC-BinLLM above, which effectively captures the semantics of binary code well. Building on this, we directly leverage its encoder to create the semantic encoder of FoC-Sim. As illustrated in Figure 4 ② , the encoder takes a pseudo-code lifted from the binary function and generates an embedding as its semantic representation. Specifically, we apply mean pooling on the hidden states in the last layer of the encoder to produce this representation. J. ACM, Vol. 37, No. 4, Article . Publication date: July 2024. 16 Shang et al. Table 6. Summary of statistical features used in FoC at basic block-level and function-level. * This feature is categorized as either a general feature (G) or a cryptographic feature (C). 1 The basic block-level statistical features are used to create the ACFG in Figure 4. 2 The function-level statistical features are used to create the Cryptographic Features in Figure 4. Furthermore, we employ a Graph Convolutional Network (GCN) to capture the information of the function's control structure, which is essential for the binary code similarity problem, as demonstrated by previous research [57]. As shown in Figure 4, we extract the control-flow graph (CFG) of the binary function and create Attributed CFG (ACFG) using the statistical features at the basic block level. Specifically, each basic block in ACFG is represented as a 200-dimensional vector. The features we used in the basic block level are detailed in Table 6. The GCN model uses a feature encoder to generate feature embedding for each node, followed by message propagation to aggregate information from neighboring nodes along the edges of the ACFG. For each node 𝑣 𝑖 , its hidden state in 𝑙 -th layer is denoted as ℎ ( 𝑙 ) 𝑖 (for ( 𝑖 = 1 , 2 , ..., 𝑛 ) ). In each layer of the GCN, the message aggregation process can be described as follows:  « ‹ where 𝑁 ( 𝑣 𝑖 ) is the set of neighboring nodes, 𝑑𝑒𝑔 ( 𝑣 𝑖 ) is the degree of node 𝑣 𝑖 , W ( 𝑙 ) represents the weights for the 𝑙 -th layer of the GCN, and 𝑅𝑒𝐿𝑈 denotes the activation function we used. We utilize a 5-layer GCN to aggregate information from neighboring nodes, meaning that each node can potentially access information from neighbors within five jumps. Finally, through a summation readout operation, the hidden states of all nodes are aggregated to obtain a vector for the representation of the entire graph, namely the function structure embedding. 5.2.2 Cryptographic Features. Given that we focus on the cryptography domain, we identify a set of features used to distinguish binary functions implemented from different algorithms better. All of the features we used are detailed in Table 6. Previous research, such as Dispatcher [58] has highlighted that encryption routines use a high percentage of bitwise arithmetic. Similarly, ReFormat [5] also found that the processing of message decryption typically contains significant arithmetic and bitwise operations. Drawing inspiration from these researches, as well as previous works on BCSD [57, 59, 60], we use the count of J. ACM, Vol. 37, No. 4, Article . Publication date: July 2024. FoC: Figure out the Cryptographic Functions in Stripped Binaries with LLMs 17 arithmetic and logic opcodes as features at the basic block level, along with the Bag-of-Words (BoW) representation of frequent arithmetic opcodes. Meanwhile, at the function level, we employ the discriminator designed in Section 3.3 to identify keywords from the pseudo-code. We then create a BoW vector of the cryptographic class corresponding to the keywords, which incorporates possible string and symbol information. Finally, the BoW of keywords, together with the number of basic blocks, edges, and callees, et.al., constitutes a 65-dimensional vector as the function's Cryptographic Features, as mentioned in Figure 4. As illustrated in Figure 4, all features from both levels are integrated into the final function embedding. 5.2.3 Embedding Fusion & Model Training. As previously discussed, the semantic encoder generates semantic embeddings for pseudo-code, the GCN generates structural information embeddings for ACFGs, and we handcraft the embeddings from statistical cryptographic features. We use a single-layer MLP to fuse these embeddings, which can be formalized as:  To train FoC-Sim, the similar function pairs are sampled from our cryptographic binary dataset, where functions with the same function name whithin the same file from the same project are treated as similar, and others are not. We employ the MultipleNegativesRankingLoss [61] as the loss function. As illustrated in Equation 6, it processes mini-batch samples size of 𝑁 , denoted as B , containing only similar pairs, and these sample pairs are not drawn from the same group pairwise:  « ‹ where 𝑉 𝑖 and 𝑉 + 𝑖 are the function embeddings of a pair of similar samples, 𝜏 is a temperature parameter, and 𝑠𝑖𝑚 represents the similarity function of embeddings. Additionally, as shown in Figure 4, the semantic encoder is the largest module in our similarity model. To improve training efficiency, we can freeze its parameters, as our evaluation indicated that it has already been welltrained in the first part.",
  "6 EXPERIMENTAL SETUP": "In this section, we first explore our research questions in Section 6.1. Subsequently, we details the dataset, and evaluation metrics in Section 6.2 and the training details in Section 6.3.",
  "6.1 Research Questions": "In the evaluation experiments, we aim to answer the following Research Questions (RQs) : RQ1: How does FoC-BinLLM perform in summarizing semantics in cryptographic stripped binaries? (Section 7.1) RQ2: How does FoC-Sim perform in binary code similarity detection, especially in the cryptography domain? (Section 7.2) RQ3: How does FoC demonstrate practical ability in real-world scenarios? (Section 7.3) RQ4: How does each component of FoC contribute to its overall performance? (Section 7.4) RQ1 and RQ2 are employed to evaluate the effectiveness of FoC-BinLLM and FoC-Sim on their respective tasks. RQ3 is used to explore the practical ability of FoC in analyzing cryptographic viruses and identifying vulnerable cryptographic implementations in real-world firmware. RQ4 aims to explore the contribution of each component within FoC on performance through ablation studies. J. ACM, Vol. 37, No. 4, Article . Publication date: July 2024. 18 Shang et al.",
  "6.2 Experiment Settings": "6.2.1 Dataset. In Table 2 we list the statistics of our collected cryptographic binary dataset. Since using only the cryptographic dataset may lead to a biased model, we build a general dataset from the GNU repositories 3 (widely used in related works on binary analysis tasks [18, 26, 28, 62, 63]) using the same compilation environment, and add it to our cryptographic training set mentioned in Table 2. Then we prevent data leakage from code shared between projects by using MD5 deduplication. Further, we remove textually similar data via MinHash [64] (threshold 4 =0.95) to prevent overfitting. Finally, we split 5% of the training data as the validation set. The statistics of the final dataset are shown in Table 7. Table 7. Statistics of the final dataset used in the evaluation. 6.2.2 Evaluation Metrics. For the binary code summarization task, we adopt three metrics BLEU-4, METEOR, and ROUGE-L for evaluation, which are widely used in related works [16, 65-67]. BLEU-4. BLEU is the abbreviation for BiLingual Evaluation Understudy [68], which is a widely adopted metric for evaluating the quality of generated summaries. It is a variant of the precision metric, evaluating the similarity between a generated summary and a reference summary by calculating n-gram precision while also applying a penalty for overly short length. The score is calculated as:  where 𝐵𝑃 denotes the brevity penalty for short generated sequence, 𝑤 1 to 𝑤 𝑛 are positive weights summing to 1. 𝑃 𝑛 is the ratio of the subsequences with length 𝑛 in the generated summary that are also in the reference. In this work, we follow [17, 18] and use BLEU-4, i.e. N =4. METEOR. METEOR is the abbreviation for Metric for Evaluation of Translation with Explicit ORdering [69], which is proposed to improve the evaluation of text ordering. METEOR is a recalloriented metric that evaluates a generated summary by aligning it with a reference summary and computing a sentence-level similarity score. It is computed as follows:  where frag is the fragmentation fraction, P and R are the unigram precision and recall. 𝛼 , 𝛽 , and 𝛾 are penalty parameters. In this work, we follow [70] and keep 𝛼 , 𝛽 , and 𝛾 as default values of 0.9, 3.0, and 0.5, respectively. ROUGE-L. ROUGE is the abbreviation for Recalloriented Understudy for Gisting Evaluation [71]. ROUGE-L is a variant of ROUGE, which is computed based on the longest common subsequence (LCS) between two summaries. Specifically, the LCS-based F-measure ( 𝐹 𝑙𝑐𝑠 ) is called ROUGE-L. Given a generated summary X and a reference summary Y , ROUGE-L is calculated as:  where n and m represent the lengths of X and Y respectively, and 𝛽 = 𝑃 𝑙𝑐𝑠 / 𝑅 𝑙𝑐𝑠 . 3 http://ftp.gnu.org/gnu 4 https://github.com/bigcode-project/bigcode-dataset/blob/main/near_deduplication/minhash_deduplication.py J. ACM, Vol. 37, No. 4, Article . Publication date: July 2024. FoC: Figure out the Cryptographic Functions in Stripped Binaries with LLMs 19 For the binary code similarity detection task, we follow previous studies [28, 29, 72] and conduct experimental evaluation in two scenarios: · One-to-one Comparison . Given two binary functions, determine whether they are similar or dissimilar. · One-to-many Search . Given a binary function to be queried, retrieve similar functions from a pool of candidate functions. We select three widely used metrics, AUC, Recall@K, and MRR@K, from earlier works [28, 29, 72, 73] for a comprehensive evaluation. AUC. AUC is used to assess One-to-one Comparison scenarios and is the abbreviation for Area Under the Curve, which the curve is termed Receiver Operating Characteristic (ROC) curve. Specifically, for a pair of binary functions with similarity calculated as r , assuming the threshold is 𝛽 , if the similarity score r is greater than or equal to 𝛽 , the function pair is considered a positive result, otherwise it is considered a negative result. For a homologous pair, its similarity score r greater than or equal to 𝛽 is classified as a TP (True Positive), while a score below 𝛽 is considered a FN (False Negative). Conversely, for a non-homologous pair, its similarity score r greater than or equal to 𝛽 results in a FP (False Positive), and a score below 𝛽 is identified as a TN (True Negative). Subsequently, the TPR (True Positive Rate) and FPR (False Positive Rate) at this threshold 𝛽 are calculated as follows:  The ROC curves can be generated by plotting points with coordinates corresponding to FPRs and TPRs across various thresholds 𝛽 . Then AUC can be obtained by calculating the area under the ROC curve. Recall@k. Recall@ k is used to evaluate One-to-many Search scenarios. Given a query binary function f ∈ F and a target binary function pool P , where P contains a function 𝑓 𝑔𝑡 that is similar to f , and | P |-1 functions that are dissimilar to f , our objective is to retrieve the Top-k functions from the pool P that have the highest similarity to f . The retrieved functions are ranked according to a similarity score Rank 𝑓 𝑖 , which represents the position of the function 𝑓 𝑖 in the retrieved list. The indicator function 𝑔 and Recall@ k are defined as follows:   Where F represents the total number of queries. MRR@k. MRR@ k is also used to evaluate One-to-many Search scenarios and MRR is the abbreviation for Mean Reciprocal Rank. It is used to assess whether the retrieved similar function 𝑓 𝑔𝑡 is ranked higher in the retrieved list. While Recall@ k emphasizes the retrieval coverage rate, MRR@ k places greater emphasis on the order and positional relationship within the list. It is calculated as:",
  "6.3 Training Details": "6.3.1 Environment. Our experimental environment is a machine running on Ubuntu 20.04 OS, equipped with a 48-core Intel Xeon Gold 5220 CPU (2.0GHz, 42MB L3 Cache), 256GB RAM, and 10 * J. ACM, Vol. 37, No. 4, Article . Publication date: July 2024. 20 Shang et al. Table 8. Comparison with existing methods on binary code summarization. 1 Average time cost on each binary function. NVIDIA RTX 3090 GPU, each with 24GB of VRAM. These GPUs run Nvidia driver version 550.90.07 along with CUDA version 12.4. We employ BinKit [74] to build a cross-compiling environment to construct our binary dataset detailed in Section 3. We then use IDA Pro [1] to decompile binary functions from stripped binaries and use srcML [75] to extract metadata from source code. As for model training, we use Python language with PyTorch [76] and Transformers [77] to implement our models, and accelerate the training with DeepSpeed [78] in ZeRO2. 6.3.2 Model & Training Setting. We initialize the weights of our binary LLM (FoC-BinLLM) with CodeT5p-220m[43]. By default, FoC-BinLLM is configured with a 12-layer encoder and a 12-layer decoder, 768 hidden size, and a vocabulary size of 32,100. It supports an input length of 1024 tokens and has 38.11% of its parameters trainable. FoC-Sim consists of a semantic encoder initialized from FoC-BinLLM, a 5-layer GCN, and a 256-dimensional single-layer MLP. During training the FoC-BinLLM, we employ the Adam optimizer with 1e-4 learning rate, 0.1 weight decay rate, 64 batch size, and 4 training epochs in total (1 epoch for Task3 and 3 epochs for Task1 & Task2). While training the FoC-Sim, the Adam optimizer is used with 1e-3 learning rate, 1e-5 weight decay rate, 128 batch size, and 110,000 training steps.",
  "7 EXPERIMENTAL RESULTS": "",
  "7.1 Answer to RQ1: Performance in Cryptographic Binary Code Summarization": "The purpose of this research question is to evaluate the effectiveness of FoC-BinLLM in summarizing semantics in cryptographic stripped binaries and compare it with the state-of-the-art approaches. We conduct experiments on the cryptographic datasets described in Table 2. Baselines. To effectively compare the performance of FoC-BinLLM, we select several representative baseline methods. Binary code summarization has only been proposed recently, and there are few related works. Currently, the research works focusing on this issue include BinT5 [17] and HexT5 [18]. BinT5 is the first model focused on binary code summarization, which is fine-tuned on decompiled code based on CodeT5. HexT5 proposes a unified pre-trained model also based on CodeT5 for binary code information inference tasks, which includes binary code summarization. Additionally, general LLMs can also generate summaries for binary code through in-context learning. We select an open-source LLM, Mixtral [79], and a closed-source LLM, ChatGPT [53], as baselines. Mixtral is publicly released by the Mistral-AI team, and we focus on its Mixtral-8x7B-Instruct-v0.1 5 version, which is a Sparse Mixture of Experts (SMoE) generation model. ChatGPT is one of the most advanced and widely-used LLMs developed by OpenAI. We access its chatgpt-3.5-turbo-1106 6 model through OpenAI's API. 5 https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1 6 https://platform.openai.com/docs/models J. ACM, Vol. 37, No. 4, Article . Publication date: July 2024. FoC: Figure out the Cryptographic Functions in Stripped Binaries with LLMs 21 Table 9. Comparison with existing methods on function name prediction. 1 Average time cost on each binary function. Results Analysis. Table 8 shows the detailed performance. FoC-BinLLM performs impressively on the test set. Specifically, it achieves scores of 41.34%, 14.47%, and 40.20% on the ROUGE-L, BLEU-4, and METEOR metrics, respectively, outperforming all existing baseline methods. This demonstrates that FoC-BinLLM has higher accuracy and expressiveness in generating natural language summaries of cryptographic binary code, and can effectively capture the key semantics of binary functions. The expert models BinT5 and HexT5, both exhibit a significant performance degradation compared to the results reported in their articles, which may be attributed to our cryptographic binary surpassing the domain where they collect their dataset, especially in the complexity of the cryptographic algorithm. In contrast, FoC-BinLLM is able to handle these challenges better, outperforming 29.72%, 13.32%, and 30.29% on average in the three metrics, showing its unique advantages in the cryptographic domain task. Compared with general LLMs such as Mixtral and ChatGPT, FoC-BinLLM also performs well in three key metrics, outperforming them by 3.28%, 2.15%, and 5.59% on average respectively. It is worth noting that FoC-BinLLM has only 220M parameters, while Mixtral (8x7B) and ChatGPT have much larger scales than it, but show significant limitations in cryptographic binary codes. This comparison reflects that customized training of models for specific domain tasks can often effectively improve model performance. In terms of time overhead, FoC-BinLLM also shows a clear advantage in computational efficiency. As shown in Table 8, FoC-BinLLM takes an average of 0.1533 seconds to process each binary function, which is much lower than the 9.0232 seconds required by Mixtral, due to its smaller model scale. This makes FoC-BinLLM more practical in application scenarios that require large-scale batch processing. ChatGPT is accessed through an API, and its time depends on network conditions, so its time is not evaluated. Function Name Prediction. Since function name prediction is one of the training tasks of FoCBinLLM, we evaluate the performance of FoC-BinLLM in comparison with recent related methods, such as SymLM [62] and XFL [80], on the task. SymLM leverages the pre-trained Trex [27] model to extract execution flow information and concatenates the function embeddings to capture calling context information. Additionally, it employs the CodeWordNet model to alleviate the problem of ambiguous function names (e.g., synonyms, abbreviations, etc.). XFL introduces and information aggregation strategy by concatenating global and contextual embeddings to preserve both types of information. it also utilizes PfastreXML [81] and a binary function embedding to effectively perform multi-label classification of function name tokens. Furthermore, similar to the binary code summarization experiment, we select two large language models, Mixtral-8x7B-Instruct-v0.1 and ChatGPT-3.5-turbo-1106 , and apply an in-context learning approach for function name prediction as our baselines. The results are shown in Table 9. FoC-BinLLM achieves Precision, Recall, and F1-score of 30.2%, 31.7%, and 32.6%, respectively, significantly outperforming baseline methods. This demonstrates its superior capability in binary function name prediction, enabling it to more accurately capture the J. ACM, Vol. 37, No. 4, Article . Publication date: July 2024. 22 Shang et al. Table 10. Comparison with existing methods on the number of cryptographic primitive classes identified in binaries. 1 Average time cost on each binary function. 2 In parentheses is the number of false positive cryptographic primitives. behavioral characteristics of binary functions. Compared to expert models specifically designed for binary function name prediction such as SymLM and XFL, FoC-BinLLM improves the F1-score by 19.2% and 15.3%, respectively. The relatively poor performance of SymLM and XFL can be attributed to their insufficient generalization capabilities and lack of cryptography domain-specific knowledge. Furthermore, despite having only 220M parameters, which is substantially fewer than the general LLMs Mixtral and ChatGPT, FoC-BinLLM still achieves remarkable performance gains, with an F1-score improvement of 14.0% over Mixtral and 13.1% over ChatGPT. This highlights the value of domain-specific models in binary analysis. Additionally, FoC-BinLLM demonstrates strong efficiency, with an inference time of only 0.062 seconds per function, underscoring its practicality in real-time analysis scenarios. Cryptographic Algorithm Identification. In the process of cryptographic binary analysis, we are very concerned about which cryptographic primitives are used. For example, when detecting weak cryptographic algorithms, analysts need to know which primitives are used in binaries. Therefore, based on summary generation, we further expand the application scenarios of the model, and use the generated natural language summary to identify the classes of cryptographic primitives. Specifically, we utilize the keyword-based discriminator introduced in Section 3.3 to analyze the summary generated by the model and automatically identify the involved cryptographic primitives. We select the popular tools and existing related work Wherescrypto[15] as the baseline methods. FindCrypt2, findcrypt-yara, and Signsrch are based on cryptographic constant values and signatures. Wherescrypto offers only executable for 32-bit binary and supports four cryptographic algorithms (i.e., AES, SHA1, MD5, and XTEA). Other methods mentioned in Section 2.3 cannot be reproduced due to various reasons, such as not yet being open-sourced or dependencies being inaccessible. We conduct an experiment with four x86_64 binaries from our test set in Table 2 and manually inspect them, annotating a total of 70 instances of primitive classes. The results in Table 10 present the number of primitive classes correctly identified by each method. Compared with other methods, FoC-BinLLM has successfully identified the most instances, 46 out of 70, with 5 false positives (which is related to the randomness and openness of the LLM generation.). The other methods have no false positives, benefiting from their design, but the number of successful identifications is much less than that of FoC-BinLLM. Answering RQ1: FoC-BinLLM demonstrates superior performance in summarizing semantics in cryptographic stripped binaries. The binary code summaries it generates achieve scores of 41.34%, 14.47%, and 40.20% on the ROUGE-L, BLEU-4, and METEOR metrics, respectively, significantly outperforming all existing baseline methods and demonstrating higher accuracy and expressiveness. In addition, it can effectively identify the classes of cryptographic primitives in the generated summaries, providing critical support for security analysis. J. ACM, Vol. 37, No. 4, Article . Publication date: July 2024. FoC: Figure out the Cryptographic Functions in Stripped Binaries with LLMs 23 Table 11. Results of binary code similarity detection on the general dataset. 1 Code Semantics (S), Graph Structure (G), Feature Engineering (F). 2 Average time cost of 100 function similarity comparisons, including model inference and similarity score calculations.",
  "7.2 Answer to RQ2: Performance in Binary Code Similarity Detection": "In this RQ, we discuss the performance of FoC-Sim in binary code similarity detection and conduct experiments on both general and cryptographic datasets. We adopt two experimental scenarios, namely the One-to-one Comparison and One-to-many Search mentioned in Section 6.2.2. Following the previous works [57, 72], we identify five different sub-tasks: (1) XO: the function pairs have different optimizations. (2) XC: the function pairs have different compilers, compiler versions, and optimizations. (3) XC+XB: the function pairs have different compilers, compiler versions, optimizations, and bitness. (4) XA: the function pairs have different architectures and bitness. (5) XM: the function pairs have different compilers, compiler versions, optimizations, architectures, and bitness.",
  "7.2.1 Performance on the General Dataset .": "Firstly, we conduct experiments on a benchmark [57] released by Cisco in 2022 to evaluate the effectiveness of FoC-Sim in the general dataset. This dataset contains 7 popular open-source projects, compiled using two compiler series (GCC and Clang), each with four different versions, three different architectures (x86, ARM, and MIPS), two different bitness modes (32 and 64 bits), and five optimization levels (O0-O3, Os). Following its original experiment setup [57], for One-to-one Comparison, we sample 50k positive pairs and 50k negative pairs for each sub-task, and for One-to-many Search, we sample 1,400 positive pairs and 140k negative pairs, that is 100 negative pairs for each positive one. Baselines. This benchmark [57] includes the following baselines: (1) Zeek [82], which performs dataflow analysis on the lifted code (VEX IR) at the basic-block level and computes strands. Then, a two-layer fully-connected neural network is trained to learn the cross-architecture similarity task. (2) Gemini [60] extracts hand-crafted features for each basic block, and uses GNN to learn the CFG representation of the function. (3) SAFE [24] first uses a word2vec model to generate instruction embeddings, and then proposes a self-attention network to aggregate instruction embeddings into a function embeddings. (4) Asm2Vec [83] uses random walks on the CFG to sample instruction sequences, and then uses the PV-DM model to generate function embeddings. (5) GMN [84] is based on a variant of the GNN network that jointly reasons on a pair of CFGs. Results Analysis. As shown in Table 11, FoC-Sim significantly outperforms all baseline methods in all settings of the general dataset. Specifically, in the One-to-one Comparison scenario, FoCSim achieves 99%, 98%, 97%, and 99% AUC metric on the XC, XC+XB, XA, and XM sub-tasks, respectively, which is 14%, 12%, 11%, and 13% higher than the most advanced baseline method GMN. This cross-platform and cross-compilation option stability is the key advantage of FoC-Sim. J. ACM, Vol. 37, No. 4, Article . Publication date: July 2024. 24 Shang et al. Table 12. Results of binary code similarity detection on the cryptographic dataset. 1 Code Semantics (S), Graph Structure (G), Feature Engineering (F). 2 Average time cost of 100 function similarity comparisons, including model inference and similarity score calculations. In the more complex One-to-many Search scenario, FoC-Sim's advantage is more obvious. On the three metrics MRR@10, Recall@1, Recall@10 on the XM sub-task, FoC-Sim achieves 83%, 78%, and 95% respectively, which is 30%, 33% and 37% higher than the GMN method. The GMN model mainly relies on the control-flow graph (CFG) and the bag-of-words (BoW) of Opcode, which is a subset of the information fused by our similarity model. Besides that, FoC-Sim benefits from the semantic information provided by our binary LLM, which could be an essential factor for its better performance. The baseline methods Zeek, SAFE, and Asm2Vec achieve Recall@1 scores of only 0.13, 0.16, and 0.07, respectively, under the XM sub-task, highlighting their limited ability to handle complex scenarios such as cross-compiler environments and cross-architecture. This limitation arises from their model design, which heavily relies on low-level instruction embeddings, making it challenging to capture code variations introduced by compiler optimizations and architectural differences. In contrast, Gemini and GMN perform slightly better, with Recall@1 scores of 0.28 and 0.45, respectively. This improvement may be attributed to the structural information they rely on, such as the control-flow graph, which is robust to the architecture, enabling it to still identify partial similarities in a complex compilation environment.",
  "7.2.2 Performance on the Cryptographic Dataset .": "Subsequently, we further evaluate the effectiveness of FoC-Sim in the cryptographic datasets mentioned in Table 2. Similarly, following the previous works [72], we sample 50k positive pairs and 50k negative pairs for each sub-task for One-to-one Comparison, and for One-to-many Search, we sample 1,400 positive pairs and 140k negative pairs, i.e., 100 negative pairs for each positive one. Baselines. To further evaluate, in addition to the baselines used in the benchmark in Section 7.2.1, we also employ recent state-of-the-art BCSD methods, i.e., PalmTree [26], Trex [27], jTrans [28], Asteria-Pro [29] and RCFG2Vec [85]. PalmTree is based on BERT and performs self-supervised pre-training on large-scale unlabeled binary corpora to generate instruction embeddings that can be used to calculate function similarity. Trex utilizes transfer-learning-based models to learn execution semantics from micro-traces to generate function embeddings. jTrans manages to embed control-flow information into Transformer and pre-trains it on a large-scale dataset. AsteriaPro uses the Tree-LSTM to encode the AST into a representation vector and then reorders the J. ACM, Vol. 37, No. 4, Article . Publication date: July 2024. FoC: Figure out the Cryptographic Functions in Stripped Binaries with LLMs 25 Fig. 8. Results of Recall@1 under different pool sizes (XM sub-task in One-to-many Search). 1.0 2.0 3.0 4.0 log10 ( Poolsize ) 0.0 0.2 0.4 0.6 0.8 1.0 Recall@1 PalmTree Trex jTrans Asteria-Pro RCFG2Vec FoC-Sim candidate functions using function call relationship. RCFG2Vec uses syntax tree-based instruction embedding technology and acyclic graph neural network to solve the OOV problem and longdistance dependencies between basic blocks in existing methods respectively. Especially, we did not evaluate the performance of PalmTree on XA sub-task, because it only supports the x86 architecture. Results Analysis. The results are shown in Table 12. FoC-Sim also demonstrates excellent performance on cryptographic dataset. Specifically, in the One-to-one Comparison scenario, FoC-Sim achieves over 99% AUC on all four sub-tasks, surpassing the best-performing baseline, RCFG2Vec, by 2.6%, 2.1%, 6.4%, and 5.2%, respectively. Especially in the One-to-many Search scenario, FoCSim excelles with 94.0% MRR@10, 91.0% Recall@1, and 99.0% Recall@10 on the XM sub-task, significantly higher than the baseline RCFG2Vec, which achieves only 77.5%, 69.4%, and 89.1%. Notably, all of the baseline methods generate semantic embeddings directly from assembly code, which is highly sensitive to variations in the compilation environment, particularly in XA task. Instead, FoC-Sim can benefit from the cross-architecture capabilities provided by the pseudocode. Furthermore, compared with general dataset, FoC-Sim has greater advantages over baseline methods on cryptographic datasets. This is due to the fact that we have integrated cryptographicrelated feature information into function embedding based on the characteristics of cryptographic algorithms. FoC-Sim also offers high inference efficiency. As indicated in Table 12, FoC-Sim requires an average of 62.26ms to perform 100 function similarity comparisons, which includes the model inference and similarity score calculation process. This time is comparable to methods like SAFE and PalmTree, and significantly outperforms GMN, Asm2Vec, jTrans, Trex, Asteria-Pro and RCFG2Vec. Although Zeek achieves a faster time of 28.14ms, considering the substantial performance improvements offered by FoC-Sim, the slight increase in time cost is a reasonable trade-off. Additionally, as shown in Figure 8, we further analyze the impact of different pool sizes on the Recall@1 in the One-to-many Search scenario. Specifically, we select the five baseline methods that performed well in the XM sub-task in Table 12, namely PalmTree, Trex, jTrans, AsteriaPro, and RCFG2Vec, and gradually increase the pool size from 10 to 10,000. The experimental results demonstrate that as the pool size increases, the task difficulty rises significantly, leading to a noticeable drop in Recall@1 for all methods. However, FoC-Sim consistently outperforms the baseline methods across all pool sizes. Notably, the performance degradation of FoC-Sim is relatively small as the pool size increases. Even at the maximum pool size of 10,000, FoC-Sim maintains high performance, showcasing its superior adaptability and robustness in handling large-scale and complex real-world scenarios. These results further validate the effectiveness of FoC-Sim. J. ACM, Vol. 37, No. 4, Article . Publication date: July 2024. 26 Shang et al. Fig. 9. An example of analyzing binary cryptographic function in a virus with FoC. Label: mbedtls_x509_crt_init main ······ Pseudo Code: Predicted Name: ssl_init Summary: Initialize the SSL context __int64 __fastcall sub_1F8F0(_QWORD *a1) { int v1; // ecx unsigned __int64 v2; // rdi ··· v2 = (unsigned __int64)(a1 + 1); result = 0LL;\\n  *(_QWORD *)(v2 + 360) = 0LL; memset((void *)(v2 & 0xFFFFFFFFFFFFFFF8LL), 0, 8 * ((v1 - (v2 & 0xFFFFFFF8) + 376) >> 3)); return result; } Label: mbedtls_x509_crt_parse Pseudo Code: Predicted Name: pem_read_cert Summary: Reads a PEM-encoded certificate from a file __int64 __fastcall sub_48380(_QWORD *a1, char *a2, __int64 a3, int *a4, unsigned __int64 a5){ int v9; // eax __int64 result; // rax ... if ( !a3 || a2[a3 - 1] )  goto LABEL_3; result = sub_458C0(&v21, \\\"----BEGIN RSA PRIVATE KEY- ---\\\", \\\"----END RSA PRIVATE KEY----\\\", a2, a4, a5, &v20); ... return 4294951680LL; } __int64 __fastcall sub_2BD10(char *ptr, char *src, size_t n){ char *v3; // r14 ··· if ( !n || src[n - 1] || !strstr(src, \\\"-----BEGIN CERTIFICATE-----\\\") )\\n return sub_2B000(ptr, src, v4); ··· return v5; }",
  "Label: mbedtls_pk_parse_key": "Pseudo Code: Predicted Name: decode_private_key Summary: Decode a private key from a buffer and return the type of the decode key.",
  "Label: mbedtls_ssl_write": "Pseudo Code: Predicted Name: ssl_check_ctr_renegotiate Summary: Check if the TLS handshake is valid and renegotiate. __int64 __fastcall sub_1C980(_DWORD *a1, __int64 a2, unsigned __int64 a3, __int64 a4, __int64 a5, int a6){ unsigned int v7; // eax ··· if ( v7 ){ sub_20C10(a1, 1LL, \"ssl_tls.c\", 6829LL, \"ssl_check_ctr_renegotiate\", v7); return v12; } else { if ( a1[2] == 16 || (v13 = sub_1BF80(a1, 2LL, v8, v9, v10, v11), (v14 = v13) == 0) ){ ··· } return v14; } } Answering RQ2: FoC-Sim has shown impressive performance in binary code similarity detection on both general dataset and cryptographic dataset, and outperforms all baseline methods in all sub-tasks. Thanks to the cryptographic-related feature information designed based on the characteristics of the cryptographic algorithm, FoC-Sim's advantage on the cryptographic dataset is more obvious. In terms of inference efficiency, FoC-Sim also shows competitive results.",
  "7.3 Answer to RQ3: Performance in Practical Ability": "In this section, we explore the performance of FoC in two real-world scenarios: (1) analyzing cryptographic functions in a virus, and (2) retrieving cryptographic implementations in firmware.",
  "7.3.1 Cryptographic Function Analysis in Virus .": "We employ FoC-BinLLM to analyze an open-source Linux Remote Access Trojan (RAT) sample named splinter 7 . Since the source code is publicly available, we are able to determine that the sample utilizes the MbedTLS cryptographic library to implement the encrypted communication module. However, it is quite challenging for defenders to understand the binary code in its executable file, especially in the absence of symbol information. This makes it more difficult to analyze its encryption implementation at the binary level. Results Analysis. We show a part of our analysis in Figure 9, where each box represents a binary function in the virus and the corresponding prediction result. We manually obtain the corresponding function name from the source code (highlighted in green) and judge how well our model predictions mathches the facts. We start our analysis from the entry point of the program, the main function, and then analyze the callee functions within. Initially, we encounter a series of context initialization functions, such as the function sub_1F8F0 . FoC-BinLLM conducts automated analysis, and predicts its function name 7 https://github.com/tuian/splinter J. ACM, Vol. 37, No. 4, Article . Publication date: July 2024. FoC: Figure out the Cryptographic Functions in Stripped Binaries with LLMs 27 Table 13. Results of vulnerable cryptographic functions detection in real-world firmware. For x/y in a cell, x denotes the number of vulnerabilities discovered, and y denotes the total number of potential vulnerabilities. as ssl_init and generates an functional summary, describing that its role is to initialize the SSL context. After initialization, the Trojan program continue to call functions such as certificate parsing, key parsing, and encrypted communication. As the analysis deepened, we find that FoC-BinLLM maintains a high accuracy when processing multiple subsequent function calls. It is worth noting that for the mbedtls_ssl_write function, although the predicted name given by FoC-BinLLM is ssl_check_ctr_renegotiate , which does not match the original name, it mentions the behavior of handshake verification in the summary, which still reflects the purpose of the function. These results demonstrate that FoC-BinLLM has potential in automated malware analysis and provides powerful support for security analysts.",
  "7.3.2 Cryptographic Implementation Detection in Firmware.": "To further explore the practical capabilities of our similarity model FoC-Sim, we utilize it to detect cryptographic implementation functions in binaries. To do this, we obtain the firmware to create a firmware database and compile the vulnerable functions and their patched functions to create a vulnerability database . Subsequently, we perform three search tasks. (1) Utilizing vulnerability database, we apply FoC-Sim to detect vulnerable cryptographic functions in the firmware database. It is considered a successful identification if the vulnerable functions are found among the top 10 most similar in all functions from a suspicious file. (2) We evaluate the ability of FoC-Sim to distinguish between vulnerable functions and patched functions. Specifically, a vulnerable function from the firmware database is considered to be successfully distinguished if it has a higher similarity to the vulnerable version rather than the patched version. (3) We attempt to utilize FoC-Sim to detect cryptographic functions from real-world stripped binaries. Vulnerability & Firmware Database. We first build a vulnerability database containing vulnerable functions and their patched functions related with 16 CVEs. These vulnerability information J. ACM, Vol. 37, No. 4, Article . Publication date: July 2024. 28 Shang et al. Table 14. Results of detecting cryptographic functions in real-world stripped binaries. is primarily gathered from the CVDdetails 8 website, and the open-source cryptographic libraries to which these vulnerabilities are attributed are widely used in IoT firmware, including OpenSSL, mbedTLS, and Libgcrypt. We then download firmware from three popular IoT vendors, including NETGEAR, TP-LINK, and Xiaomi, and collect cryptographic libraries from them to build a firmware database . We determine the existence of vulnerable functions based on the version number of the library file. Results Analysis. Table 13 shows the detailed experimental results. The \"Vulnerability Detection\" column in Table 13 demonstrates that FoC-Sim is capable of accurately detecting the majority of vulnerable functions across the firmware of different vendors. Specifically, it identified 91 out of 97 functions in NETGEAR firmware, 168 out of 181 in TP-LINK firmware, and 319 out of 330 in Xiaomi firmware. Asshowninthe'Vulnerability/Patch Distinction' column, FoC-Sim has the potential to effectively distinguish vulnerable functions from patched functions. It correctly distinguishes 70 out of 97 functions in NETGEAR firmware, 142 out of 181 in TP-Link, and 289 out of 330 in Xiaomi. It demonstrates that FoC-Sim has the ability to overcome the challenge C3 . However, we observe a failed case in Libgcrypt . With manual inspection, we find that there is a huge difference between the vulnerable functions from the firmware database and the vulnerability database in both text and structure. In other words, the vulnerable function in Libgcrypt has multiple significantly different versions, while our vulnerable database only includes one. Compared to simple homology vulnerability detection, distinguishing between vulnerable functions and patched functions is a more difficult and highly specialized task, requiring more nuanced comparisons of binary code. Numerous studies have focused on binary code patch detection to tackle this challenge. For example, Fiber [86] generates binary-level patch signatures from sourcelevel patches and performs a signature match between target and reference binaries using syntactic features such as control flow graphs (CFGs) and basic blocks. This process aligns instructions and generates symbolic constraints to identify patches. BinXray [87] performs binary-level patch detection by extracting execution traces as signatures from binary functions, and matching them to the target function to confirm the presence of a patch. RoBin [88] improves vulnerability verification through patch semantic analysis, providing a deeper understanding of how to exploit the semantic differences between vulnerable and patched versions to more accurately detect patches. As a binary code similarity calculation engine, FoC-Sim has the potential to achieve more effective patch detection capabilities in the future by incorporating the targeted innovative methods in patch detection from the above works. Furthermore, we evaluate the ability of FoC-Sim to detect cryptographic functions in real-world stripped binaries. Specifically, we first use 1,336 binary files (containing 2,213,334 functions) from 14 projects collected in Table 2 as a Database Server of known cryptographic functions. Then, we 8 https://www.cvedetails.com/ J. ACM, Vol. 37, No. 4, Article . Publication date: July 2024. FoC: Figure out the Cryptographic Functions in Stripped Binaries with LLMs 29 (a) Ablation of parameters used in FoC-BinLLM. 200 5K 10K 15K 20K 25K 30K steps 0 1 2 3 4 5 train_loss FoC_BinLLM-220m FoC_BinLLM-2b 500 700 900 1100 1300 1500 steps 0.5 1.0 1.5 2.0 2.5 train_loss FoC_BinLLM-220m_wo_Task3 FoC_BinLLM-220m (b) Ablation of Task3 used in FoC-BinLLM. Fig. 10. Loss curves of ablation study of FoC-BinLLM. select 92 unstripped binaries of 10 firmware from the constructed firmware database and annotate 2,000 cryptographic related function using the keyword-based discriminator mentioned in Section 3.3. Subsequently, we strip symbols from these binaries to generate fully stripped test samples to simulate real reverse analysis scenarios. Finally, we compare the similarity of these 2,000 functions with the function in the Database Server one by one and calculate their Recall@K metrics. Note that we ensure that each function has at least one homologous function in the Database Server . The experimental results, as shown in Table 14, indicate that when the retrieval range is small, FoC-Sim achieves a Recall@1 score of 0.714, meaning the model can accurately identify most cryptographic functions when faced with a smaller pool of candidates. As the retrieval range increases, FoC-Sim's recognition ability significantly improves, particularly achieving a Recall@100 of 0.955, demonstrating its strong effectiveness in real-world cryptographic function detection scenarios. Removing different components of FoC-Sim leads to varying degrees of performance degradation, but it still maintains strong identification ability, with the code semantics being the most critical factor affecting detection performance. Answering RQ3: In the process of analyzing the encrypted communication module in the virus sample, FoC-BinLLM accurately predict the function names and summaries of related functions. FoC-Sim can effectively retrieve potential vulnerable cryptographic functions in the firmware, and correctly distinguish the subtle differences between the vulnerable functions and the patched versions. Furthermore, FoC-Sim can effectively detect cryptographic functions in real-world stripped binaries. In summary, the experimental results show the practical ability of FoC in real-world scenarios.",
  "7.4 Answer to RQ4: Ablation Studies": "In this research problem, we conduct ablation studies on FoC-BinLLM and FoC-Sim respectively, to evaluate the contribution of their individual components to the overall performance.",
  "7.4.1 Ablation Studies of FoC-BinLLM .": "For FoC-BinLLM, we mainly analyze the impact of different model parameter scales and training tasks on the performance of binary code summarization. Parameters of Binary LLM. We first explore the impact of model parameters on FoC-BinLLM. For effective comparison, we further train a scaled FoC-BinLLM with 2B parameters on the same dataset, using the same training tasks and strategies. The experimental results indicate that the J. ACM, Vol. 37, No. 4, Article . Publication date: July 2024. 30 Shang et al. Table 15. Ablation study results of FoC-Sim model. performance of the 2B parameter scale model is improved, achieving a ROUGE-L score of 43.96%, compared to 41.34% for the 220M model. As shown in Figure 10 (a), the model with 2B parameters exhibits a significantly lower loss convergence bound and a faster convergence speed. This indicates that larger-scale models can better capture the semantic information in complex binary code, suggesting a promising direction for future improvements. Training-Task of Binary LLM. Additionally, we investigate the effect of the Binary-Source Contrastive Learning task (Task3) on the model. As described in Section 5.1, Task3 is designed to facilitate the base model's rapid adaptation to the binary domain. We trained models both with and without Task3. Since Task1 and Task2 are directly related to the final objectives of our tasks, we did not conduct ablation studies on them. As illustrated in Figure 10 (b), the model trained with Task3 exhibits lower training loss at the same steps, indicating that this task effectively enhances the model's adaptability to binary code.",
  "7.4.2 Ablation Studies of FoC-Sim .": "As described in Section 5.2, we build FoC-Sim for generating function embeddings, which incorporates various information from binary functions, including code semantics, control structures, and cryptographic features. These information sources collectively support the execution of the similarity detection task. To evaluate the contribution of each information source, we compare the performance of FoC-Sim on the cryptographic dataset by removing each of the three sources. The experimental results are shown in Table 15. We observe that the absence of any of the three information sources will lead to performance degradation, especially in the more complex One-to-many Search scenario. Specifically, the code semantics from our binary LLM has the most significant impact on the model performance. After removing it, the Recall@1 metric drops by 18%, which shows that code semantics plays a key role in helping the model understand the binary code function and behavior. Control structures and cryptographic features also affect performance. After removing them, the Recall@1 metric drops by 4% and 1%, respectively. This ablation study further verify the unique role of each information source in similarity detection. Answering RQ4: For FoC-BinLLM, increasing the model parameter scale can slightly boosts the performance of summary generation. The inclusion of Task3, Binary-Source Contrastive Learning, facilitates rapid adaptation of the model to the binary domain. As for FoC-Sim, each information source contributes to the model's effectiveness in similarity detection, particularly in more complex One-to-many Search scenarios, where code semantics have the greatest impact.",
  "8 DISCUSSION": "In this section, we discuss the limitations of our method and explore potential avenues for future research. J. ACM, Vol. 37, No. 4, Article . Publication date: July 2024. FoC: Figure out the Cryptographic Functions in Stripped Binaries with LLMs 31 Quality of Summaries. In Section 3.2, during the process of automated semantic label creation, we utilize an LLM to generate semantic summaries, which is actually the process of translating source code into natural language. The quality of these summaries largely depends on the LLM used. Although we can utilize the discriminator to ensure the accuracy of key semantics within the cryptography domain, creating higher-quality semantic labels for binary code in more general domains remains a valuable direction for further research. Primitive Classes of Discriminator. As mentioned in Section 3.1, we have investigated popular open-source cryptographic repositories, collected and organized a set of the most common cryptographic primitives, including keywords associated with them. Although our collection has been as exhaustive as possible, some omissions are inevitable, especially in extreme cases or special applications. In the future, systematically studying and expanding the range of cryptographic primitives will be valuable work, helping us better understand which cryptographic algorithms are currently secure and in which scenarios they are appropriate. Function Context. In the case study of virus analysis (discussed in Section 7.3), we observe that FoC occasionally makes wrong predictions, which may be caused by the lack of contextual information when analyzing a single function. Binary functions often have cross-references with other functions in the file, and human analysts leverage the calling context or call chain to analyze a function comprehensively. However, FoC treats binary functions as independent analysis objects, leading to insufficient information access inherently. Similar issues are also mentioned in related works SymLM [62] and Cati [89]. In summary, future work can consider incorporating contextual information such as dependencies and cross-references between functions into the semantic analysis process of the model. Obfuscated Binaries. In this paper, we have not considered obfuscated binaries, which were treated as orthogonal work here. However, handling obfuscated binaries is crucial in many realworld applications, such as malware analysis, where attackers may intentionally use obfuscation techniques to hide malicious code. Current research, such as Aligot [13] and CryptoHunt [14], has explored methods for cryptographic algorithm detection in obfuscated binaries, primarily relying on input-output relationships of loop structures. These methods, while effective to an extent, are labor-intensive and may struggle with sophisticated or novel obfuscation techniques. For LLMbased methods, like our FoC, future work could explore the inclusion of obfuscated samples during training to improve the model's robustness to obfuscation.",
  "9 RELATED WORKS": "",
  "9.1 Binary Analysis": "Binary analysis is a fundamental technique in the field of software security and reverse engineering, playing a crucial role in understanding and manipulating compiled code without the need for access to the original source code. It is widely used in software vulnerability detection [72, 90], malware analysis [91, 92], and software maintenance [17]. For a long time, binary analysis has primarily relied on traditional tools and techniques. For instance, static analysis, uses tools such as IDA Pro [1] and Ghidra [2] to reveal the structure of the code, identify control flow, and detect potential vulnerabilities without executing the binary code. Dynamic analysis, in contrast, through tools such as Pintools [93] and Valgrind [94], tracks the execution status of the program during its execution and detects runtime exceptions. Although these traditional techniques are widely used, they still have several notable drawbacks. First, they are usually labor-intensive, heavily dependent on the analyst's expertise, and require a lot of manual intervention. In addition, real-world binaries often lack symbolic information such as function J. ACM, Vol. 37, No. 4, Article . Publication date: July 2024. 32 Shang et al. names and variable labels. This lack of readability exacerbates the difficulty of analysis and limits the effectiveness of traditional techniques when applied to complex binaries. In recent years, deep learning technology has developed rapidly, and many data-driven methods have been integrated into the process of binary analysis. These methods utilize large datasets and advanced algorithms to enhance the efficiency and accuracy of various aspects of binary analysis. For example, NERO [63], NFRE [95], and SymLM [62] explore recovering function names from disassembled code. DIRECT [96], OSPREY [97], and DIRTY [98] focus on recovering variable names and types. Debin [99] and Cati [89] try to predict debug information and types from stripped binaries. Additionally, BinT5 [17] and HexT5 [18] generate summaries for binary code, providing a description of the code's functionality. PalmTree [26], Trex [27], and jTrans [28] are employed to generate semantic embeddings of binary code for code similarity detection. With the rapid development of Large Language Model (LLM) technology, its exceptional performance in natural language processing and program understanding has prompted researchers to explore its application to binary code analysis related tasks. For instance, DeGPT [100] optimizes the readability and accuracy of decompilation results by using an LLM, like ChatGPT, to post-process the output of traditional decompilers. Nova [101], built upon the DeepSeek-Coder model [102], introduces a hierarchical attention mechanism and contrastive learning techniques to better capture the high-level semantics of binary code, significantly improving accuracy and interpretability in the analysis of complex binary programs. Similarly, LLM4Decompile [103], which is also based on DeepSeek-Coder, is fine-tuned specifically for binary code decompilation tasks. Meanwhile, the Machine Language Model (MLM) [104], as a closed-source LLM product specifically for machine language, has demonstrated great potential in the structural analysis, semantic analysis, and security analysis of binary programs, addressing challenges such as missing binary information and difficulties in semantic comprehension. The integration of these deep learning methods and large language models expand the potential of binary analysis, in order to cope with increasingly complex software systems. In this work, we mainly focus on two aspects: binary code summarization and binary code similarity detection.",
  "9.2 LLM for SE": "The emergence and rapid development of Large Language Models (LLMs) have triggered disruptive changes across many fields, including software engineering (SE) [105, 106]. An increasing number of research works are exploring the application of LLMs in various software engineering tasks, with their impact spanning across the the entire software development life cycle, from requirements engineering [107], software design [108] and development [109], software quality assurance [110], to software maintenance and management [111, 112]. These LLMs are trained on massive amounts of natural language text and programming code. With the common patterns and semantic structures learned from natural language and code, LLMs are increasingly blurring the boundaries between human language and programming language. Therefore, in recent years, many LLMs focusing on source code tasks have emerged. For instance, Codex [19], released by OpenAI in 2021, contains 12B parameters and is specifically optimized for programming languages. It was trained on 159GB of code data publicly available on Github, supporting dozens of programming languages. In the same year, Safesforce introduced CodeT5 [38], a code-aware LLM based on T5 [113], which excels in multiple tasks, including code generation, summarization, and code translation. CodeT5+ [43] is an enhanced version of CodeT5, released in 2023, with parameter scale ranging from 220M to 16B. It is pre-trained on a multilingual dataset containing over 51.5B tokens, further improving the model's performance in code-related tasks. Similarly, Meta AI launched CodeLlama [46] in 2023, based on the Llama model [42], with parameter J. ACM, Vol. 37, No. 4, Article . Publication date: July 2024. FoC: Figure out the Cryptographic Functions in Stripped Binaries with LLMs 33 versions of 7B, 13B, and 34B, positioning it as one of the most advanced LLMs designed specifically for code-related tasks. These LLMs that focus on source code perform excellently in complex tasks such as code generation, code summarization, code search, and code semantic understanding. This outstanding performance gives us reason to believe that LLMs also have great potential in handling binary code tasks. As an important component of the software engineering field, the introduction of LLMs is expected to significantly improve work efficiency and bring innovative solutions to this challenging field.",
  "10 CONCLUSION": "In this paper, our work addresses the challenges that existing works did not and provides a public cryptographic dataset for future research on the current issue. We present FoC, a novel LLMbased framework for the analysis of cryptographic functions in stripped binaries. Our evaluation results demonstrate that FoC-BinLLM can summarize function semantics in natural language, and outperforms ChatGPT by 14.61% on the ROUGE-L score. Furthermore, FoC-Sim achieves 52% higher Recall@1 than previous methods on the cryptographic dataset for the BCSD task, which compensates for the intrinsic weakness of the prediction of our generative models. The two components of FoC have shown practical ability in cryptographic virus analysis and 1-day vulnerability detection.",
  "ACKNOWLEDGMENTS": "This work was supported in part by the Natural Science Foundation of China under Grant U20B2047, 62072421, 62002334, 62102386 and 62121002, and by Open Fund of Anhui Province Key Laboratory of Cyberspace Security Situation Awareness and Evaluation under Grant CSSAE-2021-007.",
  "REFERENCES": "[1] Hex-Rays SA. IDA Pro. https://www.hex-rays.com/products/ida, 2023. [2] NationalSecurityAgency. Ghidra. https://github.com/NationalSecurityAgency/ghidra, 2023. [3] polymorf. findcrypt-yara. https://github.com/polymorf/findcrypt-yara, 2022. [4] Sirmabus. Ida_signsrch. https://github.com/nihilus/IDA_Signsrch, 2015. [5] Zhi Wang, Xuxian Jiang, Weidong Cui, Xinyuan Wang, and Mike Grace. Reformat: Automatic reverse engineering of encrypted messages. In Computer Security-ESORICS 2009: 14th European Symposium on Research in Computer Security, Saint-Malo, France, September 21-23, 2009. Proceedings 14 , pages 200-215. Springer, 2009. [6] Felix Gröbert, Carsten Willems, and Thorsten Holz. Automated identification of cryptographic primitives in binary programs. In Recent Advances in Intrusion Detection: 14th International Symposium, RAID 2011, Menlo Park, CA, USA, September 20-21, 2011. Proceedings 14 , pages 41-60. Springer, 2011. [7] Léonard Benedetti, Aurélien Thierry, and Julien Francq. Detection of cryptographic algorithms with grap. Cryptology ePrint Archive , 2017. [8] Juanru Li, Zhiqiang Lin, Juan Caballero, Yuanyuan Zhang, and Dawu Gu. K-hunt: Pinpointing insecure cryptographic keys from execution traces. In Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security , pages 412-425, 2018. [9] Patrick Kochberger and Florian Seitl. Detecting cryptography through ir visualization. In 2018 International Conference on Software Security and Assurance (ICSSA) , pages 25-29. IEEE, 2018. [10] Ruoxu Zhao, Dawu Gu, Juanru Li, and Yuanyuan Zhang. Automatic detection and analysis of encrypted messages in malware. In International Conference on Information Security and Cryptology , pages 101-117. Springer, 2013. [11] Jizhong Li, Liehui Jiang, and Hu Shu. Binary code level cyclic feature recognition of cryptographic algorithm. Computer engineering and design , 35(8):2628-2632, 2014. [12] Pierre Lestringant, Frédéric Guihéry, and Pierre-Alain Fouque. Automated identification of cryptographic primitives in binary code with data flow graph isomorphism. In Proceedings of the 10th ACM Symposium on Information, Computer and Communications Security , pages 203-214, 2015. [13] Joan Calvet, José M Fernandez, and Jean-Yves Marion. Aligot: Cryptographic function identification in obfuscated binary programs. In Proceedings of the 2012 ACM conference on Computer and communications security , pages 169-182, J. ACM, Vol. 37, No. 4, Article . Publication date: July 2024. 34 Shang et al. 2012. [14] Dongpeng Xu, Jiang Ming, and Dinghao Wu. Cryptographic function detection in obfuscated binaries via bit-precise symbolic loop mapping. In 2017 IEEE Symposium on Security and Privacy (SP) , pages 921-937. IEEE, 2017. [15] Carlo Meijer, Veelasha Moonsamy, and Jos Wetzels. Where's crypto?: Automated identification and classification of proprietary cryptographic primitives in binary code. In 30th USENIX Security Symposium (USENIX Security 21) , pages 555-572, 2021. [16] Weisong Sun, Chunrong Fang, Yuchen Chen, Quanjun Zhang, Guanhong Tao, Yudu You, Tingxu Han, Yifei Ge, Yuling Hu, Bin Luo, et al. An extractive-and-abstractive framework for source code summarization. ACM Transactions on Software Engineering and Methodology , 33(3):1-39, 2024. [17] Ali Al-Kaswan, Toufique Ahmed, Maliheh Izadi, Anand Ashok Sawant, Premkumar Devanbu, and Arie van Deursen. Extending source code pre-trained language models to summarise decompiled binaries. In 2023 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER) , pages 260-271. IEEE, 2023. [18] Jiaqi Xiong, Guoqiang Chen, Kejiang Chen, Han Gao, Shaoyin Cheng, and Weiming Zhang. Hext5: Unified pretraining for stripped binary code information inference. In 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE) , pages 774-786. IEEE, 2023. [19] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 , 2021. [20] Ben Wang and Aran Komatsuzaki. Gpt-j-6b: a 6 billion parameter autoregressive language model (2021). https: // github.com/kingoflolz/mesh-transformer-jax , 2022. [21] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source autoregressive language model. arXiv preprint arXiv:2204.06745 , 2022. [22] Frank F Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. A systematic evaluation of large language models of code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming , pages 1-10, 2022. [23] Lannan Luo, Jiang Ming, Dinghao Wu, Peng Liu, and Sencun Zhu. Semantics-based obfuscation-resilient binary code similarity comparison with applications to software and algorithm plagiarism detection. IEEE Transactions on Software Engineering , 43(12):1157-1177, 2017. [24] Luca Massarelli, Giuseppe Antonio Di Luna, Fabio Petroni, Roberto Baldoni, and Leonardo Querzoni. Safe: Selfattentive function embeddings for binary similarity. In Detection of Intrusions and Malware, and Vulnerability Assessment: 16th International Conference, DIMVA 2019, Gothenburg, Sweden, June 19-20, 2019, Proceedings 16 , pages 309-329. Springer, 2019. [25] Yue Duan, Xuezixiang Li, Jinghan Wang, and Heng Yin. Deepbindiff: Learning program-wide code representations for binary diffing. In Proceedings of the 2020 Network and Distributed Systems Security Symposium (NDSS) , 2020. [26] Xuezixiang Li, Qu Yu, and Heng Yin. Palmtree: Learning an assembly language model for instruction embedding. Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security , 2021. [27] Kexin Pei, Zhou Xuan, Junfeng Yang, Suman Jana, and Baishakhi Ray. Learning approximate execution semantics from traces for binary function similarity. IEEE Transactions on Software Engineering , 49(4):2776-2790, 2022. [28] Hao Wang, Wenjie Qu, Gilad Katz, Wenyu Zhu, Zeyu Gao, Han Qiu, Jianwei Zhuge, and Chao Zhang. jtrans: jump-aware transformer for binary code similarity detection. In Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis , pages 1-13, 2022. [29] Shouguo Yang, Chaopeng Dong, Yang Xiao, Yiran Cheng, Zhiqiang Shi, Zhi Li, and Limin Sun. Asteria-pro: Enhancing deep learning-based binary code similarity detection by incorporating domain knowledge. ACM Transactions on Software Engineering and Methodology , 33(1):1-40, 2023. [30] Mehmet Adalier and Antara Teknik. Efficient and secure elliptic curve cryptography implementation of curve p-256. In Workshop on elliptic curve cryptography standards , volume 66, pages 2014-2017, 2015. [31] NIST FIPS Pub. 197: Advanced encryption standard (aes). Federal information processing standards publication , 197(441):0311, 2001. [32] CVE-2014-0160. Available from MITRE, CVE-ID CVE-2014-0160., December 3 2013. [33] Gregory Hill and Xavier Bellekens. Cryptoknight: generating and modelling compiled cryptographic primitives. Information , 9(9):231, 2018. [34] Xiao Li, Yuanhai Chang, Guixin Ye, Xiaoqing Gong, and Zhanyong Tang. Genda: A graph embedded network based detection approach on encryption algorithm of binary program. Journal of Information Security and Applications , 65:103088, 2022. [35] Ilfak Guilfanov. Findcrypt2. https://hex-rays.com/blog/findcrypt2/, 2006. J. ACM, Vol. 37, No. 4, Article . Publication date: July 2024. FoC: Figure out the Cryptographic Functions in Stripped Binaries with LLMs 35 [36] Chenxia Zhao, Fei Kang, Ju Yang, and Hui Shu. A review of cryptographic algorithm recognition technology for binary code. In Journal of Physics: Conference Series , volume 1856, page 012015. IOP Publishing, 2021. [37] OpenSSL. Openssl. https://github.com/openssl/openssl, 2023. [38] Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. arXiv preprint arXiv:2109.00859 , 2021. [39] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223 , 2023. [40] Ben Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165 , 1, 2020. [41] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research , 24(240):1-113, 2023. [42] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023. [43] Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi D. Q. Bui, Junnan Li, and Steven C. H. Hoi. Codet5+: Open code large language models for code understanding and generation, 2023. [44] Frank F Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. A systematic evaluation of large language models of code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming , pages 1-10, 2022. [45] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with evol-instruct, 2023. [46] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950 , 2023. [47] Jie Qin, Jie Wu, Weifeng Chen, Yuxi Ren, Huixia Li, Hefeng Wu, Xuefeng Xiao, Rui Wang, and Shilei Wen. Diffusiongpt: Llm-driven text-to-image generation system. arXiv preprint arXiv:2401.10061 , 2024. [48] Hanzhuo Huang, Yufan Feng, Cheng Shi, Lan Xu, Jingyi Yu, and Sibei Yang. Free-bloom: Zero-shot text-to-video generator with llm director and ldm animator. Advances in Neural Information Processing Systems , 36, 2024. [49] Xin Jin, Jonathan Larson, Weiwei Yang, and Zhiqiang Lin. Binary code summarization: Benchmarking chatgpt/gpt-4 and other large language models. arXiv preprint arXiv:2312.09601 , 2023. [50] I. UNIX International. Dwarf debugging information format version 4. https://dwarfstd.org/doc/DWARF4.pdf, 2010. [51] Ruoyu Zhang, Yanzeng Li, Yongliang Ma, Ming Zhou, and Lei Zou. Llmaaa: Making large language models as active annotators. arXiv preprint arXiv:2310.19596 , 2023. [52] Ruixuan Xiao, Yiwen Dong, Junbo Zhao, Runze Wu, Minmin Lin, Gang Chen, and Haobo Wang. Freeal: Towards human-free active learning in the era of large language models. arXiv preprint arXiv:2311.15614 , 2023. [53] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems , 35:27730-27744, 2022. [54] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. Codegen: An open large language model for code with multi-turn program synthesis. ICLR , 2023. [55] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. Codesearchnet challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436 , 2019. [56] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science , 378(6624):1092-1097, 2022. [57] Andrea Marcelli, Mariano Graziano, Xabier Ugarte-Pedrero, Yanick Fratantonio, Mohamad Mansouri, and Davide Balzarotti. How machine learning is solving the binary function similarity problem. In 31st USENIX Security Symposium (USENIX Security 22) , pages 2099-2116, 2022. [58] Juan Caballero, Pongsin Poosankam, Christian Kreibich, and Dawn Song. Dispatcher: Enabling active botnet infiltration using automatic protocol reverse-engineering. In Proceedings of the 16th ACM conference on Computer and communications security , pages 621-634, 2009. [59] Jian Gao, Yu Jiang, Zhe Liu, Xin Yang, Cong Wang, Xun Jiao, Zijiang Yang, and Jiaguang Sun. Semantic learning and emulation based cross-platform binary vulnerability seeker. IEEE Transactions on Software Engineering , 47(11):25752589, 2019. [60] Xiaojun Xu, Chang Liu, Qian Feng, Heng Yin, Le Song, and Dawn Song. Neural network-based graph embedding for cross-platform binary code similarity detection. In Proceedings of the 2017 ACM SIGSAC conference on computer and communications security , pages 363-376, 2017. J. ACM, Vol. 37, No. 4, Article . Publication date: July 2024. 36 Shang et al. J. ACM, Vol. 37, No. 4, Article . Publication date: July 2024. FoC: Figure out the Cryptographic Functions in Stripped Binaries with LLMs 37 privacy (sp) , pages 472-489. IEEE, 2019. [84] Yujia Li, Chenjie Gu, Thomas Dullien, Oriol Vinyals, and Pushmeet Kohli. Graph matching networks for learning the similarity of graph structured objects. In International conference on machine learning , pages 3835-3845. PMLR, 2019. [85] Weilong Li, Jintian Lu, Ruizhi Xiao, Pengfei Shao, and Shuyuan Jin. Rcfg2vec: Considering long-distance dependency for binary code similarity detection. In Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering , pages 770-782, 2024. [86] Hang Zhang and Zhiyun Qian. Precise and accurate patch presence test for binaries. In 27th USENIX Security Symposium (USENIX Security 18) , pages 887-902, 2018. [87] Yifei Xu, Zhengzi Xu, Bihuan Chen, Fu Song, Yang Liu, and Ting Liu. Patch based vulnerability matching for binary programs. In Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis , pages 376-387, 2020. [88] Shouguo Yang, Zhengzi Xu, Yang Xiao, Zhe Lang, Wei Tang, Yang Liu, Zhiqiang Shi, Hong Li, and Limin Sun. Towards practical binary code similarity detection: Vulnerability verification via patch semantic analysis. ACM Transactions on Software Engineering and Methodology , 32(6):1-29, 2023. [89] Ligeng Chen, Zhongling He, and Bing Mao. Cati: Context-assisted type inference from stripped binaries. In 2020 50th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN) , pages 88-98. IEEE, 2020. [90] Jayakrishna Vadayath, Moritz Eckert, Kyle Zeng, Nicolaas Weideman, Gokulkrishna Praveen Menon, Yanick Fratantonio, Davide Balzarotti, Adam Doupé, Tiffany Bao, Ruoyu Wang, et al. Arbiter: Bridging the static and dynamic divide in vulnerability discovery on binary programs. In 31st USENIX Security Symposium (USENIX Security 22) , pages 413-430, 2022. [91] Jian Gao, Xin Yang, Ying Fu, Yu Jiang, and Jiaguang Sun. Vulseeker: A semantic learning based vulnerability seeker for cross-platform binary. In Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering , pages 896-899, 2018. [92] Junzhe Wang, Matthew Sharp, Chuxiong Wu, Qiang Zeng, and Lannan Luo. Can a deep learning model for one architecture be used for others? { Retargeted-Architecture } binary code analysis. In 32nd USENIX Security Symposium (USENIX Security 23) , pages 7339-7356, 2023. [93] Intel Corporation. Pin - A Dynamic Binary Instrumentation Tool. https://www.intel.com/content/www/us/en/ developer/articles/tool/pin-a-dynamic-binary-instrumentation-tool.html, 2023. [94] Valgrind. https://valgrind.org/, 2024. [95] HanGao, Shaoyin Cheng, Yinxing Xue, and Weiming Zhang. A lightweight framework for function name reassignment based on large-scale stripped binaries. In Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis , pages 607-619, 2021. [96] Vikram Nitin, Anthony Saieva, Baishakhi Ray, and Gail Kaiser. Direct: A transformer-based model for decompiled variable name recov-ery. NLP4Prog 2021 , page 48, 2021. [97] Zhuo Zhang, Yapeng Ye, Wei You, Guanhong Tao, Wen-chuan Lee, Yonghwi Kwon, Yousra Aafer, and Xiangyu Zhang. Osprey: Recovery of variable and data structure via probabilistic analysis for stripped binary. In 2021 IEEE Symposium on Security and Privacy (SP) , pages 813-832. IEEE, 2021. [98] Qibin Chen, Jeremy Lacomis, Edward J Schwartz, Claire Le Goues, Graham Neubig, and Bogdan Vasilescu. Augmenting decompiler output with learned variable names and types. In 31st USENIX Security Symposium (USENIX Security 22) , pages 4327-4343, 2022. [99] Jingxuan He, Pesho Ivanov, Petar Tsankov, Veselin Raychev, and Martin Vechev. Debin: Predicting debug information in stripped binaries. In Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security , pages 1667-1680, 2018. [100] Peiwei Hu, Ruigang Liang, and Kai Chen. Degpt: Optimizing decompiler output with llm. In Proceedings 2024 Network and Distributed System Security Symposium (2024). https://api. semanticscholar. org/CorpusID , volume 267622140, 2024. [101] Jiang Nan, Wang Chengxiao, Liu Kevin, Xu Xiangzhe, Tan Lin, Zhang Xiangyu, and Babkin Petr. Nova: Generative language models for assembly code with hierarchical attention and contrastive learning. arXiv preprint arXiv:2311.13721 , 2024. [102] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, et al. Deepseek-coder: When the large language model meets programming-the rise of code intelligence. arXiv preprint arXiv:2401.14196 , 2024. [103] Hanzhuo Tan, Qi Luo, Jing Li, and Yuqun Zhang. Llm4decompile: Decompiling binary code with large language models. arXiv preprint arXiv:2403.05286 , 2024. [104] Huaqing.AI. Machine language model, mlm. https://mlm01.com/, 2024. [105] Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy, and Haoyu Wang. Large language models for software engineering: A systematic literature review. arXiv preprint arXiv:2308.10620 , 2023. J. ACM, Vol. 37, No. 4, Article . Publication date: July 2024. 38 Shang et al. [106] Quanjun Zhang, Chunrong Fang, Yang Xie, Yaxin Zhang, Yun Yang, Weisong Sun, Shengcheng Yu, and Zhenyu Chen. A survey on large language models for software engineering. arXiv preprint arXiv:2312.15223 , 2023. [107] Giriprasad Sridhara, Sourav Mazumdar, et al. Chatgpt: A study on its utility for ubiquitous software engineering tasks. arXiv preprint arXiv:2305.16837 , 2023. [108] Shantanu Mandal, Adhrik Chethan, Vahid Janfaza, SM Mahmud, Todd A Anderson, Javier Turek, Jesmin Jahan Tithi, and Abdullah Muzahid. Large language models based automatic synthesis of software specifications. arXiv preprint arXiv:2304.09181 , 2023. [109] Yihong Dong, Xue Jiang, Zhi Jin, and Ge Li. Self-collaboration code generation via chatgpt. arXiv preprint arXiv:2304.07590 , 2023. [110] Wei Tang, Mingwei Tang, Minchao Ban, Ziguo Zhao, and Mingjun Feng. Csgvd: A deep learning approach combining sequence and graph embedding for source code vulnerability detection. Journal of Systems and Software , 199:111623, 2023. [111] Mohammed Alhamed and Tim Storer. Evaluation of context-aware language models and experts for effort estimation of software maintenance issues. In 2022 IEEE International Conference on Software Maintenance and Evolution (ICSME) , pages 129-138. IEEE, 2022. [112] Lehuan Zhang, Shikai Guo, Yi Guo, Hui Li, Yu Chai, Rong Chen, Xiaochen Li, and He Jiang. Context-based transfer learning for structuring fault localization and program repair automation. ACM Transactions on Software Engineering and Methodology , 2024. [113] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research , 21(140):1-67, 2020. J. ACM, Vol. 37, No. 4, Article . Publication date: July 2024.",
  "keywords_parsed": [],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "IDA Pro"
    },
    {
      "ref_id": "b2",
      "title": "Ghidra"
    },
    {
      "ref_id": "b3",
      "title": "findcrypt-yara"
    },
    {
      "ref_id": "b4",
      "title": "Ida_signsrch"
    },
    {
      "ref_id": "b5",
      "title": "Reformat: Automatic reverse engineering of encrypted messages"
    },
    {
      "ref_id": "b6",
      "title": "Automated identification of cryptographic primitives in binary programs"
    },
    {
      "ref_id": "b7",
      "title": "Detection of cryptographic algorithms with grap"
    },
    {
      "ref_id": "b8",
      "title": "K-hunt: Pinpointing insecure cryptographic keys from execution traces"
    },
    {
      "ref_id": "b9",
      "title": "Detecting cryptography through ir visualization"
    },
    {
      "ref_id": "b10",
      "title": "Automatic detection and analysis of encrypted messages in malware"
    },
    {
      "ref_id": "b11",
      "title": "Binary code level cyclic feature recognition of cryptographic algorithm"
    },
    {
      "ref_id": "b12",
      "title": "Automated identification of cryptographic primitives in binary code with data flow graph isomorphism"
    },
    {
      "ref_id": "b13",
      "title": "Aligot: Cryptographic function identification in obfuscated binary programs"
    },
    {
      "ref_id": "b14",
      "title": "Cryptographic function detection in obfuscated binaries via bit-precise symbolic loop mapping"
    },
    {
      "ref_id": "b15",
      "title": "Where's crypto?: Automated identification and classification of proprietary cryptographic primitives in binary code"
    },
    {
      "ref_id": "b16",
      "title": "An extractive-and-abstractive framework for source code summarization"
    },
    {
      "ref_id": "b17",
      "title": "Extending source code pre-trained language models to summarise decompiled binaries"
    },
    {
      "ref_id": "b18",
      "title": "Hext5: Unified pretraining for stripped binary code information inference"
    },
    {
      "ref_id": "b19",
      "title": "Evaluating large language models trained on code"
    },
    {
      "ref_id": "b20",
      "title": "Gpt-j-6b: a 6 billion parameter autoregressive language model"
    },
    {
      "ref_id": "b21",
      "title": "Gpt-neox-20b: An open-source autoregressive language model"
    },
    {
      "ref_id": "b22",
      "title": "A systematic evaluation of large language models of code"
    },
    {
      "ref_id": "b23",
      "title": "Semantics-based obfuscation-resilient binary code similarity comparison with applications to software and algorithm plagiarism detection"
    },
    {
      "ref_id": "b24",
      "title": "Safe: Self-attentive function embeddings for binary similarity"
    },
    {
      "ref_id": "b25",
      "title": "Deepbindiff: Learning program-wide code representations for binary diffing"
    },
    {
      "ref_id": "b26",
      "title": "Palmtree: Learning an assembly language model for instruction embedding"
    },
    {
      "ref_id": "b27",
      "title": "Learning approximate execution semantics from traces for binary function similarity"
    },
    {
      "ref_id": "b28",
      "title": "jtrans: jump-aware transformer for binary code similarity detection"
    },
    {
      "ref_id": "b29",
      "title": "Asteria-pro: Enhancing deep learning-based binary code similarity detection by incorporating domain knowledge"
    },
    {
      "ref_id": "b30",
      "title": "Efficient and secure elliptic curve cryptography implementation of curve p-256"
    },
    {
      "ref_id": "b31",
      "title": "NIST FIPS Pub. 197: Advanced encryption standard (aes)"
    },
    {
      "ref_id": "b32",
      "title": "CVE-2014-0160"
    },
    {
      "ref_id": "b33",
      "title": "Cryptoknight: generating and modelling compiled cryptographic primitives"
    },
    {
      "ref_id": "b34",
      "title": "Genda: A graph embedded network based detection approach on encryption algorithm of binary program"
    },
    {
      "ref_id": "b35",
      "title": "Findcrypt2"
    },
    {
      "ref_id": "b36",
      "title": "A review of cryptographic algorithm recognition technology for binary code"
    },
    {
      "ref_id": "b37",
      "title": "Openssl"
    },
    {
      "ref_id": "b38",
      "title": "Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation"
    },
    {
      "ref_id": "b39",
      "title": "A survey of large language models"
    },
    {
      "ref_id": "b40",
      "title": "Language models are few-shot learners"
    },
    {
      "ref_id": "b41",
      "title": "Palm: Scaling language modeling with pathways"
    },
    {
      "ref_id": "b42",
      "title": "Llama: Open and efficient foundation language models"
    },
    {
      "ref_id": "b43",
      "title": "Codet5+: Open code large language models for code understanding and generation"
    },
    {
      "ref_id": "b44",
      "title": "A systematic evaluation of large language models of code"
    },
    {
      "ref_id": "b45",
      "title": "Wizardcoder: Empowering code large language models with evol-instruct"
    },
    {
      "ref_id": "b46",
      "title": "Code llama: Open foundation models for code"
    },
    {
      "ref_id": "b47",
      "title": "Diffusiongpt: Llm-driven text-to-image generation system"
    },
    {
      "ref_id": "b48",
      "title": "Free-bloom: Zero-shot text-to-video generator with llm director and ldm animator"
    },
    {
      "ref_id": "b49",
      "title": "Binary code summarization: Benchmarking chatgpt/gpt-4 and other large language models"
    },
    {
      "ref_id": "b50",
      "title": "Dwarf debugging information format version 4"
    },
    {
      "ref_id": "b51",
      "title": "Llmaaa: Making large language models as active annotators"
    },
    {
      "ref_id": "b52",
      "title": "Freeal: Towards human-free active learning in the era of large language models"
    },
    {
      "ref_id": "b53",
      "title": "Training language models to follow instructions with human feedback"
    },
    {
      "ref_id": "b54",
      "title": "Codegen: An open large language model for code with multi-turn program synthesis"
    },
    {
      "ref_id": "b55",
      "title": "Codesearchnet challenge: Evaluating the state of semantic code search"
    },
    {
      "ref_id": "b56",
      "title": "Competition-level code generation with alphacode"
    },
    {
      "ref_id": "b57",
      "title": "How machine learning is solving the binary function similarity problem"
    },
    {
      "ref_id": "b58",
      "title": "Dispatcher: Enabling active botnet infiltration using automatic protocol reverse-engineering"
    },
    {
      "ref_id": "b59",
      "title": "Semantic learning and emulation based cross-platform binary vulnerability seeker"
    },
    {
      "ref_id": "b60",
      "title": "Neural network-based graph embedding for cross-platform binary code similarity detection"
    },
    {
      "ref_id": "b84",
      "title": "Graph matching networks for learning the similarity of graph structured objects"
    },
    {
      "ref_id": "b85",
      "title": "Rcfg2vec: Considering long-distance dependency for binary code similarity detection"
    },
    {
      "ref_id": "b86",
      "title": "Precise and accurate patch presence test for binaries"
    },
    {
      "ref_id": "b87",
      "title": "Patch based vulnerability matching for binary programs"
    },
    {
      "ref_id": "b88",
      "title": "Towards practical binary code similarity detection: Vulnerability verification via patch semantic analysis"
    },
    {
      "ref_id": "b89",
      "title": "Cati: Context-assisted type inference from stripped binaries"
    },
    {
      "ref_id": "b90",
      "title": "Arbiter: Bridging the static and dynamic divide in vulnerability discovery on binary programs"
    },
    {
      "ref_id": "b91",
      "title": "Vulseeker: A semantic learning based vulnerability seeker for cross-platform binary"
    },
    {
      "ref_id": "b92",
      "title": "Can a deep learning model for one architecture be used for others? Retargeted-Architecture binary code analysis"
    },
    {
      "ref_id": "b93",
      "title": "Pin - A Dynamic Binary Instrumentation Tool"
    },
    {
      "ref_id": "b94",
      "title": "Valgrind"
    },
    {
      "ref_id": "b95",
      "title": "A lightweight framework for function name reassignment based on large-scale stripped binaries"
    },
    {
      "ref_id": "b96",
      "title": "Direct: A transformer-based model for decompiled variable name recovery"
    },
    {
      "ref_id": "b97",
      "title": "Osprey: Recovery of variable and data structure via probabilistic analysis for stripped binary"
    },
    {
      "ref_id": "b98",
      "title": "Augmenting decompiler output with learned variable names and types"
    },
    {
      "ref_id": "b99",
      "title": "Debin: Predicting debug information in stripped binaries"
    },
    {
      "ref_id": "b100",
      "title": "Degpt: Optimizing decompiler output with llm"
    },
    {
      "ref_id": "b101",
      "title": "Nova: Generative language models for assembly code with hierarchical attention and contrastive learning"
    },
    {
      "ref_id": "b102",
      "title": "Deepseek-coder: When the large language model meets programming-the rise of code intelligence"
    },
    {
      "ref_id": "b103",
      "title": "Llm4decompile: Decompiling binary code with large language models"
    },
    {
      "ref_id": "b104",
      "title": "Machine language model, mlm"
    },
    {
      "ref_id": "b105",
      "title": "Large language models for software engineering: A systematic literature review"
    },
    {
      "ref_id": "b106",
      "title": "A survey on large language models for software engineering"
    },
    {
      "ref_id": "b107",
      "title": "Chatgpt: A study on its utility for ubiquitous software engineering tasks"
    },
    {
      "ref_id": "b108",
      "title": "Large language models based automatic synthesis of software specifications"
    },
    {
      "ref_id": "b109",
      "title": "Self-collaboration code generation via chatgpt"
    },
    {
      "ref_id": "b110",
      "title": "Csgvd: A deep learning approach combining sequence and graph embedding for source code vulnerability detection"
    },
    {
      "ref_id": "b111",
      "title": "Evaluation of context-aware language models and experts for effort estimation of software maintenance issues"
    },
    {
      "ref_id": "b112",
      "title": "Context-based transfer learning for structuring fault localization and program repair automation"
    },
    {
      "ref_id": "b113",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer"
    }
  ]
}