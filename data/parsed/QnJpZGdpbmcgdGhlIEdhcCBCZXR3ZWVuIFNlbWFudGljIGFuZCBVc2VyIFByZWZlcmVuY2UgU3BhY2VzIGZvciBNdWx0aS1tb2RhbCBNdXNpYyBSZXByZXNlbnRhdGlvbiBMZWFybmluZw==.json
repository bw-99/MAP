{"Bridging the Gap Between Semantic and User Preference Spaces for Multi-modal Music Representation Learning": "Xiaofeng Pan \u2217 Jing Chen \u2217 pxfvintage@163.com thunsir@gmail.com NetEase Inc. Hangzhou, China Haitong Zhang Menglin Xing Jiayi Wei zhanghaitong03@corp.netease.com NetEase Inc. Hangzhou, China Xuefeng Mu Zhongqian Xie hzmuxuefeng@corp.netease.com NetEase Inc. Hangzhou, China", "Abstract": "", "ACMReference Format:": "Recent works of music representation learning mainly focus on learning acoustic music representations with unlabeled audios or further attempt to acquire multi-modal music representations with scarce annotated audio-text pairs. They either ignore the language semantics or rely on labeled audio datasets that are difficult and expensive to create. Moreover, merely modeling semantic space usually fails to achieve satisfactory performance on music recommendation tasks since the user preference space is ignored. In this paper, we propose a novel H ierarchical T wo-stage C ontrastive L earning (HTCL) method that models similarity from the semantic perspective to the user perspective hierarchically to learn a comprehensive music representation bridging the gap between semantic and user preference spaces. We devise a scalable audio encoder and leverage a pre-trained BERT model as the text encoder to learn audio-text semantics via large-scale contrastive pre-training. Further, we explore a simple yet effective way to exploit interaction data from our online music platform to adapt the semantic space to user preference space via contrastive fine-tuning, which differs from previous works that follow the idea of collaborative filtering. As a result, we obtain a powerful audio encoder that not only distills language semantics from the text encoder but also models similarity in user preference space with the integrity of semantic space preserved. Experimental results on both music semantic and recommendation tasks confirm the effectiveness of our method.", "CCS Concepts": "\u00b7 Information systems \u2192 Recommender systems ; Music retrieval .", "Keywords": "Multi-modal, Representation Learning, Music Recommendation \u2217 The first two authors contributed equally to this paper. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ICMR '25, Chicago, IL, USA \u00a9 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-1877-9/2025/06 https://doi.org/10.1145/3731715.3733471 Xiaofeng Pan [1], Jing Chen, Haitong Zhang, Menglin Xing, Jiayi Wei, and Xuefeng Mu, Zhongqian Xie . 2025. Bridging the Gap Between Semantic and User Preference Spaces for Multi-modal Music Representation Learning. In Proceedings of the 2025 International Conference on Multimedia Retrieval (ICMR '25), June 30-July 3, 2025, Chicago, IL, USA. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3731715.3733471", "1 Introduction": "Music representations that combine multi-modal song information would help achieve high performance across various downstream tasks [7, 9, 13]. With the rapid progress of deep learning, self-supervised learning methods (e.g., MERT [14] and Audio-MAE [4]) and contrastive learning methods (e.g., COLA [6] and CLMR [8]) have been proven effective in learning meaningful representations with unlabeled audios. Further, CLAP [1] proposes to learn audio concepts from natural language supervision. It requires labeled audio datasets which are difficult, expensive and time-consuming to create for music in particular due to the high technicality and subjectivity. All these methods overlook the rich semantic information conveyed in textual music metadata, especially lyrics, which deliver both linguistic and musical messages in the form of natural language [11] and play a key role in music understanding. Besides, interaction data between users and songs, a new modality of music, are available in vast quantities but usually ignored in general-purpose music representation learning. As demonstrated in [12], simply incorporating semantic representations into search and recommendation tasks yields only marginal improvements. Therefore, several methods [2, 15] propose to learn music representations by combining the above all modalities with contrastive learning. They follow the idea of collaborative filtering, assuming songs that co-occur in a user behavior sequence or a human-created playlist to be similar. However, this assumption is flawed since user behaviors usually carry heavy noise and have no inherent connection to the key notion of similarity in contrastive learning. Guiding the model towards an uncertain similarity metric is likely to bring conflicts with semantic similarity modeling, e.g., songs not similar in semantic space may easily co-occur in user behaviors and therefore be considered as \"similar\". Based on these observations, we propose a novel H ierarchical T wo-stage C ontrastive L earning (HTCL) method that models similarity from the semantic perspective (i.e., audio and text) to the user perspective (i.e., song and song) hierarchically to bridge the gap between semantic and user preference spaces. In contrast to previous works (e.g., MERT, Audio-MAE, CLAP) that train on audio ICMR '25, June 30-July 3, 2025, Chicago, IL, USA Xiaofeng Pan et al. clips and aggregate clip-level representations for full audio representations, we directly model full-length music audio for simplicity, efficiency, and representation integrity. To be specific, we leverage a pre-trained BERT model to encode text information of a song and integrate open-world knowledge. Meanwhile, a scalable audio encoder is devised based on the convolutional neural network (CNN) [5] and Transformer [10] structure, allowing for flexible handling of computational and storage issues. In the first stage, we sample songs from our music platform (60 million DAU) and organize them into audio-text pairs to build a large-scale contrastive pre-training dataset, enabling the learning of multi-modal semantics and relationships between them. In the second stage, we utilize interaction logs from the Similar Recommendation Channel in our platform where users can see the trigger song of each recommended song, so we can collect music pairs that users assume to be similar and construct a dataset with millions of <trig_audio, rec_audio, rec_text> triplets for further contrastive fine-tuning. We believe that similarity voted by users is much more solid than hypothetical \"similarity\" mined by strategies in existing works [2, 6, 8, 15]. In this way, fewer conflicts are introduced into semantic space when modeling user-preferred similarity. Our contributions can be summarized as follows: \u00b7 To the extent of our knowledge, this is the first study of multimodal music representation learning paying attention to the gap between semantic and user preference spaces. We propose a simple yet effective method named HTCL, which models similarity from semantic to user perspectives hierarchically to bridge the gap. \u00b7 We devise a scalable audio encoder to distill language semantics from a pre-trained BERT model, learning audio-text semantics via large-scale contrastive pre-training. Further, we propose an effective approach to exploit interaction data via contrastive fine-tuning to adapt the semantic space to user preference space. \u00b7 Our method is evaluated on real-world datasets from our platform and outperforms state-of-the-art (SOTA) approaches on both music semantic and recommendation tasks. Further, we conduct extensive analysis to confirm the effectiveness of our design. The code and part of evaluation datasets is publicly available 1 to facilitate reproducibility and further research.", "2 Proposed Method": "As shown in Figure 1, the first stage of our HTCL focuses on modeling semantic similarity from a lower-level perspective (i.e., audio and text), while the second stage attends to a higher-level perspective (i.e., song-to-song relationships) to capture user-preferred similarity. The detailed model design will be presented in the following sections.", "2.1 Contrastive Pre-training": "For contrastive learning, we extract the first two minutes of audio (padding shorter tracks with silence) as X \ud835\udc4e and concatenate textual music metadata (e.g., title, lyrics, artists) in a simple format as X \ud835\udc61 , creating abundant training pairs. The HTCL mainly consists of two encoders, i.e., a text encoder E \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 and an audio encoder E \ud835\udc4e\ud835\udc62\ud835\udc51\ud835\udc56\ud835\udc5c . We utilize a pre-trained BERT 2 1 https://github.com/AaronPanXiaoFeng/HTCL 2 https://huggingface.co/google-bert/bert-base-multilingual-uncased model as text encoder to combine open-world knowledge and alleviate training difficulties. It aims to capture the rich linguistic and musical messages embedded in X \ud835\udc61 . Meanwhile, we design E \ud835\udc4e\ud835\udc62\ud835\udc51\ud835\udc56\ud835\udc5c to extract meaningful features from X \ud835\udc4e . Due to the large size of audio files, directly using raw waveforms as inputs would result in substantial storage and computational costs, making it impractical to handle tens of millions of training samples. Therefore, we transform X \ud835\udc4e into Mel-spectrograms [3] x \ud835\udc5a\ud835\udc52\ud835\udc59 with a \ud835\udc47 \u210e ms Hanning window that shifts every \ud835\udc47 \ud835\udc60 ms, which aligns better with the perception of human auditory system toward audios. Then we adopt the Transformer as backbone to capture the local and global patterns in the audio signals. Since the computational complexity of Transformer is proportional to the square of the input sequence length, we use a multi-layer 1-dimensional CNN to further compress x \ud835\udc5a\ud835\udc52\ud835\udc59 with minimal information loss before it is fed into the Transformer. In this way, both efficiency and scalability can be achieved. Finally, we obtain semantic representations for downstream tasks as follows:   where z \ud835\udc61 and z \ud835\udc4e denotes the textual representation and acoustic representation, respectively. For a batch with \ud835\udc35 training samples, we formulate the objective of contrastive pre-training as follows:    where \u00b7 refers to inner product and \ud835\udf0f is a temperature parameter to scale the range of logits. \ud835\udc3f \ud835\udc4e \u2192 \ud835\udc61 and \ud835\udc3f \ud835\udc61 \u2192 \ud835\udc4e measure the similarity between audios and texts from opposite directions. We use the symmetric loss function \ud835\udc3f \ud835\udc4e,\ud835\udc61 to jointly train the audio encoder and the text encoder, with a small learning rate applied to E \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 to tackle the catastrophic forgetting problem.", "2.2 Contrastive Fine-tuning": "In the second stage, we aim to adapt the learned semantics to the user preference space with the integrity of semantic space preserved. To achieve this, we explore a simple yet effective way to exploit interaction data between users and songs. Different from existing works [2, 15] that generate co-occurring song pairs for contrastive learning, we build training samples based on user favored songs in the Similar Recommendation Channel where users can see the trigger song of each recommended song. We consider that the similarity voted (i.e., favored) by users is more solid, since co-occurring song pairs mined from user behaviors usually carry heavy noise and are not inherently similar. Specifically, we construct a dataset with millions of <trig_audio, rec_audio, rec_text> triplets and attempt to minimize the distance between representations of the recommended song and its trigger audio. With the pre-trained E \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 and E \ud835\udc4e\ud835\udc62\ud835\udc51\ud835\udc56\ud835\udc5c , we obtain representations for the trigger audio, recommended audio and textual music Bridging the Gap Between Semantic and User Preference Spaces for Multi-modal Music Representation Learning ICMR '25, June 30-July 3, 2025, Chicago, IL, USA Figure 1: (a) Overall framework of HTCL: two-stage contrastive learning. (b) Model structure of the audio encoder. Lyric, Song Name, Singer, etc. Audio Encoder Audio Encoder Text Encoder Transformer Multi-layer 1D CNN [CLS] Waveform Mel-spectrogram Final representation (a) (b) L a , t L a , a L a , f loss for stage two loss for both stages Fusion Layer metadata, i.e., z \ud835\udc47 \ud835\udc4e , z \ud835\udc45 \ud835\udc4e and z \ud835\udc45 \ud835\udc61 . The z \ud835\udc45 \ud835\udc4e and z \ud835\udc45 \ud835\udc61 are concatenated and fed into a MLP (i.e., the Fusion Layer) to compute z \ud835\udc45 \ud835\udc53 , i.e., representation of the fused semantics. For contrastive fine-tuning, we formulate training losses as follows:  scenario: music genre and language classification. Existing public datasets mainly consist of audio clips, whereas our goal is to classify the full music audio. Therefore, we construct a dataset consisting of 30k full songs, sampled evenly across 24 genres and 26 languages to ensure a comprehensive evaluation. The dataset will be made publicly available for future research. where \ud835\udc4e \ud835\udc47 , \ud835\udc4e \ud835\udc45 and \ud835\udc53 \ud835\udc45 refer to the trigger audio, the recommended audio and the fused semantics, respectively. \ud835\udc3f \ud835\udc4e,\ud835\udc4e and \ud835\udc3f \ud835\udc4e,\ud835\udc53 are devised to align the trigger audio with the recommended audio and the trigger audio with the fused semantics, respectively. Each term in the right of Equation 5 can be calculated in a similar way with Equation 2 and 3. Besides, \ud835\udc3f \ud835\udc4e,\ud835\udc61 is calculated to strengthen the relationships between audio and text modalities of the recommended song, which is considered as a strong positive sample for contrastive learning as it's favored by users. By minimizing the sum of \ud835\udc3f \ud835\udc4e,\ud835\udc4e , \ud835\udc3f \ud835\udc4e,\ud835\udc53 and \ud835\udc3f \ud835\udc4e,\ud835\udc61 , we develop a powerful audio encoder that not only distills language semantics from the text encoder but also models user-preferred similarity.", "3 Experiments": "", "3.1 Experimental Setup": "3.1.1 Datasets. We establish five datasets for the two-stage training and the downstream evaluations. \u00b7 Pre-training and fine-tuning. For the first-stage contrastive pre-training, we build a 50-million-song dataset from our platform's music library with quality and diversity considered. For the secondstage contrastive fine-tuning, we collect favor behaviors from the Similar Recommendation Channel where users can see the trigger song of each recommended song, building a dataset with approximately 4 million triplets of <trig_audio, rec_audio, rec_text> through deduplication and sampling. The detailed statistics are shown in Table 1. \u00b7 Music Semantic Tasks. We evaluate music representations in semantic space using two fundamental tasks from our business \u00b7 Recommendation Tasks. We evaluate music representations in user preference space during both matching and ranking stages of music recommendation. The matching dataset is derived from 3-month logs of our primary recommendation scenario, while the ranking dataset is built from 8-day logs. The detailed statistics are shown in Table 1. 3.1.2 Competitors. MERT [14] and Audio-MAE [4] are representative SOTA methods focusing on acoustic representation learning, while CLAP [1] learns multi-modal representations from audiotext pairs. Additionally, we developed two variants of HTCL, i.e., HTCL_w_CF and HTCL_w/o_text . Rather than using the dataset described in Section 3.1.1, HTCL_w_CF follows the idea of collaborative filtering to generate co-occurring song pairs [2, 15] for contrastive fine-tuning. HTCL_w/o_text differs from HTCL by excluding the text modality during contrastive fine-tuning. To ensure fair comparison, we conduct experiments using representations obtained from the audio encoders of various methods. 3.1.3 Implementation Details and Evaluation Metrics. Wetransform raw waveform into 128-dimensional Mel-spectrograms with 128ms Hanning window that shifts every 96ms, resulting in 1x1251x128 feature map for a 120-second song. The audio encoder of our HTCL contains a 3-layer 1-dimensional CNN that has 512 channels with strides (2,2,2) and kernel widths (5,3,3), following by a 12-layer Transformer using 12-head attention with hidden_size=768. We perform contrastive pre-training on 8 A100-80G GPUs (batch size: 800, learning rate: 1e-3) and fine-tuning on 3 A100-80G GPUs (batch size: 240, learning rate: 1e-4). To preserve open-world knowledge and mitigate catastrophic forgetting, we apply a small learning rate (3e-5) to the pre-trained BERT. The Adam optimizer is utilized ICMR '25, June 30-July 3, 2025, Chicago, IL, USA Table 1: Statistics of the established datasets Table 2: Results of comparison experiments throughout. To ensure fair comparison, Audio-MAE and CLAP are also trained on our pre-training dataset following their empirically optimal hyperparameter settings. For music semantic tasks, we feed music representations into a single-layer MLP for genre and language classification, using accuracy ( ACC ) as evaluation metric. For matching stage of music recommendation, we choose next item prediction as evaluation task. For each user, we randomly select a favored song as the target and collect 30 previously favored songs as triggers. We then retrieve the top 10 similar songs for each trigger based on the cosine distance between music representations, using hit rate ( HR@100 ) as evaluation metric. For ranking stage of music recommendation, we predict click-through and favor probabilities (i.e., music CTR and CVR tasks), using AUC as evaluation metric. We train DIN [16] models on the first 7 days of ranking dataset and evaluate on the last day, with embeddings initialized by music representations.", "3.2 Experimental Results": "The comparison results are presented in Table 2 and the major observations are summarized as follows. \u00b7 Across both music semantic and recommendation tasks, CLAP outperforms SOTA acoustic representation methods MERT and Audio-MAE, particularly in language classification, emphasizing the importance of the text modality. However, the gap between semantic and user preference spaces results in limited performance gains when applying CLAP's multi-modal representations to music recommendation, particularly in ranking tasks. \u00b7 HTCL_w_CF attempts to bridge this gap through an additional contrastive fine-tuning stage. However, it performs poorly on music semantic tasks, as its fine-tuning stage assumes co-occurring songs in user behaviors to be similar, potentially conflicting with semantic similarity modeling. Moreover, it fails to achieve satisfactory improvements in ranking tasks, as what it learns is highly homogenized with ranking models trained on user behaviors. \u00b7 HTCL yields the best performance across all tasks, achieving further improvements in music semantic tasks and significantly outperforming the runner-up in music recommendation. These results confirm the impacts of the gap between semantic and user preference spaces, and validate the effectiveness of our proposed method. Besides, HTCL_w/o_text underperforms CLAP in music", "Xiaofeng Pan et al.": "CLAP label 0.0175 abel 0,0150 0.0125 0.0100 0,0075 0.0050 0.0025 0 4 HTCL 0.0200 label label 0.0175 0.0150 0.0125 0.0100 0.0075 20050 0.0025 score Score Figure 2: Distance distributions of anchor-positive and anchor-negative pairs calculated with representations from different methods. semantic tasks, further emphasizing the importance of the text modality, even during contrastive fine-tuning.", "3.3 Effectiveness analysis": "Taking a step further, we investigate whether our approach can learn user-preferred similarity on the basis of semantic similarity, which we consider as the main reason for performance gains. First, we randomly sampled 100k positive and negative samples respectively from 3-day logs of our Heuristic Search Channel where each sample is a song recommended based on a user's currently searched song (defined as the anchor). Therefore, positive samples, i.e., favored recommendations with corresponding anchors, can serve as good indicators of user-preferred similarity. We then computed the cosine distance score between representations of each sample and its anchor. As shown in Figure 2, for HTCL, the distance distribution of anchor-positive pairs noticeably shifts to the right compared to that of anchor-negative pairs, indicating that user-favored recommended songs tend to have higher similarity scores with their corresponding anchors. In contrast, for CLAP, distance distributions of anchor-positive and anchor-negative pairs are barely distinguishable. This visualization empirically demonstrates that HTCL's representations capture user-preferred similarity more effectively than prior methods. Besides, with large-scale and diverse song collections utilized during contrastive pre-training, we can effectively guide the learning of HTCL through fine-tuning on a relatively small yet high-quality set of similar song pairs, while maintaining robust generalization capabilities.", "4 Conclusion": "In this paper, we investigate the challenges for multi-modal music representations and pay attention to the gap between semantic and user preference spaces. We propose a novel method named HTCL to combine modalities of audio, text, and user interactions to bridge the gap. By modeling similarity hierarchically in semantic and user preference spaces, our HTCL achieves better flexibility and generalization to downstream tasks. Future work will enhance the Fusion Layer via advanced multi-modal fusion techniques (replacing the simple MLP) and optimize music recommendation with multi-modal representations. Bridging the Gap Between Semantic and User Preference Spaces for Multi-modal Music Representation Learning ICMR '25, June 30-July 3, 2025, Chicago, IL, USA", "References": "[1] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang. 2023. Clap learning audio concepts from natural language supervision. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) . IEEE, 1-5. [2] Andres Ferraro, Xavier Favory, Konstantinos Drossos, Yuntae Kim, and Dmitry Bogdanov. 2021. Enriched music representations with multiple cross-modal contrastive learning. IEEE Signal Processing Letters 28 (2021), 733-737. [3] Yuan Gong, Yu-An Chung, and James Glass. 2021. Ast: Audio spectrogram transformer. arXiv preprint arXiv:2104.01778 (2021). [4] Po-Yao Huang, Hu Xu, Juncheng Li, Alexei Baevski, Michael Auli, Wojciech Galuba, Florian Metze, and Christoph Feichtenhofer. 2022. Masked autoencoders that listen. Advances in Neural Information Processing Systems 35 (2022), 2870828720. [5] Zewen Li, Fan Liu, Wenjie Yang, Shouheng Peng, and Jun Zhou. 2021. A survey of convolutional neural networks: analysis, applications, and prospects. IEEE transactions on neural networks and learning systems 33, 12 (2021), 6999-7019. [6] Aaqib Saeed, David Grangier, and Neil Zeghidour. 2021. Contrastive learning of general-purpose audio representations. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) . IEEE, 3875-3879. [7] Zhexu Shen, Liang Yang, Zhihan Yang, and Hongfei Lin. 2023. More than simply masking: Exploring pre-training strategies for symbolic music understanding. In Proceedings of the 2023 ACM International Conference on Multimedia Retrieval . 540-544. [8] Janne Spijkervet and John Ashley Burgoyne. 2021. Contrastive learning of musical representations. arXiv preprint arXiv:2103.09410 (2021). [9] Stefanos Stoikos, David Kauchak, Douglas Turnbull, and Alexandra Papoutsaki. 2023. Cross-Language Music Recommendation Exploration. In Proceedings of the 2023 ACM International Conference on Multimedia Retrieval . 664-668. [10] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems . 5998-6008. [11] Kento Watanabe and Masataka Goto. 2020. Lyrics information processing: Analysis, generation, and applications. In Proceedings of the 1st Workshop on NLP for Music and Audio (NLP4MusA) . 6-12. [12] Jia-Qi Yang, Chenglei Dai, OU Dan, Ju Huang, De-Chuan Zhan, Qingwen Liu, Xiaoyi Zeng, and Yang Yang. 2023. COURIER: Contrastive User Intention Reconstruction for Large-Scale Pre-Train of Image Features. arXiv preprint arXiv:2306.05001 (2023). [13] Kaixing Yang, Xukun Zhou, Xulong Tang, Ran Diao, Hongyan Liu, Jun He, and Zhaoxin Fan. 2024. BeatDance: A Beat-Based Model-Agnostic Contrastive Learning Framework for Music-Dance Retrieval. In Proceedings of the 2024 International Conference on Multimedia Retrieval . 11-19. [14] LI Yizhi, Ruibin Yuan, Ge Zhang, Yinghao Ma, Xingran Chen, Hanzhi Yin, Chenghao Xiao, Chenghua Lin, Anton Ragni, Emmanouil Benetos, et al. 2023. MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training. In The Twelfth International Conference on Learning Representations . [15] Xinping Zhao, Ying Zhang, Qiang Xiao, Yuming Ren, and Yingchun Yang. 2023. Bootstrapping Contrastive Learning Enhanced Music Cold-Start Matching. In Companion Proceedings of the ACM Web Conference 2023 . 351-355. [16] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through rate prediction. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1059-1068."}
