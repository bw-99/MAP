{"AURO: Reinforcement Learning for Adaptive User Retention Optimization in Recommender Systems": "Zhenghai Xue zhenghai001@e.ntu.edu.sg Nanyang Technological University Singapore Bin Yang yangbin@kuaishou.com Kuaishou Technology Beijing, China", "Lantao Hu": "hulantao@kuaishou.com Kuaishou Technology Beijing, China", "Qingpeng Cai \u2217": "caiqingpeng@kuaishou.com Kuaishou Technology Beijing, China", "Peng Jiang": "jiangpeng@kuaishou.com Kuaishou Technology Beijing, China", "Bo An": "boan@ntu.edu.sg Nanyang Technological University Skywork AI Singapore", "Abstract": "", "CCS Concepts": "The field of Reinforcement Learning (RL) has garnered increasing attention for its ability of optimizing user retention in recommender systems. A primary obstacle in this optimization process is the environment non-stationarity stemming from the continual and complex evolution of user behavior patterns over time, such as variations in interaction rates and retention propensities. These changes pose significant challenges to existing RL algorithms for recommendations, leading to issues with dynamics and reward distribution shifts. This paper introduces a novel approach, AURO, to address this challenge. To navigate the recommendation policy in non-stationary environments, AURO introduces an state abstraction module in the policy network. The module is trained with a new value-based loss function, aligning its output with the estimated performance of the current policy. As the policy performance of RL is sensitive to environment drifts, the loss function enables the state abstraction to be reflective of environment changes and notify the recommendation policy to adapt accordingly. Additionally, the nonstationarity of the environment introduces the problem of implicit cold start, where the recommendation policy continuously interacts with users displaying novel behavior patterns. AURO encourages exploration guarded by performance-based rejection sampling to maintain a stable recommendation quality in the cost-sensitive online environment. Extensive empirical analysis are conducted in a user retention simulator, the MovieLens dataset, and a live shortvideo recommendation platform, demonstrating AURO's superior performance against all evaluated baseline algorithms 1 . 1 Code is available at Github. This work is licensed under a Creative Commons Attribution 4.0 International License. WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia \u00a9 2025 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-1274-6/25/04 https://doi.org/10.1145/3696410.3714956 \u00b7 Information systems \u2192 Recommender systems ; \u00b7 Computing methodologies \u2192 Reinforcement learning .", "Keywords": "Reinforcement Learning; recommender systems; user retention; non-stationary environments", "ACMReference Format:": "Zhenghai Xue, Qingpeng Cai, Bin Yang, Lantao Hu, Peng Jiang, Kun Gai, and Bo An. 2025. AURO: Reinforcement Learning for Adaptive User Retention Optimization in Recommender Systems. In Proceedings of the ACM Web Conference 2025 (WWW '25), April 28-May 2, 2025, Sydney, NSW, Australia. ACM,NewYork,NY,USA,11pages.https://doi.org/10.1145/3696410.3714956", "1 Introduction": "Recent advances in recommender systems have shown promising results in enhancing user retention through the application of Reinforcement Learning (RL) [6, 45, 55], primarily due to RL's capacity to effectively manage delayed reward signals [7, 37] and promote efficient exploration strategies [5, 11]. This focus on long-term engagement, as opposed to short-term feedback, is driven by its direct correlation with key performance indicators such as daily active users (DAU) and user dwell time. Nonetheless, the dynamic and ever-changing nature of large-scale online recommendation platforms presents a significant challenge to RL-based recommendation algorithms, with constant shifts in user behavior patterns. For instance, stock traders may exhibit increased activity on a news recommendation application during periods of significant financial news, and promotions might boost user interactions with recommended products in a shopping application. From the perspective of RL, different user behavior patterns will lead to constantly evolving environment dynamics and reward functions. Due to the poor generalization ability of standard RL methods [23, 46], policies that behave well during training can struggle in the deployment phase due to the evolving recommendation environment. Kun Gai gai.kun@qq.com Unaffiliated Beijing, China To improve the generalization ability of RL in dynamic environments, current algorithms in Meta-RL or zero-shot policy generalization [30, 46] attempt to explicitly identify one unique environment parameter as the latent feature corresponding to certain environments. But in the recommendation task, user behaviors exhibit distinct patterns in different time periods, including the positive ratio of immediate feedback, such as click, like, comment, and hate, as well as long-term feedback such as the distribution of user return time, as demonstrated in the motivating example in Sec. 3.1. These factors collectively contribute to a non-stationary recommendation environment, one that cannot be simplified to a single hidden parameter. The non-stationary environment also leads to the challenge of implicit cold start [20], where the recommendation policy continually interacts with users exhibiting new behavior patterns. Therefore, the policy needs to be able to actively adapt to new settings. The exploration ability of some RL methods [5, 11] facilitates such requirement of rapid policy adaptation, but they can be overly optimistic and propose unreliable exploratory actions in the cost-sensitive recommendation environment. To conclude, the dynamic and evolving recommendation environment leads to two critical challenges: to accurately identify complex environment changes and to make safe adaptations to these changes. In this work, we propose a new approach termed A daptive U ser R etention O ptimization (AURO) that simultaneously addresses both challenges. As RL policies are sensitive to distribution shift [6, 46], the policy performance will be a reliable and universal signal for detecting changes in the recommendation environment. Therefore, to identify non-stationarity actively and universally, we introduce an additional state abstraction module in the policy network, aligning the state abstraction output with state value functions, which serve as proxy performance measure of the current policy. The state abstraction module will then generate different outputs in correspondence with environment changes and notify the learning policy when it is no longer suitable for new user behavior patterns. For rapid policy adaptation, optimism under uncertainty [10, 11] is a common approach for rapid adaptation with exploratory actions. To ensure a safe exploration process, AURO proposes to generate a pool of candidate exploration actions and filter them based on their estimated state-action values. This approach significantly lowers the likelihood of choosing exploration actions that could have adverse effects, thereby enhancing the stability and reliability of the online training process. To assess the effectiveness of AURO when optimizing user retention in various recommendation tasks, we conduct experiments with a user retention simulator [49] and the MovieLens dataset. We also fully launch AURO in a popular short video recommendation platform. The outcomes of these experiments indicate that AURO outperforms contemporary state-of-the-art methods in both the training stability and the adaptation capacity within complex and non-stationary recommendation environments.", "2 Background": "Fig. 1 illustrates the connection between RL and the practical recommendation process. In this framework, users are treated as environments and the RL policy operates by feeding actions to the environment. As the number of candidate items can be very large and make discrete action selection burdensome, we follow the practice in literature [6, 45] and employ \ud835\udc58 -dimensional continuous actions. At step \ud835\udc61 , the RL policy generates the actions \ud835\udc4e \ud835\udc61 . A pretrained deep scoring model is used to predict a \ud835\udc58 dimensional score \ud835\udc65 \ud835\udc56 = ( \ud835\udc65 \ud835\udc56 1 , \ud835\udc65 \ud835\udc56 2 , \u00b7 \u00b7 \u00b7 , \ud835\udc65 \ud835\udc56\ud835\udc58 ) for each candidate item \ud835\udc56 , where each dimension evaluates the item in a particular aspect. The scoring model will be treated as a black box in this paper. Subsequently, a predefined ranking function \ud835\udc53 is employed to compute the final ranking score \ud835\udc53 ( \ud835\udc4e \ud835\udc61 , \ud835\udc65 \ud835\udc56 ) for each selected item \ud835\udc56 . The system then recommends the top\ud835\udc5b items to the user according to the ranking score. User Open App Candidate Items RL Policy Scoring Model State \ud835\udc94 \ud835\udc95 Top-n Items Immediate Reward \ud835\udc93 \ud835\udc95 \ud835\udc8a\ud835\udc8e Action \ud835\udc82 \ud835\udc95 Leave App Return APP Retention Reward \ud835\udc93 \ud835\udc95 \ud835\udc93\ud835\udc86 According to the formulation in Fig. 1, fluctuations in user behavior patterns can make the recommendation environment nonstationary. To model the recommendation task in such environments, we employ the Hidden Parameter Markov Decision Process (HiP-MDP) [13] < S , A , \u0398 , \ud835\udc47 , \ud835\udc5f , \ud835\udefe, \ud835\udf0c 0 > that attributes the nonstationarity to an agnostic hidden parameter distribution \u0398 . In the HiP-MDP, S is the continuous state space. A is the \ud835\udc58 -dimensional continuous action space. \u0398 is the joint hidden parameter distribution. When the episode starts, \ud835\udf03 \u223c \u0398 is sampled reflecting the current status of the non-stationary environment. Both \u0398 and \ud835\udf03 are invisible to RL algorithms. \ud835\udc47 is the transition function. At step \ud835\udc61 , \ud835\udc47 updates the user profile and browsing history in state \ud835\udc60 \ud835\udc61 according to the action \ud835\udc4e \ud835\udc61 and current hidden parameters \ud835\udf03 , generating the next state distribution \ud835\udc60 \ud835\udc61 + 1 \u223c \ud835\udc47 \ud835\udf03 (\u00b7| \ud835\udc60 \ud835\udc61 , \ud835\udc4e \ud835\udc61 , \ud835\udf03 ) . \ud835\udc5f is the reward function. To optimize user retention, the time gap of users returning to the recommendation platform is used to calculate the retention reward assigned to the last step of an episode \ud835\udc5f \ud835\udf03 ( \ud835\udc60 0 , \ud835\udc4e 0 , \ud835\udc60 1 , \ud835\udc4e 1 , \u00b7 \u00b7 \u00b7 , \ud835\udc60 \ud835\udc61 , \ud835\udc4e \ud835\udc61 , \ud835\udf03 ) = \ud835\udf06 \u00d7 user return time, where \ud835\udf06 is the predefined weighing coefficient. The reward is set to zero at other steps. \ud835\udefe is the discount factor that determines how much the policy accounts for rewards in future steps. \ud835\udf0c 0 is the initial state distribution. When the session starts, \ud835\udc60 0 is randomly sampled according to \ud835\udf0c 0 and the current hidden parameter \ud835\udf03 : \ud835\udc60 0 \u223c \ud835\udf0c 0 (\u00b7| \ud835\udf03 ) . We aim at maximizing the expected accumulated return of the policy \ud835\udf0b : \ud835\udf02 \ud835\udc47 ( \ud835\udf0b ) = \ud835\udc38 \ud835\udf0b,\ud835\udc47, \u0398 [ \u02dd \u221e \ud835\udc61 = 0 \ud835\udefe \ud835\udc61 \ud835\udc5f \ud835\udf03 ( \ud835\udc60 0 , \ud835\udc4e 0 , \u00b7 \u00b7 \u00b7 , \ud835\udc60 \ud835\udc61 , \ud835\udc4e \ud835\udc61 , \ud835\udf03 )] , where the expectation is computed with \ud835\udc4e \ud835\udc61 \u223c \ud835\udf0b (\u00b7| \ud835\udc60 \ud835\udc61 ) , \ud835\udc60 \ud835\udc61 + 1 \u223c \ud835\udc47 \ud835\udf03 (\u00b7| \ud835\udc60 \ud835\udc61 , \ud835\udc4e \ud835\udc61 , \ud835\udf03 ) , and \ud835\udf03 \u223c \u0398 . The state-action value function \ud835\udc44 \ud835\udf0b ( \ud835\udc60, \ud835\udc4e ) denotes the expected return after taking action \ud835\udc4e at state \ud835\udc60 : \ud835\udc44 \ud835\udf0b ( \ud835\udc60, \ud835\udc4e ) = \ud835\udc38 \ud835\udf0b,\ud835\udc47, \u0398 \u02dd \u221e \ud835\udc61 = 0 \ud835\udefe \ud835\udc61 \ud835\udc5f \ud835\udf03 ( \ud835\udc60, \ud835\udc4e, \u00b7 \u00b7 \u00b7 , \ud835\udc60 \ud835\udc61 , \ud835\udc4e \ud835\udc61 , \ud835\udf03 ) . The state value function, or the V-function, is defined as \ud835\udc49 \ud835\udf0b ( \ud835\udc60 ) = E \ud835\udc4e \u223c \ud835\udf0b (\u00b7| \ud835\udc60 ) \ud835\udc44 \ud835\udf0b ( \ud835\udc60, \ud835\udc4e ) .", "3 AURO: Adaptive User Retention Optimization 3.1 Motivating Example": "The main focus of this paper is to address the challenge of nonstationary environment arisen from multiple fluctuation factors of user behaviors. To empirically justify the existence of such issue characterize the complex and non-stationary nature of the recommendation environment, where several different aspects of user behaviors evolve over time. in practical recommendation systems, we conduct a data-driven study in a popular short-video recommendation platform, which is also used in live experiments in Sec. 4.3. To avoid the influence of different recommended items and interaction histories, we select interactions that occur at the beginning of each recommendation session, i.e., the \ud835\udc60 0 of the trajectory. For the purposes of this paper, we are primarily interested in investigating the users' return-time distribution and the positive ratio of immediate feedback among all data, including click, like, follow, comment, and hate. We visualize the normalized interaction ratio across different dates and hours in Fig. 2 (left). The distribution of the user return time in three weeks is also illustrated in Fig. 2 (right). We also demonstrate the normalized standard deviations of several immediate interaction ratio in Tab. 1. The return probability and interaction ratio exhibit large variability over time. For example, the average probability of user returning to the application in the next day can be as low as about 70% in week 1, and as high as about 81% in week 2. Among immediate signals, the 'click' and 'like' signals exhibit relatively more stability, while the other three signals fluctuates significantly, with 3 to 4 times higher standard deviation than 'like'. For example, the 'follow' signal is about twice more frequent on day 5 than on day 9 and the 'comment' signal is three times more frequent on 8 a.m. than on 3 and 5 a.m. Furthermore, we can hardly identify any clear pattern in the changes of interaction frequency between dates within a month or hours within a day. Therefore, it becomes challenging to manually extract environment information with rules and feed to the policy network. Overall, these findings", "3.2 Framework Overview": "To tackle the aformentioned challenges, we introduce a new paradigm called A daptive U ser R etention O ptimization (AURO). Its architecture is shown in Fig. 3 and the algorithm procedure is listed in Appendix C. As discussed in Sec. 2, the recommendation policy takes user features, item features, and item history as input. They are processed by embedding networks and transformers, before being concatenated together and generating the vectorized state. In the following part of this section, we discuss two key contributions of AURO, namely the state abstraction network and the guarded online exploration procedure.", "3.3 Universal Non-stationarity Identification with Value-based State Abstraction": "In this paper, we introduce an extra state abstraction network and align its outputs with the expected performance of the current policy, which is more universal in reflecting environment nonstationarity. When a new hidden parameter \ud835\udf03 in HiP-MDP is sampled, the policy performance will be degraded due to dynamics and reward distribution shift, and therefore change accordingly with the evolvement of the dynamic environment. The output of a well-aligned state abstraction network can then serve as a universal approach of identifying the current environment status in face of non-stationarity. To achieve the abstraction-performance alignment, we enforce the \ud835\udc59 2-distance in the state abstraction space to be close to a performance-related distance measure \ud835\udc51 that reflects policy performance. The loss function for updating the state abstraction network \ud835\udf19 can then be expressed as In the Actor-Critic architecture of RL, the state value function \ud835\udc49 can serve as the critic and can evaluate the performance of the learning policy. If both two states \ud835\udc60 \ud835\udc56 , \ud835\udc60 \ud835\udc57 have high state values, the policy will perform well on both of them, so the latent variable \ud835\udf19 ( \ud835\udc60 \ud835\udc56 ) should be close to \ud835\udf19 ( \ud835\udc60 \ud835\udc57 ) . If two states have different state values, their corresponding latent variables should be far from each other. Therefore, we choose the following distance measure based on the state value function \ud835\udc49 : User Features Item Features Item History Embedding Network Transformers Embedding Network User Embedding Item Embedding History Embedding Concat State Vector State Abstraction Network Moving Average Abstraction Loss \ud835\udf53 \" \ud835\udf53 Actor Network \ud835\udc82 \ud835\udfcf \ud835\udc82 \ud835\udfd0 \u22ef \ud835\udc82 \ud835\udc8c Action \ud835\udc78, \ud835\udc7d Values Critic Network Candidate Actions (a) User State Encoding Module (b) Actor-Critic Module with State Abstraction Generate Rejection Sampling Exploration Action (c) Exploration Module By setting the distance function to have extreme values with different inputs of estimated policy performances, the state abstractor \ud835\udf19 ( \ud835\udc60 ) will be provided with more precise and simplified signals to discriminate different environments. As illustrated in Fig. 3, the state abstraction \ud835\udf19 ( \ud835\udc60 ) also serves as an additional input to the state-value function \ud835\udc49 for more accurate value estimation. The comparison of value error 2 during training between different algorithms is in Fig. 6 (d). The value estimation in AURO is more accurate than other algorithms without state abstraction. This justifies the feasibility of using state value functions as proxies of policy performance. Meanwhile, lower value error corresponds to better training stability in face of non-stationary environments, demonstrating the effectiveness of the additional state abstraction module. To determine whether \ud835\udc49 ( \ud835\udc60 \ud835\udc56 , \ud835\udf19 ( \ud835\udc60 \ud835\udc56 )) and \ud835\udc49 ( \ud835\udc60 \ud835\udc57 , \ud835\udf19 ( \ud835\udc60 \ud835\udc57 )) are close, we rank a batch of input states \ud835\udc60 1 , \ud835\udc60 2 , \u00b7 \u00b7 \u00b7 , \ud835\udc60 \ud835\udc35 with size \ud835\udc35 by their state values and divide them into \ud835\udc5b categories \ud835\udc36 1 , \ud835\udc36 2 , \u00b7 \u00b7 \u00b7 , \ud835\udc36 \ud835\udc5b , where \ud835\udc5b is a hyperparameter. States that are assigned to the same category are considered to have similar values. We denote \ud835\udc57 \u2208 \ud835\udc41 ( \ud835\udc56 ) if \ud835\udc60 \ud835\udc56 and \ud835\udc60 \ud835\udc57 fall into the same category. Plugging Eq. (2) into the original loss function Eq. (1), we get 3 where the first term makes state abstractions in the same category closer, and the second term pushes them in different categories away from each other. In practice, the state-value function \ud835\udc49 is updated alongside policy training and the state batch is randomly sampled from the replay buffer that keeps updating. Therefore, the output \ud835\udf19 ( \ud835\udc60 \ud835\udc56 ) can be unstable, which is undesirable when using \ud835\udf19 ( \ud835\udc60 \ud835\udc56 ) as part of the policy input. To mitigate this problem, we incorporate the moving average \u02dc \ud835\udf19 \ud835\udc58 = ( 1 -\ud835\udf02 ) \u02dc \ud835\udf19 \ud835\udc58 + \ud835\udf02 | \ud835\udc36 \ud835\udc58 | \u02dd \ud835\udc60 \ud835\udc56 \u2208 \ud835\udc36 \ud835\udc58 \ud835\udf19 ( \ud835\udc60 \ud835\udc56 ) , \ud835\udc58 = 1 , 2 , \u00b7 \u00b7 \u00b7 , \ud835\udc5b of each state categories to both terms in Eq. (3). We transform Eq. (3) to make it related with the average state abstraction \u00af \ud835\udf19 \ud835\udc58 = 1 | \ud835\udc36 \ud835\udc58 | \u02dd \ud835\udc60 \ud835\udc56 \u2208 \ud835\udc36 \ud835\udc58 \ud835\udf19 ( \ud835\udc60 \ud835\udc56 ) . With regard to the first term (denoted as \ud835\udc3d same ( \ud835\udf19 ) ), we have To maximize \ud835\udc3d ( \ud835\udf19 ) with states from different categories (denoted as \ud835\udc3d diff ( \ud835\udf19 ) ), we have By minimizing the last term in Eq. (4), we are minimizing an upperbound of the original loss function. The average state abstraction \u00af \ud835\udf19 \ud835\udc58 can be replaced with the moving average \u02dc \ud835\udf19 \ud835\udc58 , giving rise to the final loss function that is used during training: In practice, in addition to the value-based loss function \ud835\udc3d ( \ud835\udf19 ) , the policy optimization loss is also backpropagated to the state abstraction network during training to accelerate the training process.", "3.4 Guarded Online Exploration for Implicit Cold Start": "In the non-stationary recommendation environment illustrated in Sec. 3.1, the recommendation policy is continuously facing users with new behavior patterns. From the perspective of HiP-MDP, \ud835\udc4e Q !\" \ud835\udc60, \ud835\udc4e \ud835\udc4e ! \u2207 ! Q \"# \ud835\udc60, \ud835\udc4e Possible Exploration action 1.0 0.5 0.0 0.5 1.0 Action (Relative) 1e 1 0.18 0.17 0.16 0.15 0.14 0.13 Upper-bound Q-value 4 2 0 2 4 Action (Relative) 1e 2 0.17 0.16 0.15 0.14 0.13 Upper-bound Q-value Original Action Exploration Action with Fixed Step Size different \ud835\udf03 at different episodes will lead to new environment dynamics \ud835\udc47 and reward functions \ud835\udc5f . Although the state abstraction network in Sec. 3.3 can help policies identify environment novelty, the policy itself needs to be able to actively adapt to new environment parameters. This is similar to the well-known cold-start issue in recommendation [21, 33], but happens implicitly during online training. where \ud835\udeff is the hyper-parameter controlling the gap of action particles. It can be set to a small value and does not need extra tuning, as shown by the ablation study in Tab. 2. By choosing from several candidate actions, the exploration module manages to find actions with higher state-action values more efficiently and reduces the risk of adopting dangerous actions that have low values. We also visualize the effectiveness of this exploration technique in Fig. 6. An ideal approach to deal with such issue in RL is exploration. We consider the approach of optimism under uncertainty [11] with the optimistic state-action value estimation, defined as \ud835\udc44 UB ( \ud835\udc60, \ud835\udc4e ) = \ud835\udf07 \ud835\udc44 ( \ud835\udc60, \ud835\udc4e ) + \ud835\udefd\ud835\udf0e \ud835\udc44 ( \ud835\udc60, \ud835\udc4e ) , where \ud835\udf07 \ud835\udc44 and \ud835\udf0e \ud835\udc44 represent the mean and standard deviation of the outputs from the Q networks, respectively, and \ud835\udefd is a hyper-parameter. The exploration action \ud835\udc4e \ud835\udc38 can be calculated by extending the original policy output \ud835\udc4e \ud835\udc47 in the direction of gradient ascent of \ud835\udc44 UB ( \ud835\udc60, \ud835\udc4e ) [11]: where [\u2207 \ud835\udc4e \ud835\udc44 UB ( \ud835\udc60, \ud835\udc4e )] \ud835\udc4e = \ud835\udc4e \ud835\udc47 is the gradient of \ud835\udc44 UB with respect to the action \ud835\udc4e \ud835\udc47 , and \ud835\udeff is the step size hyperparameter. However, determining the extent to which the original action should be extended, i.e., the step size \ud835\udeff , can be challenging, as a small step size may lead to inefficient exploration, and a large step size can result in inaccurate gradient approximation. As illustrated in Fig.4 (left), the state-action function can exhibit multiple peaks, and an improper step size may cause the exploration action to miss a local optimum. The challenge of selecting an appropriate step size \ud835\udeff is more pronounced in recommendation tasks, as the landscape of stateaction values in such tasks can exhibit high complexity, which is illustrated in Fig. 4 (middle and right). Another challenge is that recommendation tasks can be risk-sensitive: users may disengage from the application and cease the recommendation process if they encounter recommended items that fail to capture their interest. To mitigate the aforementioned issues, we propose to generate a set of actions with optimistic exploration and filter out candidate actions that may lead to poor recommendation quality with rejection sampling. The actions \ud835\udc4e \ud835\udc58 , \ud835\udc58 = 1 , 2 , \u00b7 \u00b7 \u00b7 , \ud835\udc41 are located near the original policy output \ud835\udf0b ( \ud835\udc60, \ud835\udf19 ( \ud835\udc60 )) , in the direction of the gradient \u2207 \ud835\udc4e \ud835\udc44 UB ( \ud835\udc60, \ud835\udc4e ) . The exploration action \ud835\udc4e \ud835\udc38 is selected form the action set according to the average Q-value: \ud835\udc4e \ud835\udc38 = arg max \ud835\udc4e \ud835\udf07 \ud835\udc44 ( \ud835\udc60, \ud835\udc4e \ud835\udc58 ) , \ud835\udc4e \ud835\udc58 = \ud835\udc4e \ud835\udc47 + \ud835\udc58\ud835\udeff [\u2207 \ud835\udc4e \ud835\udc44 UB ( \ud835\udc60, \ud835\udc4e )] \ud835\udc4e = \ud835\udf0b ( \ud835\udc60 ) (7)", "4 Experiments": "To evaluate and analyse the practical performance of AURO, we conduct extensive experiments to investigate the following research questions (RQs): RQ1: Can AURO optimize user retention better than baseline algorithms, when applied to recommendation datasets and simulators? RQ2: How does each component of AURO contribute to overall performance? RQ3: Can AURO perform well in online A/B tests of large-scale live recommendation platforms? To answer these questions, the state abstraction module in AURO is used to generate recommendation policies in the modified MovieLens1M dataset and the KuaiSim retention simulator [49]. We conduct ablation studies and visualizations to investigate the contribution of each component of AURO to the overall performance. AURO is also deployed in a dynamic, large-scale, real-world short video recommendation platform to perform live A/B test.", "4.1 Experiments in Retention Simulator": "Setup AURO adopts a novel paradigm that focuses on optimizing user retention, rather than immediate user feedback. However, recommendation simulators that have been widely used [19, 35, 40] cannot simulate user retention behaviors. The KuaiSim retention simulator [49] aims to simulate long-term user retention on short video recommendation platforms. It has verified several recommendation algorithms [6, 26, 27] that also investigate user retentions. The KuaiSim simulator contains a user leave module, which predicts whether the user will leave the session and terminate the episode; and a user return module, which predicts the probability of the user returning to the platform on each day as a multinomial distribution. The probabilities of user leaving and returning are altered in each episode to capture the dynamic nature of user behaviors and create a non-stationary evaluation environment. More information on the simulator setup is in Appendix D.1. 0 1 2 3 4 5 Episode 1e4 1.5 1.6 1.7 1.8 1.9 2.0 Average Return Days ( ) 0 1 2 3 4 5 Episode 1e4 0.650 0.675 0.700 0.725 0.750 0.775 0.800 0.825 0.850 Return Probability @ Day 1 ( ) CEM TD3 SAC ESCP RLUR AURO (Ours) 0 1 2 3 4 5 Episode 1e4 2.0 1.8 1.6 1.4 1e 2 Retention Reward ( ) For baseline algorithms, we include non-RL recommendation methods (CEM [12], DIN [52]), state-of-the-art value-based RL algorithms (DDPG [24], TD3 [17], SAC [18]), RL algorithms facilitating efficient exploration (OAC [11], RND [5]), context encoder-based RL algorithms (ESCP [30]), and RL-based recommendation algorithms for optimizing user retention (RLUR [6], OSPIA [43], RL-LTV [20]). Detailed descriptions of the baseline algorithms are in Appendix D.2. In simulator-based experiments, we choose three metrics to evaluate the algorithms: the users' average return days (lower is better), the users' return probability on the next day (higher is better), and the retention reward defined in Sec. 2 (higher is better). All algorithms for comparison are run for 50,000 training steps with five different random seeds. Performance Comparison The training curve for AURO, as well as baseline algorithms CEM, TD3, SAC, ESCP, and RLUR, are shown in Fig. 5. The table for comparisons with all baseline algorithms at timestep 20K is in Tab. 2 (left). CEM and DIN perform worse than the other RL-based algorithms. This highlights the effectiveness of RL in optimizing user retention. The performance of TD3 and SAC exhibits improvements in the early stage of training, but deteriorates as training proceeds. Without explicit modeling of the environment distribution shift, the policies they obtain are loosely coupled with specific user behavior patterns, leading to suboptimal performance. The exploration algorithms OAC and RND do not exhibit large performance improvements compared to TD3 and SAC, largely due to their unsafe exploration procedure. RLUR and OSPIA take into account the bias of long-term recommendation optimization and outperform TD3 and SAC. But they suffer from unstable training and take more steps to converge than AURO. The ESCP algorithm incorporates a context encoder that can capture the environment non-stationary to some extent. But it explicitly relies on a single hidden parameter, and cannot model environment fluctuations universally. As a result, it has a stable training curve, but exhibits suboptimal overall performance. Compared with baseline algorithms, AURO shows a stable training curve and the best overall performance. The stability is due to the state abstraction module which enables the algorithm to fit different environment dynamics and reward functions. The good asymptotic performance can be attributed to the safe and efficient exploration module that quickly navigates to high-reward regions when the environment changes. Ablations and Analyses We conduct ablation studies in the modified KuaiSim simulator to analyze the role of each module in AURO. We also investigate the effect of different values of hyperparameters \ud835\udc5b and \ud835\udeff . As shown in Tab. 2 (right), removing any 0.5 1.0 1.5 2.0 (a) Latent Dimension 1 1.5 1.0 0.5 0.0 Latent Dimension 2 1 0 1 (b) Action (Relative) 1e 2 0.18 0.17 0.16 0.15 0.14 Upper-bound Q-value 4 2 0 2 4 (c) Action (Relative) 1e 2 0.17 0.16 0.15 0.14 0.13 Upper-bound Q-value Original Action Exploration Action with Fixed Step Size AURO Exploration Action 0.0 0.2 0.4 0.6 0.8 1.0 (d) Episode 1e4 0.00 0.05 0.10 0.15 Value Loss TD3 SAC RLUR AURO (Ours) of these components will lead to a drop in the overall algorithm's performance. Among them, the exploration module has the most significant impact. The comparison emphasizes the necessity of exploration when optimizing the sparse retention signal. The performance of AURO will also decrease without state abstraction or training the state abstraction network without the value-based auxiliary loss. This demonstrates the effectiveness of the state abstraction module and the new value-based loss function proposed in Sec. 3.3. Different values of \ud835\udc5b or \ud835\udeff will make little influence on AURO's performance, demonstrating its robustness to hyperparameter selections. 3,4,5, respectively. To ensure conservative policy update during offline training, we remove the exploration module and add BC loss when training policies with online RL algorithms, similar to the TD3+BC [15] algorithm. We use Normalised Capped Importance Sampling (NCIS) [39], a standard offline evaluation method, to make offline algorithm evaluations. The comparative results are listed in Tab. 3, where AURO achieves the highest performance. This demonstrates the effectiveness of the state abstraction module in state representation even without explicit changes in user behaviors. Wevisually demonstrate two of the key components of the AURO algorithm in Fig. 6. Fig. 6 (a) illustrates the outputs of the state abstraction module with a batch of states as input. States with similar values exhibit closely gathered abstractions , while those in different value categories tend to have distinct abstractions. In this way, the state abstraction can help the learning policy identify whether it will perform well in the current environment. Fig. 6 (b,c) showthelandscapes of state-action values when the action is altered in one dimension. Our exploration policy can find a better action (vertical green line) than the exploration action generated with a fixed step size (vertical red line). Fig. 6 (d) shows the comparison of value loss between AURO and baseline algorithms, where AURO has the lowest value error during training. This demonstrates the effectiveness of the state abstraction module in enhancing value estimation.", "4.2 Experiments in MovieLens Dataset": "To verify the effectiveness of AURO in different recommendation tasks, we make evaluations with the well-known recommendation dataset MovieLens-1M. It contains 1,000,209 anonymous ratings of approximately 3,900 movies, generated by 6,040 MovieLens users. We sample the data of 5000 users as the training set, and use the data of the remaining users as the test set. Following previous researches in user retention optimization [42, 45], we assume that average user return days is proportional to the movie ratings in MovieLens-1M. We assign retention reward of -1 with rating score 1, and retention rewards 1,2,3 for rating scores", "4.3 Experiments in Live Experiments": "Setup The MDP setup in the live experiments is similar to that described in Sec. 2. The algorithms are incorporated in a candidateranking system of a popular short-video recommendation platform. The live experiment is run continuously for two weeks. It involves an average of 25 million active users and billions of interactions each day. With such long time period and large scale of involved users, the recommendation environment can exhibit large deviations, as shown by the analysis in Sec. 3.1 based on data collected in this platform. For comparative analysis, users are randomly split into several buckets. The first bucket runs the baseline algorithm RLUR, and the remaining buckets run algorithms AURO, TD3, and ESCP. We do not consider as many baseline algorithms as in dataset or simulator based experiments, because online experiments are cost-sensitive and poor algorithms can potentially bore or annoy platform users. In live experiments, the retention signal can be noisy and sometimes influenced by other module of the live platform. Therefore, the main metric we use is the rate of users returning to the platform averaged between 7 days, rather than the 1-day retention rate. We also focus on the application dwell time, as well as immediate user responses including video click-through rate (CTR) (click and watch the video), like rate (like the video), comment rate (provide comments on the video), and unlike rate (unlike the video). These metrics are standard evaluation criteria for recommendation algorithms and are empirically shown to be related to long-term user experiences [42]. Performance Analysis The comparative results are shown in Tab. 4. The statistics are permillage or percentage improvements compared with RLUR. AURO exhibits superior performance in all evaluation metrics than baseline algorithms, including TD3, ESCP, and RLUR. Specifically, AURO is the only algorithm that achieves performance improvement in the application dwell time. AURO also improves the rate of user comments by 8.392\u2030, which is almost 5 times larger than the improvements of TD3. These empirical results demonstrate AURO's effectiveness and scalability when applied to live recommendation platforms. From a practical perspective, some naive algorithm modifications in live environments can hardly simultaneously improve all performance metrics in Tab. 4. This phenomenon has been reported in literature [7] and is also observed in our live experiment. In Tab. 4 the ESCP algorithm can increase CTR and like rate, but witness drops in dwell time and comment rate. In contrast, our AURO algorithm is capable of improving all performance metrics, meanwhile highlighting the large drop in hate rate. This can be because previous approaches only recommends common items that lead to high CTR/like rate, such as short videos that are very amusing or goods that are very cheap. Meanwhile they have limited abilities in recommending user-oriented items that really consider the dynamic behavior of each user, as demonstrated by the poor user dwell time and comment rate. Thanks to our state abstraction module for policy adaptation, our AURO algorithm can recommend up-to-date, user-oriented items that captures the dynamic user interests. The large drop in AURO's user hate rate provides evidence for such inference.", "5 Related Work": "RL for Recommendation In Reinforcement Learning (RL) [38], a learning agent interacts with the environment [31] or exploits offline dataset [16] to optimize the cumulative reward obtained throughout a trajectory. RL has been found promising for recommendation tasks [1, 29, 34, 48]. Traditional approaches propose to learn an additional prediction model [2, 41] or explicitly construct high-fidelity simulators [35] to enhance the sample efficiency of RL algorithms. They can also be categorized into value-based [9, 25] or policy-based [8] methods. Recently, increasing attention has been paid to optimizing user retention with RL [55]. RLUR [6] and OSPIA [43] deal with issues that comes along in this field, including delayed reward, uncertainty, and instability of training. They will also serve as baselines in the experiments. Other approaches focus on reward engineering, employing the constrained actor-critic method [7] or incorporating human preferences [44]. Our paper also aims at optimizing user retention and focuses on mitigating the challenge of non-stationary environments that has largely been ignored in previous methods. Adaptive Policy Training with RL There are a handful of RL algorithms that train adaptive policies in evolving environments with distribution shift. Meta-RL algorithms [3, 32] can adapt to new environments by fine-tuning a small amount of data from the test environment. Zero-shot adaptation algorithms like ESCP [30], CaDM [22] and SRPO [46] can fit new environments without additional data. A key component enabling rapid adaptation of RL policies is the context encoder [30], which takes a stack of history states as input and generates a dense vector that represents different contexts. The encoder can be trained with variational inference [14, 54] or auxiliary losses [30]. Its output can be used as additional inputs to the environment model [22] or the policy network [46]. Other approaches focus on representation learning [47] or importance sampling [28]. However, none of these methods specifically consider the complexity of the recommender systems and can be unreliable or inefficient when applied directly. We leave relevant researches on recommendation with evolving user interests in Appendix B.", "6 Conclusion": "In this work, we address the challenge of non-stationary environments when optimizing user retention with RL. Through datadriven analyses, we identify several fluctuation factors in user behavior, such as the positive ratio of immediate feedback and the user return time distribution, as the main causes of non-stationarity. To tackle this challenge, we propose the AURO algorithm for training adaptive recommendation policies. AURO utilizes a new valuebased contrastive loss to train an additional state abstraction module in the policy network. The state abstraction enables RL policies to identify different sorts of fluctuations in user behavior patterns universally. To facilitate safe and efficient policy adaptation, we combine the idea of optimism under uncertainty with rejection sampling to boost exploration. Experimental results demonstrate that AURO outperforms state-of-the-art methods in optimising long-term and non-stationary user retention. It also ensures stable recommendation quality in face of distribution shift of environment dynamics and reward function. Ethics Statement While our approach involves real user logs, the user data we used have been stripped of all sensitive privacy information. Each user is denoted by an anonymous user id, and sensitive features are encrypted in the form of one-hot vectors.", "Acknowledgments": "This research is supported by the National Research Foundation Singapore and DSO National Laboratories under the AI Singapore Programme (AISGAward No: AISG2-GC-2023-009). We thank Shuchang Liu for helpful discussions.", "References": "[1] Mohammad Mehdi Afsar, Trafford Crump, and Behrouz H. Far. 2023. Reinforcement Learning based Recommender Systems: A Survey. ACM Comput. Surv. 55, 7 (2023), 145:1-145:38. [2] Xueying Bai, Jian Guan, and Hongning Wang. 2019. A Model-Based Reinforcement Learning with Adversarial Training for Online Recommendation. In NeurIPS . [3] Jacob Beck, Risto Vuorio, Evan Zheran Liu, Zheng Xiong, Luisa M. Zintgraf, Chelsea Finn, and Shimon Whiteson. 2023. A Survey of Meta-Reinforcement Learning. CoRR abs/2301.08028 (2023). [4] William Brown and Arpit Agarwal. 2022. Diversified Recommendations for Agents with Adaptive Preferences. In NeurIPS . [5] Yuri Burda, Harrison Edwards, Amos J. Storkey, and Oleg Klimov. 2019. Exploration by random network distillation. In ICLR . [6] Qingpeng Cai, Shuchang Liu, Xueliang Wang, Tianyou Zuo, Wentao Xie, Bin Yang, Dong Zheng, Peng Jiang, and Kun Gai. 2023. Reinforcing User Retention in a Billion Scale Short Video Recommender System. In WWW . [7] Qingpeng Cai, Zhenghai Xue, Chi Zhang, Wanqi Xue, Shuchang Liu, Ruohan Zhan, Xueliang Wang, Tianyou Zuo, Wentao Xie, Dong Zheng, Peng Jiang, and Kun Gai. 2023. Two-Stage Constrained Actor-Critic for Short Video Recommendation. In WWW . [8] Haokun Chen, Xinyi Dai, Han Cai, Weinan Zhang, Xuejian Wang, Ruiming Tang, Yuzhou Zhang, and Yong Yu. 2019. Large-Scale Interactive Recommendation with Tree-Structured Policy Gradient. In AAAI . [9] Shi-Yong Chen, Yang Yu, Qing Da, Jun Tan, Hai-Kuan Huang, and Hai-Hong Tang. 2018. Stabilizing Reinforcement Learning in Dynamic Environment with Application to Online Recommendation. In KDD . [10] Wang Chi Cheung, David Simchi-Levi, and Ruihao Zhu. 2020. Reinforcement Learning for Non-Stationary Markov Decision Processes: The Blessing of (More) Optimism. In ICML . [11] Kamil Ciosek, Quan Vuong, Robert Loftin, and Katja Hofmann. 2019. Better Exploration with Optimistic Actor Critic. In NeurIPS . [12] Lih-Yuan Deng. 2006. The Cross-Entropy Method: A Unified Approach to Combinatorial Optimization, Monte-Carlo Simulation, and Machine Learning. Technometrics 48, 1 (2006), 147-148. [13] Finale Doshi-Velez and George Dimitri Konidaris. 2016. Hidden Parameter Markov Decision Processes: A Semiparametric Regression Approach for Discovering Latent Task Parametrizations. In IJCAI . [14] Fan Feng, Biwei Huang, Kun Zhang, and Sara Magliacane. 2022. Factored Adaptation for Non-Stationary Reinforcement Learning. In NeurIPS . [15] Scott Fujimoto and Shixiang Shane Gu. 2021. A Minimalist Approach to Offline Reinforcement Learning. In NeurIPS . [16] Scott Fujimoto, David Meger, and Doina Precup. 2019. Off-Policy Deep Reinforcement Learning without Exploration. In ICML . [17] Scott Fujimoto, Herke van Hoof, and David Meger. 2018. Addressing Function Approximation Error in Actor-Critic Methods. In ICML . [18] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor. In ICML . [19] Eugene Ie, Chih-Wei Hsu, Martin Mladenov, Vihan Jain, Sanmit Narvekar, Jing Wang, Rui Wu, and Craig Boutilier. 2019. RecSim: A Configurable Simulation Platform for Recommender Systems. CoRR abs/1909.04847 (2019). [20] Luo Ji, Qi Qin, Bingqing Han, and Hongxia Yang. 2021. Reinforcement Learning to Optimize Lifetime Value in Cold-Start Recommendation. In CIKM . [21] Xuan Nhat Lam, Thuc Vu, Trong Duc Le, and Anh Duc Duong. 2008. Addressing cold-start problem in recommendation systems. In Proceedings of the 2nd international conference on Ubiquitous information management and communication . [22] Kimin Lee, Younggyo Seo, Seunghyun Lee, Honglak Lee, and Jinwoo Shin. 2020. Context-aware Dynamics Model for Generalization in Model-Based Reinforcement Learning. In ICML . [23] Quanyi Li, Zhenghao Peng, Lan Feng, Qihang Zhang, Zhenghai Xue, and Bolei Zhou. 2023. MetaDrive: Composing Diverse Driving Scenarios for Generalizable Reinforcement Learning. IEEE Trans. Pattern Anal. Mach. Intell. 45, 3 (2023), 3461-3475. [24] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2015. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971 (2015). [25] Dong Liu and Chenyang Yang. 2019. A Deep Reinforcement Learning Approach to Proactive Content Pushing and Recommendation for Mobile Users. IEEE Access 7 (2019), 83120-83136. [26] Shuchang Liu, Qingpeng Cai, Zhankui He, Bowen Sun, Julian J. McAuley, Dong Zheng, Peng Jiang, and Kun Gai. 2023. Generative Flow Network for Listwise Recommendation. In KDD . [27] Shuchang Liu, Qingpeng Cai, Bowen Sun, Yuhao Wang, Ji Jiang, Dong Zheng, Peng Jiang, Kun Gai, Xiangyu Zhao, and Yongfeng Zhang. 2023. Exploration and", "Algorithm 1 The workflow of AURO": "1: Input: The state abstraction network \ud835\udf19 \ud835\udf11 , the deterministic policy \ud835\udf0b \ud835\udf03 , the state-action value function \ud835\udc44 \ud835\udf13 , the replay buffer \ud835\udc37 , training steps \ud835\udc41 , and the training horizon \ud835\udc3b . 2: Initialize the networks and the replay buffer. 3: for 1, 2, 3, . . . , \ud835\udc41 do 4: for \ud835\udc61 = 1, 2, . . . , \ud835\udc3b do 5: Obtain \ud835\udc67 \ud835\udc61 from \ud835\udf19 \ud835\udf11 ( \ud835\udc60 \ud835\udc61 ) and sample \ud835\udc4e \ud835\udc47 from \ud835\udf0b \ud835\udf03 ( \ud835\udc60 \ud835\udc61 , \ud835\udc67 \ud835\udc61 ) . 6: Get the exploration action \ud835\udc4e \ud835\udc38 with Eq. (7) and set \ud835\udc4e \ud835\udc61 = \ud835\udc4e \ud835\udc38 . 7: Interact with the simulator, get transition data ( \ud835\udc60 \ud835\udc61 + 1 , \ud835\udc5f \ud835\udc61 , \ud835\udc51 \ud835\udc61 + 1 , \ud835\udc60 \ud835\udc61 , \ud835\udc4e \ud835\udc61 , \ud835\udc67 \ud835\udc61 ) , and add it to \ud835\udc37 . 8: end for 9: Update the state abstraction network \ud835\udf19 \ud835\udf11 according to Eq. (5). 10: Use the replay buffer \ud835\udc37 and the TD3 [17] algorithm to update the policy and value network parameters \ud835\udf03 and \ud835\udf13 . 11: end for", "A Detailed Derivations": "The loss \ud835\udc3d same ( \ud835\udf19 ) ) can be derived as follows: (8) The loss \ud835\udc3d diff ( \ud835\udf19 ) can be derived as follows:", "B Additional Related Work": "", "B.1 Recommendation with Evolving User Interests": "Previous point-wise or list-wise recommendation models have incorporated the concept of evolving user behavior, primarily focusing on changes in the distribution of user interests [36]. However, our paper considers other user behaviors related to the long-term user experience, such as the rate of immediate response and the distribution of user return time. To model the evolving user interests, [52, 53] learn the representation of user interests from historical behaviors and performs click-through rate prediction. Brown and Agarwal [4] proposes to ensure that sufficiently diversified content is recommended to the user in face of adaptive preferences. But the algorithm is analysed in the bandit setting without empirical justifications. Zhao et al. [51] predicts users' shift in tastes during training and incorporates these predictions into a post-ranking network. Another approach involves predicting the distributions of the top-k target customers and training the recommendation model accordingly [50]. It is also worth noting that some studies have indicated that these forms of distribution adjustment may negatively impact the overall recommendation accuracy [50]. Instead of predicting user behaviors and train recommendation models beforehand, our paper focus on the identification of new distributions and the ability to rapidly adapt to them.", "C Algorithm": "The detailed algorithm workflow of AURO is in Alg. 1. The main differences between AURO and TD3 are: 1. The policy takes an additional latent variable \ud835\udc67 \ud835\udc61 as input . The latent variable is the output of the state abstraction module \ud835\udf19 \ud835\udf11 , which is trained with the loss specified in Eq. (5) 2. The exploration action \ud835\udc4e \ud835\udc38 is generated with Eq. (7) rather than by adding Gaussian noise to the original action \ud835\udc4e \ud835\udc47 . Algorithmic Limitations and Failure Cases This paper focuses on the task of optimizing user retention, without considering the combination of retention signals with immediate user feedback. It will be interesting to investigate how to balance these two kinds of reward signals. One potential failure case of our algorithm is when users exhibit unusual or adversarial patterns that have never been witnessed by the algorithm. For example, users may suddenly get bored of the app and dislike every item that is recommended. Our state abstraction module may fail to adapt such behavior pattern and recommend appropriate items. Another case is stubborn users that do not click any novel item that they are unfamiliar with. Our exploration module may lead to inferior retention performance on such users, compared with greedy recommendation methods. We would also like to mention that in our live recommendation platform, RL-based algorithms are only one step of the long recommendation pipeline. Items selected by AURO will not be directly presented to users before further ranking and filtering. The filtering mechanism ensure that no dangerous or illegal items will be presented to users. When applying similar RL-based recommendation algorithms to real-world, it is very important for future practitioners to be aware of post processing, such as item filtering, for safe and robust recommendation.", "D Additional Experiment Details": "", "D.1 Setup": "According to KuaiSim [49], the network architecture of the retention simulator is similar to the policy network. We assume the immediate user response follows a Bernoulli distribution and the user return time in days follows a geometric distribution. The simulator is trained in a style of supervised learning and is updated by likelihood maximization on the training data. The hyperparameters for the AURO algorithm during training are specified in Tab. 5.", "D.2 Baselines": "In the comparative experiments in the KuaiSim simulator, we consider the following baselines: \u00b7 CEM [12]: The Cross Entropy Method, which is commonly used as a surrogate for RL in recommendation tasks. \u00b7 DIN [52]: The Deep Interest Network that learns the representation of user interests from historical behaviors and performs click-through rate prediction. We keep the original input of DIN unchanged and use transformers, which are of the same structure with AURO, as the backbone network. This help us make fair comparisons. \u00b7 DDPG [24]: A value-based off-policy RL algorithm with a deterministic policy updated with deterministic policy gradients. \u00b7 TD3 [17]: A value-based off-policy RL algorithm that on top of DDPG, incorporates a pair of Q-networks to mitigate overestimation. \u00b7 SAC [18]: A value-based off-policy RL algorithm with a stochastic policy and the maximum-entropy RL objective. \u00b7 OAC [11]: An RL-based exploration algorithm that incorporates the idea of optimism under uncertainty, and obtains a separate optimistic action during the training phase. \u00b7 RND [5]: An RL-based exploration algorithm that encodes the state input to fit the output of a random network. States with larger encoding error will be assigned with higher intrinstic reward. \u00b7 ESCP [30]: The state-of-the-art environment sensitive contextual Meta-RL approach that explicitly identifies hidden parameters as additional policy inputs. During training, ESCP assumes access to the hidden parameters, which is unavailabe in live experiments. In practice, we use the level of user activeness as the proxy hidden parameter in ESCP. \u00b7 OSPIA [43]: An one-step policy improvement algorithm that biases the policy towards recommendations with higher longterm user engagement metrics. \u00b7 RLUR [6]: An RL-based recommendation algorithm especially designed for optimizing long-term user engagement.", "D.3 Computation Cost of the Exploration Module": "AURO requires addtional steps in action selection, computing the gradient of the Q-function and sampling among candidate actions. But apart from action selection, RL training involves interacting with the environment and updating the policy with gradient decent. These two parts will take up more time than action selection. We conduct empirical studies and exhibit in Tab. 6 the average time cost of action selection in one training step. The total time cost of one training step is also shown for comparison. According to the results, although the exploration module lead to an addtional 129% of computation cost, it only costs less than 10% more total time. Also, during deployment the exploration module is not included, so it adds no more computation cost."}
