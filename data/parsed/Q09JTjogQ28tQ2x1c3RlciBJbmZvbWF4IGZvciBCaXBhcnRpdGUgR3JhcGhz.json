{"title": "COIN: Co-Cluster Infomax for Bipartite Graphs", "authors": "Baoyu Jing; Yuchen Yan; Yada Zhu; Hanghang Tong", "pub_date": "", "abstract": "Bipartite graphs are powerful data structures to model interactions between two types of nodes, which have been used in a variety of applications, such as recommender systems, information retrieval, and drug discovery. A fundamental challenge for bipartite graphs is how to learn informative node embeddings. Despite the success of recent self-supervised learning methods on bipartite graphs, their objectives are discriminating instance-wise positive and negative node pairs, which could contain cluster-level errors. In this paper, we introduce a novel cocluster infomax (COIN) framework, which captures the cluster-level information by maximizing the mutual information of co-clusters. Different from previous infomax methods which estimate mutual information by neural networks, COIN could easily calculate mutual information. Besides, COIN is an end-to-end coclustering method which can be trained jointly with other objective functions and optimized via back-propagation. Furthermore, we also provide theoretical analysis for COIN. We theoretically prove that COIN is able to effectively increase the mutual information of node embeddings and COIN is upper-bounded by the prior distributions of nodes. We extensively evaluate the proposed COIN framework on various benchmark datasets and tasks to demonstrate the effectiveness of COIN.", "sections": [{"heading": "Introduction", "text": "Graphs have attracted plenty of attention in recent years [48,17,22,26,69,12,25,55,58,60]. The bipartite graph is a powerful representation formalism to model interactions between two types of nodes, which has been used in a variety of real-world applications. For example, in recommender systems [1,53], users, items and their interactions (e.g. buy) is a natural bipartite graph; in information retrieval [3,21], clickthrough between queries and webpages can be conveniently modeled by a bipartite graph; in drug discovery [41,57], chemical interactions (e.g. nuclear receptor) between drugs and target proteins can also be represented by a bipartite graph.\nOne fundamental challenge for bipartite graphs is how to extract informative node embeddings, such that they can be easily used for downstream tasks (e.g. link prediction). In recent years, self-supervised learning has become a popular paradigm to learn node embeddings without human labels [34,55,10,68,67]. Despite their success in extracting high-quality node embeddings, which have great performance on downstream tasks, most of them are designed for homogeneous graphs [20,43,48,63,69,13,31,51] and heterogeneous graphs [39,24,15,23,12]. Thus they are suboptimal to bipartite graphs [6,16]. Several methods have been specifically proposed for bipartite graphs. For example, BiNE [16] learns embeddings by maximizing the similarity of neighbors sampled by random walks; NeuMF [22] and GC-MC [5] train neural networks by reconstructing the edges; BiGI [6] and EGLN [61] further improve the quality of node embeddings by maximizing the mutual information between local and global representations.\nAlthough promising results have been achieved by the aforementioned methods, they learn node embeddings by discriminating instance-wise positive pairs (e.g. local neighbors) and negative pairs NeurIPS 2022 GLFrontiers Workshop. arXiv:2206.00006v2 [cs.LG] 2 Nov 2022 (e.g. randomly sampled unconnected node pairs), but ignore the cluster-level information. Such a practice restricts the quality of the learned embeddings since instance-wise negative pairs might contain cluster-level errors [7,32,23]. Clusters naturally exist in real-world bipartite graphs, such as the categories of items in a user-item graph and the topics of documents in a document-keywork graph. Regardless of the cluster information, one might wrongly pair two nodes within the same cluster as a negative pair, which will lead to errors in downstream tasks [23,32].\nDue to the duality between two types of nodes in a bipartite graph, co-clustering two types of nodes simultaneously usually yields better results than traditional clustering algorithms [9,56]. In this paper, we introduce a novel co-cluster infomax (COIN) framework to incorporate cluster-level information into the node embeddings of bipartite graphs. Given a bipartite graph, COIN first uses neural networks to cluster two types of nodes into co-clusters and then maximizes the mutual information of the co-clusters. There are two advantages of the proposed COIN framework. Firstly, COIN directly calculates the mutual information of the co-clusters, rather than estimating mutual information via neural networks [4] in prior works [6,48], which could be unreliable for complex distributions in practice [46]. Secondly, COIN is an end-to-end co-clustering method, which is differentiable and can be trained jointly with other objective functions.\nWe further present the theoretical analysis and empirical evaluation of COIN. In theoretical analysis, we prove that (1) maximizing the mutual information of co-clusters will increase the mutual information of the node embeddings, and (2) the mutual information of co-clusters is upper-bounded by the prior distribution assumed over the bipartite graph. In empirical evaluation, we extensively evaluate the proposed COIN on various public real-world benchmark datasets and downstream tasks to demonstrate the effectiveness of COIN.\nThe contributions of this paper are summarized as follows:\n\u2022 We introduce a novel framework COIN for self-supervised learning on bipartite graphs, which incorporates cluster information by maximizing the mutual information of co-clusters.\n\u2022 We theoretically prove that COIN maximizes the mutual information between the embeddings of two types of nodes, and COIN is upper-bounded by the prior distribution.\n\u2022 We extensively evaluate COIN on various benchmark datasets and downstream tasks, including link prediction, recommendation, and clustering, to demonstrate its effectiveness.", "publication_ref": ["b48", "b17", "b22", "b26", "b67", "b12", "b25", "b55", "b58", "b60", "b1", "b53", "b3", "b21", "b41", "b57", "b34", "b55", "b10", "b66", "b65", "b20", "b43", "b48", "b61", "b67", "b13", "b31", "b51", "b39", "b24", "b15", "b23", "b12", "b6", "b16", "b16", "b22", "b5", "b6", "b7", "b32", "b23", "b23", "b32", "b9", "b56", "b4", "b6", "b48", "b46"], "figure_ref": [], "table_ref": []}, {"heading": "Preliminary", "text": "Self-Supervised Learning for Bipartite Graphs. We denote a bipartite graph as G = (U, V, E), where U and V are two disjoint sets of nodes, and E \u2286 U \u00d7 V is the set of edges. Our goal is to train a graph encoder\n(U, V) = E(G) to extract informative node embeddings U \u2208 R |U |\u00d7d , V \u2208 R |V |\u00d7d from G,\nwhere d is the size of hidden dimension. When there is no ambiguity, we also use U , V as the random variables for nodes, U, V as the random variables for node embeddings. Correspondingly, u, v and u, v are used as the indices for U , V and U, V.\nCo-Clustering. Given a biparitite graph G = (U, V, E), the soft co-clustering aims to map nodes U and\nV into N K |U | and N L |V | clusters via the function \u03c6 = (\u03c6 U , \u03c6 V ), where \u03c6 U (u) \u2208 R N K and \u03c6 V (v) \u2208 R N L produce the conditional probabilities of cluster assignments p(k|u), p(l|v) for nodes u, v. Here k \u2208 [1, \u2022 \u2022 \u2022 , N K ] and l \u2208 [1, \u2022 \u2022 \u2022 , N L ]\nare the indices of the clusters. Furthermore, we use K and L to denote random variables of co-clusters.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Methodology", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Co-Cluster Infomax", "text": "Prior deep infomax based frameworks [20,48,24,39] maximize the mutual information of local and global representations, which ignore the co-cluster information and the neural mutual information estimators could be unreliable in practice. Different from these works, COIN directly calculates and then maximizes the mutual information of co-clusters I(K; L) to capture cluster-level information. An overview of COIN is presented in Figure 1. Mutual Information of Co-Clusters. To calculate the mutual information of co-clusters I(K; L), we need to obtain the joint distribution p(k, l) and marginal distributions p(k) and p(l). To calculate p(k, l), we decompose p(k, l) by two other easy-to-obtain distributions p(u, v) and p(k, l|u, v): p(k, l) = u,v p(k, l|u, v)p(u, v). Here, p(u, v) is the prior assumption over the joint distribution of nodes u, v, which preserves the structure of G. For simplicity, we define the prior p(u, v)\n= 1 Z if (u, v) is connected, where Z is a normalization parameter, p(u, v) = 0 otherwise.\nWith the prior distribution p(u, v), the next step is to obtain the conditional joint distribution p(k, l|u, v). Given a bipartite graph G = (U, V, E), we use a co-clustering algorithm \u03c6 = (\u03c6 U , \u03c6 V ) to obtain cluster distributions for each node. We define \u03c6 U and \u03c6 V via neural networks, which are illustrated in Figure 1. They are comprised of two components: (1) a shared graph encoder to obtain node embeddings (U, V) = E(G), and (2) separate cluster networks to obtain conditional probabilities of clusters C U (u) \u2208 R N K and C V (v) \u2208 R N L , where u \u2208 U and v \u2208 V are embeddings of nodes u and v. Since neural encoders are usually treated as deterministic and injective functions in practice [49], we have p(k, l|u, v) = p(k, l|u, v), where u and v are obtained by E. Furthermore, since cluster networks C U and C V have separate sets of parameters, therefore, it is natural to assume p(k, l|u, v) = p(k|u)p(l|v). As a result, p(k, l|u, v) = p(k|u)p(l|v). Finally, combining p(k|u)p(l|v) with the assumed prior distribution p(u, v), we have\np(k, l) = u,v p(k|u)p(l|v)p(u, v)(1)\nwhere u, v are obtained by E, and p(k|u), p(l|v) are obtained by C U , C V .\nGiven the joint distribution of co-clusters p(k, l), we can easily calculate the marginal distributions p(k) = l p(k, l) and p(l) = k p(k, l). According to the definition of mutual information, we can directly calculate I(K; L) by I(K; L) = k,l p(k, l) log p(k,l) p(k)p(l) . Since p(k, l), p(k) and p(l) are calculated by neural networks, which are fully differentiable, we can directly maximize I(K; L). [32,33,23], since it is unable to capture local information such as the connectivity between two nodes u, v. Therefore, an instance-wise objective function is usually used together with the clustering objective function. In this paper, we use an objective function, shown in Equation ( 2), based on the popular InfoNCE [37] as the instance-wise objective function, which pulls connected u, v together and pushes away unconnected u, v in the embedding space, as illustrated in the middle part of Figure 1.", "publication_ref": ["b20", "b48", "b24", "b39", "b49", "b32", "b33", "b23", "b37"], "figure_ref": ["fig_0", "fig_0", "fig_0"], "table_ref": []}, {"heading": "Instance-Wise Objective Function. A clustering objective function alone is incapable of achieving the optimal performance", "text": "max L I = max u,v e S(u,v) e S(u,v) + e S(u,v )(2)\nwhere S is the similarity measuring function, and v is the embedding of the negative sample v , which is randomly sampled from\nV \u2286 V , such that (u, v ) is not connected: (u, v ) / \u2208 E for \u2200v \u2208 V .\nMost of the recent InfoNCE based studies [32,63,69,14,66] use the normalized temperature-scaled cross entropy loss (NT-Xent) variant of InfoNCE. They instantiate S with unlearnable functions (e.g. cosine similarity and dot product) and normalize the similarity scores with a temperature parameter \u03c4 . However, unlearnable functions might not be the optimal choice for complex manifolds in the embedding space [35], and tuning the hyper-parameter \u03c4 could be time-consuming. To tackle with this issue, we use a multi-layer perceptron (MLP) to instantiate S.\nOverall Objective Function. The overall objective function is a linear combination of the mutual information of co-clusters I(K; L) and the instance-wise objective function L I :\nmax L = max \u03bbI(K; L) + L I (3\n)\nwhere \u03bb is the co-efficient for I(K; L).", "publication_ref": ["b32", "b61", "b67", "b14", "b64", "b35"], "figure_ref": [], "table_ref": []}, {"heading": "Theoretical Analysis", "text": "We provide theoretical analysis for the proposed COIN framework with respect to the mutual information I(K; L) of the co-clusters K and L.\nWe study the relationship between the mutual information of co-clusters I(K; L) and the mutual information of embeddings I(U; V). Based on Lemma 1, we first introduce Theorem 1, where we prove that I(K; L) is a variational lower-bound for I(U; V). This theorem implies that maximizing I(K; L) will increase I(U; V). Lemma 1. For COIN, the following inequality holds:\nlog p(k)p(l) \u2265 u,v p(u, v|k, l) log p(u, k)p(v, l) p(u, v|k, l)(4)\nwhere\nk \u2208 {1, \u2022 \u2022 \u2022 , N K }, l \u2208 {1, \u2022 \u2022 \u2022 , N L }, u \u2208 U, v \u2208 V.\nProof. Please refer to Lemma A.1 in Appendix A.\nTheorem 1 (Variational Bound). The mutual information I(U; V) of embeddings U and V is lower-bounded by the mutual information of co-clusters I(K; L):\nI(K; L) \u2264 I(U; V)(5)\nProof. According to the definition of mutual information and Lemma 1, we have:\nI(K; L) = k,l p(k, l) log p(k, l) log p(k)p(l) \u2264 k,l p(k, l) u,v p(u, v|k, l) log p(k, l)p(u, v|k, l) p(u, k)p(v, l) = R (6)\nBy merging p(k, l) with p(u, v|k, l), we have:\nR = u,v,k,l p(u, v, k, l) log p(u, v, k, l) p(u, k)p(v, l) = u,v,k,l p(u, v, k, l) log p(k, l|u, v)p(u, v) p(u)p(k|u)p(v)p(l|v)(7)\nSince cluster networks C U and C V have separate sets of parameters and inputs, therefore, it is natural to have p(k, l|u, v) = p(k|u)p(l|v). As a result, we have:\nR = u,v,k,l p(u, v, k, l) log p(u, v) p(u)p(v) = u,v p(u, v) log p(u, v) p(u)p(v) = I(U; V)(8)\nTake the above result back to Inequality (6), and we will obtain Inequality (5).\nWe also study the relationship between mutual information of co-clusters I(K; L) and mutual information of nodes I(U ; V ), where I(U ; V ) is calculated based on the prior joint distribution p(U, V ) assumed over nodes U and V . In Theorem 2, we derive the different between I(K; L) and I(U ; V ). Note that different from the theory in [9], which considers hard clustering, we consider soft clustering, and thus the proofs and conclusions are different. From Theorem 2, we can obtain that I(U ; V ) -I(K; L) = D KL (p||q) \u2265 0, which implies that the learning ability of COIN is upper-bounded by the prior distribution p(U, V ). Theorem 2 (Mutual Information Difference). Given the prior joint distribution p(U, V ) assumed on U , V , and a soft co-clustering function \u03c6 = (\u03c6 U , \u03c6 V ), where \u03c6 U and \u03c6 V are deterministic functions mapping nodes U ,V to their cluster distributions p(K|U ) and p(L|V ), we have:\nI(U, V ) -I(K; L) = D KL (p(K, L, U, V )||q(K, L, U, V ))(9)\nwhere D KL denotes the KL-divergence, and q(K, L, U, V ) = p(K, L)p(U |K)p(V |L).\nProof. Please refer to Theorem A.2 in Appendix A.", "publication_ref": ["b5", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "In this section, we extensively evaluate the effectiveness of COIN on a variety of benchmark datasets and downstream tasks, and provide empirical evaluation results for COIN.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Setup", "text": "Datasets. We evaluate the proposed COIN on six public benchmark datasets with three different downstream tasks. The descriptions of the datasets are presented in Table 1. Wikipedia contains the edit relationship between authors and pages, which is used for link prediction. We use the data processed by [6], which has two different splits: 50% and 40% for training. ML-100K and ML-10M [19] contain the ratings of users for movies, which are used for top-K recommendation. WebKB, Wisconsin, and IMDB are document-keyword interaction datasets, which are used for co-clustering [56]. Further descriptions and the links of the datasets are provided in Appendix B.\nEvaluation Metrics. For link prediction, we use Area Under the ROC and Precision-Recall Curves, i.e. AUC-ROC and AUC-PR. For top-K recommendation, F1 score, Normalized Discounted Cumulative Gain (NDCG), Mean Average Precision (MAP), and Mean Reciprocal Rank (MRR) are used.\nFor co-clustering, we use Normalized Mutual Information (NMI).\nComparison Methods. We compare the proposed COIN with three different groups of baseline methods:\n(1) bipartite graph methods: BiNE [16] and PinSage [62] learn node embeddings based on random walks; GC-MC [5] and IGMC [64] are matrix completion based methods; NeuMF [22] and NGCF [54] are collaborative filtering based methods; BiGI [6] is an infomax contrastive learning method.\n(2) co-clustering methods: CCInfo [9] is an information theoretic method; SCC [8] and SBC [30] are spectral methods; DRCC [18], CCMod [2] and SCMK [27] are matrix analysis based methods; DeepCC [56] is a deep learning method. (3) graph embedding methods: DeepWalk [43], LINE [47], Node2vec [17] and Metapath2vec [11] are random walk based embedding methods; VGAE [29] learns node embeddigns by reconstructing the adjacency matrix; HDI [24] maximizes high-order mutual information; DMGI [39] and HDMI [24] are extensions of DGI [48] to multiplex heterogeneous graphs. The results of baseline methods are copied from respective papers, except for GraphCL, HDI and HDMI. For GraphCL, HDI and HDMI, we use their official implementations and set the embedding size as 128, which is the same as COIN and other baselines.\nBipartite Graph Encoder. We design a simple L-layer bipartite graph encoder E to capture 2-hop neighbor information. Given the embeddings of U after the (l -1)-th layer U l-1 \u2208 R |U |\u00d7d , the updating function of the l-th layer is: where Training Details. For the encoder E, we set the number of layers L = 2, the hidden dimension d = 128, \u03b4 1 and \u03b4 2 are LeakyReLU (negative slope is 0.1) and Tanh activation functions respectively. The similarity function S is a two-layer MLP:\nVl = \u03b4 2 ([\u03b4 1 (A V U U l-1 W l 1 )||V l-1 ]W l 2 )(10)\nU l = \u03b4 2 ([\u03b4 1 (A U V Vl W l 3 )||U l-1 ]W l 4 )(11)\nA V U \u2208 R |V |\u00d7|U | , A U V \u2208 R |U |\u00d7|V | are\nS(u, v) = W 2 (Tanh(W 1 [u||v])), where W 1 \u2208 R d\u00d72d , W 2 \u2208 R d\u00d7d , d = 128.\nFor simplicity, we keep N K = N L . For Wikipedia, the number of epochs is 50, the number of clusters N K = N L = 4 and \u03bb = 10. For ML-100K and ML-10M, the number of epochs is 100, the number of clusters N K = N L = 5 and \u03bb = 1. For the co-clustering datasets, the number of epochs is 100 and \u03bb is tuned within [0.01, 0.1, 1, 10]. The numbers of clusters are 4, 3, 17 for WebKB, Wisconsin and IMDB. The learning rate is fixed as 0.0005, and the optimizer is Adam [28]. Dropout with p = 0.5 is applied to each layer of E. COIN is implemented by PyTorch [40] and trained with one NVIDIA Tesla V-100 GPU. We run COIN three times and report its mean and standard deviation. We will release the code upon publication.", "publication_ref": ["b6", "b19", "b56", "b16", "b5", "b62", "b22", "b54", "b6", "b9", "b8", "b30", "b18", "b2", "b27", "b56", "b43", "b47", "b17", "b11", "b29", "b24", "b39", "b24", "b48", "b28", "b40"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Main Results", "text": "Link Prediction. Given the learned embeddings U, V and the edges E, we train a logistic regression classifier, which will then be evaluated on the test data. The experimental results are presented in Table 2. Comparing the bipartite graph embedding methods with the graph embedding methods, we can observe that bipartite graph methods usually have higher scores. For random walk based methods, BiNE and PinSage have higher scores than DeepWalk and Node2vec. For contrastive learning methods, COIN performs better than HDMI and GraphCL. This observation indicates the necessity of introducing methods specifically for bipartite graphs. Within the group of bipartite graph methods, recent contrastive learning methods, (COIN and BiGI) achieve much higher scores than the traditional random walk methods (BiNE and PinSage), matrix completion methods (GC-MC and IGMC), and collaborative filtering methods (NeuMF and NGCF). That is to say, contrastive learning is a better paradigm for learning node embeddings. Finally, among all the methods, COIN achieves the best scores on all metrics, indicating the power of COIN.\nTop-K Recommendation. The results on ML-100K and ML-10M are presented in Tables 34. For ML-100K, comparing the two recent contrastive learning methods BiGI and HDMI from the bipartite graph embedding group and the graph embedding group, it can be observed that BiGI performs much better (e.g. 23.36 v.s. 20.51 on F1@10), indicating the importance of designing particular models for bipartite graphs. The proposed COIN can further outperform BiGI on all of the metrics (e.g. 72.76 v.s. 68.78 on MRR@10), indicating the effectiveness of the proposed COIN. For ML-10M, BiGI and HDMI are competitive to each other. Nevertheless, COIN significantly outperforms both BiGI and HDMI to a large margin (e.g. 21.39 v.s. 16.12 v.s. 15.37 on F1@10). This result again demonstrates the necessity of tailoring methods for bipartite graphs and the effectiveness of COIN.\nCo-Clustering. The proposed COIN can directly produce the cluster probabilities for the given node u or v. We assign the cluster index with the highest probability as the cluster assignment for the given node. The experimental results are presented in Table 5. Among all the baseline methods, the deep learning method (DeepCC) achieves the highest NMI scores on all datasets, showing the power of deep neural networks. COIN has further improvements over DeepCC, indicating the effectiveness of the proposed COIN for discovering the clusters.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1", "tab_2"]}, {"heading": "Ablation Study", "text": "We conduct the ablation study on the Wikipedia datasets, the results of which are shown in Table 6. Firstly, we study the impact of the mutual information objective I(K; L) by comparing the full model COIN with the version w/o I(K; L). Evidently, the full model is significantly better than w/o I(K; L), showing the importance of capturing the cluster-level information. Secondly, we replace the MLP instantiation of the similarity function S in Equation 2 with commonly used cosine similarity and dot product. It can be observed in Table 6 that there are significant performance drops, which suggests that the manifolds of the embeddings are complex and MLP is a better way for calculating the similarity between nodes than simple cosine similarity or dot product. In most of the prior works, to improve the performance, cosine similarity and dot product are normalized by a temperature parameter \u03c4 . However, tuning the extra parameter \u03c4 is very time-consuming, which brings extra burdens for experiments. Thirdly, we study the influence of the prior distribution p(U, V ). If we assume the random variables U and V are independent p(U, V ) = p(U )p(V ), then there will be significant performance drops, as shown in Table 6. This observation indicates that the learning ability of COIN is bounded by p(U, V ), which empirically supports Theorem 2. ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_3", "tab_3", "tab_3"]}, {"heading": "Sensitivity Experiments", "text": "We study the impact of the number of clusters on the WebKB and Wisconsin datasets, and study the coefficient \u03bb for mutual information on the Wikipedia datasets. As shown in Figure 2a-2b, COIN performs the best when the number of clusters is around the ground truth number of classes (4 for WebKB and 5 for Wisconsin). As shown in Figure 3a-2c, the performance of COIN only has small variances for different \u03bb. COIN achieves the best performance when \u03bb = 10.", "publication_ref": [], "figure_ref": ["fig_2", "fig_3"], "table_ref": []}, {"heading": "Convergence of Mutual Information", "text": "We present the mutual information of co-clusters I(K; L) for each training epoch on Wiki (50%), Wiki (40%), ML-100K, and ML-10M in Figure 3. I(K; L) increases rapidly in the first 10 epochs for all the datasets, after which I(K; L) moves slowly towards the limits. The results show that COIN has a good convergence ability.", "publication_ref": [], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Related Work", "text": "Graph Embedding. A fundamental challenge for graph learning is to extract informative node embeddings [55,43,48,69,39,59]. DeepWalk [43] and node2vec [17] use the random walk to sample node pairs and maximizes the similarities for random walk neighbors. LINE [47] and SDNE [50] consider both of the first-order and second-order proximity information when learning node embeddings. VGAE [29] and ARVGA [38] are graph auto-encoder methods which learn node embeddings by reconstructing the adjacency matrices. Recent self-supervised graph learning methods extract node embeddings by optimizing contrastive objectives. DGI [48] maximizes the mutual information of local and global representation of graphs. MVGRL [20] maximizes mutual information of different views. GMI [42] introduces the graphical mutual information. GraphCL [63] generates positive and negative node pairs via various augmentations. GCA [69] generates data augmentation adaptively according to pre-defined probabilities. HDI [24] introduces a high-order mutual inforamtion objective. All of the above methods are designed for homogeneous graphs. Metapath2vec [11] is a random walk based method, which extends DeepWalk and node2vec to heterogeneous graphs. Recently, DMGI [39] and HDMI [24] extends DGI and HDI to multiplex heterogeneous graphs. Although these methods have achieved impressive scores on downstream tasks, they are not tailored for bipartite graphs and thus usually have sub-optimal performance compared with bipartite graph embedding methods.\nBipartite Graph Embedding. Bipartite graphs have been widely used to model interactions between two disjoint sets of nodes, such as the user-item interaction in recommender systems. Inspired by random walk based graph embeddings methods [43,17], BiNE [16] uses both explicit connections between nodes and random walks extracted implicit relationships between nodes to learn node embeddings. IGE [65] learns node embeddings based on the direct connection between nodes and the attributes of the edges. GC-MC [5] trains a graph convolutional network and learns node embeddings by recovering the link between two nodes. PinSage [62] is a web-scale method which combines graph convolutional network with random walks. NeuMF [22] is a neural network based collaborative filtering method. NGCF [54] incorporates the high-order collaborative signals to improve the quality  Co-Clustering. Co-clustering aims to partition rows and columns of a co-occurrence matrix into co-clusters simultaneously [8,9]. In practice, co-clustering algorithms usually have impressive improvements over traditional one-way clustering algorithms [56]. InfoCC [9] is an informationtheoretic method, which co-clusters document-word interaction matrices by a mutual information based objective. SCC [8] and SBC [30] are based on spectral analysis. DRCC [18] is a seminonnegative matrix tri-factorization method with geometric regularization. BCC [45], LDCC [44] and MPCCE [52] are Bayesian approaches. SOBG [36] performs co-clustering by learning a new graph similarity matrix. CCMod [2] obtains co-clusters by maximizing graph modularity. SCMK [27] is a kernel based method. DeepCC [56] is the first deep learning based co-clustering method, which uses auto-encoders for dimension reduction and a variant of the Gaussian mixture model to obtain cluster assignments. Different from DeepCC, which is specifically designed for the co-clustering task, COIN is a self-supervised learning method, which is able to deal with various downstream tasks.", "publication_ref": ["b55", "b43", "b48", "b67", "b39", "b59", "b43", "b17", "b47", "b50", "b29", "b38", "b48", "b20", "b42", "b61", "b67", "b24", "b11", "b39", "b24", "b43", "b17", "b16", "b63", "b5", "b22", "b54", "b8", "b9", "b56", "b9", "b8", "b30", "b18", "b45", "b44", "b52", "b36", "b2", "b27", "b56"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "In this paper, we introduce a novel co-cluster infomax (COIN) framework for self-supervised learning on bipartite graphs. COIN is able to capture the cluster-level information of bipartite graphs by maximizing the mutual information of co-clusters. There are two advantages of COIN. On the one hand, COIN calculates the mutual information rather than estimates the mutual information as in prior works. On the other hand, COIN is an end-to-end clustering framework which can be trained and optimized with other differentiable objectives. Furthermore, we also provide the theoretical analysis of COIN. We theoretically prove that COIN can maximize the mutual information of embeddings of the two types of nodes, and COIN is upper-bounded by the prior distribution assumed over the bipartite graph. Finally, extensive empirical evaluation on diverse benchmark datasets and downstream tasks demonstrates the effectiveness of the proposed COIN. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Theoretical Analysis", "text": "Lemma A.1. For COIN, the following inequality holds:\nlog p(k)p(l) \u2265 u,v p(u, v|k, l) log p(u, k)p(v, l) p(u, v|k, l)(12)\nwhere\nk \u2208 {1, \u2022 \u2022 \u2022 , N K }, l \u2208 {1, \u2022 \u2022 \u2022 , N L }, u \u2208 U, v \u2208 V.\nProof. We have p(k) = u p(u, k), p(l) = v p(v, l), and thus p(k)p(l) = u,v p(u, k)p(v, l).\nAs a result, we have:\nlog p(k)p(l) = log u,v p(k, u)p(l, v) p(u, v|k, l) p(u, v|k, l) \u2265 u,v p(u, v|k, l) log p(u, k)p(v, l) p(u, v|k, l)(13)\nwhere the inequality holds according to Jensen's inequality.\nTheorem A.1 (Variational Bound). The mutual information I(U; V) of embeddings U and V is lower-bounded by the mutual information of co-clusters I(K; L):\nI(K; L) \u2264 I(U; V)(14)\nProof. According to the definition of mutual information and Lemma A.1, we have:\nI(K; L) = k,l p(k, l) log p(k, l) log p(k)p(l) \u2264 k,l p(k, l) u,v p(u, v|k, l) log p(k, l)p(u, v|k, l) p(u, k)p(v, l) = R(15)\nBy merging p(k, l) with p(u, v|k, l), we have:\nR = u,v,k,l p(u, v, k, l) log p(u, v, k, l) p(u, k)p(v, l) = u,v,k,l p(u, v, k, l) log p(k, l|u, v)p(u, v) p(u)p(k|u)p(v)p(l|v)(16)\nSince cluster networks C U and C V have separate sets of parameters and inputs, therefore, it is natural to have p(k, l|u, v) = p(k|u)p(l|v). As a result, we have:\nR = u,v,k,l p(u, v, k, l) log p(u, v) p(u)p(v) = u,v p(u, v) log p(u, v) p(u)p(v) = I(U; V)(17)\nTake the above result back to Inequality (15), and we will obtain Inequality (14).\nTheorem A.2 (Mutual Information Difference). Given a soft co-clustering function \u03c6 = (\u03c6 U , \u03c6 V ), where \u03c6 U and \u03c6 V are deterministic functions mapping nodes u,v to their cluster distributions p(K|u) and p(L|v), and the prior joint distribution p(U, V ) assumed on U , V , we have:\nI(U, V ) -I(K; L) = D KL (p(K, L, U, V )||q(K, L, U, V ))(18)\nwhere D KL denotes the KL-divergence, and q(K, L, U, V ) = p(K, L)p(U |K)p(V |L).\nProof. According to the definition of mutual information, we have: \nI(U ; V ) -I(K; L) = k,l,u,v p(k, l, u, v) log p(u, v) p(u)p(v) - k,l,u,v p(k, l, u, v) log p(k, l) p(k)p(l)(19)\nIn the second equality, p(k, l|u, v) = p(k|u)p(l|v) comes from the fact that \u03c6 U takes U as the input and produces p(K|U ), and thus given U , K is conditional independent to V and L. Similarly, given V , L is conditional independent to U and K. In the third equality, Bayes's rule is applied.  \nwhere p = p(K, L, U, V ) and q = p(K, L)p(U |K)p(V |L). It is obvious that k,l,u,v q = 1.", "publication_ref": ["b15", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "B Datasets", "text": "We evaluate the proposed COIN on six public benchmark datasets with three different tasks. The descriptions of the datasets are presented in Table 7.\nWikipedia The Wikipedia datasetfoot_0 contains the edit relationship between authors and pages, which is used for link prediction [16,6]. We use the data processed by [6], which has two different splits 50% and 40% for training.\nML-100K This datasetfoot_1 [19] is collected through the MovieLensfoot_2 website, which contains 100 thousand movie ratings from 943 users on 1682 movies. Each user has rated at least 20 movies, and the relation between users and items are binary. We use the data processed by [6]. This data is used for top-K recommendation.\nML-10M This datasetfoot_3 [19] is collected through the MovieLensfoot_4 website, which contains 10 million movie ratings. All users selected had rated at least 20 movies, and the relation between users and items are binary. We use the data processed by [6]. This data is used for top-K recommendation.\nWebKB The WebKB dataset is about the information of the web pages, which is formulated as a document-keyword interaction bipartite graph. It contains 4 different classes. This dataset is processed by [56], and it is used for co-clustering.\nWisconsin The Wisconsin dataset contains 265 documents with 5 classes, i.e. student, project, course, staff and faculty. We use the version processed by [56], and it is used for co-clustering.\nIMDB The IMDB dataset is a document-keyword interaction bipartite graph, where documents are descriptions for movies. Following [56], we use it for co-clustering.", "publication_ref": ["b16", "b6", "b6", "b19", "b6", "b19", "b6", "b56", "b56", "b56"], "figure_ref": [], "table_ref": ["tab_5"]}], "references": [{"ref_id": "b0", "title": "Performance (%) of top-K recommendation on ML-100K Method F1@10 NDCG@3 NDCG@5 NDCG@10 MAP@3 MAP@5 MAP@10 MRR@3 MRR@5 MRR@10", "journal": "DeepWalk", "year": "", "authors": ""}, {"ref_id": "b1", "title": "Recommender systems", "journal": "Springer", "year": "2016", "authors": "C Charu;  Aggarwal"}, {"ref_id": "b2", "title": "Co-clustering document-term matrices by direct maximization of graph modularity", "journal": "", "year": "2015", "authors": "Melissa Ailem; Fran\u00e7ois Role; Mohamed Nadif"}, {"ref_id": "b3", "title": "Agglomerative clustering of a search engine query log", "journal": "", "year": "2000", "authors": "Doug Beeferman; Adam Berger"}, {"ref_id": "b4", "title": "Mutual information neural estimation", "journal": "PMLR", "year": "2018", "authors": "Mohamed Ishmael Belghazi; Aristide Baratin; Sai Rajeshwar; Sherjil Ozair; Yoshua Bengio; Aaron Courville; Devon Hjelm"}, {"ref_id": "b5", "title": "Graph convolutional matrix completion", "journal": "", "year": "2017", "authors": "Rianne Van Den; Thomas N Berg; Max Kipf;  Welling"}, {"ref_id": "b6", "title": "Bipartite graph embedding via mutual information maximization", "journal": "", "year": "2021", "authors": "Jiangxia Cao; Xixun Lin; Shu Guo; Luchen Liu; Tingwen Liu; Bin Wang"}, {"ref_id": "b7", "title": "Unsupervised learning of visual features by contrasting cluster assignments", "journal": "Advances in Neural Information Processing Systems", "year": "2020", "authors": "Mathilde Caron; Ishan Misra; Julien Mairal; Priya Goyal; Piotr Bojanowski; Armand Joulin"}, {"ref_id": "b8", "title": "Co-clustering documents and words using bipartite spectral graph partitioning", "journal": "", "year": "2001", "authors": " Inderjit S Dhillon"}, {"ref_id": "b9", "title": "Information-theoretic coclustering", "journal": "", "year": "2003", "authors": "Subramanyam Inderjit S Dhillon; Dharmendra S Mallela;  Modha"}, {"ref_id": "b10", "title": "Data augmentation for deep graph learning: A survey", "journal": "", "year": "2022", "authors": "Kaize Ding; Zhe Xu; Hanghang Tong; Huan Liu"}, {"ref_id": "b11", "title": "metapath2vec: Scalable representation learning for heterogeneous networks", "journal": "", "year": "2017", "authors": "Yuxiao Dong; Nitesh V Chawla; Ananthram Swami"}, {"ref_id": "b12", "title": "New frontiers of multi-network mining: Recent developments and future trend", "journal": "", "year": "2021", "authors": "Boxin Du; Si Zhang; Yuchen Yan; Hanghang Tong"}, {"ref_id": "b13", "title": "Adversarial graph contrastive learning with information regularization", "journal": "", "year": "2022", "authors": "Shengyu Feng; Baoyu Jing; Yada Zhu; Hanghang Tong"}, {"ref_id": "b14", "title": "Ariel: Adversarial graph contrastive learning", "journal": "", "year": "2022", "authors": "Shengyu Feng; Baoyu Jing; Yada Zhu; Hanghang Tong"}, {"ref_id": "b15", "title": "A view-adversarial framework for multi-view network embedding", "journal": "ACM", "year": "2020", "authors": "Dongqi Fu; Zhe Xu; Bo Li; Hanghang Tong; Jingrui He"}, {"ref_id": "b16", "title": "Bine: Bipartite network embedding", "journal": "", "year": "2018", "authors": "Ming Gao; Leihui Chen; Xiangnan He; Aoying Zhou"}, {"ref_id": "b17", "title": "node2vec: Scalable feature learning for networks", "journal": "", "year": "2016", "authors": "Aditya Grover; Jure Leskovec"}, {"ref_id": "b18", "title": "Co-clustering on manifolds", "journal": "", "year": "2009", "authors": "Quanquan Gu; Jie Zhou"}, {"ref_id": "b19", "title": "The movielens datasets: History and context", "journal": "Acm transactions on interactive intelligent systems (tiis)", "year": "2015", "authors": "Maxwell Harper; Joseph A Konstan"}, {"ref_id": "b20", "title": "Contrastive multi-view representation learning on graphs", "journal": "PMLR", "year": "2020", "authors": "Kaveh Hassani; Amir Hosein; Khasahmadi "}, {"ref_id": "b21", "title": "Birank: Towards ranking on bipartite graphs", "journal": "IEEE Transactions on Knowledge and Data Engineering", "year": "2016", "authors": "Xiangnan He; Ming Gao; Min-Yen Kan; Dingxian Wang"}, {"ref_id": "b22", "title": "Neural collaborative filtering", "journal": "", "year": "2017", "authors": "Xiangnan He; Lizi Liao; Hanwang Zhang; Liqiang Nie; Xia Hu; Tat-Seng Chua"}, {"ref_id": "b23", "title": "X-goal: Multiplex heterogeneous graph prototypical contrastive learning", "journal": "", "year": "2022", "authors": "Baoyu Jing; Shengyu Feng; Yuejia Xiang; Xi Chen; Yu Chen; Hanghang Tong"}, {"ref_id": "b24", "title": "Hdmi: High-order deep multiplex infomax", "journal": "", "year": "2021", "authors": "Baoyu Jing; Chanyoung Park; Hanghang Tong"}, {"ref_id": "b25", "title": "Network of tensor time series", "journal": "", "year": "2021", "authors": "Baoyu Jing; Hanghang Tong; Yada Zhu"}, {"ref_id": "b26", "title": "Multiplex graph neural network for extractive text summarization", "journal": "", "year": "2021", "authors": "Baoyu Jing; Zeyu You; Tao Yang; Wei Fan; Hanghang Tong"}, {"ref_id": "b27", "title": "Twin learning for similarity and clustering: A unified kernel approach", "journal": "", "year": "2017", "authors": "Zhao Kang; Chong Peng; Qiang Cheng"}, {"ref_id": "b28", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2014", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"ref_id": "b29", "title": "Variational graph auto-encoders", "journal": "", "year": "2016", "authors": "N Thomas; Max Kipf;  Welling"}, {"ref_id": "b30", "title": "Spectral biclustering of microarray data: coclustering genes and conditions", "journal": "Genome research", "year": "2003", "authors": "Yuval Kluger; Ronen Basri; Joseph T Chang; Mark Gerstein"}, {"ref_id": "b31", "title": "Graph communal contrastive learning", "journal": "", "year": "2022", "authors": "Bolian Li; Baoyu Jing; Hanghang Tong"}, {"ref_id": "b32", "title": "Prototypical contrastive learning of unsupervised representations", "journal": "", "year": "2020", "authors": "Junnan Li; Pan Zhou; Caiming Xiong; Steven Hoi"}, {"ref_id": "b33", "title": "Contrastive clustering", "journal": "", "year": "2021", "authors": "Yunfan Li; Peng Hu; Zitao Liu; Dezhong Peng; Joey Tianyi Zhou; Xi Peng"}, {"ref_id": "b34", "title": "Graph self-supervised learning: A survey", "journal": "", "year": "2021", "authors": "Yixin Liu; Shirui Pan; Ming Jin; Chuan Zhou; Feng Xia; Philip S Yu"}, {"ref_id": "b35", "title": "Poincar\u00e9 embeddings for learning hierarchical representations", "journal": "Advances in neural information processing systems", "year": "2017", "authors": "Maximillian Nickel; Douwe Kiela"}, {"ref_id": "b36", "title": "Learning a structured optimal bipartite graph for co-clustering", "journal": "Advances in Neural Information Processing Systems", "year": "2017", "authors": "Feiping Nie; Xiaoqian Wang; Cheng Deng; Heng Huang"}, {"ref_id": "b37", "title": "Representation learning with contrastive predictive coding", "journal": "", "year": "2018", "authors": "Aaron Van Den Oord; Yazhe Li; Oriol Vinyals"}, {"ref_id": "b38", "title": "Adversarially regularized graph autoencoder for graph embedding", "journal": "", "year": "2018", "authors": "Ruiqi Shirui Pan; Guodong Hu; Jing Long; Lina Jiang; Chengqi Yao;  Zhang"}, {"ref_id": "b39", "title": "Unsupervised attributed multiplex network embedding", "journal": "", "year": "2020", "authors": "Chanyoung Park; Donghyun Kim; Jiawei Han; Hwanjo Yu"}, {"ref_id": "b40", "title": "Pytorch: An imperative style, high-performance deep learning library", "journal": "Advances in neural information processing systems", "year": "2019", "authors": "Adam Paszke; Sam Gross; Francisco Massa; Adam Lerer; James Bradbury; Gregory Chanan; Trevor Killeen; Zeming Lin; Natalia Gimelshein; Luca Antiga"}, {"ref_id": "b41", "title": "Bipartite graphs in systems biology and medicine: a survey of methods and applications", "journal": "GigaScience", "year": "2018", "authors": " Georgios A Pavlopoulos; I Panagiota; Athanasia Kontou; Costas Pavlopoulou; Evripides Bouyioukos; Pantelis G Markou;  Bagos"}, {"ref_id": "b42", "title": "Graph representation learning via graphical mutual information maximization", "journal": "", "year": "2020", "authors": "Zhen Peng; Wenbing Huang; Minnan Luo; Qinghua Zheng; Yu Rong; Tingyang Xu; Junzhou Huang"}, {"ref_id": "b43", "title": "Deepwalk: Online learning of social representations", "journal": "", "year": "2014", "authors": "Bryan Perozzi; Rami Al-Rfou; Steven Skiena"}, {"ref_id": "b44", "title": "Latent dirichlet co-clustering", "journal": "IEEE", "year": "2006", "authors": "Mahdi Shafiei; Evangelos E Milios"}, {"ref_id": "b45", "title": "Bayesian co-clustering", "journal": "IEEE", "year": "2008", "authors": "Hanhuai Shan; Arindam Banerjee"}, {"ref_id": "b46", "title": "Understanding the limitations of variational mutual information estimators", "journal": "", "year": "2019", "authors": "Jiaming Song; Stefano Ermon"}, {"ref_id": "b47", "title": "Line: Largescale information network embedding", "journal": "", "year": "2015", "authors": "Jian Tang; Meng Qu; Mingzhe Wang; Ming Zhang; Jun Yan; Qiaozhu Mei"}, {"ref_id": "b48", "title": "Deep graph infomax", "journal": "ICLR (Poster)", "year": "2019", "authors": "Petar Velickovic; William Fedus; Pietro William L Hamilton; Yoshua Li\u00f2; Devon Bengio;  Hjelm"}, {"ref_id": "b49", "title": "Extracting and composing robust features with denoising autoencoders", "journal": "", "year": "2008", "authors": "Pascal Vincent; Hugo Larochelle; Yoshua Bengio; Pierre-Antoine Manzagol"}, {"ref_id": "b50", "title": "Structural deep network embedding", "journal": "", "year": "2016", "authors": "Daixin Wang; Peng Cui; Wenwu Zhu"}, {"ref_id": "b51", "title": "Augmentation-free graph contrastive learning", "journal": "", "year": "2022", "authors": "Haonan Wang; Jieyu Zhang; Qi Zhu; Wei Huang"}, {"ref_id": "b52", "title": "Nonparametric bayesian co-clustering ensembles", "journal": "SIAM", "year": "2011", "authors": "Pu Wang; Kathryn B Laskey; Carlotta Domeniconi; Michael I Jordan"}, {"ref_id": "b53", "title": "Graph learning based recommender systems: A review", "journal": "", "year": "2021", "authors": "Shoujin Wang; Liang Hu; Yan Wang; Xiangnan He; Z Quan;  Sheng; Longbing Mehmet A Orgun; Francesco Cao; Philip S Ricci;  Yu"}, {"ref_id": "b54", "title": "Neural graph collaborative filtering", "journal": "", "year": "2019", "authors": "Xiang Wang; Xiangnan He; Meng Wang; Fuli Feng; Tat-Seng Chua"}, {"ref_id": "b55", "title": "Self-supervised on graphs: Contrastive, generative, or predictive", "journal": "", "year": "2021", "authors": "Lirong Wu; Haitao Lin; Zhangyang Gao; Cheng Tan; Stan Z Li"}, {"ref_id": "b56", "title": "Deep co-clustering", "journal": "SIAM", "year": "2019", "authors": "Dongkuan Xu; Wei Cheng; Bo Zong; Jingchao Ni; Dongjin Song; Wenchao Yu; Yuncong Chen; Haifeng Chen; Xiang Zhang"}, {"ref_id": "b57", "title": "Prediction of drug-target interaction networks from the integration of chemical and genomic spaces", "journal": "Bioinformatics", "year": "2008", "authors": "Yoshihiro Yamanishi; Michihiro Araki; Alex Gutteridge; Wataru Honda; Minoru Kanehisa"}, {"ref_id": "b58", "title": "Dynamic knowledge graph alignment", "journal": "", "year": "2021", "authors": "Yuchen Yan; Lihui Liu; Yikun Ban; Baoyu Jing; Hanghang Tong"}, {"ref_id": "b59", "title": "Bright: A bridging algorithm for network alignment", "journal": "", "year": "2021", "authors": "Yuchen Yan; Si Zhang; Hanghang Tong"}, {"ref_id": "b60", "title": "Dissecting cross-layer dependency inference on multi-layered inter-dependent networks", "journal": "", "year": "", "authors": "Yuchen Yan; Qinghai Zhou; Jinning Li; Tarek Abdelzaher; Hanghang Tong"}, {"ref_id": "b61", "title": "Graph contrastive learning with augmentations", "journal": "Advances in Neural Information Processing Systems", "year": "2020", "authors": "Yuning You; Tianlong Chen; Yongduo Sui; Ting Chen; Zhangyang Wang; Yang Shen"}, {"ref_id": "b62", "title": "Inductive matrix completion based on graph neural networks", "journal": "", "year": "2019", "authors": "Muhan Zhang; Yixin Chen"}, {"ref_id": "b63", "title": "Learning node embeddings in interaction graphs", "journal": "", "year": "2017", "authors": "Yao Zhang; Yun Xiong; Xiangnan Kong; Yangyong Zhu"}, {"ref_id": "b64", "title": "Tackling oversmoothing of gnns with contrastive learning", "journal": "", "year": "2021", "authors": "Lecheng Zheng; Dongqi Fu; Jingrui He"}, {"ref_id": "b65", "title": "Contrastive learning with complex heterogeneity", "journal": "", "year": "2022", "authors": "Lecheng Zheng; Jinjun Xiong; Yada Zhu; Jingrui He"}, {"ref_id": "b66", "title": "Mentorgnn: Deriving curriculum for pre-training gnns", "journal": "ACM", "year": "2022", "authors": "Dawei Zhou; Lecheng Zheng; Dongqi Fu; Jiawei Han; Jingrui He"}, {"ref_id": "b67", "title": "Graph contrastive learning with adaptive augmentation", "journal": "", "year": "2021", "authors": "Yanqiao Zhu; Yichen Xu; Feng Yu; Qiang Liu; Shu Wu; Liang Wang"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Overview of COIN. The proposed COIN is comprised of a co-clustering function \u03c6 = (\u03c6 U , \u03c6 V ), instance-wise objective L I and mutual information of co-clusters I(K; L). \u03c6 U and \u03c6 V (1) share the same graph encoder E, which maps nodes u, v into embeddings u, v, and (2) have separate cluster networks C U and C V , which produce probabilities of different clusters p u and p v for nodes u and v. L I pulls positive node pairs closer (green solid arrow) and pushes negative node pairs far apart (red dashed arrow). I(K; L) is calculated based on p u and p v .", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "row normalized adjacency matrices; W \u2208 R d\u00d7d denotes the learnable weight matrix; \u03b4 denotes the activation function; [\u2022||\u2022] denotes the concatenation operation; d is the size of hidden dimension; U 0 is the randomly initialized embedding matrix. The updating function for V is the same as U , except for the specific weights of W.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: Sensitivity experiments for (a)(b) cluster numbers and (c)(d) \u03bb of mutual information loss.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: Mutual information of co-clusters v.s. the number of epochs", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "=k,l,u,v p(k, l, u, v) log p(u, v)p(k)p(l) p(u)p(v)p(k, l)(20)Denoting the argument of the logarithm term as \u2206, we have:\u2206 = \u2206 \u2022 p(k, l|u, v) p(k, l|u, v) = p(k)p(l) p(u)p(v)p(k, l)\u2022 p(k, l, u, v) p(k|u)p(l|v) = p(k, l, u, v) p(k, l)p(u|k)p(v|l)", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Take\u2206 back to Equation(20), we have:I(U ; V ) -I(K; L) = k,l,u,v p(k, l, u, v) log p(k, l, u, v) p(k, l)p(u|k)p(v|l) = D KL (p||q)", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Descriptions of datasets", "figure_data": "DatasetTaskEvaluation|U ||V ||E|Density# ClassWikipediaLink PredictionAUC-ROC, AUC-PR15,0003,21464,0950.1%-ML-100KTop-K RecommendationF1, NDCG, MAP, MRR9431,682100,0006.3%-ML-10MTop-K RecommendationF1, NDCG, MAP, MRR69,87810,67710,000,0541.3%-WebKBCo-ClusteringNMI4,1991,000342,8828.2%4WisconsinCo-ClustreingNMI265170325,4795.6%5IMDBCo-ClusteringNMI617187820,1561.7%17"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Performance (%) of link prediction on Wikepedia", "figure_data": "MethodWiki (50%)Wiki (40%)AUC-ROCAUC-PRAUC-ROCAUC-PRDeepWalk87.1985.3081.6080.29LINE66.6971.4964.2869.89Node2vec89.3788.1288.4187.55VGAE87.8186.9386.3285.74Metapath2vec87.2084.9486.7584.63GraphCL94.4094.8893.6794.25HDI93.7494.5093.0093.86DMGI93.0293.1192.0192.14HDMI94.1894.8693.5794.23PinSage94.2793.9592.7992.56BiNE94.3393.9393.1593.34GC-MC91.9092.1991.4091.74IGMC92.8593.1091.9092.19NeuMF92.6293.3891.4792.63NGCF94.2694.0793.0693.37BiGI94.9194.7594.0894.02COIN95.3095.0594.5394.44"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Performance (%) of NMI on co-clustering datasets", "figure_data": "DatasetK-Means SCC [8] SBC [30] CCMod [2] DRCC [18] CCInfo [9] SCMK [27] DeepCC [56] COINWebKB26.131.113.040.131.939.710.040.543.0Wisconsin37.535.438.235.120.439.342.946.749.3IMDB13.925.520.621.66.918.718.426.831.9"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Ablation study on Wikepedia", "figure_data": "MethodWiki (50%)Wiki (40%)AUC-ROCAUC-PRAUC-ROCAUC-PRCOIN95.3095.0594.5394.44w/o I(K; L)94.7394.6293.8693.76S=cosine similarity94.8493.9894.1292.99S=dot product94.7694.5694.0893.76p(U, V ) = p(U )p(V )94.6694.5593.9693.81"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "of the 31st ACM International Conference on Information & Knowledge Management, pages 2341-2351, 2022. [61] Yonghui Yang, Le Wu, Richang Hong, Kun Zhang, and Meng Wang. Enhanced graph learning for collaborative filtering via mutual information maximization. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval,", "figure_data": "pages 71-80, 2021.[62] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and JureLeskovec. Graph convolutional neural networks for web-scale recommender systems. InProceedings of the 24th ACM SIGKDD international conference on knowledge discovery &data mining, pages 974-983, 2018."}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Descriptions of datasets", "figure_data": "DatasetTaskEvaluation|U ||V ||E|Density# ClassWikipediaLink PredictionAUC-ROC, AUC-PR15,0003,21464,0950.1%-ML-100KTop-K RecommendationF1, NDCG, MAP, MRR9431,682100,0006.3%-ML-10MTop-K RecommendationF1, NDCG, MAP, MRR69,87810,67710,000,0541.3%-WebKBCo-ClusteringNMI4,1991,000342,8828.2%4WisconsinCo-ClustreingNMI265170325,4795.6%5IMDBCo-ClusteringNMI617187820,1561.7%17"}], "formulas": [{"formula_id": "formula_0", "formula_text": "(U, V) = E(G) to extract informative node embeddings U \u2208 R |U |\u00d7d , V \u2208 R |V |\u00d7d from G,", "formula_coordinates": [2.0, 108.0, 492.76, 395.5, 21.44]}, {"formula_id": "formula_1", "formula_text": "V into N K |U | and N L |V | clusters via the function \u03c6 = (\u03c6 U , \u03c6 V ), where \u03c6 U (u) \u2208 R N K and \u03c6 V (v) \u2208 R N L produce the conditional probabilities of cluster assignments p(k|u), p(l|v) for nodes u, v. Here k \u2208 [1, \u2022 \u2022 \u2022 , N K ] and l \u2208 [1, \u2022 \u2022 \u2022 , N L ]", "formula_coordinates": [2.0, 108.0, 560.93, 396.17, 33.04]}, {"formula_id": "formula_2", "formula_text": "= 1 Z if (u, v) is connected, where Z is a normalization parameter, p(u, v) = 0 otherwise.", "formula_coordinates": [3.0, 106.83, 317.9, 397.17, 21.75]}, {"formula_id": "formula_3", "formula_text": "p(k, l) = u,v p(k|u)p(l|v)p(u, v)(1)", "formula_coordinates": [3.0, 238.22, 476.47, 265.78, 19.61]}, {"formula_id": "formula_4", "formula_text": "max L I = max u,v e S(u,v) e S(u,v) + e S(u,v )(2)", "formula_coordinates": [3.0, 227.76, 665.33, 276.25, 27.93]}, {"formula_id": "formula_5", "formula_text": "V \u2286 V , such that (u, v ) is not connected: (u, v ) / \u2208 E for \u2200v \u2208 V .", "formula_coordinates": [3.0, 218.95, 713.2, 286.8, 8.96]}, {"formula_id": "formula_6", "formula_text": "max L = max \u03bbI(K; L) + L I (3", "formula_coordinates": [4.0, 244.36, 180.26, 255.77, 9.65]}, {"formula_id": "formula_7", "formula_text": ")", "formula_coordinates": [4.0, 500.13, 180.58, 3.87, 8.64]}, {"formula_id": "formula_8", "formula_text": "log p(k)p(l) \u2265 u,v p(u, v|k, l) log p(u, k)p(v, l) p(u, v|k, l)(4)", "formula_coordinates": [4.0, 207.06, 332.03, 296.94, 26.35]}, {"formula_id": "formula_9", "formula_text": "k \u2208 {1, \u2022 \u2022 \u2022 , N K }, l \u2208 {1, \u2022 \u2022 \u2022 , N L }, u \u2208 U, v \u2208 V.", "formula_coordinates": [4.0, 134.47, 366.85, 212.16, 9.68]}, {"formula_id": "formula_10", "formula_text": "I(K; L) \u2264 I(U; V)(5)", "formula_coordinates": [4.0, 265.23, 435.52, 238.77, 8.99]}, {"formula_id": "formula_11", "formula_text": "I(K; L) = k,l p(k, l) log p(k, l) log p(k)p(l) \u2264 k,l p(k, l) u,v p(u, v|k, l) log p(k, l)p(u, v|k, l) p(u, k)p(v, l) = R (6)", "formula_coordinates": [4.0, 115.64, 475.23, 388.36, 26.91]}, {"formula_id": "formula_12", "formula_text": "R = u,v,k,l p(u, v, k, l) log p(u, v, k, l) p(u, k)p(v, l) = u,v,k,l p(u, v, k, l) log p(k, l|u, v)p(u, v) p(u)p(k|u)p(v)p(l|v)(7)", "formula_coordinates": [4.0, 118.6, 527.39, 385.4, 26.91]}, {"formula_id": "formula_13", "formula_text": "R = u,v,k,l p(u, v, k, l) log p(u, v) p(u)p(v) = u,v p(u, v) log p(u, v) p(u)p(v) = I(U; V)(8)", "formula_coordinates": [4.0, 148.74, 589.8, 355.26, 26.91]}, {"formula_id": "formula_14", "formula_text": "I(U, V ) -I(K; L) = D KL (p(K, L, U, V )||q(K, L, U, V ))(9)", "formula_coordinates": [5.0, 187.06, 208.18, 316.94, 9.65]}, {"formula_id": "formula_15", "formula_text": "Vl = \u03b4 2 ([\u03b4 1 (A V U U l-1 W l 1 )||V l-1 ]W l 2 )(10)", "formula_coordinates": [5.0, 223.89, 690.65, 280.11, 13.17]}, {"formula_id": "formula_16", "formula_text": "U l = \u03b4 2 ([\u03b4 1 (A U V Vl W l 3 )||U l-1 ]W l 4 )(11)", "formula_coordinates": [5.0, 221.98, 707.38, 282.02, 13.17]}, {"formula_id": "formula_17", "formula_text": "A V U \u2208 R |V |\u00d7|U | , A U V \u2208 R |U |\u00d7|V | are", "formula_coordinates": [6.0, 135.53, 269.72, 165.32, 11.23]}, {"formula_id": "formula_18", "formula_text": "S(u, v) = W 2 (Tanh(W 1 [u||v])), where W 1 \u2208 R d\u00d72d , W 2 \u2208 R d\u00d7d , d = 128.", "formula_coordinates": [6.0, 107.4, 347.7, 397.84, 23.17]}, {"formula_id": "formula_19", "formula_text": "log p(k)p(l) \u2265 u,v p(u, v|k, l) log p(u, k)p(v, l) p(u, v|k, l)(12)", "formula_coordinates": [14.0, 207.06, 114.12, 296.94, 26.35]}, {"formula_id": "formula_20", "formula_text": "k \u2208 {1, \u2022 \u2022 \u2022 , N K }, l \u2208 {1, \u2022 \u2022 \u2022 , N L }, u \u2208 U, v \u2208 V.", "formula_coordinates": [14.0, 134.47, 148.69, 212.16, 9.68]}, {"formula_id": "formula_21", "formula_text": "log p(k)p(l) = log u,v p(k, u)p(l, v) p(u, v|k, l) p(u, v|k, l) \u2265 u,v p(u, v|k, l) log p(u, k)p(v, l) p(u, v|k, l)(13)", "formula_coordinates": [14.0, 125.02, 199.51, 378.98, 26.38]}, {"formula_id": "formula_22", "formula_text": "I(K; L) \u2264 I(U; V)(14)", "formula_coordinates": [14.0, 265.23, 278.55, 238.77, 8.99]}, {"formula_id": "formula_23", "formula_text": "I(K; L) = k,l p(k, l) log p(k, l) log p(k)p(l) \u2264 k,l p(k, l) u,v p(u, v|k, l) log p(k, l)p(u, v|k, l) p(u, k)p(v, l) = R(15)", "formula_coordinates": [14.0, 113.15, 317.99, 390.85, 26.91]}, {"formula_id": "formula_24", "formula_text": "R = u,v,k,l p(u, v, k, l) log p(u, v, k, l) p(u, k)p(v, l) = u,v,k,l p(u, v, k, l) log p(k, l|u, v)p(u, v) p(u)p(k|u)p(v)p(l|v)(16)", "formula_coordinates": [14.0, 116.11, 369.67, 387.89, 26.91]}, {"formula_id": "formula_25", "formula_text": "R = u,v,k,l p(u, v, k, l) log p(u, v) p(u)p(v) = u,v p(u, v) log p(u, v) p(u)p(v) = I(U; V)(17)", "formula_coordinates": [14.0, 148.74, 431.6, 355.26, 26.91]}, {"formula_id": "formula_26", "formula_text": "I(U, V ) -I(K; L) = D KL (p(K, L, U, V )||q(K, L, U, V ))(18)", "formula_coordinates": [14.0, 187.06, 522.1, 316.94, 9.65]}, {"formula_id": "formula_27", "formula_text": "I(U ; V ) -I(K; L) = k,l,u,v p(k, l, u, v) log p(u, v) p(u)p(v) - k,l,u,v p(k, l, u, v) log p(k, l) p(k)p(l)(19)", "formula_coordinates": [14.0, 121.46, 578.04, 382.54, 26.88]}], "doi": ""}
