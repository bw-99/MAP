{"TD3: Tucker Decomposition Based Dataset Distillation Method for Sequential Recommendation": "", "Jiaqing Zhang": "", "Mingjia Yin": "", "Hao Wang \u2217": "jiaqing.zhang@mail.ustc.edu.cn University of Science and Technology of China & State Key Laboratory of Cognitive Intelligence, Hefei, China mingjia-yin@mail.ustc.edu.cn University of Science and Technology of China & State Key Laboratory of Cognitive Intelligence, Hefei, China Yawen Li warmly0716@126.com Beijing University of Posts and Telecommunications, Beijing, China Yuyang Ye yuyang.ye@rutgers.edu Rutgers Business School, Newark, NJ, USA Junping Du junpingd@bupt.edu.cn Beijing University of Posts and Telecommunications, Beijing, China wanghao3@ustc.edu.cn University of Science and Technology of China & State Key Laboratory of Cognitive Intelligence, Hefei, China Xingyu Lou xingyulou93@gmail.com Sun Yat-sen University, Guangzhou, China Enhong Chen cheneh@ustc.edu.cn University of Science and Technology of China & State Key Laboratory of Cognitive Intelligence, Hefei, China", "Abstract": "In the era of data-centric AI, the focus of recommender systems has shifted from model-centric innovations to data-centric approaches. The success of modern AI models is built on large-scale datasets, but this also results in significant training costs. Dataset distillation has emerged as a key solution, condensing large datasets to accelerate model training while preserving model performance. However, condensing discrete and sequentially correlated user-item interactions, particularly with extensive item sets, presents considerable challenges. This paper introduces TD3 , a novel T ucker D ecomposition based D ataset D istillation method within a meta-learning framework, designed for sequential recommendation. TD3 distills a fully expressive synthetic sequence summary from original data. To efficiently reduce computational complexity and extract refined latent patterns, Tucker decomposition decouples the summary into four factors: synthetic user latent factor , temporal dynamics latent factor , shared item latent factor , and a relation core that models their interconnections. Additionally, a surrogate objective in bi-level optimization is proposed to align feature spaces extracted from models trained on both original data and synthetic sequence summary beyond the na\u00efve performance matching approach. In the inner-loop , an augmentation technique allows the learner to closely fit the synthetic summary, ensuring an accurate update of it in the outer-loop . \u2217 Corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WWW'25, Sydney, NSW, Australia \u00a9 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-1274-6/25/04 https://doi.org/10.1145/3696410.3714613 To accelerate the optimization process and address long dependencies, RaT-BPTT is employed for bi-level optimization. Experiments and analyses on multiple public datasets have confirmed the superiority and cross-architecture generalizability of the proposed designs. Codes are released at https://github.com/USTC-StarTeam/TD3.", "CCS Concepts": "\u00b7 Information systems \u2192 Data mining .", "Keywords": "Recommender Systems; Dataset Distillation; Bi-level Optimization", "ACMReference Format:": "Jiaqing Zhang, Mingjia Yin, Hao Wang, Yawen Li, Yuyang Ye, Xingyu Lou, Junping Du, and Enhong Chen. 2025. TD3: Tucker Decomposition Based Dataset Distillation Method for Sequential Recommendation. In Proceedings of the ACM Web Conference 2025 (WWW '25), April 28-May 2, 2025, Sydney, NSW, Australia. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/ 3696410.3714613", "1 Introduction": "To address the persistent challenge of information overload from the Internet, Recommendation Systems (RS) have become essential tools by suggesting personalized content to users [13, 51, 59, 66, 72, 75]. Among them, Sequential Recommendation Systems (SRS) capture user preferences in the evolving form through chronological interaction sequences [12, 19, 44, 52, 63, 82]. However, as recommendation models become increasingly complex, the primary constraint affecting recommendation performance gradually shifts towards the quantity and quality of recommendation data [20], leading to the emergence of data-centric recommendations, as shown in fig. 1. Moreover, several emerging AI companies have prioritized data for its numerous benefits, including improved accuracy, faster WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia Jiaqing Zhang et al. Model-Centric Recommender System: Optimizing Recommendation Model Data-Centric Recommender System: Optimizing Recommendation Data Tremendous Datasets Settled Model and Distillation Algorithm Informative Data Summary Settled Datasets Model Designs and Evaluation Metrics Best Designed Model Figure 1: Comparison of the data-centric recommender system through the lens of dataset distillation approach with the traditional model-centric recommender system. The key difference lies in their distinct optimization objectives. deployment, and standardized workflows. These collective initiatives in academia and industry highlight the growing recognition of data-centric approaches as essential for innovation [26, 36, 39, 49]. Several initiatives have already been dedicated to the data-centric movement. A notable work launched by [71] aims to acquire an informative and generalizable training dataset for sequential recommender systems and asks the participants to iterate on the sequential recommendation dataset regeneration mostly focusing on improving the data quality. Another separate line is dataset distillation (DD) [53], which focuses on both data quality and data quantity. Unlike heuristic data pruning methods that directly select data points from original datasets, DD methods are designed to generate novel data points and have emerged as a solution for creating high-quality and informative data summaries. The utility of DD approaches has been witnessed in several fields, including federated learning [18, 28, 54, 64], continual learning [9, 34, 69, 76], graph neural network [5, 11, 68, 77] and recommender systems [41, 42, 50]. Significant progress has been made in DD for non-sequential recommender systems [41, 50, 57, 58]. Methods like \u221e -AE [41] and DConRec [57] distill user-item interaction matrices, while CGM [50] condenses categorical recommendation data in clickthrough rate (CTR) scenarios. Additionally, TF-DCon [58] employs large language models (LLMs) to condense user and item content for content-based recommendations. However, applying DD to sequential recommendation systems presents challenges due to several inherent complexities. (1) Maintaining sequential correlations : User-item interactions are sequentially correlated, reflecting the dynamic evolution of user preferences. Existing DD methods generate multiple synthetic interactions independently. Although it is possible to trivially organize these interactions into a sequence, this fails to capture the sequential correlations essential for modeling user behavior over time. (2) Optimization dilemma : In DD, the distilled dataset is typically parameterized as a learnable matrix, enabling fully differentiable distillation through a bi-level optimization process [2]. In sequential settings, this optimization becomes more difficult because the parameterized dataset size increases with sequence length. The enlarged parameter space further exacerbates convergence issues in the bi-level optimization process. To address these challenges, we introduce TD3 to efficiently reduce computational complexity and extract streamlined latent patterns by decomposing the summary into four components: (1) Synthetic User Latent Factor ( U ), which represents synthetic user representations; (2) Temporal Dynamics Latent Factor ( T ), which captures temporal contextual information; (3) Shared Item Latent Factor ( V ), which characterizes items within the set and aligns with the item embedding table; and (4) Relation Core ( G ), which models the interrelationships among the factors. After decomposition, each factor is represented as a two-dimensional tensor, with its size determined by the sequence number, maximum sequence length, and item set size. Additionally, component V shared with the item embedding table does not require learning during distillation, making TD3 suitable for large item sets and long sequences. To address the final challenge, we propose an enhanced bi-level optimization objective to align feature spaces from models trained on both original and synthetic data. During inner-loop training, an augmentation technique allows the learner model to deeply fit the synthetic summary, ensuring accurate updates of it in the outerloop . This approach accelerates convergence and, in conjunction with RaT-BPTT [6], minimizes computational costs while ensuring effective distillation. The contributions are concretely summarized: \u00b7 Westudy a novel problem in sequential recommendation: distilling a compressed yet informative synthetic sequence summary that retains essential information from the original dataset. \u00b7 We introduce TD3, which employs Tucker decomposition to separate the factors influencing the size of the synthetic summary, thereby reducing computational and storage complexity. \u00b7 Augmented learner training in inner-loop ensures precise synthetic data updates and feature space alignment loss is proposed beyond the na\u00efve bi-level optimization objective for a better loss landscape to optimize while minimizing computational costs and preserving long dependencies through RaT-BPTT. \u00b7 Empirical studies on public datasets have confirmed the superiority and cross-architecture generalizability of TD3's designs.", "2 Related Work": "Sequential Recommendation (SR) aims to leverage historical sequences to better capture current user intent [38]. In recent years, there has been rapid advancement in model-centric SR approaches, focusing from Markov chains [15] and factorization [40] to RNNs [16, 24], CNNs [46, 67], and GNNs [55, 60, 65]. Models like SASRec [19] and BERT4Rec [45] leverage self-attention mechanisms to learn the influence of each interaction on target behaviors. Recently, the emergence of LLMs has further enriched SR model design, for instance, [14, 70] leverage LLMs to uncover latent relationships [10], while RecFormer [23] represents items as item \"sentences\", and SAID [17] employs LLMs to learn semantically aligned item ID embeddings. With the emergence of the concept of data-centric recommendation, more works shift the focus to recommendation data enhancement. DR4SR [71] proposes to regenerate an informative and generalizable training dataset for sequential recommendations. FMLP-Rec [83] and HSD [74] adopt learnable filters for data denoising. DiffuASR [29] proposes a diffusion augmentation for higher quality data generation. ASReP [30] focused on generating fabricated data for long-tailed sequences. TD3 WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia Figure 2: An illustration of Tucker decomposition. The left part shows a three-dimensional synthetic sequence summary, with the third dimension representing the probability distribution over the entire item set. The right part illustrates the tucker decomposition, composed of a core tensor and factor matrices. The user tensor, temporal tensor, and core tensor are the parameters to be learned, while the item tensor shares values with the trained item embedding table. Decomposition Core Tensor User Tensor Temporal Tensor Item Embedding Synthetic Users Total Items Time Steps Dataset Distillation (DD) compresses large training datasets into smaller ones while preserving similar performance [8, 33]. There have been several lines of methods, that prioritize different aspects of information. Performance matching based methods focus on optimizing loss at the final training stage. For example, Farzi [42] distills auto-regressive data in latent space to produce a latent data summary and a decoder, although its parameters scale linearly with the vocabulary size and sequence length. \u221e -AE [41] uses neural tangent kernels (NTKs) to approximate an infinitely wide autoencoder and synthesizes fake users through sampling-based reconstruction, while DConRec [57] distills synthetic datasets by sampling useritem pairs from a learnable probabilistic matrix, both tailored for collaborative filtering data in the form of user-item-rating triples. Another line of research focuses on data matching , encouraging synthetic data to replicate the behavior of target data. CGM [50], following the gradient matching paradigm [79] that mimics the influence on model parameters by matching the gradients of the target and synthetic data in each iteration, optimizes a new form of synthetic data rather than condensing discrete one- or multi-hot data in CTR scenarios. Furthermore, TF-DCon [58] utilizes large language models (LLMs) to condense item and user content for content-based recommendations, although this approach is hardly applicable to the contexts of ID-based sequential recommendation. Tucker Decomposition (TD) decomposes a tensor into a set of factor matrices and a smaller core tensor [47]. It can be viewed as a kind of principal component analysis approach for high-order tensors. In particular, when the super-diagonal elements in the core tensor of tucker equal 1 and other elements equal 0, tucker decomposition degrades into canonical decomposition [62]. In three-mode case, A tucker decomposition of a tensor \ud835\udc4b \u2208 R \ud835\udc3c 1 \u00d7 \ud835\udc3c 2 \u00d7 \ud835\udc3c 3 is:  where \u00d7 \ud835\udc5b indicating the tensor product along the n-th mode, each \ud835\udc34 ( \ud835\udc5b ) \u2208 R \ud835\udc3c \ud835\udc5b \u00d7 \ud835\udc45 \ud835\udc5b is called the factor matrix , and G \u2208 R \ud835\udc45 1 \u00d7 \ud835\udc45 2 \u00d7 \ud835\udc45 3 is the core tensor , show the level of interaction between all factors.", "3 Methodology": "This section introduces TD3, which distills discrete and complex sequential recommendation datasets into fully expressive synthetic sequence summaries in latent space.", "3.1 Overview": "Notation. Suppose we are given a large training dataset T \u225c { x \ud835\udc56 } | T | \ud835\udc56 = 1 , where x \ud835\udc56 \u225c [ \ud835\udc65 \ud835\udc56 \ud835\udc57 \u2208 V] | x \ud835\udc56 | \ud835\udc57 = 1 is an ordered sequence of items, with each item \ud835\udc65 \ud835\udc56 \ud835\udc57 belonging to the set of all possible items V . We denote the user set of the training dataset as U , where |U| = |T | . Our goal is to learn a differentiable function \u03a6 \ud835\udf03 ( i.e. SASRec) with parameters \ud835\udf03 , which predicts the next item \ud835\udc65 \ud835\udc56 + 1 given the previous sequence \ud835\udc65 1: \ud835\udc56 . The parameters of this function are optimized by minimizing an empirical loss over the training set :  where function \u2113 T (\u00b7 , \u00b7) represents the next item prediction loss, and \ud835\udf03 T is the minimizer of L T , which reflects the generalization performance of the model \u03a6 \ud835\udf03 T . Our objective is to generate a small set of condensed synthetic sequence summary S \u2208 R \ud835\udf07 \u00d7 \ud835\udf01 \u00d7|V| consisting of \ud835\udf07 fake sequences of maximum length \ud835\udf01 and |S| \u226a |T | . Similar to eq. (2), once the condensed set is learned, the parameters \ud835\udf03 can be trained on this set as follows :  where \u02dc x \ud835\udc56 \u225c [ S[ \ud835\udc56, \ud835\udc57, : ] ] \ud835\udf01 \ud835\udc57 = 1 , \u2113 S (\u00b7 , \u00b7) measures the distance between probability distributions, \ud835\udf13 (\u00b7 , \u00b7) is matrix product, E is the item embedding table and L S is the generalization performance of \u03a6 \ud835\udf03 S . We wish the generalization performance of \u03a6 \ud835\udf03 S to be close to \u03a6 \ud835\udf03 T :  As the synthetic set S is significantly smaller, we expect the optimization in eq. (3) to be significantly faster than in eq. (2) . Problem. The objective of achieving comparable generalization performance by training on synthetic data can be formulated in an alternative way. As proposed in [53], this can be framed as a meta-learning problem using bi-level optimization. In this approach, the inner-loop trains the learner models on synthetic data, while the outer-loop evaluates its quality using \u2113 T (\u00b7 , \u00b7) on the original dataset, updating the synthetic summary via gradient descent. More formally, the bi-level optimization problem can be expressed as :  The primary approach for addressing bi-level optimization problems is truncated backpropagation through time (T-BPTT) [37, 56] in reverse mode. When the inner-loop learner updated uses gradient WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia Jiaqing Zhang et al. Figure 3: Illustration of TD3. In step 1, the learner is trained to get the best checkpoint for feature space alignment and item embedding for the shared item latent factor , and checkpoints from early epochs are saved to form a pool for initialization in each outer-loop . Step 2 visualizes a single outer-loop step, where the meta-gradient is computed from both the test loss and the feature space alignment loss. Step 3 demonstrates that the distilled summary enables training of other networks with similar performance to models trained on real data, while significantly reducing training time and memory usage.", "Step 1: Model Pretraining": "Item-Embedding Trained-Model Early Epoch Pretrained Learner Model Pool \u2026 Training Data Pretraining", "Step 2: Synthetic Data Distilling": "Training with Synthetic Summary \u00d7 T inner-steps Test Loss Feature Space Alignment Loss RaT-BPTT forward Training Data Synthetic Summary", "Step 3: Model Training": "Comparable Performance Training Data Synthetic Summary Significantly less training time and memory usage. descent with a learning rate \ud835\udf02 , the meta-gradient with respect to the distilled sequence summary is obtained as follows :  The advantage of this decomposition lies in its ability to decouple the factors influencing the size of S , thereby reducing data dimensionality as well as computational and storage complexity while preserving key feature information. This approach enables more efficient processing of high-dimensional data and enhances the understanding of its structure and characteristics. where \ud835\udc47 represents the total optimization and unrolling steps that we perform in inner-step with loss L \ud835\udc46 ( \ud835\udf03 ) , but T-BPTT only propagates backward through a smaller window of \ud835\udc40 steps.", "3.2 Synthetic Summary Decomposition": "The discrete nature of user-item interaction records in sequential recommendation data complicates the direct use of gradient methods to distill an informative summary in the same format as the original data. Inspired by prior research [27, 32, 42], we choose to distill in the latent space. Consequently, we define the synthetic sequence summary as S \u2208 R \ud835\udf07 \u00d7 \ud835\udf01 \u00d7|V| , a three-dimensional probability tensor that contains \ud835\udf07 synthetic users, each with up to \ud835\udf01 interaction records. The third dimension of S represents the size of the entire item set |V| , where S \ud835\udc56 \ud835\udc57 : captures the interaction information of synthetic user \ud835\udc56 at position \ud835\udc57 by synthesizing the total original item information, with each item weighted differently, ensuring that the summary preserves the critical points. Considering that the size of S is \ud835\udf07 \u00d7 \ud835\udf01 \u00d7 |V| , an increase in any of its dimensions leads to substantial growth in the overall tensor size, thereby escalating computational and storage requirements, especially when the original item set is large. Inspired by Tucker decomposition, S can be decomposed into the products of several smaller factor tensors and a core tensor, where the dimension of each sub-tensor is determined by only a single dimension, as visualized in fig. 2 and formalized as follows:  where U \u2208 R \ud835\udf07 \u00d7 \ud835\udc51 1 , T \u2208 R \ud835\udf01 \u00d7 \ud835\udc51 2 , V \u2208 R | V | \u00d7 \ud835\udc51 3 , G \u2208 R \ud835\udc51 1 \u00d7 \ud835\udc51 2 \u00d7 \ud835\udc51 3 , with \u00d7 \ud835\udc5b indicating the tensor product along the n-th mode and \ud835\udc51 \ud835\udc5b \u226a |V| . Empirically, \ud835\udc51 1 = \ud835\udc51 2 . More importantly, V is shared with the trained item embedding table, with no parameters needed to be trained.", "3.3 Enhanced Bi-Level Optimization": "Existing methods are computationally expensive for generating synthetic datasets with satisfactory generalizability, as optimizing synthetic data requires differently initialized networks [73]. To accelerate dataset distillation, we propose 1) augmented learner training , which enables the learner model to effectively fit the synthetic summary, supporting precise and comprehensive updates of synthetic data. Additionally, as mentioned in [35, 61], bias and poorly conditioned loss landscapes arise from truncated unrolling in TBPTT, we further propose 2) feature space alignment which aligns feature spaces from models trained on both original and synthetic data, combined with 3) random truncated backpropagation through time (RaT-BPTT) proposed by [6] to reduce bias in TBPTT and create a more favorable loss landscape for optimization. 3.3.1 Augmented Learner Training. As shown in eq. (7), we define the synthetic sequences summary as a three-dimensional probabilistic tensor obtained from the factors of the tucker decomposition and a core matrix via the mode product operation: S \u2208 R \ud835\udf07 \u00d7 \ud835\udf01 \u00d7|V| . Under this settings, we use Kullback-Leibler (KL) Divergence as the loss function in eq. (3), and the inner objective is defined as:   where x is the input, y is the target, \u03a6 \ud835\udf03 S \ud835\udc57 (\u00b7) is the learner model trained on synthetic data in \ud835\udc57 -th step, \u02c6 y is output of the learner model and D \ud835\udc3e\ud835\udc3f (\u00b7 || \u00b7) is the discrete pointwise KL-divergence. TD3 WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia However, this approach only uses the previous 1 to \ud835\udf01 -1 interactions to predict the \ud835\udf01 -th interaction probability. We propose further enhancing the prediction by randomly sampling middle positions, which can significantly improve the diversity of training, strengthen contextual understanding, enhance the model's generalization ability, and reduce reliance on specific sequence patterns. This strategy helps the model capture sequence information more comprehensively, improving its practical application performance. 3.3.2 Feature Space Alignment. We propose an enhancement to the current outer-loop test accuracy objective by integrating a metric that ensures the alignment of feature spaces between models trained on the original dataset and those trained on a synthetic sequence summary. As highlighted in [21], the conventional metalearning framework is predominantly concerned with equating the performance of models trained on original data with those trained on synthetic data. However, from the perspective of loss surfaces, this strategy primarily attempts to replicate the local minima of the target data through distilled data [22]. Despite its initial efficacy, this method faces significant challenges due to the presence of poorly conditioned loss landscapes [43]. In light of these limitations, our goal is to optimize the synthetic data S such that the learner \u03a6 \ud835\udf03 S trained on them achieves not only comparable generalization performance to \u03a6 \ud835\udf03 T but also converges to a similar solution in the feature space. The enhanced objective can be formulated as:  where L F ( \ud835\udf03 \u2217 ) is the feature space alignment loss with a mean squared error (MSE) by optimizing synthetic summary S directly. Unlike the most basic meta-learning framework, by ensuring that the feature representations of the synthetic data trained model are closely aligned with those of the original data trained model, we aim to foster a more robust and reliable learning process. This approach not only enhances the model's ability to generalize but also improves its stability across diverse datasets, thereby addressing the inherent challenges posed by poorly conditioned loss landscapes. Through this enhancement, we seek to advance the field of metalearning, paving the way for more effective and efficient training paradigms, going beyond simply matching performance metrics. 3.3.3 Random Truncated Backpropagation Through Time. Training neural networks on distilled data is challenging largely due to the pronounced non-convexity of the optimization process. One common approach to capture long-term dependencies in this context is Backpropagation Through Time (BPTT), although it suffers from slow optimization and excessive memory demands. TBPTT, which limits unrolled steps, is a more efficient alternative. Yet, TBPTT introduces its drawbacks, such as bias from the truncation [61] and poorly conditioned loss landscapes, especially with long unrolls [48]. To address these issues, [6] propose the Random Truncated Backpropagation Through Time (RaT-BPTT) method, which", "Algorithm 1: Optimization for TD3": "Input : T : original dataset; [ \ud835\udc6e , U , T , V ] : core tensor and tucker factors for generating synthetic summary; \u0398 : pretrained model parameters; N : total unrolling steps for BPTT; W : truncated window size; \ud835\udefc : learning rate for the synthetic data; \ud835\udf02 : learning rate for the learner model; 1 // Outer loop: update synthetic sequences summary ) 2 while not converged do 3 \u25b7 Initialize learner's parameter \ud835\udf03 0 \u223c \u0398 4 \u25b7 Sample a mini-batch of original data B T \u223c T 5 \u25b7 Uniformly sample the ending unrolling step M \u223c \ud835\udc48 ( W , N 6 // Inner loop: update learner model parameters 7 for \ud835\udc5b \u2190 1 , . . . , \ud835\udc40 do 8 \u25b7 Sample a mini-batch of synthetic user : 9 B U \u223c U 10 \u25b7 Generate a mini-batch of synthetic summary : 11 B S = \ud835\udc6e \u00d7 1 B U \u00d7 2 T \u00d7 3 V 12 // Start Random Truncked Backpropagation Through time 13 if n = M - W - 1 then 14 \u25b7 start accumulating gradients 15 end if 16 \u25b7 Update learner's parameter by gradient descent : 17 \ud835\udf03 \ud835\udc5b = \ud835\udf03 \ud835\udc5b - 1 - \ud835\udf02 \u2207L S (B S ; \ud835\udf03 \ud835\udc5b - 1 ) 18 end for 19 \u25b7 Compute test loss and feature space alignment loss : 20 L( \ud835\udf03 \ud835\udc40 ) = L T ( \ud835\udf03 \ud835\udc40 ) + 1 2 \u2225 \u03a6 \ud835\udf03 T ( B T ) - \u03a6 \ud835\udf03 \ud835\udc46 \ud835\udc40 ( B T ) \u2225 2 \ud835\udc39 21 \u25b7 Update synthetic data summary S = S - \ud835\udefc \u2207S L( \ud835\udf03 \ud835\udc40 ) 22 end while Output: synthetic sequence summary S . combines randomization with truncation in BPTT. This approach unrolls within a randomly anchored and fixed-size window along the training trajectory and aggregates gradients within this window. The random window ensures that the RaT-BPTT gradient serves as a random subsample of the full BPTT gradient, covering the entire trajectory, while the truncated window improves gradient stability and reduces memory usage. As a result, RaT-BPTT enables faster training and better performance. It can be formulated as follows:  where \ud835\udc40 is the random number of total unrolled steps in the innerloop , and \ud835\udc4a represents the number of steps included in the backward, with only the final \ud835\udc4a steps being used for backpropagation. Previous studies have demonstrated that diverse models in inner optimization improve robustness of the synthetic data [1, 78]. Moreover, pretrained models significantly enhance dataset distillation by providing better initialization, faster convergence, and higher-quality synthetic data [31, 42]. Building on these insights, we maintain a pool of pretrained learner models from early training epochs, capturing various stages of learning. At each outer step of the distillation process, we randomly select models from this pool to ensure that the training signal remains diverse and representative for optimizing the synthetic dataset. The comprehensive training procedure for our proposed method is detailed in Algorithm 1. WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia Jiaqing Zhang et al. Table 1: Statistical information of experimental datasets.", "4 Experiments": "In this section, we present and analyze the experiments on four public datasets, aiming to", "4.1 Settings": "Training Datasets. To evaluate the distillation method proposed in this paper, we conduct experiments on four commonly used and publicly available datasets with variable statistics in table 1. 1) Amazon 1 includes Amazon product reviews and metadata infomation. For our empirical study, we mainly focus on the categories of \"Magazine_Subscriptions\". 2) MovieLens 2 is maintained by GroupLens and it contains movie ratings from the MovieLens recommendation service. We use the \"ml-100k\" and \"ml-1m\" versions for our experiments. 3) Epinions 3 was collected by [80] from Epinions.com, a popular online consumer review website. It describes consumer reviews and also contains trust relationships amongst users and spans more than a decade, from January 2001 to November 2013. Evaluation Metrics. We adopt the leave-one-out strategy for evaluation, following prior research [3, 71 ? ]. For each sequence, the most recent interaction is used for testing, the second for validation, and the rest for training. To expedite evaluation, as in previous studies [19], we randomly sample 100 negative items to rank with the ground-truth item, which closely approximates full-ranking results while significantly reducing the computational cost. We assess performance using HR, NDCG, and MRR. HR@k checks if the target item appears within the top-k recommendations, NDCG@k considers the item's rank, and MRR@k computes the average reciprocal rank of the first relevant item, with k \u2208 { 5 , 10 , 20 } . Implementation Details. Weimplement TD3 using PyTorch and develop recommendation models based on the library of Recbole [81]. Throughout the distillation process, we use SASRec [19] serves as the learner across all datasets, utilizing attention heads \u2208 { 1 , 2 } , layers \u2208 { 1 , 2 } , hidden size \u2208 { 64 , 128 } , inner size \u2208 { 64 , 128 , 256 } , and attention dropout probability of 0.5 and hidden dropout probability of 0.2. For magazine and epinions dataset, we set \ud835\udc51 1 , \ud835\udc51 2 \u2208 { 8 , 16 } , while for ml-100k and ml-1m dataset, we set \ud835\udc51 1 , \ud835\udc51 2 \u2208 { 16 , 32 , 64 } . To evaluate the cross-architecture generalization of the proposed TD3, we employ GRU4Rec [16], BERT4Rec [45], and NARM [24] for performance assessment. For all distilled datasets, we apply the Adam optimizer [4] for both the inner-loop and outer-loop optimization. 1 http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ 2 https://grouplens.org/datasets/movielens/ 3 https://cseweb.ucsd.edu/~jmcauley/datasets.html#social_data Table 2: Comparison of TD3 with existing dataset distillation techniques and heuristic sampling based methods. In the outer-loop , the synthetic sequence summary optimizer is configured with a learning rate \ud835\udefc \u2208 { 0 . 01 , 0 . 03 } and a weight decay of 0.0001, using a cosine scheduler to adjust the learning rate throughout the process. In the inner-loop , the learner optimizer employs a learning rate \ud835\udf02 \u2208 { 0 . 003 , 0 . 005 , 0 . 01 } , with a weight decay of 0.00005. The inner steps are set to 200, using a random truncated window of 40 for backpropagation through time in RaTBPTT, implemented via the Higher [7] package across all datasets.", "4.2 Overall Performance": "Weevaluated TD3's performance across various synthetic sequence summary sizes and diverse datasets. In table 3, we compare models trained on full original datasets with those using various-sized synthetic sequence summaries. Additionally, table 2 and fig. 4 compare TD3's performance with Farzi and heuristic sampling methods: random sampling , which selects sequences uniformly, and longest sampling , which selects sequences in descending order of length. Our findings are as follows: 1) TD3 achieves comparable training performance, even with substantial data compression. This shows that small-batch synthetic summaries distilled from the original dataset effectively capture essential information, preserving data integrity for model training. 2) In datasets such as Magazine and Epinions, models trained on TD3-distilled summaries outperform those trained on the original datasets. This highlights the value of high-quality, smaller datasets over larger, noisier ones, underscoring the importance of data quality in model training. 3) As illustrated in table 2 and fig. 4, TD3 is more sample-efficient than Farzi and heuristic methods, showing superior data utilization. These results illustrate the transformative potential of data distillation in improving sequential recommendation systems. This approach represents a shift towards a data-centric paradigm in recommender systems, where prioritizing data quantity and quality and strategic compression can create more robust and efficient algorithms, reducing computational costs and storage demands. This evolution paves the way for the next generation of recommendation algorithms, focusing on maximizing value from the minimal data. TD3 WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia Table 3: Comparison of TD3's performance (%) across various size synthetic data and the full dataset. Bold numbers denote the best-performing distilled summary. Underlined numbers denote that the distilled summary outperforms the full dataset. Superscript * means improvements are statistically significant with p<0.05, while ** means p<0.01. Table 4: Wall-clock runtime and storage costs are detailed for each operation. Distillation covers 30 epochs but usually needs less computation. Original data training uses early stopping, while synthetic data training is fixed at 200 or 500 epochs.", "4.3 Time and Memory Analysis": "4.3.1 Theoretical Memory Complexity. As discussed in Section 2, Farzi [42] decomposes synthetic data S \u2208 R \ud835\udf07 \u00d7 \ud835\udf01 \u00d7|V| to a latent summary D \u2208 R \ud835\udf07 \u00d7 \ud835\udf01 \u00d7 \ud835\udc51 and a decoder M \u2208 R \ud835\udc51 \u00d7|V| , where \ud835\udc51 \u226a |V| . To highlight the advantages of employing Tucker decomposition for three-dimensional data, we perform a comparative analysis of our proposed approach with that of Farzi. In particular, we investigate the computational footprint associated with the bi-level optimization framework by evaluating memory usage during a single outer-loop step, providing insights into the efficiency gains achieved through our method as follows: and \ud835\udf01 , the inequality ( \ud835\udf07 + \ud835\udf01 ) \u00b7 \ud835\udc51 1 + \ud835\udc51 2 1 \u00b7 \ud835\udc51 3 \u226a \ud835\udf07 \u00b7 \ud835\udf01 \u00b7 \ud835\udc51 + \ud835\udc51 \u00b7 |V| holds, the method proposed in our work will offer a spatial advantage.  where |B T | and |B S | denote the batch size of real and synthetic data, respectively, while \ud835\udc51 3 represents the item embeddings' hidden dimension. Additionally, \ud835\udc51 , \ud835\udc51 1 , and \ud835\udc51 3 are of the same order of magnitude and much smaller than |V| . When the item set is very large, or the distilled sequence summary requires larger values of \ud835\udf07 4.3.2 Empirical Computational Complexity. The data distillation process consumes considerable wall-clock time and GPU memory, so we conducted a detailed quantitative analysis of these requirements for both distillation and model training on the original and distilled datasets, as shown in table 4. Wall-clock time is reported in single A100 (80GB) GPU hours. While distillation generally takes longer and uses more memory than training on the original data, its cost is often amortizable in real-world scenarios where multiple models must be trained on the same dataset. The amortization, based on the ratios in the table, varies by dataset and distillation scale. Notably, for large datasets, where training time is typically lengthy, training on significantly reduced distilled data can shorten this process by several orders of magnitude. This trade-off substantially decreases future training time, making distillation a one-time cost that yields long-term benefits for various downstream tasks, such as hyperparameter tuning and architecture exploration. Hence, the distillation process and its amortization will be well justified. WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia Jiaqing Zhang et al. Figure 4: Illustration of the performance comparison of the TD3 against: Farzi , random sampling and longest sampling . 0.1 1 0 5 10 15 20 25 Synthetic Summary Size MRR@10 Magazine TD3 Distillation Full-Data Longest Sampling Random Sampling Farzi Distillation 0.1 1 0 10 20 30 40 Synthetic Summary Size MRR@10 ML-100k TD3 Distillation Full-Data Longest Sampling Random Sampling Farzi Distillation", "4.4 Cross-Architecture Generalization": "Since the synthetic sequence summary is carefully tailored for optimizing a specific learner model, we assess its generalizability across various unseen architectures, as shown in table 5. We first distill the Epinions dataset using SASRec [19], resulting in a condensed synthetic summary of size [ 50 \u00d7 20 ] . This summary is then used to train several alternative architectures, including GRU4Rec [16], which models sequential behavior using Gated Recurrent Units (GRU); NARM [25], which enhances GRU4Rec with an attention mechanism to emphasize user intent; and BERT4Rec [45], which uses bidirectional self-attention to learn sequence representations. The models trained on the synthetic summary demonstrate strong generalization across diverse architectures, maintaining high predictive performance. In some cases, they even outperform models trained on the original dataset, highlighting the effectiveness of our proposed TD3 method in enabling cross-architecture transferability.", "4.5 Ablation Studies": "To analyze our method's components, we conducted ablation studies on ML-100k . Results for Feature Space Alignment (FSA) and Augmented Learner Training (ALT) are in table 6. FSA and ALT complement each other. ALT significantly boosts TD3's performance by enhancing contextual understanding and reducing dependence on specific sequence patterns, improving sequence information capture and data updates in the outer-loop . FSA alone also enhances performance across metrics by strengthening the objective function, aiding convergence to a similar solution in the feature space, and Table 5: Evaluation of generalization performance on unseen architectures using a synthetic summary of size [ 50 \u00d7 20 ] distilled from the Epinions dataset via SASRec. Table 6: Ablation performance of a model trained on a [50 \u00d7 50] synthetic summary distilled from the ML-100k. \u2717 indicates a module was not used, while \u2713 indicates the opposite. maintaining performance on original and synthetic data. Utilizing both in the inner-loop and outer-loop maximizes distillation benefits.", "5 Conclusion": "In this paper, we propose TD3 which distills a large discrete sequential recommendation dataset into an informative synthetic summary, which is decomposed into four factors inspired by Tucker decomposition in latent space. TD3 offers several advantages, including the decoupling of factors that influence the size of S , thereby reducing data dimensionality, computational costs, and storage complexity while preserving essential feature information. Additionally, we introduce an enhanced bi-level optimization approach featuring an augmented learner training strategy in the inner-loop , ensuring the learner deeply fits the summary and a feature-space alignment surrogate objective in the outer-loop , ensuring optimal learning of synthetic data parameters. Experiments and analyses confirm the effectiveness and necessity of the proposed designs. While our work offers significant advantages, it does have certain limitations, notably the computational demands of the bi-level optimization process, which still can be challenging for scaling with larger models and datasets. However, this cost is often offset in scenarios where multiple models need training on the same dataset, substantially reducing training time in the future. Distillation becomes a one-time cost with long-term benefits for various tasks. In future work, we aim to develop a more time-efficient dataset distillation method that scales to larger datasets without sacrificing performance. Additionally, We also intend to use dataset distillation for cross-domain knowledge transfer, allowing the information from one domain to be reused in other domains, thereby enhancing the framework's versatility in different recommendation contexts. TD3 WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia", "References": "[1] George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei A Efros, and Jun-Yan Zhu. Dataset distillation by matching training trajectories. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4750-4759, 2022. [2] Stephan Dempe. Bilevel optimization: theory, algorithms, applications and a bibliography. Bilevel optimization: advances and next challenges , pages 581-672, 2020. [3] Jacob Devlin. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018. [4] P Kingma Diederik. Adam: A method for stochastic optimization. (No Title) , 2014. [5] Qizhang Feng, Zhimeng Stephen Jiang, Ruiquan Li, Yicheng Wang, Na Zou, Jiang Bian, and Xia Hu. Fair graph distillation. Advances in Neural Information Processing Systems , 36:80644-80660, 2023. [6] Yunzhen Feng, Shanmukha Ramakrishna Vedantam, and Julia Kempe. Embarrassingly simple dataset distillation. In The Twelfth International Conference on Learning Representations , 2023. [7] Edward Grefenstette, Brandon Amos, Denis Yarats, Phu Mon Htut, Artem Molchanov, Franziska Meier, Douwe Kiela, Kyunghyun Cho, and Soumith Chintala. Generalized inner loop meta-learning. arXiv preprint arXiv:1910.01727 , 2019. [8] Jianyang Gu, Saeed Vahidian, Vyacheslav Kungurtsev, Haonan Wang, Wei Jiang, Yang You, and Yiran Chen. Efficient dataset distillation via minimax diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15793-15803, 2024. [9] Jianyang Gu, Kai Wang, Wei Jiang, and Yang You. Summarizing stream data for memory-restricted online continual learning. arXiv preprint arXiv:2305.16645 , 2, 2023. [10] Wei Guo, Hao Wang, Luankang Zhang, Jin Yao Chin, Zhongzhou Liu, Kai Cheng, Qiushi Pan, Yi Quan Lee, Wanqi Xue, Tingjia Shen, et al. Scaling new frontiers: Insights into large recommendation models. arXiv preprint arXiv:2412.00714 , 2024. [11] Mridul Gupta, Sahil Manchanda, Sayan Ranu, and Hariprasad Kodamana. Mirage: Model-agnostic graph distillation for graph classification. arXiv preprint arXiv:2310.09486 , 2023. [12] Yongqiang Han, Hao Wang, Kefan Wang, Likang Wu, Zhi Li, Wei Guo, Yong Liu, Defu Lian, and Enhong Chen. End4rec: Efficient noise-decoupling for multibehavior sequential recommendation. arXiv preprint arXiv:2403.17603 , 2024. [13] Yongqiang Han, Likang Wu, Hao Wang, Guifeng Wang, Mengdi Zhang, Zhi Li, Defu Lian, and Enhong Chen. Guesr: A global unsupervised data-enhancement with bucket-cluster sampling for sequential recommendation. In International Conference on Database Systems for Advanced Applications , pages 286-296. Springer, 2023. [14] Jesse Harte, Wouter Zorgdrager, Panos Louridas, Asterios Katsifodimos, Dietmar Jannach, and Marios Fragkoulis. Leveraging large language models for sequential recommendation. In Proceedings of the 17th ACM Conference on Recommender Systems , pages 1096-1102, 2023. [15] Ruining He and Julian McAuley. Fusing similarity models with markov chains for sparse sequential recommendation. In 2016 IEEE 16th international conference on data mining (ICDM) , pages 191-200. IEEE, 2016. [16] Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. Session-based recommendations with recurrent neural networks. arXiv preprint arXiv:1511.06939 , 2015. [17] Jun Hu, Wenwen Xia, Xiaolu Zhang, Chilin Fu, Weichang Wu, Zhaoxin Huan, Ang Li, Zuoli Tang, and Jun Zhou. Enhancing sequential recommendation via llm-based semantic embedding learning. In Companion Proceedings of the ACM on Web Conference 2024 , pages 103-111, 2024. [18] Chun-Yin Huang, Kartik Srinivas, Xin Zhang, and Xiaoxiao Li. Overcoming data and model heterogeneities in decentralized federated learning via synthetic anchors. arXiv preprint arXiv:2405.11525 , 2024. [19] Wang-Cheng Kang and Julian McAuley. Self-attentive sequential recommendation. In 2018 IEEE international conference on data mining (ICDM) , pages 197-206. IEEE, 2018. [20] Riwei Lai, Li Chen, Rui Chen, and Chi Zhang. A survey on data-centric recommender systems. arXiv preprint arXiv:2401.17878 , 2024. [21] Shiye Lei and Dacheng Tao. A comprehensive survey of dataset distillation. IEEE Transactions on Pattern Analysis and Machine Intelligence , 2023. [22] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. Advances in neural information processing systems , 31, 2018. [23] Jiacheng Li, Ming Wang, Jin Li, Jinmiao Fu, Xin Shen, Jingbo Shang, and Julian McAuley. Text is all you need: Learning language representations for sequential recommendation. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining , pages 1258-1267, 2023. [24] Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Tao Lian, and Jun Ma. Neural attentive session-based recommendation. In Proceedings of the 2017 ACM on [50] Cheng Wang, Jiacheng Sun, Zhenhua Dong, Ruixuan Li, and Rui Zhang. Gradient matching for categorical data distillation in ctr prediction. In Proceedings of the 17th ACM Conference on Recommender Systems , pages 161-170, 2023. WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia Jiaqing Zhang et al."}
