{"AutoDenoise: Automatic Data Instance Denoising for Recommendations": "Weilin Lin City University of Hong Kong mlmr.lin@gmail.com", "Xiangyu Zhao \u2217": "City University of Hong Kong xianzhao@cityu.edu.hk Yejing Wang City University of Hong Kong yejing.wang@my.cityu.edu.hk", "Yuanshao Zhu": "City University of Hong Kong Southern University of Science and Technology Wanyu Wang City University of Hong Kong wanyuwang4-c@my.cityu.edu.hk zhuys2019@mail.sustech.edu.cn", "ABSTRACT": "Historical user-item interaction datasets are essential in training modern recommender systems for predicting user preferences. However, the arbitrary user behaviors in most recommendation scenarios lead to a large volume of noisy data instances being recorded, which cannot fully represent their true interests. While a large number of denoising studies are emerging in the recommender system community, all of them suffer from highly dynamic data distributions. In this paper, we propose a Deep Reinforcement Learning (DRL) based framework, AutoDenoise, with an Instance Denoising Policy Network, for denoising data instances with an instance selection manner in deep recommender systems. To be specific, AutoDenoise serves as an agent in DRL to adaptively select noise-free and predictive data instances, which can then be utilized directly in training representative recommendation models. In addition, we design an alternate two-phase optimization strategy to train and validate the AutoDenoise properly. In the searching phase, we aim to train the policy network with the capacity of instance denoising; in the validation phase, we find out and evaluate the denoised subset of data instances selected by the trained policy network, so as to validate its denoising ability. We conduct extensive experiments to validate the effectiveness of AutoDenoise combined with multiple representative recommender system models.", "CCS CONCEPTS": "\u00b7 Information systems \u2192 Recommender systems .", "KEYWORDS": "Instance Denoising, Recommender System, Reinforcement learning \u2217 Xiangyu Zhao is the corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WWW'23, April 30-May 4, 2023, Austin, TX, USA \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-9416-1/23/04...$15.00 https://doi.org/10.1145/3543507.3583339", "ACMReference Format:": "Weilin Lin, Xiangyu Zhao, Yejing Wang, Yuanshao Zhu, and Wanyu Wang. 2023. AutoDenoise: Automatic Data Instance Denoising for Recommendations. In Proceedings of the ACM Web Conference 2023 (WWW '23), April 30-May 4, 2023, Austin, TX, USA. ACM, New York, NY, USA, 9 pages. https: //doi.org/10.1145/3543507.3583339", "1 INTRODUCTION": "Recommender System (RS) is an essential technology in the era of information explosion, which can deliver a favorable experience for users and considerable economic benefits for companies [8, 13, 16]. As a data-driven technique, RS models user preference based on previous data instances (i.e., interaction logs) [25], thereby substantially improving the quality of online service platforms (e.g., e-commerce sites and streaming video applications) [5, 27]. In General, RS is modeled based on recorded data instances from historical user-item interactions. However, recording user behavior with the above approach inevitably raises challenges for modeling RS. First, the existing recommendation dataset collects all historical data instances indiscriminately, while some of them may be noisy instances that do not reflect the real intention of users [12]. For example, in click-through rate (CTR) prediction [3, 9, 17, 28, 31, 38, 40, 45], some click actions may come from user curiosity or mistake. Furthermore, the noisy instances have almost the same traits as noise-free instances [14], which renders a non-trivial task for human experts to separate them in the recording. To address the above challenges, denoising training data is attracting growing attention from researchers. Recent studies have shown that by using denoising techniques in recommender systems, models can be trained in a more efficient manner with better performance within comparable computational cost [7, 11, 32, 37]. Typically, denoising methods involve a 'searching' behavior to figure out noise and a 'deciding' behavior to execute denoising actions. According to the methods of denoising, existing efforts can be roughly categorized into two groups: selection-based and reweighting-based methods. For selection-based methods [6, 35], they aim to train a selection network to discard noisy instances, thereby feeding the model with more representative data. For reweighting-based methods [29, 30], they tend to lower the contributions of the noisy instances by assigning lower weights throughout the model training. In practice, they consider the loss values as the indicator to discriminate noise from noise-free instances, i.e., high values imply noisy data instances during training. WWW'23, April 30-May 4, 2023, Austin, TX, USA Weilin Lin, et al. Figure 1: Overview of the proposed framework. Prediction Batch 1 Batch n Batch N Validation Phase Policy Network X Reward Function Trained Policy Network Searching Phase User ID Item ID Target Recommender System Model Selected Instances Param. Init. / Epoch X Initialized Recommender System Model Selected Subset However, the above methods suffer from the following limitations. First, selection-based methods rely heavily on sampling distributions for decision-making [36], while the complex and dynamic data distribution in the contemporary online environment tends to limit their performance. In other words, the more dynamic environment is, the more biased selection may occur. Second, the reweighting-based methods influencing the model training processes lack transferability. Existing reweighting-based methods require specific configurations in the case of a given model or recommendation task, which is time-consuming and challenging to transfer to other settings. Therefore, a promising instance denoising scheme should be robust and easily-transfer. To bridge this research gap, we propose a Deep Reinforcement Learning (DRL) based instance denoising framework, which can model various complex data distributions of real-world datasets and filter a noise-free subset without noise. We propose this framework based on three main motivations: (i) The reinforcement learning methods are empirically proven to be effective in the optimumsearching problems [18, 19, 39, 41-44], which can also have the potential to distinguish the noisy data instances effectively. (ii) Policy network in DRL is used to select instances from a collection of mini-batch, allowing it to capture fine-grained patterns from various data distributions. Such design exactly facilitates mitigating the biased selection problem of selection-based methods. (iii) Separately training the prediction model and the policy network for the denoising and the prediction processes, owns natural transferability with the noise-free data subset from the denoising policy network. Nevertheless, applying the DRL-based approach to instance denoising will encounter the following challenges. On the one hand, the data distribution varies dramatically among mini-batches, rendering it challenging to design an appropriate reward function to optimize the policy network. What's more, since the policy network and RS model need to be optimized simultaneously in this setting, it is challenging to train them properly with the same data batch. To address the above challenges, we propose a DRL-based framework, AutoDenoise, with an instance denoising policy network. AutoDenoise scores each instance of a mini-batch by two probabilities for 'select/deselect' actions with the policy network, and evaluates the performance of the sampled instances as 'select' with the RS model. Meanwhile, we design an instance-level reward function based on the results of the RS model. Specifically, this reward function compares a data instance's current loss with its losses in previous searching epochs, for learning to distinguish the noisy data instances in different distributions. Moreover, we propose an alternate two-phase optimization strategy, i.e., the searching phase and the validation phase, to properly train and validate the policy network. After that, we train a randomly initialized RS model on the selected data subset from scratch to evaluate the effectiveness of the new data subset. The main contributions of this work can be summarized as follows: (i) We propose a DRL-based instance denoising framework, which adaptively filters out noises in each input mini-batch of data instances. To the best of our knowledge, this is a pioneering effort in instance denoising for CTR prediction; (ii) We design a novel two-phase training process to effectively train and validate the policy network, as well as generate a transferable noise-free data subset; (iii) We validate the effectiveness of AutoDenoise on three public benchmark datasets and prove the transferability of the denoised datasets.", "2 FRAMEWORK": "In this section, we will introduce the overview, the proposed two phases, and the optimization methods of AutoDenoise framework.", "2.1 Overview": "As illustrated in Figure 1, the whole framework consists of two running phases, i.e., the searching phase and the validation phase. Searching Phase. In this phase, DRL is adopted to automatically select instances with sequential optimization for the model and policy network. Specifically, the policy network serves as a AutoDenoise: Automatic Data Instance Denoising for Recommendations WWW'23, April 30-May 4, 2023, Austin, TX, USA Figure 2: The selection procedure of the policy network. Softmax Softmax 0.4 0.8 0.3 X Sample Selected Instances Actions Input mini-Batch unfixed or k M Embedding Softmax Policy Network 0.6 0.2 0.7 DRL agent, which performs 'select' and 'deselect' actions with corresponding probabilities, thereby removing noisy instances. Then, the selected noise-free instances are fed into an RS model, and the reward can be calculated by the previous and current losses of the RS model, which are used to optimize the policy network. Validation Phase. Unlike the searching phase, this phase aims to evaluate the performance of the policy network. In detail, after finishing the searching phase of each epoch, i.e., one iteration on the entire training set, the process switches to the validation phase. In the validation phase, the policy network predicts the entire training set in batches and selects the top\ud835\udc58 instances with the highest probability of 'select' action, which can be considered as noise-free data. Finally, an initialized RS model is trained with a noise-free subset until convergence, for which the final test performance is presented as the policy network denoising capability.", "2.2 Searching Phase": "In this section, we focus on introducing core elements of policy networks and DRL construction (e.g., environment, state, rewards, etc.). As an instance denoising task for applying DRL, we consider each mini-batch as the state of the input policy network and then execute the DRL actions ('select' or 'deselect' ) based on the outcome probabilities. DRL interacts with the environment by feeding selected instances into the RS model and calculates the corresponding rewards, aiming to maximize the reward of the policy network. Next, we will dive into the details of these components. Environment. RS serves as the environment in DRL, which receives a mini-batch of selected instances and outputs the predictions. To fairly evaluate and optimize the policy network, we initialize its parameters at the beginning of each training epoch to make the output predictions comparable with previous ones. State. Thestate is the input mini-batch, which consists of a bunch of data instances. Suppose that we have \ud835\udc40 instances for a mini-batch and \ud835\udc41 batches in total, the whole training set \ud835\udc7f = GLYPH<8> \ud835\udc7f \ud835\udc5b 1 , ..., \ud835\udc7f \ud835\udc5b \ud835\udc40 GLYPH<9> \ud835\udc41 \ud835\udc5b = 1 , where \ud835\udc7f \ud835\udc5b \ud835\udc5a represent the \ud835\udc5a \ud835\udc61\u210e instance of the \ud835\udc5b \ud835\udc61\u210e mini-batch in the training epoch. To compare the prediction results at the instance level for every training epoch, we fix their sequence order. In other words, all \ud835\udc7f \ud835\udc5b \ud835\udc5a have an identical position throughout the training process. Since the state is the input mini-batch, the \ud835\udc5b \ud835\udc61\u210e mini-batch can be expressed in the general form, i.e., \ud835\udc60 \ud835\udc5b = [ \ud835\udc7f \ud835\udc5b 1 \u00b7 \u00b7 \u00b7 \ud835\udc7f \ud835\udc5b \ud835\udc5a \u00b7 \u00b7 \u00b7 \ud835\udc7f \ud835\udc5b \ud835\udc40 ] . The function of the policy network is to score each instance in \ud835\udc60 \ud835\udc5b with probability and determine the 'select/deselect' action. Policy and Action. The policy and action can be considered as the 'searching' and 'deciding' behaviors in the instances of denoising, respectively. Therefore, we propose a policy network to search for the noise-free instances based on the state \ud835\udc60 \ud835\udc5b and execute the action \ud835\udc4e \ud835\udc5b \ud835\udc5a for each instance in the mini-batch \ud835\udc60 \ud835\udc5b . The whole selection process is illustrated in Figure 2. To be specific, given a batch of data \ud835\udc60 \ud835\udc5b = [ \ud835\udc7f \ud835\udc5b 1 \u00b7 \u00b7 \u00b7 \ud835\udc7f \ud835\udc5b \ud835\udc5a \u00b7 \u00b7 \u00b7 \ud835\udc7f \ud835\udc5b \ud835\udc40 ] , the policy network first transforms each instance \ud835\udc7f \ud835\udc5b \ud835\udc5a to a dense vector \ud835\udc6c \ud835\udc5b \ud835\udc5a through the embedding lookup operation,  where A consists of the learnable weight matrixes for all feature fields 1 . \ud835\udc6c \ud835\udc5b \ud835\udc5a denotes the concatenated feature embedding vector corresponding to \ud835\udc7f \ud835\udc5b \ud835\udc5a . Subsequently, \ud835\udc6c \ud835\udc5b \ud835\udc5a is fed to the Multilayer perceptron (MLP) with the nonlinear activation to obtain a finegrained feature representation [23]. Suppose that the MLP has { \ud835\udc3f + 1 } layers in total, the above operation of \ud835\udc59 \ud835\udc61\u210e ( \ud835\udc59 \u2208 [ 1 , \ud835\udc3f ] ) layer can be formulated as:  where \ud835\udc89 \ud835\udc59 is the output of the \ud835\udc59 \ud835\udc61\u210e layer. \ud835\udc7e \ud835\udc59 and \ud835\udc83 \ud835\udc59 are the learnable weight matrix and bias vector for the corresponding layer, respectively, and \ud835\udf0e (\u00b7) denotes the nonlinear activation function. In general, we use \ud835\udc45\ud835\udc52\ud835\udc3f\ud835\udc62 as the activation function for hidden layers. Then, we apply the \ud835\udc46\ud835\udc5c\ud835\udc53 \ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65 function to the output layer, so as to retrieve the probability of 'select' and 'deselect' actions for every instance. In this setting, the action \ud835\udc4e \ud835\udc5b \ud835\udc5a can be sampled from the output probabilities. To reminder, \ud835\udc89 0 = \ud835\udc6c \ud835\udc5b \ud835\udc5a represent the first fully-connected layer and the last layer \ud835\udc89 \ud835\udc3f is the output layer. Reward. To optimize the policy network in the desired way, i.e., learning to distinguish the noise instances, the reward design should be associated with the performance of the noise instance identification model. Following this idea, we define the reward according to the prediction error of the RS model, i.e., the value of loss \ud835\udc3f comparing the prediction results and the corresponding ground-truth labels. In order to avoid optimization conflicts among data instances, we consider each instance separately in the training search phase. Specifically, we create a matrix \ud835\udc73\ud835\udc74\ud835\udc82\ud835\udc95 \u2208 R \ud835\udc36 \u00d7 \ud835\udc40\ud835\udc41 to store the loss values of previous and current searching epochs for each instance, where \ud835\udc36 and \ud835\udc40\ud835\udc41 are the numbers of stored epochs and the number of loss values. Given the loss \ud835\udc3f \ud835\udc5a\ud835\udc5b ( \ud835\udc5a \ud835\udc61\u210e instance of 1 A feature field is a group of feature values belonging to the same category, e.g., the feature field 'gender' comprises two values, 'female' and 'male'. WWW'23, April 30-May 4, 2023, Austin, TX, USA Weilin Lin, et al. the \ud835\udc5b \ud835\udc61\u210e batch) for the current epoch, the reward \ud835\udc45 \ud835\udc5a\ud835\udc5b is defined as the difference between the corresponding instance's averaged loss values in the past \ud835\udc36 epochs and the loss in the current epoch:  where \ud835\udc45 \ud835\udc5a\ud835\udc5b > 0 means that the data instance's loss in the current epoch is smaller than its averaged loss values in the past \ud835\udc36 epochs, indicating that the policy network conducts better action on the instance in the current epoch, and vice versa. For fair comparison and improvement incentive, we initialize the parameters of the RS model at the beginning of each training epoch and overwrite the loss values of the earliest stored epoch with the latest one. Under this setting, the reward can be viewed as the prediction improvement. Since the parameters of the policy network are different, the reward can reflect the optimization results and be used to adjust them. Considering the action-sampling behaviors of policy network would introduce randomness, which cannot represent its actual selection ability, we choose to use the average value of \ud835\udc3f \ud835\udc50 \ud835\udc5a\ud835\udc5b in Equation (3) to make the current reward \ud835\udc45 \ud835\udc5a\ud835\udc5b more reliable.", "2.3 Validation Phase": "As mentioned in the training phase, the RS model is initialized in each training epoch and usually not converged. Therefore, we design a validation phase to fully evaluate the performance of the policy network and collect the best noise-free subset. To implement this, we split the validation phase into noise-free subset selection and RS model training. For subset selection, we iterate the entire training set and select the data instance with \ud835\udc58 highest confidence in each mini-batch, i.e., the top-k selection strategy. After obtaining the selected subset, we initialize the parameters of the RS model and train it until convergence, which is alignment with the typical recommendation system training setup. Finally, with the same test and validation sets as in the searching phase, we can evaluate the quality of the selected subsets and the denoising ability of the trained policy network 2 . Only the policy network and noise-free subset with the best testing performance can be adopted. Individual Selection & Top-k Selection . Compared with the individual selection used in the searching phase, we adopt top-k selection in the validation phase, i.e., we select the \ud835\udc58 prediction instances with the highest confidence by a trained policy network for each mini-batch. The motivation for this scheme is individual selection provides a 'local' view for policy network optimization, while the top-k can offer a 'global' view enabling the selection of a more representative noise-free subset. From a policy training perspective, a 'local' view at the instance level enables the policy network to learn the effectiveness of each instance and filter out noisy instances. Nevertheless, the 'local' view may not be the best choice for decision-making, because the contribution of weaklyselected instances may be minor compared to strongly-selected instances during model training, which may affect the model that uses the average of the loss values for optimization. In this sense, weakly-selected instances can also be considered as a kind of noise, requiring a 'global' constraint from the top-k selection strategy. 2 In practice, we set a total of 50 training epochs in the searching and validation phases to train and evaluate the policy network. Algorithm 1 \ud835\udc42\ud835\udc5d\ud835\udc61 _ \ud835\udc3c : Optimization Algorithm for Searching Phase Input : \ud835\udc41 mini-Batches of data \ud835\udc7f = GLYPH<8> \ud835\udc7f \ud835\udc5b 1 , ..., \ud835\udc7f \ud835\udc5b \ud835\udc40 GLYPH<9> \ud835\udc41 \ud835\udc5b = 1 , policy network \u0398 , initial RS model \ud835\udc7e , loss matrix \ud835\udc73\ud835\udc74\ud835\udc82\ud835\udc95 Output : trained policy network \u0398 \u2217 and full loss vector \ud835\udc73\ud835\udc7d\ud835\udc86\ud835\udc84 \u2217 1: Create empty \ud835\udc73\ud835\udc7d\ud835\udc86\ud835\udc84 2: Initialize \ud835\udc7e 3: for all \ud835\udc5b \u2208 [ 1 , \ud835\udc41 ] do 4: \ud835\udc60 \ud835\udc5b \u2190[ \ud835\udc7f \ud835\udc5b 1 \u00b7 \u00b7 \u00b7 \ud835\udc7f \ud835\udc5b \ud835\udc5a \u00b7 \u00b7 \u00b7 \ud835\udc7f \ud835\udc5b \ud835\udc40 ] 5: Sample \ud835\udc4e 1 \u223c \u0398 ( \ud835\udc4e | \ud835\udc60 \ud835\udc5b ) 6: Select instances \ud835\udc7f \ud835\udc60\ud835\udc59\ud835\udc50\ud835\udc61 1 7: Estimate L \ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59 ( \ud835\udc7e ) by Equation (4) 8: Estimate \ud835\udc45 \ud835\udc5b by Equation (3) with \ud835\udc73\ud835\udc74\ud835\udc82\ud835\udc95 9: Update \u0398 by Equation (8) 10: Sample \ud835\udc4e 2 \u223c \u0398 ( \ud835\udc4e | \ud835\udc60 ) 11: Select instances \ud835\udc7f \ud835\udc60\ud835\udc59\ud835\udc50\ud835\udc61 2 12: Estimate L \ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59 ( \ud835\udc7e ) by Equation (4) 13: Append L \ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59 ( \ud835\udc7e ) to \ud835\udc73\ud835\udc7d\ud835\udc86\ud835\udc84 14: Update \ud835\udc7e by Equation (5) 15: end for 16: \u0398 \u2217 \u2190 \u0398", "2.4 Optimization Method": "With the working procedures mentioned above, we will further detail the optimization methods for updating both the RS model and the policy network in this section. 2.4.1 Recommender System Model. Since RS models in the searching phase and validation phase are identical, we introduce model parameters and the optimization method together. We denote the parameters of the RS model as \ud835\udc7e . The CTR prediction could be regarded as a binary classification task under a supervised manner, where the input instances contain both features and the label. Thus, we apply binary-cross-entropy loss function L \ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59 to optimize \ud835\udc7e :  where \ud835\udc66 \ud835\udc5a and \u02c6 \ud835\udc66 \ud835\udc5a are the ground truth label and the probability predicted by the RS model for the \ud835\udc5a \ud835\udc61\u210e instance in a mini-batch, respectively. As we described in Section 2.2, the loss value can be treated as the performance of the input selected instances in the searching phase. By minimizing the Equation (4), we obtain well-trained \ud835\udc7e as follows, where \ud835\udefc is learning rate:  2.4.2 Policy Network. As it is a reinforcement learning framework, the optimal policy network can be parameterized by \u0398 . Given the reward function, i.e., Equation (3), the objective function required optimization is formulated as:  where \ud835\udc4e \u223c \u0398 ( \ud835\udc4e | \ud835\udc60 ) means an action \ud835\udc4e is sampled via the policy \u0398 ( \ud835\udc4e | \ud835\udc60 ) in state \ud835\udc60 . To maximize the above function, we apply a policy gradient algorithm, named REINFORCE [33], and adopt Monte-Carlo sampling to simplify the estimation of the gradient AutoDenoise: Automatic Data Instance Denoising for Recommendations WWW'23, April 30-May 4, 2023, Austin, TX, USA Algorithm 2 \ud835\udc42\ud835\udc5d\ud835\udc61 _ \ud835\udc3c \ud835\udc3c : Optimization Algorithm for Validation Phase Input : \ud835\udc41 mini-Batches of data \ud835\udc7f = GLYPH<8> \ud835\udc7f \ud835\udc5b 1 , ..., \ud835\udc7f \ud835\udc5b \ud835\udc40 GLYPH<9> \ud835\udc41 \ud835\udc5b = 1 , trained policy network \u0398 \u2217 , initial RS model \ud835\udc7e Output : Selected data subset \ud835\udc7f \ud835\udc60\ud835\udc62\ud835\udc4f , well-trained RS model \ud835\udc7e \u2217 1: Create empty \ud835\udc7f \ud835\udc60\ud835\udc62\ud835\udc4f 2: Initialize \ud835\udc7e 3: for all \ud835\udc5b \u2208 [ 1 , \ud835\udc41 ] do 4: \ud835\udc60 \ud835\udc5b \u2190[ \ud835\udc7f \ud835\udc5b 1 \u00b7 \u00b7 \u00b7 \ud835\udc7f \ud835\udc5b \ud835\udc5a \u00b7 \u00b7 \u00b7 \ud835\udc7f \ud835\udc5b \ud835\udc40 ] 5: Sample \ud835\udc4e 3 \u223c \u0398 \u2217 ( \ud835\udc4e | \ud835\udc60 \ud835\udc5b ) 6: Select instances \ud835\udc7f \ud835\udc60\ud835\udc59\ud835\udc50\ud835\udc61 3 7: Append \ud835\udc7f \ud835\udc60\ud835\udc59\ud835\udc50\ud835\udc61 3 to \ud835\udc7f \ud835\udc60\ud835\udc62\ud835\udc4f 8: end for 9: while RS model \ud835\udc7e not converged do 10: Sample a batch of data from \ud835\udc7f \ud835\udc60\ud835\udc62\ud835\udc4f 11: Update \ud835\udc7e by Equation (5) 12: end while \u2207 \u0398 \ud835\udc3d ( \u0398 ) , which can be expressed as:  where \ud835\udc47 is the sample number, and we set \ud835\udc47 = 1 here to improve the computational efficiency. The parameters \u0398 can be updated as:  where \ud835\udefd is the learning rate. 2.4.3 Optimization Algorithm for Searching Phase. Based on the optimization methods for the RS model and the policy network mentioned above, we can conduct the two-phase training to optimize and evaluate the proposed framework. The searching phase is illustrated in Algorithm 1. In this phase, we iterate the whole training set and alternatively optimize \u0398 and \ud835\udc7e to obtain a wellperformed policy. For a fair comparison of the reward, we initialize the RS model \ud835\udc7e at the beginning of this phase (line 2). During the training epoch, we first sample a mini-batch of data instances as the state \ud835\udc60 \ud835\udc5b for the policy network (line 4). Next, we sample actions \ud835\udc4e 1 for every data instance in the batch (line 5) and collect them as a selective batch \ud835\udc7f \ud835\udc60\ud835\udc59\ud835\udc50\ud835\udc61 1 (line 6), which will be fed into the RS model and gain the corresponding loss values (line 7). With this loss value and the saved ones from previous epochs 3 , i.e., in \ud835\udc73\ud835\udc74\ud835\udc82\ud835\udc95 , we can calculate the reward (line 8) and update our policy network (line 9). Similarly, the optimization of the RS model (line 10-14) follows a similar procedure as in line 5 to line 9. However, we sample new actions \ud835\udc4e 2 according to the updated \u0398 (line 10), and update \ud835\udc7e (line 14) rather than \u0398 as line 9. The updated policy network in the final mini-batch will be returned as a well-trained policy network (line 16). What's more, to record loss values for future calculation of 3 For the first \ud835\udc36 epochs in searching phase, where previous \ud835\udc36 losses are now available, we design a warm-up train method in Section 2.4.5. Algorithm 3 Overall Optimization Algorithm of AutoDenoise Input : \ud835\udc41 mini-Batches of data \ud835\udc7f = GLYPH<8> \ud835\udc7f \ud835\udc5b 1 , ..., \ud835\udc7f \ud835\udc5b \ud835\udc40 GLYPH<9> \ud835\udc41 \ud835\udc5b = 1 , Warm-up epoch \ud835\udc36 , Training epoch \ud835\udc47 , empty loss matrix \ud835\udc73\ud835\udc74\ud835\udc82\ud835\udc95 = [ \ud835\udc73\ud835\udc7d\ud835\udc86\ud835\udc84 1 , ..., \ud835\udc73\ud835\udc7d\ud835\udc86\ud835\udc84 \ud835\udc36 ] , initial RS model \ud835\udc7e and policy network \u0398 Output : well-trained policy network parameters \u0398 \u2217 and the selected data subset \ud835\udc7f \ud835\udc60\ud835\udc62\ud835\udc4f 1: for \ud835\udc56 = 1; \ud835\udc56 < = \ud835\udc36 ; \ud835\udc56 + + do 2: Initialize \ud835\udc7e 3: while RS model \ud835\udc7e not converged do 4: Sample a batch of data [ \ud835\udc7f \ud835\udc5b 1 \u00b7 \u00b7 \u00b7 \ud835\udc7f \ud835\udc5b \ud835\udc5a \u00b7 \u00b7 \u00b7 \ud835\udc7f \ud835\udc5b \ud835\udc40 ] 5: Estimate L \ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59 ( \ud835\udc7e ) by Equation (4) 6: Append L \ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59 ( \ud835\udc7e ) to \ud835\udc73\ud835\udc7d\ud835\udc86\ud835\udc84 \ud835\udc56 7: Update \ud835\udc7e by Equation (5) 8: end while 9: end for 10: for \ud835\udc61 = 1; \ud835\udc61 < = \ud835\udc47 ; \ud835\udc61 + + do 11: \u0398 \u2217 , \ud835\udc73\ud835\udc7d\ud835\udc86\ud835\udc84 \u2217 \u2190 \ud835\udc42\ud835\udc5d\ud835\udc61 _ \ud835\udc3c ( \ud835\udc7f , \u0398 , \ud835\udc7e , \ud835\udc73\ud835\udc74\ud835\udc82\ud835\udc95 ) 12: \ud835\udc73\ud835\udc7d\ud835\udc86\ud835\udc84 \ud835\udc61 % \ud835\udc36 \u2190 \ud835\udc73\ud835\udc7d\ud835\udc86\ud835\udc84 \u2217 13: \ud835\udc7f \ud835\udc60\ud835\udc62\ud835\udc4f , \ud835\udc7e \u2217 \u2190 \ud835\udc42\ud835\udc5d\ud835\udc61 _ \ud835\udc3c \ud835\udc3c ( \ud835\udc7f , \u0398 \u2217 , \ud835\udc7e ) 14: Test \ud835\udc7e \u2217 15: if \ud835\udc7e \u2217 perform the best then 16: Save \ud835\udc7f \ud835\udc60\ud835\udc62\ud835\udc4f and \u0398 \u2217 17: end if 18: end for reward baseline, we create an empty loss vector \ud835\udc73\ud835\udc7d\ud835\udc86\ud835\udc84 (line 1) and record the test loss value (line 13) during the iteration. Specifically, the policy network \u0398 from the previous optimization step cannot represent the optimized performance of the current batch of data. We utilize every mini-batch of data twice in this phase to sequentially optimize \u0398 and \ud835\udc7e (line 9 and 14), thus avoiding disturbing the returning loss and the optimizing environment. 2.4.4 Optimization Algorithm for Validation Phase. In Algorithm 2, we demonstrate the whole procedure of the validation phase, including the selection process (line 3-8) and the full-training process of the RS model (line 9-12). In this phase, we aim to evaluate the true performance of the trained policy network \u0398 \u2217 by training the RS model to converge on the selected subset. Specifically, we first create an empty set \ud835\udc7f \ud835\udc60\ud835\udc62\ud835\udc4f (line 1) and initialize \ud835\udc7e (line 2). Then, we iterate the whole training set for subset selection with \u0398 \u2217 . Similar to the procedure in the searching phase, every mini-batch will follow the sampling (line 5) and selection (line 6) processes. Then the obtained noise-free data batch \ud835\udc7f \ud835\udc60\ud835\udc59\ud835\udc50\ud835\udc61 3 is appended to \ud835\udc7f \ud835\udc60\ud835\udc62\ud835\udc4f (line 7). After that, we will train the initialized \ud835\udc7e to converge with \ud835\udc7f \ud835\udc60\ud835\udc62\ud835\udc4f (line 9-12). And we consider the performance of the well-trained \ud835\udc7e \u2217 as our validation result. 2.4.5 Overall Optimization Algorithm. With the optimization algorithms for the searching phase and validation phase, we will further detail the overall optimization algorithm in this subsection. The whole algorithm is depicted in Algorithm 3. Warm-up Train . To build the loss matrix \ud835\udc73\ud835\udc74\ud835\udc82\ud835\udc95 for reward calculation, we conduct a Warm-up Train before the searching phase. Specifically, only the RS model will be trained under the WWW'23, April 30-May 4, 2023, Austin, TX, USA Weilin Lin, et al. Table 1: Overall performance with different backbone recommendation models. ' * ' indicates the statistically significant improvements (i.e., two-sided t-test with \ud835\udc5d < 0 . 05) over the w/o version. Table 2: Overall performance comparison with baselines. (Backbone model: DeepFM) ' * ' indicates the statistically significant improvements over the best baseline. \u2191 : higher is better; \u2193 : lower is better. same settings as in the searching phase, i.e., being initialized at the beginning of every epoch and using the same loss function, denoted as L \ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59 . After iterating \ud835\udc36 epochs, we can obtain a { \ud835\udc36 \u00d7 \ud835\udc40\ud835\udc41 } matrix and start the searching phase with it. The whole optimizing procedure consists of a warm-up train (line 1-9) and two training phases within the testing process (line 10-18). In the warm-up train, we first initialize the parameters of RS model \ud835\udc7e and train it to converge for \ud835\udc36 epochs (line 1-9), where we sample a mini-batch (line 4) for model optimization (line 5) and save the corresponding loss values (line 6) for the reward calculation. After that, we follow Algorithm 1 (line 11) and Algorithm 2 (line 13) to train and evaluate the policy network. With the \ud835\udc7e \u2217 optimized by the selected subset, we can finally evaluate its performance (line 14) and save the optimal policy and the subset (line 15-17). The saved \ud835\udc7f \ud835\udc60\ud835\udc62\ud835\udc4f after the total \ud835\udc47 epochs is considered to be the noisefree subset. In the practical inference stage, we replace it with the original training set for model training. 3.1.2 Evaluation Metrics. To fairly evaluate recommendation performance, we select the AUC (Area Under the ROC Curve) scores and Logloss (logarithm of the loss value) as metrics, which are widely used for click-through prediction tasks [3, 9]. In practice, higher AUC scores or lower logloss values at the 0.001-level indicate a significant improvement [3].", "3 EXPERIMENT": "In this section, we conduct extensive experiments on three public datasets to investigate the effctiveness of AutoDenoise.", "3.1 Experimental Settings": "3.1.1 Datasets. We evaluate the model performance of AutoDenoise and baselines on three public datasets with different densities, MovieLens1M 4 (4.47%), KuaiRec-Small 5 (99.62%), and Netflix 6 (0.02%). For all datasets, we randomly select 80% as the training set, 10% as the validation set, and the remaining 10% as a test set. 4 https://grouplens.org/datasets/movielens/1m/ 5 https://github.com/chongminggao/KuaiRec 6 https://www.kaggle.com/datasets/netflix-inc/netflix-prize-data 3.1.3 Implementation Details. The implementation of AutoDenoise is based on a PyTorch-based public library 7 , which involves sixteen state-of-the-art RS models. We develop the policy network of AutoDenoise as an individual class so that it can easily incorporate with different backbone RS models, like in Table 2. Specifically, we implement it as a two-layer MLP with additional embedding layer and output layer. Every fully-connected layer in the MLP is stacked with a linear layer, a batch normalization operation, a \ud835\udc45\ud835\udc52\ud835\udc3f\ud835\udc48 activation function, and a dropout operation ( rate = 0.2 ). The output layer is implemented with a softmax activation. For the hyper-parameter, we set both the embedding size in the embedding layer and the dimension size of the two fully-connected layers as \ud835\udc51 = 16. The dimension of the output layer is set as 2, so as to generate two action probabilities for every instance.", "3.2 Overall Performance": "This section evaluates the effectiveness of AutoDenoise. As in Table 1, we compare the performances of recommendation models trained on a noise-free dataset denoised by AutoDenoise ( w ), against models trained on the original dataset with noise ( w/o ), and then evaluate them on the identical test dataset. We choose various advanced backbone recommendation models, including FM [24], Wide & Deep [3], DCN [28], and DeepFM [9]. In addition, we also compare the denoising quality of AutoDenoise with 7 https://github.com/rixwew/pytorch-fm AutoDenoise: Automatic Data Instance Denoising for Recommendations WWW'23, April 30-May 4, 2023, Austin, TX, USA Table 3: Transferability test. ' * ' indicates the statistically significant improvements over the best baseline. \u2191 : higher is better; \u2193 : lower is better. state-of-the-art instance selection methods (Drop3 [34], BDIS [2], LSBo [15], LSSm [15]) and instance denoising methods (T_CE [29], R_CE [29]) 8 in Table 2. We could conclude that: \u00b7 In light of the result in Table 1, integrating AutoDenoise can boost the performance for all backbone models on all datasets. Since KuaiRec-Small is far denser than MovieLens1M and Netflix (Data density: 99.62% \u226b 4.47% > 0.02%), the less remarkable improvement may come from the dominant contribution of clean instances, while noisy instances contribute less to user preference modeling, reasoning that the dense user-item interactions can accurately show the whole picture of a user. On the contrary, for sparse datasets MovieLens1M and Netflix, the noise-free subset from AutoDenoise can improve more in performance. \u00b7 In Table 2, all denoising methods fail to enhance the performance of DeepFM, which may be attributed to two reasons. (i) For instance selection methods (Drop3 [34], BDIS [2], LSBo [15], LSSm [15]), they tend to compress the dataset, resulting in discarding excessive samples and missing critical information. (ii) For denoising models like T-CE and R-CE [29], they improve the AUC score on a highly sparse dataset (Netflix, 0.02%), yet are unable to provide benefit on a dense dataset. The reason is that T-CE and R-CE highly depend on the negative sampling strategy, which samples unobserved user-item interactions. However, for the highly-dense dataset as KuaiRec-Small (99.62%), it would be hard to sample effective unobserved interactions. On the contrary, with the excellent exploring ability of the DRL structure, AutoDenoise is compatible with all datasets of diverse densities.", "3.3 Transferability Test": "In practice, recommender system models are updated frequently, causing it time-consuming and costly to train a denoising framework for each model. Therefore, we examine the transferability of AutoDenoise in this section. Specifically, based on DeepFM, after obtaining the noise-free dataset selected by AutoDenoise, we directly use it to train other backbone recommendation models (e.g., FM [24], Wide & Deep [3], and DCN [28]) and investigate their performances. Results are presented in Table 3, where 'Normal' means training on the original data with noise, i.e., the ' w/o ' in Table 1. We can observe that all backbone models' performances are enhanced by training on the 8 These two baselines could be applied to general denoising tasks. We do not test the performance of other denoising models since the different task settings as discussed in Section 3.1. Figure 3: Ablation study on MovieLens1M. FM Wide&Deep DCN IPNN DeepFM 0 . 79 0 . 80 0 . 81 AUC Normal AD-v AD noise-free dataset selected by AutoDenoise. These experiments validate that AutoDenoise's output has a strong transferability and the potential to be utilized in commercial recommender systems.", "3.4 Ablation Study": "We investigate the core components of AutoDenoise through a range of experiments in this section. As we discussed in Section 2.3, a vital part of AutoDenoise is how to generate denoising results in the validation stage, i.e., adopting individual selection or top\ud835\udc58 selection. Therefore, we design a variant of AutoDenoise, named AutoDenoise-v , to figure out the impact of different selection strategies. The only difference compared to origin AutoDenoise is that AutoDenoise-v applies individual selection in policy validation while AutoDenoise adopts top-k selection. We test their AUC scores on MovieLens1M, and the results are illustrated in Figure 3. It can be observed that AutoDenoise (AD) outperforms AutoDenoise-v (AD-v) on all models, and AutoDenoisev cannot generate noise-free datasets to improve IPNN performance. The reason is that AutoDenoise-v focuses on individual scores while neglecting the global situation. Besides, AutoDenoise could capture batch-wise global information to generate a robust policy by adopting a top-k selection strategy, which compares scores between the same batch of data. This experiment validates the rationality of our model design.", "3.5 Parameter Analysis": "In this section, we conduct experiments on MovieLens1M with DeepFM to investigate the sensitivity of two critical hyperparameters: (i) the number of warm-up training epochs \ud835\udc36 ; (ii) the number of selected samples \ud835\udc3e . The results are visualized in Figure 4, where two Y-axes are used to scale the AUC scores and logloss values, respectively. The X-axis in Figure 4 (a) represents the number of WWW'23, April 30-May 4, 2023, Austin, TX, USA Weilin Lin, et al. Figure 4: Parameter analysis on MovieLens1M. 2 3 4 5 6 C 0 . 802 0 . 804 0 . 806 0 . 808 0 . 810 AUC (a) Warm-up epochs 0 . 52 0 . 54 0 . 56 0 . 58 0 . 60 Logloss 90 93 94 95 96 97 98 99 /epsilon1 0 . 802 0 . 804 0 . 806 0 . 808 AUC (b) Selection ratio 0 . 52 0 . 54 0 . 56 0 . 58 Logloss warm-up epochs \ud835\udc36 , and X-axis in Figure 4 (b) is the selected ratio \ud835\udf16 = \ud835\udc3e / \ud835\udc40 . For both figures, blue lines illustrate the AUC score, and red lines are the Logloss value. In addition, we also mark the 'Normal' DeepFM performance as a dashed line for visual comparison (AUC 0 . 8048, Logloss 0 . 5306). We can find that: \u00b7 With the changing hyperparameters, AutoDenoise is superior to the Normal in most cases (2 \u2264 \ud835\udc36 \u2264 6, 95% \u2264 \ud835\udf16 \u2264 99%), which can fully demonstrate its robustness. We achieve the best performance with the default setting (i.e., \ud835\udc36 \u2217 = 4 , \ud835\udf16 \u2217 = 98%). \u00b7 From Figure 4 (a), the parameter \ud835\udc36 directly influence the reward computation in Equation (3). For \ud835\udc36 < \ud835\udc36 \u2217 , AutoDenoise suffers from the randomness of too few warm-up epochs, i.e., the first term in Equation (3), resulting in performance degradation. However, a large warm-up epoch leads to a smooth average value, and the loss value of every batch, i.e., the last term in Equation (3), have a more significant impact on the training process, leading to an unstable reward and impairing the denoising accuracy. \u00b7 As illustrated in Figure 4 (b), it is crucial to make appropriate discards of instances. On one hand, dropping too many instances results in inferior performance with missing valuable information. On the other hand, if we keep excessive instances, the model inevitably introduces noises and fails to get the best performance.", "3.6 Case Study": "Weillustrate the effectiveness of AutoDenoise with examples in this section. Specifically, we select a user (ID: 5051) from MovieLens1M and visualize part of the training set in Table 4, all test data instances in Table 5. We present titles and genres of movies for the training set, together with the ground-truth click signals. For test data, we additionally exhibit the prediction result of DeepFM [9] trained on original noisy data (Normal) and noise-free data (AutoDenoise). In Table 4, AutoDenoise detects the noise The Wizard of Oz with a positive click signal. This movie is a children's and musical movie, while all the other movies the users prefer are mainly action, crime, and war movies. By dropping this noise, AutoDenoise contributes to correcting the prediction of DeepFM on Wonderland (from 0.46 to 0.57, in Table 5), which is a documentary movie about war that the user usually enjoy. This case study also demonstrates the potential interpretability of AutoDenoise.", "4 RELATED WORK": "In this section, we will briefly introduce the related works to our framework, i.e., instance selection and denoising. Instance Selection. Instance selection plays an important role in selecting the most predictive data instances to scale down the training set without performance degradation of predictive model [20]. Table 4: Training dataset for user 5051. Table 5: Test dataset for user 5051. It can be divided into two groups, i.e., wrapper methods [1, 4, 10, 34], and filter methods [21, 22, 26], whose selection criterion is based on the model performance and a selection function, respectively. CNN [10] is one of the earliest proposed methods in the field of instance selection. It keeps the instances near the class border and drops the internal ones to generate an effective data subset. DROP [34], a series of representative wrapper methods, relies on the instance associates, i.e., the k nearest neighbors on k-NN, to conduct the selection manner. LSSm and LSBo [15] are methods based on local set [1], i.e., the largest hypersphere that contains cases in the same class. LSSm defines and considers usefulness and harmfulness as measures to decide whether to remove an instance, while LSBo relies on the class borders to make the decision. In contrast to previous instance selection methods, AutoDenoise uses a policy network to automatically distinguish noisy instances rather than a fixed selection rule, which can better discover the inner relationship in a highly dynamic data distribution. Denoising. The selection manner also works well in the fields of denoising, which can be categorized into selection-based method [6, 7, 32, 35], along with the reweighting-based methods [29, 30]. The WBPR [7] assigns higher selecting probabilities to the missing interactions of the popular items since it considers them to be the real negative instances. IR [32] finds out the negative interactions and changes their label to revise the unreliable behaviors from users. R-CE and T-CE [29] consider that noisy instances would have high loss value, thus they dynamically assign lower weight to the highloss instances as well as truncate the ones with weight lower than the threshold in two strategies, respectively. However, the above models either suffer from selection bias or insufficient transferability, which are solved by AutoDenoise with its DRL framework.", "5 CONCLUSION": "We propose a DRL-based instance denoising method, AutoDenoise, to improve the recommendation performance of RS models by adaptively selecting the noise-free instances in every mini-batch of data and training the RS model with the selected noise-free subset. We design a two-phase optimization strategy to properly train and evaluate the proposed framework. The extensive experiments validate the effectiveness and compatibility of AutoDenoise and its selected noise-free data subset. Further experiments prove the transferability of the noise-free subset, i.e., the noise-free subset selected with one RS model can transfer well to other state-of-the-art backbone RS models with significant performance improvement. AutoDenoise: Automatic Data Instance Denoising for Recommendations WWW'23, April 30-May 4, 2023, Austin, TX, USA", "ACKNOWLEDGEMENTS": "This research was partially supported by APRC - CityU New Research Initiatives (No.9610565, Start-up Grant for New Faculty of City University of Hong Kong), SIRG - CityU Strategic Interdisciplinary Research Grant (No.7020046, No.7020074), HKIDS Early Career Research Grant (No.9360163), Huawei Innovation Research Program and Ant Group (CCF-Ant Research Fund).", "REFERENCES": "[1] Henry Brighton and Chris Mellish. 2002. Advances in instance selection for instance-based learning algorithms. Data mining and knowledge discovery 6, 2 (2002), 153-172. [2] Qingqiang Chen, Fuyuan Cao, Ying Xing, and Jiye Liang. 2022. Instance Selection: A Bayesian Decision Theory Perspective. Proceedings of the AAAI Conference on Artificial Intelligence 36, 6, 6287-6294. https://doi.org/10.1609/aaai.v36i6.20578 [3] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems . 7-10. [4] Chien-Hsing Chou, Bo-Han Kuo, and Fu Chang. 2006. The generalized condensed nearest neighbor rule as a data reduction method. In 18th international conference on pattern recognition (ICPR'06) , Vol. 2. IEEE, 556-559. [5] James Davidson, Benjamin Liebald, Junning Liu, Palash Nandy, Taylor Van Vleet, Ullas Gargi, Sujoy Gupta, Yu He, Mike Lambert, Blake Livingston, et al. 2010. The YouTube video recommendation system. In Proceedings of the fourth ACM conference on Recommender systems . 293-296. [6] Jingtao Ding, Guanghui Yu, Xiangnan He, Fuli Feng, Yong Li, and Depeng Jin. 2019. Sampler design for bayesian personalized ranking by leveraging view data. IEEE transactions on knowledge and data engineering 33, 2 (2019), 667-681. [7] Zeno Gantner, Lucas Drumond, Christoph Freudenthaler, and Lars SchmidtThieme. 2012. Personalized ranking for non-uniformly sampled items. In Proceedings of KDD Cup 2011 . PMLR, 231-247. [8] Zhabiz Gharibshah, Xingquan Zhu, Arthur Hainline, and Michael Conway. 2020. Deep learning for user interest and response prediction in online display advertising. Data Science and Engineering 5, 1 (2020), 12-26. [9] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. In Proc. of IJCAI . [10] Peter Hart. 1968. The condensed nearest neighbor rule (corresp.). IEEE transactions on information theory 14, 3 (1968), 515-516. [11] Kaixi Hu, Lin Li, Qing Xie, Jianquan Liu, and Xiaohui Tao. 2021. What is Next when Sequential Prediction Meets Implicitly Hard Interaction?. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management . 710-719. [12] Yifan Hu, Yehuda Koren, and Chris Volinsky. 2008. Collaborative filtering for implicit feedback datasets. In 2008 Eighth IEEE international conference on data mining . Ieee, 263-272. [13] PN Vijaya Kumar and V Raghunatha Reddy. 2014. A survey on recommender systems (RSS) and its applications. International Journal of Innovative Research in Computer and Communication Engineering 2, 8 (2014), 5254-5260. [14] Dongha Lee, SeongKu Kang, Hyunjun Ju, Chanyoung Park, and Hwanjo Yu. 2021. Bootstrapping user and item representations for one-class collaborative filtering. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval . 317-326. [15] Enrique Leyva, Antonio Gonz\u00e1lez, and Ra\u00fal P\u00e9rez. 2015. Three new instance selection methods based on local sets: A comparative study with several approaches from a bi-objective perspective. Pattern Recognition 48, 4 (2015), 1523-1537. [16] Tzu-Heng Lin, Chen Gao, and Yong Li. 2019. Cross: Cross-platform recommendation for social e-commerce. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval . 515-524. [17] Weilin Lin, Xiangyu Zhao, Yejing Wang, Tong Xu, and Xian Wu. 2022. AdaFS: Adaptive Feature Selection in Deep Recommender System. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 3309-3317. [18] Haochen Liu, Xiangyu Zhao, Chong Wang, Xiaobing Liu, and Jiliang Tang. 2020. Automated embedding size search in deep recommender systems. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . 2307-2316. [19] Kunpeng Liu, Yanjie Fu, Pengfei Wang, Le Wu, Rui Bo, and Xiaolin Li. 2019. Automating feature subspace exploration via multi-agent reinforcement learning. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 207-215. [20] J Arturo Olvera-L\u00f3pez, J Ariel Carrasco-Ochoa, J Mart\u00ednez-Trinidad, and Josef Kittler. 2010. A review of instance selection methods. Artificial Intelligence Review 34, 2 (2010), 133-143. [21] J Arturo Olvera-L\u00f3pez, J Ariel Carrasco-Ochoa, and J Fco Mart\u00ednez-Trinidad. 2008. Prototype selection via prototype relevance. In Iberoamerican congress on pattern recognition . Springer, 153-160. [22] Thanapant Raicharoen and Chidchanok Lursinsap. 2005. A divide-and-conquer approach to the pairwise opposite class-nearest neighbor (POC-NN) algorithm. Pattern recognition letters 26, 10 (2005), 1554-1567. [23] Hassan Ramchoun, Youssef Ghanou, Mohamed Ettaouil, and Mohammed Amine Janati Idrissi. 2016. Multilayer perceptron: Architecture optimization and training. (2016). [24] Steffen Rendle. 2010. Factorization machines. In 2010 IEEE International conference on data mining . IEEE, 995-1000. [25] Francesco Ricci, Lior Rokach, and Bracha Shapira. 2011. Introduction to recommender systems handbook. In Recommender systems handbook . Springer, 1-35. [26] Jos\u00e9 C Riquelme, Jes\u00fas S Aguilar-Ruiz, and Miguel Toro. 2003. Finding representative patterns with ordered projections. pattern recognition 36, 4 (2003), 1009-1018. [27] Jian Wang and Yi Zhang. 2013. Opportunity model for e-commerce recommendation: right product; right time. In Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval . 303-312. [28] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network for ad click predictions. In Proceedings of the ADKDD'17 . 1-7. [29] Wenjie Wang, Fuli Feng, Xiangnan He, Liqiang Nie, and Tat-Seng Chua. 2021. Denoising implicit feedback for recommendation. In Proceedings of the 14th ACM international conference on web search and data mining . 373-381. [30] Yu Wang, Xin Xin, Zaiqiao Meng, Joemon M Jose, Fuli Feng, and Xiangnan He. 2022. Learning Robust Recommenders through Cross-Model Agreement. In Proceedings of the ACM Web Conference 2022 . 2015-2025. [31] Yejing Wang, Xiangyu Zhao, Tong Xu, and Xian Wu. 2022. Autofield: Automating feature selection in deep recommender systems. In Proceedings of the ACM Web Conference 2022 . 1977-1986. [32] Zitai Wang, Qianqian Xu, Zhiyong Yang, Xiaochun Cao, and Qingming Huang. 2021. Implicit Feedbacks are Not Always Favorable: Iterative Relabeled One-Class Collaborative Filtering against Noisy Interactions. In Proceedings of the 29th ACM International Conference on Multimedia . 3070-3078. [33] Ronald J Williams. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning 8, 3 (1992), 229-256. [34] D Randall Wilson and Tony R Martinez. 2000. Reduction techniques for instancebased learning algorithms. Machine learning 38, 3 (2000), 257-286. [35] Wenhui Yu and Zheng Qin. 2020. Sampler design for implicit feedback data by noisy-label robust learning. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . 861-870. [36] Fajie Yuan, Xin Xin, Xiangnan He, Guibing Guo, Weinan Zhang, Chua Tat-Seng, and Joemon M Jose. 2018. fBGD: Learning embeddings from positive unlabeled data with BGD. (2018). [37] Chi Zhang, Yantong Du, Xiangyu Zhao, Qilong Han, Rui Chen, and Li Li. 2022. Hierarchical Item Inconsistency Signal Learning for Sequence Denoising in Sequential Recommendation. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management . 2508-2518. [38] Xiangyu Zhao. 2022. Adaptive and automated deep recommender systems. ACM SIGWEB Newsletter Spring (2022), 1-4. [39] Xiangyu Zhao, Changsheng Gu, Haoshenglun Zhang, Xiwang Yang, Xiaobing Liu, Hui Liu, and Jiliang Tang. 2021. DEAR: Deep Reinforcement Learning for Online Advertising Impression in Recommender Systems. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 35. 750-758. [40] Xiangyu Zhao, Haochen Liu, Wenqi Fan, Hui Liu, Jiliang Tang, and Chong Wang. 2021. AutoLoss: Automated Loss Function Search in Recommendations. In Proceedings of the 27th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 3959-3967. [41] Xiangyu Zhao, Long Xia, Jiliang Tang, and Dawei Yin. 2019. Deep reinforcement learning for search, recommendation, and online advertising: a survey. ACM SIGWEB Newsletter Spring (2019), 1-15. [42] Xiangyu Zhao, Long Xia, Liang Zhang, Zhuoye Ding, Dawei Yin, and Jiliang Tang. 2018. Deep Reinforcement Learning for Page-wise Recommendations. In Proceedings of the 12th ACM Recommender Systems Conference . ACM, 95-103. [43] Xiangyu Zhao, Liang Zhang, Zhuoye Ding, Long Xia, Jiliang Tang, and Dawei Yin. 2018. Recommendations with Negative Feedback via Pairwise Deep Reinforcement Learning. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . ACM, 1040-1048. [44] Xiangyu Zhao, Liang Zhang, Zhuoye Ding, Dawei Yin, Yihong Zhao, and Jiliang Tang. 2017. Deep Reinforcement Learning for List-wise Recommendations. arXiv preprint arXiv:1801.00209 (2017). [45] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through rate prediction. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1059-1068."}
