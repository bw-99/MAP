{"Data-efficient Fine-tuning for LLM-based Recommendation": "Xinyu Lin xylin1028@gmail.com National University of Singapore Wenjie Wang \u2217 wenjiewang96@gmail.com Singapore National University of Singapore Singapore Yongqi Li liyongqi0@gmail.com The Hong Kong Polytechnic University Hong Kong SAR, China", "Shuo Yang": "syang98@hku.hk The University of Hong Kong Hong Kong SAR, China", "Fuli Feng \u2217": "fulifeng93@gmail.com University of Science and Technology of China Hefei, China Yinwei Wei weiyinwei@hotmail.com Monash University Melbourne, Australia", "Tat-Seng Chua": "dcscts@nus.edu.sg National University of Singapore Singapore", "ABSTRACT": "Leveraging Large Language Models (LLMs) for recommendation has recently garnered considerable attention, where fine-tuning plays a key role in LLMs' adaptation. However, the cost of fine-tuning LLMs on rapidly expanding recommendation data limits their practical application. To address this challenge, few-shot fine-tuning offers a promising approach to quickly adapt LLMs to new recommendation data. We propose the task of data pruning for efficient LLMbased recommendation, aimed at identifying representative samples tailored for LLMs' few-shot fine-tuning. While coreset selection is closely related to the proposed task, existing coreset selection methods often rely on suboptimal heuristic metrics or entail costly optimization on large-scale recommendation data. To tackle these issues, we introduce two primary objectives for the data pruning task in the context of LLM-based recommendation: 1) high accuracy aims to identify the influential samples that can lead to high overall performance; and 2) high efficiency underlines the low costs of the data pruning process. To pursue the two objectives, we propose a novel data pruning method incorporating two scores, namely influence score and effort score, to efficiently identify the influential samples. Particularly, the influence score is introduced to accurately estimate the influence of removing each sample on the overall performance. To achieve low costs of the data pruning process, we employ a small-sized surrogate model to replace LLMs to obtain the influence score. Considering \u2217 Corresponding author. This work is supported by the CCCD Key Lab of Ministry of Culture and Tourism. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '24, July 14-18, 2024, Washington, DC, USA \u00a9 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0431-4/24/07 https://doi.org/10.1145/3626772.3657807 the potential gap between the surrogate model and LLMs, we further propose an effort score to prioritize some hard samples specifically for LLMs. We instantiate the proposed method on two competitive LLM-based recommender models, and empirical results on three real-world datasets validate the effectiveness of our proposed method. In particular, our method uses only 2% samples to surpass the full data fine-tuning, reducing time costs by 97%.", "CCS CONCEPTS": "\u00b7 Information systems \u2192 Recommender systems .", "KEYWORDS": "Data Pruning, LLM-based Recommendation, Efficient Fine-tuning", "ACMReference Format:": "Xinyu Lin, Wenjie Wang \u2217 , Yongqi Li, Shuo Yang, Fuli Feng, Yinwei Wei, and Tat-Seng Chua. 2024. Data-efficient Fine-tuning for LLM-based Recommendation. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '24), July 14-18, 2024, Washington, DC, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3626772.3657807", "1 INTRODUCTION": "Leveraging Large Language Models (LLMs) for recommendation has demonstrated promising efficacy across various tasks, including Click-Through Rate (CTR) prediction [4], sequential recommendation [35], and explainable recommendation [11]. To build LLM-based recommender models, it is crucial to fine-tune LLMs on recommendation data for two primary reasons: 1) there exists a significant gap between previous LLMs' tuning tasks and the recommendation tasks [4], and 2) the rapid and continuous update of recommendation data necessitates frequent fine-tuning of LLMs [38]. For example, there are approximately 160 million new videos and 942 billion interactions emerging on TikTok per day 1 . Thus, frequent fine-tuning is imperative to incorporate up-todate item information and enhance user behavior comprehension. 1 https://www.tiktok.com/transparency/. SIGIR '24, July 14-18, 2024, Washington, DC, USA Conference'17, July 2017, Washington, DC, USA Xinyu Lin et al. Anon. 0.031 0.023 0.015 Figure 1: image To overcome the above issues, we summarize two principal objecFigure 1: (a) reveals that BIGRec achieves remarkable performance with only hundreds of samples. (b) shows the low costs of surrogate models. 0.0000 0.0025 0.0050 0.0075 0 128 256 512 1024 2048 BIGRec SASRec (full data) (a) Few-shot Performance. (Recall@10) (a) Few-shot performance on MicroLens-50K. 0.013 0.018 0.023 0.1 0.3 0.5 1 2 Games Recall@10 Recall@20 (a) Effect of ! w.r.t. Recall (Recall@10) (Recall@20) Figure 2: The comparison of training costs between an LLM (BIGRec) and a surrogate model (SASRec) 3 . (b)  The  comparison  of  training  costs between  an  LLM  (BIGRec)  and  a surrogate model (SASRec). The statistics  are  based  on  NVIDIA  RTX A5000 on Games. based recommender models to real-world platforms. \u00b7 We propose a novel data pruning method to discover the influential samples for LLM-based recommendation, which effectively and efficiently assesses the influence of removing a sample on empirical risk. \u00b7 We conduct extensive experiments on three real-world datasets, demonstrating the effectiveness of DEALRec in achieving both high efficiency and accuracy. \u00b7 To achieve high efficiency, one possible solution is to train a surrogate model for sample selection, e.g., using a small-sized traditional recommender model, which can drastically reduce the GPU memory usage and the training time compared to LLMs (see Figure 1(b)). However, there exists a gap between LLMs and surrogate models, attributable to their divergent capabilities in learning user behaviors (refer to Figure 3). As such, influential samples selected by surrogate models might deviate from the ones on LLMs, potentially hurting the adaptation of LLMs. tives for data pruning in the context of LLM-based recommendation: 1) high accuracy, which focuses on selecting the samples that can lead to low empirical risk; and 2) high efficiency, which emphasizes the low costs of the data pruning process, i.e., eliminating the dependency of well-trained LLMs on the full data. Nevertheless, However, fine-tuning LLMs on large-scale recommendation data demands substantial computational resources and time costs [26], thereby diminishing the practicality of LLM-based recommender models in real-world applications. As such, it is essential to enhance the fine-tuning efficiency of LLM-based recommender models. /u1D466.alt /u1D461 denotes the /u1D461 -th token of /u1D466.alt , and /u1D466.alt < /u1D461 sequence preceding /u1D466.alt . In summary, this work offers three major contributions: In summary, this work offers three major contributions: \u00b7 We introduce a data pruning task to identify the influential samples tailored for efficient LLM-based recommender finetuning, unlocking the remarkable potential of applying LLM-\u00b7 To achieve high accuracy, it is essential to measure the influence of removing each training sample on the empirical risk. However, assessing the influence of all samples is costly, as it requires the leaving-one-out retraining for each sample [43]. 2 TASK FORMULATION In this section, we first introduce LLM-based recommender models and uncover the challenge of real-world applicability. Thereafter, we formulate the task of data pruning for LLM-based recommendation and compare the related work on coreset selection. \u00b7 LLM-based recommender models. To leverage the competent capabilities of LLMs, LLM-based recommendation typically utilize powerful LLMs directly as the recommender models. Since LLMs are not particularly trained on the recommendation data, fine-tuning is the necessary and key step for LLMs to learn the item knowledge and understand user behavior. Let U and I denote the sets of users and items, respectively. We present each training sample, i.e., user sequence, as /u1D460 = ( /u1D465 , /u1D466.alt ) , where /u1D465 = [ /u1D456 1 , /u1D456 2 , . . . , /u1D456 | /u1D465 | ] is the user's historical interactions in chronological order, and /u1D466.alt is the next interacted item of the user 4 , where { /u1D456 1 , . . . , /u1D456 | /u1D465 | , /u1D466.alt } \u2282 I . Formally, given the user sequences of the training set D = { /u1D460 /u1D462 | /u1D462 \u2208 U} , the target is to fine-tune an LLM for recommendation tasks. The learnable parameters ( /u1D719 \u2208 \u03a6 ) of an LLM is optimized by minimizing the negative log-likelihood of the next interacted item /u1D466.alt conditioned on input /u1D465 : min /u1D719 \u2208 \u03a6 {L /u1D43F/u1D43F/u1D440 /u1D719 = -| /u1D466.alt | \u2211 /u1D461 = 1 log /u1D443 /u1D719 ( /u1D466.alt /u1D461 | /u1D466.alt < /u1D461 , /u1D465 )} , (1) where represents the token To address the challenges, we propose a novel D ata pruning method, to E fficiently identify the influenti A l samples for L LMbased Rec ommender fine-tuning (shorted as DEALRec). DEALRec leverages two scores, namely influence score and effort score, to identify the influential samples. The influence score is formulated to estimate the influence of removing each training sample on the empirical risk. It is calculated by extending the influence function [15] via chain rules and second-order optimization techniques [24]. To efficiently calculate the influence score for all samples, DEALRec employs a simple yet effective symmetric property to accelerate the calculation, requiring only the estimation once for all samples ( cf. Section 3.1). Thereafter, DEALRec uses a traditional recommender model as a surrogate model to obtain the influence score and introduces the effort score to mitigate the gap between the surrogate model and LLMs. The effort score is obtained by calculating the gradient norm of a sample loss w.r.t. the parameters of LLMs, intuitively measuring the effort of LLMs to fit a specific sample. By regularizing the influence score with the effort scor , DEALRec identifies the influ ntial samples that encompass both the representativeness of th full data and the significance to LLMs. We instantiate DEALRec on two LLM-based recommender models and conduct extensive experiments on three real-world datasets, validating the superiority of DEALRec in terms of both efficiency and accuracy. The code and datasets are available at https://github.com/Linxyhaha/DEALRec. /u1D461 While fine-tuning LLMs has demonstrated effectiveness in recommendation tasks [30], its practical application is hindered by the high resource costs required by LLMs and the continuous influx of new recommendation data [35]. Hence, it is essential to enhance \u00b7 We introduce a data pruning task to identify the influential samples tailored for efficient LLM-based recommender finetuning, unlocking the remarkable potential of applying LLMbased recommender models to real-world platforms. approach is to reduce the costs by few-shot fine-tuning with randomly selected samples [4]. Nevertheless, the random samples might lose some crucial information for LLMs to acquire the latest information on user behavior or items, e.g., trending items. In this \u00b7 We conduct extensive experiments on three real-world datasets, demonstrating the effectiveness of DEALRec in achieving both high efficiency and accuracy. samples selected by surrogate models might deviate from the ones on LLMs, potentially hurting the adaptation of LLMs. To address the challenges, we propose a novel D ata pruning method, to E fficiently identify the influenti A l samples for L LMbased Rec ommender fine-tuning (shorted as DEALRec). DEALRec leverages two scores, namely influence score and effort score, to identify the influential samples. The influence score is formulated to estimate the influence of removing each training sample on the empirical risk. It is calculated by extending the influence function [16] via chain rules and second-order optimization techniques [25]. To efficiently calculate the influence score for all samples, DEALRec employs a simple yet effective symmetric property to accelerate the calculation, requiring only the estimation once for all samples ( cf. Section 3.1). Thereafter, DEALRec uses a traditional recommender model as a surrogate model to obtain the influence score and introduces the effort score to mitigate the gap between the surrogate model and LLMs. The effort score is obtained by calculating the gradient norm of a sample loss w.r.t. BIGRec achieves remarkable performance with only hundreds of samples on Games A closely related literature to this data pruning task is coreset selection [13]. It tries to select a small but representative subset from the full data, aiming to achieve comparabl performance. Existing coreset selection me hods generally fall into two categories 2 : 1) Heuristic methods select hard or diverse samples based on predefined metrics [30, 34, 49]. Such heuristic methods do not estimate the impact of selected samples on empirical risk, possibly leading to suboptimal coreset selection. 2) Optimization-based methods mainly optimize the selection of subsets to minimize the empirical risk [5, 50]. However, these methods are inapplicable to large-scale recommendation datasets due to the complex and costly bi-level or discrete optimization problem [17]. Worse still, both heuristic and optimization-based methods rely on the model well-trained by the full data to select the coreset, e.g., calculating pre-defined scores or optimizing the data subset based on the well-trained model ( cf. Section 2). As such, it is infeasible to directly apply these methods for LLM-based recommendation because of the high training costs of LLMs on the large-scale full recommendation data. the efficiency of LLM-based recommender fine-tuning. \u00b7 Data pruning for efficient LLM-based recommendation. To achieve efficient LLM-based recommendation, a promising \u00b7 We propose a novel data pruning method to discover the influential samples for LLM-based recommendation, which effectively and efficiently assesses the influence of removing a sample on empirical risk.", "light, we introduce the task of data pruning for efficient LLM-based 2 TASK FORMULATION": "of both efficiency and accuracy. The code and datasets are available 2 More detailed related work is discussed and compared in Section 4 and 5. pursuing the two objectives faces two challenges: \u00b7 To achieve high accuracy, it is essential to measure the influence of removing each training sample on the empirical risk. However, assessing the influence of all samples is costly, as it requires the leaving-one-out retraining for each sample [39]. \u00b7 To achieve high efficiency, one possible solution is to train a surrogate model for sample selection, e.g., using a small-sized traditional recommender model, which can drastically reduce the GPU memory usage and the training time compared to LLMs (see Table ?? ). However, there exists a gap between LLMs and surrogate models, attributable to their divergent capabilities in learning user behaviors (refer to Figure 4). As such, influential Fortunately, the rich world knowledge encoded in LLMs offers a promising solution for efficient fine-tuning: few-shot fine-tuning . Previous studies have uncovered that LLMs have the potential to quickly adapt to recommendation tasks by fine-tuning on randomly sampled few-shot data [3, 4, 27] (Figure 1(a)), significantly reducing training time and computational costs. Despite its efficiency, randomly sampled data may lack sufficient representativeness to enable LLMs to effectively comprehend new items and user behaviors. To combat this issue, we introduce the task of data pruning for efficient LLM-based recommendation , which aims to identify representative samples tailored for LLMs' few-shot finetuning. the parameters of LLMs, intuitively measuring the effort of LLMs to fit a specific sample. By regularizing the influence score with the effort score, DEALRec identifies the influential samples that encompass both the representativeness of the full data and the significance to LLMs. We instantiate DEALRec on two LLM-based recommender models and conduct extensive experiments on three real-world datasets, validating the superiority of DEALRec in terms To overcome the above issues, we summarize two principal objectives for data pruning in the context of LLM-based recommendation: 1) high accuracy, which focuses on selecting the samples that can lead to low empirical risk; and 2) high efficiency, which emphasizes the low costs of the data pruning process, i.e., eliminating the dependency of well-trained LLMs on the full data. Nevertheless, pursuing the two objectives faces two challenges: at https://anonymous.4open.science/r/DEALRec/. recommendation, which aims to identify a set of representative samples particularly for LLMs' few-shot fine-tuning. Formally, given all training samples D = { /u1D460 /u1D462 | /u1D462 \u2208 U} , the target of data 4 Our main focus lies in sequential recommendation, which holds notable practical In this section, we first introduce LLM-based recommender models and uncover the challenge of real-world applicability. Thereafter, we formulate the task of data pruning for LLM-based recommendation and compare the related work on coreset selection. significance by intricately considering the temporal aspect in real-world scenarios. Data-efficient Fine-tuning for LLM-based Recommendation SIGIR '24, July 14-18, 2024, Washington, DC, USA \u00b7 LLM-based recommender models. To leverage the competent capabilities of LLMs, LLM-based recommendation typically utilize powerful LLMs directly as the recommender models. Since LLMs are not particularly trained on the recommendation data, fine-tuning is the necessary and key step for LLMs to learn the item knowledge and understand user behavior. Let U and I denote the sets of users and items, respectively. We present each training sample, i.e., user sequence, as \ud835\udc60 = ( \ud835\udc65,\ud835\udc66 ) , where \ud835\udc65 = [ \ud835\udc56 1 , \ud835\udc56 2 , . . . , \ud835\udc56 | \ud835\udc65 | ] is the user's historical interactions in chronological order, and \ud835\udc66 is the next interacted item of the user 3 , where { \ud835\udc56 1 , . . . , \ud835\udc56 | \ud835\udc65 | , \ud835\udc66 } \u2282 I . Formally, given the user sequences of the training set D = { \ud835\udc60 \ud835\udc62 | \ud835\udc62 \u2208 U} , the target is to fine-tune an LLM for recommendation tasks. The learnable parameters ( \ud835\udf19 \u2208 \u03a6 ) of an LLM is optimized by minimizing the negative log-likelihood of the next interacted item \ud835\udc66 conditioned on input \ud835\udc65 :  While fine-tuning LLMs has demonstrated effectiveness in recommendation tasks [29], its practical application is hindered by the high resource costs required by LLMs and the continuous influx of new recommendation data [38]. Hence, it is essential to enhance the efficiency of LLM-based recommender fine-tuning. where \ud835\udc66 \ud835\udc61 denotes the \ud835\udc61 -th token of \ud835\udc66 , and \ud835\udc66 < \ud835\udc61 represents the token sequence preceding \ud835\udc66 \ud835\udc61 .", "\u00b7 Data pruning for efficient LLM-based recommendation.": "\u00b7 Retrospect of coreset selection. As the closely related work to this data pruning task, coreset selection methods generally fall into two groups: To achieve efficient LLM-based recommendation, a promising approach is to reduce the costs by few-shot fine-tuning with randomly selected samples [4]. Nevertheless, the random samples might lose some crucial information for LLMs to acquire the latest information on user behavior or items, e.g., trending items. In this light, we introduce the task of data pruning for efficient LLM-based recommendation, which aims to identify a set of representative samples particularly for LLMs' few-shot fine-tuning. Formally, given all training samples D = { \ud835\udc60 \ud835\udc62 | \ud835\udc62 \u2208 U} , the target of data pruning is to select a subset S \u2282 D , such that the LLMs trained on the subset S can yield good performance on the testing set. The size of S is controlled by the given selection ratio \ud835\udc5f , i.e., |S| = \ud835\udc5f |D| . 1) Heuristic methods [7, 10, 44] typically design some heuristic strategies to select samples based on an empirical minimizer:  where L(\u00b7) is the loss function of the task, e.g., image classification [16] or CTR prediction [14], and \ud835\udc3b (\u00b7) denotes the heuristic strategy such as selecting samples with larger prediction entropy [7], or clustering the samples based on the sample representations [6]. However, this group of methods designs the strategy \ud835\udc3b (\u00b7) intuitively and fails to explicitly consider the influence of a sample on the empirical risk. This might lead to suboptimal selection, thereby declining the performance of the model trained by the selected subset. 3 Our main focus lies in sequential recommendation, which holds notable practical significance by intricately considering the temporal aspect in real-world scenarios. Figure 2: Overview of DEALRec. DEALRec first trains a surrogate model on the full training samples. Subsequently, it calculates the influence score, which is then regularized by the effort score, to identify influential samples. Surrogate Model Optimize Influence Score Calculation LLMs + Effort Score Calculation \u00d7# \u00d7$ most influential least influential after training training Samples 2) Optimization-based methods [5, 22, 23, 48] mainly utilize bilevel optimization techniques to learn the best subset chosen for training:  Besides, there is also some work that employs discrete optimization problems based on the empirical minimizer \u02c6 \ud835\udf03 in Eq. (2). Nevertheless, they struggle to be applied to large-scale datasets e.g., recommendation data, due to the complex solving of the optimization problem [17]. Furthermore, as shown in Eq. (2-3), previous coreset selection methods usually require the model to be trained over original training samples D , which however is infeasible for LLM-based recommender models due to the continuous influx of data and the high resource costs of LLMs ( cf. Section 1). \u00b7 Drawing upon the above insights, we consider two objectives for data pruning: 1) high accuracy emphasizes the low empirical risk of the model trained on the selected samples, and 2) high efficiency focuses on the low costs of the data pruning process, breaking free from the heavy fine-tuning of LLMs for data pruning.", "3 DEALREC": "To pursue efficient LLM-based recommendation, we propose a novel data pruning method DEALRec, which involves two key components, i.e., the influence score to estimate the influence on empirical risk, and the effort score as a regularization to mitigate the gap between surrogate model and LLMs. The overview of our method is presented in Figure 2.", "3.1 Influence Score": "To achieve good overall performance with the model trained on the pruned dataset S , the key lies in the ability to assess the influence on the empirical risk, i.e., overall performance, caused by removing a sample in training. However, simply assessing the the influence by removing each sample is impractical, because it requires brute force leaving-one-out-retraining for \ud835\udc5b = |D| times. To overcome this challenge, we propose an efficient approximation of the influence for all samples by extending influence on parameter change ( i.e., a classic result from influence function [24]) via chain rule and secondorder optimization techniques. We further utilize the symmetric property to speed up the calculation of the influence score. SIGIR '24, July 14-18, 2024, Washington, DC, USA Xinyu Lin et al. \u00b7 Influence on parameter change. To estimate the influence on empirical risk for each sample, we first start with the classic result [28] from research on influence function [8], which gives us the estimation of the parameter change caused by upweighting a sample \ud835\udc60 for training. Considering a training sample \ud835\udc60 is upweighted by a small \ud835\udf16 , the empirical minimizer can be rewritten as: According to [28], the influence of upweighting a sample \ud835\udc60 on the parameter change is then given as:   where \ud835\udc3b \u02c6 \ud835\udf03 = 1 \ud835\udc5b \u02dd \ud835\udc60 \ud835\udc56 \u2208D \u2207 2 \ud835\udf03 L( \ud835\udc60 \ud835\udc56 , \u02c6 \ud835\udf03 ) is the Hessian and positive definite by assumption, I param ( \ud835\udc60 ) \u2208 R \ud835\udc5a , and \ud835\udc5a is the number of parameters. Notably, assigning -1 \ud835\udc5b to \ud835\udf16 is equivalent to removing the sample \ud835\udc60 from training. As such, the parameter change of removing a training sample \ud835\udc60 can be linearly approximated as:  where \u02c6 \ud835\udf03 -\ud835\udc60 = arg min \ud835\udf03 \u2208 \u0398 \u02dd \ud835\udc60 \ud835\udc56 \u2208D ,\ud835\udc60 \ud835\udc56 \u2260 \ud835\udc60 L( \ud835\udc60 \ud835\udc56 , \ud835\udf03 ) . Based on Eq. (6), an intuitive approach to assess the sample influence for model training is to utilize the L2 norm of a sample's influence on parameter change or an additional discrete optimization problem as proposed in [50]. Nevertheless, large parameter changes do not necessarily lead to performance improvements. Besides, calculating Eq. (6) for all training samples can be computationally costly [17] and is infeasible for recommendation data. To alleviate the issues, we propose an efficient approximation for the influence of removing a sample on the empirical risk. \u00b7 Influence on empirical risk. Based on the parameter change obtained via the influence function, we can then estimate the influence of upweighting a training sample \ud835\udc60 by a small \ud835\udf16 on the loss of an arbitrary sample \ud835\udc60 \u2032 : Similarly, the influence of removing a training sample \ud835\udc60 on the loss of an arbitrary sample \ud835\udc60 \u2032 can be linearly approximated as:    We can then obtain the influence of removing a sample \ud835\udc60 on the empirical risk ( i.e., influence score) by However, it is non-trivial to directly obtain \ud835\udc3b -1 \u02c6 \ud835\udf03 as forming and inverting \ud835\udc3b \u02c6 \ud835\udf03 = 1 \ud835\udc5b \u02dd \ud835\udc60 \ud835\udc56 \u2208D \u2207 2 \ud835\udf03 L( \ud835\udc60 \ud835\udc56 , \u02c6 \ud835\udf03 ) requires O( \ud835\udc5b\ud835\udc5a 2 + \ud835\udc5a 3 ) with", "Algorithm 1 Procedure of HVP Estimation": "Input: Original training dataset D , parameters of a well-trained model \u02c6 \ud835\udf03 , iteration number \ud835\udc47 . 4: Randomly sample a training sample \ud835\udc60 \ud835\udc61 \u2208 D ; 1: Compute \u02dd \ud835\udc56 1 \ud835\udc5b \u2207 \ud835\udf03 L( \ud835\udc60 \ud835\udc56 , \u02c6 \ud835\udf03 ) for \u2200 \ud835\udc56 \u2208 { 1 , . . . , \ud835\udc5b } . 2: Initialize \u02dc \ud835\udc3b -1 0 h \u02dd \ud835\udc56 1 \ud835\udc5b \u2207 \ud835\udf03 L( \ud835\udc60 \ud835\udc56 , \u02c6 \ud835\udf03 ) i = \u02dd \ud835\udc56 1 \ud835\udc5b \u2207 \ud835\udf03 L( \ud835\udc60 \ud835\udc56 , \u02c6 \ud835\udf03 ) . 3: for all \ud835\udc61 \u2208 { 1 , . . . , \ud835\udc47 } do 5: Calculate \u2207 2 \ud835\udf03 L( \ud835\udc60 \ud835\udc61 ) as the unbiased estimator of \ud835\udc3b ; 6: \u02dc \ud835\udc3b -1 \ud835\udc61 h \u02dd \ud835\udc56 1 \ud835\udc5b \u2207 \ud835\udf03 L( \ud835\udc60 \ud835\udc56 , \u02c6 \ud835\udf03 ) i \u2190 \u02dd \ud835\udc56 1 \ud835\udc5b \u2207 \ud835\udf03 L( \ud835\udc60 \ud835\udc56 , \u02c6 \ud835\udf03 )+ GLYPH<2> \ud835\udc3c - \u2207 2 \ud835\udf03 L( \ud835\udc60 \ud835\udc61 ) GLYPH<3> \u02dc \ud835\udc3b -1 \ud835\udc61 -1 h \u02dd \ud835\udc56 1 \ud835\udc5b \u2207 \ud835\udf03 L( \ud835\udc60 \ud835\udc56 , \u02c6 \ud835\udf03 ) i ; \u22b2 7: \u02dc \ud835\udc3b -1 h \u02dd \ud835\udc56 1 \ud835\udc5b \u2207 \ud835\udf03 L( \ud835\udc60 \ud835\udc56 , \u02c6 \ud835\udf03 ) i \u2190 \u02dc \ud835\udc3b -1 \ud835\udc47 h \u02dd \ud835\udc56 1 \ud835\udc5b \u2207 \ud835\udf03 L( \ud835\udc60 \ud835\udc56 , \u02c6 \ud835\udf03 ) i . Output: Unbiased estimation \u02dc \ud835\udc3b -1 h \u02dd \ud835\udc56 1 \ud835\udc5b \u2207 \ud835\udf03 L( \ud835\udc60 \ud835\udc56 , \u02c6 \ud835\udf03 ) i .  \ud835\udc5b training samples and \ud835\udf03 \u2208 R \ud835\udc5a . This results in cumbersome calculation of influence scores for all training samples. \u00b7 Efficient estimation of influence score. To achieve efficient computation of influence score, we utilize stochastic-based HessianVector Products (HVP) [1] to efficiently approximate \ud835\udc3b -1 \u02c6 \ud835\udf03 \u2207 \ud835\udf03 L( \ud835\udc60, \u02c6 \ud835\udf03 ) . The idea of stochastic-based HVP estimation is to iteratively obtain an unbiased estimator of \ud835\udc3b \u02c6 \ud835\udf03 and approach the unbiased estimation of HVP, i.e., \ud835\udc3b -1 \u02c6 \ud835\udf03 \u2207 \ud835\udf03 L( \ud835\udc60, \u02c6 \ud835\udf03 ) . Specifically, we omit the \u02c6 \ud835\udf03 subscript for clarity and write the first \ud835\udc57 terms in Taylor expansion of \ud835\udc3b -1 as \ud835\udc3b -1 \ud835\udc57 def = \u02dd \ud835\udc57 \ud835\udc56 = 0 ( \ud835\udc3c -\ud835\udc3b ) \ud835\udc56 , which can be further rewritten recursively as \ud835\udc3b -1 \ud835\udc57 = \ud835\udc3c + ( \ud835\udc3c -\ud835\udc3b ) \ud835\udc3b -1 \ud835\udc57 -1 . From the validity of the Taylor expansion, we have \ud835\udc3b -1 \ud835\udc57 \u2192 \ud835\udc3b -1 as \ud835\udc57 \u2192 \u221e . Thereafter, denoting \u2207 \ud835\udf03 L( \ud835\udc60, \u02c6 \ud835\udf03 ) as \ud835\udc63 , the update iteration for the estimated \ud835\udc3b -1 \u02c6 \ud835\udf03 \u2207 \ud835\udf03 L( \ud835\udc60, \u02c6 \ud835\udf03 ) at step \ud835\udc61 can be written as: where \ud835\udc60 \ud835\udc61 is a training sample randomly drawn from D , and \u2207 2 \ud835\udf03 L( \ud835\udc60 \ud835\udc61 ) is an unbiased estimator of the \ud835\udc3b at step \ud835\udc61 for fast-to-compute HVP [24]. Despite that stochastic-based HVP can alleviate the computation burdens of the estimation, calculating the influence score for each sample is still costly due to the independent \ud835\udc5b estimations of \ud835\udc3b -1 \u02c6 \ud835\udf03 \u2207 \ud835\udf03 L( \ud835\udc60, \u02c6 \ud835\udf03 ) for each \ud835\udc60 \u2208 D (refer to Eq. (9)).  To further enhance the efficiency of acquiring influence scores for all samples, we use symmetric property to rewrite Eq. (9) into: The reformulation is based on the assumption that L(\u00b7) has continuous second-order derivatives, which is consistent with the assumption for influence function [24], leading to the fact that \ud835\udc3b -1 \u02c6 \ud835\udf03 is symmetric. Since \ud835\udc3b -1 \u02c6 \ud835\udf03 h \u02dd \ud835\udc56 1 \ud835\udc5b \u2207 \ud835\udf03 L( \ud835\udc60 \ud835\udc56 , \u02c6 \ud835\udf03 ) i \u2208 R \ud835\udc5a is a constant vector for any sample \ud835\udc60 \u2208 D , we can efficiently obtain influence scores for all samples by only applying HVP estimation once for \ud835\udc3b -1 \u02c6 \ud835\udf03 h \u02dd \ud835\udc56 1 \ud835\udc5b \u2207 \ud835\udf03 L( \ud835\udc60 \ud835\udc56 , \u02c6 \ud835\udf03 ) i . The detailed HVP estimation process is illustrated in Algorithm 1.  Data-efficient Fine-tuning for LLM-based Recommendation SIGIR '24, July 14-18, 2024, Washington, DC, USA Figure 3: (a) depicts the different learning ability due to the prior knowledge in LLMs. (b) presents the distributions of effort scores of LLM and surrogate model on Games dataset 4 . 0 1 2 3 4 \u2026 Effort Score \u2026\u2026 \u2026 Gap 1k 0 2k 3k 4k Number of Samples (Users) LLMs with Knowledge Rec Data (b) (a) score \u2248 3.2 Small Effort Large Effort Surrogate Model adapt adapt", "(a) Depicts the some users are easier for LLMs to learn. (b) Presents the distributions of discrepancy scores on LLM and 3.2 Gap Regularization": "surrogate model, showing the different learning ability of the two models. As shown in Eq. (11), assessing the influence score of a sample requires the optimized parameters \u02c6 \ud835\udf03 well-trained over all training samples D . Nevertheless, this poses challenges for LLM-based recommender models due to the continuous influx of large-scale new data in real-world scenarios. In this light, we propose to utilize a surrogate model to replace the LLMs and introduce an effort score as a gap regularization to complement the learning ability gap between LLMs and the surrogate models. \u00b7 Surrogate model. To reduce the costs, we propose utilizing a surrogate model, e.g., a small-sized traditional recommender model, to compute the influence scores. Nevertheless, since LLMs acquire rich world knowledge during the pre-training stage, they intricately possess different learning abilities compared to the surrogate model (Figure 3(a)). Therefore, the influential samples on LLMs might deviate from the ones for LLMs. \u00b7 Effort score. To compensate for the gap, we introduce the effort score, which aims to capture significant samples particularly for LLMs. Specifically, we define the effort score of a sample, i.e., a user sequence, \ud835\udc60 as:  where \ud835\udf19 is the learnable parameters of LLMs 5 . Intuitively, it measures the learning effort of LLMs to fit a specific user sequence, and a larger score indicates a harder sample for LLMs to learn. To elaborate, Eq. (12) measures the change in the model parameters, which can be interpreted as the discrepancy from the current knowledge encoded in LLMs' parameters to the latest item knowledge or user behavior. As such, the effort score can emphasize significant samples particularly for LLMs, supplementing the different learning ability of the surrogate model (Figure 3(b)). \u00b7 Overall score. By injecting the signals of LLMs' learning ability into the calculation of influence score, we can obtain the final score of each user sequence for LLM-based recommender fine-tuning:  4 We obtain the effort scores for surrogate model by calculating the gradient norm of the parameters of the surrogate model (Eq. (12)). 5 The learnable parameters can be either the whole parameters of LLMs or the learnable parameters from parameter-efficient training, e.g., LoRA [19].", "Algorithm 2 Procedure of DEALRec": "Input: Original training dataset D , randomly initialized parameters of surrogate model \ud835\udf03 , pre-trained parameters of LLM \ud835\udf19 . \ud835\udc56 \u02c6 \ud835\udc5b \u2207 \ud835\udf03 L( \ud835\udc60 \ud835\udc56 , \ud835\udf03 ) via HVP estimation. 5: G = { \ud835\udc3a 1 , . . . , \ud835\udc3a \ud835\udc3e } \u2190 Split training samples D into \ud835\udc3e groups according to the final score \ud835\udc3c \ud835\udc60 with even range width.  |D| \ud835\udc3e \u230b . 8: \ud835\udc58 \u2217 = arg min \ud835\udc58 | \ud835\udc3a \ud835\udc58 | ; 6: S \u2190 \u2205 , \ud835\udc35 \u2190 \u230a \ud835\udc5f 7: while G \u2260 \u2205 do 9: S \ud835\udc58 \u2217 \u2190 randomly select min { \ud835\udc35, | \ud835\udc3a \ud835\udc58 \u2217 | } samples from \ud835\udc3a \ud835\udc58 \u2217 ; 11: 10: S \u2190 S \u222a S \ud835\udc58 \u2217 ; G \u2190 G \\ { \ud835\udc3a \ud835\udc58 \u2217 } ; \ud835\udc5f |G| \u2190 \u230a \u230b \ud835\udc35 |D|-|S| ; \u22b2 Update sampling budget Output: Selected samples S for few-shot fine-tuning. where \ud835\udf06 is a hyper-parameter to balance the strength of the gap regularization. Notably, the gap regularization would suppress the easy samples with smaller effort scores while emphasizing the samples that are more difficult to learn, i.e., larger effort scores. Intuitively, DEALRec identifies the influential samples with two key considerations: 1) the influence score focuses on selecting the representative samples from the full dataset, capturing collaborative filtering information for low empirical risk; and 2) the effort score highlights the non-trivial samples that are significant to the learning of LLMs. The effectiveness of the two scores is empirically validated in Section 4.3.1.", "3.3 Few-shot Fine-tuning": "Based on the final influential score obtained via Eq. (13), we can select a subset of data S for LLMs' few-shot fine-tuning, given an expected selection ratio \ud835\udc5f . \u00b7 Few-shot data coverage. A straightforward approach is to select the data greedily, i.e., rank the samples based on the overall scores, and then select the top\ud835\udc5f percentage of the training data. However, greedily selecting the samples with higher scores might result in very similar samples with low data coverage, which leads to: 1) Inadequacy of samples from other areas, thus hurting the bounded empirical risk [57] and lowering the overall performance ( cf. Section 4.2). 2) Poor utilization of training samples because of the redundant samples with similar patterns, thereby causing suboptimal selection for few-shot fine-tuning. \u00b7 Coverage-enhanced sample selection. To address the above issues, we follow [57] to select the users based on the idea of stratified sampling. The core idea is to maintain the budget for the samples in different areas of training distribution, such that the data coverage will be improved to ensure a high-probability bound for the empirical risk (refer to [57] for detailed proof). In detail, we first divide the samples into \ud835\udc3e groups according to their overall scores. We then iteratively sample \ud835\udc5b \ud835\udc60 user sequences from the group with the fewest samples and discard that group after sampling, where \ud835\udc5b \ud835\udc60 is the average sampling budget for all groups SIGIR '24, July 14-18, 2024, Washington, DC, USA Xinyu Lin et al. Table 1: Statistics of the three datasets. and is initialized with \u230a \ud835\udc5f | D| \ud835\udc3e \u230b . If the group size is smaller than the average sampling budget, we select all users from this group and update the average sampling budget for the remaining groups (see Algorithm 2). Based on the selected few-shot samples S , we optimize the learnable parameters ( \ud835\udf19 \u2208 \u03a6 ) of LLMs:  \u00b7 Instantiation. To instantiate DEALRec on LLM-based recommender models, we first employ a surrogate model to train on original training samples D and calculate the influence score for all samples via Eq. (11), where the L(\u00b7) can be any form of the loss function from the surrogate model, e.g., BPR [37]. We then obtain the effort score for LLMs via Eq. (12), where \ud835\udf19 can be the learnable parameters from any backend LLM-based recommender models. Eventually, we apply the stratified sampling to select the samples for LLMs' few-shot fine-tuning. The detailed data pruning process of DEALRec is demonstrated in Algorithm 2.", "4 EXPERIMENT": "We conduct extensive experiments on three real-world datasets to answer the following research questions: \u00b7 RQ1: How does our proposed DEALRec perform compared to the coreset selection baselines for LLM-based recommendation and the models trained with full data? \u00b7 RQ3: How does DEALRec perform under different selection ratios and how does DEALRec improve the overall performance? \u00b7 RQ2: How do the different components of DEALRec ( i.e., influence score, gap regularization, and stratified sampling) affect the performance, and is DEALRec generalizable to different surrogate models?", "4.1 Experimental Settings": "4.1.1 Datasets. We conduct experiments on three real-world recommendation datasets: 1) Games is from the Amazon review datasets 6 , which covers interactions between users and video games with rich textual features. 2) MicroLens-50K 7 is a newly released micro-video recommendation dataset [33]. It contains 50 \ud835\udc58 users' interactions with micro-videos and their associated multimodal features. 3) Book is also from Amazon review datasets, containing users' interactions with extensive books. For Games and Book, we follow previous work and discard the interactions with the ratings < 4. For the three datasets, we sort all user-item interactions according to the global timestamps, and then split the interactions into training, validation, and testing sets with the ratio of 8:1:1. Besides, we consider two different fine-tuning settings as follows: 6 https://jmcauley.ucsd.edu/data/amazon/. 7 https://github.com/westlake-repl/MicroLens/. 1) Few-shot fine-tuning fine-tunes LLM-based recommender models with limited samples at a fixed size, e.g., 1024-shot, obtained via different data pruning methods. 2) Full fine-tuning utilizes all samples to fine-tune LLM-based recommender models without data pruning. 4.1.2 Baselines. We compare DEALRec with the random sampling and several competitive coreset selection methods, including difficulty-based methods and diversity-based methods: 1) Random obtains the data subset via random sampling, which is a popular and strong baseline in data-efficient training [13]. 2) GraNd [34] is a representative coreset selection method that selects the difficult samples with larger gradient norms during training. 3) EL2N [34] proposes to select the difficult samples with larger errors between the labels and the prediction from the model trained by the original dataset. 4) CCS [57] is a competitive method that selects the samples considering both high data coverage and sample importance. We use EL2N as the importance metric for CCS. 5) TF-DCon [49] is a recently proposed data pruning method for content-based recommendation, which clusters the user sequences based on the user representations obtained from both well-trained recommender models and LLMs for selection. 6) RecRanker [30] proposes a sampling strategy to select high-quality user sequences. It selects the users with more interactions for better user modeling and utilizes a cluster-based sampling strategy to enhance user diversity. We do not perform optimization-based methods for comparison because of the inapplicability of complex bi-level or discrete optimization for LLMs on large-scale recommendation data ( cf. Section 2). We instantiate our proposed DEALRec and all baselines on two competitive backend LLM-based recommender models: 1) BIGRec [3] utilizes the item title to present the user sequence for recommendation generation; 2) TIGER [35] learns extra tokens from item features to present items, and then converts the user sequence into the sequence of the new item token for next-item generation. \u00b7 Evaluation. We employ the widely used metrics Recall@ \ud835\udc3e and NDCG@ \ud835\udc3e to evaluate the models [18], with \ud835\udc3e set to 10 and 20 for Games, and \ud835\udc3e = 20 and 50 for MicroLens-50K and Book 8 . 4.1.3 Implementation. As for the two backend LLM-based recommender models, we follow the original settings in their paper for implementation. We employ LLaMA-7B for BIGRec and transformer-based architecture for TIGER as in their paper [35]. All fine-tuning experiments are conducted on four NVIDIA RTX A5000 GPUs. Besides, we adopt the parameter-efficient fine-tuning technique LoRA [19] to fine-tune BIGRec and fully fine-tune the parameters of TIGER. We utilize SASRec [20], a representative sequential recommender model, as the surrogate model in DEALRec. We set the iteration number \ud835\udc47 for HVP estimation at 5000, and search the regularization strength \ud835\udf06 in { 0 . 1 , 0 . 3 , 0 . 5 , 1 . 0 , 2 . 0 } . For cluster-based methods, the number of clusters \ud835\udc3e is explored in { 25 , 50 , 75 } . As for the coreset selection methods that require the training of LLMs, we consider a feasible implementation [7] by executing them on the same surrogate model as DEALRec. 8 We report metrics@20 and @50 because of the challenging modeling of user behavior on book and micro-video recommendations, where the temporal shifts of user interests and the item feature is stronger and thus more difficult to capture [45, 46]. Data-efficient Fine-tuning for LLM-based Recommendation SIGIR '24, July 14-18, 2024, Washington, DC, USA Table 2: Overall performance comparison between the baselines and DEALRec instantiated on two competitive LLM-based recommender models on three datasets. For each backend model, the bold results highlight the best results while the second-best ones are underlined. \u2217 implies the improvements over the second-best results are statistically significant ( \ud835\udc5d -value < 0.01) under one-sample t-tests. We run all experiments for 3 times with different random seeds and report the averaged results. Table 3: Performance comparison between DEALRec under 1024-shot fine-tuning and the full fine-tuning of the BIGRec in terms of both accuracy and time costs. '%Improve.' denotes the relative improvement achieved by DEALRec compared to the full fine-tuning. Models are trained for 50 epochs with the early stopping strategy.", "4.2 Overall Performance (RQ1)": "The results of the baselines and DEALRec with two competitive backend LLM-based recommender models on three datasets under few-shot fine-tuning (1024 samples) are presented in Table 2, from which we have the following observations: it maintains easy samples for selection, thus compensating the knowledge of recommendation data from high-density areas. \u00b7 All methods with BIGRec typically yield better performance than those with TIGER, which is attributed to two reasons: 1) BIGRec employs a larger LLM ( i.e., LLaMA-7B) compared to TIGER, thereby benefiting from the stronger generalization ability of large-sized LLMs [27]; and 2) BIGRec leverages item titles to present the user sequence, leading to better utilization of world knowledge in LLMs. In contrast, TIGER learns extra item tokens for LLMs. This might result in cold-start item issues since only limited item tokens are learned while others are maintained randomly initialized under the few-shot fine-tuning setting. \u00b7 Among all coreset selection baselines, difficulty-based (GraNd, EL2N) methods generally perform better than diversity-based methods (TF-DCon, RecRanker). This is reasonable since diversity-based methods merely heuristically encourage selecting users with divergent preference, which lacks the assessments of their contributions to the model training. In contrast, GraNd and EL2N use pre-defined metrics to measure the sample difficulty and select the samples with larger scores, which encourages selecting the samples that are more informative for models' optimization. Besides, CCS improves EL2N in most cases, as \u00b7 DEALRec significantly outperforms all coreset selection methods across the three datasets. The consistent performance improvements on both backend models validate the superiority of DEALRec in identifying influential samples for LLMs' adaptation to the recommendation data. The superior performance is attributed to: 1) the accurate and efficient estimation of the influence on empirical risk, i.e., overall performance by removing a sample in training; and 2) the gap regularization based on the effort score to penalize the easy samples for LLMs. By emphasizing the non-trivial samples specifically for LLMs, gap regularization alleviates the learning ability gap between the surrogate model and the LLMs. \u00b7 Another interesting observation is that random sampling yields competitive performance or even outperforms other coreset selection methods in some cases, which might attributed to two possible reasons: 1) Uniformly selected user sequences preserve high coverage of the original training distribution compared to other baselines, which ensures a high probability of guaranteed bound for low empirical risk [57]. This observation is also consistent with the findings in [13]. 2) The inferior performance of some coreset selection methods also might be caused by the implementation settings (Section 4.1.3), where they may suffer from the learning ability gap between the surrogate model and LLMs. ( cf. Section 3.2). SIGIR '24, July 14-18, 2024, Washington, DC, USA Xinyu Lin et al. Figure 4: Ablation study of the influence score, effort score, and coverage-enhanced sample selection strategy. 0.008 0.011 0.013 0.016 0.018 0.022 0.026 0.030 Games Recall@20 NDCG@20 Greedy DEALRec (a) w/o IS w/o \ud835\udf39 \ud835\udc94 0.0082 0.0087 0.0092 0.0097 0.0100 0.0106 0.0112 0.0118 MicroLens-50K Recall@20 NDCG@20 Greedy DEALRec (b) w/o IS w/o \ud835\udf39 \ud835\udc94 Table 4: Performance comparison between DEALRec with different surrogate models and the BIGRec under full training. 'Time' presents the time costs for training the surrogate model on a single NVIDIA RTX A5000. \u00b7 Comparison with full fine-tuning. We further compare DEALRec with BIGRec under full training w.r.t. accuracy and efficiency, as presented in Table 3. We can find that: 1) DEALRec achieves higher performance compared to the model trained by the full data, indicating the effectiveness of DEALRec for high accuracy. The inferior performance of BIGRec under full training also implies that not all user sequences are informative for model training, or even harmful to the training, e.g., false negative interactions. This has also been observed in CTR prediction [48] and has been discussed in [2] from the view of data redundancy. 2) DEALRec significantly reduces the time costs for LLMs' fine-tuning (97.11% reduction of fine-tuning costs on average). With the remarkably declined training costs, DEALRec has the potential to facilitate real-world applications of LLM-based recommender models.", "4.3 In-depth Analysis": "4.3.1 Ablation Study (RQ2). To study the effectiveness of each component of DEALRec, i.e., influence score, effort score, and coverage-enhanced sample selection strategy, we separately remove the Influence Score (IS) and effort score \ud835\udeff \ud835\udc60 , referred to as 'w/o IS' and 'w/o \ud835\udeff \ud835\udc60 ', respectively. Besides, we replace the coverage-enhanced sample selection strategy by greedily selecting the samples with higher scores, denoted as 'Greedy'. From the results presented in Figure 4, we can observe that: removing either the influence score or effort score will cause performance drops. This validates the effectiveness of 1) the assessment of overall performance change caused by removing samples from training; 2) additional signals of learning ability captured from LLMs as regularization, alleviating the gap between the surrogate model and the LLMs. Moreover, simply selecting the samples with higher overall scores might weaken the learning of distinct user behaviors and item knowledge (inferior performance of 'Greedy'), as discussed in Section 3.3. Figure 5: Performance of DEALRec with different selection ratio \ud835\udc5f w.r.t. accuracy and efficiency on Games. 0.01 0.016 0.022 0.028 0.2% 0.5% 1% 1.5% 2% 4% Full Training (a) Effect of \ud835\udc93 w.r.t. Recall (Recall@20) 0.925 0.967 1.009 0 0.6 1.2 1.8 0.2% 0.5% 1% 1.5% 2% 4% %Reduction (b) Effect of \ud835\udc93 w.r.t. time costs (Time Costs (h)) (%Reduction)", "4.3.2 Robustness on different surrogate model (RQ2). To": "further assess the generalization ability of DEALRec on different surrogate models, we employ three representative sequential recommender models, i.e., BERT4Rec [41], SASRec [20], and DCRec [51] as the surrogate models, respectively. From the results in Table 4, we can find that: 1) DEALRec with the three surrogate models consistently outperforms BIGRec under full finetuning. This demonstrates the strong robustness of DEALRec on different surrogate models. 2) Different surrogate models cause some fluctuations in accuracy. This is reasonable because different model architectures express user behavior and item knowledge differently, possibly resulting in varied selected samples which will affect the performance. 3) SASRec exhibits the least time costs for training and achieves competitive performance among the three surrogate models. Therefore, SASRec could be a good choice of surrogate model for DEALRec in real-world deployments. 4.3.3 Effect of selection ratio \ud835\udc93 (RQ3). To investigate the effect of selection ratio \ud835\udc5f on DEALRec on both accuracy and efficiency, we vary the ratio \ud835\udc5f from 0 . 2% (128-shot) to 4% (4096-shot) and present the results in Figure 5. It is observed that: 1) The recommendation accuracy rapidly improves as the number of selected samples increases from 0 . 2% to 1%, surpassing the full training when \ud835\udc5f = 1%. Besides, if we continuously increase the selection ratio from 2% to 4%, the benefits from additional samples gradually diminish and only minor improvements in accuracy are observed. We suspect that the gap between and the recommendation data mainly resides in a small subset of the representative user behaviors, which is what DEALRecaims to identify. 2) Meanwhile, although the time costs for fine-tuning LLMs gradually increase because of additional samples, the cost reduction compared to the full training still reaches over 94%. 3) Empirically, setting \ud835\udc5f = 1% is recommended to achieve comparable performance to full fine-tuning and low costs. 4.3.4 User group evaluation (RQ3). To study how DEALRec achieves superior overall performance, we test DEALRec over user sequences of different difficulties. Specifically, we calculate the loss of each user sequence via the model trained by randomly selected few-shot samples; we then divide the users into three groups according to their loss values, from the easier samples with smaller loss (Group 1) to the harder samples with larger loss (Group 3). The results of each group of DEALRec and Random on Games are presented in Figure 6. We can find that 1) the performance of both DEALRec and Random gradually declines from Group 1 to Group 3, because users with larger loss are more difficult to predict. Nevertheless, 2) DEALRec consistently outperforms Random in Data-efficient Fine-tuning for LLM-based Recommendation SIGIR '24, July 14-18, 2024, Washington, DC, USA Figure 6: Performance of DEALRec over easy to difficult samples (Group 1 to Group 3). 0.01 0.024 0.038 0.052 Games Random DEALRec Group 1 Group 2 Group 3 (a) Performance w.r.t. Recall@20 \u2191 11.42% \u2191 37.29% \u2191 5.98% 0 0.01 0.02 Games Random DEALRec Group 1 Group 2 Group 3 (b) Performance w.r.t. NDCG@20 \u2191 8.41% \u2191 37.50% \u2191 7.02% Figure 7: Performance of DEALRec with different \ud835\udf06 . 0.015 0.023 0.031 0.013 0.018 0.023 0.1 0.3 0.5 1 2 Games Recall@10 Recall@20 (a) Effect of \ud835\udf40 w.r.t. Recall (Recall@10) (Recall@20) 0.000 0.010 0.019 0.004 0.012 0.02 0.1 0.3 0.5 1 2 Games NDCG@10 NDCG@20 (b) Effect of \ud835\udf40 w.r.t. NDCG (NDCG@10) (NDCG@20) each group, which validates the effectiveness of DEALRec in considering the influence on overall performance. 4.3.5 Effect of regularization strength \ud835\udf40 . We vary \ud835\udf06 from 0 . 1 to 2 for DEALRec and evaluate the performance as in Figure 7. From the figures, we can find that: 1) As we incrementally increase the value of \ud835\udf06 , the overall trend of accuracy has been observed to be generally improved. This is due to the gap between the surrogate model and LLMs as discussed in Section 3.2, emphasizing the necessity to regularize the influence score to be aligned with the learning ability of the LLMs. 2) However, blindly pursuing larger lambda is not necessarily beneficial. We should carefully balance between the performance-driven influential samples from the surrogate model and the difficult samples for the LLMs.", "5 RELATED WORK": "", "5.1 LLM-based Recommendation": "Leveraging LLMs for recommendation has gained remarkable attention recently [36, 52], showcasing their potential across various recommendation tasks [4, 12, 27]. Some early studies explore the recommendation ability of powerful LLMs through in-context-learning ability [9, 42]. Nevertheless, the performance of LLMs is limited without extra fine-tuning over the domain-specific recommendation data [4]. To fully leverage the potential of LLMs for recommendation, a series of work studies various fine-tuning strategies tailored for recommendation tasks [12, 26, 31, 32, 53, 54]. However, fine-tuning LLMs requires extensive computational resources and time costs, thus hindering real-world applications. Therefore, it is crucial to enhance the fine-tuning efficiency of LLMbased recommender models. In this work, we propose the task of data pruning for efficient LLM-based recommendation, aiming to identify representative samples for LLMs' few-shot fine-tuning.", "5.2 Coreset Selection": "Coreset selection has been widely studied in both traditional machine learning and deep learning [47, 50], benefiting many downstream tasks such as data-efficient learning [44], neural architecture search [40], and active learning [39]. It aims to select a small but representative subset from the full data that can lead to comparable model performance. Previous work mainly falls into two groups: 1) Heuristic methods [7, 10, 44] typically assume difficult or diverse samples are informative for model training. 2) Optimization-based methods [21, 25, 50] leverages the bi-level or discrete optimization techniques to optimize the data subset that can minimize the empirical risk. However, heuristic methods might be suboptimal since they overlook the impact of selected samples on empirical risk. And optimization-based methods fail to be applied to LLM-based recommendation due to the cumbersome calculation for complex optimization. Furthermore, previous methods usually rely on the training of the model on full data for selection, which is infeasible for LLM-based recommendation ( cf. Section 2). \u00b7 Data Condensation [56] is another potential solution to achieve data-efficient training. However, it is intrinsically different from our proposed task of data pruning. While it aims to synthesize a small but informative dataset [55], our task targets to identify existing samples that are representative. Besides, previous work mainly works for continuous data, which is inapplicable to LLMbased recommendation [48]. TF-DCon [49] is recently proposed for content-based recommendation and we compare it in Section 4.2.", "6 CONCLUSION": "In this work, we proposed the task of data pruning for efficient LLM-based recommendation, which aims to identify representative samples tailored for LLMs' few-shot fine-tuning. Furthermore, we posited two objectives for this data pruning task: 1) high accuracy targets to select the samples that can lead to low empirical risk; and 2) high efficiency strives to consume low costs for the data pruning process. To this end, we proposed a novel data pruning method, namely DEALRec, to efficiently identify the influential samples with two scores. 1) The influence score is formulated to estimate the influence of sample removal on empirical risk, which is extended from the influence function and is accelerated through the symmetric property. 2) We introduced a small-sized surrogate model to calculate the influence score efficiently and proposed the effort score to bridge the gap between the surrogate model and LLMs. Empirical results validate the effectiveness of DEALRec in achieving both high efficiency and high accuracy. This work proposes a data pruning task for LLM fine-tuning, opening up a new research direction for efficient LLM-based recommendation and leaving many promising future directions for future work. 1) It is worthwhile to apply DEALRec to more LLM-based recommender models on more cross-domain datasets, improving fine-tuning performance with limited resources. 2) Due to the limited context window length of LLMs, it is promising to select the informative interacted items in users' interaction sequences for LLMs' fine-tuning. 3) Enhancing the inference efficiency of LLM-based recommender models is also a crucial problem for their real-world deployments. SIGIR '24, July 14-18, 2024, Washington, DC, USA Xinyu Lin et al.", "REFERENCES": "[2] Sharat Agarwal, Himanshu Arora, Saket Anand, and Chetan Arora. 2020. Contextual diversity for active learning. In ECCV . Springer, 137-153. [1] Naman Agarwal, Brian Bullins, and Elad Hazan. 2016. Second-order stochastic optimization in linear time. stat 1050 (2016), 15. [3] Keqin Bao, Jizhi Zhang, Wenjie Wang, Yang Zhang, Zhengyi Yang, Yancheng Luo, Fuli Feng, Xiangnaan He, and Qi Tian. 2023. A bi-step grounding paradigm for large language models in recommendation systems. arXiv:2308.08434 . [5] Zal\u00e1n Borsos, Mojmir Mutny, and Andreas Krause. 2020. Coresets via bilevel optimization for continual learning and streaming. NeurIPS 33 (2020), 1487914890. [4] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. Tallrec: An effective and efficient tuning framework to align large language model with recommendation. In RecSys . ACM. [6] Chengliang Chai, Jiayi Wang, Nan Tang, Ye Yuan, Jiabin Liu, Yuhao Deng, and Guoren Wang. 2023. Efficient coreset selection with cluster-based methods. In KDD . ACM, 167-178. [8] R Dennis Cook. 1977. Detection of influential observation in linear regression. Technometrics 19, 1 (1977), 15-18. [7] C Coleman, C Yeh, S Mussmann, B Mirzasoleiman, P Bailis, P Liang, J Leskovec, and M Zaharia. 2020. Selection via Proxy: Efficient Data Selection for Deep Learning. In ICLR . [9] Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si, Chen Xu, Zhongxiang Sun, Xiao Zhang, and Jun Xu. 2023. Uncovering chatgpt's capabilities in recommender systems. In RecSys . ACM, 1126-1132. [11] Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, and Jiawei Zhang. 2023. Chat-rec: Towards interactive and explainable llms-augmented recommender system. arXiv:2303.14524 . [10] Vitaly Feldman and Chiyuan Zhang. 2020. What neural networks memorize and why: Discovering the long tail via influence estimation. NeurIPS 33 (2020), 2881-2891. [12] Yuqi Gong, Xichen Ding, Yehui Su, Kaiming Shen, Zhongyi Liu, and Guannan Zhang. 2023. An Unified Search and Recommendation Foundation Model for Cold-Start Scenario. In CIKM . 4595-4601. [14] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. In IJCAI . 1725-1731. [13] Chengcheng Guo, Bo Zhao, and Yanbing Bai. 2022. Deepcore: A comprehensive library for coreset selection in deep learning. In DEXA . Springer, 181-195. [15] Frank R Hampel. 1974. The influence curve and its role in robust estimation. Journal of the american statistical association 69, 346 (1974), 383-393. [17] Muyang He, Shuo Yang, Tiejun Huang, and Bo Zhao. 2023. Large-scale Dataset Pruning with Dynamic Uncertainty. arXiv:2306.05175 . [16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In CVPR . IEEE, 770-778. [18] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for recommendation. In SIGIR . 639-648. [20] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recommendation. In ICDM . IEEE, 197-206. [19] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv:2106.09685 . [21] Krishnateja Killamsetty, Sivasubramanian Durga, Ganesh Ramakrishnan, Abir De, and Rishabh Iyer. 2021. Grad-match: Gradient matching based data subset selection for efficient deep model training. In ICML . PMLR, 5464-5474. [23] Krishnateja Killamsetty, Xujiang Zhao, Feng Chen, and Rishabh Iyer. 2021. Retrieve: Coreset selection for efficient and robust semi-supervised learning. NeurIPS 34 (2021), 14488-14501. [22] Krishnateja Killamsetty, Durga Sivasubramanian, Ganesh Ramakrishnan, and Rishabh Iyer. 2021. Glister: Generalization based data subset selection for efficient and robust learning. In AAAI , Vol. 35. 8110-8118. [24] Pang Wei Koh and Percy Liang. 2017. Understanding black-box predictions via influence functions. In ICML . PMLR, 1885-1894. [26] Lei Li, Yongfeng Zhang, and Li Chen. 2023. Prompt distillation for efficient llm-based recommendation. In CIKM . 1348-1357. [25] Suraj Kothawade, Vishal Kaushal, Ganesh Ramakrishnan, Jeff Bilmes, and Rishabh Iyer. 2022. PRISM: A Unified Framework of Parameterized Submodular Information Measures for Targeted Data Subset Selection and Summarization. In AAAI . [27] Xinyu Lin, Wenjie Wang, Yongqi Li, Fuli Feng, See-Kiong Ng, and Tat-Seng Chua. 2023. A multi-facet paradigm to bridge large language model and recommendation. arXiv:2310.06491 . [29] Qijiong Liu, Nuo Chen, Tetsuya Sakai, and Xiao-Ming Wu. 2024. ONCE: Boosting Content-based Recommendation with Both Open- and Closed-source Large Language Models. In WSDM . ACM. [28] Robert F Ling. 1984. Residuals and influence in regression. [30] Sichun Luo, Bowei He, Haohan Zhao, Yinya Huang, Aojun Zhou, Zongpeng Li, Yuanzhang Xiao, Mingjie Zhan, and Linqi Song. 2023. RecRanker: Instruction Tuning Large Language Model as Ranker for Top-k Recommendation. arXiv:2312.16018 . [32] Zheqi Lv, Wenqiao Zhang, Shengyu Zhang, Kun Kuang, Feng Wang, Yongwei Wang, Zhengyu Chen, Tao Shen, Hongxia Yang, Beng Chin Ooi, et al. 2023. DUET: A Tuning-Free Device-Cloud Collaborative Parameters Generation Framework for Efficient Device Model Generalization. In WWW . ACM, 3077-3085. [31] Zheqi Lv, Wenqiao Zhang, Zhengyu Chen, Shengyu Zhang, and Kun Kuang. 2024. Intelligent Model Update Strategy for Sequential Recommendation. In WWW . ACM. [33] Yongxin Ni, Yu Cheng, Xiangyan Liu, Junchen Fu, Youhua Li, Xiangnan He, Yongfeng Zhang, and Fajie Yuan. 2023. A Content-Driven Micro-Video Recommendation Dataset at Scale. arXiv:2309.15379 (2023). [35] Shashank Rajput, Nikhil Mehta, Anima Singh, Raghunandan H Keshavan, Trung Vu, Lukasz Heldt, Lichan Hong, Yi Tay, Vinh Q Tran, Jonah Samost, et al. 2023. Recommender Systems with Generative Retrieval. In NeurIPS . Curran Associates, Inc. [34] Mansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite. 2021. Deep learning on a data diet: Finding important examples early in training. NeurIPS 34, 20596-20607. [36] Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, and Chao Huang. 2024. Representation learning with large language models for recommendation. In WWW . ACM. [38] Noveen Sachdeva, Mehak Dhaliwal, Carole-Jean Wu, and Julian McAuley. 2022. Infinite recommendation networks: a data-centric approach. NeurIPS 35, 3129231305. [37] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2009. BPR: Bayesian personalized ranking from implicit feedback. In UAI . AUAI Press, 452-461. [39] Ozan Sener and Silvio Savarese. 2018. Active learning for convolutional neural networks: A core-set approach. (2018). [41] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer. In CIKM . 1441-1450. [40] Jae-hun Shim, Kyeongbo Kong, and Suk-Ju Kang. 2021. Core-set sampling for efficient neural architecture search. arXiv:2107.06869 . [42] Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin, and Zhaochun Ren. 2023. Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent. In EMNLP . ACL, 14918-14937. [44] Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J Gordon. 2018. An empirical study of example forgetting during deep neural network learning. arXiv:1812.05159 . [43] Haoru Tan, Sitong Wu, Fei Du, Yukang Chen, Zhibin Wang, Fan Wang, and Xiaojuan Qi. 2023. Data Pruning via Moving-one-Sample-out. arXiv:2310.14664 . [45] Wenjie Wang, Xinyu Lin, Liuhui Wang, Fuli Feng, Yunshan Ma, and Tat-Seng Chua. 2023. Causal Disentangled Recommendation Against User Preference Shifts. TOIS (2023). [47] Kai Wei, Rishabh Iyer, and Jeff Bilmes. 2015. Submodularity in data subset selection and active learning. In ICML . PMLR, 1954-1963. [46] Wenjie Wang, Xinyu Lin, Liuhui Wang, Fuli Feng, Yinwei Wei, and Tat-Seng Chua. 2023. Equivariant Learning for Out-of-Distribution Cold-start Recommendation. In MM . 903-914. [48] Jiahao Wu, Wenqi Fan, Shengcai Liu, Qijiong Liu, Rui He, Qing Li, and Ke Tang. 2023. Dataset condensation for recommendation. arXiv:2310.01038 . [50] Shuo Yang, Zeke Xie, Hanyu Peng, Min Xu, Mingming Sun, and Ping Li. 2023. Dataset pruning: reducing training data by examining generalization influence. (2023). [49] Jiahao Wu, Qijiong Liu, Hengchang Hu, Wenqi Fan, Shengcai Liu, Qing Li, Xiao-Ming Wu, and Ke Tang. 2023. Leveraging Large Language Models (LLMs) to Empower Training-Free Dataset Condensation for Content-Based Recommendation. arXiv:2310.09874 . [51] Yuhao Yang, Chao Huang, Lianghao Xia, Chunzhen Huang, Da Luo, and Kangyi Lin. 2023. Debiased Contrastive Learning for Sequential Recommendation. In WWW . 1063-1073. [53] Honglei Zhang, Fangyuan Luo, Jun Wu, Xiangnan He, and Yidong Li. 2023. LightFR: Lightweight federated recommendation with privacy-preserving matrix factorization. TOIS 41, 4 (2023), 1-28. [52] Honglei Zhang, He Liu, Haoxuan Li, and Yidong Li. 2024. TransFR: Transferable Federated Recommendation with Pre-trained Language Models. arXiv:2402.01124 . [54] Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne Xin Zhao, Leyu Lin, and Ji-Rong Wen. 2023. Recommendation as instruction following: A large language model empowered recommendation approach. arXiv:2305.07001 . [56] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. 2020. Dataset Condensation with Gradient Matching. In ICLR . [55] Bo Zhao and Hakan Bilen. 2023. Dataset condensation with distribution matching. In WACV . IEEE, 6514-6523. [57] Haizhong Zheng, Rui Liu, Fan Lai, and Atul Prakash. 2022. Coverage-centric Coreset Selection for High Pruning Rates. In ICLR ."}
