{
  "RLTP: Reinforcement Learning to Pace for Delayed Impression Modeling in Preloaded Ads": "Penghui Wei â€  Alibaba Group wph242967@alibaba-inc.com Yongqiang Chen â€  Alibaba Group shiwei.cyq@alibaba-inc.com Shaoguo Liu Alibaba Group shaoguo.lsg@alibaba-inc.com Liang Wang Alibaba Group liangbo.wl@alibaba-inc.com Bo Zheng Alibaba Group bozheng@alibaba-inc.com",
  "ABSTRACT": "To increase brand awareness, many advertisers conclude contracts with advertising platforms to purchase traffic and deliver advertisements to target audiences. In a whole delivery period, advertisers desire a certain impression count for the ads, and they expect that the delivery performance is as good as possible. Advertising platforms employ real-time pacing algorithms to satisfy the demands. However, the delivery procedure is also affected by publishers. Preloading is a widely used strategy for many types of ads (e.g., video ads) to make sure that the response time for displaying is legitimate, which results in delayed impression phenomenon. In this paper, we focus on a new research problem of impression pacing for preloaded ads, and propose a R einforcement L earning T o P ace framework RLTP . It learns a pacing agent that sequentially produces selection probabilities in the whole delivery period. To jointly optimize the objectives of impression count and delivery performance, RLTP employs tailored reward estimator to satisfy guaranteed impression count, penalize over-delivery and maximize traffic value. Experiments on large-scale datasets verify that RLTP outperforms baselines by a large margin. We have deployed it online to our advertising platform, and it achieves significant uplift to core metrics including delivery completion rate and click-through rate.",
  "CCS CONCEPTS": "",
  "Â· Information systems â†’ Online advertising ; Â· Computing methodologies â†’ Sequential decision making .": "",
  "ACMReference Format:": "Penghui Wei, Yongqiang Chen, Shaoguo Liu, Liang Wang and Bo Zheng. 2023. RLTP: Reinforcement Learning to Pace for Delayed Impression Modeling in Preloaded Ads. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '23), August 6-10, 2023, Long Beach, CA, USA. ACM, New York, NY, USA, 11 pages. https: //doi.org/10.1145/3580305.3599900 â€  Co-first authorship. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. KDD '23, August 6-10, 2023, Long Beach, CA, USA Â© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0103-0/23/08...$15.00 Figure 1: The overall delivery procedure. Due to the preloading strategy from the publisher, the ad filled by the advertising platform cannot be immediately displayed at current request. We call this phenomenon as delayed impression . Online Advertising Publisher Platform 2. Traffic Request Pacing 1. Open App Fill-in an Ad Preloading 3. Real-time Decision: Select or Not Strategy 1 (Next Time) 8. Statistics Impression Counter 7. Delayed & Metrics Impression",
  "1 INTRODUCTION": "Merchants usually achieve their customized marketing goals via delivering advertisements on online publishers. To increase brand awareness, many advertisers conclude contracts with advertising platforms to purchase traffic, and deliver advertisements to target audiences. Figure 1 gives an illustration of delivery procedure for brand advertising. When a user opens an App and browses a specific ad exposure position (i.e., sends an ad request to the publisher and ad exchange), the advertising platform estimates the traffic value and determines whether to select this request. If it makes a selection decision, it will fill-in an ad - which is expected to be displayed to the user - to the online publisher. Finally, the publisher determines a final ad that will be displayed based on its own strategies, such as preloading and frequency controlling. Therefore, at current request, it is possible that the ad filled by the platform is not displayed, but a preloaded ad from another advertising platform is displayed. For system performance and user experience, preloading is a widely used strategy for many types of ads (e.g., video ads) to ensure that the response time for displaying is legitimate. If publisher employs preloading, the filled ad from advertising platform may be displayed at one of the next requests sent by the same user (e.g., the next time the user opens the App, as in Figure 1), or even may not be displayed if the user does not send any requests after this time. Therefore, delayed impressions of ads is a typical phenomenon in the ad delivery procedure that exists preloading strategy. https://doi.org/10.1145/3580305.3599900 KDD '23, August 6-10, 2023, Long Beach, CA, USA Penghui Wei, Yongqiang Chen, Shaoguo Liu, Liang Wang, & Bo Zheng To ensure the effectiveness of ad delivery, in a whole delivery period (e.g., one day), advertisers usually have two types of demands: 1) The first is about impression count . Advertisers desire a certain impression count for an ad to reach target audiences. Advertising platforms need to guarantee that the actual impression count is as close as possible to this desired value, and also avoid over-delivery which means that the actual impression count is much larger than the guaranteed one. Besides, it is also desirable that all impressions are evenly distributed to all time windows of a period, in other words we seek the smoothness of the overall delivery. The advantage is that the ads can reach active audiences at different time periods. 2) The second demand is about delivery performance . Advertisers also expect that the delivery performance, evaluated by click-through rate (CTR) and post-click metrics like conversion rate, is as good as possible. This demand reflects that advertisers have a strong desire to obtain high-value traffic. From the perspective of advertising platforms, the delayed impression phenomenon brings challenges to optimize the above objectives. If the delivery procedure is not affected by publishers' strategies (i.e., after the advertising platform makes a selection decision to a request, the filled ad will be immediately displayed to the user), the above demands can be satisfied by traditional pacing algorithms [2, 5, 21]: we can adjust the selection probability at each time window (e.g., 5 minutes) based on the difference between actual impressions and expected impressions to satisfy impression count demand, and perform grouped adjusting [21] based on predicted CTR to satisfy performance demand. However, with preloading strategy, advertising platforms can only determine the selection of requests but the final impressions are controlled by publishers. Therefore, without immediate feedbacks, it is challenging for platforms to guarantee advertiser demands. In this paper, we focus on a new research problem of impression pacing for preloaded ads, aiming to guarantee that both impression count and delivery performance demands are satisfied. Formally, we formulate this problem as a sequential decision process. At each time window in a delivery period, the pacing algorithm produces a selection probability that determines whether to select a given traffic request in this window. The current selection probability should be affected by previous ones and also affects next ones, because the utility of the produced sequential decisions is evaluated after the whole delivery period is terminated. The problem of impression pacing for preloaded ads has two characteristics. 1) The feedback signal is typically delayed due to the delayed impression phenomenon, i.e., we observe whether the demands are satisfied until the end point of whole delivery period. 2) Besides, it is hard to exactly mimic or model the delay situation , because the publishers may change their strategy and advertising platforms are unaware of this. Such two characteristics inspire us to employ reinforcement learning, where the policy 1) is encouraged to obtain long-term cumulative rewards other than immediate ones, and 2) is learned via a trial-and-error interaction process , without the need of estimating environment dynamics. Motivated by the above considerations, we propose RLTP , a R einforcement L earning T o P ace framework. In RLTP , the combination of user and online publisher is regarded as the environment that provides states and rewards to train the pacing agent that sequentially produces selection probability at each decision step. To jointly optimize the objectives of impression count and delivery performance, the reward estimator in RLTP considers key aspects to guide the agent policy learning via satisfying the guaranteed impression count, maximizing the traffic value, and penalizing the over-delivery as well as the drastic change of selection probability. We conduct offline and online experiments on large-scale industrial datasets, and results verify that RLTP significantly outperforms well-designed pacing algorithms for preloaded ads. The contributions of this paper are: Â· To the best of our knowledge, this work is the first attempt that focuses on impression pacing algorithms for preloaded ads, which is very challenging for advertising platforms due to the delayed impression phenomenon. Â· To jointly optimize both objectives (impression count and delivery performance) for preloaded ads, we propose a reinforcement learning to pace framework RLTP for delayed impression modeling. We design tailored reward estimator to satisfy impression count, penalize over-delivery and maximize traffic value. Â· Experiments on large-scale industrial datasets show that RLTP consistently outperforms compared algorithms. It has been deployed online in a large advertising platform, and results verify that RLTP achieves significant uplift to delivery completion rate and click-through rate.",
  "2 PRELIMINARIES": "We start from the formulation of impression pacing problem for preloaded ads. We then introduce a vanilla solution based on PID controller to tackle the problem, and show its limitations.",
  "2.1 Problem Formulation": "2.1.1 Objective . For a whole delivery period, let ğ‘ target denote the desired impression count (set by the advertiser). We assume that the number of traffic requests ğ‘ req from publisher is sufficient:  however the value of ğ‘ req is unknown before delivery. During ad delivery, advertisers have two types of demands that advertising platforms need to guarantee: Â· Impression count : for a whole delivery period, the total actual impression count ğ‘ imp meets the condition of ğ‘ imp â‰¤ ğ‘ target + ğœ– , the closer the better:  where ğœ– â‰¥ 0 is a tolerance value of over-delivery, and usually very small. Serious over-delivery (i.e., ğ‘ imp > ğ‘ target + ğœ– ) is unacceptable, which results in reimbursement. Besides, the delivery process should be smoothness , which means that all impressions are desirable to be as evenly as possible distributed over the time windows of the period. Â· Delivery performance : In this work, we employ CTR as the metric for delivery performance. We aim to maximize the overall performance and ensure that the ads are displayed to high-value traffic. RLTP: Reinforcement Learning to Pace for Delayed Impression Modeling in Preloaded Ads KDD '23, August 6-10, 2023, Long Beach, CA, USA Time Windows in a Whole Delivery Period All Traffic N? N288 Requests req req (User) Select Traffic Fill-in Ad Nreq 0288 (Advertising Platform) Actual Impression (Publisher) Figure 2: (Better viewed in color) Illustration of delayed impression phenomenon under the preloading strategy controlled by publisher. At each time window ğ‘‡ ğ‘– , the actual impression ads may come from the filled ads at current ğ‘‡ ğ‘– and previous windows { ğ‘‡ 1 , ğ‘‡ 2 , . . . , ğ‘‡ ğ‘– -1 } . 2.1.2 Delayed Impression in Preloaded Ads . Due to the publisher's preloading strategy, although the advertising platform determines to select a given traffic and then fills in an ad, its impression time is usually at one of the next requests sent by the same user, or it even may not be displayed if the user does not send any requests after this time in the whole delivery period. We call this phenomenon as delayed impression . Figure 2 gives an illustration of delayed impression in a whole delivery period. Consider that the whole delivery period is divided into a number of equal-length time windows { ğ‘‡ 1 , ğ‘‡ 2 , . . . , ğ‘‡ ğ¿ } , where the interval length of each window is denoted as Î” ğ‘‡ . At each time window ğ‘‡ ğ‘– 's start point, the pacing algorithm of our advertising platform need to produce a selection probability ğ‘ ğ‘– âˆˆ [ 0 , 1 ] that determines whether to select a given traffic request in this time window, and the goal is to satisfy advertiser demands of impression count and delivery performance. Consider the ğ‘– -th time window ğ‘‡ ğ‘– , let ğ‘ ğ‘– req denote the number of total requests sent from users, and thus ğ‘ ğ‘– Â· ğ‘ ğ‘– req requests are selected by our advertising platform and then they will be filled in ads. However, under the preloading strategy from online publisher, the actual impression count of these filled ads can only be observed at the end point of the whole period due to the delayed impression phenomenon. The feedback signal for learning pacing algorithm is typically delayed. Formally, let ğ‘ ğ‘– imp denote the actual impression count of the selected ads in time window ğ‘‡ ğ‘– . As shown in Figure 2, these impressions are scattered over the time windows { ğ‘‡ ğ‘– , ğ‘‡ ğ‘– + 1 , . . . , ğ‘‡ ğ¿ } :  here we use delayed factor ğ›½ ğ‘— to denote the proportion between the impression count in time window ğ‘‡ ğ‘— and total impression count ğ‘ ğ‘– imp . At the end of time window ğ‘‡ ğ‘– , we can only observe impressions ğ›½ ğ‘– ğ‘ ğ‘– imp . Online publishers control the delayed factors and change them according to traffic dynamics and marketing goals. From the perspective of advertising platforms, it is hard to exactly mimic or model the delay situation (i.e., delayed factors), because they are unaware of such change from publishers in realtime. Therefore, it is very challenging for advertising platforms to satisfy both impression count and delivery performance demands for preloaded ads. 2.1.3 Impression Pacing for Preloaded Ads . We formulate the impression pacing problem for preloaded ads as a Markov decision process (MDP), which can be represented as a five-tuple (S , A , P , R , ğ›¾ ) : Â· S is the state space. For the time window ğ‘‡ ğ‘– , the state ğ’” ğ‘– reflects the delivery status after windows { ğ‘‡ 1 , . . . , ğ‘‡ ğ‘– -1 } . Â· A is the action space. The action ğ‘ ğ‘– at time window ğ‘‡ ğ‘– reflects the selection probability. Â· P : S Ã— A Ã— S â†’ [ 0 , 1 ] is the transition function that describes the probability ğ‘ ( ğ’” â€² | ğ’” , ğ‘ ) of transforming from state ğ’” to another state ğ’” â€² after taking the action ğ‘ . Â· R : SÃ—A â†’ R is the reward function, where ğ‘Ÿ ( ğ’” , ğ‘ ) denotes the reward value of taking the action ğ‘ at the state ğ’” . To achieve the delivery objective of advertisers, the reward ğ‘Ÿ () should reflect the delivery completeness, smoothness and performance. Â· ğ›¾ âˆˆ [ 0 , 1 ] is the discount factor that balances the importance of immediate reward and future reward. Note that we can also define a cost function C : S Ã— A â†’ R that penalizes over-delivery (the constraint in Equation 2), and reformulate the problem as a constrained Markov decision process [3]. For simplicity, we employ Lagrange function that transforms it to an unconstrained form. Given the above MDP formulation of impression pacing problem for preloaded ads, the optimization objective is to learn an impression pacing policy ğœ‹ : S â†’ A via maximizing the expected cumulative reward:  The key is to design reward estimator which satisfies both impression count and delivery performance demands (Section 2.1.1). Without loss of generality, in our work let 1-day or 1-week be the whole delivery period, and 5-minutes be the interval Î” ğ‘‡ of each time window. In the situation of 1-day delivery period, the pacing policy need to produce sequential selection probabilities at 0:00, 0:05, ..., 23:50 and 23:55, containing ğ¿ = 288 decisions.",
  "2.2 Vanilla Solution and its Limitations": "Traditional pacing algorithms in online advertising [1, 2, 5, 7, 14, 21] focus on the budget pacing problem, aiming to spend budget smoothly over time for reaching a range of target audiences, and various algorithms are proposed. However they cannot be directly adopted to our problem, because in their algorithms the feedback signals used for controlling are immediate , while in our problem the impression signals can only be observed at the end of the delivery period due to the preloading strategy. To our knowledge, there is no existing studies that can tackle the impression pacing problem with delayed impressions of preloaded ads. KDD '23, August 6-10, 2023, Long Beach, CA, USA Penghui Wei, Yongqiang Chen, Shaoguo Liu, Liang Wang, & Bo Zheng 2.2.1 Vanilla Solution: Prediction-then-Adjusting . To tackle the delayed impression problem, we introduce a vanilla pacing algorithm modified from [10]. It contains two stages: 1) prediction stage, which employs a deep neural network (DNN) to predict actual impression count at current time window, and 2) adjusting stage, which employs a proportional-integral-derivative (PID) controllerr [4] to adjust selection probability based on the difference of desired impression count and predicted impression count. This is the current version of our production system's pacing policy. The motivation is to estimate 'immediate' feedbacks for controlling current delivery via employing historial delivery data. Specifically, at time window ğ‘‡ ğ‘– 's start point, the first stage uses historial delivery data to learn a prediction model that mapping observed impression count to actual impression count Ë† ğ‘ ğ‘– imp for each time window. The estimated actual values are used as 'immediate' feedbacks for further controlling. The model input contains statistical information from previous windows such as observed impression count, completion rate and averaged selection probability. An instantiation of the prediction model is a deep neural network [11]. Then the second stage is a PID controller, which adjusts the initial selection probability at each time window based on the difference between the estimated actual impression rate Ë† ğ‘ ğ‘– imp / ğ‘ target and expected impression rate e ğ‘ ğ‘– target / ğ‘ target to sastify impression count demand. Here, the expected count e ğ‘ ğ‘– target for each window is given in advance based on prior knowledge, such as the desired delivery speed distribution over the current delivery, and previous statistics from historical delivery logs. Formally, the adjusted selection probability ğ‘ â€² ğ‘– is computed as:   where ğ¾ P , ğ¾ I and ğ¾ D are non-negative hyperparameters. To further satisfy delivery performance demand, inspired by [21], the PID controller maintains multiple groups of { ğ¾ P , ğ¾ I , ğ¾ D } , where each group represents different performance levels. Each request falls into a specific group based on a CTR prediction model. The intuition is that if a request has higher predicted CTR, the adjusted selection probability should also be higher. 2.2.2 Limitations . The above solution has advantages such as simple and easy-to-deployment. It also has unavoidable limitations: 1) The first stage estimates 'immediate' feedbacks for current delivery based on historial processes, which inevitably brings estimation bias because publishers may change delayed strategy. 2) The second stage requires the expected counts of all time windows for controlling. The values are static, and computed based on the statistics from historical data. However, the timeliness is weak because the static values are unaware of real-time responses from users and publishers during the current delivery period. Besides, there are many hyperparameters that are hard to set. 3) Finally, the two stages may not adapt to each other very well, because they work independently and thus it is hard to obtain optimal pacing performance. Figure 3: Overview of our reinforcement learning to pace framework RLTP for impression pacing on preloaded ads. State s",
  "3 PROPOSED APPROACH: REINFORCEMENT LEARNING TO PACE": "During the ad delivery procedure, to handle the delayed impression phenomenon under preloading strategy for satisfying advertiser demands, we employ reinforcement learning (RL) as the paradigm of our pacing algorithm. The advantage is that the pacing policy is learned through a trial-and-error interaction process guided by elaborate reward estimator and optimizes long-term value, which tackles the delayed feedback issue and does not need to exactly mimic the delay situation.",
  "3.1 Overview of Pacing Agent": "We propose a R einforcement L earning T o P ace framework RLTP , which learns a pacing agent that sequentially produces selection probability at each decision step. The ad delivery system is regarded as the environment, and the policy of RLTP is the agent. At each episode of ğ¿ -step decisions for time windows { ğ‘‡ 1 , ğ‘‡ 2 , . . . , ğ‘‡ ğ¿ } , let ğœ = ( ğ’” 1 , ğ‘ 1 , ğ‘Ÿ 1 , . . . , ğ’” ğ¿ , ğ‘ ğ¿ , ğ‘Ÿ ğ¿ ) denote the trajectory of interaction, where each state is observed at the start point of the time window, while each reward is received at the end point of the window. The environment provides states ğ’” 1: ğ¿ and rewards ğ‘Ÿ 1: ğ¿ to train the pacing agent. The agent makes sequential selection decisions ğ‘ 1: ğ¿ based on current policy ğœ‹ , and interacts with the environment for further improving the utility of policy. Figure 3 gives an overview of our proposed RLTP framework. The network architecture is based on Dueling DQN [20], which approximates state-action value function via two separate deep neural networks. To jointly optimize the two objectives of impression count and delivery performance, RLTP employs tailored reward estimator which considers four key aspects to guide the agent policy learning: satisfying the guaranteed impression count, encouraging smoothness, maximizing the traffic value, and penalizing the overdelivery as well as the drastic change of selection probability. To effectively fuse multiple reward values to a unified scalar, we further parameterize the fusion weights and learns them with network parameters together. RLTP: Reinforcement Learning to Pace for Delayed Impression Modeling in Preloaded Ads KDD '23, August 6-10, 2023, Long Beach, CA, USA We describe the basic elements in the MDP, including state representation (Section 3.2), action space (Section 3.3) and reward estimator (Section 3.4). We then introduce how to approximate the state-action value function with deep neural networks and the optimization of network parameters (Section 3.5).",
  "3.2 State Representation: Delivery Status": "At each time window ğ‘‡ ğ‘– 's start point, the state representation ğ’” ğ‘– âˆˆ S reflects the delivery status after windows { ğ‘‡ 1 , . . . , ğ‘‡ ğ‘– -1 } . Specifically, we design the state representation ğ’” ğ‘– that consists of statistical features, user features, ad features and context features: Â· Statistical features : describing delivery status. Feature list is as follows: -Observed impression count sequence of previous ğ‘– -1 time windows: { ğ‘ 1 , . . . , ğ‘ ğ‘– -1 } -Cumulatively observed impression count of previous windows: ğ‘ 1: ğ‘– -1 = Ë ğ‘– -1 ğ‘¡ = 1 ğ‘ ğ‘¡ -Current delivery completion rate: ğ· ğ‘– -1 = ğ‘ 1: ğ‘– -1 / ğ‘ target -Delivery completion rate sequence of previous ğ‘– -1 time windows: { ğ· 1 , . . . , ğ· ğ‘– -1 } - The difference of completion rates between two adjacent windows: Î” ğ‘– -1 = ( ğ‘ 1: ğ‘– -1 -ğ‘ 1: ğ‘– -2 ) / ğ‘ target - Averaged CTR of previous ğ‘– -1 time windows: ğ¶ğ‘‡ğ‘… 1: ğ‘– -1 = Ë ğ‘– -1 ğ‘¡ = 1 ğ¶ ğ‘¡ / Ë ğ‘– -1 ğ‘¡ = 1 ğ‘ ğ‘¡ , where ğ¶ âˆ— denotes click count. -CTR sequence of previous windows: { ğ¶ğ‘‡ğ‘… 1 , . . . , ğ¶ğ‘‡ ğ‘… ğ‘– -1 } - Averaged selection probability of previous ğ‘– - 1 time win- dows: ğ‘ 1: 1 ğ‘– - 1 ğ‘– - 1 = Ë ğ‘– - ğ‘¡ = ğ‘ ğ‘¡ Â· Context features : describing context information of traffic requests, including decision time (the values of week, hour and minute). 1 1 Â· User and Ad features : describing user profile and ad information. They contribute to maximize the selected traffic value, but also make state space extremely large. In practice, we use pretrained user and ad embeddings and they are not updated during agent learning. See Appendix A.2 for details. We consider window ğ‘‡ ğ‘– -1's averaged user/ad embedding. We perform discretization to continuous features and transform their values to IDs. Thus for both discrete and continuous features, we represent them as embeddings. 1 The state representation ğ’” ğ‘– is the concatenation of all features' embeddings.",
  "3.3 Action Space": "At the time window ğ‘‡ ğ‘– , given the state representation ğ’” ğ‘– , the pacing agent need to produce a selection probability ğ‘ ğ‘– âˆˆ [ 0 , 1 ] as an action for all traffic requests in this time window. If the total number of requests in window ğ‘‡ ğ‘– is ğ‘ ğ‘– req , there will be ğ‘ ğ‘– Â· ğ‘ ğ‘– req requests that are selected by the pacing agent to fill-in ads. For ease of handling, we choose discrete action space for producing selection probability at each decision time. Specifically, the action space of selection probability ğ‘ ğ‘– is defined with a step size of 0.02: A = { 0 , 0 . 02 , 0 . 04 , . . . , 0 . 98 , 1 . 0 } . In practice we found that the above discrete space is enough to obtain ideal pacing performance. Note that in experiments, we compare the discrete action space with continuous action space, see Section 4.2.3 for analysis. 1 For sequence feature, we represent it as the mean-pooling of its elements' embeddings.",
  "3.4 Tailored Reward Estimator": "The core of our RLTP framework is the reward estimator, which guides the learning of the pacing agent under the delayed impression situation. At the time window ğ‘‡ ğ‘– 's start point, the agent receives a state representation ğ’” ğ‘– and takes an action ğ‘ ğ‘– produced by its current policy. At ğ‘‡ ğ‘– 's end point (and also ğ‘‡ ğ‘– + 1's start point), the agent receives a reward ğ‘Ÿ ğ‘– that reflects the delivery completeness, smoothness and performance, and the state transforms to ğ’” ğ‘– + 1. We design tailored reward estimator to jointly optimize the two objectives of impression count and delivery performance. Formally, wedecomposethe reward to four terms: 1) satisfying the guaranteed impression count, 2) penalizing the over-delivery, 3) penalizing the drastic change of selection probability, and 4) maximizing the value of selected traffic. 3.4.1 Satisfying Guaranteed Impression Count . The first term aims to encourage that the actual impression count is as closer as possible to the desired value ğ‘ target . At the end point of window ğ‘‡ ğ‘– , let ğ‘ 1: ğ‘– be the cumulatively observed impression count of previous windows, and we set the reward term as:  In this way, if the final impression count is less than the demand ğ‘ target , the pacing agent will still receive positive reward because over-delivery is not happened. 3.4.2 Penalizing Over-Delivery . The second term aims to avoid over-delivery (i.e., ğ‘ imp > ğ‘ target + ğœ– ). At the end point of window ğ‘‡ ğ‘– , we set the second reward term as:  If the observed impression count ğ‘ 1: ğ‘– is larger than ğ‘ target + ğœ– , this term returns a large negative value to illustrate that the agent's previous decisions are fail to guarantee the impression count demand and result in over-delivery. 3.4.3 Penalizing Drastic Change of Selection Probability . The third term is for delivery smoothness to reach active audiences at different time periods. We measure the difference of observed impression counts between adjacent two windows to produce:  ï£³ where c > 0 is a small constant. If the difference is less than c, this term returns a positive value, and the pacing agent is encouraged to smoothly select traffic during the whole delivery period. 3.4.4 Maximizing Selected Traffic Value . The fourth term aims to encourage that the pacing agent can bring good delivery performance. At the end point of window ğ‘‡ ğ‘– , let ğ¶ 1: ğ‘– be the cumulatively click count of previous windows, and we set the reward term as:  where ğ¶ğ‘‡ğ‘… base is a predefined CTR that the policy should beat. KDD '23, August 6-10, 2023, Long Beach, CA, USA Penghui Wei, Yongqiang Chen, Shaoguo Liu, Liang Wang, & Bo Zheng Finally, the reward ğ‘Ÿ ğ‘– for each decision step is the weighted sum of the above four terms:  where ğœ‚ âˆ— is the weight factor. Next section we will detail how to obtain the factors.",
  "3.5 Network Architecture and Optimization": "We adopt Dueling DQN [20] as the network architecture of RLTP , which is a modified version of DQN [18]. Further, to effective fuse the reward terms, we parameterize the weight factors.",
  "3.5.1 Approximate Value Functions with Neural Networks .": "Let ğ‘… ğ‘– def = Ë ğ¿ ğ‘— = ğ‘– ğ›¾ ğ‘— -ğ‘– ğ‘Ÿ ğ‘— be the discounted cumulative reward of decision step ğ‘‡ ğ‘– . For the pacing policy ğœ‹ , a Q-function ğ‘„ ğœ‹ ( ğ’” , ğ‘ ) and a value function ğ‘‰ ğœ‹ ( ğ’” ) represent the long-term values of the state-action pair ( ğ’” , ğ‘ ) and the state ğ’” respectively:  The difference of them represents each action's quality, which is defined as the advantage function ğ´ ğœ‹ ( ğ’” , ğ‘ ) :  Due to the large space of state-action pairs, Dueling DQN employs two separate deep neural networks ğ‘‰ Î˜ (Â·) and ğ´ Î¦ (Â·) to approximate ğ‘‰ ğœ‹ ( ğ’” ) and ğ´ ğœ‹ ( ğ’” , ğ‘ ) respectively:  where ğ‘‰ Î˜ (Â·) and ğ´ Î¦ (Â·) are a stack of fully-connected layers. Î˜ and Î¦ denote learnable parameters. State and action are represented as embeddings to be fed in the networks. The approximated Q-function and the pacing policy are:  3.5.2 Learning Reward Fusion Factors ğœ‚ . The reward estimator contains multiple terms (Equation 10) and each has a factor ğœ‚ âˆ— . To obtain appropriate factors for reward fusion, we parameterize the factors as a trainable vector ğœ¼ = ( ğœ‚ 1 , ğœ‚ 2 , ğœ‚ 3 , ğœ‚ 4 ) âŠ¤ , and feed it into the neural networks to learn the factors [13]:  3.5.3 Optimization . The pacing agent interacts with the environment to collect trajectories as training data (replay buffer). The pacing policy is optimized via the following objective by gradient descent, which takes a batch of ( ğ’” ğ‘– , ğ‘ ğ‘– , ğ‘Ÿ ğ‘– , ğ’” ğ‘– + 1 ) sampled from replay buffer at each iteration:  Note that in real world applications, it is impossible that the pacing policy directly interacts with production system to perform online training. We design a simulator as the environment to perform offline training. See Section 4.1.1 for details.",
  "4 EXPERIMENTS": "We conduct offline and online experiments on real world datasets, and aim to answer the following research questions: Â· RQ1 (effectiveness): Does our RLTP outperform competitors algorithms of impression pacing for preloaded ads? Â· RQ2 (impression count demand): Does the learned policy of RLTP satisfy the impression count demand of advertisers, as well as the smoothness requirement during ad delivery? Â· RQ3 (delivery performance demand): Does the learned policy of RLTP select high-value traffic during ad delivery? Â· RQ4 (online results): Does our RLTP achieve improvements on core metrics in industrial advertising platform?",
  "4.1 Experimental Setup": "In real world advertising platforms, deploying a random policy online then directly performing interaction with production systems to train the policy is almost impossible, because the optimization of an RL agent need huge numbers of samples collected from the interaction process, and this will hurt user experiences and waste advertisers' budgets. Therefore, we train an offline simulator using the historial log collected from a behavior policy (i.e., the current pacing algorithm of our production system), and the training and offline evaluation processes of RLTP are based on the interaction with the simulator. 4.1.1 Offline Datasets and Simulator . The historial log comes from a one-week delivery process on our advertising platform, which is collected by the pacing algorithm described in Section 2.2. The log records the following information: (1) timestamps of traffic requests (2) selection decisions of behavior policy (3) timestamps of actual impressions (4) user feedbacks (i.e., click or not) (5) meta-data of user and ad profiles. The number of total requests in the historial log is 8.85 million , and the actual impression count is 1.12 million . We regard each day as one delivery period, and the pacing policy need to produce sequential selection probabilities at 0:00, 0:05, ..., 23:50 and 23:55, containing 288 decisions . We utilize the historial log to build our offline simulator that is used for learning and evaluating the pacing agent. We preprocess this log to the format of quads (state, action, reward, next state), see Section 3.2, 3.3 and 3.4 for the definitions of such elements. The simulator is built via learning a state transition model Ë† P( ğ’” , ğ‘ )â†’ ğ’” â€² and a reward model Ë† R( ğ’” , ğ‘ ) â†’ ğ‘Ÿ . Based on the preprocessed historial log, we can obtain a large size of triplets ( ğ’” ğ‘– , ğ‘ ğ‘– , ğ’” ğ‘– + 1 ) and ( ğ’” ğ‘– , ğ‘ ğ‘– , ğ‘Ÿ ğ‘– ) and then use them to train models for simulation. In practice, recall that the statistical features of state representation (see Section 3.2) and the final form of reward (see Section 3.4) are the combination from observed impression count ğ‘ âˆ— and click count ğ¶ âˆ— , thus we simplify the simulator learning via estimating them in state ğ’” ğ‘– + 1 given those in ğ’” ğ‘– and action ğ‘ ğ‘– , The model architecture is a neural network, and it has multi-head outputs for producing different features (i.e., observed impression count and click count). The mean squared error loss function is used to optimize continuous output. See Appendix A.1 for more details. RLTP: Reinforcement Learning to Pace for Delayed Impression Modeling in Preloaded Ads KDD '23, August 6-10, 2023, Long Beach, CA, USA Table 1: Offline experimental results of competitors. 4.1.2 Evaluation Metrics . For offline experiments, we measure different aspects of the pacing agent to evaluate the learned policy: Â· Delivery completion rate : the proportion of actual impression count to the desired impression count after the whole delivery period, which should be close to 100%. Â· Click-through rate (CTR) : the ratio of click count to actual impression count after the whole delivery period. It reflects the traffic value and delivery performance. Â· Cumulative reward : it directly measures the final utility of the learned pacing agent. 4.1.3 Implementation Details . In offline experiments, we set the desired impression count ğ‘ target as 0.15 million , and ğœ– is 10% of it. We duplicately run 3 times and report the averaged metric. See Appendix A.2 for more details.",
  "4.2 RQ1: Comparative Results": "4.2.1 Comparative Algorithms . We compare the RLTP to welldesigned baselines for preloaded ads impression pacing. In traditional pacing algorithms, the feedback signals used for controlling need to be immediate , while in our problem the impression signals can only be observed at the end of the delivery period due to the preloading strategy. Thus, existing pacing algorithms cannot be directly adopted to tackle delayed impression without modification. We design two baselines to impression pacing for preloaded ads. Â· Statistical Rule-based Adjusting . Based on historial delivery logs, we statistically summarize the mapping between observed impression count and actual count for each time window. This mapping is utilized for next delivery: At each time window, we use the observed count to lookup the mapping that transforms it to actual count (but it may not be accurate). If it is near to the desired count, we directly set the selection probability as zero to avoid over-delivery. Â· Prediction-then-Adjusting , modified from the state-ofthe-art framework [10] (Section 2.2). It is a two-stage solution that contains an actual impression count prediction model and a PID controller. The first stage estimates potential delayed count as immediate feedback signals for the controlling of the second stage. Prediction-then-Adjusting is the current policy of our production system and now serving the main traffic, thus it is a strong baseline. To compare continuous and discrete action spaces, we also introduce a variant of RLTP as a baseline: Â· RLTP-continuous . We modify the network architecture from the Dueling DQN to proximal policy optimization (PPO) [19] for adapting continuous action space, which learns to produce selection probability directly using a policy function ğœ‹ ( ğ‘ | ğ’” ) . Note that learning factors cannot be applied to it. Figure 4: For each competitor, the trend of delivery completion rate during all 288 time windows in the delivery period. RLTP StatRule-based-Adjust RLTP-Episode-0 Pred-then-Adjust 1 1 Time Window (b) Delivery completion rate. (a) Cumulative reward. 5000 1 4000 1 3000 2000 M 1000 Episode Episode Figure 5: The curves of cumulative reward and delivery completion rate during training (30,000 episodes). 4.2.2 Comparative Results . Table 1 shows the results of comparative algorithms, where the two baselines' performances are also evaluated by the simulator. We observe that RLTP achieves the best results on all three metrics, verifying its effectiveness to preloaded ads pacing. The main limitation of two baselines is that both of them rely on statistics from historical delivery procedures. This results in that their timeliness is problematic when the traffic distribution has changed compared to historical statistics, restricting their pacing performances. Although the Prediction-then-Adjusting algorithm considers real-time responses in the first stage's prediction, it still employs a prior expected impression count as the goal for controlling. In contrast, the pacing agent of RLTP is aware of real-time delivery status via well-designed state representation, avoiding over-delivery and selecting high-value traffic effectively. For each of the two baselines and RLTP , Figure 4 illustrates the trend of completion rate at all 288 time windows in the whole delivery period during evaluation. Compared to RLTP , the Stat. Rule-based Adjust. algorithm results in serious over-delivery, and the completion rate of Pred.-then-Adjust. is also not reasonable. Figure 5 shows the curves of cumulative reward and completion rate during RLTP 's training process. At the beginning of training, the pacing agent brings serious over-delivery (i.e., the completion rate is larger than 300%). As the increasing of training episode, the cumulative reward of the agent continuously gains and the delivery rate tends to coverage and finally becomes around 100%. This demonstrates that the reward estimator effectively guides the agent policy learning to meet the demand of impression count. 4.2.3 CompareContinuousandDiscreteActionSpaces . From Table 1, we observe that the RLTP based on discrete action space performs slightly better than the RLTP-continuous . Furthermore, KDD '23, August 6-10, 2023, Long Beach, CA, USA Penghui Wei, Yongqiang Chen, Shaoguo Liu, Liang Wang, & Bo Zheng Figure 6: At different training episodes, the trends of cumulative reward at all 288 decision steps. Episode-2e4 6000 Episode=3e4 Episode = 5e4 Episode-0 5000 Episode-0.5e4 1 4000 3000 1 2000 1000 Decision Step compared to RLTP-continuous , we observe that RLTP has following advantages: 1) The number of training episodes is far less than that of RLTP-continuous , thus its training efficiency is better, and 2) the consistency between policy's selection probability and environment's traffic CTR is also more reasonable. Based on the above reasons, we choose RLTP based on discrete action space as the final version. Detailed illustrations are shown in Appendix B.",
  "4.3 RQ2: Analysis on Impression Count and Smoothness Demands": "Weshowhowthelearnedpacingagentof RLTP avoids over-delivery to guarantee the impression count demand. We then analyze how it performs on smoothness during the whole delivery period. 4.3.1 Avoiding Over-Delivery . Figure 6 shows the trend of cumulative reward within an episode (288 decision steps). We observe that, at early episodes, the cumulative reward drops drastically after 100 decision steps, which means that there exists over-delivery (the reward term ğ‘Ÿ ( 2 ) brings a large negative value). At later episodes, the cumulative reward always grows with decision step, verifying that the pacing agent gradually learns ideal policy that avoids over-delivery via the trial-and-error interaction with environment. 4.3.2 Delivery Smoothness . According to the slope rate of each trend line in Figure 4, we observe that our RLTP 's pacing agent makes most of ad impressions evenly distributed to 288 time windows and thus performs smooth delivery. However the two baselines do not guarantee the smoothness well. This shows that the learned policy of RLTP can reach active audiences at diverse time periods to improve ads popularity.",
  "4.4 RQ3: Analysis on Delivery Performance": "In this section, we turn to analyze how the pacing agent of RLTP achieves good delivery performance. 4.4.1 Effect of Traffic Value Modeling . To verify the effect of maximizing selected traffic value during policy, Figure 7 (a) compares the CTR of selected traffic by two pacing agents: the one is our agent that trained using our reward estimator, and the other's training process does not consider the fourth reward term ğ‘Ÿ ( 4 ) that maximizes the selected traffic value. We observe that without modeling traffic value during policy learning, the agent misses potential high-value traffic. This verifies that explicitly incorporating traffic value reward is effective to help improve delivery performance. RLTP-wlo-r4 RLTP 0.07 0.06 004 0.03 Time Window RLTP RLTP w/o r4 CTR of Env. 0.085 0,080 1 0-160 1 0.075 0 125 0,070 0075 0.065 Time Window (a) The trend of CTR. (b) The consistency of selection probability w.r.t. traffic CTR of historical log. Figure 7: Effects of maximizing traffic value.",
  "4.4.2 Correlation between Policy's Selection Probability and": "Environment's Traffic CTR . We analyze whether the learned pacing agent possesses the ability of dynamically adjusting selection probability based on environment's traffic value. Specifically, we first compute the traffic CTR of different time windows according to the historial log , which reflects the environment's traffic value trend. We expect that if a time window's traffic CTR is higher, our pacing can increase the selection probability to obtain high-value traffic as much as possible and improve delivery performance. Figure 7 (b) compares the trends of agent's selection probability and environment's traffic CTR. We can see that the consistency of RLTP 's agent and environment is better than the agent that does not consider the fourth reward term. Concretely, we highlight several decision period segments to illustrate the consistency differences of the two agents. This demonstrates that RLTP 's agent is sensitive to environment's traffic CTR and can dynamically adjust selection probability to achieve higher delivery performance.",
  "4.5 RQ4: Online Experiments": "In this section, we first introduce how to deploy the RLTP framework online to our brand advertising platform, and then report the online experimental results in production traffic. 4.5.1 Online Deployment . The online architecture of RLTP for impression pacing generally contains two parts. One is the pacing agent learning module that performs policy training, and the other is the online decision module that produces selection probability in real-time for ad delivery. I. For the pacing agent learning module, the offline training procedure is introduced in Section 3, which returns a learned policy ğœ‹ via the interaction process with the simulator. To deploy the pacing policy to production system, the key is that it should be adapt to new traffic distribution and user interests. Therefore, the pacing agent learning module is running in a cyclical fashion, in which fresh ad delivery logs are utilized to incrementally update the simulator model, and the update cycle is typically one day. After the simulator model is updated, we restore the pacing policy ğœ‹ and incrementally update its learnable parameters via small numbers of (e.g., hundreds of) episodes with the updated simulator. II. For the online decision module, it first loads the update-todate pacing policy that consists of state embedding tables and value functions, and also accesses the desired impression count ğ‘ target at RLTP: Reinforcement Learning to Pace for Delayed Impression Modeling in Preloaded Ads KDD '23, August 6-10, 2023, Long Beach, CA, USA User Publisher Advertiser User Behaviors (Impression/Click) Batched Logs Simulator Learning Policy Learning Request Advertising Platform Ad Profiles Desired Impression Count Model Export Fill-in Ad Real-time Statistics Pacing Agent Pretrained Embeddings Stat./Context Features User/Ad Features User/Ad ID Selection Probability Sync Online Decision User Publisher Advertiser User Behaviors (Impression/Click) Batched Logs Model Learning Request Ad Profiles Desired Impression Count Model Export Fill-in Ad Real-time Statistics Actual Count Prediction Pretrained Embeddings Stat./Context Features Adjusted Probability Sync Advertising Platform (a) RLTP (b) Prediction-then-Adjusting Figure 8: Online deployment for RLTP and Prediction-then-Adjusting . the start time of the current delivery period (typically at 0:00). Then it cyclically executes the following adjusting steps for sequentially producing selection probabilities (typically 5-minutes per cycle): Step1 Calculate the observed impression count from the start time to now. If it has been larger than the target count or the current time window is the last one, set the selection probability to zero immediately; otherwise, turn to Step 2. Step2 Collect the statistical features and context features (see Section 3.2) in real-time, and lookup averaged user and ad embeddings from database. Then feed these features to the loaded pacing policy's state embedding tables, and finally obtain the state representation ğ’” . Step3 Feed the state representation to the loaded pacing policy's value functions ğ‘‰ Î˜ (Â·) and ğ´ Î¦ (Â·) , and produce a set of action values { ğ‘„ ( ğ’” , ğ‘ )} ğ‘ âˆˆA . Then choose the action with the highest value as the selection probability ğ‘ . Step4 For all traffic requests sent by online publisher at this time window, select and fill-in our ad with probability ğ‘ . If a request is selected, send a key-value pair of (unique identification, impression=0) to our impression counter. The impression counter is in charge of constantly listening whether a filled ad is displayed on the publisher, and sets impression=1 after display. Then turn to Step 1. Figure 8 shows the comparison of online deployment for RLTP and Prediction-then-Adjusting . Compared to the two-stage solution, RLTP simplifies the online architecture to a unified agent. More importantly, unlike the Prediction-then-Adjusting which needs estimates actual impressions and sets 'hard' expected impression counts based on historical delivery procedures to guide the controlling for current delivery, RLTP does not rely on such estimation and setting. Instead, the pacing policy is learned to maximize long-term rewards via trail-and-error interaction process with the simulator, which performs relatively 'soft' controlling. For both algorithm design and system architecture, our RLTP framework takes a next step to the problem of impression pacing for preloaded ads. 4.5.2 Online A/B Test . We conduct online A/B test for one week on our brand advertising system. The base bucket employs the two-stage algorithm Prediction-then-Adjusting (the strongest algorithm for our production system currently), and the test bucket employs the pacing policy learned by our RLTP framework. The online evaluation metrics contain delivery completion rate and CTR, measuring whether the pacing algorithm guarantees impression count and delivery performance demands respectively. The online A/B test shows that the delivery completion rates of the base and test buckets are 118.4% and 111.6% respectively, and the test bucket brings the relative improvement of +2.6% on CTR. The results demonstrate that RLTP brings significant uplift on core online metrics and satisfies advertisers' demands better.",
  "5 RELATED WORK": "Generally, the problem of impression pacing for preloaded ads is related to three research fields. The first is pacing control algorithms [1, 2, 5, 7, 14, 21]. Existing studies focus on budget pacing that aims to spend advertiser budget smoothly over time. Representative approaches [2, 21] adjust the probability of participating in an auction to control the budget spending speed, and they rely on immediate feedbacks (e.g., win/lose a bid) for effective adjusting. The second is the allocation problem of guaranteed display advertising [6, 9, 10, 12, 16, 24, 25]. These work formulates a matching problem for maximizing the number of matched user requests (supply nodes) and ad contracts (demand nodes) on a bipartite graph, where the delivery probabilities are computed offline and then the delivery speed is controlled online by pacing algorithms. Although the impression count demand is considered in the matching problem, these studies can not tackle the delayed impression issue. The third is delayed feedback modeling [8, 17, 22, 23]. Existing work focuses on modeling users' delayed behavior, such as purchase. Such delay nature is caused by user inherent decision, other than online publishers. Thus the delayed pattern is essentially different to our problem that the delayed situation is mainly controlled by online publishers and also affected by delayed requests from users. Overall, current studies do not touch the impression pacing problem under preloading strategy from online publishers. Our work makes the first attempt and tackles this problem with a reinforcement learning framework.",
  "6 CONCLUSION": "We focus on a new research problem of impression pacing for preloaded ads, which is very challenging for advertising platforms due to the delayed impression phenomenon. To jointly optimize both impression count and delivery performance objectives, we propose a reinforcement learning to pace framework RLTP , which employs tailored reward estimator to satisfy impression count, penalize over-delivery and maximize traffic value. Experiments show that RLTP consistently outperforms compared algorithms. It has been deployed online in our advertising platform, achieving significant uplift to delivery completion rate and CTR. KDD '23, August 6-10, 2023, Long Beach, CA, USA Penghui Wei, Yongqiang Chen, Shaoguo Liu, Liang Wang, & Bo Zheng",
  "REFERENCES": "[1] Zoe Abrams, Ofer Mendelevitch, and John Tomlin. 2007. Optimal delivery of sponsored search advertisements subject to budget constraints. In Proceedings of EC . [2] Deepak Agarwal, Souvik Ghosh, Kai Wei, and Siyu You. 2014. Budget pacing for targeted online advertisements at Linkedin. In Proceedings of KDD . [3] Eitan Altman. 1999. Constrained Markov decision processes: stochastic modeling . Routledge. [4] Stuart Bennett. 1993. Development of the PID controller. IEEE Control Systems Magazine 13, 6 (1993). [5] Anand Bhalgat, Jon Feldman, and Vahab Mirrokni. 2012. Online allocation of display ads with smooth delivery. In Proceedings of KDD . [6] Vijay Bharadwaj, Peiji Chen, Wenjing Ma, Chandrashekhar Nagarajan, John Tomlin, Sergei Vassilvitskii, Erik Vee, and Jian Yang. 2012. SHALE: an efficient algorithm for allocation of guaranteed display advertising. In Proceedings of KDD . [7] Christian Borgs, Jennifer Chayes, Nicole Immorlica, Kamal Jain, Omid Etesami, and Mohammad Mahdian. 2007. Dynamics of bid optimization in online advertisement auctions. In Proceedings of WWW . [8] Olivier Chapelle. 2014. Modeling delayed feedback in display advertising. In Proceedings of KDD . [9] Peiji Chen, Wenjing Ma, Srinath Mandalapu, Chandrashekhar Nagarjan, Jayavel Shanmugasundaram, Sergei Vassilvitskii, Erik Vee, Manfai Yu, and Jason Zien. 2012. Ad serving using a compact allocation plan. In Proceedings of EC . [10] Xiao Cheng, Chuanren Liu, Liang Dai, Peng Zhang, Zhen Fang, and Zhonglin Zu. 2022. An Adaptive Unified Allocation Framework for Guaranteed Display Advertising. In Proceedings of WSDM . [11] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks for youtube recommendations. In Proceedings of RecSys . [12] Zhen Fang, Yang Li, Chuanren Liu, Wenxiang Zhu, Yu Zheng, and Wenjun Zhou. 2019. Large-Scale Personalized Delivery for Guaranteed Display Advertising with Real-Time Pacing. In Proceedings of ICDM . [13] Eli Friedman and Fred Fontaine. 2018. Generalizing Across Multi-Objective Reward Functions in Deep Reinforcement Learning. arXiv preprint arXiv:1809.06364 abs/1809.06364 (2018). [14] Sahin Cem Geyik, Luthfur Chowdhury, Florian Raudies, Wen Pu, and Jianqiang Shen. 2020. Impression Pacing for Jobs Marketplace at LinkedIn. In Proceedings of CIKM . [15] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation learning on large graphs. Proceedings of NeurIPS 30 (2017). [16] S. Ali Hojjat, John Turner, Suleyman Cetintas, and Jian Yang. 2014. Delivering Guaranteed Display Ads under Reach and Frequency Requirements. In Proceedings of AAAI . [17] Sofia Ira Ktena, Alykhan Tejani, Lucas Theis, Pranay Kumar Myana, Deepak Dilipkumar, Ferenc HuszÃ¡r, Steven Yoo, and Wenzhe Shi. 2019. Addressing delayed feedback for continuous training with neural networks in CTR prediction. In Proceedings of RecSys . [18] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. 2013. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602 (2013). [19] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 (2017). [20] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. 2016. Dueling network architectures for deep reinforcement learning. In Proceedings of ICML . PMLR. [21] Jian Xu, Kuang-chih Lee, Wentong Li, Hang Qi, and Quan Lu. 2015. Smart pacing for effective online ad campaign optimization. In Proceedings of KDD . [22] Jia-Qi Yang, Xiang Li, Shuguang Han, Tao Zhuang, De-Chuan Zhan, Xiaoyi Zeng, and Bin Tong. 2021. Capturing delayed feedback in conversion rate prediction via elapsed-time sampling. In Proceedings of AAAI , Vol. 35. [23] Shota Yasui, Gota Morishita, Fujita Komei, and Masashi Shibata. 2020. A feedback shift correction in predicting conversion rates under delayed feedback. In Proceedings of WWW . [24] Hong Zhang, Lan Zhang, Lan Xu, Xiaoyang Ma, Zhengtao Wu, Cong Tang, Wei Xu, and Yiguo Yang. 2020. A Request-level Guaranteed Delivery Advertising Planning: Forecasting and Allocation. In Proceedings of KDD . [25] Jia Zhang, Zheng Wang, Qian Li, Jialin Zhang, Yanyan Lan, Qiang Li, and Xiaoming Sun. 2017. Efficient Delivery Policy to Minimize User Traffic Consumption in Guaranteed Advertising. In Proceedings of AAAI .",
  "SUPPLEMENTARY MATERIAL": "We provide more implementation details and computation resource for reproducibility in Section A. We also show the comparison between RLTP-continuous and RLTP in Section B to justify why we choose discrete action space.",
  "A IMPLEMENTATION DETAILS": "",
  "A.1 Simulator Learning": "The offline data is the historial log collected from a one-week delivery process, containing 8.85 million requests and 1.12 million impressions. The function of the offline simulator is to provide next state and four reward terms given current state and sampled action. Recall that all statistical features of state and reward terms of reward estimator are the combination of the observed impression count and click count. Therefore, in practice we simplify the simulator learning via estimating the two values, and both next state and reward can be computed via the estimated values. Specifically, we organize the historial log to the form of ( inputs1 , inputs2 , outputs ) based on adjacent time windows: ( inputs1 = ğ‘– -th window's observed impression/click counts and user/ad features, inputs2 = ğ‘– -th window's selection probability, outputs = ( ğ‘– + 1 ) -th window's observed impression/click counts). The simulation model is a deep neural network that estimates outputs given inputs1 and inputs2 . Note that user/ad features are retrieved from pretrained embeddings and are not updated. After training, the simulation model estimates next windows impression/click counts to produce next state and four reward terms, thus it can be used to perform interaction with pacing agent.",
  "A.2 Policy Learning": "In our offline experiments, we set the desired impression count ğ‘ target as 0.15 million for one delivery period, and ğœ– is 10% of it. In RLTP , the user and ad features in state representations are represented using pretrained user and ad embeddings, which are not updated during agent learning. Specifically, our user and ad embeddings are produced by a graph neural network model [15], where the node set contains users and ads, and user-ad click behaviors are regarded as edges. The learned embeddings are stored in database and can be retrieved by the pacing agent. Both the value function ğ‘‰ Î˜ (Â·) and the advantage function ğ´ Î¦ (Â·) are three-layered neural networks, with output sizes of [200, 100, 1] and [200, 100, 50] respectively. Each state/context feature used for state representation is represented as a 4-dim embedding, and each candidate action id ğ‘ âˆˆ A also uses 4-dim embedding. The base CTR in the fourth reward term is set to the averaged CTR of historial log. The discount factor ğ›¾ in culumative reward formulation is set to 0.99. We train the pacing agent for around 30,000 episodes, and duplicately run 3 times to report the averaged metric in the Section 4.",
  "A.3 Computation Resource": "Both the simulator learning and the policy learning procedures are done on 8 Tesla V100 GPUs.",
  "B DETAILED COMPARISON OF DISCRETE AND CONTINUOUS ACTION SPACES": "In Section 4.2.3, we breifly introduce the advantages of RLTP (which employs discrete action space) compared to RLTP-continuous . Here we give concrete illustrations. First, we compare the training efficiency. We observe that the RLTP-continuous needs around 200,000 episodes to achieve ideal pacing policy as shown in Figure 9 (a), while RLTP only needs RLTP: Reinforcement Learning to Pace for Delayed Impression Modeling in Preloaded Ads KDD '23, August 6-10, 2023, Long Beach, CA, USA (a) Delivery completion rate during training (200,000 episodes). (b) Selection probability at all 288 steps. RLTP RLTP w/o r4 CTR of Env. RLTP-continuous RLTP-continuous 0.085 8 1 0,080 1 0.075 6 0.070 0.065 0.25 0 50 0.75 1.00 1 50 1.75 2.00 Episode Time Window Figure 9: Training efficiency and traffic value of RLTPcontinuous . around 30,000 episodes as in Figure 5. In industrial applications, training efficiency is a key factor for production system, thus we prefer RLTP for online deployment. Second, we show the delivery performance. From Table 1 we can see that CTR metric of RLTP and RLTP-continuous is similar. We further investigate the trend of selection probability. Figure 9 (b) illustrates the trend of selection probability of RLTP-continuous 's pacing agent, and we observe that the consistency between selection probability and environment's traffic CTR is not as good as RLTP shown in Figure 7 (b). The above results demonstrate that RLTP shows advantages compared to RLTP-continuous . Therefore we choose RLTP with discrete action space as the final version, and in future work we shall explore how to improve RLTP-continuous for production system.",
  "keywords_parsed": [
    "None"
  ]
}