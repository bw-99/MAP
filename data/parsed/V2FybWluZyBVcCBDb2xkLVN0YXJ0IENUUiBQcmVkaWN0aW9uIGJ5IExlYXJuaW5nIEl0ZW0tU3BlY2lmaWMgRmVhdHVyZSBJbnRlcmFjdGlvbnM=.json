{"Warming Up Cold-Start CTR Prediction by Learning Item-Specific Feature Interactions": "Yaqing Wang Baidu Inc. Baidu Research Beijing, China wangyaqing01@baidu.com Hongming Piao City University of Hong Kong Department of Computer Science Hong Kong SAR, China hpiao6-c@my.cityu.edu.hk Daxiang Dong Baidu Inc. Baidu AI Cloud Beijing, China dongdaxiang@baidu.com Quanming Yao Tsinghua University Department of Electronic Engineering Beijing, China qyaoaa@tsinghua.edu.cn Jingbo Zhou Baidu Inc. Baidu Research Beijing, China zhoujingbo@baidu.com", "ABSTRACT": "In recommendation systems, new items are continuously introduced, initially lacking interaction records but gradually accumulating them over time. Accurately predicting the click-through rate (CTR) for these items is crucial for enhancing both revenue and user experience. While existing methods focus on enhancing item ID embeddings for new items within general CTR models, they tend to adopt a global feature interaction approach, often overshadowing new items with sparse data by those with abundant interactions. Addressing this, our work introduces EmerG, a novel approach that warms up cold-start CTR prediction by learning item-specific feature interaction patterns. EmerG utilizes hypernetworks to generate an item-specific feature graph based on item characteristics, which is then processed by a Graph Neural Network (GNN). This GNNis specially tailored to provably capture feature interactions at any order through a customized message passing mechanism. We further design a meta learning strategy that optimizes parameters of hypernetworks and GNN across various item CTR prediction tasks, while only adjusting a minimal set of item-specific parameters within each task. This strategy effectively reduces the risk of overfitting when dealing with limited data. Extensive experiments on benchmark datasets validate that EmerG consistently performs the best given no, a few and sufficient instances of new items.", "CCS CONCEPTS": "\u00b7 Information systems \u2192 Recommender systems ; \u00b7 Computing methodologies \u2192 Supervised learning ; Neural networks . Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. KDD '24, August 25-29, 2024, Barcelona, Spain \u00a9 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0490-1/24/08...$15.00 https://doi.org/10.1145/3637528.3671784", "KEYWORDS": "Cold-Start Recommendation, Warm Up, Click-Through Rate Prediction, Few-Shot Learning, Hypernetworks, New Items", "ACMReference Format:": "Yaqing Wang, Hongming Piao, Daxiang Dong, Quanming Yao, and Jingbo Zhou. 2024. Warming Up Cold-Start CTR Prediction by Learning ItemSpecific Feature Interactions. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '24), August 2529, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https: //doi.org/10.1145/3637528.3671784", "1 INTRODUCTION": "The cold-start problem presents a significant challenge in recommender systems [25], particularly evident as new items transition from having no user interactions (termed as cold-start phase) to accumulating a few initial clicks (termed as warm-up phase) in the industry landscape. Deep learning models, renowned for their capability to capture complex feature interactions, have shown promise in improving click-through rate (CTR) predictions, a critical metric for assessing the likelihood of user engagement with various items (e.g., movies, commodities, music) [1, 5, 9, 36]. However, these models typically rely on extensive datasets to achieve optimal performance, a requirement that poses a limitation in cold-start and warm-up phases. With their substantial parameter size, these models struggle to adapt efficiently to these phases characterized by limited interaction records, thereby exacerbating the challenge of making accurate CTR predictions and updating models without incurring significant costs. Recent studies have focused on enhancing the initialization of item ID embeddings as a strategy to mitigate the item cold-start problem in recommender systems, which allows subsequent updates through gradient descent as interaction records become available in the warm-up phase [23, 24, 42, 45]. Then, they leverage general CTR backbones for further processing. However, they overlook a crucial aspect: the distinctiveness of feature interaction patterns across different users and items. This oversight limits the ability of these models to fully capture the nuanced dynamics of user-item interactions, potentially impacting the accuracy and effectiveness of CTR predictions in scenarios where personalized recommendations are crucial. For example, comparing high-priced luxury items with KDD '24, August 25-29, 2024, Barcelona, Spain Yaqing Wang, Hongming Piao, Daxiang Dong, Quanming Yao, and Jingbo Zhou low-priced daily necessities reveals distinct interaction patterns. For high-priced luxury items, the interaction between the item's price and the user's income level is pivotal. Specifically, the secondorder feature interaction <price, income> can be a determining factor in the user's willingness to purchase such items. As for lowpriced daily necessities, the impact of the user's income level on purchasing decisions diminishes. In these instances, other feature interactions, such as those between the user's age and the item's category, become relatively more important. This variation underscores the necessity of modeling item-specific feature interaction patterns. While existing works all learn a global feature interaction pattern between users and items, which overwhelms new items with a limited number of interaction records by old items with abundant interaction records. Recognizing the crucial role of feature interactions, we introduce EmerG to address CTR prediction of newly emerging items with incremental interaction data (from no interaction records to few and then abundant records) through the learning of item-specific feature graphs. Our contributions can be summarized as follows: \u00b7 We propose a unique method that emphasizes item-specific feature interactions, addressing the challenge of new item CTR prediction by reducing the overshadowing effect of older items with extensive data. Utilizing hypernetworks, we construct itemspecific feature graphs with nodes as features and edges as their interactions, capturing complex interaction patterns unique to each item. We use a graph neural network (GNN) with a customized message passing process designed to provably capture feature interactions at any orders, which can be combined into nuanced and accurate predictions. \u00b7 To mitigate overfitting given limited data, we adopt a metalearning strategy that optimizes parameters of hypernetworks and GNN across different item CTR prediction tasks with a few adjustments to item-specific parameters within each task. Besides, hypernetworks and GNN learned this way are expected to generalize to each task easily. \u00b7 We conduct extensive experiments on benchmark datasets and validate that EmerG performs the best for CTR prediction of emerging items. We also evaluate the performance given more training data, and find that EmerG consistently performs the best. Visualization of adjacency matrices which record item-specific feature graphs shows that EmerG can learn item-specific feature interactions properly.", "2 RELATED WORKS": "We briefly review four groups of methods relevant to CTR prediction of newly emerging items. A. General CTR Models. General CTR models are applied universally without prioritizing underrepresented items. They mainly focus on modeling complex feature interactions, which is crucial for enhancing CTR prediction accuracy [3]. Historical advancements in this area show a progression from simple first-order interactions captured by linear models like logistic regression [27] to second-order interactions modeled by Factorization Machines (FM) [26], and to high-order interactions addressed by higher-order FMs (HOFMs) [2]. Various deep-learning models then automate the learning of these complex patterns. Both Wide&Deep [5] and DeepFM [9] employ hybrid architectures to handle second-order and higher interactions. DIN [43] dynamically captures user interests through an attention mechanism that adapts to varying ad features. AutoInt [28] introduces a multi-head self-attention mechanism [30] to model high-order feature interactions. LorentzFM [40] explores feature interactions in hyperbolic space to minimize parameter size. AFN [6] converts the power of each feature into the coefficient to be learned. FinalMLP [21] uses two multi-layer perceptron (MLP) networks in parallel, and equips them with feature selection and interaction aggregation layers. FINAL [44] introduces a factorized interaction layer for exponential growth in feature interaction learning. Considering that feature interactions can be conceptualized as graphs with nodes representing user and item features, graph neural network (GNN) [15] are used. Fi-GNN [17] learns to generate feature graphs where the edges are established according to the similarity between feature embeddings. FIVES [39] learns a global feature graph where edges are established by differentiable search from large-scale CTR datasets, thus new items with limited data can be underrepresented. GMT [22] models the interactions among items, users, and their features into a large heterogeneous graph, then feeds the local neighborhood of the target user-item pair for prediction. However, these general CTR models, designed for extensive datasets, often struggle during the cold-start & warm-up phases due to their substantial parameter size, which can lead to overfitting when adapted for new items. In contrast, EmerG introduces a specialized GNN that operates on item-specific feature graphs, generated via hypernetworks learned from diverse CTR tasks, ensuring precise modeling of feature interactions for new items with minimal parameter adjustments. B. Methods for New Items without Interaction Records. Several methods specifically address the cold-start phase, where new items lack interaction records, while still maintaining model performance on older, established items. DropoutNet [31] trains neural networks with a dropout mechanism on input samples to infer missing data. Heater [46] employs a multi-gate mixture-of-experts approach to generate item embeddings. GAR [4] adopts an adversarial training strategy between a generator and a recommender to produce new item embeddings that mimic the distribution of old embeddings, deceiving the recommender systems. ALDI [12] learns to transfer the behavioral information of old items to new items. However, these methods do not consider the incorporation of incoming interaction records for new items and typically require re-training to accommodate the evolving interaction history of these items. C. Methods for New Items with A Few Interaction Records. In scenarios where only a few instances of new items are available, which correspond to warm-up phases, few-shot learning [35] present a natural solution. Few-shot learning targets at generalizing to new tasks with a few labeled samples, which has been applied to image classification [8], query intent recognition [34] and drug discovery [33, 38, 41]. For CTR prediction, existing works typically approach the problem as a \ud835\udc41 -way \ud835\udc3e -shot task, where each of the \ud835\udc41 new items is associated with \ud835\udc3e labeled instances, then utilize the classic gradient-based meta-learning strategy [8]. MeLU [16] leverages this strategy to selectively adapt model parameters for new items through gradient descents. MAMO [7] enhances adaptation by incorporating an external memory mechanism. MetaHIN [20] EmerG KDD '24, August 25-29, 2024, Barcelona, Spain utilizes heterogeneous information networks to exploit the rich semantic relationships between users and items. PAML [32] employs social relations to facilitate information sharing among similar users. More recent approaches have shifted from user-specific finetuning via gradient descent to amortization-based methods. These methods directly map user interaction histories to user-specific parameters, thus modulating the main network without iterative adjustments. TaNP [18] learns to modulate item-specific parameters based on item interaction records. ColdNAS [37] employs neural architecture search to optimize the modulation function and its application within the network. However, these methods struggle to handle new items that lack interaction records and cannot dynamically incorporate additional interaction records of new items as they become available. D. Methods for Emerging Items with Incremental Interaction Records. To mirror the industry's dynamic evolution of new items, progressing from no interaction records to few and then abundant records, models are developed to manage these transitions smoothly. Existing efforts primarily enhance item ID embeddings for general CTR backbones. MetaE [24] employs gradient-based meta-learning to train an embedding generator. MWUF [45] transforms unstable item ID embeddings into stable ones using meta networks. CVAR [42] decodes new item ID embeddings from a distribution over item side information, circumventing additional data processing. GME [23] leverages information from neighboring old items for new item ID embedding generation. However, these methods generally optimize initial item ID embeddings while maintaining a global feature interaction pattern, thus failing to capture the unique characteristics and interaction dynamics of these items. In contrast, our EmerG addresses new item CTR prediction by tailoring feature interactions to each item with the help of hypernetworks. By learning with item-specific feature interactions, the risk of overwhelming new items by old items with abundant data is alleviated and prediction accuracy is enhanced.", "3 PROBLEM FORMULATION": "Let V = { \ud835\udc63 \ud835\udc56 } denote a set of items where each item \ud835\udc63 \ud835\udc56 is associated with \ud835\udc41 \ud835\udc63 item features such as item ID, type and price. Similarly, let U = { \ud835\udc62 \ud835\udc57 } denote a set of users where each user \ud835\udc62 \ud835\udc57 is also associated with \ud835\udc41 \ud835\udc62 user features such as user ID, age and hometown. When a user \ud835\udc62 \ud835\udc57 clicks through an item \ud835\udc63 \ud835\udc56 , label \ud835\udc66 \ud835\udc56,\ud835\udc57 = 1. Otherwise, \ud835\udc66 \ud835\udc56,\ud835\udc57 = 0. During learning, the predictor is learned from a set of CTR prediction tasks T old = {T \ud835\udc56 } \ud835\udc41 \ud835\udc61 \ud835\udc61 = 1 sampled from old items, which can rapidly generalize to predict for tasks from new items that are unseen during training. Each task T \ud835\udc56 corresponds to an old item \ud835\udc63 \ud835\udc56 , with a training set S \ud835\udc56 = {( \ud835\udc63 \ud835\udc56 , \ud835\udc62 \ud835\udc57 , \ud835\udc66 \ud835\udc56,\ud835\udc57 )} \ud835\udc41 \ud835\udc60 \ud835\udc57 = 1 containing existing interaction histories associated with \ud835\udc63 \ud835\udc56 and a test set Q \ud835\udc56 = {( \ud835\udc63 \ud835\udc56 , \ud835\udc62 \ud835\udc57 , \ud835\udc66 \ud835\udc56,\ud835\udc57 )} \ud835\udc41 \ud835\udc5e \ud835\udc57 = 1 containing interactions to predict whether \ud835\udc62 \ud835\udc57 clicks through \ud835\udc63 \ud835\udc56 . \ud835\udc41 \ud835\udc60 and \ud835\udc41 \ud835\udc5e are the number of interactions in S \ud835\udc56 and Q \ud835\udc56 respectively. During testing, we consider CTR prediction for new items that start with no interaction records, then gradually gather a few, and eventually accumulate sufficient interaction records. Consider a task T \ud835\udc58 associated with a new item \ud835\udc63 \ud835\udc58 which is not considered during training, we handle three phases: \u00b7 Cold-start phase: No training set is provided. \u00b7 Warm-up phase: A training set S \ud835\udc58 containing a few interaction records of \ud835\udc63 \ud835\udc58 is given. There can be multiple warm-up phases where interaction records are gradually accumulated. \u00b7 Common phase: The training interaction records of \ud835\udc63 \ud835\udc58 are accumulated to be sufficient. For all three phases, the performance is evaluated on test set Q \ud835\udc58 .", "4 THE PROPOSED EMERG": "Aligning with the established understanding that feature interactions are crucial, we propose EmerG (Figure 1) to capture the uniqueness of items through their associated feature interaction patterns. We design two key components in EmerG: (ii) hypernetworks shared across different tasks to generate item-specific adjacent matrices encoding feature graphs; and (i) a GNN that operates on the generated item-specific feature graphs, whose message passing mechanism is specially tailored to provably capture feature interactions at any order. As we consider cold-start & warm-up phases, we further design a meta learning strategy that optimizes parameters of hypernetworks and GNN across various item CTR prediction tasks, while only adjusting a small set of item-specific parameters within each task. This strategy effectively reduces the risk of overfitting when dealing with limited data.", "4.1 Embedding Layer": "Given an instance ( \ud835\udc62, \ud835\udc63 ) , embedding layer maps the user features of \ud835\udc62 and item features of \ud835\udc63 into dense vectors. For the \ud835\udc5a th feature \ud835\udc53 \ud835\udc5a , \ud835\udc5a \u2208 [ 1 , \ud835\udc41 \ud835\udc63 + \ud835\udc41 \ud835\udc62 ] , its feature embedding e \ud835\udc5a is obtained as  \uf8f3 where \ud835\udc7e \ud835\udc52,\ud835\udc5a represents the embedding matrix corresponding to the \ud835\udc5a th feature, one-hot ( \ud835\udc53 \ud835\udc5a ) represents the one-hot vector of singlevalued feature \ud835\udc53 \ud835\udc5a , and multi-hot ( \ud835\udc53 \ud835\udc5a ) represents the multi-hot vector of multi-valued feature \ud835\udc53 \ud835\udc5a .", "4.2 Item-Specific Feature Graph Generation": "We employ hypernetworks, following the strategy of Ha et al. [10], to generate item-specific feature graphs. Hypernetworks, small neural networks trained to generate parameters for a larger main network, present a unique challenge in their application, as their integration is highly problem-specific. In EmerG, hypernetworks are used to produce the initial adjacency matrix A ( 1 ) \ud835\udc56 , encoding the item-specific feature graph for the first GNN layer. We streamline the process by allowing subsequent GNN layers to derive their adjacency matrices from the initial A ( 1 ) \ud835\udc56 , optimizing storage efficiency without compromising the model's specificity to each item. Consider task T \ud835\udc56 for item \ud835\udc63 \ud835\udc56 . For item features \ud835\udc53 1 , . . . , \ud835\udc53 \ud835\udc41 \ud835\udc63 , item feature embeddings are denoted as e 1 ,\ud835\udc56 , . . . , e \ud835\udc41 \ud835\udc63 ,\ud835\udc56 respectively. The feature graph [17, 39] is a graph where each node corresponds to a feature \ud835\udc53 \ud835\udc5a , and the edge between two nodes records their interaction. We let our hypernetworks, which consists of \ud835\udc41 \ud835\udc63 + \ud835\udc41 \ud835\udc62 subnetworks, produce a dense item-specific \u00af A ( 1 ) \ud835\udc56 \u2208 R ( \ud835\udc41 \ud835\udc63 + \ud835\udc41 \ud835\udc62 ) \u00d7 ( \ud835\udc41 \ud835\udc63 + \ud835\udc41 \ud835\udc62 ) which encodes the feature graph to be used in the first GNN layer. KDD '24, August 25-29, 2024, Barcelona, Spain Yaqing Wang, Hongming Piao, Daxiang Dong, Quanming Yao, and Jingbo Zhou Figure 1: Illustration of the proposed EmerG, designed to enhance CTR predictions of newly emerging items through the learning of item-specific feature interaction patterns. EmerG uses hypernetworks to generate an initial item-specific adjacency matrix for a feature graph, with nodes representing user and item features and edges denoting their interactions, based on item feature embeddings. Higher-order adjacency matrices for subsequent GNN layers are generated from the initial matrix, reducing both model complexity and storage requirements. The GNN's message passing process is tailored to capture \ud835\udc59 -order feature interactions at the \ud835\udc59 -1 th layer, enabling nuanced integration of various interaction orders for accurate predictions. EmerG optimizes the parameters of hypernetworks and GNN across diverse CTR prediction tasks to enhance generalization, while utilizing minimal item-specific parameters to capture the uniqueness of new items, which are adaptable with the introduction of additional item instances. Embedding Layer Click or Not Item-Specific Feature Graph With Feature Embeddings GNN Layer Predictor GNN Layer Feature Embeddings Item Features User Features High-Order Iteration High-Order Iteration one-hot(5) Hypernetworks Item-specific Adjacency Matrix MLP 5 MLP 4 MLP 1 MLP 2 one-hot(4) one-hot(3) one-hot(2) MLP 3 one-hot(1) Pretrained, then Meta-Trained Meta-Trained Deterministic Computation Denote the \ud835\udc5a th row of \u00af A ( 1 ) \ud835\udc56 as [ \u00af A ( 1 ) \ud835\udc56 ] \ud835\udc5a : , which is calculated as:  where MLP W \ud835\udc4e denotes a multi-layer perceptron (MLP) with parameter W \ud835\udc4e . Then, we generate \u00af A ( \ud835\udc59 ) \ud835\udc56 as  from the above-mentioned considerations, we expect that nodes disconnected in low-order feature graphs to be disconnected in highorder feature graphs. For example, if the message is not propagated from node \ud835\udc5b 2 to node \ud835\udc5b 1 in the \ud835\udc59 th GNN layer, the message of \ud835\udc5b 2 will be not propagated to \ud835\udc5b 1 in higher GNN layers. Thus, we apply (6) to obtain the final A ( \ud835\udc59 ) \ud835\udc56 . This (3) returns \u00af A ( \ud835\udc59 ) \ud835\udc56 as the matrix product of \ud835\udc59 copies of \u00af A ( 1 ) \ud835\udc56 . Therefore, [ \u00af A ( \ud835\udc59 ) \ud835\udc56 ] \ud835\udc5a\ud835\udc5b records the number of \ud835\udc59 -hop paths from node \ud835\udc5a to node \ud835\udc5b . In this way, we only need to keep one adjacency matrix (i.e., \u00af A ( 1 ) \ud835\udc56 ) for each item \ud835\udc63 \ud835\udc56 no matter how many GNN layers are used, which reduces parameter size. Further, we take the following steps to refine adjacency matrices:    where normalize (\u00b7) applies min-max normalization to scale all the elements of a matrix to be in the range [ 0 , 1 ] and sets the diagonal elements of a matrix directly as 1, sparsify (\u00b7 , \ud835\udc3e ) keeps the top \ud835\udc3e largest elements and set the rest as 0, and mask (\u00b7 , \u02dc A ( \ud835\udc59 -1 ) \ud835\udc56 ) sets all zero elements of \u02dc A ( \ud835\udc59 -1 ) \ud835\udc56 in \u02dc A ( \ud835\udc59 ) \ud835\udc56 as zero. From (4) to (6), we first sparsify the dense \u00af A ( \ud835\udc59 ) \ud835\udc56 by (4) such that only two highly related features are connected. Further, because of the commutative law of \u2299 , we transform \u02c6 A ( \ud835\udc59 ) \ud835\udc56 into a symmetric matrix by (5). Apart", "4.3 Customized Message Passing Process on Item-Specific Feature Graph": "Upon the learned item-specific feature graphs, we use a GNN with a customized message passing process designed to provably capture feature interactions at any orders, which are then explicitly combined into the final CTR predictions. We first describe the general mechanism of message passing. At the \ud835\udc59 th GNN layer, node embedding h ( \ud835\udc59 ) \ud835\udc5a of feature \ud835\udc53 \ud835\udc5a is updated as  where UPD ( \ud835\udc59 ) (\u00b7) updates node embedding of \ud835\udc53 \ud835\udc5a as h ( \ud835\udc59 ) \ud835\udc5a , AGG ( \ud835\udc59 ) (\u00b7) aggregates node embeddings of neighbor nodes, N( \ud835\udc53 \ud835\udc5a ) contains neighbor nodes of node corresponding to feature \ud835\udc53 \ud835\udc5a , and h ( 0 ) \ud835\udc5a = e \ud835\udc5a . After \ud835\udc41 \ud835\udc59 layers, node embedding h \ud835\udc5a = h ( \ud835\udc41 \ud835\udc59 ) \ud835\udc5a is returned as the final feature representation. In EmerG, we realize (7) as  EmerG KDD '24, August 25-29, 2024, Barcelona, Spain where \u2299 is the element-wise product, A ( \ud835\udc59 -1 ) \ud835\udc56 is the final itemspecific adjacency matrix obtained by (6), and W ( \ud835\udc59 -1 ) \ud835\udc54 is a learnable parameter. Unlike existing GNNs [15, 17, 39] that aggregate h ( \ud835\udc59 -1 ) \ud835\udc5a with h ( \ud835\udc59 -1 ) \ud835\udc5b , we aggregate h ( \ud835\udc59 -1 ) \ud835\udc5a with h ( 0 ) \ud835\udc5b . In this way, as Proposition 4.1 shows, the output of ( \ud835\udc59 -1 ) th GNN layer is \ud835\udc59 -order feature interactions, which enables EmerG to explicitly model arbitraryorder feature interaction. Proposition 4.1 (Efficacy of EmerG.). With the customized message passing process defined in (8) , the ( \ud835\udc59 -1 ) th GNN layer captures \ud835\udc59 -order feature interactions. The proof is in Appendix A.1. One may consider integrating residual connections into GNN to model arbitrary-order feature interaction. However, as analyzed in Appendix A.2, incorporating residual connections will significantly elevate the maximum order of feature interaction. With different orders of feature interactions, we then explicitly combine all nodes embeddings of each node \ud835\udc53 \ud835\udc5a into the updated node embeddings \u02c6 H \ud835\udc5a of \ud835\udc53 \ud835\udc5a by multi-head attention:   where H \ud835\udc5a = [ h ( 0 ) \ud835\udc5a ; . . . ; h ( \ud835\udc41 \ud835\udc59 ) \ud835\udc5a ] contains \ud835\udc41 \ud835\udc59 row vectors with length \ud835\udc41 \ud835\udc51 , and \ud835\udc41 \u210e is the number of attention heads. Then, we estimate the contribution factor for each of the \ud835\udc41 \ud835\udc63 + \ud835\udc41 \ud835\udc62 features as  where MLP W \ud835\udc50, 1 is parameterized by W \ud835\udc50, 1 . Finally, we predict whether item \ud835\udc63 and user \ud835\udc62 interact as  where W \ud835\udc50, 2 is a trainable parameter.", "4.4 Learning and Inference": "To reduce the risk of overfitting when dealing with limited data, we introduce a meta learning strategy that optimizes parameters of hypernetworks and GNN across various item CTR prediction tasks, while only adjusting a minimal set of item-specific parameters within each task. For simplicity, we denote hypernetworks as hyper \ud835\udf3d hyper where \ud835\udf3d hyper = W \ud835\udc4e is the shared trainable parameter. Then, we denote the GNN as GNN \ud835\udf3d GNN , \ud835\udf53 \ud835\udc56 , where \ud835\udf3d GNN represents shared parameters including parameters of embedding layers { W \ud835\udc52,\ud835\udc5a } \ud835\udc41 \ud835\udc63 + \ud835\udc41 \ud835\udc62 \ud835\udc5a = 1 , parameters of GNN layers { W ( \ud835\udc59 ) \ud835\udc54 } \ud835\udc41 \ud835\udc59 \ud835\udc59 = 1 , W \ud835\udc5e,\u210e , W \ud835\udc58,\u210e , W \ud835\udc63,\u210e , parameters of predictor W \ud835\udc50, 1 , W \ud835\udc50, 2 , and \ud835\udf53 \ud835\udc56 represents item-specific parameters  which includes item-specific adjacency matrix A ( 1 ) generated by hypernetworks and the randomized item ID embedding e ID ,\ud835\udc56 of item \ud835\udc63 \ud835\udc56 . We target at learning \ud835\udf3d \u2217 GNN , \ud835\udf3d \u2217 hyper , which can achieve good coldstart & warm-up performance on new item \ud835\udc63 \ud835\udc56 by only generating \ud835\udf53 \ud835\udc56 and warming up \ud835\udf53 \ud835\udc56 with gradient descent. We optimize EmerG w.r.t. the following objective calculated across \ud835\udc41 \ud835\udc61 tasks from T old :  where \ud835\udefe is a hyperparameter to balance the contribution of two loss terms. In particular, the first term can represent the performance of cold-start phase [24] as the model has not been exposed to the labels in S \ud835\udc56 . We compute LS \ud835\udc56 ( \ud835\udf3d GNN , \ud835\udf53 \ud835\udc56 ) as  where BCE ( \ud835\udc66, \u02c6 \ud835\udc66 ) = -\ud835\udc66 log ( \u02c6 \ud835\udc66 ) - ( 1 -\ud835\udc66 ) log ( 1 -\u02c6 \ud835\udc66 ) is the binary cross entropy. The second term in (12) represents the performance of warm-up phase after updating \ud835\udf53 \ud835\udc56 by a few new item instances provided in S \ud835\udc56 . We compute LQ \ud835\udc56 ( \ud835\udf3d GNN , \ud835\udf53 \u2032 \ud835\udc56 ) as  with \ud835\udf53 \u2032 \ud835\udc56 obtained by performing gradient descent steps w.r.t (13):  where \ud835\udefc 1 is the learning rate. Algorithm 1 summarizes the training procedure of EmerG.", "Algorithm 1 The training procedure of EmerG.": "1: randomly initialize \ud835\udf3d hyper and \ud835\udf3d GNN ; 2: pretrain \ud835\udf3d GNN by old item instances; 3: for T \ud835\udc56 in T old do 4: sample S \ud835\udc56 and Q \ud835\udc56 for T \ud835\udc56 ; 5: randomly initialize item ID embedding e ID ,\ud835\udc56 for T \ud835\udc56 ; 6: obtain feature embeddings e 1 ,\ud835\udc56 , . . . , e \ud835\udc41 \ud835\udc63 + \ud835\udc41 \ud835\udc62 ,\ud835\udc56 by (1); 7: generate \u00af A ( 1 ) \ud835\udc56 of the first GNN layer by (2); 8: get item-specific parameter \ud835\udf53 \ud835\udc56 = { \u00af A ( 1 ) \ud835\udc56 , h ID ,\ud835\udc56 } ; 9: obtain A ( \ud835\udc59 ) \ud835\udc56 of subsequent GNN layers by (3)-(6); 10: obtain feature representations h 1 ,\ud835\udc56 , . . . , h \ud835\udc41 \ud835\udc63 + \ud835\udc41 \ud835\udc62 ,\ud835\udc56 by (8); 11: obtain prediction \u02c6 \ud835\udc66 \ud835\udc56 \ud835\udc57 by (10) for ( \ud835\udc63 \ud835\udc56 , \ud835\udc62 \ud835\udc57 , \ud835\udc66 \ud835\udc56,\ud835\udc57 ) \u2208 S \ud835\udc56 ; 12: update \ud835\udf53 \ud835\udc56 as \ud835\udf53 \u2032 \ud835\udc56 by (15) with learning rate \ud835\udefc 1 ; 13: optimize \ud835\udf3d hyper and \ud835\udf3d GNN w.r.t. (12) by gradient descents with learning rate \ud835\udefc 2 ; 14: end for By learning from a set of tasks T old , the learned \ud835\udf3d \u2217 GNN , \ud835\udf3d \u2217 hyper encode common knowledge. Consider task T \ud835\udc58 of new item \ud835\udc63 \ud835\udc58 . When \ud835\udc63 \ud835\udc58 has no interaction record, namely |S \ud835\udc58 | = 0, we obtain its taskspecific parameter \ud835\udf53 \ud835\udc58 and test its performance on the test set as cold-start phase performance. Given a few new item instances, we can update \ud835\udf53 \ud835\udc58 to take in the supervised information. Once \ud835\udf53 \ud835\udc58 is updated, we measure performance on the test set as warm-up phase performance. Algorithm 2 describes the testing procedure. KDD '24, August 25-29, 2024, Barcelona, Spain Yaqing Wang, Hongming Piao, Daxiang Dong, Quanming Yao, and Jingbo Zhou", "Algorithm 2 The testing procedure of EmerG.": "1: optimized \ud835\udf3d \u2217 hyper and \ud835\udf3d \u2217 GNN ; 2: Consider task T \ud835\udc58 of a new item \ud835\udc63 \ud835\udc58 , given S \ud835\udc58 with a few interaction records of \ud835\udc63 \ud835\udc58 and Q \ud835\udc58 for evaluating CTR prediction performance of \ud835\udc63 \ud835\udc58 ; 3: if |S \ud835\udc58 | = 0 then 4: randomly initialize item ID embedding e ID ,\ud835\udc58 for T \ud835\udc58 ; 5: obtain feature embeddings e 1 ,\ud835\udc58 , . . . , e \ud835\udc41 \ud835\udc63 + \ud835\udc41 \ud835\udc62 ,\ud835\udc58 by (1); 6: generate \u00af A ( 1 ) \ud835\udc58 of the first GNN layer by (2); 7: get item-specific parameter \ud835\udf53 \ud835\udc58 = { \u00af A ( 1 ) \ud835\udc58 , e ID ,\ud835\udc58 } ; 8: obtain A ( \ud835\udc59 ) \ud835\udc58 of subsequent GNN layers by (3)-(6); 9: obtain feature representations h 1 ,\ud835\udc58 , . . . , h \ud835\udc41 \ud835\udc63 + \ud835\udc41 \ud835\udc62 ,\ud835\udc58 by (8); 10: obtain prediction \u02c6 \ud835\udc66 \ud835\udc58\ud835\udc57 by (10) for ( \ud835\udc63 \ud835\udc58 , \ud835\udc62 \ud835\udc57 , \ud835\udc66 \ud835\udc58,\ud835\udc57 ) \u2208 S \ud835\udc58 ; 11: measure performance on Q \ud835\udc58 ; 12: else 13: update A ( \ud835\udc59 ) \ud835\udc58 , \ud835\udc59 \u2208 { 1 \u00b7 \u00b7 \u00b7 \ud835\udc41 \ud835\udc59 } by (3) to (6); 14: update feature representations h 1 ,\ud835\udc58 , . . . , h \ud835\udc41 \ud835\udc63 + \ud835\udc41 \ud835\udc62 ,\ud835\udc58 by (8); 15: update \ud835\udf53 \ud835\udc58 as \ud835\udf53 \u2032 \ud835\udc58 by (15); 16: obtain prediction \u02c6 \ud835\udc66 \ud835\udc58\ud835\udc57 by (10) for ( \ud835\udc63 \ud835\udc58 , \ud835\udc62 \ud835\udc57 , \ud835\udc66 \ud835\udc58,\ud835\udc57 ) \u2208 S \ud835\udc58 ; 17: measure performance on Q \ud835\udc58 ; 18: end if", "5 EXPERIMENTS": "", "5.1 Experimental Settings": "Datasets. We use two benchmark datasets: (i) MovieLens [11]: a dataset containing 1 million interaction records on MovieLens, whose item features include movie ID, title, year of release, genres and user features include user ID, age, gender, occupation and zip-code; and (ii) Taobao [29]: a collection of 26 million ad click records on Taobao, whose item features include ad ID, position ID, category ID, campaign ID, advertiser ID, brand, price and user features include user ID, Micro group ID, cms_group_id, gender, age, consumption grade, shopping depth, occupation and city level. Following existing works [24, 42], we binarize the ratings of MovieLens, setting rating smaller than 4 as 0 and the others as 1. Data Split. We adopt the public data split [24, 42, 45], group items according to their frequency: (i) old items which are items appearing in more than \ud835\udc41 interaction records, where \ud835\udc41 = 200 in MovieLens and \ud835\udc41 = 2000 in Taobao; and (ii) new items which are items appearing in less than \ud835\udc41 and larger than 3 \ud835\udc3e interaction records, where \ud835\udc3e is set to 20 and 500 for MovieLens and Taobao respectively. To mimic the dynamic process where new items are gradually clicked by more users, the interaction records associated with new items are sorted by timestamp. We consider three successive warm-up phases, labeled as A, B, and C, each of which involves the introduction of a set of \ud835\udc3e new interaction records for each item. The rest interaction records form testing data for evaluation. Experiment Pipeline. We adopt pipeline of existing works [24, 42, 45] to assess how a model adapts to new items over time. First, we use old item instances to pretrain the model, and directly evaluate the model performance on testing data of new items as cold-start phase performance. Then, we measure how model performs as it learns from a few training data in successive warm-up phases. In particular, we use the training data in warm-up phases A, B and C to sequentially update the model, and evaluate the performance of corresponding updated models on testing data. Evaluation Metric. Following existing works [42], the performance is evaluated by (i) Area Under the Curve (AUC) [19] which represents the degree of separability, and (ii) F1 score [13] which is a harmonic mean of the precision and recall. Both AUC and F1 vary between 0 (worst) and 1 (best).", "5.2 Performance Comparison": "We compare the proposed EmerG 1 with the following four groups of baselines: A General CTR backbones pretrained using old item instances and fine-tuned by new item instances, including DeepFM [9], Wide&Deep [5], AutoInt [40], AFN [6], Fi-GNN [17], recent FinalMLP [21] and FINAL [44]. B Methods for new items without interaction records, including DropoutNet [31] and ALDI [12]. C Methods for new items with a few interaction records, including MeLU [16], MAMO [7], TaNP [18], and ColdNAS [37]. These methods cannot incorporate new item instances dynamically. Therefore, to accommodate the training interaction records provided in warm-up phases A, B, and C, we adopt a phased approach: initially, we use \ud835\udc3e interaction records from phase A as the support set to assess testing performance. Subsequently, we combine 2 \ud835\udc3e records from phases A and B as the support set for a second evaluation. Finally, we incorporate 3 \ud835\udc3e records from all three warm-up phases-A, B, and C-as the support set to conduct a third assessment of testing performance. D Methods for emerging items with incremental interaction records, which are the most relevant to ours. Existing works mainly equip general CTR backbones with the ability to generate and warmup item ID embeddings for new items, including MetaE [24], MWUF [45], GME [23], and CVAR [42]. We use the classic DeepFM as the CTR backbone. Results of equipping these methods with other backbones are reported in Appendix C.1. We implement the compared methods using public codes of the respective authors. More implementation details are provided in Appendix B. Performance for Cold-Start & Warm-Up Phases. Table 1 shows the results. As shown, cold-start methods designed for cold-start & warm-up phases generally perform better. EmerG consistently performs the best in all four phases, validating the effectiveness of capturing item-specific feature interaction by hypernetworks. Few-shot methods for N-way K-shot settings obtains good performance in warm-up phase A. However, as the number of samples increases, it is unable to achieve greater performance improvement without complete retraining. General CTR backbones which are fine-tuned using the training sets perform worse, where FinalMLP performs the best. Recall that they randomly initialize item-specific parameters for new items, fine-tuning pretrained models by a small number of new item instances is not enough to obtain good performance. Particularly, note that the GNN-based CTR model Fi-GNN which uses item-user-specific feature interaction graphs perform 1 Our code is available at https://github.com/LARS-group/EmerG. EmerG KDD '24, August 25-29, 2024, Barcelona, Spain Table 1: Test performance obtained on MovieLens and Taobao. The best results are bolded, the second-best results are underlined. not well. This shows that too much freedom is not beneficial to capture feature interaction patterns under cold-start & warm-up phases. While in EmerG, we utilize hypernetworks to generate item-specific feature graphs, which is then processed by a GNN with customized message passing mechanism to capture arbitraryorder feature interaction, and optimize parameters by meta learning strategy. All these design considerations contributes the best performance obtained by EmerG. For computational overhead, EmerG is relatively more efficient in terms of both time and computational resources. See Appendix C.2 for a detailed comparison. Performance Given Sufficient Training Samples. One might question how EmerG performs with an abundance of training samples for new items, referred to as the common phase, especially in comparison to traditional CTR backbones. Here, we set aside samples from original testing samples of new items (so the test set is smaller), use them to augment the experiment pipeline with more training samples, and evaluate the performance on the smaller test set. We compare EmerG with baselines which perform the best among CTR backbones and few-shot methods in Table 1. Figure 2 shows the KDD '24, August 25-29, 2024, Barcelona, Spain Yaqing Wang, Hongming Piao, Daxiang Dong, Quanming Yao, and Jingbo Zhou testing AUC (%) with the number of training samples. Experimental results measured by testing F1 (%) are similar. As shown, all methods get better performance given more training samples. The classic CTR backbone DeepFM gradually outperforms CVAR which equips DeepFM with additional modules to generate item ID embeddings for new items. In contrast, EmerG consistently performs the best and converges to better performance than the others. This validates the effectiveness of our EmerG which can nicely capture item-specific feature interaction at different orders. Figure 2: Comparing EmerG with DeepFM and CVAR given sufficient training samples. 0 200 400 600 800 1000 1200 Number of Samples 70 . 0 72 . 5 75 . 0 77 . 5 80 . 0 82 . 5 85 . 0 AUC (%) DeepFM CVAR EmerG (a) MovieLens. 0 350 700 1050 1400 1750 Number of Samples 58 60 62 64 66 AUC (%) DeepFM CVAR EmerG (b) Taobao.", "5.3 Model Analysis": "5.3.1 Ablation Study. We compare the proposed EmerG with the following variants: (i) w/ random graph generates the adjacency matrix A ( 1 ) in (8) randomly; (ii) w/o sparsification does not apply (4) to sparsify the adjacency matrices; (iii) w/o mask does not apply (6) to enforce nodes which are disconnected in low-order feature graphs to be disconnected in higher-order feature graphs; (iv) w/ shared graph employs global shared adjacency matrices for all items, in contrast to EmerG, which utilizes item-specific adjacency matrices; (v) w/o meta learns both GNN and hypernetworks from old items, without forming tasks and employ a meta-learning strategy; and (vi) w/o inner directly uses \ud835\udf53 \ud835\udc56 and does not update it to \ud835\udf53 \u2032 \ud835\udc56 within each task. Figure 3 shows the results. As shown, 'w/ random graph\" performs worse than EmerG which shows that the item-specific feature graphs generated by hypernetworks is meaningful. The performance gain of EmerG over 'w/o sparsification\" shows that a sparse feature graph where only closely-related nodes are connected can let the GNN model concentrate on useful messages. Comparing 'w/o mask\" to EmerG, the performance drop validates our assumption in (6). The mask operation also prevents the adjacency matrices from being too dense, which can be beneficial to prune unnecessary feature interactions and provide better explainability. We can also observe that'w/ shared graph\" performs worse than EmerG. This validates that using global shared adjacency matrices cannot capture the various feature interaction patterns between different users and items. Finally, EmerG defeats 'w/o meta\" and 'w/o inner\", which underscores the necessity of both meta-learning across tasks and inner updates within each task. 5.3.2 Effect of Number of GNN Layers. As demonstrated in Proposition 4.1, we have customized the message passing process of the GNN to ensure that the \ud835\udc59 th layer encapsulates \ud835\udc59 -order feature interactions. Furthermore, we optimize this process by generating Cold-Start Phase Warm-Up Phase A Warm-Up Phase B Warm-Up Phase C 67 . 5 70 . 0 72 . 5 75 . 0 77 . 5 80 . 0 82 . 5 85 . 0 AUC (%) w/o meta w/o inner w/ random graph w/o sparsification w/o mask w/ shared graph EmerG (a) MovieLens. Figure 3: Ablation study on MovieLens and Taobao. Cold-Start Phase Warm-Up Phase A Warm-Up Phase B Warm-Up Phase C 56 58 60 62 64 66 AUC (%) w/o meta w/o inner w/ random graph w/o sparsification w/o mask w/ shared graph EmerG (b) Taobao. adjacency matrices for subsequent GNN layers directly from the initial matrix provided by hypernetworks. This approach not only streamlines the architecture but also facilitates the extension to additional layers, thereby capturing higher-order feature interactions with ease. In this context, we investigate the influence of the number of GNN layers on performance across various datasets. Figure 4: Varying the number of GNN layers in EmerG. Cold Warm A Warm B Warm C 70 . 0 72 . 5 75 . 0 77 . 5 80 . 0 82 . 5 85 . 0 AUC (%) 1 layer 2 layer(ours) 3 layer (a) MovieLens. Cold Warm A Warm B Warm C 58 60 62 64 66 AUC (%) 1 layer 2 layer 3 layer(ours) (b) Taobao. Figure 4 shows the results. As can be seen, EmerG with different layer numbers obtain the best performance on different datasets: EmerG with 2 GNN layers performs the best on MovieLens while EmerG with 3 GNN layers achieves the best performance on Taobao. This shows that different datasets requires different number of GNN layers: larger datasets such as Taobao may need higher-order features than smaller ones such as MovieLens. By design, EmerG can easily meet this requirement. 5.3.3 Different Feature Interaction Functions. We consider using different feature interaction functions. Table 2 shows the results. EmerG KDD '24, August 25-29, 2024, Barcelona, Spain Figure 5: Visualizations of item-specific adjacency matrices of movie Lawnmower Man 2: Beyond Cyberspace ( A ( \ud835\udc56 ) W ) and movie Waiting to Exhale ( A ( \ud835\udc56 ) L ) in MovieLens generated by EmerG. 1.0 gender 0.8 age occupation item ID 0, | 0.2 title genres Aw start phase. in cold-start phase. (c) A in cold-start phase. in warm-up phase A in warm-up phase B. AL in warm-up phase C. 1 1 1 1 code 1 1 1 1 1 1 1 code 1 1 1 1 1 1 1 1 usr iten item 7 item item item cold - As shown, using element-wise product performs the best, which is adopted in EmerG. to changes in the age of people who like the same movie, which validates the efficacy of the learned third-order feature interaction. Table 2: Test performance in AUC (%) obtained on MovieLens and Taobao. The best results are bolded.", "5.4 Case Study": "Finally, we take movie Lawnmower Man 2: Beyond Cyberspace and movie Waiting to Exhale from MovieLens as new items, and visualize their adjacency matrices which record the item-specific feature graphs in Figure 5. As can be seen, EmerG learns different task-specific feature graphs for different items. Comparing Figure 5(a) with Figure 5(b), we find that Lawnmower Man 2: Beyond Cyberspace has a particularly important second-order feature interaction \u27e8 genres , title \u27e9 . The genre of Lawnmower Man 2: Beyond Cyberspace is science fiction while the genre of Waiting to Exhale is comedy. For a science fiction, its title often reflects its world view or theme, which is the key for people to judge whether they are interested. As for a comedy work, whether it is interesting or not is often irrelevant to the title. Besides, EmerG can capture meaningful higher-order feature interactions. As shown, both \u27e8 year , age \u27e9 and \u27e8 year , zip-code \u27e9 are important second-order feature interactions, they contribute to discovering the third-order feature interaction \u27e8 year , age , zip-code \u27e9 . In Figure 5(c), the relation between nodes of year, age and zip-code all become relatively important although <age, zip-code> is not important in Figure 5(b). It is easy to understand that the year of movies determines the age of people who are more likely to watch them. For example, elderly people generally prefer watching old movies. Apart from this, location which is indicated by zip-code also plays an important role: people in developed areas tend to be more receptive to new things. Therefore, area changes may lead We can also observe that the item-specific feature graphs generated by our hypernetworks can roughly capture the feature interactions before seeing any training samples of new items. Although they are continuously optimized using training sets of warm-up phases, the changes are not sharp. As can be seen, the second-order feature interaction patterns are similar in Figure 5(b) and Figure 5(d) to Figure 5(f), with small changes to accommodate the training samples. Overall, we conclude that EmerG can learn reasonable adjacency matrices to capture item-specific feature interactions at different orders.", "6 CONCLUSION": "In this study, we underscore the critical role of feature interactions and introduce EmerG, a novel solution designed to capture the unique interaction patterns of items, effectively handling CTR prediction of newly emerging items with incremental interaction records. Our approach leverages hypernetworks to construct itemspecific feature graphs, with nodes representing features and edges denoting their interactions, thus enabling the model to discern the intricate interaction patterns that characterize each item. We incorporate a graph neural network (GNN) equipped with a specialized message passing process, crafted to capture feature interactions across all orders, facilitating precise CTR predictions. To combat overfitting in scenarios with sparse data, we implement a metalearning strategy that finely tunes parameters of hypernetworks and GNN across various item CTR prediction tasks, necessitating only minimal modifications to item-specific parameters for each task. Experimental results on real-world datasets show EmerG obtains the state-of-the-art performance on CTR prediction for new items that have no interaction history, a few interactions, or a substantial number of interactions. We expect this approach can be used to warm-up cold-start problems in other applications such as drug recommendation in the future.", "ACKNOWLEDGMENT": "We thank the anonymous reviewers for their valuable comments. This work is supported by National Key Research and Development Program of China under Grant 2023YFB2903904 and National Natural Science Foundation of China under Grant No. 92270106. KDD '24, August 25-29, 2024, Barcelona, Spain", "REFERENCES": "[1] Jiang Bian, Jizhou Huang, Shilei Ji, Yuan Liao, Xuhong Li, Qingzhong Wang, Jingbo Zhou, Dejing Dou, Yaqing Wang, and Haoyi Xiong. 2023. Feynman: Federated Learning-based Advertising for Ecosystems-Oriented Mobile Apps Recommendation. IEEE Transactions on Services Computing 16, 5 (2023), 33613372. [2] Mathieu Blondel, Akinori Fujino, Naonori Ueda, and Masakazu Ishihata. 2016. Higher-order factorization machines. In Advances in Neural Information Processing Systems . 3351-3359. [3] Olivier Chapelle, Eren Manavoglu, and Romer Rosales. 2014. Simple and scalable response prediction for display advertising. ACM Transactions on Intelligent Systems and Technology 5, 4 (2014), 1-34. [4] Hao Chen, Zefan Wang, Feiran Huang, Xiao Huang, Yue Xu, Yishi Lin, Peng He, and Zhoujun Li. 2022. Generative adversarial framework for cold-start item recommendation. In International ACM SIGIR Conference on Research and Development in Information Retrieval . 2565-2571. [5] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Workshop on Deep Learning for Recommender Systems . 7-10. [6] Weiyu Cheng, Yanyan Shen, and Linpeng Huang. 2020. Adaptive factorization network: Learning adaptive-order feature interactions. In AAAI Conference on Artificial Intelligence . 3609-3616. [7] Manqing Dong, Feng Yuan, Lina Yao, Xiwei Xu, and Liming Zhu. 2020. MAMO: Memory-augmented meta-optimization for cold-start recommendation. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 688-697. [8] Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-agnostic metalearning for fast adaptation of deep networks. In International Conference on Machine Learning . 1126-1135. [9] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: A factorization-machine based neural network for CTR prediction. In International Joint Conference on Artificial Intelligence . 1725-1731. [10] David Ha, Andrew Dai, and Quoc V Le. 2017. Hypernetworks. In International Conference on Learning Representations . [11] F Maxwell Harper and Joseph A Konstan. 2015. The MovieLens datasets: History and context. ACMTransactions on Interactive Intelligent Systems 5, 4 (2015), 1-19. [12] Feiran Huang, Zefan Wang, Xiao Huang, Yufeng Qian, Zhetao Li, and Hao Chen. 2023. Aligning distillation for cold-start item recommendation. In International ACM SIGIR Conference on Research and Development in Information Retrieval . 1147-1157. [13] Hao Huang, Haihua Xu, Xianhui Wang, and Wushour Silamu. 2015. Maximum F1score discriminative training criterion for automatic mispronunciation detection. IEEE/ACM Transactions on Audio, Speech, and Language Processing 23, 4 (2015), 787-797. [14] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. In International Conference on Learning Representations . [15] Thomas N Kipf and Max Welling. 2017. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations . [16] Hoyeop Lee, Jinbae Im, Seongwon Jang, Hyunsouk Cho, and Sehee Chung. 2019. MeLU: Meta-learned user preference estimator for cold-start recommendation. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 1073-1082. [17] Zekun Li, Zeyu Cui, Shu Wu, Xiaoyu Zhang, and Liang Wang. 2019. Fi-GNN: Modeling feature interactions via graph neural networks for CTR prediction. In ACM International Conference on Information and Knowledge Management . 539-548. [18] Xixun Lin, Jia Wu, Chuan Zhou, Shirui Pan, Yanan Cao, and Bin Wang. 2021. Task-adaptive neural process for user cold-start recommendation. In The Web Conference . 1306-1316. [19] Charles X Ling, Jin Huang, Harry Zhang, et al. 2003. AUC: A statistically consistent and more discriminating measure than accuracy. In International Joint Conference on Artificial Intelligence . 519-524. [20] Yuanfu Lu, Yuan Fang, and Chuan Shi. 2020. Meta-learning on heterogeneous information networks for cold-start recommendation. In ACMSIGKDDConference on Knowledge Discovery and Data Mining . 1563-1573. [21] Kelong Mao, Jieming Zhu, Liangcai Su, Guohao Cai, Yuru Li, and Zhenhua Dong. 2023. FinalMLP: An enhanced two-stream MLP model for CTR prediction. In AAAI Conference on Artificial Intelligence . 4552-4560. [22] Erxue Min, Yu Rong, Tingyang Xu, Yatao Bian, Da Luo, Kangyi Lin, Junzhou Huang, Sophia Ananiadou, and Peilin Zhao. 2022. Neighbour interaction based click-through rate prediction via graph-masked transformer. In International ACM SIGIR Conference on Research and Development in Information Retrieval . 353-362. [23] Wentao Ouyang, Xiuwu Zhang, Shukui Ren, Li Li, Kun Zhang, Jinmei Luo, Zhaojie Liu, and Yanlong Du. 2021. Learning graph meta embeddings for cold-start ads in click-through rate prediction. In International ACM SIGIR Conference on Research and Development in Information Retrieval . 1157-1166. Yaqing Wang, Hongming Piao, Daxiang Dong, Quanming Yao, and Jingbo Zhou [24] Feiyang Pan, Shuokai Li, Xiang Ao, Pingzhong Tang, and Qing He. 2019. Warm up cold-start advertisements: Improving CTR predictions via learning to learn ID embeddings. In International ACM SIGIR Conference on Research and Development in Information Retrieval . 695-704. [25] Yoon-Joo Park and Alexander Tuzhilin. 2008. The long tail of recommender systems and how to leverage it. In ACM Conference on Recommender Systems . 11-18. [26] Steffen Rendle. 2010. Factorization machines. In IEEE International Conference on Data Mining . 995-1000. [27] Matthew Richardson, Ewa Dominowska, and Robert Ragno. 2007. Predicting clicks: Estimating the click-through rate for new ads. In International Conference on World Wide Web . 521-530. [28] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, and Jian Tang. 2019. AutoInt: Automatic feature interaction learning via selfattentive neural networks. In ACM International Conference on Information and Knowledge Management . 1161-1170. [29] Tianchi. 2018. Taobao Display Ads Click Dataset. https://tianchi.aliyun.com/ dataset/dataDetail?dataId=56. [30] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems . 5998-6008. [31] Maksims Volkovs, Guangwei Yu, and Tomi Poutanen. 2017. DropoutNet: Addressing cold start in recommender systems. In Advances in Neural Information Processing Systems . 4957-4966. [32] Li Wang, Binbin Jin, Zhenya Huang, Hongke Zhao, Defu Lian, Qi Liu, and Enhong Chen. 2021. Preference-adaptive meta-learning for cold-start recommendation.. In International Joint Conference on Artificial Intelligence . 1607-1614. [33] Yaqing Wang, Abulikemu Abuduweili, Quanming Yao, and Dejing Dou. 2021. Property-aware relation networks for few-shot molecular property prediction. In Advances in Neural Information Processing Systems . 17441-17454. [34] Yaqing Wang, Song Wang, Yanyan Li, and Dejing Dou. 2022. Recognizing medical search query intent by few-shot learning. In International ACM SIGIR Conference on Research and Development in Information Retrieval . 502-512. [35] Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. 2020. Generalizing from a few examples: A survey on few-shot learning. Comput. Surveys 53, 3 (2020), 1-34. [36] Yan Wen, Chen Gao, Lingling Yi, Liwei Qiu, Yaqing Wang, and Yong Li. 2023. Efficient and Joint Hyperparameter and Architecture Search for Collaborative Filtering. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 2547-2558. [37] Shiguang Wu, Yaqing Wang, Qinghe Jing, Daxiang Dong, Dejing Dou, and Quanming Yao. 2023. ColdNAS: Search to modulate for user cold-start recommendation. In The Web Conference . 1021-1031. [38] Shiguang Wu, Yaqing Wang, and Quanming Yao. 2024. PACIA: Parameterefficient adapter for few-shot molecular property prediction. In International Joint Conference on Artificial Intelligence . [39] Yuexiang Xie, Zhen Wang, Yaliang Li, Bolin Ding, Nezihe Merve G\u00fcrel, Ce Zhang, Minlie Huang, Wei Lin, and Jingren Zhou. 2021. FIVES: Feature interaction via edge search for large-scale tabular data. In ACMSIGKDDConference on Knowledge Discovery and Data Mining . 3795-3805. [40] Canran Xu and Ming Wu. 2020. Learning feature interactions with lorentzian factorization machine. In AAAI Conference on Artificial Intelligence . 6470-6477. [41] Quanming Yao, Zhenqian Shen, Yaqing Wang, and Dejing Dou. 2024. Propertyaware relation networks for few-shot molecular property prediction. IEEE Transactions on Pattern Analysis and Machine Intelligence (2024). [42] Xu Zhao, Yi Ren, Ying Du, Shenzheng Zhang, and Nian Wang. 2022. Improving item cold-start recommendation via model-agnostic conditional variational autoencoder. In International ACM SIGIR Conference on Research and Development in Information Retrieval . 2595-2600. [43] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through rate prediction. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 1059-1068. [44] Jieming Zhu, Qinglin Jia, Guohao Cai, Quanyu Dai, Jingjie Li, Zhenhua Dong, Ruiming Tang, and Rui Zhang. 2023. FINAL: Factorized interaction layer for CTR prediction. In International ACM SIGIR Conference on Research and Development in Information Retrieval . 2006-2010. [45] Yongchun Zhu, Ruobing Xie, Fuzhen Zhuang, Kaikai Ge, Ying Sun, Xu Zhang, Leyu Lin, and Juan Cao. 2021. Learning to warm up cold item embeddings for coldstart recommendation with meta scaling and shifting networks. In International ACM SIGIR Conference on Research and Development in Information Retrieval . 1167-1176. [46] Ziwei Zhu, Shahin Sefati, Parsa Saadatpanah, and James Caverlee. 2020. Recommendation for new users and new items via randomized training and mixture-ofexperts transformation. In International ACM SIGIR Conference on Research and Development in Information Retrieval . 1121-1130. EmerG KDD '24, August 25-29, 2024, Barcelona, Spain", "A MESSAGE PASSING MECHANISM": "", "A.1 Proof of Proposition 4.1": "Proof. In (8), node embedding h ( \ud835\udc59 -1 ) \ud835\udc5a aggregates from multiple first-order node embeddings h ( 0 ) \ud835\udc5b to generate h ( \ud835\udc59 ) \ud835\udc5a . Therefore,    After merging all nodes embedding with multi-head self-attention, all orders of features we can obtain after \ud835\udc59 -1 GNN layers are  \u25a1", "A.2 Comparing with GNN with Residual Connections": "Although GNN with residual connections can model arbitrary-order feature interaction [17], the maximum order of feature interaction will increase drastically. To see this, instead of using (8), (7) can be realized with residual connection as  As h ( \ud835\udc59 -1 ) \ud835\udc5a and h ( \ud835\udc59 -1 ) \ud835\udc5b have the same maximum order, when we aggregate them via \u2299 , we have:  In other words, using residue connections will lead to too many noisy high-order feature interactions. Consequently, it is also challenging to determine which feature interactions contribute to the prediction, resulting in low interpretability.", "B IMPLEMENTATION DETAILS": "All results are averaged over five runs and are obtained on a 32GB NVIDIA Tesla V100 GPU. We use Adam optimizer [14]. To search for the appropriate hyperparameters, we set aside 20% old items and form validation set using their samples. The performance is then directly evaluated on the validation set of these items, which corresponds to cold-start phase. When the hyperparameters are found by grid search, we put back samples of these old items, then follow the experiment pipeline described in Section 5.1 and report the results. The hyperparameters and their range used by EmerG are summarized in Table 3.", "C MORE EXPERIMENTAL RESULTS": "", "C.1 Comparing with Existing Methods Equipped with Different Backbones": "In Section 5.2, we employ DeepFM as the backbone for methods in Group D. Despite this, results in Table 1 indicate that FinalMLP generally surpasses DeepFM, particularly in the warm-up phases, though not in the cold-start phases. Therefore, we further integrate FinalMLP, the top-performing backbone from Group A, into methods in Group D. Results are reported in Table 4. Notably, FinalMLP, Table 3: Hyperparameters used by EmerG. \ud835\udc41 \u2032 = \ud835\udc41 \ud835\udc63 + \ud835\udc41 \ud835\udc62 . when utilized as a backbone for cold-start methods, does not exceed the performance of configurations using DeepFM. This suggests that more recent CTR backbones cannot effectively handle the CTR prediction of newly emerging items.", "C.2 Computational Overhead": "Table 5 shows a detailed comparison of the computational overhead for all compared methods. As indicated, EmerG demonstrates relatively lower time and space requirements. Table 5: Computational overhead of compared methods on MovieLens. Time is reported as seconds per epoch. KDD '24, August 25-29, 2024, Barcelona, Spain Yaqing Wang, Hongming Piao, Daxiang Dong, Quanming Yao, and Jingbo Zhou Table 4: Comparing EmerG with methods for emerging items with incremental interaction records, using various backbones. Test performance obtained on MovieLens and Taobao. The best results are bolded, the second-best results are underlined."}
