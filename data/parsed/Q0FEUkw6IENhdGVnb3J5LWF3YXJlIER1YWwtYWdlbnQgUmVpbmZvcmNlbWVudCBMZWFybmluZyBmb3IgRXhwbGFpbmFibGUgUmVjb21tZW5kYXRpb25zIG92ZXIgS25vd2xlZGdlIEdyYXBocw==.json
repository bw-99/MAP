{"title": "CADRL: Category-aware Dual-agent Reinforcement Learning for Explainable Recommendations over Knowledge Graphs", "authors": "Shangfei Zheng; Hongzhi Yin; Tong Chen; Xiangjie Kong; Jian Hou; Pengpeng Zhao", "pub_date": "2024-08-06", "abstract": "Knowledge graphs (KGs) have been widely adopted to mitigate data sparsity and address cold-start issues in recommender systems. While existing KGs-based recommendation methods can predict user preferences and demands, they fall short in generating explicit recommendation paths and lack explainability. As a step beyond the above methods, recent advancements utilize reinforcement learning (RL) to find suitable items for a given user via explainable recommendation paths. However, the performance of these solutions is still limited by the following two points. (1) Lack of ability to capture contextual dependencies from neighboring information. (2) The excessive reliance on short recommendation paths due to efficiency concerns. To surmount these challenges, we propose a category-aware dual-agent reinforcement learning (CADRL) model for explainable recommendations over KGs. Specifically, our model comprises two components: (1) a category-aware gated graph neural network that jointly captures context-aware item representations from neighboring entities and categories, and (2) a dual-agent RL framework where two agents efficiently traverse long paths to search for suitable items. Finally, experimental results show that CADRL outperforms state-of-the-art models in terms of both effectiveness and efficiency on large-scale datasets.", "sections": [{"heading": "I. INTRODUCTION", "text": "Recommender systems have emerged as indispensable tools for personalized information retrieval by providing users with suitable items [20] [37] [29]. Despite their widespread adoption in the industry, recommender systems suffer from the limitations of data sparsity and cold starts [42], e.g., some items are purchased or viewed by only a few users or none at all. In this regard, researchers have equipped recommender systems with the capability to leverage knowledge graphs (KGs), aiming to effectively integrate diverse structural knowledge to alleviate the limitations [45]. The uniqueness of KGbased recommender systems lies in mapping users, items, and attributes (e.g., brand or feature) as entities and utilizing semantic relations (e.g., also viewed or purchased) to link various types of entities [16]. By integrating these structural data into the recommendation process, KG-based recommendation methods not only improve the recommendation accuracy but also establish a foundation for the traceability of results [18].\nIn the literature, there has been a long line of studies in KGbased recommendation methods, such as embedding-based methods [9] [24] and neural network-based techniques [23] [12]. While these recommendation methods leverage structural information to achieve notable progress, they act as blackbox systems that lack explainability [51] [61]. To enhance the explainability, path-based recommendation methods [1] [32] first traverse entities and relations over KGs and then capture the connection patterns in user-item pairs, which can explicitly model user preferences for items [62] [2] [36]. However, these traditional path-based methods cannot explore all paths for each user-item pair due to the lack of dynamic decisionmaking ability [58].\nRecent advancements in KG-based recommender systems reveal that reinforcement learning (RL)-based recommendation methods achieve promising and explainable results over KGs [46]. Unlike traditional path-based methods, existing RLbased methods transform the recommendation task into a multi-hop reasoning process where the target is to train an agent to make optimal decisions (i.e., optimal selection of paths) and maximize the accumulated rewards (e.g., finding suitable items). Representatively, PGPR [51] trains an agent guided by a multi-hop scoring function to derive dynamic decisions from a given user to appropriate items. ADAC [61] builds upon the technical framework of PGPR, further integrating the generative adversarial imitation learning to enhance its convergence rate. Furthermore, subsequent work primarily focuses on carefully designed RL frameworks [41] [58] and convincing explanations [3] [46]. However, the performance of these methods is still limited by the following two challenges.\nChallenge I. The inability to capture contextual dependencies from neighboring information reduces the effectiveness of the item representation in the aforementioned RL-based methods. In fact, items with supplementary descriptions emerge as the most important entity type in KG-based recommender systems by virtue of their direct impact on recommendation results [20]. It is essential to capture valuable representations of items during the recommendation process. Specifically, (1) contextual dependencies from neighboring entities provide high-order semantic information to the current item's representations. On the one hand, the contextual dependencies from neighboring entities of the item type can capture the connection patterns and contextual semantics. By way of illustration, the item \"BULLS Shorts\" enhances the potential relevance of basketball and sports in the semantics by receiving the contextual information from the neighboring entities of the item type \"AJ III\" and \"Michael Jordan's Jersey\" in Fig. 1.\nOn the other hand, neighboring entities of the attribute type provide more supplementary information for the semantics of the current item. As shown in Fig. 1, the connection between \"BULLS Shorts\" and the attribute entity \"BULLS Clothing\" indicates that the Shorts are a piece of clothing associated with BULLS. (2) Contextual dependencies from neighboring item-categories contain the inherent meta-data of neighboring items, reflecting the transfer pattern of neighboring items. For example, \"BULLS Shorts\" are closely linked to the neighboring category of basketball equipment in the KG, which highlights cross-selling opportunities or complementary relationships. Obviously, capturing contextual dependencies from neighboring entities and item-categories simultaneously enhance the effectiveness of item representation.\nChallenge II. The second challenge is that the existing RLbased methods rely heavily on short recommendation chains (i.e., the maximum length of a recommendation path is commonly limited to 3) due to efficiency concerns. This limitation undermines the effectiveness of the recommendation policies and fails to satisfy the demand for long-sequence recommendations [62]. Specifically, (1) short path lengths can slightly alleviate the issue of inefficient recommendations by reducing the combination of semantic relations over KGs [51], but they limit the optimization of recommendation policies. First, the stability of the recommendation policy diminishes when these methods are unable to infer potential items beyond three-hop paths [66]. For instance, the target item \"Michael Jordan's Jersey\" is inferred via a 5-hop path, while the 3-hop item \"AJ Headband\" is not the answer for \"User 2\". Second, short paths involving only a few entities and relations are inadequate to precisely capture users' interests and preferences, which leads to false results [30] and misleads recommendation policies [45]. (2) In reality, users frequently browse and purchase products across multiple item-categories [5]. This practical demand necessitates recommender systems to adopt long recommendation paths to capture users' evolving interests across categories. However, it is non-trivial to directly extend the above single-agent RL-based recommendation methods [51] [61] [41] [58] [36] [46] [3] to scenarios involving recommendation long paths. This is because they tend to generate large action spaces as the path lengths increase, thereby impairing recommendation efficiency [21]. To make matters worse, these methods struggle to rapidly acquire sufficient reward signals over long recommendation paths, exacerbating the negative impact of the sparse reward dilemma on the RL framework [39]. Consequently, it is crucial to efficiently use long paths and improve the accuracy of explainable recommendations.\nIn light of the aforementioned challenges, we propose a novel model entitled CADRL (Category-aware Dual-Agent Reinforcement Learning). It draws inspiration from two key insights: (1) high-order entity representations can be derived from neighboring entities and categories in KGs [26]; (2) multi-agent RL with collaborative decision-making capabilities excels at efficiently finding targets over long paths [7] [56]. A notable distinction of our model from existing KGbased recommendation methods is that CADRL elegantly captures high-order representations of items from neighboring information as well as conducts explainable recommendations via a novel dual-agent RL framework. Specifically, CADRL contains two components. (1) To solve Challenge I, a category-aware gated graph neural network (CGGNN) is designed to generate context-aware item representations by jointly capturing contextual dependencies from neighboring entities and item-categories. Its gated graph neural network module adeptly extracts semantic information with low noise from the entity level. Then, a category-aware graph attention network module obtains shared features from the neighboring item-category level to form the final item representation. (2) To solve Challenge II, we propose a dual-agent reinforcement learning (DARL) framework to conduct explainable recommendations and efficiently discover suitable items through the collaborative reward mechanism and shared policy networks. In conclusion, this paper presents the following contributions:\n\u2022 To the best of our knowledge, we are the first to point out the necessity of exploiting contextual dependencies from neighboring information and the need for designing dual-agent RL to efficiently conduct explainable recommendations without relying solely on short paths. ", "publication_ref": ["b19", "b28", "b41", "b44", "b15", "b17", "b8", "b50", "b60", "b0", "b31", "b61", "b35", "b57", "b45", "b50", "b60", "b40", "b2", "b45", "b19", "b61", "b50", "b65", "b29", "b44", "b1", "b4", "b50", "b40", "b20", "b38", "b25"], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "II. RELATED WORK", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A. Reinforcement Learning", "text": "Reinforcement learning is a machine learning paradigm in which one or more agents learn to make decisions by interacting with the environment using MDPs [39]. Typically, the goal of RL is to use a policy network to approximate the decisions taken in different states to maximize the longterm cumulative reward [47]. Existing RL methods can be divided into two categories: single-agent RL and multi-agent RL methods [15]. In the former, only a single agent interacts with the environment during the learning process, and its decision-making is hindered by the limitations of the action space and sparse rewards [19]. The latter trains collaborative decision-making abilities and shares collective intelligence for multiple agents to address the issue of large action spaces [35], thereby efficiently using long sequences to complete tasks [7].", "publication_ref": ["b38", "b46", "b14", "b18", "b34", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "B. KG-based Recommendation", "text": "A KG is a semantic network composed of entities and relations, it has been widely used in recommender systems [65]. KG-based recommender systems, designed to fulfill the personalized demands of users, map the connections between users and items into a semantic structure [13]. Most existing KG-based recommendation methods first transform useritem and item-item pair interactions into semantic relations between entities in KGs [68] [27] [31] [53], and then fully exploit the implicit patterns in structural information to provide more accurate recommendation results for users. Generally, existing KG-based recommendation methods can be broadly categorized into two types: embedding-based and path-based methods [40]. Specifically, embedding-based methods utilize knowledge graph embedding [8] or neural network techniques [50] [55] [63] to enhance the representation of items and users. These methods demonstrate that KG-based recommender systems utilizing structured data can improve recommendation accuracy [23]. However, they cannot effectively model the connection patterns and interpret the recommended results using semantics in KGs [61]. Path-based methods use predefined connection patterns (e.g., meta-paths [44], meta-graphs [60], and rules [32]) to model the paths between users and items for recommendations. These methods capture complex relations among entities to improve the explainability of KG-based recommendation methods but face two limitations: (1) lack of dynamic decision-making capabilities; (2) fully exploring all paths for each user-item pair is impractical in large-scale KGs [51].", "publication_ref": ["b64", "b12", "b67", "b30", "b52", "b39", "b7", "b49", "b62", "b22", "b60", "b43", "b59", "b31", "b50"], "figure_ref": [], "table_ref": []}, {"heading": "C. RL-based Explainable Recommendations over KGs", "text": "Recently, RL has been applied to KG-based recommender systems to integrate recommendation and explainability by offering recommendation paths. Related methods transform the recommendation problem of finding suitable items for a user into a multi-hop reasoning problem, starting from an initial user entity to target item entities. Technically, these methods utilize Markov Decision Process (MDP) to dynamically discover the optimal paths over KGs. This decision process is as intuitive as \"taking a walk\" over KGs, which naturally yields explainable results for recommender systems. Specifically, PGPR [51] is the pioneering work applying RL to conduct explainable recommendations over KGs. It trains a single agent with an innovative soft reward strategy and action pruning strategy to conduct recommendations and path-finding simultaneously. Subsequent studies build upon this technical idea. ADAC [61] jointly models both demonstrations and historical user preferences to obtain accurate recommendation results with fast convergence. To alleviate the problem of huge action space, UCPR [41] adopts a multi-view reinforcement learning framework to simplify the path search process from the perspective of user demand. Considering that the entity representation cannot be updated by knowledge reasoning, INFER [58] first constructs a subgraph based on the reasoning paths to input the path information into the GNN layer, and then designs a joint learning framework to allow representation and recommendation modules to enhance each other. Additionally, to address the challenge that existing work ignores the sentiment on relations (e.g., user satisfaction), SAPL [36] designs a sentiment-aware reinforcement learning method to conduct accurate and convincing recommendations in sentiment-aware KGs. To further improve the accuracy and the rationality of the recommendation results, ReMR [46] and CogER [3] fully explore the structural information of KGs using multi-view RL and multi-system RL, respectively. Although these methods effectively perform explainable recommendations over KGs, they ignore the following details.\n(1) Contextual dependencies from both the entity and category levels can add high-order semantic information to the representation of items. (2) Single-agent RL methods tend to rely on short paths and ignore the diverse long path combinations, thereby limiting the performance of the recommendations.", "publication_ref": ["b50", "b60", "b40", "b57", "b35", "b45", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "III. PRELIMINARIES AND DEFINITIONS", "text": "A KG G = {E, R, T } is essentially a multi-relation graph, where E represents the entity set and R denotes a range of semantic relations. T = {(e s , r, e d ) | e s , e d \u2208 E, r \u2208 R} represents a collection of triplets in G, where e s , e d , and r denote the head entity, tail entity, and the semantic relation between these entities, respectively. Users, items, and attributes are mapped as entities as well as the interactions connecting them are modeled as semantic relations in the scenario of KG-based recommendation. Following previous studies [41] [52], the entity set E includes subsets such as the user set U, the item set V, the feature set F, and the brand set B, where U, V, F, B \u2286 E, U \u2229 V \u2229 F \u2229 B = \u2205. The relation set R comprises various types of relations (e.g. purchase or also bought) that capture semantic connections between entities. Following previous work [3], the inverse relation is also appended to the triplets, i.e., each triplet (e s , ", "publication_ref": ["b40", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "+ +", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Softmax Softmax", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Action Entity Agent", "text": "Category Agent r, e d ) is equivalent to the triplet (e d , r -1 , e s ). Without loss of generality, we ensure the reachability of recommendation paths by converting (?, r, e d ) to (e d , r -1 , ?). To facilitate a deep understanding of our methodology, we present our definitions as follows.\nu i i b i i i i Guidance u i b i u i b i R p e R p c R e R e R\nDefinition 1: Multi-hop Reasoning. Given a query (e s , r, ?), where \"?\" denotes the missing entity in KGs, the goal of multi-hop reasoning is to infer the missing entity by relation paths shorter or equal L hops, where L \u2208 Z \u22651 , Z represents the set of integers.\nDefinition 2: Contextual Dependency in KGs. Contextual dependency at the entity and entity-category levels reveals the inherent influence of neighboring topology and high-order semantics on entity representation in KGs. Given an entity v i and its representation h vi , the context-dependent processes propagated by its neighboring entity set N vi and neighboring category set N c vi are expressed respectively as\nh vi \u2190 f e (N vi ), h vi \u2190 f c (N c vi )\n, where f e and f c are propagation functions. Definition 3: Markov Decision Process. MDP is a mathematical framework in RL used to model the interaction between agents and their environment. It includes the state space S, action space A, transition function P , and reward function R. The agent selects an action a \u2208 A, transitions to the next state according to P , and receives a reward value.\nDefinition 4: Category Knowledge Graph. The category knowledge graph G c can be viewed as a dense virtual mapping of KG G. It is represented as a directed graph G c = (V c , R, T c ), where V c represents categories of items in G, R denotes semantic relations, and T c represents a collection of triplets in G c . Two categories are connected in G c if there is at least one relation r \u2208 R between entities in the two categories. G c captures hierarchical and taxonomic category association, providing high-level semantic knowledge.\nProblem: Our work focuses on RL-based explainable recommendations over KGs and uses RL technology to infer the recommended item set V u = {v u,i |i = 1, ..., n} for a specific user u \u2208 U through multi-hop reasoning. Semantic combinations in L-hop recommendation paths \"u\nr1 -\u2192 e 1 r2 -\u2192 ... r L -\u2192 v u,i\n\" provide explainable evidence for why a particular recommendation process is considered. The problem formulation is defined as follows:\n\u2022 Our inputs comprise the user set U, the item set V, the observed interactions V u , and G. \u2022 Our outputs are the recommended item set V u = {v u,i |i = 1, ..., n} and corresponding L-hop recommendation paths \u03c4 (u, v u,i ).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "IV. METHODOLOGY", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A. Overview of CADRL", "text": "As shown in Fig. 2, CADRL contains two components: (1) a category-aware gated graph neural network (CGGNN) that generates high-order item representations by capturing contextual dependencies from neighboring entities and categories in a low-noise manner; (2) a dual-agent reinforcement learning (DARL), which leverages collaborative agents to efficiently conduct explainable recommendation based on the high-order representation obtained from CGGNN. In what follows, we elaborate on the design of both components.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B. Category-aware Gated Graph Neural Network", "text": "Existing methods [51] [61] [41] [58] [36] [46] [3] are powerless to capture contextual dependencies from neighboring entities and categories simultaneously, which limits the effectiveness of item representation [67] [28]. To address the problem, we introduce CGGNN in this subsection. It jointly learns the semantics and topology of neighboring items and categories, generating high-order item representations in a low-noise manner. Compared with existing representation methods in the field of KG-based recommendation, the main technical difference of CADRL lies in the following two aspects. (1) We comprehensively consider the contextual dependencies from neighboring entities and categories, thereby providing extensive semantic and topological information for high-order item representations. (2) To ensure the effectiveness of high-order item representation, we incorporate a gated recurrent unit and an attention mechanism to minimize noise interference. Notably, CGGNN exclusively generates highorder representations for items without extending to other entity types. This design choice is motivated by three main factors. (1) Items directly influence recommendation outcomes and are pivotal within KG-based recommender systems [6].\n(2) Items have an obvious semantic hierarchy than other types of entities [20]. (3) This design principle minimizes computational costs during the calculation process, particularly in large-scale KG-based recommender systems.\nSpecifically, CGGNN includes a gated graph neural network (GGNN) module and a category-aware graph attention network (CGAN) module. Initial representations of all entities are obtained using TransE [4]. GGNN leverages the adaptive propagation layer and gated aggregation layer to capture finegrained contextual dependencies from neighboring entities. Subsequently, CGAN employs an attention mechanism to perform weighted calculations on common features among neighboring categories, thereby generating high-order contextaware representations for items.\n1) Gated Graph Neural Network: Typically, existing knowledge representation techniques utilize Graph Neural Networks (GNNs) to capture multi-hop contextual features in the field of knowledge graph completion [43] [14]. However, these solutions are not suitable for RL-based explainable recommendations over KG. This is because they overlook the fact that neighboring relations of an item provide varying semantic strength for shopping behavior [11]. For instance, the contextual information provided by \"AJ Basketball bought together -\u2192 Michael Jordan's Jersey\" holds more significance than \"AJ Basketball produced by", "publication_ref": ["b50", "b40", "b66", "b27", "b0", "b5", "b19", "b2", "b3", "b42", "b13", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "-\u2192", "text": "Air Jordan (AJ)\" when executing recommendations for \"User 2\" in Fig. 1. To make matters worse, existing GNN-based representation methods introduce noise into the recommendation process due to the semantic decay among multi-hop neighboring entities [34]. To address these issues, we design GGNN including an adaptive propagation layer and a gated aggregation layer.\nAdaptive Propagation Layer: we first obtain the representation t i,r,j of the triplet (v i , r, e j ), which encompasses sufficient topological information by concatenating the entity representations h vi , h ej and their relation representation h r [59]. Noted here, e j belongs to other entity types besides user, i.e., e j \u2208 E \u222a V \u222a F \u222a B. Then, we additionally integrate the purchase relation h rp into the triplet representation t i,r,j to assess the semantic strength of relevant relations with shopping behavior in context dependency. Based on this, we calculate semantic correlation between \u03b1 i,r . Next, we define the information propagation of neighboring entities in the k-th GNN layer. The above process is defined as follows,\nt i,r,j = \u03c3(W 1 [h vi \u2295 h ej \u2295 h r \u2295 h rp ])(1)\n\u03b1 i,r = \u03c3(W 2 t i,r,j + b)(2)\nn k vi = (r,j)\u2208Ni(vi) \u03b1 i,r W (k-1) in (h (k-1) ej \u2022 h (k-1) r ) + (r,j)\u2208No(vi) \u03b1 i,r W (k-1) out (h (k-1) ej \u2022 h (k-1) r )(3)\nwhere \u2295 and \u2022 represent the concatenation and elementwise product, respectively. \u03c3 denotes the sigmoid activation function. In addition, \u03b1 i,r is the attention weight of the triplet (v i , r, e j ). N i and N o represent the incoming and outgoing neighbors of v i , respectively. W k-1 in and W k-1 out \u2208 R d\u00d7d denote the transformation matrices, respectively. Based on this, we obtain the representation n k vi of contextual dependencies passed from neighboring entities to the item v i .\nGated Aggregation Layer: After obtaining the above representation, the gated aggregation layer fuses n vi with the self-embedding h vi to update the item's representation. Furthermore, we employ the Gated Recurrent Units (GRUs) to eliminate the noise introduced by the semantic decay of multihop neighbor entities in the update process. The calculation process is as follows:\nz (k) i = \u03c3(W z1 n (k) vi + W self h (k-1) vi ) (4) v(k) i = \u03c3(W v1 n (k) vi + W v2 h (k-1) vi )(5)\nv (k) i = tanh(W v1 n (k) vi + W v2 (v (k) i \u2022 h (k-1) vi ))(6)\nh(k) vi = (1 -z (k) i ) \u2022 h (k-1) vi + z (k) i \u2022 v (k) i(7)\nwhere W z1 , W v1 , W v1 , W self , W v2 , W v2 \u2208 R d\u00d7d are learnable parameters. z i and vi \u2208 R d are the update gate and reset gate, dictating the preservation and filtration of information during the fusion process. The item representation hvi that fully captures contextual dependency from neighboring entities is the trade-off embedding between h vi and n vi .\n2) Category-aware Graph Attention Network: Existing recommendation methods over KGs often ignore contextual dependencies from category-level, thereby failing to capture common item attributes [17]. To solve the above problem, we introduce CGAN, aimed at learning high-order semantic information from neighboring item-categories. Note that, CGAN only calculates the category information for items, as other entity types lack obvious semantic hierarchies in KGs [40].\nFollowing the existing work [51], we initialize itemcategories C 1 , C 2 ...C n using predefined category labels and then employ the mean embedding of all items within each item-category as the category representation. Subsequently, we utilize the LeakyReLU activation function to calculate the aggregation coefficient \u03b2 vicx between the current item v i and its neighboring categories in the m-th GNN layer:\n\u03b2 (m-1) vicx = LeakyRelu(W (m-1) ic ( h(m-1) vi \u2295 h (m-1) cx )) (8)\nwhere c x \u2208 N c vi represents the neighboring category for the item v i , and W ic is a weight matrix.\nNext, we further calculate the attention weight \u03b1 vicx as follows:\n\u03b1 (m-1) vicx = exp(\u03b2 (m-1) vicx ) cy\u2208N c v i exp(\u03b2 (m-1) vicy )(9)\nAfterwards, we can obtain the contextual dependency from neighboring categories,\nh (m) v c i = cx\u2208N c v i \u03b1 (m-1) vicx h (m-1) cx (10)\nFinally, we synthesize both types of contextual dependencies to update the final embedding of the item v i ,\nh vi = hvi + \u03b4h v c i (11)\nwhere \u03b4 serves as a trade-off factor. Based on this, CGGNN captures contextual dependencies from neighboring entities and categories simultaneously as well as generates a highorder representation for items.", "publication_ref": ["b33", "b58", "b16", "b39", "b50"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "C. Dual-agent Reinforcement Learning", "text": "Typically, existing RL-based explainable recommendation methods over KGs employ single-agent RL frameworks to conduct explainable recommendations over short paths [51] [61]. However, the insufficient feedback of reward signals and the large action space constrain the performance of these methods as path length increases. To address the above problems, we propose a novel dual-agent RL framework to efficiently find items over long recommendation paths. The main innovations of this component are twofold. (1) The novel dual-agent RL framework overcomes the inherent technical limitations of existing RL-based methods that rely on short paths. (2) Our RL framework with a collaborative paradigm provides sufficient reward signals and reduces action spaces, offering a new perspective for RL-based explainable recommendations.\nTechnically, DARL contains two agents: the category agent and the entity agent. The former quickly traverses over G c and is committed to finding the item-category where the target item is located, offering milestone-like category-level guidance to the latter to narrow the action spaces. The latter efficiently walks over various types of entities within the categories based on milestone-like hints to identify suitable items for the user. Moreover, the dual agents share historical paths and rewards with each other through shared policy networks and a collaborative reward mechanism. Consequently, DARL comprehensively leverages both global and local perspectives (i.e., category and entity views) to efficiently optimize the recommendation decision-making process.\n1) Category Agent: The category agent considers an itemcategory as an abstract node and selects different nodes as subsequent actions until reaching the target category. Note that, the target category is where the target item lies, while the initial category contains the item directly associated with the user. Category agent interacts with categories in G c through a 4-tuple of MDP (State, Action, Transition, Reward).\nSpecifically, each state at recommendation step l is represented as s c l = (u, c s , c l ), where c l \u2208 G c denotes the current category, and c s indicates the initial category. The state space S c is conceptualized as a collection of states. In addition, an action space A c contains the set of valid actions A c l at recommendation step l, which drives the category agent to move from the current category to the next. Formally, A c l is formulated as\nA c l = {c \u2032 , | (c l , c \u2032 ) \u2208 G c }.\nConsidering that the category-level recommendation path is shorter than that of the entity-level, the self-loop action is set to prevent infinite unrolling of paths and synchronize with the entity agent. The next state s c l+1 is updated by the previous state s c l according\nto the transition function P c : S c \u00d7 A c \u2192 S c . As a crucial element in the RL framework, the terminal reward function Rc of the category agent is defined as Rc = 1(c L ), where 1 is an indicator function, i.e., this agent receives a reward signal of 1 if it reaches the target category. Otherwise, the value of Rc is 0. Notably, the final reward R c of the category agent is defined in Eq. ( 20).\n2) Entity Agent: Similar to the single-agent explainable recommendation methods [51] [61] [41], the goal of the entity agent is to reach the appropriate items by traversing G. Formally, each state at the recommendation step l is denoted as s e l = (u, e l ), where e l \u2208 E, and U, V, F, B \u2286 E. Note that, if the current entity type is an item, its representation h v is derived from CGGNN, otherwise, the entity representation is generated by TransE. Furthermore, the terminal state is s e L = (u, v L ). For state s e l at the step l, the valid action of our entity agent is formulated as A e l = {(r \u2032 , e \u2032 ) | (e l , r \u2032 , e \u2032 ) \u2208 G}, where A e l \u2208 A e . In addition, the transition function P e of the entity agent defines the process from a current state to the next state. Within a maximum number of recommendation steps L, the reward is defined as Re = 1 Vu (e L ). The value of 1 Vu (e L ) is 1 when e L \u2208 V u and 0 when e L \u0338 \u2208 V u . Note that, the final reward R e of the entity agent is defined in Eq. (21).\n3) Shared Policy Networks: The proposed shared policy networks aim to learn path-finding policies to maximize the expected cumulative reward during recommendation. Specifically, we design two networks \u03c0 c \u03b8 and \u03c0 e \u03b8 to model the behavioral policies of category and entity agents, enabling the networks to achieve optimization goals collaboratively. Following [56], we first use LSTM to encode the searched trajectories and then calculate the historical sequence of the entity and category agents. For the category agent, the historical information at step l is y c l = {u 0 , c 0 , c 1 ,...,c l } \u2208 Y c . Similarly, the entity agent's historical information is defined as y e l = {u 0 , r 0 , e 0 ,..., r l , e l } \u2208 Y e . This historical information can be encoded by two separate LSTMs,\ny c 0 = LSTM c (0, [u 0 ; c 0 ]), y e 0 = LSTM e (0, [u 0 ; h r0 ; h e0 ])(12)\ny c l = LSTM c (W c [y c l-1 ; y e l-1 ], c l-1 ), l>0,(13)\ny e l = LSTM e (W e [y e l-1 ; y c l-1 ], [h r l-1 ; h e l-1 ]), l>0,(14)\nwhere W e and W c are transformation matrices that prevent the exponential increase of dimensions. As mentioned above, if the current entity e l is an item, its representation h v l is from CGGNN. For simplicity of representation, the representations of the different types of entities are uniformly denoted as h e . After modifying the internal structure of the LSTM to complete the history shaping, each hidden state of the agent ensures that necessary path information is shared with another agent to achieve coordinated action selection and the reduction of action spaces. The set of possible actions A c and A e are encoded by stacking the embeddings of all actions:\nA c \u2208 R |A c |\u00d77d , A e \u2208 R |A e |\u00d77d\n. The shared policy networks are defined as follows,\n\u03c0 c \u03b8 (a c l |s c l ) = softmax(A c l \u00d7 W c 2 ReLu(W c 1 [u 0 ; c l ; y c l ]))(15)\n\u03c0 e \u03b8 (a e l |s e l ) = softmax(A e l \u00d7 W e 2 ReLu(W e 1 [h e l ; h r l ; y e l ])) (16) Based on this, shared policy networks can simultaneously calculate category-level and entity-level information to predict the next action for dual agents.", "publication_ref": ["b50", "b0", "b50", "b40", "b20", "b55", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "4) Collaborative Reward Mechanism:", "text": "The default terminal rewards of the dual agents only consider whether they can achieve the pre-defined goal, neglecting the guidance and consistency of paths. This exacerbates the negative impact of sparse reward dilemma and large action spaces on the recommendation performance as the number of hops increases. Specifically, (1) Guidance: the entity agent requires stagewise guidance from the category agent to reduce the action space and enhance efficiency. (2) Consistency: the category agent traverses over G c with a densely connected structure, resulting in more diverse paths and difficulty maintaining consistency with entity level paths. However, the overlap of the paths of dual agents can enhance the effectiveness of their decision-making [38]. Based on this, we propose a collaborative reward mechanism in this subsection. It models the causal influence between dual agents by generating partner rewards, and then combines their respective terminal rewards to form final reward functions. The innovation of this collaborative reward mechanism lies in two key aspects: (1) allowing the category agent to provide efficient path-finding guidance for the entity agent by evaluating different actions [48]; (2) constraining the category agent to generate categorylevel movement trajectories consistent with entity-level paths.\nCounterfactual actions are introduced to model the path guidance from various categories. Specifically, we first define a joint probability for the potential actions of the entity agent, denoted as p(a e l |a c l , s e l ). Then, an optional action \u00e3c l within the action set A c l is introduced to replace the intervention of action a c l during the recommendation process. Next, we calculate the conditional probability distribution p(a e l |\u00e3 c l , s e l ) based on the action and state spaces. Essentially, the category agent spontaneously poses a retrospective question: how does the introduction of other categories affect the path-finding of the entity agent? Following this, the marginal probability p(a e l |s e l ) = \u00e3c l p(a e l |\u00e3 c l , s e l )p(\u00e3 c l |s e l ) without considering a c l is obtained by averaging the resulting probability distribution of the entity agent. Noted here, the discrepancy between p(a e l |s e l ) and p(a e l |a c l , s e l ) quantifies the causal impact of the category on the entity agent. Therefore, we utilize the Kullback-Leibler Divergence to formalize the above process and define partner reward R pc from the category agent to the entity agent at the recommendation step l. \nR pc l = 1 1 + exp(-\u03c6 l )(17)\nIn addition, the currently accessed category must be semantically similar to the item corresponding to the step l. We measure the state similarity of two agents through the cosine similarity function.\nR pe l = s c l \u2022 s e l \u2225s c l \u2225 \u2022 \u2225s e l \u2225(19)\nwhere R pe l is a partner reward to control the intensity of path consistency.\nThe final rewards of the category and entity agents at the step l are defined as R c (s c l ) and R e (s e l ), respectively.\nR c (s c l ) = Rc (s c L ) + \u03b1 pe R pe l , l \u2208 [1, L](20)\nR e (s e l ) = Re (s e L ) + \u03b1 pc R pc l , l \u2208 [1, L](21)\nwhere \u03b1 pe and \u03b1 pc denote reward discount factors. Note that, dual agents can obtain the sufficient reward at every step by integrating terminal and partner rewards, which ensures performance stability [67] [15]. Finally, we update the model parameters by using the REIN-FORCE algorithm [49] with the stochastic gradient. The dualagent RL framework fully utilizes shared policy networks and the collaborative reward mechanism to narrow the action space and eliminate the sparse reward dilemma, thereby enhancing the model's effectiveness and efficiency over KGs.", "publication_ref": ["b37", "b47", "b48"], "figure_ref": [], "table_ref": []}, {"heading": "V. EXPERIMENTS", "text": "In this section, we first introduce the experimental setup and baselines, followed by conducting experiments to investigate the following research questions (RQs). RQ1: Does the recommendation accuracy of CADRL consistently outperform state-of-the-art (SOTA) baselines on real-world datasets? RQ2: Is CADRL more efficient than existing RL-based recommendation methods over KGs? RQ3: How do the main components of CADRL affect recommendation performance? RQ4: What are the contributions of the technological modules in CADRL to recommendation performance? RQ5: What role does path length play in RL-based recommendation models? RQ6: What impact do different key hyper-parameters have on the performance of CADRL? RQ7: How do we evaluate the explainability of our proposed CADRL? A. Experimental Setup 1) Datasets: We report the recommendation results of CADRL on public real-world datasetsfoot_0 , i.e., Beauty, Clothing and Cell Phones datasets from the Amazon e-commerce data collection [33] [22]. Following the data processing in studies [61] [3], each dataset is constructed as an individual KG containing 4 types of entities (i.e., User, Item, Brand and Feature) and 14 types of relations (Purchase, Mention, Described by, Produced by, Also bought, Also viewed, Bought together, and their 7 corresponding inverse relations). Note that, categories belong to the top-level ontology in KGs and are not at the same level as entities [25] [54]. Thus, unlike prior work [51] that treats a category as an entity, our work regards the concept of a category as an abstract collection of items. Following the principle of data partitioning   II.\n2) Evaluation Protocol: Following previous studies [61] [46], we evaluate the performance of recommendation methods using widely-used metrics, including Precision (Prec.), Recall, Normalized Discounted Cumulative Gain (NDCG), and Hit Ratio (HR). The above indicator scores positively correlated with recommendation performance. Notably, we calculate the ranking metrics based on the top-10 recommended items for each user in the test set.\n3) Hyper-Parameters: Similar to existing methods [61] [3], CADRL is trained for 50 epochs using Adam optimizer with a learning rate of 0.0001 across all datasets. In addition, the embedding size for all entities, relations, and categories is set to 100. The maximum sizes for A c and A e are set to 10 and 50, respectively. Amazon's metadata has defined item's category details [51], which determines the number of categories obtained by grouping the items to be 248, 206, and 1,193 on Beauty, Cell Phones, and Clothing, respectively. Moreover, the number of GNN layers k, m, reward discount factors \u03b1 pe , \u03b1 pc and the maximum recommendation length L are set to [3, 2, 0.6, 0.5, 6] and [3, 2, 0.4, 0.5, 6] on Beauty and Cell Phones, respectively, while they are set to [3, 2, 0.4, 0.4, 7] on the Clothing dataset, respectively. The trade-off factors \u03b4 are set to 0.3, 0.4, and 0.4 on Clothing, Cell Phones, and Beauty, respectively. These parameters represent the optimal values obtained through parameter tuning.", "publication_ref": ["b32", "b21", "b60", "b24", "b53", "b50", "b60", "b50"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "B. Baselines", "text": "To investigate the performance of CADRL, we use the following four categories of recommendation models as baselines in our experimental comparisons.\n\u2022 KG embedding models: CKE [57] extracts structural representations of items by integrating the heterogeneous information from KGs, while KGAT [45] refines embeddings through the GAT mechanism to capture high-order connectivities within KGs. \u2022 Neural networks-based models: DeepCoNN [64] utilizes cooperative deep neural networks to encode both users and items for rating prediction. In addition, RippleNet [42] iteratively expands the user's potential interests along relations in an end-to-end manner, which can be used to calculate the final purchase probability for users. \u2022 Traditional path models: RuleRec [32] constructs a ruleguided mechanism to predict items based on the metaknowledge of recommendation paths. HeteroEmbed [1] is a path-based SOTA recommendation model that traverses potential recommendation paths over KGs by learning user-item connective patterns. \u2022 RL-based models: This group of models is closely related to our work and is elaborated upon in Section II. We compare not only the well-known PGPR [51], CAFE [52], and ADAC [61], but also the latest models in this domain, such as INFER [58], ReMR [46], UCPR [41],\nand CogER [3]. Notably, UCPR [41] is a SOTA RL-based method that demonstrates strong performance over KGs.", "publication_ref": ["b56", "b44", "b63", "b41", "b31", "b0", "b50", "b51", "b60", "b57", "b45", "b40", "b2", "b40"], "figure_ref": [], "table_ref": []}, {"heading": "C. Recommendation Accuracy (RQ1)", "text": "In this subsection, the results of recommendation accuracy are illustrated in Table I (all scores are reported as percentages). The most competitive baseline results are underlined, while the best results are highlighted in bold. Specifically, we provide the following insightful analyses. (1) CADRL consistently surpasses existing baselines across all metrics on realworld datasets. This performance advantage primarily arises from two key aspects. Firstly, the high-order item representation of CADRL fully captures contextual dependencies and  . This sparsity within categories weakens the effectiveness of CADRL in leveraging categories as optimal metadata for inferring user preferences.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D. Efficiency Study (RQ2)", "text": "Following previous work [52], we investigate efficiency by comparing the time consumption of various methods in recommendation and pathfinding. To ensure a fair comparison, we use only path-based and RL-based methods with their optimal parameter settings, including the SOTA path-based method HeteroEmbed and the 3-hop RL-based explainable recommendation methods PGPR, UCPR, and CAFE. Specifically, the efficiency of these methods is evaluated using two metrics: (1) the empirical running time to infer suitable items for 1k users; and (2) the time consumed to generate 10k recommendation paths. The results are reported in Table III.\nWe first observe that CADRL exhibits the fastest performance across all test cases. This is because CADRL reduces the action space through the collaboration of dual agents at both the entity and category levels. Furthermore, the ad-ditional path sorting process increases the running time of PGPR, resulting in its inefficiency in both recommendation and pathfinding. Different from Table I ", "publication_ref": ["b51"], "figure_ref": [], "table_ref": ["tab_4", "tab_2"]}, {"heading": "E. Ablation Study (RQ3)", "text": "The primary objective of the ablation study is to assess the contributions of two components (i.e., CGGNN and DARL) in CADRL. Given the necessity of the entity agent for explainable recommendations, the first variant CADRL w/o DARL only retains the structure of a single agent and masks the participation of the category agent in the policy networks and reward functions. Compared to existing RL-based explainable recommendation methods, the RL design that only includes binary terminal rewards of CADRL w/o DARL is the simplest and most basic. Additionally, CADRL w/o CGGNN removes CGGNN and adopts solely the static representation generated by TransE. Table IV presents the experimental results of the CADRL and its variants.\nWe have the following observations. (1) Undoubtedly, CADRL w/o DARL without the involvement of the dualagent collaboration mechanism has the lowest performance among the above methods. One potential explanation is that it lacks stage-wise guidance from the category agent and easily suffers from sparse reward dilemmas [7]. (2) Although the performance of CADRL w/o DARL is diminished due to Notably, with the exclusion of the dual-agent collaboration mechanism, the reward function of CADRL w/o DARL consists solely of basic terminal rewards, which do not address the sparse reward dilemma. To report in detail the impact of the reward function as well as the other modules on the recommendation effectiveness of CADRL, we perform the effectiveness analysis in the following subsection.", "publication_ref": ["b6", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "F. Effectiveness Analysis (RQ4)", "text": "To investigate the impact of the modules on the aforementioned components, we introduce several variants of the method by removing specific technology modules to conduct an effectiveness analysis. Specifically, we first study the recommendation performance of two modules within CGGNN, namely the Gated Graph Neural Network (GGNN) and the Category-aware Graph Attention Network (CGAN). Subsequently, we explore the effectiveness of the Shared Policy Networks (SPN) and the Collaborative Reward Mechanism (CRM) within DARL. Note that, the experimental results in this subsection are conducted on Beauty and Cell Phones to observe obvious numerical differences.\n1) The contribution of various modules in CGGNN: GGNN and CGAN capture contextual dependencies at the entity level and category level, respectively, aiming to generate highorder representations of items. To verify the effectiveness of these modules, we design the following variant models: (1) RGGNN: this variant removes only the GGNN from CADRL, preventing the capture of entity-level contextual dependencies and relying solely on static representations by TransE for items. (2) RCGAN: only the CGAN module is removed from CADRL, leaving only GGNN in this variant model to produce item representations.\nThe results are presented in Fig. 3. There are three main discoveries. (1) The variant models outperform the SOTA baseline UCPR in all metrics, which illustrates the recommendation effectiveness of both modules in CGGNN. (2) The experimental results for all indicators of RCGAN surpass those of RGGNN, indicating that the contribution of GGNN to the model's performance is overall greater than that of CGAN. One potential reason is that entity-level context dependence can encapsulate basic graph structure and finegrained semantic information [58]. (3) The performance of CADRL surpasses that of the variant models, underscoring the crucial role of the mutual effect between two-level contextual dependencies in enhancing recommendation accuracy.\n2) The contribution of various modules in DARL: The innovation of DARL is that the two agents collaboratively utilize historical interaction information and partner rewards. Considering the necessity of reward functions for the RL framework, we design the following variant models to further study SPN and CRM by removing the technical modules. (1) RSHI: the shared historical information of the dual agents is entirely removed, i.e., y is not calculated in Eq. ( 15) and (16). By comparing the performance of RSHI and CADRL, we can analyze the impact of the collaboration strategy in the SPN module on CADRL. (2) RCRM: we remove the partner rewards in all reward functions of this variant, i.e., only the terminal reward is included in Eq. ( 20) and (21).\nAs illustrated in Fig. 4, we have the following insightful analysis: (1) Undoubtedly, CADRL maintains the highest performance among all methods, demonstrating its recommendation robustness. (2) The performance of RSHI is generally higher than that of RCRM but lower than that of CADRL. The reason is that the absence of shared historical information hinders DARL from capturing global preference sequences, thereby diminishing recommendation accuracy [51].\n(3) RCRM outperforms the SOTA baseline UCPR, but its reliance on binary terminal rewards leads to the sparse reward dilemma. In addition, compared with RCRM, the performance advantage of CADRL illustrates that the collaborative reward mechanism in the DARL component has the ability to solve this dilemma.", "publication_ref": ["b0", "b1", "b57", "b15", "b20", "b50"], "figure_ref": [], "table_ref": []}, {"heading": "G. Path Length Study (RQ5)", "text": "Existing RL-based explainable recommendation methods follow the idea of multi-hop reasoning over KGs [65] [30] to  limit the recommendation path length. We argue that explainable recommendation methods over KGs encounter a more complex real-world scenario than multi-hop reasoning, where a user's interaction sequence often exceeds three hops [10].\nTo evaluate the rationality and the effectiveness of path length, we analyze the recommendation performance by extending the path length of various RL-based explainable recommendation methods in this subsection.\nThe experimental results are shown in Fig. 5, from which we derive the following insightful analysis. (1) CADRL breaks through the path-length limitation of existing RL-based explainable recommendation methods (i.e., their maximum path length is 3), achieving superior recommendation performance in longer paths. This success comes from the dual-agent RL collaboratively exploiting the latent semantic combination over extended paths to maintain performance advantages on all datasets. (2) The optimal path length of CADRL on the Clothing, Cell Phones and Beauty datasets are 7, 6 and 6, respectively. The growth trend accelerates initially and then decelerates, eventually turning negative upon reaching the optimal value. This phenomenon is attributed to the noise intervention along the extended path in the recommendation process. (3) Existing RL-based baselines set the maximum path length to 3 through a hyperparameter. To compare the impact of path length, we increase the maximum length in the baselines to observe the experimental result. In Fig. 5, the baselines produce a large number of incorrect results and reduce their performance when the maximum length exceeds three hops. This is because these single-agent RL baselines suffer from the sparse reward dilemma and semantic dilution over extended paths [39].", "publication_ref": ["b64", "b9", "b38"], "figure_ref": ["fig_3", "fig_3"], "table_ref": []}, {"heading": "H. Impact of Parameters (RQ6)", "text": "To answer RQ6, we perform parameter sensitivity analysis in this subsection. Notably, we investigate the performance fluctuations of CADRL with the most critical hyper-parameters due to space constraints.\n1) Impact of the trade-off factor \u03b4: Fig. 6(a) illustrates the experimental results with varying trade-off factors \u03b4. As the value of \u03b4 changes, CADRL exhibits varying degrees of fluctuation in NDCG across different datasets. Specifically, the optimal \u03b4 values are 0.3, 0.4, and 0.4 for Clothing, Cell Phones, and Beauty, respectively. The optimal \u03b4 value on Clothing is slightly lower than that on the other datasets. The above experimental results indicate that the number of categories is negatively correlated with the value of \u03b4. This is because a small \u03b4 value prevents CADRL from over-relying on category information when generating item representations in the dataset with more categories.\n2) Impact of the reward discount factor \u03b1 pe : To further investigate the collaborative reward mechanism, we evaluate the impact of the reward discount factor \u03b1 pe in Eq. (20). As illustrated by the results in Fig. 6(b), the optimal \u03b1 pe values are 0.6, 0.4, and 0.4 for Clothing, Cell Phones, and Beauty, respectively. The key observations are as follows. (1) The \u03b1 pe value is the highest on the Clothing dataset. One potential reason is that an appropriate \u03b1 pe value can effectively alleviate the negative impact of item sparsity within each category on the path consistency of dual agents in this dataset. (2) The performance of CADRL fluctuates obviously on the Beauty dataset due to the data sparsity. Nevertheless, the performance of CADRL across different \u03b1 pe values generally surpasses that of existing SOTA baselines, which further demonstrates the robustness of CADRL performance. 3) Impact of the reward discount factor \u03b1 pc : Similarly, we investigate the optimal \u03b1 pc value on different datasets. The experimental results displayed in Fig. 6(c) reveal that CADRL's performance initially improves and then declines across all datasets. This suggests that an appropriate \u03b1 pc value can enhance the effectiveness of stage-wise guidance from the category agent. Consequently, the optimal values are determined to be 0.5, 0.5, and 0.4 for Clothing, Cell Phones, and Beauty, respectively.", "publication_ref": ["b19", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "I. Case Study (RQ7)", "text": "To investigate how CADRL utilizes the entity and category levels to perform explainable recommendations on KGs, we provide a real-world case study as shown in Fig. 7. The area above the grey horizontal line represents the abstract categories to which the items belong, while the area below is a KG fragment in the Beauty dataset. We derive the following key insights by observing these intuitive recommendation paths.\n(1) Compared with PGPR and UCPR using the optimal path length setting, CADRL can use a 5-hop path to accurately find a suitable item for the user. The category agent captures the cross-selling opportunities between Cleanser and Face, while the entity agent combines the semantic associations of items and purchase behaviors in KG by receiving guidance and sharing path information. Dual agents collaboratively explore the next step at different levels until they reach the final goal, naturally forming an explainable provenance for the recommendation results. (2) The green and purple paths in Fig. 7 intuitively demonstrate the \"myopic\" limitation of existing RLbased explainable recommendation methods. This can explain why there are many meaningless paths and false results in the output of existing RL-based methods over KGs. In contrast, the category agent can act as a \"myopic glasses\" for the entity agent in our proposed model, further correcting CADRL's \"visual acuity\" (i.e., accurately find potential recommendation results beyond 3 hops).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "VI. CONCLUSION AND FUTURE WORK", "text": "In this work, we study the problem how to leverage contextual dependencies to efficiently conduct explainable recommendations without relying solely on short paths. To address this issue, we propose CADRL, which comprises CGGNN and DARL. Specifically, CGGNN captures fine-grained contextual dependencies within the neighboring structure from both the entity and category levels, generating high-order representations for items. To provide users with more suitable items by diverse semantic combinations on KGs, DARL introduces a dual-agent RL structure with a collaborative mechanism to explore long paths from the category and entity levels. Through extensive experiments on several real-world datasets, we draw two valuable conclusions. First, CADRL surpasses SOTA baselines in terms of efficiency and effectiveness in explainable recommendation tasks. Second, CADRL can infer more suitable items using long paths with the support of a collaborative dual-agent structure. In future work, we would like to employ large language models to perform personalized recommendations by modeling the complex evolution of user interests over KGs.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Learning heterogeneous knowledge base embeddings for explainable recommendation", "journal": "Algorithms", "year": "2018", "authors": "Q Ai; V Azizi; X Chen; Y Zhang"}, {"ref_id": "b1", "title": "Reinforcement recommendation reasoning through knowledge graphs for explanation path quality", "journal": "Knowl. Based Syst", "year": "2023", "authors": "G Balloccu; L Boratto; G Fenu; M Marras"}, {"ref_id": "b2", "title": "Cognition-aware knowledge graph reasoning for explainable recommendation", "journal": "", "year": "2023", "authors": "Q Bing; Q Zhu; Z Dou"}, {"ref_id": "b3", "title": "Translating embeddings for modeling multi-relational data", "journal": "NeurIPS", "year": "2013", "authors": "A Bordes; N Usunier; A Garc\u00eda-Dur\u00e1n; J Weston; O Yakhnenko"}, {"ref_id": "b4", "title": "Category-aware collaborative sequential recommendation", "journal": "", "year": "2021", "authors": "R Cai; J Wu; A San; C Wang; H Wang"}, {"ref_id": "b5", "title": "Explicable recommendation based on knowledge graph", "journal": "Expert Syst. Appl", "year": "2022", "authors": "X Cai; L Xie; R Tian; Z Cui"}, {"ref_id": "b6", "title": "Multi-agent reinforcement learning: A review of challenges and applications", "journal": "Applied Sciences", "year": "2021", "authors": "L Canese; G C Cardarilli; L Di Nunzio; R Fazzolari; D Giardino; M Re; S Span\u00f2"}, {"ref_id": "b7", "title": "Knowledge graph embedding: A survey from the perspective of representation spaces", "journal": "ACM Comput. Surv", "year": "2024", "authors": "J Cao; J Fang; Z Meng; S Liang"}, {"ref_id": "b8", "title": "Unifying knowledge graph learning and recommendation: Towards a better understanding of user preferences", "journal": "WWW", "year": "2019", "authors": "Y Cao; X Wang; X He; Z Hu; T Chua"}, {"ref_id": "b9", "title": "Sequential recommendation with graph neural networks", "journal": "", "year": "2021", "authors": "J Chang; C Gao; Y Zheng; Y Hui; Y Niu; Y Song; D Jin; Y Li"}, {"ref_id": "b10", "title": "Ir-rec: An interpretive rulesguided recommendation over knowledge graph", "journal": "Inf. Sci", "year": "2021", "authors": "J Chen; J Yu; W Lu; Y Qian; P Li"}, {"ref_id": "b11", "title": "Attentive knowledge-aware graph convolutional networks with collaborative guidance for personalized recommendation", "journal": "", "year": "2022", "authors": "Y Chen; Y Yang; Y Wang; J Bai; X Song; I King"}, {"ref_id": "b12", "title": "A comprehensive survey of knowledge graph-based recommender systems: Technologies, development, and contributions", "journal": "Inf", "year": "2021", "authors": "J Chicaiza; P V D\u00edaz"}, {"ref_id": "b13", "title": "MRGAT: multirelational graph attention network for knowledge graph completion", "journal": "Neural Networks", "year": "2022", "authors": "G Dai; X Wang; X Zou; C Liu; S Cen"}, {"ref_id": "b14", "title": "A survey on multi-agent deep reinforcement learning: from the perspective of challenges and applications", "journal": "Artif. Intell. Rev", "year": "2021", "authors": "W Du; S Ding"}, {"ref_id": "b15", "title": "Metakg: Meta-learning on knowledge graph for cold-start recommendation", "journal": "IEEE Trans. Knowl. Data Eng", "year": "2023", "authors": "Y Du; X Zhu; L Chen; Z Fang; Y Gao"}, {"ref_id": "b16", "title": "Deep reinforcement learning framework for category-based item recommendation", "journal": "IEEE Trans. Cybern", "year": "2022", "authors": "M Fu; A Agrawal; A A Irissappane; J Zhang; L Huang; H Qu"}, {"ref_id": "b17", "title": "Enhanced multitask learning and knowledge graph-based recommender system", "journal": "IEEE Trans. Knowl. Data Eng", "year": "2023", "authors": "M Gao; J Li; C Chen; Y Li; J Zhang; Z Zhan"}, {"ref_id": "b18", "title": "Multi-agent deep reinforcement learning: a survey", "journal": "Artif. Intell. Rev", "year": "2022", "authors": "S Gronauer; K Diepold"}, {"ref_id": "b19", "title": "A survey on knowledge graph-based recommender systems", "journal": "IEEE Trans. Knowl. Data Eng", "year": "2022", "authors": "Q Guo; F Zhuang; C Qin; H Zhu; X Xie; H Xiong; Q He"}, {"ref_id": "b20", "title": "Exploration in deep reinforcement learning: From single-agent to multiagent domain", "journal": "IEEE Transactions on Neural Networks and Learning Systems", "year": "2023", "authors": "J Hao; Y Tianpei; H Tang; C Bai; P Liu; Z Wang"}, {"ref_id": "b21", "title": "Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering", "journal": "WWW", "year": "2016", "authors": "R He; J J Mcauley"}, {"ref_id": "b22", "title": "Knowledge-aware coupled graph neural network for social recommendation", "journal": "", "year": "2021", "authors": "C Huang; H Xu; Y Xu; P Dai; L Xia; M Lu; L Bo; H Xing; X Lai; Y Ye"}, {"ref_id": "b23", "title": "Improving sequential recommendation with knowledge-enhanced memory networks", "journal": "", "year": "2018", "authors": "J Huang; W X Zhao; H Dou; J Wen; E Y Chang"}, {"ref_id": "b24", "title": "Improving knowledge graph embeddings with ontological reasoning", "journal": "", "year": "2021", "authors": "N Jain; T Tran; M H Gad-Elrab; D Stepanova"}, {"ref_id": "b25", "title": "A survey on knowledge graphs: Representation, acquisition, and applications", "journal": "IEEE Trans. Neural Networks Learn. Syst", "year": "2022", "authors": "S Ji; S Pan; E Cambria; P Marttinen; P S Yu"}, {"ref_id": "b26", "title": "Discovering collaborative signals for next POI recommendation with iterative seq2graph augmentation", "journal": "", "year": "2021", "authors": "Y Li; T Chen; Y Luo; H Yin; Z Huang"}, {"ref_id": "b27", "title": "Learning knowledge graph embedding with heterogeneous relation attention networks", "journal": "IEEE Trans. Neural Networks Learn. Syst", "year": "2022", "authors": "Z Li; H Liu; Z Zhang; T Liu; N N Xiong"}, {"ref_id": "b28", "title": "Learning compact compositional embeddings via regularized pruning for recommendation", "journal": "", "year": "2023", "authors": "X Liang; T Chen; Q V H Nguyen; J Li; H Yin"}, {"ref_id": "b29", "title": "Multi-hop knowledge graph reasoning with reward shaping", "journal": "", "year": "2018", "authors": "X V Lin; R Socher; C Xiong"}, {"ref_id": "b30", "title": "Self-supervised dynamic hypergraph recommendation based on hyper-relational knowledge graph", "journal": "", "year": "2023", "authors": "Y Liu; H Xuan; B Li; M Wang; T Chen; H Yin"}, {"ref_id": "b31", "title": "Jointly learning explainable rules for recommendation with knowledge graph", "journal": "WWW", "year": "2019", "authors": "W Ma; M Zhang; Y Cao; W Jin; C Wang; Y Liu; S Ma; X Ren"}, {"ref_id": "b32", "title": "Image-based recommendations on styles and substitutes", "journal": "", "year": "2015", "authors": "J J Mcauley; C Targett; Q Shi; A Van Den;  Hengel"}, {"ref_id": "b33", "title": "Learning attentionbased embeddings for relation prediction in knowledge graphs", "journal": "", "year": "2019", "authors": "D Nathani; J Chauhan; C Sharma; M Kaul"}, {"ref_id": "b34", "title": "A review of cooperative multi-agent deep reinforcement learning", "journal": "Appl. Intell", "year": "2023", "authors": "A Oroojlooy; D Hajinezhad"}, {"ref_id": "b35", "title": "Reinforcement learning over sentiment-augmented knowledge graphs towards accurate and explainable recommendation", "journal": "", "year": "2022", "authors": "S Park; D Chae; H Bae; S Park; S Kim"}, {"ref_id": "b36", "title": "A systematic review and research perspective on recommender systems", "journal": "J. Big Data", "year": "2022", "authors": "D Roy; M Dutta"}, {"ref_id": "b37", "title": "PRIMAL: pathfinding via reinforcement and imitation multi-agent learning", "journal": "IEEE Robotics Autom. Lett", "year": "2019", "authors": "G Sartoretti; J Kerr; Y Shi; G Wagner; T K S Kumar; S Koenig; H Choset"}, {"ref_id": "b38", "title": "Reinforcement learning algorithms: A brief survey", "journal": "Expert Syst. Appl", "year": "2023", "authors": "A K Shakya; G Pillai; S Chakrabarty"}, {"ref_id": "b39", "title": "A survey of research hotspots and frontier trends of recommendation systems from the perspective of knowledge graph", "journal": "Expert Syst. Appl", "year": "2021", "authors": "B Shao; X Li; G Bian"}, {"ref_id": "b40", "title": "User-centric path reasoning towards explainable recommendation", "journal": "", "year": "2021", "authors": "C Tai; L Huang; C Huang; L Ku"}, {"ref_id": "b41", "title": "Ripplenet: Propagating user preferences on the knowledge graph for recommender systems", "journal": "", "year": "2018", "authors": "H Wang; F Zhang; J Wang; M Zhao; W Li; X Xie; M Guo"}, {"ref_id": "b42", "title": "GRL: knowledge graph completion with gan-based reinforcement learning", "journal": "Knowl. Based Syst", "year": "2020", "authors": "Q Wang; Y Ji; Y Hao; J Cao"}, {"ref_id": "b43", "title": "Meta-path enhanced knowledge graph convolutional network for recommender systems", "journal": "ICBK", "year": "2021", "authors": "R Wang; M Wu; S Ji"}, {"ref_id": "b44", "title": "KGAT: knowledge graph attention network for recommendation", "journal": "", "year": "2019", "authors": "X Wang; X He; Y Cao; M Liu; T Chua"}, {"ref_id": "b45", "title": "Multi-level recommendation reasoning over knowledge graphs with reinforcement learning", "journal": "WWW", "year": "2022", "authors": "X Wang; K Liu; D Wang; L Wu; Y Fu; X Xie"}, {"ref_id": "b46", "title": "Deep reinforcement learning: A survey", "journal": "IEEE Trans. Neural Networks Learn. Syst", "year": "2024", "authors": "X Wang; S Wang; X Liang; D Zhao; J Huang; X Xu; B Dai; Q Miao"}, {"ref_id": "b47", "title": "Variational counterfactual prediction under runtime domain corruption", "journal": "IEEE Trans. Knowl. Data Eng", "year": "2024", "authors": "H Wen; T Chen; L K Chai; S Sadiq; J Gao; H Yin"}, {"ref_id": "b48", "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "journal": "Machine Learning", "year": "1992", "authors": "R J Williams"}, {"ref_id": "b49", "title": "Graph neural networks in recommender systems: A survey", "journal": "ACM Comput. Surv", "year": "2023", "authors": "S Wu; F Sun; W Zhang; X Xie; B Cui"}, {"ref_id": "b50", "title": "Reinforcement knowledge graph reasoning for explainable recommendation", "journal": "", "year": "2019", "authors": "Y Xian; Z Fu; S Muthukrishnan; G De Melo; Y Zhang"}, {"ref_id": "b51", "title": "CAFE: coarse-to-fine neural symbolic reasoning for explainable recommendation", "journal": "", "year": "2020", "authors": "Y Xian; Z Fu; H Zhao; Y Ge; X Chen; Q Huang; S Geng; Z Qin; G De Melo; S Muthukrishnan; Y Zhang"}, {"ref_id": "b52", "title": "Knowledge enhancement for contrastive multi-behavior recommendation", "journal": "", "year": "2023", "authors": "H Xuan; Y Liu; B Li; H Yin"}, {"ref_id": "b53", "title": "A domain knowledge graph construction method based on wikipedia", "journal": "J. Inf. Sci", "year": "2021", "authors": "H Yu; H Li; D Mao; Q Cai"}, {"ref_id": "b54", "title": "Hetefedrec: Federated recommender systems with model heterogeneity", "journal": "", "year": "2024", "authors": "W Yuan; L Qu; L Cui; Y Tong; X Zhou; H Yin"}, {"ref_id": "b55", "title": "Learning to walk with dual agents for knowledge graph reasoning", "journal": "", "year": "2022", "authors": "D Zhang; Z Yuan; H Liu; X Lin; H Xiong"}, {"ref_id": "b56", "title": "Collaborative knowledge base embedding for recommender systems", "journal": "", "year": "2016", "authors": "F Zhang; N J Yuan; D Lian; X Xie; W Ma"}, {"ref_id": "b57", "title": "A joint framework for explainable recommendation with knowledge reasoning and graph representation", "journal": "DASFAA", "year": "2022", "authors": "L Zhang; R Fang; T Yang; M Hu; T Li; C Shi; D Wang"}, {"ref_id": "b58", "title": "Knowledge graph reasoning with relational digraph", "journal": "WWW", "year": "2022", "authors": "Y Zhang; Q Yao"}, {"ref_id": "b59", "title": "Meta-graph based recommendation fusion over heterogeneous information networks", "journal": "", "year": "2017", "authors": "H Zhao; Q Yao; J Li; Y Song; D L Lee"}, {"ref_id": "b60", "title": "Leveraging demonstrations for reinforcement recommendation reasoning over knowledge graphs", "journal": "", "year": "2020", "authors": "K Zhao; X Wang; Y Zhang; L Zhao; Z Liu; C Xing; X Xie"}, {"ref_id": "b61", "title": "Time-aware path reasoning on knowledge graph for recommendation", "journal": "ACM Trans. Inf. Syst", "year": "2023", "authors": "Y Zhao; X Wang; J Chen; Y Wang; W Tang; X He; H Xie"}, {"ref_id": "b62", "title": "Keyword-aware continuous knn query on road networks", "journal": "", "year": "2016", "authors": "B Zheng; K Zheng; X Xiao; H Su; H Yin; X Zhou; G Li"}, {"ref_id": "b63", "title": "Joint deep modeling of users and items using reviews for recommendation", "journal": "", "year": "2017", "authors": "L Zheng; V Noroozi; P S Yu"}, {"ref_id": "b64", "title": "Multi-hop knowledge graph reasoning in few-shot scenarios", "journal": "IEEE Trans. Knowl. Data Eng", "year": "2024", "authors": "S Zheng; W Chen; W Wang; P Zhao; H Yin; L Zhao"}, {"ref_id": "b65", "title": "MMKGR: multi-hop multi-modal knowledge graph reasoning", "journal": "", "year": "2023", "authors": "S Zheng; W Wang; J Qu; H Yin; W Chen; L Zhao"}, {"ref_id": "b66", "title": "DREAM: adaptive reinforcement learning based on attention mechanism for temporal knowledge graph reasoning", "journal": "", "year": "2023", "authors": "S Zheng; H Yin; T Chen; Q V H Nguyen; W Chen; L Zhao"}, {"ref_id": "b67", "title": "Improving conversational recommender systems via knowledge graph based semantic fusion", "journal": "", "year": "2020", "authors": "K Zhou; W X Zhao; S Bian; Y Zhou; J Wen; J Yu"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Fig. 1 .1Fig. 1. A small fragment of a knowledge graph formed by the interactions of multiple users with the same shopping preferences. The red arrow is a 5-hop recommendation path starting from User 2 to a recommended item. To easily follow our work, we exemplify this fragment throughout the paper.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Fig. 5 .5Fig. 5. NDCG of the varying recommendation step L for RL-based models.", "figure_data": ""}, {"figure_label": "67", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Fig. 6 .Fig. 7 .67Fig. 6. The performance of CADRL with the varying number of key hyper-parameters on different datasets.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "We conduct extensive experiments on several real-world benchmark datasets. The results showcase the superiority of CADRL compared to state-of-the-art baselines.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Overview of CADRL. Different colored circles represent various types of entities, including items, brands, features, and users. After obtaining high-order representations of items from CGGNN, DARL fully interacts with the KG. Dual agents receive the state and reward as well as output the next action until the appropriate items are inferred.", "figure_data": ""}, {"figure_label": "I", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "OF RECOMMENDATION ACCURACY ON AMAZON DATASETS.", "figure_data": "ClothingCell PhonesBeautyModelNDCGRecallHRPrec.NDCGRecallHRPrec.NDCGRecallHRPrec.CKE1.2072.0263.4240.3123.4215.9979.2690.9173.0874.8319.2751.149KGAT2.4394.1426.0270.6034.3457.68610.9461.0925.0238.37814.1811.531DeepCoNN1.0661.8852.6650.1863.1645.4788.4060.8542.7324.5618.2311.013RippleNet1.7493.1884.8260.4934.1356.6569.6810.9374.3686.89412.1801.417RuleRec1.8173.2054.9150.5044.3326.7799.7821.0224.4726.93212.2741.479HeteroEmbed2.5144.4826.4610.6214.5857.91311.4811.1245.2218.74114.4131.612PGPR2.3623.9635.8520.5964.3407.24810.2461.0894.6187.53812.8141.492ReMR2.5344.4966.5170.6354.6017.96311.5971.1885.2798.77314.6861.647ADAC2.6124.5116.6350.6644.6277.98811.6411.2015.3218.82214.9021.687INFER2.7264.5836.7350.7014.7048.04911.7171.2735.3888.87314.9851.736CogER2.7754.6136.8040.7734.9328.33612.2431.3065.4739.05315.2141.826CAFE2.8024.6576.8750.7985.3138.88613.0311.3726.1639.71615.8272.352UCPR2.9784.9697.3430.8635.5429.45413.7211.4636.53310.43116.9192.592CADRL3.2595.3687.9630.9376.39710.75215.6671.6917.73812.46220.4293.134Improv.9.44%8.03%8.44%8.57%15.43%13.73%14.18%15.58%18.44%19.47%20.75%20.91%"}, {"figure_label": "III", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "COMPUTATIONAL COST WITH RUNNING TIME REGARDING RECOMMENDATION AND PATH FINDING.", "figure_data": "Time(s)ClothingCell PhonesBeautyRec. (1k users)Find (10k paths)Rec. (1k users)Find (10k paths)Rec. (1k users)Find (10k paths)PGPR217.232 \u00b1 4.127 18.635 \u00b1 0.402 262.218 \u00b1 4.327 21.720 \u00b1 0.472 274.501 \u00b1 5.741 22.108 \u00b1 0.512HeteroEmbed50.266 \u00b1 1.31815.827 \u00b1 0.30744.246 \u00b1 1.02416.682 \u00b1 0.40146.734 \u00b1 1.16618.682 \u00b1 0.396UCPR30.635 \u00b1 1.13414.293 \u00b1 0.29529.578 \u00b1 1.02514.981 \u00b1 0.30233.775 \u00b1 1.14614.473 \u00b1 0.302CAFE22.043 \u00b1 1.05213.114 \u00b1 0.26821.880 \u00b1 1.02213.381 \u00b1 0.33724.761 \u00b1 1.06112.533 \u00b1 0.342CADRL20.625 \u00b1 1.02512.205 \u00b1 0.11318.386 \u00b1 1.01711.334 \u00b1 0.25121.592 \u00b1 1.02810.516 \u00b1 0.235"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": ", UCPR does not show the strongest efficiency advantage among the baselines. A potential explanation is that the modeling of demand portfolios at each step increases UCPR's time consumption. Compared with the most efficient baseline CAFE, CADRL has an overall time advantage on all datasets. Also, the maximum and average action space of CAFE are O(|E|\u00d7|L|) and O(|A e |\u00d7|L|) while those of CADRL are reduced to O( |E| |C| \u00d7|L|) and O( |A e | |A c | \u00d7|L|), where L \u226a |C| and |A c |. This explains why CADRL is more efficient than SOTA baselines even though it uses more hops. In brief, Table III, together with Table I, reveals that CADRL's recommendation efficiency and effectiveness surpass those of existing SOTA methods across varying-scale datasets.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "u i i b i i i i Guidance u i b i u i b i R p e R p c R e R e R", "formula_coordinates": [4.0, 404.36, 85.2, 144.71, 29.18]}, {"formula_id": "formula_1", "formula_text": "h vi \u2190 f e (N vi ), h vi \u2190 f c (N c vi )", "formula_coordinates": [4.0, 48.96, 456.78, 251.06, 22.65]}, {"formula_id": "formula_2", "formula_text": "r1 -\u2192 e 1 r2 -\u2192 ... r L -\u2192 v u,i", "formula_coordinates": [4.0, 268.96, 260.69, 113.14, 458.6]}, {"formula_id": "formula_3", "formula_text": "t i,r,j = \u03c3(W 1 [h vi \u2295 h ej \u2295 h r \u2295 h rp ])(1)", "formula_coordinates": [5.0, 98.14, 635.09, 201.89, 9.72]}, {"formula_id": "formula_4", "formula_text": "\u03b1 i,r = \u03c3(W 2 t i,r,j + b)(2)", "formula_coordinates": [5.0, 127.62, 652.81, 172.4, 9.72]}, {"formula_id": "formula_5", "formula_text": "n k vi = (r,j)\u2208Ni(vi) \u03b1 i,r W (k-1) in (h (k-1) ej \u2022 h (k-1) r ) + (r,j)\u2208No(vi) \u03b1 i,r W (k-1) out (h (k-1) ej \u2022 h (k-1) r )(3)", "formula_coordinates": [5.0, 79.26, 668.81, 220.76, 52.22]}, {"formula_id": "formula_6", "formula_text": "z (k) i = \u03c3(W z1 n (k) vi + W self h (k-1) vi ) (4) v(k) i = \u03c3(W v1 n (k) vi + W v2 h (k-1) vi )(5)", "formula_coordinates": [5.0, 366.46, 233.99, 196.58, 33.32]}, {"formula_id": "formula_7", "formula_text": "v (k) i = tanh(W v1 n (k) vi + W v2 (v (k) i \u2022 h (k-1) vi ))(6)", "formula_coordinates": [5.0, 345.49, 272.49, 217.55, 14.07]}, {"formula_id": "formula_8", "formula_text": "h(k) vi = (1 -z (k) i ) \u2022 h (k-1) vi + z (k) i \u2022 v (k) i(7)", "formula_coordinates": [5.0, 357.82, 293.17, 205.22, 14.07]}, {"formula_id": "formula_9", "formula_text": "\u03b2 (m-1) vicx = LeakyRelu(W (m-1) ic ( h(m-1) vi \u2295 h (m-1) cx )) (8)", "formula_coordinates": [5.0, 335.39, 568.08, 227.65, 14.02]}, {"formula_id": "formula_10", "formula_text": "\u03b1 (m-1) vicx = exp(\u03b2 (m-1) vicx ) cy\u2208N c v i exp(\u03b2 (m-1) vicy )(9)", "formula_coordinates": [5.0, 371.12, 631.53, 191.92, 33.48]}, {"formula_id": "formula_11", "formula_text": "h (m) v c i = cx\u2208N c v i \u03b1 (m-1) vicx h (m-1) cx (10)", "formula_coordinates": [5.0, 379.95, 695.99, 183.09, 26.35]}, {"formula_id": "formula_12", "formula_text": "h vi = hvi + \u03b4h v c i (11)", "formula_coordinates": [6.0, 139.75, 80.44, 160.27, 13.89]}, {"formula_id": "formula_13", "formula_text": "A c l = {c \u2032 , | (c l , c \u2032 ) \u2208 G c }.", "formula_coordinates": [6.0, 110.54, 660.25, 116.26, 12.55]}, {"formula_id": "formula_14", "formula_text": "y c 0 = LSTM c (0, [u 0 ; c 0 ]), y e 0 = LSTM e (0, [u 0 ; h r0 ; h e0 ])(12)", "formula_coordinates": [6.0, 323.19, 485.99, 239.85, 22.98]}, {"formula_id": "formula_15", "formula_text": "y c l = LSTM c (W c [y c l-1 ; y e l-1 ], c l-1 ), l>0,(13)", "formula_coordinates": [6.0, 354.93, 509.77, 208.11, 12.88]}, {"formula_id": "formula_16", "formula_text": "y e l = LSTM e (W e [y e l-1 ; y c l-1 ], [h r l-1 ; h e l-1 ]), l>0,(14)", "formula_coordinates": [6.0, 327.89, 527.84, 235.15, 12.88]}, {"formula_id": "formula_17", "formula_text": "A c \u2208 R |A c |\u00d77d , A e \u2208 R |A e |\u00d77d", "formula_coordinates": [6.0, 311.98, 665.3, 251.06, 23.61]}, {"formula_id": "formula_18", "formula_text": "\u03c0 c \u03b8 (a c l |s c l ) = softmax(A c l \u00d7 W c 2 ReLu(W c 1 [u 0 ; c l ; y c l ]))(15)", "formula_coordinates": [6.0, 320.94, 707.28, 242.1, 13.04]}, {"formula_id": "formula_19", "formula_text": "R pc l = 1 1 + exp(-\u03c6 l )(17)", "formula_coordinates": [7.0, 130.25, 632.95, 169.77, 56.66]}, {"formula_id": "formula_21", "formula_text": "R pe l = s c l \u2022 s e l \u2225s c l \u2225 \u2022 \u2225s e l \u2225(19)", "formula_coordinates": [7.0, 400.01, 78.04, 163.03, 26.31]}, {"formula_id": "formula_22", "formula_text": "R c (s c l ) = Rc (s c L ) + \u03b1 pe R pe l , l \u2208 [1, L](20)", "formula_coordinates": [7.0, 360.04, 162.67, 203.0, 13.91]}, {"formula_id": "formula_23", "formula_text": "R e (s e l ) = Re (s e L ) + \u03b1 pc R pc l , l \u2208 [1, L](21)", "formula_coordinates": [7.0, 359.84, 181.68, 203.19, 13.91]}], "doi": ""}
