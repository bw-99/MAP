{
  "Always Strengthen Your Strengths: A Drift-Aware Incremental Learning Framework for CTR Prediction": "Congcong Liu*, Fei Teng*, Xiwei Zhao, Zhangang Lin, Jinghe Hu, Jingping Shao JD.com Beijing, China {liucongcong25,tengfei49,zhaoxiwei,linzhangang,hujinghe,shaojingping}@jd.com tf20@mails.tsinghua.edu.cn",
  "ABSTRACT": "Click-through rate (CTR) prediction is of great importance in recommendation systems and online advertising platforms. When served in industrial scenarios, the user-generated data observed by the CTR model typically arrives as a stream. Streaming data has the characteristic that the underlying distribution drifts over time and may recur. This can lead to catastrophic forgetting if the model simply adapts to new data distribution all the time. Also, it's inefficient to relearn distribution that has been occurred. Due to memory constraints and diversity of data distributions in large-scale industrial applications, conventional strategies for catastrophic forgetting such as replay, parameter isolation, and knowledge distillation are difficult to be deployed. In this work, we design a novel drift-aware incremental learning framework based on ensemble learning to address catastrophic forgetting in CTR prediction. With explicit error-based drift detection on streaming data, the framework further strengthens well-adapted ensembles and freezes ensembles that do not match the input distribution avoiding catastrophic interference. Both evaluations on offline experiments and A/B test shows that our method outperforms all baselines considered.",
  "ACMReference Format:": "Congcong Liu*, Fei Teng*, Xiwei Zhao, Zhangang Lin, Jinghe Hu, Jingping Shao . 2023. Always Strengthen Your Strengths: A Drift-Aware Incremental Learning Framework for CTR Prediction. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '23), July 23-27, 2023, Taipei, Taiwan. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3539618.3591948",
  "1 INTRODUCTION": "Click-through rate (CTR) prediction is a critical task in recommender systems and online advertising platforms. Most previous works mainly concentrates on better feature interaction[6, 12, 22, 31] and extensive user behavior modeling [13, 33, 35, 36] based on deep neural networks trained with offline batch manner. However, in real-world production systems such as online advertising platforms, CTR models are often trained with incremental learning framework to serve streaming data from massive users Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '23, July 23-27, 2023, Taipei, Taiwan ¬© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-9408-6/23/07...$15.00 https://doi.org/10.1145/3539618.3591948 Figure 1: Empirical observation of drifting and recurred patterns. (a) CTR in real production within a typical 2 days is shown in relative scale. (b) Impressed ad data of two days are grouped every eight hours, and the KL distance between groupsofdataiscalculated and unified. smaller value means larger similarity. 00:00 06:00 12:00 18:00 00:00 06:00 12:00 18:00 00:00 Time 0.6 1.2 0.0 1.8 2.4 3.0 CTR (Relative Scale) (a) Variation of CTR 0.00 0.85 0.23 0.21 0.52 0.27 0.86 0.00 0.91 0.94 0.19 0.85 0.23 0.90 0.00 0.22 0.54 0.15 0.21 0.93 0.22 0.00 0.62 0.30 0.52 0.19 0.54 0.62 0.00 0.38 0.27 0.85 0.15 0.30 0.38 0.00 0.0 0.2 0.4 0.6 0.8 16 ~ 24 24 ~ 32 32 ~ 40 40 ~ 48 8 ~ 16 0 ~ 8 0 ~ 8 8 ~ 16 16 ~ 24 24 ~ 32 32 ~ 40 40 ~ 48 (b) 8-hour versus 8-hour similarity [32]. Frequently, the distribution of the streaming data drifts, manifested as patterns of rapid changes over time[24, 29], especially during the promotion period such as 11.11 shopping Festival in Taobao and JD.com, and prime day in Amazon. Furthermore, historical distribution may recur in the future [24, 26]. Figure. 1 provides an empirical observation of the drift phenomena in temporal distribution illustrated by CTR over time (a), and recurred patterns annotated by red box (b). Vanilla incremental learning methods such as IncCTR proposed in [32] that keeping adapt to newest distribution observed may lead to catastrophic forgetting or catastrophic interference, since knowledge of the previously learned distribution may abruptly lost due to the adaptation to current distribution [11, 16]. Furthermore, it's obviously inefficient to relearn distribution that has been occurred. Many effort has been devoted to overcoming the problem of catastrophic forgetting in neural network learning[5, 16, 18, 19, 23], which can basically be grouped into three classes[9], 1) knowledge distillation[10, 15, 20]: the influence of new knowledge on the network is reduced by introducing a regular term, thereby protecting old knowledge from being overwritten by new knowledge; 2) replay[1, 27]: prevent the model from forgetting old knowledge by retaining important old task data and reviewing the old tasks; 3) parameter isolation[2, 28]: freeze the model parameters for storing old knowledge and adopt the new parameters for learning the new knowledge. Above methods have been widely used in many fields such as image classification and object detection[10, 20, 27]. However, it's difficult to deploy these methods to large-scale CTR SIGIR '23, July 23-27, 2023, Taipei, Taiwan Liu and Teng, et al. prediction for industrial scenarios due to memory constraints and diversity of data distributions. Previous psychological research[3, 25] show that human learners can strengthen their learning performance in memory tasks by filtering task-irrelevant information to avoid retroactive interference phenomena[30] , i.e., new knowledge interfering with existing knowledge. Similarly, neural network learners may also benefit from filtering out task-relevant information under drifting distributions based on performance metrics such as AUC, i.e., acquiring task-relevant information with high auc for memory consolidation. In this paper, we propose a drift-aware incremental learning framework for CTR prediction, Always Strengthen Your Strengths (ASYS), which explicitly detects drift of distribution with error ratebased method and integrates it into an ensemble learning framework to guide the update of different sub-learners. To better detect drift and preserve historical information, we improved on ADWIN [4], a widely used two-time window-based drift detection algorithm, to examine the adaptability of learners to incoming distribution with AUC as a performance metric. ASYS aims to preserve differential information and prevent distribution corruption by multi-learner structures and modulating the update mechanism of learners, i.e., updates to strengthen the learner adapted to the current distribution and freezing updates of unadapted ones to avoid catastrophic interference. Recent works with ensemble learning schemes learn differential knowledge between different ensembles and adopt a weighting scheme to aggregate final result for better performance in CTR prediction [17, 21, 34]. However, these works adopted adaptive way to update models that do not explicitly detect drift of distributions and thereby is ineffective to address catastrophic forgetting. This novel drift-aware framework greatly alleviate catastrophic forgetting when the distribution drifts over time and recur frequently. To validate the effectiveness and efficiency of the proposed method, we conduct extensive experiments against various competing baselines for CTR prediction on a real-world industrial dataset and public dataset. Besides, we further demonstrate the performance of our ASYS through a rigorous online A/B test in an online advertising system. An intuitive qualitative analysis of AUC values over time is provided to illustrate the superiority of ASYS in enhancing model performance and handling catastrophic forgetting. The contributions of this paper are in four folds: 1). We proposed a novel drift-aware framework, ASYS , for serving high-velocity user-generated streaming data. To the best of our knowledge, this is the first attempt to incorporate explicit drift detection into ensemble learning to alleviate catastrophic forgetting in CTR prediction. 2). We developed a novel drift detection method based on ADWIN, which is more suitable for us to use to address catastrophic forgetting. With explicit drift detection, our ASYS is with good generality and better interpretability. 3). We achieve significant improvements on a real-world industrial dataset over all incremental learning baselines. A rigorous A/B test further demonstrates the excellent performance over a highly optimized baseline models. 4). We give a theoretical and qualitative analysis for better understanding. Figure 2: An overview of the proposed ASYS. ‚Ä¶ D0 ‚Ä¶ D2 Dt-1 Dt Dt+1 ‚Ä¶ D1 ‚Ä¶ Online Data Stream ‚Ä¶ ùíü ùë°- 1 ‚Ä¶ ùíü ùë°-ùëá Offline Data Storage O Offline Batch Training Shuffle Data Backbone ‚Ä¶ Drift Indicator ùë∞ ùíï . ùíä Drift Detection Module Current Input ùê±ùë° , ùëñ Predicted CTR ‡∑ú ùë¶ùë° , ùëñ True Label ùë¶ùë° , ùëñ ‚Ä¶ ‚Ä¶ Dt-1 Dt Dt+1 Main Module All learner's pCTR ‡∑ù ùíöùë° , ùëñ User ‚Ä¶ Update Backbone ASYS Main Module",
  "2 METHODOLOGY": "",
  "2.1 Problem Definition": "In this work, we consider the problem as learning on stream data for CTR prediction. Given as stream of data {D ùë° } +‚àû ùë° = 1 where D ùë° = [ x ùë° , y ùë° ] = GLYPH<8> x ùë°,ùëñ , ùë¶ ùë°,ùëñ GLYPH<9> ùëÅ ùë° ùëñ = 1 represents the training data at time step ùë° . x ùë°,ùëñ ‚àà R ùëì denotes the features of the ùëñ -th impressed advertisement at time step ùë° and ùë¶ ùë°,ùëñ ‚àà { 0 , 1 } denotes ground truth label that shows whether the user clicks the impressed advertisement or not. As illustrated on the top right of Fig. 2, the proposed ASYS consists of two modules: the Drift Detection Module detects whether drift occurs for every learner and determines which learners to update to preserve historical information, and the Main Module uses input feature to predict the CTR and trains the model as directed by Drift Detection Module . In this way, we can train each learner better and facilitate finer CTR prediction in Main Module .",
  "2.2 Main Module": "In the main module, a backbone structure first converts the input sample x ùë°,ùëñ to embedding e ùë°,ùëñ of the sample  The backbone structure can be suitable DNNs such as deep interest network (DIN) [36] or other networks. Then, the extracted embedding e ùë°,ùëñ is fed into ùëö learners and they output their own predicted CTR (pCTR). The structure of each learner is an MLP network followed by a sigmoid activation function. At inference time, we aggregate all pCTRs to obtain predicted CTR of the sample x ùë°,ùëñ , but when training the model, we aggregate these pCTRs under the instruction of drift detection module,   where w ùë° = [ ùë§ ( 1 ) ùë° , ¬∑ ¬∑ ¬∑ , ùë§ ( ùëö ) ùë° ] ùëá ‚àà R ùëö is the aggregation weights at time step ùë° , I ùë° ‚àà { 0 , 1 } ùëö is the indicator given by drift detection module at time step ùë° , and ÀÜ y ùë°,ùëñ = [ ÀÜ ùë¶ ( 1 ) ùë°,ùëñ , ¬∑ ¬∑ ¬∑ ÀÜ ùë¶ ( ùëö ) ùë°,ùëñ ] ùëá ‚àà [ 0 , 1 ] ùëö is the concatenation of the learners' pCTR, where ÀÜ ùë¶ ( ùëò ) ùë°,ùëñ = ùêøùëíùëéùëüùëõùëíùëü ( ùëò ) ( e ùë°,ùëñ ) . Always Strengthen Your Strengths: A Drift-Aware Incremental Learning Framework for CTR Prediction SIGIR '23, July 23-27, 2023, Taipei, Taiwan",
  "Algorithm 1: ASYS for CTR Prediction.": "Input: Data D ùë° , Length ùêø Output: Predicted CTR ÀÜ ùë¶ ùëñùëõùëì ùëíùëü ùë°,ùëñ 1 ÀÜ y ùë°,ùëñ , w ùë° ‚Üê MainModule ( x ùë°,ùëñ ) ; // Inference: predict CTR ; 2 ÀÜ ùë¶ ùëñùëõùëì ùëíùëü ùë°,ùëñ ‚Üê w ùë° ¬∑ ÀÜ y ùë°,ùëñ ; // Aggregate pCTR of all Learners ; // Train: update the model ; for each learner ùëò do 3 ùëéùë¢ùëê ùëò ùë° ‚Üê AUC ( ÀÜ ùë¶ ( ùëò ) ùë°,ùëñ , y ùë° ) ; // Compute auc ; 4 Àú ùëä ùëò ‚Üê ùëä ùëò ùë° -1 ‚à™ { ùëéùë¢ùëê ùëò ùë° } ; // Extend sliding window ; 5 ùúñ ùëò ùë° ‚Üê max ùêø ùëêùë¢ùë° ( ùúñ ùëùùëéùëüùë°ùëñùëúùëõ -ùúñ ùëêùë¢ùë° ) ; // Detection ; if not drift // Update sliding window and indicator ; 6 ùëä ùëò ùë° , ùêº ( ùëò ) ùë° ‚Üê Àú ùëä ùëò \\ ùëä ùëò 1 ,ùë° -1 , 1; else 7 ùëä ùëò ùë° , ùêº ( ùëò ) ùë° ‚Üê ùëä ùëò ùë° -1 , 0; end if end for 8 ÀÜ ùë¶ ùë°ùëüùëéùëñùëõ ùë°,ùëñ ‚Üê ùëÅùëúùëüùëöùëéùëô ( w ùë° ‚äô I ùë° ) ¬∑ ÀÜ y ùë°,ùëñ ; // Aggregate pCTR ; 9 Use ÀÜ ùë¶ ùë°ùëüùëéùëñùëõ ùë°,ùëñ to compute loss and do backpropagation. The symbol ‚äô in Eqn. (2) means element-wise multiplication, and the ùëÅùëúùëüùëöùëéùëô operator aims to force the sum of the aggregation weight of all learners to be 1. Then we use ÀÜ ùë¶ ùë°ùëüùëéùëñùëõ ùë°,ùëñ to compute loss and do backpropagation. The rule to update aggregation weights w ùë° at time step ùë° can be IncCTR[32], MoE[14], AdaMoE[21] or others.",
  "2.3 Drift Detection Module": "To examine the fitness of the learner to incoming distribution, we build on ADWIN [4] for distribution drift detection, a widely used error rate based method. Unlike ADWIN which develops for drift detection and adaptation to new distribution, we perform drift detection to protect the information of the historical distribution from being corrupted in case of a drifted new distribution occurs. Based on the purpose of assessing model validity and thus detecting drift, we use AUC as a metric for learners' performance. We keep a sliding window of fixed length ùêø for each learner to store the historical information and put the first ùêø AUCs sequentially into the corresponding sliding window as initial historical information. Every time a new AUC arrives, we assume that the AUC will significantly decrease if the current distribution drifts, thus we test whether the observed average of the more recent AUCs is significantly lower than that of the older AUCs. For ùëò th-learner at time step ùë° , denote the AUC as ùëéùë¢ùëê ùëò ùë° and the sliding window as ùëä ùëò ùë° = { ùëä ùëò ùëô,ùë° } ùêø ùëô = 1 . Denote Àú ùëä ùëò = { Àú ùëä ùëò ùëô } ùêø + 1 ùëô = 1 as the temporary extended sliding window, where Àú ùëä ùëò ùëô = ùëä ùëò ùëô,ùë° for ùëô ‚â§ ùêø , and Àú ùëä ùëò ùêø + 1 = ùëéùë¢ùëê ùëò ùë° + 1 . Like ADWIN[4], we get a partion of Àú ùëä ùëò , and the cut point is ùêø ùëêùë¢ùë° . Specifically, we test whether ùúñ ùëùùëéùëüùë°ùëñùëúùëõ is lower than ùúñ ùëêùë¢ùë° , where  ùëõ 0 is the harmonic mean of ùêø ùëêùë¢ùë° and ùêø + 1 -ùêø ùëêùë¢ùë° , and ùõø is confidence level. The formula of ùúñ ùëêùë¢ùë° is different from that of [4] because we use one-sized test instead of two-sized test. Theorem 3.1 in [4] ensures that the probability of such a detection making a Type I error does not exceed ùõø . Inspired by exponential histogram technology[4, 7, 8] which guarantees ( 1 + ùúñ ) -multiplicative approximation, we iterate ùêø ùëêùë¢ùë° in ùëÜ : = { ùêø + 1 -2 ùëß | ùëß ‚àà Z ‚â• 0 , ùêø + 1 -2 ùëß > 0 } and let ùúñ ùëò ùë° = min ùêø ùëêùë¢ùë° ( ùúñ ùëùùëéùëüùë°ùëñùëúùëõ -ùúñ ùëêùë¢ùë° ) . Two situations that may occur: ¬∑ max ùëò ùúñ ùëò ùë° > 0, which means not all learners' distribution drifts. In this case, we freeze learners that do not satisfy ùúñ ùëò ùë° > 0, i.e., ùêº ( ùëò ) ùë° = I ùúñ ùëò ùë° > 0 . Besides, we update the sliding window for learners not frozen, ùëä ùëò ùë° + 1 = Àú ùëä ùëò \\ ùëä ùëò 1 ,ùë° , for other learners ùëä ùëò ùë° + 1 = ùëä ùëò ùë° . We prevent the current information from corrupting their preserved historical information by freezing some learners and preventing the backpropagation of their parameter gradients. ¬∑ max ùëò ùúñ ùëò ùë° < 0, which means the current information drifts for all learners. Denote ùëò 0 = arg max ùëò ùúñ ùëò ùë° , at this time we freeze all except ùëò 0th-learner to make a trade-off between retaining historical information and learning current information. We briefly illustrate the effect of the length ùêø of the sliding window. If ùêø is too small, the fluctuation will be more likely detected as distribution drift, and the result will be unstable. If ùêø is too large, the smaller AUC in the past is still retained when the larger AUC enters the sliding window. According to Eqn. (3), ùúñ ùëêùë¢ùë° will become smaller, cause drift to be more difficult to detect and the drift detection module will gradually fail.",
  "2.4 Theoretical Analysis of ASYS": "Theorem 1. ( Upper Bound for Estimation Error ) Suppose ùëÉ ( ùë•,ùë¶ ) and ùëÉ ‚Ä≤ ( ùë•,ùë¶ ) are two joint distributions of ùë• and ùë¶ . For any Lipschitz continuous ùëì of Lipschitz constant ùêø and ùë¶ ‚àà { 0 , 1 } , x ùëñ ‚àº ùëÉ (¬∑| ùë¶ ) and x ùëó ‚àº ùëÉ ‚Ä≤ (¬∑| ùë¶ ) , denote E ( x ùëñ ,ùë¶ )‚àº ùëÉ ‚Ä≤ | | ùëì ( x ùëñ ) -ùë¶ | | = ùúñ ùëÉ ‚Ä≤ , we have  Theoretical Understanding of ASYS. The above theorem can be easily proved by norm inequality and definition of Lipschitz continuity. Suppose sequential finite data D , D ‚Ä≤ is sampled from ùëÉ ( ùë•,ùë¶ ) , ùëÉ ‚Ä≤ ( ùë•,ùë¶ ) respectively, and we get function ùëì by training on D and D ‚Ä≤ . Drift can be triggered by 3 source from [24]: ùëÉ ( ùë• ) ‚â† ùëÉ ‚Ä≤ ( ùë• ) (source 1), ùëÉ ( ùë¶ | ùë• ) ‚â† ùëÉ ‚Ä≤ ( ùë¶ | ùë• ) (source 2), both (source 3). When ùëÉ ‚Ä≤ drift relative to ùëÉ , ùëÉ ‚Ä≤ ( ùë• | ùë¶ ) drift relative to ùëÉ ( ùë• | ùë¶ ) no matter which kind of drift appears since the Bayes formula ùëÉ ( ùë• | ùë¶ ) ‚àù ùëÉ ( ùë• ) ùëÉ ( ùë¶ | ùë• ) , thus term E x ùëñ , x ùëó ‚àº ùëÉ ( ¬∑ | ùë¶ ) ,ùëÉ ‚Ä≤ ( ¬∑ | ùë¶ ) ùêø | | x ùëñ -x ùëó | | will be large, the upper bound will be large after training on D ‚Ä≤ . Our ASYS freezes those learners whose indicator is 0, thus keep the upper bound of each learner at a low level. Each model therefore corresponds to a class of distributions with a lower upper bound on the data sampled from such distributions, which alleviate the catastrophic forgetting. Besides, the variance of the predicted CTR is the weighted sum of the variance of all pCTRs. When there exists some learners with lower upper bound for recurring distribution, the variance of the predicted CTR will also be lower, improving model's performance.",
  "3 EXPERIMENTS": "",
  "3.1 Offline Experiment": "Datasets. 1). The industrial dataset is constructed from the user logs of real production systems. The dataset contains about 6 billion SIGIR '23, July 23-27, 2023, Taipei, Taiwan Liu and Teng, et al. Table 1: The Overall AUC on Avazu and Industrial dataset over 3-Run, std ‚âà 10 -4 . #(Lea.) denotes number of learners. 00:00 12:00 24:00 36:00 48:00 60:00 72:00 0.7500 0.7525 0.7550 0.7575 0.7600 0.7625 0.7650 0.7675 0.7700 IncCTR IncCTR+ASYS MoE MoE+ASYS AdaMoE AdaMoE+ASYS 00:00 12:00 24:00 36:00 48:00 60:00 72:00 0.7500 0.7525 0.7550 0.7575 0.7600 0.7625 0.7650 0.7675 0.7700 =0.01 =0.05 =0.1 (a) AUC/10Min of different models (b) AUC/10Min of different ùõø Figure 3: AUC/10Min of different (a) models and (b) ùõø . ad impression records within three days, sorted in chronological order. Then it will be divided into data chunks D ùë° , each containing 2048 samples. 2). The public dataset follows settings of AdaMoE [21] which reorganized the Avazu 1 dataset chronologically. Baseline. We apply ASYS with the following baseline models. ¬∑ IncCTR : The pCTR is simply the average of all learners' output. ¬∑ MoE : The outputs of learners are aggregated by a gate network. ¬∑ AdaMoE : The update of aggregation module is decoupled from regular MoE following settings in [21]. Settings. We perform offline batch training by shuffling the offline data in storage, and then update the backbone as shown in Fig. 2. An ensemble of DIN [36] and DCN[31] is used as the backbone with output dimension of 1024. Each learner (expert) is a 2-layer-MLP with [512, 256] hidden units with a ReLU activation function, followed by a linear layer with the sigmoid function to obtain the predicted CTR. Adam optimizer is used for all models. For hyper-parameters of considered baselines, we follow the same setting with [21]. Evaluation Metrics. We adopt AUC (Area Under ROC Curve) as the evaluation metric. In our setup, data is streamed and used for training only once. For data chunk D ùë° at time step ùë° , its first 80% of data are used as training samples D ùë°ùëüùëéùëñùëõ ùë° , and the rest 20% as testing samples D ùë°ùëíùë†ùë° ùë° . At each time step ùë° , D ùë°ùëüùëéùëñùëõ ùë° is used to train the model first. Then, the trained model is used to D ùë°ùëíùë†ùë° ùë° to obtain pCTRs. The above train-predict process continues until the entire data stream is consumed. To compute overall AUC , we use the 1 https://www.kaggle.com/c/avazu-ctr-prediction/data pCTRs and labels of all test samples. Consistent with [21],we further collect the stored pCTR and ground truth label of test samples every 10 minutes to calculate AUC every 10 minutes (AUC/10Min) to better study the model performance to stream data. Quantitative Results. The confidence level is set to ùõø = 0 . 05. As shown in Table. 1, ASYS overwhelms all baseline models with #Lea. varying from 3 to 12 in both industrial dataset and the public Avazu dataset. When #Lea. is nine, ASYS improves the overall AUC by 0.34% on average over all baseline methods in industrial dataset which is significant in CTR prediction [6, 12, 31]. Similar results are observed over public dataset with 0.18%-0.35% improvements. For ASYS, we obtain the best result when #Lea. is nine in industrial dataset and six in in Avazu dataset. We can see that too many learners can result in the decrease of overall AUC. The reason may be that too many parameters cause underfitting. Qualitative Results. To further study how the models handle catastrophic forgetting in streaming data, we plot the AUC/10Min in Fig. 3(a). The #Lea. is set to nine. Similar results can be observed with other #Lea. We can observe that the distribution of time periods such as 00:00 and 12:00 of each day is similar everyday, reflecting the periodicity of the distribution. For all baselines, the AUCs drops significantly at 00:00, indicating that the model catastrophically forgets the distribution around 00:00 after learning the distribution at other times. After adding our ASYS to each baseline model, the drop is greatly alleviated and AUC is greatly improved. Influence of confidence level in drift detection module. We set the baseline model as AdaMoE because of its best performance and investigate the impact of ùõø , as shown in Fig. 3(b). We find that AUC under ùõø = 0 . 05 is better than ùõø = 0 . 01 and ùõø = 0 . 1, and the drop of AUC for ùõø = 0 . 01 is indeed larger than ùõø = 0 . 05 and ùõø = 0 . 1. The reason might be that the smaller ùõø is, the looser the detection is and the degree of overcoming catastrophic forgetting is not enough. While when ùõø is large, the probability of making Type I error is larger that is easier to misdetect leading to insufficient training. We empirically find ùõø = 0 . 05 achieves a better tradeoff between strict and lenient detection of ASYS, and yields the best overall AUC.",
  "3.2 Online A/B Test": "To conduct online A/B test, we deploy the proposed ASYS model in the online advertising system. The online A/B test lasts for 7 days (from 2022-Aug-25 to 2022-Sep-1). Compared to a highly optimized baseline, our ASYS contributes to 2.62% CTR and 3.56% eCPM (Effective Cost Per Mille) gain.",
  "4 CONCLUSION": "In this paper, we introduce a novel incremental learning framework, ASYS, to address the catastrophic forgetting in CTR prediction. The experiments show that our method overwhelms all other incremental learning methods on a real-world production dataset and public dataset. The online A/B test results further demonstrate the effectiveness of the proposed method. A theoretical analysis and qualitative results are provided to illustrate the superior performance of ASYS on data streams. To the best of our knowledge, this is the first work to explicitly detect drift to handle catastrophic forgetting in the CTR prediction. Always Strengthen Your Strengths: A Drift-Aware Incremental Learning Framework for CTR Prediction SIGIR '23, July 23-27, 2023, Taipei, Taiwan",
  "REFERENCES": "[1] 2021. Multi-Domain Multi-Task Rehearsal for Lifelong Learning. 35 (May 2021), 8819-8827. https://doi.org/10.1609/aaai.v35i10.17068 [2] Rahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars. 2017. Expert gate: Lifelong learning with a network of experts. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 3366-3375. [3] Tarek Amer, Jordana S Wynn, and Lynn Hasher. 2022. Cluttered memory representations shape cognition in old age. Trends in Cognitive Sciences (2022). [4] Albert Bifet and Ricard Gavalda. 2007. Learning from time-changing data with adaptive windowing. In Proceedings of the 2007 SIAM International Conference on Data Mining . SIAM, Minneapolis, Minnesota, USA, 443-448. [5] Pei-Hung Chen, Wei Wei, Cho-Jui Hsieh, and Bo Dai. 2021. Overcoming Catastrophic Forgetting by Bayesian Generative Regularization. In Proceedings of the 38th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 139) , Marina Meila and Tong Zhang (Eds.). PMLR, 1760-1770. https://proceedings.mlr.press/v139/chen21v.html [6] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st Workshop on Deep Learning for Recommender Systems . ACM, Boston, MA, USA, 7-10. [7] Mayur Datar, Aristides Gionis, Piotr Indyk, and Rajeev Motwani. 2002. Maintaining Stream Statistics over Sliding Windows. SIAM J. Comput. 31, 6 (2002), 1794-1813. https://doi.org/10.1137/S0097539701398363 arXiv:https://doi.org/10.1137/S0097539701398363 [8] Mayur Datar and Rajeev Motwani. 2016. The Sliding-Window Computation Model and Results . Springer Berlin Heidelberg, Berlin, Heidelberg, 149-165. https: //doi.org/10.1007/978-3-540-28608-0_7 [9] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ale≈° Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. 2022. A Continual Learning Survey: Defying Forgetting in Classification Tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence 44, 7 (2022), 3366-3385. https://doi.org/10. 1109/TPAMI.2021.3057446 [10] Prithviraj Dhar, Rajat Vikram Singh, Kuan-Chuan Peng, Ziyan Wu, and Rama Chellappa. 2019. Learning Without Memorizing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . [11] Robert M French. 1999. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences 3, 4 (1999), 128-135. [12] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: A Factorization-Machine based Neural Network for CTR Prediction. In Proceedings of the 26th International Joint Conference on Artificial Intelligence . ijcai.org, Melbourne, Australia, 1725-1731. [13] Zai Huang, Mingyuan Tao, and Bufeng Zhang. 2021. Deep User Match Network for Click-Through Rate Prediction. In proceedings of the 44th ACM SIGIR Conference on Research and Development in Information Retrieval . ACM, Virtual Event, Canada, 1890-1894. [14] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. 1991. Adaptive mixtures of local experts. Neural computation 3, 1 (1991), 79-87. [15] Minsoo Kang, Jaeyoo Park, and Bohyung Han. 2022. Class-Incremental Learning by Knowledge Distillation With Adaptive Feature Consolidation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . 16071-16080. [16] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. 2017. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences 114, 13 (2017), 3521-3526. https://doi.org/10.1073/pnas.1611835114 arXiv:https://www.pnas.org/doi/pdf/10.1073/pnas.1611835114 [17] Bartosz Krawczyk, Leandro L Minku, Jo√£o Gama, Jerzy Stefanowski, and Micha≈Ç Wo≈∫niak. 2017. Ensemble learning for data stream analysis: A survey. Information Fusion 37 (2017), 132-156. [18] Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, and Byoung-Tak Zhang. 2017. Overcoming Catastrophic Forgetting by Incremental Moment Matching. In Advances in Neural Information Processing Systems , I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.), Vol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2017/file/ f708f064faaf32a43e4d3c784e6af9ea-Paper.pdf [19] Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. 2019. Learn to Grow: A Continual Structure Learning Framework for Overcoming Catastrophic Forgetting. In Proceedings of the 36th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 97) , Kamalika Chaudhuri and Ruslan Salakhutdinov (Eds.). PMLR, 3925-3934. https: //proceedings.mlr.press/v97/li19m.html [20] Zhizhong Li and Derek Hoiem. 2018. Learning without Forgetting. IEEE Transactions on Pattern Analysis and Machine Intelligence 40, 12 (2018), 2935-2947. https://doi.org/10.1109/TPAMI.2017.2773081 [21] Congcong Liu, Yuejiang Li, Xiwei Zhao, Changping Peng, Zhangang Lin, and Jingping Shao. 2022. Concept Drift Adaptation for CTR Prediction in Online Advertising Systems. arXiv preprint arXiv:2204.05101 (2022). [22] Congcong Liu, Yuejiang Li, Jian Zhu, Fei Teng, Xiwei Zhao, Changping Peng, Zhangang Lin, and Jingping Shao. 2022. Position Awareness Modeling with Knowledge Distillation for CTR Prediction. In Proceedings of the 16th ACM Conference on Recommender Systems . 562-566. [23] Huihui Liu, Yiding Yang, and Xinchao Wang. 2021. Overcoming Catastrophic Forgetting in Graph Neural Networks. Proceedings of the AAAI Conference on Artificial Intelligence 35, 10 (May 2021), 8653-8661. https://doi.org/10.1609/aaai. v35i10.17049 [24] Jie Lu, Anjin Liu, Fan Dong, Feng Gu, Joao Gama, and Guangquan Zhang. 2018. Learning under concept drift: A review. IEEE Transactions on Knowledge and Data Engineering 31, 12 (2018), 2346-2363. [25] Dillon H Murphy and Alan D Castel. 2022. Differential effects of proactive and retroactive interference in value-directed remembering for younger and older adults. Psychology and Aging (2022). [26] Sasthakumar Ramamurthy and Raj Bhatnagar. 2007. Tracking recurrent concept drift in streaming data using ensemble classifiers. In Sixth International Conference on Machine Learning and Applications (ICMLA 2007) . 404-409. https://doi.org/10. 1109/ICMLA.2007.80 [27] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H. Lampert. 2017. iCaRL: Incremental Classifier and Representation Learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) . [28] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. 2018. Overcoming Catastrophic Forgetting with Hard Attention to the Task. In Proceedings of the 35th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 80) , Jennifer Dy and Andreas Krause (Eds.). PMLR, 4548-4557. https://proceedings.mlr.press/v80/serra18a.html [29] Alexey Tsymbal. 2004. The problem of concept drift: Definitions and related work. Computer Science Department, Trinity College Dublin 106, 2 (2004), 58. [30] Endel Tulving and Joseph Psotka. 1971. Retroactive inhibition in free recall: Inaccessibility of information available in the memory store. Journal of experimental Psychology 87, 1 (1971), 1. [31] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network for ad click predictions. In Proceedings of the ADKDD . ACM, Halifax, NS, Canada, 12:1-12:7. [32] Yichao Wang, Huifeng Guo, Ruiming Tang, Zhirong Liu, and Xiuqiang He. 2020. A practical incremental method to train deep ctr models. arXiv preprint arXiv:2009.02147 (2020). [33] Zhibo Xiao, Luwei Yang, Wen Jiang, Yi Wei, Yi Hu, and Hao Wang. 2020. Deep multi-interest network for click-through rate prediction. In Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM) . ACM, Virtual Event, Ireland, 2265-2268. [34] Yang Yang, Da-Wei Zhou, De-Chuan Zhan, Hui Xiong, and Yuan Jiang. 2019. Adaptive deep models for incremental learning: Considering capacity scalability and sustainability. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD) . ACM, Anchorage, AK, USA, 74-82. [35] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Deep interest evolution network for click-through rate prediction. In The 33rd AAAI Conference on Artificial Intelligence (AAAI) . AAAI Press, Honolulu, Hawaii, USA, 5941-5948. [36] Guorui Zhou, Xiaoqiang Zhu, Chengru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for clickthrough rate prediction. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD) . ACM, London, UK, 1059-1068.",
  "keywords_parsed": [],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "Multi-Domain Multi-Task Rehearsal for Lifelong Learning"
    },
    {
      "ref_id": "b2",
      "title": "Expert gate: Lifelong learning with a network of experts"
    },
    {
      "ref_id": "b3",
      "title": "Cluttered memory representations shape cognition in old age"
    },
    {
      "ref_id": "b4",
      "title": "Learning from time-changing data with adaptive windowing"
    },
    {
      "ref_id": "b5",
      "title": "Overcoming Catastrophic Forgetting by Bayesian Generative Regularization"
    },
    {
      "ref_id": "b6",
      "title": "Wide & deep learning for recommender systems"
    },
    {
      "ref_id": "b7",
      "title": "Maintaining Stream Statistics over Sliding Windows"
    },
    {
      "ref_id": "b8",
      "title": "The Sliding-Window Computation Model and Results"
    },
    {
      "ref_id": "b9",
      "title": "A Continual Learning Survey: Defying Forgetting in Classification Tasks"
    },
    {
      "ref_id": "b10",
      "title": "Learning Without Memorizing"
    },
    {
      "ref_id": "b11",
      "title": "Catastrophic forgetting in connectionist networks"
    },
    {
      "ref_id": "b12",
      "title": "DeepFM: A Factorization-Machine based Neural Network for CTR Prediction"
    },
    {
      "ref_id": "b13",
      "title": "Deep User Match Network for Click-Through Rate Prediction"
    },
    {
      "ref_id": "b14",
      "title": "Adaptive mixtures of local experts"
    },
    {
      "ref_id": "b15",
      "title": "Class-Incremental Learning by Knowledge Distillation With Adaptive Feature Consolidation"
    },
    {
      "ref_id": "b16",
      "title": "Overcoming catastrophic forgetting in neural networks"
    },
    {
      "ref_id": "b17",
      "title": "Ensemble learning for data stream analysis: A survey"
    },
    {
      "ref_id": "b18",
      "title": "Overcoming Catastrophic Forgetting by Incremental Moment Matching"
    },
    {
      "ref_id": "b19",
      "title": "Learn to Grow: A Continual Structure Learning Framework for Overcoming Catastrophic Forgetting"
    },
    {
      "ref_id": "b20",
      "title": "Learning without Forgetting"
    },
    {
      "ref_id": "b21",
      "title": "Concept Drift Adaptation for CTR Prediction in Online Advertising Systems"
    },
    {
      "ref_id": "b22",
      "title": "Position Awareness Modeling with Knowledge Distillation for CTR Prediction"
    },
    {
      "ref_id": "b23",
      "title": "Overcoming Catastrophic Forgetting in Graph Neural Networks"
    },
    {
      "ref_id": "b24",
      "title": "Learning under concept drift: A review"
    },
    {
      "ref_id": "b25",
      "title": "Differential effects of proactive and retroactive interference in value-directed remembering for younger and older adults"
    },
    {
      "ref_id": "b26",
      "title": "Tracking recurrent concept drift in streaming data using ensemble classifiers"
    },
    {
      "ref_id": "b27",
      "title": "iCaRL: Incremental Classifier and Representation Learning"
    },
    {
      "ref_id": "b28",
      "title": "Overcoming Catastrophic Forgetting with Hard Attention to the Task"
    },
    {
      "ref_id": "b29",
      "title": "The problem of concept drift: Definitions and related work"
    },
    {
      "ref_id": "b30",
      "title": "Retroactive inhibition in free recall: Inaccessibility of information available in the memory store"
    },
    {
      "ref_id": "b31",
      "title": "Deep & cross network for ad click predictions"
    },
    {
      "ref_id": "b32",
      "title": "A practical incremental method to train deep ctr models"
    },
    {
      "ref_id": "b33",
      "title": "Deep multi-interest network for click-through rate prediction"
    },
    {
      "ref_id": "b34",
      "title": "Adaptive deep models for incremental learning: Considering capacity scalability and sustainability"
    },
    {
      "ref_id": "b35",
      "title": "Deep interest evolution network for click-through rate prediction"
    },
    {
      "ref_id": "b36",
      "title": "Deep interest network for clickthrough rate prediction"
    }
  ]
}