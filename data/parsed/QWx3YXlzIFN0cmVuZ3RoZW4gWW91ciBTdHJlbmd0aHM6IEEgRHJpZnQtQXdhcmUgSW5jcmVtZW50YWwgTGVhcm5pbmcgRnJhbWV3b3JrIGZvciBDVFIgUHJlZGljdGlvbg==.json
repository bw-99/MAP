{"Always Strengthen Your Strengths: A Drift-Aware Incremental Learning Framework for CTR Prediction": "Congcong Liu*, Fei Teng*, Xiwei Zhao, Zhangang Lin, Jinghe Hu, Jingping Shao JD.com Beijing, China {liucongcong25,tengfei49,zhaoxiwei,linzhangang,hujinghe,shaojingping}@jd.com tf20@mails.tsinghua.edu.cn", "ABSTRACT": "Click-through rate (CTR) prediction is of great importance in recommendation systems and online advertising platforms. When served in industrial scenarios, the user-generated data observed by the CTR model typically arrives as a stream. Streaming data has the characteristic that the underlying distribution drifts over time and may recur. This can lead to catastrophic forgetting if the model simply adapts to new data distribution all the time. Also, it's inefficient to relearn distribution that has been occurred. Due to memory constraints and diversity of data distributions in large-scale industrial applications, conventional strategies for catastrophic forgetting such as replay, parameter isolation, and knowledge distillation are difficult to be deployed. In this work, we design a novel drift-aware incremental learning framework based on ensemble learning to address catastrophic forgetting in CTR prediction. With explicit error-based drift detection on streaming data, the framework further strengthens well-adapted ensembles and freezes ensembles that do not match the input distribution avoiding catastrophic interference. Both evaluations on offline experiments and A/B test shows that our method outperforms all baselines considered.", "ACMReference Format:": "Congcong Liu*, Fei Teng*, Xiwei Zhao, Zhangang Lin, Jinghe Hu, Jingping Shao . 2023. Always Strengthen Your Strengths: A Drift-Aware Incremental Learning Framework for CTR Prediction. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '23), July 23-27, 2023, Taipei, Taiwan. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3539618.3591948", "1 INTRODUCTION": "Click-through rate (CTR) prediction is a critical task in recommender systems and online advertising platforms. Most previous works mainly concentrates on better feature interaction[6, 12, 22, 31] and extensive user behavior modeling [13, 33, 35, 36] based on deep neural networks trained with offline batch manner. However, in real-world production systems such as online advertising platforms, CTR models are often trained with incremental learning framework to serve streaming data from massive users Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '23, July 23-27, 2023, Taipei, Taiwan \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-9408-6/23/07...$15.00 https://doi.org/10.1145/3539618.3591948 00:00 06:00 12:00 18:00 00:00 06:00 12:00 18:00 00:00 Time 0.6 1.2 0.0 1.8 2.4 3.0 CTR (Relative Scale) (a) Variation of CTR 0.00 0.85 0.23 0.21 0.52 0.27 0.86 0.00 0.91 0.94 0.19 0.85 0.23 0.90 0.00 0.22 0.54 0.15 0.21 0.93 0.22 0.00 0.62 0.30 0.52 0.19 0.54 0.62 0.00 0.38 0.27 0.85 0.15 0.30 0.38 0.00 0.0 0.2 0.4 0.6 0.8 16 ~ 24 24 ~ 32 32 ~ 40 40 ~ 48 8 ~ 16 0 ~ 8 0 ~ 8 8 ~ 16 16 ~ 24 24 ~ 32 32 ~ 40 40 ~ 48 [32]. Frequently, the distribution of the streaming data drifts, manifested as patterns of rapid changes over time[24, 29], especially during the promotion period such as 11.11 shopping Festival in Taobao and JD.com, and prime day in Amazon. Furthermore, historical distribution may recur in the future [24, 26]. Figure. 1 provides an empirical observation of the drift phenomena in temporal distribution illustrated by CTR over time (a), and recurred patterns annotated by red box (b). Vanilla incremental learning methods such as IncCTR proposed in [32] that keeping adapt to newest distribution observed may lead to catastrophic forgetting or catastrophic interference, since knowledge of the previously learned distribution may abruptly lost due to the adaptation to current distribution [11, 16]. Furthermore, it's obviously inefficient to relearn distribution that has been occurred. Many effort has been devoted to overcoming the problem of catastrophic forgetting in neural network learning[5, 16, 18, 19, 23], which can basically be grouped into three classes[9], 1) knowledge distillation[10, 15, 20]: the influence of new knowledge on the network is reduced by introducing a regular term, thereby protecting old knowledge from being overwritten by new knowledge; 2) replay[1, 27]: prevent the model from forgetting old knowledge by retaining important old task data and reviewing the old tasks; 3) parameter isolation[2, 28]: freeze the model parameters for storing old knowledge and adopt the new parameters for learning the new knowledge. Above methods have been widely used in many fields such as image classification and object detection[10, 20, 27]. However, it's difficult to deploy these methods to large-scale CTR prediction for industrial scenarios due to memory constraints and diversity of data distributions. Previous psychological research[3, 25] show that human learners can strengthen their learning performance in memory tasks by filtering task-irrelevant information to avoid retroactive interference phenomena[30] , i.e., new knowledge interfering with existing knowledge. Similarly, neural network learners may also benefit from filtering out task-relevant information under drifting distributions based on performance metrics such as AUC, i.e., acquiring task-relevant information with high auc for memory consolidation. In this paper, we propose a drift-aware incremental learning framework for CTR prediction, Always Strengthen Your Strengths (ASYS), which explicitly detects drift of distribution with error ratebased method and integrates it into an ensemble learning framework to guide the update of different sub-learners. To better detect drift and preserve historical information, we improved on ADWIN [4], a widely used two-time window-based drift detection algorithm, to examine the adaptability of learners to incoming distribution with AUC as a performance metric. ASYS aims to preserve differential information and prevent distribution corruption by multi-learner structures and modulating the update mechanism of learners, i.e., updates to strengthen the learner adapted to the current distribution and freezing updates of unadapted ones to avoid catastrophic interference. Recent works with ensemble learning schemes learn differential knowledge between different ensembles and adopt a weighting scheme to aggregate final result for better performance in CTR prediction [17, 21, 34]. However, these works adopted adaptive way to update models that do not explicitly detect drift of distributions and thereby is ineffective to address catastrophic forgetting. This novel drift-aware framework greatly alleviate catastrophic forgetting when the distribution drifts over time and recur frequently. To validate the effectiveness and efficiency of the proposed method, we conduct extensive experiments against various competing baselines for CTR prediction on a real-world industrial dataset and public dataset. Besides, we further demonstrate the performance of our ASYS through a rigorous online A/B test in an online advertising system. An intuitive qualitative analysis of AUC values over time is provided to illustrate the superiority of ASYS in enhancing model performance and handling catastrophic forgetting. The contributions of this paper are in four folds: 1). We proposed a novel drift-aware framework, ASYS , for serving high-velocity user-generated streaming data. To the best of our knowledge, this is the first attempt to incorporate explicit drift detection into ensemble learning to alleviate catastrophic forgetting in CTR prediction. 2). We developed a novel drift detection method based on ADWIN, which is more suitable for us to use to address catastrophic forgetting. With explicit drift detection, our ASYS is with good generality and better interpretability. 3). We achieve significant improvements on a real-world industrial dataset over all incremental learning baselines. A rigorous A/B test further demonstrates the excellent performance over a highly optimized baseline models. 4). We give a theoretical and qualitative analysis for better understanding. \u2026 D0 \u2026 D2 Dt-1 Dt Dt+1 \u2026 D1 \u2026 Online Data Stream \u2026 \ud835\udc9f \ud835\udc61- 1 \u2026 \ud835\udc9f \ud835\udc61-\ud835\udc47 Offline Data Storage O Offline Batch Training Shuffle Data Backbone \u2026 Drift Indicator \ud835\udc70 \ud835\udc95 . \ud835\udc8a Drift Detection Module Current Input \ud835\udc31\ud835\udc61 , \ud835\udc56 Predicted CTR \u0ddc \ud835\udc66\ud835\udc61 , \ud835\udc56 True Label \ud835\udc66\ud835\udc61 , \ud835\udc56 \u2026 \u2026 Dt-1 Dt Dt+1 Main Module All learner's pCTR \u0ddd \ud835\udc9a\ud835\udc61 , \ud835\udc56 User \u2026 Update Backbone ASYS Main Module", "2 METHODOLOGY": "", "2.1 Problem Definition": "In this work, we consider the problem as learning on stream data for CTR prediction. Given as stream of data {D \ud835\udc61 } +\u221e \ud835\udc61 = 1 where D \ud835\udc61 = [ x \ud835\udc61 , y \ud835\udc61 ] = GLYPH<8> x \ud835\udc61,\ud835\udc56 , \ud835\udc66 \ud835\udc61,\ud835\udc56 GLYPH<9> \ud835\udc41 \ud835\udc61 \ud835\udc56 = 1 represents the training data at time step \ud835\udc61 . x \ud835\udc61,\ud835\udc56 \u2208 R \ud835\udc53 denotes the features of the \ud835\udc56 -th impressed advertisement at time step \ud835\udc61 and \ud835\udc66 \ud835\udc61,\ud835\udc56 \u2208 { 0 , 1 } denotes ground truth label that shows whether the user clicks the impressed advertisement or not. As illustrated on the top right of Fig. 2, the proposed ASYS consists of two modules: the Drift Detection Module detects whether drift occurs for every learner and determines which learners to update to preserve historical information, and the Main Module uses input feature to predict the CTR and trains the model as directed by Drift Detection Module . In this way, we can train each learner better and facilitate finer CTR prediction in Main Module .", "2.2 Main Module": "In the main module, a backbone structure first converts the input sample x \ud835\udc61,\ud835\udc56 to embedding e \ud835\udc61,\ud835\udc56 of the sample The backbone structure can be suitable DNNs such as deep interest network (DIN) [36] or other networks. Then, the extracted embedding e \ud835\udc61,\ud835\udc56 is fed into \ud835\udc5a learners and they output their own predicted CTR (pCTR). The structure of each learner is an MLP network followed by a sigmoid activation function. At inference time, we aggregate all pCTRs to obtain predicted CTR of the sample x \ud835\udc61,\ud835\udc56 , but when training the model, we aggregate these pCTRs under the instruction of drift detection module, where w \ud835\udc61 = [ \ud835\udc64 ( 1 ) \ud835\udc61 , \u00b7 \u00b7 \u00b7 , \ud835\udc64 ( \ud835\udc5a ) \ud835\udc61 ] \ud835\udc47 \u2208 R \ud835\udc5a is the aggregation weights at time step \ud835\udc61 , I \ud835\udc61 \u2208 { 0 , 1 } \ud835\udc5a is the indicator given by drift detection module at time step \ud835\udc61 , and \u02c6 y \ud835\udc61,\ud835\udc56 = [ \u02c6 \ud835\udc66 ( 1 ) \ud835\udc61,\ud835\udc56 , \u00b7 \u00b7 \u00b7 \u02c6 \ud835\udc66 ( \ud835\udc5a ) \ud835\udc61,\ud835\udc56 ] \ud835\udc47 \u2208 [ 0 , 1 ] \ud835\udc5a is the concatenation of the learners' pCTR, where \u02c6 \ud835\udc66 ( \ud835\udc58 ) \ud835\udc61,\ud835\udc56 = \ud835\udc3f\ud835\udc52\ud835\udc4e\ud835\udc5f\ud835\udc5b\ud835\udc52\ud835\udc5f ( \ud835\udc58 ) ( e \ud835\udc61,\ud835\udc56 ) .", "Algorithm 1: ASYS for CTR Prediction.": "Input: Data D \ud835\udc61 , Length \ud835\udc3f Output: Predicted CTR \u02c6 \ud835\udc66 \ud835\udc56\ud835\udc5b\ud835\udc53 \ud835\udc52\ud835\udc5f \ud835\udc61,\ud835\udc56 1 \u02c6 y \ud835\udc61,\ud835\udc56 , w \ud835\udc61 \u2190 MainModule ( x \ud835\udc61,\ud835\udc56 ) ; // Inference: predict CTR ; 2 \u02c6 \ud835\udc66 \ud835\udc56\ud835\udc5b\ud835\udc53 \ud835\udc52\ud835\udc5f \ud835\udc61,\ud835\udc56 \u2190 w \ud835\udc61 \u00b7 \u02c6 y \ud835\udc61,\ud835\udc56 ; // Aggregate pCTR of all Learners ; // Train: update the model ; for each learner \ud835\udc58 do 3 \ud835\udc4e\ud835\udc62\ud835\udc50 \ud835\udc58 \ud835\udc61 \u2190 AUC ( \u02c6 \ud835\udc66 ( \ud835\udc58 ) \ud835\udc61,\ud835\udc56 , y \ud835\udc61 ) ; // Compute auc ; 4 \u02dc \ud835\udc4a \ud835\udc58 \u2190 \ud835\udc4a \ud835\udc58 \ud835\udc61 -1 \u222a { \ud835\udc4e\ud835\udc62\ud835\udc50 \ud835\udc58 \ud835\udc61 } ; // Extend sliding window ; 5 \ud835\udf16 \ud835\udc58 \ud835\udc61 \u2190 max \ud835\udc3f \ud835\udc50\ud835\udc62\ud835\udc61 ( \ud835\udf16 \ud835\udc5d\ud835\udc4e\ud835\udc5f\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b -\ud835\udf16 \ud835\udc50\ud835\udc62\ud835\udc61 ) ; // Detection ; if not drift // Update sliding window and indicator ; 6 \ud835\udc4a \ud835\udc58 \ud835\udc61 , \ud835\udc3c ( \ud835\udc58 ) \ud835\udc61 \u2190 \u02dc \ud835\udc4a \ud835\udc58 \\ \ud835\udc4a \ud835\udc58 1 ,\ud835\udc61 -1 , 1; else 7 \ud835\udc4a \ud835\udc58 \ud835\udc61 , \ud835\udc3c ( \ud835\udc58 ) \ud835\udc61 \u2190 \ud835\udc4a \ud835\udc58 \ud835\udc61 -1 , 0; end if end for 8 \u02c6 \ud835\udc66 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b \ud835\udc61,\ud835\udc56 \u2190 \ud835\udc41\ud835\udc5c\ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc59 ( w \ud835\udc61 \u2299 I \ud835\udc61 ) \u00b7 \u02c6 y \ud835\udc61,\ud835\udc56 ; // Aggregate pCTR ; 9 Use \u02c6 \ud835\udc66 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b \ud835\udc61,\ud835\udc56 to compute loss and do backpropagation. The symbol \u2299 in Eqn. (2) means element-wise multiplication, and the \ud835\udc41\ud835\udc5c\ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc59 operator aims to force the sum of the aggregation weight of all learners to be 1. Then we use \u02c6 \ud835\udc66 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b \ud835\udc61,\ud835\udc56 to compute loss and do backpropagation. The rule to update aggregation weights w \ud835\udc61 at time step \ud835\udc61 can be IncCTR[32], MoE[14], AdaMoE[21] or others.", "2.3 Drift Detection Module": "To examine the fitness of the learner to incoming distribution, we build on ADWIN [4] for distribution drift detection, a widely used error rate based method. Unlike ADWIN which develops for drift detection and adaptation to new distribution, we perform drift detection to protect the information of the historical distribution from being corrupted in case of a drifted new distribution occurs. Based on the purpose of assessing model validity and thus detecting drift, we use AUC as a metric for learners' performance. We keep a sliding window of fixed length \ud835\udc3f for each learner to store the historical information and put the first \ud835\udc3f AUCs sequentially into the corresponding sliding window as initial historical information. Every time a new AUC arrives, we assume that the AUC will significantly decrease if the current distribution drifts, thus we test whether the observed average of the more recent AUCs is significantly lower than that of the older AUCs. For \ud835\udc58 th-learner at time step \ud835\udc61 , denote the AUC as \ud835\udc4e\ud835\udc62\ud835\udc50 \ud835\udc58 \ud835\udc61 and the sliding window as \ud835\udc4a \ud835\udc58 \ud835\udc61 = { \ud835\udc4a \ud835\udc58 \ud835\udc59,\ud835\udc61 } \ud835\udc3f \ud835\udc59 = 1 . Denote \u02dc \ud835\udc4a \ud835\udc58 = { \u02dc \ud835\udc4a \ud835\udc58 \ud835\udc59 } \ud835\udc3f + 1 \ud835\udc59 = 1 as the temporary extended sliding window, where \u02dc \ud835\udc4a \ud835\udc58 \ud835\udc59 = \ud835\udc4a \ud835\udc58 \ud835\udc59,\ud835\udc61 for \ud835\udc59 \u2264 \ud835\udc3f , and \u02dc \ud835\udc4a \ud835\udc58 \ud835\udc3f + 1 = \ud835\udc4e\ud835\udc62\ud835\udc50 \ud835\udc58 \ud835\udc61 + 1 . Like ADWIN[4], we get a partion of \u02dc \ud835\udc4a \ud835\udc58 , and the cut point is \ud835\udc3f \ud835\udc50\ud835\udc62\ud835\udc61 . Specifically, we test whether \ud835\udf16 \ud835\udc5d\ud835\udc4e\ud835\udc5f\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b is lower than \ud835\udf16 \ud835\udc50\ud835\udc62\ud835\udc61 , where \ud835\udc5b 0 is the harmonic mean of \ud835\udc3f \ud835\udc50\ud835\udc62\ud835\udc61 and \ud835\udc3f + 1 -\ud835\udc3f \ud835\udc50\ud835\udc62\ud835\udc61 , and \ud835\udeff is confidence level. The formula of \ud835\udf16 \ud835\udc50\ud835\udc62\ud835\udc61 is different from that of [4] because we use one-sized test instead of two-sized test. Theorem 3.1 in [4] ensures that the probability of such a detection making a Type I error does not exceed \ud835\udeff . Inspired by exponential histogram technology[4, 7, 8] which guarantees ( 1 + \ud835\udf16 ) -multiplicative approximation, we iterate \ud835\udc3f \ud835\udc50\ud835\udc62\ud835\udc61 in \ud835\udc46 : = { \ud835\udc3f + 1 -2 \ud835\udc67 | \ud835\udc67 \u2208 Z \u2265 0 , \ud835\udc3f + 1 -2 \ud835\udc67 > 0 } and let \ud835\udf16 \ud835\udc58 \ud835\udc61 = min \ud835\udc3f \ud835\udc50\ud835\udc62\ud835\udc61 ( \ud835\udf16 \ud835\udc5d\ud835\udc4e\ud835\udc5f\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b -\ud835\udf16 \ud835\udc50\ud835\udc62\ud835\udc61 ) . Two situations that may occur: \u00b7 max \ud835\udc58 \ud835\udf16 \ud835\udc58 \ud835\udc61 > 0, which means not all learners' distribution drifts. In this case, we freeze learners that do not satisfy \ud835\udf16 \ud835\udc58 \ud835\udc61 > 0, i.e., \ud835\udc3c ( \ud835\udc58 ) \ud835\udc61 = I \ud835\udf16 \ud835\udc58 \ud835\udc61 > 0 . Besides, we update the sliding window for learners not frozen, \ud835\udc4a \ud835\udc58 \ud835\udc61 + 1 = \u02dc \ud835\udc4a \ud835\udc58 \\ \ud835\udc4a \ud835\udc58 1 ,\ud835\udc61 , for other learners \ud835\udc4a \ud835\udc58 \ud835\udc61 + 1 = \ud835\udc4a \ud835\udc58 \ud835\udc61 . We prevent the current information from corrupting their preserved historical information by freezing some learners and preventing the backpropagation of their parameter gradients. \u00b7 max \ud835\udc58 \ud835\udf16 \ud835\udc58 \ud835\udc61 < 0, which means the current information drifts for all learners. Denote \ud835\udc58 0 = arg max \ud835\udc58 \ud835\udf16 \ud835\udc58 \ud835\udc61 , at this time we freeze all except \ud835\udc58 0th-learner to make a trade-off between retaining historical information and learning current information. We briefly illustrate the effect of the length \ud835\udc3f of the sliding window. If \ud835\udc3f is too small, the fluctuation will be more likely detected as distribution drift, and the result will be unstable. If \ud835\udc3f is too large, the smaller AUC in the past is still retained when the larger AUC enters the sliding window. According to Eqn. (3), \ud835\udf16 \ud835\udc50\ud835\udc62\ud835\udc61 will become smaller, cause drift to be more difficult to detect and the drift detection module will gradually fail.", "2.4 Theoretical Analysis of ASYS": "Theorem 1. ( Upper Bound for Estimation Error ) Suppose \ud835\udc43 ( \ud835\udc65,\ud835\udc66 ) and \ud835\udc43 \u2032 ( \ud835\udc65,\ud835\udc66 ) are two joint distributions of \ud835\udc65 and \ud835\udc66 . For any Lipschitz continuous \ud835\udc53 of Lipschitz constant \ud835\udc3f and \ud835\udc66 \u2208 { 0 , 1 } , x \ud835\udc56 \u223c \ud835\udc43 (\u00b7| \ud835\udc66 ) and x \ud835\udc57 \u223c \ud835\udc43 \u2032 (\u00b7| \ud835\udc66 ) , denote E ( x \ud835\udc56 ,\ud835\udc66 )\u223c \ud835\udc43 \u2032 | | \ud835\udc53 ( x \ud835\udc56 ) -\ud835\udc66 | | = \ud835\udf16 \ud835\udc43 \u2032 , we have Theoretical Understanding of ASYS. The above theorem can be easily proved by norm inequality and definition of Lipschitz continuity. Suppose sequential finite data D , D \u2032 is sampled from \ud835\udc43 ( \ud835\udc65,\ud835\udc66 ) , \ud835\udc43 \u2032 ( \ud835\udc65,\ud835\udc66 ) respectively, and we get function \ud835\udc53 by training on D and D \u2032 . Drift can be triggered by 3 source from [24]: \ud835\udc43 ( \ud835\udc65 ) \u2260 \ud835\udc43 \u2032 ( \ud835\udc65 ) (source 1), \ud835\udc43 ( \ud835\udc66 | \ud835\udc65 ) \u2260 \ud835\udc43 \u2032 ( \ud835\udc66 | \ud835\udc65 ) (source 2), both (source 3). When \ud835\udc43 \u2032 drift relative to \ud835\udc43 , \ud835\udc43 \u2032 ( \ud835\udc65 | \ud835\udc66 ) drift relative to \ud835\udc43 ( \ud835\udc65 | \ud835\udc66 ) no matter which kind of drift appears since the Bayes formula \ud835\udc43 ( \ud835\udc65 | \ud835\udc66 ) \u221d \ud835\udc43 ( \ud835\udc65 ) \ud835\udc43 ( \ud835\udc66 | \ud835\udc65 ) , thus term E x \ud835\udc56 , x \ud835\udc57 \u223c \ud835\udc43 ( \u00b7 | \ud835\udc66 ) ,\ud835\udc43 \u2032 ( \u00b7 | \ud835\udc66 ) \ud835\udc3f | | x \ud835\udc56 -x \ud835\udc57 | | will be large, the upper bound will be large after training on D \u2032 . Our ASYS freezes those learners whose indicator is 0, thus keep the upper bound of each learner at a low level. Each model therefore corresponds to a class of distributions with a lower upper bound on the data sampled from such distributions, which alleviate the catastrophic forgetting. Besides, the variance of the predicted CTR is the weighted sum of the variance of all pCTRs. When there exists some learners with lower upper bound for recurring distribution, the variance of the predicted CTR will also be lower, improving model's performance.", "3 EXPERIMENTS": "", "3.1 Offline Experiment": "Datasets. 1). The industrial dataset is constructed from the user logs of real production systems. The dataset contains about 6 billion 00:00 12:00 24:00 36:00 48:00 60:00 72:00 0.7500 0.7525 0.7550 0.7575 0.7600 0.7625 0.7650 0.7675 0.7700 IncCTR IncCTR+ASYS MoE MoE+ASYS AdaMoE AdaMoE+ASYS 00:00 12:00 24:00 36:00 48:00 60:00 72:00 0.7500 0.7525 0.7550 0.7575 0.7600 0.7625 0.7650 0.7675 0.7700 =0.01 =0.05 =0.1 (a) AUC/10Min of different models ad impression records within three days, sorted in chronological order. Then it will be divided into data chunks D \ud835\udc61 , each containing 2048 samples. 2). The public dataset follows settings of AdaMoE [21] which reorganized the Avazu 1 dataset chronologically. Baseline. We apply ASYS with the following baseline models. \u00b7 IncCTR : The pCTR is simply the average of all learners' output. \u00b7 MoE : The outputs of learners are aggregated by a gate network. \u00b7 AdaMoE : The update of aggregation module is decoupled from regular MoE following settings in [21]. Settings. We perform offline batch training by shuffling the offline data in storage, and then update the backbone as shown in Fig. 2. An ensemble of DIN [36] and DCN[31] is used as the backbone with output dimension of 1024. Each learner (expert) is a 2-layer-MLP with [512, 256] hidden units with a ReLU activation function, followed by a linear layer with the sigmoid function to obtain the predicted CTR. Adam optimizer is used for all models. For hyper-parameters of considered baselines, we follow the same setting with [21]. Evaluation Metrics. We adopt AUC (Area Under ROC Curve) as the evaluation metric. In our setup, data is streamed and used for training only once. For data chunk D \ud835\udc61 at time step \ud835\udc61 , its first 80% of data are used as training samples D \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b \ud835\udc61 , and the rest 20% as testing samples D \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 \ud835\udc61 . At each time step \ud835\udc61 , D \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b \ud835\udc61 is used to train the model first. Then, the trained model is used to D \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 \ud835\udc61 to obtain pCTRs. The above train-predict process continues until the entire data stream is consumed. To compute overall AUC , we use the pCTRs and labels of all test samples. Consistent with [21],we further collect the stored pCTR and ground truth label of test samples every 10 minutes to calculate AUC every 10 minutes (AUC/10Min) to better study the model performance to stream data. Quantitative Results. The confidence level is set to \ud835\udeff = 0 . 05. As shown in Table. 1, ASYS overwhelms all baseline models with #Lea. varying from 3 to 12 in both industrial dataset and the public Avazu dataset. When #Lea. is nine, ASYS improves the overall AUC by 0.34% on average over all baseline methods in industrial dataset which is significant in CTR prediction [6, 12, 31]. Similar results are observed over public dataset with 0.18%-0.35% improvements. For ASYS, we obtain the best result when #Lea. is nine in industrial dataset and six in in Avazu dataset. We can see that too many learners can result in the decrease of overall AUC. The reason may be that too many parameters cause underfitting. Qualitative Results. To further study how the models handle catastrophic forgetting in streaming data, we plot the AUC/10Min in Fig. 3(a). The #Lea. is set to nine. Similar results can be observed with other #Lea. We can observe that the distribution of time periods such as 00:00 and 12:00 of each day is similar everyday, reflecting the periodicity of the distribution. For all baselines, the AUCs drops significantly at 00:00, indicating that the model catastrophically forgets the distribution around 00:00 after learning the distribution at other times. After adding our ASYS to each baseline model, the drop is greatly alleviated and AUC is greatly improved. Influence of confidence level in drift detection module. We set the baseline model as AdaMoE because of its best performance and investigate the impact of \ud835\udeff , as shown in Fig. 3(b). We find that AUC under \ud835\udeff = 0 . 05 is better than \ud835\udeff = 0 . 01 and \ud835\udeff = 0 . 1, and the drop of AUC for \ud835\udeff = 0 . 01 is indeed larger than \ud835\udeff = 0 . 05 and \ud835\udeff = 0 . 1. The reason might be that the smaller \ud835\udeff is, the looser the detection is and the degree of overcoming catastrophic forgetting is not enough. While when \ud835\udeff is large, the probability of making Type I error is larger that is easier to misdetect leading to insufficient training. We empirically find \ud835\udeff = 0 . 05 achieves a better tradeoff between strict and lenient detection of ASYS, and yields the best overall AUC.", "3.2 Online A/B Test": "To conduct online A/B test, we deploy the proposed ASYS model in the online advertising system. The online A/B test lasts for 7 days (from 2022-Aug-25 to 2022-Sep-1). Compared to a highly optimized baseline, our ASYS contributes to 2.62% CTR and 3.56% eCPM (Effective Cost Per Mille) gain.", "4 CONCLUSION": "In this paper, we introduce a novel incremental learning framework, ASYS, to address the catastrophic forgetting in CTR prediction. The experiments show that our method overwhelms all other incremental learning methods on a real-world production dataset and public dataset. The online A/B test results further demonstrate the effectiveness of the proposed method. A theoretical analysis and qualitative results are provided to illustrate the superior performance of ASYS on data streams. To the best of our knowledge, this is the first work to explicitly detect drift to handle catastrophic forgetting in the CTR prediction.", "REFERENCES": "[1] 2021. Multi-Domain Multi-Task Rehearsal for Lifelong Learning. 35 (May 2021), 8819-8827. https://doi.org/10.1609/aaai.v35i10.17068 [2] Rahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars. 2017. Expert gate: Lifelong learning with a network of experts. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 3366-3375. [3] Tarek Amer, Jordana S Wynn, and Lynn Hasher. 2022. Cluttered memory representations shape cognition in old age. Trends in Cognitive Sciences (2022). [4] Albert Bifet and Ricard Gavalda. 2007. Learning from time-changing data with adaptive windowing. In Proceedings of the 2007 SIAM International Conference on Data Mining . SIAM, Minneapolis, Minnesota, USA, 443-448. [5] Pei-Hung Chen, Wei Wei, Cho-Jui Hsieh, and Bo Dai. 2021. Overcoming Catastrophic Forgetting by Bayesian Generative Regularization. In Proceedings of the 38th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 139) , Marina Meila and Tong Zhang (Eds.). PMLR, 1760-1770. https://proceedings.mlr.press/v139/chen21v.html [6] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st Workshop on Deep Learning for Recommender Systems . ACM, Boston, MA, USA, 7-10. [7] Mayur Datar, Aristides Gionis, Piotr Indyk, and Rajeev Motwani. 2002. Maintaining Stream Statistics over Sliding Windows. SIAM J. Comput. 31, 6 (2002), 1794-1813. https://doi.org/10.1137/S0097539701398363 arXiv:https://doi.org/10.1137/S0097539701398363 [8] Mayur Datar and Rajeev Motwani. 2016. The Sliding-Window Computation Model and Results . Springer Berlin Heidelberg, Berlin, Heidelberg, 149-165. https: //doi.org/10.1007/978-3-540-28608-0_7 [9] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ale\u0161 Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. 2022. A Continual Learning Survey: Defying Forgetting in Classification Tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence 44, 7 (2022), 3366-3385. https://doi.org/10. 1109/TPAMI.2021.3057446 [10] Prithviraj Dhar, Rajat Vikram Singh, Kuan-Chuan Peng, Ziyan Wu, and Rama Chellappa. 2019. Learning Without Memorizing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . [11] Robert M French. 1999. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences 3, 4 (1999), 128-135. [12] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: A Factorization-Machine based Neural Network for CTR Prediction. In Proceedings of the 26th International Joint Conference on Artificial Intelligence . ijcai.org, Melbourne, Australia, 1725-1731. [13] Zai Huang, Mingyuan Tao, and Bufeng Zhang. 2021. Deep User Match Network for Click-Through Rate Prediction. In proceedings of the 44th ACM SIGIR Conference on Research and Development in Information Retrieval . ACM, Virtual Event, Canada, 1890-1894. [14] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. 1991. Adaptive mixtures of local experts. Neural computation 3, 1 (1991), 79-87. [15] Minsoo Kang, Jaeyoo Park, and Bohyung Han. 2022. Class-Incremental Learning by Knowledge Distillation With Adaptive Feature Consolidation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . 16071-16080. [16] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. 2017. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences 114, 13 (2017), 3521-3526. https://doi.org/10.1073/pnas.1611835114 arXiv:https://www.pnas.org/doi/pdf/10.1073/pnas.1611835114 [17] Bartosz Krawczyk, Leandro L Minku, Jo\u00e3o Gama, Jerzy Stefanowski, and Micha\u0142 Wo\u017aniak. 2017. Ensemble learning for data stream analysis: A survey. Information Fusion 37 (2017), 132-156. [18] Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, and Byoung-Tak Zhang. 2017. Overcoming Catastrophic Forgetting by Incremental Moment Matching. In Advances in Neural Information Processing Systems , I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.), Vol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2017/file/ f708f064faaf32a43e4d3c784e6af9ea-Paper.pdf [19] Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. 2019. Learn to Grow: A Continual Structure Learning Framework for Overcoming Catastrophic Forgetting. In Proceedings of the 36th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 97) , Kamalika Chaudhuri and Ruslan Salakhutdinov (Eds.). PMLR, 3925-3934. https: //proceedings.mlr.press/v97/li19m.html [20] Zhizhong Li and Derek Hoiem. 2018. Learning without Forgetting. IEEE Transactions on Pattern Analysis and Machine Intelligence 40, 12 (2018), 2935-2947. https://doi.org/10.1109/TPAMI.2017.2773081 [21] Congcong Liu, Yuejiang Li, Xiwei Zhao, Changping Peng, Zhangang Lin, and Jingping Shao. 2022. Concept Drift Adaptation for CTR Prediction in Online Advertising Systems. arXiv preprint arXiv:2204.05101 (2022). [22] Congcong Liu, Yuejiang Li, Jian Zhu, Fei Teng, Xiwei Zhao, Changping Peng, Zhangang Lin, and Jingping Shao. 2022. Position Awareness Modeling with Knowledge Distillation for CTR Prediction. In Proceedings of the 16th ACM Conference on Recommender Systems . 562-566. [23] Huihui Liu, Yiding Yang, and Xinchao Wang. 2021. Overcoming Catastrophic Forgetting in Graph Neural Networks. Proceedings of the AAAI Conference on Artificial Intelligence 35, 10 (May 2021), 8653-8661. https://doi.org/10.1609/aaai. v35i10.17049 [24] Jie Lu, Anjin Liu, Fan Dong, Feng Gu, Joao Gama, and Guangquan Zhang. 2018. Learning under concept drift: A review. IEEE Transactions on Knowledge and Data Engineering 31, 12 (2018), 2346-2363. [25] Dillon H Murphy and Alan D Castel. 2022. Differential effects of proactive and retroactive interference in value-directed remembering for younger and older adults. Psychology and Aging (2022). [26] Sasthakumar Ramamurthy and Raj Bhatnagar. 2007. Tracking recurrent concept drift in streaming data using ensemble classifiers. In Sixth International Conference on Machine Learning and Applications (ICMLA 2007) . 404-409. https://doi.org/10. 1109/ICMLA.2007.80 [27] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H. Lampert. 2017. iCaRL: Incremental Classifier and Representation Learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) . [28] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. 2018. Overcoming Catastrophic Forgetting with Hard Attention to the Task. In Proceedings of the 35th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 80) , Jennifer Dy and Andreas Krause (Eds.). PMLR, 4548-4557. https://proceedings.mlr.press/v80/serra18a.html [29] Alexey Tsymbal. 2004. The problem of concept drift: Definitions and related work. Computer Science Department, Trinity College Dublin 106, 2 (2004), 58. [30] Endel Tulving and Joseph Psotka. 1971. Retroactive inhibition in free recall: Inaccessibility of information available in the memory store. Journal of experimental Psychology 87, 1 (1971), 1. [31] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network for ad click predictions. In Proceedings of the ADKDD . ACM, Halifax, NS, Canada, 12:1-12:7. [32] Yichao Wang, Huifeng Guo, Ruiming Tang, Zhirong Liu, and Xiuqiang He. 2020. A practical incremental method to train deep ctr models. arXiv preprint arXiv:2009.02147 (2020). [33] Zhibo Xiao, Luwei Yang, Wen Jiang, Yi Wei, Yi Hu, and Hao Wang. 2020. Deep multi-interest network for click-through rate prediction. In Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM) . ACM, Virtual Event, Ireland, 2265-2268. [34] Yang Yang, Da-Wei Zhou, De-Chuan Zhan, Hui Xiong, and Yuan Jiang. 2019. Adaptive deep models for incremental learning: Considering capacity scalability and sustainability. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD) . ACM, Anchorage, AK, USA, 74-82. [35] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Deep interest evolution network for click-through rate prediction. In The 33rd AAAI Conference on Artificial Intelligence (AAAI) . AAAI Press, Honolulu, Hawaii, USA, 5941-5948. [36] Guorui Zhou, Xiaoqiang Zhu, Chengru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for clickthrough rate prediction. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD) . ACM, London, UK, 1059-1068."}
