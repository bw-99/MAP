{
  "HAMUR: Hyper Adapter for Multi-Domain Recommendation": "Xiaopeng Li City Univerisity of HongKong Hongkong, China xiaopli2-c@cityu.edu.hk Fan Yan Huawei Noah's Ark Lab Shenzhen, China yanfan6@huawei.com Xiangyu Zhao B City Univerisity of HongKong Hongkong, China xianzhao@cityu.edu.hk Yichao Wang Huawei Noah's Ark Lab Shenzhen, China wangyichao5@huawei.com Bo Chen Huawei Noah's Ark Lab Shenzhen, China chenbo116@huawei.com Ruiming Tang Huawei Noah's Ark Lab Shenzhen, China tangruiming@huawei.com",
  "ABSTRACT": "",
  "KEYWORDS": "Multi-Domain Recommendation (MDR) has gained significant attention in recent years, which leverages data from multiple domains to enhance their performance concurrently. However, current MDR models are confronted with two limitations. Firstly, the majority of these models adopt an approach that explicitly shares parameters between domains, leading to mutual interference amongthem. Secondly, due to the distribution differences among domains, the utilization of static parameters in existing methods limits their flexibility to adapt to diverse domains. To address these challenges, we propose a novel model H yper A dapter for MU lti-Domain R ecommendation ( HAMUR ). Specifically, HAMUR consists of two components: (1). Domain-specific adapter, designed as a pluggable module that can be seamlessly integrated into various existing multidomain backbone models, and (2). Domain-shared hyper-network, which implicitly captures shared information among domains and dynamically generates the parameters for the adapter. We conduct extensive experiments on two public datasets using various backbone networks. The experimental results validate the effectiveness and scalability of the proposed model. And we release our code implementation publicly 12 .",
  "CCS CONCEPTS": "",
  "¬∑ Information systems ‚Üí Recommender systems ;": "1 https://github.com/Applied-Machine-Learning-Lab/HAMUR 2 https://gitee.com/mindspore/models/tree/master/research/recommend/HAMUR B Corresponding authors. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CIKM '23, October 21-25, 2023, Birmingham, United Kingdom ¬© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0124-5/23/10...$15.00 https://doi.org/10.1145/3583780.3615137 Multi-Domain Recommendation, Recommendation Systems, CTR Prediction",
  "ACMReference Format:": "Xiaopeng Li, Fan Yan, Xiangyu Zhao B , Yichao Wang, Bo Chen, Huifeng Guo B , and Ruiming Tang. 2023. HAMUR: Hyper Adapter for Multi-Domain Recommendation. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management (CIKM '23), October 21-25, 2023, Birmingham, United Kingdom. ACM, New York, NY, USA, 10 pages. https: //doi.org/10.1145/3583780.3615137",
  "1 INTRODUCTION": "Click-through rate (CTR) prediction is crucial for recommender systems, which find extensive application in e-commerce, music radio, social media, and online recommendations, among others [11, 30]. Traditional CTR models typically concentrate on a single domain [9, 23, 43, 44], limiting data collection and model training in that specific domain, and it is also faced with two limitations, data sparseness, and cold start problems. Cross-Domain Recommendation (CDR) approaches have been proposed, which make use of the extensive data available in the source domain to supplement the target domain. However, with the business growth and the increasing demand for personalized recommendations, models must cater to multiple diverse domains within the system. But due to the varying distribution characteristics of different domains, simply merging multi-domain data would introduce domain bias issues [7]. On the other hand, constructing separate models for each domain would impose significant computational and manual maintenance costs [38, 39]. Therefore, the critical research challenge lies in designing a unified framework that efficiently integrates multiple domains while optimizing overall predictive performance. In fact, the main challenge in multi-domain recommendation is to strike a balance between inter-domain sharing and intra-domain independence [14, 34]. Existing multi-domain recommendation models fall into two categories: (1). Hard sharing models, which typically employ the multi-task learning paradigm, sharing information in the bottom layers while designing individual domain towers at the top [27, 33]. Other hard-sharing strategies, such as Huifeng Guo B Huawei Noah's Ark Lab Shenzhen, China huifeng.guo@huawei.com CIKM '23, October 21-25, 2023, Birmingham, United Kingdom Xiaopeng Li et al. Figure 1: Existing Multi-domain sharing methods. Domain Shared Part Domain Output (a) Hard Share Domain Info Features Domain Shared Part Domain Info Domain Output Features (b) Soft Share STAR [34], utilize a star topology, where one network is shared among different domains and combined with different domainspecific networks through element-wise multiplication. (2). Softsharing models, which indirectly share domain parameters among domains by incorporating auxiliary modules, such as cells, units, or layers, into the main network to carry domain-specific information [21, 42]. For example, domain information is shared through gating networks in the PEPNet model [7], where a gate neural unit is proposed to amplify valid signals, while M2M [42] introduces meta units generated based on specific scenario knowledge. On the other hand, some models adopt various fusion strategies, e.g., ADI [21] employs a fusion layer that concatenates domain-related information generated by both shared and specific networks. Despite the effectiveness demonstrated by the aforementioned approaches in MDR, they still encounter two limitations. Firstly, relying on explicit sharing, particularly hard-sharing, can lead to significant bias in certain domains, as the parameter-sharing paradigm is artificially prioritized. Secondly, both hard-sharing and soft-sharing methods employ static parameters, which exhibit limited scalability and cannot ensure customized modeling patterns for diverse domains. Consequently, it is imperative to explore alternative approaches that incorporate dynamic sharing parameters so as to enable flexible adaptation to rapidly evolving domains and accommodating varying distributions among different domains. In this paper, we propose Hyper Adapter for MUlti-domain Recommendation (HAMUR) to address the above challenges of multi-domain adaptation through adapter technique in the natural language processing (NLP) field [19]. However, considering the disparities in data characteristics between NLP and recommendations, the model's parameters necessitate frequent updates in the recommendations system, which could be labor-intensive if adjusting the adapter for each fine-tuning iteration. Therefore, we refrain from using the two-stage pre-training and fine-tuning approach. Instead, we directly integrate the adapter into the backbone model, employing end-to-end training to facilitate dynamic adaptation to changing data distributions during online deployment. Additionally, we design a hyper-network that is shared among different domains, implicitly capturing domain-specific patterns and dynamically generating the weights of adapters. The input of the hyper-network is the instance-level embedding that carries domain-specific information. We validate the effectiveness of our proposed HAMUR against state-of-art MDR models on two public datasets. The main contributions of this work can be summarized as follows: ¬∑ We propose HAMUR, a novel solution to tackle the challenge of predicting click-through rates (CTR) in a multi-domain setting. Our approach seamlessly integrates with various existing backbones as a plug-and-play component; ¬∑ To account for the variations in data distribution across different domains, we propose a hyper-network that is shared among all domains, generating adapter weight parameters dynamically. This strategy enables the adapter to quickly capture domainspecific patterns, leading to robust generalization across all domains. Furthermore, the hyper-network can implicitly capture shared information across domains; ¬∑ Based on experimental results on two public datasets, our approach surpasses existing state-of-the-art methods in terms of effectiveness. Furthermore, we evaluate the effectiveness of our method by substituting diverse backbone networks, demonstrating the scalability of HAMUR.",
  "2 METHODOLOGY": "In this section, we begin by formulating the task of multi-domain click-through rate (CTR) prediction. We then provide an overview of the architecture of our proposed method. Subsequently, we present a detailed description of our method, which includes the domain adapter cell and the hyper-network. In the end, we will discuss task optimization to conclude this section.",
  "2.1 Problem Formation": "This paper focuses on the CTR prediction task in Multi-Domain Recommendation (MDR). Traditional CTR prediction models are trained on a single domain, where various distinguishing features such as user profiles, historical behaviors, item features, and context features are taken as input ùíô . After passing through the embedding layer, the raw features are then mapped into low-dimensional embedding vectors ùíÜ , which will be fed into the prediction model afterward, and the predicted CTR will be obtained. The label ùë¶ ‚àà { 0 , 1 } indicates whether a click is occurred or not. In contrast to single-domain prediction, an MDR task involves a new feature ùíë ‚àà { 1 , ..., ùê∑ } , where ùíë represents the domain indicator for samples from ùê∑ domains. The objective of CTR in MDR is to construct a unified model F that can accurately predict ÀÜ ùëù for ùê∑ domains simultaneously. It could be written as:  for all ùê∑ domains, the problem can be formulated as:  where ùëë refers to the ùëë -th domain, ùúÉ refers to the parameters of MDR model F , and our objective is to minimize the loss function L ùê∂ùëáùëÖ to get the best MDR model F for the recommender system.",
  "2.2 Framework Overview": "The structure of our model is shown in Figure 2. Our approach integrates the adapter and dynamic weight generation techniques HAMUR: Hyper Adapter for Multi-Domain Recommendation CIKM '23, October 21-25, 2023, Birmingham, United Kingdom Figure 2: An illustration of the overall architecture of our proposed model. Sparse  Features Adaptater Domain d Adapter Domain 1 Domain 0 Dense  Features Domain  Indicator Embedding Layer Projection Projection Non-Linear Domain Norm Domain Specific ‚Ä¶ matrix multiplication Adapter Adapter Representation Matrix Hypernetwork Reshape ùëº ùíÖ ‚Ä¶ Domain Share ùëæ ùüé ùíñ ùíç ùëæ ùüé ùíñ ùíì ùëæ ùüé ùíó ùíç ùëæ ùüé ùíó ùíì ùëΩ ùíÖ Domain 0 Domain 1 Domain d ùë∞ ùëæ ùíÖ ùíñ ùíç ùëæ ùíÖ ùíñ ùíì ùëæ ùíÖ ùíó ùíç ùëæ ùíÖ ùíó ùíì ùë∞ ùë∞ ùëæ ùüé ùíñ ùíç ùëæ ùüé ùíñ ùíì ùëæ ùüé ùíó ùíç ùëæ ùüé ùíó ùíì ùëæ ùüé ùíñ ùíç ùëæ ùüé ùíñ ùíì ùëæ ùüé ùíó ùíç ùëæ ùüé ùíó ùíì together for MDR modeling. Specifically, we propose a domainspecific adapter cell that can be seamlessly incorporated into existing MDR models. Additionally, to capture the underlying similarities among different domains, we propose a domain-shared hyper-network to generate parameters dynamically for the adapter. ¬∑ Domain-specific adapter cell : To capture the domain personality, we propose to utilize a small network named adapter as a pluggable component for MDR modeling. It is designed as a bottleneck shape component that comprises four layers, a down projection layer, a non-linear layer, an up projection layer, and a domain normalization layer, besides a skip connecting established from the beginning to the end of the adapter. Compared with the adapter [19] in the field of NLP, which is used for fine-tuning Large pre-trained Language Models (LLM). We have forsaken the two-stage approach and instead integrated the adapter directly into the backbone network to facilitate end-to-end training. We use the adapter as a pluggable component, which could be easily integrated into various backbone networks such as MLP [2], DCN [35], Wide & Deep [8], DeepFM [15], and so on. ¬∑ Domain-shared hyper-network : In order to address the issue of domain bias, we present a solution in the form of a shared hyper-network, which is used for generating parameters for the domain adapters. To improve computational efficiency, the matrix low-rank decomposition method is employed, whereby the target matrix is decomposed into the product of three matrices. Specifically, the hyper-network input comprises instances with domain information ( ùíô , ùíë ) and generates an instance-leveled domain representation matrix ùë∞ . Matrices ùëæ ùë¢ ùëô ùëë , ùëæ ùë¢ ùëü ùëë , ùëæ ùë£ ùëô ùëë , and ùëæ ùë£ ùëü ùëë are then multiplied with the matrix ùë∞ to obtain matrices ùëº ùëë and ùëΩ ùëë , which serve as adapter's parameters for domain ùëë .",
  "2.3 Domain-Specific Adapter Cell": "The domain-specific adapter cell has been designed to capture each domain's unique characteristics. In this section, the structure of domain-specific adapter cells and domain adapter normalization layer will be introduced subsequently. 2.3.1 Domain Adapter Layers. In the field of NLP, fine-tuning involves the insertion of small modules called adapters into pretrained models without any modifications to the architecture or parameters of the original model. However, frequently updating model parameters is necessary in the context of recommender systems. This is particularly evident in scenarios such as e-commerce websites where promotional activities are carried out or in video websites where new videos are released. In order to align with these changes, the model parameters must be updated accordingly. If we persist with the two-stage adjustment approach for the adapter in each model update, it will be time-consuming and labor-intensive, which is unacceptable. So we design the adapter as a plug-in into the existing multi-domain model directly, thereby enabling its participation in end-to-end training. The structure of the adapter is shown in Figure 2. The adapter cell, denoted as ùë® ùëë , is designed with the shape of the bottleneck. It includes four layers, a down-projection ùëº ùëë ‚àà R ‚Ñé √ó ùë† , a sigmoid nonlinear layer ùúé , an up-projection ùëΩ ùëë ‚àà R ùë† √ó ‚Ñé , and a domain normalization layer, besides a residual connection from the beginning of the cell to the end. ‚Ñé denotes the input dimension, and ùë† denotes the bottleneck dimension, ùë† << ‚Ñé . Through down-projection, input data is compressed into low-dimensional embedding. It is followed by a non-linear layer and feature reconstruction via up-projection. The mathematical formulation for the whole domain-specific adapter CIKM '23, October 21-25, 2023, Birmingham, United Kingdom Xiaopeng Li et al. cell is defined as:  where ùíô represents the hidden input state, ùê∑ùëÅ ùëë denotes domain normalization and will be elaborated upon in the subsequent section. ùëº ùëë and ùëΩ ùëë are generated through a shared hyper-network, which will be described in Section 2.4, and ùëë represents the ùëë -th domain. 2.3.2 Domain Adapter Normalization. The normalization layer in the adapter design conventionally utilizes layer normalization. In the NLP field, the length of the input sequence varies, and there is a semantic correlation between individual tokens in one sentence. Hence, it is reasonable to use layer normalization to normalize the tokens in each sentence. However, this approach is not applicable in the area of recommender system, where the embeddings of distinct feature fields represent different meanings. Directly normalizing across feature fields would be an irrational practice. Consequently, we opt for batch normalization to normalize the instances of the entire mini-batch. In the case of multi-domain CTR prediction, different from singledomain tasks, the data distribution is locally Independent Identically Distributed (IID) in each individual domain. Hence, we propose the use of domain normalization, which is expressed as:  where ùõæ ùëë and ùõΩ ùëë are learnable parameters for domain-specific scale and bias parameters. ‚äô is the element-wise multiplication. ùúá and ùúé represent the mean and standard deviation of the data ùíô .",
  "2.4 Domain Shared Hyper-Network": "In order to capture implicit information shared between different domains and dynamically generate parameters, we propose the inter-domain sharing hyper-network. Additionally, to improve the generation efficiency of the target matrix, we draw inspiration from [40] and adopt a low-rank decomposition method to decompose the target matrix into the product of three matrices. During the generation process, the hyper-network generates only a core expression matrix ùë∞ , which reduces memory usage and significantly improves operational efficiency. The following section will provide a detailed description. 2.4.1 Parameters Generation. This subsection will elaborate on how we use hyper-network to generate parameters dynamically. The input includes two parts, the raw features ùíô , and the domain indicator ùíë . They will be fed into the embedding layer. A mathematical expression could be formulated as:  It is worth noting that our generation process is in instance-level. Therefore, for the convenience of description, we rewrite Equation (5). Assuming our current batch size is ùêµ , then ùíÅ could be rewritten as:  The domain-shared hyper-network H( . ) is a shallow neural network. For each instance, ùíõ ùëñ , it will be fed into the domain-shared hyper-network H( . ) . After that, a one-dimensional representation ùíâ ùëñ will be obtained, an expression containing different domain commonality. Then, we reshape ùíâ ùëñ to form the matrix ùë∞ ùëñ to facilitate matrix operations in the subsequent stages. formally, this process is expressed as:   For ùë∞ ùëñ , we name it the representation matrix for the ùëñ -th instance. 2.4.2 Low-Rank Decomposition. This part will explain how we leverage low-rank matrix decomposition method to enhance computational efficiency. As previously mentioned, we generate one matrix ùë∞ for each instance, but we require two target matrices, ùëº ùëë and ùëΩ ùëë , so we utilize the low-rank decomposition method to address this issue, drawing inspiration from other literatures [40]. Specifically, for the target matrices ùëº ùëñ ùëë ‚àà R ùë† √ó ‚Ñé and ùëΩ ùëñ ùëë ‚àà R ‚Ñé √ó ùë† , we decompose these two matrices into three matrices multiplication, respectively, expressed as:   where ùëæ ùë¢ ùëô ùëë ‚àà R ùë† √ó ùëò , ùëæ ùë¢ ùëü ùëë ‚àà R ùëò √ó ‚Ñé , ùëæ ùë£ ùëô ùëë ‚àà R ‚Ñé √ó ùëò , ùëæ ùë£ ùëü ùëë ‚àà R ùëò √ó ùë† , and ùë∞ ùëñ ‚àà R ùëò √ó ùëò . During model training, ùëæ ùë¢ ùëô ùëë , ùëæ ùë¢ ùëü ùëë , ùëæ ùë£ ùëô ùëë , and ùëæ ùë£ ùëü ùëë are the inherent parameters of the model that participate in the training process, thus we only need to generate the matrix ùë∞ ùëñ via the hyper-network. There are two primary reasons for utilizing low-rank decomposition. Firstly, it improves computational efficiency. By generating only one matrix ùë∞ ùëñ with a size of ùëò √ó ùëò directly, as opposed to directly generating the target matrices ùëº ùëñ ùëë and ùëΩ ùëñ ùëë , we can significantly improve the operating efficiency. Secondly, the design enables the parameters ùëæ ùë¢ ùëô ùëë , ùëæ ùë¢ ùëü ùëë , ùëæ ùë£ ùëô ùëë , and ùëæ ùë£ ùëü ùëë in the ùëë -th domain to share domain-specific information across instances, thereby effectively capturing private domain patterns.",
  "2.5 Integration with Backbone Model": "This section will introduce how the adapter integrates into the backbone model. As we mentioned in Section 2.1, the base MDR model is denoted as:  where F ùëë ( ùúÉ ùëë ) denotes ùëë -th domain model, and it could be any kind of existing backbone network, like MLP [2], DCN [35], Wide & Deep [8], and for all the ùê∑ models, they are domain independent. Upon integration with the domain-specific adapter cell, the model can be represented as F ùëë ( ùúÉ ùëë , ùë® ùëë ) , where the adapter is incorporated as an integral part of the model. It is important to note that these ùê∑ models can no longer be regarded as domain-independent since the adapters are generated through a domain-shared hypernetwork. Technically, we chose to insert the adapter at the top level of the backbone network. It was motivated by the hierarchical nature of the model architecture. The lower layers of the model are responsible for feature extraction and intersection, while the upper layers produce a more refined feature mapping and the final prediction. Given our objective of facilitating rapid adaptation of HAMUR: Hyper Adapter for Multi-Domain Recommendation CIKM '23, October 21-25, 2023, Birmingham, United Kingdom",
  "Algorithm 1: The Algorithm for HAMUR.": "1 Input: features ùíô , domain indicator ùíë , ground-truth labels ùíö , batch size ùêµ . Output: Prediciton click label ÀÜ ùë¶ . 2 while not converged do 3 Sample a mini-batch data ( ùíô , ùíë , ùë¶ ) with size ùêµ ; 4 Get ùíÅ via Equation (5); 5 foreach ùíõ ùëñ ‚àà ùíÅ do 6 foreach ùëë = 1 to ùê∑ do 7 Get domain parameters ùëæ ùë¢ ùëô ùëë , ùëæ ùë¢ ùëü ùëë , ùëæ ùë£ ùëô ùëë , ùëæ ùë£ ùëü ùëë ; 8 Get domain-specific backbone F ùëë ( ùúÉ ùëë ) ; 9 if ùëù ùëñ == ùëë then 10 Calculate ùë∞ ùíä via Equation (6), (7); 11 Calculate ùëº ùëñ ùëë and ùëΩ ùëñ ùëë via Equation (8), (9); 12 Get Domain adapter ùë® ùëë via Equation (3); 13 Calculate the CTR result via Equation (10); 14 else 15 Continue; 16 end 17 end 18 end 19 Calculate loss via Equation (11); 20 Gradient calculation and update respective parameters; 21 end the adapter to new domains, it would be counterproductive to introduce changes to the feature distribution prematurely, as this would weaken the feature extraction capability. Therefore, placing the adapter at the bottom of the model would be inappropriate. Overall, for the ùëñ -th instance, the output of CTR could be formulated as:",
  "2.6 Optimization": "The overall optimization goal of our model is:  The objective function applied in our model is the cross entropy loss function. ÀÜ ùë¶ ùëë ùëñ and ùë¶ ùëë ùëñ denotes the prediction and the ground truth in the ùëë -th domain, respectively. The calculation process could be tracked in Algorithm 1. Feature embeddings are obtained through the embedding layer (line 3-4), then for each instance, we select domain parameters ùëæ ùë¢ ùëô ùëë , ùëæ ùë¢ ùëü ùëë , ùëæ ùë£ ùëô ùëë , ùëæ ùë£ ùëü ùëë (line 7) and domain backbone network F ùëë ( ùúÉ ùëë ) (line 8). Afterward, we derived the adapter's parameters ùëº ùëñ ùëë and ùëΩ ùëñ ùëë via Equation (8) and Equation (9) (line 10-12). Then we integrate the adapter with the backbone network together to get the prediction ÀÜ ùë¶ ùëë ùëñ (line 13). Loss is calculated, followed by a parameters update (line 19-20).",
  "3 EXPERIMENT": "In this section, we conduct extensive experiments to verify the effectiveness of our proposed framework. Our experiments are designed to answer the following questions. Table 1: The statistics of datasets of each domain. ¬∑ RQ1: How does our proposed method compare to other state-ofthe-art methods in terms of performance? ¬∑ RQ2: Asthe adapter is a pluggable unit, how does its performance vary when used with different backbone networks? ¬∑ RQ3: How does the size of the generated presentation matrix ùêº ùëñ impact the performance? ¬∑ RQ4: How does the structure of the hyper-network affect the result?",
  "3.1 Experimental Settings": "3.1.1 Dataset. We conduct experiments on two public datasets. The statistics of the two datasets are shown in Table 1. ¬∑ MovieLens 3 . This dataset describes people's preferences for movies, containing 7 user features and 2 item features. We use the user feature 'age' to divide the dataset into three different domains. Besides, we randomly split this dataset into training, validation, and test sets with a ratio of 8:1:1. ¬∑ Ali-CCP 4 . This dataset is collected from Alimama's traffic logs. The dataset consists of 13 user features, 5 item features, 4 combination features, and 2 label features - clicks and purchases. In this article, we only use clicks as the target label. We use the contextual feature '301' (representing the position) to divide the data into three domains. This dataset is already split into training, validation, and test sets by default. 3.1.2 Baseline. Wecompare our model with the multiple baselines, and they are listed as follows: ¬∑ STAR [34]. STAR is a novel model with star topology that can cater to multiple domains using a unified model. It comprises shared centered parameters and multiple domain-specific parameters, which capture specific behaviors in different domains to facilitate more refined CTR prediction. The centered parameters are utilized to learn general patterns across all domains, thereby enabling the acquisition and transfer of common knowledge among all domains. ¬∑ APG [40]. The APG proposes an innovative approach to deep CTR models that can dynamically generate parameters for CTR prediction models, leading to more accurate results. In our comparison experiments, we choose this method as a comparison to generative parametric methods. ¬∑ SharedBottom [6]. The sharedbottom model shares the bottom layer in multi-task learning. In multi-domain learning, we substitute each task tower as a domain tower. 3 https://grouplens.org/datasets/movielens/ 4 https://tianchi.aliyun.com/dataset/408 CIKM '23, October 21-25, 2023, Birmingham, United Kingdom Xiaopeng Li et al. Table 2: Performance Comparison Results. ' * ' indicates significance level test ùëù < 0 . 05 that compare HAMUR over the best baseline model. ¬∑ MMOE [26]. The MMOE is an approach for multi-task learning that utilizes a group of bottom networks, called 'expert'. It is designed to gain knowledge about the fundamental feature representations. For each task, a distinct gate network is employed, and the results of the assembled experts are then passed to merge the information. For our MDR task, we treat each task tower as a domain tower and the task-specific gate network as a domain-specific gate network. 3.1.3 Evaluation Metric. We use two evaluation metrics Area Under the ROC (AUC) and LogLoss in our experiments to evaluate the test performance of models. Generally, a higher AUC or a lower Logloss value represents superior performance [15].",
  "3.2 Performance Comparison (RQ1)": "Theoffline comparison on MovieLens and Ali-CCP between HAMUR and baseline models are shown in Table 2, respectively. More experiment implementation details can be found in Appendix A.1. We summarize the observations as follows: ¬∑ HAMUR consistently outperforms various kinds of state-of-theart models over all datasets by a significant margin, demonstrating the effectiveness of HAMUR in MDR tasks. Compared with the baseline model STAR, HAMUR utilizes a hyper-network to share domain information rather than directly multiplying a shared fully connected network. HAMUR is capable of capturing domain characteristics with flexibility. In contrast to APG, which generates weights for the backbone network, HAMUR generates parameters for each domain-specific adapter, thereby retaining the specialties of each domain. Compared with the multi-task models SharedBottom and MMOE, HAMUR explicitly models the domain information better to capture the distinct distribution characteristics between different domains. ¬∑ HAMUR can boost the performance of models on different domains, especially in the domain where training samples are rare, such as the 2# domain in Ali-CCP. As our model operates at an instance level, it can effectively incorporate enough information from other domains, ultimately improving overall performance. ¬∑ Our model has demonstrated promising performance across two different datasets, which can be attributed to the efficiency of the hyper-network. The hyper-network is adept at capturing finegrained domain similarity at the instance level, thereby enabling effective characterization of data distribution characteristics even for disparate datasets.",
  "3.3 Compatibility Experiment (RQ2)": "As mentioned above, our proposed HAMUR is a pluggable component to different backbone networks. So, in this section, we choose three different networks MLP [2], DCN [35], Wide & Deep [8], and the result is shown in Table 3. The experiment comprises an independent backbone network test and a backbone network with our proposed HAMUR test, akin to the ablation experiment setup. The performance of the model experiment is reported on two datasets. For more experiment implementation details, please refer to Appendix A.2. We summarize the observations as follows: ¬∑ Our proposed HAMUR is notably capable of comprehending domain distinctions and effectively incorporating domain representations into existing backbone networks, leading to improved prediction results across all different backbone models. This advantage could be attributed to the design of the adapter. The domain-specific adapter can significantly change the distribution of the original domain. ¬∑ Besides, our results demonstrate that HAMUR can consistently improve the performance of different types of backbone models across various datasets, underscoring its efficacy and potency. We attribute this to our proposed hyper-network. Because it implicitly captures domain features for different distributions. ¬∑ Moreover, for different backbones, The lift that our model brings varies. For example, wide & deep improve significantly on AliCCP after combining our model. We attribute this as the consequence of the inner bias of different models. Given that our HAMUR: Hyper Adapter for Multi-Domain Recommendation CIKM '23, October 21-25, 2023, Birmingham, United Kingdom Table 3: Compatibility Experiment Results. network achieves excellent migration performance, other backbones that would like to expand into multiple domain models do not necessitate a complete restructure. Integrating our HAMUR model alone can produce outstanding performance. Our approach holds promising implications for practical implementation.",
  "3.4 Hyper Parameters Experiment (RQ3 & RQ4)": "0.815 0.6290 AUC 0.813 STAR 0.811 0.6285 AUC 0.809 STAR 0.807 6280 0.805 0.803 0.6275 10 20 30 40 50 15 30 45 (b) AUC v.s. K on MovieLens on Ali-CCP 3.4.1 Dimension of Presentation Matrix (RQ3). As discussed in Section 2.4, utilizing low-rank dimension allows for decomposing the target matrices ùëº and ùëΩ into three matrix multiplications. We only need to generate the representation matrix ùë∞ ùëñ ‚àà R ùëò √ó ùëò for each instance. The value of ùëò determines the size of matrix ùë∞ ùëñ thus impacts the representation ability of the hyper-network. To investigate the effect of ùëò on the performance of our model, we conducted experiments with varying values of ùëò . The results are presented in Figure 3. Concretely, we conducted a performance comparison experiment of various ùëò values against the baseline model STAR. From Figure 3 (a) and Figure 3 (b), we can see that even for different datasets, the curve shows the same trend, revealing that as ùëò increases, the model's effectiveness improves. STAR exhibited a slight advantage over HAMUR at lower ùëò values, but our model achieved significant AUC enhancements as ùëò values increased. A larger ùëò is needed to exceed STAR in Ali-CCP. We assume this is caused by the extremely uneven distribution of Ali-CCP in each domain. However, given that our model operates at the instance level and generates a matrix for each instance, choosing an appropriate ùëò value is crucial to avoid the excessive computational burden. Therefore, selecting a suitable value of ùëò within the limits of available computing resources is necessary. 3.4.2 Dimension of Hyper-network (RQ4). As discussed in Section 2.4.1, the architecture of the hyper-network is a shallow neural Figure 3: The effect of ùêæ on AUC on different datasets. network. To be exact, it constitutes a single-layered Deep Neural Network. To investigate the impact of various dimensions of the hidden state of the hypernet on the model's performance, we conducted experiments on two datasets, and the outcomes are presented in Figure 4. AUC 0.813 Best AUC 0.812 0.811 100 200 300 400 50o AUC v.s Hypernet-dim on MovieLens Figure 4: The effect of hyper-network dimensions on different datasets. 0.6305 AUC Best AUC 6300 0.6295 6290 100 200 300 400 50o (b) AUC v.s. Hypernet-dim on Ali-CCP The findings in two public datasets reveal that augmenting the dimensionality of the hyper-network does not improve the model's efficacy. Actually, there is a decline in performance after reaching the peak, demonstrating that hyper-network is susceptible to overfitting. Mere augmentation of dimensionality fails to enhance the CIKM '23, October 21-25, 2023, Birmingham, United Kingdom Xiaopeng Li et al. expressive potential of the hyper-network, instead, yields detrimental outcomes. This hints that there is no need to choose a significant dimension of the hyper-network.",
  "4 RELATED WORK": "This section will briefly introduce some topics related to our study. We will first introduce the recent studies of MDR and followed by a brief review of dynamic weight generation in deep neural networks. The latest research and relevant literature on adapters will be presented at the end of this section.",
  "4.1 Multi-Domain Recommendation": "The MDR model is an approach that seeks to enhance the performance of recommendation systems models across multiple domains by constructing a unified model that serves all domains simultaneously. In contrast to the cross-domain recommendation approaches [13, 14, 20, 24, 37], cross-domain recommendation models only focus on transferring knowledge from a single source domain to a target domain, however, MDR models seek to optimize performance across all domains. Various methods have been proposed to address multi-domainrelated problems in recommendation systems. We categorize the present strategies into different perspectives. (1). From the perspective of causal inference. CausalInt [38] proposed TransNet to simulate a counterfactual intervention and integrate domain information. (2). From the perspective of embedding. AdaptDHM [22] proposed a distribution adaptation module that captures the commonalities and distinctions between distinct embedding clusters, thus executing different domain strategies. (3). From the perspective of pruning. Adasparse [41] proposed a domain pruner, which takes different pruning methods for different domains. (4). From the Generative Adversarial Network (GAN) perspective. AFT [18] proposed a domain-specific masked encoder to capture domainrelated feature interactions in the generator. In the multi-domain discriminator, AFT utilizes knowledge representation learning to model relations between items, users, and domain indicators. (5). From the perspective of the attention mechanism. SAR-Net [33] proposes attention layers to capture users' cross-domain interest transfer and design a domain-specific and domain-shared experts network pipeline. (6). From the perspective of topology. STAR [34] proposed a star topology with a shared fully connected network at the center, which is multiplied by domain-specific fully connected networks in the domain-specific calculation. (7). From the perspective of meta-learning. M2M [42] proposed a meta-unit to simultaneously learn domain-related and task-related information to achieve multi-domain multi-task learning. Unlike the above methods, our proposed method is a pluggable component with strong scalability that could be combined with multiple backbone models.",
  "4.2 Dynamic Weight Generation In Deep Neural Networks": "Weight generation originated from evolutionary computing, which addresses the challenge of operating in large search spaces comprising millions of weight parameters. It is challenging to manipulate such vast search spaces directly. Therefore, weight generation was developed as an alternative approach to overcome this limitation. Hyper-network [16] was proposed using a small network to generate weights for RNN, achieving outstanding performance. These weight-generation methods have been used in Other areas like computer vision [17, 29], nature language processing [10, 28], reinforcement learning [4, 32], speech [12]. Dynamic weight generation approaches have also been widely used in the context of recommendation systems. CAN [5] proposes a co-action unit in which the items model's weights are generated dynamically. APG [40] generates different models' weight matrices and gets great performance. M2M [42] generates meta-units' weight matrix with scenario knowledge. Compared with these approaches, we follow the same paradigm of parameter generation, but in our method, we generate parameters for the adapters.",
  "4.3 Adapter": "The adapter [31] module was originally proposed in the field of computer vision, which is developed as a tunable deep neural network architecture that can be steered on the fly to diverse visual objects. This module was deployed and used in nature language processing by [19]. Transfer learning was initially employed through pre-trained feature vectors and fine-tuning of pre-trained language models (PLMs). The adapter presents a novel approach: integrating a small number of parameters into the model, which can be trained only during the fine-tuning of downstream tasks while keeping the parameters of the pre-training model unaltered. The existing adapters can be divided into the following three types, series adapters, parallel adapters, and Low-Rank Adaption models (LoRA). In the series adapter, the adapter cell is added in series behind each multi-head attention layer and a feed-forward layer of the transformer block. The parallel adapter connects adapter cells in parallel to each multi-head attention layer and feed-forward layer. Whereas LoRA introduces trainable low-rank factorization matrices in the existing layers of LLMs, enabling the model to adapt to new data while keeping the original LLMs fixed to preserve existing knowledge. This approach offers several benefits. (1). The requirement for training only a small number of parameters of the adapter module leads to reduced training costs and enhanced portability. (2). For continual learning of different tasks, adapters solely need to be trained, while other parameters remain unaltered, thereby preventing forgetting past knowledge when learning new tasks. The adapter has demonstrated remarkable performance across diverse tasks. Its utilization in neural machine translation [3], relation and classification [36], entity typing [36], and even multimodal tasks [25] have yielded impressive outcomes. In our paper, we proposed HAMUR. As far as we know, it is the first time that the adapter has been used in MDR and adapts to the end-to-end training method of the recommendation system.",
  "5 CONCLUSION": "In this paper, we proposed a novel model, Hyper Adapter MultiDomain Recommendation Model (HAMUR) for Multi-Domain Recommendation (MDR). HAMUR comprises two components, namely, a domain-shared hyper-network that dynamically captures domainshared information and a domain-specific adapter that can be easily integrated into different existing networks. The model offers CIKM '23, October 21-25, 2023, Birmingham, United Kingdom HAMUR: Hyper Adapter for Multi-Domain Recommendation Table 4: The Hypter parameters of Baseline Comparison Experiment. several innovative features: firstly, it utilizes a hyper-network to implicitly capture domain-shared information, thereby avoiding the domain-bias problem associated with hard-share-parameters methods. Secondly, to the best of our knowledge, HAMUR is the first model to use an adapter in MDR, and the adapter is directly integrated into the model to achieve end-to-end training. Lastly, extensive experiments on two public datasets demonstrate the superior performance of HAMUR. Future research will investigate the model in fine-grained domains and verify its feasibility in more complex multi-task scenarios.",
  "ACKNOWLEDGMENTS": "This research was partially supported by Huawei (Huawei Innovation Research Program), APRC - CityU New Research Initiatives (No.9610565, Start-up Grant for New Faculty of City University of Hong Kong), CityU - HKIDS Early Career Research Grant (No.9360163), Hong Kong ITC Innovation and Technology Fund Midstream Research Programme for Universities Project (No. ITS/034/22MS), SIRG - CityU Strategic Interdisciplinary Research Grant (No.7020046, No.7020074), SRG-Fd - CityU Strategic Research Grant (No.7005894). We thank MindSpore [1] for the partial support of this work, which is a new deep learning computing framework.",
  "A EXPERIMENT DETAILS": "",
  "A.1 Performance Comparison Experiment Details": "The following setups are adopted to ensure a fair comparison in the baseline experiment. ¬∑ Backbone models of STAR, APG, and HAMUR are implemented using the same dimension of MLP. Specifically, for the MovieLens dataset, a 2-layer MLP is chosen. For the Ali-CCP dataset, a 7layer MLP is selected. ¬∑ The dimension of the hyper-network in HAMUR, the auxiliary network in STAR, and the expert network are set to be identical without loss of generality. ¬∑ The location of the adapter that has been plugged is situated on the top layer of the base model. Conctreatly, in the dataset of MovieLens, the position is situated between the first and second layers. But in the dataset of Ali-CCP, we insert two adapter cells at the top layer of the base model. For details, refer to Table 4. Figure 5: Different Backbone Network. Adapter Adapter Adapter (c) Deep Cross Network DNN CN (b) Wide & Deep (a) MLP Wide Deep Table 5: The Hypter parameters of Compatibility Experiment.",
  "A.2 Compatibility Experiment Details": "For Different backbone network experiments, we compare three networks: MLP, DCN, and Wide & Deep. The general structures are shown in Figure 5. The following setups are employed for conducting the experiments. ¬∑ We design two kinds of experiments, integrating HAMUR in the backbone network and without HAMUR in the backbone network. Taking DCN as an example, each domain will construct a DCN independently as a backbone network. In the experiment ingratiating the backbone network and HAMUR, domain-specific adapters will be inserted in each backbone network, and a domainshared hyper-network will also be constructed to capture domainshared information. ¬∑ The adapter is integrated into the Deep Network for DCN and into the deep side for Wide & Deep, and for MLP, we follow the setting elaborated in the Appendix A.1. ¬∑ Various insertion positions of the adapter are tested through extensive experiments, and it is observed that the positions closer to the model's terminus produced the most favorable performance. Consequently, all adapters are integrated between the last and penultimate layers. For details, refer to Table 5. CIKM '23, October 21-25, 2023, Birmingham, United Kingdom Xiaopeng Li et al.",
  "REFERENCES": "[1] 2020. MindSpore. https://www.mindspore.cn/ [2] Shunichi Amari. 1967. A theory of adaptive pattern classifiers. IEEE Transactions on Electronic Computers 3 (1967), 299-307. [3] Ankur Bapna, Naveen Arivazhagan, and Orhan Firat. 2019. Simple, scalable adaptation for neural machine translation. arXiv preprint arXiv:1909.08478 (2019). [4] Jacob Beck, Matthew Thomas Jackson, Risto Vuorio, and Shimon Whiteson. 2023. Hypernetworks in meta-reinforcement learning. In Conference on Robot Learning . PMLR, 1478-1487. [5] Weijie Bian, Kailun Wu, Lejian Ren, Qi Pi, Yujing Zhang, Can Xiao, Xiang-Rong Sheng, Yong-Nan Zhu, Zhangming Chan, Na Mou, et al. 2020. CAN: Feature Co-Action for Click-Through Rate Prediction. arXiv preprint arXiv:2011.05625 (2020). [6] Rich Caruana. 1997. Multitask learning. Machine learning 28 (1997), 41-75. [7] Jianxin Chang, Chenbin Zhang, Yiqun Hui, Dewei Leng, Yanan Niu, and Yang Song. 2023. PEPNet: Parameter and Embedding Personalized Network for Infusing with Personalized Prior Information. arXiv preprint arXiv:2302.01115 (2023). [8] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems . 7-10. [9] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks for youtube recommendations. In Proceedings of the 10th ACM conference on recommender systems . 191-198. [10] Rumen Dangovski, Li Jing, Preslav Nakov, Miƒáo Tataloviƒá, and Marin Soljaƒçiƒá. 2019. Rotational unit of memory: a novel representation unit for RNNs with scalable applications. Transactions of the Association for Computational Linguistics 7 (2019), 121-138. [11] James Davidson, Benjamin Liebald, Junning Liu, Palash Nandy, Taylor Van Vleet, Ullas Gargi, Sujoy Gupta, Yu He, Mike Lambert, Blake Livingston, et al. 2010. The YouTube video recommendation system. In Proceedings of the fourth ACM conference on Recommender systems . 293-296. [12] Yingjun Du, Nithin Holla, Xiantong Zhen, Cees GM Snoek, and Ekaterina Shutova. 2021. Meta-learning with variational semantic memory for word sense disambiguation. arXiv preprint arXiv:2106.02960 (2021). [13] Wenqi Fan, Xiangyu Zhao, Qing Li, Tyler Derr, Yao Ma, Hui Liu, Jianping Wang, and Jiliang Tang. 2023. Adversarial Attacks for Black-Box Recommender Systems Via Copying Transferable Cross-Domain User Profiles. IEEE Transactions on Knowledge and Data Engineering (2023). [14] Jingtong Gao, Xiangyu Zhao, Bo Chen, Fan Yan, Huifeng Guo, and Ruiming Tang. 2023. AutoTransfer: Instance Transfer for Cross-Domain Recommendations. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1478-1487. [15] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. arXiv preprint arXiv:1703.04247 (2017). [16] David Ha, Andrew Dai, and Quoc V Le. 2016. HyperNetworks. arXiv preprint arXiv:1609.09106 (2016). [17] Sungsoo Ha. 2019. An End-to-end Deep Convolutional Neural Network for a Multiscale Image Matching and Localization Problem . Technical Report. Brookhaven National Lab.(BNL), Upton, NY (United States). [18] Xiaobo Hao, Yudan Liu, Ruobing Xie, Kaikai Ge, Linyao Tang, Xu Zhang, and Leyu Lin. 2021. Adversarial feature translation for multi-domain recommendation. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining . 2964-2973. [19] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. In International Conference on Machine Learning . PMLR, 2790-2799. [20] Guangneng Hu, Yu Zhang, and Qiang Yang. 2018. Conet: Collaborative cross networks for cross-domain recommendation. In Proceedings of the 27th ACM international conference on information and knowledge management . 667-676. [21] Yuchen Jiang, Qi Li, Han Zhu, Jinbei Yu, Jin Li, Ziru Xu, Huihui Dong, and Bo Zheng. 2022. Adaptive domain interest network for multi-domain recommendation. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management . 3212-3221. [22] Jinyun Li, Huiwen Zheng, Yuanlin Liu, Minfang Lu, Lixia Wu, and Haoyuan Hu. 2022. AdaptDHM: Adaptive Distribution Hierarchical Model for Multi-Domain CTR Prediction. arXiv preprint arXiv:2211.12105 (2022). [23] Muyang Li, Zijian Zhang, Xiangyu Zhao, Wanyu Wang, Minghao Zhao, Runze Wu, and Ruocheng Guo. 2023. AutoMLP: Automated MLP for Sequential Recommendations. In Proceedings of the Web Conference 2023 . [24] Pan Li and Alexander Tuzhilin. 2020. Ddtcdr: Deep dual transfer cross domain recommendation. In Proceedings of the 13th International Conference on Web Search and Data Mining . 331-339. [25] Zhiqiu Lin, Samuel Yu, Zhiyi Kuang, Deepak Pathak, and Deva Ramanan. 2023. Multimodality helps unimodality: Cross-modal few-shot learning with multimodal models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 19325-19337. [26] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. 2018. Modeling task relationships in multi-task learning with multi-gate mixture-ofexperts. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1930-1939. [27] Weizhi Ma, Min Zhang, Chenyang Wang, Cheng Luo, Yiqun Liu, and Shaoping Ma. 2018. Your Tweets Reveal What You Like: Introducing Cross-media Content Information into Multi-domain Recommendation.. In IJCAI . 3484-3490. [28] Rabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, and James Henderson. 2021. Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks. arXiv preprint arXiv:2106.04489 (2021). [29] Tony CW Mok and Albert CS Chung. 2021. Conditional deformable image registration with convolutional neural network. In Medical Image Computing and Computer Assisted Intervention-MICCAI 2021: 24th International Conference, Strasbourg, France, September 27-October 1, 2021, Proceedings, Part IV 24 . Springer, 35-45. [30] Shumpei Okura, Yukihiro Tagami, Shingo Ono, and Akira Tajima. 2017. Embedding-based news recommendation for millions of users. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining . 1933-1942. [31] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. 2017. Learning multiple visual domains with residual adapters. Advances in neural information processing systems 30 (2017). [32] Sahand Rezaei-Shoshtari, Charlotte Morissette, Francois R Hogan, Gregory Dudek, and David Meger. 2023. Hypernetworks for zero-shot transfer in reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 37. 9579-9587. [33] Qijie Shen, Wanjie Tao, Jing Zhang, Hong Wen, Zulong Chen, and Quan Lu. 2021. Sar-net: a scenario-aware ranking network for personalized fair recommendation in hundreds of travel scenarios. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management . 4094-4103. [34] Xiang-Rong Sheng, Liqin Zhao, Guorui Zhou, Xinyao Ding, Binding Dai, Qiang Luo, Siran Yang, Jingshan Lv, Chi Zhang, Hongbo Deng, et al. 2021. One model to serve all: Star topology adaptive recommender for multi-domain ctr prediction. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management . 4104-4113. [35] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network for ad click predictions. In Proceedings of the ADKDD'17 . 1-7. [36] Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Guihong Cao, Daxin Jiang, Ming Zhou, et al. 2020. K-adapter: Infusing knowledge into pre-trained models with adapters. arXiv preprint arXiv:2002.01808 (2020). [37] Yejing Wang, Zhaocheng Du, Xiangyu Zhao, Bo Chen, Huifeng Guo, Ruiming Tang, and Zhenhua Dong. 2023. Single-shot Feature Selection for Multi-task Recommendations. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval . 341-351. [38] Yichao Wang, Huifeng Guo, Bo Chen, Weiwen Liu, Zhirong Liu, Qi Zhang, Zhicheng He, Hongkun Zheng, Weiwei Yao, Muyu Zhang, et al. 2022. Causalint: Causal inspired intervention for multi-scenario recommendation. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 4090-4099. [39] Yuhao Wang, Xiangyu Zhao, Bo Chen, Qidong Liu, Huifeng Guo, Huanshuo Liu, Yichao Wang, Rui Zhang, and Ruiming Tang. 2023. PLATE: A PromptEnhanced Paradigm for Multi-Scenario Recommendations. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1498-1507. [40] Bencheng Yan, Pengjie Wang, Kai Zhang, Feng Li, Hongbo Deng, Jian Xu, and Bo Zheng. 2022. Apg: Adaptive parameter generation network for click-through rate prediction. Advances in Neural Information Processing Systems 35 (2022), 24740-24752. [41] Xuanhua Yang, Xiaoyu Peng, Penghui Wei, Shaoguo Liu, Liang Wang, and Bo Zheng. 2022. AdaSparse: Learning Adaptively Sparse Structures for Multi-Domain Click-Through Rate Prediction. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management . 4635-4639. [42] Qianqian Zhang, Xinru Liao, Quan Liu, Jian Xu, and Bo Zheng. 2022. Leaving no one behind: A multi-scenario multi-task meta learning approach for advertiser modeling. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining . 1368-1376. [43] Xiangyu Zhao, Long Xia, Liang Zhang, Zhuoye Ding, Dawei Yin, and Jiliang Tang. 2018. Deep Reinforcement Learning for Page-wise Recommendations. In Proceedings of the 12th ACM Recommender Systems Conference . ACM, 95-103. [44] Xiangyu Zhao, Liang Zhang, Zhuoye Ding, Long Xia, Jiliang Tang, and Dawei Yin. 2018. Recommendations with Negative Feedback via Pairwise Deep Reinforcement Learning. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . ACM, 1040-1048.",
  "keywords_parsed": [
    "Multi-Domain Recommendation",
    "Recommendation Systems",
    "CTR Prediction"
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "MindSpore"
    },
    {
      "ref_id": "b2",
      "title": "A theory of adaptive pattern classifiers"
    },
    {
      "ref_id": "b3",
      "title": "Simple, scalable adaptation for neural machine translation"
    },
    {
      "ref_id": "b4",
      "title": "Hypernetworks in meta-reinforcement learning"
    },
    {
      "ref_id": "b5",
      "title": "CAN: Feature Co-Action for Click-Through Rate Prediction"
    },
    {
      "ref_id": "b6",
      "title": "Multitask learning"
    },
    {
      "ref_id": "b7",
      "title": "PEPNet: Parameter and Embedding Personalized Network for Infusing with Personalized Prior Information"
    },
    {
      "ref_id": "b8",
      "title": "Wide & deep learning for recommender systems"
    },
    {
      "ref_id": "b9",
      "title": "Deep neural networks for youtube recommendations"
    },
    {
      "ref_id": "b10",
      "title": "Rotational unit of memory: a novel representation unit for RNNs with scalable applications"
    },
    {
      "ref_id": "b11",
      "title": "The YouTube video recommendation system"
    },
    {
      "ref_id": "b12",
      "title": "Meta-learning with variational semantic memory for word sense disambiguation"
    },
    {
      "ref_id": "b13",
      "title": "Adversarial Attacks for Black-Box Recommender Systems Via Copying Transferable Cross-Domain User Profiles"
    },
    {
      "ref_id": "b14",
      "title": "AutoTransfer: Instance Transfer for Cross-Domain Recommendations"
    },
    {
      "ref_id": "b15",
      "title": "DeepFM: a factorization-machine based neural network for CTR prediction"
    },
    {
      "ref_id": "b16",
      "title": "HyperNetworks"
    },
    {
      "ref_id": "b17",
      "title": "An End-to-end Deep Convolutional Neural Network for a Multiscale Image Matching and Localization Problem"
    },
    {
      "ref_id": "b18",
      "title": "Adversarial feature translation for multi-domain recommendation"
    },
    {
      "ref_id": "b19",
      "title": "Parameter-efficient transfer learning for NLP"
    },
    {
      "ref_id": "b20",
      "title": "Conet: Collaborative cross networks for cross-domain recommendation"
    },
    {
      "ref_id": "b21",
      "title": "Adaptive domain interest network for multi-domain recommendation"
    },
    {
      "ref_id": "b22",
      "title": "AdaptDHM: Adaptive Distribution Hierarchical Model for Multi-Domain CTR Prediction"
    },
    {
      "ref_id": "b23",
      "title": "AutoMLP: Automated MLP for Sequential Recommendations"
    },
    {
      "ref_id": "b24",
      "title": "Ddtcdr: Deep dual transfer cross domain recommendation"
    },
    {
      "ref_id": "b25",
      "title": "Multimodality helps unimodality: Cross-modal few-shot learning with multimodal models"
    },
    {
      "ref_id": "b26",
      "title": "Modeling task relationships in multi-task learning with multi-gate mixture-of-experts"
    },
    {
      "ref_id": "b27",
      "title": "Your Tweets Reveal What You Like: Introducing Cross-media Content Information into Multi-domain Recommendation"
    },
    {
      "ref_id": "b28",
      "title": "Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks"
    },
    {
      "ref_id": "b29",
      "title": "Conditional deformable image registration with convolutional neural network"
    },
    {
      "ref_id": "b30",
      "title": "Embedding-based news recommendation for millions of users"
    },
    {
      "ref_id": "b31",
      "title": "Learning multiple visual domains with residual adapters"
    },
    {
      "ref_id": "b32",
      "title": "Hypernetworks for zero-shot transfer in reinforcement learning"
    },
    {
      "ref_id": "b33",
      "title": "Sar-net: a scenario-aware ranking network for personalized fair recommendation in hundreds of travel scenarios"
    },
    {
      "ref_id": "b34",
      "title": "One model to serve all: Star topology adaptive recommender for multi-domain ctr prediction"
    },
    {
      "ref_id": "b35",
      "title": "Deep & cross network for ad click predictions"
    },
    {
      "ref_id": "b36",
      "title": "K-adapter: Infusing knowledge into pre-trained models with adapters"
    },
    {
      "ref_id": "b37",
      "title": "Single-shot Feature Selection for Multi-task Recommendations"
    },
    {
      "ref_id": "b38",
      "title": "Causalint: Causal inspired intervention for multi-scenario recommendation"
    },
    {
      "ref_id": "b39",
      "title": "PLATE: A Prompt-Enhanced Paradigm for Multi-Scenario Recommendations"
    },
    {
      "ref_id": "b40",
      "title": "Apg: Adaptive parameter generation network for click-through rate prediction"
    },
    {
      "ref_id": "b41",
      "title": "AdaSparse: Learning Adaptively Sparse Structures for Multi-Domain Click-Through Rate Prediction"
    },
    {
      "ref_id": "b42",
      "title": "Leaving no one behind: A multi-scenario multi-task meta learning approach for advertiser modeling"
    },
    {
      "ref_id": "b43",
      "title": "Deep Reinforcement Learning for Page-wise Recommendations"
    },
    {
      "ref_id": "b44",
      "title": "Recommendations with Negative Feedback via Pairwise Deep Reinforcement Learning"
    }
  ]
}