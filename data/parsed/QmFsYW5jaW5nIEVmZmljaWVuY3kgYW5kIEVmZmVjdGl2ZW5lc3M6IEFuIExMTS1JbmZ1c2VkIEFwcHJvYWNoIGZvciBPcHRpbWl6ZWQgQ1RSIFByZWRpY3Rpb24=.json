{"Balancing Efficiency and Effectiveness: An LLM-Infused Approach for Optimized CTR Prediction": "Guoxiao Zhang \u2217 Yi Wei \u2217 zhangguoxiao@meituan.com weiyi20@meituan.com Meituan Beijing, China Yadong Zhang Meituan Beijing, China zhangyadong05@meituan.com Huajian Feng Hunan University Changsha, China huajianfeng@hnu.edu.cn", "Abstract": "Click-Through Rate (CTR) prediction is essential in online advertising, where semantic information plays a pivotal role in shaping user decisions and enhancing CTR effectiveness. Capturing and modeling deep semantic information, such as a user's preference for \"H\u00e4agen-Dazs' HEAVEN strawberry light ice cream\" due to its health-conscious and premium attributes, is challenging. Traditional semantic modeling often overlooks these intricate details at the user and item levels. To bridge this gap, we introduce a novel approach that models deep semantic information end-to-end, leveraging the comprehensive world knowledge capabilities of Large Language Models (LLMs). Our proposed LLM-infused CTR prediction framework( M ulti-level Deep S emantic Information Infused CTR model via D istillation, MSD) is designed to uncover deep semantic insights by utilizing LLMs to extract and distill critical information into a smaller, more efficient model, enabling seamless end-to-end training and inference. Importantly, our framework is carefully designed to balance efficiency and effectiveness, ensuring that the model not only achieves high performance but also operates with optimal resource utilization. Online A/B tests conducted on the Meituan sponsored-search system demonstrate that our method significantly outperforms baseline models in terms of Cost Per Mile (CPM) and CTR, validating its effectiveness, scalability, and balanced approach in real-world applications.", "CCS Concepts": "", "\u00b7 Information systems \u2192 Information retrieval .": "\u2217 Both authors contributed equally to this research. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia \u00a9 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-1331-6/2025/04 https://doi.org/10.1145/3701716.3715211 Qiang Liu Meituan Beijing, China liuqiang43@meituan.com", "Keywords": "CTR Prediction, Knowledge Distillation, LLMs, Recommendation Systems", "ACMReference Format:": "Guoxiao Zhang, Yi Wei, Yadong Zhang, Huajian Feng, and Qiang Liu. 2025. Balancing Efficiency and Effectiveness: An LLM-Infused Approach for Optimized CTR Prediction. In Companion Proceedings of the ACM Web Conference 2025 (WWW Companion '25), April 28-May 2, 2025, Sydney, NSW, Australia. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3701716.3715211", "1 Introduction": "Click-Through Rate (CTR) prediction plays an important role in recommender systems and online advertising [1, 4, 6, 7, 11, 13, 18]. Lately, some works are proposed to capture semantic information by involving Pretrained Language Models (PLMs) [3, 9, 10, 12, 13, 15, 17]. As is shown in Figure 1, semantic information can be categorized as explicit information and implicit information, where explicit information is directly obtainable from the texture features and implicit information is inferred by relevant world knowledge. Compared with those works of BERT series [12, 13, 17], generative Large Language Models (LLMs) provide a fresh technological approach for recommender systems to capture deep semantic information[3, 9, 10, 15], due to their capacity for leveraging world knowledge to infer implicit information. Specifically, CTRL[9] integrates semantic information from LLMs into traditional ID-based models using contrastive learning. ClickPrompt [10] first aligns the collaborative and semantic knowledge from ID and textual features via the prompt interface, then tunes the CTR model without PLM for inference efficiency. KAR [15] acquires two types of external knowledge from LLMs-the reasoning knowledge on user preferences and the factual knowledge on items, which are transformed into augmented vectors to be compatible with the recommendation task. BAHE[3] employs the LLM's pre-trained low layers to extract embeddings of user behaviors and stores them in the offline database, then utilizes the deeper, trainable layers of the LLM to generate comprehensive user embeddings. However, it is important to note that, due to limited consideration of efficiency, most of these studies(ClickPrompt[10], KAR [15], CTRL[9]) remain largely Guoxiao Zhang, Yi Wei, Yadong Zhang, Huajian Feng, and Qiang Liu WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia experimental and have not been applied in practical industrial settings for CTR prediction. Besides, BAHE[3] chooses to partially utilize information from LLMs, which restricts the exploration of the effectiveness of LLMs for CTR estimation. Figure 1: Semantic information at both item and user levels: explicit and implicit key features with corresponding reasoning processes extracted by LLMs. Weber Portable Gas Grill Ben & Jerry's Mint Chocolate Cookie Pint \u2026 \u2026 Johnsonville Brats Original Pork Bratwurst Links, 18 ct. infer Implicit Information Quality : Premium Well-known The user tends to choose well-known and reputable brands, indicating a preference for quality and reliability in products. Lifestyle and Activity : Outdoor Activities Social Activities The purchase of a portable gas grill suggests the user enjoys outdoor cooking activities such as camping, or barbecues. Explicit Information Product Types: Meat and Dairy Products Grilling Equipment The user purchased ice cream, grilling equipment, and sausages. Flavor: Mint Chocolate Cookie Pork Bratwurst Size: Pint 18-count Package Chose Mint Chocolate Cookie in pint size and Original Pork Bratwurst in an 18-count packages. Item Level User Level Large Language Models Implicit Information Calorie Content : Low-Calorie. The \"Zero Sugar\" label suggests that the beverage is low in calories. Target Audience : Health-Conscious. The zero sugar variant is typically aimed at consumers who are mindful of their sugar intake. Consumption Scenarios : Gatherings. The 1.25L size is convenient for sharing during gatherings. Explicit Information Brand : Coca-Cola. The product name directly mentions the brand as Coca- Cola. Type : Zero Sugar Beverage. The product name indicates it is a zero sugar variant of a beverage. Volume : 1.25L. The product name specifies the volume as 1.25 liters. Coca-Cola Zero Sugar 1.25L infer Coca-Cola, Zero Sugar Beverage, 1.25L, Low-Calorie, Health-Conscious, Suitable for Gatherings Meat and Dairy Products, Grilling Equipment, Mint Chocolate Cookie, Pint Ice Cream, 18-count Package Pork Bratwurst Premium and Well-known Brands, Outdoor or Social Activities To balance efficiency and effectiveness, inspired by the fact Knowledge distillation is an effective model compression method that can transfer the internal capabilities of LLMs to smaller ones[5], we propose an LLM-infused approach via Knowledge Distillation. Specifically, our approach includes the Multi-level Knowledge Distillation Module(MKDM) and Multi-level Knowledge Integration Module(MKIM). For MKDM, we propose a prompt-based approach combined with Chain-of-Thought (CoT) reasoning to generate reasoning processes and extract both explicit and implicit information at both item and user levels, shown in Figure 1. The MIDM comprises three components. Initially, we incorporate a feature adaptation layer to map the semantic embeddings into the appropriate feature space. Subsequently, we employ LoRA (Low-Rank Adaptation) to fine-tune the parameters of the LLM. Finally, we develop a Frequency-Adaptive k-Near Items Fusion to enhance integration with the CTR model. Our contributions include: \u00b7 Introducing an LLM-infused CTR Prediction Approach through Knowledge Distillation Paradigm to balance efficiency and effectiveness. \u00b7 Developing Multi-level Knowledge Distillation Module and Multi-level Knowledge Integration Module. \u00b7 Successfully deploying this method on the Meituan Recommendation platform, achieving a 2.12% rise in CTR and a 2.59% enhancement in Cost Per Mile (CPM) during online A/B tests.", "2 Methodology": "In this section, we present our framework, depicted in Figure 2, which consists of two main modules: the Multi-level Knowledge Distillation Module and the Multi-level Knowledge Integration Module. We first introduce the process of distilling LLMs at both item and user levels, followed by integrating the distilled model into the CTR prediction framework. Figure 2: The overall framework of our proposed LLMinfused CTR prediction framework(MSD). Target Item t Interacted Item 1 t 1 -> N concatenate Interacted Sequence t Distilled LLM \ud83d\udd25 Feature Adaptor 1 -> N Selector 1 -> K Fusion Text Feature CTR Model ChatGPT Seed Knowledge Prompt t steer drive Knowledge Generation LLM Learning Objective generated knowledge train Knowledge Distillation ID Feature Dense Feature Distilled LLM \ud83d\udd25 \u2744 M ulti-level K nowledge D istillation M odule M ulti-level K nowledge I ntegration M odule \u2a01", "2.1 Multi-level Knowledge Distillation Module": "This module is designed to efficiently distill semantic knowledge at both item and user levels from LLMs into a more efficient one. Knowledge generation and knowledge distillation are the main components of this module. We provide a detailed description of them in this section. 2.1.1 Knowledge Generation. High-quality and diverse data are essential to ensure that the distilled model generalizes well across different contexts and retains critical semantic information. To guide the LLMs produce high-quality data, we first manually selected outputs from the ChatGPT as reference, which include key phrases and the rationale. Then, the prompt will be enhanced by dynamically chosen reference output as an example for in-context learning[2]. Our prompt template for item level is illustrated in Figure 3. To ensure the dataset is representative and diverse, stratified and importance sampling are adopted, considering categorical information and posterior statistics. For user level data, the generation procedure follows a similar protocol. Figure 3: Item Level Prompt templates guide the LLM in generating outputs containing explicit (highlighted in green) and implicit (highlighted in blue) information, utilizing incontext learning. Please summarize the key features of a given product description using keywords, separated by commas. The summary should include the following two types of information : 1. ** Explicit Information **: Directly extractable information from the product description. Example: For 'Master Kong Spicy Beef Instant Noodles', the textual information includes the brand 'Master Kong', the flavor 'Spicy Beef'. 2. ** Implicit Information **: Information inferred from the product description. Example: For \"[Bai Guo Yuan] Fresh Litchi 250g\", the brand \"Bai Guo Yuan\" suggests it is a well-known fruit chain store, indicating that the product is of high quality. Task Description Please output the complete thought process based on the above requirements and example. Product Name: Tang Sunshine Sweet Orange Flavored Solid Drink 400g Thought Process: Output Please carefully read the following example. The output should follow the thought process and format demonstrated below. Product Name: COSTA Classic Blend Coffee Beans 200g Thought Process : Explicit Information Brand: COSTA. The product name directly mentions the brand as COSTA. Type: Coffee Beans. The product name clearly states it is coffee beans. Blend: Classic Blend. The product name mentions it is a classic blend, indicating a specific proportion of mixed coffee beans. Weight: 200g. The product name specifies the weight as 200g. Implicit Information Quality: Professional Grade. COSTA is a well-known coffee brand, usually implying high quality and professionalism in its products. Flavor: Rich Aroma, Full-bodied. Classic blend coffee beans typically have a rich aroma and full-bodied flavor, which can be inferred from the brand and product type. Suitable Scenarios: Home or Office Use.  The 200g packaging is suitable for daily use at home or in the office. Key Features : COSTA, Classic Blend, Coffee Beans, 200g, Professional Grade, Rich Aroma, Well-known Brand, Home and Office Use Dynamic One-Shot Example a dynamic example with thought process 2.1.2 Knowledge Distillation. Following [14], we leverage knowledge distillation to transfer semantic and contextual reasoning Balancing Efficiency and Effectiveness: An LLM-Infused Approach for Optimized CTR Prediction WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia capabilities from the LLMs teacher model \ud835\udc47 to a more computationally efficient student model \ud835\udc46 . The student model is trained to replicate the output \ud835\udc66 \ud835\udc47 of the teacher model by minimizing the distillation loss as follows:   where y \ud835\udc47,\ud835\udc61 represents the \ud835\udc61 -th token of the teacher model's output sequence, y \ud835\udc46, < \ud835\udc61 are the preceding tokens in the student model's output sequence and x are the input tokens.", "2.2 Multi-level Knowledge Integration Module": "The MKIM is designed to seamlessly integrate semantic insights at both the item and user levels within the CTR prediction framework. It comprises three key components: 1) LoRA, which enhances alignmentandperformance by fine-tuning distilled LLMs with Low-Rank Adaptors for computational efficiency; 2) Feature Adaptors, which compress and transform LLM output embeddings using a MultiLayer Perceptron (MLP) to ensure effective integration into the CTR model; 3) Frequency-Adaptive Relevant Items Fusion, which improves item embedding robustness through masking and pooling operations. The transformed item and user embeddings are then concatenated to form the input for CTR tasks. 2.2.1 LoRA. Following [8], we finetune the distilled LLMs for CTR tasks using Low-Rank adaptors. Specifically, for a pre-trained weight matrix W 0 \u2208 R \ud835\udc51 \u00d7 \ud835\udc58 , we constrain its update by representing the latter with a low-rank decomposition W0 + \u0394 W = BA , where B \u2208 R \ud835\udc51 \u00d7 \ud835\udc5f , A \u2208 R \ud835\udc5f \u00d7 \ud835\udc58 , and the rank \ud835\udc5f \u226a min ( \ud835\udc51, \ud835\udc58 ) . During training, W 0 is frozen and does not receive gradients, while the A and B contain trainable parameters. 2.2.2 Feature Adaptors. For the raw output embeddings of the LLMs, we define e u as the user-level embedding, \ud835\udc3f as the length of user behavior sequence, e t as the target item embedding and e i as the item embeddings in user behavior sequence, where \ud835\udc56 ranges from 1 to \ud835\udc3f . We apply a Multi-Layer Perceptron (MLP) as the feature adaptor to generate the projected embeddings e \u2032 u , e \u2032 t and e \u2032 i as follows:    2.2.3 Frequency-Adaptive Relevant Items Fusion. To enhance the robustness of item embeddings during training, the item-level features e item are obtained as follows:    where e \u2032 t represents the target item, e \u2032 masked_i is gained from e \u2032 i with frequency-guided stochastic masking. The top-k similar item embeddings are then processed through a fusion layer to produce the final item embedding. We utilize the element-wise addition as our fusion operation.", "3 EXPERIMENTS": "", "3.1 Experimental Setup": "3.1.1 Dataset. We conducted experiments using both a public dataset and one real-world business dataset. The key statistics of these datasets are presented in Table 1. \u00b7 KDD Cup 2012 : A CTR prediction dataset comprising advertising logs from Tencent, which include queries and user information. We randomly selected 10% of the users from the complete dataset for computation efficiency. \u00b7 Meituan : A real-world dataset, sourced from Meituan's recommendation platform, contains extensive user-item interaction data that reflects actual business scenarios. Table 1: Statistics of datasets used in our experiments. 3.1.2 Baselines. To demonstrate the effectiveness of our proposed framework, we compare it with the popular CTR baseline models (parameter settings following [7]): \u00b7 DIN [18]: a leading sequential CTR model that uses an attention mechanism to identify target-related interests. \u00b7 DeepFM [6]: a popular CTR model that merges factorization machines with deep learning techniques for feature interaction. \u00b7 DeepCharMatch [4]: a method that directly models semantic information at the character level using query-ad pairs. \u00b7 SuKD [13]: a model that incorporates additional NLP knowledge in the fine-tuning of PLMs. \u00b7 BERT4CTR[12]: an effective framework for integrating PLMs with non-textual features. \u00b7 PRINT [7]: a BERT-based model that improves CTR prediction by accounting for personalized incentives in query-ad semantic relevance. 3.1.3 Evaluation Metrics. To thoroughly assess our approach, we adopted distinct metrics for evaluating the effectiveness of the distillation process and the model's overall performance while examining the relationship between these two evaluations. Distillation Evaluation Metric. To evaluate the effectiveness of our distillation process, we calculate the F1 score based on predicted key phrases. Cosine similarity is applied to determine phrase equivalence, leveraging embeddings generated by a BERT model. We manually annotated a subset of the LLM's outputs to establish a ground truth. WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia Guoxiao Zhang, Yi Wei, Yadong Zhang, Huajian Feng, and Qiang Liu Model Performance Metric. The primary metric for evaluating the model's performance is the Area Under the ROC Curve (AUC), a widely recognized measure of a model's capability to distinguish between positive and negative instances. In addition, we utilize the Relative Improvement (RelaImpr) metric [16] to quantify the comparative enhancement our model achieves over existing models.", "3.2 Overall Performance": "In this section, we evaluate the performance of our novel LLMinfused CTR prediction framework against PRINT[7] and other baseline models across the two datasets. As detailed in Table 2, our approach demonstrates superior performance. Our framework leverages the deep semantic capabilities of Large Language Models as well as end-to-end training and inference processes. Compared to PRINT, our approach achieves a notable improvement in AUC, with increases of 0.25% and 0.63% over the KDD Cup 2012 and Meituan datasets, respectively. Table 2: Performance comparison of our proposed MSD.", "3.3 Ablation Study for Our Framework": "We performed a comprehensive ablation study to evaluate the contribution of each module within our framework. As depicted in Table 3, each module significantly contributes to the overall performance improvement of the complete model. Table 3: Ablation study of the MSD over the Meituan datasets.", "3.4 Effectiveness of LLM Distillation": "Our experiments demonstrate a positive correlation between the knowledge distillation process and the performance of the CTR model, highlighting the critical role of an effective distillation strategy in enhancing model performance. Specifically, when the F1 score increases from 0 . 05 to 0 . 53, the AUC score of the CTR model shows a significant improvement. However, beyond an F1 score Figure 4: Relationship between distillation metric F1 score and CTR model performance metric AUC on the Meituan dataset. The initial low F1 score improves to 0.73 with additional fine-tuning data and data augmentation strategies, such as key phrase shuffling and dictionary substitutions. Meituan 0.7085 0.7080 0.7075 0.7070 0.7065 0.7060 0.7055 0.05 0.53 0.61 0.68 0.73 of 0 . 60, the rate of increase in the AUC score begins to diminish as shown in Figure 4. This observation suggests that the current method can enhance CTR model performance only up to a certain level. To further improve the CTR model, future research could explore more sophisticated designs for the distillation task to overcome this performance plateau.", "3.5 Online Deployment and A/B Test Results": "To reduce the inference time of the CTR model, we implement a combination of pre-computation and a hierarchical caching system, as inspired by PRINT[7]. For target item embeddings, we store pre-computed embeddings of top items ranked by exposure counts in Redis. If an item is not found in the cache, its embedding is computed in real time after the recall phase and before being fed into the CTR model. These newly computed embeddings are then cached in Redis, utilizing an eviction policy to manage storage efficiently. Table 4: Results of Online A/B Testing. Between October 20, 2024, and October 30, 2024, we conducted an online A/B test on Meituan's sponsored search advertising system to validate our proposed framework. As illustrated in Table 4, our model demonstrates a significant improvement over the naive implementation of the LLM, achieving a 2.12% increase in ClickThrough Rate (CTR) and a 2.59% increase in Cost Per Mille (CPM).", "4 Conclusion": "In this paper, we present an innovative LLM-infused framework for CTR prediction designed to effectively utilize semantic information in CTR tasks. By introducing the MKDM and MKIM, our framework captures deep semantic insights at both item and user levels, Balancing Efficiency and Effectiveness: An LLM-Infused Approach for Optimized CTR Prediction addressing the limitations of traditional ID-based recommendation systems and existing methods that attempt to integrate LLMs or BERT into CTR tasks. Our model significantly enhances the AUC on the Meituan dataset and achieves a notable increase in system revenue by 2.59%, and CTR by 2.12% in the Meituan sponsoredsearch system. This approach opens new avenues for incorporating LLMs into CTR prediction, and we hope our work inspires further research and exploration in this field.", "References": "[1] Weijie Bian, Kailun Wu, Lejian Ren, Qi Pi, Yujing Zhang, Can Xiao, Xiang-Rong Sheng, Yong-Nan Zhu, Zhangming Chan, Na Mou, et al. 2020. Can: Feature coaction for click-through rate prediction. arXiv preprint arXiv:2011.05625 (2020). [2] Jacopo D'Abramo, Andrea Zugarini, and Paolo Torroni. 2024. Dynamic FewShot Learning for Knowledge Graph Question Answering. arXiv preprint arXiv:2407.01409 (2024). [3] Binzong Geng, Zhaoxin Huan, Xiaolu Zhang, Yong He, Liang Zhang, Fajie Yuan, Jun Zhou, and Linjian Mo. 2024. Breaking the length barrier: Llm-enhanced CTR prediction in long textual user behaviors. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval . 2311-2315. [4] Jelena Gligorijevic, Djordje Gligorijevic, Ivan Stojkovic, Xiao Bai, Amit Goyal, and Zoran Obradovic. 2019. Deeply supervised model for click-through rate prediction in sponsored search. Data Mining and Knowledge Discovery 33 (2019), 1446-1467. [5] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. 2024. MiniLLM: Knowledge distillation of large language models. In The Twelfth International Conference on Learning Representations . [6] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. arXiv preprint arXiv:1703.04247 (2017). [7] Zhaolin Hong, Haitao Wang, Chengjie Qian, Wei Chen, Tianqi He, Yajie Zou, Qiang Liu, and Xingxing Wang. 2024. PRINT: Personalized Relevance Incentive Network for CTR Prediction in Sponsored Search. In Companion Proceedings of the ACM on Web Conference 2024 . 190-195. [8] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia language models. arXiv preprint arXiv:2106.09685 (2021). [9] Xiangyang Li, Bo Chen, Lu Hou, and Ruiming Tang. 2023. Ctrl: Connect tabular and language model for ctr prediction. CoRR (2023). [10] Jianghao Lin, Bo Chen, Hangyu Wang, Yunjia Xi, Yanru Qu, Xinyi Dai, Kangning Zhang, Ruiming Tang, Yong Yu, and Weinan Zhang. 2024. ClickPrompt: CTR Models are Strong Prompt Generators for Adapting Language Models to CTR Prediction. In Proceedings of the ACM on Web Conference 2024 . 3319-3330. [11] Qi Pi, Guorui Zhou, Yujing Zhang, Zhe Wang, Lejian Ren, Ying Fan, Xiaoqiang Zhu, and Kun Gai. 2020. Search-based user interest modeling with lifelong sequential behavior data for click-through rate prediction. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management . 2685-2692. [12] Dong Wang, Kav\u00e9 Salamatian, Yunqing Xia, Weiwei Deng, and Qi Zhang. 2023. BERT4CTR: An Efficient Framework to Combine Pre-trained Language Model with Non-textual Features for CTR Prediction. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 5039-5050. [13] Dong Wang, Shaoguang Yan, Yunqing Xia, Kav\u00e9 Salamatian, Weiwei Deng, and Qi Zhang. 2022. Learning Supplementary NLP Features for CTR Prediction in Sponsored Search. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 4010-4020. [14] Yuling Wang, Changxin Tian, Binbin Hu, Yanhua Yu, Ziqi Liu, Zhiqiang Zhang, Jun Zhou, Liang Pang, and Xiao Wang. 2024. Can Small Language Models be Good Reasoners for Sequential Recommendation?. In Proceedings of the ACM on Web Conference 2024 . 3876-3887. [15] Yunjia Xi, Weiwen Liu, Jianghao Lin, Xiaoling Cai, Hong Zhu, Jieming Zhu, Bo Chen, Ruiming Tang, Weinan Zhang, and Yong Yu. 2024. Towards open-world recommendation with knowledge augmentation from large language models. In Proceedings of the 18th ACM Conference on Recommender Systems . 12-22. [16] Ling Yan, Wu-Jun Li, Gui-Rong Xue, and Dingyi Han. 2014. Coupled group lasso for web-scale ctr prediction in display advertising. In International conference on machine learning . PMLR, 802-810. [17] HaoYang, Ziliang Wang, Weijie Bian, and Yifan Zeng. 2023. Practice on Effectively Extracting NLP Features for Click-Through Rate Prediction. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management . 4887-4893. [18] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through rate prediction. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1059-1068."}
