{
  "CCMB: A Large-scale Chinese Cross-modal Benchmark": "Xiangyang Ji Tsinghua University Beijing, China xyji@tsinghua.edu.cn",
  "ABSTRACT": "Vision-language pre-training (VLP) on large-scale datasets has shown premier performance on various downstream tasks. In contrast to plenty of available benchmarks with English corpus, largescale pre-training datasets and downstream datasets with Chinese corpus remain largely unexplored. In this work, we build a largescale high-quality Chinese Cross-Modal Benchmark named CCMB for the research community, which contains the currently largest public pre-training dataset Zero and five human-annotated finetuning datasets for downstream tasks. Zero contains 250 million images paired with 750 million text descriptions, plus two of the five fine-tuning datasets are also currently the largest ones for Chinese cross-modal downstream tasks. Along with the CCMB, âˆ— Both are second authors â€  Corresponding author Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. MM'23, October 29-November 3, 2023, Ottawa, ON, Canada Â© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0108-5/23/10...$15.00 https://doi.org/10.1145/3581783.3611877",
  "Yafeng Deng â€ ": "360 AI Research Tsinghua University Beijing, China dengyafeng@gmail.com we also develop a VLP framework named R2D2, applying a preR anking + R anking strategy to learn powerful vision-language representations and a two-way distillation method (i.e., target-guided D istillation and feature-guided D istillation) to further enhance the learning capability. With the Zero and the R2D2 VLP framework, we achieve state-of-the-art performance on twelve downstream datasets from five broad categories of tasks including image-text retrieval, image-text matching, image caption, text-to-image generation, and zero-shot image classification. The datasets, models, and codes are available at https://github.com/yuxie11/R2D2",
  "CCS CONCEPTS": "Â· Computing methodologies â†’ Artificial intelligence .",
  "KEYWORDS": "large-scale datasets, vision-language pre-training",
  "ACMReference Format:": "Chunyu Xie, Heng Cai, Jincheng Li, Fanjing Kong, Xiaoyu Wu, Jianfei Song, Henrique Morimitsu, Lin Yao, Dexin Wang, Xiangzheng Zhang, Dawei Leng, Baochang Zhang, Xiangyang Ji, and Yafeng Deng. 2023. CCMB: A Large-scale Chinese Cross-modal Benchmark. In Proceedings of the 31st ACMInternational Conference on Multimedia (MM '23), October 29-November 3, 2023, Ottawa, ON, Canada. ACM, New York, NY, USA, 13 pages. https: //doi.org/10.1145/3581783.3611877 MM'23, October 29-November 3, 2023, Ottawa, ON, Canada Chunyu Xie et al.",
  "1 INTRODUCTION": "Vision-language pre-training (VLP) mainly learns the semantic correspondence between vision and natural language. Previous works [4, 21, 34, 38] explore the VLP model and achieve significant improvement on various vision-language (V+L) tasks. These methods are supported by massive data [32], excellent architectures such as Transformer [37], and cross-modal models such as CLIP [30]. There are plenty of available benchmarks with English corpus, such as Conceptual Captions [33], SBU Captions [28], and LAION [32]. Differently, large-scale pre-training datasets and downstream datasets with Chinese corpus are relatively few. M6-Corpus [24] is a multi-modal pre-training dataset in Chinese but not publicly available. BriVL (also called WenLan) [11] constructs a visionlanguage dataset called WSCD, but only releases 5M image-text pairs. Wukong [12] is a newly published pre-training dataset with 100M image-text pairs. Most existing downstream Chinese datasets mainly focus on retrieval tasks, such as Flickr30k-CN [16] and COCO-CN [23], which are not sufficient for a complete evaluation of VLP models. Besides, Flickr30k-CN tries to translate English cross-modal downstream datasets into Chinese, which, however, fails to cover Chinese idioms and often causes translation errors. In this paper, we introduce a large-scale Chinese cross-modal benchmark called CCMB, including a pre-training dataset (Zero) and five downstream datasets. Specifically, Zero consists of 250 million images and 750 million descriptive texts, which is the largest public Chinese V+L pre-training dataset. Zero is collected from the search engine with images and corresponding textual descriptions, by filtering from 5 billion image-text data by user click-through rate (CTR). Compared to existing pre-training datasets, Zero is high-quality due to the user CTR filtering method and the diverse textual information for each image. Table 1 shows an overview of V+L pre-training datasets. Together with the pre-training dataset, we provide 5 high-quality human-annotated downstream datasets. To the best of our knowledge, two of them are the first proposed datasets for the Chinese image-text matching task, which is also important for evaluating VLP models. They are also the largest Chinese V+L downstream datasets. For the image-text retrieval task, we provide 3 datasets, especially our Flickr30k-CNA, which is a more comprehensive and accurate human-annotated dataset than Flickr30k-CN [16]. The statistics of the public and our proposed downstream datasets are shown in Table 2. From the perspective of cross-modal learning, existing methods are mainly categorized as single-stream and dual-stream. Most single-stream methods ( e.g. , [3, 22, 29]) employ an extra object detector to extract the patch embedding and then align patches and words. As illustrated in [19], object detectors are annotationexpensive and computing-expensive, because they require bounding box annotations during pre-training and high-resolution (such as 600 Ã— 1000) images during inference. On the other hand, for dualstream architectures ( e.g. , [11, 30, 39]), it is non-trivial to model the fine-grained associations between image and text, since the corresponding representations reside in their own semantic space. Some works [18, 19, 34] omit the object detection module and combine dual-stream architecture with single-stream architecture, showing powerful performance on multi-modal downstream tasks. Inspired by this line of work, we introduce a VLP framework called R2D2, a combination architecture of dual-stream and singlestream. We apply global contrastive pre-ranking to obtain imagetext representations and fine-grained ranking to further improve model performance. Besides, we introduce a two-way distillation method into the model, consisting of target-guided distillation and feature-guided distillation. The target-guided distillation increases the robustness when learning from noisy labels, while featureguided distillation aims to improve the generalization performance. Weapplymaskedlanguagemodeling with enhanced training, which improves the capability of the model while reducing the training cost. To summarize, our main contributions are as follows: Â· We construct the largest public Chinese vision-language pretraining dataset, containing 250 million images and 750 million corresponding texts. It is high-quality due to the filtering method by user CTR and the diverse textual information for each image. We provide five human-annotated cross-modal downstream datasets, two of which are currently the largest Chinese visionlanguage downstream datasets. Â· We introduce a vision-language pre-training framework named R2D2 for cross-modal learning. Our proposed method achieves state-of-the-art performance on twelve downstream datasets from five broad categories of vision-language tasks, showing the superior ability of our pre-trained model.",
  "2 RELATED WORK": "",
  "2.1 Vision-Language Datasets": "Chinese vision-language benchmark requires images and highquality Chinese texts, which are hard to obtain and still rare for the research community's reach. To this end, existing public datasets [16, 23] use machine translation to adapt their English versions [2, 43] to Chinese, but the data quality is sacrificed due to machine translation errors. Newly reported datasets with Chinese texts [11, 12, 24] are proposed for Chinese VLP. However, they are either not publicly available or lack sufficient downstream tasks. In this paper, we propose a Chinese vision-language benchmark that covers a large-scale pre-training dataset and five downstream datasets.",
  "2.2 Vision-Language Pre-training Learning": "The vision-language pre-training architectures can be categorized as: single-stream and dual-stream. Most existing single-stream models [3, 17, 21, 27, 29] concatenate image and text as a single input to model the interactions between image and text within a transformer model [37]. On the other hand, popular dual-stream models [10, 14, 20, 26, 30, 39, 41] aim to align image and text into a unified semantic space via contrastive learning. Besides, some works [18, 19, 34] align the individual features of images and texts in a dual-stream architecture, and then fuse the features in a unified semantic space via a single-stream architecture. These works show that a combined architecture of dual-stream and single-stream achieves better performance than only one. R2D2 explores the effective signals via an image-text cross encoder and a text-image cross encoder while also maintaining the bottom dual-stream architecture. Moreover, we improve masked language modeling with enhanced training and propose a two-way distillation to stabilize the model representations for vision-language pre-training. CCMB: A Large-scale Chinese Cross-modal Benchmark MM'23, October 29-November 3, 2023, Ottawa, ON, Canada Table 1: Statistics of the vision-language pre-training datasets. The details of Zero can refer to Section 3.1.",
  "3 CCMB": "",
  "3.1 Pre-training Dataset": "Existing public pre-training datasets suffer from two limitations. First, the image-text pairs are collected usually by their co-occurrence relationship coarsely from third-party search engines or websites. Thus, the collected pairs are inherently noisy. Second, the text corpus lacks diversity as each image usually has one corresponding text description. To overcome these drawbacks, we collect a new dataset for Chinese image-text pre-training, called Zero. To this end, we first collect 5 billion image-text data from an image search engine. We try to mitigate the noise of these imagetext pairs via user click-through rate (CTR) and obtain about 250 million images and 750 million corresponding texts, namely Zero. In other words, each image in Zero is with about 3 textual descriptions, i.e. , 'Title', 'Content', and 'ImageQuery'. 'Title' and 'Content' come from the source webpage containing the image. 'Title' is the title of the webpage, and 'Content' represents the surrounding text of the image in the webpage. 'ImageQuery' is the user search query for the corresponding image. The average length of 'Title', 'Content', and 'ImageQuery' is 18, 29, and 5, respectively. We show an example in Figure 1 and more examples in Appendix. How to remove the irrelevant content? We apply a series of filtering strategies to construct the Zero. For images, we filter out images with dimensions smaller than 100 pixels or aspect ratio out of the range [1/4, 4]. We then filter images that contain sensitive information, such as sexual, violent scenes, etc. For texts, we remove texts shorter than 2 words or longer than 128 words. Moreover, we remove image-text pairs that contain sensitive words and personal names in the text. Why use CTR instead of random selection? After collecting 5 billion image-text data, we can randomly select a part of them to conduct pre-training experiments under the consideration of computational resources and time cost. However, random selection brings noises in pre-training, which may degrade model performance. To address this, we use an inherent metric CTR in search engine data. The CTR indicates the number of times that users click on an image for a given text. We observe that image-text pairs with low CTR are irrelevant in most cases. That is, an image-text pair Table 2: Statistics of the vision-language downstream datasets. Our downstream datasets can refer to Section 3.2. with high CTR is strongly correlated due to user interaction. We then rank all 5 billion image-text data by CTR and filter the top 250 million images and the corresponding texts as Zero. Why use three types of text instead of one? Each image has three kinds of textual descriptions in raw data. During pre-training, we construct an image-text pair per iteration by randomly selecting one of them. Besides, we also conduct a variety of combinations of different types of textual information, such as without 'Title'. We find the best mode is to use all three types of text instead of other combinations, which brings more data diversity and potentially improves the model performance. Why is Zero large-scale and high-quality? As shown in Table 1, the proposed Zero is a large-scale Chinese pre-training dataset with 250 million images and 750 million corresponding texts. To the best of our knowledge, Zero is the largest Chinese pre-training image-text dataset. Relying on the CTR filtering method and the diverse textual descriptions, Zero is high-quality because a small amount of pre-training data (about 10% of Zero, 23M) surpasses the previous state-of-the-art [12], which uses 100M image-text pairs. More details can be found in Section 5.3.",
  "3.2 Downstream Dataset": "We perform human annotation in downstream datasets, i.e. , ICM, IQM, ICR, IQR, and Flickr30k-CNA. For the first four datasets, the selection strategy of image-text pairs is the same as the collection of the pre-training dataset Zero. We divide the training set, validation set, and test set with a ratio of 8:1:1. We check that the downstream data do not appear in the pre-training dataset via the hash value of images. Then, 15 human annotators carefully label the imagetext pairs. Specifically, the human annotators verify whether an image-text pair is relevant. That is, the human annotators mark them as positive or negative pairs until a pre-defined data size is reached. Note that we do not rewrite the caption or query for each image in these downstream datasets. For Flickr30k-CNA, we gather 6 professional English and Chinese linguists to meticulously translate all data of Flickr30k [43] and double-check each sentence. The details of each dataset are as follows. Image-Caption Matching Dataset (ICM). ICM is collected for the image-text matching task. The image-text matching task is MM'23, October 29-November 3, 2023, Ottawa, ON, Canada Chunyu Xie et al. Title: å¤§æ²¼å›½ç«‹å…¬å›­ï¼Œè¿™é‡Œæ°´æ¸…ç™½äº‘è“å¤©ï¼Œå¤§æ²¼ã€å°æ²¼ã€è¼èœæ²¼ä¸‰ä¸ªé«˜å±±æ¹–çš†å±äºå¤§æ²¼å›½å®šå…¬å›­ (Onuma National Park is with clear water, white cloud and blue sky. All three alpine lakes (i.e., Onuma, Konuma, and Uzbekistan) belong to Onuma National Park.) Content: å¤§æ²¼å›½å®šå…¬å›­åŒ…å«å¤§æ²¼ã€å°æ²¼å’Œè¼èœæ²¼ã€‚é£æ™¯æœ€å¥½çš„å°±åœ¨å¤§æ²¼çš„æ¹–è¾¹é™„è¿‘ã€‚å¤§æ²¼æ˜¯ç”±é©¹å²³ç«å±±å–· å‘åç”Ÿæˆçš„é¢ç§¯ 24 å¹³æ–¹å…¬é‡Œçš„æ¹–æ³Šï¼Œæœ‰å¤§å° 126 ä¸ªå²›å±¿ã€ 32 æ¹–æ¹¾æ‰€ç»„æˆï¼Œè¿™äº›å²›å±¿ç”± 18 åº§æ¡¥æ¢è¿æ¥çš„æ™¯è±¡å åˆ†ç§€ç¾ï¼Œå¯Œæœ‰æ¬§æ´²é£å‘³çš„é£æ™¯ã€‚ (Onuma National Park consists of Onuma, Konuma, and Uzbekistan. The best scenery is near the lake of Onuma. Onuma is a lake with an area of 24 square kilometers formed after the eruption of Komagatake volcano. It consists of 126 islands and 32 bays. The view of these islands connected by 18 bridges is very beautiful, full of Europeanstyle scenery.) ImageQuery: æ°´æ¸…ç™½äº‘è“å¤© (clear water, white cloud and blue sky) Figure 1: An example of Zero. More samples can be found in Appendix. a binary classification task, aiming to predict whether an imagetext pair is matched. Each image has a corresponding caption text. We first select image-text pairs beyond the 5 billion data. Then, human annotators manually perform a 2nd round manual correction, obtaining 400,000 image-text pairs, including 200,000 positive cases and 200,000 negative cases. We keep the ratio of positive and negative pairs consistent in each of the train/val/test sets. Image-Query Matching Dataset (IQM). This is a dataset also for the image-text matching task. Different from ICM, we use the search query instead of detailed description text. Similarly, IQM contains 200,000 positive cases and 200,000 negative cases. ICM and IQM are the largest Chinese vision-language downstream datasets. Image-Caption Retrieval Dataset (ICR). We collect 200,000 image-text pairs under the rules described in ICM. It contains imageto-text and text-to-image retrieval tasks. Image-Query Retrieval Dataset (IQR). IQR is also proposed for the image-text retrieval task. We collect 200,000 queries and the corresponding images as the annotated image-query pairs similar to IQM. We show examples of the above four datasets in Figure B in Appendix. Flickr30k-CNA Dataset. Former Flickr30k-CN [16] translates the training and validation sets of Flickr30k [43] using machine translation, and manually translates the test set. We check the machine-translated results and find two kinds of problems. (1) Some sentences have language problems and translation errors. (2) Some sentences have poor semantics. In addition, the different translation ways prevent the model from achieving accurate performance. We gather 6 professional English and Chinese linguists to meticulously re-translate all data of Flickr30k and double-check each sentence. We name this dataset as Flickr30k-Chinese All (Flickr30k-CNA). We show some cases of the difference between Flickr30k-CN and Flickr30k-CNA in Appendix.",
  "4 METHODOLOGY": "",
  "4.1 Model Architecture": "From Figure 2, the architecture contains a text encoder, an image encoder, and two cross encoders. The text encoder is a BERT [8] using the tokenizer of RoBERTa-wwm-ext [5]. For the image encoder, we adopt the Vision Transformer (ViT) [9]. The two cross encoders are multi-layer transformers. The text encoder and image encoder transform texts and images into sequences of hidden states separately. Then the text and image hidden states interact in the two cross encoders through cross-attention.",
  "4.2 Pre-training Objectives": "We jointly optimize R2D2 with the following four objectives. To fully explore the matching relationship between image and text pairs, we design a mechanism of pre-ranking + ranking, named global contrastive pre-ranking (GCPR) and fine-grained ranking (FGR). To further enhance the capability of the model, we propose a two-way distillation (TwD) strategy consisting of target-guided distillation and feature-guided distillation. We adopt masked language modeling (MLM) with enhanced training (ET) to effciently learn the representation of cross-modal models. We conduct the ablation study to verify the effectiveness of each pre-training strategy in Section 5.3. Global Contrastive Pre-Ranking. Our global contrastive preranking method is similar to that of CLIP [30], aiming to align the representation of multi-modal data ( e.g. , paired image and text). The open-source CLIP implementation [30] only performs backpropagation of the contrastive loss from the local GPU, where negative samples are not fully utilized. Instead, We back-propagate the gradients across all ğ‘˜ GPUs. Inspired by MoCo [13], we also introduce a queue mechanism. In practice, two queues with a fixed size ğ‘€ aim to maintain the recent image and text representations from the momentum-updated encoders, respectively. For each image ğ¼ ğ‘– and the corresponding text ğ‘‡ ğ‘– , the softmax-normalized similarity score of image-to-text and text-to-image can be defined as:  where ğ‘› is the batch size of one GPU, ğ‘˜ is the number of GPUs, ğœ is a learnable temperature parameter, and sim (Â· , Â·) denotes the cosine similarity between a pair of image-text. Considering the effectiveness of features in the queue decreases with increasing time steps, we also maintain a weighted queue ğ‘¤ to mark the reliability of the corresponding position features. Specifically, we decay each element in the queue by a factor of 0.99 per iteration, except for the new incoming item. Let D denote the training data and y (Â· , Â·) CCMB: A Large-scale Chinese Cross-modal Benchmark MM'23, October 29-November 3, 2023, Ottawa, ON, Canada Figure 2: The overall architecture of the proposed framework. The image encoder and the text encoder aim to learn individual features of image and text, respectively. Then, the image features (green circled arrow) are fed into the text-image cross encoder. Similarly, the text features (red circled arrow) are fed into the image-text cross encoder. During pre-training, we apply global contrastive pre-ranking (GCPR), fine-grained ranking (FGR), two-way distillation (TwD), and mask language modeling (MLM) with enhanced training (ET) as pre-training objectives. Image Encoder 1 2 3 4 5 6 7 8 9 CLS 0 Linear Projector Cross Attention Text Encoder Cross Attention 1 2 3 4 5 6 7 8 9 CLS 0 Tokenizer Global Contrastive Pre-Ranking (GCPR) and Two-way Distillation GCPR Fine-Grained Ranking (FGR) FGR MLM Image Token Text Token Positional Embedding Image CLS Text CLS Self  Attention Add & Norm Cross  Attention Add & Norm Feed  Forward Add & Norm Nx Image-Text Cross Encoder Self  Attention Add & Norm Cross  Attention Add & Norm Feed  Forward Add & Norm xN Text-Image Cross Encoder Mask Token (A panda from Sichuan is sunbathing) ä¸€åªæ¥è‡ªå››å·çš„ç†ŠçŒ«æ­£åœ¨é˜³å…‰ä¸‹æ²æµ´ denote the ground-truth one-hot label. The global contrastive preranking (GCPR) loss is calculated by the weighted cross-entropy loss L ğ‘¤ (Â·) , as shown in Equation (2).  distillation to learn from pseudo-targets generated by the momentum model following ALBEF [19]. In practice, we replace the target in Equation (2) with the pseudo-targets. More details about the training process of TgD can be found in Appendix. Besides, target-guided distillation and feature-guided distillation both adopt a teacher-student paradigm. For convenience, we call the combination of TgD and FgD as two-way distillation (TwD).  Fine-Grained Ranking. As aforementioned, we apply global contrastive pre-ranking to obtain the individual representations of images and texts, respectively. Relying on these representations, we next perform Fine-Grained Ranking (FGR) loss. To be specific, this is a binary classification task, aiming to predict whether an image-text pair is matched. Formally, we denote â„ ğ¼ [ ğ¶ğ¿ğ‘† ] and â„ ğ‘‡ [ ğ¶ğ¿ğ‘† ] as the output representations of two cross encoders. Given an image representation â„ ğ¼ [ ğ¶ğ¿ğ‘† ] and a text representation â„ ğ‘‡ [ ğ¶ğ¿ğ‘† ] , we feed the representations into a fully-connected layer ğ‘” (Â·) to get the predicted probabilities respectively. Let y denote the ground-truth label of binary classification, we then compute the FGR loss by the cross-entropy loss L ğ‘ (Â·) as:  The selection strategy of negative pairs is in Appendix. Two-way Distillation. Relying on the momentum-updated encoders in contrastive learning, we introduce target-guided distillation (TgD) to decrease the risk of learning from noisy labels, and feature-guided distillation (FgD) to improve the generalization performance of the pre-trained model. We conduct target-guided Below are the details of FgD. Taking the text encoder as the example below, the teacher character is the momentum-updated text encoder and the student is the text encoder. Here, the weights of the teacher are updated by all past text encoders via exponentialmoving-average. To further improve the capability of the model, we apply a masking strategy to the inputs. In practice, we feed complete inputs into the teacher and masked inputs into the student. Relying on the momentum mechanism, we aim to make the features of the student closer to that of the teacher. Formally, the predicted distributions ( i.e. , P ğ‘¡ ( ğ‘‡ ) , P ğ‘  ( ğ‘‡ ) ) of the teacher and the student are defined as follows, respectively.  where ğ‘“ ğ‘¡ (Â·) and ğ‘“ ğ‘  (Â·) denote the networks of the teacher and the student, respectively. Moreover, ğœ‡ is a momentum-updated mean of ğ‘“ ğ‘¡ (Â·) , and ğ‘‘ is the dimension of the features. ğœ ğ‘¡ and ğœ ğ‘  are the temperature parameters of the teacher and the student, respectively, which can sharpen the distribution of the features. Note that we do not use ğœ‡ for P ğ‘  to avoid collapse in feature-guided distillation. We can obtain similar formulations for P ğ‘  ( ğ¼ ) and P ğ‘¡ ( ğ¼ ) . We perform MM'23, October 29-November 3, 2023, Ottawa, ON, Canada Chunyu Xie et al. the feature-guided distillation by the cross-entropy loss, and the loss ğ¿ FgD is defined as:  Through experiments in Section 5.3, we observe a noticeable performance gain by performing FgD. Masked Language Modeling with Enhanced Training. We apply a masked language modeling loss to the text-image cross encoder to improve the ability to model the relationship between image and text at the token level. 15% of the text tokens are masked in the input. All of these tokens are replaced with the [ ğ‘€ğ´ğ‘†ğ¾ ] token. The forward operations of MLM [8] and FGR are executed individually in most VLP models [3, 19, 34], increasing the computational cost of pre-training. In our model, the MLM task utilizes masked text and corresponding images together for denoising, which enhances the interaction between text and images. Since FGR relies heavily on this interaction ability, we propose enhanced training (ET), applying FGR and MLM loss on text tokens with masking simultaneously. Experiments in Section 5.3 show that ET can reduce the computational cost of R2D2 while maintaining the accuracy of the model. For simplicity, L MLM denotes the loss of the MLM task with enhanced training. Our model is trained with the full objective:",
  "5 EXPERIMENTS": "",
  "5.1 Implementation Details": "The number of transformer layers for the text encoder, and the two cross encoders are 12, 6, and 6, respectively. The text encoder is initialized from RoBERTa-wwm-ext [5] while the two cross encoders are randomly initialized. Following Wukong [12], we use the image encoder of 12-layers ViT-Base and 24-layers ViT-Large initialized from CLIP [30], and freeze it during pre-training. The resolution of the input image is 224 Ã— 224 in pre-training and fine-tuning. The dimension of the feature vectors of both image and text is 768. We pre-train models with 15 epochs using a batch size of 4096 on 128 NVIDIA A100 GPUs. ğœ in Equation 1 is initialized to 0.07 and can be learned during the training process. We set ğœ ğ‘  = 0.1 and ğœ ğ‘¡ = 0.04 in Equation 4. Moreover, the momentum is set as ğ‘š = 0 . 995, and the queue size is 36,864. We adopt the Adam optimizer and the cosine learning rate schedule with a linear warmup [25]. The pre-trained model is adapted to five vision-language downstream tasks: imagetext retrieval, image-text matching, image caption, text-to-image generation, and zero-shot image classification. More details can refer to Appendix.",
  "5.2 Comparisons with State-of-the-art": "For both image-to-text retrieval and text-to-image retrieval tasks, wereport Recall@1 (R@1), Recall@5 (R@5), Recall@10 (R@10), and Mean Recall (R@M). The results of BriVL [11] and Wukong [12] are excerpted from their paper. Wukong reproduces the CLIP-style [30] and FILIP-style [42] models. Their results are also included. From Table 3, our models outperform state-of-the-art on all datasets. Table 3: Comparisons with state-of-the-art models on imagetext retrieval task. CNA represents our Flickr30k-CNA. Moreover, R2D2ViT-L outperforms R2D2ViT-B . These results indicate that our framework is able to learn better fine-grained associations between image and text. We report the results of Flickr30k-CNA on the test set of Flickr30k-CN for a fair comparison. R2D2 fine-tuned on Flickr30k-CNA outperforms that on Flickr30k-CN, since the quality of human-translated Flickr30k-CNA is much higher than that of machine-translated Flickr30k-CN. Table 4 reports the comparison with existing methods on other V+L downstream tasks. Unlike the image-text retrieval task, there are few datasets for the Chinese image-text matching (ITM) task. Thus, we introduce image-caption matching dataset (ICM) and image-query matching dataset (IQM) for the Chinese ITM task and show the corresponding results. Also, we evaluate Wukong and BriVL on these datasets for the ITM task. We use Area Under Curve (AUC) as the metric. For the image captioning task, fine-tuning is conducted on the training split of AIC-ICC [40]. We adopt four widely-used evaluation metrics: BLEU, METEOR, ROUGE-L, and CCMB: A Large-scale Chinese Cross-modal Benchmark MM'23, October 29-November 3, 2023, Ottawa, ON, Canada Table 4: Comparison with state-of-the-art models on downstream vision-language tasks. Table 5: Comparison with state-of-the-arts which combine dual-stream and single-stream architectures. Classification represents zero-shot image classification. We report R@M, AUC, CIDEr, FID, and Top-1 accuracy for five V+L downstream tasks respectively. Table 6: Effect of the proposed pre-training dataset. Classification represents zero-shot image classification. We report R@M, AUC, CIDEr, FID, and Top-1 accuracy for five V+L downstream tasks respectively. Table 7: Effect of different components of R2D2. Note that we conduct ablation studies and report the average results on all downstream datasets. Generation and classification represent text-to-image generation and zero-shot image classification, respectively. R@* denotes the result for the image-text retrieval task. We report AUC, CIDEr, FID, and Top-1 accuracy for image-text matching, image caption, text-to-image generation, and zero-shot image classification tasks respectively. CIDEr following BriVL. Table 4 also presents text-to-image generation results on ECommerce-T2I dataset 1 [24]. The metric of Frechet Inception Distance (FID) is reported. We evaluate our pre-trained models on ImageNet[6] for the zero-shot image classification task. Class labels are translated from English. Top-1 and Top-5 accuracy are reported. Our model achieves state-of-the-art performance on these V+L downstream tasks. ALBEF [19] and FLAVA [34] also combine dual-stream unimodal encoders and single-stream multimodal encoders. They are pretrained with English Corpus, lacking the ability to perform Chinese downstream tasks. To make a comparison with these methods, We pre-train ALBEF, FLAVA, and R2D2 on the first 1% of Zero. We replace the text encoder and tokenizer of these baselines with the same as ours. Considering that ALBEF and FLAVA use ViT-Base as the image encoder, we show the comparative performance of 1 https://tianchi.aliyun.com/muge MM'23, October 29-November 3, 2023, Ottawa, ON, Canada Chunyu Xie et al. R2D2ViT-B in Table 5. In summary, the results on various tasks demonstrate the superiority of our framework.",
  "5.3 Ablation Study": "Effect of the Proposed Pre-training Dataset. To demonstrate the effectiveness of our proposed pre-training dataset, we provide comparison results of our R2D2 framework pre-trained on the 100M Wukong dataset [12] and the proposed Zero in Table 6. Wukong is the previous largest publicly available Chinese image-text pretraining dataset. For simplicity, we define R2D2 ViT-L as R2D2 in the ablation study. R2D2 pre-trained on the 23M pre-training dataset (a subset of Zero) achieves better results than the ones on the much larger 100M Wukong dataset. This improvement verifies the high quality of our Zero dataset, which is filtered by user clickthrough rate and provides diverse text descriptions along with each image, compared to previous datasets. Moreover, we achieve the best results on the whole pre-training dataset, i.e. , Zero with 250M high-quality image-text pairs. Effect of Fine-Grained Ranking (FGR). We conduct subsequent ablation studies on the first 1% of Zero. We first train a restricted version of R2D2 using only the global contrastive preranking and the two-way distillation strategy. We denote it as PRD2. This restricted setting is conceptually similar to CLIP [30]. R2D2 outperforms PRD2 on the downstream tasks, indicating that the two cross encoders can effectively interact with image and text information through cross-attention. Effect of Enhanced Training (ET). From the third row of Table 7, R2D2 (with ET) performs slightly better than R2D2 w/o ET. Furthermore, R2D2 uses less computational resources than R2D2 w/o ET. R2D2 requires 154.0 GFLOPs and can run at 1.4 iterations per second (Iter/s), while without ET we get 168.8 GFLOPS and 1.1 Iter/s. This indicates that ET is able to both reduce the computational cost and improve the capability of the learning process. Effect of Masked Language Modeling (MLM). Compared to R2D2 w/o MLM, R2D2 obtains better performance on all downstream tasks. MLM allows R2D2 to learn robust representations by masking data. These results indicate that MLM is indeed effective for downstream tasks. Effect of Two-way Distillation (TwD). The proposed twoway distillation is composed of target-guided distillation (TgD) and feature-guided distillation (FgD). By analyzing the two components of TwD, we see that performing feature alignment is important, since the model w/o FgD shows a more noticeable drop in performance. Although milder, removing TgD also causes a reduction in performance. These results indicate that both components are relevant and TwD is an effective way to improve the generalization performance of the pre-trained model.",
  "5.4 Further Experiments": "Zero-shot Tasks. In this section, we conduct zero-shot transfer experiments. From Table 8, our R2D2ViT-L achieves the best performance on Flickr30k-CN, COCO-CN, MUGE, AIC-ICC, ICR, and IQR. For example, R2D2ViT-L achieves 80.5% R@M on COCO-CN, an absolute 5.3% gain over the previous best performance. These results demonstrate sound generalization ability of R2D2. The results of R2D2ViT-L on Flickr30k-CNA are the same as that of Flickr30k-CN, Table 8: Zero-shot results on image-text retrieval task. since we use the same test set for a fair comparison. In this way, we do not report the results of R2D2ViT-L on Flickr30k-CNA. In addition, the AUC scores of R2D2ViT-L on ICM and IQM are 89.8% and 84.5%, respectively. Entity-conditioned Image Visualization. In this experiment, we visualize the attention map of images on COCO-CN. Specifically, we first extract an entity from the Chinese text and calculate the attention score of an image-entity pair. Here, we select the third layer of the text-image cross encoder following [19]. Figure D in Appendix shows that R2D2 learns well to align text with the correct content inside the image.",
  "6 CONCLUSION": "In this paper, we introduce a large-scale Chinese cross-modal benchmark called CCMB and a vision-language framework named R2D2. CCMB includes a high-quality pre-training dataset Zero, which is the largest Chinese cross-modal dataset, and five human-annotated downstream datasets, two of which are the largest Chinese visionlanguage downstream datasets and the first proposed datasets for the Chinese image-text matching task. R2D2 adopts a framework of pre-ranking + ranking for cross-modal learning, boosted with feature-guided distillation, target-guided distillation, and enhanced training. After pre-training, R2D2 achieves state-of-the-art results on fine-tuning and zero-shot settings on twelve downstream datasets of five vision-language tasks. We expect that the good cross-modal benchmark and framework will encourage a plethora of engineers to develop more effective methods in specific realworld scenarios. Acknowledgement. This work was supported by the National Key Research and Development Program of China (No.2018AAA010 0400). CCMB: A Large-scale Chinese Cross-modal Benchmark MM'23, October 29-November 3, 2023, Ottawa, ON, Canada",
  "REFERENCES": "[1] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. 2021. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 3558-3568. [2] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr DollÃ¡r, and C Lawrence Zitnick. 2015. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325 (2015). [3] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. 2020. Uniter: Universal image-text representation learning. In European Conference on Computer Vision . 104-120. [4] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. 2021. Unifying vision-andlanguage tasks via text generation. In International Conference on Machine Learning . 1931-1942. [5] Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, and Guoping Hu. 2020. Revisiting Pre-Trained Models for Chinese Natural Language Processing. In Conference on Empirical Methods in Natural Language Processing . 657-668. [6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 248-255. [7] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. 2021. RedCaps: Web-curated image-text data created by the people, for the people. Advances in Neural Information Processing Systems Track on Datasets and Benchmarks (2021). [8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the Human Language Technology Conference of the NAACL . 41714186. [9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 2021. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations . [10] Fartash Faghri, David J Fleet, Jamie Ryan Kiros, and Sanja Fidler. 2018. Vse++: Improving visual-semantic embeddings with hard negatives. In Proceedings of the British Machine Vision Conference . [11] Nanyi Fei, Zhiwu Lu, Yizhao Gao, Guoxing Yang, Yuqi Huo, Jingyuan Wen, Haoyu Lu, Ruihua Song, Xin Gao, Tao Xiang, et al. 2022. Towards artificial general intelligence via a multimodal foundation model. Nature Communications 13, 1 (2022), 1-13. [12] Jiaxi Gu, Xiaojun Meng, Guansong Lu, Lu Hou, Minzhe Niu, Hang Xu, Xiaodan Liang, Wei Zhang, Xin Jiang, and Chunjing Xu. 2022. Wukong: 100 Million Largescale Chinese Cross-modal Pre-training Dataset and A Foundation Framework. arXiv preprint arXiv:2202.06767 (2022). [13] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 9729-9738. [14] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021. Scaling up visual and visionlanguage representation learning with noisy text supervision. In International Conference on Machine Learning . 4904-4916. [15] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. 2017. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International Journal of Computer Vision 123, 1 (2017), 32-73. [16] Weiyu Lan, Xirong Li, and Jianfeng Dong. 2017. Fluency-guided cross-lingual image captioning. In Proceedings of the 25th ACM international conference on Multimedia . 1549-1557. [17] Gen Li, Nan Duan, Yuejian Fang, Ming Gong, and Daxin Jiang. 2020. Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training. In Proceedings of the AAAI Conference on Artificial Intelligence . 11336-11344. [18] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning . [19] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. 2021. Align before fuse: Vision and language representation learning with momentum distillation. Advances in Neural Information Processing Systems (2021). [20] Kunpeng Li, Yulun Zhang, Kai Li, Yuanyuan Li, and Yun Fu. 2019. Visual semantic reasoning for image-text matching. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 4654-4662. [21] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. 2019. Visualbert: A simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557 (2019). [22] Wei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, and Haifeng Wang. 2021. Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . MM'23, October 29-November 3, 2023, Ottawa, ON, Canada Chunyu Xie et al.",
  "A DETAILS OF ZERO": "We illustrate several representative examples of Zero in Figure A. There are 3 types of text fields associated with each image: 'Title', 'Content' and 'ImageQuery'.",
  "B EXAMPLES OF THE PROPOSED DOWNSTREAM DATASETS": "Figure B illustrates examples of ICM, IQM, ICR, and IQR. Figure C highlights some cases of the difference between Flickr30k-CN and our proposed Flickr30k-CNA.",
  "C MORE IMPLEMENTATION DETAILS": "Training process of target-guided distillation. We use targetguided distillation following the settings of the momentum distillation in ALBEF [19]. Our goal is to generate soft targets to replace the ground-truth labels in Equation (2). During training, we perform the following processes. i. For image and text, we use a momentumupdated encoder as the teacher model respectively, which contains the exponential-moving-average weights. ii. We use the teacher models to obtain the corresponding image-text features and compute their similarity scores. iii. We combine the above similarity scores with ground-truth labels via a coefficient parameter to generate the final soft targets. iv. We replace the ground-truth labels in Equation (2) with the generated soft targets. Selection Strategy of Negative Pairs in Image-text Matching. We obtain hard negative samples by sampling in a mini-batch. Given an image in the mini-batch, we select the corresponding negative text by ranking the contrastive scores of the current batch. We choose the higher score except for the original positive text of the image. In this way, we construct one image-text negative pair for image-text matching loss. The negative images of each text are similar to the description above. Fine-tuning Strategy of Image-Text retrieval. We jointly optimize the GCPR loss (Equation 2) and the FGR loss (Equation 3). We extract the individual features of images and texts via our dual-stream encoder and compute the similarity of all image-text pairs. During inference, we use the top-K strategy to rank the scores in the two cross encoders. We extract the individual features of images and texts via dual-stream encoders. For each image feature, we select the top-K candidate text features and construct K imagetext pairs. We feed the K image-text pairs into two cross encoders to calculate similarity scores. We obtain two K-dimensional score matrices and average them to obtain a final K-dimensional score matrix for ranking. Here, we adjust the K on different downstream datasets. We fine-tune the pre-trained model with 20 epochs on 7 downstream datasets, including Flickr30k-CN, COCO-CN, AIC-ICC, MUGE, ICR, IQR, and Flickr30k-CNA. K is set as 128, 256, 32, 64, 64, 64, 128, respectively. The batchsize is 32 and the learning rate is 1 ğ‘’ -5 . For both image-to-text retrieval (TR) and text-to-image retrieval (IR) tasks, we report Recall@1 (R@1), Recall@5 (R@5), Recall@10 (R@10), and Mean Recall (R@M). For AIC-ICC and MUGE, we report their results on the validation sets, since their test sets are not released. For ICR and IQR, we also report the results on the validation sets in this paper. For Flickr30k-CNA, we show the performance on the test set of Flickr30k-CN for a fair comparison in the main paper. For the remaining downstream datasets, we report the results on the test sets. Following [12], we select the first 10,000 images with the corresponding 50,000 texts when testing on AICICC. In particular, we only provide IR scores on MUGE since it only has IR settings. Fine-tuning Strategy of Image-Text Matching. This task predicts whether an image-text pair is matched or not. During finetuning, we only apply the FGR loss (Equation 3). We fine-tune the models with 5 epochs using a batchsize of 64. The initial learning rate is 1 ğ‘’ -5 . Additionally, we report the results on the validation sets of ICM and IQM. Fine-tuning Strategy of Image Caption. Given an image, the goal of the image-caption task is to generate a caption to describe the image. Similar to Transformer[37], the image-caption model consists of an encoder and a decoder, where the encoder aims to extract the embedding of the given image and the decoder generates tokens of the caption. In specific, we use the image encoder and the text-image cross encoder of R2D2 to initialize the image-caption encoder and decoder, respectively. We fine-tune the image-caption model on the training split of AIC-ICC [40] with 20 epochs. The batchsize is 128 and the learning rate is 1 ğ‘’ -4 . Fine-tuning Strategy of Text-to-Image Generation. Textto-image generation requires the model to generate an image corresponding to the input text. Following DALL-E 2 [31], we build a generation model, including a CLIP-based module, a prior module and a decoder module. Specifically, the dual-stream weights of R2D2 are used to initialize the CLIP-based module. We fine-tune the CLIP-based module and fix it in the next step. Then, we train the prior module to generate image embeddings for given texts. Finally, we fix two former modules and train a diffusion decoder to invert the image embeddings to generate images. All three components of the generation model are fine-tuned on the ECommerce-T2I dataset with 20 epochs, respectively. The batchsize is 16 and the learning rate is 1 ğ‘’ -4 . Fine-tuning Strategy of Zero-shot Image Classification. Given an image, the zero-shot image classification task aims to predict the corresponding class label. Following [12], we use R2D2 to conduct zero-shot image classification task on ImageNet[6]. All the class labels in ImageNet are translated into Chinese.",
  "D ENTITY-CONDITIONED IMAGE VISUALIZATION": "In this experiment, we visualize the attention map of images on COCO-CN. From Figure D, R2D2 has the ability to capture the salient areas when given an image with complex backgrounds, such as the images of 'A train' and 'A bull'. Moreover, we analyze some bad cases in Figure E. We find that the attention score is disturbed when two adjacent entities are present in an image. This phenomenon is particularly evident for objects with similar colors or categories. CCMB: A Large-scale Chinese Cross-modal Benchmark MM'23, October 29-November 3, 2023, Ottawa, ON, Canada Title: äº”å¤§åœ°ç¼å¥‡è§‚æ¬£èµ (View of the five fissure wonders) Content: å¥‰èŠ‚åœ°ç¼äº¦ç§°å¤©äº•å³¡åœ°ç¼ï¼Œå…¨é•¿æœ‰ 37 å…¬é‡Œï¼Œæœ€å¤§æ·±åº¦æœ‰ 229 ç±³ï¼Œè€Œæœ€çª„å¤„ä»… 2 ç±³ã€è€Œå³¡è°·é«˜åº¦è¾¾ 900 ç±³ï¼Œå½¢æˆæ°”åŠ¿å®ä¼Ÿçš„'ä¸€çº¿å¤©'ï¼Œè¢«å²©æº¶ä¸“å®¶ç§°ä½œ'ä¸–ç•Œå–€æ–¯ç‰¹å³¡è°·å¥‡ä¸­ä¹‹ç¨€'ã€‚å³¡è°·ä¸Šæ®µè¾ƒä¸ºå¼€é˜”ï¼Œä½† æ„ˆå¾€ä¸‹æ„ˆç‹­çª„ï¼Œä¸Šéƒ¨å®½ 10 è‡³ 30 ç±³ï¼Œè°·åº•å®½ä»… 1 è‡³ 30 ç±³ï¼Œæ‚¬å´–æœ€æ·±å¤„è¾¾ 300 ç±³ (Fengjie fissure, also known as Tianjingxia fissure, has a total length of 37 kilometers and a maximum depth of 229 meters. The narrowest point is only 2 meters and the height of the canyon is 900 meters, forming a magnificent \"one-line sky\". The Fengjie fissure is called \"the rarest karst canyon in the world\" by karst experts. The upper part of the fissure is relatively open, but it becomes narrower as it goes down. The upper part is 10 to 30 meters wide, the bottom of the valley is only 1 to 30 meters wide, and the deepest cliff is 300 meters.) ImageQuery: å¤©äº•å³¡åœ°ç¼ (TianJingXia fissure) Title: è‹±å® ç‰©ç‹—æˆ´å¢¨é•œç©¿æ½®è£… , ç™¾å˜æ—¶è£…é€ å‹å—çƒ­æ§ (British pet dogs wear sunglasses and trendy clothes. The ever-changing fashion styles are popular.) Content: ä¸€åªåå«æ‰˜æ–¯ç‰¹ (Toast) çš„æŸ¥å°”æ–¯ç‹å°çŒçŠ¬ä¸ç”¨æ‹¥æœ‰ä¸“å±äºè‡ªå·±çš„æ¼‚äº®æ‰‹æåŒ… (A King Charles Spaniel named Toast doesn't have its own fancy handbag.) ImageQuery: æˆ´å¢¨é•œçš„ç‹— , æˆ´å¢¨é•œçš„äºº , ç‹—æˆ´å¢¨é•œ , å¢¨é•œç‹—ç‹— , æˆ´å¢¨é•œçš„ç‹—ç‹—å›¾ç‰‡ , å® ç‰©æˆ´å¢¨é•œ , æ¼‚äº®çš„å® ç‰© ç‹—é€ å‹ , å® ç‰©æˆ´å¢¨é•œå’Œå›´å·¾ , æ©™è‰²çš„å® ç‰©ç‹— , å°çŒçŠ¬æˆ´å¢¨é•œ , èˆ”è„š , æ—¶è£…é€ å‹ , ç‹—ç‹—èˆ”è„š , å°ç‹—æˆ´å¢¨é•œ , ç‹—ç‹—æˆ´ å¢¨é•œ (Dog with sunglasses)",
  "Title: ç¾å‘†äº† !25 ä¸‡ç›†é²œèŠ±é½èšå°æ¦„èŠèŠ±å±•": "(Stunningly beautiful! 250,000 pots of flowers gathered at the Xiaolan chrysanthemum exhibition.) Content: å¤§ç«‹èŠã€ç›†æ™¯èŠã€æ‚¬å´–èŠ (Dali chrysanthemum, bonsai chrysanthemum, cliff chrysanthemum) ImageQuery: å¤§ç«‹èŠ (Dali chrysanthemum) Title: é›¶åŸºç¡€å­¦ç»˜ç”» -å½©é“…ã€Šç´«çº¢è‰²ç™¾åˆèŠ±ã€‹ (Zero Basic Learning Painting - Color Lead \"Fuchsia Lily\") Content: æœ€ç»ˆçš„æ•ˆæœå¦‚å›¾ï¼Œèƒ½å‡ºè¿™æ ·çš„æ•ˆæœï¼ŒçœŸçš„æ˜¯ä¸€å±‚å±‚æ¶‚å‡ºæ¥çš„ (The final view is shown in the figure. To achieve such a view, it is painted layer by layer.) ImageQuery: å½©é“…ç™¾åˆ , å½©é“…ç™¾åˆç»˜ç”»å¤§å…¨ (Color lead lily, color lead lily painting Daquan) Title: èŒ¶ç™¾æˆï¼Œä¸€ç§èƒ½ä½¿èŒ¶æ±¤çº¹è„‰å½¢æˆç‰©è±¡çš„æ°‘é—´è‰ºæœ¯ (Tea Baixi, a folk art that can make the veins of tea soup form objects.) Content: ä¹Œé¾™èŒ¶æ±¤æ˜¾ç°çš„èŒ¶ç™¾æˆå›¾ (Tea Baixi shown in Oolong tea soup) ImageQuery: èŒ¶ç™¾æˆ (Tea Baixi ) Figure A: Examples of Zero. MM'23, October 29-November 3, 2023, Ottawa, ON, Canada Chunyu Xie et al. 4 è¿™ä¹ˆæ™´å¥½çš„å¤©ï¼Œå½“ç„¶å¼€å¾—å¿«ï¼å¤§ å®¶ä¸€å®šè¦æŠ“ä½æœºä¼šï¼Œå»æ¬£èµæ´›é˜³ å¸‚è¿™ä¸€å¹´ä¸€åº¦çš„æèŠ±æ»¡å±±ã€‚ ç´«ä¹ç”¨ä»€ä¹ˆèŠ±ç›† What flower pot does Zi Le use æ©æ–½ æ°‘æ— æœé¥° Enshi National Costume è¿™åœºé›¨é›ªå¤©æ°”å°†æŒç»­åˆ°ä»Šå¤©æ—© ä¸Šï¼Œé¢„è®¡å¹³åŸåœ°åŒºçš„ç§¯é›ªå°†è¾¾ åˆ°1-4cmã€‚ On such a sunny day, of course it drives fast! Everyone must seize the opportunity to appreciate the annual apricot blossoms in Luoyang City. The rain and snow will continue until this morning, with 1-4cm of snow expected in the plains.",
  "Figure B: Image-text examples of ICM, IQM, ICR and IQR from left to right.": "Flickr30k: A little girl covered in paint sits in front of a painted rainbow with her hands in a bowl. Flickr30k-CN: ä¸€ä¸ªå°å¥³å­©åœ¨æ²¹æ¼†å‰ååœ¨ä¸€ä¸ªå½©è™¹çš„å‰é¢åŒæ‰‹åœ¨ç¢—é‡Œã€‚ Flickr30k-CNA: ä¸€ä¸ªæ¶‚æ»¡æŸ“æ–™çš„å°å¥³å­©ååœ¨ç”»å¥½çš„å½©è™¹å‰ï¼ŒæŠŠå¥¹çš„æ‰‹æ”¾åœ¨ä¸€ä¸ªè£…é¢œæ–™çš„ç¢—é‡Œã€‚ Flickr30k: A man with reflective safety  clothes and ear protection drives a John Deere tractor on a road. Flickr30k-CN: ä¸€ä¸ªç”·äººç”¨åå…‰å®‰å…¨æœè£…å’Œè€³æœµä¿æŠ¤é©±åŠ¨çš„é“è·¯ä¸Šçº¦ç¿°è¿ªå°”æ‹–æ‹‰æœºã€‚ Flickr30k-CNA: ä¸€ä¸ªç©¿ç€åå…‰å®‰å…¨æœï¼Œå¸¦ç€è€³æŠ¤çš„ç”·å­åœ¨è·¯ä¸Šå¼€ç€ä¸€è¾†çº¦ç¿°è¿ªå°”æ‹–æ‹‰æœºã€‚ Flickr30k: A black dog and a white dog with brown spots are staring at each other in the street. Flickr30k-CN: ä¸€åªé»‘è‰²çš„ç‹—å’Œä¸€åªæ£•è‰²çš„ç™½è‰²ç‹—åœ¨è¡—ä¸Šç›¯ç€å¯¹æ–¹ã€‚ Flickr30k-CNA: ä¸€åªé»‘ç‹—å’Œä¸€åªå¸¦æœ‰æ£•è‰²æ–‘ç‚¹çš„ç™½ç‹—ç«™åœ¨è¡—ä¸Šï¼Œäº’ç›¸ç›¯ç€å¯¹æ–¹ã€‚ Figure C: Comparisons of Flickr30k, Flickr30k-CN and our proposed Flickr30k-CNA. MM'23, October 29-November 3, 2023, Ottawa, ON, Canada CCMB: A Large-scale Chinese Cross-modal Benchmark ä¸€åªé»‘è‰²çš„ç‹— (A black dog) ä¸€åªç‹—åœ¨æ°´é‡Œ (A dog in water) è‹é¹­ (Heron) åäº†çš„é©¬æ¡¶ (A broken toilet) ä¸€å¤´ (A bull) æ‹¿ç€æ»‘æ¿çš„äºº (A man with a surfboard) é£æœº (Airplane) é¦™è•‰ (Banana) ä¸€è¾†ç«è½¦ (A train) é”®ç›˜ (Keyboard) è‚‰ (Meat) æ¶ˆé˜²æ “ (Fire hydrant) Figure D: More Examples of entity-conditioned image visualization. Figure E: Bad cases of entity-conditioned image visualization. ç‹— (Dog) åœè½¦æ ‡å¿— (Stop sign) æ‘©æ‰˜è½¦ (Motobike) æ©˜å­ (Orange) STOP 'IDED HIGHWAY",
  "keywords_parsed": [
    "large-scale datasets",
    "vision-language pre-training"
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts"
    },
    {
      "ref_id": "b2",
      "title": "Microsoft coco captions: Data collection and evaluation server"
    },
    {
      "ref_id": "b3",
      "title": "Uniter: Universal image-text representation learning"
    },
    {
      "ref_id": "b4",
      "title": "Unifying vision-and-language tasks via text generation"
    },
    {
      "ref_id": "b5",
      "title": "Revisiting Pre-Trained Models for Chinese Natural Language Processing"
    },
    {
      "ref_id": "b6",
      "title": "Imagenet: A large-scale hierarchical image database"
    },
    {
      "ref_id": "b7",
      "title": "RedCaps: Web-curated image-text data created by the people, for the people"
    },
    {
      "ref_id": "b8",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding"
    },
    {
      "ref_id": "b9",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale"
    },
    {
      "ref_id": "b10",
      "title": "Vse++: Improving visual-semantic embeddings with hard negatives"
    },
    {
      "ref_id": "b11",
      "title": "Towards artificial general intelligence via a multimodal foundation model"
    },
    {
      "ref_id": "b12",
      "title": "Wukong: 100 Million Largescale Chinese Cross-modal Pre-training Dataset and A Foundation Framework"
    },
    {
      "ref_id": "b13",
      "title": "Momentum contrast for unsupervised visual representation learning"
    },
    {
      "ref_id": "b14",
      "title": "Scaling up visual and vision-language representation learning with noisy text supervision"
    },
    {
      "ref_id": "b15",
      "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations"
    },
    {
      "ref_id": "b16",
      "title": "Fluency-guided cross-lingual image captioning"
    },
    {
      "ref_id": "b17",
      "title": "Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training"
    },
    {
      "ref_id": "b18",
      "title": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation"
    },
    {
      "ref_id": "b19",
      "title": "Align before fuse: Vision and language representation learning with momentum distillation"
    },
    {
      "ref_id": "b20",
      "title": "Visual semantic reasoning for image-text matching"
    },
    {
      "ref_id": "b21",
      "title": "Visualbert: A simple and performant baseline for vision and language"
    },
    {
      "ref_id": "b22",
      "title": "Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning"
    }
  ]
}