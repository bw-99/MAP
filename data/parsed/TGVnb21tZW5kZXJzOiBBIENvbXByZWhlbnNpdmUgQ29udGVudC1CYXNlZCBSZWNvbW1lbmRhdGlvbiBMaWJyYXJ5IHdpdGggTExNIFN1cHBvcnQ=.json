{
  "Legommenders : A Comprehensive Content-Based Recommendation Library with LLM Support": "Qijiong Liu The Hong Kong Polytechnic University Hong Kong SAR liu@qijiong.work Lu Fan The Hong Kong Polytechnic University Hong Kong SAR cslfan@comp.polyu.edu.hk Xiao-Ming Wu The Hong Kong Polytechnic University Hong Kong SAR xiao-ming.wu@polyu.edu.hk",
  "ABSTRACT": "We present Legommenders, a unique library designed for contentbased recommendation that enables the joint training of content encoders alongside behavior and interaction modules, thereby facilitating the seamless integration of content understanding directly into the recommendation pipeline. Legommenders allows researchers to effortlessly create and analyze over 1,000 distinct models across 15 diverse datasets. Further, it supports the incorporation of contemporary large language models, both as feature encoder and data generator, offering a robust platform for developing state-of-the-art recommendation models and enabling more personalized and effective content delivery.",
  "KEYWORDS": "Content-based Recommendation, LLM for RS, Library ACMReference Format: MIND User Behavior Item Vectors Candidate Vector(s) Candidate Vector(s) User Vector Candidate Item(s) … Goodreads MovieLens Multi-Modal Multi-A  ribute Dataset Processor Content Operator Behavior Operator Click Predictor    Cacher Cacher Qijiong Liu, Lu Fan, and Xiao-Ming Wu. 2018. Legommenders : A Comprehensive Content-Based Recommendation Library with LLM Support. In Proceedings of Make sure to enter the correct conference title from your rights confirmation emai (Conference acronym 'XX). ACM, New York, NY, USA, 5 pages. https://doi.org/XXXXXXX.XXXXXXX",
  "1 INTRODUCTION": "In online content discovery, recommender systems play a pivotal role as navigators, significantly enhancing user experiences through personalized content delivery. Traditionally, recommender systems have predominantly relied on transductive learning mechanisms [10, 30]. This approach utilizes static user and item identifiers (IDs) to generate predictions based on existing data. While effective within the confines of known datasets, this method presents limitations. It struggles to adapt to new users and items, often referred to as the 'cold start' problem, and is less responsive to shifts in user preferences over time. Modern recommender systems have shifted from transductive learning to inductive learning, utilizing inherent content features and user historical behaviors to create more dynamic models [8, 20, Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference acronym 'XX, June 03-05, 2018, Woodstock, NY © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/18/06 https://doi.org/XXXXXXX.XXXXXXX Figure 1: Overview of the Legommenders package. 22-24, 32, 34, 37, 38, 42]. Specifically, a content-based recommendation model typically consists of three components: (1) A content operator that generates embeddings for both the candidate item and each item in the user's behavior sequence. (2) A behavior operator that fuses the user sequence into a unified user embedding. (3) A click predictor that calculates the click probability for the user on the given item. Notably, the content operator can either be trained jointly with the other two modules or be decoupled from them. However, most existing recommender system libraries [4, 40, 43] employ a decoupled design, which fails to adapt the content encoder to specific recommendation scenarios. Typically, item embeddings are generated by a pretrained content encoder and used as an initial step. Although this approach enhances model efficiency, it has a significant limitation: the pretrained embeddings are often too general and not well-aligned with the specific recommendation context, leading to suboptimal recommendations. In contrast, our Legommenders library offers a unique and innovative feature by enabling the joint training of content operators alongside other modules. This capability allows for the seamless integration of content understanding directly into the recommendation pipeline. As shown in Figure 1, Legommenders comprises four core components: the dataset processor, content operator, behavior operator, and click predictor. By combining these built-in operators and predictors in the configuration files, researchers can design over 1,000 recommendation models, utilizing 15 content operators, 8 Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Liu and Wu, et al. Table 1: Comparison to exisiting recommendation benchmarks ( ✓ | - | × means totally | partially | not met, respectively). 'Partially met' indicates incomplete availability. behavior operators, and 9 click predictors. Remarkably, 95% of these models have not been tested or published before. These models can be evaluated across multiple content-based recommendation scenarios using over 15 datasets. Moreover, Legommenders is the first library to inherently support large language models (LLMs). It not only accepts augmented data generated by LLMs for training but also integrates open-source LLMs as content operators/encoders. This dual capability allows Legommenders to improve data quality and generate superior data embeddings using LLMs. Legommenders is designed to offer researchers and practitioners a comprehensive, flexible, and user-friendly platform for conducting experiments and analyses in content-based recommendation in the era of LLMs, aiming to facilitate new research directions in the field. The Legommenders library, including code, data, and documentation, is accessible at: https://github.com/Jyonn/Legommenders.",
  "2 COMPARISON TO EXISTING BENCHMARKS": "Despite the success of previous research, there remains a significant lack of standardized benchmarks and uniform evaluation protocols for content-based recommendation systems. As summarized in Table 1, traditional recommendation libraries typically accept only ID-based features and do not utilize large language models (LLMs) for content encoding. The recent library Ducho [4] offers multimodal feature extraction for downstream recommendation models but maintains a decoupled design. In contrast, Legommenders is currently the only library that supports end-to-end training of content operators, behavior operators, and click predictors. Furthermore, we have developed an inference caching pipeline that achieves up to a 50x speedup in evaluation. By enabling easy modular combinations, we provide over 1,000 models, which is six times more than the largest existing model libraries.",
  "3 LEGOMMENDERS: DETAILS AND USAGE": "python lego.py \\ --data config/data/mind.yaml \\ --model config/model/nrms.yaml \\ --mode train > Figure 2: A quick use of Legommenders. In this section, we first introduce the supported recommendation tasks, followed by a detailed discussion of the key components. We then present our caching pipeline and conclude with an overview of the algorithm flow.",
  "3.1 Recommendation Tasks": "The Legommenders library supports two fundamental recommendation tasks: matching and ranking . In the matching task, given a user and a set of 𝐾 + 1 candidate items (one positive and 𝐾 negative), the model performs a 𝐾 + 1 classification task to identify the positive item, which is formulated as:  where 𝑓 ( 𝑥 𝑢 , 𝑥 𝑖 ) is mostly a simple dot operation, calculates the user-item feature relevance. The training objective is to maximize the likelihood of the correct positive item:  where 𝑦 𝑢𝑖 is the binary label for item 𝑖 , and D is the dataset of user-item pairs. In the ranking task, the model predicts the click probability for a given user-item pair, denoted as:  where 𝑓 ( 𝑥 𝑢 , 𝑥 𝑖 ) can be deep CTR models and trained to minimize the mean squared error between the predicted and actual labels:  The task can be configured with the use_neg_sampling: false and neg_count: 0 parameters in the model configuration file, as depicted in Figure 3.",
  "3.2 Dataset Processor": "Legommenders provides a range of built-in processors for various recommendation scenarios, including news recommendation (MIND [36], PENS [3], Adressa [9], and Eb-NerD [16]), book recommendation (Goodreads [29] and Amazon Books [12]), movie recommendation (Movielens [11] and Netflix [1]), and music recommendation (Amazon CDs [12] and Last.fm [25]) Dataset-specific processors convert the data into a unified format using the UniTok library 1 , which includes an item content table, a user behavior table, and an interaction table. 1 https://pypi.org/project/UniTok/ Legommenders Conference acronym 'XX, June 03-05, 2018, Woodstock, NY : [500, 500, 500] : huggyllama/llama-7b : (32, 128, 0.1) name : DCN meta : content : Null behavior : Pooling predictor : DCN config : use_neg_sampling : false use_item_content : false hidden_size : 64 content_hidden_size : 64 predictor : dnn_units dnn_activations : ReLU dnn_dropout : 0.1 dnn_batch_norm : false cross_num : 3 name : DIRE meta : content : Llama behavior : Attention predictor : Dot config : use_neg_sampling : true use_item_content : true hidden_size : 64 neg_count : 4 content : key use_lora behavior : num_attention_heads : 8 use_sep_token : true",
  "Model DIRE": "with LLM Feature Encoding : data/${name}/news : data/${name}/news name : MIND item : depot attributes : - title - category ... e MIND Dataset",
  "Model DCN": "name : MIND-augmented item : depot attributes : - title-augmented - category ...",
  "e MIND Dataset": "with LLM Feature Engineering",
  "Figure 3: Examples for model and dataset configurations.": "In contrast to FuxiCTR [43] and BARS [41], which merge multiple tables into one, Legommenders adheres to the second normal form, significantly reducing data redundancy. This decoupled storage design allows Legommenders to easily accommodate augmented data generated by large language models simply by modifying the selected item attributes, as shown in Figure 3.",
  "3.3 Content Operator": "Legommenders decomposes content-based recommendation models into the content operator, behavior operator, and click predictor. This modular design allows for flexible selection and combination of these components to create new recommenders. The built-in content operators include average pooling used by text-based CTR models [10, 30], convolutional neural networks (CNN) [17] used by NAML [32] and LSTUR [2], Attention [28] used by NRMS [33] and PREC [21], and Fastformer [35]. Legommenders supports the use of LLMs as content operators, such as BERT [15], LLaMA [27], and other open-source models available on Hugging Face 2 . Building on insights from previous works [19, 21, 34], we propose a training method that freezes the lower layers while fine-tuning the upper layers, including LoRAbased PEFT [13]. This approach achieves up to 100x training acceleration compared to full fine-tuning, as it requires only the parameter ' --mode split --layer <N> '. Additionally, Legommenders is compatible with identifier-based recommenders by setting use_item_content: false and using a randomly initialized item embedding table, such as DCN [30] model in Figure 3. It is compatible with decoupled content operator designs, like Ducho [4], by setting use_item_content: false and using an embedding table from pretrained models. 2 https://huggingface.co",
  "Algorithm 1 Python-style Code for Training and Inference": "class Legommenders: def forward(self, user , item , labels): content_op = self.content_cacher if self.content_op and self.training: content_op = self.content_op user = content_op(user) item = content_op(item) behavior_op = self.behavior_cacher if self.training: behavior_op = self.behavior_op user = behavior_op(user) scores = self.predictor(user, item) if self.training: return self.loss_fct(scores , labels) return scores",
  "3.4 Behavior Operator and Click Predictor": "The built-in behavior operators encompass several mechanisms, including average pooling, which is utilized by CTR models; Additive Attention [5], employed by NAML [32]; GRU [7], used by LSTUR [2]; and Attention, which is implemented in NRMS [33], BST [6], and PLM-NR [34]. Additionally, PolyAttention is utilized by MINER [18], among others. The built-in click predictors include the dot product, a method widely used in numerous matching-based models and various CTR models [10, 30, 31], which rely on feature interaction modules as their core design.",
  "3.5 Caching Pipeline": "During inference and evaluation, the model parameters are fixed. Traditional recommendation libraries and content-based recommender systems dynamically encode user and item embeddings for each user-item pair in the test set and calculate the click probability in real-time. This results in redundant computations, which can be exacerbated by the cascaded design of content and behavior operators. To mitigate this, we propose content and behavior cachers, as illustrated in Figure 1, which precompute and store embeddings for all items and users during the inference phase. During subsequent inferences, only the lightweight click predictor is required. The acceleration gained from caching becomes more pronounced as the frequency of repeated users and items, as well as with the sizes of the content and behavior operators. In some cases, this approach can yield up to 50x inference speedup. The caching mechanism is enabled by default and seamlessly integrated into the recommendation model, demonstrated in Algorithm 1.",
  "4 EXPERIMENTS": "In this section, we present a selection of benchmark results for representative models on the MIND dataset. These results illustrate the robust modular composition capabilities of Legommenders and its support for LLMs. Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Liu and Wu, et al. Table 2: Selected benchmark results on the MIND dataset. 'ContentOp' and 'BehaviorOp' denote Content Operator and Behavior Operator, respectively. 'Original' and 'Augmented' refer to the original MIND dataset and the GPT-augmented dataset as in ONCE [19]. Null(Llama1) indicates the decoupled design using Llama for item embedding extraction. All baselines use the same hyperparameters, including embedding dimension, number of attention heads, learning rate, and others, to ensure consistency. Due to space limitations, we will provide the full configurations in our repository for reproducibility. The results show that: 1) models trained on GPT-augmented datasets consistently outperform those using the original datasets; 2) baselines incorporating more complex language models tend to achieve better performance; 3) LLM-finetuning scheme outperforms the decoupled design. These findings highlight the strong content understanding capabilities of LLMs, further underscoring the contribution of our library.",
  "5 CONCLUSION": "Wehave introduced Legommenders, a library designed for contentbased recommendation systems, which stands out due to its ability to jointly train content operators, behavior operators, and click predictors for inductive learning, its modular design, and its support for LLMs as both content encoders and data generators. We believe that Legommenders will serve as a valuable tool, significantly accelerating research within the recommendation community.",
  "REFERENCES": "[1] Nicholas Ampazis. 2008. Collaborative filtering via concept decomposition on the netflix dataset. In ECAI , Vol. 8. 26-30. [2] Mingxiao An, Fangzhao Wu, Chuhan Wu, Kun Zhang, Zheng Liu, and Xing Xie. 2019. Neural news recommendation with long-and short-term user representations. In Proceedings of the 57th annual meeting of the association for computational linguistics . 336-345. [3] Xiang Ao, Xiting Wang, Ling Luo, Ying Qiao, Qing He, and Xing Xie. 2021. PENS: A dataset and generic framework for personalized news headline generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) . 82-92. [4] Matteo Attimonelli, Danilo Danese, Daniele Malitesta, Claudio Pomo, Giuseppe Gassi, and Tommaso Di Noia. 2024. Ducho 2.0: Towards a More Up-to-Date Unified Framework for the Extraction of Multimodal Features in Recommendation. In WWW . ACM. [5] Dzmitry Bahdanau. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473 (2014). [6] Qiwei Chen, Huan Zhao, Wei Li, Pipei Huang, and Wenwu Ou. 2019. Behavior sequence transformer for e-commerce recommendation in alibaba. In Proceedings of the 1st international workshop on deep learning practice for high-dimensional sparse data . 1-4. [7] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555 (2014). [8] Junchen Fu, Xuri Ge, Xin Xin, Alexandros Karatzoglou, Ioannis Arapakis, Jie Wang, and Joemon M Jose. 2024. IISAN: Efficiently adapting multimodal representation for sequential recommendation with decoupled PEFT. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval . 687-697. [9] Jon Atle Gulla, Lemei Zhang, Peng Liu, Özlem Özgöbek, and Xiaomeng Su. 2017. The adressa dataset for news recommendation. In Proceedings of the international conference on web intelligence . 1042-1048. [10] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. arXiv preprint arXiv:1703.04247 (2017). [11] F Maxwell Harper and Joseph A Konstan. 2015. The movielens datasets: History and context. Acm transactions on interactive intelligent systems (tiis) 5, 4 (2015), 1-19. [12] Ruining He and Julian McAuley. 2016. Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering. In proceedings of the 25th international conference on world wide web . 507-517. [13] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021). [14] Dmytro Ivchenko, Dennis Van Der Staay, Colin Taylor, Xing Liu, Will Feng, Rahul Kindi, Anirudh Sudarshan, and Shahin Sefati. 2022. Torchrec: a pytorch domain library for recommendation systems. In Proceedings of the 16th ACM Conference on Recommender Systems . 482-483. [15] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of NAACL-HLT . 4171-4186. [16] Johannes Kruse, Kasper Lindskow, Saikishore Kalloori, Marco Polignano, Claudio Pomo, Abhishek Srivastava, Anshuk Uppal, Michael Riis Andersen, and Jes Frellsen. 2024. EB-NeRD a large-scale dataset for news recommendation. In Proceedings of the Recommender Systems Challenge 2024 . 1-11. [17] Yann LeCun, Bernhard Boser, John Denker, Donnie Henderson, Richard Howard, Wayne Hubbard, and Lawrence Jackel. 1989. Handwritten digit recognition with a back-propagation network. Advances in neural information processing systems 2 (1989). [18] Jian Li, Jieming Zhu, Qiwei Bi, Guohao Cai, Lifeng Shang, Zhenhua Dong, Xin Jiang, and Qun Liu. 2022. MINER: Multi-interest matching network for news recommendation. In Findings of the Association for Computational Linguistics: ACL 2022 . 343-352. [19] Qijiong Liu, Nuo Chen, Tetsuya Sakai, and Xiao-Ming Wu. 2024. Once: Boosting content-based recommendation with both open-and closed-source large language models. In Proceedings of the 17th ACM International Conference on Web Search and Data Mining . 452-461. [20] Qijiong Liu, Hengchang Hu, Jiahao Wu, Jieming Zhu, Min-Yen Kan, and XiaoMing Wu. 2024. Discrete Semantic Tokenization for Deep CTR Prediction. In Companion Proceedings of the ACM on Web Conference 2024 . 919-922. [21] Qijiong Liu, Jieming Zhu, Quanyu Dai, and Xiao-Ming Wu. 2022. Boosting deep CTR prediction with a plug-and-play pre-trainer for news recommendation. In Proceedings of the 29th International Conference on Computational Linguistics . 2823-2833. [22] Qijiong Liu, Jieming Zhu, Quanyu Dai, and Xiao-Ming Wu. 2024. Benchmarking News Recommendation in the Era of Green AI. In Companion Proceedings of the ACM on Web Conference 2024 . 971-974. [23] Qijiong Liu, Jieming Zhu, Lu Fan, Zhou Zhao, and Xiao-Ming Wu. 2024. STORE: Streamlining Semantic Tokenization and Generative Recommendation with A Single LLM. arXiv preprint arXiv:2409.07276 (2024). [24] Qijiong Liu, Jieming Zhu, Yanting Yang, Quanyu Dai, Zhaocheng Du, Xiao-Ming Wu, Zhou Zhao, Rui Zhang, and Zhenhua Dong. 2024. Multimodal pretraining, adaptation, and generation for recommendation: A survey. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 6566-6576. [25] Markus Schedl. 2016. The lfm-1b dataset for music retrieval and recommendation. In Proceedings of the 2016 ACM on international conference on multimedia retrieval . 103-110. [26] Weichen Shen. 2017. DeepCTR: Easy-to-use,Modular and Extendible package of deep-learning based CTR models. https://github.com/shenweichen/deepctr. [27] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Conference acronym 'XX, June 03-05, 2018, Woodstock, NY",
  "Legommenders": "Azhar, et al. 2023. LLaMA: open and efficient foundation language models. arXiv. arXiv preprint arXiv:2302.13971 (2023). [28] A Vaswani. 2017. Attention is all you need. Advances in Neural Information Processing Systems (2017). [29] Mengting Wan and Julian J. McAuley. 2018. Item recommendation on monotonic behavior chains. In Proceedings of the 12th ACM Conference on Recommender Systems, RecSys 2018, Vancouver, BC, Canada, October 2-7, 2018 , Sole Pera, Michael D. Ekstrand, Xavier Amatriain, and John O'Donovan (Eds.). ACM, 86-94. [30] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network for ad click predictions. In Proceedings of the ADKDD'17 . 1-7. [31] WMWang, YH Peng, J Hu, and ZM Cao. 2009. Collaborative robust optimization under uncertainty based on generalized dynamic constraints network. Structural and multidisciplinary optimization 38 (2009), 159-170. [32] Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang, and Xing Xie. 2019. Neural News Recommendation with Attentive Multi-View Learning. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19 . 3863-3869. https://doi.org/10.24963/ijcai.2019/536 [33] Chuhan Wu, Fangzhao Wu, Suyu Ge, Tao Qi, Yongfeng Huang, and Xing Xie. 2019. Neural news recommendation with multi-head self-attention. In Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP) . 6389-6394. [34] Chuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng Huang. 2021. Empowering news recommendation with pre-trained language models. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval . 1652-1656. [35] Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang, and Xing Xie. 2021. Fastformer: Additive attention can be all you need. arXiv preprint arXiv:2108.09084 (2021). [36] Fangzhao Wu, Ying Qiao, Jiun-Hung Chen, Chuhan Wu, Tao Qi, Jianxun Lian, Danyang Liu, Xing Xie, Jianfeng Gao, Winnie Wu, and Ming Zhou. 2020. MIND: A Large-scale Dataset for News Recommendation. In Proceedings of the 58th Annual",
  "keywords_parsed": [
    "Content-based Recommendation",
    " LLM for RS",
    " Library ACMReference Format: MIND User Behavior Item Vectors Candidate Vector(s) Candidate Vector(s) User Vector Candidate Item(s) … Goodreads MovieLens Multi-Modal Multi-A  ribute Dataset Processor Content Operator Behavior Operator Click Predictor    Cacher Cacher Qijiong Liu",
    " Lu Fan",
    " and Xiao-Ming Wu. 2018. Legommenders : A Comprehensive Content-Based Recommendation Library with LLM Support. In Proceedings of Make sure to enter the correct conference title from your rights confirmation emai (Conference acronym 'XX). ACM",
    " New York",
    " NY",
    " USA",
    " 5 pages. https://doi.org/XXXXXXX.XXXXXXX"
  ]
}