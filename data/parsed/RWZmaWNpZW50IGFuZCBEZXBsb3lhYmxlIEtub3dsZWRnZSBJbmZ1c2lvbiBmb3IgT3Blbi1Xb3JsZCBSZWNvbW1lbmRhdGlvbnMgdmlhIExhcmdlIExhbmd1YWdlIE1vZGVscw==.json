{"Efficient and Deployable Knowledge Infusion for Open-World Recommendations via Large Language Models": "YUNJIA XI, Shanghai Jiao Tong University, China WEIWEN LIU, Huawei Noah's Ark Lab, China JIANGHAO LIN, Shanghai Jiao Tong University, China MUYAN WENG, Shanghai Jiao Tong University, China XIAOLING CAI, Consumer Business Group, Huawei, China HONG ZHU, Consumer Business Group, Huawei, China JIEMING ZHU, Huawei Noah's Ark Lab, China BO CHEN, Huawei Noah's Ark Lab, China RUIMING TANG, Huawei Noah's Ark Lab, China YONG YU, Shanghai Jiao Tong University, China WEINAN ZHANG, Shanghai Jiao Tong University, China Recommender system plays a pervasive role in today's online services, yet its closed-loop nature, i.e. , training and deploying within a specific closed domain, constrains its access to open-world knowledge. Recently, the emergence of large language models (LLMs) has shown promise in bridging this gap by encoding extensive world knowledge and demonstrating advanced reasoning capabilities. However, previous attempts to directly implement LLMs as recommenders fall short in meeting the demanding requirements of industrial recommender systems, particularly in terms of online inference latency and offline resource efficiency. In this work, we propose an Open-World Recommendation Framework with Efficient and Deployable Knowledge Infusion from Large Language Models, dubbed REKI , to acquire two types of external knowledge about users and items from LLMs. Specifically, we introduce factorization prompting to elicit accurate knowledge reasoning on user preferences and items. With factorization prompting, we develop individual knowledge extraction and collective knowledge extraction tailored for different scales of recommendation scenarios, effectively reducing offline resource consumption. Subsequently, the generated user and item knowledge undergoes efficient transformation and condensation into augmented vectors through a hybridized expert-integrated network , ensuring its compatibility with the recommendation task. The obtained vectors can then be directly used to enhance the performance of any conventional recommendation model. We also ensure efficient inference by preprocessing and prestoring the knowledge from the LLM. Extensive experiments demonstrate that REKI significantly outperforms the state-of-the-art baselines and is compatible with a diverse array of recommendation algorithms and tasks. Now, REKI has been deployed to Huawei's news and music recommendation platforms and gained a 7% and 1.99% improvement during the online A/B test. CCS Concepts: \u00b7 Information systems \u2192 Recommender systems . Additional Key Words and Phrases: Recommender Systems; Large Language Models; Knowledge Augmentation Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Manuscript submitted to ACM", "ACMReference Format:": "Yunjia Xi, Weiwen Liu, Jianghao Lin, Muyan Weng, Xiaoling Cai, Hong Zhu, Jieming Zhu, Bo Chen, Ruiming Tang, Yong Yu, and Weinan Zhang. 2018. Efficient and Deployable Knowledge Infusion for Open-World Recommendations via Large Language Models. In . ACM, New York, NY, USA, 31 pages. https://doi.org/XXXXXXX.XXXXXXX", "1 INTRODUCTION": "Recommender systems (RSs) play a pervasive role in contemporary online services, enriching user experiences across diverse domains like music streaming [75], movie exploration [39], and e-commerce [23]. Nonetheless, one of the main characteristics of recommender systems lies in their closed-loop nature - recommender models are trained and deployed within closed systems, utilizing internally generated data to train and optimize themselves. Due to their insulated nature, classical recommender models confine their data and knowledge to specific application domains [39, 95] and isolate from the knowledge in the external world, limiting their ability to access a broader range of information and insights. Consequently, this closed-loop nature may increase the possibility of generating outdated, repetitive, and imprecise recommendations [38]. To this end, researchers have revealed that if we expand the knowledge horizon beyond the limited domains, predictive accuracy and generalization capabilities of recommender systems could be significantly boosted [14, 45]. In this work, we refer to external world knowledge capable of enhancing recommendations as open-world knowledge . Specifically, it typically comprises two categories, namely knowledge augmentation for the two crucial elements in recommendations, users and items. The item knowledge often involves factual information obtainable from the web or induced from its textual description. In contrast, user knowledge is more complex and dynamic, usually entailing a delicate understanding of user preferences and profiles. Therefore, we propose a departure from the traditional closed-loop systems . Instead, we advocate for recommender systems to embrace an open-world systems paradigm, actively seeking open-world knowledge from the external world. This shift emphasizes the need to move beyond learning from narrowly defined data, fostering a more dynamic and adaptive recommender system that engages with a broader spectrum of knowledge. On the one hand, the inclusion of factual knowledge related to items contributes valuable common-sense information about candidate items, thereby elevating the quality of recommendations. Taking the movie recommendation as an example, the external world it needs contains additional movie features such as lots, related reports, awards, and critic reviews which are often absent in the recommendation dataset. This augmentation expands the scope of the original data and enhances the effectiveness of recommendation. On the other hand, knowledge inferred from user behavior history on the user side facilitates a more holistic understanding of users, playing a pivotal role in improving recommendation performance. Unraveling the underlying preferences and motivations that drive user behaviors provides deeper insights and clues about users. Aspects such as personality, occupation, intentions, preferences, and tastes can be reflected in user behavior history [54]. The reasoning of preferences can extend to integrating temporal factors ( e.g. , holiday-themed movie preferences during Christmas) or responding to external events ( e.g. , an increased interest in health products during a pandemic). This preference reasoning goes beyond basic behavior patterns identified by classical recommenders, delivering human-like recommendations supported by clear evidence. Previous efforts have explored injecting external knowledge into recommender systems through methods such as knowledge graphs [19, 78] or cross-domain recommendations [20, 34, 72]. Knowledge graphs are incorporated into the recommender system as side information, explicitly providing rigorously validated external world knowledge to enhance recommendation precision. Cross-domain recommendation methods facilitate the transfer of knowledge from source domains to target domains, thereby strengthening the target domain's recommender system with additional insights. However, constructing comprehensive and accurate knowledge graphs or multi-domain datasets requires considerable extra human effort, and the accessible knowledge remains limited. Moreover, they only focus on extracting factual knowledge from the external world and overlook the reasoning knowledge on user preferences [19]. For instance, knowledge graphs used in RS usually focus on complement knowledge for items and rarely consider users [19, 77, 78] since the user-side knowledge is highly dynamic and challenging to capture within a static knowledge graph. Recent strides in large language models (LLMs) have brought about revolutions in learning paradigms across various research domains, and they also offer substantial promise in bridging the gap between conventional recommender systems and open-world knowledge [7, 92, 96]. Notably, LLMs like GPT-4 [60] and LLaMA [74], owing to their vast model scale and corpus size, have memorized a vast array of knowledge and demonstrated remarkable capabilities encompassing problem-solving, logical reasoning, and creative writing [5]. By learning from an extensive corpus of internet texts, LLMs have successfully encoded a broad spectrum of world knowledge, ranging from basic factual information to intricate societal norms and logical structures [5]. Particularly noteworthy is the proficiency of LLMs in leveraging their encoded knowledge to perform basic logical reasoning that aligns with established facts and relationships [82, 93]. This inherent capacity of LLMs to possess factual knowledge about items and exhibit logical reasoning skills is pivotal in bridging the gap between conventional recommender systems and open-world knowledge. In recent research, numerous studies have explored the application of LLMs for recommendation, which usually transform recommendation tasks and user profiles into prompts [3, 9, 15, 16, 51]. It has been observed that LLMs are particularly well-suited for applications such as conversational recommendation [15, 24], zero-shot/few-shot recommendation [3, 27, 50, 51, 79], explainable recommendation [15, 55], and cold-start scenarios [17, 46, 71]. In the realm of traditional recommendation tasks, such as sequential recommendation and top-N recommendation, the integration of collaborative signals in LLM-based recommendations has indicated the potential to outperform classical recommendation models [47, 76]. However, in industrial recommendation scenarios characterized by a multitude of users and items and stringent efficiency requirements, directly using LLMs as recommenders still faces significant challenges, which primarily stem from two key shortcomings of deployment: 1) Online Inference Latency. To cater to the real-time demands of users, industrial recommendation systems typically have strict latency requirements (usually within 100 milliseconds [85]). However, the inherent limitation of LLMs - the excessive number of model parameters - will introduce significant inference latency, making it impractical to directly leverage LLMs as recommenders in industrial settings. With billions of users and thousands of user behaviors, LLMs struggle to meet the low latency requirement of industrial recommendation systems. Moreover, the extensive model size also brings much online computational overhead and hinders the feasibility of incorporating real-time user feedback. 2) Offline Resource Consumption. The term 'resource' here encompasses both time and computational resources. Drawing insights from prior research [3, 16, 50], the good performance of LLMs in recommendation scenarios often entails fine-tuning LLMs on specific recommendation data. However, industrial recommendation scenarios typically involve a vast array of users and items, with user behavior distributions evolving over time. Achieving satisfactory results in such scenarios demands continuous fine-tuning of LLMs on regularly updated and substantial amounts of data, consequently leading to significant offline time cost and computational resource consumption. Additionally, fine-tuning LLMs often demands a considerable amount of time. During this period, the online distribution of user behavior may have undergone some changes. This temporal misalignment could result in a situation where a fine-tuned LLM may no longer be suitable for the current online data distribution, leading to inaccuracy recommendations. Therefore, it is impractical to replace existing recommendation processes and conventional recommendation models (CRMs) with LLMs in industrial recommendations. As a result, we do not engage in fine-tuning LLMs on recommendation data; instead, we focus on efficiently extracting knowledge about users and items from LLMs to augment CRMs. This approach allows us to preserve the advantage of CRMs in terms of inference latency while effectively leveraging open-world knowledge from LLMs as well. Despite the appealing benefits of this approach, efficient extraction and utilization of knowledge from LLMs pose considerable challenges. 1) How to extract recommendation-related knowledge from LLMs. Knowledge related to items may encompass various aspects, while knowledge related to users may involve complex reasoning. When facing complex tasks, LLMs often suffer from the issue of compositional gap , where LLMs have difficulty in generating correct answers to the compositional problem like recommending items to users, whereas they can correctly answer all its sub-problems [65]. Therefore, LLMs may also encounter the issue of compositional gap when confronted with knowledge reasoning and extraction encompassing multiple facets, thereby preventing us from fully exploiting the open-world knowledge encoded in LLMs [9, 36]. 2) Howtoensureefficient knowledge extraction in large-scale scenarios. Although the removal of fine-tuning LLMs has saved considerable time and computational resources, knowledge generation may still be inefficient when dealing with a large volume of users and items. Industrial recommendation scenarios often involve a massive number of users and items, potentially achieving a billion-level. Dealing with such a great number of users and items makes utilizing LLMs to generate knowledge for each user and item a time-consuming and resource-intensive task. 3) How to integrate the open-world knowledge into CRMs. The open-world knowledge generated by LLMs manifests in human-like texts, instead of neural representations that can be directly fed into CRMs. Even in instances where some LLMs are open-sourced, their latent hidden states are typically large dense vectors ( e.g. , 4096 for each token in LLaMA-7B [74]), which are highly abstract and not readily digestible to CRMs. Hence, it is pivotal to effectively transform the output knowledge to align with the recommendation space without loss of information or misinterpretation. Furthermore, due to the hallucination problem [33], the knowledge produced by LLMs can occasionally be unreliable or misleading, possibly introducing much noise in recommendation. Consequently, it is imperative to extract open-world knowledge from LLMs efficiently while enhancing the accessibility and reliability of the knowledge, which fully unleashes the potential of open-world recommender systems. To address the above challenges, we propose an Open-World Recommendation Framework with Efficient and Deployable Knowledge Infusion from LLMs, (dubbed REKI 1 ). REKI is a model-agnostic framework that efficiently bridges classical recommender systems and open-world knowledge. It first leverages LLMs to generate open-world knowledge and then applies CRMs to model collaborative signals and LLM-enhanced knowledge. In this way, we integrate the strengths of both LLMs and CRMs to significantly improve the model's predictive accuracy while concurrently maintaining the low online inference latency of CRMs. Specifically, REKI consists of two stages: (1) knowledge extraction and (2) knowledge integration. To avoid the compositional gap for knowledge extraction, we propose factorization prompting to break down the complex preference reasoning problem into several vital factors, and then generate the reasoning knowledge on users and the factual knowledge on items. With factorization prompting, we devise two distinct approaches, individual knowledge extraction and collective knowledge extraction , to ensure efficient knowledge extraction for various scales of recommendation scenarios. The individual knowledge extraction is designed for small-scale scenarios with limited users and items, where LLMs are employed to generate individual knowledge for each user and item. Collective knowledge extraction is designed for large-scale scenarios with massive users and items ( e.g. , million- or even billion-level), where we first cluster items and users and then generate collective knowledge for each cluster, significantly reducing offline resource consumption. Then, the knowledge integration stage transforms the generated knowledge into augmented vectors in the recommendation space. In this stage, we propose hybridized expert integration network (HEIN) to reduce dimensionality and ensemble multiple experts for robust knowledge learning, thus increasing the reliability and availability of the generated knowledge. Next, the recommendation model incorporates the augmented vectors with original in-domain features for optimization, combining both the in-domain recommendation knowledge and the open-world knowledge. Our main contributions can be summarized as follows: \u00b7 We present an efficient and deployable open-world recommender system, REKI, which bridges the gap between the in-domain knowledge recommendation and the open-world knowledge from LLMs. To the best of our knowledge, this is the first practical solution that introduces logical reasoning with LLMs for user preferences to the recommendation domain. \u00b7 REKI transforms the open-world knowledge into dense vectors in recommendation space, compatible with any recommendation task and models. We also release the code of REKI and the generated textual knowledge from LLMs 2 to facilitate future research. \u00b7 REKI is industrial-friendly and deployable for practical applications. We devise collective knowledge extraction for large-scale scenarios to achieve offline resource efficiency. The knowledge augmentation can be preprocessed and prestored for fast training and inference, avoiding the large inference latency when adopting LLMs to RSs. Now, REKI has been deployed to Huawei's news and music recommendation platforms and gained a 7% and 1.99% improvement in the online A/B test. This is one of the first successful attempts at deploying LLM-based recommender to real-world applications. Extensive experiments conducted on public datasets also show that REKI significantly outperforms the state-of-the-art models and is compatible with various recommendation algorithms. We believe that REKI not only sheds light on a way to inject the knowledge from LLMs into the recommendation models, but also provides a practical framework for open-world recommender systems in large-scale applications.", "2 RELATED WORK": "This section reviews studies on recent advances in recommendation with pretrained language models (PLMs).", "2.1 PLM as Recommender Itself": "The emergence of pretrained language models (PLMs) has brought tremendous success in Natural Language Processing (NLP), and PLMs also show great potential in other domains like recommendations [76, 84]. One promising direction is to leverage PLMs as the primary driver of recommendations, allowing PLMs to directly accomplish the recommendation tasks [42, 84, 87]. For example, LMRecSys [91] is one of the earliest attempts to transfer the session-based recommendation task into prompts and evaluate the performance of BERT [37] and GPT-2 [68] in movie recommendation. Later, P5 [16] and M6-Rec [8] finetune pretrained language models (T5 [69] or M6 [48]) by converting multiple recommendation tasks to natural language sequences to incorporate knowledge and semantics inside the training corpora for personalization. Similarly, RecFormer [41] models user preferences and item features as language representations for the sequential recommendation. In this earlier stage, the sizes of the language models for recommendation are relatively small ( e.g. , under billions of parameters), and finetuning is usually involved for better performance. With the scaling of the model size and corpus volume, especially with the emergence of ChatGPT [60], LLMs have shown uncanny capability in a wide variety of tasks [13]. One of the unique abilities of LLMs is reasoning, which emerges only when the model size surpasses a certain threshold. Zero-shot learning or in-context learning is widely used since finetuning LLMs requires lots of resources. Several studies apply LLMs as recommenders and achieve some preliminary results [9, 15, 27, 36, 51, 79]. For instance, ChatRec [15] employs LLMs as a recommender system interface for conversational multi-round recommendations. Some researchers empirically study conversational recommendation tasks using representative large language models in a zero-shot setting [24]. Liu et al . [51] investigate whether ChatGPT can serve as a recommender with task-specific prompts and report the zero-shot performance. Hou et al . [27] further report the zero-shot ranking performance of LLMs with historical interaction data. However, directly using LLMs as recommenders generally falls behind state-of-the-art recommendation algorithms, implying the importance of domain knowledge and collaborative signals for recommendation tasks [9, 36, 47]. Therefore, there are also methods exploring the incorporation of recommendation collaborative signals into LLMs through parameter-efficient finetuning approaches [52]. For example, TALLRec [3] finetunes LLaMA-7B model [74] with a LoRA [28] architecture on recommendation data. Another study [22] finetunes an Open-AI ada model 3 on recommendation data but finds its performance lag behind utilizing the embedding of LLM for similarity matching or as an initialization for recommendation model. Those methods achieve good recommendation performance by finetuning LLMs on recommendation data. Nevertheless, there are a large number of users and items in industrial recommendation scenarios, and the distribution of user behavior tends to change over time. Obtaining satisfactory results on such data usually entails continuous finetuning of LLMs on regularly updated and large amounts of data, leading to significant time consumption and substantial GPU resource usage. Moreover, these preliminary studies mainly overlook the inference latency during deployment, offline resource consumption, and compositional gap issues of LLMs. In this work, we propose factorization prompting to extract both user and item knowledge from LLMs, alleviating the compositional gap. For large-scale scenarios with massive users and items, we design collective knowledge extraction, which first conducts the user and item clustering and then extracts knowledge for those clusters. This approach significantly reduces the number of knowledge generation, thus improving offline resource efficiency. The extracted knowledge is then adapted to the recommendation domain as augmented representation vectors, which can be prestored for fast training and inference.", "2.2 PLM as Component of Traditional Recommender": "Unlike the above approaches, where PLMs are used as the primary driver of recommendations, another category of work leverages PLMs as an auxiliary component in traditional RSs. Here, PLMs are usually adopted to encode the textual features ( e.g. , item descriptions, user reviews) or provide extra knowledge for classical recommendations for better user or item representations [11, 25, 26, 67]. For example, U-BERT [67] leverages the embeddings of user review texts encoded by BERT [37] to complement user representations. ZEREC [11] incorporates traditional recommender systems with PLMs to generalize from a single training dataset to other unseen testing domains. UniSRec [26] utilizes BERT [37] to encode user behaviors and therefore learns universal sequence representations for downstream recommendation. Built upon UniSRec, VQ-Rec [25] further adopts vector quantization techniques to map language embeddings into discrete codes, balancing the semantic knowledge and domain features. The above methods usually use small PLMs ( e.g. , BERT-base with 110M parameters) to convert texts to dense vectors, the semantic information of which could be limited and thus fail to provide strong assistance for traditional recommender systems. Another line of work adopts large language models with billion-level parameters, focusing on encoding or prompting external open-world knowledge from LLMs. For instance, some researchers propose S&R Multi-Domain Foundation model [17], which finetunes ChatGLM2-6B [12] to extract domain invariant features for promoting search and recommendation performance in cold-start scenarios. Some researchers also find that LLMs provide competitive recommendation performance for pure language-based preferences in the near cold-start case in comparison to item-based CF methods [71]. LLM-Rec [56] investigates various prompting strategies to generate augmented input text from GPT-3 ( text-davinci-003 ), which improves the recommendation capabilities. Another work [58] utilizes InstructGPT (175B) [61] for authoring synthetic narrative queries from user-item interactions and train retrieval models for narrative-driven recommendations on synthetic data. TagGPT [40] provides a system-level solution of tag extraction and multi-modal tagging in a zero-shot fashion equipped with GPT-3.5 ( gpt-3.5-turbo ). Although these methods have made early attempts at utilizing LLMs, they either simply use LLMs as fixed encoders to convert original texts into dense vectors without generating additional textual knowledge, or do not make specific designs to incorporate the generated textual information into traditional recommender systems. This may lead to the instability of RS due to the noise or high dimension of LLM embeddings. In this paper, we propose factorization prompting to break down the complex preference reasoning problem into several vital factors to generate open-world knowledge, avoiding the compositional gap. Two knowledge generation approaches, individual and collective knowledge extraction, are introduced to generate knowledge effectively under different scales of recommendation scenarios. Then, we devise a hybridized expert integration network to reduce dimensionality and ensemble multiple experts for robust knowledge learning, thus increasing the reliability and availability of the generated knowledge. The above two stages can be preprocessed and prestored for fast training and inference, which avoids the significant inference latency when using LLMs in RSs. Now, our proposed REKI has been deployed in Huawei news and music recommendation platforms and has gained significant improvements.", "3 PRELIMINARIES": "In this section, we formulate the recommendation task and introduce the notations. The core task of recommendation is to precisely estimate the user preference towards each candidate item given a certain context, which can be formulated as a classification problem over multi-field categorical data. The dataset is denoted as D = {( \ud835\udc65 1 , \ud835\udc66 1 , \ud835\udc50 1 ) , . . . , ( \ud835\udc65 \ud835\udc56 , \ud835\udc66 \ud835\udc56 , \ud835\udc50 \ud835\udc56 ) , . . . , ( \ud835\udc65 \ud835\udc5b , \ud835\udc66 \ud835\udc5b , \ud835\udc50 \ud835\udc5b )} , where \ud835\udc65 \ud835\udc56 represents the categorical features for the \ud835\udc56 -th instance, \ud835\udc66 \ud835\udc56 denotes the corresponding label, and \ud835\udc50 \ud835\udc56 is the context. Usually, \ud835\udc65 \ud835\udc56 contains sparse one-hot vectors from multiple fields, such as item ID and genre. We can denote the feature of the \ud835\udc56 -th instance as \ud835\udc65 \ud835\udc56 = [ \ud835\udc65 \ud835\udc56, 1 , \ud835\udc65 \ud835\udc56, 2 , . . . , \ud835\udc65 \ud835\udc56,\ud835\udc39 ] with \ud835\udc39 being the number of field and \ud835\udc65 \ud835\udc56,\ud835\udc58 , \ud835\udc58 = 1 , . . . , \ud835\udc39 being the feature of the corresponding field. Recommendation models usually aim to learn a function \ud835\udc53 (\u00b7) with parameters \ud835\udf03 that can accurately predict \u02c6 \ud835\udc66 \ud835\udc56 = \ud835\udc53 ( \ud835\udc65 \ud835\udc56 ; \ud835\udf03 ) for each sample \ud835\udc65 \ud835\udc56 . In practice, industrial recommender systems are confronted with massive users and Knowledge Extraction Prompt Generator Scenario-specific Factors Individual Knowledge Extraction Collective Knowledge Extraction User Group Item Group User Prompt Item Prompt Collective User Information Item Information Knowledge Integration User, Item, context features CRM Efficient Clustering User Prompt Item Prompt Individual LLM Knoweldge Encoder Movie Genre Theme Time ... User Representation Item Representation User Augmented Vector Item Augmented Vector Gating Gating User Expert Item Expert Shared Expert Hybridized Expert Intergration Network (HEIN) items. Thus, their recommendation is usually divided into multiple stages, i.e. , candidate generation, ranking, and reranking [53], where different models are used to narrow down the relevant items. However, these classical recommendation models are typically trained on a specific recommendation dataset ( i.e. , a closed-loop system), overlooking the potential benefits of accessing open-world knowledge.", "4 METHODOLOGY": "We first provide an overview of our proposed Open-World Recommendation Framework with Efficient and Deployable Knowledge Augmented from Large Language Models, dubbed REKI, and then elaborate on the details of each component.", "4.1 Overview": "As shown in Figure 1, we design REKI to extract open-world knowledge from LLMs and incorporate it into RSs. This framework is model-agnostic and consists of the following two stages: Knowledge Extraction Stage leverages our designed factorization prompting to extract recommendation-relevant knowledge from LLMs. We first decompose the complex reasoning tasks by identifying major scenario-specific factors that determine user preferences and item characteristics e.g. , theme, genre, and, director. Then, according to different scales of recommendation scenarios, we devise two knowledge generation approaches with those factors. The individual knowledge extraction for the small scenarios utilizes LLMs to generate individual knowledge about the scenario-specific factors for each item and user. In collective knowledge extraction for large scenarios, we first cluster the user and item into groups, respectively, and then require LLMs to generate shared knowledge for each group according to scenario-specific factors. Thus, we can obtain open-world knowledge about the user and item beyond the original recommendation dataset. Finally, textual knowledge obtained from LLMs is encoded into dense representations by a knowledge encoder. Knowledge Integration Stage converts open-world knowledge into compact and relevant representations suitable for recommendations, bridging the gap between LLMs and RSs. First, We devise a hybridized expert integration network (HEIN) to transform the hign-dimension knowledge representations from the semantic space 4 to the recommendation space. In this way, we obtain the user augmented vector for user preferences and the item augmented vector for each candidate item. Next, the user and item augmented vectors are integrated into an existing conventional recommendation model (CRM), enabling it to leverage both domain knowledge and open-world knowledge during the recommendation. The knowledge extraction stage is conducted through preprocessing. The hybridized expert integration network and CRM are jointly trained in an end-to-end manner.", "4.2 Knowledge Extraction": "As the model size scales up, LLMs can encode a vast array of world knowledge and have shown emergent behaviors such as the reasoning ability [29, 66]. This opens up new possibilities for incorporating user-side knowledge of user preferences and factual knowledge for candidate items in recommender systems. However, it is non-trivial to extract those two kinds of knowledge from LLMs due to the following three challenges. Firstly, when facing tasks that require the reasoning ability like user preference reasoning [65], LLMs often suffer from the compositional gap where the model fails at generating the correct answer to the compositional question but can correctly answer all its sub-questions. Users' clicks on items are motivated by multiple key aspects, and users' interests are diverse and multifaceted, which involve multiple reasoning steps. To this end, LLMs may not be able to produce accurate reasoning for user preference knowledge directly. Expecting LLMs to provide precise recommendations in one step as in previous works [15, 27, 51] might be overly ambitious. Secondly, as for factual knowledge about items, LLMs contain massive world knowledge, yet not all of it is useful for recommendation. When the request to an LLM is too general, the generated factual knowledge may be correct but useless, as it may not align with the inferred user preferences. For example, an LLM may infer that a user prefers highly acclaimed movies that have received multiple awards, while the generated factual knowledge is merely about the storyline of the target movie. This mismatch between user-side knowledge and item factual knowledge can limit the LLM-enhanced performance of RSs. Lastly, more importantly, the majority of recommendation systems in the industry may face the challenge of dealing with billions of users and items. Given the enormous parameter size of LLMs, the time and resources for generating knowledge can be considerable. Considering the frequent model updates required by industrial recommendation systems and resource constraints, generating knowledge for each user and item becomes impractical in large-scale recommendation scenarios. Additionally, fine-tuning LLMs often demands a considerable amount of time. During this period, the online distribution of user behavior may undergo some changes. This temporal misalignment could result in a situation where a fine-tuned LLM may no longer be suitable for the current online data distribution, leading to inaccuracy recommendations. Therefore, inspired by the success of Factorization Machines [70] in RSs, we design factorization prompting to first explicitly \"factorize\" the user preference and item knowledge into several major factors for effectively extracting the open-world knowledge from LLMs. Then, with the factorized factors incorporated into the prompt design, the complex reasoning and knowledge extraction problem can be broken down into simpler subproblems for each factor, thus alleviating the compositional gap of LLMs. In addition, we devise distinct knowledge generation approaches for recommendation scenarios of different scales. For relatively smaller scenarios, we implement individual knowledge extraction, where the item prompt and user prompt are designed for each user and item, facilitating the generation of individual knowledge. In contrast, we introduce collective knowledge extraction for larger scenarios with a considerable number of users and items, which involves clustering users and items first and then prompting collective knowledge for each cluster. Finally, textual knowledge obtained from LLMs is encoded into dense representations by a knowledge encoder ( i.e. , open-sourced language models like BERT [37]). 4.2.1 Scenario-specific Factors. The factorization prompting first extracts scenario-specific factors that comprehensively cover the critical aspects for both user preferences and item characteristics. Those factors might vary for different recommendation scenarios. To determine the factors for different scenarios, we rely on a combination of interactive collaboration with LLMs and expert opinions. For example, in movie recommendation, given a prompt ' List the important factors or features that determine whether a user will be interested in a movie , ' LLMs can provide some potential factors. Then, we involve human experts to confirm and refine the outputs to acquire the final scenario-specific factors for movie recommendation - including genre, actors, directors, theme, mood, production quality, and critical acclaim . Similarly, in news recommendation, we may obtain factors like topic, source, region, style, freshness, clarity, and impact . The collaborative process between LLMs and experts ensures that our chosen factors encompass the critical dimensions of user preference and item characteristics for each scenario. Note that the specification of these factors is required only once for each scenario and it does not demand much domain expertise with the aid of LLMs. This shows that the proposed method can be easily generalized to different scenarios with little human intervention and manual effort. After obtaining the scenario-specific factors, we have designed individual knowledge extraction and collective knowledge extraction for small and large-scale scenarios, which are detailed in Section 4.2.2 and Section 4.2.3, respectively. 4.2.2 Individual Knowledge Extraction. Our first prompt engineering approach - individual knowledge extraction - is designed explicitly for smaller scenarios where the number of users and items is not substantial. In such scenarios, it is feasible to employ LLMs to generate individual knowledge for each user and item. Specifically, we propose to apply the LLM as a preference reasoner to infer user preferences, and a knowledge provider to acquire external factual knowledge for candidate items. Therefore, we design two types of prompts accordingly: user prompt and item prompt, as illustrated in Figure 2(a) and Figure 2(b). User prompt is constructed with the user's profile description, behavior history, and scenario-specific factors. Figure 2(a) shows an example of the prompt and real response from the LLM, where the user profile description and behavior history provide the LLM with the necessary context and user-specific information to understand the user's preferences. Scenario-specific factors can instruct the LLM to analyze user preference from different facets and allow the LLM to recall the relevant knowledge more effectively and comprehensively. For example, in the factor of genre, the LLM infers user preference for genres such as thriller, comedy, and animation based on user's positive ratings on thriller movies like What Lies Beneath , Scream , as well as comedy animations like Toy Story and Aladdin . With the designed prompt, LLM can successfully analyze the user's preferences toward corresponding factors, which is beneficial for recommendations. Item prompt is designed to fill the knowledge gap between the candidate items and the generated user-side knowledge. Since the dataset in RS may lack relevant knowledge about scenario-specific factors from items, we need to extract corresponding knowledge from LLM to align the generated user and item knowledge. As illustrated in Figure 2(b), an item prompt consists of two parts - the target item description and the scenario-specific factors. This", "Individual:\u00a0Item Prompt": "Templete: Given a user who is {{ user description }}, this user's movie viewing history over time is listed below: {{ user history }}. Analyze user's preferences (consider factors like {{ scenario-specific factors }}). Provide clear explanations based on details from the user's viewing history and other pertinent factors. Content: User description :\u00a0female,\u00a025-34, sales/marketing User history : What Lies Beneath (2000), 5 star; Ghost (1990), 3 star; Aladdin (1992), 4 star; Toy Story (1995), 5 star; Scream (1996), 5 star; The Silencce of the Lambs(1991), 5 stars ... Scenario-specific factors :\u00a0genre, director, actors, time period, country, character, plot/theme, mood/tone, critical acclaim ... Response: Genre: The user enjoys a mix of different genres, including drama, thriller, comedy, and animation . Plot & Character: She has given high ratings to What Lies Beneath, Toy Story ... suggesting she enjoys movies with strong plot, character development ... Critical Acclaim & Award: Many of the movies, such as Toy Story and The Silence of the Lambs, are critically acclaimed and have won awards ... ... Templete: Introduce movie {{ item description }} and describe its attributes precisely (including but not limited to {{ scenario-specific factors }}). Content: Item description : Roman Holiday Scenario-specific factors :\u00a0genre, director, actors, time period, country, character, plot/theme, mood/tone, critical acclaim/award ... Response: Genre & Time: It is a classic romantic comedy film in 1953 . Director & Actors: It was directed by William Wyler and stars Audrey Hepburn ... Tone & Mood: It has a light and playful tone throughout, with a touch of melancholy at end... Critical Acclaim & Award: It was a critical and commercial success ... ...", "Individual: User Prompt": "", "Collective: User/Item Prompt": "Templete: Given a set of movies consisting of\u00a0{{ set description }} describe the common characteristics of these movies\u00a0 (including but not limited to {{ scenario-specific factors }}). Content: Set description: Johns (1996), The Fabulous Baker Boys (1989), Lolita (1997), Nell (1994), The Talented Mr. Ripley (1999),\u00a0Eyes Wide Shut (1999),\u00a0Velvet Goldmine (1998)\u00a0... Scenario-specific factors :\u00a0genre, director, actors, time period, country, character, plot/theme, mood/tone, critical acclaim/award ... Response: Genre: ... genre diversity, like drama, romance, mystery ... Theme: They\u00a0explore complex and often controversial aspects of human relationships ... Character: Many of these films feature complex characters dealing with moral dilemmas ... Critical Acclaim & Award: Many\u00a0of films, such as Eyes Wide Shut, have received critical acclaim ... ... (a) (b) (c) prompt can guide LLM in compensating for the missing knowledge within the dataset. For instance, in Figure 2(b), LLM supplements Roman Holiday with \"a light and playful tone\" , which is rarely recorded in the datasets. In this way, LLM provides external knowledge that aligns with user preferences, allowing for more accurate and personalized recommendations. By combining the two kinds of prompts, we enable the LLM to act as both a preference reasoner and a knowledge provider, thereby extracting individual user and item knowledge from LLMs. We refer to the generated textual response with the user and item prompt as the user knowledge and the item knowledge , respectively. This approach of extracting knowledge for each user and item proves effective in scenarios characterized by a relatively limited number of users and items. However, owing to the substantial time and resource consumption involved in generating knowledge through LLMs, the feasibility of extracting knowledge for each user and item becomes questionable when we confront larger recommendation scenarios housing a significant number of users and items ( e.g. , million-level or even billion-level). 4.2.3 Collective Knowledge Extraction. For larger recommendation scenarios with a massive number of users and items, we introduce collective knowledge extraction. This approach first conducts user and item clustering, with each cluster represented by a set of items. Subsequently, we have formulated a unified prompt designed for both the user and item to extract knowledge for each cluster in Figure 2(c). Regulating cluster size through clustering algorithms affords us the flexibility to manage the number of clusters, thereby controlling the time and resource consumption during the knowledge generation. Conventional clustering methods, such as k-means [1] and standard hierarchical clustering [57], often face challenges in handling data within large-scale recommendation scenarios. Firstly, they exhibit high complexity when dealing with large-scale and high-dimension data. Secondly, they lack scalability, struggling to accommodate the continuous arrival of new users or items in dynamic recommendation scenarios. To address these limitations, we choose a streaming hierarchical clustering approach, StreaKHC [21] algorithm to cluster users and items. Leveraging a scalable point-set kernel, StreaKHC is capable of handling large volumes of streaming data in recommendation scenarios. It employs a top-down search strategy to recursively add a new point to the most similar child node, resulting in a time complexity of \ud835\udc42 ( \ud835\udc5b ) where \ud835\udc5b is the number of points in the node. Additionally, as a hierarchical clustering algorithm, it forms a tree structure, allowing us to change the number of clusters without regenerating the entire tree. With this clustering algorithm, we first perform user and item clustering. The clustered results are then processed and incorporated into the user/item prompts, which are fed into the LLM for knowledge extraction. In the following discussion, we will first introduce item clustering and prompts, followed by user clustering and prompts. Item clustering is a relatively straightforward process. Initially, we pre-train embeddings for all items, item attributes, and user profiles using traditional recommendation models such as DIN [95]. Subsequently, these item and attribute embeddings are then fed into the StreaHKC algorithm for clustering, resulting in multiple clusters. Each item cluster, denoted as \ud835\udc50 \ud835\udc4e \ud835\udc56 , comprises multiple items represented as \ud835\udc50 \ud835\udc4e \ud835\udc56 = { \ud835\udc4e 1 , . . . , \ud835\udc4e \ud835\udc58 , . . . , \ud835\udc4e \ud835\udc5a \ud835\udc56 } , where \ud835\udc4e \ud835\udc58 represents an item, and \ud835\udc5a \ud835\udc56 denotes the number of items within that cluster. The maximum value of \ud835\udc5a can be controlled indirectly through parameters in the StreaHKC algorithm, so we can regulate the number of clusters. The StreaHKC algorithm forms a tree structure. Hence, when some new items arrive, we conduct incremental training to obtain embeddings for these new items. Then, we can efficiently introduce new nodes into this tree and obtain new clusters through a splitting algorithm. Item prompt in collective knowledge extraction is designed for extracting the commonalities within a set of items, i.e. , collective knowledge. Prompt for item clusters, as depicted in Figure 2(c), comprises set description and scenario-specific factors. Here, the set description refers to each cluster obtained from item clustering, represented by a set of items. Similar to prompts in individual knowledge extraction, the item prompt requires scenario-specific factors to guide LLMs in analyzing the shared characteristics of this set of items. For instance, in Figure 2(c), regarding themes, the LLM can infer that this set of movies \"explores complex and often contradictory aspects of human relationships.\" In terms of characters, \"these movies often feature complex characters.\" Through this prompt, LLMs can effectively reason and extract the commonalities within a set of items, i.e. , their collective knowledge. Under the guidance of this prompt, LLM can efficiently generate the item knowledge comprising multiple facets of an item cluster. User clustering is similar to item clustering, and it utilizes the same embeddings pre-trained mentioned in the item clustering phase. As users exhibit complex and dynamic behavior, we represent users using their click history and profile features. Specifically, for a user \ud835\udc62 \ud835\udc58 with click history \u210e \ud835\udc58 = { \ud835\udc4e 1 , . . . , \ud835\udc4e \ud835\udc58 , . . . , \ud835\udc4e \ud835\udc60 \ud835\udc56 } of length \ud835\udc60 \ud835\udc56 , the average of embeddings for all items recently clicked by the user serves as a part of user representation, while the embedding of user profile features forms the other part. After inputting all the user representations into the StreaKHC algorithm, we obtain each user cluster, \ud835\udc50 \ud835\udc62 \ud835\udc56 = { \ud835\udc62 1 , . . . , \ud835\udc62 \ud835\udc58 , . . . , \ud835\udc62 \ud835\udc61 \ud835\udc56 } , where \ud835\udc62 \ud835\udc58 represents a user and \ud835\udc61 \ud835\udc56 is the number of users in that cluster. User prompt becomes more intricate than item prompt, because it is challenging for untuned LLMs to comprehend users typically identified by IDs without corresponding textual descriptions in recommendation. To this end, we typically represent the user by his or her recent historical behaviors, i.e. , a set of items, as we do in the individual knowledge extraction. Here, for a user cluster, we also represent it by a set of items, i.e. , the common historical items from all users within this cluster, enabling LLMs to comprehend the cluster for knowledge reasoning and extraction. To achieve this, we extract common historical items from all users within each cluster \ud835\udc50 \ud835\udc62 \ud835\udc56 , forming a new representation for the user cluster \u02c6 \ud835\udc50 \ud835\udc62 \ud835\udc56 . Mathematically, we first obtain the number of times \ud835\udc41 \ud835\udc58 that each item \ud835\udc4e \ud835\udc58 has been clicked by all users in the cluster \ud835\udc50 \ud835\udc62 \ud835\udc56 and then select the top \ud835\udc51 items with the highest \ud835\udc41 \ud835\udc58 values as new representation for the updated user cluster \u02c6 \ud835\udc50 \ud835\udc62 \ud835\udc56 , i.e. , where \ud835\udc41 \ud835\udc58 denotes the number of user in cluster \ud835\udc50 \ud835\udc62 \ud835\udc56 that have been clicked item \ud835\udc4e \ud835\udc58 , \u210e \ud835\udc57 is historical clicked item set of user \ud835\udc62 \ud835\udc57 , 1 \u210e \ud835\udc57 ( \ud835\udc4e \ud835\udc58 ) represents whether the item \ud835\udc4e \ud835\udc58 appears in the click history \u210e \ud835\udc57 , and I is the whole item set. Function Rank ( \ud835\udc4e \ud835\udc58 ) denotes the position of \ud835\udc4e \ud835\udc58 when all the items \ud835\udc4e \ud835\udc58 \u2208 I are arranged by descending order of \ud835\udc41 \ud835\udc58 . In this way, we have successfully represented a user cluster with a set of items. In other words, the format of our user cluster is now identical to that of the previous item cluster. Therefore, we can extract user collective knowledge from the user cluster using a similar prompt and process with the item prompt, as illustrated in Figure 2(c). 4.2.4 Knowledge Encoder. The knowledge generated by LLMs is usually in the form of text, which cannot be directly leveraged by traditional RSs that typically process categorical features. Therefore, we design the knowledge encoder module to encode the generated textual knowledge into dense vectors and aggregate them effectively. To harness the potential of textual knowledge generated by LLMs, we employ encoders like BERT [37] and ChatGLM [12] to obtain the encodings for each token within the text. Then, we require an aggregation process that combines each token to generate the user representation \ud835\udc5f \ud835\udc62 \ud835\udc56 \u2208 R \ud835\udc5a and the item representation \ud835\udc5f \ud835\udf04 \ud835\udc56 \u2208 R \ud835\udc5a of size \ud835\udc5a as follows where \ud835\udc58\ud835\udc59\ud835\udc54 \ud835\udc62 \ud835\udc56 and \ud835\udc58\ud835\udc59\ud835\udc54 \ud835\udf04 \ud835\udc56 denote the user knowledge and item knowledge generated by LLMs of the \ud835\udc56 -th instance in the dataset. Here, various aggregation functions can be employed, such as the representation of the [CLS] token and average pooling. In practice, we primarily adopt average pooling. Note that the knowledge encoder is devised for situations where we only have access to the textual outputs of LLMs via API calls. The separate knowledge encoder can be eliminated if we adopt open-sourced LLMs ( e.g. , LLaMA [74]) and the dense hidden states are available.", "4.3 Knowledge Integration": "The knowledge generated by LLMs presents new challenges in harnessing its potential to assist recommendation models: 1) The knowledge representations obtained in the preceding stage are typically large dense vectors (such as 4096 when taking ChatGLM [12] as the encoder), and they lie in a semantic space which is significantly different from the recommendation space. Utilizing such vectors directly in CRMs may lead to training instability. Therefore, it is pivotal to effectively transform the output knowledge to align with the recommendation space without loss of information or misinterpretation for the quality of recommendations. 2) Another challenge is that not all knowledge generated by LLMs is reliable. LLMs may suffer from hallucination problem [33], resulting in noise or unreliable information. Without careful handling, there is a risk that this noise information may be introduced into the CRMs, thereby causing a decline in recommendation accuracy. To address these challenges, we have devised hybridized expert integration network (HEIN). The HEIN converts dense vectors from the semantic space to the recommendation space. It introduces multiple sets of experts to perform dimensionality reduction and space transformation on knowledge representation \ud835\udc5f \ud835\udc5d \ud835\udc56 and \ud835\udc5f \ud835\udc59 \ud835\udc56 . Using gating mechanisms to combine the results from multiple experts enhances the stability and robustness of the transformation, thereby reducing the impact of noise information on CRMs. Thus, HEIN increases the reliability and availability of generated knowledge and bridges the gap between LLMs and RSs. Then, the output of HEIN will be integrated into a CRM as additional input features, enabling it to leverage open-world knowledge. 4.3.1 Hybridized Expert Integration Network. To effectively transform and compact the attained aggregated representations from the semantic space to the recommendation space, we propose a hybridized expert integration network (HEIN). The aggregated representations capture diverse knowledge from multiple aspects, so we employ a structure that mixes shared and dedicated experts, inspired by the Mixture of Experts (MoE) [31] approach. This allows us to fuse knowledge from different facets and benefit from the inherent robustness offered by multiple experts. In particular, to fully exploit the shared information of the user representation and the item representation, we have designed both shared experts and dedicated experts for each kind of representation. The shared experts capture the common aspects, such as shared features, patterns, or concepts, that are relevant to both user and item knowledge. User and item representations also have their dedicated sets of experts to capture the unique characteristics specific to the user or item knowledge. Mathematically, denote S \ud835\udc60 , S \ud835\udc62 , and S \ud835\udf04 as the sets of shared experts and dedicated experts for user and item knowledge with the expert number of \ud835\udc5b \ud835\udc60 , \ud835\udc5b \ud835\udc62 and \ud835\udc5b \ud835\udf04 . The output is the user augmented vector \u02c6 \ud835\udc5f \ud835\udc62 \ud835\udc56 \u2208 R \ud835\udc5e and the item augmented vector \u02c6 \ud835\udc5f \ud835\udf04 \ud835\udc56 \u2208 R \ud835\udc5e of size \ud835\udc5e ( \ud835\udc5e is much less than the original dimension \ud835\udc5a ), which are calculated as follows where \ud835\udc54 \ud835\udc62 (\u00b7) and \ud835\udc54 \ud835\udf04 (\u00b7) are the gating networks for user and item representations, and their outputs \ud835\udefc \ud835\udc62 \ud835\udc56 and \ud835\udefc \ud835\udf04 \ud835\udc56 are of size \ud835\udc5b \ud835\udc60 + \ud835\udc5b \ud835\udc62 and \ud835\udc5b \ud835\udc60 + \ud835\udc5b \ud835\udf04 . Here \ud835\udc52 (\u00b7) denotes the expert network, and \ud835\udefc \ud835\udc62 \ud835\udc56,\ud835\udc52 and \ud835\udefc \ud835\udf04 \ud835\udc56,\ud835\udc52 are the weights of expert \ud835\udc52 (\u00b7) generated by the gating network for user and item, respectively. Here, each expert network \ud835\udc52 (\u00b7) is designed as a Multi-Layer Perceptron (MLP), facilitating dimensionality reduction and space transformation. Empirical experiments show that about 5 experts are sufficient to generate a satisfying performance 5 . 4.3.2 Knowledge Utilization. Once we have obtained the user augmented vector and the item augmented vector, we can then incorporate them into backbone recommendation models. This section explores a straightforward approach where these augmented vectors are directly treated as additional input features . Specifically, we use them as additional feature fields in recommendation models, allowing them to interact with other features explicitly. During training, the HEIN module is jointly optimized with the backbone model to ensure the transformation process adapts to the current data distribution. Generally, REKI can be formulated as which is enhanced by the the user augmented vector \u02c6 \ud835\udc5f \ud835\udc62 \ud835\udc56 and the item augmented vector \u02c6 \ud835\udc5f \ud835\udf04 \ud835\udc56 . Importantly, REKI only modifies the input of the backbone model and is independent of the design and loss function of the backbone model, so it is flexible and compatible with various backbone model designs. Furthermore, it can be extended to various recommendation tasks, such as sequential recommendation and direct recommendation, by simply adding two augmented vectors in the input. By incorporating the knowledge augmented vectors, REKI combines both the open-world knowledge and the recommendation domain knowledge in a unified manner to provide more informed and personalized recommendations.", "4.4 Speed-up Approaches for Online Inference": "Our proposed REKI framework adopts LLMs to generate knowledge for users and items. Due to the immense scale of the model parameters, the inference of LLMs takes extensive computation time and resources, and the inference time may not meet the latency requirement in real-world recommender systems with large user and item sets. To address this, we employ an acceleration strategy to prestore knowledge representations \ud835\udc5f \ud835\udc62 \ud835\udc56 and \ud835\udc5f \ud835\udf04 \ud835\udc56 generated by the knowledge encoder or the LLM into a database. As such, we only use the LLM and knowledge encoder once before training backbone models. During the training and inference of the backbone model, relevant representations are retrieved from the database. Besides, the efficiency of offline knowledge generation with LLM can further be enhanced via quantization or hardware acceleration techniques. If we have stricter requirements for inference time or storage efficiency, we can detach the HEIN module from the model after training and further prestore augmented vectors i.e. , \u02c6 \ud835\udc5f \ud835\udc62 \ud835\udc56 and \u02c6 \ud835\udc5f \ud835\udf04 \ud835\udc56 , for inference. The dimension of the augmented vectors ( e.g. , 32) is usually much smaller than that of the knowledge representations ( e.g. , 4096), which improves the storage efficiency. Additionally, prestoring the augmented vectors reduces the inference time to nearly the same as the original backbone model, and we have provided experimental verification in Section 5.4.2. In particular, assume the inference time complexity of the backbone model is \ud835\udc42 ( \ud835\udc53 ( \ud835\udc5b,\ud835\udc5a )) , where \ud835\udc5b is the number of fields and \ud835\udc5a is the embedding size. The polynomial function \ud835\udc53 ( \ud835\udc5b,\ud835\udc5a ) varies depending on different backbone models. With REKI, the inference time complexity is \ud835\udc42 ( \ud835\udc53 ( \ud835\udc5b + 2 , \ud835\udc5a )) = \ud835\udc42 ( \ud835\udc53 ( \ud835\udc5b,\ud835\udc5a )) , which is equivalent to the complexity of the original model. Since item features are relatively fixed and do not change frequently, it is natural and feasible to prestore the item knowledge for further use. Moreover, user behaviors evolve over time, making it challenging for LLMs to provide real-time user-side knowledge about behaviors. However, considering that long-term user preferences are relatively stable, and the backbone model already emphasizes modeling recent user behaviors, it is unnecessary to require LLMs to have access to real-time behaviors. Therefore, LLM can infer long-term preferences based on users' long-term behaviors, allowing for conveniently prestoring the generated knowledge without frequent updates. As such, the inference overhead of LLMs can also be significantly reduced. The backbone models can capture ever-changing short-term preferences with timely model updates. This can take better advantage of both LLMs and recommendation models. Similar to common practice for cold start users or items [90], we use default vectors when encountering new users or items at inference time. Subsequently, we will generate the knowledge for those new users and items offline and add them to the database.", "5 EXPERIMENT": "In this section, we aim to address the following research questions (RQs) and comprehensively evaluate the performance and versatility of REKI. \u00b7 RQ1: What improvements can REKI bring to backbone models on different tasks, such as CTR prediction and reranking? \u00b7 RQ2: How does REKI perform compared with other PLM-based baseline methods? \u00b7 RQ3: Does the knowledge from LLM outperform other knowledge-based methods, such as the knowledge graph? \u00b7 RQ4: Does REKI gain performance improvement when deployed online? \u00b7 RQ5: Does the acceleration strategy and collective knowledge extraction enhance the online inference and offline resource efficiency? \u00b7 RQ6: How do the user knowledge and item knowledge generated by the LLM, as well as different knowledge integration approaches, contribute to performance improvement?", "5.1 Experimental Settings": "5.1.1 Datasets. Our experiments are conducted on two public datasets, MovieLens-1M 6 and Amazon-Book 7 . \u00b7 MovieLens-1M contains 1 million ratings provided by 6000 users for 4000 movies. Following the data processing similar to DIN [95], we convert the ratings into binary labels by labeling ratings of 4 and 5 as positive and the rest as negative. The data is split into training and testing sets based on user IDs, with 90% assigned to the training set and 10% to the testing set. The dataset contains user features like age, gender, occupation, and item features like item ID and category. The inputs to the models are user features, user behavior history (the sequence of viewed movies with their ID, category, and corresponding ratings), and target item features. \u00b7 Amazon-Book [59] is the 'Books' category of the Amazon Review Dataset. After filtering out the less-interacted users and items, we remain 11,906 users and 17,332 items with 1,406,582 interactions. The preprocessing is similar to MovieLens-1M, with the difference being the absence of user features. Additionally, ratings of 5 are regarded as positive and the rest as negative. The detailed statistics of the processed datasets are shown in Table 1. 5.1.2 Backbone Models. Because REKI is a model-agnostic framework, various tasks and models in traditional ID-based recommendation can serve as the backbone model, taking as input the knowledge-augmented vectors generated by REKI. Here, we select two crucial recommendation tasks: CTR prediction and reranking , to validate the effectiveness of REKI across various tasks. CTR Prediction. CTR prediction aims to anticipate how likely a user is to click on an item, usually used in the ranking stage of recommendation. We choose 9 representative CTR models as our backbone models, which can be categorized into user behavior models and feature interaction models. User Behavior Models emphasize modeling sequential dependencies of user behaviors. \u00b7 DIN [95] utilizes attention to model user interests dynamically with respect to a certain item. \u00b7 DIEN [94] extends DIN by introducing an interest evolving mechanism to capture the dynamic evolution of user interests over time. Feature Interaction Models focus on modeling feature interactions between different feature fields. \u00b7 DeepFM [18] is a classic CTR model that combines factorization machine (FM) and neural network to capture low-order and high-order feature interactions. \u00b7 xDeepFM [44] leverages the power of both deep network and Compressed Interaction Network to generate feature interactions at the vector-wise level. \u00b7 DCN [80] incorporates cross-network architecture and the DNN model to learn the bounded-degree feature interactions. \u00b7 DCNv2 [81] is an improved framework of DCN which is more practical in large-scale industrial settings. \u00b7 FiBiNet [30] can dynamically learn the feature importance by Squeeze-Excitation network and fine-grained feature interactions by bilinear function. \u00b7 FiGNN [43] converts feature interactions into modeling node interactions on the graph for modeling feature interactions in an explicit way. \u00b7 AutoInt [73] adopts a self-attentive neural network with residual connections to model the feature interactions explicitly. Reranking. Reranking reorders the items from the previous ranking stage by modeling the list-wise influence and derives a list that yields more utility and user satisfaction [53]. We implement the following 4 state-of-the-art reranking models as backbone models. \u00b7 DLCM [2] first applies GRU to encode and rerank the top results. \u00b7 PRM [64] employs self-attention to model the mutual influence between any pair of items and users' preferences. \u00b7 SetRank [63] learns permutation-equivariant representations for the inputted items via self-attention. \u00b7 MIR [86] models the set-to-list interactions between candidate set and history list with personalized long-short term interests. 5.1.3 PLM-based Baselines. As for baselines, we compare REKI with methods that leverage pretrained language model to enhance recommendation, such as P5 [16], UniSRec [26], VQRec [25], TALLRec [3], and LLM2DIN [22]. \u00b7 P5 [16] is a text-to-text paradigm that unifies recommendation tasks and learns different tasks with the same language modeling objective during pretraining. \u00b7 UniSRec [26] designs a universal sequence representation learning approach for sequential recommenders, which introduces contrastive pretraining tasks to effective transfer across scenarios. \u00b7 VQ-Rec [25] uses Vector-Quantized item representations and a text-to-code-to-representation scheme, achieving effective cross-domain and cross-platform sequential recommendation. \u00b7 TALLRec [3] finetunes LLaMa-7B [74] with a LoRA architecture on recommendation tasks and enhances the recommendation capabilities of LLMs in few-shot scenarios. In our experiment, we implement TALLRec with LLaMa-2-7B-chat 8 , since it has better performance and ability of instruction following. \u00b7 LLM2DIN initializes DIN with item embeddings obtained from chatGLM[12], following [22]. We utilize the publicly available code of these three models and adapt the model to the CTR task with necessary minor modifications. We also align the data and features for all the methods to ensure fair comparisons. 5.1.4 Evaluation Metrics. On the CTR prediction task, we employ widely-used AUC (Area under the ROC curve) and LogLoss (binary cross-entropy loss) as evaluation metrics following [18, 73, 81, 95]. A higher AUC value or a lower Logloss value, even by a small margin ( e.g. , 0.001), can be viewed as a significant improvement in CTR prediction performance, as indicated by previous studies [44, 49, 81]. As for reranking task, several widely used metrics, NDCG@K [32] and MAP@K [88], are adopted, following previous work [2, 64, 86]. 5.1.5 Implementation Details. We utilize a widely-used LLM API to generate the knowledge we need. During our experiment, we utilized approximately 33,000 knowledge messages 9 , with an average token length of around 550. And the total cost is about $150, including some preliminary exploratory experiments. In the previous methods, we have designed two approaches for knowledge generation: generating individual knowledge for each item and user (referred to as REKI-I in experiments) and clustering items and users before generating collective knowledge for each cluster (referred to as REKI-C in experiments). Apart from the difference in knowledge generation, the encoders for REKI-I and REKI-C in the experimental section are also different. Unless explicitly stated, REKI-I employs ChatGLM-6B as the encoder, while REKI-C uses BERT as the encoder. We will explain the reason for this choice and compare the performance of two variants in Section 5.4.1. Besides, to compensate for the lack of individual information in REKI-C, we also utilize the same encoder to encode the recent history of each user and the descriptions of items as additional augmented vectors. If not specified, REKI generally represents REKI-I. Then, after the encoder, the average pooling is utilized as the aggregation function in Eq. (2). Each expert in HEIN is implemented as an MLP with a hidden layer size of [128, 32]. The number of experts varies slightly across different backbone models, typically with 1-5 shared experts and 1-6 dedicated experts. We keep the embedding size of the backbone model as 32, and the output layer MLP size as [200, 80]. For the item clustering in REKI-C, we set the maximum number of items in each cluster to 80. For user clustering, the maximum number of users per cluster is set to 100 and 150 on MovieLens-1M and Amazon-Books datasets, respectively. When selecting common historical items for user clustering, we set the threshold \ud835\udc51 in Eq. (1) to 15. Other parameters, such as batch size and learning rate, are determined through grid search to achieve the best results. For fair comparisons, the parameters of the backbone model and the baselines are also tuned to achieve their optimal performance.", "5.2 Effectiveness Comparison": "5.2.1 Improvement over Backbone CTR Prediction Models (RQ1). On the CTR prediction task, we implement our proposed REKI-I and REKI-C upon 9 representative CTR models, and the results are shown in Table 2. From the table, we can have the following observations: First, as a model-agnostic framework, REKI can be applied to various types of baseline models, whether focusing on feature interaction or behavior modeling, and it significantly improves the performance of backbone CTR models on both datasets. For example, when using the feature interaction method FiBiNet as the backbone model on MovieLens-1M, REKI-I achieves a 1.49% increase in AUC and a 2.27% decrease in LogLoss. On the Amazon-Books dataset, applying REKI-I with the behavior modeling approach DIN as the backbone model resulted in a 1.38% improvement in AUC and a 2.77% improvement in Logloss. Those improvements demonstrate the effectiveness of incorporating open-world knowledge from LLMs into RSs. Moreover, with the equipment of REKI-I and REKI-C, the selected 9 representative CTR models on two datasets all achieve an AUC improvement of about 1-1.5%, indicating the universality of the REKI. Second, REKI-I and REKI-C show more remarkable improvement in feature interaction models compared to user behavior models. Taking MovieLens-1M as an example, REKI-I exhibited improvements in the user modeling model DIN, with a 1.24% increase in AUC, while in the feature interaction model FiBiNet, it achieved a 1.49% improvement in AUC. Notably, REKI demonstrated greater enhancements when applied to the feature interaction model. This may be because the knowledge augmented vectors generated by REKI are utilized more effectively by the feature interaction layer than the user behavior modeling layer. The dedicated feature interaction design may better exploit the information contained in the knowledge vectors. Generally, in CTR tasks, REKI-C exhibits a performance decrease of 0.1%-0.2% compared to REKI-I. This is because using REKI-C with clustered collective knowledge may result in some information loss compared to individual knowledge generated for each user. However, relative to REKI-I, REKI-C can significantly reduce the time required for the knowledge generation, training, and inference time of the backbone model, as presented in Section 5.4.1. Therefore, considering the substantial improvement in efficiency, such a level of accuracy loss is acceptable. The detailed efficiency analysis will also be provided in Section 5.4. 5.2.2 Improvement over Backbone Reranking Models (RQ1). Wealso incorporate REKI into the state-of-the-art reranking models. The results on MovieLens-1M and Amazon-Books dataset are presented in Table 3 and Table 4 respectively, from which the following observations can be made: (i) REKI significantly enhances the performance of backbone reranking models. For example, when PRM is employed as the backbone, REKI-I achieves a remarkable increase of 5.71% and 4.71% in MAP@7 and NDCG@7 on the Amazon-Books dataset. This indicates that the open-world knowledge extracted by REKI from LLMs can also be applied to reranking tasks, improving the performance of reranking. (ii) REKI demonstrates more pronounced improvements in methods that do not involve history modeling, such as DLCM, PRM, and SetRank. The user preference knowledge provided by REKI is particularly advantageous for these methods. However, the enhancement is slightly smaller in MIR, which adequately explores the relationship between history and candidates. This may be because REKI's reasoning of user preferences and the reranking model's modeling of historical behavior have some overlap, thus providing a more significant enhancement for methods that lack history modeling. (iii) The performance of REKI-C and REKI-I on reranking tasks is similar to that on CTR tasks, with REKI-I generally performing slightly better than REKI-C. However, this effect is also influenced by the backbone model and may exhibit some exceptions. For example, when using DLCM as the backbone model, REKI-C performs better than REKI-I on the MovieLens-1M dataset. This indicates that REKI-C can achieve performance similar to REKI-I in reranking tasks while efficiently completing knowledge extraction. Table 4. The comparison of REKI and backbone reranking models on Amazon-Books dataset. Note that REKI-I and REKI-C both demonstrate significant improvements over the backbone rerankin models (t-test with \ud835\udc5d -value < 0.05). 5.2.3 Improvement over Baselines (RQ2). Next, we compare REKI with recent baselines using language models or sequence representation pretraining on the CTR task. The results are presented in Table 5. (i) We observe that REKI significantly outperforms models based on pretrained language models. For instance, with DIN as the backbone on Amazon-Books, REKI-I achieves a 0.91% improvement in AUC and a 1.27% improvement in LogLoss over the strongest baseline TALLRec. This validates the effectiveness of incorporation between LLMs and CRMs and the open-world knowledge extracted by REKI. 5.2.4 Improvement over Other Knowledge-based Method (RQ3). In Figure 3, we compare knowledge from LLMs and other sources, such as knowledge graph (KG), with DCNv1, DeepFM, and DIN as the backbone on MovieLens-1M dataset. We utilize a knowledge graph from LODrecsys [10], which maps items of MovieLens-1M to DBPedia entities. Then, the entity embedding of each item is extracted following KTUP [6] and used as an additional feature for the backbone model. The legend ' None ' denotes the backbone model without knowledge enhancement, while ' KG ', ' LLM ', and ' Both ' represent the backbone model enhanced by knowledge from KG, LLM, and both sources, respectively. Note that the individual knowledge generated in REKI-I is utilized for LLM . From Figure 3, we notice that knowledge from both KG and LLM can bring performance improvements; however, the enhancement of KG is much smaller compared to that of LLM. This might be attributed to the fact that KG only \u2217 denotes statistically significant improvement over the second best baselines which is underlined (t-test with \ud835\udc5d -value < 0.05). DCNv1 DeepFM DIN 0.780 0.785 0.790 0.795 0.800 AUC None KG LLM Both possesses manually annotated item-side knowledge while it lacks knowledge for user preferences. According to our analysis in the next subsection 5.5.1, the knowledge for user preference usually contributes more gains compared to item factual knowledge. Furthermore, the simultaneous utilization of the two kinds of knowledge does not exhibit significant improvement over using LLM alone. This suggests that LLM may already encompass the typical knowledge within KG, making knowledge derived solely from LLM sufficient.", "5.3 Deployment & Online A/B Test (RQ4)": "5.3.1 System Design. In this section, we briefly introduce the system design of REKI during the deployment process, as illustrated in Figure 4. The entire system consists of two phases: the offline training phase and the online inference phase. The offline phase involves the knowledge generation of LLM and the training of the conventional recommendation model (CRM). During the knowledge generation, we extract features corresponding to users and items from the feature database based on the requirements of the scenario, which are used to construct prompts for LLM. Then, the LLM is employed to generate respective knowledge for users and items. Subsequently, a smaller language model encodes the generated knowledge into knowledge representations. These representations are stored in the knowledge database for subsequent training and inference of CRM. In the offline phase, we also conduct training for the CRM model. This process involves constructing training samples using the feature database and knowledge database and then completing the training. During the online inference phase, the CRM is deployed to the online system. The deployed CRM, along with data from the feature database and knowledge database, is utilized for inference, generating recommended results for users. User feedback is stored in the feature database for future training iterations. Sample Construction CRM Training Knowledge Generation Feedback Deployed CRM Online Knowledge Database Offline Feature Database LLM 5.3.2 Online A/B Test. To validate the effectiveness of REKI, we conducted two online experiments on Huawei's news and music platforms, respectively. In the news scenario with a relatively small number of users, we extracted knowledge for each user and item, deploying REKI-I. In the music scenario involving a larger number of users and items, we adopt REKI-C for efficient knowledge reasoning and generation. REKI shows significant improvement over the original baselines on the online A/B test for both scenarios. Now, REKI has already been deployed online and serves the main traffic. In Huawei's news scenario, we focus on the task of news recall, for which we have adapted REKI-I into a recall model. During the knowledge reasoning and generation stage, we generate knowledge using Huawei's large language model PanGu[89]. Initially, we provide the LLM with the titles of the 20 most recently clicked news articles by a user. With the coarse-grained new categories, such as sports, finance, society, technology, etc., as scenario-specific factors, the LLM is required to infer the user's fine-grained preferences within those news categories. Once we obtain the user's fine-grained preferences, another smaller language model, Baize, designed by Huawei, encodes those preferences and candidate news articles. Those encodings are then used to calculate similarity scores, and news articles with higher similarity scores will be retrieved. The experimental group has deployed the aforementioned version of REKI-I, while the control group utilizes the original recall model as our baseline. During the online A/B test, REKI exhibited a 7% improvement on the Recall metric compared with the baseline, resulting in significant business benefits. In the music scenario, we utilize REKI for the CTR prediction task, which we have mentioned in our experiments. This scenario involves a larger number of users and items, leading us to adopt REKI-C. In this scenario, 30% of users were randomly selected into the experimental group, and another 30% were in the control group. Both groups used the same base model for generating recommendations. The difference lies in their inputs, where the input of the experimental group included the knowledge representation augmented by REKI-C. In large-scale domains, storage efficiency is also a crucial aspect that we need to consider. Therefore, we employ Principal Component Analysis (PCA) to reduce the dimension of knowledge representations to 64 and store them in a vector database. Additionally, we adopt incremental update, which only processes the updated items and users daily and adds them to the vector database. In a 7-day online A/B test, REKI demonstrated a 1.99% increase in song play count, a 1.73% increase in the number of devices for song playback, and a 2.04% increase in total duration. Now, REKI has already been deployed online and serves the main traffic in this scenario. This indicates that REKI can be successfully implemented in industrial settings and improve the recommendation experience for real-world users.", "5.4 Efficiency Study (RQ5)": "5.4.1 Offline Efficiency. To study the offline efficiency of our proposed model, we first compared the performance and efficiency of two variants of REKI, namely REKI-I and REKI-C , which utilize individual and collective forms of knowledge, respectively, with base model DIN and the strongest baseline TALLREC. REKI-I requires generating individual knowledge using LLM for each item and user, while REKI-C generates collective knowledge for clusters of items and users, reducing the number of knowledge generations. For both REKI-I and REKI-C, we employ three different sizes of encoders: ChatGLM-6B [12], BERT-110M [37], and tinyBERT-14.7M [35]. Regarding offline efficiency, we compared the number of knowledge generation calls for the entire dataset ( # call in the table) and the training time per batch for each model, which can be regarded as measures of offline resource consumption (both time and computational resource). For REKI-I and REKI-C, # call represents the number of knowledge generation calls on LLM API. For baseline TALLREC, # call denotes the number of times the LLM is invoked to generate the final CTR predictions, corresponding to the sum of training and testing data samples. All experiments in Table 6 are conducted on the same device and environment, and the batch size of training is set to 256. The only exception is that the GPU memory size limitation prevents us from setting the batch size of TALLREC to 256, so we present the training time with the batch size set to 1. From these experiments, we draw the following conclusions. Firstly, REKI demonstrates significantly higher efficiency in both time and computation resource consumption compared to approaches like TALLREC that directly utilize LLMs for recommendation. This efficiency arises from the fact that REKI-I only needs to generate knowledge for each user and item, whereas TALLREC needs to generate knowledge for all samples, and the quantity of samples is typically much larger than that of users and items. For instance, in the # call metric of MovieLens-1M dataset, the knowledge generation frequency of REKI-I is one-sixteenth of TALLREC, which indicates that REKI-I incurs significantly fewer computational resources and less time consumption than TALLREC. Moreover, regarding both training times, REKI-I also outperforms TALLREC by a significant margin, roughly being only one-tenth of TALLREC. This also provides strong evidence of REKI's efficiency over TALLREC. Secondly, for knowledge generation, collective knowledge extraction used by REKI-C effectively reduces the number of knowledge generation calls without losing much accuracy. For example, on MovieLens, # call for REKI-C is one twenty-sixth of REKI-I, but the two variants show comparable prediction accuracy. Moreover, with more miniature encoders such as BERT and tinyBERT, REKI-C even outperforms REKI-I. Considering that knowledge generation involves larger language models, which are slower, using REKI-C can effectively improve the deployment efficiency of the model. Lastly, with the same encoder, REKI-C exhibits longer training times compared to REKI-I, but REKI-C is more friendly to more miniature encoders. With the same encoder, REKI-C has approximately twice the training time of REKI-I, as REKI-C generates and utilizes more additional features. However, REKI-C is more friendly to smaller encoders. With encoders like BERT and tinyBERT, REKI-C exhibits an improvement over REKI-I, possibly because collective knowledge is more accessible for smaller encoders to comprehend. Therefore, in practical use, adopting REKI-C with a smaller encoder model, such as BERT, can lead to significant accelerations in knowledge generation and training time compared to REKI-I with ChatGLM, thereby enhancing model efficiency. This is why we employ ChatGLM-6B and BERT as default encoders for REKI-I and REKI-C in the preceding experiments. 5.4.2 Inference Latency. To quantify the actual inference latency of REKI and assess the effectiveness of the acceleration strategies, we compare the inference time of REKI-I based on DIN with LLM API, strongest baseline TALLRec, and base DIN model in Table 7. For LLMAPI , we follow the zero-shot user rating prediction in [36] that provides the user viewing history and ratings as prompt and invokes the API to predict user ratings on candidate items. Since this approach does not allow setting a batch size, the table presents the average response time per sample. For REKI, we evaluate the two acceleration strategies as introduced in Section 4.4: REKI \ud835\udc64 / \ud835\udc3b\ud835\udc38\ud835\udc3c\ud835\udc41 , where the hybridized expert integration network (HEIN) participates in the inference stage, and REKI \ud835\udc64 / \ud835\udc5c \ud835\udc3b\ud835\udc38\ud835\udc3c\ud835\udc41 , where HEIN is detached from inference. As we have different version of knowledge generation, which also impact the inference speed, we present four variants of REKI in Table 7: REKI-I \ud835\udc64 / \ud835\udc3b\ud835\udc38\ud835\udc3c\ud835\udc41 , REKI-I \ud835\udc64 / \ud835\udc5c \ud835\udc3b\ud835\udc38\ud835\udc3c\ud835\udc41 , REKI-C \ud835\udc64 / \ud835\udc3b\ud835\udc38\ud835\udc3c\ud835\udc41 , and REKI-C \ud835\udc64 / \ud835\udc5c \ud835\udc3b\ud835\udc38\ud835\udc3c\ud835\udc41 . The experiments of REKI and base model are all conducted on the same device and environment, with a batch size of 256. With the same device, we also test TALLRec and only showcase the average inference time per sample, since 32G memory cannot handle LLM with a batch size of 256. Table 7 presents the average inference time, from which we draw the following conclusions. Firstly, adopting LLM for direct inference is not feasible for RSs due to its large computational latency. The response latency of LLM API is 4-6 seconds, which does not meet the real-time requirement of RSs, which typically demands a response latency of within 100ms. Even if we finetune a relatively small LLM, such as TALLRec based on LLaMa2-7B, its inference latency is still close to 1 second, which is unbearable for industrial scenarios. Secondly, both acceleration methods of REKI achieve an inference time within 100ms, satisfying the low latency requirement. When using the prestoring knowledge representation approach, REKI-C \ud835\udc64 / \ud835\udc3b\ud835\udc38\ud835\udc3c\ud835\udc41 has a shorter inference time than REKI-I \ud835\udc64 / \ud835\udc5c \ud835\udc3b\ud835\udc38\ud835\udc3c\ud835\udc41 . This is because REKI-C \ud835\udc64 / \ud835\udc5c \ud835\udc3b\ud835\udc38\ud835\udc3c\ud835\udc41 employs BERT as the encoder, resulting in knowledge representation with a dimension of 768. On the other hand, REKI-I \ud835\udc64 / \ud835\udc5c \ud835\udc3b\ud835\udc38\ud835\udc3c\ud835\udc41 uses ChatGLM-6B as the encoder, producing representations with a dimension of 4096, which requires more processing time. Importantly, if we employ the approach of prestoring user and item augmented vectors, the knowledge generation method no longer has a significant impact. In other words, the actual inference time of REKI-I \ud835\udc64 / \ud835\udc5c \ud835\udc3b\ud835\udc38\ud835\udc3c\ud835\udc41 and REKI-C \ud835\udc64 / \ud835\udc5c \ud835\udc3b\ud835\udc38\ud835\udc3c\ud835\udc41 become nearly identical, almost the same as that of the backbone model. This demonstrates the effectiveness of our proposed REKI and acceleration strategies.", "5.5 Ablation Study (RQ6)": "5.5.1 User and Item Knowledge. To study the impact of knowledge generated by LLMs, we conduct an ablation study on user and item knowledge on the Amazon-Books dataset. The two knowledge generation approaches, individual knowledge extraction (REKI-I) and collective knowledge extraction (REKI-C), both produce user and item knowledge, so here we use REKI-I as an example. We select DCNv2, AutoInt, and DIN as backbone models and compare their performance with different knowledge enhancements, as shown in Figure 5. The legend ' None ' represents the backbone model without any knowledge enhancement. ' Item ' and ' User ' indicate the backbone models enhanced with item knowledge and user knowledge, respectively, while ' Both ' represents the joint use of both knowledge types. DCNv2 AutoInt DIN 0.820 0.825 0.830 0.835 0.840 0.845 AUC None Item User Both From Figure 5, we observe that both item knowledge and user knowledge can improve the performance of backbone models, with user knowledge exhibiting a larger improvement. This could be attributed to the fact that user knowledge inferred by the LLMs captures in-depth user preferences, thus compensating for the backbone model's limitations in reasoning underlying motives and intentions. Additionally, the joint use of both user and item enhancements outperforms using either one alone, even achieving a synergistic effect where 1 + 1 > 2. One possible explanation is that user knowledge contains external information that is not explicitly present in the raw data. When used independently, this external knowledge could not be matched with candidate items. However, combining the externally generated item factual knowledge from the LLMs aligned with user knowledge allows RSs to gain a better understanding of items according to user preferences. 5.5.2 Knowledge Encoders and Semantic Transformation. We employ two different language models, BERT [37] and ChatGLM [12], to investigate the impact of different knowledge encoders on REKI-I 's performance. Additionally, we design several variants to demonstrate how the representations generated by knowledge encoders are utilized. REKI(LR) applies average pooling to token representations from the knowledge encoder and directly feeds the result into a linear layer to obtain prediction scores, without utilizing a backbone CTR model. REKI(MLP) replaces the linear layer of REKI(LR) with an MLP. REKI-MLP and REKI-MoE replace the HEIN module with an MLP and a Mixture-of-Experts (MoE), respectively. The original REKI and the two variants, REKI-MLP and REKI-MoE, all adopt DIN as the backbone model. Table 8 presents their performance, from which we draw the following conclusions. Firstly, we can observe that, overall, variants with ChatGLM as the knowledge encoder outperform those with BERT. The performance of REKI(LR) and REKI(MLP) can be considered as a measure of the quality of the encoded representations, since they directly adopt the representations for prediction. Considering REKI(LR) and REKI(MLP), the superior performance of ChatGLM over BERT indicates that ChatGLM performs better in preserving the information within knowledge from LLMs, which may be attributed to the larger size and better text comprehension of ChatGLM (6 billion) compared to BERT (110 million). With ChatGLM on MovieLens-1M, REKI(MLP) is even close to base(DIN), validating the effectiveness of our generated open-world knowledge. Secondly, the performance of REKI benefits from complex semantic transformation structures, but the knowledge encoder also limits it. The results on ChatGLM show that a simple MLP is less effective than MoE and our designed HEIN outperforms the MoE, indicating that the transformation from semantic space to recommendation space entails a complex network structure. However, with BERT as knowledge encoder, REKI-MoE and REKI-MLP exhibit similar performance, suggesting that information from BERT is limited and using an MoE is sufficient in this case.", "6 BROADER IMPACT": "When incorporating LLMs into RSs, it is imperative to give serious consideration to privacy and security issues. While LLMs offer remarkable capabilities, they also come with potential risks of compromising user privacy and generating harmful content [4, 62, 83]. We also consider these two issues while designing our proposed framework, REKI. Regarding privacy, REKI leverages LLMs' reasoning ability and factual knowledge without finetuning LLMs, ensuring that LLMs do not retain or remember user-specific data. Besides, while experiments may involve the use of external APIs on public datasets, real-world implementations usually rely on in-house models, e.g. , we utilize Huawei's own LLM PanGu[89] for online A/B test as described in section 5.3, preventing the leakage of user privacy information through external APIs. Furthermore, compared to methods that directly display LLM-generated content to users [9, 15, 51], REKI takes a more proactive stance to mitigate concerns about harmful content and hallucination knowledge generated by LLMs. REKI first converts the textual content from LLMs to robust representations and then incorporates those representations into traditional RSs. This integration avoids displaying harmful or misleading content generated by LLMs while enabling us to utilize filtering mechanisms commonly used in traditional RSs to screen out potential harmful item recommendations. Through those measures, REKI strives to deliver robust recommendation performance while safeguarding user data privacy and the security of recommendation content.", "7 CONCLUSION": "Our work presents REKI, a framework for effectively incorporating open-world knowledge into recommender systems by exploiting large language models. In REKI, we introduce factorization prompting for accurate knowledge reasoning on users and items and develop individual and collective knowledge extraction for different scales of scenarios, effectively reducing offline resource consumption. Subsequently, a hybridized expert-integrated network is designed to efficiently transform the generated user and item knowledge into augmented vectors to enhance any conventional recommendation model. We also ensure efficient inference by preprocessing and prestoring the generated knowledge. Extensive experiments demonstrate that REKI significantly outperforms the state-of-the-art baselines and is compatible with various recommendation algorithms and tasks. REKI has been deployed to Huawei's news and music recommendation platforms, gaining a 7% and 1.99% improvement during the online A/B test. In this work, we have explored how to reduce the knowledge LLMs need to generate to improve efficiency and reduce resource consumption. In the future, we will investigate ways to accelerate the recommendation knowledge generation process of LLMs, such as through quantization and speculative decoding, to further reduce the resource consumption of deploying LLMs in recommendation systems.", "REFERENCES": "[1] Mohiuddin Ahmed, Raihan Seraj, and Syed Mohammed Shamsul Islam. 2020. The k-means algorithm: A comprehensive survey and performance evaluation. Electronics 9, 8 (2020), 1295. [5] S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712 (2023). [22] Jesse Harte, Wouter Zorgdrager, Panos Louridas, Asterios Katsifodimos, Dietmar Jannach, and Marios Fragkoulis. 2023. Leveraging Large Language Models for Sequential Recommendation. In Proceedings of the 17th ACM Conference on Recommender Systems . 1096-1102. [26] Yupeng Hou, Shanlei Mu, Wayne Xin Zhao, Yaliang Li, Bolin Ding, and Ji-Rong Wen. 2022. Towards Universal Sequence Representation Learning for Recommender Systems. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 585-593. [30] Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. 2019. FiBiNET: Combining Feature Importance and Bilinear Feature Interaction for Click-through Rate Prediction. In Proceedings of the 13th ACM Conference on Recommender Systems . 169-177. [36] Wang-Cheng Kang, Jianmo Ni, Nikhil Mehta, Maheswaran Sathiamoorthy, Lichan Hong, Ed Chi, and Derek Zhiyuan Cheng. 2023. Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction. arXiv preprint arXiv:2305.06474 (2023). [56] Hanjia Lyu, Song Jiang, Hanqing Zeng, Yinglong Xia, and Jiebo Luo. 2023. Llm-rec: Personalized recommendation via prompting large language models. arXiv preprint arXiv:2307.15780 (2023). [86] Yunjia Xi, Weiwen Liu, Jieming Zhu, Xilong Zhao, Xinyi Dai, Ruiming Tang, Weinan Zhang, Rui Zhang, and Yong Yu. 2022. Multi-Level Interaction Reranking with User Behavior History. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval . [88] Yisong Yue, Thomas Finley, Filip Radlinski, and Thorsten Joachims. 2007. A support vector method for optimizing average precision. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval . 271-278. [90] Mi Zhang, Jie Tang, Xuchen Zhang, and Xiangyang Xue. 2014. Addressing Cold Start in Recommender Systems: A Semi-Supervised Co-Training Algorithm. In Proceedings of the 37th International ACM SIGIR Conference on Research & Development in Information Retrieval (SIGIR '14) . 73-82. [96] Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, and Ji-Rong Wen. 2023. Large language models for information retrieval: A survey. arXiv preprint arXiv:2308.07107 (2023)."}
