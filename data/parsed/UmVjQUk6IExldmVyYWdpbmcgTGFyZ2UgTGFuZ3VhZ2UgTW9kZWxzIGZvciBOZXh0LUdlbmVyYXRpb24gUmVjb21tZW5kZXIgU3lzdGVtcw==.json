{
  "RecAI: Leveraging Large Language Models for Next-Generation Recommender Systems": "Jianxun Lian jialia@microsoft.com Microsoft Research Asia Beijing, China",
  "Yuxuan Lei": "leiyuxuan@mail.ustc.edu.cn University of Science and Technology of China Hefei, China",
  "Xu Huang": "xuhuangcs@mail.ustc.edu.cn University of Science and Technology of China Hefei, China",
  "Jing Yao": "jingyao@microsoft.com Microsoft Research Asia Beijing, China",
  "Wei Xu": "xu_wei@ruc.edu.cn Renmin University Beijing, China",
  "ABSTRACT": "This paper introduces RecAI, a practical toolkit designed to augment or even revolutionize recommender systems with the advanced capabilities of Large Language Models (LLMs). RecAI provides a suite of tools, including Recommender AI Agent, Recommendationoriented Language Models, Knowledge Plugin, RecExplainer, and Evaluator, to facilitate the integration of LLMs into recommender systems from multifaceted perspectives. The new generation of recommender systems, empowered by LLMs, are expected to be more versatile, explainable, conversational, and controllable, paving the way for more intelligent and user-centric recommendation experiences. We hope the open-source of RecAI can help accelerate evolution of new advanced recommender systems. The source code of RecAI is available at https://github.com/microsoft/RecAI.",
  "Xing Xie": "xingx@microsoft.com Microsoft Research Asia Beijing, China intelligence, such as engaging in smooth conversations, executing logical and mathematical reasoning, following detailed instructions to complete tasks, and assisting in the troubleshooting of software development issues. Consequently, a diverse set of applications is now transitioning toward the integration of LLMs, either to bolster existing models or to implement them as the principal framework.",
  "CCS CONCEPTS": "· Information systems → Recommender systems .",
  "KEYWORDS": "Recommender Systems; Large Language Models",
  "ACMReference Format:": "Jianxun Lian, Yuxuan Lei, Xu Huang, Jing Yao, Wei Xu, and Xing Xie. 2024. RecAI: Leveraging Large Language Models for Next-Generation Recommender Systems. In Companion Proceedings of the ACM Web Conference 2024 (WWW'24 Companion), May 13-17, 2024, Singapore, Singapore. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3589335.3651242 Recommender systems (RSs) function as a specialized type of information retrieval system, designed to capture a user's preferences from their profile and behavioral history. RSs can curate a selection of items to present to the user, thereby simplifying the process of discovering preferred choices within an extensive database of items. Impressed by the remarkable ability of LLMs, there is burgeoning interest in how LLMs can transform the landscape of next-generation RSs. However, directly applying LLMs as recommender models is not feasible. On one hand, the knowledge boundary of LLMs is limited to the information available up to the point of their last training update. The specific item catalog and the attributes of items within a particular recommendation context may not be fully captured by LLMs. On the other hand, user preference patterns are not only domain-specific but also subject to rapid evolution. Consequently, traditional recommender models require frequent retraining or fine-tuning with up-to-date data to capture the unique and shifting patterns that diverge from the general world knowledge encoded in LLMs.",
  "1 INTRODUCTION": "Large language models (LLMs) have been rigorously pretrained on massive amounts of data sourced from the internet. With the expansion of their model parameters from the hundreds of millions to the hundreds of billions, LLMs have demonstrated emerging general Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WWW'24 Companion, May 13-17, 2024, Singapore, Singapore © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0172-6/24/05 https://doi.org/10.1145/3589335.3651242 This paper investigates the possibilities of utilizing LLMs to advance RSs. The vision is for the next wave of recommender systems, empowered by LLMs, to exhibit heightened intelligence and versatility. This includes the ability to generate explanations for recommendations, facilitate item suggestions through conversational interfaces, and offer enhanced user control. To achieve these objectives, we introduce RecAI, a lightweight toolkit to integrate LLMs into RSs from a comprehensive and diverse set of perspectives. Currently, RecAI comprises five foundational pillars, each one corresponds to an independent application scenario: · Recommender AI Agent . This is an LLM-driven AI agent, where the LLMs act as the \"brain\" responsible for user interaction, as well as for reasoning, planning, and task execution. Traditional recommender models act as \"tools\", enhancing the LLMs by providing specialized capabilities. · Recommendation-oriented LM . Fine-tuning language models is an effective strategy for integrating domain-specific knowledge WWW'24 Companion, May 13-17, 2024, Singapore, Singapore Jianxun and Yuxuan, et al. into models. We introduce two types of models: RecLM-emb and RecLM-gen . RecLM-emb converts diverse text types, such as natural conversations and unstructured attributes, into embeddings for item retrieval. RecLM-gen is a generative language model. After fine-tuned with in-domain data, it excels at understanding domain information and collaborative patterns. · RecExplainer . Most deep learning-based recommender models are opaque, acting as \"black boxes.\" RecExplainer is designed to leverage LLMs' ability to elucidate the workings of embeddingbased recommender models by interpreting the underlying hidden representations. · Knowledge Plugin . This supplements LLMs by dynamically incorporating domain-specific knowledge into prompts without altering the LLMs themselves. This is particularly beneficial when LLMs cannot be fine-tuned - either due to only API availability or constraints like lack of GPU resources. · Evaluator . RecAI includes a tool for assessing LLM-augmented recommender systems in an convenient manner. It encompasses the evaluation of embedding-based and generative recommendations, explanation capabilities, and conversation abilities. In the following sections, we will introduce details for each pillar.",
  "2 RECOMMENDER AI AGENT": "The remarkable achievements of LLMs have inspired researchers to envision a future where RSs are more versatile, interactive, and usercentric. However, the use of LLMs as independent recommender models is constrained by their lack of domain-specific knowledge. Traditional recommender models are tailored to specific recommendation tasks through training on domain-specific data, presenting an opportunity for synergy. Combining the strengths of both LLMs and specialized recommender models into a unified framework emerges as a promising approach. This synthesis is an LLM-based agent framework, wherein recommender models serve as specialized tools for tasks like item retrieval or click-through rate (CTR) prediction, while LLMs operate as the core intelligence, facilitating smooth interactions with users and employing contextual reasoning to determine the most suitable tools for the current conversational context. We name this AI agent framework InteRecAgent [3]. Memory, task planning, and tool-learning are three critical components for AI agents. In InteRecAgent, we also tail these three We define a core suite of three distinct tool types within InteRecAgent to enable effective communication with users: (1) Information Query : The InteRecAgent addresses user queries alongside recommending items. For instance, on a gaming platform, it can answer questions about game details like release dates and prices by querying a backend database with SQL. (2) Item Retrieval : This tool suggests a list of potential items based on a user's criteria. InteRecAgent differentiates between \"hard conditions\" (explicit user specifications) and \"soft conditions\" (preferences requiring semantic matching). SQL tools and item-to-item matching based on embeddings are used to fulfill these conditions, respectively. (3) Item Ranking : Ranking tools predict user preferences on the shortlisted items using user profiles and/or user history, ensuring recommendations align with both the user's immediate needs and their overall preferences. These shortlisted items may either be derived from the item retrieval process or be provided by the user. components to address specific challenges in the recommendation scenario. A simple illustration of InteRecAgent can refer to Figure 1. Figure 1: An overview of the InteRecAgent. Users interact with an LLM in natural language. The LLM comprehends users' intention and makes a tool-execution plan to fetch the correct items or information from the specific domain. Based on tools' results, the LLM generate response for users. Chain init state dynamic demo plan execute reflection Candidate Bus Item Retrieval Info. Query Item Ranking Tools User LLM rechain ... No Yes Profile Memory Tool's results Task planning and execution Memory . To effectively manage the flow of item candidates within InteRecAgent and address input context length limitations, we introduce two key modules: the Candidate Bus and User Profile. The Candidate Bus serves as a dedicated memory system for storing current candidates and tracking tool outputs, facilitating the streamlined processing of item lists and tool execution records. This ensures efficient interaction among tools without burdening the LLM's input prompt. User Profiles are constructed from conversation histories and differentiated into long-term and short-term memories. This segmentation tackles the complexities of lifelong learning scenarios while emphasizing users' immediate requests, allowing for refined and adaptive recommendations. Tool-learning . In our quest to make the InteRecAgent framework more accessible and cost-effective, we explore the potential of training smaller language models (SLMs) like the 7B-parameter Llama to emulate GPT-4's adeptness at following instructions. We create RecLlama, a fine-tuned version of Llama-7B, using a specialized dataset generated by GPT-4 that contains pairs of [instructions, tool execution plans]. To ensure dataset quality and diversity, we combine data from user simulator-agent dialogues with crafted dialogues covering various tool execution scenarios. We find that RecLlama can significantly outperform some LLMs such as GPT-3.5turbo and Text-davinci-003 in serving as the brain in InteRecAgent. Detailed evaluations of InteRecAgent are presented in [3]. Task Planning . We adopt a plan-first approach for the InteRecAgent, diverging from the traditional step-by-step method. Initially, the LLM devises a comprehensive execution plan based on the user's intentions from the dialogue. Subsequently, it strictly follows this plan, sequentially invoking tools that interface through the Candidate Bus. The plan phase incorporates user input, context, tool descriptions, and demonstration for in-context learning to create a tool utilization plan. The execution phase then follows the plan, with each tool's output tracked except for the final output, which informs the LLM's response. To enhance planning, we use dynamic, high-quality demonstrations, selecting examples most similar to the current user's intent. The plan-first approach reduces API calls and latency, crucial for conversational interaction, and improves planning capability with efficient demonstration strategies. RecAI: Leveraging Large Language Models for Next-Generation Recommender Systems WWW'24 Companion, May 13-17, 2024, Singapore, Singapore",
  "3 RECOMMENDATION-ORIENTED LANGUAGE MODEL": "Traditional RSs typically handle structured data, such as sequences of item IDs, to infer user preferences. However, this structured approach is not well-suited to the strengths of large language models (LLMs), which are adept at processing natural language. In realworld interactions, users often provide a wealth of information in their conversations, ranging from explicit requests to subtle indications of their preferences, articulated in their natural language. LLMs are capable of interpreting these user intents and translating them into natural language-based directives for subsequent processing. Therefore, there's a critical demand for RSs capable of assimilating diverse textual inputs - from casual dialogues to unstructured product descriptions - and returning items that closely match the intricacies of the query. To this end, we propose fine-tuning language models specifically for recommendation tasks. Depending on whether the approach is embedding-based or generative, we introduce two distinct types of models: RecLM-emb and RecLM-gen, as illustrated in Figure 2. Figure 2: A graphical comparison of two RecLM structures. Transformer emb Transformer input tokens input tokens output tokens (a) RecLM-emb (b) RecLM-gen",
  "3.1 RecLM-emb": "Previous research has built general-purpose text embedding models using contrastive pre-training on expansive datasets to enhance semantic text matching. Yet, these models often do not perform well in zero-shot item retrieval tasks. The main issue is their generalized representations, which don't adequately capture the specific details of items mentioned in variously structured queries. To overcome this limitation, we have crafted ten matching tasks that address different facets of item representation and compiled a fine-tuning dataset tailored for item retrieval. Utilizing this dataset, we introduce our embedding-based Recommendation Language Model [5], RecLM-emb, designed to retrieve items based on textual input of any form. After fine-tuning, RecLM-emb demonstrates a notable enhancement in performance on item retrieval tasks. It also shows effectiveness in conversational scenarios, thereby enhancing the capabilities of LLM-based recommender agents like Chat-Rec [1]. Moreover, RecLM-emb has the potential to unifying search and recommendation service or for generating refined semantic representations to support downstream rankers.",
  "3.2 RecLM-gen": "In contrast to embedding-based LMs, the generative recommendation LM, abbreviated as RecLM-gen [6], decodes responses directly into natural language. When it comes to recommending items, the names of these items are seamlessly integrated into the dialogue. As such, RecLM-gen manages user-system interactions in an endto-end fashion, eliminating the need for intermediary steps like embedding-based retrieval or tool invocation. The advantages of RecLM-gen are three-fold. Firstly, domainspecific fine-tuning equips the LM to better recognize item names and unique collaborative patterns, thereby surpassing the accuracy of general-purpose LMs in recommendations. Secondly, integrating RecLM-gen as the core intelligence of the Recommender AI Agent framework significantly lowers system costs compared to larger, more costly LMs. Lastly, RecLM-gen facilitates seamless, real-time user interactions by generating tokens streamingly, unlike traditional AI agent frameworks that rely on multiple backend LLM calls for context reasoning and tool interaction, which can introduce delays of 10-20 seconds as per our observations. [2] reveals that with carefully crafted prompt engineering and bootstrapping techniques, zero-shot LLMs can serve as competent ranking models. Nonetheless, our observations suggest that finetuning with domain-specific data can lead to even more substantial improvements in recommendation performance. A fine-tuned 7B Llama-2-chat model can surpass GPT-4 in item ranking tasks. In RecAI, we offer the fine-tuning scripts for RecLM-gen, enabling users to replicate and build upon our results.",
  "4 KNOWLEDGE PLUGIN": "In scenarios where fine-tuning LLMs is not feasible - due to only having access to LLM APIs or facing constraints in terms of GPU resources or time - we must find alternative ways to introduce domain-specific knowledge. Notably, the input context window size of LLMs is expanding, as evidenced by GPT-4-turbo's increase to 128k tokens and Claude 2.1's support for up to 200k tokens. This expansion provides an opportunity to include selected domain patterns directly into the input. As a instantiation of applying the DOKE paradigm to RSs, we focus on boosting LLMs' performance on the item ranking. Our specialized knowledge extractor gathers item attributes and collaborative filtering signals, tailoring this information to the user's preferences and the set of candidate items. It then conveys this information either through natural language explanations or as reasoning paths on a knowledge graph, thereby yielding more interpretable recommendations. Through is way, our experimental results across different recommendation benchmarks demonstrate that DOKE markedly enhances LLM performance, proving its efficiency and adaptability. For additional details, please refer to [7]. Motivated by this, we propose the Domain-specific Knowledge Enhancement (DOKE) paradigm, which bypasses the need for parameter modification and instead uses prompts to integrate domain knowledge. The core idea of DOKE includes three steps: (1) extracting domain relevant knowledge, (2) selecting knowledge pertinent to the current sample to fit within prompt length constraints, and (3) formulating this knowledge into natural language.",
  "5 RECEXPLAINER": "Model interpretability is crucial for creating reliable RSs, as it provides insights into system reliability, aids in detecting bugs, helps identify biases, and drives innovation. One major approach in this research field is training self-explainable surrogate models to mimic WWW'24 Companion, May 13-17, 2024, Singapore, Singapore Jianxun and Yuxuan, et al. the behavior of more complex models. However, surrogate models tend to compromise model accuracy and typically generate explanations in fixed, less intuitive formats like lists of feature importance or decision rules. To implement the three alignment methods - behavioral, intentional, and hybrid alignment - we define six tasks to fine-tune an LLM to align with a target recommender model's predictions. These tasks include teaching the LLM to predict the next item a user may like, learning to rank items, classifying interests, detailing item characteristics, maintaining general intelligence through ShareGPT training, and reconstructing user history for intention alignment. This comprehensive training regimen equips the LLM to replicate the recommender model's logic. Thus, together with the LLM's own reasoning capabilities and world-knowledge, LLMs can generate model explanations with higher fidelity and robustness in the recommendation scenario. For more technical details and evaluations, please refer to [4]. LLMs offer a new perspective for surrogate modeling that avoids a hard trade-off between model complexity and interpretability. Meanwhile, LLMs have the capability to produce natural language explanations, making them more user-friendly and convincing. In this context, we explore the use of an LLM as a surrogate model for explainability in recommender models. We start with a behavior alignment approach, where the LLM is fine-tuned to predict items based on user profiles, closely mirroring the recommendation model's output. While this method provides useful insights, it does not delve into the internal logic of the model. To address this, we propose intention alignment , wherein the LLM learns to process the recommender model's embeddings. Similar to how vision-language multimodal models process visual data, this approach aims to enable the LLM to understand the information within user/item embeddings, allowing it to explain the reasoning behind a recommender model's suggestions. We find that combining these two methods into a hybrid alignment strategy, which incorporates both textual information and embeddings, can more effectively address interpretation inaccuracies and enhance the overall interpretability. This integrated approach combines the benefits of both behavior and intention alignment, providing a stronger and more comprehensive explanation mechanism.",
  "6 EVALUATOR": "RecAI provides a tool for automatic evaluation across five key dimensions: Embedding-based recommendation . RecAI evaluator supports embedding-based matching models like our RecLM-emb or OpenAI's text embedding API 1 . Once user/item embeddings are inferred, the subsequent evaluation procedure aligns with the conventional evaluation process. Generative recommendation . LLM-based RSs enable natural language engagement, which can occasionally result in item names being generated with minor inaccuracies, such as incorrect punctuation. To accommodate these potential discrepancies, we employ fuzzy matching to ensure our name validation process remains adaptable without being too strict. 1 https://platform.openai.com/docs/guides/embeddings Conversation . We assess conversational recommendation efficacy through a GPT-4-powered user simulator that engages with the system to solicit item suggestions. System performance is gauged by its success in referencing the simulator's target items during the interaction. Chit-chat . Users might initiate non-recommendation dialogues, like asking \"how to write a research paper.\" The RS is expected to adeptly manage such inquiries. An LLM, such as GPT-4, critiques the system's replies for their helpfulness, relevance, and thoroughness. Explanation . The system delivers explanations for its recommendations, which are then evaluated by an independent LLM like GPT-4, serving as a judge to appraise the informativeness, persuasiveness, and helpfulness of these explanations. We measure the first three dimensions using NDCG and Recall metrics compared to ground truths. For Explanation and Chit-Chat, we utilize pairwise comparisons for a solid evaluation, where a judge contrasts outputs from two models, tallying wins, losses, and ties to gauge overall performance.",
  "7 CONCLUSIONS": "We present RecAI, a toolkit designed to leverage LLMs to forge recommender systems that emulate human-like interactions. RecAI is structured around multiple pillars, each aimed at addressing a variety of real-world applications through diverse techniques. For instance, engineers aiming to evolve their industrial recommender systems into conversational interfaces can deploy the Recommender AI Agent framework, thus preserving the value of their existing recommender models. Researchers looking to rapidly develop a conversational recommender system with minimal costs might opt for the Chat-Rec framework, integrating RecLM-emb for retrieval and RecLM-gen as the generative LLM. We anticipate that the next generation of recommender systems, powered by LLMs, will offer increased versatility, interactivity, and user control. We hope RecAI can accelerate this transformative process, providing the tools necessary for the industry and academia to build more sophisticated, engaging, and responsive recommendation systems.",
  "REFERENCES": "[1] Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, and Jiawei Zhang. 2023. Chat-rec: Towards interactive and explainable llms-augmented recommender system. arXiv preprint arXiv:2303.14524 (2023). [3] Xu Huang, Jianxun Lian, Yuxuan Lei, Jing Yao, Defu Lian, and Xing Xie. 2023. Recommender ai agent: Integrating large language models for interactive recommendations. arXiv preprint arXiv:2308.16505 (2023). [2] Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley, and Wayne Xin Zhao. 2023. Large language models are zero-shot rankers for recommender systems. arXiv preprint arXiv:2305.08845 (2023). [4] Yuxuan Lei, Jianxun Lian, Jing Yao, Xu Huang, Defu Lian, and Xing Xie. 2023. RecExplainer: Aligning Large Language Models for Recommendation Model Interpretability. arXiv preprint arXiv:2311.10947 (2023). [6] Wensheng Lu, Jianxun Lian, Wei Zhang, Guanghua Li, Mingyang Zhou, Hao Liao, and Xing Xie. 2024. Aligning Large Language Models for Controllable Recommendations. arXiv:2403.05063 [cs.IR] [5] Yuxuan Lei, Jianxun Lian, Jing Yao, Mingqi Wu, Defu Lian, and Xing Xie. 2024. Aligning Language Models for Versatile Text-based Item Retrieval. arXiv:2402.18899 [cs.IR] [7] Jing Yao, Wei Xu, Jianxun Lian, Xiting Wang, Xiaoyuan Yi, and Xing Xie. 2023. Knowledge Plugins: Enhancing Large Language Models for Domain-Specific Recommendations. arXiv preprint arXiv:2311.10779 (2023).",
  "keywords_parsed": [
    "Recommender Systems",
    " Large Language Models"
  ]
}