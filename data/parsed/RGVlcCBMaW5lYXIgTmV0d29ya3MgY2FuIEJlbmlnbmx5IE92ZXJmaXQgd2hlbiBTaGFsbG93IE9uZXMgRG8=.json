{
  "Deep Linear Networks can Benignly Overfit when Shallow Ones Do": "Niladri S. Chatterji Stanford University niladri@cs.stanford.edu Philip M. Long Google plong@google.com February 8, 2023",
  "Abstract": "Webound the excess risk of interpolating deep linear networks trained using gradient flow. In a setting previously used to establish risk bounds for the minimum /u1D4C1 2 -norm interpolant, we show that randomly initialized deep linear networks can closely approximate or even match known bounds for the minimum /u1D4C1 2 -norm interpolant. Our analysis also reveals that interpolating deep linear models have exactly the same conditional variance as the minimum /u1D4C1 2 -norm solution. Since the noise affects the excess risk only through the conditional variance, this implies that depth does not improve the algorithm's ability to 'hide the noise'. Our simulations verify that aspects of our bounds reflect typical behavior for simple data distributions. We also find that similar phenomena are seen in simulations with ReLU networks, although the situation there is more nuanced.",
  "1 Introduction": "Recent empirical studies [Zha+17; Bel+19] have brought to light the surprising phenomenon that overparameterized neural network models trained with variants of gradient descent generalize well despite perfectly fitting noisy data. This seemingly violates the once widely accepted principle that learning algorithms should trade off between some measure of the regularity of a model, and its fit to the data. To understand this, a rich line of research has emerged to establish conditions under which extreme overfitting-fitting the data perfectlyis benign in simple models [see BHM18; Has+22; Bar+20]. Another closely connected thread of research to understand generalization leverages the recognition that training by gradient descent engenders an implicit bias [see NTS15; Sou+18; JT19]. These results can be paraphrased as follows: training until the loss is driven to zero will produce a model that, among models that interpolate the data, minimizes some data-independent regularity criterion. 1 Our paper continues this study of benign overfitting but with a more complex model class, deep linear networks. Deep linear networks are often studied theoretically [see, e.g., SMG14; ACH18], because some of the relevant characteristics of deep learning in the presence of nonlinearities are also present in linear networks but in a setting that is more amenable to analysis. The analyses of linear networks have included a number of results on implicit bias [see, e.g., Azu+21; Min+21]. Recently, one of these analyses [Azu+21], of two-layer networks trained by gradient flow with a 'balanced' initialization, was leveraged in an analysis of benign overfitting [CLB22]. (For a mapping /u1D465 → /u1D465 /u1D44A /u1D463.alt parameterized by a hidden layer /u1D44A ∈ ℝ /u1D451×/u1D45A and an output layer /u1D463.alt ∈ ℝ /u1D45A×1 , initial values of /u1D463.alt and /u1D44A are balanced if /u1D463.alt /u1D463.alt ⊤ = /u1D44A ⊤ /u1D44A .) Min et al. [Min+21] analyzed implicit bias in two-layer linear networks under more general conditions including the unbalanced case. In this paper, we analyze benign overfitting in deep linear networks of arbitrary depth trained by gradient flow. Our first main result is a bound on the excess risk. The bound is in terms of some characteristics of the joint distribution of the training data previously used to analyze linear regression with the standard parameterization, including notions of the effective rank of the covariance matrix, and it holds under similar conditions on the data distribution. Another key quantity used in the bound concerns the linear map Θ computed by the network after training-it is the norm of the projection of this map onto the subspace orthogonal to the span of the training examples. This norm can further be bounded in terms of its value at initialization, and a quantity that reflects how rapidly training converged. In contrast with previous analyses on two-layer networks [CLB22], this analysis holds whether this initialization is balanced or not. Our second main result is a high-probably risk bound that holds for networks in which the first and last layers are initialized randomly, and the middle layers are all initialized to the identity. Our bound holds whenever the scale of the initialization of the first layer is small enough, and the scale of the initialization of the last layer is large enough. This includes the extreme case where the first layer is initialized to zero. As the scale of the initialization of the first layer goes to zero, our bound approaches the known bound for the minimum /u1D4C1 2 -norm interpolator with the standard parameterization. Our final main theoretical result illustrates our bounds using a simple covariance matrix used in previous work [Bar+20; CL22] which might be viewed as a canonical case where overfitting is benign for linear regression with the standard parameterization. These bounds were obtained in the absence of a precise characterization of the implicit bias of gradient flow for deep linear networks, or a closed-form formula for the model produced. A key point of our analysis is that the projection of the linear map Θ computed by the interpolating network onto the span of the rows of the design matrix /u1D44B is exactly equal to minimum /u1D4C1 2 -norm interpolant Θ/u1D4C1 2 . The risk of Θ naturally decomposes into contributions from this projection and Θ/u1D44B ⟂ = Θ - Θ/u1D4C1 2 . We can use previous analyses of Θ/u1D4C1 2 to bound the former. 2",
  "Deep Linear Networks": "Figure 3. Excess risk and distance from the minimum /u1D4C1 2 -norm interpolator of three-layer linear networks trained by gradient descent on data generated by an underlying linear model as the input dimension varies. The model is trained on /u1D45B = 100 points drawn from the generative model /u1D466 = /u1D465 Θ ⋆ + /u1D714 , where /u1D465 ∼ N (0, Σ) and /u1D714 ∼ N (0, 1) . The excess risk is defined as /u1D53C /u1D465 [ ‖/u1D465 Θ - /u1D465 Θ ⋆ ‖ 2 ] . In line with our theory, we find that when the initialization scale is small, final solution is close to the minimum /u1D4C1 2 -norm interpolator and the resulting excess risk is small. 80 Q2 = 0 3.0 Q2 = 60 Q2 = 10 2.0 40 920 1.0 500 1000 1500 2000 500 1000 1500 2000 Dimension Dimension 10-8 Inspired by our theory, we ran simulations to study the excess risk of several linear networks and ReLU networks as a function of both the initialization scale and dimension. 1 In line with our theoretical upper bounds, we find that for deep linear networks as the initialization scale of either the first layer (/u1D6FC ) or the last layer (/u1D6FD ) is large, the excess risk of the model is larger (see Figure 1). In deep ReLU networks (see Figure 2), we find an asymmetry in the roles of /u1D6FC and /u1D6FD . The excess risk increases when we increase /u1D6FC , but is largely unaffected by the scale of the initialization of the final layer /u1D6FD . In all of our figures we report the average over 20 runs. We also report the 95% confidence interval assuming that the statistic of interest follows a Gaussian distribution. Setup for deep linear models. For Figures 1 and 3 the generative model for the underlying data was /u1D466 = /u1D465 Θ ⋆ + /u1D714 , where 1. Θ ⋆ ∈ ℝ /u1D451×3 is drawn uniformly over the set of matrices with unit Frobenius norm. The output dimension /u1D45E = 3 ; 2. the covariates /u1D465 ∼ N (0, Σ) , where the eigenvalues of Σ are as follows: /u1D706 1 = … = /u1D706 10 = 1 and /u1D706 11 = … = /u1D706 /u1D451 = 0.01 ; 3. the noise /u1D714 is drawn independently from N (0, 1) . 1 Code at https://github.com/niladri-chatterji/Benign-Deep-Linear 24 For these figures the number of samples /u1D45B = 100 across all experiments. All of the models are trained on the squared loss with full-batch gradient descent with step-size 10 -4 , until the training loss is smaller than 10 -7 . For the top half of Figure 1 and Figure 3 when we vary the initialization scale of the first layer /u1D6FC , we initialize all of the middle layers to the identity, and initialize entries of the last layer with i.i.d. draws from N (0, 1) . We train models that have 2 hidden layers (/u1D43F = 3) . The width of the middle layers /u1D45A is set to be 10(/u1D451 + /u1D45E) , where /u1D451 is the input dimension and /u1D45E is the output dimension. For the bottom half of Figure 1 when we vary the initialization scale of the last layer /u1D6FC , we initialize all of the middle layers to the identity, and initialize entries of the first layer with i.i.d. draws from N (0, 1) . Setup for deep ReLU models. For Figure 2 the generative model for the underlying data was /u1D466 = /u1D453 ⋆ (/u1D465 ) + /u1D714 , where 1. /u1D453 ⋆ (/u1D465 ) is a two-layer feedforward ReLU network with width 10 and output dimension 3 which was randomly initialized according to LeCun initialization; 2. the covariates /u1D465 ∼ N (0, /u1D43C 10×10 ) ; 3. the noise /u1D714 is drawn independently from N (0, 1) . The networks are trained on /u1D45B = 500 samples. Again, all of the models are trained on the squared loss with full-batch gradient descent with step-size 10 -4 , until the training loss is smaller than 10 -7 . For the right half of Figure 2 when we vary the initialization scale of the last layer /u1D6FC , we initialize all of the middle layers to the identity, and initialize entries of the first layer with i.i.d. draws from N (0, 1) . We train models that have /u1D43F = 3 layers. The width of the middle layers (/u1D45A) is set to be 50 . For left half of Figure 2 when we vary the initialization scale of the first layer /u1D6FC , we initialize all of the middle layers to the identity, and initialize entries of the last layer with i.i.d. draws from N (0, 1) .",
  "Deep ReLU Networks": "0.5 0.40 0.4 0.38 0.36 0.3 0.34 0.32 0.30 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 x10-6 x10-6 More details of the experiments are described in Section 7. Intuitively, the harm from overfitting arises from fitting the noise, and the effect of fitting the noise is analyzed in the conditional variance of the estimator. In the setting studied here, as in linear regression with the standard parameterization, the conditional variance is entirely determined by the projection of Θ onto the span of the rows of the data matrix /u1D44B which is equal to Θ/u1D4C1 2 . Thus, when learning deep linear networks with quadratic loss, aspects of training that affect the inductive bias, such as the initialization, architecture, etc., do not affect this variance term-no matter how they are chosen, the distribution of the variance term is determined by Θ/u1D4C1 2 . To see an effect of implicit bias in deep linear networks on the consequence of fitting the noise, we must analyze a loss function other than the quadratic loss. Our upper bounds reveal no benefit in representing linear transformations by deep networks, and, in our simulations, we see no benefit with random initialization. This is because non-zero random initialization usually contributes additional error to the bias as the random initialization is typically a poor guess for the regression function. (In rare cases it could reduce the bias, though, if by chance it approximates the regression function.) Our analysis also leverages the effect of imbalanced initialization on implicit bias-our treatment partially extends the results by Min et al. [Min+21] from the two-layer case to the deep case, and then combines them with our general risk bound. 4 Organization. In Section 2 we describe our problem setting and our assumptions. Then in Section 3 we present our main results and in Sections 4, 5 and 6 we prove these results. We provide additional simulations and simulation details in Section 7. We conclude with a discussion in Section 8. In Appendix A we highlight other related work on benign overfitting, implicit bias, and on linear networks. Finally, in Appendix B we present omitted technical details.",
  "2 Preliminaries": "This section includes notational conventions and a description of the setting.",
  "2.1 Notation": "Given a vector /u1D463.alt , let ‖/u1D463.alt ‖ denote its Euclidean norm. Given a matrix /u1D440 , let ‖/u1D440 ‖ denote its Frobenius norm and let ‖/u1D440 ‖ /u1D45C /u1D45D denote its operator norm. For any /u1D457 ∈ ℕ , we denote the set {1, … , /u1D457} by [/u1D457 ] . We will use /u1D450 , /u1D450 ′ , /u1D450 1 , /u1D450 /u1D465 , … to denote positive absolute constants, which may take different values in different contexts.",
  "2.2 Setting": "We analyze linear regression with /u1D451 inputs and /u1D45E outputs from /u1D45B examples. Throughout the paper we assume that /u1D451 > /u1D45B . Although we assume throughout that the input dimension /u1D451 is finite, it is straightforward to extend our results to infinite /u1D451 . Let /u1D44B ∈ ℝ /u1D45B×/u1D451 be the data matrix, and /u1D44C ∈ ℝ /u1D45B×/u1D45E be the response matrix, and let /u1D465 1 , … , /u1D465 /u1D45B ∈ ℝ 1×/u1D451 be the rows of /u1D44B and /u1D466 1 , … , /u1D466 /u1D45B ∈ ℝ 1×/u1D45E be the rows of /u1D44C . For random (/u1D465 , /u1D466 ) ∈ ℝ 1×/u1D451 × ℝ 1×/u1D45E , let  be an arbitrary optimal linear regressor. We let Ω = /u1D44C - /u1D44B Θ ⋆ ∈ ℝ /u1D45B×/u1D45E be the noise matrix. Define the excess risk of an estimate Θ ∈ ℝ /u1D451×/u1D45E to be  where /u1D465 , /u1D466 are test samples that are independent of Θ . Denote the second moment matrix of the covariates by Σ ∶= /u1D53C[/u1D465 ⊤ /u1D465 ] ∈ ℝ /u1D451×/u1D451 with eigenvalues /u1D706 1 ≥ … ≥ /u1D706 /u1D451 ≥ 0 . We will use the following definitions of the 'effective rank' that Bartlett et al. [Bar+20] previously used in the analysis of the excess risk of the minimum /u1D4C1 2 -norm interpolant. Definition 2.1. Given any /u1D457 ∈ [/u1D451 ] , define /u1D460 /u1D457 ∶= ∑ /u1D456>/u1D457 /u1D706 /u1D456 and  5 We define the index /u1D458 below. The value of /u1D458 shall help determine what we consider the 'tail' of the covariance matrix. Definition 2.2. For a large enough constant /u1D44F (that will be fixed henceforth), define  where the minimum of the empty set is defined as ∞ . We are now ready to introduce the assumptions of our paper. Assumptions. Let /u1D450 /u1D465 and /u1D450 /u1D466 denote absolute constants. (A.1) The samples (/u1D465 1 , /u1D466 1 ), … , (/u1D465 /u1D45B , /u1D466 /u1D45B ) are drawn i.i.d. (A.2) The covariates /u1D465 and responses /u1D466 are mean-zero. (A.3) The covariates /u1D465 satisfy /u1D465 = Σ 1/2 /u1D462 , where /u1D462 is isotropic and has components that are independent /u1D450 /u1D465 -sub-Gaussian random variables, that is, for all /u1D719 ∈ ℝ /u1D451  (A.4) The difference /u1D466 -/u1D465 Θ ⋆ is /u1D450 /u1D466 -sub-Gaussian, conditionally on /u1D465 ; that is, for all /u1D719 ∈ ℝ /u1D45E  (note that this implies that /u1D53C [/u1D466 ∣ /u1D465 ] = /u1D465 Θ ⋆ and /u1D53C [ ‖/u1D466 - /u1D465 Θ ⋆ ‖ 2 ] ≤ /u1D450/u1D45E ). (A.5) Almost surely, the projection of the data /u1D44B on the space orthogonal to any eigenvector of Σ spans a space of dimension /u1D45B . All the constants going forward may depend on the values of /u1D450 /u1D465 and /u1D450 /u1D466 . The assumptions made here are standard in the benign overfitting literature [see Bar+20; CLB22]. They are satisfied for example in the case where /u1D465 is a mean-zero Gaussian whose covariance Σ has full rank, /u1D451 > /u1D45B , and the noise /u1D466 - /u1D465 Θ ⋆ is independent and Gaussian.",
  "2.3 Deep Linear Models": "We analyze linear models represented by deep linear networks with /u1D45A hidden units at each layer. We denote the weight matrices by /u1D44A 1 , … , /u1D44A /u1D43F , where /u1D44A 1 ∈ ℝ /u1D45A×/u1D451 , /u1D44A 2 , … , /u1D44A /u1D43F-1 ∈ ℝ /u1D45A×/u1D45A , and /u1D44A /u1D43F ∈ ℝ /u1D45E×/u1D45A . The standard representation of the network's linear transformation, denoted by Θ ∈ ℝ /u1D451×/u1D45E , is  6 Define /u1D443 /u1D44B to be the projection onto the row span of /u1D44B , that is, /u1D443 /u1D44B ∶= /u1D44B ⊤ (/u1D44B /u1D44B ⊤ ) -1 /u1D44B . Let  For /u1D45B datapoints (/u1D465 1 , /u1D466 1 ), … , (/u1D465 /u1D45B , /u1D466 /u1D45B ) , where /u1D465 /u1D456 ∈ ℝ 1×/u1D451 and /u1D466 /u1D456 ∈ ℝ 1×/u1D45E , the training loss is given by  Wewill analyze the generalization properties of deep linear models trained with gradient flow, that is, for all /u1D457 ∈ [/u1D43F] ,  We study the following random initialization scheme in our paper. Definition 2.3. (Random initialization) Given /u1D6FC , /u1D6FD > 0 , the entries of the first layer /u1D44A (0) 1 and the last layer /u1D44A (0) /u1D43F are initialized using i.i.d. draws from N (0, /u1D6FC 2 ) and N (0, /u1D6FD 2 ) respectively. The remaining layers /u1D44A (0) 2 , … , /u1D44A (0) /u1D43F-1 are initialized to the identity /u1D43C /u1D45A . A similar initialization scheme has been studied previously [ZLG20]. Our analysis will show that starting from random initialization the scale of the network grows in a controlled manner which is captured by the following definition. Definition 2.4. Wesay that training is perpetually Λ bounded if, for all /u1D461 ≥ 0 and all /u1D446 ⊆ [/u1D43F] ,  In our subsequent analysis, this notion of perpetually Λ bounded shall allow us to control the behavior of the network in the null space of the data matrix /u1D44B .",
  "2.4 The Minimum /u1D4C1 2 -norm Interpolant": "It will be helpful to compare the generalization of the deep linear model with the result of applying the minimum /u1D4C1 2 -norm interpolant resulting from the standard parameterization. Definition 2.5. For any /u1D44B ∈ ℝ /u1D45B×/u1D451 and /u1D44C ∈ ℝ /u1D45B×/u1D45E , define Θ/u1D4C1 2 = /u1D44B ⊤ (/u1D44B /u1D44B ⊤ ) -1 /u1D44C . Under Assumption (A.5), the matrix /u1D44B /u1D44B ⊤ is full rank and therefore Θ/u1D4C1 2 is well defined. As previously noted, the excess risk of this canonical interpolator has been studied in prior work [see Bar+20; TB20]. 7",
  "3 Main Results": "In this section, we present our excess risk bounds. Our first result applies to any deep linear model trained until interpolation. Second, we shall specialize this result to the case where the model is randomly initialized. Lastly, we present an excess risk bound for a randomly initialized network in a setting with a spiked covariance matrix.",
  "3.1 Excess Risk bound for Deep Linear Models": "The following theorem is an excess risk bound for any deep linear model trained until it interpolates in terms of the rate of convergence of its training, along with the effective ranks of the covariance matrix. Theorem 3.1. Under Assumptions (A.1)-(A.5), there is an absolute constant /u1D450 > 0 such that, for all /u1D6FF < 1/2 and all depths /u1D43F > 1 , the following holds. With probability at least 1 - /u1D450/u1D6FF , if Θ = lim/u1D461→∞ Θ (/u1D461 ) for a perpetually Λ -bounded training process for which lim/u1D461 →∞ L (Θ (/u1D461 ) ) = 0 , and /u1D45B ≥ /u1D450 max{/u1D45F 0 , /u1D458 , log(1//u1D6FF )} , then  where and  This bound shows that the conditional bias of the estimator Θ is upper bounded by the conditional bias of the minimum /u1D4C1 2 -norminterpolant Bias (Θ/u1D4C1 2 ) plus GLYPH<4> , which is the additional bias incurred by the component of Θ outside the row span of /u1D44B . This additional term GLYPH<4> depends not only on the eigenvalues of the covariance matrix but also on the specifics of the optimization procedure such as the initial linear model ( Θ (0) ), the size of the weights throughout training (Λ) and the rate of decay of the loss. Essentially matching lower bounds (up to constants) on the variance term are known [Bar+20]. Interestingly, the conditional variance of the interpolator Θ , is in fact identical to the conditional variance of the minimum /u1D4C1 2 -norm interpolant. This follows because, as we will show in the proof, the component of the interpolator Θ in the row span of /u1D44B is in fact equal to Θ/u1D4C1 2 , and the conditional variance depends only on this component within the row span of /u1D44B . The variance captures the effect of perfectly fitting the noise in the data, and our analysis shows that the harm incurred by fitting the noise is unaffected by parameterizing a linear model as a deep linear model.    8",
  "3.2 Excess Risk Bound under Random Initialization": "Our next main result establishes a high-probability bound on the excess risk, and in particular on GLYPH<4> , when the network is trained after a random initialization (see Definition 2.3). Theorem 3.2. Under Assumptions (A.1)-(A.5), there is an absolute constant /u1D450 > 0 such that, for all /u1D6FF < 1/2 , if · the initialization scales /u1D6FD and /u1D6FC satisfy /u1D6FD ≥ /u1D450 max { 1, /u1D706 1/4 1 √ /u1D43F/u1D45B ( √ ‖Θ ⋆ ‖/u1D706 1/4 +/u1D45E 1/4 ) √ /u1D460 /u1D458 } and /u1D6FC ≤ 1 ; · the width /u1D45A ≥ /u1D450 max { /u1D451 + /u1D45E + log(1//u1D6FF ), /u1D43F 2 /u1D6FC 2 /u1D706 1 /u1D460 0 /u1D45B 2 /u1D45E log(/u1D45B//u1D6FF ) /u1D6FD 2 /u1D460 2 /u1D458 } ; · the network is trained using random initialization as described in Definition 2.3; · the number of samples satisfies /u1D45B ≥ /u1D450 max{/u1D45F 0 , /u1D458 , log(1//u1D6FF )} , then, with probability at least 1 - /u1D450/u1D6FF ,  where    Note that the bound on GLYPH<4> of Theorem 3.2 can be made arbitrarily small by decreasing /u1D6FC while keeping the other parameters fixed. When /u1D6FC = 0 , our bound shows that the model has the same risk as the minimum /u1D4C1 2 -norm interpolant. Regarding the role of overparameterization, we find that one component of our bound on GLYPH<4> gets smaller as the width /u1D45A is increased. However, our bound gets larger as we increase depth /u1D43F . Recall from the simulation in Figure 1 that as the initialization of the last layer /u1D6FD approaches 0 , the model produced by gradient descent gets closer to the minimum /u1D4C1 2 -norm interpolant. Our bound on GLYPH<4> does not approach 0 as /u1D6FD →0 , and we do not know how to prove that this happens in general with high probability. As mentioned earlier, the bound on the conditional variance, which captures the effect of fitting the noise, is sharp up to constants, however we do not know whether the upper bound on the conditional bias, and specifically GLYPH<4> in Theorem 3.2, can be improved. It is also unclear whether conditions on /u1D6FD and /u1D45A can be relaxed. Next, to facilitate the interpretation of our bounds, we apply Theorem 3.2 in a canonical setting where benign overfitting occurs for the minimum /u1D4C1 2 -norm interpolant. 9 Definition 3.3 ( (/u1D458 , /u1D700) -spike model) . For 0 < /u1D700 < 1 and /u1D458 ∈ ℕ , a (/u1D458 , /u1D700) -spike model is a setting where the eigenvalues of Σ are /u1D706 1 = … = /u1D706 /u1D458 = 1 and /u1D706 /u1D458 +1 = … = /u1D706 /u1D451 = /u1D700 . The (/u1D458 , /u1D700) -spike model is a setting where there are /u1D458 high variance directions, and many (/u1D451 - /u1D458 ) low variance directions that can be used to 'hide' the energy of the noise. Note that, in this model, if /u1D451 ≥ /u1D450 /u1D45B and /u1D45B ≥ /u1D450 /u1D458 for a large enough constant /u1D450 , then /u1D458 satisfies the requirement of Definition 2.2, since /u1D45F /u1D458 = /u1D700 (/u1D451 -/u1D458 )//u1D700 = /u1D451 -/u1D458 ≥ /u1D44F/u1D45B . Since this covariance matrix has full rank, it may be used in one of the concrete settings where all of our assumptions are satisfied described at the end of Section 2.2. Corollary 3.4. Under Assumptions (A.1)-(A.5), there is an absolute constant /u1D450 > 0 , such that, for any 0 < /u1D700 < 1 and /u1D458 ∈ ℕ , if Σ is an instance of the (/u1D458 , /u1D700) -spike model, for any input dimension /u1D451 , output dimension /u1D45E , depth /u1D43F > 1 , and number of samples /u1D45B , there are initialization scales /u1D6FC > 0 and /u1D6FD > 0 such that the following holds. For all /u1D6FF < 1/2 , if · the width /u1D45A ≥ /u1D450 (/u1D451 + /u1D45E + log(1//u1D6FF )) ; · the network is trained as described in Section 2.3; · the input dimension /u1D451 ≥ /u1D450 /u1D45B ; · the number of samples /u1D45B ≥ /u1D450 max{/u1D458 + /u1D700 /u1D451 , log(1//u1D6FF )} , then, with probability at least 1 - /u1D450/u1D6FF ,  Corollary 3.4 gives the simple bound obtained by a choice of parameters that includes a sufficiently small value of /u1D6FC . For larger values of /u1D6FC the bound of Theorem 3.2 may behave differently in the case of the (/u1D458 , /u1D700) -spike model. We find that if we regard ‖Θ ⋆ ‖ 2 as a constant then, the excess risk approaches zero if  which recovers the known sufficient conditions for the minimum /u1D4C1 2 -norm interpolant to benignly overfit in this setting. One example is where  and /u1D45B→∞ . 10",
  "4 Proof of Theorem 3.1": "The proof of Theorem 3.1 needs some lemmas, which we prove first. Throughout this section the assumptions of Theorem 3.1 are in force. A key point is that the projection of any interpolator onto the row span of /u1D44B , including the model output by training a deep linear network, is the minimum /u1D4C1 2 -norm interpolant. Lemma 4.1. For any interpolator Θ , Θ/u1D44B = /u1D443 /u1D44B Θ = Θ/u1D4C1 2 . Proof. Since Θ interpolates the data  Recall that Θ/u1D44B = /u1D44B ⊤ (/u1D44B /u1D44B ⊤ ) -1 /u1D44B Θ = /u1D443 /u1D44BΘ , where /u1D443 /u1D44B projects onto the row span of /u1D44B . Continuing, we get that  glyph[squaresolid] Using the formula for the minimum /u1D4C1 2 -norm interpolant, we can now write down an expression for the excess risk. Lemma 4.2. The excess risk of any interpolator Θ of the data satisfies  with probability at least 1 - /u1D6FF over the noise matrix Ω = /u1D44C - /u1D44B Θ ⋆ , where  Proof. We have /u1D466 - /u1D465 Θ ⋆ is conditionally mean-zero given /u1D465 , thus  Since Θ interpolates the data, by Lemma 4.1 we know that  11 Now because /u1D44C = /u1D44B Θ ⋆ + Ω we find that  where (/u1D456 ) follows by using the cyclic property of the trace, and (/u1D456 /u1D456 ) follows by the definition of the matrices /u1D435 and /u1D436 . Let /u1D714 1 , … , /u1D714 /u1D45E denote the columns of the error matrix Ω . Then  Invoking [Bar+20, Lemma S.2] bounds each term in the sum by /u1D450 /u1D45E log(/u1D45E//u1D6FF )Tr(/u1D436 ) with probability at least 1 - /u1D6FF //u1D45E . A union bound completes the proof. glyph[squaresolid] To work on the first term in the upper bound of the excess risk, we would like an upper bound on ‖Θ (/u1D461 ) /u1D44B ⟂ ‖ . Toward this end, we first establish a high-probability bound on ‖/u1D44B ‖ /u1D45C /u1D45D . Lemma 4.3. There is a constant /u1D450 > 0 such that for any /u1D6FF ∈ (0, 1) , if /u1D45B ≥/u1D450 max{/u1D45F 0 , log ( 1 /u1D6FF ) }, with probability at least 1 - /u1D6FF , ‖/u1D44B ‖ /u1D45C /u1D45D ≤ /u1D450 √ /u1D706 1 /u1D45B . Proof. By [KL17, Lemma 9], with probability at least 1 - /u1D6FF  Recalling that /u1D45B ≥ /u1D450 max{/u1D45F 0 , log(1//u1D6FF )} , this implies that, with probability at least 1-/u1D6FF , ‖/u1D44B ‖ /u1D45C /u1D45D ≤ /u1D450 √ /u1D45B‖Σ‖ /u1D45C /u1D45D . glyph[squaresolid] Next, we will calculate a formula for the time derivative of Θ (/u1D461 ) . Its definition will make use of products of matrices before and after a given layer. 12 Definition 4.4. For /u1D457 ∈ [/u1D43F] define /u1D434 /u1D457 = ∏ /u1D457+1 /u1D458 =/u1D43F /u1D44A (/u1D461 ) /u1D458 and /u1D435 /u1D457 = ∏ 1 /u1D458 =/u1D457 -1 /u1D44A (/u1D461 ) /u1D458 . Now we are ready for our lemma giving the time derivative of Θ (/u1D461 ) . Lemma 4.5. At any time /u1D461 ≥ 0 ,  Proof. Let us suppress the superscript (/u1D461 ) to ease notation. The gradient flow dynamics is defined as  where  So by the chain rule of differentiation,  glyph[squaresolid] Toward the goal of proving a high-probability bound on ‖Θ (/u1D461 ) /u1D44B ⟂ ‖ , we next bound its rate of growth. Lemma 4.6. There is a constant /u1D450 > 0 such that, if /u1D45B ≥ /u1D450 max{/u1D45F 0 , log(1//u1D6FF )} , with probability at least 1 - /u1D6FF , if training is perpetually Λ bounded, then, for all /u1D461 ≥ 0 ,  13 Proof. Given matrices /u1D434 and /u1D435 , we let /u1D434 ⋅ /u1D435 = Tr(/u1D434 ⊤ /u1D435) denote the matrix inner product. By the chain rule,  where (/u1D456 ) follows by the formula derived in Lemma 4.5. Let us consider a particular term in the sum above,  In the case where /u1D457 = 1 , the RHS is equal to  since /u1D443 /u1D44B ⟂ /u1D44B ⊤ = 0 . In the case /u1D457 > 1 , we have  completing the proof. glyph[squaresolid] Lemma 4.7. There is a constant /u1D450 > 0 such that, if /u1D45B ≥ /u1D450 max{/u1D45F 0 , log(1//u1D6FF )} , with probability at least 1 - /u1D6FF , if training is perpetually Λ bounded, then, for all /u1D461 ≥ 0 ,  14 Proof. Let us consider one of the terms in the RHS of Lemma 4.6. We have  where (/u1D456 ) follows since for any matrices ‖/u1D434/u1D435‖ ≤ ‖/u1D434‖ /u1D45C /u1D45D ‖/u1D435‖ , and (/u1D456 /u1D456 ) follows since training is perpetually Λ bounded. Summing over layers /u1D457 = 2, … , /u1D43F , we get that,  Now note that,  which in turn implies that, when ‖Θ (/u1D461 ) /u1D44B ⟂‖ > 0 , we have  If, for all /u1D460 ∈ [0, /u1D461 ] , we have ‖Θ (/u1D461 ) /u1D44B ⟂‖ ≠ 0 , then by integrating this differential inequality we conclude that  Otherwise, if /u1D447 = sup{/u1D460 ∶ ‖Θ (/u1D460 ) /u1D44B ⟂‖ = 0} ,  which implies (7). Applying Lemma 4.3 which is a high probability upper bound on ‖/u1D44B ‖ /u1D45C /u1D45D completes the proof. glyph[squaresolid] 15 Armed with these lemmas, we are now ready to prove the first of our main results. Proof of Theorem 3.1. Combining Lemma 4.2 with Lemmas 6 and 11 by Bartlett et al. [Bar+20] to bound Tr(/u1D436 ) we get, with probability at least 1 - /u1D450/u1D6FF ,  We begin by bounding the first term in the RHS above. Let /u1D703 ⋆ 1 , … , /u1D703 ⋆ /u1D45E be the columns of Θ ⋆ and /u1D703 /u1D44B ⟂ ,1 , … , /u1D703 /u1D44B ⟂ ,/u1D45E be the columns of Θ/u1D44B ⟂ . By invoking [CLB22, Eq. 54] for each of the /u1D45E outputs, and applying a union bound, we find that with probability at least 1-/u1D450 /u1D45E(/u1D6FF //u1D45E ) = 1-/u1D450/u1D6FF ,  Define Bias ( GLYPH<2> /u1D4C1 2 ) ∶= 2/u1D450 /u1D460 /u1D458 /u1D45B ‖Θ ⋆ ‖ 2 and let GLYPH<4> ∶= 2/u1D450 /u1D460 /u1D458 /u1D45B ‖Θ/u1D44B ⟂ ‖ 2 . The bound on GLYPH<4> follows by invoking Lemma 4.7. glyph[squaresolid]",
  "5 Proof of Theorem 3.2": "The assumptions of Theorem 3.2 are in force throughout this section. Before starting its proof, we establish some lemmas. Definition 5.1. For a large enough absolute constant /u1D450 , we say that the network enjoys a /u1D6FF -good initialization if  and  The following proposition is proved in Appendix B. It guarantees that for wide networks, optimization is successful starting from random initialization. 16 Proposition 5.2. There is a constant /u1D450 such that, given any /u1D6FF ∈ (0, 1) , if the initialization scales /u1D6FC and /u1D6FD , along with the network width /u1D45A , satisfy  /u1D6FC ≤ 1, then with probability at least 1 - /u1D6FF : 1. the initialization is /u1D6FF -good; 2. training is perpetually /u1D450 (/u1D6FC + 1//u1D43F)/u1D6FD bounded; 3. for all /u1D461 > 0 , we have that  The reader may notice that the roles of /u1D6FC and /u1D6FD in Proposition 5.2 are asymmetric. We focused on that case that /u1D6FC is small because the updates of /u1D44A 1 are in the span of the rows of /u1D44B , which is not necessarily the case for the other layers, including /u1D44A /u1D43F . This means that the scale of /u1D44A 1 in the null space of /u1D44B remains the same as it was at initialization, so that a small scale at initialization pays dividends throughout training. The next lemma shows that the projection of the model computed by the network onto the null space of /u1D44B is the same as the model obtained by projecting the first layer weights, and combining them with the other layers. Lemma 5.3. For all /u1D461 ≥ 0 ,   Proof. By definition  Therefore, where  glyph[squaresolid] 17 The subsequent lemma shows that the projection of the first layer onto the null space of /u1D44B does not change during training.  Proof. We have  glyph[squaresolid] By using the previous two lemmas regarding the first layer weights /u1D44A 1 we can now prove an alternate bound on ‖Θ (/u1D461 ) /u1D44B ⟂ ‖ . In contrast to the previous bound that we derived in Lemma 4.7, here the initial scale of /u1D44A 1 plays a role in controlling the growth in ‖Θ (/u1D461 ) /u1D44B ⟂ ‖ . Lemma 5.5. There is constant /u1D450 > 0 such that, if training is perpetually Λ bounded, then, for all /u1D461 ≥ 0 ,  Proof. Let us once again consider one of the terms in the RHS of Lemma 4.6. We have  18 Continuing by using the fact that for any matrices ‖/u1D434/u1D435‖ ≤ ‖/u1D434‖ /u1D45C /u1D45D ‖/u1D435‖ , we get that  since ‖/u1D44A (0) 1,/u1D44B ⟂ ‖ /u1D45C /u1D45D ≤ ‖/u1D44A (0) 1 ‖ /u1D45C /u1D45D , where (/u1D456 ) follows since training is Λ perpetually bounded and so  and (ii) follows since by Lemma 5.4, /u1D44A (/u1D461 ) 1,/u1D44B ⟂ = /u1D44A (0) 1,/u1D44B ⟂ . Summing over layers /u1D457 = 2, … , /u1D43F , we get that,  Thus, we have that  which in turn implies that, when ‖Θ (/u1D461 ) /u1D44B ⟂‖ ≠ 0 , we have  Therefore, by integrating this differential inequality as in the proof of Lemma 4.7, we conclude that  glyph[squaresolid] We also need a lemma that bounds the Frobenius norm of the data matrix /u1D44B . Lemma 5.6. There is a constant /u1D450 > 0 such that for any /u1D6FF ∈ (0, 1) , if /u1D45B ≥ /u1D450 log(1//u1D6FF ) , then with probability at least 1 - /u1D6FF , ‖/u1D44B ‖ ≤ /u1D450 √ /u1D45B/u1D460 0 . 19 Proof. The rows of /u1D44B are /u1D45B i.i.d. draws from a distribution, where each sample can be written as /u1D465 /u1D456 = Σ 1/2 /u1D462 /u1D456 , where /u1D462 /u1D456 has components that are independent /u1D450 /u1D465 -sub-Gaussian random variables. Define /u1D462 stacked ∶= (/u1D462 1 , /u1D462 2 , … , /u1D462 /u1D45B ) ∈ ℝ /u1D451/u1D45B to be concatenation of the vectors /u1D462 1 , … , /u1D462 /u1D45B and define Σ 1/2 stacked ∈ ℝ /u1D451/u1D45B×/u1D451 /u1D45B to be a block diagonal matrix with Σ 1/2 ∈ ℝ /u1D451×/u1D451 repeated /u1D45B times along its diagonal. Then,  Now, /u1D462 stacked is an isotropic, /u1D450 /u1D465 -sub-Gaussian random vector. Therefore, by applying [Ver18, Theorem 6.3.2] we know that the sub-Gaussian norm [Ver18, Definition 2.5.3] of ‖/u1D44B‖ = ‖Σ 1/2 stacked /u1D462 stacked ‖ is  Therefore, by Hoeffding's bound [Ver18, Proposition 2.5.2] we get that  Setting /u1D702 2 = /u1D45B/u1D460 0 //u1D706 1 = /u1D45B/u1D45F 0 and noting that /u1D45B ≥ log(1//u1D6FF ) ≥ log(1//u1D6FF )//u1D45F 0 completes the proof. glyph[squaresolid] Finally, we have a simple lemma that bounds the Frobenius norm of the responses /u1D44C . Lemma 5.7. There is a constant /u1D450 > 0 such that for any /u1D6FF ∈ (0, 1) , if /u1D45B ≥ /u1D450 log(1//u1D6FF ) , then with probability at least 1 - /u1D6FF , ‖/u1D44C ‖ ≤ /u1D450(‖/u1D44B ‖ /u1D45C /u1D45D ‖Θ ⋆ ‖ + √ /u1D45E/u1D45B) . Proof. Note that /u1D44C = /u1D44B Θ ⋆ + Ω , and therefore  where the last inequality follows since for any matrices ‖/u1D434/u1D435‖ ≤ ‖/u1D434‖ /u1D45C /u1D45D ‖/u1D435‖ . Now each entry in Ω ∈ ℝ /u1D45B×/u1D45E is a zero-mean and /u1D450 /u1D466 -sub-Gaussian. Therefore, by Bernstein's bound [Ver18, Theorem 2.8.1],  Now /u1D53C [ ‖Ω‖ 2 ] = /u1D45B/u1D53C [ ‖/u1D466 - /u1D465 Θ ⋆ ‖ 2 ] ≤ /u1D450 2 /u1D45E /u1D45B , by Assumption (A.4), and 2 exp(-/u1D45E/u1D45B) ≤ /u1D6FF since /u1D45B ≥ /u1D450 log(1//u1D6FF ) ≥ /u1D450 log(1//u1D6FF )//u1D45E . Thus, with probability at least 1 - /u1D6FF  Combining this with Eq. (8) completes the proof. glyph[squaresolid] With all of the pieces in place we are now ready to prove the theorem. Proof of Theorem 3.2. Define a 'good event' E as the intersection of the following events: 20 · E 1 , the excess risk bound stated in Theorem 3.1 holds. · E 2 , the bounds stated in Proposition 5.2 hold. · E 3 , ‖/u1D44B ‖ /u1D45C /u1D45D ≤ /u1D450 √ /u1D706 1 /u1D45B . · E 4 , ‖/u1D44B ‖ ≤ /u1D450 √ /u1D460 0 /u1D45B . · E 5 , /u1D70E /u1D45A/u1D456 /u1D45B (/u1D44B) ≥ √ /u1D460 /u1D458 /u1D450 . · E 6 , ‖/u1D44C ‖ ≤ /u1D450 ( ‖/u1D44B ‖ /u1D45C /u1D45D ‖Θ ⋆ ‖ + √ /u1D45E/u1D45B ) . Now, Theorem 3.1 and Proposition 5.2 each hold with probability at least 1 - /u1D450 /u1D6FF . Lemma 4.3 implies that the event E 3 holds with probability at least 1 - /u1D6FF . By Lemma 5.6, the event E 4 holds with probability at least 1 - /u1D6FF . For E 5 , notice that  where /u1D44B ∶/u1D458 are the first /u1D458 columns of /u1D44B and /u1D44B /u1D458 ∶ are the last /u1D451 - /u1D458 columns of /u1D44B . Since /u1D45B ≥ /u1D450 log(1//u1D6FF ) , by [Bar+20, Lemma 9] we know that with probability at least 1 - /u1D6FF √  Finally, by Lemma 5.7 event E 6 holds with probability at least 1 - /u1D6FF . Therefore, by a union bound the good event E holds with probability at least 1-/u1D450 ′ /u1D6FF . Let us assume that this event occurs going forward in the proof. Proposition 5.2 guarantees that the training process is /u1D450 2 (/u1D6FC + 1//u1D43F)/u1D6FD -perpetually bounded and the loss converges to zero. Therefore, by applying Theorem 3.1, the risk is bounded by  where    In the rest of the proof we shall bound the term GLYPH<4> . For this, we would like to apply Proposition 5.2, which we can, since /u1D6FC ≤ 1 ,   21 and  Thus, by Proposition 5.2 we know that for all /u1D461 > 0 ,  Integrating the RHS above we get that  Proposition 5.2 also guarantees that the initialization is /u1D6FF -good. That is, ‖/u1D44A (0) 1 ‖ /u1D45C /u1D45D ≤ /u1D450 11 /u1D6FC and ‖/u1D44A (0) /u1D43F ‖ ≤ /u1D450 11 /u1D6FD . So,  where (/u1D456 ) follows since the initialization was good, and the ranks of /u1D44A (0) 1 and /u1D44A (0) /u1D43F are bounded by /u1D451 and /u1D45E respectively. 22 Plugging the bounds obtained in Eqs. (10) and (11) into Eq. (9) we have that  This completes our proof.",
  "6 Proof of Corollary 3.4": "When Σ is an instance of the (/u1D458 , /u1D700) -spike model we find that  First, for a large enough /u1D450 1 , we set  Given this choice of /u1D6FD , for any /u1D45E , /u1D45B, /u1D458 , /u1D451 , /u1D43F, if /u1D6FC > 0 is chosen to be small enough then,  Also by the assumption on the number of samples, /u1D45B ≥ /u1D450 2 max{/u1D458 + /u1D700 /u1D451 , log(1//u1D6FF )} ≥ /u1D450 2 max{/u1D45F 0 , /u1D458 , log(1//u1D6FF )} . We are now in position to invoke Theorem 3.2. By this theorem we get that,  Recall from above that the upper bound on GLYPH<4> scales with /u1D6FC 2 . Thus, for small enough /u1D6FC it is a lower order term. glyph[squaresolid] 23",
  "7 Additional Simulations and Details": "",
  "8 Discussion": "Wehave provided upper bounds on the excess risk for deep linear networks that interpolate the data with respect to the quadratic loss, and presented simulation studies that verify that the some aspects of our bounds reflect typical behavior. As mentioned in the introduction, our analysis describes a variety of conditions under which the generalization behavior of interpolating deep linear networks is similar, or the same, as the behavior of the minimum /u1D4C1 2 -norm interpolant with the standard parameterization. Among other things, this motivates study of loss functions other than the quadratic loss used in this work. The softmax loss would be a natural choice. 25 Looking at our proofs, it appears that the only way that a deep linear parameterization can promote benign overfitting is for the function computed by the network at initialization to approximate the regression function. (Formalizing this with a lower bound, possibly in the case of random initialization, or with an arbitrary initialization and a randomly chosen regression function Θ ⋆ , is a potential topic for further research.) The benefits of a good approximation to the regression function at initialization has been explored in the case of two-layer linear networks [CLB22]. Extending this analysis to deep networks is a potential subject for further study. We focused on a particular random initialization scheme in this paper, it is possible to study other initialization schemes as well. For example, we believe that, if the width /u1D45A of the network is somewhat larger, a similar analysis should go through without our simplifying assumption that /u1D44A 2 , … , /u1D44A /u1D43F-1 are initialized exactly to the identity, and instead are initialized randomly. Recently, Mallinar et al. [Mal+22] established conditions under which interpolation with the minimum /u1D4C1 2 -norm intepolator is 'tempered', achieving risk within a constant factor of the Bayes risk. Here we show that the risk of interpolating deep linear networks is (nearly) equal to the risk of the minimum /u1D4C1 2 -norm interpolator, this implies that when the minimum /u1D4C1 2 -norm interpolator is tempered, so is the output of the deep linear model. We hope that our techniques lay the groundwork for other results about tempered overfitting. While here we analyzed the network obtained by the continuous-time gradient flow it is straightforward to use our techniques to obtain similar results for gradient descent with small enough step-size at the expense of a more involved analysis. As mentioned after the statement of Theorem 3.2, its bounds could potentially be improved. (We have not attempted to prove any lower bounds in this work.) In Figure 1 we found that when the scale of the random initialization of either the first (/u1D6FC ) or the last layer (/u1D6FD ) goes to zero the trained model approach the minimum /u1D4C1 2 -norm interpolator. Understanding why and when this happens is an avenue for future research. Finally, examining the extent to which the effects described here carry over when nonlinearities are present is a natural next step.",
  "Acknowledgements": "We thank anonymous reviewers for their careful reading of an earlier version of this paper and their valuable feedback.",
  "A Additional Related Work": "In this appendix, we describe a wider variety of related work. 26",
  "A.1 Benign Overfitting and Double Descent": "This subsection includes descriptions of some of the most closely related work that we know. For a wider sample, we point the interested reader to a couple of surveys [BMR21; Bel21]. Techniques have also been developed to upper bound the excess risk of the sparsityinducing minimum /u1D4C1 1 -norm interpolant [Koe+21; LW21; WDY22; Don+22]. Furthermore, lower bounds on the excess risk that show that sparsity can be incompatible with benign overfitting, and that the excess risk of sparse interpolators maybe exponentially larger than that of dense interpolators have also been derived [CL22]. Papers have studied the excess risk of the minimum /u1D4C1 2 -norm interpolant [Bar+20; Has+22; Mut+20; BL21], which is obtained as a result of minimizing the squared loss using gradient descent with no explicit regularization. While these previous papers directly analyzed the closed form expression of the minimum /u1D4C1 2 -norm interpolant, followup work [NDR20; Koe+21; CL20; CLG22] employed tools from uniform convergence to analyze its excess risk. Prior work [KLS20; WX20; TB20] analyzed ridge regression with small or even negative regularization, and identified settings where using zero or even negative regularization can be optimal. Kernel ridgeless regression has been actively studied [LR20; MM19]. Careful theoretical analysis and simulations have revealed that kernel 'ridgeless' regression can lead to multiple descent curves [LRZ20]. A handful of papers have analyzed the risk of random features models [MM19; LZG21]. Furthermore, this phenomenon has been studied in nearest neighbor models [BHM18], latent factor models [BSW22] and the Nadaraya-Watson estimator [BRT19] with a singular kernel. Several papers have also studied benign overfitting in linear classification of the canonical maximum /u1D4C1 2 -margin classifier [Mon+19; DKT22; CL21; HMX21; Mut+21; WT21; CGB21], the maximum /u1D4C1 1 -margin classifier [LS22], and classifiers obtained by minimizing polynomiallytailed classification losses [Wan+21]. Results have also been obtained on data that is linearly separable with two-layer leaky ReLU networks [FCB22], and with two-layer convolutional networks with smooth nonlinearities [Cao+22]. Shamir [Sha22] highlighted the importance of the choice of the loss function, since an interpolator that benignly overfits with respect one loss function may not with respect to another one.",
  "A.2 Implicit Bias": "In the closely related problem of matrix factorization, several papers [Gun+17; Aro+19] established conditions under which the solution of gradient flow converges to the minimum nuclear norm solution. This rank minimization behavior of gradient flow was also shown to approximately hold for ReLU networks [TVS22]. Papers have also studied the implicit bias of gradient descent for diagonal linear networks and found that it depends on the scale of the initialization and step-size [Woo+20; YKM21; Nac+22]. They found that depending on the scale of the initialization and step-size 27 the network converges to the minimum /u1D4C1 2 -norm solution (kernel regime), or to the minimum /u1D4C1 1 -norm solution (rich regime) or to a solution that interpolates between these two norms. Gunasekar et al. [Gun+18b] and Jagadeesan, Razenshteyn, and Gunasekar [JRG22] studied linear convolutional networks and found that under this parameterization of a linear model, gradient descent implicitly minimizes norms of the Fourier transform of the predictor. Techniques have also been developed to study the implicit bias of mirror descent [Gun+18a; Li+22]. As a counterpoint to this line of research, Arora et al. [Aro+19] and Razin and Cohen [RC20] raised the possibility that the implicit bias of deep networks may be unexplainable by a simple function such as a norm. Li, Luo, and Lyu [LLL21] showed that under certain conditions gradient flow with infinitesimal initialization is equivalent to a simple heuristic rank minimization algorithm. Vardi and Shamir [VS21] showed that it might be impossible altogether to capture the implicit bias of even two-layer ReLU networks using any functional form. For linear classifiers, minimizing exponentially-tailed losses including the logistic loss leads to the maximum /u1D4C1 2 -margin classifier [Sou+18; JT19; NSS19]. Ji and Telgarsky [JT18] found that this is also the case when the linear classifier is parameterized as a deep linear classifier.",
  "A.3 Optimization of Deep Linear Networks": "Several papers have studied deep linear networks as a means to understand the benefit of overparameterization while optimizing nonlinear networks. Arora, Cohen, and Hazan [ACH18] argued that depth promotes a form of implicit acceleration when performing gradient descent on deep linear networks. While other papers [DH19] showed that for wide enough networks that are randomly initialized by Gaussians the loss converges at a linear rate. Other papers have analyzed the convergence rate under other initialization schemes such as orthogonal initialization [HXP20] and near-identity initialization [BHL19; ZLG20]. Rates of convergence for accelerated methods such as Polyak's heavy ball method have also been established [WLA21]. Saxe, McClelland, and Ganguli [SMG14] analyzed the effect of initialization and step-size in training deep linear networks. Kawaguchi [Kaw16] identified a number of properties of the loss landscape of deep linear networks, including the absence of suboptimal local minima; Achour, Malgouyres, and Gerchinovitz [AMG21] characterized global minima, strict saddles, and non-strict saddles for these landscapes.",
  "B Proof of Proposition 5.2": "In this appendix, we prove Proposition 5.2. Its proof uses some technical lemmas, which we derive first. It is useful to recall the definition of a /u1D6FF -good initialization from above. 28 Definition 5.1. For a large enough absolute constant /u1D450 , we say that the network enjoys a /u1D6FF -good initialization if  and  The next two lemmas shall be useful in showing that our initialization scheme leads to a /u1D6FF -good initialization. First, to bound the singular values of the weight matrices at initialization we apply the following result from Vershynin [Ver10]. LemmaB.1. There exists a constant /u1D450 such that given any /u1D6FF ∈ (0, 1) if /u1D45A ≥ /u1D450 (/u1D451 +/u1D45E +log(1//u1D6FF )) , then with probability at least 1 - /u1D6FF /2  Proof. By [Ver10, Corollary 5.35] we have that with probability at least 1 - 4/u1D452 -/u1D702 2 /2  So since /u1D45A ≥ /u1D450 (/u1D451 + /u1D45E + log(1//u1D6FF )) , where /u1D450 is a large enough constant, by picking /u1D702 = √ /u1D45A/8 we get that with probability at least 1 - /u1D6FF /2   completing the proof. glyph[squaresolid] The next lemma shows that the loss is controlled at initialization. Lemma B.2. There is a positive constant /u1D450 such that, for any /u1D6FF ∈ (0, 1) , provided that /u1D45A ≥ /u1D450 (/u1D451 + /u1D45E + log(1//u1D6FF )) , with probability at least 1 - /u1D6FF /2  Proof. The lemma directly follows by invoking [ZLG20, Proposition 3.3]. glyph[squaresolid] 29 The next lemma shows that if the weights remain close to their initial values then the loss decreases at a certain rate. LemmaB.3. If /u1D6FD ≥ 2 and there was a good initialization (see Definition 5.1), at any time /u1D461 > 0 if, for all /u1D457 ∈ [/u1D43F]  then  Proof. By the chain rule, we have that  Further, observe that  Continuing by applying [ZLG20, Lemma B.3] to the RHS of the inequality above we get that,  30 where (/u1D456 ) follows since on a good initialization for all /u1D457 ∈ [/u1D43F] , /u1D70E min (/u1D44A (0) /u1D457 ) ≥ min{/u1D6FD/2, 1} ≥ 1 and by assumption ‖/u1D44A (/u1D461 ) /u1D457 -/u1D44A (0) /u1D457 ‖ /u1D45C /u1D45D < 1/(2/u1D43F) . Inequality (/u1D456 /u1D456 ) follows since /u1D6FD ≥ 2 , and there was a good initialization (which implies that the event in Lemma B.1 occurs). glyph[squaresolid] The next lemma shows that if the weight matrices remain close throughout the path of gradient flow then the loss decreases. Lemma B.4. If /u1D6FD ≥ 2 and there was a good initialization (see Definition 5.1), given any /u1D461 > 0 if for all 0 ≤ /u1D460 < /u1D461 and for all /u1D457 ∈ [/u1D43F]  then,  Proof. Byassumption ‖/u1D44A (/u1D460 ) /u1D457 -/u1D44A (0) /u1D457 ‖ /u1D45C /u1D45D < 1/(2/u1D43F) for all 0 ≤ /u1D460 < /u1D461 . Thus, by invoking Lemma B.3 we know that for all 0 ≤ /u1D460 < /u1D461 ,  which implies  Integrating both sides we get that  We also need the lemma that controls the growth of the operator norm of /u1D44A (/u1D461 ) /u1D457 . Lemma B.5. There is a positive absolute constant /u1D450 such that if   then on a good initialization, given any /u1D461 > 0 if for all 0 ≤ /u1D460 < /u1D461  then  glyph[squaresolid] 31 Proof. Applying [ZLG20, Lemma A.1] with /u1D434 = /u1D435 = /u1D43C we have, for all /u1D460 ≥ 0 ,  By the definition of gradient flow,  which implies that,  where (/u1D456 ) follows by applying Lemma B.4. On a good initialization we have that  Plugging this into the previous inequality we find that √  So for our lemma to be satisfied it suffices if √  32 and  glyph[squaresolid] Armed with these lemmas, we are now ready to prove the main result of this appendix. Recall its statement from above. Proposition 5.2. There is a constant /u1D450 such that, given any /u1D6FF ∈ (0, 1) , if the initialization scales /u1D6FC and /u1D6FD , along with the network width /u1D45A , satisfy  /u1D6FC ≤ 1, then with probability at least 1 - /u1D6FF : 1. the initialization is /u1D6FF -good; 2. training is perpetually /u1D450 (/u1D6FC + 1//u1D43F)/u1D6FD bounded; 3. for all /u1D461 > 0 , we have that  Proof. We will first prove that Part 1 of the proposition holds with probability 1 - /u1D6FF and then prove the other two parts assuming that the initialization was good. Proof of Part 1 . By invoking Lemmas B.1 and B.2 we have that a good initialization holds with probability at least 1 - /u1D6FF . Proof of Parts 2 and 3 . Assume that the initialization was good. We claim that, for all /u1D461 > 0 , · for all /u1D457 ∈ [/u1D43F] , ‖/u1D44A (/u1D461 ) /u1D457 - /u1D44A (0) /u1D457 ‖ /u1D45C /u1D45D < 1 2/u1D43F and · L (Θ (/u1D461 ) ) < L (Θ (0) ) exp ( -/u1D6FD 2 /u1D70E 2 min (/u1D44B ) 4/u1D452 ⋅ /u1D461 ) . 33 Assume for contradiction that this does not hold. Since ‖/u1D44A (/u1D461 ) /u1D457 - /u1D44A (0) /u1D457 ‖ /u1D45C /u1D45D 1 2/u1D43F and L (Θ (/u1D461 ) ) -L (Θ (0) ) exp ( -/u1D6FD 2 /u1D70E 2 min (/u1D44B ) 4/u1D452 ⋅ /u1D461 ) are continuous functions of /u1D461 , by the Intermediate Value Theorem there is a least value /u1D461 0 > 0 such that one of these quantities equals zero. But Lemmas B.4 and B.5 contradict this. This proves the first bound on the loss in Part 3 . For the second bound on the loss, we note that on a good initialization  Finally, we show that the training is perpetually /u1D450 (/u1D6FC + 1//u1D43F)/u1D6FD bounded. Recall that this means that, for all /u1D461 ≥ 0 , for all /u1D446 ⊆ [/u1D43F] ,  This follows since on a good initialization, ‖/u1D44A (0) 1 ‖ /u1D45C /u1D45D ≤ 2/u1D6FC , ‖/u1D44A (0) /u1D43F ‖ /u1D45C /u1D45D ≤ 2/u1D6FD and for all /u1D457 ∈ {2, … , /u1D43F - 1} , ‖/u1D44A (0) /u1D457 ‖ /u1D45C /u1D45D = 1 . Further, by Lemma B.5 we know that for all /u1D461 ≥ 0 , ‖/u1D44A (/u1D461 ) /u1D457 -/u1D44A (0) /u1D457 ‖ /u1D45C /u1D45D ≤ 1/(2/u1D43F) . Putting these two facts together, proves that the process is perpetually /u1D450 (/u1D6FC + 1//u1D43F)/u1D6FD bounded. glyph[squaresolid]",
  "References": "[AMG21] El Mehdi Achour, François Malgouyres, and Sébastien Gerchinovitz. 'The loss landscape of deep linear neural networks: a second-order analysis'. In: arXiv preprint arXiv:2107.13289 (2021) (Cited on page 28). [ACH18] Sanjeev Arora, Nadav Cohen, and Elad Hazan. 'On the optimization of deep networks: implicit acceleration by overparameterization'. In: International Conference on Machine Learning (ICML) . 2018 (Cited on pages 2, 28). [Aro+19] Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. 'Implicit regularization in deep matrix factorization'. In: Advances in Neural Information Processing Systems (NeurIPS) . 2019 (Cited on pages 27, 28). [Azu+21] Shahar Azulay, Edward Moroshko, Mor Shpigel Nacson, Blake Woodworth, Nathan Srebro, Amir Globerson, and Daniel Soudry. 'On the implicit bias of initialization shape: beyond infinitesimal mirror descent'. In: International Conference on Machine Learning (ICML) . 2021 (Cited on page 2). [BHL19] Peter Bartlett, David Helmbold, and Philip Long. 'Gradient descent with identity initialization efficiently learns positive-definite linear transformations by deep residual networks'. In: Neural Computation (2019) (Cited on page 28). [BL21] Peter Bartlett and Philip Long. 'Failures of model-dependent generalization bounds for least-norm interpolation'. In: Journal of Machine Learning Research (JMLR) (2021) (Cited on page 27). 34 [Bar+20] Peter Bartlett, Philip Long, Gábor Lugosi, and Alexander Tsigler. 'Benign over- 35 [CLG22] Geoffrey Chinot, Matthias Löffler, and Sara van de Geer. 'On the robustness of 36 [JT18] Ziwei Ji and Matus Telgarsky. 'Gradient descent aligns the layers of deep lin- 37 [LS22] Tengyuan Liang and Pragya Sur. 'A precise high-dimensional asymptotic the- 38 [RC20] Noam Razin and Nadav Cohen. 'Implicit regularization in deep learning may 39 [Woo+20] Blake Woodworth, Suriya Gunasekar, Jason Lee, Edward Moroshko, Pedro Savarese, 40",
  "keywords_parsed": [],
  "references_parsed": [
    {
      "ref_id": "b21",
      "title": "The loss landscape of deep linear neural networks: a second-order analysis"
    },
    {
      "ref_id": "b18",
      "title": "On the optimization of deep networks: implicit acceleration by overparameterization"
    },
    {
      "ref_id": "b19",
      "title": "Implicit regularization in deep matrix factorization"
    },
    {
      "ref_id": "b21",
      "title": "On the implicit bias of initialization shape: beyond infinitesimal mirror descent"
    },
    {
      "ref_id": "b19",
      "title": "Gradient descent with identity initialization efficiently learns positive-definite linear transformations by deep residual networks"
    },
    {
      "ref_id": "b21",
      "title": "Failures of model-dependent generalization bounds for least-norm interpolation"
    },
    {
      "ref_id": "b20",
      "title": "Benign over-"
    },
    {
      "ref_id": "b22",
      "title": "On the robustness of"
    },
    {
      "ref_id": "b18",
      "title": "Gradient descent aligns the layers of deep lin"
    },
    {
      "ref_id": "b22",
      "title": "A precise high-dimensional asymptotic the"
    },
    {
      "ref_id": "b20",
      "title": "Implicit regularization in deep learning may"
    },
    {
      "ref_id": "b20",
      "title": "Woodworth, Suriya Gunasekar, Jason Lee, Edward Moroshko, Pedro Savarese"
    }
  ]
}