{"Multi-objective Learning to Rank by Model Distillation": "Jie Tang Airbnb San Francisco, USA jie.tang@airbnb.com Huiji Gao Airbnb San Francisco, USA huiji.gao@airbnb.com Liwei He Airbnb San Francisco, USA liwei.he@airbnb.com", "ABSTRACT": "In online marketplaces, search ranking's objective is not only to purchase or conversion (primary objective), but to also the purchase outcomes(secondary objectives), e.g. order cancellation(or return), review rating, customer service inquiries, platform long term growth. Multi-objective learning to rank has been widely studied to balance primary and secondary objectives. But traditional approaches in industry face some challenges including expensive parameter tuning leads to sub-optimal solution, suffering from imbalanced data sparsity issue, and being not compatible with ad-hoc objective. In this paper, we propose a distillation-based ranking solution for multi-objective ranking, which optimizes the end-to-end ranking system at Airbnb across multiple ranking models on different objectives along with various considerations to optimize training and serving efficiency to meet industry standards. We found it performs much better than traditional approaches, it doesn't only significantly increases primary objective by a large margin but also meet secondary objectives constraints and improve model stability. We also demonstrated the proposed system could be further simplified by model self-distillation. Besides this, we did additional simulations to show that this approach could also help us efficiently inject ad-hoc non-differentiable business objective into the ranking system while enabling us to balance our optimization objectives.", "CCS CONCEPTS": "\u00b7 Computing methodologies \u2192 Neural networks ; \u00b7 Information systems \u2192 Learning to rank ; \u00b7 Applied computing \u2192 Online shopping .", "KEYWORDS": "Search Ranking, Multi-objective optimization, learning to rank, model distillation", "1 INTRODUCTION": "Online marketplaces such as Amazon, eBay, Walmart, Airbnb, Doordash, Uber provide search functions for users to find their preferred product items. While these platforms vary in the types of services or goods they offer, from retail products to food delivery to rentals, they share similar challenges in their search ranking algorithms. In general, aiming for higher conversion rates is insufficient, as it is crucial to consider the outcomes of those conversions, such as order cancellations or returns, customer service inquiries, review ratings, and the long-term growth of the marketplace. Although", "Sanjeev Katariya": "Airbnb San Francisco, USA sanjeev.katariya@airbnb.com a higher conversion rate can increase marketplace revenues, the associated costs of cancellations, returns, and customer service can significantly eat into profits. Additionally, an overemphasis on short-term conversion gains can jeopardize long-term growth by favoring established items over new listings, thus limiting the opportunities for new products to be discovered. To efficiently balance multi-objectives in search ranking, multiobjective optimization technologies have been recently studied for search ranking. For example, [29] reviewed multi-objective recommender systems and ranking algorithms, it classified most of previous work into two categories: scalarization methods (e.g. [11]) and multi-objective evolutionary algorithms (MOEAs) (e.g.[18]). In industrial practice, scalarization is much more popular than MOEAs given it's simple and also easy to be scaled to large training data efficiently. This paper's proposed approach is also connected to scalarization techniques, and thus, we will concentrate on comparing it with current scalarization methods. The most popular and intuitive approach in industry is model fusion [6], which trains one model for each objective independently, then combines them by weighted sum. This approach is simple but the drawback is also obvious: since objectives could interfere each other(the model which is optimized for one objective could hurt another one), simply combining them would lead to a sub-optimal solution. Therefore a better approach is assigning weight to each objective's cost function, then sum them together as a single objective cost function, such that the multi-objective optimization(MOO) problem is converted to single objective optimization problem[14]. This approach is also called scalarization method, it's widely adopted in industry[11] [24] [10] [9] [21]. Another related method is \ud835\udf16 -Constraint method[12] which takes one primary objective as optimization goal while secondary objectives as constraints. Besides optimizing aggregated objective cost functions, some study aggregated multiple labels into one label thus it could also construct a single objective cost function from multiobjective cost functions[6], while as [12] pointed out, it could still be considered as a special type of scalarization method. Those methods were discussed a lot in past regarding how to do better optimization, how to find better trade-off among optimization solutions. But one thing people may ignore yet important is imbalanced data across different objectives, e.g. there could be much more training data for clicks and conversions, but much less data for order cancellation and customer service inquiries etc due to event frequency. The scalarization approach is often implemented as multi-task learning scheme [21] [20] [28], one advantage of multi-task learning is the shared bottom layers could learn shared representation across all tasks(objectives), this way task(objective) with less data could benefit from task(objective) with more data. While this advantage could also harm learning when correlation among tasks is low, this could especially be true in marketplace search ranking, indeed objectives are often in opposite directions : focusing more on long term growth may hurt conversions (which are usually measured in short term), reducing customer service inquiries may hurt both conversions and long term growth since model may rank well-established items with higher quality/price to top. Thus with more and more conflicted objectives are added into multi-task model, it's expected to observe less efficient learning and sub-optimal solution. Though multi-task learning approach could be sub-optimal when objectives are not highly correlated, it still performs much better than simple model fusion thanks to the power of shared representation. This is also verified in Airbnb practice[21], the multi-task learning system implemented in Airbnb co-trains models for all objectives and achieved significant business metrics improvements. One learning from this system is tuning two sets of parameters could be very challenging: one for training loss weights which combines objectives' loss functions, another one for online score aggregation weights at serving stage. The second one is usually determined in ad-hoc way and sometimes it asks for online grid search which is cost-inefficient, even so the found weights still can't be guaranteed to be optimal. Besides this, we found the online score aggregation could be unstable, the weights tuned for one set of models may not work well with another set of models. Thus the model performance would be degraded over time or expensive tuning has to be done each time when any model is updated. The instability is not only from ad-hoc weight tuning, in our experiments it shows even simply retraining could also cause not small metric change, such that it may break balance which was optimized in previous model for different objectives. Besides the the challenges mentioned above, we also found another issue ignored by previous studies: nowadays most of ranking systems are deep learning models, the scalarization method assumes each objective cost function is differentiable, therefore the aggregated single objective cost function could be trained by backpropagation. While in practice, this is not always true, sometimes the objective could even be just an ad-hoc rule : e.g. show reasonable percentage of new items thus it would help long term growth. In this case, such objective can't be included into optimization directly, it has to be some manual tuning after the model is trained and deployed, though such manual tuning would degrade model performance. Recently, another trend is to apply model distillation into ranking and recommender systems [22] [26] [16], [23], almost all existing methods in this domain focus on two topics : 1) distillate large and complicated model into compact and efficient model; 2)distillate ensemble models into one single model. The main purpose is to save inference cost while retain model performance. Upon observing that the loss functions of model distillation and the scalarization method for multi-objective optimization share a similar structure, we realized the integration of model distillation and the multi-objective learning to rank could help address challenges discussed above. Therefore in this paper, we describe a general model distillation approach for optimizing multi-objective in search ranking and recommender system. To our best knowledge, this is the first attempt to apply model distillation in multi-objective learning to rank. Our major contributions include: \u00b7 We reformulate the multi-objective learning to rank problem (MO-LTR) as a model distillation problem which could mitigate imbalanced data issue, get rid of online score aggregation weights tuning, therefore we achieve better primary objective while meet secondary objectives constraints. \u00b7 We extend the distillation based multi-objective ranking algorithm by introducing soft-label concept into MO-LTR and demonstrate it could help reduce model irreproducibility and simplify proposed ranking system by self-distilling softlabels. \u00b7 We also show that ad-hoc non-differentiable business objectives could be easily injected into ranking model by revising soft-labels, thus non-differentiable objective could be easily included into MO-LTR.", "2 RELATED WORK": "Learning to rank(LTR) has been a popular research topic for decades, one important branch of previous studies is to evolve loss schemes: \u00b7 The point-wise loss predicts action probability (e.g. pCTR, pCVR) for each item separately[13]. \u00b7 The pair-wise loss looks at two items each time, and converts ranking to a binary classification problem : whether item A is better than item B. [3] [4] \u00b7 The list-wise loss considers the whole list of items and try to approximate the optimal order. [5] In early days, these studies only considered optimizing single objective: NDCG, since the major LTR application that time was web search. Recently, with emergence of online marketplace, search and recommender system become very popular in this new domain. Unlike web search, online marketplace is a two-sided market, both user journey and merchant journey is much longer than web search user: a typical online marketplace user would do comparison shopping to purchase one item by browsing lots of items, then later the purchased item could still be cancelled or returned by user; if user is not satisfied with the purchase, he or she may complain to customer service. Accordingly merchant would also go through the similar journey. In this case, only optimizing for NDCG is not enough, industrial companies start applying multi-objective optimization(MOO) into learning-to-rank: [19] applied MOO to talent match system, [6] applied MOO to balance two objectives revenue and purchase, [15] optimized multi-objectives including relevance, purchase, quality, rating, return, etc, [11] proposed a pareto-eficient algorithm to balance GMV and CTR, [21] optimized Airbnb search journey with multi-task learning by considering objectives including click, booking, cancellation, rejection, [27] optimized for preranking/ranking consistency, etc. Most of these researches applied scalarization method which converts multi-objective optimization to single-objective optimization:  2 where \ud835\udf14 \ud835\udc58 is assigned weight to \ud835\udc58 \ud835\udc61\u210e objective, it represents priority or importance of \ud835\udc58 \ud835\udc61\u210e objective, and could be assigned manually or adaptively, \ud835\udc36 \ud835\udc58 ( \ud835\udc65 ) is the cost function of \ud835\udc58 \ud835\udc61\u210e objective, \ud835\udc4b is training data. Similar to [6], we could further rewrite \ud835\udc36 \ud835\udc58 ( \ud835\udc4b ) as loss function  where \ud835\udc53 ( \ud835\udf03 \ud835\udc60 , \ud835\udf03 \ud835\udc58 , \ud835\udc4b \ud835\udc56 ) is the ML model for \ud835\udc58 \ud835\udc61\u210e objective, \ud835\udf03 \ud835\udc60 is shared model parameters across all objectives, \ud835\udf03 \ud835\udc58 is model parameters specifically for \ud835\udc58 \ud835\udc61\u210e objective, { \ud835\udc4b \ud835\udc56 , \ud835\udc3f \ud835\udc56,\ud835\udc58 } is feature vector and label of each training example for \ud835\udc58 \ud835\udc61\u210e objective. With this setup, there could be following 4 variants:", "(1) \ud835\udf03 \ud835\udc60 = \u2205 , \ud835\udf03 \ud835\udc58 \u2260 \u2205": "\u00b7 models don't share any parameters, but they are jointly trained with an aggregated loss function. At serving time, final ranking score is aggregated from all models' scores. (2) \ud835\udf03 \ud835\udc60 \u2260 \u2205 , \ud835\udf03 \ud835\udc58 \u2260 \u2205 \u00b7 This is a typical multi-task learning setup for deep learning model, models share bottom layers to learn shared representation. At serving time, final ranking score is aggregated from all models' scores. e.g. [21] [20] (3) \ud835\udf03 \ud835\udc60 \u2260 \u2205 , \ud835\udf03 \ud835\udc58 = \u2205 \u00b7 With this setup, there is only one single model to fit multiobjectives. At serving time, the model score could be directly used as ranking score. e.g. [6] [11] (4) \ud835\udf03 \ud835\udc60 = \u2205 , \ud835\udf03 \ud835\udc58 = \u2205 \u00b7 There is no trainable parameter for each model, this means each model is pre-trained to optimize \ud835\udc36 \ud835\udc56 ( \ud835\udc65 \ud835\udc56 ) separately. At serving time, final ranking score is aggregated from all models' scores. Both Variants 1) and 2) are multiple-task learning setup, while 2) is more popular since it's expected that the shared bottom layer could learn shared and better representation, and also help data sparsity issue. There are two sets of weights need to be tuned in 1) and 2): one for \ud835\udf14 \ud835\udc56 in training loss (1), another one for scores aggregation weights at model serving stage. Advantage of variant 3) is score from the single model could be directly used at serving time, while training one single model to fit multi-objective is more challenging. Variant 4) is actually model fusion, it's also popular due to its simplicity, but it may perform worse. To our best knowledge, the most relevant previous work to ours is [6], which proposed two approaches: (1) Stochastic label aggregation: For each training example, the label is randomly sampled from a label set, each label in this label set is mapped to one objective. (2) Two-phase model combination: At first step, each model is trained to optimize different objectives separately. At second step, the model scores are features of another model which would be trained with stochastic label aggregation approach. Though[6] shows some theoretical advantage of their approaches, as we pointed earlier it may not be efficient with extremely imbalanced training data across objectives, since with the single model setup, minor objective may be overwhelmed by objective with much larger data. In Airbnb ranking system, training data among objectives are highly imbalanced, in worst cast, the label imbalance ratio could be more than 10, while the training data in [6] is much more balanced: 2 datasets are well balanced, another one's unbalance ratio is 3. Also all 4 scalarization variants mentioned above can't handle non-differentiable objective e.g. manual rules which are usually applied in industrial world.", "3 PROPOSED FRAMEWORK": "", "3.1 Problem Statement": "Given a search query \ud835\udc5e \ud835\udc56 , there could be \ud835\udc5b matched items(returned from retrieval stage), the goal of learning to rank is to assign a score \ud835\udc60 \ud835\udc57 to \ud835\udc57 \ud835\udc61\u210e item \ud835\udc61 \ud835\udc56,\ud835\udc57 so that those items could be ranked in descending order of scores. In ML terminology, here the training example is { \ud835\udc5e \ud835\udc56 , \ud835\udc4b \ud835\udc56 , \ud835\udc3f \ud835\udc56 } , where \ud835\udc4b \ud835\udc56 = { \ud835\udc65 \ud835\udc56,\ud835\udc57 } \ud835\udc5b \ud835\udc57 = 1 , \ud835\udc65 \ud835\udc56,\ud835\udc57 \u2208 \ud835\udc45 \ud835\udc5a \u00d7 1 is feature vector of \ud835\udc61 \ud835\udc56,\ud835\udc57 . \ud835\udc3f \ud835\udc56 = { \ud835\udc59 \ud835\udc56,\ud835\udc57 } \ud835\udc5b \ud835\udc57 = 1 , \ud835\udc59 \ud835\udc56,\ud835\udc57 \u2208 \ud835\udc45 \ud835\udc3e \u00d7 1 is label vector of \ud835\udc61 \ud835\udc56,\ud835\udc57 , \ud835\udc3e is number of objectives. Ideally for each item, it's expected there is one label for each objective, but due to data sparsity, the label could be missing, thus though it's assumed all objectives share the same set of training data, some objective could only be trained over a small subset of training data when it's trained separately. For each objective, its cost function is defined like (2)  where \ud835\udf03 is trainable model parameters. Since \ud835\udc4b \ud835\udc56 contains \ud835\udc5b items, the \ud835\udc59\ud835\udc5c\ud835\udc60\ud835\udc60 (\u00b7) function here is Listwise Loss[5]. \ud835\udc53 ( \ud835\udf03, \ud835\udc4b ) \u2208 \ud835\udc45 \ud835\udc5b \u00d7 1 is prediction of the model which optimize those \ud835\udc3e objectives' cost function at the same time and also find better balance among them, \ud835\udc53 ( \ud835\udf03, \ud835\udc4b \ud835\udc56 ) | \ud835\udc5d is denoted as prediction of \ud835\udc5d \ud835\udc61\u210e item in \ud835\udc4b \ud835\udc56", "3.2 From Multi-Objective Optimization To Model Distillation": "In practical operations, major online marketplaces usually takes one business objective(e.g. CVR) as primary objective, while considering other secondary business objectives as constraints(e.g. cancellations, returns, review ratings). This is more aligned with \ud835\udf16 -Constraint method[12] [29], it could be written as   where \ud835\udc36 \ud835\udc58 ( \ud835\udc4b ) is \ud835\udc58 \ud835\udc61\u210e objective's cost function, here \ud835\udc36 1 ( \ud835\udc4b ) is set as primary objective cost function, \ud835\udf00 \ud835\udc58 is upper bound cost of each secondary objective. If each objective cost function is optimized separately, for each objective a model \ud835\udc53 \ud835\udc58 ( \ud835\udf03 \u2217 \ud835\udc58 , \ud835\udc4b ) could be trained without considering other objectives by only optimizing \ud835\udc36 \ud835\udc58 ( \ud835\udc4b ) . The corresponding cost \ud835\udc36 \u2217 \ud835\udc58 ( \ud835\udc65 ) could be considered as the lower bound of \ud835\udc36 \ud835\udc58 ( \ud835\udc4b ) , since a model optimizes for multiple objectives can't perform better than a dedicated model. This way we could rewrite each constraint as  This means we could tolerate \ud835\udc58 \ud835\udc61\u210e objective \ud835\udc36 \ud835\udc58 ( \ud835\udc4b ) performs at most \ud835\udf00 \u2032 \ud835\udc58 worse than \ud835\udc36 \u2217 \ud835\udc58 ( \ud835\udc4b ) . 3 \ud835\udc36 \ud835\udc58 ( \ud835\udc4b ) could be rewritten with (2.1) as function of the underline model \ud835\udc53 ( \ud835\udf03, \ud835\udc4b )   where \ud835\udc53 ( \ud835\udf03, \ud835\udc4b ) is the single model tries to optimize multi-objectives { \ud835\udc36 \ud835\udc58 ( \ud835\udc4b )} , \ud835\udc53 \ud835\udc58 ( \ud835\udf03 \u2217 \ud835\udc58 , \ud835\udc4b ) is the model trained for optimizing \ud835\udc36 \ud835\udc58 ( \ud835\udc4b ) only. If each objective cost function is Lipschitz continuous, we have  where \ud835\udc40 is a constant called as Lipschitz constant. Combine (4) and (7), we could find a surrogate constraint  So now we could rewrite (3) as  This optimization problem optimizes primary objective with a model \ud835\udc53 ( \ud835\udf03, \ud835\udc65 ) which also approximates each objective's own optimization solution \ud835\udc53 \ud835\udc58 ( \ud835\udf03 \u2217 \ud835\udc58 , \ud835\udc4b ) at the same time. With Lagrangian relaxation, we could further rewrite (9) as  Here Euclidean distance could be replaced with KL distance, and in LTR context, cross entropy (which is Lipschitz continuous) is usually adopted as loss function for each objective. Since KL distance is equivalent to cross entropy, (10) becomes  where \ud835\udc36\ud835\udc38 (\u00b7) is the cross-entropy loss, (11) could be further simplified as (short proof could be found in A.1)  In (9), it also makes sense to approximate primary objective's own optimization solution as additional constraint, thus we have  Now we have a loss(single objective) function which is very similar to model distillation loss function (e.g. [8] [26]): the first term is a loss between the prediction and primary objective's ground truth label (it's also referred as \"hard label\" in this paper), the second term is a distillation loss pushes model to approximate a soft-label which is computed by aggregating each objective's own optimization solution. From (13), this soft-label is defined as  which could also be considered as model fusion of each objective's own optimization solution. Thus (13) shows model fusion and model distillation could be combined together for multi-objective optimization. So to optimize multi-objectives in context of learning-to-rank, this formula tells us that we need to (1) Train a model \ud835\udc53 \ud835\udc58 ( \ud835\udf03 \u2217 \ud835\udc58 , \ud835\udc4b ) for each objective separately without considering other objectives. This is done by training model with each objective's label separately. (2) For each training example, soft-label is generated as \u02dd \ud835\udc3e \ud835\udc58 = 1 \ud835\udf14 \ud835\udc58 \ud835\udc53 \ud835\udc58 ( \ud835\udf03 \u2217 \ud835\udc58 , \ud835\udc4b ) (3) Run model distillation by considering both soft-labels and primary objective's ground-truth labels(hard labels), then we get a model which optimizes primary objective under secondary objectives' constraints.", "4 MULTI-OBJECTIVE LEARNING TO RANK SYSTEM": "", "4.1 System Overview": "To design a multi-objective learning to rank system as Eq (13) described, at first we rewrite Eq(13) by introducing an extra weight \ud835\udefc to balance learning from hard-labels and soft-labels, since as experiment in section 5 showed assigning equal weights to hardlabel and soft-label is not optimal in practice. Thus by also expanding \ud835\udc36\ud835\udc38 (\u00b7) , the loss function (13) is rewritten as  There are two components in loss function: (1) first one is hard label loss, which could be any kind of learning-to-rank loss. Given there are \ud835\udc5b items in each training example, Listwise Loss(which is softmax cross entropy) [5] is adopted as our hard label loss function, thus \ud835\udc53 ( \ud835\udf03, \ud835\udc4b \ud835\udc56 ) | \ud835\udc5d = \ud835\udc52 \ud835\udc67 \ud835\udc56,\ud835\udc5d \u02dd \ud835\udc5b \ud835\udc57 = 1 \ud835\udc52 \ud835\udc67 \ud835\udc56,\ud835\udc57 where \ud835\udc67 \ud835\udc56,\ud835\udc5d is \ud835\udc4b \ud835\udc56 's \ud835\udc5d \ud835\udc61\u210e item's score output from model, and \ud835\udc53 ( \ud835\udf03, \ud835\udc4b \ud835\udc56 ) | \ud835\udc5d is denoted as softmax prediction of \ud835\udc5d \ud835\udc61\u210e item. \ud835\udc59 \ud835\udc56 is primary objective's label vector where \ud835\udc59 \ud835\udc56,\ud835\udc5d is hard label of \ud835\udc5d \ud835\udc61\u210e item in training example \ud835\udc4b \ud835\udc56 . (2) the second component is soft-label distillation loss, it's also a cross-entropy loss following the first distillation paper [8] , the soft-label \u02c6 \ud835\udc59 \ud835\udc56 = \u02dd \ud835\udc3e \ud835\udc58 = 1 \ud835\udf14 \ud835\udc58 \ud835\udc60 \ud835\udc58 like how it's defined in (14). Similar to [8], temperature is also applied to softmax inside \ud835\udc53 ( \ud835\udf03, \ud835\udc4b \ud835\udc56 ) . (3) As [8] proposed, a weighted average is also used to combine hard-label loss and soft-label loss. This is equivalent to construct a new label \ud835\udc59 \u2032 \ud835\udc56 = \ud835\udefc\ud835\udc59 \ud835\udc56 + ( 1 -\ud835\udefc ) \u02c6 \ud835\udc59 \ud835\udc56 by injecting ground truth label into soft-label. Figure 1 shows the training graph of multi-objective learning to rank system with model distillation (MO-LTR-MD) based on (15). Unless mentioned otherwise all models here are deep learning 4 models, specifically they are all MLP models without advanced model structure: Figure 1: Model training graph LTR model Pre-  trained Model  for objective_1 Pre-  trained Model  for objective_K . ...... Soft Label Loss hard Label Student Teacher_1 Teacher_K \u00b7 The hard label is ground truth label of primary objective. It depends on business requirement, usually it could be conversion, click, etc. For most of online marketplaces, conversion is considered as primary objective. \u00b7 As mentioned in section 3 , soft-label is computed from a few pre-trained models: -Model is pre-trained for each objective separately, such pre-trained model optimizes each objective cost function independently without considering other objectives, and it's the best effort we could achieve for each objective. -For each training example \ud835\udc4b , soft label is computed as \u02dd \ud835\udc3e \ud835\udc58 = 1 \ud835\udf14 \ud835\udc58 \ud835\udc60 \ud835\udc58 , where \ud835\udc60 \ud835\udc58 = \ud835\udc53 \ud835\udc58 ( \ud835\udf03 \u2217 \ud835\udc58 , \ud835\udc4b ) is ranking score computed from each pre-trained model. -During training stage, each pre-trained model is frozen and non-trainable, all the pre-trained models are teacher models in model distillation terminology. During training time, all models(LTR model and pre-trained models) are loaded into the computation graph, while all pre-trained models' (teacher models') weights are frozen, therefore pre-trained models only contribute to soft-label computation during forward propagation. Later during serving time, all pre-trained models are discard, only the LTR model (student model) is exported for serving.", "4.2 What does soft-label encode?": "Before diving into soft-label, let's recall how hard label (ground truth label) works, in learning to rank scenery hard labels are collected from users' actions, e.g. clicks, purchases, cancellations,returns, etc. Hard labels don't only encode users' preferences but also the ranking model's preference, since only items preferred by model could be shown to users so that labels could be collected from those items. But hard label is highly sparse, e.g. in Airbnb search, each search result page shows a finite site of listings, in most of time booking label is attributed to only one listing among those listings, other labels like cancellations, customer service inquiries could even much sparser. Thus hard label can only help us get partial ground truth : the booked listing is ranked higher than unbooked listings by user, but the oder of unbooked listings is not exposed by users. At another side, soft-label is purely from model score which only encodes model's preference, user preference is missing from soft-label. With soft-label, full order of all listings could be generated, though this order comes from another model, it still gives new model a good reference (especially when the teacher models are also production models) . The new model could balance knowledge from soft-label and its own hard label during training. In section 3.2, soft-label is defined as aggregation of multiple pre-trained models, therefore it actually encodes a simple multiobjective ranking system's preference, this simple system can't be optimal, but there are a few advantages in practice: (1) By introducing soft-label, a dense soft-label vector could be assigned to each training example, this could help mitigate imbalanced data issue. While for other multi-objective algorithms like [6], when dataset is imbalanced across objectives, they may have to do heavily training data down-sampling or up-sampling, which may hurt model training efficiency. (2) Soft-label acts as good regularizer(similar to label smoothing regularization[25]). Soft-label gives multi-objective LTR model feasible prior knowledge since it encodes knowledge from each objective's pre-trained model. It's especially beneficial when the model is bootstrapped from a multi-task learning system [21] whose score aggregates each objective model score in the similar way with soft-label. (3) Soft-label could work well with non-differentiable objective. Besides encoding model preference, as it's shown in section 4.3 soft-label could also encode ad-hoc non-differentiable business objective efficiently. (4) Soft-label could carry ranking knowledge efficiently and pass to new version of ranking models. In industrial practice, model needs to be regularly retrained so that it could catch new customer trends. Such operation usually takes coldstart approach which trains model from scratch each time, therefore the model totally forgets what it learned before. As section 4.4 shows, by applying soft-label, what model learned before could be passed to new version of model efficiently and also help reduce model irreproducibility and instability.", "4.3 Ad-hoc Business Objective": "It's very common in practice some objective cost can't be expressed as a differentiable loss function, especially if it comes from ad-hoc business requirements, e.g. show more new items in search results to help new business owner and also marketplace's long term growth, or uprank more higher quality items in top search results to improve marketplace's branding. Such objectives themselves are vague and also can't be optimized by learning from past data, since they are usually in opposite direction to the user behaviors model could observe. For example, users tend to purchase well-established items, model could easily learn this from data and upranks wellestablished items accordingly, but it's hard to learn that new items need to be upranked. Though it's still possible item-to-item collaborative filtering could help new items, the features(e.g. number of reviews, review ratings, etc) which are highly correlated to some objectives(e.g. CTR, CVR) could still bias the learning heavily towards well-established items, since those features are missing in new items. Thus it's hard for traditional multi-objective optimization to include such ad-hoc non-differentiable objective. One solution could be completing such features for new items by averaging similar well-established items' feature values, but the learning efficiency can't be guaranteed. There is no quantitative 5 way to know the real impact to new items by doing this. And also blindly assuming new items perform at average level may not be fair to items underperformed in past, it may even hurt user experience and other business objectives like CVR. Another solution is to directly revise training data labels. Certainly this can't be done to hard label, e.g. let purchase as 1, nonpurchase as 0, if a small boost \ud835\udefd > 0 is given to new item's label , while keep other non-purchased items labels as 0, it will end up with purchased item > new items > other items. This doesn't make any sense since it places new items on top of all other items with purchases before. But such boost could work perfectly for softlabels, e.g. if there is a list of items which are sorted by soft-labels as { \ud835\udc61 1 , \ud835\udc61 2 , \ud835\udc61 3 , \ud835\udc61 4 , \ud835\udc61 5 , \ud835\udc61 6 } in descending order, where \ud835\udc61 6 is new item, now a boost \ud835\udefd is given to score of \ud835\udc61 6, \ud835\udc61 6 could be ranked higher than \ud835\udc61 5 or even \ud835\udc61 1. \ud835\udefd could be tuned to control how often and how strong this flip could happen, also since this is applied to soft-labels which have the similar power with hard-label in loss function, the learning could be very efficient. One may argue this boost \ud835\udefd could be directly added into ranking score at serving stage, but such change is far more powerful to hurt business metrics, as experiment showed in section 5.5 , it will cause model performance degrade. While adding boost \ud835\udefd into soft-label and letting model to balance with other objectives could yield better results.", "4.4 Training Operational Overhead And Irreproducibility": "In industrial practice, ranking model has to be updated regularly to catch latest consumer trend, users' personalized new preference, etc. Such update could be done daily, weekly, or monthly as offline batch training from scratch(cold start), or even in real-time as online continuous training. One issue of retraining model from scratch is, it will forget what was learned in last version and potentially cause irreproducibility and instability issue. As [2] pointed out, even same model was trained twice with same data, metrics difference would still be observed between two models. Indeed if our ranking model is trained twice with the same training data, not small difference from side-by-side comparison would be observed. Such irreproducibility would cause more severe instability issue when model is retrained with new data, in general it's expected model trained with new training data would not hurt our metrics at least. But due to the irreproducibility issue, by chance it may end up with a model which performs worse. This is especially important to multi-objective learning to rank, since model is optimized to balance multi-objectives, it's expected that balance among objectives is continuously stable. There may be also some concern of training operational overhead for the proposed system in Figure 1, since the soft label is computed from pre-trained models, this implies those pre-trained models also have to be retrained regularly(such retraining would also have irreproducibility issue), and the training becomes two-step procedure. To address the operational overhead and irreproducibility issues, inspired from Born-Again Neural Networks [7] and [17], we pass soft-labels down to new version of model during each model retraining in self-distillation way. Figure 2 shows how it works: Figure 2: Transfer knowledge among models by soft-label LTR model . Student_V0 LTR model Student_V1 ...... LTR model Student_V2 soft-  label soft-  label soft-  label hard-  label hard-  label hard-  label (1) First version of student model \ud835\udc49 0 is trained as Figure 1 describes, it distillates soft-label which is computed from model fusion of each objective's pre-trained model. (2) Then to retrain model \ud835\udc49 1 over latest training date range, model \ud835\udc49 1 is trained from hard label, and also soft-label from ranking score of student model \ud835\udc49 0, instead of the soft-label from model fusion (Note: there is no model structure change from \ud835\udc49 0 to \ud835\udc49 1). (3) Thus after student model \ud835\udc49 1 , the model training is decoupled from pre-trained models, it keeps distilling itself to transfer knowledge learned from each objective's pre-trained model down to new versions. With this design, the training complexity is significantly reduced: we don't need to maintain and update pre-trained models, instead we distill such knowledge into soft-labels and pass soft-labels along the model training path. But certainly, if needed, those pre-trained models could be plugged-in back any time. Some experiments were also done in section 5.3 and 5.4 to show self-distillation has no negative impact to business metrics and also this innovative approach could help reduce model training irreproducibility and instability.", "5 EXPERIMENTAL RESULTS": "", "5.1 Experiment Setup": "In this Airbnb experiment, the search ranking system is a multiobjective learning to rank system, the primary objective is booking(CVR), while secondary objectives include user cancellation rate, host cancellation rate, host rejection rate, platform long term growth, etc. For booking (CVR), we collect booked listings and attribute [1] them back to searches contain those booked listings, the same attribution process is also done for secondary objectives' labels. The proposed model (MO-LTR-MD) is trained with around 360 millions training examples collected from last a few months which only contains booking label. While each objective's pre-trained model is trained with a shared training dataset contains 500 millions training examples collected in the same date range, each training example in this dataset has multiple labels : bookings, clicks, cancellations, etc. These pre-trained models are co-trained by a multi-task learning system [21]. The baseline model is the same multi-task learning system, it's also trained with the same training dataset with 500 millions examples. As mentioned in section 1, the baseline model was trained by combining objectives' loss functions with a set of loss weights, at serving time the ranking score was aggregated from each objective's 6 Table 1: Model Training and Serving Comparison score by another set of score weights, both weights need to be tuned and tested. While for the proposed MO-LTR-MD model, only one distillation weight \ud835\udefc in Eq (15) needs to be tuned, some grid search was done for \ud835\udefc , it's found \ud835\udefc = 0 . 2 works best for our model. The weights to compute soft-label (14) came from our production setup which was a model fusion system (we can't share the absolute values of those weights here for protecting our core business data). Then when we moved to self-distillation stage, the soft-label comes from a score of last version of student model, the weights in (14) were not needed any more. Table 1 summarizes the training and serving advantage compared to baseline model: (1) Much less training data: 360M V.S. 500M. (2) Much less loss component weights tuning during training: 1 V.S. K+, where K is number of objectives (3) No score fusion weights tuning at serving stage: 0 V.S. K", "5.2 Overall model performance": "We evaluate our new system (MO-LTR-MD) with both offline evaluation and online AB test. For offline evaluation, we use Normalized Discountd Cumulative Gain(NDCG) with binary relevance score, where booking is assigned a relevance score of 1 and all other search impressions are assigned a relevance score of 0. The offline evaluation was done over 7 days of data which is not overlapped with our training data, it showed significant improvement of +1.1% NDCG compared to baseline model which is the multi-task learning system[21]. We also ran an AB test for 3 weeks, the control model is also the multi-task learning system [21], while treatment model is the proposed MO-LTR-MD system. This AB test showed +0.37% booking(CVR) gain with p_val = 0.02. As expected, all other secondary objectives' changes were neutral. The AB test showed soft-label does encode knowledge from each objective's pre-trained model, and more importantly because it could act as a regularizer and also mitigate imbalanced data issue, the model could do better job for balancing primary and secondary objectives, such that MO-LTR-MD system could improve NDCG and CVR with such big gain. We also compared the model training cost and serving latency. MO-LTR-MD model was trained with less training data, though pre-trained models were loaded to generate soft-label during training, the total training time was still same compared to baseline model. Also since only student model was served from MO-LTRMDsystem, while multiple ranking models were served in baseline system, the serving latency was significantly reduced : -1.6% from AB test.", "5.3 Self-Distillation Test": "As we stated in section 4.4, to make sure multi-objective optimization is stable across model retrainings, and also to reduce operational overhead introduced by model distillation, we extend MOLTR-MD system as Figure 2 describes. But there are two questions raised for self-distillation : 1) Would self-distillation hurt business metrics given pre-trained models are ablated in this process? 2) If new training data is added into model training, during self-distillation the pre-trained models are absent and thus can't be updated, would this hurt model and business metrics? We designed a test with following steps to address those concerns: (1) At first a student model was trained by distilling each objective's pre-trained model as Figure 1 describes, the generated model is \ud835\udc49 0 model in Figure 2 , which still depends on pretrained models. (2) Then as Figure 2 shows another model \ud835\udc49 1 was trained with soft-label as ranking score from \ud835\udc49 0 model (This is done by doing \ud835\udc49 0 model inference over each training example), the training data time window is right shifted a few months . (3) As a fair comparison model \ud835\udc49 0 was retrained as baseline model, training data time window was also right shifted by the same number of months. The offline test shows the NDCG is almost same by comparing \ud835\udc49 1 model and retrained \ud835\udc49 0 model. This offline test shows transferring ranking knowledge to updated model with only soft-label would not hurt even in absence of pre-trained models, and adding new training data also doesn't dilute soft-label's power. We also did AB test to verify whether such self-distillation ( knowledge transfer with soft-label) would hurt primary-objective and secondary objectives metrics, the result shows everything is neutral. Thus we could make conclusion that soft-label could efficiently encode and transfer multi-objective optimization knowledge among models. This finding helped us significantly simply the ranking system from Figure 1 to Figure 3: from now on, model \ud835\udc49 \ud835\udc5b would be trained from soft-labels computed by model \ud835\udc49 \ud835\udc5b -1 and also ground-truth(hard) labels. There is no dependency to pre-trained models any more. Figure 3: Self distillation LTR model V1 LTR model V0 . Student Teacher hard Label Loss Soft Label 7 Table 2: Model Irreproducibility", "5.4 Model Irreproducibility Test": "To demonstrate soft-label could help reduce irreproducibility, at first we trained a set of same baseline models with same training dataset, baseline models share the same model structure with the student model in MO-LTR-MD system, we only used ground truth label(hard label) to train the baseline models multiple times with different random initial weights. We also trained a set of same MO-LTR-MD models with same training dataset and also different random initial weights. At first we compared models by side-by-side(SxS) comparison system, which looks at two sets of search results, one from the baseline version of ranking model and the other from treatment version we're testing. The system will review the pages in each set of results, and compute average difference percentage(change rate) between two sets of results. In our system, we applied Kendall's \ud835\udf0f as \"a non-parametric measure of relationships between columns of ranked data\". Thus we ran SxS among baseline models, and also SxS among MO-LTR-MD models to evaluate model irreproducibility among different retraining. Those SxS could help us understand how search results are flipped by models trained with the same data but different random initial weights. We also computed Relative Prediction Difference(PD) adopted by [2] to measure irreproducibility, it's defined as  Where \u02c6 \ud835\udc66 \ud835\udc56, 1 and \u02c6 \ud835\udc66 \ud835\udc56, 2 are predictions from model's different retraining. From results in Table 2, we could find that with soft-label, SxS change rate(Kenall's \ud835\udf0f > 0 . 02 ) is reduced by 53%, PD is reduced by 11% , both indicate the model irreproducibility is significantly reduced.", "5.5 Ad-hoc Business Objective Test": "Finally we want to test efficiency of injecting ad-hot business objective with soft-label. For protecting our core business data, we can't disclose the real ad-hoc business objective applied in our system. Instead we simulate some ad-hoc objective which is never applied in our system, e.g. boosting more high review rating items in top search results. At first we built our baseline by manually giving items with high review rating a score boost at serving time  where \ud835\udc60 \ud835\udc56 is ranking score computed from baseline model at serving time, \ud835\udf0c is rating threshold, we simply add a boost \ud835\udefc to \ud835\udc60 \ud835\udc56 if review rating \ud835\udc5f \ud835\udc56 is better than \ud835\udf0c . Table 3: Boosting Impact To NDCG Then we built a model by revising soft-label and the model is trained using same data with the baseline model:  Here we will give soft-label \u02c6 \ud835\udc59 \ud835\udc56 a boost \ud835\udefd if review rating \ud835\udc5f \ud835\udc56 is better than \ud835\udf0c . During our offline simulations, we carefully tune \ud835\udefc and \ud835\udefd to make sure both baseline score boost and our soft-label boost show almost same high rating listings percentage in our SxS test. As Table 3 shows, if we directly give ranking score a boost, we observed -0 . 5% NDCG loss compared to baseline model without score boosting. While if we instead boost soft-labels at training time, to get the same impact to high rating listings, we only have to sacrifice -0 . 1% NDCG loss compared to the same baseline model. Whydoes soft-label injection hurt metric much less ? Deep learning model training usually is a non-convex optimization problem, thus it may end up with different local minimums. As mentioned before soft-label could be considered as a regularizer, by injecting some boost into regularizer, we actually pushed model towards the local minimum more aligned with ad-hoc business objective.", "6 LEARNING AND FUTURE WORK": "Our experiments demonstrated the proposed MO-LTR-MD system doesn't only help us find better optimization solution and improve model irrproducibility, but also shows advantage to work with adhoc non-differentiable objective. One interesting learning we had is that during our self-distillation test, the multi-objective optimization ranking knowledge encoded by soft-label could be transferred to new model in stable way by self-distillation. This is verified by our offline simulation and also online test, though at online test we only tested a model with two rounds of self-distillation, we would do more follow up online tests to make sure the knowledge transfer would not be decayed in long run, especially most of metrics for secondary objectives can only be checked in online test. In our experiment setup, we bootstrap our distillation based system from existing production model fusion setup so we don't need to tune weights of soft-labels in Eq (14). While if we are building a multi-objective learning to rank system from scratch, those weights need to be set, as Figure 4 shows we could fold weights learning into model training by importing a MoE layer [9], this work could be done in future when we redesign our multiobjective ranking system.", "7 ACKNOWLEDGMENTS": "We'd like to thank Do-kyum Kim for inspiring us to start this work, and also lots of good discussion along the whole path, and also Chunhow Tan, Han Zhao for the discussion and support. We would also like to extend our thanks to Alex Deng who builds Airbnb interleaving system which enables agile model development and 8 Figure 4: Multi-objective LTR with MoE layer LTR model Pre-  trained Model  for objective_1 Pre-  trained Model  for objective_K . X X MoE layer ...... Student Teacher_1 Teacher_K hard Label Loss Soft Label online test, also to Pavan Tapadia and Sherry Chen for product support. Finally, we would like to thank the entire Airbnb Relevance team for valuable discussions.", "REFERENCES": "[1] [n. d.]. About attribution models . https://support.google.com/google-ads/answer/ 6259715?hl=en [2] Rohan Anil, Sandra Gadanho, Da Huang, and Nijith Jacob etc. 2022. On the Factory Floor: ML Engineering for Industrial-Scale Ads Recommendation Models. In Recsys 2022 Workshop on Online Recommender Systems and User Modeling . ACM, New York, NY. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender. 2005. Learning to rank using gradient descent. In ICML '05: Proceedings of the 22nd International Conference on Machine learning . 89-96. [4] Chris J.C. Burges. 2010. From RankNet to LambdaRank to LambdaMART: An Overview . Technical Report MSR-TR-2010-82. [5] Z. Cao, T. Qin, T.-Y. Liu, M.-F. Tsai, , and H. Li. 2007. Learning to rank: from pairwise approach to listwise approach. In ICML '07: Proceedings of the 24th International Conference on Machine learning . 129-136. [6] David Carmel, Elad Haramaty, Arnon Lazerson, and Liane Lewin-Eytan. 2020. Multi-Objective Ranking Optimization for Product Search Using Stochastic Label Aggregation. In WWW'20: Proceedings of The Web Conference 2020 . ACM, New York, NY, 373-383. [7] Tommaso Furlanello, Zachary C. Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar. 2018. Born-Again Neural Networks. In ICML2018: Proceedings of the 35 th International Conference on Machine Learning . [8] Jeff Dean Geoffrey Hinton, Oriol Vinyals. 2014. Distilling the Knowledge in a Neural Network. In NIPS 2014 Deep Learning Workshop . [9] Yulong Gu, Zhuoye Ding, and Shuaiqiang Wang etc. 2020. Deep Multifaceted Transformers for Multi-objective Ranking in Large-Scale E-commerce Recommender Systems. In CIKM '20: Proceedings of the 29th ACM International Conference on Information and Knowledge Management . ACM, New York, NY, 2493-2500. [10] Guy Hadash, Oren Sar Shalom, and Rita Osadchy. 2018. Rank and rate: multi-task learning for recommender systems. In RecSys '18: Proceedings of the 12th ACM Conference on Recommender Systems . ACM, New York, NY, 451-454. [11] Xiao Lin, Hongjie Chen, and Changhua Pei etc. 2019. A Pareto-Eficient Algorithm for Multiple Objective Optimization in E-Commerce Recommendation. In RecSys '19: Proceedings of the 13th ACM Conference on Recommender Systems . ACM, New York, NY, 20-28. [12] Debabrata Mahapatra, Chaosheng Dong, Yetian Chen, and Michinari Momma. 2023. Multi-Label Learning to Rank through Multi-Objective Optimization. In KDD'23: Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . ACM, New York, NY, 4605-4616. [13] H. Brendan McMahan, Gary Holt, D.Sculley, and Michael Young etc. 2013. Ad Click Prediction: a View from the Trenches. In KDD '13: Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining . ACM, New York, NY, 1222-1230. [14] Kaisa Miettinen. 1998. Nonlinear multiobjective optimization . International Series in Operations Research and Management Science, Vol. 12. Springer, New York, NY. [15] Michinari Momma, Alireza Bagheri Garakani, and Yi Sun. 2019. Multi-objective Relevance Ranking. In Proceedings of ACM SIGIR Workshop on eCommerce (SIGIR 2019 eCom) . ACM, New York, NY. [16] Franco Maria Nardini, Cosimo Rulli, Salvatore Trani, and Rossano Venturini. 2023. Distilled Neural Networks for Efficient Learning to Rank. IEEE Transactions on Knowledge and Data Engineering 35 (May 2023), 4695-4712. https://doi.org/ 10.1109/TKDE.2022.3152585 [17] Zhen Qin, Le Yan, Yi Tay, Honglei Zhuang, Xuanhui Wang, Michael Bendersky, , and Marc Najork. 2021. Born again neural rankers. arXiv:2109.15285 [18] Marco Tulio Ribeiro, Anisio Lacerda, Adriano Veloso, and Nivio Ziviani. 2012. Pareto-efficient hybridization for multi-objective recommender systems. In RecSys '12: Proceedings of the sixth ACM conference on Recommender systems . ACM, New York, NY, 19-26. [19] Mario Rodriguez, Christian Posse, and Ethan Zhang. 2012. Multiple objective optimization in recommender systems. In RecSys '12: Proceedings of the sixth ACM conference on Recommender systems . ACM, New York, NY, 11-18. [20] Ozan Sener and Vladlen Koltun. 2018. Multi-Task Learning as Multi-Objective Optimization. In NIPS'18: Proceedings of the 32nd International Conference on Neural Information Processing Systems . 525-536. [21] ChunHow Tan, Austin Chan, Malay Haldar, and Jie Tang etc. 2023. Optimizing Airbnb Search Journey with Multi-task Learning. In KDD '23: Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . ACM, New York, NY, 4872-4881. [22] Jiaxi Tang and Ke Wang. 2018. Ranking Distillation: Learning Compact Ranking Models With High Performance for Recommender System. In KDD '18: Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . ACM, New York, NY, 2289-2298. [23] Honguk Woo and Hyunsung Lee amd Sangwoo Cho. 2022. An Efficient Combinatorial Optimization Model Using Learning-to-Rank Distillation. In The ThirtySixth AAAI Conference on Artificial Intelligence (AAAI-22) . 8666-8674. [24] Ruobing Xie, Yanlei Liu, and Shaoliang Zhang etc. 2021. Personalized Approximate Pareto-Efficient Recommendation. In WWW'21: Proceedings of the Web Conference 2021 . ACM, New York, NY, 3839-3849. [25] Li Yuan, Francis EH Tay, and Guilin Li etc. 2020. Revisiting Knowledge Distillation via Label Smoothing Regularization. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . [26] Honglei Zhang, Zhen Qin, Shuguang Han, and Xuanhui Wang etc. 2021. Ensemble Distillation for BERT-Based Ranking Models. In ICTIR '21: Proceedings of the 2021 ACM SIGIR International Conference on Theory of Information Retrieval . ACM, New York, NY, 131-136. [27] Zhishan Zhao, Jingyue Gao, and Yu Zhang etc. 2023. COPR: Consistency-Oriented Pre-Ranking for Online Advertising. In CIKM '23: Proceedings of the 32nd ACM International Conference on Information and Knowledge Management . ACM, New York, NY, 4974-4980. [28] Zhe Zhao, Lichan Hong, and Li Wei etc. 2019. Recommending what video to watch next: a multitask ranking system. In RecSys '19: Proceedings of the 13th ACM Conference on Recommender Systems . ACM, New York, NY, 43-51. [29] Yong Zheng and David (Xuejun) Wang. 2022. A survey of recommender systems with multi-objective optimization. Neurocomputing 474 (Feb. 2022), 141-153. https://doi.org/10.1016/j.neucom.2021.11.041", "8 APPENDICES": "", "8.1 A.1": "Short proof that we could simplify (11) to (12)   9"}
