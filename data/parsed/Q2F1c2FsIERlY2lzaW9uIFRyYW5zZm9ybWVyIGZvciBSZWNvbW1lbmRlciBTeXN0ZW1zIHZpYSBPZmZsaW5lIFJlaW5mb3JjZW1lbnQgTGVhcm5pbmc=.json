{"Causal Decision Transformer for Recommender Systems via Offline Reinforcement Learning": "Siyu Wang The University of New South Wales Sydney, Australia siyu.wang5@student.unsw.edu.au Xiaocong Chen The University of New South Wales Sydney, Australia xiaocong.chen@unsw.edu.au Dietmar Jannach", "Lina Yao": "University of Klagenfurt Klagenfurt, Austria dietmar.jannach@aau.at Data61, CSIRO Eveleigh, Australia The University of New South Wales Sydney, Australia lina.yao@unsw.edu.au Not for distribution. Reinforcement Learning (RL)-based Recommender Systems (RS) have been proven effective in a wide range of applications, including e-commerce, advertising, and streaming services, especially considering that users' interests are constantly changing in the real world [6]. In RLRS, an agent interacts with the environment by taking actions (e.g., recommending items to users) and receiving feedback in the form of rewards (e.g., user actions on recommended items). The agent uses the feedback to improve its policy over time, with the ultimate goal of maximizing the long-term reward (e.g., increasing user satisfaction or engagement with the system). However, RLRS often face two major challenges: i) The reward function used to reflect the user's interest is hard to formulate; ii) Due to the nature of the reinforcement learning algorithm, it can only be trained in small-scale simulation platforms, and most of the large existing datasets can not be directly used to optimize the recommendation algorithms. Unpublished working draft. ABSTRACT Reinforcement learning-based recommender systems have recently gained popularity. However, the design of the reward function, on which the agent relies to optimize its recommendation policy, is often not straightforward. Exploring the causality underlying users' behavior can take the place of the reward function in guiding the agent to capture the dynamic interests of users. Moreover, due to the typical limitations of simulation environments (e.g., data inefficiency), most of the work cannot be broadly applied in large-scale situations. Although some works attempt to convert the offline dataset into a simulator, data inefficiency makes the learning process even slower. Because of the nature of reinforcement learning (i.e., learning by interaction), it cannot collect enough data to train during a single interaction. Furthermore, traditional reinforcement learning algorithms do not have a solid capability like supervised learning methods to learn from offline datasets directly. In this paper, we propose a new model named the causal decision transformer for recommender systems (CDT4Rec). CDT4Rec is an offline reinforcement learning system that can learn from a dataset rather than from online interaction. Moreover, CDT4Rec employs the transformer architecture, which is capable of processing large offline datasets and capturing both short-term and long-term dependencies within the data to estimate the causal relationship between action, state, and reward. To demonstrate the feasibility and superiority of our model, we have conducted experiments on six real-world offline datasets and one online simulator. 1 INTRODUCTION", "CCS CONCEPTS": "\u00b7 Information systems \u2192 Recommender systems ; \u00b7 Computing methodologies \u2192 Reinforcement learning .", "KEYWORDS": "RecommenderSystems, Deep Learning, Offline Reinforcement Learning, Transformer Unpublished working draft. Not for distribution. Conference'17, July 2017, Washington, DC, USA 2023. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn 2023-08-29 01:02. Page 1 of 1-10. In RL, the reward function plays a crucial role in evaluating the effectiveness of the current action. When it comes to RLRS, the reward function is used to assess whether a recommended item is a good fit for a specific user. This evaluation can be considered a representation of the user's interests or behavior logic. However, defining the reward function can be a challenging task. Some researchers have chosen to omit it altogether and instead learn the recommendation policy directly from expert demonstrations [7, 8]. This approach, however, requires pre-training an expert agent in a simulation environment. Unfortunately, the lack of suitable simulation environments makes achieving this goal challenging. Meanwhile, the majority of recommendation algorithms proposed in the literature rely on data-driven approaches, with offline datasets used for both training and testing. However, such an approach is incompatible with the reinforcement learning paradigm, which necessitates online training and evaluation. To address the aforementioned two challenges, one possible solution is to integrate the established data-driven approach into the reinforcement learning framework and avoid the design of reward function. Recently, data-driven methods, such as transformers [29], have attracted significant attention. Transformers are renowned for their ability to handle large datasets, and the success of BERT4Rec [27] Conference'17, July 2017, Washington, DC, USA Wang, et al. demonstrated the effectiveness of transformers in recommendation systems. Further research [4, 35] has shown that transformers can effectively handle sparse and high-dimensional data. Therefore, in this work, we investigate the use of transformers in RLRS to enhance their capacity for processing large datasets. Decision Transformer [3] and Trajectory Transformer [15] are two typical offline reinforcement learning methods that leverage transformers. Both approaches view offline RL as a sequence modeling problem and train transformer models on collected data. Decision Transformer shows that by training an autoregressive model on sequences of states, actions, and returns, agents can learn to generate optimal behavior with limited experience [3]. On the other hand, the Trajectory Transformer demonstrates that sequence modeling in reinforcement learning is a more reliable method for predicting long-term outcomes in environments that satisfy the Markov property [15]. However, both Decision Transformer and Trajectory Transformer were designed primarily for robot learning. They may not be suitable for addressing the unique challenges faced by RLRS (i.e., understanding user behavior and learning users' dynamic interests). Addressing these challenges requires understanding the causal logic behind their observed behaviors. Hence, we develop a causal mechanism in the transformer to estimate the causal relationship between action, state, and reward to predict users' potential feedback on the actions taken by the system. In this way, we can avoid designing a reward function and instead estimate the reward by relying on the causality in the user's recent behavior. The contributions of this work are as follows: \u00b7 To avoid the reward function design, we design a causal mechanism to estimate the reward based on the user's recent behavior. \u00b7 We propose a method named causal decision transformer (CDT4Rec), which adopts transformer and offline reinforcement learning as the main framework to empower the proposed method to use existing real-world datasets. \u00b7 To the best of our knowledge, this is the first work that uses offline reinforcement learning and transformer in a recommender system. \u00b7 Extensive experiments on six public datasets and one online environment demonstrate the superiority of the proposed method. the agent. Formally, components mentioned above in the MDP can be represented as a tuple (S , A , P , R , \ud835\udefe ) , in which: \u00b7 State S : state space. \ud835\udc60 \ud835\udc61 \u2208 S is the state in time step \ud835\udc61 , and \ud835\udc60 \ud835\udc47 is the terminal state, where \ud835\udc47 is the final time step of an episode. \u00b7 Action A : action space. A( \ud835\udc60 \ud835\udc61 ) is set of actions possible in state \ud835\udc60 . \u00b7 Transition Probability P : denoted as \ud835\udc5d ( \ud835\udc60 \ud835\udc61 + 1 | \ud835\udc60 \ud835\udc61 , \ud835\udc4e \ud835\udc61 ) \u2208 P , is the probability of transitioning to state \ud835\udc60 \ud835\udc61 + 1, from \ud835\udc60 \ud835\udc61 with \ud835\udc4e \ud835\udc61 . \u00b7 Reward R : S \u00d7 A \u2192 R is the reward distribution, where \ud835\udc45 ( \ud835\udc60, \ud835\udc4e ) is the reward that an agent receives for taking action \ud835\udc4e when observing the state \ud835\udc60 . \u00b7 Discount-rate Parameter \ud835\udefe : \ud835\udefe \u2208 [ 0 , 1 ] is the discount factor for future rewards. As a result, a dataset for an offline RL-based recommender system can be described formally as D = {( \ud835\udc60 \ud835\udc62 \ud835\udc61 , \ud835\udc4e \ud835\udc62 \ud835\udc61 , \ud835\udc60 \ud835\udc62 \ud835\udc61 + 1 , \ud835\udc5f \ud835\udc62 \ud835\udc61 )} , following the MDP (S , A , P , R , \ud835\udefe ) . For each user \ud835\udc62 at the time step \ud835\udc61 , we have the following elements: a current state \ud835\udc60 \ud835\udc62 \ud835\udc61 \u2208 S , items recommended by the agent (or recommender system) via taking action \ud835\udc4e \ud835\udc61 , and the user's feedback \ud835\udc5f \ud835\udc62 \ud835\udc61 . Unpublished working draft. Not for distribution. Given (S , A , P , R , \ud835\udefe ) , an RL agent behaves following its policy \ud835\udf0b , which is a mapping from states to actions to be taken when in those states. The RL objective, \ud835\udc3d ( \ud835\udf0b ) , can then be written as an expectation under the trajectory distribution ( \ud835\udc60 0 , \ud835\udc4e 0 , ..., \ud835\udc60 \ud835\udc47 , \ud835\udc4e \ud835\udc47 ) : \ud835\udc3d ( \ud835\udf0b ) = E \ud835\udf0f \u223c \ud835\udc5d \ud835\udf0b ( \ud835\udf0f ) GLYPH<20> \u221e \u2211\ufe01 \ud835\udc58 = 0 \ud835\udefe \ud835\udc58 \ud835\udc5f ( \ud835\udc60 \ud835\udc61 , \ud835\udc4e \ud835\udc61 ) GLYPH<21> (1) Offline reinforcement learning is defined as the data-driven formulation of RL problems that improve with more data training. In an offline RL problem, the goal is still to train the agent to maximize the total reward it receives over time, as expressed in Equation (1). The fundamental difference between offline RL and RL is that offline RL just uses offline data and does not require any further online interaction [20]. Thus, a static dataset of transitions, such as data collected previously or human demonstrations, is presented to the offline RL learning system. As a result, the agent in offline RL is deprived of the ability to explore and interact with the environment to collect more transitions. Formally, we consider the transition dataset D to be the training dataset for the policy, from which the offline RL algorithms try to gain adequate insight into the dynamical systems underlying the MDP. We denote the distribution over states and actions in D as \ud835\udf0b \ud835\udefd . The states in state-action pairs ( \ud835\udc60, \ud835\udc4e ) \u2208 D are sampled following \ud835\udc60 \u223c \ud835\udc51 \ud835\udf0b \ud835\udefd ( \ud835\udc60 ) , and the actions are sampled following \ud835\udc4e \u223c \ud835\udf0b \ud835\udefd ( \ud835\udc4e | \ud835\udc60 ) .", "2 PROBLEM FORMULATION": "Let U = { \ud835\udc62 0 , \ud835\udc62 1 , ..., \ud835\udc62 \ud835\udc5b } denote a set of users and I = { \ud835\udc56 0 , \ud835\udc56 1 , ..., \ud835\udc56 \ud835\udc5a } denote a set of items. The datasets in a recommendation problem consist of the information and historical interaction trajectories of users spanning time steps \ud835\udc61 = 1 , ..., \ud835\udc47 . A standard recommendation problem can be described as an agent aiming to achieve a specific goal, which learns from interactions with users, such as recommending items and receiving feedback. This process can be formulated as a RL problem, where an agent is trained to interact with an environment. Normally, RL is described as a Markov Decision Process (MDP) [28]. Specifically, the agent interacts with the environment continually, choosing actions based on the current state of the environment and the environment responding to those actions and providing a new state to", "3 METHODOLOGY": "", "3.1 Model Architecture": "Here, we introduce a new C asual D ecision T ransformer for RLbased R ecommender S ystem ( CDT4Rec ), which formulates the problem of offline reinforcement learning as a sequence modeling problem. Our framework is built upon Decision Transformer [3]. As illustrated in Figure 1, CDT4Rec consists of \ud835\udc3f stacked multi-input transformer blocks. CDT4Rec receives a trajectory representation as the input to the first transformer block. The output sequence from the L-th transformer block is utilized to construct two distinct final 2023-08-29 01:02. Page 2 of 1-10. Causal Decision Transformer for Recommender Systems via Offline Reinforcement Learning Conference'17, July 2017, Washington, DC, USA Gt-k st-k at-k Emb. Emb. Emb. Positional encoding Gt-1 st-1 Emb. Emb. Emb. Positional encoding Emb. Emb. Emb. Positional encoding Multi-input Causal Transformer Block \u2026 K \u2026 Multi-input Causal Transformer Block L \u2026 \u2026 \u2026 Causal Decoder at r t \u2026 ^ ^ Causal Decoder at-1 r t-1 ^ ^ Causal Decoder at-2 r t-2 ^ ^ G s a B B B \u03a8 a Pred. Pred . s p ap N e r t ^ N g at ^ Masked Multi Self Attention Masked Multi Cross Attention Layer Norm Feed Forward Layer Norm \u03a8 s Unpublished working draft. Not for distribution. at-1 Gt st at Figure 1: An overview of our CDT4Rec architecture. RTGs, states, and actions are fed into linear embeddings, to which an absolute positional embedding is added. The tokens are then passed through L stacked multi-input blocks, which use both self-attention and cross-attention mechanisms with causal masks. This results in the generation of two balanced representations at the output of the L-th block. These representations are then fed into two additional prediction layers. On top of that are two networks \ud835\udc41 \ud835\udc52 and \ud835\udc41 \ud835\udc54 , which are used to estimate the reward and generate the predicted action, respectively. representations: one for action prediction and the other for state prediction. Additionally, two separate prediction layers are used above the final representations to make predictions for actions and states, respectively. After that, we train two networks to estimate the reward and generate the action. For training, we have access to observational data D and define a trajectory representation as a sequence of three tokens: returnsto-go (RTG), states, and actions. For simplicity, we omit the user index \ud835\udc62 unless necessary. Formally, the trajectory representation for autoregressive learning and generation is as follows: \ud835\udf0f = ( \ud835\udc3a 1 , \ud835\udc60 1 , \ud835\udc4e 1 , \ud835\udc3a 2 , \ud835\udc60 2 , \ud835\udc4e 2 , ..., \ud835\udc3a \ud835\udc47 , \ud835\udc60 \ud835\udc47 , \ud835\udc4e \ud835\udc47 ) , (2) where the RTG \ud835\udc3a \ud835\udc61 is defined as the sum of the discounted rewards that the agent receives over the future: \ud835\udc3a \ud835\udc61 = \ud835\udc5f \ud835\udc61 + \ud835\udefe\ud835\udc5f \ud835\udc61 + 1 + ... + \ud835\udefe \ud835\udc47 -\ud835\udc61 \ud835\udc5f \ud835\udc47 = \ud835\udc47 \u2211\ufe01 \ud835\udc58 = \ud835\udc61 \ud835\udefe \ud835\udc58 -\ud835\udc61 \ud835\udc5f \ud835\udc58 (3) In a recommender system, the user's actions are usually represented as binary, either a click or no click. However, this representation of the action is not informative enough to understand the user's interests. Learning to generate actions based on this type of dataset is also inadequate for explaining the user's click behavior. In RL, on the other hand, the reward function is frequently seen as the most succinct description of a task. The expected reward is used to construct the learning objective for the reinforcement learning algorithm. As a result, the reward function describes the agent's goals and directs policy optimization. The reward function is critical everywhere, and this is also the case when using RL for RS. In RLRS, the reward function explains a user's behavior and reflects their interests. In the context of RS, the reward function is designed to partially reflect the logic behind user actions, similar to the concept of causality, which is used to identify the impact of different variables on a model's performance and to understand the reasons behind certain decisions or actions. In this work, we formulate the reward function as a problem of estimating the causal effects of the action on the reward by giving user history trajectories: E ( \ud835\udc5f ( \ud835\udc4e \ud835\udc61 , \ud835\udc60 \ud835\udc61 )| \ud835\udf0f ) . The reason for choosing the expected discounted return is that we anticipate the proposed framework to generate future actions during test time that would yield the desired returns. The generated actions should be compatible with the selection of the agent, which aims to maximize the expected discounted return. In the following sections, the major components of CDT4Rec are introduced bottom-up: embedding layer, transformer layer, and causal layer. Let \ud835\udc3e \u2265 1 denote the context length, representing the latest \ud835\udc3e time steps fed into the RL causal transformer. Accordingly, \ud835\udc60 \ud835\udc61 -\ud835\udc3e + 1: \ud835\udc61 represents the sequence of the last \ud835\udc3e states at time \ud835\udc61 , and \ud835\udc3a \ud835\udc61 -\ud835\udc3e + 1: \ud835\udc61 represents the expected discounted return for the last \ud835\udc3e time steps. Further, let \ud835\udc35 \u2265 1 denote the prediction range of the B-step prediction, and \ud835\udc4e \ud835\udc61 : \ud835\udc61 + \ud835\udc35 -1 denote the associated length-B actions generated by the transformer. Specifically, the generated action at time \ud835\udc61 is based on \ud835\udc60 \ud835\udc61 -\ud835\udc3e + 1: \ud835\udc61 and \ud835\udc3a \ud835\udc61 -\ud835\udc3e + 1: \ud835\udc61 , which can be formulated as \ud835\udc4e \ud835\udc61 = \ud835\udf0b ( \ud835\udc60 \ud835\udc61 -\ud835\udc3e + 1: \ud835\udc61 , \ud835\udc3a \ud835\udc61 -\ud835\udc3e + 1: \ud835\udc61 ) . 2023-08-29 01:02. Page 3 of 1-10.", "3.2 Embedding Layer": "We learn a linear layer to obtain token embeddings for RTGs, states, and actions, respectively, which is followed by layer normalization as in [3]. In addition, to determine the exact positions of these tokens within a trajectory, we apply position encoding to the input embeddings at the bottoms of the transformer layer stacks. This is essential for capturing the causality behind the users' behavior as one user's behavior is causally determined by the previous one and the recommendation made by the system. And the order of users' Conference'17, July 2017, Washington, DC, USA Wang, et al. behaviors can reflect their actual interests. For example, some users begin their search with higher-priced items but end up purchasing more cost-effective items, while others begin their search with lower-priced items but purchase items from well-known companies. The time step information in the trajectories may reflect different user preferences and habits. previous ( \ud835\udc59 -1 ) block:  Because the RTG sequence for the recommender system lacks time step information, we must learn an embedding for each time step in order to stitch the sub-trajectories together. To help determine the exact positions of tokens inside a trajectory, we use an absolute positional embedding that uses a linear layer to obtain the embedding for each time step in a trajectory. Following the transformer layer from [3, 9], each transformer block begins with a Masked Multi-Head Self-Attention sub-layer over the input tokens followed by a Masked Multi-Head Cross-Attention sub-layer and a Position-wise Feed-Forward layer. Masked Multi-Head Self-Attention. Rather than performing a single attention function, we use multi-head self-attention, which is scaled dot-product attention that adapts numerous concurrent attention heads. Benefitting from the multi-head attention, the model can pay attention to inputs from many representation subspaces simultaneously at distinct positions. The attention head takes as input a matrix of queries \ud835\udc44 , a matrix of keys \ud835\udc3e , and a matrix of values \ud835\udc49 , where \ud835\udc44, \ud835\udc3e,\ud835\udc49 \u2208 \ud835\udc45 \ud835\udc47 \u00d7( \ud835\udc51 \u210e / \ud835\udc5b \u210e ) and \ud835\udc5b \u210e is the number of heads. The query, key, and value embeddings are projected from the input tokens with different projection matrices. Then the attention function is computed on a collection of queries at the same time. The output matrix is as follows:  The output representation produced by each head is:  where the projections for each head \ud835\udc4a \ud835\udc44 \ud835\udc57 , \ud835\udc4a \ud835\udc44 \ud835\udc57 , \ud835\udc4a \ud835\udc44 \ud835\udc57 \u2208 \ud835\udc45 \ud835\udc51 \u210e \u00d7( \ud835\udc51 \u210e / \ud835\udc5b \u210e ) are the learnable parameter matrices. Multi-head attention, in particular, applies the \u210e attention functions in parallel to build output representations and then outputs the projected concatenation of the different heads:  We adopt a variant of multi-head attention where certain input positions are masked with a causal mask [3]. This is used to ensure that the attention mechanism does not attend to future tokens when generating a predicted action sequence, such that autoregressive models can be applied without violating the assumption that future tokens are not available at the time of creation. Unpublished working draft. Not for distribution. Specifically, a one-time-step embedding is appended to three tokens: RTG, state, and reward. And the input representation for each token is constructed by summing its embedding and positional embedding: \ud835\udc3a 0 \ud835\udc61 = Linear \ud835\udc3a ( \ud835\udc3a \ud835\udc61 ) + \ud835\udc5d \ud835\udc61 \ud835\udc60 0 \ud835\udc61 = Linear \ud835\udc60 ( \ud835\udc60 \ud835\udc61 ) + \ud835\udc5d \ud835\udc61 \ud835\udc4e 0 \ud835\udc61 = Linear \ud835\udc4e ( \ud835\udc4e \ud835\udc61 ) + \ud835\udc5d \ud835\udc61 , (4) where \ud835\udc5d \ud835\udc61 \u2208 \ud835\udc43 is the \ud835\udc51 -dimensional absolute positional embedding for time step \ud835\udc61 . In addition to the absolute positional embedding we used, there is another form of positional embedding called relative positional embedding [10]. Relative positional embedding is defined as encoding the position of a unit relative to other units by considering the distance between two tokens. Huang et al. [14] analyzed the storage complexity of absolute and relative position embedding methods. Consider the following transformer model: \ud835\udc4f layers, \u210e attention heads per layer, and a maximum sequence length of \ud835\udc5a . The absolute and relative position embedding parameter sizes are \ud835\udc5a\ud835\udc51 and \ud835\udc4f\u210e ( 2 \ud835\udc5a -1 ) \ud835\udc51 , respectively. And the runtime storage complexity is O( \ud835\udc5a\ud835\udc51 ) for the absolute method and O( \ud835\udc4f\u210e\ud835\udc5a 2 \ud835\udc51 ) for the majority of the relative methods. Earlier research suggests that relative position embeddings outperform absolute position embedding in some situations [14, 22]. However, given the nature of reinforcement learning and the intricacy of the recommendation problem, adding too much complexity via positional embedding may make RL training harder to converge. Moreover, the \ud835\udc5a we used in this paper is related to the length of users' history and may easily reach very large values, which may easily result in a too-large space complexity for practical problems. As a result, we employ absolute positional embedding in our method.", "3.3 Transformer Layer": "As demonstrated by Figure 1, our transformer layer contains \ud835\udc3f identical blocks. Let \ud835\udc59 = 1 , ..., \ud835\udc3f be the index of the transformer blocks from bottom to top. For time step \ud835\udc61 , we iteratively compute hidden representations simultaneously at each layer \ud835\udc59 for each RTG, state, and action, denoted by \ud835\udc3a \ud835\udc59 \ud835\udc61 , \ud835\udc60 \ud835\udc59 \ud835\udc61 and \ud835\udc4e \ud835\udc59 \ud835\udc61 , respectively. The input for each transformer block is three parallel sequences of hidden representations. Let \ud835\udc51 \u210e be the size of the hidden states and \ud835\udc3b \ud835\udc59 = ( \ud835\udc3b \ud835\udc59 1 , ....\ud835\udc3b \ud835\udc59 \ud835\udc47 ) \u22a4 \u2208 R \ud835\udc47 \u00d7 \ud835\udc51 \u210e be the hidden representations of any one of the \ud835\udc3a \ud835\udc59 , \ud835\udc60 \ud835\udc59 and \ud835\udc4e \ud835\udc59 for each layer \ud835\udc59 at each time step \ud835\udc61 . The first block, in particular, is fed with the sequence of Equation (4) as input. The input for all blocks with \ud835\udc59 \u2265 2 is the output of the Masked Multi-Head Cross-Attention. In contrast to the selfattention layer, which only takes into account the information within a single transformer subnetwork, the cross-attention layer is capable of exchanging information between parallel transformer subnetworks globally. Specifically, self-attention computes all of the keys, queries, and values using hidden states from a single transformer subnetwork for the attention calculation. On the other hand, cross-attention infers the queries based on the hidden states within the transformer subnetwork and uses the output from the self-attention layers in two other transformer subnetworks as keys and values. This allows the cross-attention layer to exchange information between all three transformer subnetworks. To prevent queries from attending to future keys and values, a causal mask is introduced to the cross-attention as well. Position-wise Feed-Forward Network. To transform the output of the cross-attention layer into a new representation that is more suitable for the task, we employ a Position-wise Feed-Forward 2023-08-29 01:02. Page 4 of 1-10. Causal Decision Transformer for Recommender Systems via Offline Reinforcement Learning Conference'17, July 2017, Washington, DC, USA Network (FFN) in each block. The position-wise FFN consists of two fully connected layers with an activation function applied between them. We use a Gaussian Error Linear Unit (GELU) activation [13] instead of the standard ReLu activation, following the transformer in [24]:  The output of the FFN is a sequence of hidden states of the same length as the input: the dataset and reformulate the trajectories as the inputs of the transformer. We summarize the training procedures for CDT4Rec in Algorithm 1. Let \ud835\udf03 \ud835\udc52 and \ud835\udf03 \ud835\udc54 denote the trainable parameters for the reward estimation network \ud835\udc41 \ud835\udc52 and action generation network \ud835\udc41 \ud835\udc54 , respectively. Further, let \ud835\udf03 \ud835\udc60 and \ud835\udf03 \ud835\udc4e denote all trainable parameters for predicting the state and action. We fit the reward estimation network \ud835\udc41 \ud835\udc52 , state prediction, and action prediction by minimizing the factual loss of the reward: L \ud835\udc41 \ud835\udc52 ( \ud835\udf03 \ud835\udc52 , \ud835\udf03 \ud835\udc60 , \ud835\udf03 \ud835\udc4e )  In addition, the element in the input sequence is transformed independently at each position, without considering the other elements in the sequence.", "3.4 Causal Layer": "The ultimate output, represented as \ud835\udc3b \ud835\udc3f \ud835\udc61 = ( \ud835\udc3a \ud835\udc3f \ud835\udc61 -\ud835\udc3e + 1 , \ud835\udc60 \ud835\udc3f \ud835\udc61 -\ud835\udc3e + 1 , \ud835\udc4e \ud835\udc3f \ud835\udc61 -\ud835\udc3e + 1 , ..., \ud835\udc3a \ud835\udc3f \ud835\udc61 , \ud835\udc60 \ud835\udc3f \ud835\udc61 , \ud835\udc4e \ud835\udc3f \ud835\udc61 ) , is the final representation of the input trajectory after hierarchically processing it through all \ud835\udc3f transformer blocks. We segment the output into three sets: a set of RTGs, a set of states and a set of actions, which can be represented as: These sets are further used to build two separate final representations for the action and state prediction. As the action selection is based on the equation \ud835\udc4e \ud835\udc61 = \ud835\udf0b ( \ud835\udc60 \ud835\udc61 -\ud835\udc3e + 1: \ud835\udc61 , \ud835\udc3a \ud835\udc61 -\ud835\udc3e + 1: \ud835\udc61 ) , we use the hidden states \ud835\udc3a \ud835\udc61 -\ud835\udc3e + 1: \ud835\udc61 and \ud835\udc60 \ud835\udc61 -\ud835\udc3e + 1: \ud835\udc61 to build the final representations for the action prediction. The final representation is constructed by taking an element-wise summation of these two hidden states. A fully-connected linear layer and GELU activation are applied to obtain this representation: Similarly, the final representations for the state prediction are constructed by using \ud835\udc60 \ud835\udc61 -\ud835\udc3e + 1: \ud835\udc61 and \ud835\udc4e \ud835\udc61 -\ud835\udc3e + 1: \ud835\udc61 , because the transition of the state is based on its previous state and action: Dropout [26] is also applied to the output of the fully-connected linear layer to help prevent overfitting. Two fully-connected networks are put on top of the action prediction and state prediction for the final prediction task: predict the potential reward \ud835\udc5f \ud835\udc61 and generate action \ud835\udc4e \ud835\udc61 . The action prediction and state prediction are passed through the fully-connected networks, called reward estimation network \ud835\udc41 \ud835\udc52 to make the final predictions of potential reward \ud835\udc5f \ud835\udc61 . And the potential reward \ud835\udc5f \ud835\udc61 and action prediction are received by the action generation network \ud835\udc41 \ud835\udc54 to generate action \ud835\udc4e \ud835\udc61 .", "3.5 Training Procedure": "To train the model, we use a dataset of recommendation trajectories. We first use Deep Deterministic Policy Gradient (DDPG) [21] to train an expert RL agent, and then we use this expert in the environment to collect a series of expert trajectories to be the dataset. We sample mini-batches of sequences with context length K from  For the action generation network \ud835\udc41 \ud835\udc54 , we fit the \ud835\udc41 \ud835\udc54 to generate the final action by minimizing the cross entropy loss:   input: Offline data D , context length \ud835\udc3e , batch size \ud835\udc35 , number of iterations \ud835\udc5b \ud835\udc56\ud835\udc61 , model parameters \ud835\udf03 \ud835\udc52 , \ud835\udf03 \ud835\udc54 , \ud835\udf03 \ud835\udc4e , \ud835\udf03 \ud835\udc60 ; for \ud835\udc56 = 1 , ..., \ud835\udc5b \ud835\udc56\ud835\udc61 do Compute the RTG sequence according to Equation (3); Sample a sub-trajectory \ud835\udf0f \ud835\udc3e of length \ud835\udc3e from \ud835\udf0f ; end a_pred = Transformer \ud835\udf03 r_esti = Transformer \ud835\udc52 ,\ud835\udf03 \ud835\udc54 ,\ud835\udf03 \ud835\udc4e ( \ud835\udc3e ) ; \ud835\udf03 \ud835\udf0f \ud835\udc52 ,\ud835\udf03 \ud835\udc60 ,\ud835\udf03 \ud835\udc4e ( \ud835\udc3e ) ; \ud835\udf03 Unpublished working draft. Not for distribution. \ud835\udc3b \ud835\udc3f \ud835\udc61 = ( \ud835\udc3a \ud835\udc61 -\ud835\udc3e + 1: \ud835\udc61 , \ud835\udc60 \ud835\udc61 -\ud835\udc3e + 1: \ud835\udc61 , \ud835\udc4e \ud835\udc61 -\ud835\udc3e + 1: \ud835\udc61 ) (11) \u02dc \u03a8 \ud835\udc4e \ud835\udc61 = \ud835\udc3a \ud835\udc61 -\ud835\udc3e + 1: \ud835\udc61 \u2295 \ud835\udc60 \ud835\udc61 -\ud835\udc3e + 1: \ud835\udc61 \u03a8 \ud835\udc4e \ud835\udc61 = GELU ( \u02dc \u03a8 \ud835\udc4e \ud835\udc61 \ud835\udc4a \ud835\udc5f 1 + \ud835\udc4f \ud835\udc5f 1 ) (12) \u02dc \u03a8 \ud835\udc60 \ud835\udc61 = \ud835\udc60 \ud835\udc61 -\ud835\udc3e + 1: \ud835\udc61 \u2295 \ud835\udc4e \ud835\udc61 -\ud835\udc3e + 1: \ud835\udc61 \u03a8 \ud835\udc60 \ud835\udc61 = GELU ( \u02dc \u03a8 \ud835\udc4e \ud835\udc61 \ud835\udc4a \ud835\udc5f 2 + \ud835\udc4f \ud835\udc5f 2 ) (13) The overall training objective for CDT4Rec is: Algorithm 1: CDT4Rec Training Compute the trajectory sampling probability \ud835\udc5d ( \ud835\udf0f ) = | \ud835\udf0f |/ \u02dd \ud835\udf0f \u2208D | \ud835\udf0f | ; Sample B trajectories out of D ; for each sampled trajectory \ud835\udf0f do \ud835\udc52 , \ud835\udf03 \ud835\udc54 , \ud835\udf03 \ud835\udc4e , \ud835\udf03 \ud835\udc60 \u2190 one gradient update using the sampled \ud835\udf0f \ud835\udc3e . end", "4 EXPERIMENTS": "In this section, we report the outcomes of experiments that focus on the following three main research questions: \u00b7 RQ1 : How does CDT4Rec compare with other traditional deep RL algorithms in online recommendation environments and offline dataset environments? \u00b7 RQ2 : How does the number of trajectories affect the performance of CDT4Rec compared to other offline reinforcement learning approaches? \u00b7 RQ3 : How do the hyper-parameters affect the performance in the online simulation environment? \ud835\udf0f 2023-08-29 01:02. Page 5 of 1-10. Conference'17, July 2017, Washington, DC, USA Wang, et al. We concentrate our hyper-parameters study on online simulation settings since they are more closely suited to real-world environments, whereas offline datasets are fixed and do not reflect users' dynamic interests.", "4.1 Datasets and Environments": "In this section, we compare our proposed algorithm, CDT4Rec, with other state-of-the-art algorithms on real-world datasets and within an online simulation environment. We implement our model in PyTorch and conduct all our experiments on a server with two Intel Xeon CPU E5-2697 v2 CPUs with 6 NVIDIA TITAN X Pascal GPUs, 2 NVIDIA TITAN RTX, and 768 GB memory. In order to evaluate the performance of the proposed method, we need to transfer those offline datasets into simulation environments so that the reinforcement learning agent can interact. We convert those recorded data into an interactive environment, following the existing works [5]. Specifically, we adopt LSTM as the state encoder for temporal information to ensure the temporal relation can be captured. We convert the feedback data into a binary format by categorizing ratings above 75% of the maximum rating as positive feedback, which will provide a positive incentive signal for the agent. Ratings lower than 75% are classified as negative feedback. The evaluation process is the same as described in [34]. Firstly, we introduce six public real-world representative datasets from different recommendation domains for offline experiments which vary significantly in sparsity. Table 1 contains the overall statistics of the datasets. \u00b7 Amazon CD 1 : This is a collection of product review datasets crawled from Amazon.com by [23]. They separated the data into various datasets based on Amazon's top-level product categories. We use the 'CD' category in this work. \u00b7 LibraryThing: LibraryThing 2 is an online service to help people catalog their books easily. It includes social relationships between users and is frequently used to study social recommendation algorithms. \u00b7 MovieLens: This is a popular benchmark dataset for recommender systems. In this work, we will use two different scales of the MovieLens datasets, MovieLens-1M 3 and MovieLens20M 4 . \u00b7 GoodReads: It is a dataset from the book review website GoodReads 5 by [30]. It contains information on users' multiple interactions regarding items, including rating, review text, etc. \u00b7 Netflix: This is a well-known benchmark dataset from the Netflix Prize Challenge 6 . It only contains rating information. \u00b7 Book-Crossing 7 : This is another book related dataset proposed by [36]. This dataset is similar to MovieLens which only contains rating information. Table 1: Statistics of the datasets used in our offline experiments. Unpublished working draft. Not for distribution. In addition, we also conduct an experiment on a real online simulation platform to validate the proposed method. We use VirtualTB [25] as the major online platform in this work. VirtualTB mimics a real-world online retail environment for recommender systems. It is trained using hundreds of millions of genuine Taobao data points, one of China's largest online retail sites. The VirtualTB simulator creates a 'live' environment by generating customers and interactions, allowing the agent to be tested with virtual customers and the recommendation system. It uses the GAN-for-Simulating Distribution (GAN-SD) technique with an extra distribution constraint to produce varied clients with static and dynamic features. The dynamic attributes represent changing interests throughout an interactive process. It also employs the Multi-agent Adversarial Imitation Learning (MAIL) technique to concurrently learn the customers' policies and the platform policy to provide the most realistic interactions possible. In terms of evaluation metrics, we use the click-through rate (CTR) for the online simulation platform as the CTR is one of the built-in evaluation metrics of the VirtualTB simulation environment. For offline dataset evaluation, we employ a variety of evaluation metrics, including recall, precision, and normalized discounted cumulative gain (nDCG). 4.2 Baselines Most of the existing works are evaluating their methods on offline datasets, and very few works provide a public online simulator evaluation. As there are two types of experiments, we provide two sets of baselines to be used for different experimental settings. Firstly, we will introduce the baselines for the online simulator, which are probably the most popular benchmarks in reinforcement learning: \u00b7 Deep Deterministic Policy Gradient (DDPG) [21] is an off-policy method for environments with continuous action spaces. DDPG employs a target policy network to compute an action that approximates maximization to deal with continuous action spaces. 1 https://nijianmo.github.io/amazon/index.html 2 https://www.librarything.com/ 3 https://grouplens.org/datasets/movielens/1m/ 4 https://grouplens.org/datasets/movielens/20m/ 5 https://www.goodreads.com/ 6 https://www.kaggle.com/datasets/netflix-inc/netflix-prize-data 7 http://www2.informatik.uni-freiburg.de/ cziegler/BX/ \u00b7 Soft Actor Critic (SAC) [12] is an off-policy maximum entropy Deep Reinforcement Learning approach that optimizes a stochastic policy. It employs the clipped double-Q method and entropy regularisation that trains the policy to maximize a trade-off between expected return and entropy. \u00b7 Twin Delayed DDPG (TD3) [11] is an algorithm that improves baseline DDPG performance by incorporating three 2023-08-29 01:02. Page 6 of 1-10. Causal Decision Transformer for Recommender Systems via Offline Reinforcement Learning Conference'17, July 2017, Washington, DC, USA key tricks: learning two Q-functions instead of one, updating the policy less frequently, and adding noise to the target action. \u00b7 Decision Transformer (DT) [3] is an offline reinforcement learning algorithm that incorporates the transformer as the major network component to infer actions. Moreover, the following recommendation algorithms are used for offline evaluations which come from two different categories: transformerbased methods and reinforcement learning-based methods. \u00b7 SASRec [16] is a well-known baseline that uses the selfattention mechanism to make sequential recommendations. \u00b7 BERT4Rec [27] is a recent transformer based method for recommendation. It adopts BERT to build a recommender system. \u00b7 S3Rec [35] is BERT4Rec follow-up work that uses transformer architecture and self-supervised learning to maximize mutual information. \u00b7 KGRL [5] is a reinforcement learning-based method that utilizes the capability of Graph Convolutional Network (GCN) to process the knowledge graph information. \u00b7 TPGR [2] is a model that uses reinforcement learning and binary tree for large-scale interactive recommendations. \u00b7 PGPR [32] is a knowledge-aware model that employs reinforcement learning for explainable recommendations. We note that SASRec, BERT4Rec, and S3Rec are not suitable for the reinforcement learning evaluation procedure. In order to evaluate the performance of those models, we feed the trajectory representation \ud835\udf0f as an embedding into those models for training purposes and use the remaining trajectories for testing purposes 8 .", "4.3 Online Simulator Experiments (RQ1)": "Firstly, we will outline the procedures for conducting an online simulator experiment using offline reinforcement learning-based methods. Unlike traditional reinforcement learning algorithms (DDPG, SAC, etc.), we begin by training an expert agent using DDPG. We then employ this expert in the environment to collect a set of expert trajectories. It is important to note that the expert can only access a portion of the environment since we are collecting a fixed number of random trajectories as recorded data. These expert trajectories are treated as background knowledge and used to pre-train the offline reinforcement learning-based methods. Subsequently, the offline reinforcement learning method will conduct fine-tuning during the interaction with the simulation environment. Unpublished working draft. Not for distribution. causal effect on the users' decision process. Moreover, the introduction of cross-attention has apparently helped to generate better embeddings. Figure 2: Overall comparison result with variance between the baselines and CDT4Rec in the VirtualTaobao simulation environment. 1.0 0.8 0.6 5 DDPG SAC TD3 CDTARec DT 80000 100000", "4.4 Offline Dataset Experiments (RQ1)": "The overall results of the offline dataset experiments can be found in Table 2. We can find that CDT4Rec outperforms all baselines, including transformer-based methods and reinforcement learningbased methods. We can see that on some datasets, transformerbased methods are better than reinforcement learning-based methods, but not to a significant extent.", "4.5 Number of Trajectories Study (RQ2)": "In our online simulator experiments, we compared CDT4Rec with the aforementioned related reinforcement learning algorithms. The results of this comparison can be found in Figure 2. We can clearly find that the proposed CDT4Rec method and DT lead to a significant improvement at the very start of the episode. One main reason is that DT and CDT4Rec can access more recorded offline trajectories from the simulation environments due to the offline RL configuration. In order to answer this question (i.e., RQ2), we have conducted an experiment on different numbers of trajectories. We use DT as the major baseline, and the results are shown in Figure 3. We can find that CDT4Rec outperforms DT over a range of trajectory lengths. Moreover, when the number of trajectories is larger than 20k, the trend of CDT4Rec does not change significantly. We can conclude that CDT4Rec reaches its best performance with 20k trajectories. Furthermore, we can see that the number of trajectories has no significant effect on CDT4Rec, as CDT4Rec performs well in all three numbers of trajectories, indicating that CDT4Rec can learn effectively even with a smaller dataset. While DT still has room for improvement, the performance of DT varies strongly, and the trend becomes more stable when the number of trajectories comes to 30k.", "4.6 Hyper-parameters Study (RQ3)": "DT performs worse than CDT4Rec. One main reason is that CDT4Rec introduces an extra causal layer to estimate the potential 8 https://drive.google.com/file/d/1zFCWn_QPpqRGG1mxGt_YVYUcQmZn_3qE/ view?usp=sharing 2023-08-29 01:02. Page 7 of 1-10. In this section, we will investigate the impact of hyper-parameters. Here, we consider two different hyper-parameters: context length and the number of trajectories used for training. The context length determines the number of steps on which the agent's present action depends, and the number of trajectories means the number Conference'17, July 2017, Washington, DC, USA Wang, et al. Table 2: The overall results of our model comparison with several state-of-the-art models on different datasets. The highest results are in bold and the second highest are marked by * 30k Trajectories 0.6 0,4 CDTARec CDTARec CDTARec 0.0 DT 20000 40000 60000 80000 100000 20000 40000 60000 80000 100000 20000 40000 60000 80000 100000 Episode Episode Episode Unpublished working draft. Not for distribution. Figure 3: Performance Comparison Between CDT4Rec and DT for Different Numbers of Trajectories of trajectories included in the dataset. The results can be found in Figure 4. In our experiments, we find that when the context length is 2, CDT4Rec leads to the best performance. One plausible reason may be that user's behavior is highly determined by the most recent behavior (i.e., the previous one). Based on the results regarding the number of trajectories in the dataset, we observed that the performance did not vary considerably. This could be attributed to the fact that CDT4Rec is capable of learning effectively and performing well even with a smaller dataset. 2023-08-29 01:02. Page 8 of 1-10. Causal Decision Transformer for Recommender Systems via Offline Reinforcement Learning Conference'17, July 2017, Washington, DC, USA 1.0 1.0 0.8 0.8 0.6 0.6 g 5 0.4 0.4 context length =2 0.2 context length =4 0.2 context length =6 Ik Trajectories context length =8 20k Trajectories context length=10 0.0 30k Trajectories 20000 40000 80000 100000 20000 40000 80000 (a) Context Length (b) Number of Trajectories Offline RL. Recent studies have begun to investigate the possibility of integrating data-driven learning into the reinforcement learning framework. Kumar et al. [19] proposed conservative Qlearning for offline RL aiming to learn a lower bound of the value function. MOReL [17] is an algorithmic framework for model-based offline RL, which uses an offline dataset to learn a near-optimal policy by training on a Pessimistic MDP. Chen et al. [3] presented an approach in which the offline RL problem is modeled as a conditional sequence modeling problem. They trained a transformer on collected offline data, utilizing a sequence modeling objective, with the goal of generating optimal actions. Janner et al. [15] also adopted a transformer architecture and treat the offline RL as a sequence modeling problem. Kostrikov et al. [18] introduced FisherBRC for offline RL that employs a simple critic representation and regularization technique to ensure that the learned policy remains consistent with the collected data. However, these works are not applied for recommendation tasks and we are interested in the feasibility of employing offline RL for recommender systems. In this work, we design a causal decision transformer for recommender systems (CDT4Rec) to address two major challenges: i) The difficulty of manually designing a reward function; ii) How to incorporate an offline dataset to reduce the amount of required expensive online interaction but maintain the performance. Our experimental results reveal that our proposed CDT4Rec model outperforms both existing RL algorithms and the state-of-the-art transformer-based recommendation algorithms. Unpublished working draft. Not for distribution. Figure 4: Hyper-parameters Study for CDT4Rec 5 RELATED WORK RL-basedRecommenderSystems. Reinforcement learning-based recommendation methods view the interactive process of making recommendations as a Markov Decision Process (MDP) [6]. This approach can be divided into two categories: model-based and model-free methods. Bai et al. [1] proposed a model-based method that uses a generative adversarial training approach to jointly learn the user behavior model and update the policy for the recommendation. Recently, there has been a trend in the literature toward using model-free techniques for RL-based recommendations. Zhao et al. [33] proposed a page-wise recommendation framework based on deep RL to optimize the selection of items on a page by taking into account real-time feedback from users. Chen et al. [5] introduced knowledge graphs into the RL framework to improve the efficiency of the decision-making process. Chen et al. [7] designed a generative inverse reinforcement learning approach for online recommendation that automatically extracts a reward function from user behavior. However, these are all online RL-based frameworks while we are interested in offline RL-based recommendations. Transformer in Recommender Systems. Recent developments in the field of sequential recommendations have seen a growing interest in incorporating transformer architectures. Sun et al. [27] proposed the model BERT4Rec that uses the bidirectional self-attention network to model user behavior sequences in sequential recommendation tasks. Wu et al. [31] designed a personalized transformer architecture that effectively incorporates personalization in selfattentive neural network architectures, by incorporating SSE regularization. Chen et al. [4] introduced a method in which they incorporated the sequential signals of users' behavior sequences into a recommendation system on a real-world e-commerce platform. The method applied a self-attention mechanism to learn a better representation of each item in a user's behavior sequence by taking into account the sequential information. 6 CONCLUSION In the future, we plan to investigate further how to better estimate the causal effect of users' decisions so that we can better estimate the reward function via the collected trajectory. Moreover, we will work on further improving the offline reinforcement learning algorithm as we can find that there is still room for improvement.", "REFERENCES": "[1] Xueying Bai, Jian Guan, and Hongning Wang. 2019. A model-based reinforcement learning with adversarial training for online recommendation. Advances in Neural Information Processing Systems 32 (2019). [2] Haokun Chen, Xinyi Dai, Han Cai, Weinan Zhang, Xuejian Wang, Ruiming Tang, Yuzhou Zhang, and Yong Yu. 2019. Large-scale interactive recommendation with tree-structured policy gradient. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 33. 3312-3320. 2023-08-29 01:02. Page 9 of 1-10. Conference'17, July 2017, Washington, DC, USA Wang, et al. [3] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. 2021. Decision Transformer: Reinforcement Learning via Sequence Modeling. In Advances in Neural Information Processing Systems . https://openreview.net/forum?id=a7APmM4B9d [4] Qiwei Chen, Huan Zhao, Wei Li, Pipei Huang, and Wenwu Ou. 2019. Behavior sequence transformer for e-commerce recommendation in alibaba. In Proceedings of the 1st International Workshop on Deep Learning Practice for High-Dimensional Sparse Data . 1-4. [5] Xiaocong Chen, Chaoran Huang, Lina Yao, Xianzhi Wang, Wei liu, and Wenjie Zhang. 2020. Knowledge-guided Deep Reinforcement Learning for Interactive Recommendation. In 2020 International Joint Conference on Neural Networks (IJCNN) . 1-8. https://doi.org/10.1109/IJCNN48605.2020.9207010 melnychuk22a.html [23] Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019. Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP) . Association for Computational Linguistics, Hong Kong, China, 188-197. https://doi.org/10.18653/v1/D19-1018 [24] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving Language Understanding by Generative Pre-Training. (2018). [6] Xiaocong Chen, Lina Yao, Julian McAuley, Guanglin Zhou, and Xianzhi Wang. 2023. Deep reinforcement learning in recommender systems: A survey and new perspectives. Knowledge-Based Systems 264 (2023), 110335. [7] Xiaocong Chen, Lina Yao, Aixin Sun, Xianzhi Wang, Xiwei Xu, and Liming Zhu. 2021. Generative inverse deep reinforcement learning for online recommendation. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management . 201-210. [25] Jing-Cheng Shi, Yang Yu, Qing Da, Shi-Yong Chen, and An-Xiang Zeng. 2019. Virtual-Taobao: Virtualizing Real-World Online Retail Environment for Reinforcement Learning. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 33. 4902-4909. [26] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Journal of Machine Learning Research 15, 56 (2014), 1929-1958. http://jmlr.org/papers/v15/srivastava14a.html [8] Xiaocong Chen, Lina Yao, Xianzhi Wang, Aixin Sun, and Quan Z Sheng. 2022. Generative adversarial reward learning for generalized behavior tendency inference. IEEE Transactions on Knowledge and Data Engineering (2022), 1-12. https://doi.org/10.1109/TKDE.2022.3186920 [27] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer (CIKM '19) . Association for Computing Machinery, New York, NY, USA, 1441-1450. https://doi.org/10.1145/3357384.3357895 [28] Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An introduction . MIT press. [9] Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. 2021. Attention is not all you need: Pure attention loses rank doubly exponentially with depth. In International Conference on Machine Learning . PMLR, 2793-2803. [10] Philipp Dufter, Martin Schmitt, and Hinrich Sch\u00fctze. 2022. Position information in transformers: An overview. Computational Linguistics 48, 3 (2022), 733-763. [11] Scott Fujimoto, Herke van Hoof, and David Meger. 2018. Addressing Function Approximation Error in Actor-Critic Methods. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018 (Proceedings of Machine Learning Research, Vol. 80) . PMLR, 1582-1591. http://proceedings.mlr.press/v80/fujimoto18a.html [12] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018 (Proceedings of Machine Learning Research, Vol. 80) . PMLR, 1856-1865. http: //proceedings.mlr.press/v80/haarnoja18b.html [13] Dan Hendrycks and Kevin Gimpel. 2016. Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units. CoRR abs/1606.08415 (2016). arXiv:1606.08415 http://arxiv.org/abs/1606.08415 [14] Zhiheng Huang, Davis Liang, Peng Xu, and Bing Xiang. 2020. Improve Transformer Models with Better Relative Position Embeddings. In Findings of the Association for Computational Linguistics: EMNLP 2020 . Association for Computational Linguistics, Online, 3327-3335. https://doi.org/10.18653/v1/2020.findingsemnlp.298 [15] Michael Janner, Qiyang Li, and Sergey Levine. 2021. Offline Reinforcement Learning as One Big Sequence Modeling Problem. In Advances in Neural Information Processing Systems , Vol. 34. Curran Associates, Inc., 1273-1286. https://proceedings.neurips.cc/paper_files/paper/2021/file/ 099fe6b0b444c23836c4a5d07346082b-Paper.pdf [16] Wang-Cheng Kang and Julian McAuley. 2018. Self-Attentive Sequential Recommendation. In 2018 IEEE International Conference on Data Mining (ICDM) . 197-206. https://doi.org/10.1109/ICDM.2018.00035 [17] Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. 2020. MOReL: Model-Based Offline Reinforcement Learning. In Advances in Neural Information Processing Systems , Vol. 33. Curran Associates, Inc., 21810-21823. https://proceedings.neurips.cc/paper_files/paper/2020/file/ f7efa4f864ae9b88d43527f4b14f750f-Paper.pdf Unpublished working draft. Not for distribution. [29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems , I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.), Vol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/ 2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf [30] Mengting Wan and Julian McAuley. 2018. Item Recommendation on Monotonic Behavior Chains. In Proceedings of the 12th ACM Conference on Recommender Systems (Vancouver, British Columbia, Canada) (RecSys '18) . Association for Computing Machinery, New York, NY, USA, 86-94. https://doi.org/10.1145/ 3240323.3240369 [31] Liwei Wu, Shuqing Li, Cho-Jui Hsieh, and James Sharpnack. 2020. SSE-PT: Sequential recommendation via personalized transformer. In Fourteenth ACM Conference on Recommender Systems . 328-337. [32] Yikun Xian, Zuohui Fu, S Muthukrishnan, Gerard de Melo, and Yongfeng Zhang. 2019. Reinforcement Knowledge Graph Reasoning for Explainable Recommendation. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval . 285-294. [33] Xiangyu Zhao, Long Xia, Liang Zhang, Zhuoye Ding, Dawei Yin, and Jiliang Tang. 2018. Deep reinforcement learning for page-wise recommendations. In Proceedings of the 12th ACM Conference on Recommender Systems . 95-103. [34] Xiangyu Zhao, Liang Zhang, Zhuoye Ding, Long Xia, Jiliang Tang, and Dawei Yin. 2018. Recommendations with negative feedback via pairwise deep reinforcement learning. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 1040-1048. [35] Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, and Ji-Rong Wen. 2020. S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management . 1893-1902. [36] Cai-Nicolas Ziegler, Sean M. McNee, Joseph A. Konstan, and Georg Lausen. 2005. Improving Recommendation Lists through Topic Diversification. In Proceedings of the 14th International Conference on World Wide Web (Chiba, Japan) (WWW '05) . Association for Computing Machinery, New York, NY, USA, 22-32. https: //doi.org/10.1145/1060745.1060754 [18] Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum. 2021. Offline reinforcement learning with fisher divergence critic regularization. In International Conference on Machine Learning . PMLR, 5774-5783. [19] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. 2020. Conservative q-learning for offline reinforcement learning. Advances in Neural Information Processing Systems 33 (2020), 1179-1191. [20] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. 2020. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643 (2020). [21] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2016. Continuous control with deep reinforcement learning. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings . http://arxiv.org/abs/1509.02971 [22] Valentyn Melnychuk, Dennis Frauen, and Stefan Feuerriegel. 2022. Causal Transformer for Estimating Counterfactual Outcomes. In Proceedings of the 39th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 162) . PMLR, 15293-15329. https://proceedings.mlr.press/v162/ 2023-08-29 01:02. Page 10 of 1-10."}
