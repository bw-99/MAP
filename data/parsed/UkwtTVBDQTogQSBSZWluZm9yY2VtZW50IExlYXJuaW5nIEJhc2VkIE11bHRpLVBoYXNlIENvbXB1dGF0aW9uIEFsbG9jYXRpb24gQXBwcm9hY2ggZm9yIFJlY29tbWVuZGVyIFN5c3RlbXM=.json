{"RL-MPCA: A Reinforcement Learning Based Multi-Phase Computation Allocation Approach for Recommender Systems": "Jiahong Zhou \u2217 Meituan, Beijing, China zhoujiahong02@meituan.com Shunhui Mao Meituan, Beijing, China maoshunhui@meituan.com Guoliang Yang Meituan, Beijing, China yangguoliang@meituan.com Bo Tang Meituan, Beijing, China tangbo17@meituan.com Qianlong Xie Meituan, Beijing, China xieqianlong@meituan.com Xingxing Wang Meituan, Beijing, China wangxingxing04@meituan.com Lebin Lin Meituan, Beijing, China linlebin@meituan.com Dong Wang Meituan, Beijing, China wangdong07@meituan.com", "ABSTRACT": "Recommender systems aim to recommend the most suitable items to users from a large number of candidates. Their computation cost grows as the number of user requests and the complexity of services (or models) increases. Under the limitation of computation resources (CRs), how to make a trade-off between computation cost and business revenue becomes an essential question. The existing studies focus on dynamically allocating CRs in queue truncation scenarios (i.e., allocating the size of candidates), and formulate the CR allocation problem as an optimization problem with constraints. Some of them focus on single-phase CR allocation, and others focus on multi-phase CR allocation but introduce some assumptions about queue truncation scenarios. However, these assumptions do not hold in other scenarios, such as retrieval channel selection and prediction model selection. Moreover, existing studies ignore the state transition process of requests between different phases, limiting the effectiveness of their approaches. This paper proposes a Reinforcement Learning (RL) based MultiPhase Computation Allocation approach (RL-MPCA), which aims to maximize the total business revenue under the limitation of CRs. RL-MPCA formulates the CR allocation problem as a Weakly Coupled MDP problem and solves it with an RL-based approach. Specifically, RL-MPCA designs a novel deep Q-network to adapt to various CR allocation scenarios, and calibrates the Q-value by introducing multiple adaptive Lagrange multipliers (adaptive\ud835\udf06 ) to avoid violating the global CR constraints. Finally, experiments on the offline simulation environment and online real-world recommender system validate the effectiveness of our approach. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WWW'23, April 30-May 4, 2023, Austin, TX, USA \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-9416-1/23/04...$15.00 https://doi.org/10.1145/3543507.3583313", "CCS CONCEPTS": "\u00b7 Information systems \u2192 Recommender systems ; Online advertising ; Computational advertising .", "KEYWORDS": "Computation Resource Allocation, Deep Reinforcement Learning, Recommender System, Weakly Coupled MDP", "ACMReference Format:": "Jiahong Zhou, Shunhui Mao, Guoliang Yang, Bo Tang, Qianlong Xie, Lebin Lin, Xingxing Wang, and Dong Wang. 2023. RL-MPCA: A Reinforcement Learning Based Multi-Phase Computation Allocation Approach for Recommender Systems. In Proceedings of the ACM Web Conference 2023 (WWW '23), April 30-May 4, 2023, Austin, TX, USA. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3543507.3583313", "1 INTRODUCTION": "Recommender systems aim to recommend the most suitable items to users from a large number of candidates and expect to gain revenue from users' views, clicks, and purchases. They are playing an increasingly important role in e-commerce platforms [20]. Retrieval Coarse-ranking Fine-ranking Large candidate set Top ~10 4 Top ~10 3 Top ~10 2 Channel 1 Channel 2 Channel N User Multiple Channels Computation-intensive models Sorting and Truncation Industrial recommender systems are often designed as cascading architectures [11, 24]. As shown in Figure 1, a typical recommender system consists of several stages, including retrieval, coarseranking, fine-ranking, etc. In these stages, online advertising systems (a kind of recommender system applied to online advertising) generally contain several computation-intensive services or models, including bid models [19, 37], prediction services [14, 43], etc. These services require a lot of computation resources 1 (CRs). Take the display advertising system of Meituan Waimai platform 2 (hereinafter referred to as Meituan advertising system), for example. It consumes a lot of CRs in both the retrieval stage and the fine-ranking stage. As the number of user requests increases dramatically, the system's CR consumption rises accordingly. Due to the limitation of CRs, recommender systems need to make a trade-off between CR cost and business revenue when the traffic exceeds the system load. From the perspective of CR utilization efficiency, the goal of recommender systems is to maximize the total business revenue under the CR constraint. To address the challenges of huge traffic and a large number of candidate items, the real-world recommender systems usually use two types of strategies: static strategies and dynamic strategies [21, 38]. Static strategies select suitable fixed rules through stress testing and practical experience to allocate CRs. They also provide fixed downgrades to cope with unexpected traffic. Static strategies require constant manual intervention to adapt to quick changes in traffic, and fixed downgrades provided by static strategies are generally detrimental to business revenue and user experience. Dynamic strategies [21, 38] dynamically allocate CRs for requests based on the value of requests. They prioritize allocating CRs to more valuable requests to achieve better revenue. Compared to static strategies, dynamic strategies are more efficient in utilizing CRs and require fewer manual intervention. Recommender systems with multiple stages have various CR allocation scenarios. Based on the application scenario, we summarize the dynamic CR allocation methods into three types: Elastic Channel , Elastic Queue , and Elastic Model : \u00b7 Elastic Channel : dynamically adjust the retrieval strategy . A typical recommender system contains multiple retrieval channels. When CRs are insufficient, static strategies usually use fixed rules to drop some retrieval channels with high computation consumption. Different to static strategies, Elastic Channel dynamically adjusts the retrieval strategy for each request according to the online environment and the features of the request. \u00b7 Elastic Queue : dynamically adjust the length of queue . Under the limitation of CRs, recommender systems cannot provide the prediction service and ranking service for all candidate items. In static strategies, before entering the prediction service and ranking service, the queue of items needs to be truncated to a global fixed length. In contrast, Elastic Queue dynamically adjusts truncation for each request length according to the online environment and the features of the request. \u00b7 Elastic Model : dynamically select prediction models . Recommender systems often provide multiple prediction models with different computation consumption for one prediction service. A complex model achieves better revenue while taking more computation consumption. When CRs are insufficient, static strategies usually use fixed rules to downgrade high computation consumption models to low consumption models. In contrast, Elastic Model dynamically adjusts the prediction model for each request according to the online environment and the features of the request. Recently, some dynamic strategies [21, 38] have been proposed to achieve 'personalized\" CR allocation. DCAF [21] focuses on a single CR allocation phase. CRAS [38] focuses on multi-phase queue truncation problems, but it introduces some assumptions about Elastic Queue scenario. For example, it uses the queue length to represent the computation cost when modeling the CR allocation problem, and assumes that the revenue varies logarithmically with the queue length. However, these assumptions do not hold in Elastic Channel and Elastic Model scenarios. Moreover, existing studies ignore the state transition process of requests between different phases, which limits the effectiveness of their approaches. To address the limitations of existing studies, we propose RLMPCA, which formulates the CR allocation problem as a Weakly Coupled Markov Decision Process (Weakly Coupled MDP) [26] problem and solves it with an RL-based approach. Compared to Constrained Markov Decision Process (CMDP) [4], Weakly Coupled MDP allows global weakly coupled constraints across sub-MDPs. Thus, it can model the problem of CR allocation across requests better than CMDP [2, 7, 10]. Our main contributions are summarized as follows: (1) We propose an innovative CR allocation solution for recommender systems. To the best of our knowledge, this is the first work that formulates the CR allocation problem as a Weakly Coupled MDP problem and solves it with an RL-based approach. (2) We design a novel multi-scenario compatible Q-network adapting to the various CR allocation scenarios, then calibrate Q-value by introducing multiple adaptive Lagrange multipliers (adaptive\ud835\udf06 ) to avoid violating the global CR constraints in training and serving. (3) We validate the effectiveness of our proposed RL-MPCA 3 approach through offline experiments and online A/B tests. Offline experiment results show that RL-MPCA can achieve better revenue than baseline approaches while satisfying the CR constraints. Online A/B tests demonstrate the effectiveness of RL-MPCA in real-world industrial applications.", "2 RELATED WORK": "", "2.1 CR Allocation and RL for Recommender Systems": "Recommender systems have been a popular topic in industry and academia in recent years. Most studies focus on improving the business revenue under the assumption of sufficient CRs [41, 42]. Some of these studies focus on applying RL to recommender systems, including recommendations [12, 18, 44], real-time bidding[30, 35], ad slots allocation [23, 36, 41], etc. Some studies concern CR consumption and try to reduce it through model compression [13, 28]. All the above studies rarely focus on CR allocation. As an exception, DCAF [21] and CRAS [38] propose two 'personalized' CR allocation approaches. They formulate the Elastic Queue CR allocation problem as an optimization problem, and then solve it with linear programming algorithms. Different from the above studies, our proposed RL-MPCA uses an RL-based dynamic CR allocation approach to improve the effectiveness.", "2.2 RL and Weakly Coupled MDPs": "A Weakly Coupled MDP [26] comprises multiple sub-MDPs, which are independent except that global resource constraints weakly couple them [10]. Due to the linking constraints, the scale of the problem grows exponentially in the number of sub-problems [10]. Some studies try to relax Weakly Coupled MDP to CMDP [4] and then solve it [2, 10]. The solutions to the CMDP problem include CPO [1], RCPO [31], IPO [25], etc. They focus on the internal constraints of MDP. Recently, some studies focus on directly solving Weakly Coupled MDP problems. BCORLE( \ud835\udf06 ) [40] solves it with \ud835\udf06 -generalization. BCRLSP [9] first trains the unconstrained reinforcement model and then imposes a global constraint on the model with linear programming methods in near real-time. Both BCORLE and BCRLSP guarantee that budget allocations strictly satisfy a single global constraint. CrossDQN [23] attempts to make the model avoid violating a single global constraint by introducing auxiliary batch-level loss. It uses a soft version of argmax to solve the problem of non-derivability of the native argmax function, which makes the model unable to strictly satisfy the global constraints during both offline training and online serving. Offline RL methods aim to learn effective policies from a fixed dataset without further interaction with the environment [17]. Offpolicy methods (e.g., DQN [27], DDQN [32]) can be directly applied to Offline RL while ignoring the out-of-distribution (OOD) problem. To solve the OOD problem, some offline RL methods are also proposed, including BCQ [16], CQL [22], COMBO [39], etc. BCQ addresses the problem of extrapolation error via restricting the action space to force the agent towards behaving close to on-policy with respect to a subset of the given data. In addition, REM [3] enforces optimal Bellman consistency on random convex combinations of multiple Q-value estimates to enhance the generalization capability in the offline setting. In the experiments of this paper, we choose three popular methods (DDQN, BCQ, and REM) as base models. Essentially, our proposed RL-MPCA only modifies the Q-network, so it can also apply to other Q-learning methods. In addition, we can also consider the Weakly Coupled MDP problem as a black-box optimization problem, then solve it with evolutionary algorithms, such as Cross-Entropy Method (CEM) [29] and Natural Evolution Strategies (NES) [34].", "3 PROBLEM FORMULATION": "", "3.1 Original Problem Description": "The recent work [21] formulated the single-phase CR allocation problem as a knapsack problem. Similarly, we formulate the multiphase CR allocation problem as a knapsack problem. Wesuppose there are \ud835\udc40 online requests { \ud835\udc56 = 1 , . . . , \ud835\udc40 } in a given time slice, and the maximum computation budget of the system in this time slice is \ud835\udc36 . For each request \ud835\udc56 , \ud835\udc47 phases need to make computation decisions, and \ud835\udc41 \ud835\udc61 actions can be taken for the specified phase \ud835\udc61 . We define \ud835\udc57 1 , . . . , \ud835\udc57 \ud835\udc47 as a complete decision process of a request, and the decision action of phase \ud835\udc61 is \ud835\udc57 \ud835\udc61 ( \ud835\udc57 \ud835\udc61 \u2208 { 1 , . . . , \ud835\udc41 \ud835\udc61 } ). Meanwhile, for request \ud835\udc56 , if the decision process is \ud835\udc57 1 , . . . , \ud835\udc57 \ud835\udc47 , we use \ud835\udc49\ud835\udc4e\ud835\udc59\ud835\udc62\ud835\udc52 \ud835\udc56,\ud835\udc57 \ud835\udc56 ,..., \ud835\udc57 \ud835\udc47 and \ud835\udc36\ud835\udc5c\ud835\udc60\ud835\udc61 \ud835\udc56,\ud835\udc57 \ud835\udc56 ,..., \ud835\udc57 \ud835\udc47 to represent the expected revenue and computation cost, respectively. \ud835\udc65 \ud835\udc56,\ud835\udc57 \ud835\udc61 is the indicator that request \ud835\udc56 is assigned action \ud835\udc57 in phase \ud835\udc61 . In phase \ud835\udc61 , for request \ud835\udc56 , there is one and only one action \ud835\udc57 \ud835\udc61 can be taken. InEq. (2) above assumes that all phases share an overall computation budget. However, in a real-world online recommender system, the CRs of each phase are often relatively independent. For example, recommender systems often deploy prediction and retrieval services on different clusters for ease of maintenance, and their CRs cannot be shared. Considering that each phase has a separate CRs budget, we replace the global constraint (InEq. (2)) with multiple constraints InEq. (5), where \ud835\udc36\ud835\udc5c\ud835\udc60\ud835\udc61 \ud835\udc56,\ud835\udc57 \ud835\udc61 represents the computation cost when the decision of phase \ud835\udc61 is \ud835\udc57 \ud835\udc61 for request \ud835\udc56 , and \ud835\udc36 \ud835\udc61 is the computation budget of phase \ud835\udc61 . This paper focuses on the scenario of single-constraint CR allocation at each phase. If there is more than one constraint per phase, we can relax multiple constraints in the same phase and combine them into one.", "3.2 Weakly Coupled MDP Problem Formulation": "The decision results before phase \ud835\udc61 affect the input state of phase \ud835\udc61 . To better describe our approach, we take a three-phase CR allocation situation as an example in this paper. It contains one Elastic Channel phase, one Elastic Queue phase, and one Elastic Model phase, which is a typical case of recommender system CR allocation. As shown in Figure 2, for request \ud835\udc56 , the decision result of Elastic Channel phase determines the real retrieval queue, and it directly affects the input state of Elastic Queue phase. Similarly, the decision result of Elastic Queue phase affects the input state of Elastic Model phase. Therefore, to better adapt to the state transition process, in the multi-phase joint CR allocation, we introduce the 'state' of the request. Channel $GLYPH<c=29,font=/HRICCM+PingFangSC-Regular>GLYPH<c=3,font=/HRICCM+PingFangSC-Regular>1HDUE\\GLYPH<c=3,font=/HRICCM+PingFangSC-Regular>6KRS Channel %GLYPH<c=29,font=/HRICCM+PingFangSC-Regular>GLYPH<c=3,font=/HRICCM+PingFangSC-Regular>+RWGLYPH<c=3,font=/HRICCM+PingFangSC-Regular>VWRUH Channel ;GLYPH<c=29,font=/HRICCM+PingFangSC-Regular>GLYPH<c=3,font=/HRICCM+PingFangSC-Regular>;;;;;;GLYPH<c=3,font=/HRICCM+PingFangSC-Regular> \u2026\u2026\u2026\u2026\u2026\u2026 \u2026\u2026\u2026\u2026\u2026\u2026 Elastic Channel Phase Elastic  Queue Phase Elastic Model Phase Queue 1: GLYPH<c=3,font=/HRICCM+PingFangSC-Regular> GLYPH<c=20,font=/HRICCM+PingFangSC-Regular>GLYPH<c=19,font=/HRICCM+PingFangSC-Regular> Queue 2: GLYPH<c=3,font=/HRICCM+PingFangSC-Regular> GLYPH<c=21,font=/HRICCM+PingFangSC-Regular>GLYPH<c=19,font=/HRICCM+PingFangSC-Regular> \u2026\u2026\u2026\u2026\u2026\u2026 Queue X: GLYPH<c=3,font=/HRICCM+PingFangSC-Regular> ;; \u2026\u2026\u2026\u2026\u2026\u2026 Model 2: DNN Model Model X: XXX Model \u2026\u2026\u2026\u2026\u2026\u2026 Other Phases request i Queue with predictions Response Sorted Items Model 1: Linear Model Retrieval 1: {A, C} Retrieval 2: {A, B} Retrieval X: {\u2026,X} Truncated Queue \u2026\u2026 Multi-channel Combination Retrieval In this paper, we formulate the CR allocation problem as a Weakly Coupled MDP [26] problem. Formally, the Weakly Coupled MDP consists of a tuple of six elements ( S , A , R , P , \ud835\udefe, C ), which are defined as follows: \u00b7 State Space S . For phase \ud835\udc61 of request \ud835\udc56 , \ud835\udc60 \ud835\udc56 \ud835\udc61 \u2208 S consists of user information \ud835\udc62 , time slice information \ud835\udc61\ud835\udc60 , context information \ud835\udc50 , ad items information { \ud835\udc4e\ud835\udc51 1 , . . . , \ud835\udc4e\ud835\udc51 \ud835\udc41 \ud835\udc4e\ud835\udc51 } , and the CR allocation decision results of phase \ud835\udc61 -1. \u00b7 Action Space A . Our CR allocation situation has three phases with different action spaces. The actions of the Elastic Channel phase, the Elastic Queue phase, and the Elastic Model phase are the retrieval strategy number, the truncation length, and the prediction model number, respectively. \u00b7 Reward R . For request \ud835\udc56 , after the agent takes action for the final phase, the system returns the final sorted items to the user. The user browses the items and gives feedback, including order price \ud835\udc5d\ud835\udc5f\ud835\udc56\ud835\udc50\ud835\udc52 \ud835\udc5c and advertising fee \ud835\udc53 \ud835\udc52\ud835\udc52 \ud835\udc4e\ud835\udc51 of request \ud835\udc56 . The reward \ud835\udc5f ( \ud835\udc60 \ud835\udc61 , \ud835\udc4e \ud835\udc61 ) is the weighted sum of them: \u00b7 Transition Probability P . \ud835\udc43 ( \ud835\udc60 \ud835\udc61 + 1 | \ud835\udc60 \ud835\udc61 , \ud835\udc4e \ud835\udc61 ) is the state transition probability from phase \ud835\udc61 to phase \ud835\udc61 + 1 after taking action \ud835\udc4e \ud835\udc61 . For each request \ud835\udc56 , trajectory ( \ud835\udf0f \ud835\udc56 ) is its whole state transition process in the recommender system. \u00b7 Discount Factor \ud835\udefe . \ud835\udefe \u2208 [ 0 , 1 ] is the discount factor for future rewards. \u00b7 Global Constraint C . Each phase has its global constraint that couples sub-MDPs. InEq. (5) defines these constraints.", "4 METHODOLOGY": "Shared embedding lookup table Common Features ... ... Original Ads Sequence ... concate ... ... Candidate Actions of Elastic Queue ... ... ... Selection Unit Concatenate Concatenate Concatenate Elastic Channel Net Elastic Queue Net Elastic Model Net Constraint Layer Constraint Layer Constraint Layer Decision Result of Elastic Queue phase = 1 phase = 2 phase = 3 select the q-logits of a specific phase based on phase number ... ... ... ... user profile features context features time slice features Constraint Layer element-wise element-wise Deep Q-Network (DQN) [27] and its improved versions [16, 32, 33] are very popular in solving sub-MDPs with discrete actions. The Q-network is the essential structure of these models, \ud835\udc44 \ud835\udf03 ( \ud835\udc60, \ud835\udc4e ) . To adapt to various CR allocation scenarios, we design a novel deep Q-Network with multiple separate networks (As Figure 3 shows). In particular, the state space of each phase is defined as follows: \u00b7 In the Elastic Channel phase, the candidate action space is the retrieval strategy numbers. For a recommender system with \ud835\udc41 \ud835\udc5f retrieval channels, the number of retrieval strategies is \ud835\udc41 \ud835\udc50 = 2 \ud835\udc41 \ud835\udc5f and the candidate action space is { 1 , . . . , \ud835\udc41 \ud835\udc50 } . For example, for three candidate retrieval channels { \ud835\udc34, \ud835\udc35,\ud835\udc36 } , retrieval strategy ( 0 , 1 , 1 ) indicates that channel A is not retrieved, and channels B and C are retrieved. We convert the indicator vector as a binary value, then the strategy number of the strategy ( 0 , 1 , 1 ) is the integer 3. \u00b7 In the Elastic Queue phase, the action space is the truncation length. To reduce the candidate action space, we can put the candidate actions into buckets, e.g., set every ten adjacent truncation lengths as one bucket. Then the candidate action space is { 10 , 20 , . . . } . \u00b7 In the Elastic Model phase, the candidate action space is the prediction model numbers { 1 , . . . , \ud835\udc41 \ud835\udc5a } . Each phase of CR allocation has its own action spaces. As Figure 3 shows, we model each phase using separate networks to adapt the different action spaces. In the last layer of the Q-Network, we use the selection unit to select the q-logits of a specific phase based on phase number \ud835\udc61 .", "4.1 Constraint Layer": "For any phase \ud835\udc61 , suppose that we have the optimal policy \ud835\udf0b \u2217 \u00ac \ud835\udc61 that satisfies the constraints of all phases except \ud835\udc61 . Then the decision problem for current phase \ud835\udc61 can be modeled separately as the following single-phase CR allocation problem with a single constraint: By constructing and solving the Lagrange dual problem, we have the optimal solution to this problem. The proof is provided in Appendix B. For request \ud835\udc56 , the optimal action of phase \ud835\udc61 is \ud835\udc4e \u2217 \ud835\udc61 : where \ud835\udf06 \ud835\udc61 \u2265 0 is the Lagrange multiplier. Further, we use \ud835\udc44 \ud835\udf0b \u2217 \u00ac \ud835\udc61 ( \ud835\udc60 \ud835\udc61 , \ud835\udc4e \ud835\udc61 ) to represent the expected cumulative reward for taking action \ud835\udc4e \ud835\udc61 in state \ud835\udc60 \ud835\udc61 and subsequent actions are decided following policy \ud835\udf0b \u2217 \u00ac \ud835\udc61 . For phase \ud835\udc61 of request \ud835\udc56 , we have: where \ud835\udc36\ud835\udc5c\ud835\udc60\ud835\udc61 ( \ud835\udc60 \ud835\udc61 , \ud835\udc4e \ud835\udc61 ) is the computation cost for taking \ud835\udc4e \ud835\udc61 in \ud835\udc60 \ud835\udc61 , determined by ( \ud835\udc60 \ud835\udc61 , \ud835\udc4e \ud835\udc61 ) , and independent of both prior and subsequent strategies. Thus, for phase \ud835\udc61 , the optimal action for request \ud835\udc56 in state \ud835\udc60 \ud835\udc61 is \ud835\udc4e \u2217 \ud835\udc61 : Compared to the action selection formula in original DQN [27] networks, we only need to add a layer (Constraint Layer) to obtain the optimal action that satisfies the CR constraints. Then the optimal action is: 4.1.1 Adaptive\ud835\udf06 in Offline Model Training. As mentioned in DCAF [21] and CRAS [38], Assumptions (4.1) and (4.2) usually hold in general recommender systems. Assumption4.1. \ud835\udc49\ud835\udc4e\ud835\udc59\ud835\udc62\ud835\udc52 \ud835\udc56,\ud835\udc4e \ud835\udc61 is monotonically increasing with \ud835\udc36\ud835\udc5c\ud835\udc60\ud835\udc61 \ud835\udc56,\ud835\udc4e \ud835\udc61 . Assumption4.2. \ud835\udc49\ud835\udc4e\ud835\udc59\ud835\udc62\ud835\udc52 \ud835\udc56,\ud835\udc4e \ud835\udc61 \ud835\udc36\ud835\udc5c\ud835\udc60\ud835\udc61 \ud835\udc56,\ud835\udc4e \ud835\udc61 is monotonically decreasing with \ud835\udc36\ud835\udc5c\ud835\udc60\ud835\udc61 \ud835\udc56,\ud835\udc4e \ud835\udc61 . From our observations, they also hold for most requests in Meituan advertising system. However, it is worth noting that our assumptions differ from those of CRAS. CRAS uses the queue length to represent the computation cost, while we make no assumptions about the relationship between computation cost and queue length (or other actions). Given a fixed { \ud835\udc49\ud835\udc4e\ud835\udc59\ud835\udc62\ud835\udc52 \ud835\udc56,\ud835\udc4e \ud835\udc61 } \ud835\udc40 \ud835\udc56 = 1 and variable \ud835\udf06 \ud835\udc61 , the optimal action of phase \ud835\udc61 is \ud835\udc4e \u2217 \ud835\udc61 (Eq. 11). For each request \ud835\udc56 , its optimal action \ud835\udc4e \u2217 \ud835\udc61 varies with \ud835\udf06 \ud835\udc61 , then the total computation cost \u02c6 \ud835\udc36 \ud835\udc61 ( \ud835\udf06 \ud835\udc61 ) (Eq. 19) and the total revenue \u02c6 \ud835\udc45 \ud835\udc61 ( \ud835\udf06 \ud835\udc61 ) (Eq. 20) of phase \ud835\udc61 vary with \ud835\udf06 \ud835\udc61 . We can obtain the optimal \ud835\udf06 \ud835\udc61 which satisfies the CR constraint and maximizes \u02c6 \ud835\udc45 \ud835\udc61 ( \ud835\udf06 \ud835\udc61 ) through updating \ud835\udf06 \ud835\udc61 iteratively based on \u02c6 \ud835\udc36 \ud835\udc61 ( \ud835\udf06 \ud835\udc61 ) . Lemma 4.1. Suppose Assumptions (4.1) and (4.2) hold, for any \ud835\udf06 \ud835\udc58 \ud835\udc61 , let \ud835\udf06 \ud835\udc58 + 1 \ud835\udc61 be: where \ud835\udc36 \ud835\udc61 is the computation budget of phase \ud835\udc61 and \ud835\udefc \u2208 R + is learning rate of \ud835\udf06 . Then, the following conclusion holds: \u00b7 Conclusion 1. \u02c6 \ud835\udc36 \ud835\udc61 ( \ud835\udf06 \ud835\udc58 + 1 \ud835\udc61 ) \u2264 \u02c6 \ud835\udc36 \ud835\udc61 ( \ud835\udf06 \ud835\udc58 \ud835\udc61 ) will holds if \u02c6 \ud835\udc36 \ud835\udc61 ( \ud835\udf06 \ud835\udc58 \ud835\udc61 ) > \ud835\udc36 \ud835\udc61 . \u00b7 Conclusion 2. \u02c6 \ud835\udc45 \ud835\udc61 ( \ud835\udf06 \ud835\udc58 + 1 \ud835\udc61 ) \u2265 \u02c6 \ud835\udc45 \ud835\udc61 ( \ud835\udf06 \ud835\udc58 \ud835\udc61 ) will holds if \u02c6 \ud835\udc36 \ud835\udc61 ( \ud835\udf06 \ud835\udc58 \ud835\udc61 ) < \ud835\udc36 \ud835\udc61 . \u00b7 Conclusion 3. \ud835\udf06 \ud835\udc58 + 1 \ud835\udc61 = \ud835\udf06 \ud835\udc58 \ud835\udc61 will holds if \u02c6 \ud835\udc36 \ud835\udc61 ( \ud835\udf06 \ud835\udc58 \ud835\udc61 ) = \ud835\udc36 \ud835\udc61 . Proof. Suppose Assumptions (4.1) and (4.2) hold, \u02c6 \ud835\udc36 \ud835\udc61 ( \ud835\udf06 \ud835\udc61 ) is monotonically decreasing with \ud835\udf06 \ud835\udc61 (see more details in [21]). Further, under Assumption 4.2, \u02c6 \ud835\udc45 \ud835\udc61 ( \ud835\udf06 \ud835\udc61 ) \u02c6 \ud835\udc36 \ud835\udc61 ( \ud835\udf06 \ud835\udc61 ) is monotonically decreasing with \ud835\udf06 \ud835\udc61 , and under Assumption 4.1, \u02c6 \ud835\udc45 \ud835\udc61 ( \ud835\udf06 \ud835\udc61 ) is monotonically decreasing with \ud835\udf06 \ud835\udc61 . \u00b7 when \u02c6 \ud835\udc36 \ud835\udc61 ( \ud835\udf06 \ud835\udc58 \ud835\udc61 ) > \ud835\udc36 \ud835\udc61 , we have \ud835\udf06 \ud835\udc58 + 1 \ud835\udc61 > \ud835\udf06 \ud835\udc58 \ud835\udc61 , then \u02c6 \ud835\udc36 \ud835\udc61 ( \ud835\udf06 \ud835\udc58 + 1 \ud835\udc61 ) \u2264 \u02c6 \ud835\udc36 \ud835\udc61 ( \ud835\udf06 \ud835\udc58 \ud835\udc61 ) holds. \u00b7 when \u02c6 \ud835\udc36 \ud835\udc61 ( \ud835\udf06 \ud835\udc58 \ud835\udc61 ) < \ud835\udc36 \ud835\udc61 , we have \ud835\udf06 \ud835\udc58 + 1 \ud835\udc61 < \ud835\udf06 \ud835\udc58 \ud835\udc61 , then \u02c6 \ud835\udc45 \ud835\udc61 ( \ud835\udf06 \ud835\udc58 + 1 \ud835\udc61 ) \u2265 \u02c6 \ud835\udc45 \ud835\udc61 ( \ud835\udf06 \ud835\udc58 \ud835\udc61 ) holds. \u00b7 when \u02c6 \ud835\udc36 \ud835\udc61 ( \ud835\udf06 \ud835\udc58 \ud835\udc61 ) = \ud835\udc36 \ud835\udc61 , we have \ud835\udf06 \ud835\udc58 + 1 \ud835\udc61 = \ud835\udf06 \ud835\udc58 \ud835\udc61 holds. \u25a1 In summary, Lemma (4.1) specifies that it is feasible to update \ud835\udf06 with formula (21). Initially, conclusion 1 of Lemma (4.1) indicates that when the total computation cost exceeds the computation budget, updating \ud835\udf06 \ud835\udc61 with formula (21) will obtain less total computation cost. It helps to avoid violating the constraint. Furthermore, conclusion 2 of Lemma (4.1) indicates that when the total computation cost is less than computation budget, updating \ud835\udf06 \ud835\udc61 with formula (21) will obtain a better total revenue. Finally, conclusion 3 of Lemma (4.1) indicates that when the total computation cost equals to computation budget, updating \ud835\udf06 \ud835\udc61 with formula (21) will obtain the original value of \ud835\udf06 \ud835\udc61 . Updating \ud835\udf06 \ud835\udc61 with formula (21) until convergence, we will obtain the optimal \ud835\udf06 \u2217 \ud835\udc61 , where \u02c6 \ud835\udc36 \ud835\udc61 ( \ud835\udf06 \u2217 \ud835\udc61 ) = \ud835\udc36 \ud835\udc61 .", "Algorithm 1 Offline Training of RL-MPCA (Based on DDQN)": "Input: Dataset D , number of iteration \ud835\udc3c , mini-batch size \ud835\udc41 , adaptive\ud835\udf06 update times \ud835\udc3e 1: Initialize Q-network \ud835\udc44 \ud835\udf03 , target Q net \ud835\udc44 \ud835\udf03 \u2032 ( \ud835\udf03 \u2032 \u2190 \ud835\udf03 ), Lagrange multipliers \ud835\udf40 = ( \ud835\udf06 1 , . . . , \ud835\udf06 \ud835\udc47 ) 2: for \ud835\udc56 = 1 , . . . , \ud835\udc3c do 3: Sample batch D \ud835\udc56 of \ud835\udc41 transitions ( \ud835\udc60 \ud835\udc61 , \ud835\udc4e \ud835\udc61 , \ud835\udc5f \ud835\udc61 , \ud835\udc60 \ud835\udc61 + 1 ) from D 4: \ud835\udc4e \ud835\udc61 + 1 = arg max \ud835\udc4e \ud835\udc61 + 1 GLYPH<16> \ud835\udc44 \ud835\udf03 ( \ud835\udc60 \ud835\udc61 + 1 , \ud835\udc4e \ud835\udc61 + 1 ) -\ud835\udf06 \ud835\udc56 \ud835\udc61 + 1 \ud835\udc36\ud835\udc5c\ud835\udc60\ud835\udc61 ( \ud835\udc60 \ud835\udc61 + 1 , \ud835\udc4e \ud835\udc61 + 1 ) GLYPH<17> 5: \ud835\udf03 \u2190 arg min \ud835\udf03 \u02dd D \ud835\udc56 GLYPH<0> \ud835\udc5f \ud835\udc61 + \ud835\udefe\ud835\udc44 \ud835\udf03 \u2032 ( \ud835\udc60 \ud835\udc61 + 1 , \ud835\udc4e \ud835\udc61 + 1 ) -\ud835\udc44 \ud835\udf03 ( \ud835\udc60 \ud835\udc61 , \ud835\udc4e \ud835\udc61 ) GLYPH<1> 2 6: for \ud835\udc58 = 1 , . . . , \ud835\udc3e do 7: For \ud835\udc60 \ud835\udc61 \u2208 D \ud835\udc56 , take \ud835\udc4e \ud835\udc58 \ud835\udc61 with (22) 8: For \ud835\udc61 \u2208 { 1 , . . . , \ud835\udc47 } , update \ud835\udf06 \ud835\udc56,\ud835\udc58 + 1 \ud835\udc61 with (23) 9: end for 10: \ud835\udf40 \ud835\udc56 + 1 \u2190 \ud835\udf40 \ud835\udc56 11: Every \ud835\udc41 \ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc52\ud835\udc61 steps reset \ud835\udf03 \u2032 \u2190 \ud835\udf03 12: end for Output: \ud835\udc44 \ud835\udf03 , \ud835\udf40 = ( \ud835\udf06 1 , . . . , \ud835\udf06 \ud835\udc47 ) As described in Algorithm 1, we dynamically update the \ud835\udf06 in the offline training phase. At iteration step \ud835\udc56 , we take a mini-batch of samples D \ud835\udc56 (a bigger batch is generally taken here, e.g., 8192 samples per batch), and update \ud835\udf40 = ( \ud835\udf06 1 , . . . , \ud835\udf06 \ud835\udc47 ) \ud835\udc3e times. At the \ud835\udc58 -th update, for each \ud835\udc60 \ud835\udc61 in D \ud835\udc56 , take action \ud835\udc4e \ud835\udc58 \ud835\udc61 with: and for each phase \ud835\udc61 \u2208 { 1 , . . . , \ud835\udc47 } at the \ud835\udc58 -th update, update \ud835\udf06 \ud835\udc56,\ud835\udc58 + 1 \ud835\udc61 with: where \ud835\udefc \u2208 R + is the learning rate of adaptive\ud835\udf06 , and \ud835\udc36 \ud835\udc61 (D \ud835\udc56 ) is the maximum CR budget that the system can allocate for dataset D \ud835\udc56 at phase \ud835\udc61 . \ud835\udc36 \ud835\udc61 (D \ud835\udc56 ) can be calculated through an offline fixed rule, which is designed by stress testing and practical experience. Algorithm 1 describes the training process of DDQN-based RLMPCA. Essentially, the Constraint Layer module of RL-MPCA only modifies the Q-network, so it can also apply to other Q-learning methods. Take a popular offline RL method with Q-network, REM [3], for example. The only difference between Algorithm 1 and the REM-based RL-MPCA approach is Q-network \ud835\udc44 \ud835\udf03 . Specifically, for REM model with \ud835\udc3b heads, we replace \ud835\udc44 \ud835\udf03 with \ud835\udc44 \ud835\udc45\ud835\udc38\ud835\udc40 \ud835\udf03 = \u02dd \u210e \ud835\udefd \u210e \ud835\udc44 \u210e \ud835\udf03 , where \ud835\udf37 = ( \ud835\udefd 1 , . . . , \ud835\udefd \ud835\udc3b ) is categorical distribution, which is randomly drawed for each mini-batch (see more details in [3]). 4.1.2 \ud835\udf06 Correction in Offline Model Evaluation . After the offline model is trained, the \ud835\udf40 -calibrated Q-value guarantees that the agent's decisions satisfy the CR constraints on all training datasets. However, when applying \ud835\udf40 to the real online system, it still faces the following problems: (1) The online and offline data distributions are inconsistent because the behavioral policy of collecting offline data differs from the target policy, which leads to the possibility that \ud835\udf40 may not satisfy the CR constraints in the online system. (2) The traffic of the recommender system varies over time, and the existing \ud835\udf40 cannot satisfy the CR constraints on each time slice. To solve problem (1), we build an offline simulation system, which interacts with the agent and gives feedback on the computation cost and revenue in imitation of the real online environment. Through the evaluation in the simulation system, we select the optimal \ud835\udf40 \u2217 that satisfies the CR constraints in order of the decision phases. When both Assumptions (4.1) and (4.2) hold, we can find optimal \ud835\udf40 \u2217 through bisection search in each phase (please refer to [21] for the detailed proof). Otherwise, we can find optimal \ud835\udf40 \u2217 through grid search [6]. To solve problem (2), we select the optimal \ud835\udf40 \u2217 for each time slice based on the offline simulation system. We observe that the traffic of the recommender system generally varies periodically except for special holidays. Taking Meituan advertising system as an example, its traffic variation cycle is one day. Therefore, we can divide a day into multiple time slices with similar traffic distribution in the same time slice. Considering that the cost of training a separate model for each time slice is expensive and not easy to maintain, we first train a uniform model for all time slices, and then solve a separate \ud835\udf40 for each time slice. Alternatively, for systems with non-periodic traffic, a possible solution is to use the traffic from the previous time slice to represent the current time slice. Specifically, we can update \ud835\udf40 in near real-time, thus allowing \ud835\udf40 to automatically adapt to irregular traffic changes.", "4.2 System Architecture": "We illustrate the overview of the architecture in Figure 4. In each phase, for instance, in the Elastic Queue phase, we need to allocate and control CRs through Computation Allocation System and Computation Control System. Computation Allocation System aims to maximize the total business revenue under the CR constraints. Computation Control System aims to guarantee system stability by means of feedback control. Dynamic allocation of CRs poses a significant challenge in guaranteeing the stability of recommender systems. We use Flink [8] to collect real-time system load information, such as failure rate, CPU utilization, etc., and then use PID [5] control algorithms to achieve feedback control. When the system load exceeds the target value of the PID, the PID will control the consumption of CRs. For instance, in the Elastic Queue scenario, when the system's failure rate rises above the target value, the PID Controller will reduce the upper bound of the queue for all requests. The result of online A/B tests shows that the Computation Control System reduces the degradation rate by 0.1 percentage point, provides automatic and timely responses to unexpected traffic, and guarantees the stability of recommender systems. Monitor System Real-time Collection (Flink) Pid Controller System Information Computation Control System Computation Allocation System Recommender System RL Model Samples Generation Train Large Candidate Set User Elastic Channel Elastic Queue Elastic Model Log allocate allocate allocate control control control Feedback control Dynamic allocate Metrics (failure rate, CPU utilization,\u2026) Database", "5 EXPERIMENTS": "Our experiments aim to study four questions: (1) Does adaptive\ud835\udf06 of constraint layer help to avoid violating the global CR constraints in the training process? (2) After \ud835\udf06 correction, does the model with constraint layer satisfy the global CR constraints and improve business revenue? (3) How does RL-MPCA approach perform in comparison to other state-of-the-art CR allocation approaches and RL algorithms? (4) How do different hyper-parameter settings affect the performance of RL-MPCA? To answer these questions, we conduct various experiments in a three-phase joint modeling CR allocation situation, which contains one Elastic Channel phase, one Elastic Queue phase, and one Elastic Model phase.", "5.1 Offline Experiments": "To demonstrate the performance of the proposed RL-MPCA, we evaluate and compare various related approaches for CR allocation on a real-world dataset. In offline experiments, we use the simulation system to evaluate these approaches. 5.1.1 Dataset. We run random exploratory policies and superior policies (see more details about behavioral policies in Appendix F) to collect the dataset on Meituan advertising system during July and August 2022. Finally, we sample 568,842,204 requests from 101,368,290 users as the dataset, which includes user profile features, context features, time slice features, etc. 5.1.2 Offline Simulation System . It is dangerous to deploy a model to an online system when its effect is unknown, which may significantly damage the online revenue of the recommender system and cause the online service to crash. To solve this problem, we build an offline simulation system, which can imitate the online real-world environment to interact with the model (agent) and give feedback on the computation consumption and revenue. More details about the offline simulation system are described in Appendix A. 5.1.3 Evaluation Metrics. We use computation ( \ud835\udc50\ud835\udc5c\ud835\udc60\ud835\udc61 ) and revenue ( \ud835\udc5f\ud835\udc52\ud835\udc61\ud835\udc62\ud835\udc5f\ud835\udc5b ) to evaluate the performance of approaches in offline experiments. The computation cost of each phase is defined as the sum of the CR consumption of all requests in that phase (see more details in Appendix C). To facilitate analysis, we define total computation cost as ( \ud835\udc50\ud835\udc5c\ud835\udc60\ud835\udc61 = \u02dd \ud835\udc61 ( \u02c6 \ud835\udc36 \ud835\udc61 \ud835\udc36 \ud835\udc61 -1 ) ). \ud835\udc5f\ud835\udc52\ud835\udc61\ud835\udc62\ud835\udc5f\ud835\udc5b is defined as the total revenue of all requests, specifically, \ud835\udc5f\ud835\udc52\ud835\udc61\ud835\udc62\ud835\udc5f\ud835\udc5b = \u02dd \ud835\udc53 \ud835\udc52\ud835\udc52 \ud835\udc4e\ud835\udc51 + \u02dd \ud835\udc5d\ud835\udc5f\ud835\udc56\ud835\udc50\ud835\udc52 \ud835\udc5c . With reference to D4RL [15], to facilitate the analysis of the effectiveness of different approaches while ignoring the impact of our application scenarios, we normalize scores by: 5.1.4 Hyper-parameters Settings. RL-MPCAcontainsseveral hyperparameters. We employed the grid search [6] to determine the hyper-parameter values. Appendix D provides the hyper-parameters of experiments. 5.1.5 Baselines. We compare RL-MPCA with several baselines. Our situation has only one Elastic Queue phase (the two other phases are Elastic Channel and Elastic Model). In the situation containing only one Elastic Queue phase, the modeling methods of DCAF and CRAS are consistent. Therefore, in the later experiments, we only show the details of DCAF. \u00b7 Static . Static approach allocates CRs with global fixed rules, including fixed retrieval channels, fixed truncation length of candidate items, and fixed prediction models. \u00b7 DCAF . DCAF [21] formulates the CR allocation problem as an optimization problem with constraints, then solves the optimization problem with linear programming algorithms. In online A/B tests, we use fixed rules in Elastic Channel phase and Elastic Model phase, and DCAF is deployed in the Elastic Queue phase. \u00b7 ES-MPCA . Before RL-MPCA, we designed an evolutionary strategies based multi-phase computation allocation approach (ES-MPCA, see more details in Appendix E), which has been deployed on Meituan advertising system. \u00b7 Ex-RCPO . RCPO [31] solves a CMDP problem by introducing the penalized reward functions (i.e., calibrate rewards with Lagrange multiplier \ud835\udf06 ). We replace adaptive\ud835\udf06 of RLMPCA with the penalized reward functions when training the model, and name it Ex-RCPO. \u00b7 Ex-BCORLE( \ud835\udf06 ) . BCORLE [40] solves a single-constraint budget allocation problem with \ud835\udf06 -generalization. It cannot be directly applied to the multi-constraint CR allocation. We extend BCORLE from single\ud835\udf06 to multi\ud835\udf06 , and name it Ex-BCORLE. \u00b7 Ex-BCRLSP . BCRLSP [9] solves the single-constraint budget allocation problem by calibrating Q-value in near realtime. It cannot be directly applied to the multi-constraint CR allocation. We extend BCORLE from single\ud835\udf06 to multi\ud835\udf06 , and name it Ex-BCORLE. \u00b7 Ex-CrossDQN . CrossDQN [23] solves a single-constraint ads allocation problem by introducing auxiliary batch-level loss when training the model. We replace adaptive\ud835\udf06 of RLMPCA with auxiliary batch-level loss when training the model, and name it Ex-CrossDQN. Jogoo lhu 500oo Figure 5: Offline experiment results for adaptive\ud835\udf06 and \ud835\udf06 correction on multiple Deep Q-Network models. Agents are evaluated every 5,000 steps, and averaged over 5 seeds. 5.1.6 Offline Experiment Results. To answer question (1) and question (2), we train multiple models: DDQN, BCQ, REM, and their improved versions of introducing adaptive\ud835\udf06 , then use the simulation system to evaluate them. As shown in Figure 5, during the training process, introducing adaptive\ud835\udf06 can control the CRs of the model always around the target constraints for each phase (Figure 5.a.III and Figure 5.b). However, the CRs of models swing around the target constraints due to the inconsistent distribution of the mini-batch sampled during training and the evaluation dataset (see more details in Section 4.1.2). After the \ud835\udf06 correction, for each phase, the CRs of models strictly satisfy the constraints except for BCQ+ \ud835\udf06 (BCQ with adaptive\ud835\udf06 ), and Figure 5.a.IV shows the total CRs of all phases. Adaptive\ud835\udf06 allows the model to learn the Q-value under the case that CRs conform to the constraint (or in the near range of the constraints) at each phase. Thus, we can observe that the effectiveness of all three models improves after introducing adaptive\ud835\udf06 (Figure 5.a.II). An interesting phenomenon is that after introducing adaptive\ud835\udf06 , the CRs of BCQ instead cannot be stably calibrated to conform to the constraint. A potential reason is that the BCQ model contains an imitation component, which causes the BCQ model to imitate the behavioral strategy. As a result, the value of \ud835\udf06 changes in an unknown direction during the training process, and eventually, the Q value cannot be calibrated to satisfy the target constraints. To answer question (3), we compare RL-MPCA to the state-ofthe-art CR allocation approach DCAF and other related approaches. The results are shown in Table 1. Experiment results show that RL-MPCA outperforms other approaches in \ud835\udc5f\ud835\udc52\ud835\udc61\ud835\udc62\ud835\udc5f\ud835\udc5b when the CR constraints are satisfied.", "5.2 Online A/B test Results": "5.1.7 Hyper-parameter Analysis. To answer question (4), We compare the effect of two critical parameters, \ud835\udefc and \ud835\udc3e , on the performance of RL-MPCA. \ud835\udefc is the learning rate of adaptive\ud835\udf06 . \ud835\udc3e is \ud835\udf06 update times in one global step. Hyper-parameter \ud835\udefc . Like the learnng rate of the model's common parameters, the learning rate of adaptive\ud835\udf06 \ud835\udefc cannot be too big or too small. Too small a learning rate will lead to slow learning, while too big a learning rate will cause \ud835\udf06 to swing around the optimal value. Table 2 shows the model performance at different learning rates, and we finally choose 0 . 1 as the parameter value. Hyper-parameter \ud835\udc3e . A bigger \ud835\udc3e indicates more updates to the \ud835\udf06 at once update of the model parameter during training, which will make constraints easier to be satisfied. As seen in Table 2, the \ud835\udc5f\ud835\udc52\ud835\udc61\ud835\udc62\ud835\udc5f\ud835\udc5b increases with the increase of \ud835\udc3e . However, a bigger \ud835\udc3e also means more time consumption for training. To trade off the revenue and time, we choose 10 as the parameter value. We also evaluate the RL-MPCA approach for two weeks in the online environment. In online A/B tests, we compare our proposed RL-MPCA approach with several previous strategies deployed on Meituan advertising system. Table 3 lists the performance of several primary online metrics, including gross merchandise volume per mille (GPM, i.e., \ud835\udc3a\ud835\udc43\ud835\udc40 = avg ( \ud835\udc5d\ud835\udc5f\ud835\udc56\ud835\udc50\ud835\udc52 \ud835\udc5c ) \u2217 1000, where avg ( \ud835\udc5d\ud835\udc5f\ud835\udc56\ud835\udc50\ud835\udc52 \ud835\udc5c ) is the average of \ud835\udc5d\ud835\udc5f\ud835\udc56\ud835\udc50\ud835\udc52 \ud835\udc5c ), cost per mille (CPM, i.e., \ud835\udc36\ud835\udc43\ud835\udc40 = avg ( \ud835\udc53 \ud835\udc52\ud835\udc52 \ud835\udc4e\ud835\udc51 ) \u2217 1000, where avg ( \ud835\udc53 \ud835\udc52\ud835\udc52 \ud835\udc4e\ud835\udc51 ) is the average of \ud835\udc53 \ud835\udc52\ud835\udc52 \ud835\udc4e\ud835\udc51 ), click-through rate (CTR), and post-click conversion rate (CVR). RL-MPCA outperforms all other approaches, and ES-MPCA and DCAF take second and third place, respectively.", "6 CONCLUSION AND FUTURE WORK": "This paper proposes a Reinforcement Learning based Multi-Phase Computation Allocation approach, RL-MPCA, for recommender systems. RL-MPCA creatively formulates the computation resource (CR) allocation problem as a Weakly Coupled MDP problem and solves it with an RL-based approach. Besides, RL-MPCA designs a novel multi-scenario compatible Q-network adapting to various CR allocation scenarios, and calibrates Q-value by introducing multiple adaptive Lagrange multipliers (adaptive\ud835\udf06 ) to avoid violating the global CR constraints when maximizing the business revenue. Both offline experiments and online A/B tests validate the effectiveness of our proposed RL-MPCA approach. In future work, we plan to explore more general CR allocation approaches and more CR allocation application scenarios. Moreover, we plan to explore a new simulation scheme to capture the stochastic variation of response time and system load and then jointly model the response time constraint and the CR constraint to improve the system's availability.", "REFERENCES": "[1] Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. 2017. Constrained policy optimization. In International conference on machine learning . PMLR, 2231. [2] Daniel Adelman and Adam J Mersereau. 2008. Relaxations of weakly coupled stochastic dynamic programs. Operations Research 56, 3 (2008), 712-727. [3] Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. 2020. An optimistic perspective on offline reinforcement learning. In International Conference on Machine Learning . PMLR, 104-114. [4] Eitan Altman. 1999. Constrained Markov decision processes . Routledge. [5] Kiam Heong Ang, Gregory Chong, and Yun Li. 2005. PID control system analysis, design, and technology. IEEE transactions on control systems technology 13, 4 (2005), 559-576. [6] James Bergstra and Yoshua Bengio. 2012. Random search for hyper-parameter optimization. Journal of machine learning research 13, 2 (2012). [7] Craig Boutilier and Tyler Lu. 2016. Budget allocation using weakly coupled, constrained Markov decision processes. (2016). [8] Paris Carbone, Asterios Katsifodimos, Stephan Ewen, Volker Markl, Seif Haridi, and Kostas Tzoumas. 2015. Apache flink: Stream and batch processing in a single engine. Bulletin of the IEEE Computer Society Technical Committee on Data Engineering 36, 4 (2015). [9] Fanglin Chen, Xiao Liu, Bo Tang, Feiyu Xiong, Serim Hwang, and Guomian Zhuang. 2022. BCRLSP: An Offline Reinforcement Learning Framework for Sequential Targeted Promotion. arXiv preprint arXiv:2207.07790 (2022). [10] Yi Chen, Jing Dong, and Zhaoran Wang. 2021. A primal-dual approach to constrained markov decision processes. arXiv preprint arXiv:2101.10895 (2021). [11] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks for youtube recommendations. In Proceedings of the 10th ACM conference on recommender systems . 191-198. [12] Yang Deng, Yaliang Li, Fei Sun, Bolin Ding, and Wai Lam. 2021. Unified conversational recommendation policy learning via graph-based reinforcement learning. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1431-1441. [13] Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, R\u00e9mi Gribonval, Herve Jegou, and Armand Joulin. 2020. Training with quantization noise for extreme model compression. arXiv preprint arXiv:2004.07320 (2020). [14] Yufei Feng, Fuyu Lv, Weichen Shen, Menghan Wang, Fei Sun, Yu Zhu, and Keping Yang. 2019. Deep session interest network for click-through rate prediction. arXiv preprint arXiv:1905.06482 (2019). [15] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. 2020. D4rl: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219 (2020). [16] Scott Fujimoto, Edoardo Conti, Mohammad Ghavamzadeh, and Joelle Pineau. 2019. Benchmarking batch deep reinforcement learning algorithms. arXiv preprint arXiv:1910.01708 (2019). [17] Scott Fujimoto, David Meger, and Doina Precup. 2019. Off-policy deep reinforcement learning without exploration. In International conference on machine learning . PMLR, 2052-2062. [18] Rong Gao, Haifeng Xia, Jing Li, Donghua Liu, Shuai Chen, and Gang Chun. 2019. DRCGR: Deep reinforcement learning framework incorporating CNN and GANbased for interactive recommendation. In 2019 IEEE International Conference on Data Mining (ICDM) . IEEE, 1048-1053. [19] Yue He, Xiujun Chen, Di Wu, Junwei Pan, Qing Tan, Chuan Yu, Jian Xu, and Xiaoqiang Zhu. 2021. A Unified Solution to Constrained Bidding in Online Display Advertising. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining . 2993-3001. [20] Farah Tawfiq Abdul Hussien, Abdul Monem S Rahma, and Hala Bahjat Abdul Wahab. 2021. Recommendation systems for e-commerce systems an overview. In Journal of Physics: Conference Series , Vol. 1897. IOP Publishing, 012024. [21] Biye Jiang, Pengye Zhang, Rihan Chen, Xinchen Luo, Yin Yang, Guan Wang, Guorui Zhou, Xiaoqiang Zhu, and Kun Gai. 2020. DCAF: A Dynamic computation resource allocation Framework for Online Serving System. arXiv preprint arXiv:2006.09684 (2020). [22] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. 2020. Conservative q-learning for offline reinforcement learning. Advances in Neural Information Processing Systems 33 (2020), 1179-1191. [23] Guogang Liao, Ze Wang, Xiaoxu Wu, Xiaowen Shi, Chuheng Zhang, Yongkang Wang, Xingxing Wang, and Dong Wang. 2022. Cross dqn: Cross deep q network for ads allocation in feed. In Proceedings of the ACM Web Conference 2022 . 401409. [24] Shichen Liu, Fei Xiao, Wenwu Ou, and Luo Si. 2017. Cascade ranking for operational e-commerce search. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . 1557-1565. [25] Yongshuai Liu, Jiaxin Ding, and Xin Liu. 2020. IPO: Interior-point policy optimization under constraints. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 34. 4940-4947. [26] Nicolas Meuleau, Milos Hauskrecht, Kee-Eung Kim, Leonid Peshkin, Leslie Pack Kaelbling, Thomas L Dean, and Craig Boutilier. 1998. Solving very large weakly coupled Markov decision processes. In AAAI/IAAI . 165-172. [27] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. 2015. Human-level control through deep reinforcement learning. nature 518, 7540 (2015), 529-533. [28] Antonio Polino, Razvan Pascanu, and Dan Alistarh. 2018. Model compression via distillation and quantization. arXiv preprint arXiv:1802.05668 (2018). [29] Reuven Y Rubinstein and Dirk P Kroese. 2004. The cross-entropy method: a unified approach to combinatorial optimization, Monte-Carlo simulation, and machine learning . Vol. 133. Springer. [30] Pingzhong Tang, Xun Wang, Zihe Wang, Yadong Xu, and Xiwang Yang. 2020. Optimized Cost per Mille in Feeds Advertising. In Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems . 1359-1367. [31] Chen Tessler, Daniel J Mankowitz, and Shie Mannor. 2018. Reward constrained policy optimization. arXiv preprint arXiv:1805.11074 (2018). [32] Hado Van Hasselt, Arthur Guez, and David Silver. 2016. Deep reinforcement learning with double q-learning. In Proceedings of the AAAI conference on artificial intelligence , Vol. 30. [33] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. 2016. Dueling network architectures for deep reinforcement learning. In International conference on machine learning . PMLR, 1995-2003. [34] Daan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun, Jan Peters, and J\u00fcrgen Schmidhuber. 2014. Natural evolution strategies. The Journal of Machine Learning Research 15, 1 (2014), 949-980. [35] Di Wu, Xiujun Chen, Xun Yang, Hao Wang, Qing Tan, Xiaoxun Zhang, Jian Xu, and Kun Gai. 2018. Budget constrained bidding by model-free reinforcement learning in display advertising. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management . 1443-1451. [36] Ruobing Xie, Shaoliang Zhang, Rui Wang, Feng Xia, and Leyu Lin. 2021. Hierarchical reinforcement learning for integrated recommendation. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 35. 4521-4528. [37] Chaoqi Yang, Junwei Lu, Xiaofeng Gao, Haishan Liu, Qiong Chen, Gongshen Liu, and Guihai Chen. 2020. MoTiAC: Multi-objective actor-critics for real-time bidding. arXiv preprint arXiv:2002.07408 (2020). [38] Xun Yang, Yunli Wang, Cheng Chen, Qing Tan, Chuan Yu, Jian Xu, and Xiaoqiang Zhu. 2021. Computation Resource Allocation Solution in Recommender Systems. arXiv preprint arXiv:2103.02259 (2021). [39] Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn. 2021. Combo: Conservative offline model-based policy optimization. Advances in neural information processing systems 34 (2021), 28954-28967. [40] Yang Zhang, Bo Tang, Qingyu Yang, Dou An, Hongyin Tang, Chenyang Xi, Xueying Li, and Feiyu Xiong. 2021. BCORLE ( \ud835\udf06 ): An Offline Reinforcement Learning and Evaluation Framework for Coupons Allocation in E-commerce Market. Advances in Neural Information Processing Systems 34 (2021), 2041020422. [41] Xiangyu Zhao, Changsheng Gu, Haoshenglun Zhang, Xiwang Yang, Xiaobing Liu, Jiliang Tang, and Hui Liu. 2021. Dear: Deep reinforcement learning for online advertising impression in recommender systems. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 35. 750-758. [42] Xiangyu Zhao, Xudong Zheng, Xiwang Yang, Xiaobing Liu, and Jiliang Tang. 2020. Jointly learning to recommend and advertise. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 3319-3327. [43] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through rate prediction. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1059-1068. [44] Sijin Zhou, Xinyi Dai, Haokun Chen, Weinan Zhang, Kan Ren, Ruiming Tang, Xiuqiang He, and Yong Yu. 2020. Interactive recommender system via knowledge graph-enhanced reinforcement learning. In Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval . 179188.", "A SIMULATION SYSTEM": "The offline simulation system contains two modules: the request simulation module and the revenue estimation module. For a given request, the request simulation module is responsible for interacting with an agent and generating interaction results. The revenue estimation module is a deep neural network model based on supervised learning, which evaluates the simulation results and predicts the user views, clicks, and purchases for each request. Although the offline simulation system requires a lot of time and computation resources, the prediction results of the revenue estimation module are relatively accurate because the request simulation module can generate detailed information about the requests. Finally, after calibrating the output of the revenue estimation model, our offline simulation system can achieve fairly confident revenue estimation results. As Figure 6 shows, for each request \ud835\udc56 , the interaction of the simulation system and the agent involves multiple steps. Agent Simulation System Elastic Channel Ads Ad list Elastic Queue Elastic Model Reward Prediction request \ud835\udc56 Step 2 Step 1 Step 4 Step 3 Step 6 Step 5 Step 7 Step 8 \u00b7 Step 1 . The simulation system constructs and feeds the initial state \ud835\udc60 \ud835\udc56 1 to the agent. \u00b7 Step 2 . The agent takes Elastic Channel action \ud835\udc4e \ud835\udc56 1 based on state \ud835\udc60 \ud835\udc56 1 . \u00b7 Step 3 . The simulation system retrieves the ads with action \ud835\udc4e \ud835\udc56 1 , and feeds state \ud835\udc60 \ud835\udc56 2 (including the retrieval ad list) to the agent. \u00b7 Step 4 . The agent takes Elastic Queue action \ud835\udc4e \ud835\udc56 2 based on state \ud835\udc60 \ud835\udc56 2 . \u00b7 Step 5 . The simulation system simulates the truncation operation with the truncation length corresponding to action \ud835\udc4e \ud835\udc56 2 , and feeds state \ud835\udc60 \ud835\udc56 3 (including the truncated ad list) to the agent. \u00b7 Step 6 . The agent takes Elastic Model action \ud835\udc4e \ud835\udc56 3 based on state \ud835\udc60 \ud835\udc56 3 . \u00b7 Step 7 . The simulation system provides the prediction service for ads with the prediction model corresponding to action \ud835\udc4e \ud835\udc56 3 , and outputs state \ud835\udc60 \ud835\udc56 4 (including the truncated ad list and its prediction scores). \u00b7 Step 8 . The simulation system takes state \ud835\udc60 \ud835\udc56 4 as input features, and predicts the final revenue (i.e., user views, clicks, and purchases) with a supervised learning based deep neural network model (see the architecture in Figure 7). DCiick Concal Corcal Concai", "B PROOF": "To slove the single-phase computation resource (CR) allocation problem in Section 4.1, we introduce a Lagrange multiplier \ud835\udf06 \ud835\udc61 , and construct the dual problem: In phase \ud835\udc61 , for request \ud835\udc56 , there is one and only one action \ud835\udc4e \ud835\udc61 can be taken. Then the dual problem above can be further transformed as: Thus, we have the global optimal solution to original problem, \ud835\udc65 \ud835\udc56,\ud835\udc4e \u2217 \ud835\udc61 = 1 when: Note that a similar proof has been provided in [9], but the constraint definition of our optimization problem is different from it.", "C COMPUTATION COST ESTIMATION": "Essentially, CRs include computing resources, memory resources, network transmission resources, etc. In real industrial applications, computation cost estimation aims to find a metric that is easy to calculate and can be directly mapped to the amount of computation consumed. CRAS uses queue length as the computation cost metric, which is simple and feasible in Elastic Queue scenarios, and we have verified this in Meituan advertising system. However, queue length does not apply to Elastic Channel and Elastic Model scenarios. Specifically, in Elastic Channel, the primary metric affecting the CR consumption of the retrieval service is the number of requests entering the service. In Elastic Model, the primary metrics affecting the resource consumption of the prediction service are the number of requests and the total number of ads entering the model. During the model training, we use the number of requests entering the retrieval channel and the number of requests entering the complex prediction model as the computation cost evaluation metrics to facilitate the evaluation of system computation. Because the Elastic Queue guarantees the number of ads entering the prediction model, it is reasonable to ignore the number of ads in Elastic Model when training the model. In the offline experiments and online A/B tests, we also ensured that the number of ads entering the complex prediction model did not exceed the target value.", "D HYPER-PARAMETERS": "Table 4 lists the hyper-parameters of experiments.", "E ES-MPCA": "Same as RL-MPCA (see more details in Section 3.2), ES-MPCA also formulates the multi-phase CR allocation problem as a Weakly Coupled MDP problem. The difference is that ES-MPCA solves it with an evolutionary strategies based (ES-based) approach. To solve the Weakly Coupled MDP problem, we consider it as a blackbox optimization problem, aiming to maximize the total business revenue under the CR constraints. In this paper, we use Cross-Entropy Method (CEM) [29] to solve the black-box optimization problem. ES-MPCA designs the actions as: where \ud835\udc50\u210e\ud835\udc4e\ud835\udc5b\ud835\udc5b\ud835\udc52\ud835\udc59\ud835\udc44\ud835\udc62\ud835\udc5c\ud835\udc61\ud835\udc4e , \ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc62\ud835\udc52\ud835\udc3f\ud835\udc52\ud835\udc5b and \ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59\ud835\udc44\ud835\udc62\ud835\udc5c\ud835\udc61\ud835\udc4e are retrieval strategy number, truncation length and prediction model number, respectively. ( \ud835\udf3d \ud835\udc50 , \ud835\udf3d \ud835\udc5e , \ud835\udf3d \ud835\udc5a ) and ( \ud835\udc99 \ud835\udc50 , \ud835\udc99 \ud835\udc5e , \ud835\udc99 \ud835\udc5a ) are parameters and features, respectively. Algorithm 2 Offline Training of ES-MPCA (Based on CEM) Input: Number of iteration \ud835\udc3c , the number of parameters \ud835\udc41 \ud835\udc4e\ud835\udc59\ud835\udc59 = \ud835\udc41 \ud835\udc50\u210e\ud835\udc4e\ud835\udc5b\ud835\udc5b\ud835\udc52\ud835\udc59 + \ud835\udc41 \ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc62\ud835\udc52 + \ud835\udc41 \ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59 , the number of parameters sampled \ud835\udc41 \ud835\udc60\ud835\udc4e\ud835\udc5a\ud835\udc5d\ud835\udc59\ud835\udc52 , the number of parameters retained \ud835\udc41 \ud835\udc5f\ud835\udc52\ud835\udc61\ud835\udc4e\ud835\udc56\ud835\udc5b . 1: Initialize mean \ud835\udf41 0 = ( \ud835\udf07 0 1 , . . . , \ud835\udf07 0 \ud835\udc41 \ud835\udc4e\ud835\udc59\ud835\udc59 ) and variance \ud835\udf48 0 = ( \ud835\udf0e 0 1 , . . . , \ud835\udf0e 0 \ud835\udc41 \ud835\udc4e\ud835\udc59\ud835\udc59 ) of parameters. 2: for \ud835\udc56 = 1 , . . . , \ud835\udc3c do 3: Draw sample { \ud835\udf3d 1 , . . . , \ud835\udf3d \ud835\udc41 \ud835\udc60\ud835\udc4e\ud835\udc5a\ud835\udc5d\ud835\udc59\ud835\udc52 } \u223c \ud835\udc41 ( \ud835\udf41 \ud835\udc56 -1 , \ud835\udf48 \ud835\udc56 -1 ) 4: Evaluate { \ud835\udf3d 1 , . . . , \ud835\udf3d \ud835\udc41 \ud835\udc60\ud835\udc4e\ud835\udc5a\ud835\udc5d\ud835\udc59\ud835\udc52 } by simulation system (Reward Evaluation) 5: Sort { \ud835\udf3d 1 , . . . , \ud835\udf3d \ud835\udc41 \ud835\udc60\ud835\udc4e\ud835\udc5a\ud835\udc5d\ud835\udc59\ud835\udc52 } by the reward \ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc4e\ud835\udc5f\ud835\udc51 = \u02dd \ud835\udc49\ud835\udc4e\ud835\udc59\ud835\udc62\ud835\udc52 ( \ud835\udf3d ) -\u02dd \ud835\udc61 \ud835\udf06 \ud835\udc61 min { \ud835\udc36 \ud835\udc61 -\u02dd \ud835\udc36\ud835\udc5c\ud835\udc60\ud835\udc61 \ud835\udc61 ( \ud835\udf3d ) , 0 }) 6: Take top\ud835\udc41 \ud835\udc5f\ud835\udc52\ud835\udc61\ud835\udc4e\ud835\udc56\ud835\udc5b parameters { \ud835\udf3d 1 , . . . , \ud835\udf3d \ud835\udc41 \ud835\udc5f\ud835\udc52\ud835\udc61\ud835\udc4e\ud835\udc56\ud835\udc5b } , then calculate their mean \ud835\udf41 \ud835\udc56 and variance \ud835\udf48 \ud835\udc56 7: end for Algorithm 2 describes the training process of CEM-based ESMPCA. By imposing an extremely large penalty on the parameters that violate the constraint ( \ud835\udf06 \ud835\udc61 is generally an extremely large value, e.g., for each phase \ud835\udc61 , \ud835\udf06 \ud835\udc61 = 10 8 in our experiments), ES-MPCA always guarantees that the final output optimal parameters \ud835\udf3d \u2217 are those that satisfy the CR constraints. Experiment results show that the optimal parameters \ud835\udf3d \u2217 outputted by ES-MPCA always exactly satisfy the CR constraints (i.e., for each phase \ud835\udc61 , \u02dd \ud835\udc36\ud835\udc5c\ud835\udc60\ud835\udc61 \ud835\udc61 ( \ud835\udf3d \u2217 ) = \ud835\udc36 \ud835\udc61 holds), which is consistent with the assumptions and conclusions in Section 4.1.", "F BEHAVIORAL POLICIES": "In this section, we provide a detailed introduction to behavioral policies. Random exploratory policies randomly make decisions in each phase to explore the revenues under different actions, including randomly selecting retrieval channels, truncation lengths, and prediction models. Superior policies include ES-based policies and RL-based policies. We train them on a random dataset collected by random exploratory policies. More details of ES-based policies are provided in Appendix E.", "G ONLINE SERVING": "After model training (Algorithm 1) and \ud835\udf06 -correction (see more details in Section 4.1.2), we obtain the trained network \ud835\udc44 \ud835\udf03 and trained constraint parameter \ud835\udf40 = ( \ud835\udf06 1 , . . . , \ud835\udf06 \ud835\udc47 ) . Algorithm 3 shows the process of online serving for a given request.", "Algorithm 3 Online Serving of RL-MPCA": "Input: Trained Network \ud835\udc44 \ud835\udf03 , trained constraint parameter \ud835\udf40 = ( \ud835\udf06 1 , . . . , \ud835\udf06 \ud835\udc47 ) 1: Initialize state \ud835\udc60 0 2: for \ud835\udc61 = 1 , . . . , \ud835\udc47 do 3: Take action \ud835\udc4e \u2217 \ud835\udc61 = arg max \ud835\udc4e \ud835\udc61 ( \ud835\udc44 \ud835\udf03 ( \ud835\udc60 \ud835\udc61 , \ud835\udc4e \ud835\udc61 ) -\ud835\udf06 \ud835\udc61 \ud835\udc36\ud835\udc5c\ud835\udc60\ud835\udc61 ( \ud835\udc60 \ud835\udc61 , \ud835\udc4e \ud835\udc61 )) 4: Execute allocation following \ud835\udc4e \u2217 \ud835\udc61 5: Observe the next state from system 6: end for"}
