{
  "JobFormer: Skill-Aware Job Recommendation with Semantic-Enhanced Transformer": "ZhiHao Guan, Jia-Qi Yang, Yang Yang, Member, IEEE Hengshu Zhu, Senior Member, IEEE Wenjie Li, and Hui Xiong, Fellow, IEEE Abstract -Job recommendation aims to provide potential talents with suitable job descriptions (JDs) consistent with their career trajectory, which plays an essential role in proactive talent recruitment. In real-world management scenarios, the available JD-user records always consist of JDs, user profiles, and click data, in which the user profiles are typically summarized as the user's skill distribution for privacy reasons. Although existing sophisticated recommendation methods can be directly employed, effective recommendation still has challenges considering the information deficit of JD itself and the natural heterogeneous gap between JD and user profile. To address these challenges, we proposed a novel skill-aware recommendation model based on the designed semantic-enhanced transformer to parse JDs and complete personalized job recommendation. Specifically, we first model the relative items of each JD and then adopt an encoder with the local-global attention mechanism to better mine the intra-job and inter-job dependencies from JD tuples. Moreover, we adopt a two-stage learning strategy for skill-aware recommendation, in which we utilize the skill distribution to guide JD representation learning in the recall stage, and then combine the user profiles for final prediction in the ranking stage. Consequently, we can embed rich contextual semantic representations for learning JDs, while skill-aware recommendation provides effective JD-user joint representation for click-through rate (CTR) prediction. To validate the superior performance of our method for job recommendation, we present a thorough empirical analysis of large-scale real-world and public datasets to demonstrate its effectiveness and interpretability. Index Terms -Skill-Aware Representation, Transformer, Job Recommendation ✦",
  "1 INTRODUCTION": "Job recommendation aims at providing the right jobs to the right job seekers. In recent years, online recruitment data has experienced explosive growth. According to the report from The Insight Partners [1], the global online recruitment market size is expected to grow from $29.29 billion in 2021 to $47.31 billion by 2028. As a result, it is crucial for recruitment platforms to develop effective job recommendation systems that not only help companies quickly recruit candidates for specific positions, but also meet the needs of job seekers for an efficient and personalized job search experience. In real-world recruitment scenarios, the available recruitment records typically only include job descriptions (JDs), · Zhihao Guan and Yang Yang are with the Nanjing University of Science and Technology, Nanjing 210094, China. E-mail: zhguan,yyang@njust.edu.cn · Jia-Qi Yang is with State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China. Email:yangjq@lamda.nju.edu.com · Hengshu Zhu is with Career Science Lab, BOSS Zhipin, Beijing 100028, China. E-mail:zhuhengshu@gmail.com · Wenjie Li is with the Hong Kong Polytechnic University, Hong Kong, China. E-mail: wenjie.li@polyu.edu.hk. · Hui Xiong is with the Artificial Intelligence Thrust, The Hong Kong University of Science and Technology, Guangzhou, China. E-mail: xionghui@ust.hk Yang Yang (Corresponding Author) is with PCA Lab, Key Lab of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, and Jiangsu Key Lab of Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of Science and Technology. Fig. 1. A motivating example of two-stage skill-aware job recommendation (a). The JD contains multiple items including duties and requirements (b), and the corresponding user can be represented with personal skill distribution (c). Note that actually the items in JD correspond to the skill labels, and the degrees of demands correspond to the skill distributions. (a) (b) (c) Duties ： 1.Research natural language processing. 2.Design deep learning model. 3.Develop dialog system. Requirements ： 1.Skill at C/C++, python. 2.Experienced in deep learning project. 3.Published NLP papers. 4.Have team communication and cooperation skills. JD: Artificial Intelligence Algorithm Engineer Personal Skill Distribution 0.17 0.15 0.13 0.11 0.09 0.07 0.05 Machine Learning C/C++ Python NLP Deep Learning Dialog System Research Paper ... Distribution Skills ... ... Recall JD Pool Candidate List Top K JDs Ranking User Profiles user profiles, and click data. It is worth noting that users generally do not upload complete user profiles, primarily because complete user profiles may involve sensitive information, which could lead to privacy concerns or identity theft in case of improper use. Therefore, user profiles are usually summarized as personal skill distribution. With the 2 rapid development of deep learning (DL), intelligent recommendation systems have revolutionized the recruitment field [2, 3, 4, 5]. A natural and straightforward idea is to extract available information from JDs and user profiles, and rely on many existing sophisticated recommendation methods to accomplish job recommendation. For example, content-enriched recommendation approaches can model semantic relevance between user profiles and JDs from two aspects: the general features of user profiles and JDs, as well as the textual content information [6]. The former focuses on the feature interactions of user profile and JD, e.g., [7, 8, 9] explore the possibility of adopting neural models to automatically discover complex higher-order feature interactions for click-through rate (CTR) prediction and recommendation. The latter focuses more on multi-level automatic representation learning of textual content, e.g., [10, 11] employ the specially designed neural network to model the talent resumes and job descriptions respectively, which are jointly trained as a binary classification problem (i.e., consistent or inconsistent). Considering that these approaches cannot effectively mine users' interest preferences, a few approaches also explore to generate recommendation lists in a two-stage manner, which contains a recall stage for forming a candidate set and a ranking stage for ranking candidate items based on their relevance to user interests [12, 13]. However, these approaches are problematic since the JD itself is insufficiently informative, and there is a natural heterogeneous gap between JD (represented by short texts in Figure 1 (b)) and user profiles (represented by skill distributions in Figure 1 (c), which inevitably leads to a decline in recommendation effectiveness. To this end, this paper proposes a skill-aware recommendation method with a semantic-enhanced transformer. We first consider enhancing the semantic representation of JD by aggregating complementary information from neighbor JDs, which provides a more comprehensive understanding of the duties and requirements for a specific position, since neighbor JDs typically contain information related to the same domain or position. Specifically, we first model the relative items of each JD, and then adopt an encoder with the local-global attention mechanism to better mine the intra-job and inter-job dependencies from JD tuples. Moreover, in order to better mitigate the heterogeneous gap between JD and user profiles, as well as effectively discover users' interest preferences, our idea is to employ a twostage learning strategy for skill-aware job recommendation. To be specific, as shown in Figure 1 (a), we first utilize user profiles (i.e., personal skill distributions) to guide the representation learning of JDs in the recall stage, which enables to recall a set of candidate JDs in the embedding space according to the relevance metric function (i.e., cosine similarity). Then, in the ranking stage, we further predict the click-through rate between candidate JDs and the user for a personalized job recommendation. The major contributions can be summarized as follows: · Develop the semantic-enhanced transformer, which encodes the job descriptions with the designed localglobal attention mechanism for rich contextual semantics; · Propose the JobFormer, a two-stage method for job recommendation, which leverages skill-aware JD representation to mitigate the heterogeneous gap between job descriptions and user profiles, as well as to promote recommendation performance; · Empirically show the superiority of JobFormer on realworld datasets. We achieve state-of-the-art results and better interpretability for job recommendation.",
  "2 RELATED WORK": "In this paper, we aim to learn an effective job recommendation method with skill-aware JD representation. Therefore, our study is related to job recommendation systems and JD representation learning.",
  "2.1 Job Recommendation Systems": "Job recommendation is a core component of recruitment platforms, and it has been extensively studied in the literature [14, 15]. Early approaches treated this problem as a job-resume matching problem [16, 17], and obtained matching capabilities based on the collaborative filtering assumption. However, this approach overemphasizes the interaction between user profiles and job postings, which can result in limited recommendation performance when interaction data is sparse. To mitigate this challenge, recent studies have focused more on utilizing intelligent techniques to mine textual information, aiming to enhance the semantic representation of job and user profiles. Around this problem, [18] employed TF-IDF statistical approach to encode JDs and resumes. [19] developed a generalized linear mixed model (GLMix), a fine-grained model at the user or item level, in the LinkedIn job recommendation system, and generated 20% to 40% more job applications for job seekers. Thanks to the advances in deep neural networks (DNNs) that are extensively used in the field of natural language processing (NLP), DNN-based approaches are proposed and have demonstrated state-of-the-art results. For instance, [11] developed a novel end-to-end neural model, which projects both job postings and candidate resumes onto a shared latent representation for joint representation learning. [10] designed an ability-aware neural network, which extracts the ability-aware representations for job postings and resumes simultaneously by hierarchical representation structures. These methods emphasize the significance of considering effective representations of multi-modal input (i.e., job descriptions and resumes) in job recommendation tasks. Nevertheless, previous job recommendation approaches usually only consider the relevance between JDs and user profiles, without paying more attention to users' interest preferences for JDs. Moreover, due to the complex heterogeneity between JDs and user profiles (e.g., personal skill distribution), it remains challenging to improve job recommendation performance.",
  "2.2 JD Representation with Deep Learning": "Generally, the JD representation problem based on textual data can be categorized as the tasks of text mining, which is highly relevant to Natural Language Processing techniques, such as text classification [20, 21], machine translation [22, 23], and reading comprehension [24, 25]. Recently, 3 due to the advanced performance and flexibility of deep learning, more and more researchers have attempted to leverage deep neural networks to address text mining problems. In contrast to traditional approaches that heavily rely on effective manually designed representations and input features (e.g., N-gram model [26], Bag-of-words model [27] and parse trees [28]), the deep learning-based approaches can automatically learn effective feature representations from a large-scale text corpus. Among various deep learning models, traditional deep learning models (e.g., convolutional neural network (CNN) [29] and recurrent neural network (RNN) [30]) and transformer-based models are two representative and extensively used approaches, which can provide practical ways for JD representation from different perspectives. Specifically, CNNs can effectively extract local semantics and hierarchical relationships in textural data. For instance, [31] proposed to encode the job based on CNN. [11] have shown that the power of CNN on person-job fit tasks, even only using a few one-dimensional convolutional layers to learn JD representation. Furthermore, RNN-based models have also achieved remarkable performance. For example, [10] designed a word-level semantic representation for both job requirements and job seekers' experiences based on the Recurrent Neural Network. Similarly, [32] adopt the RNNs with GRU units to propagate information along the word sequence of job posting. Compared with traditional deep learning models, transformer-based models are more 'natural' in modeling sequential textual data, and learning the contextual dependency and global semantic representation with self-attention mechanism. For example, [4] developed a hierarchical self-attention text representation model for developing the semantic matching model, in which a BERTbased encoder is first adopted to represent jobs, and then a transformer-based encoder is used to represent the overall text document based on learned sentence embeddings. Although transformer processes textual data efficiently, the information deficit of JD itself reduces the parsing of JD semantics. In this paper, we follow some outstanding ideas in the above works according to the properties of job recommendation and propose a two-stage method JobFormer based on the semantic-enhanced transformer with skill-aware JD representation. Therefore, JobFormer can not only improve the performance of job recommendation, but also enhance the model interpretability in practical scenarios.",
  "3 PRELIMINARIES": "In company talent management, the available JD-user records usually include job descriptions, user profiles (i.e., personal skill distribution) and click data. Specifically, a job description contains multiple items with short text forms, which describe duties and requirements. Without any loss of generality, we utilize j = { j 1 , j 2 , · · · , j M } to denote the items (e.g., the JD about artificial intelligence algorithm engineer as shown in Figure 1 (b)), M denotes the total number of duties and requirements, and we fix M as the maximum number of items for all JDs with padding mask as [33]. For user profiles, we adopt all the skills summarized as global label space (i.e., C skills totally), and then acquire the skill ratings from experts. Lastly, we normalize the summarized skill ratings as the label distribution with softmax operator according to [34], i.e., y = { y 1 , y 2 , · · · , y C } , satisfying the constraint y c ∈ [0 , 1] and ∑ c y c = 1 . To simplify our problem, we assume that a job can be represented by its duties and requirements, and the skill distribution of a candidate can mainly reflect his competency. As a matter of fact, as shown in Figure 1 (b), the duties and requirements in JD correspond to the personal skills, e.g., 'Design deep learning model' indicates that the user needs to have 'deep learning' skill. The degree of demands corresponds to the skill distributions, e.g., considering that 'deep learning' is repeatedly mentioned in the JD, and the demand is high (i.e., 'Design deep learning model' in duties and 'Experienced in deep learning project' in requirements), the degree of skill 'deep learning' should be high. A straightforward approach is to utilize label distribution learning (LDL) to predict the skill distribution of the input JDs and match it with the user profiles (i.e., personal skill distribution) according to the cosine similarity. However, this approach only recommends JDs to users from the perspective of relevance, which lacks the mining of users' interest preferences. Therefore, our idea is first to recall a set of candidate JDs and then utilize the click data to optimize the ranking ability of the model on the candidate JDs for a personalized job recommendation. Along this line, we can formally define the problems: Definition 1. (Skill-Aware Representation Learning for JD Recall). Given a set of successful person-job records D , each record ( j , y ) ∈ D is the corresponding JD and skill distribution. The target of JD recall can be formulated as learning a predictive model f for predicting the skill distribution of the input j , and then recalling a set of candidate JDs from a large-scale JD pool based on their relevance to user profiles. Definition 2. (Click-Through Rate Prediction for JD Ranking). Given a set of candidate sets D ′ with their corresponding click data, D ′ ⊂ D . JD ranking aims to further discover user-interesting JDs from candidate JDs, which can be defined as a CTR prediction task, i.e., predicting the probability of a user clicking on the candidate JDs.",
  "4 JOBFORMER": "As shown in Figure 2, following a widely used paradigm in real-world recommendation systems, JobFormer contains a recall stage for candidate JDs generation and a ranking stage for candidate JDs ranking. Specifically, in the recall stage, we first leverage the TextCNN [35] to encode the JD for diverse item-level representations, which are fed into the localglobal transformer to capture rich contextual semantics. The learned JD representations are further calculated similarity scores with user profiles (e.g., personal skill distribution) to recall candidate JDs from a large-scale JD pool. Finally, in the ranking stage, candidate JDs are combined with user profiles for CTR prediction with a click predictor. Next, we will describe each component of our JobFormer in detail.",
  "4.1 Item-Level Encoder": "As shown in Figure 1 (b), each job description j includes a set of items, including the duties and requirements. It is 4 Fig. 2. An illustration of the proposed JobFormer, which includes a recall stage for candidate JDs generation and a ranking stage for candidate JDs ranking. In the recall stage, the JD and its neighbors constitute the JD tuple, and the item-level encoder aims for the item representation, which acts as the input token for the semantic-enhanced transformer. Then the designed semantic-enhanced transformer encodes both the intra-job and inter-job information to acquire more discriminative JD representation, which is further recalled as candidate JDs according to the JD-user cosine similarity. Lastly, in the ranking stage, recalled candidate JDs are combined with user profiles for CTR prediction via a click predictor and ranked for a personalized job recommendation. M { d JD Pool Store Candidate JDs Similarity Score Query User Profiles Global JD Embedding User Profiles Embedding Rank Top-K JDs L +1 Semantic-Enhanced Transformer Semantic-Enhanced Transformer JobFormer Click Predictor Click Predictor JD Ranking } M { d L +1 Local-Global Attention Add&Norm Add&Norm FFN Q K V ˆ cls u ... ... Local Head Local Attention Global Attention Scale Softmax Scale Softmax Q Q V V K K Global Head d / N ( L + 1) M d / N M C Head 1 Head 2 ... ... ... Head Nl Head Nl + 1 Head Nl + Ng Layers N × Q K V Linear MatMul Scale Linear Linear MatMul Feed Forward CTR Prediction JD Recall JD Representation 1. Research natural language processing. 2. Design deep learning model. 3. Develop dialog system. ... ... ... ... ... 4. Skill at C/C++, python. 5. Experienced in deep learning project. 6. Have team communication and cooperation skills. 1. Experienced in computer vision project. 2. Familiar with machine methods. 3. Skill at a programming language. 4. Master the basic methods of image classification. 5. Have excellent problem analysis skills. 1. Experienced in project management. 2. Understand natural language technology. 3. Skill at Python or Java. 4. Design requirements document. 5. Strong learning ability and communication skills. Item-Level Encoder JD 1 JD 2 JDL +1 notable that these items are in short text forms and have no contextual information, which may lead to semantic confusion if we directly concatenate these items into long sentence for representation learning. For example, the word 'Design' in 'Design deep learning model' only describes the duty of deep learning model, and has no contribution to other duties or requirements, so there is no contextual relationship. Based on this idea, we need to model the items separately to obtain the item-level representations.",
  "4.2 Semantic-Enhanced Transformer": "Without any loss of generality, we adopt a shared TextCNN to process the items separately. In detail, given an item j m with S words, the corresponding matrix can be represented as: j m = [ x 1 , x 2 , · · · , x S ] ∈ R S × d . Then we apply two one-dimensional convolutional layers on the input layer considering that one-dimensional convolution can deal with an unfixed-length sequence [35]. To reduce the training cost, we apply Batch Normalization [36] followed by a Rectified Linear Unit (ReLU) layer [37] and a one-dimensional max-pooling layer on the outputs of onedimensional convolutional layers. Therefore, each item is inputted to the shared TextCNN:  where u m ∈ R d denotes the representations of m -th item. Based on the item-level representations, the key challenges to encode the JD are: 1) Various importance. Different duties and requirements have various importance in job description. Take the JD in Figure 1 (b) as an example, 'Design deep learning model' and 'Experienced in deep learning project' are more important than 'Priority for published papers' considering that deep learning skill is repeatedly mentioned and has a high demand (e.g., 'Design' and 'Experienced'), while 'published papers' is a supplementary condition. 2) Deficient information. Single JD may ignore some derived information. Take the JD in Figure 1 (b) as an example, many items are related to 'machine learning' skill, which is needed for the post (i.e., the successfully accepted user is considered for this skill). To overcome these problems, we design the transformer with local-global attention heads to measure the importance of intra-job items and integrate the inter-job information. Local Encoder. To comprehensively encode each JD by considering the dependencies between items, we employ the transformer encoder [33] as the backbone, which can encode the relationships among items by adopting the selfattention mechanism. Specifically, as shown in Figure 2, with the item representations, a job can be denoted as 5 U l = [ u 1 , u 2 , · · · , u M ] ∈ R M × d , where d is the dimension of hidden states. The identical block contains two sub-layers: 1) The first sub-layer utilizes multi-head attention to learn the correlated representations. 2) The second sub-layer adopts position-wise feed-forward network (FFN). In multi-head attention layer, the input representations can be used to compute three matrices: Q , K , and V corresponding to queries, keys, and values. The dot-product similarity between queries and keys determines attention distributions:  where Q l ∈ R M × d Nl , K l ∈ R M × d Nl , V l ∈ R M × d Nl , and W Q l ∈ R d × d Nl , W K l ∈ R d × d Nl , W V l ∈ R d × d Nl are learnable matrices. N l denotes the number of local heads. The activation function σ can be used as softmax here. It is notable that the multi-head attention is defined as the local attention here in literature, which aims to encode the intra-job information. Global Encoder. To introduce the extra neighbor JDs as complementary information, we further propose the joint modeling strategy with local-global attention. In detail, we first select L neighbors for j , i.e., N ( j ) = { j 1 , j 2 , · · · , j L } . Without any loss of generality, we adopt the euclidean distance according to the global embedding ˆ u cls and title of JD (e.g., the 'Artificial Intelligence Algorithm Engineer' as shown in Figure 1 (b)), i.e., ∥ ˆ u n 1 ,cls -ˆ u n 2 ,cls ∥ 2 2 , t ( u n 1 ) = t ( u n 2 ) , where t ( · ) represents the title representation with one-hot form, n 1 and n 2 represent the index of ˆ u . Thereby, we can concatenate L + 1 JDs as input with special token [ SEP ] for separation, i.e., U g = [ U l , N ( U l )] ⊤ ∈ R ( L +1) M × d . As shown in Figure 2, we add N g parallel heads as global attention. The dot-product similarity can be reformulated as:  where Q g ∈ R ( L +1) M × d Ng , K g ∈ R ( L +1) M × d Ng , V g ∈ R ( L +1) M × d Ng , and W Q g ∈ R d × d Ng , W K g ∈ R d × d Ng , W V g ∈ R d × d Ng are learnable matrices. The activation function σ can be used as softmax here. Finally, local-global attention is composed of N = N l + N g parallel heads, and d N l = d N g = d/N . For each JD, the results of local head and corresponding global head are concatenated, and the FFN can be reformulated as:  where id ( U l ) denotes the corresponding index of local U l in Att ( U g ) . W 1 , W 2 , and W U are matrices for linear transformation, b 1 and b 2 are the bias terms. Meanwhile, each sublayer is followed by dropout [38], shortcut connection [39], and layer normalization [40]. We can also add the special token [ CLS ] for each JD to learn the global representations, and the attention value depicts the importance of each item. Finally, for each JD, we can acquire the global JD embedding, i.e., ˆ u cls , from the [ CLS ] token, and individual embedding from other tokens. Therefore, the local head attentions are responsible for capturing local dependencies based on local details, i.e., the intra-job items, and global head attentions are designed to model the long-term dependencies between JDs, i.e., the inter-job items. The combination of local and global attention enables our JobFormer to dynamically model local items and capture the global dependencies of similar JDs. Consequently, for each JD j , we can acquire both the JD-level and item-level representations, i.e., ˆ j = [ˆ u cls , ˆ u 1 , ˆ u 2 , · · · , ˆ u M ] ∈ R ( M +1) × d , ˆ u cls ∈ R d is the JD global representation.",
  "4.3 Skill-Aware Representation Learning for JD Recall": "With the JD-level representations ˆ u cls , we can directly predict the skill distribution with a simple classifier, e.g., a fully connected network g with softmax operator. For simplicity, we can adopt the KL-divergence [41] between ground-truth and prediction:  KL ( a, b ) = a log a b is the KL-divergence that penalizes difference. However, the direct prediction ignores the correlations between different skills, i.e., a skill can help to learn another skill under certain conditions. For example, as shown in Figure 1, when the user has a high description degrees on skills 'Deep Learning' and 'Dialog System', the skill 'Natural Language Processing' is more likely to have a higher description degree than skill 'Software Engineering'. Because 'Deep Learning' and 'Dialog System' are usually related to 'Natural Language Processing'. Therefore, taking skill correlations into consideration can include more data information and achieve better performance [42, 43, 44], we consider two aspects in the recall stage: 1) skill correlation enhancement. 2) relation consistency. Skill correlation enhancement. In most real-world applications, skill correlations are usually local, i.e., different instances have specific skill correlations. For example, the needed users who are proficient in natural language processing should have higher similarity values among 'Deep Learning', 'Dialog System' and 'Natural Language Processing' than the needed users who are only familiar with natural language processing. Based on this idea, we collect additional information about the accepted users to assist in predicting the skill distribution, including the position name (which can reflect the skills the user has mastered) and position level (which can reflect the user's skill proficiency). In detail, we first encode this additional user information e aui with a fully connected layer and then adopt a bidirectional ranking loss [45] with margin α to match JD (i.e., ˆ u cls ) and additional user information (i.e., e aui ):  where [ x ] + ≡ max ( x, 0) and s is a similarity score function (i.e., cosine similarity). ˆ u cls ∗ and e aui ∗ represent the corresponding hardest negative JD and hardest negative additional user information within a minibatch, respectively. Relation consistency. To constrain the structure of neighbors, we define a relation consistency constraint using a 6 metric learning-based constraint, inspired by the linguistic structuralism [46] that relations can better present the knowledge than individual examples. Specifically, each JD and its neighbors can be denoted as a bag of L + 1 instances, i.e., Ψ( j ) , and the pairwise similarity of JDs representations and predictions should be consistent. Therefore, we constrain the KL divergence of the similarity vectors calculated by the predictions and representations. The JD representations can be denoted as ˆ u i,cls and the predictions can be formulated as g (ˆ u i,cls ) , where ˆ u i,cls = SE -Transformer ( TextCNN ( j i )) , SE -Transformer denotes the semantic-enhanced transformer. Therefore, the objective of relation consistency can be formulated as:  Φ is a relation prediction function with softmax operator, which measures the relation energy of the given tuple. In detail, Φ aims to measure the similarities, using the predictions as an example:  where d n 1 ,n 2 = KL ( g (ˆ u n 1 ,cls ) , g (ˆ u n 2 ,cls )) measures the distance. q n 1 ,n 2 denotes the relative instance-wise similarity. Finally, we pull the [ q n 1 ,n 2 ] into vector form. Φ(ˆ u 1 ,cls , · · · , ˆ u L +1 ,cls ) is calculated in the same way, with d n 1 ,n 2 = ∥ ˆ u n 1 ,cls -ˆ u n 2 ,cls ∥ 2 . Since the structure has higherorder properties than a single output, it can transfer knowledge more effectively and is more suitable for consistency measures. We define the total loss by combining the Eq. 4, Eq. 5, and Eq. 6:  where λ and µ is the balance parameter. To effectively minimize the target function with the guidance of auxiliary neighbors, each training batch consists groups of JDs. Each group contains a central JD together with its neighbor JDs as [47]. During inference, each test JD can be conditioned by a set of neighbors from training JDs to construct JD tuples. Finally, we compare the relevance degree between predicted skill distribution and personal ground-truth, i.e., sc ( g (ˆ u cls ) , y ) , sc denotes the cosine similarity function, and then recall candidate JDs from a large-scale JD pool according to the similarities.",
  "4.4 Click-Through Rate Prediction for JD Ranking": "The ranking stage aims to further discover user-interesting JDs from a small number of candidate JDs. As shown in Figure 2, the candidate JD and user profiles (i.e., personal skill distribution) are used to compute a click score via click predictor for personalized JD ranking. More specifically, we first train a cross-attention module to obtain joint embedding using candidate JD global embedding ˆ u cls ∈ R d and user profiles s ∈ R n , where d and n represent the dimension of embedding and the total number of skills, respectively:  where Q c ∈ R 1 × d , K c ∈ R 1 × n , V c ∈ R 1 × n , and W Q c ∈ R d × d , W K c ∈ R n × n , W V c ∈ R n × n are learnable matrices. The activation function σ can be used as softmax here. Then, the joint embedding e joint ∈ R d is fed into two fully connected layers with sigmoid function to output the predicted click probability:  Furthermore, we define the click-through rate prediction task as a binary classification problem, where a JD-user click interaction is assigned a target value 1, otherwise 0. Specifically, we use the cross-entropy as the loss function:  where R + and R -are the positive and negative click records. Finally, the click scores of candidate JDs are used for personalized ranking.",
  "5 EXPERIMENTS": "In this section, we demonstrate the effectiveness of JobFormer by verifying the following problems: · The recall and ranking performance compared with state-of-the-art baselines; · The performance compared with various variants; · Sensitivity analyses of parameters; · Interpretability of JobFormer.",
  "5.1 Data Description": "We conduct our validation on a real-world talent recruitment dataset, which is provided by a high-tech company in China. To protect the privacy of candidates, all the application records are anonymized by deleting personal information. The dataset contains 37540 successful job applications. Indeed, the low acceptance ( ≈ 1% ) clearly validates the importance of job recommendation in online recruitment. In detail, we analyze some statistics of our dataset. We find that the number of applications is relatively steady. Besides, it is notable that each job posting may accept multiple users. We find that the number of job postings with respect to the number of their successfully accepted users, roughly follows a long-tail distribution, and the vast majority of acceptances are controlled within 3 users. Thereby, we randomly select a user to constitute the person-job pair considering that the skill distributions of these users are similar. Finally, 15046 person-job pairs are kept in total.",
  "5.2 Implementations": "The settings of our experiments include the word embedding of item encoder, structure of semantic-enhanced transformer, and training details for the recall and ranking stages. For item encoder, we first employ TextCNN with two one-dimensional convolutions (i.e., kernel sizes are 2 and 3) as the item-level encoder. The dimension of word is d = 512 . Note that TextCNN can process an unfixedlength sequence [35]. The semantic-enhanced transformer is with 4 layers and 8 heads, i.e., N = 8 , including local heads 7 TABLE 1 Experimental results of different approaches on JD recall task. Higher Recall and NDCG rates mean better performance. TABLE 2 Experimental results of different approaches on JD ranking task. Higher",
  "5.3 Baseline Approaches": "AUC and MRR mean better performance. N l = 6 and global heads N g = 2 . The classifier g is with two fully connected layers. For semantic-enhanced transformer, we set the maximum number of items in each JD as M = 40 , the excessive parts are removed. The number of neighbor is L = 2 , the parameter λ = 0 . 2 , µ = 0 . 4 . Following [48], we initialized all the parameters in JobFormer with uniform distribution in [ -√ 6 / ( n in + n out ) , √ 6 / ( n in + n out )] , where n in , n out denote the number of input and output units, respectively. The number of negative samples associated with positive one is 200 and 3 for the recall and ranking stages, respectively. In all experiments, the batch size is set to 32. The optimization method is Adaptive Moment Estimation (Adam), the learning rate is searched in 0.5, 0.1, 0.05, 0.01, 0.005, 0.001 to find the best settings for each task and annealed by 0.8 every 3 epochs. Finally, we set the learning rate as 0.001. The ratio of dropout is 0.1 and the maximal number of epochs is 30. We run the following experiments with the implementation of an environment on NVIDIA Tesla V100 SXM2 GPUs. The code is available at https://github.com/data-ming-and-application/JAT. To verify the effectiveness of JobFormer, we compare it with various baseline methods: 1) Traditional word embedding-based recommendation methods, i.e., NPA [49], NAML [50], NRMS [51], and CNE-SUE [52]. 2) BERT-based pre-training recommendation methods, i.e., UNBERT [53] and MINER [54]. 3) CTR ranking methods, i.e., MaskNet [55] and FRNet [56]: · NPA: a news recommendation method which adopts the personalized attention mechanism to model text semantic information; · NAML: a news recommendation method with attentive multi-view learning to obtain news representation and attentive pooling to learn user representation; · NRMS: a news recommendation method which utilizes multi-head self-attention networks to extract finegrained representations from the news title and user history respectively; · CNE-SUE: a news recommendation framework consisting of collaborative news encoding and structural user encoding to enhance news and user representation learning; · UNBERT: a BERT-based approach which leverages the pre-trained model to enhance textual representation and capture multi-grained signals at both word-level and news-level; · MINER: a BRET-based pre-training model which employs a poly attention scheme to learn multiple interest vectors for each user; · MaskNet: a CTR ranking framework which takes advantage of instance-guided mask to solve the inefficiency of the feedforward neural network in CTR prediction; · FRNet: a CTR perdition model with a feature refinement module to learn context-aware feature representations by integrating the original and complementary feature representations with bit-level weights. Given the person-job data, the comparison methods for the recall stage consider the JD as input and personal skill TABLE 3 Experimental results of different local-global heads and neighbors on JD recall task. (JF: JobFormer) 8 distribution as ground-truth for training. For the ranking stage (i.e., CTR prediction task), we utilize the candidate JDs along with the click data to predict the probability of positive feedback, i.e. click, taking place on a JD. To measure the performances, we consider two aspects of evaluations: 1) JD recall, which aims to verify that the personal skill distribution can find the candidate JDs from a large-scale JD pool. Following [57], we adopt the widely-used metrics to evaluate JD recall performance, i.e., Recall@ K and NDCG@ K (we set K = 20 , 40 , 60 , 80 , 100 ). 2) JD ranking, which aims to discover user-interesting JDs from candidate JD pool according to click scores. Specifically, we adopt AUC and MRR to evaluate JD ranking performance. Generally, the larger the values are, the better acquiring results.",
  "5.4 JD Recall Performance": "Firstly, we verify the effectiveness of JobFormer in the JD recall task. We repeat the experiment on each method 5 times and show results in Table 1. Referring to this table, we have several findings: 1) Traditional word embeddingbased recommendation methods outperform BERT-based pre-training recommendation methods. This indicates that word embedding can better encode the JD, mainly due to the JD structure and the relatively small-scale of our dataset. Thereby, we introduce the word embedding and TextCNN to encode JD texts in model design. 2) NRMS performs the best among all the word embedding-based methods on most criteria. The reason lies in that NRMS adopts deep models such as Transformer to learn contextual representations of JD items by capturing their interactions. This phenomenon indicates that multi-head self-attention can more effectively mine JD semantic information. 3) JobFormer achieves the best performance compared with other baselines on all criteria, e.g., JobFormer exceeds NRMS 2.31% on Recall@ 20 , 1.18% on NDCG@ 20 . The results validate the effectiveness of designed modules (e.g., local-global transformer) for processing JD.",
  "5.5 JD Ranking Performance": "Next, we further carry out the JD ranking task to discover user-interesting JDs from candidate JDs. In detail, we rank the candidate JDs according to the predicted click scores between candidate JD global representation and user profiles (i.e., personal skill distribution) for a personalized job recommendation. The ranking performance of different comparison methods is shown in Table 2. We acquire a similar conclusion to the JD recall task that JobFormer achieves the best performance compared with other baselines. Note that for CTR ranking methods (i.e., FRNet and MaskNet), we utilize the raw samples of candidate JDs recalled by JobFormer to predict the click scores. JobFormer continues to perform best, mainly because JD representations obtain rich intra-job and inter-job information via semantic-enhanced transformer in the recall stage.",
  "5.6 Ablation Study": "To verify the effectiveness of each module, we conduct more ablation studies, including: 1) LSTM+ and TextCNN+, we utilize the LSTM/TextCNN as item-level encoder, and then input the item representations to the transformer encoder, which is trained with KL divergence as the loss function. 2) local-global+, we adopt the TextCNN as item-level encoder, and then input the item representations to the semanticenhanced transformer encoder with local-global attention, which is trained with KL divergence as the loss function. 3) w/o M , we remove the skill correlation enhancement M ( · ) in JobFormer. 4) w/o R , we remove the relation consistency R ( · ) in JobFormer. The bottom section of Table 1 records the results, which reveal that: 1) TextCNN+ performs better than LSTM+, which verifies that TextCNN can better model the JD items considering that TextCNN can process an unfixedlength sequence to capture more information [35], but the LSTM usually requires a fixed-length sentence. 2) localglobal+ can improve performance, which indicates that local-global attention can effectively model the intra-job and inter-job information. 3) w/o M performs superior to the local-global+, which shows that the relation consistency can further improve the recall performance. 4) w/o R performs best in variants, which indicates that it is crucial to consider skill correlations. 5) JobFormer performs best compared with all the variants, which reveals that all the designed modules can promote the prediction of skill distribution in the recall stage. 9 Distribution Skill Labels 0.17 0.15 0.13 0.11 0.09 0.07 0.05 Python Java HTML5 IOS Android JavaScript MySQL ... User 1 Distribution Skill Labels 0.16 0.14 0.12 0.10 0.08 0.06 0.04 Big Recommend ation System Data Vision Deep Learning Knowledge Graph Algorithm Image ... Mining Machine Data User 2 User 1 Distribution Skill Labels 0.17 0.15 0.13 0.11 0.09 0.07 0.05 Python Java HTML5 IOS Android JavaScript MySQL ...",
  "User 2": "",
  "JD 2: Front-end Development Engineer": "",
  "JD 2: Machine Learning Engineer": "",
  "Duties ：": "",
  "Requirements ：": "1.Familiar with basic methods of data analysis; 2.Have some research in any field of CV/NLP; 3.Preferred to publish papers in top international conferences or journals; 4.Certified image recognition engineer is preferred. Fig. 3. (Best viewed in color when zoomed in.) Qualitative success results of JD recall given user queries. For each user query, we show the top-3 ranked JD text. The first two rows exhibit the results of JobFormer, and the last two rows give the results of the state-of-the-art NRMS model. We observe that our JobFormer can find the correct results (i.e., red marked) in the first-ranked JDs, and NRMS is inferior to JobFormer. Fig. 4. Influence of Balance Parameters. The figures in the first row are the results of λ , and the second row gives the results of µ . Recall@20 Recall@40 67.00 69.50 72.00 74.50 77.00 79.50 82.00 84.50 90.00 91.00 92.00 93.00 94.00 95.00 96.00 97.00 98.00 Recall@60 Recall@80 Recall@100 32.50 33.00 33.50 34.00 34.50 35.00 35.50 36.00 36.50 NDCG@20 NDCG@40 36.50 36.70 36.90 37.10 37.30 37.50 37.70 37.90 38.10 38.30 38.50 NDCG@60 NDCG@80 NDCG@100 66.00 68.50 71.00 73.50 76.00 78.50 81.00 83.50 86.00 Recall@20 Recall@40 90.00 91.00 92.00 93.00 94.00 95.00 96.00 97.00 98.00 Recall@60 Recall@80 Recall@100 31.00 32.00 33.00 34.00 35.00 36.00 37.00 NDCG@20 NDCG@40 36.00 36.50 37.00 37.50 38.00 38.50 39.00 NDCG@60 NDCG@80 NDCG@100",
  "JD 2: Android R&D Engineer": "",
  "JD 1: Android R&D Engineer": "",
  "JD 1: AI algorithm R&D Engineer": "",
  "JD 1: PHP Senior R&D Engineer": "",
  "JD 3: Java R&D Engineer": "",
  "Requirements": "： 1.Have a good grasp of multithreading, high concurrency, IO, and reflection; 2.Master Spring MVC, MyBatis and other mainstream development frameworks; 3.Understand database and other software engineering methods, and have practical experience in large development projects.",
  "JD 3: Data Mining Algorithm Engineer": "",
  "JD 3: AI algorithm R&D Engineer": "",
  "JD 1: Automated Driving Decision Planning Engineer Duties ：": "1.Responsible for the development of autonomous vehicle predictive decision planning system; 2.Design the core driving scenario processing strategy; 3.Complete visual algorithm development and effect verification.",
  "JD 2: Data Analytics R&D Engineer Duties ：": "1.Responsible for multi-dimensional data analysis using data mining algorithms; 2.Construct a complete user lifecycle behavior data system; 3.Responsible for the construction of data application product system.",
  "5.7 Parameter Analysis": "Influence of Local-Global Heads. Considering that JobFormer with various local heads and global heads can have different emphasis on intra-job information and inter-job information, we fix the total heads as N = 8 and tune the local-global heads in { ( N l = 4 , N g = 4) , ( N l = 5 , N g = 3) , ( N l = 6 , N g = 2) , ( N l = 7 , N g = 1) } , to empirically investigate the impact on JD recall performance. The top section of Table 3 depicts the results, the performance (including Recall and NDCG) of JobFormer firstly increases and then decreases. The JobFormer acquires best performance when local-global is ( N l = 6 , N g = 2) . This phenomenon confirms that the intra-job information is essential for learning skill-aware representation, but the neighbor JD can also provide additional supplementary information, i.e., the performance of ( N l = 7 , N g = 1) is worse than ( N l = 6 , N g = 2) . Therefore, we need proper attention to Experimental results of different approaches on public dataset. 10 TABLE 4",
  "JD 1: Vision Algorithm Engineer.": "",
  "JD 2: C++ Senior R&D Engineer.": "1.Research and development of image/face recognition algorithm;",
  "Duties:": "JD1 2.Analyze the online algorithm badcase and improve the model ability;",
  "Requirements:": "2.Familiar with C++ and proficient in using STL; 1.3 or more years of C++ development experience; 3.Familiar with H265, VP9 and other mainstream audio coding formats; 4.Master FEC, STUN, TURN and other technologies.",
  "User 1: Senior Algorithm Engineer.": "1.Image Algorithm(0.153); 3.Deep Learning(0.131);",
  "Skills:": "2.C++(0.112); 6.Software Testing(0.052); JD2 4.Machine Learning(0.068); 5.Node.JS(0.066); 3.Automated Testing(0.092); 7.Automatic Driving(0.057); 7.Requirement Analysis(0.048); ...... 8.Data Warehouse(0.042); 4.PHP(0.083); 8.Python(0.034); ...... Fig. 5. (Best viewed in color when zoomed in.) The example of interpretable CTR prediction. Machine Learning Automatic Driving Knowledge Graph Data Mining Data Structure Big Data Image Algorithm Python Deep Learning Data Operation 5 6 7 8 9 10 Attention Weights Skills System Integration Software Testing C++ Python C# Test Development Data structure Cloud Computing Java PHP 3 4 5 6 7 8 Attention Weights Skills C++ Deep Learning Hadoop Python APP Design Image Algorithm JavaScript Product Management Content Editor Data Mining 0 1 2 3 4 Attention Weights Skills Case 1 JD1 CTR: 0.87 Case 2 JD2 CTR: 0.82 Case 3 JD1 CTR: 0.38 neighbor information. Number of Neighbors. To validate the influence of neighbors on the JD representation learning, we incorporate neighbors with different numbers (i.e., L ∈ { 1 , 2 , 3 , 4 , 5 } ) to empirically investigate the impact of neighbors on JD recall task. Note that we fix the local-global heads as ( N l = 6 , N g = 2) . The bottom section of Table 3 depicts the results, the performance of JobFormer also increases firstly, and then decreases on various criteria. The reason is that more neighbors can even bring noise and the over-smooth problem. Influence of Balance Parameters. To explore the influence of hyper-parameters, we tune the λ, µ ∈ { 0 . 1 , 0 . 2 , 0 . 4 , 0 . 6 , 0 . 8 } to conduct more experiments. The top section of Figure 4 depicts the performance of different λ . With the increase of λ , the results of JD recall first increase and then decrease. This shows that skill correlation enhancement has a promoting effect, but over-considering skill correlation may introduce bias for prediction. The bottom section of Figure 4 depicts the performance with different µ . The results of JD recall first increase with the increase of µ , and then decrease after µ > 0 . 4 . This shows that relation consistency actually has a promoting effect, but the label distribution prediction loss (i.e., KL-divergence) still has more contributions to model learning.",
  "User 2: Senior Software R&D Engineer.": "1.Test Development(0.124);",
  "5.8 Results on Public Dataset": "Furthermore, we also evaluate JobFormer on public dataset, i.e., SBU 3DFE dataset [58]. The SBU 3DFE database contains 2,500 facial expression images. A 243-dimensional feature vector is extracted from each image by the method of Local Binary Patterns (LBP). Each image is scored by 23 persons on the 6 basic emotion labels (i.e., happiness, sadness, surprise, fear, anger, and disgust) with a 5-level scale. We score 11 and normalize them into a label distribution over all the 6 emotion labels following [58]. Table 4 records the results, we can acquire the similar analyses as private dataset that JobFormer can also achieve the best recall performance. This phenomenon validates the generalization of JobFormer.",
  "5.9 Case Study": "JD Recall Visualization. Firstly, we evaluate whether JobFormer could effectively recall user-related JDs from a largescale JD pool. Figure 3 shows the recall results of top3 JDs given the corresponding user query. We find that most of the recalled candidate JDs according to the cosine similarity are exactly matched (title displayed in red font) with the ground-truth. Other outputs are also reasonable. Using the first row of Figure 3 as an example, the 2nd and 3rd candidate JDs also have corresponding keywords such as 'front-end', 'W3C standards' are related to skill 'HTML5', and 'Spring MVC', 'MyBatis' are related to skill 'Java'. Meanwhile, other skills (e.g., 'MySQL') are also required for Front-end Development Engineer and Java R&D Engineer. Besides, we also exhibit the results of the state-ofthe-art recommendation model NRMS, which is inferior to JobFormer that it only matches the correct JD on 2nd or 3rd location. Interpretable CTR Prediction. To verify whether JobFormer could highlight the most critical skills that have strong contributions to CTR prediction, we present the attention weights of three different JDs with their corresponding user profiles, the results are recorded in Figure 5. In detail, we take the cumulative attention weights of each dimension of [ CLS ] token as the level of emphasis the entire JD places on personal skills. Considering the page limitation, we only show the top-10 skills for interpretability analysis. From the figures, we find that: 1) the skill-aware JD representation can accurately capture most of the relevant skills. For example, the skills 'Machine Learning', 'Automatic Driving', 'Data Mining', 'Image Algorithm' and 'Deep Learning' directly correspond to the skills of user1, which have the higher attention weights (i.e., 9.61, 9.54, 7.94, 7.19, 6.78). Other skills are also highly relevant to Vision Algorithm Engineer. Furthermore, the predicted click score (0.87) means that JobFormer is more likely to recommend JD1 to user1. 2) JobFormer can further efficiently identify JDs that mismatch with the current user. For example, the predicted click score in Case3 (0.38) is much lower than Case1 and Case2, because JD1 is intended for a computer vision position, while user2 is involved in the software development industry. Therefore, the obtained skill attention weights are relatively low.",
  "5.10 Computational Efficiency": "To evaluate the efficiency, we exhibit the computational times of JobFormer and compared deep models on JD recall task. Specifically, all of our experiments are conducted on a server with 2-core CPU@2.40GHz, 160GB RAM, and a NVIDIA Tesla V100 SXM2 GPU. As shown in Figure 6, we observe that the training time is 82 minutes, which is much shorter than the comparison BERT-based recommendation method MINER. Although the training time of our method is slightly higher than NRMS and NNR, our performance Fig. 6. The training and testing efficiency of JobFormer and compared deep models on JD recall task. MINER 0 20 40 60 80 100 140 120 0 10 20 30 40 50 70 Train Test 60 Methods Training Time (minutes) Testing Time (milliseconds) NRMS NNR JobFormer outperforms these two methods. Moreover, after the training process, the average cost of each instance in testing set is 28ms. It clearly validates that our model can be effectively used in the real-world management analysis system.",
  "6 CONCLUSION": "In this paper, we propose a skill-aware representation method (JobFormer) that can utilize job descriptions and user profiles (personal skill distribution) to accomplish personalized job recommendation. Our method contains a recall stage and a ranking stage. In the recall stage, we first leverage semantic-enhanced transformer to parse JDs and guide the representation learning of JD via personal skill distribution. In detail, we design an encoder with the local-global attention mechanism to mine the intra-job and inter-job dependencies from JD tuples. With the skill-aware JD representation, we can recall a portion of JDs relevant to the user as a candidate set from a large-scale JD pool according to the JD-user cosine similarity. In the ranking stage, candidate JDs are further computed with user profiles for CTR prediction and ranked for personalized JD recommendation. Experiments on real-world and public datasets can well demonstrate the effectiveness and interpretability of JobFormer.",
  "ACKNOWLEDGMENT": "National Key RD Program of China (2022YFF0712100), NSFC (61906092, 62006118, 62276131), Natural Science Foundation of Jiangsu Province of China under Grant (BK20200460), Jiangsu Shuangchuang (Mass Innovation and Entrepreneurship) Talent Program. Young Elite Scientists Sponsorship Program by CAST, CAAI-Huawei MindSpore Open Fund (CAAIXSJLJJ-2021-014B), the Fundamental Research Funds for the Central Universities (NO.NJ2022028, No.30922010317)",
  "REFERENCES": "[1] The Insight Partners. (2023) Online recruitment market. [Online]. Available: https://www.theinsightpartners. com/reports/online-recruitment-market 12 [2] S. Bian, W. X. Zhao, Y. Song, T. Zhang, and J.-R. Wen, 'Domain adaptation for person-job fit with transferable deep global match network,' in Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP) , 2019, pp. 4810-4820. [3] K. Yao, J. Zhang, C. Qin, P. Wang, H. Zhu, and H. Xiong, 'Knowledge enhanced person-job fit for talent recruitment,' in ICDE , Virtual, 2022, pp. 3467-3480. [4] S. Bian, X. Chen, W. X. Zhao, K. Zhou, Y. Hou, Y. Song, T. Zhang, and J.-R. Wen, 'Learning to match jobs with resumes from sparse interaction data using multi-view coteaching network,' in Proceedings of the 29th ACM International Conference on Information & Knowledge Management , 2020, pp. 65-74. [5] K. Yao, C. Qin, H. Zhu, C. Ma, J. Zhang, Y. Du, and H. Xiong, 'An interactive neural network approach to keyphrase extraction in talent recruitment,' in Proceedings of the 30th ACM International Conference on Information & Knowledge Management , 2021, pp. 2383-2393. [6] L. Wu, X. He, X. Wang, K. Zhang, and M. Wang, 'A survey on accuracy-oriented neural recommendation: From collaborative filtering to information-rich recommendation,' IEEE Transactions on Knowledge and Data Engineering , vol. 35, no. 5, pp. 4425-4445, 2022. [7] H.-T. Cheng, L. Koc, J. Harmsen, T. Shaked, T. Chandra, H. Aradhye, G. Anderson, G. Corrado, W. Chai, M. Ispir et al. , 'Wide & deep learning for recommender systems,' in Proceedings of the 1st workshop on deep learning for recommender systems , 2016, pp. 7-10. [8] H. Guo, R. Tang, Y. Ye, Z. Li, and X. He, 'Deepfm: a factorization-machine based neural network for ctr prediction,' arXiv preprint arXiv:1703.04247 , 2017. [9] X. Wang, X. He, F. Feng, L. Nie, and T.-S. Chua, 'Tem: Treeenhanced embedding model for explainable recommendation,' in Proceedings of the 2018 world wide web conference , 2018, pp. 1543-1552. [10] C. Qin, H. Zhu, T. Xu, C. Zhu, L. Jiang, E. Chen, and H. Xiong, 'Enhancing person-job fit for talent recruitment: Anability-aware neural network approach,' in SIGIR , Ann Arbor, MI, 2018, pp. 25-34. [11] C. Zhu, H. Zhu, H. Xiong, C. Ma, F. Xie, P. Ding, and P. Li, 'Person-job fit: Adapting the right talent for the right job with joint representation learning,' ACMTMIS. , vol. 9, pp. 12:1-12:17, 2018. [12] C. Wu, F. Wu, T. Qi, and Y. Huang, 'Two birds with one stone: Unified model learning for both recall and ranking in news recommendation,' arXiv preprint arXiv:2104.07404 , 2021. [13] T. Qi, F. Wu, C. Wu, Y. Huang, and X. Xie, 'Uni-fedrec: A unified privacy-preserving news recommendation framework for model training and online serving,' arXiv preprint arXiv:2109.05236 , 2021. [14] K. Kenthapadi, B. Le, and G. Venkataraman, 'Personalized job recommendation system at linkedin: Practical challenges and lessons learned,' in Proceedings of the eleventh ACM conference on recommender systems , 2017, pp. 346-347. [15] Y.-C. Chou and H.-Y. Yu, 'Based on the application of ai technology in resume analysis and job recommendation,' in 2020 IEEE International Conference on Computational Electromagnetics (ICCEM) . IEEE, 2020, pp. 291-296. [16] M. Diaby, E. Viennet, and T. Launay, 'Toward the next generation of recruitment tools: an online social networkbased job recommender system,' in Proceedings of the 2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining , 2013, pp. 821-828. [17] Y. Lu, S. El Helou, and D. Gillet, 'A recommender system for job seeking and recruiting website,' in Proceedings of the 22nd International Conference on World Wide Web , 2013, pp. 963-966. [18] K. V. Deshpande, S. Pan, and J. R. Foulds, 'Mitigating demographic bias in ai-based resume filtering,' in Adjunct publication of the 28th ACM conference on user modeling, adaptation and personalization , 2020, pp. 268-275. [19] X. Zhang, Y. Zhou, Y. Ma, B.-C. Chen, L. Zhang, and D. Agarwal, 'Glmix: Generalized linear mixed models for large-scale response prediction,' in Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining , 2016, pp. 363-372. [20] G. Lu, J. Gan, J. Yin, Z. Luo, B. Li, and X. Zhao, 'Multitask learning using a hybrid representation for text classification,' Neural Computing and Applications , vol. 32, pp. 6467-6480, 2020. [21] B. Tian, Y. Zhang, J. Wang, and C. Xing, 'Hierarchical interattention network for document classification with multitask learning.' in IJCAI , 2019, pp. 3569-3575. [22] Y. Xia, T. He, X. Tan, F. Tian, D. He, and T. Qin, 'Tied transformers: Neural machine translation with shared encoder and decoder,' in Proceedings of the AAAI conference on artificial intelligence , vol. 33, no. 01, 2019, pp. 5466-5473. [23] R. Weng, H. Yu, S. Huang, S. Cheng, and W. Luo, 'Acquiring knowledge from pre-trained model to neural machine translation,' in Proceedings of the AAAI conference on artificial intelligence , vol. 34, no. 05, 2020, pp. 9266-9273. [24] J. Berant, V. Srikumar, P.-C. Chen, A. Vander Linden, B. Harding, B. Huang, P. Clark, and C. D. Manning, 'Modeling biological processes for reading comprehension,' in Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) , 2014, pp. 1499-1510. [25] K. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman, and P. Blunsom, 'Teaching machines to read and comprehend,' Advances in neural information processing systems , vol. 28, 2015. [26] S. I. Wang and C. D. Manning, 'Baselines and bigrams: Simple, good sentiment and topic classification,' in Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) , 2012, pp. 90-94. [27] M. Lan, C. L. Tan, J. Su, and Y. Lu, 'Supervised and traditional term weighting methods for automatic text categorization,' IEEE transactions on pattern analysis and machine intelligence , vol. 31, no. 4, pp. 721-735, 2008. [28] C. Cherry and C. Quirk, 'Discriminative, syntactic language modeling through latent svms,' in Proceedings of the 8th Conference of the Association for Machine Translation in the Americas: Research Papers , 2008, pp. 65-74. [29] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, 'Gradientbased learning applied to document recognition,' Proceedings of the IEEE , vol. 86, no. 11, pp. 2278-2324, 1998. [30] J. L. Elman, 'Finding structure in time,' Cognitive science , vol. 14, no. 2, pp. 179-211, 1990. [31] D. Shen, H. Zhu, C. Zhu, T. Xu, C. Ma, and H. Xiong, 'A joint learning approach to intelligent job interview assessment.' in IJCAI , vol. 18, 2018, pp. 3542-3548. [32] R. Yan, R. Le, Y. Song, T. Zhang, X. Zhang, and D. Zhao, 'Interview choice reveals your preference on the market: To improve job-resume matching through profiling memories,' in Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining , 2019, pp. 914-922. [33] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, 'Attention is all you need,' in NeurIPS , Long Beach, CA, 2017, pp. 59986008. [34] X. Geng and R. Ji, 'Label distribution learning,' in ICDM , TX, USA, 2013, pp. 377-383. [35] N. Kalchbrenner, E. Grefenstette, and P. Blunsom, 'A convolutional neural network for modelling sentences,' in ACL , Baltimore, MD, 2014, pp. 655-665. [36] S. Ioffe and C. Szegedy, 'Batch normalization: Accelerat- 13 ing deep network training by reducing internal covariate shift,' in ICML , Lille, France, 2015, pp. 448-456. [37] V. Nair and G. E. Hinton, 'Rectified linear units improve restricted boltzmann machines,' in ICML , Haifa, Israel, 2010, pp. 807-814. [38] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, 'Dropout: a simple way to prevent neural networks from overfitting,' JMLR , vol. 15, pp. 1929-1958, 2014. [39] K. He, X. Zhang, S. Ren, and J. Sun, 'Deep residual learning for image recognition,' in CVPR , Las Vegas, NV, 2016, pp. 770-778. [40] L. J. Ba, J. R. Kiros, and G. E. Hinton, 'Layer normalization,' CoRR , vol. abs/1607.06450, 2016. [41] D. J. MacKay and D. J. Mac Kay, Information theory, inference and learning algorithms . Cambridge university press, 2003. [42] X. Jia, W. Li, J. Liu, and Y. Zhang, 'Label distribution learning by exploiting label correlations,' in AAAI , New Orleans, Louisiana, 2018, pp. 3310-3317. [43] Y. Yang, Z. Fu, D. Zhan, Z. Liu, and Y. Jiang, 'Semisupervised multi-modal multi-instance multi-label deep network with optimal transport,' IEEE TKDE , vol. 33, pp. 696-709, 2021. [44] S. Huang and Z. Zhou, 'Multi-label learning by exploiting label correlations locally,' in AAAI , vol. 26, Toronto, Canada, 2012, pp. 949-955. [45] F. Faghri, D. J. Fleet, J. R. Kiros, and S. Fidler, 'Vse++: Improving visual-semantic embeddings with hard negatives,' arXiv preprint arXiv:1707.05612 , 2017. [46] P. Matthews, AShort History of Structural Linguistics . Cambridge University Press, 2001. [47] X. Jia, Z. Li, X. Zheng, W. Li, and S. Huang, 'Label distribution learning with label correlations on local samples,' IEEE TKDE , vol. 33, pp. 1619-1631, 2021. [48] X. Glorot and Y. Bengio, 'Understanding the difficulty of training deep feedforward neural networks,' in AISTATS , vol. 9, Sardinia, Italy, 2010, pp. 249-256. [49] C. Wu, F. Wu, M. An, J. Huang, Y. Huang, and X. Xie, 'Npa: neural news recommendation with personalized attention,' in Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining , 2019, pp. 2576-2584. [50] --,'Neural news recommendation with attentive multiview learning,' arXiv preprint arXiv:1907.05576 , 2019. [51] C. Wu, F. Wu, S. Ge, T. Qi, Y. Huang, and X. Xie, 'Neural news recommendation with multi-head self-attention,' in Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP) , 2019, pp. 6389-6394. [52] Z. Mao, X. Zeng, and K.-F. Wong, 'Neural news recommendation with collaborative news encoding and structural user encoding,' arXiv preprint arXiv:2109.00750 , 2021. [53] Q. Zhang, J. Li, Q. Jia, C. Wang, J. Zhu, Z. Wang, and X. He, 'Unbert: User-news matching bert for news recommendation.' in IJCAI , 2021, pp. 3356-3362. [54] J. Li, J. Zhu, Q. Bi, G. Cai, L. Shang, Z. Dong, X. Jiang, and Q. Liu, 'Miner: multi-interest matching network for news recommendation,' in Findings of the Association for Computational Linguistics: ACL 2022 , 2022, pp. 343-352. [55] Z. Wang, Q. She, and J. Zhang, 'Masknet: Introducing feature-wise multiplication to ctr ranking models by instance-guided mask,' arXiv preprint arXiv:2102.07619 , 2021. [56] F. Wang, Y. Wang, D. Li, H. Gu, T. Lu, P. Zhang, and N. Gu, 'Enhancing ctr prediction with context-aware feature representation learning,' in Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval , 2022, pp. 343-352. [57] W. Krichene and S. Rendle, 'On sampled metrics for item recommendation (extended abstract),' in IJCAI , Virtual Event, 2021, pp. 4784-4788. [58] L. Yin, X. Wei, Y. Sun, J. Wang, and M. J. Rosato, 'A 3d facial expression database for facial behavior research,' in FGR , Southampton, UK, 2006, pp. 211-216. Zhihao Guan is working towards the Ph.D. degree with the School of Computer Science and Engineering, in Nanjing University of Science and Technology, China. His research interests lie primarily in deep learning and data mining, including cross-modal learning. Jia-Qi Yang earned his M.E. degree in 2021 and is currently pursuing a Ph.D. at the State Key Laboratory for Novel Software Technology, Nanjing University, China. His research are primarily focused on machine learning and data mining, with specific expertise in uncertainty calibration, recommendation systems, and AI for science. He serves as a Program Committee member and Reviewer for prestigious conferences like AAAI, NeurIPS, ICLR, and others. Yang Yang received the Ph.D. degree in computer science, Nanjing University, China in 2019. At the same year, he became a faculty member at Nanjing University of Science and Technology, China. He is currently a Professor with the school of Computer Science and Engineering. His research interests lie primarily in machine learning and data mining, including heterogeneous learning, model reuse, and incremental mining. He serves as PC in leading conferences such as IJCAI, AAAI, ICML, NIPS, etc. Hengshu Zhu (SM'19) is currently a principal data scientist & architect at Baidu Inc. He received the Ph.D. degree in 2014 and B.E. degree in 2009, both in Computer Science from University of Science and Technology of China (USTC), China. His general area of research is data mining and machine learning, with a focus on developing advanced data analysis techniques for innovative business applications. He has published prolifically in refereed journals and conference proceedings, including IEEE Trans- actions on Knowledge and Data Engineering (TKDE), IEEE Transactions on Mobile Computing (TMC), ACM Transactions on Information Systems (ACM TOIS), ACM Transactions on Knowledge Discovery from Data (TKDD), ACM SIGKDD, ACM SIGIR, WWW, IJCAI, and AAAI. He has served regularly on the organization and program committees of numerous conferences, including as a program co-chair of the KDD Cup-2019 Regular ML Track, and a founding co-chair of the first International Workshop on Organizational Behavior and Talent Analytics (OBTA) and the International Workshop on Talent and Management Computing (TMC), in conjunction with ACM SIGKDD. He was the recipient of the Distinguished Dissertation Award of CAS (2016), the Distinguished Dissertation Award of CAAI (2016), the Special Prize of President Scholarship for Postgraduate Students of CAS (2014), the Best Student Paper Award of KSEM-2011, WAIM-2013, CCDM-2014, and the Best Paper Nomination of ICDM-2014. He is the senior member of IEEE, ACM, and CCF. Wenjie Li received the PhD degree in systems engineering and engineering management from the Chinese University of Hong Kong, Hong Kong, in 1997. She is currently an associate professor with the Department of Computing, The Hong Kong Polytechnic University. Her main research interests include online social network analysis, natural language processing, and document summarization. 14 Hui Xiong (Fellow'20) is a Professor at the Artificial Intelligence Thrust, The Hong Kong University of Science and Technology. Xiong's research interests include data mining, mobile computing, and their applications in business. Xiong received his PhD in Computer Science from University of Minnesota, USA. He has served regularly on the organization and program committees of numerous conferences, including as a Program Co-Chair of the Industrial and Government Track for the 18th ACM SIGKDD Interna- tional Conference on Knowledge Discovery and Data Mining (KDD), a Program Co-Chair for the IEEE 2013 International Conference on Data Mining (ICDM), a General Co-Chair for the 2015 IEEE International Conference on Data Mining (ICDM), and a Program Co-Chair of the Research Track for the 2018 ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. He received the 2021 AAAI Best Paper Award and the 2011 IEEE ICDM Best Research Paper award. For his outstanding contributions to data mining and mobile computing, he was elected an AAAS Fellow and an IEEE Fellow in 2020.",
  "keywords_parsed": [
    "None"
  ]
}