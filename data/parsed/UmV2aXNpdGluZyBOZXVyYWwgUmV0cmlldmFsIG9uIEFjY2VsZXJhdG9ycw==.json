{"Revisiting Neural Retrieval on Accelerators": "Jiaqi Zhai jiaqiz@meta.com Meta Platforms, Inc. Menlo Park, CA, USA Zhaojie Gong zhaojieg@meta.com Meta Platforms, Inc. Menlo Park, CA, USA Yueming Wang yuemingw@meta.com Meta Platforms, Inc. Menlo Park, CA, USA Xiao Sun sunx@meta.com Meta Platforms, Inc. Menlo Park, CA, USA Zheng Yan zyan@meta.com Meta Platforms, Inc. Menlo Park, CA, USA Fu Li leaf123@meta.com Meta Platforms, Inc. Menlo Park, CA, USA", "ABSTRACT": "Xing Liu xingl@meta.com Meta Platforms, Inc. Menlo Park, CA, USA", "1 INTRODUCTION": "Retrieval finds a small number of relevant candidates from a large corpus for information retrieval and recommendation applications. A key component of retrieval is to model (user, item) similarity, which is commonly represented as the dot product of two learned embeddings. This formulation permits efficient inference, commonly known as Maximum Inner Product Search (MIPS). Despite its popularity, dot products cannot capture complex user-item interactions, which are multifaceted and likely high rank. We hence examine non-dot-product retrieval settings on accelerators, and propose mixture of logits (MoL), which models (user, item) similarity as an adaptive composition of elementary similarity functions. This new formulation is expressive, capable of modeling high rank (user, item) interactions, and further generalizes to the long tail. When combined with a hierarchical retrieval strategy, h-indexer , we are able to scale up MoL to 100M corpus on a single GPU with latency comparable to MIPS baselines. On public datasets, our approach leads to uplifts of up to 77.3% in hit rate (HR). Experiments on a large recommendation surface at Meta showed strong metric gains and reduced popularity bias, validating the proposed approach's performance and improved generalization.", "CCS CONCEPTS": "\u00b7 Information systems \u2192 Learning to rank ; Recommender systems ; \u00b7 Computing methodologies \u2192 Machine learning algorithms .", "KEYWORDS": "information retrieval, recommender systems, candidate generation, non-MIPS retrieval, hierarchical retrieval, nearest neighbor search", "ACMReference Format:": "Jiaqi Zhai, Zhaojie Gong, Yueming Wang, Xiao Sun, Zheng Yan, Fu Li, and Xing Liu. 2023. Revisiting Neural Retrieval on Accelerators. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '23), August 6-10, 2023, Long Beach, CA, USA. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3580305.3599897 Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). KDD '23, August 6-10, 2023, Long Beach, CA, USA \u00a9 2023 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-0103-0/23/08. https://doi.org/10.1145/3580305.3599897 Retrieval, also known as candidate generation, is the process of selecting a small number, typically 10-1000, of relevant candidates from a large corpus consisting of millions to billions of items. Retrieval plays a crucial role in large scale information retrieval and recommendation systems as the first stage of the funnel. Retrieval must capture (user, item) similarity well by being highly contextualized and personalized; for instance, an individual may want to read important economic news on weekday mornings, but watch lighthearted comedies in the evenings. In the deep learning era, (user, item) similarity in retrieval is commonly formulated as the dot product of two embeddings, each parameterized by a deep neural network. This setting is widely adopted throughout industry (e.g., Google [3, 45], Meta [13], Alibaba [26, 48], etc.). The dot product setting empirically works well for millions of items [3], and is computationally efficient; for instance, the inference problem in this setup can be formulated as Maximum Inner Product Search (MIPS). MIPS has been extensively studied, and permits various optimizations such as locality sensitive hashing and product quantization [10, 33]. The relationship between users and items in real world, however, demonstrates a significant level of complexity, and may not be approximated well by dot products. There are two evidences. First, dot product-based models produce low-rank recommendations. Consider the log probability of showing item \ud835\udc65 to user \ud835\udc62 in the inner product setting, ln \ud835\udc5d ( \ud835\udc65 | \ud835\udc62 ) = \u27e8 \ud835\udc53 \ud835\udf03 ( \ud835\udc62 ) , \ud835\udc54 \ud835\udf03 ( \ud835\udc65 )\u27e9 -Z \ud835\udc62 , where \ud835\udc53 \ud835\udf03 ( \ud835\udc62 ) , \ud835\udc54 \ud835\udf03 ( \ud835\udc65 ) \u2208 R \ud835\udc51 . The ln \ud835\udc5d ( \ud835\udc65 | \ud835\udc62 ) matrix hence has a rank of at most \ud835\udc51 + 1. Commonly used \ud835\udc51 values are up to 100s-200s [3, 18, 25], which is far less than the rank of actual datasets. As shown in Table 1, even for small benchmark datasets, dot-product based 256-d models can only explain up to 7.11%-41.26% of the variance. Second, highly expressive neural networks (e.g., high order feature interactions [1, 9, 41, 43]; sequential encoders such as RNNs and Table 1: Rank analysis of ln \ud835\udc5d ( \ud835\udc65 | \ud835\udc62 ) for various public datasets. KDD '23, August 6-10, 2023, Long Beach, CA, USA Jiaqi Zhai, Zhaojie Gong, Yueming Wang, Xiao Sun, Zheng Yan, Fu Li, and Xing Liu Figure 1: Overview of the proposed architecture. Green boxes indicate tensors that are cachable for inference. (..., k u , D): MoL embeddings (..., k x , D): MoL embeddings User-side raw features/embeddings (..., D'): User-side embeddings (..., D'): Item-side embeddings Adaptive Embedding Compression Adaptive Embedding Compression (..., k u \u22c6 k x ): MoL logits Item-side raw features/embeddings User-side Item-side (..., k u \u22c6 k x ): Cross (..., k u \u22c6 k x ): MoL gating weights Main logits MoL gating components H-indexer logits Hierarchical retrieval (e.g., h-indexer): |X| -> top k' Intermediate items k': ~10^5 scale Full corpora X |X|: up to 100M Top k items k: ~100 Main similarity function (e.g., mixture-of-logits): k' -> top k (a) Overall architecture.                               (b) Main similarity function, mixture-of-logits (MoL).                                          (c) h-indexer. Item-side MLP User-side MLP Attention [36, 38, 49]) have been the go-to choice for ranking models. A more expressive similarity function, therefore, would better optimize metrics such as HR@10 for the head parts of the corpus. Despite the above, leveraging expressive similarity functions in retrieval has remained an open ended problem due to (a) the difficulty of training models that generalize, (b) their high inference costs. It's not sufficient to port an arbitrary neural architecture to retrieval; Rendel et al. [32] have recently shown that with properly tuned hyper parameters, a learned dot product can substantially outperform multilayer perceptron (MLP)-based learned similarity functions. It's also hard for MLPs to approximate dot products despite MLPs being universal function approximators [12, 32]. Computational cost is another major blocker; modern information retrieval applications may need to handle corpus with up to billions of items [6, 39]. We lack efficient alternatives to MIPS at this scale. One line of work [50-52] proposes to learn explicit tree structures for the items and reduce top\ud835\udc58 similarity to beam search at inference time, which enabled the first successful non-MIPS setup in production. The applications of these approaches have been limited: first, it is generally harder to learn explicit structures - for instance, prior knowledge are often needed to properly initialize the tree [50, 51], but the tree then needs to change dynamically as the corpus evolves; second, when binary tree structures are used [50, 51], inference latency can be high as beam search requires 22 non-parallelizable steps just to traverse a corpus with 4M items. To address the above problems, we propose modeling techniques and infra optimizations that enable complex interactions of (user, item) pairs to be modeled at the retrieval stage while supporting efficient inference. We consider the accelerator setting in particular, given that the enablement and proliferation of accelerators like GPUs and TPUs [17] can significantly mitigate the training and serving cost of complex similarity functions. Our first contribution is to propose a specific class of high rank similarity functions, mixture of logits (MoL), in Section 3. MoL is designed with efficient training and inference on accelerators in mind. It not only significantly outperforms dot-product based baselines in hit rate, but also better generalizes to torso and long tail where there are limited training examples by utilizing gating designs and component-level hypersphere embeddings. Our second contribution is to enable efficient training and inference with expressive similarity functions (e.g., MoL), on modern accelerators (Section 4). We propose a hierarchical retrieval strategy, and in particular, an accelerator-friendly algorithm for very large corpus called h-indexer . Combined with other optimizations like quantization and kernel fusions, h-indexer enables us to scale up retrieval to hundreds of millions of items with latency comparable to MIPS setups. It's worth remarking that, as highlighted in Figure 2, the serving cost, e.g., latency and throughput - on GPUs in this work - does not necessarily increase with compute thanks to the significantly higher GPU utilization of our model. Our hierarchical design, unlike dot-product, can also harness more compute with constant GPU memory budget, which is critical given limited memory capacity in production. Our final contribution is to conduct extensive experiments on public and industrial datasets (Section 5) with MoL and h-indexer. Our proposed method achieves up to 77 . 3% gain in HR on standard benchmark datasets like MovieLens and Amazon Reviews, and up to +27.6% gain at HR@50 over strong baselines on one of the largest recommendation system deployments at Meta. The strong performance of MoL validates our hypothesis that expressive similarity functions are needed to capture dynamics of real-world datasets at retrieval stage. The proposed method also reduces Matthew effect, showing that MoL indeed generalizes well over the entire corpus. We finally compare the proposed approach with related works in Section 6 and conclude in Section 7.", "2 OVERVIEW": "", "2.1 The Retrieval Problem Setting": "In a standard retrieval setup, we are given a user \ud835\udc62 (or more generally, context \ud835\udc50 ) and would like to predict the item \ud835\udc65 's that is most positively associated with the given user, where \ud835\udc65 's are selected out of a corpora X . The user \ud835\udc62 can be characterized by various features, like categorical features (e.g., user id, demographics, accounts that the user follows, etc.), float features (e.g., various counters), etc. Regardless of the features used, we aim to learn a function \ud835\udf19 parameterized by \ud835\udf03 that encodes similarity between user \ud835\udc62 and item \ud835\udc65 as \ud835\udf19 \ud835\udf03 ( \ud835\udc62, \ud835\udc65 ) . For a given \ud835\udf19 , the goal is to learn \ud835\udf03 such that we order Revisiting Neural Retrieval on Accelerators KDD '23, August 6-10, 2023, Long Beach, CA, USA \ud835\udf19 \ud835\udf03 ( \ud835\udc62, \ud835\udc65 ) 's for \ud835\udc65 's that are positively associated with the user ahead of other \ud835\udf19 \ud835\udf03 ( \ud835\udc62, \ud835\udc65 \u2032 ) 's for \ud835\udc65 \u2032 \u2208 X . It's useful to formulate the similarity between user \ud835\udc62 and item \ud835\udc65 as a probability distribution \ud835\udc5d ( \ud835\udc65 | \ud835\udc62 ) [3, 48]. We do so by considering \ud835\udf19 \ud835\udf03 ( \ud835\udc65,\ud835\udc62 ) as unnormalized logits and pass them through softmax:  Specific to information retrieval and recommendation setting, the size of X can be very large, potentially in the range of millions to billions for practical problem settings. Dot products (two tower, dual encoder setups, etc.) are hence commonly used. In this setup, we learn user and item representations as two \ud835\udc51 -dimensional embeddings, \ud835\udc53 \ud835\udf03 ( \ud835\udc62 ) \u2208 R \ud835\udc51 and \ud835\udc54 \ud835\udf03 ( \ud835\udc65 ) \u2208 R \ud835\udc51 . We recommend item \ud835\udc65 to user \ud835\udc62 with probability proportional to their inner products, \ud835\udf19 \ud835\udf03 ( \ud835\udc65,\ud835\udc62 ) \u223c \u27e8 \ud835\udc53 \ud835\udf03 ( \ud835\udc62 ) , \ud835\udc54 \ud835\udf03 ( \ud835\udc65 )\u27e9 ; recall that we obtain normalized probability distribution with softmax, hence  where \ud835\udc4d \ud835\udc62 is the partition function.", "2.2 Architecture": "Figure 1 shows our main architecture used in the rest of the paper. We highlight the main components of this architecture below. Overall flow . Our design decomposes retrieval into multiple stages. These stages run in a cascading fashion to produce the final top \ud835\udc58 candidates. One example is illustrated in Figure 1(a), where an accelerator friendly algorithm, h-indexer , is used to find a large number of candidates ( \ud835\udc58 \u2032 = 10 5 ) out of a 100M corpus, and then a complex similarity function (e.g., mixture-of-logits ) is used to find the final top \ud835\udc58 (e.g., 100) candidates. We show in Section 5.2.1 that this design significantly improves throughput without degrading recall for suitably chosen values of \ud835\udc58 \u2032 . Main similarity function . The hierarchical retrieval design enables complex neural networks to be used when modeling (user, item) similarities. In Section 3, we discuss one such instance of similarity functions, mixture of logits , that significantly outperforms dot products. When designing this architecture, we aggressively make intermedidate tensors available for caching (green boxes in Fig. 1). For instance, we can cache item-side gating weights and combine them cheaply with non-cachable weights at inference time. Figure 2: Infra efficiency in production: GPU utilization and peak memory scaling with serving FLOPs. 200 GPU util (this work) memory (this work) GPU util (MIPS) memory (MIPS) 80 @ 150 60 100 1 Baseline 40 50 8 8 This work 20 0.5 1.0 1.5 2.0 2.5 Serving Cost (TFLOPs) h-indexer . We found that a simple, but highly optimized dot product combined with specialized top\ud835\udc58 algorithm works well for up to 100M corpus. This stage is co-trained with the main similarity function. We discuss this design in details in Section 4.1.", "3 MIXTURE OF LOGITS: AN ACCELERATOR-AWARE MODEL DESIGN": "In this section, we propose a new high rank similarity function, mixture of logits (MoL). MoL is designed for neural retrieval settings on accelerators. The basic idea in MoL is to parameterize \ud835\udf19 \ud835\udf03 ( \ud835\udc62, \ud835\udc65 ) as an adaptive mixture of more elementary logits,  where \ud835\udf0b \ud835\udc58,\ud835\udf03 ( \ud835\udc65,\ud835\udc62 ) are adaptive gating weights and \ud835\udeff \ud835\udc58,\ud835\udf03 ( \ud835\udc65,\ud835\udc62 ) are elementary logits. MoL achieves high rank in two ways. First, since \ud835\udeff \ud835\udc58,\ud835\udf03 ( \ud835\udc65,\ud835\udc62 ) in Equation 3 can be parameterized by any form of neural networks, \ud835\udf19 \ud835\udf03 ( \ud835\udc65,\ud835\udc62 ) has arbitrarily high rank. Second, as \ud835\udf0b \ud835\udc58,\ud835\udf03 ( \ud835\udc65,\ud835\udc62 ) takes \ud835\udc65 and \ud835\udc62 as input, we can create a high rank matrix ln \ud835\udc5d ( \ud835\udc65 | \ud835\udc62 ) by combining low rank similarity functions, e.g., dot products, in a cost-efficient fashion. For instance, using \ud835\udc58 groups of dot products (of potentially different embedding dimensions), we can derive  Although the elementary logits \u27e8 \ud835\udc53 \ud835\udc58,\ud835\udf03 ( \ud835\udc62 ) , \ud835\udc54 \ud835\udc58,\ud835\udf03 ( \ud835\udc65 )\u27e9 themselves have ranks of at most \ud835\udc51 (inner products of \ud835\udc51 -dimensional embeddings), Equation 4 can define an arbitrarily high rank probability distribution, as long as \ud835\udf0b \ud835\udc58,\ud835\udf03 ( \ud835\udc65,\ud835\udc62 ) depends on both item \ud835\udc65 and user \ud835\udc62 . Otherwise, suppose \ud835\udf0b \ud835\udc58,\ud835\udf03 ( \ud835\udc65,\ud835\udc62 ) only depends on \ud835\udc62 , we have  which defines \ud835\udf19 \ud835\udc40\ud835\udc5c\ud835\udc3f ( \ud835\udc65,\ud835\udc62 ) of rank at most \ud835\udc58\ud835\udc51 when all embeddings are of dimensionality \ud835\udc51 . Similar argument holds on the item side. Empirically this formulation does enable us to achieve high rank, as shown in Section 5.1.2.", "3.1 Embedding Sharing and Adaptive Embedding Compression": "Implementing Equation 4 directly requires \ud835\udc58 matrix multiplication ( mm ) / batched matrix multiplication ( bmm ) operations. We can accelerate training and inference as follows: (a) make embedding dimensions used across different mixture components the same, and (b) reuse embeddings across these groups and enable \ud835\udc58 to be different on the user and on the item side. Specifically, we consider \ud835\udc58 \ud835\udc62 embeddings of dimensionality \ud835\udc51 on the user side and \ud835\udc58 \ud835\udc65 embeddings of the same dimension on the item side. We can then take dot products between all \ud835\udc58 \ud835\udc62 -by\ud835\udc58 \ud835\udc65 pairs of embeddings:  KDD '23, August 6-10, 2023, Long Beach, CA, USA Jiaqi Zhai, Zhaojie Gong, Yueming Wang, Xiao Sun, Zheng Yan, Fu Li, and Xing Liu These logits in Equation 6 can be computed with one \ud835\udc4f\ud835\udc5a\ud835\udc5a for inference, and one \ud835\udc5a\ud835\udc5a for training as will be shown in Section 3.2. 3.1.1 Adaptive Logits Selection. The \ud835\udc58 \ud835\udc62 user-side and \ud835\udc58 \ud835\udc65 item-side embeddings in Equation 6 can be interpreted as (transformations) of user-side and item-side features (e.g., [1, 9]). The drawback is that there could be hundreds to thousands of such features in production settings, which makes computing \ud835\udc58 \ud835\udc62 \u00d7 \ud835\udc58 \ud835\udc65 logits prohibitively expensive. We control computational cost as follows: \u00b7 user-side / context-side . We reduce computational cost by compressing the feature embeddings adaptively ('Adaptive Embedding Compression' in Figure 1(b)). We transform the \ud835\udc58 \u2032 \ud835\udc62 embeddings \ud835\udc63 \u2032 \ud835\udc56 's into \ud835\udc58 \ud835\udc62 embeddings \ud835\udc63 \ud835\udc56 's of the same dimensionality:  \u00b7 item-side . Besides computational challenge, accelerator memory limits how many embeddings we can store. On a 16GB accelerator and with a 10M corpora where \ud835\udc51 = 128, we can only cache 6 item-side embeddings with FP16. Besides using Equation 7 for compression, we can generate some item-side embeddings on the fly with learned MLPs, which trades off compute for memory.", "3.2 Shared Negatives and Gating Function Decomposition": "So far we have omitted discussions about how \ud835\udf0b \ud835\udc58,\ud835\udf03 ( \ud835\udc65,\ud835\udc62 ) is defined. Modernneural retrieval models are commonly trained with sampled softmax losses [3, 26, 36, 45], and utilizing more negatives generally improves model quality [45]. Even if we share the negatives for samples in a mini-batch, the large number of negatives can still make popular gating/attention architectures which are originally designed for ranking models prohibitively expensive in terms of computation and memory. For example, in AttentionFM [43], the attention weights for logits are computed as \ud835\udefc \ud835\udc62\ud835\udc65 = MLP ( \ud835\udc63 \ud835\udc62 \u2299 \ud835\udc63 \ud835\udc65 ) where \u2299 is element-wise dot product. With the extra item dimension in large batch negative training, the computational cost for attention weights becomes \ud835\udc42 ( \ud835\udc35\ud835\udc4b ( \ud835\udc58 \ud835\udc62 \ud835\udc58 \ud835\udc65 ) 2 \ud835\udc37 \ud835\udc53 ) for one linear layer, where \ud835\udc35 is mini batch size, \ud835\udc4b is number of negatives, and \ud835\udc37 \ud835\udc53 is the feature embedding dimension. This requires 35184 . 4 GFLOPs and 274.9GB HBM for configurations discussed in Table 2 ( \ud835\udc37 \ud835\udc53 = 256). Therefore, in addition to sharing negatives across users in a mini-batch as done in standard negative sampling [3, 40, 45] and removing element-wise dot products, we further propose to rewrite the gating function with decomposition to reduce the computational cost of materializing and performing matrix multiplications over large ( \ud835\udc35, \ud835\udc4b, \ud835\udc37 ) tensors. This is done by parameterizing \ud835\udf0b \ud835\udf03 ( \ud835\udc65,\ud835\udc62 ) as  where \ud835\udf0b \ud835\udc4b \ud835\udf03 ( \ud835\udc65 ) is a DNN that takes item-side feature of dimensionality \ud835\udc37 \ud835\udc4b as input, \ud835\udf0b \ud835\udc48 \ud835\udf03 ( \ud835\udc62 ) is a DNN that takes user-side feature of dimensionality \ud835\udc37 \ud835\udc48 as input, and \ud835\udf0b \ud835\udc4b\ud835\udc48 \ud835\udf03 ( \ud835\udc65 | \ud835\udc62 ) takes cross features of dimensionality \ud835\udc37 \ud835\udc4b\ud835\udc48 (e.g. \ud835\udc58 \ud835\udc65 \u00b7 \ud835\udc58 \ud835\udc62 ) as input, and \ud835\udf0e \ud835\udf03 (\u00b7 , \u00b7 , \u00b7) is some nonlinearity (e.g., softmax after linear combination of inputs). Assuming we use two-layer MLPs with \ud835\udc3e (or \ud835\udc3e \ud835\udc48 , \ud835\udc3e \ud835\udc4b , \ud835\udc3e \ud835\udc4b\ud835\udc48 ) as hidden dim and \ud835\udc3f as output dim, the original computational cost of \ud835\udf0b \ud835\udf03 ( \ud835\udc65,\ud835\udc62 ) is \ud835\udc42 ( \ud835\udc35\ud835\udc4b\ud835\udc3e ( \ud835\udc37 + \ud835\udc3f )) , which is reduced to \ud835\udc42 ( \ud835\udc35\ud835\udc3e ( \ud835\udc37 \ud835\udc48 + \ud835\udc3f ) + \ud835\udc4b\ud835\udc3e ( \ud835\udc37 \ud835\udc4b + Table 2: Effect of \ud835\udf0b \ud835\udf03 ( \ud835\udc65,\ud835\udc62 ) decomposition. \ud835\udc35 = 2048 , \ud835\udc4b = 4096 , \ud835\udc37 = 1024 ( \ud835\udc37 \ud835\udc48 = 768 , \ud835\udc37 \ud835\udc4b = 128 , \ud835\udc37 \ud835\udc4b\ud835\udc48 = 128 ), \ud835\udc3e = 256 , \ud835\udc3f = 128 . \ud835\udc3f ) + \ud835\udc35\ud835\udc4b\ud835\udc3e ( \ud835\udc37 \ud835\udc4b\ud835\udc48 + \ud835\udc3f )) with gating function decomposition. With decomposition of user, item, user-item tensors, we are able to reduce the computational cost of the most expensive part (with \ud835\udc37 \ud835\udc4b\ud835\udc48 \u226a \ud835\udc37 ) while maintaining model quality. We show the benefit of this approach for practical settings in Table 2. Finally, we regularize \ud835\udf0b \ud835\udf03 ( \ud835\udc65 | \ud835\udc62 ) by forcing it to take the form of a probability distribution per (user, item) pair, i.e., \u02dd \ud835\udc58 \ud835\udf0b \ud835\udc58,\ud835\udf03 ( \ud835\udc65 | \ud835\udc62 ) = 1. We found that applying dropout on top of this distribution to be helpful, which encourages more balanced mixture component utilization similar to Mixture-of-Experts [14, 16].", "3.3 Component-level Hypersphere Embeddings": "The large number of component-level embeddings used in Equation 4 and 6 could introduce two problems, preventing generalization of the model to the torso and long tail in retrieval setting: \u00b7 The gating function \ud835\udf0b \ud835\udf03 ( \ud835\udc65,\ud835\udc62 ) can make component-level embeddings undertrained (esp. in online training settings), which in turn introduces training instability over time. \u00b7 They may exacerbate overfitting or require significantly more negatives (more compute) to properly fit the model. We propose L2 normalizing the component embeddings as a simple and effective regularization to fix the problem. The intuition is as follows. L2 normalization forces all MoL component embeddings in Equation 6 to lay on the surface of a \ud835\udc51 -dimensional hypersphere. Denote the volume of a \ud835\udc51 -dimensional hypersphere by \ud835\udc49 ( \ud835\udc51 ) . We have \ud835\udc49 ( \ud835\udc51 ) \u2265 1 \u221a \ud835\udc51 -1 \ud835\udc49 ( \ud835\udc51 -1 ) ; as \ud835\udc51 grows to infinity, the majority of the volume of a unit hypersphere lies near its 'equator' which is in turn a ( \ud835\udc51 -1 ) dimensional hypersphere. Further, the majority of a unit hypersphere's volume lies near its surface. This property encourages component-level hypersphere embeddings, even if undertrained, to be near-orthogonal to each other: lim \ud835\udc51 \u2192 inf \u27e8 \ud835\udc53 \ud835\udc58 \ud835\udc62 ( \ud835\udc62 ) , \ud835\udc54 \ud835\udc58 \ud835\udc65 ( \ud835\udc65 )\u27e9/|| \ud835\udc53 \ud835\udc58 \ud835\udc62 ( \ud835\udc62 )|| 2 /|| \ud835\udc54 \ud835\udc58 \ud835\udc65 ( \ud835\udc65 )|| 2 = 0. In contrast, without normalization, assume the dimensions of each embedding are i.i.d. in [-1, 1], the limit of the above equation goes to infinity. Formally, the modified MoL setup can be found in Equation 9, where the term \ud835\udf0f ( \ud835\udf0f > 1) rescales post softmax output range back to ( 0 , 1 ) , similar to temperature in L2-normalized softmax.  Besides solving the problem of undertrained component-level embeddings, the other nice property of Equation 9 is that we force MoL logit components to measure angular distances instead of both vector magnitudes and angular distances, which helps avoiding degrading the model to only learn about popular items given impressions on items generally follow a power-law distribution. We present ablation studies in Section 5.1.3. Revisiting Neural Retrieval on Accelerators KDD '23, August 6-10, 2023, Long Beach, CA, USA", "3.4 Final MoL Algorithm": "Pseudocode for the final mixture-of-logits model using dot products as mixture components can be found in Algorithm 1, which is also illustrated in Figure 1(b). In practice, we use simple 2-layer MLPs for \ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5f\ud835\udc66\ud835\udc4a\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61\ud835\udc39\ud835\udc5b , \ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc5a\ud835\udc4a\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61\ud835\udc39\ud835\udc5b , and \ud835\udc50\ud835\udc5f\ud835\udc5c\ud835\udc60\ud835\udc60\ud835\udc4a\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61\ud835\udc39\ud835\udc5b and set their output size to be \ud835\udc58 \ud835\udc62 \u00b7 \ud835\udc58 \ud835\udc65 . We use a simple \ud835\udc50\ud835\udc5c\ud835\udc5a\ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc39\ud835\udc5b , \ud835\udc53 ( \ud835\udc62\ud835\udc64,\ud835\udc65\ud835\udc64,\ud835\udc50\ud835\udc64 ) = \ud835\udc62\ud835\udc64 \u00b7 \ud835\udc65\ud835\udc64 + \ud835\udc50\ud835\udc64 followed by SiLU nonlinearity [7].", "Algorithm 1 Mixture of Logits (MoL) Algorithm.": "1: procedure MoL( \ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4e,\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5f\ud835\udc66,\ud835\udc58,\ud835\udc58 \u2032 , \ud835\udc58 \ud835\udc62 , \ud835\udc58 \ud835\udc65 , \ud835\udc51, \ud835\udf0f ) 2: \ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc5f\ud835\udc38\ud835\udc5a\ud835\udc4f\ud835\udc60 \u2190 l2Norm ( \ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc5f\ud835\udc38\ud835\udc5a\ud835\udc4f\ud835\udc43\ud835\udc5f\ud835\udc5c\ud835\udc57 ( \ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5f\ud835\udc66 )) 3: \ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc5a\ud835\udc38\ud835\udc5a\ud835\udc4f\ud835\udc60 \u2190[] \u22b2 Of shape ( \ud835\udc58 \ud835\udc65 , \ud835\udc51 ) \u00d7 \ud835\udc58 \u2032 4: for \ud835\udc65 \u2208 \ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4e do \u22b2 Cachable 5: \ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc5a\ud835\udc38\ud835\udc5a\ud835\udc4f\ud835\udc60.\ud835\udc4e\ud835\udc5d\ud835\udc5d\ud835\udc52\ud835\udc5b\ud835\udc51 ( l2Norm ( \ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc5a\ud835\udc38\ud835\udc5a\ud835\udc4f\ud835\udc43\ud835\udc5f\ud835\udc5c\ud835\udc57 ( \ud835\udc65 ))) 6: end for 7: \ud835\udc50\ud835\udc59 \u2190 \ud835\udc5a\ud835\udc5a ( \ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc5f\ud835\udc38\ud835\udc5a\ud835\udc4f\ud835\udc60,\ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc5a\ud835\udc38\ud835\udc5a\ud835\udc4f\ud835\udc60.\ud835\udc5f\ud835\udc52\ud835\udc60\u210e\ud835\udc4e\ud835\udc5d\ud835\udc52 (- 1 , \ud835\udc51 ) .\ud835\udc61 ())/ \ud835\udf0f 8: \ud835\udc50\ud835\udc59 \u2190 \ud835\udc50\ud835\udc59.\ud835\udc63\ud835\udc56\ud835\udc52\ud835\udc64 ( \ud835\udc58 \ud835\udc62 , \ud835\udc58 \u2032 , \ud835\udc58 \ud835\udc65 ) .\ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc5a\ud835\udc62\ud835\udc61\ud835\udc52 ( 1 , 0 , 2 ) .\ud835\udc5f\ud835\udc52\ud835\udc60\u210e\ud835\udc4e\ud835\udc5d\ud835\udc52 ( \ud835\udc58 \u2032 , - 1 ) 9: \ud835\udc54\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5b\ud835\udc54\ud835\udc4a\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61\ud835\udc60 \u2190 decomposedGating ( \ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4e,\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5f\ud835\udc66,\ud835\udc50\ud835\udc59 ) 10: \ud835\udc60\ud835\udc56\ud835\udc5a\ud835\udc56\ud835\udc59\ud835\udc4e\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc56\ud835\udc52\ud835\udc60 \u2190( \ud835\udc54\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5b\ud835\udc54\ud835\udc4a\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61\ud835\udc60 \u00b7 \ud835\udc50\ud835\udc59 ) .\ud835\udc60\ud835\udc62\ud835\udc5a (- 1 ) 11: return \ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc60\ud835\udc5c\ud835\udc5f\ud835\udc61 ( \ud835\udc60\ud835\udc56\ud835\udc5a\ud835\udc56\ud835\udc59\ud835\udc4e\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc56\ud835\udc52\ud835\udc60 ) [ : \ud835\udc58 ] 12: end procedure 13: procedure decomposedGating( \ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4e,\ud835\udc62, \ud835\udc4e\ud835\udc59\ud835\udc59\ud835\udc3f\ud835\udc5c\ud835\udc54\ud835\udc56\ud835\udc61\ud835\udc60 ) 14: \ud835\udc62\ud835\udc4a\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61\ud835\udc60 \u2190 \ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc5f\ud835\udc4a\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61\ud835\udc39\ud835\udc5b ( \ud835\udc62 ) 15: \ud835\udc65\ud835\udc4a\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61\ud835\udc60 \u2190 \ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc5a\ud835\udc4a\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61\ud835\udc39\ud835\udc5b ( \ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4e ) \u22b2 Cachable 16: \ud835\udc50\ud835\udc5f\ud835\udc5c\ud835\udc60\ud835\udc60\ud835\udc4a\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61\ud835\udc60 \u2190 \ud835\udc50\ud835\udc5f\ud835\udc5c\ud835\udc60\ud835\udc60\ud835\udc4a\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61\ud835\udc39\ud835\udc5b ( \ud835\udc4e\ud835\udc59\ud835\udc59\ud835\udc3f\ud835\udc5c\ud835\udc54\ud835\udc56\ud835\udc61\ud835\udc60 ) 17: return softmax ( \ud835\udc50\ud835\udc5c\ud835\udc5a\ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc39\ud835\udc5b ( \ud835\udc62\ud835\udc4a\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61\ud835\udc60,\ud835\udc65\ud835\udc4a\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61\ud835\udc60,\ud835\udc50\ud835\udc5f\ud835\udc5c\ud835\udc60\ud835\udc60\ud835\udc4a\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61\ud835\udc60 )) \u22b2 Of shape ( \ud835\udc58 \u2032 , \ud835\udc58 \ud835\udc62 \u00b7 \ud835\udc58 \ud835\udc65 ) 18: end procedure The time complexity for caching item-side computations in MoL is | X | \u00b7 ( \ud835\udc42 ( \ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc5a\ud835\udc38\ud835\udc5a\ud835\udc4f\ud835\udc43\ud835\udc5f\ud835\udc5c\ud835\udc57 ) + \ud835\udc42 ( \ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc5a\ud835\udc4a\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61\ud835\udc39\ud835\udc5b )) , which is negligible on accelerators. Assuming all 2-layer MLPs have a hidden dimension of \ud835\udc51 \u210e , user embedding input dimension is \ud835\udc51 \ud835\udc62 , and all cachable computation are cached, the MoL stage has a computational cost of \ud835\udc42 ( \ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc5f\ud835\udc38\ud835\udc5a\ud835\udc4f\ud835\udc43\ud835\udc5f\ud835\udc5c\ud835\udc57 ) + \ud835\udc42 ( \ud835\udc58 \ud835\udc62 \u00b7 \ud835\udc51 + \ud835\udc58 \u2032 \u00b7 \ud835\udc58 \ud835\udc62 \u00b7 \ud835\udc58 \ud835\udc65 \u00b7 \ud835\udc51 ) + \ud835\udc42 ( \ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc5f\ud835\udc4a\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61\ud835\udc39\ud835\udc5b ) + \ud835\udc58 \u2032 \u00b7 \ud835\udc42 ( \ud835\udc50\ud835\udc5f\ud835\udc5c\ud835\udc60\ud835\udc60\ud835\udc4a\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61\ud835\udc39\ud835\udc5b ) + \ud835\udc58 \u2032 \u00b7 \ud835\udc42 ( \ud835\udc50\ud835\udc5c\ud835\udc5a\ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc39\ud835\udc5b ) + \ud835\udc42 ( \ud835\udc58 \u2032 \u00b7 \ud835\udc58 \ud835\udc62 \u00b7 \ud835\udc58 \ud835\udc65 ) . This cost is dominated by the user-item cross parts: \ud835\udc42 ( \ud835\udc58 \u2032 \u00b7 \ud835\udc58 \ud835\udc62 \u00b7 \ud835\udc58 \ud835\udc65 \u00b7 \ud835\udc51 + \ud835\udc58 \u2032 \u00b7 \ud835\udc42 ( \ud835\udc50\ud835\udc5f\ud835\udc5c\ud835\udc60\ud835\udc60\ud835\udc4a\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61\ud835\udc39\ud835\udc5b ) + \ud835\udc58 \u2032 \u00b7 \ud835\udc42 ( \ud835\udc50\ud835\udc5c\ud835\udc5a\ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc39\ud835\udc5b )) = \ud835\udc42 ( \ud835\udc58 \u2032 \ud835\udc58 \ud835\udc62 \ud835\udc58 \ud835\udc65 ( \ud835\udc51 + \ud835\udc51 \u210e )) .", "4 HIERARCHICAL RETRIEVAL AND INFRASTRUCTURE OPTIMIZATIONS": "We made the following optimizations to enable large-scale MoL based retrieval model training and serving in production.", "4.1 H-indexer Design and Optimization": "In the hierarchical retrieval design (Section 2.2), the first stage finds a large number of candidates ( \ud835\udc58 \u2032 \u223c 10 5 ) with the goal of not missing any candidates that would have originally been selected by the final similarity function in top \ud835\udc58 = 100 \u223c 1000. The high value of \ud835\udc58 \u2032 makes blockwise based top-k algorithms such as FAISS [15] and TPUKNN [2] less applicable, where small \ud835\udc58 \u2032 \ud835\udc60 ( \u2264 2048) are preferred due to register size limit or algorithm design. We therefore propose an approximate top-k design for very large \ud835\udc58 s, h-indexer , with pseudocode in Algorithm 2. We discuss optimizations used below. Algorithm 2 h-indexer top\ud835\udc58 algorithm for large \ud835\udc58 s ( \ud835\udc58 \u2032 s). \ud835\udf06 is the selection ratio for estimating top \ud835\udc58 similarity threshold. 1: procedure h-indexer( \ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4e,\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5f\ud835\udc66,\ud835\udc58, \ud835\udf06 ) 2: \ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc51\ud835\udc3c\ud835\udc5b\ud835\udc51\ud835\udc56\ud835\udc50\ud835\udc52\ud835\udc60 \u2190 permutation ( 0 .. | \ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4e | - 1 ) [ : \ud835\udf06 ] 3: \ud835\udc60\ud835\udc4e\ud835\udc5a\ud835\udc5d\ud835\udc59\ud835\udc52\ud835\udc51\ud835\udc46\ud835\udc56\ud835\udc5a\ud835\udc56\ud835\udc59\ud835\udc4e\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc56\ud835\udc52\ud835\udc60 \u2190[] 4: for \ud835\udc65 \u2208 \ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4e [ \ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc51\ud835\udc3c\ud835\udc5b\ud835\udc51\ud835\udc56\ud835\udc50\ud835\udc52\ud835\udc60 ] do 5: \ud835\udc60\ud835\udc4e\ud835\udc5a\ud835\udc5d\ud835\udc59\ud835\udc52\ud835\udc51\ud835\udc46\ud835\udc56\ud835\udc5a\ud835\udc56\ud835\udc59\ud835\udc4e\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc56\ud835\udc52\ud835\udc60.\ud835\udc4e\ud835\udc5d\ud835\udc5d\ud835\udc52\ud835\udc5b\ud835\udc51 ( dotProduct ( \ud835\udc65, \ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5f\ud835\udc66 )) 6: end for 7: \ud835\udc61 \u2190 nthElement ( \ud835\udc60\ud835\udc4e\ud835\udc5a\ud835\udc5d\ud835\udc59\ud835\udc52\ud835\udc51\ud835\udc46\ud835\udc56\ud835\udc5a\ud835\udc56\ud835\udc59\ud835\udc4e\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc56\ud835\udc52\ud835\udc60,\ud835\udc58 /| \ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4e | \u00b7 \ud835\udf06 ) 8: \ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc56\ud835\udc50\ud835\udc52\ud835\udc60 \u2190[] 9: for ( \ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc56\ud835\udc50\ud835\udc52, \ud835\udc65 ) \u2208 enumerate ( \ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4e ) do 10: if dotProduct ( \ud835\udc65, \ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5f\ud835\udc66 ) > \ud835\udc61 then 11: \ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc56\ud835\udc50\ud835\udc52\ud835\udc60.\ud835\udc4e\ud835\udc5d\ud835\udc5d\ud835\udc52\ud835\udc5b\ud835\udc51 ( \ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc56\ud835\udc50\ud835\udc52 ) 12: end if 13: end for 14: return \ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc56\ud835\udc50\ud835\udc52\ud835\udc60 15: end procedure 4.1.1 dot-product optimization. To control the computational cost of dot-product stage, we used a low embedding dimension (64). Meanwhile, dot product is memory-bounded and its arithmetic intensity - a measure indicating the degree of an op being computebound over memory-bound [42] - only scales with batch size and reversely with input byte-width (byteW),  where corpus size (X) and embedding dimension (D) are far greater than batch size (B). With this observation, we boosted the GPU utilization by batching user requests and applying INT8 quantization. Dot product shows 1 . 5 \u00d7 speed boost compared to the half precision thanks to INT8. Further, the output of INT8 GEMM is INT32, which can be digested by top-k selection directly without applying scale/bias to de-quantize to FP32 as in the regular practice. 4.1.2 approximate top-k. Finding top\ud835\udc58 \u2032 in a corpus with \ud835\udc4b items has a time complexity of \u03a9 ( \ud835\udc4b log \ud835\udc58 \u2032 ) , which is exacerbated by the large \ud835\udc58 \u2032 number. We therefore adopted an approximate top-k approach to randomly sample a small portion of the corpus to estimate the threshold for top \ud835\udc58 \u2032 items, and subsequently finding up to \ud835\udc58 \u2032 samples that meet that threshold. This reduces time complexity to \u03a9 ( \ud835\udc4b + \ud835\udc5f\ud835\udc4b log \ud835\udc58 \u2032 ) with the sampling ratio \ud835\udc5f ranges from 0.01 to 0.1. In production settings, this change is \u223c 2 . 5 \u00d7 faster than exact top-k and reduces h-indexer latency by 30%. 4.1.3 index select. We further optimized the index select op after top-k selection in Algorithm 2. Fetching and concatenating 10 5 item embeddings out of a 10M pool is expensive with a throughput of around 0.15 TB/s. We designed an optimized GPU kernel with 2 \u00d7 throughput by leveraging data locality and efficient caching [20].", "4.2 Op Fusion for Latency and Memory Saving": "We also performed Op fusion to further reduce the serving cost for the MoL stage and achieved 30% performance improvement. While GEMM ops are highly optimized on GPUs, Armdahl's law dictates that non-GEMM ops would become a bottleneck for the end-to-end latency. The read/write cost of input/output tensors KDD '23, August 6-10, 2023, Long Beach, CA, USA between HBM and GPU's caches is significantly exacerbated by the large item size (10 5 ). Fusing consecutive non-GEMM ops, such as \ud835\udc46\ud835\udc56\ud835\udc3f\ud835\udc48 ( \ud835\udc65 ) = \ud835\udc65 \u00b7 Sigmoid ( \ud835\udc65 ) , and fusing non-GEMM ops as epilogues of GEMM could efficiently reduce R/W induced latency. Op fusion can also reduce memory usage for MoL. For example, the op below requires expansion of input tensors by \ud835\udc35 = 8 or \ud835\udc3e \u2032 = 10 5 for the operands in SoftMax, which can be avoided in a fused kernel.", "\ud835\udc42\ud835\udc62\ud835\udc61 = [ \ud835\udc35, \ud835\udc3e \u2032 , \ud835\udc37 ] \u00d7 SoftMax ([ \ud835\udc35, \ud835\udc37 ] + [ \ud835\udc3e \u2032 , \ud835\udc37 ])": "Wealso fused line 7 and 8 in Algorithm 1. Since the size of \ud835\udc50\ud835\udc59 is much larger than \ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc5f\ud835\udc38\ud835\udc5a\ud835\udc4f\ud835\udc60 and \ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc5a\ud835\udc38\ud835\udc5a\ud835\udc4f\ud835\udc60 , fusing the GEMM op and the permute op can greatly save memory and improve throughput.", "4.3 BFLOAT16 Computation": "Although FP16 training has demonstrated 2 \u00d7 throughput than FP32 training [27], the range of FP16 (1-5-10) is insufficient for the backwards gradients in training. The compensating loss scaling technique discards the overflowed iteration unpredictably and results in poor reproducibility for production. We therefore explored the training in BF16 with the same range as FP32. We further aggressively applied BF16 precision to ops like SoftMax, LayerNorm, and Divison and observed no apparent degradation of the final metrics. It is important to perform these non-GEMM ops in BF16 to avoid the time- and memory-costly conversions between BF16 and FP32. Once the model is trained in BF16, we inference it in the FP16 precision. The \ud835\udc35\ud835\udc39 16 \u2192 \ud835\udc39\ud835\udc43 16 conversion introduces no quantization noise as FP16 has more mantissa bits than BF16. Eventually, training throughput is increased by 22% with BF16 computations.", "4.4 FP8 Quantized Communication": "Another innovation we performed in this study is aggressive quantization of All2All traffic to the FP8 format [28, 35]. As a costly bottleneck for recommendation model training, All2All communication that switches between data parallelism and model parallelism is especially expensive for our retrieval model thanks to the complex user embeddings from features and from sequential encoding. FP8 quantization has only 8-bit bitwidth and can handle both forward activations and backward gradients with dynamic scaling. We created a customized FP8 rowwise quantization kernel [20] to quantize all2all tensors before the communication. Since All2All does not require any arithmetic operations, the FP8 rowwise quantization kernel can be executed by A100 GPU that does not even support FP8 arithmetics. The resulting final metrics show no degradation and gained 16% throughput over 16-bit All2All communication. In combination, quantized training with BF16 computation with FP8 communication improved throughput by 37%.", "5 EXPERIMENTS": "We analyze the proposed techniques on both public datasets and large scale industrial datasets. Our proposed techniques are agnostic to the exact retrieval model setup, and apply as long as we need to model (user, item) similarities. We hence consider two settings: \u00b7 Non-sequential methods. This is a standard retrieval setup where the goal is to model the probability of recommending a candidate item given a user as in Equation 1, and is commonly used in various retrieval and recommendations settings [3, 8, 11, 25, 26, 48, 50, 51]. Note that while the model setup is not fully sequential, Jiaqi Zhai, Zhaojie Gong, Yueming Wang, Xiao Sun, Zheng Yan, Fu Li, and Xing Liu Table 3: Data statistics (after preprocessing and truncation) sequential encoders are commonly used in these setups to provide features as user-/context-side encoders [25, 26, 48, 50, 51]. \u00b7 Sequential methods. These methods predict next item based on sequential user-item interactions [18, 34, 44]. User interactions are modeled as a (higher-order) Markov chain. Wetunedhparamsforall non-dot-product setups (MLP, NeuMF [11], DeepFM [9], MoL) so that they use similar compute. We omitted AttentionFM [43] since it cannot fit into 80GB HBM for most configurations due to AFM's usage of element-wise products combined with large number of negatives, as discussed in Section 3.2.", "5.1 Public datasets": "We consider MovieLens (1M and 20M variants), Amazon Reviews (Beauty, Games, and Books subsets) for consistency with prior work [18, 26, 34, 48, 50]. We follow standard preprocessing, including discarding users and items with fewer than 5 related actions, and truncating user behaviorial sequences to 200 (MovieLens) / 50 (Amazon) [18, 34]. Statistics for all datasets can be found in Table 3. For these datasets, due to the popularity of sequential methods in recent years, we apply our proposed approach on top of a sequential baseline, SASRec [18]. SASRec trains the model to predict the next item in an autoregressive manner. SASRec represents one of the best known approaches on public datasets; for instance, on ML-1M, our SASRec implementation achieves . 223 HR@10 (Table 4), on par with best result ( . 224) from [4], and on Amazon Games, our impl's HR@10 is . 0794, which is within 10% of the best result in [4]. 5.1.1 Evaluation Methodologies. Prior work have reported metrics based on different sampling methodologies; e.g., Kang et al. used 100 randomly sampled negatives [18], and Sun et al. used 100 popularity-weighted sampled negatives [34]. For consistency with standard retrieval settings, we report metrics evaluated over the entire corpus, which is also done in [4]. We omit h-indexer given the corpus are small. We focus on HR@10 for MovieLens and HR@200 for Amazon Reviews based on prior work. 5.1.2 Experiments. We consider the baseline setting (binary cross entropy loss on top of dot products, BCE) and also other settings using sampled softmax (SS), including Dot Product, MLP, NeuMF, DeepFM, and MoL. We train all variants with Adam [21], a learning rate of 0 . 001, and a mini batch size of 128. For sampled softmax loss, we sample 128 negatives for denser datasets like ML-1M, ML20M, and Amazon Books, and 512 negatives otherwise. Given that there exists only one group of features in the sequential setting, we use learned MLPs to project user embedding at step \ud835\udc61 to \ud835\udc58 \ud835\udc62 embeddings, and item embedding to \ud835\udc58 \ud835\udc65 embeddings for MoL and DeepFM. For other parameters, we follow reported optimal settings Revisiting Neural Retrieval on Accelerators KDD '23, August 6-10, 2023, Long Beach, CA, USA when those exist, and grid-searched parameters otherwise. Detailed configurations are in Appendix A. All models are implemented in PyTorch and are run on a single A100 80GB GPU. Experiment results can be found in Table 4 and Table 6. Dot product + SS significantly outperforms baseline (BCE) in all settings, confirming the importance of sampled softmax loss in retrieval setting. Non-dot product based methods (e.g., NeuMF and MoL) tend to significantly outperform dot products on datasets that are not extremely sparse (ML-1M, ML-20M, and Amazon Books). Dot products remain competitive for highly sparse datasets, e.g., Beauty and Games (avg < 10 interactions per item). We also observe that we can indeed obtain high rank \ud835\udf19 ( \ud835\udc62, \ud835\udc65 ) distributions with non-dot-product methods, as shown in Table 5. 5.1.3 Ablation Studies. We run the following ablation studies to understand how different components in MoL contribute to overall performance. We fix the baseline as the default MoL configuration, which differs from Table 4 and 6 in that we enabled component-level hypersphere embeddings for Amazon Beauty and Games datasets. \u00b7 no-l2-norm : Ablate component-level hypersphere embeddings. \u00b7 no-gating-dropout : Ablate dropout in gating. \u00b7 50%\ud835\udc58 \ud835\udc62 \u00d7 \ud835\udc58 \ud835\udc65 : Use 50% fewer mixture components for \ud835\udc58 \ud835\udc62 and \ud835\udc58 \ud835\udc65 . \u00b7 25%-negatives : Use 75% fewer negative examples. Results can be found in Table 7. Most components show consistent gains except on the sparse Amazon datasets. Those that consistently contribute to gains ( > 2% are underlined) include component-level hypersphere embeddings, gating dropout, and number of negatives.", "5.2 Industrial Datasets": "Wenext consider a non-sequential model setup, and evaluate model performance on a proprietary industrial dataset against strong baselines. All models share the same features, labels, and losses (incl. number of negatives), and all non-MoL setups use similar compute as the most expensive MoL setup (32 \u00d7 4). Equation 7 was used to compress user-side and item-side embeddings for DeepFM [9]. Results are shown in Table 8. With sufficient model capacity, our proposed MoL model outperforms all baselines under the same compute budget. The best MoL model improves HR@50 by 23.3% compared with Dot product and 5.0% compared with DeepFM. 5.2.1 Hierarchical Retrieval. We next evaluate how our hierarchical retrieval design affects model quality and scalability. Figure 3 compares the performance of MoL with and without h-indexer. We observe that h-indexer does not degrade recall for suitably chosen values of \ud835\udc3e \u2032 (e.g., 10 5 ). While MoL + h-indexer with \ud835\udc3e \u2032 = 10 5 has almost the same recall as the MoL-only model ( \ud835\udc3e \u2032 = \ud835\udc4b ), its throughput is less sensitive to corpus size \ud835\udc4b and enables us to scale up to 100M items on a single GPU. 5.2.2 Effect of Different Mixture Components. We analyze the role of mixture components to understand how model performance scales with model capacity in Table 8. As we increase number of mixture components from 8 \u00d7 4 to 32 \u00d7 4, we can observe significant performance improvement in each scale-up. Meanwhile, the performance of Dot product starts to saturate at dim=256. 5.2.3 Popularity Bias. We observe that MoL and other non-dot product methods also reduce popularity bias. We plot a histogram Figure 3: Recall and serving cost of the two-stage hindexer/MoL model: (a) relative recall ratio vs MoL-only model under varying \ud835\udc58 \u2032 over 10M items, and (b) throughput of two-stage vs one-stage over different pool sizes. K', h-indexer's output candidates (million) Relative Recall Reduction 5e-03 1e-02 5e-02 1e-01 5e-01 1e-4 1e-3 1e-2 1e-1 1e+0 MoL final K=50 K=100 K=200 Candidate Pool Size (million) Throughput (queries/sec/card) 0 50 100 150 200 250 0.5 1.0 5.0 10.0 50.0 100.0 h-indexer (K'=100k)+MoL MoL only Figure 4: Distributions of recommendations over log-scaled recommendation frequency buckets. Lower bars in higher buckets indicates that fewer head items are shown. 0.175 Dot product MLP 0.150 NeuMF DeepFM 0.125 MoL 0.100 0,075 025 Log(# of recommendations) buckets over items to show the distributions of recommended items in Figure 4. Items recommended by MLP, NeuMF, DeepFM, and MoL have more coverage in tail segments and are less biased towards popular ones, with MoL showing the most significant reduction in head buckets.", "5.3 Online Experiments": "We conducted A/B testing for the proposed setup on a major recommendation surface at Meta serving billions of users. Over a period of 7 days, we observed a significant gain of + 3 . 2% in our main KDD '23, August 6-10, 2023, Long Beach, CA, USA Jiaqi Zhai, Zhaojie Gong, Yueming Wang, Xiao Sun, Zheng Yan, Fu Li, and Xing Liu Table 4: MovieLens (-1M and -20M) results on top of sequential methods. topline metric and + 0 . 22% completion rate compared with baseline model. We also compared the latency and throughput between the older production CPU model and the new MoL + h-indexer model on GPUs. The median serving GPU latency of our proposed architecture is 30ms, comparable with optimized CPU MIPS baseline (20ms). The latency regression is a result of request batching optimization, where we let the service wait for up to 10 ms to construct a batch from live requests. Despite high FLOPs, our design does not significantly increase serving cost due to its high arithmetic intensity. A comparable GPU MIPS implementation has 13ms latency (before batching) and 1.8 \u00d7 throughput vs MoL + h-indexer. in various works, including Factorization Machines (FMs) [31] and their deep neural network variants [9, 43], product-based NNs [29], DCNs [41], and so on. The applications of these approaches have been limited to CTR predictions, or ranking. We believe that this is largely due to the difficulty of generalizing MLPs to long tail as discussed by Rendle et al. in [32]. Compared with Attention FMs [43] and PNNs [29], MoL leverages adaptive embedding compression and gating decomposition to significantly reduce compute and memory usage, and l2 norm'ed features to improve generalization.", "6 RELATED WORK": "Dot Product as an Embedding Combiner. Dot products have been widely used to combine model outputs with class information in seminal work across image classifications (e.g., AlexNet [23]), natural language processing (e.g., Transformers [37] and BERT [5]), and information retrieval (e.g., YouTube DNNs [3]). Recently, softmax layer is observed to be a bottleneck for some NLP applications [46], and averaging multiple softmaxes has been found to be helpful in improving model perplexity. Averaging softmaxes, however, requires computation of normalization factor for each softmax, which is costly for NLP vocabularies in the range of 10 4 -10 5 [46] and cost-prohibitive for modern IR settings that need to deal with millions to billions of items [6, 39]. Our algorithm not only reduces computational cost, but also enables better efficiency via embedding sharing as discussed in Section 3.1. Second- and Higher-Order Interaction Modeling. Motivated by observations such as people often downloading food delivery apps at mealtime [9], feature interactions have been recognized as an important factor for achieving good recommendation performance Table 5: Rank of learned \ud835\udf19 ( \ud835\udc62, \ud835\udc65 ) on ML-1M ( \ud835\udc51 = 50 ). Note that a high rank matrix does not always imply high hit rate. Multi-Interest Recommendations and Short-/Long-Term Interests. A line of work related to interaction modeling is multi-interest embeddings [25, 26, 36] which aims to capture diverse preferences of a user over different topics and time periods. For cases where the embedding combiner is not userand item- dependent, like SINE and SDM [26, 36], the resulting model could be more expressive but still necessarily low rank as we've shown in Equation 5. Itemdependent approaches like MIND [25] can be viewed as a special form of MoL where there is exactly one item-side embedding and gating function takes a specific form (e.g., dot-product attention). Learned Discrete Structures for Retrieval. Yet another approach is to learn explicit structures so that retrieval can be viewed as a root-to-leaf traversal, beam search, or operating on top of binary codes directly [8, 19, 50-52]. In particular, TDM [51] first enabled expressive neural networks to be applied at retrieval time. These approaches are more sensitive to hyperparameters, and if heavy branching is involved, can be hard to scale on accelerators. Accelerating Inference for Retrieval. In the traditional dot product retrieval setting, or MIPS (Maximum Inner Product Search), product quantization and hashing techniques [10, 33] have been well studied and are widely used in the non-accelerator retrieval settings. Partitioning the item space is a complementary approach to speed up inference by reducing search space [8, 22, 24, 30, 47, 50, 51]. Search space can be partitioned with clustering [24, 47], spatialpartitioning trees [22, 30], or with approaches where the partition strategy is learned [8, 50, 51]. This line of work can be viewed as alternatives to h-indexer for hierarchical retrieval settings. A recent line of work investigates efficient MIPS-based KNN retrieval on accelerators [2, 15]. There have not been significant work on non-MIPS setups on accelerators to date. Revisiting Neural Retrieval on Accelerators KDD '23, August 6-10, 2023, Long Beach, CA, USA Table 6: Amazon Reviews (Beauty, Games, and Books) results on top of sequential methods. Table 7: Ablation studies on top of MoL and sequential methods.", "7 CONCLUSIONS AND FUTURE WORK": "We have presented new algorithms, including mixture of logits and h-indexer that enable non-dot-product retrieval models to run efficiently on accelerators with latency and throughput comparable to MIPS retrieval setups. These algorithms, which are compatible with various retrieval setups, significantly outperform baselines on both public and large industrial datasets with better recommendation quality and reduced popularity bias. Table 8: Offline results on a large-scale industrial dataset. Our work lays the foundation for many interesting future work for non-dot-product based neural retrieval models. For instance, one direction is to make the mixture components in MoL arbitrarily complex neural networks instead of dot products. Another direction could be to make a cascading retrieval setup end-to-end learnable.", "8 ACKNOWLEDGEMENTS": "We'd like to thank Yinghai Lu, Andrew Tulloch, Pawel Garbacki, Jongsoo Park, Jeff Johnson, Dmytro Ivenchko, Yi Wang, Rui Zhang, Xianjie Chen for various discussions, Jie Hua, Michael He, Pengchao Wang, Chao Yang, Jiacheng Feng, Rui Jian, Hao Lin, Brandon Burroughs, Shripad Thite, Eric Xu, Qing Liu, Jeevan Gyawali, Lei Chen, Bi Xue, Mengchi Zhang, Jianyu Huang, Haixin Liu, Jiecao Yu, Sarunya Pumma, Dhruv Choudhary, Cheng Cheng, Zhiyun Zhang, Will Feng, Fanny Yang, Hao Lu, Wenlei Xie, Bin Wen, Emily Sun, Tianshu Bao, Zexi Mao, Bugra Akyildiz, Hui Zhang, Xinyao Hu, Yun Mao, Kaushik Veeraraghavan for infra/product support, and Shilin Ding, Lin Qiao, Hong Yan, Andre Rohe, Lars Backstrom for overall support of the project. KDD '23, August 6-10, 2023, Long Beach, CA, USA", "REFERENCES": "[1] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and Hemal Shah. 2016. Wide & Deep Learning for Recommender Systems. In Proceedings of the 1st Workshop on Deep Learning for Recommender Systems (DLRS 2016) . 7-10. [2] Felix Chern, Blake Hechtman, Andy Davis, Ruiqi Guo, David Majnemer, and Sanjiv Kumar. 2022. TPU-KNN: K Nearest Neighbor Search at Peak FLOP/s. In Advances in Neural Information Processing Systems . [3] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep Neural Networks for YouTube Recommendations. In Proceedings of the 10th ACM Conference on Recommender Systems (RecSys '16) . 191-198. [4] Alexander Dallmann, Daniel Zoller, and Andreas Hotho. 2021. A Case Study on Sampling Strategies for Evaluating Neural Sequential Item Recommendation Models. In Proceedings of the 15th ACM Conference on Recommender Systems (RecSys '21) . 505-514. [5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics . 4171-4186. [6] Chantat Eksombatchai, Pranav Jindal, Jerry Zitao Liu, Yuchen Liu, Rahul Sharma, Charles Sugnet, Mark Ulrich, and Jure Leskovec. 2018. Pixie: A System for Recommending 3+ Billion Items to 200+ Million Users in Real-Time. In Proceedings of the 2018 World Wide Web Conference (WWW '18) . 1775-1784. [7] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. 2017. Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning. Neural networks : the official journal of the International Neural Network Society 107 (2017), 3-11. [8] Weihao Gao, Xiangjun Fan, Chong Wang, Jiankai Sun, Kai Jia, Wenzi Xiao, Ruofan Ding, Xingyan Bin, Hui Yang, and Xiaobing Liu. 2021. Learning An End-to-End Structure for Retrieval in Large-Scale Recommendations. In Proceedings of the 30th ACM International Conference on Information and Knowledge Management (CIKM '21) . 524-533. [9] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: A Factorization-Machine Based Neural Network for CTR Prediction. In Proceedings of the 26th International Joint Conference on Artificial Intelligence (IJCAI'17) . 1725-1731. [10] Ruiqi Guo, Sanjiv Kumar, Krzysztof Choromanski, and David Simcha. 2016. Quantization based Fast Inner Product Search. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, AISTATS 2016 , Vol. 51. 482-490. [11] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural Collaborative Filtering. In Proceedings of the 26th International Conference on World Wide Web (Perth, Australia) (WWW'17) . 173-182. [12] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. 1989. Multilayer feedforward networks are universal approximators. Neural Networks 2, 5 (1989), 359-366. [13] Jui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip Pronin, Janani Padmanabhan, Giuseppe Ottaviano, and Linjun Yang. 2020. EmbeddingBased Retrieval in Facebook Search. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . 2553-2561. [14] Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. 1991. Adaptive Mixtures of Local Experts. Neural Computation 3, 1 (1991), 79-87. [15] J. Johnson, M. Douze, and H. Jegou. 2021. Billion-Scale Similarity Search with GPUs. IEEE Transactions on Big Data 7, 03 (Jul 2021), 535-547. [16] M.I. Jordan and R.A. Jacobs. 1993. Hierarchical mixtures of experts and the EM algorithm. In Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan) , Vol. 2. 1339-1344 vol.2. [17] Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. 2017. In-datacenter performance analysis of a tensor processing unit. In Proceedings of the 44th annual international symposium on computer architecture . 1-12. [18] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recommendation. In 2018 International Conference on Data Mining (ICDM) . 197-206. [19] Wang-Cheng Kang and Julian McAuley. 2019. Candidate Generation with Binary Codes for Large-Scale Top-N Recommendation. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management (Beijing, China) (CIKM '19) . 1523-1532. [20] Daya Khudia, Jianyu Huang, Protonu Basu, Summer Deng, Haixin Liu, Jongsoo Park, and Mikhail Smelyanskiy. 2021. FBGEMM: Enabling High-Performance Low-Precision Deep Learning Inference. arXiv preprint arXiv:2101.05615 (2021). [21] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In International Conference on Learning Representations (ICLR'15) . [22] Noam Koenigstein, Parikshit Ram, and Yuval Shavitt. 2012. Efficient Retrieval of Recommendations in a Matrix Factorization Framework. In Proceedings of the 21st ACM International Conference on Information and Knowledge Management (CIKM '12) . 535-544. Jiaqi Zhai, Zhaojie Gong, Yueming Wang, Xiao Sun, Zheng Yan, Fu Li, and Xing Liu [44] Xin Xin, Alexandros Karatzoglou, Ioannis Arapakis, and Joemon M. Jose. 2020. Self-Supervised Reinforcement Learning for Recommender Systems. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development Revisiting Neural Retrieval on Accelerators KDD '23, August 6-10, 2023, Long Beach, CA, USA in Information Retrieval (SIGIR '20) . 931-940. [45] Ji Yang, Xinyang Yi, Derek Zhiyuan Cheng, Lichan Hong, Yang Li, Simon Xiaoming Wang, Taibai Xu, and Ed H. Chi. 2020. Mixed Negative Sampling for Learning Two-Tower Neural Networks in Recommendations. In Companion Proceedings of the Web Conference 2020 (WWW '20) . 441-447. [46] Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W. Cohen. 2018. Breaking the Softmax Bottleneck: A High-Rank RNN Language Model. In International Conference on Learning Representations (ICLR'18) . [47] Jiaqi Zhai, Yin Lou, and Johannes Gehrke. 2011. ATLAS: A Probabilistic Algorithm for High Dimensional Similarity Search. In Proceedings of the 2011 ACM SIGMOD International Conference on Management of Data (SIGMOD '11) . 997-1008. [48] Chang Zhou, Jianxin Ma, Jianwei Zhang, Jingren Zhou, and Hongxia Yang. 2021. Contrastive Learning for Debiased Candidate Generation in Large-Scale Recommender Systems. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 3985-3995. [49] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep Interest Network for ClickThrough Rate Prediction (KDD '18) . 10 pages. [50] Han Zhu, Daqing Chang, Ziru Xu, Pengye Zhang, Xiang Li, Jie He, Han Li, Jian Xu, and Kun Gai. 2019. Joint Optimization of Tree-Based Index and Deep Model for Recommender Systems. In Proceedings of the 33rd International Conference on Neural Information Processing Systems . Article 357. [51] Han Zhu, Xiang Li, Pengye Zhang, Guozheng Li, Jie He, Han Li, and Kun Gai. 2018. Learning Tree-Based Deep Model for Recommender Systems. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (London, United Kingdom) (KDD '18) . 1079-1088. [52] Jingwei Zhuo, Ziru Xu, Wei Dai, Han Zhu, Han Li, Jian Xu, and Kun Gai. 2020. Learning Optimal Tree Models under Beam Search. In Proceedings of the 37th International Conference on Machine Learning (ICML'20) .", "A HYPERPARAMETER SETTINGS (PUBLIC DATASETS)": "For ML-1M, Amazon Beauty, and Amazon Games datasets, we follow the original optimal hyperparams for SASRec reported in [18], specifically two encoder layers ( \ud835\udc4f = 2) with one attention head ( \u210e = 1), and a dropout rate of 0.2 for MovieLens and 0.5 for Amazon Reviews datasets. For ML-20M and Amazon Books, we kept dropout rate identical, grid-searched embedding dimension, number of encoder layers and attention heads for SASRec, and obtained \ud835\udc51 = 256, \ud835\udc4f = 4, \u210e = 8 for ML-20M and \ud835\udc51 = 64, \ud835\udc4f = 4, \u210e = 4 for Books. We grid searched MoL parameters ( \ud835\udc58 \ud835\udc62 , \ud835\udc58 \ud835\udc65 ) in ( 2 , 2 ) , ( 4 , 4 ) , ( 8 , 8 ) and fixed component-level embedding dimension to 32. For DeepFM experiments, we used the exact same configuration for user- and item- side projections as MoL, and tuned hidden dim and dropout rate additionally. Given the high computational costs of running experiments on ML-20M and Amazon Books datasets (up to 3d for 300 epochs), we shared NeuMF/DeepFM/MoL hparams between those two groups and ML-1M. Detailed configurations can be found in Table 9. KDD '23, August 6-10, 2023, Long Beach, CA, USA Jiaqi Zhai, Zhaojie Gong, Yueming Wang, Xiao Sun, Zheng Yan, Fu Li, and Xing Liu", "Table 9: Hyperparameters used for public datasets": ""}
