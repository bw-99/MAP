{
  "Rec4Ad: A Free Lunch to Mitigate Sample Selection Bias for Ads CTR Prediction in Taobao": "Jingyue Gao, Shuguang Han, Han Zhu, Siran Yang, Yuning Jiang, Jian Xu, Bo Zheng {jingyue.gjy,shuguang.sh,zhuhan.zh,siran.ysr,mengzhu.jyn,xiyu.xj,bozheng}@alibaba-inc.com Alibaba Group Beijing, China",
  "ABSTRACT": "to rank them based on expected Cost Per Mille (eCPM) [31]: Click-Through Rate (CTR) prediction serves as a fundamental component in online advertising. A common practice is to train a CTR model on advertisement (ad) impressions with user feedback. Since ad impressions are purposely selected by the model itself, their distribution differs from the inference distribution and thus exhibits sample selection bias (SSB) that affects model performance. Existing studies on SSB mainly employ sample re-weighting techniques which suffer from high variance and poor model calibration. Another line of work relies on costly uniform data that is inadequate to train industrial models. Thus mitigating SSB in industrial models with a uniform-data-free framework is worth exploring. Fortunately, many platforms display mixed results of organic items (i.e., recommendations) and sponsored items (i.e., ads) to users, where impressions of ads and recommendations are selected by different systems but share the same user decision rationales. Based on the above characteristics, we propose to leverage recommendations samples as a free lunch to mitigate SSB for ads CTR model (Rec4Ad). After elaborating data augmentation, Rec4Ad learns disentangled representations with alignment and decorrelation modules for enhancement. When deployed in Taobao display advertising system, Rec4Ad achieves substantial gains in key business metrics, with a lift of up to +6.6% CTR and +2.9% RPM.",
  "KEYWORDS": "CTR Prediction, Sample Selection Bias, Disentangled Representation, Online Advertising, System Deployment",
  "ACMReference Format:": "Jingyue Gao, Shuguang Han, Han Zhu, Siran Yang, Yuning Jiang, Jian Xu, Bo Zheng. 2023. Rec4Ad: A Free Lunch to Mitigate Sample Selection Bias for Ads CTR Prediction in Taobao. In Proceedings of Make sure to enter the correct conference title from your rights confirmation emai (Conference acronym 'XX). ACM, New York, NY, USA, 7 pages. https://doi.org/XXXXXXX.XXXXX",
  "1 INTRODUCTION": "For large-scale e-commerce platforms like Taobao, online advertising contributes a large portion of revenue. As advertisers typically pay for user clicks on advertisements (ads), a common practice is Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference acronym 'XX, June 03-05, 2023, Woodstock, NY ¬© 2023 Association for Computing Machinery. ACM ISBN 978-1-4503-XXXX-X/18/06...$15.00 https://doi.org/XXXXXXX.XXXXX  where ùëùùëêùë°ùëü is the predicted Click-Through Rate (CTR), and ùëèùëñùëë denotes the price for each click. Hence, CTR prediction serves as a fundamental component for online advertising systems. Figure 1: An illustration of the serving and updating process for CTR model in a typical online advertising system. CTR Model Candidate Select Display Feedback Update inference space training space User 1 2 3 4 5 As shown in Fig. 1, a production CTR model scores all candidates and selects the top few based on Eq. (1) for display. The displayed ads as well as user feedback (i.e., click/non-click) are then recorded, with which we continuously train new models. Due to its simplicity and robustness, such a training paradigm is widely adopted by many industrial systems [1, 19, 21]. However, since the displayed ads are not uniformly sampled from all candidates but purposely selected by the model itself, the training data distribution could be skewed from the inference distribution. This is widely known as the sample selection bias (SSB) problem [13, 32]. It violates the classical assumption of training-inference consistency and may potentially affect the model performance. Recent efforts [20, 22, 24, 28, 31] have been devoted to alleviating SSB in ranking systems. Methods based on Inverse Propensity Scoring [22, 24, 28] recover the underlying distribution by re-weighting the training samples. Despite theoretical soundness, they require a propensity model that accurately estimates sample occurrence probability, which is difficult to learn in dynamic and complicated environments. Moreover, sample re-weighting may yield un-calibrated predictions that are problematic for ads CTR models [30]. Another line of work collects uniform data via random policy, which helps train an unbiased imputation model for non-displayed items [31] or guide the CTR model training via knowledge distillation [20]. However, even small production traffic (e.g., 1%) of the uniform policy will severely cause degraded user experience and revenue loss, and the obtained uniform data of this magnitude is insufficient for training industrial models with billions of parameters. With Conference acronym 'XX, June 03-05, 2023, Woodstock, NY Gao, et al. Interest Interest angle Layer Disentangle Layer Mapping Layer Domain-specific BN omain A Domain  B Multi-Domain Features rch Guess you like Item i Item i Ad 1 ... Ad N-K ... Ad N-1 Ad N Raw Rec Sample Ad 1 Raw Rec Sample Context User Item i Most Recent K Ads Context User Ad N-1 Pseudo Ad Sample Mini Detail ... Context Sample Skeleton User id Context Join label User features User id 1 User id 2 User features Context User label Context Item i Ad N-K Figure 2: (A) Mixed results of ads (green dashed line) and recommendations (orange dashed line). (B,C) Causal Graph of CTR prediction for both ads and recommendations. Features Display Click (B) (C) (A) Ad N-1 Ad N ... Most Recent K Ads User label Context Pseudo Ad Sample Random Mapping Ad N-1 Features Display Click Features sample#1 sample#K ... Message Queue Pre-Batching these issues, we investigate how to mitigate SSB for industrial CTR models under a uniform-data-free framework. User features Ad features Assembled Sample label Ad features Ad id Join Ad id 1 Ad id 2 Ad features Features User features Ad features label Assembled Sample few if any confounders common in two systems could still remain with Œì , disentangling system-specific ‚àÜad / ‚àÜrec from Œì is already a meaningful step towards mitigating SSB, especially when uniform data is unavailable in industrial advertising systems. User Behavior Sequence User Profile Context Targe Embedding & Concatenatio Rec Projection Routing Decomposition Alignment Fusion & Prediction Loss Terms Alignment Fusion & Prediction Fusion & Predict On To this end, we propose to leverage Rec ommendation samples to mitigate SSB F or Ad s CTR prediction ( Rec4Ad ). Under this framework, recommendation samples are retrieved and mixed with ad samples for training. With raw feature embeddings, we elaborately design the representation disentanglement mechanism to dissect system-specific confounders and system-invariant user-item interest across two systems. Specifically, this mechanism consists of an alignment module and a decorrelation module with various regularizations. Finally we make prediction with disentangled and enhanced representation. Rec4Ad has been deployed to serve the main traffic of Taobao display advertising system since July of 2022. Our contributions are summarized as follows: Decomposito Rec Retrieval Mapping Decomposition Loss Terms ¬∑ We analyze the existence of SSB in CTR prediction and point out the potential to leverage recommendation samples to mitigate such bias in absence of uniform data. Rec Projection Routing ¬∑ Wepropose a novel framework named Rec4Ad, which jointly considers the recommendation and ads samples in learning disentangled representations that dissect system-specific confounders and system-invariant user-item interest. Ad Projection Fea#1 ... Fea#N ... Fea#1 Fea#N ... Fea#1 Fea#N ... Fea#1 Fea#N ... Fea#1 Fea#N Fea#1 ... Fea#N Batching Batching ... Fea#1 ... Fea#1 Training Tasks Training Tasks sample#1 sample#K ... Message Queue Fea#1 ... Fea#N Fea#1 ... Fea#N Batching Batching ... Fea#1 Fea#N Pre-Batching Inspired by causal learning [2, 29], CTR prediction can be framed as the problem of treatment effect estimation. As in Fig. 2(B), sample features compose unit X , whether to display it acts as a binary treatment T and click is the outcome Y to estimate. The root cause of SSB is attributed to existence of confounders ‚àÜ (e.g., item popularity) in X that affect both T and Y . Recent studies [9, 14] show that confounders mislead models to capture spurious correlations between the unit features and the outcome, which are non-causal and hurt generalization over the inference distribution. Hence, it is promising to mitigate SSB by disentangling confounders ‚àÜ from real user-item interest Œì in sample features, which is non-trivial in absence of randomized controlled trials (i.e., uniform data) [23]. ... Fea#1 Fea#N ... Fea#1 Fea#N ... Fea#1 Fea#N ... Fea#1 Fea#N ... Fea#1 Fea#N Training Tasks Training Tasks As shown in Fig. 2, many platforms [4, 10] display mixed results of sponsored items and organic items that are independently selected by advertising and recommendation systems. For clarity, we refer to sponsored items as ads . We refer to organic items as recommendations . This scenario has two characteristics: ¬∑ Shared decision rationales. With a unified interface design, users are unaware of whether items are sponsored or organic, making their click decision determined by real user-item interest Œì rather than sources of displayed items. ¬∑ Different selection mechanisms. Advertising and recommendation systems serve different business targets (e.g., revenue/clicks/dwell time) [10] and have different selection mechanisms as verified in Sec. 2.2. Thus their SSB-related confounders are rarely overlapped, making system-specific confounders ‚àÜad / ‚àÜrec capture a substantial portion of ‚àÜ . The above characteristics make it possible to disentangle ‚àÜad / ‚àÜrec and Œì by jointly considering samples from two sources. Compared with the uniform data, recommendation samples are of a comparable or even larger magnitude than ads samples and persist without revenue loss, making it a free lunch worthy of exploitation. Though User Behavior Sequence User Profile Context Target Ad Embedding & Concatenation ¬∑ We conduct offline and online experiments to validate the effectiveness of Rec4Ad that achieves substantial gains in business metrics (up to +6.6% CTR and +2.9% RPM). Sample Indi Mapping Fea#N Fea#N Retrieval",
  "Rec 2 PRELIMINARY": "",
  "2.1 Problem Formulation": "Input: The input includes a user set U , an ad set A , an item set I , user-ad impressions D ad , and user-item simpressions D rec ¬∑ Each user ùë¢ ‚àà U is represented by a set of features { ùë¢ 1 , ..., ùë¢ ùëö } including user profile features (e.g., age and gender) and historical behaviors (e.g., click and purchase). ¬∑ Each ad ùëé ‚àà A is a promotion campaign for a sponsored item ùëñ ‚àà I . Besides item-level features like category and brand, ùëé also has campaign-level features including ID and historical statistics, denoted by { ùëé 1 , ..., ùëé ùëõ } . ¬∑ Each impression in ùê∑ ùëéùëë is a tuple ( ùë¢, ùëé, ùëê, ùë¶ ) describing when the advertising system displayed ùëé to ùë¢ under context ùëê = { ùëê 1 , .., ùëê ùëò } such as time and device, user clicked it ( ùë¶ = 1) or not ( ùë¶ = 0). As for ùê∑ ùëüùëíùëê , the tuple changes to ( ùë¢, ùëñ, ùëê, ùë¶ ) logged by the recommendation engine. Output: Weaimtolearnamodel ùëì that predicts the click probability ùëì ( ùë¢, ùëé, ùëê ) if displaying ùëé ‚àà A to user ùë¢ ‚àà U under context ùëê .",
  "2.2 Analysis of Sample Selection Bias": "2.2.1 SSB in Ads Impressions. SSB happens when each candidate does not share equal opportunities for impression. To examine its existence, we define the metric of impression ratio (IR) to measure the opportunity of each ad in our system:  where a session refers to a user request. We first calculate IR for each ad in ùê∑ ùëéùëë , sort them by IR in descending order, and then divide Ad Random Mapping label label Rec4Ad: A Free Lunch to Mitigate Sample Selection Bias for Ads CTR Prediction in Taobao Conference acronym 'XX, June 03-05, 2023, Woodstock, NY Fea#1 Figure 3: An overall framework of Rec4Ad (best viewed in color). Rec Retrieval Mapping Ad User Behavior Sequence User Profile Context Target Ad Embedding & Concatenation Rec Projection Ad Projection Routing Sample Indicator Decorrelation Alignment Data Augmentation Disentangled Representation Prediction Prediction Loss Terms Online Serving Item i Ad N-K Ad 1 Ad N-1 Ad N ... Most Recent K Ads User label Context User label Context Pseudo Ad Sample Raw Rec Sample ... Random Mapping Ad N-1 Item i Decorrelation Pesudo Sample Mapping Sample Skeleton Daa Uniform 0.04 0,01 Group User id Ad id Context label Ad features Ad features label Features Features Figure 4: Left: Average IR for different groups of ads. Right: Model performance w.r.t different IR group of ads. User features User id 1 User features User id 2 Ad id 1 Ad id 2 Join Join User features Context Ad features Assembled Sample 40,02 40,01 Group Display Rec Projection Ad Projection Routing Decomposition Decompositon Alignment Disentangled Representation Fusion & Prediction Fusion & Prediction Online Serving 2.2.3 Mitigating SSB with Recommendation Samples. As defined before, each ad in A corresponds to an organic item in I . Thus we investigate how impressions in ùê∑ ùëüùëíùëê are distributed among organic items with ad counterparts. From Fig. 5 (Left), we find that though some groups of ads have few impressions in ùê∑ ùëéùëë , their corresponding items contribute an important portion of impressions in ùê∑ ùëüùëíùëê . It is attributed to different selection mechanism behind ùê∑ ùëéùëë and ùê∑ ùëüùëíùëê . We conduct a simulated study on ùê∑ ùëéùëë for verification, which changes the ranking function from ùëùùëêùë°ùëü ‚àó ùëèùëñùëë to ùëùùëêùë°ùëü (commonly adopted in recommendation systems) and redisplays top-10 ads. Fig. 5 (Right) illustrates IR of each original rank under two mechanisms. It is clear that impression distribution is changed, where ads with low rank in original list have opportunities to be displayed under another mechanism . Above empirical analysis show that it is promising to leverage recommendation samples in ùê∑ ùëüùëíùëê to mitigate SSB in ùê∑ ùëéùëë . Embedding & Concatenation Loss Terms Click Dad pctr bid pctr 1 10 Group Original rank Drec Context Figure 5: Left: Impression Distribution in ùê∑ ùëéùëë and ùê∑ ùëüùëíùëê . Right: Simulated results under different ranking mechanisms. Rec sample#1 sample#K ... Message Queue ... Fea#N Fea#1 ... Fea#N Pre-Batching them equally into 12 groups. Fig. 4 (Left) shows the average IR for each group (the red line) compared with the ideal uniform data (the blue line). We find that impressions on ùê∑ ùëéùëë are distributed among different ads in an extremely imbalanced way , where the IR of the first group is nearly 200 times that of the last group. Batching Batching ... Fea#1 Fea#N Fea#1 Fea#1 ... Fea#N ... Fea#N ... Fea#1 Fea#N ... Fea#1 Fea#N ... Fea#1 Fea#N Training Tasks Training Tasks 2.2.2 Influence of SSB. Since base CTR model is trained on ùê∑ ùëéùëë , we analyze its ranking and calibration performance under the imbalanced ad impressions. For ranking performance, we use the metric of AUC, and the calibration performance is measured by the Expected Calibration Error (ECE) [30]. Details of two metrics are introduced in Sec. 4.1. Fig. 4 (Right) shows online model performance on different groups of ads with descending IR. It is observed",
  "3 METHODOLOGY": "User Behavior Sequence User Profile Target Ad Sample Indicator Retrieval Mapping Ad Data Augmentation Fig. 3 shows two stages of Rec4Ad in deployment: data augmentation and disentangled representation learning , which constructs and leverages recommendation (rec) samples, respectively.",
  "3.1 Data Augmentation": "3.1.1 Retrieving Recommendation Samples. To ensure user experience, the percentage of ad impressions in all impressions is usually limited to a low threshold. Typically, we have | ùê∑ ùëéùëë | ‚â™ | ùê∑ ùëüùëíùëê | , making it intractable to consume entire ùê∑ ùëüùëíùëê owing to multiplied training resources. Moreover, not all rec samples are useful for enhancing ads model due to difference between A and I . Thus we retrieve rec samples that are closely related to advertising system. Let ùêº ( ùëé ) ‚àà I denote the item that ùëé ‚àà A is advertised for. We define an item set I ùëéùëë containing all items with related ads:  that model tends to perform worse on ads with lower IR than on those with higher IR . It is consistent with our assumption that model does not generalize well on ads with few impressions and validates the necessity to handle SSB for improvement. Conference acronym 'XX, June 03-05, 2023, Woodstock, NY Gao, et al. We discard rec samples whose items fail to occur in I ùëéùëë , since they fail to provide complementary impressions that relate to any ad of interest. In this way, we retrieve a subset of rec samples:  3.1.2 Pseudo Sample Mapping. The reasons to map retrieved rec samples to pseudo ad samples are two-fold. First, it allows rec samples and ad samples to have uniform input format, which facilitates efficient feature joining and batch processing. Second, pseudo samples scattering in the U √ó A space help learn a CTR prediction model for ads, compared with the U√óI space. In Fig. 3, we maintain an item-ads index where key is item ùëñ and values are their related ads ùê¥ ( ùëñ ) = { ùëé ‚àà A| ùêº ( ùëé ) = ùëñ } . To select an ad from ùê¥ ( ùëñ ) , we do not take their impressions into consideration, which avoids introducing selection bias in the advertising system. Instead, we adopt a recentùêæ -random strategy. We randomly select an ad ùëé ‚Ä≤ from most recent ùêæ ads related to ùëñ , where ùêæ is a fixed hyperparameter. After mapping, we obtain the set of pseudo samples:",
  "3.2 Disentangled Representation Learning": "3.2.1 Original Representation. we embed raw features of sample ( ùë¢, ùëé, ùëê, ùë¶ ) into low-dimensional vectors [ ùëí ( ùë¢ ) , ùëí ( ùëé ) , ùëí ( ùëê )] . Operations like attention mechanism [34] are further employed to aggregate embeddings of user behavior sequences. We concatenate these results together to obtain intermediate representation e . Batch Normalization (BN) [15] is commonly used in training of industrial CTR models [25] to stabilize convergence. It calculate statistics over training data for normalization during serving. However, when incorporating ùê∑ Àú ùëüùëíùëê into training, BN statistics are calculated based on ad and rec samples but only used to normalize ad samples during online serving. The distribution discrepancy between two kinds of samples weakens the effectiveness of BN. To deal with this problem, we design source-aware BN (SABN), which adaptively normalize samples according to their sources. Let ùë† indicate which kind the sample is, SABN works as follows:  where ùõæ ùë† , ùõΩ ùë† , ùúá ùë† , ùúé 2 ùë† are source-specific parameters for normalization. Then we feed the normalized representation e ‚Ä≤ into MLP (Multi-Layer Perception) layers for a compact representation x that captures feature interactions among user, ad, and context. We add superscripts on representations (e.g., x ùëéùëë / x ùëüùëíùëê ) to denote its source. 3.2.2 Alignment. Since users are usually unaware of the difference between ad and rec impressions, their click decisions can be assumed independent of underlying systems, which are commonly determined by their interest. To identify user-item interest Œì behind click decision, we propose to extract invariant representations shared between ùê∑ ùëéùëë and ùê∑ Àú ùëüùëíùëê . In other words, samples in ùê∑ ùëéùëë and ùê∑ Àú ùëüùëíùëê should be indistinguishably distributed in the invariant representation space. To achieve this goal, we first apply projection layers over original representations of ad and rec samples:  A direct method to align { x ùëéùëë ùëñùëõùë£ } and { x ùëüùëíùëê ùëñùëõùë£ } is minimizing their Wasserstein or MMD distribution distance [7, 11]. However, these metrics are computationally inefficient and hard to estimate accurately over mini-batches. Instead, we train a sample discriminator ùêª to implicitly align them in an adversary way. Particularly, ùêª is a binary classifier that predicts whether the sample is from ùê∑ ùëéùëë or ùê∑ Àú ùëüùëíùëê based on x ùëñùëõùë£ . Optimized with cross entropy loss, ùêª aims to distinguish two kinds of samples as accurate as possible:  While ùêª tries to minimize ùêø ùê¥ during training, neural layers generating invariant representations aim to make { x ùëéùëë ùëñùëõùë£ } and { x ùëüùëíùëê ùëñùëõùë£ } indistinguishable as much as possible, i.e., maximize ùêø ùê¥ . To train these two parts simultaneously, we insert a gradient reverse layer (GRL) [8] between x ùëñùëõùë£ and the discriminator. In forward propagation, GRL acts as an identity transformation. In backward propagation, it reverses gradients from subsequent layers: Forward : ùê∫ùëÖùêø ( x ùëñùëõùë£ ) = x ùëñùëõùë£ ,  where ùõº controls the scale of reversion. In this way, we tightly align { x ùëéùëë ùëñùëõùë£ } and { x ùëüùëíùëê ùëñùëõùë£ } in the invariant representation space. 3.2.3 Decorrelation. To separate system-specific confounders from original representation, we apply another set of projection layers:  If without explicit constraints, x ùëêùëúùëõ could still contain information shared across systems and prevent us from handling confounders specific to the ad system. To this end, we propose to add regularizations to further disentangle x ùëêùëúùëõ and x ùëñùëõùë£ . Borrowing the idea that disentangled representations avoid encoding variations of each other [5, 6], we penalize the cross-correlation between two sets of representations. Specifically, let p ùëñ denote the in-batch vector of ùëñ -th dimension of x ùëñùëõùë£ and q ùëó denote that of ùëó -th dimension of x ùëêùëúùëõ , their Pearson correlation can be calculated as: ùê∂ùëúùë£ ( p ùëñ , q ùëó )) = [ p ùëñ - ¬Ø pi ] ‚ä§ [ q ùëó - ¬Ø qj ] ,  where ¬Ø pi and ¬Ø qj denote in-batch mean of each dimension. Thus the objective of the decorrelation module are based on correlations of every pair of dimension cross x ùëñùëõùë£ and x ùëêùëúùëõ :  By optimizing ùêø ùê∑ , x ùëêùëúùëõ are encouraged to capture residual information independent from x ùëñùëõùë£ , i.e., system-specific confounders ‚àÜad / ‚àÜrec that are discarded by the alignment module.",
  "3.3 Prediction": "Wereconstruct final representation based on disentangled representations to predict CTR. Previous studies show that non-causal associations also potentially contribute to prediction accuracy [26, 33], Rec4Ad: A Free Lunch to Mitigate Sample Selection Bias for Ads CTR Prediction in Taobao Conference acronym 'XX, June 03-05, 2023, Woodstock, NY motivating us to consider x ùëêùëúùëõ in reconstruction instead of directly ignoring it. For simplicity, we use the concatenation operator:  With x ùë† ùëõùëíùë§ , we make predictions for ad samples and pseudo samples with source-aware layers, where cross entropy loss ùêø ùê∂ is optimized:  Thus the objective function of Rec4Ad consists of the CTR prediction loss, the alignment loss and the decorrelation loss:",
  "4 EXPERIMENTS": "",
  "4.1 Experimental Setup": "Taobao Production Dataset. We construct the dataset based on impression logs in two weeks of 2022/06 from Taobao advertising system and recommendation system. We use data of the first week for training, which contains ad and rec impressions collected under regular policy. The data of the next week are ad impressions collected under random policy of a small traffic following [20, 31], which is used to evaluate model performance against SSB. The training dataset contains 1.9 billion ad samples and 0.6 billion rec samples after retrieval, covering 0.2 billion users. The test dataset contains 18.9 million ad samples and 10.3 million users. Baselines. Rec4Ad is compared with following baselines. ¬∑ Base . We adopt DIN [34] as the vanilla model which does not account for SSB. ¬∑ DAG . The Data-Augmentation (DAG) method directly merges rec samples and ad samples to train the base model. ¬∑ IPS [16, 24]. It eliminates SSB by re-weighting samples with inverse propensity of ad impression. ¬∑ IPS-C [3] It adds max-capping to IPS weight so that its variance can be reduced. ¬∑ IV [26]. It employs user behaviors outside current system as instrumental variables for model debiasing. Metrics. For ranking ability, we use the standard AUC (Area Under the ROC Curve) metric for evaluation [12, 18]. A higher AUC indicates better ranking performance. In practice, absolute improvement of AUC by 0.001 on the production dataset is considered significant, which empirically leads to an online lift of 1% CTR . For calibration, we evaluate models with the ECE [30] metric. We first equally partition the ùëùùëêùë°ùëü range [0,1] into ùêæ buckets ùêµ 1 , ..., ùêµ ùêæ . ECE can be calculated as follows:  where ùüô ( ÀÜ ùë¶ ùëñ ‚àà ùêµ ùëò ) equals 1 only if ÀÜ ùë¶ ùëñ ‚àà ùêµ ùëò else 0. ùêæ is set to 100. A lower ECE here indicates better calibration performance. Implementation The feature embedding size is 16. We use Adam optimizer [17] with initial learning rate 0 . 001. The batch size is fixed to 6000. In data augmentation, we consider the most recent 3 ads for pseudo sample mapping. The dimensions ùëë of x ùëñùëõùë£ and x ùëêùëúùëõ is 128. The ratio ùõº of gradient reverse layer in Eq. (9) is 0.1. ùúÜ 1 and ùúÜ 2 for the alignment and the decorrelation loss in Eq. (15) is 0.005 and 0.5. For tests of significance, each experiment is repeated 5 times by random initialization and we report the average as results.",
  "4.2 Experimental Results": "Table 1: Performance Comparison of Rec4Ad and baselines. The symbol * indicates the improvements over baselines are significant with p-value < 0.01 by t-test. We omit standard deviations since they are all no more than 2 √ó 10 -4 . 4.2.1 Overall Performance. From Table 1, we find that Rec4Ad significantly performs better than all baselines. Specifically, it outperforms Base in terms of AUC by 0.0027 and outperforms the state-of-the-art IV by 0.0015. This demonstrates the effectiveness of our proposed framework in handling SSB. By dissecting confounders and user-item interest for enhanced representations, it works well over the inference space. Moreover, Rec4Ad successfully maintains even slightly better model calibration than Base, which also verifies its suitability for ads CTR prediction. We also observe that DAG performs worse than baseline both in AUC and ECE. The reason is ad samples and rec samples present different feature distributions and label distributions. Naive data augmentation actually amplifies the distribution discrepancy between training and inference. The original IPS yields worst AUC, while IPS-C with max-capping achieves higher AUC than Base. We attribute this phenomenon to high variance in estimation of propensity score. We also notice that ECE of IPS and IPS-C are all larger than 10 -3 , which verifies that sample re-weighting could change label distribution and result in calibration issues of ads CTR prediction. Table 2: Comparison among Rec4Ad and three competitive baselines on ad groups with different impression ratios. 4.2.2 Performance on Different Ad Groups. In Section 2.2.2, we show that SSB leads model to perform badly on ads with low impression ratios. To validate whether Rec4Ad mitigates such influence, we compare Rec4Ad and three competitive baselines on specific ad Conference acronym 'XX, June 03-05, 2023, Woodstock, NY Gao, et al. groups. We sort ads in descending IR as defined in Eq. (2), where the top 25% are selected as ùê∫ ùë°ùëúùëù representing ads with enough impressions and the bottom 25% are selected as ùê∫ ùëèùëúùë°ùë°ùëúùëö containing ads that are less represented in the training data. Table 2 shows that Rec4Ad achieves best ranking and calibration performance on both ùê∫ ùë°ùëúùëù and ùê∫ ùëèùëúùë°ùë°ùëúùëö . The improvements over Base are greater on ùê∫ ùëèùëúùë°ùë°ùëúùëö with AUC increased by nearly 0.003 and ECE reduced by 0.001. Thus we conclude that Rec4Ad succeeds in mitigating SSB and boosts model performance on those long-tail ads. We also observe an interesting seesaw phenomenon about IPS-C, which also greatly improves metrics on ùê∫ ùëèùëúùë°ùë°ùëúùëö but yields worse performance on ùê∫ ùë°ùëúùëù compared with Base. It is because IPSC explicitly imposes higher weights for samples with low-IR ads and lower weights for those with high-IR ads. By contrast, Rec4Ad exhibits its superiority that improvements on ùê∫ ùëèùëúùë°ùë°ùëúùëö are achieved without the cost of degraded performance on ùê∫ ùë°ùëúùëù . Figure 6: Comparison among Rec4Ad and its variants. Rec4Ad Rec4Ad w/o SABN wlo SABN wlo Decorrelation wlo Decorrelation 0.681 w/o Alignment : 0.679 Variants Variants 4.2.3 Ablation Study. We analyze the effect of key components in Rec4Ad by comparing it with variants which remove SABN, the alignment module, and the decorrelation module, respectively. Fig. 6 shows that after removing SABN, model calibration experiences an obvious degeneration. The reason is that representations of Rec and Ad samples are with different distributions, making it difficult to normalize them with shared BN parameters and leading to mis-scaled network activations as well as badly-calibrated predictions. Furthermore, we find that AUC even drops under 0 . 679 after removing the alignment module, validating that the alignment regularization is critical for co-training with ad and rec samples. It allows Rec4Ad to extract shared user-item interest behind user clicks and eliminate system-specific confounders from this part. The decorrelation module is also shown effective since the variant without this component performs worse than the default version. It is because splitting non-causal correlations alone in enhanced representations also potentially contributes to accurate predictions [26, 33]. 4.2.4 Study on Disentangled Representations. As x ùëñùëõùë£ and x ùëêùëúùëõ are expected to capture system-invariant and system-specific factors respectively, we aim to investigate their distributions over ad and rec samples. We randomly sample a hybrid batch and visualize learned representations using t-SNE [27]. As shown in Fig. 7, there is no significant difference between x ùëñùëõùë£ for ad and rec samples, suggesting the captured invariance. When it comes to x ùëêùëúùëõ , we observe that ad samples and rec samples are mostly separated in Figure 7: T-SNE visualization of x ùëñùëõùë£ (Left) and x ùëêùëúùëõ (Right) for ad samples (red) and rec samples (blue). 60 40 20 0 20 40 60 60 40 20 0 20 40 60 60 40 20 0 20 40 60 40 20 0 20 40 60 Figure 8: Training AUC of the adversary sample classifier ùêª . 0 2500 5000 7500 10000 12500 15000 17500 Training Steps 0.500 0.525 0.550 0.575 0.600 0.625 0.650 Adversary AUC different areas, indicating that this representation extracts systemspecific factors from the training data, which we believe stems from the difference in their selection mechanisms. In the alignment module, we employs an adversary sample discriminator ùêª to distinguish x ùëéùëë ùëñùëõùë£ and x ùëüùëíùëê ùëñùëõùë£ . Thus the classification performance can be used as an effective proxy to quantitatively evaluate the goodness of x ùëñùëõùë£ . Fig. 8 illustrates the adversary AUC during training. We observe that it increases at the early stage due to optimization of ùêª . Then AUC gradually decreases as the training goes on, indicating Rec4Ad tries to generate representations that confuse ùêª . Near the end of training, AUC converges to 0.5, which means ad and rec samples are indistinguishable on x ùëñùëõùë£ .",
  "4.3 Online Study": "Table 3: Key business metrics of online A/B Test. We conduct online A/B Test between Rec4Ad and production baseline from July 1 to July 7 of 2022, each with 5% randomlyassigned traffic. Two key business metrics are used in evaluation: Click-Through Rate (CTR) and Revenue Per Mille (RPM), which corresponds to user experience and platform revenue, respectively. As shown in Table 3, Rec4Ad achieves substantial gains in two largest scenes of Taobao display advertising business, Homepage and Post-Purchase , demonstrating considerable business value of Rec4Ad. For long-tail ads with few impressions. Rec4Ad achieves Rec4Ad: A Free Lunch to Mitigate Sample Selection Bias for Ads CTR Prediction in Taobao Conference acronym 'XX, June 03-05, 2023, Woodstock, NY up to 12.6% and 3.6% lift of CTR in two scenes, which are larger than the overall lift. Above results verify that Rec4Ad effectively mitigates SSB and brings solid online improvements. It has been successfully deployed in production environment to serve the main traffic of Taobao display advertising system since July of 2022.",
  "5 CONCLUSION": "In this paper, we propose a novel framework which leverages Rec ommendation samples to help mitigate sample selection bias F or Ad s CTR prediction ( Rec4Ad ). Recommendation samples are first retrieved and mapped to pseudo samples. Ad samples and pseudo samples are jointly considered in learning disentangled representations that dissect system-specific confounders brought by selection mechanisms and system-invariant user-item interest. Alignment and decorrelation modules are included in above architecture. When deployed in Taobao display advertising system, Rec4Ad achieves substantial gains in key business metrics, with a lift of up to +6.6% CTR and +2.9% RPM.",
  "REFERENCES": "[1] Rohan Anil, Sandra Gadanho, Da Huang, Nijith Jacob, Zhuoshu Li, Dong Lin, Todd Phillips, Cristina Pop, Kevin Regan, Gil I Shamir, et al. 2022. On the Factory Floor: ML Engineering for Industrial-Scale Ads Recommendation Models. arXiv preprint arXiv:2209.05310 (2022). [2] Stephen Bonner and Flavian Vasile. 2018. Causal Embeddings for Recommendation. In Proceedings of the 12th ACM conference on recommender systems . Association for Computing Machinery, New York, NY, USA, 104-112. [3] L√©on Bottou, Jonas Peters, Joaquin Qui√±onero-Candela, Denis X Charles, D Max Chickering, Elon Portugaly, Dipankar Ray, Patrice Simard, and Ed Snelson. 2013. Counterfactual Reasoning and Learning Systems: The Example of Computational Advertising. Journal of Machine Learning Research 14, 11 (2013). [4] Dagui Chen, Junqi Jin, Weinan Zhang, Fei Pan, Lvyin Niu, Chuan Yu, Jun Wang, Han Li, Jian Xu, and Kun Gai. 2019. Learning to Advertise for Organic Traffic Maximization in E-Commerce Product Feeds. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management . 2527-2535. [5] Brian Cheung, Jesse A Livezey, Arjun K Bansal, and Bruno A Olshausen. 2014. Discovering hidden factors of variation in deep networks. arXiv preprint arXiv:1412.6583 (2014). [6] Michael Cogswell, Faruk Ahmed, Ross Girshick, Larry Zitnick, and Dhruv Batra. 2015. Reducing overfitting in deep networks by decorrelating representations. arXiv preprint arXiv:1511.06068 (2015). [7] Marco Cuturi and Arnaud Doucet. 2014. Fast computation of Wasserstein barycenters. In International conference on machine learning . PMLR, 685-693. [8] Yaroslav Ganin and Victor Lempitsky. 2015. Unsupervised domain adaptation by backpropagation. In International conference on machine learning . PMLR, 11801189. [9] Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and Wieland Brendel. 2019. ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness.. In International Conference on Learning Representations . https://openreview.net/ forum?id=Bygh9j09KX [10] Avi Goldfarb and Catherine Tucker. 2011. Online display advertising: Targeting and obtrusiveness. Marketing Science 30, 3 (2011), 389-404. [11] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Sch√∂lkopf, and Alexander Smola. 2012. A kernel two-sample test. The Journal of Machine Learning Research 13, 1 (2012), 723-773. [12] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. arXiv preprint arXiv:1703.04247 (2017). [13] James J Heckman. 1979. Sample selection bias as a specification error. Econometrica: Journal of the econometric society (1979), 153-161. [14] Ziniu Hu, Zhe Zhao, Xinyang Yi, Tiansheng Yao, Lichan Hong, Yizhou Sun, and Ed H. Chi. 2022. Improving Multi-Task Generalization via Regularizing Spurious Correlation. In Advances in Neural Information Processing Systems , Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (Eds.). https://openreview.net/forum?id=HLzjd09oRx [15] Sergey Ioffe and Christian Szegedy. 2015. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning . pmlr, 448-456. [16] Thorsten Joachims, Adith Swaminathan, and Tobias Schnabel. 2017. Unbiased learning-to-rank with biased feedback. In Proceedings of the tenth ACM international conference on web search and data mining . 781-789. [17] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014). [18] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong Sun. 2018. xdeepfm: Combining explicit and implicit feature interactions for recommender systems. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1754-1763. [19] Xiaoliang Ling, Weiwei Deng, Chen Gu, Hucheng Zhou, Cui Li, and Feng Sun. 2017. Model ensemble for click prediction in bing search ads. In Proceedings of the 26th international conference on world wide web companion . 689-698. [20] Dugang Liu, Pengxiang Cheng, Zhenhua Dong, Xiuqiang He, Weike Pan, and Zhong Ming. 2020. A General Knowledge Distillation Framework for Counterfactual Recommendation via Uniform Data. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . Association for Computing Machinery, 831-840. [21] Ning Ma, Mustafa Ispir, Yuan Li, Yongpeng Yang, Zhe Chen, Derek Zhiyuan Cheng, Lan Nie, and Kishor Barman. 2022. An Online Multi-task Learning Framework for Google Feed Ads Auction Models. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 3477-3485. [22] Zohreh Ovaisi, Ragib Ahsan, Yifan Zhang, Kathryn Vasilaky, and Elena Zheleva. 2020. Correcting for selection bias in learning-to-rank systems. In Proceedings of The Web Conference 2020 . 1863-1873. [23] Judea Pearl. 2009. Causality . Cambridge university press. [24] Tobias Schnabel, Adith Swaminathan, Ashudeep Singh, Navin Chandak, and Thorsten Joachims. 2016. Recommendations as Treatments: Debiasing Learning and Evaluation. In Proceedings of The 33rd International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 48) . PMLR, 1670-1679. [25] Xiang-Rong Sheng, Liqin Zhao, Guorui Zhou, Xinyao Ding, Binding Dai, Qiang Luo, Siran Yang, Jingshan Lv, Chi Zhang, Hongbo Deng, et al. 2021. One model to serve all: Star topology adaptive recommender for multi-domain ctr prediction. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management . 4104-4113. [26] Zihua Si, Xueran Han, Xiao Zhang, Jun Xu, Yue Yin, Yang Song, and Ji-Rong Wen. 2022. A model-agnostic causal learning framework for recommendation using search data. In Proceedings of the ACM Web Conference 2022 . 224-233. [27] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. Journal of machine learning research 9, 11 (2008). [28] Xuanhui Wang, Michael Bendersky, Donald Metzler, and Marc Najork. 2016. Learning to rank with selection bias in personal search. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval . 115-124. [29] Yixin Wang, Dawen Liang, Laurent Charlin, and David M Blei. 2020. Causal inference for recommender systems. In Fourteenth ACM Conference on Recommender Systems . 426-431. [30] Le Yan, Zhen Qin, Xuanhui Wang, Michael Bendersky, and Marc Najork. 2022. Scale Calibration of Deep Ranking Models. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 4300-4309. [31] Bowen Yuan, Jui-Yang Hsia, Meng-Yuan Yang, Hong Zhu, Chih-Yao Chang, Zhenhua Dong, and Chih-Jen Lin. 2019. Improving ad click prediction by considering non-displayed events. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management . 329-338. [32] Bianca Zadrozny. 2004. Learning and evaluating classifiers under sample selection bias. In Proceedings of the twenty-first international conference on Machine learning . 114. [33] Yang Zhang, Fuli Feng, Xiangnan He, Tianxin Wei, Chonggang Song, Guohui Ling, and Yongdong Zhang. 2021. Causal intervention for leveraging popularity bias in recommendation. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval . 11-20. [34] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through rate prediction. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1059-1068.",
  "keywords_parsed": [
    "CTR Prediction",
    "Sample Selection Bias",
    "Disentangled Representation",
    "Online Advertising",
    "System Deployment"
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "On the Factory Floor: ML Engineering for Industrial-Scale Ads Recommendation Models"
    },
    {
      "ref_id": "b2",
      "title": "Causal Embeddings for Recommendation"
    },
    {
      "ref_id": "b3",
      "title": "Counterfactual Reasoning and Learning Systems: The Example of Computational Advertising"
    },
    {
      "ref_id": "b4",
      "title": "Learning to Advertise for Organic Traffic Maximization in E-Commerce Product Feeds"
    },
    {
      "ref_id": "b5",
      "title": "Discovering hidden factors of variation in deep networks"
    },
    {
      "ref_id": "b6",
      "title": "Reducing overfitting in deep networks by decorrelating representations"
    },
    {
      "ref_id": "b7",
      "title": "Fast computation of Wasserstein barycenters"
    },
    {
      "ref_id": "b8",
      "title": "Unsupervised domain adaptation by backpropagation"
    },
    {
      "ref_id": "b9",
      "title": "ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness."
    },
    {
      "ref_id": "b10",
      "title": "Online display advertising: Targeting and obtrusiveness"
    },
    {
      "ref_id": "b11",
      "title": "A kernel two-sample test"
    },
    {
      "ref_id": "b12",
      "title": "DeepFM: a factorization-machine based neural network for CTR prediction"
    },
    {
      "ref_id": "b13",
      "title": "Sample selection bias as a specification error"
    },
    {
      "ref_id": "b14",
      "title": "Improving Multi-Task Generalization via Regularizing Spurious Correlation"
    },
    {
      "ref_id": "b15",
      "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift"
    },
    {
      "ref_id": "b16",
      "title": "Unbiased learning-to-rank with biased feedback"
    },
    {
      "ref_id": "b17",
      "title": "Adam: A method for stochastic optimization"
    },
    {
      "ref_id": "b18",
      "title": "xdeepfm: Combining explicit and implicit feature interactions for recommender systems"
    },
    {
      "ref_id": "b19",
      "title": "Model ensemble for click prediction in bing search ads"
    },
    {
      "ref_id": "b20",
      "title": "A General Knowledge Distillation Framework for Counterfactual Recommendation via Uniform Data"
    },
    {
      "ref_id": "b21",
      "title": "An Online Multi-task Learning Framework for Google Feed Ads Auction Models"
    },
    {
      "ref_id": "b22",
      "title": "Correcting for selection bias in learning-to-rank systems"
    },
    {
      "ref_id": "b23",
      "title": "Causality"
    },
    {
      "ref_id": "b24",
      "title": "Recommendations as Treatments: Debiasing Learning and Evaluation"
    },
    {
      "ref_id": "b25",
      "title": "One model to serve all: Star topology adaptive recommender for multi-domain ctr prediction"
    },
    {
      "ref_id": "b26",
      "title": "A model-agnostic causal learning framework for recommendation using search data"
    },
    {
      "ref_id": "b27",
      "title": "Visualizing data using t-SNE"
    },
    {
      "ref_id": "b28",
      "title": "Learning to rank with selection bias in personal search"
    },
    {
      "ref_id": "b29",
      "title": "Causal inference for recommender systems"
    },
    {
      "ref_id": "b30",
      "title": "Scale Calibration of Deep Ranking Models"
    },
    {
      "ref_id": "b31",
      "title": "Improving ad click prediction by considering non-displayed events"
    },
    {
      "ref_id": "b32",
      "title": "Learning and evaluating classifiers under sample selection bias"
    },
    {
      "ref_id": "b33",
      "title": "Causal intervention for leveraging popularity bias in recommendation"
    },
    {
      "ref_id": "b34",
      "title": "Deep interest network for click-through rate prediction"
    }
  ]
}