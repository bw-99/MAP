{"title": "CAPS: Context Aware Personalized POI Sequence Recommender System", "authors": "Ramesh Baral; Tao Li; Xiaolong Zhu", "pub_date": "2018-03-03", "abstract": "The revolution of World Wide Web (WWW) and smart-phone technologies have been the key-factor behind remarkable success of social networks. With the ease of availability of check-in data, the location-based social networks (LBSN) (e.g., Facebook 1 , etc.) have been heavily explored in the past decade for Point-of-Interest (POI) recommendation. Though many POI recommenders have been defined, most of them have focused on recommending a single location or an arbitrary list that is not contextually coherent. It has been cumbersome to rely on such systems when one needs a contextually coherent list of locations, that can be used for various day-to-day activities, for e.g., itinerary planning. This paper proposes a model termed as CAPS (Context Aware Personalized POI Sequence Recommender System) that generates contextually coherent POI sequences relevant to user preferences. To the best of our knowledge, CAPS is the first attempt to formulate the contextual POI sequence modeling by extending Recurrent Neural Network (RNN) and its variants. CAPS extends RNN by incorporating multiple contexts to the hidden layer and by incorporating global context (sequence features) to the hidden layers and output layer. It extends the variants of RNN (e.g., Long-short term memory (LSTM)) by incorporating multiple contexts and global features in the gate update relations. The major contributions of this paper are: (i) it models the contextual POI sequence problem by incorporating personalized user preferences through multiple constraints (e.g., categorical, social, temporal, etc.), (ii) it extends RNN to incorporate the contexts of individual item and that of whole sequence. It also extends the gated functionality of variants of RNN to incorporate the multiple contexts, and (iii) it evaluates the proposed models against two real-world data sets.", "sections": [{"heading": "INTRODUCTION", "text": "The POI recommendation systems have been very popular in the last few years. Such systems exploit the explicit historical check-in data and some implicit aspects or contexts to generate a list of POIs that are of potential interest to users. Generally, most of the POI recommenders recommend a single POI or an arbitrary list of POIs (Yuan et al. (2013); Zhang and Chow (2015)) that satisfy the personalized user preferences but may not be contextually coherent. Often, users need a list of items that are contextually coherent as per the preferences of users. Such a coherent list of recommendation can be formulated as a sequence modeling problem where the sequence of items need to adhere to users' preferences by addressing constraints that are applicable to the consecutive items and also to the whole sequence. For instance, in the case of itinerary recommendation, the distance between consecutive places, total trip time, tentative start/end time, start/end place/category, etc. can be the major concerns.\nIn comparison to the general item recommendation, the POI recommendation is more challenging as it is sensitive to various aspects (e.g., categorical, social, spatial, temporal, etc.) (Yuan et al. (2013); Zhang and Chow (2015); Baral and Li (2016); Baral et al. (2016)) and the impact of those aspects might vary on the preference of user ( Baral and Li (2017)). The POI sequence recommendation becomes even more interesting and challenging because of following major reasons:\n1. the brute-force approaches on computing all permutations of the POIs are NP-hard problem and are not useful, 2. getting a preferred list from an arbitrary list may not be optimal because the preferred list need to satisfy multiple constraints, for instance, the places in a trip should be contextually coherent, need to satisfy many constraints, such as spatial (near/far places), temporal (relevant places for a time of a day, for instance bars can be relevant at evening or night), social (as the trip might be a family, friend focused), time budget constraint of the traveler, start and end POIs, etc., 3. the contexts can be user dependent and can vary dynamically in real-time (e.g., the same context may not be always relevant to a user).\nThere are few studies that focused on POI sequence recommendation. Most of the existing systems are focused on either frequency-based user interest or the average visit durations but incorporation of major constraints (such as distance between consecutive POIs, social constraints, categorical constraints, and temporal constraints) is less explored and is of great interest in the recommendation community. This paper attempts to fill the gap by incorporating multiple constraints for POI sequence modeling.\nWe can envision some implicit relevance between POI sequence modeling and the word sequence modeling. First, the smallest elements (check-ins and words) have some contextual coherence with their neighbors. Second, most of the subsequences (check-ins made within a time duration, and a sentence) comprise of contextually coherent elements and also have contextual relation with neighbors (check-ins of preceding/succeeding time duration, and neighbor sentences). Third, the ordered sequence of elements (chronological check-ins of a user, and a text) follow some pattern which can be modeled to learn relation between subsequences (e.g., predict the next check-in and predict next word).\nBesides the aforementioned relevances, the contextual POI sequence modeling is more challenging due to the following reasons: (i) the POI sequence modeling can have longer contextual impact (e.g., unlike the language model where the n-grams can capture most of the context, the set of check-ins can be unique every day) and adhere to personalized preferences, (ii) unlike the language modeling tasks (where there is mostly a one-to-one mapping between the input and output items, e.g., language translation problems), the POI sequence modeling can have many-to-many relation (e.g., depending on the context, a user who has visited a set of locations can select multiple destinations). Sometimes the POI sequences can repeat (for instance, a user can depict same check-in pattern after some time interval) which is rare in language modeling (e.g., in next sentence prediction, the input sentences are mostly unique), and (iii) the real-time contexts can complicate the problem formulation using traditional sequence models.\nInspired from the wider popularity of RNN and its variants in sequence modeling (e.g., language modeling (machine translation) and image caption generation) (Mikolov et al. (2010); Mikolov and Zweig (2012); Ghosh et al. (2016); Bengio et al. (2003)), we attempt to cope with the challenges of contextual POI sequence modeling by incorporating the local contexts (context valid for subsequence) and global contexts (context valid for whole sequence). The local contexts (known as context now onwards in the paper) are incorporated into the recurrent module and the global contexts (known as feature of sequence now onwards in the paper) are fed to all the layers in the network. We also extend the gating mechanism of LSTM to incorporate the context and feature for personalized POI sequence modeling. To the best of our knowledge, the proposed model is the first one to address the multi-context POI sequence modeling using extended RNN and its variants.\nThe major contributions of this paper are: (i) it introduces all the major personalized user constraints (such as temporal, categorical, spatial etc.) as the influential context in the POI sequence modeling, (ii) it extends RNN and its variants to explicitly incorporate relevant contexts, and (iii) it evaluates the proposed models against two real-world datasets.", "publication_ref": ["b39", "b41", "b39", "b41", "b2", "b28", "b27", "b14", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "RELATED RESEARCH", "text": "We categorize the relevant studies using two broad notions -general data mining-based studies, and neural networkbased studies.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "General Data Mining Approaches", "text": "Most of the existing studies were focused on analysis of landmark visit patterns by exploiting the geo-spatial and temporal traces from check-ins. However, those studies mostly focused on the analysis only and ignored synthesis or recommendation of new paths. The Orienteering approach from Feillet et al. (2005) (a variant of Traveling salesman problem) was the dominating technique for earlier studies. This approach was focused on the objective of finding a  2011)), topic-modeling (Liu et al. (2011(Liu et al. ( , 2014))), and tree traversal approaches (Zhang et al. (2015)). Dunstall et al. (2003) proposed a trip planner that matched the explicit user inputs against a database to get the best matching itineraries. Tai et al. (2008) mined itineraries' data and generated sequences by implicitly incorporating users' interests via the images on Flickrfoot_0 . However, they did not focus on constructing an itinerary from scratch and also did not address the major aspects, such as the temporal influence on users and locations.\nA content-based filtering approach from Bohnert et al. (2012) exploited visitors' explicit ratings to recommend tours in a museum. Unlike a real trip, the order of recommended items and the distance between consecutive items were not focused in this system. The greedy algorithm-based approaches (De Choudhury et al. ( 2010 Most of the existing studies have exploited few contexts and have focused on personalized POI visit durations. Unlike others, our study exploits multiple contexts (temporal, spatial, categorical, and social) to generate the POI sequence. In comparison to the existing studies, this paper presents following major differences: (i) it exploits multiple contexts (temporal, spatial, categorical, and social) to model the contextual POI sequence problem, (iii) it extends the RNN by incorporating multiple contexts to efficiently model the personalized contexts in POI sequence generation. It extends the gating mechanism of variants of RNN to incorporate multiple contexts for efficient POI sequence modeling, and (iii) it evaluates the generated sequences with various relevant metrics (e.g., pairs-F1 score, diversity, displacement (see Section 4.2 for detail)) on two real-world datasets.", "publication_ref": ["b12", "b22", "b23", "b11", "b32", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Neural Network-based approaches", "text": "The Recurrent Neural Networks and its variants (e.g., Long Short-Term Memory (LSTM), Gated Recurrent Unit(GRU), etc.) were quite effective in sequence modeling problems (for instance speech recognition and machine translation) (Mikolov and Zweig (2012); Twardowski (2016); Sutskever et al. (2011)) that can exploit the dependencies between spatially correlated items (such as words in a sentence and pixels in images). Though some of the neural network-based models focused on predicting trajectories, they were more focused on collision avoidance between humans (Alahi et al. (2016)). The Social LSTM from Alahi et al. (2016) extended the approach from Graves (2013) by incorporating the spatial distance between neighbor objects and predicted the trajectory for human movement. The existing models were mainly focused on collision avoidance and ignored the constraints (such as user preferences and POI contexts) relevant to POI sequence generation.\nThough RNNs were quite popular in sequence modeling for image and text, relatively fewer studies have exploited it for the recommendation domain. A recent study from Wu et al. (2017) used the Recurrent Recommender Networks (RRN) to predict future behavioral trajectories. They exploited LSTM (Hochreiter and Schmidhuber (1997)) based model to capture the user-movie preference dynamics, in addition to a more traditional low-rank factorization. Hidasi et al. (2015) trained a GRU (Cho et al. (2014))-based model with a ranking loss to predict the next item in the user session on e-commerce system. However, they did not consider personalization. A contextual network from Smirnova and Vasile (2017) used three different techniques (concatenation, multiplication, and both) to integrate the context embedding with the input embedding and was used for next item prediction. Though some of the neural network-based recommenders exist, none of them focused on multi-context personalized POI sequence modeling.", "publication_ref": ["b27", "b33", "b31", "b0", "b0", "b37", "b18", "b16", "b9", "b30"], "figure_ref": [], "table_ref": []}, {"heading": "Methodology", "text": "In this section, we define our proposed approach. First, we define the basic concepts that are frequently used in the paper and then we elaborate the incorporation of contexts into RNN and its variants.\nContext : A context of a check-in represents the current and previous scenarios which have direct or indirect influence on the selection of next POI. Such a context (for instance current time, category of current place, category of previous place, popularity score of current place, and so forth.) can be represented as a high-dimensional vector.\nContext-aware POI sequence : Given a set of contexts C = {c i }, where i \u2208 K, is a context type, our objective is to predict or generate a sequence of POIs, that are most relevant to the given context and match the user preferences.\nGiven a user u who has made n check-ins, we define her travel history as an ordered sequence,\nH u = (V 1 , V 2 , ..., V n )\n, where V i is a check-in activity and can be represented by the triplet V i = (l i , a i , d i ) indicating the location (l i ) of the check-in, arrival time (a i ) to l i , and the departure time (d i ) from l i respectively. The travel sequences can be split into small subsequences (for instance, the check-ins made within a time interval, e.g., 8 hours, one day, etc.). As in earlier studies (Lim et al. (2015); De Choudhury et al. ( 2010)), we use the time interval of 8 hours.\nVisit duration of POI : The average visit duration (stay time (ST)) of a POI i is defined as:\nST (i) = 1 | U | u\u2208U V u \u2208H u V u .l=i 1 | V u,i | l\u2208V u,i (a l+1 -T T (l, l + 1) -a l ), (1\n)\nwhere U is the set of all users, V u,l is the set of visits made by the user u to location l, TT(a,b) is the travel time between POI a and POI b. We can use a log normal distribution to compute the travel time between two POIs visited consecutively. The stay time is 0-1 normalized and is represented as ST (i).\nWe define the user interest to a place in terms of aggregate of stay time (AST) to that place. This term in turn relies on the visit frequency and the stay time to that place, and the stay time to the places of same category.\nAST (u, i) cat = (1 -\u03b1) * F st (u, i) + \u03b1 * G st (u, i) l\u2208V u l.cat=i.cat ST (l) V u,l , F st (u, i) = ST (i) V u,i if | V u,i |> 0, i.e.\nu has some visits to location i,\nF st (u, i) = 0 otherwise, G st (u, i) = 1 l\u2208V u l.cat=i.cat 1 if \u2203V u,l \u2227 l.cat = i.cat, i.e.\n, u has some check-ins on category i.cat ,\nG st (u, i) = 0 otherwise,(2)\nwhere V u is the set of visits by user u, V u,l is the set of visits by user u to location l,\nV u,l = |V u,l | |V u |\nis the normalized visit count of user u to location l, l.cat is the category of location l, and \u03b1 is a constant tuning factor estimated using the fraction of check-ins that are of same category as location i . After incorporating social impact, the average stay time on a location i can be defined as:\nAST (u, i) = (1 -\u03c8 1 ) * AST (u, i) cat + \u03c8 * J (F u ) k\u2208F u AST (k, i) cat , J (F u ) = 1 | F u | if | F u |> 0 , i.e. u has some friends, J (F u ) = 0 otherwise,(3)\nwhere F u represents the set of friends of user u, \u03c8 1 is a constant tuning parameter estimated using the fraction of check-ins of the user u that are common to her friends. Similarly, the average stay time by a user u to a location category 'cat' can be defined as:\nAST (u) cat = (1 -\u03b3 1 ) * ( i\u2208V u i.cat=cat AST (u, i) cat ) + \u03b3 1 * ( j\u2208F u k\u2208V j k.cat=cat AST (j, k) cat ),(4)\nwhere \u03b3 1 is a tuning factor estimated using the fraction of check-ins of user u that are common to her friends and have category 'cat'. AST cat is the aggregate of average stay on the category 'cat' from all users and AST t cat gives the measure for time t.\nPreference score of POI : For a user u, the preference score (PS) of a place l at time t is defined using the following relation:\nP S(u, l, t) = \u03b2 * {(1 -\u03b8) * P(u, l) * | V u,l,t | +\u03b8 * Q(u, l) l \u2208L l .cat=l.cat | V u,l ,t | | V u,l | } + (1 -\u03b2) * AST (u, l), P(u, l) = 1 | V u,l | if | V u,l |> 0, P(u, l) = 0 otherwise, Q(u, l) = 1 l\u2208V u l.cat=i.cat 1 if \u2203V u,l \u2227 l.cat = i.cat, i.e.\n, u has some check-ins on category i.cat ,\nQ(u, l) = 0 otherwise,(5)\nwhere V u,l,t is the set of visits made by user u to location l at time t, L is the set of all locations, \u03b8 can be estimated as in Equation 2, and \u03b2 is a tuning factor estimated using TF-IDF (term frequency inverse document frequency) (Salton and McGill (1986)). This relation addresses the trade-offs between visit frequency and stay time which is crucial to reward the preferred check-ins with low frequency but reasonable stay time. The generalized preference score P S(l, t) can be easily derived from above relation by considering the visit frequencies of all the users and stay time of all the users to this location at time t. Furthermore, the usage of categorical and the social contribution can handle the cold-start items (items with no check-ins information) and cold-start users (users with no check-ins) to some extent as the relations defined above incorporate some contributions from categorical and social factors. We also handle the scenario in which places with higher preference scores are compromised due to other constraints, such as travel time and cost. To address the trade-off between constraints and preference score, we define a consolidated preference score as:\nP (u, l, t) = P S(u, l, t) * (1 - 1 m m i=1 Constraint i (l, p)),(6)\nwhere Constraint i (l, p) is a normalized numeric measure of i th constraint between the users' current location p and the target location l. For instance, the spatial constraint is the measure of distance between locations p and l which is min-max normalized by the minimum and maximum distance traveled by any user to reach location l from any other location. The above mentioned preference metrics are used as features and attributes (defined later) when we train our prediction model.", "publication_ref": ["b20", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "Network Design", "text": "We exploit RNNs because they are ideal to our problem due to the following reasons: (i)  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries of proposed Network", "text": "RNNs are ideal for sequence modeling problem because they can efficiently model the generative process of sequential information, can summarize the information in hidden states, and can generate new sequences by using the probability distribution specified by the hidden states. The hidden state h t from the input sequence (x 1 , x 2 , ...., x t ) is recursively updated as h t = f (h t-1 , x t ), where f (., .) is a non-linear transformation function (for e.g.,\nh t = tanh(U h t-1 + V x t ),\nwhere U and V are weight matrices). The overall probability of a sequence -\u2192 x = x 1 , x 2 , ....., x N can be defined as:\np( -\u2192 x ) = N t=1 p(x t | h t-1 ). (7\n)\nTraining such a regular RNN by maximizing the above function suffers from vanishing gradient (i.e., a problem with gradient based learning methods where the gradient (error signal) decreases exponentially with the value of n (number of layers) while the front layers train very slowly, see Hochreiter (1998) for further detail). While the architectural enhancements, such as long short-term memory (LSTM) from Hochreiter and Schmidhuber (1997) and Gated Recurrent Unit (GRU) from Cho et al. (2014) are believed to address the vanishing gradient problem, the context enriched models (Mikolov et al. (2010); Mikolov and Zweig (2012)) are also claimed as an effective solution. We formulate the contextual POI sequence modeling by incorporating the contexts into RNN and its variants.  3. The input vector and the output vector both have the dimension of the number of POIs. The input vector w(t) is the check-in at time t (which is a one hot encoding), and the output layer produces a probability distribution over the POIs, given the context, feature and the previous check-in. The hidden layers of RNNs are responsible to maintain the history of check-ins that form the sequence. Inspired from Mikolov and Zweig (2012), we extend the vanilla RNN to incorporate the context and feature inputs (f(t)). The context and the attribute of the current input, the current input itself, and the features representing the whole sequence are fed to the network. The features are propagated to the hidden as well as to the output layers. This helps us to retain the important features which the network will be aware at each and every iteration (see Figure 2(b)).\nC_(t-1) A_t POI_1 POI_2 POI_3 <eot> POI_2 <eot> POI_3 A_2 W U f(t) W U U A_3 W G F F F W A_1 b y_t F U b", "publication_ref": ["b17", "b18", "b9", "b28", "b27", "b27"], "figure_ref": ["fig_3", "fig_2"], "table_ref": []}, {"heading": "Basic POI Sequence Modeling", "text": "In this model, we use the regular RNN which is popular for modeling the sequential data. The capability of maintaining the session information into the recurrent states are often considered as its strength. Although they are theoretically claimed as poor candidates for problem with long session relations (due to vanishing gradient problem), architectural extensions (e.g., LSTM) or contextual extensions (e.g., Mikolov and Zweig (2012)) are some of the state-of-art techniques in language modeling domain.\nWith respect to POI sequence modeling, the RNNs can use the probability distribution specified by the hidden state to predict the next POI. The hidden state summarizes the information of the sequence observed so far (e.g., l 1 , l 2 , ...., l t-1 ). A hidden layer depends on the current input (x t ) and the input at previous step (h t-1 ), i.e. h t = f (W xh x t + W hh h t-1 ), where W xh and W hh are the weights for the current input, and for the value from previous hidden layer respectively. The probability of a POI sequence can then be defined using the chain rule:\np( -\u2192 l ) = T t=1 p(l t | h t-1 ), (8\n)\nwhere T is the number of timesteps to be considered. The negative log likelihood is used as the sequence loss to train the network:\nL(x) = - T t=1 log(p(l t | h t-1 )). (9\n)\nFor the defined network, the partial derivative of the loss can be computed using the backpropagation through time (Williams and Zipser (1995)) and the network can be trained using gradient descent. Such a sequence modeling is illustrated in Figure 2(a). This network predicts the next POI by learning from the previously fed POI sequences only and does not consider other information, such as POI attributes, current context, and other relevant features. The contextual model defined in next subsection addresses these aspects.  3. CAPS-RNN extends the regular RNN by incorporating the context to hidden layers and the feature (f(t)) to the hidden and output layers. This extension has following major advantages: (i) as the regular RNNs have no provision of explicit context propagation and as the POI sequence modeling requires the contexts to be remembered for longer span, the extension helps us to propagate the relevant contexts to make sure that the next predicted subsequence adheres to the context, (ii) the regular RNNs suffer from vanishing gradient problem (i.e. during backpropagation, the gradients decrease exponentially with the value of n (number of layers) and the front layers train very slowly, see Hochreiter (1998) for further detail). The explicit feeding of contexts to the hidden layer and the features to hidden and output layers helps alleviate the gradients, meanwhile retaining the relevant context (see Mikolov and Zweig (2012) for detail).\nThe feature contains information that is relevant to the current input and also to the whole sequence, for instance with respect to POI sequence modeling, it can be the time budget constraint, start/end place categories, the geographical vicinity of interest, etc. These are the auxiliary information that can have significant impact on the prediction tasks. The context represents the aspect that is relevant to the current input (for instance, the current time, previous check-ins which impact the current check-in). In addition to context and feature, the attribute vector which represents any information that is relevant to the current item (for e.g., the locations' category, locations' hours, locations' popularity, locations' preference score, and locations' average stay time) is also incorporated to the network. For the sake of ease, the attribute and context vectors can be combined together.\nAfter we train the network (using stochastic gradient descent), the output vector y(t) gives the probability measure of different POIs at time t, given the previous POI, context, and the feature vector. The hidden and output layers are updated using the following relations:\nc(t) = H(Uw(t) + W(c(t -1) A(t)) + Ff (t)),(10) \u0177t\n= Vc(t) + Gf (t),(11)\ny(t) = g(\u0177 t ),(12)\nwhere H(.) is a non-linear function (e.g., tanh), A(t) is the context attribute vector of current item for time t (see Eqn. 14), f(t) is the sequence feature vector (see Eqn. 13), and g(a i ) = exp(a i ) j exp(a j ) is the softmax function. The sequence feature vector contains the information relevant to the whole sequence. We use the feature vector of following format:\nf (t) =< cat start , cat end , loc start , loc end , loc dist , time start , time end >,(13)\nwhere cat start is category of starting place, cat end is category of ending place, loc start is starting place, loc end is ending place, loc dist is the distance between consecutive places, time start is starting hour, and time end is ending hour of the POI sequence. All the non-numeric elements of this vector are label encoded before feeding to the network.\nThe attribute vector contains the information relevant to the current item and current context. We use the following format for the attribute vector:\nA(l, t) =< ST (l), AST (l), AST t cat , P S(l), l.cat, l T , l dist >,\nwhere l T = l 1 , l 2 , ..., l T is the temporal popularity of location l for each hour, 'cat' =l.cat is category of location l, and l dist is the distance of location l from the previous location in the sequence. All the non-numeric elements of this vector are label encoded. The feature vector and the attribute vector can be used to incorporate additional features and attributes if required. The network is trained to learn the weight matrices (U, V, W, F, and G), and to maximize the likelihood of the training data (Mikolov (2012); Bengio et al. (2003)). The probability of a POI sequence l for the network can be defined as:\np(l) = T t=1 p(l t+1 | y t ).\n(15)", "publication_ref": ["b27", "b36", "b17", "b27", "b26", "b4"], "figure_ref": ["fig_2", "fig_3"], "table_ref": []}, {"heading": "Contextual POI Sequence modeling using LSTM", "text": "This model is termed as CAPS-LSTM. The core idea behind LSTM is the introduction of memory state and multiple gating functions to control the write, read, and removal (forget) of the information written on memory state. The information is propagated by applying these gates to the input data and data from the previous memory states.\nWe incorporate the explicit context to each LSTM cell because each cell models a subsequence and each subsequence can have potentially unique context. This explicit context to each LSTM cell and the feature of sequence to all LSTM cells help us propagate the relevant context for the sequence modeling. Due to space constraint, we only provide the update equations of our extended LSTM which incorporates the contextual information. The hidden layer of contextual LSTMs are updated using the following relations:\ni t = \u03c3(W i x t + W hi (h t-1 A t ) + W ci c t-1 + b i + W f F ), f t = \u03c3(W f x t + W hf (h t-1 A t ) + W cf c t-1 + b f + W f F ), z t = tanh(W z x t + W hc (h t-1 A t ) + b z + W f F ), c t = f t c t-1 + i t z t , o t = \u03c3(W o x t + W ho (h t-1 A t ) + W co c t-1 + b o + W f F ), h t = o t tanh(c t ). (16\n)\nwhere c t is the memory state, z t is the module that transforms information from input space x t to the memory space, and h t is the information read from the memory state. The input gate i t controls information from input z t to memory state, the forget gate f t controls information in the memory state to be forgotten, and the output gate o t controls information read from the memory state. The memory state c t is updated through a linear combination of input filtered by the input gate and the previous memory state filtered by the forget gate. The term W f is feature weight matrix, F is feature vector, A t is attribute vector as in the contextual RNN model, and is element-wise product operator. The relevant weight matrices W and biases b are subscripted accordingly.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sequence generation", "text": "After the model is trained, it can generate the personalized POI sequences. The basic idea is to train the network using a sequence data one step at a time. The sampled output from the network is then fed as the input for the next level. If we have k different POIs, and k th POI is checked-in at time t, then the input x t is one-hot encoded vector with only the k th entry set to 1. The output of the network is a multinomial distribution which is parameterized using a softmax function and can be defined as:\np(x t+1 = k | y t ) = y k t = exp(\u0177 k t ) K k =1 exp(\u0177 k t ) . (17\n)\nFrom the generated sequences, the top-k scorers (sum of preference scores (AS) of all places in a sequence) are recommended to user.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation", "text": "In this section, we describe the dataset, the evaluation baselines, evaluation metrics, and the experimental results.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Dataset", "text": "We used two real-world datasets collected from two popular LBSNs -Gowalla and Weeplaces (Liu et al. (2013)). These datasets were well defined and have the attributes relevant to the context of this paper, such as (i) the location category, (ii) geospatial co-ordinates, (iii) friendship information, and (iv) check-in time. The statistics of the dataset is summarized in Table 1. After avoiding incomplete records, the 5 most checked-in categories (and their check-in count) were: (i) Home/Work/Other: Corporate/Office (437,824), (ii) Food: Coffee Shop (267,589), (iii) Nightlife:Bar (248,565), (iv) Shop: Food& Drink:Grocery/Supermar (161,016),and (v) Travel: Train Station (152,114) for Weeplaces, and (i) Corporate Office (1,750,707), (ii) Coffee Shop (1,063,961), (iii) Mall (958,285), (iv) Grocery (884,557), and (iv) Gas & Automotive (863,199) for the Gowalla dataset. The work or home-related category (Home/Work/ Other:Corporate/Office) was popular from 6 am to 6 pm, with the highest check-ins (42,019) made at 1 pm. Similarly, the bars had highest of 21,806 check-ins at 2 am and the lowest check-ins (15,209) at 5 am. Most of the check-ins were at 12 pm -6 pm and were either in Home or Work related categories.\nThe check-in distribution of all users in the Weeplaces and Gowalla dataset is illustrated in Figure 1. Figure 8(a) and Figure 8(b) show that the frequency of daily check-ins is <50 for most of the users. This implies that most of the users have daily sequence length that is reasonable and exploiting the proposed model within this sequence length is enough to evaluate its performance. Among the many factors influencing the check-in trend of the users, distance measure is one of the major factors. Figure 5 illustrates the inverse relation between the distance of a location and likelihood of check-ins (i.e. most users preferred near places (\u22641 K.m.)). This implies that most of the users prefer to visit near places and hence the POIs within a sequence should also be within a reasonable distance. The three days' check-in distribution of three users with most check-ins in Weeplaces dataset is illustrated in Figure 6. Three different color marks are used to distinguish the check-in of different days. The overlapping check-ins on different days are overlapped in the map and are not distinguishable. This figure also illustrates that most of the users prefer check-ins to near locations and most of the users have reasonably smaller sequence length.  Unlike other studies, we did not split the dataset into different cities because we found that most of the users have check-ins spanned across multiple cities and splitting the dataset into multiple cities can disrupt the coherence of the places the users have in the dataset. The check-ins were chronologically sorted and for every user, the check-ins were splitted into subsequence of every 8 hours.  ", "publication_ref": ["b24"], "figure_ref": ["fig_11", "fig_11", "fig_8"], "table_ref": ["tab_1"]}, {"heading": "Evaluation Metrics", "text": "It is difficult to measure the correctness of the order of items using simple precision, recall, and F-score. We use the pairs-F1 metrics defined in Chen et al. (2016). It considers both the POI identity and its order by using the F1 score of every pair of POIs in a sequence and is defined as:\npairs -F 1 = 2 * P P AIR * R P AIR P P AIR + R P AIR ,(18)\nwhere P P AIR and R P AIR are the precision and recall of the ordered POI pairs respectively. We also evaluate the diversity (Bradley and Smyth ( 2001)) of locations in the sequence. The diversity of locations is measured using their categorical similarity (i.e. Similarity =1 if two places are of same category and Similarity = 0 otherwise). We use the following relation to define the diversity of items in the list:\nDiversity (c 1 , c 2 , ..., c n ) = n i=1 n j=i+1 (1 -Similarity (c i , c j )) n 2 * (n -1) (19)\nWe use the displacement metric to measures the distance (in K.m.) between the predicted POIs and the actual POIs in the sequence. It is defined as:\nDisplacement (seq a , seq e ) = k i=1 | Distance(seq a i , seq e i ) |,(20)\nwhere seq a and seq e are the actual and estimated sequences respectively.", "publication_ref": ["b8"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation Baselines", "text": "We compared our proposed method with the following baseline models:\n1. POI Popularity: This is a naive approach that relies on the popularity of POIs. For a given location, an area within a predefined radius is used to find the most popular POI (i.e. most visits in the locality) within that area which is not already included in the list. The radius is dynamically updated by a predefined factor when no location is found in the area. 2. Markov Chain-based approach: We use first order Markov Chain to generate the POI sequences. The Laplace smoothed state-transition matrix and initial probability matrix are derived from the check-in data and are personalized for each user. 3. Apriori-based approach: The most frequently checked-in place of a user is considered as the starting point because the model does not have provision of user inputs. From the starting point, we select the places that are within some threshold distance ( ) to get the candidate 1-sets. The candidate 1-sets are used to get the other candidate sets.\nAll the candidates that do not satisfy the constraints are pruned. The constraint checking procedure and candidate generation procedure continues till the trip of desired length is obtained or the candidate sets are exhausted. Every trip that has greater than 8 hours travel time are pruned. Basically, we adapt a greedy pruning approach to get rid of the less preferred routes. Among the available candidate sets, we select the one that satisfies the following criteria: (i) has higher trip score, and (ii) has lower travel time (see Eqn 6). The trip score is calculated by adding the preference scores (see Eqn. 5 and Eqn. 6) of all the places in the trip. The top-k trips with highest scores are recommended to the user. Some of the existing studies (Lu et al. (2012); Yu et al. (2016)) have also exploited the Apriori-based approach. 4. HITS-based approach (Zheng and Xie (2011)): In this, the locations are hierarchically organized into clusters/regions and the hub scores of user and authority scores of places are generated relevant to the regions. This facilitates in modeling the popularity of locations and users within a region. The inference is made by using the adjacent matrix between users and locations with respect to the region. The score of a sequence is determined using the hub scores of users who visited the sequence, and the authority scores of places weighted by the probability that people would consider the sequence. The top k popular sequences are recommended.", "publication_ref": ["b25", "b38", "b42"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental settings", "text": "We used a 5-fold cross validation to measure the performance of the models. An Ubuntu 14.04.5 LTS, 32 GB RAM, a Quadcore Intel(R) Core(TM) i7-3820 CPU @ 3.60 GHz was used to evaluate the models. The same configuration with a Tesla K20c 6 GB GPU was used to evaluate the neural network based models. We used Pythonfoot_1 as the programming platform, Numpyfoot_2 as the mathematical computation library, Pandasfoot_3 as data analysis library, and TensorFlowfoot_4 as the neural network library.\nThe users with less than 25 check-ins were ignored. The context and feature vector were estimated from the historical check-ins of the users. For each user, 7 most frequently checked-in places were taken as starting point, and 10 sequence per starting point was generated. The average metrics on these 10 sequences was observed. The POI-Popularity and Apriori models used distance threshold of 2 K.m. The contextual LSTM used 512 hidden states, and the contextual RNN used 5 layers and 256 nodes. The input sequence length was set to 25, the data was fed in batches of size 50, embedding vectors were of size 384, and the experiment was repeated for 100 epochs. The learning rate was set to 0.002, and the gradients were clipped at 5 to prevent overfitting.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Results and Discussions", "text": "The pair-wise precision, recall, and F-score of different models is illustrated in Table 2 andTable 3. The diversity and displacement-based performance is illustrated in Table 4 andTable 5 The popularity-based model performed worst among all the models. It generated almost similar sequences for all the users and hence was not relevant to personalized preferences. This might be due to the ignorance of the personalized preferences of user. The diversity measure was also quite low, which means the POIs in the generated sequences included few categories. The high displacement metrics indicates that the predicted POIs were far from the actual ones.\nThe Apriori-based model had better performance than popularity-based model. Although the Apriori-based model pruned irrelevant candidate sequences, it also did not capture the personalization aspect. This might be the reason behind its low performance. The diversity and displacement measures were also better than the popularity-based model.  The first-order Markov model relied on one previous check-in data to determine next location and hence was not able to fully model the check-in sequence generation process. However, its pair-Fscore, diversity and displacement metrics were better than popularity-based and Apriori-based models which is due to the personalization implied from separate initial-probability and state-transition tables for each user.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2", "tab_3", "tab_4", "tab_5"]}, {"heading": "Models", "text": "The HITS-based model slightly outperformed Markov-based model on all three metrics. As it relies on segregation of places into regions and finding the authority and hub scores of the places and users within that regions, its performance depends on the region generation approach. We used a radius of 10 K.m. from a specified location to generate such regions. Its performance with the radius of 5 K.m. and 15 K.m. was in par with popularity-based model.\nThough not sophisticated, the performance of vanilla RNN was in par with the HITS model. This might be because of its capability to retain information of previous items from the sequence. The regular LSTM performed slightly better than vanilla RNN due to its implicit ability to cope with vanishing gradient problem.\nThe CAPS-LSTM model was best performer in both dataset. Its performance was slightly better than CAPS-RNN in all the cases except the diversity metrics in Weeplaces dataset where CAPS-RNN was slightly better. The performance of CAPS-LSTM was significant in larger dataset (i.e. Gowalla). This is obvious because the Gowalla dataset is larger, and has more social (friendship) relations, which favored the neural networks that need lots of training data for better performance. We used the threshold of 6, 8, and 10 hours and found that the threshold of 8 hours performed better than others. This might however, vary on the dataset used. In terms of execution time, CAPS-LSTM was slower as it used multiple epochs and required lots of training time. The execution time of different models were in the order of: CAPS-LSTM \u2265 CAPS-RNN > LSTM \u2265 vanilla RNN > HITS > Apriori \u2265 Markov > Popularity-based.\nTo summarize, the CAPS-LSTM performed best, followed by the CAPS-RNN in all three metrics. This evaluation result supports our claim that extension of RNN and it's variants by incorporating item-wise contexts and sequencewise features can be efficient for sequence modeling.   A good recommendation should include items with different category that satisfy the user preferences. We can see that the CAPS-RNN and CAPS-LSTM have better diversity trend with the increasing sequence length. It is most likely to have locations of different category when sequence length is increased. This effect is clearly reflected in all neural network-based models in both datasets. The popularity model always recommends popular items and does not focus on the diversity, hence its diversity is lowest of all the models. Although the Apriori, Markov, and HITS models perform better than popularity-based model, their performance is lower than that of the neural network-based models. The increase in diversity was quite slow as we increase the sequence length. This might be because of the fine grained categories which cover only few places and most of the broader categories cover many items and after some sequence length, the newly added places overlap with the category of places already in the sequence. Figure 8 illustrates the impact of sequence length on the displacement. As the popularity model recommends only the popular items no matter how far they are, it has highest displacement among all the models. The displacement is increasing with the increasing sequence length. The CAPS-RNN and CAPS-LSTM show similar effect of sequence length on the displacement. The other models have reasonable displacement until we reach the sequence of length 25. For all the models, the increase in displacement is sharp after sequence length of 25. This is also supported by the check-in trend presented in Figure 4 which shows that the daily check-in frequency of a user is \u223c25 and as we increase the sequence length, the check-ins are most likely to be made on another day might be in another city or some place farther than the one checked-in on previous days). As the CAPS-RNN and CAPS-LSTM have better performance for subsequences, they have better performance over other models.", "publication_ref": [], "figure_ref": ["fig_11", "fig_6"], "table_ref": []}, {"heading": "Case study on generated trajectories", "text": "In this section, we provide a case study on POI sequences generated by popularity-based approach and CAPS-LSTM on Gowalla dataset. We select sequences of length 5 for two different users 'thadd-fiala', 'boon-yap' (known as u 1 , u 2 in rest of the paper) with most check-ins and analyze the relevance of the sequences for them. For u 1 , a sequence of length 5 from popularity model is {'sycamore-place-lofts-cincinnati', 'pg-gardens-cincinnati', 'lytle-parkcincinnati', 'piatt-park-cincinnati', 'sycamore-place-at-st-xavier-park-apartments-cincin'}, and their respective categories are {'Home/Work/Other:Home', 'Parks & Outdoors:Plaza / Square', 'Parks & Outdoors:Park', 'Parks & Outdoors:Plaza / Square', 'Home/Work/Other:Home'}. Similarly for user u 2 a length 5 sequence is {'starbucks-boston', 'mbta-south-station-boston', 'boston-common-boston', 'dunkin-donuts-boston', 'mbta-park-street-station-boston'}and their respective categories are {'Food:Coffee Shop', 'Travel:Train Station', 'Parks & Outdoors:Park', 'Food:Donuts', 'Travel:Train Station'}. Most of the places recommended are the popular ones and the generated sequences have less diversity. For both users, there are three different categories in the generated sequences. With the increasing sequence length, the diversity shows some increasing trend (see Figure 7) but this is lower in both datasets.\nWith CAPS-LSTM, a sequence generated for user u 1 is {'sycamore-place-lofts-cincinnati', 'pg-gardens-cincinnati', 'piatt-park-cincinnati', 'lytle-park-cincinnati','lpk-cincinnati'}and their categories are {'Home/Work/ Other: Home', 'Parks & Outdoors:Plaza/Square', 'Parks & Outdoors:Plaza/Square', 'Parks & Outdoors:Park', 'Home/Work/ Other: Corporate/ Office'}. For user u 2 a sequence is {'starbucks-boston', 'mbta-park-street-station-boston', 'boston-commonboston', 'digitas-boston-boston', 'hubspot-cambridge'}and their categories are {'Food:Coffee Shop', 'Travel:Train Station', 'Parks & Outdoors: Park', 'Nightlife: Speakeasy / Secret Spot', 'Home/Work/Other: Corporate/ Office'}. We can observe that for both users, there are four different categories in the sequence and the recommendation is more contextual. With the increasing sequence length, the diversity shows some increasing trend (see Figure 7) which is better with CAPS-LSTM among all other models.\nWith the popularity-based model, the average displacement of above sequence was 19.36 K.M. for user u 1 and it was 20.03 K.M. for user u 2 . With the CAPS-LSTM, the average displacement of above sequence was 5.02 K.M for user u 1 and it was 5.61 K.M. for user u 2 . This shows that CAPS-LSTM addresses the distance constraint better for both users. With the increasing sequence length, the displacement trend increased for both models and followed the trend as shown in Figure 8.\nLimitations The deep models need lots of training data and the valid check-in sequences (the sequences that have minimum number of check-ins) that we used might still be insufficient to exploit the full potential of the proposed model. As the model needs to be trained offline, the model in its current state might not be efficient for real-time prediction. The threshold of 8 hours might not be equally applicable for all users, all types of trips, and might vary across different datasets.", "publication_ref": [], "figure_ref": ["fig_10", "fig_10", "fig_11"], "table_ref": []}, {"heading": "Conclusion and Future Work", "text": "We formulated the contextual personalized POI sequence modeling problem by extending the recurrent neural networks and its variants. We incorporated different contexts (such as social, temporal, categorical, and spatial) by feeding them to the hidden layer and the output layer. We propagated the feature vector to all the layers of the network and retained the contextual information that was valid throughout the sequence. We evaluated the proposed model with two realworld datasets and demonstrated that the contextual models can perform better than regular models. The proposed model performed slightly better with larger datasets. There are many interesting directions to explore. We would like to incorporate the textual attributes (e.g., tags, tips, and review text) and also visual information (e.g., image of places) to define the preference of users and popularity of places. We would also like to explore other datasets for sequence", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Social lstm: Human trajectory prediction in crowded spaces", "journal": "", "year": "2016", "authors": "A Alahi; K Goel; V Ramanathan; A Robicquet; L Fei-Fei; S Savarese"}, {"ref_id": "b1", "title": "Maps: A multi aspect personalized poi recommender system", "journal": "ACM", "year": "2016", "authors": "R Baral; T Li"}, {"ref_id": "b2", "title": "Exploiting the roles of aspects in personalized poi recommender systems", "journal": "Data Mining and Knowledge Discovery", "year": "2017", "authors": "R Baral; T Li"}, {"ref_id": "b3", "title": "Geotecs: exploiting geographical, temporal, categorical and social aspects for personalized poi recommendation", "journal": "IEEE", "year": "2016", "authors": "R Baral; D Wang; T Li; S.-C Chen"}, {"ref_id": "b4", "title": "A neural probabilistic language model", "journal": "Journal of machine learning research", "year": "2003-02", "authors": "Y Bengio; R Ducharme; P Vincent; C Jauvin"}, {"ref_id": "b5", "title": "Geckommender: Personalised theme and tour recommendations for museums", "journal": "Springer", "year": "2012", "authors": "F Bohnert; I Zukerman; J Laures"}, {"ref_id": "b6", "title": "Improving recommendation diversity", "journal": "", "year": "2001", "authors": "K Springer; B Bradley;  Smyth"}, {"ref_id": "b7", "title": "Tripplanner: Personalized trip planning leveraging heterogeneous crowdsourced digital footprints", "journal": "IEEE Transactions on Intelligent Transportation Systems", "year": "2015", "authors": "C Chen; D Zhang; B Guo; X Ma; G Pan; Z Wu"}, {"ref_id": "b8", "title": "Learning Points and Routes to Recommend Trajectories", "journal": "ACM", "year": "2016", "authors": "D Chen; C S Ong; L Xie"}, {"ref_id": "b9", "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "journal": "", "year": "2014", "authors": "K Cho; B Van Merri\u00ebnboer; C Gulcehre; D Bahdanau; F Bougares; H Schwenk; Y Bengio"}, {"ref_id": "b10", "title": "Automatic construction of travel itineraries using social breadcrumbs", "journal": "ACM", "year": "2010", "authors": "M De Choudhury; M Feldman; S Amer-Yahia; N Golbandi; R Lempel; C Yu"}, {"ref_id": "b11", "title": "An automated itinerary planning system for holiday travel", "journal": "Information Technology & Tourism", "year": "2003", "authors": "S Dunstall; M E Horn; P Kilby; M Krishnamoorthy; B Owens; D Sier; S Thiebaux"}, {"ref_id": "b12", "title": "Traveling salesman problems with profits", "journal": "Transportation science", "year": "2005", "authors": "D Feillet; P Dejax; M Gendreau"}, {"ref_id": "b13", "title": "Cost-aware travel tour recommendation", "journal": "ACM", "year": "2011", "authors": "Y Ge; Q Liu; H Xiong; A Tuzhilin; J Chen"}, {"ref_id": "b14", "title": "Contextual lstm (clstm) models for large scale nlp tasks", "journal": "", "year": "2016", "authors": "S Ghosh; O Vinyals; B Strope; S Roy; T Dean; L Heck"}, {"ref_id": "b15", "title": "Generating sequences with recurrent neural networks", "journal": "", "year": "2013", "authors": "A Graves"}, {"ref_id": "b16", "title": "Session-based recommendations with recurrent neural networks", "journal": "", "year": "2015", "authors": "B Hidasi; A Karatzoglou; L Baltrunas; D Tikk"}, {"ref_id": "b17", "title": "The vanishing gradient problem during learning recurrent neural nets and problem solutions", "journal": "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems", "year": "1998", "authors": "S Hochreiter"}, {"ref_id": "b18", "title": "Long short-term memory", "journal": "Neural computation", "year": "1997", "authors": "S Hochreiter; J Schmidhuber"}, {"ref_id": "b19", "title": "Personalized travel sequence recommendation on multi-source big social media", "journal": "IEEE Transactions on Big Data", "year": "2016", "authors": "S Jiang; X Qian; T Mei; Y Fu"}, {"ref_id": "b20", "title": "Personalized Tour Recommendation Based on User Interests and Points of Interest Visit Durations", "journal": "", "year": "2015", "authors": "K H Lim; J Chan; C Leckie; S Karunasekera"}, {"ref_id": "b21", "title": "Personalized trip recommendation for tourists based on user interests, points of interest visit durations and visit recency", "journal": "Knowledge and Information Systems", "year": "2017", "authors": "K H Lim; J Chan; C Leckie; S Karunasekera"}, {"ref_id": "b22", "title": "Personalized travel package recommendation", "journal": "IEEE", "year": "2011", "authors": "Q Liu; Y Ge; Z Li; E Chen; H Xiong"}, {"ref_id": "b23", "title": "A cocktail approach for travel package recommendation", "journal": "IEEE Transactions on Knowledge and Data Engineering", "year": "2014", "authors": "Q Liu; E Chen; H Xiong; Y Ge; Z Li; X Wu"}, {"ref_id": "b24", "title": "Personalized point-of-interest recommendation by mining users' preference transition", "journal": "ACM", "year": "2013", "authors": "X Liu; Y Liu; K Aberer; C Miao"}, {"ref_id": "b25", "title": "Personalized trip recommendation with multiple constraints by mining user check-in behaviors", "journal": "ACM", "year": "2012", "authors": "E H ; -C Lu; C.-Y Chen; V S Tseng"}, {"ref_id": "b26", "title": "Statistical language models based on neural networks", "journal": "", "year": "2012-04-02", "authors": "T Mikolov"}, {"ref_id": "b27", "title": "Context dependent recurrent neural network language model", "journal": "SLT", "year": "2012", "authors": "T Mikolov; G Zweig"}, {"ref_id": "b28", "title": "Recurrent neural network based language model", "journal": "", "year": "2010", "authors": "T Mikolov; M Karafi\u00e1t; L Burget; J Cernock\u1ef3; S Khudanpur"}, {"ref_id": "b29", "title": "Introduction to modern information retrieval", "journal": "", "year": "1986", "authors": "G Salton; M J Mcgill"}, {"ref_id": "b30", "title": "Contextual Sequence Modeling for Recommendation with Recurrent Neural Networks", "journal": "ACM", "year": "2017", "authors": "E Smirnova; F Vasile"}, {"ref_id": "b31", "title": "Generating text with recurrent neural networks", "journal": "", "year": "2011", "authors": "I Sutskever; J Martens; G E Hinton"}, {"ref_id": "b32", "title": "Recommending personalized scenic itinerarywith geo-tagged photos", "journal": "IEEE", "year": "2008", "authors": "C.-H Tai; D.-N Yang; L.-T Lin; M.-S Chen"}, {"ref_id": "b33", "title": "Modelling Contextual Information in Session-Aware Recommender Systems with Neural Networks", "journal": "", "year": "2016", "authors": "B Twardowski"}, {"ref_id": "b34", "title": "The city trip planner: an expert system for tourists", "journal": "Expert Systems with Applications", "year": "2011", "authors": "P Vansteenwegen; W Souffriau; G V Berghe; D Van Oudheusden"}, {"ref_id": "b35", "title": "Improving Personalized Trip Recommendation by Avoiding Crowds", "journal": "ACM", "year": "2016", "authors": "X C Leckie; J Chan; K H Lim; T Vaithianathan"}, {"ref_id": "b36", "title": "Gradient-based learning algorithms for recurrent networks and their computational complexity", "journal": "Backpropagation: Theory, architectures, and applications", "year": "1995", "authors": "R J Williams; D Zipser"}, {"ref_id": "b37", "title": "Recurrent recommender networks", "journal": "ACM", "year": "2017", "authors": "C.-Y Wu; A Ahmed; A Beutel; A J Smola; H Jing"}, {"ref_id": "b38", "title": "Personalized travel package with multi-point-of-interest recommendation based on crowdsourced user footprints", "journal": "IEEE Transactions on Human-Machine Systems", "year": "2016", "authors": "Z Yu; H Xu; Z Yang; B Guo"}, {"ref_id": "b39", "title": "Time-aware point-of-interest recommendation", "journal": "ACM", "year": "2013", "authors": "Q Yuan; G Cong; Z Ma; A Sun; N M Thalmann"}, {"ref_id": "b40", "title": "Personalized trip recommendation with poi availability and uncertain traveling time", "journal": "ACM", "year": "2015", "authors": "C Zhang; H Liang; K Wang; J Sun"}, {"ref_id": "b41", "title": "Geosoca: Exploiting geographical, social and categorical correlations for point-of-interest recommendations", "journal": "ACM", "year": "2015", "authors": "J.-D Zhang; C.-Y Chow"}, {"ref_id": "b42", "title": "Learning travel recommendations from user-generated gps traces", "journal": "ACM Transactions on Intelligent Systems and Technology (TIST)", "year": "2011", "authors": "Y Zheng; X Xie"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Fig. 1: Check-in distribution in different dataset", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "); Vansteenwegen et al. (2011)) matched user profiles (consisting of preferred location categories, types, start/end time, and keywords) to relevant places. Lu et al. (2012) mined user check-in data and exploited the Apriori-based algorithm. Thought it was computationally intensive, it was able to find optimal trips under multiple constraints, such as social relationship, temporal property, categorical diversity, and parallel computing simultaneously. Zhang et al. (2015) exploited the uncertain travel time between two POIs and the POI availability constraints and proposed a tree-based algorithm to solve the personalized trip recommendation problem. A recent study from Wang et al. (2016) proposed a personalized trip recommender that was able to handle the crowd constraints (i.e., avoid peak hour of POIs) by extending the Ant Colony Optimization algorithm. Zheng and Xie (2011) proposed a HITS-based inference model to evaluate the degrees of user experiences and location interests based on GPS trajectory data. A heuristic-based algorithm from Chen et al. (2015) exploited user preferences, and traveling times based on different traffic conditions using trajectory patterns derived from taxi GPS traces. The heuristic search-based travel route planning system from Yu et al. (2016) exploited only user preferences and spatio-temporal constraints. Jiang et al. (2016) proposed a ranking-based system to recommend personalized travel sequences in different seasons by merging textual data and view point information extracted from images. However, they did not use the constraints for social, temporal preference of users, and temporal popularity of places. Another recent study from Lim et al. (2017) proposed time-based user interest and demonstrated advantages over the use of frequency-based popularity measures.They exploited geo-tagged images and incorporated contexts, such as visit duration, users' preferences, and start and end points. However, their model did not address the categorical, temporal, and social constraints on the POI sequence. A probabilistic model from Chen et al. (2016) incorporated POI categories and user behavior history to generate trajectories. They used Rank-SVM to rank the items and used Markov model to predict the transition between POIs.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Fig. 2 :2Fig.2: Overview of proposed system architecture. Figureashows the sequence modeling with general RNN where no contextual information is used in the hidden and output layers. Figurebshows the sequence modeling with extended RNN where the contextual information is explicitly fed to the hidden and output layer.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Fig. 3 :3Fig. 3: Unraveled view of Contextual POI Sequence Model representing the propagation of contextual information through different layers of the network. The basic POI sequence modeling with regular RNN is illustrated in Figure 2(a). The proposed contextual POI sequence modeling is illustrated in Figure 2(b). The unraveled view of the contextual RNN (Figure 2(b)) is illustrated in Figure3. The input vector and the output vector both have the dimension of the number of POIs. The input vector w(t) is the check-in at time t (which is a one hot encoding), and the output layer produces a probability distribution over the POIs, given the context, feature and the previous check-in. The hidden layers of RNNs are responsible to maintain the history of check-ins that form the sequence. Inspired fromMikolov and Zweig (2012), we extend the vanilla RNN to incorporate the context and feature inputs (f(t)). The context and the attribute of the current input, the current input itself, and the features representing the whole sequence are fed to the network. The features are propagated to the hidden as well as to the output layers. This helps us to retain the important features which the network will be aware at each and every iteration (see Figure2(b)).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "3.1.3 Contextual POI Sequence Modeling using RNN This model is termed as CAPS-RNN and represents the contextual POI sequence modeling as illustrated in Figure 2(b) and Figure", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "(a) Distribution of daily check-ins in Weeplaces dataset (b) Distribution of daily check-ins in Gowalla dataset", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Fig. 4 :4Fig. 4: (a) and (b) show the distribution of daily check-ins (X-axis represents the number of daily check-ins and Y-axis represents the fraction of users with the respective number of check-ins)", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Check-ins of 3 rd top user", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Fig. 6 :6Fig. 6: Check-in distribution of 3 highest check-in days of top three users in Weeplaces dataset (Three different color marks are used for three different days. Repeated check-ins are overlapped and not distinguished. All three users have check-ins within reasonable distance and the sequences are of reasonable length.)", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Diversity performance on Gowalla dataset", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Fig. 7 :7Fig. 7: (a) and (b) show the trend in diversity with the increasing sequence length", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Fig. 8 :8Fig. 8: (a) and (b) show the trend in displacement with the increasing sequence length", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "the POI check-in patterns are relatively casual and vary across the users. Thus a long term dependency is required which can be modeled by RNNs (or its variants like GRU and LSTM), (ii) the check-in trends are associated with many contextual attributes, such as social, categorical, temporal, and spatial. The capability of RNNs to incorporate additional attributes (e.g.,Mikolov et al. (2010);Mikolov and Zweig (2012)) is another reason behind selecting them.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Statistics of the datasets.", "figure_data": "DatasetCheck-insUsersLocationsLinksLocation CategoriesGowalla36,001,959319,0632,844,076337,545629Weeplace7,658,36815,799971,30959,97096"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": ". Pair F-Score Performance of different models ( * implies observed difference was statistically significant at 95% confidence level) on Weeplace dataset", "figure_data": "ModelsP recision P AIRRecall P AIRP air -F 1Weeplaces DatasetPOI-Popularity0.300000.166660.21428Apriori0.460790.230880.30762POI-Markov0.494110.247110.32945HITS0.499810.273360.35342Vanilla RNN0.497880.276180.35528LSTM0.515570.275000.35868CAPS-RNN0.624220.419700.50192CAPS-LSTM0.677710.431000.52690  *"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Pair F-Score Performance of different models ( * implies observed difference was statistically significant at 95% confidence level) in Gowalla dataset", "figure_data": "P recision P AIRRecall P AIRP air -F 1Gowalla DatasetPOI-Popularity0.364420.200100.25834Apriori0.469220.242760.31997POI-Markov0.499930.249810.33314HITS0.506530.279930.36058Vanilla RNN0.510010.278960.36065LSTM0.533330.440000.48219CAPS-RNN0.609140.430000.50412CAPS-LSTM0.671120.444620.53487  *ModelsDiversityDisplacement(K.m.)Weeplaces DatasetPOI-Popularity1.2000023.30785Apriori1.9000013.00000POI-Markov2.5000011.72130HITS4.0000010.55233Vanilla RNN6.2200010.27620LSTM6.7300010.00023CAPS-RNN7.118208.22990CAPS-LSTM7.091207.77014"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Diversity and displacement performance (on sequence length of 25) of models in Weeplace dataset", "figure_data": "ModelsDiversityDisplacement(K.m.)Gowalla DatasetPOI-Popularity3.2000025.22877Apriori3.3350013.00000POI-Markov3.5000011.22113HITS4.0000011.11224Vanilla RNN5.8844111.00111LSTM7.2253310.33333CAPS-RNN8.447657.77669CAPS-LSTM8.450017.71001"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Diversity and displacement performance (on sequence length of 25) of models in Gowalla dataset", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "H u = (V 1 , V 2 , ..., V n )", "formula_coordinates": [4.0, 465.86, 517.19, 86.21, 8.8]}, {"formula_id": "formula_1", "formula_text": "ST (i) = 1 | U | u\u2208U V u \u2208H u V u .l=i 1 | V u,i | l\u2208V u,i (a l+1 -T T (l, l + 1) -a l ), (1", "formula_coordinates": [4.0, 180.64, 642.64, 370.52, 38.18]}, {"formula_id": "formula_2", "formula_text": ")", "formula_coordinates": [4.0, 551.16, 648.76, 4.14, 8.3]}, {"formula_id": "formula_3", "formula_text": "AST (u, i) cat = (1 -\u03b1) * F st (u, i) + \u03b1 * G st (u, i) l\u2208V u l.cat=i.cat ST (l) V u,l , F st (u, i) = ST (i) V u,i if | V u,i |> 0, i.e.", "formula_coordinates": [5.0, 196.35, 90.05, 288.51, 59.31]}, {"formula_id": "formula_4", "formula_text": "F st (u, i) = 0 otherwise, G st (u, i) = 1 l\u2208V u l.cat=i.cat 1 if \u2203V u,l \u2227 l.cat = i.cat, i.e.", "formula_coordinates": [5.0, 127.13, 154.9, 357.73, 48.69]}, {"formula_id": "formula_5", "formula_text": "G st (u, i) = 0 otherwise,(2)", "formula_coordinates": [5.0, 384.3, 208.85, 171.0, 9.02]}, {"formula_id": "formula_6", "formula_text": "V u,l = |V u,l | |V u |", "formula_coordinates": [5.0, 426.84, 228.06, 50.15, 14.72]}, {"formula_id": "formula_7", "formula_text": "AST (u, i) = (1 -\u03c8 1 ) * AST (u, i) cat + \u03c8 * J (F u ) k\u2208F u AST (k, i) cat , J (F u ) = 1 | F u | if | F u |> 0 , i.e. u has some friends, J (F u ) = 0 otherwise,(3)", "formula_coordinates": [5.0, 168.33, 284.77, 386.98, 58.8]}, {"formula_id": "formula_8", "formula_text": "AST (u) cat = (1 -\u03b3 1 ) * ( i\u2208V u i.cat=cat AST (u, i) cat ) + \u03b3 1 * ( j\u2208F u k\u2208V j k.cat=cat AST (j, k) cat ),(4)", "formula_coordinates": [5.0, 162.26, 396.33, 393.05, 33.02]}, {"formula_id": "formula_9", "formula_text": "P S(u, l, t) = \u03b2 * {(1 -\u03b8) * P(u, l) * | V u,l,t | +\u03b8 * Q(u, l) l \u2208L l .cat=l.cat | V u,l ,t | | V u,l | } + (1 -\u03b2) * AST (u, l), P(u, l) = 1 | V u,l | if | V u,l |> 0, P(u, l) = 0 otherwise, Q(u, l) = 1 l\u2208V u l.cat=i.cat 1 if \u2203V u,l \u2227 l.cat = i.cat, i.e.", "formula_coordinates": [5.0, 111.28, 523.59, 389.44, 111.94]}, {"formula_id": "formula_10", "formula_text": "Q(u, l) = 0 otherwise,(5)", "formula_coordinates": [5.0, 408.38, 640.78, 146.93, 8.3]}, {"formula_id": "formula_11", "formula_text": "P (u, l, t) = P S(u, l, t) * (1 - 1 m m i=1 Constraint i (l, p)),(6)", "formula_coordinates": [6.0, 192.45, 101.35, 362.86, 27.83]}, {"formula_id": "formula_12", "formula_text": "h t = tanh(U h t-1 + V x t ),", "formula_coordinates": [6.0, 445.71, 593.36, 109.6, 8.8]}, {"formula_id": "formula_13", "formula_text": "p( -\u2192 x ) = N t=1 p(x t | h t-1 ). (7", "formula_coordinates": [6.0, 254.86, 623.35, 296.31, 27.71]}, {"formula_id": "formula_14", "formula_text": ")", "formula_coordinates": [6.0, 551.16, 632.74, 4.14, 8.3]}, {"formula_id": "formula_15", "formula_text": "C_(t-1) A_t POI_1 POI_2 POI_3 <eot> POI_2 <eot> POI_3 A_2 W U f(t) W U U A_3 W G F F F W A_1 b y_t F U b", "formula_coordinates": [7.0, 136.0, 8.12, 319.31, 218.01]}, {"formula_id": "formula_16", "formula_text": "p( -\u2192 l ) = T t=1 p(l t | h t-1 ), (8", "formula_coordinates": [7.0, 256.16, 571.06, 295.0, 27.71]}, {"formula_id": "formula_17", "formula_text": ")", "formula_coordinates": [7.0, 551.16, 580.45, 4.14, 8.3]}, {"formula_id": "formula_18", "formula_text": "L(x) = - T t=1 log(p(l t | h t-1 )). (9", "formula_coordinates": [7.0, 242.61, 642.7, 308.55, 27.71]}, {"formula_id": "formula_19", "formula_text": ")", "formula_coordinates": [7.0, 551.16, 652.09, 4.14, 8.3]}, {"formula_id": "formula_20", "formula_text": "c(t) = H(Uw(t) + W(c(t -1) A(t)) + Ff (t)),(10) \u0177t", "formula_coordinates": [8.0, 204.6, 331.63, 350.71, 28.01]}, {"formula_id": "formula_21", "formula_text": "= Vc(t) + Gf (t),(11)", "formula_coordinates": [8.0, 274.38, 351.31, 280.93, 8.33]}, {"formula_id": "formula_22", "formula_text": "y(t) = g(\u0177 t ),(12)", "formula_coordinates": [8.0, 279.56, 368.18, 275.74, 8.8]}, {"formula_id": "formula_23", "formula_text": "f (t) =< cat start , cat end , loc start , loc end , loc dist , time start , time end >,(13)", "formula_coordinates": [8.0, 160.26, 436.19, 395.05, 9.09]}, {"formula_id": "formula_25", "formula_text": "p(l) = T t=1 p(l t+1 | y t ).", "formula_coordinates": [8.0, 259.99, 606.8, 92.01, 27.71]}, {"formula_id": "formula_26", "formula_text": "i t = \u03c3(W i x t + W hi (h t-1 A t ) + W ci c t-1 + b i + W f F ), f t = \u03c3(W f x t + W hf (h t-1 A t ) + W cf c t-1 + b f + W f F ), z t = tanh(W z x t + W hc (h t-1 A t ) + b z + W f F ), c t = f t c t-1 + i t z t , o t = \u03c3(W o x t + W ho (h t-1 A t ) + W co c t-1 + b o + W f F ), h t = o t tanh(c t ). (16", "formula_coordinates": [9.0, 178.34, 91.63, 372.64, 81.06]}, {"formula_id": "formula_27", "formula_text": ")", "formula_coordinates": [9.0, 550.98, 163.88, 4.32, 8.3]}, {"formula_id": "formula_28", "formula_text": "p(x t+1 = k | y t ) = y k t = exp(\u0177 k t ) K k =1 exp(\u0177 k t ) . (17", "formula_coordinates": [9.0, 224.98, 362.58, 326.0, 37.29]}, {"formula_id": "formula_29", "formula_text": ")", "formula_coordinates": [9.0, 550.98, 370.78, 4.32, 8.3]}, {"formula_id": "formula_30", "formula_text": "pairs -F 1 = 2 * P P AIR * R P AIR P P AIR + R P AIR ,(18)", "formula_coordinates": [11.0, 231.99, 297.0, 323.31, 21.56]}, {"formula_id": "formula_31", "formula_text": "Diversity (c 1 , c 2 , ..., c n ) = n i=1 n j=i+1 (1 -Similarity (c i , c j )) n 2 * (n -1) (19)", "formula_coordinates": [11.0, 181.95, 378.69, 373.35, 38.06]}, {"formula_id": "formula_32", "formula_text": "Displacement (seq a , seq e ) = k i=1 | Distance(seq a i , seq e i ) |,(20)", "formula_coordinates": [11.0, 185.25, 455.72, 370.06, 27.83]}], "doi": ""}
