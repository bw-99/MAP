{
  "An Unified Search and Recommendation Foundation Model for Cold-Start Scenario": "Yuqi Gong Ant Group Beijing, China gongyuqi.gyq@antgroup.com Xichen Ding Ant Group Beijing, China xichen.dxc@antgroup.com Yehui Su Ant Group Beijing, China suyehui.syh@antgroup.com Kaiming Shen Ant Group Beijing, China kaiming.skm@antgroup.com Zhongyi Liu Ant Group Hangzhou, China zhongyi.lzy@antgroup.com",
  "ABSTRACT": "Guannan Zhang Ant Group Hangzhou, China zgn138592@antgroup.com",
  "ACMReference Format:": "In modern commercial search engines and recommendation systems, data from multiple domains is available to jointly train the multi-domain model. Traditional methods train multi-domain models in the multi-task setting, with shared parameters to learn the similarity of multiple tasks, and task-specific parameters to learn the divergence of features, labels, and sample distributions of individual tasks. With the development of large language models, LLM can extract global domain-invariant text features that serve both search and recommendation tasks. We propose a novel framework called S&R Multi-Domain Foundation, which uses LLM to extract domain invariant features, and Aspect Gating Fusion to merge the ID feature, domain invariant text features and task-specific heterogeneous sparse features to obtain the representations of query and item. Additionally, samples from multiple search and recommendation scenarios are trained jointly with Domain Adaptive Multi-Task module to obtain the multi-domain foundation model. We apply the S&R Multi-Domain foundation model to cold start scenarios in the pretrain-finetune manner, which achieves better performance than other SOTA transfer learning methods. The S&R Multi-Domain Foundation model has been successfully deployed in Alipay Mobile Application's online services, such as content query recommendation and service card recommendation, etc.",
  "CCS CONCEPTS": "· Computing methodologies → Machine learning algorithms ; · Information systems → Data mining ; Retrieval models and ranking .",
  "KEYWORDS": "search and recommendation, LLM, multi-domain recommendation Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CIKM '23, October 21-25, 2023, Birmingham, United Kingdom © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0124-5/23/10...$15.00 https://doi.org/10.1145/3583780.3614657 Yuqi Gong, Xichen Ding, Yehui Su, Kaiming Shen, Zhongyi Liu, and Guannan Zhang. 2023. An Unified Search and Recommendation Foundation Model for Cold-Start Scenario. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management (CIKM '23), October 21-25, 2023, Birmingham, United Kingdom. ACM, New York, NY, USA, 7 pages. https: //doi.org/10.1145/3583780.3614657",
  "1 INTRODUCTION": "Modern commercial recommendation systems and search engines are widely used in online service platforms such as YouTube, TikTok, Taobao, Alipay, etc. Search and recommendation can facilitate users' behaviors to browse instant videos, buy products, use services, and make payments using E-wallets. The similarity between Search (S) and Recommendation (R) makes jointly modeling S&R a promising research topic. Some work [17, 18] propose to enhance recommendation by learning from a unified sequence of search and recommendation behaviors. Others [1, 14] propose to improve personalized search by adding users' multi-interests in recommendation. Methods to model search and recommendation tasks jointly are also proposed in [21-24, 26]. JSR framework in [23] simultaneously learns retrieval and recommendation models with shared item set and optimizes a joint loss function. Researchers in [22] applied two-level transformer encoders, text encoder to learn documents and queries, session encoder to model the integrated sequence of search and browsing behaviors. [26] applied GNN to learn node embedding of user&item, and treat search query as a special attribute of edges in the graph. In industrial scenarios, there are several benefits to model S and R jointly. First, there are multiple search and recommendation scenarios in a single mobile application. The training data collected in a single domain can't fully reflect users' complete intents and is sub-optimal compared to modeling them jointly. Secondly, majority of items are shared between search and recommendation. Once users have impressions of any products, videos, and services in a recommendation scenario, they should be able to retrieve the item in Search later for repurchase or reuse purposes. Despite the similarity between Search and Recommendation, there are also difficulties in modeling them jointly, such as the data imbalance issue of multiple domains, the heterogeneous issue of different item sets (videos, products, and services), the negative transferring issue. With the latest developments in large language models (LLM) [2, 4, 5, 20, 27], the pretrain-finetune framework [7, 13, 18, 22, 26] greatly improves the performance of CIKM '23, October 21-25, 2023, Birmingham, United Kingdom Yuqi Gong et al. downstream tasks. We are inspired by the strong expressive power of natural language features and propose to build the Search and Recommendation Foundation model on top of LLMs, which extract low-level domain-invariant text features of the query (Q) and item (I). The major difference between our S&R foundation model and traditional multi-domain multi-task models is how we use the domain-invariant text features to help constrain the divergence of different tasks, which alleviates data imbalance, negative transferring, and item heterogeneous issues. To summarize, our proposed S&R Foundation model has the following key contributions: · We apply LLMs in S&R Multi-Domain Foundation model, and extract domain invariant text features to help mitigate the negative transferring and item heterogeneous issues in the multi-domain settings. · We novelly proposed the Aspect Gating Fusion (DomainSpecific Gating) to fuse the ID feature, text features from LLMs, and sparse features. The Domain Adaptive Multi-Task module is also used to extract the domain-specific query and item towers' representations. · For the cold start of new scenarios, we have conducted extensive experiments both offline and online, to show the effectiveness of supervised fine-tuning of our S&R Foundation model in downstream tasks, which is now fully deployed online and serving in Alipay's mobile application.",
  "2 PROPOSED MODEL": "",
  "2.1 Problem Formulation": "Given a set of 𝐾 search and recommendation tasks { 𝐷 𝑘 } 𝐾 𝑘 = 1 , 𝐷 𝑘 denotes the dataset for the 𝑘 -th task. We let U = { 𝑢 1 , 𝑢 2 , . . . , 𝑢 𝑁 } denote the user set, I = { 𝑖 1 , 𝑖 2 , . . . , 𝑖 𝑀 } denote the item set and Q = { 𝑞 1 , 𝑞 2 , ..., 𝑞 𝑇 } denote the search query set. In real-world scenarios, items in search and recommendation usually come from different domains and are heterogeneous. Some items are shared across multiple domains and some items belong to each specific domain. And we let I = I 1 ∪ I 2 ∪ . . . ∪ I 𝐾 denote the union of all items in 𝐾 domains, which contains 𝑀 items in total. We aim to jointly train a search and recommendation (S&R) foundation model 𝑀 𝑆 & 𝑅 𝐹𝑜𝑢𝑛𝑑𝑎𝑡𝑖𝑜𝑛 in the multi-task setting and predict the probability of user 𝑢 𝑙 click the item 𝑖 𝑙 given input query 𝑞 𝑙 as 𝑝 ( 𝑦 𝑐𝑡𝑟 𝑙 = 1 | 𝑢 𝑙 , 𝑞 𝑙 , 𝑖 𝑙 ) . And for search scenarios, additional query-item relevance score is also predicted as 𝑝 ( 𝑦 𝑠𝑖𝑚 𝑙 = 1 | 𝑞 𝑙 , 𝑖 𝑙 ) . For cold start of a new search or recommendation scenario 𝐷 ∗ , we restore parameters of embedding tables and partial network structures from the pretrained S&R foundation model 𝑀 𝑆 & 𝑅 𝐹𝑜𝑢𝑛𝑑𝑎𝑡𝑖𝑜𝑛 , and then apply supervised fine-tuning on the downstream tasks, such as click through rate (CTR) prediction, query-item relevance prediction, etc. For the search task 𝐷 𝑆 𝑘 , welet 𝐷 𝑆 𝑘 = { 𝑥 𝑙 = ( 𝑢 𝑙 , 𝑞 𝑙 , 𝑖 𝑙 ) , 𝑦 𝑙 } 𝑙 , which denotes the search ranking task given the triple input of (user, query, item) as ( 𝑢 𝑙 , 𝑞 𝑙 , 𝑖 𝑙 ) . For the recommendation task 𝐷 𝑅 𝑘 , we set search query set Q as emptyset ∅ in 𝐷 𝑅 𝑘 = { 𝑥 𝑙 = ( 𝑢 𝑙 , 𝑞 𝑙 = ∅ , 𝑖 𝑙 ) , 𝑦 𝑙 } 𝑙 .",
  "2.2 S&R Multi-Domain Foundation Model": "As illustrated in Figure 1, the S&R Multi-Domain Foundation model has three main components: the User-Query-Item encoding module, the Aspect Gating Fusion module, and the Domain-Adaptive Multi-Task module. Firstly, raw features of user, query and item pass through the embedding layers, and we extract the ID embedding, token-level text embedding and sparse features' embedding. We apply LLM to extract domain-invariant text features of query and item towers, which minimize the divergence of features' distribution cross multiple domains. Secondly, the Aspect Gating Fusion module is designed to merge different groups of ID, text, sparse features' embedding. The fusion network is to balance the relative importance of ID, text, and sparse features. Very few training samples contain ID features of cold start items and can't represent them well, and generic text features play more important role. Finally, we feed the concatenated embedding of user, query and item towers to the Domain Adaptive MTL module. The module has two outputs representing the click through rate (CTR) prediction task and the query-item relevance prediction task. The final loss function is the sum of CTR prediction loss L 𝑐𝑡𝑟 , relevance prediction loss L 𝑠𝑖𝑚 and domain adaptive regularization L 𝑟𝑒𝑔 . 2.2.1 User Query and Item Encoding. We extract three towers for user, query and item respectively. For the user tower, 𝑒 𝐼 𝐷 𝑢 ∈ R 𝐷 denotes user id embedding. 𝑒 𝑁 𝐻 𝑢 = [ 𝑥 1 , ..., 𝑥 𝑠 , . . . , 𝑥 𝑁 𝐻 ] denotes the unified sequence of both search and recommendation clicks in chronological order. Each behavior 𝑥 𝑠 is encoded as multiple layers of MLPs with inputs of ID feature, sparse feature of behavior type S or R, and other sparse features of attributes, 𝑥 𝑠 = 𝐹𝐶 ( 𝑒 𝐼 𝐷 𝑠 ⊕ 𝑒 𝑡 𝑦𝑝𝑒 𝑠 ⊕ 𝑒 𝑎𝑡𝑡𝑟 𝑠 ) . For the query ( 𝑄 ) and item ( 𝐼 ) features, we extract both domain-invariant text features, such as tokens in search query and items' title, and the domain-specific sparse features. The tokens of 𝑄 and 𝐼 go through the same tokenizer and we get the tokenized id sequences as integer tensors 𝑒 𝑇𝑜𝑘𝑒𝑛 𝑞 and 𝑒 𝑇𝑜𝑘𝑒𝑛 𝑖 . 𝑒 𝑇𝑜𝑘𝑒𝑛 𝑞 = [ 𝑒 1 𝑞 , 𝑒 2 𝑞 , . . . , 𝑒 𝐿 𝑞 𝑞 ] ∈ R 𝐿 𝑞 × 𝐷 denotes the query's tokenized id tensor of length 𝐿 𝑞 , and 𝑒 𝑇𝑜𝑘𝑒𝑛 𝑖 = [ 𝑒 1 𝑖 , 𝑒 2 𝑖 , . . . , 𝑒 𝐿 𝑖 𝑖 ] ∈ R 𝐿 𝑖 × 𝐷 denotes the item's tokenized id tensor of length 𝐿 𝑖 . For ID feature, we also embed the search query as ID feature 𝑒 𝐼 𝐷 𝑞 ∈ R 𝐷 , and item ID as 𝑒 𝐼 𝐷 𝑖 ∈ R 𝐷 . For the sparse features, we embed sparse features of 𝑄 as 𝑒 𝑆 𝑞 and sparse features of 𝐼 as 𝑒 𝑆 𝑖 . Finally, we get the feature groups of query tower as 𝑒 𝑞 = [ 𝑒 𝐼 𝐷 𝑞 , 𝑒 𝑇𝑜𝑘𝑒𝑛 𝑞 , 𝑒 𝑆 𝑞 ] and the feature groups of item tower as 𝑒 𝑖 = [ 𝑒 𝐼 𝐷 𝑖 , 𝑒 𝑇𝑜𝑘𝑒𝑛 𝑖 , 𝑒 𝑆 𝑖 ] .",
  "LLM as Domain-Invariant Feature Extractor": "We apply the pretrained Large Language Model, such as BERT [6], GPT [2], ChatGLM [8, 25], to extract domain-invariant text features on both query tower and item tower, represented as 𝜙 𝑙𝑚 ( 𝑄 ) = 𝜙 𝑙𝑚 ( 𝑒 𝑇𝑜𝑘𝑒𝑛 𝑞 ) ∈ R 𝐿 𝑞 × 𝐷 and 𝜙 𝑙𝑚 ( 𝐼 ) = 𝜙 𝑙𝑚 ( 𝑒 𝑇𝑜𝑘𝑒𝑛 𝑖 ) ∈ R 𝐿 𝑖 × 𝐷 . After mean pooling of the encoding layer, followed by shared linear projection, we get the domain-invariant text representation of query and item as 𝐸 𝑙𝑚 ( 𝑄 ) = 𝑊 𝑙𝑚 × MEAN ( 𝜙 𝑙𝑚 ( 𝑒 𝑇𝑜𝑘𝑒𝑛 𝑞 )) ∈ 𝑅 𝐻 , 𝐸 𝑙𝑚 ( 𝐼 ) = 𝑊 𝑙𝑚 × MEAN ( 𝜙 𝑙𝑚 ( 𝑒 𝑇𝑜𝑘𝑒𝑛 𝑖 )) ∈ 𝑅 𝐻 . 𝑊 𝑙𝑚 ∈ R 𝐻 × 𝐷 denotes the linear projection layer shared between query tower and item tower, 𝐻 denotes the hidden size of the learned representations. The language models' representation is useful for cold start scenarios, especially when we have few training samples to update the ID feature of new item 𝑖 ∗ and new search query 𝑞 ∗ . We also apply linear projections 𝑊 𝑞 𝐼 𝐷 , 𝑊 𝑖 𝐼 𝐷 ∈ R 𝐻 × 𝐷 to ID feature of query and item tower, and get the ID representation of query and item as 𝐸 𝐼 𝐷 ( 𝑄 ) , 𝐸 𝐼 𝐷 ( 𝐼 ) ∈ R 𝐻 . For the sparse features we have separate network (usually multiple layers of MLPs) to encode An Unified Search and Recommendation Foundation Model for Cold-Start Scenario CIKM '23, October 21-25, 2023, Birmingham, United Kingdom Figure 1: SR Multi-Domain Foundation Model Architecture User Emb Table Transformer User Encoder ID Query Encoder Query Emb Table Item Encoder LLM Sparse Fusion ID Item Emb Table LLM Sparse Fusion Emb Output A Output B Domain Embedding Domain Divergence Aspect Gating Fusion Domain K Domain 2 Domain 1 Expert 1 Expert 2 Expert n ID Sequence Sparse Domain Gating [CLS] Gating Mean Gating Domain Adaptive MTL User-Query- Item Encoding query and item as 𝐸 𝑆 ( 𝑄 ) , 𝐸 𝑆 ( 𝐼 ) ∈ R 𝐻 . Finally, we get the feature groups of query tower as [ 𝐸 𝐼 𝐷 ( 𝑄 ) , 𝐸 𝑙𝑚 ( 𝑄 ) , 𝐸 𝑆 ( 𝑄 )] and item tower as [ 𝐸 𝐼 𝐷 ( 𝐼 ) , 𝐸 𝑙𝑚 ( 𝐼 ) , 𝐸 𝑆 ( 𝐼 )] . 2.2.2 Aspect Gating Fusion. After low level networks 𝐿 0 (embedding tables) and 𝐿 1 (feature encoding layers) in Figure 1, we fuse different aspects of query and item as in literature [12]. Each aspect 𝐸 𝑎 represents some fine-grained properties of query and item, such as ID, text and sparse features. A denotes the set of aspects we extract from query and item. In S&R scenarios, we set |A| = 3 as ID, text and sparse attributes. Final representations are fused as weighted sum of different aspects' representations.  The weight vector 𝑤 ( 𝑄 ) , 𝑤 ( 𝐼 ) ∈ R | A| are outputs of a gating network, and we have different strategies to design the network. · Mean-Gating Strategy Simply mean pooling of different aspects of query and item features as 𝑤 𝑎 = 1 | A| . · [CLS]-Gating Strategy We use randomly initialized embedding 𝐸 𝐶𝐿𝑆 ( 𝑄 ) , 𝐸 𝐶𝐿𝑆 ( 𝐼 ) ∈ R 𝐻 to represent classification token [CLS] of query and item respectively.  .",
  "· Domain-Gating Strategy": "We design the domain gating strategy from the intuition that the fusion network has different weights when merging different aspects of query and item. To model the differences across domains, we randomly initialize the domain embedding 𝐸 𝐷 = [ 𝐸 𝐷 1 , 𝐸 𝐷 2 , ..., 𝐸 𝐷 𝐾 ] ∈ R 𝐾 × 𝐻 as the representations of different domains. And the domain-specific gating is calculated as  . 2.2.3 Domain Adaptive Multi-Task Learning. The input to the Domain Adaptive Multi-Task module is the concatenation of representations of user, query and item towers as x = 𝐸 ( 𝑈 ) ⊕ 𝐸 ( 𝑄 ) ⊕ 𝐸 ( 𝐼 ) . For multi-domain setting, a series of multi-task and multi-domain models are proposed, such as SharedBottom[3], MMoE[15], PLE[19], STAR[16], SAMD[11], etc. These models use shared structures (Experts or MLP layers) to model the similarity among different tasks or domains, and use individual structures to learn the domain-specific properties. The difficulty of training the multi-domain models is the domain shift phenomena. For the k-th domain 𝐷 𝑘 , the marginal distribution of input feature 𝑝 ( x 𝑘 ) and the conditional distribution of predicting output 𝑦 𝑘 as 𝑝 ( 𝑦 𝑘 | x 𝑘 ) has divergence from other domains. The well studied MTL models handle the divergence of conditional distribution. We propose to add a Domain Adaptive Layer to the input features x 𝑖 , which maps the inputs from multiple domains to a common vector space. We reuse the randomly initialized domain embedding 𝐸 𝐷 = [ 𝐸 𝐷 1 , 𝐸 𝐷 2 , ..., 𝐸 𝐷 𝐾 ] ∈ R 𝐾 × 𝐻 in section 2.2.2 and concatenate the domain embedding 𝐸 𝐷 𝑘 to feature vector x 𝑖 of instances from the k-th domain 𝐷 𝑘 , followed by domain-specific linear transformation 𝑊 𝑘 . Suppose x 𝑖 and x 𝑗 denote two instances from different domains in the same training batch, we can get the domain-adaptive representation ˆ x 𝑖 , ˆ x 𝑗 as CIKM '23, October 21-25, 2023, Birmingham, United Kingdom Yuqi Gong et al.  We apply domain adaptation [9] techniques to constrain the divergence of distributions from domains 𝑖 and 𝑗 as 𝑝 ( ˆ x 𝑖 ) and 𝑝 ( ˆ x 𝑗 ) as L 𝑟𝑒𝑔 = ˝ 𝑖,𝑗 ∈{ 1 , 2 ,...,𝐾 } 𝑑 ( 𝑝 ( ˆ x 𝑖 )|| 𝑝 ( ˆ x 𝑗 )) . In terms of divergence measurement, we compared different metrics such as JensenShannon Divergence (symmetric KL Divergence), Maximum Mean Discrepancy (MMD) [10] in the experiment section. And we find the Jensen-Shannon Divergence achieves the best performance as  . Finally, on top of the Domain Adaptive Layer we stack the standard Multi-Task module, such as MMoE to extract outputs and predict two objectives, CTR prediction 𝑦 𝑐𝑡𝑟 and query-item relevance prediction 𝑦 𝑠𝑖𝑚 . CTR Prediction Click-Through Rate (CTR) Prediction is a common task in both search and recommendation scenarios. We apply a unified scoring function 𝑦 𝑐𝑡𝑟 𝑙 = 𝑓 𝜃 ( 𝑢 𝑙 , 𝑞 𝑙 , 𝑖 𝑙 ) in the S&R foundation framework to predict CTR with the triple inputs of user, item and query as ( 𝑢, 𝑞, 𝑖 ) . For search tasks, users have explicit search query 𝑞 . And for recommendation tasks users don't have explicit intentions. So we set 𝑞 = ∅ as the default embedding in the unified scoring function. Query-Item Relevance Prediction Query-Item Relevance Prediction is a common task in search scenarios, which predicts the relevance score of query-item pair of ( 𝑞, 𝑖 ) and train a function 𝑦 𝑠𝑖𝑚 𝑙 = 𝑓 𝜙 ( 𝑞 𝑙 , 𝑖 𝑙 ) to represent query-item pair's relevance score. The relevance prediction is usually a classification task. 2.2.4 Loss of S&R Foundation model. We train the S&R foundation model in multi-domain multi-task settings, using datasets from 𝐾 domains. Each domain calculates either or both of two objectives of CTR prediction 𝑦 𝑐𝑡𝑟 𝑙 = 𝑓 𝜃 ( 𝑢 𝑙 , 𝑞 𝑙 , 𝑖 𝑙 ) and relevance prediction 𝑦 𝑠𝑖𝑚 𝑙 = 𝑓 𝜙 ( 𝑞 𝑙 , 𝑖 𝑙 ) , depending on whether the task is search or recommendation. The final objective function consists of three parts, the loss for CTR prediction L 𝑐𝑡𝑟 , the loss for relevance prediction L 𝑠𝑖𝑚 , and the loss for domain adaptive regularizer L 𝑟𝑒𝑔 .",
  "2.3 Supervised Fine-Tuning Downstream Tasks": "The pretrained S&R foundation model can benefit downstream tasks in the pretrain-finetune manner. The downstream model restores parameters from the foundation model, freezes part of the parameters and finetunes the remaining layers. We experiment different ways of freeze-finetune split. Firstly, the freeze-finetune split is between level 𝐿 0 and 𝐿 1 as in Figure 1. The pretrained embedding in level 𝐿 0 is freezed and the remaining layers from 𝐿 1 to 𝐿 𝑛 are finetuned. Secondly, the freeze-finetune split is between level 𝐿 1 and 𝐿 2. The embedding in level 𝐿 0 as well as the parameters of encoding layers in level 𝐿 1 are freezed, and the parameters from level 𝐿 2 to 𝐿 𝑛 are finetuned. Given dataset of new downstream task 𝐷 ∗ = {( 𝑢 ∗ 𝑙 , 𝑞 ∗ 𝑙 , 𝑖 ∗ 𝑙 ) , 𝑦 ∗ 𝑙 } , the domain embedding 𝐸 𝐷 ∗ ∈ R 𝐻 is randomly initialized and finetuned. In the experiment section, we thoroughly tested the performance of different ways of freeze-finetune split. We also compared the performance of pretrain-finetuning S&R Foundation model 𝑀 𝑆 & 𝑅 𝐹𝑜𝑢𝑛𝑑𝑎𝑡𝑖𝑜𝑛 with the performance of training single domain model without transfer learning.",
  "3 EXPERIMENT": "To test the effectiveness of our proposed S&R Multi-Domain Foundation model, we want to answer the following questions: · RQ1: Whether our joint S&R Multi-Domain Foundation model can achieve SOTA performance compared to other multi-domain and multi-task models? · RQ2: In terms of query and item towers' representations, what's the performance of the domain-invariant text features extracted by LLM and Aspect Gating Fusion network compared to other methods? · RQ3: Whether S&R Multi-Domain Foundation and Supervised Finetuning can help benefit cold start scenarios?",
  "3.1 Experimental Settings": "3.1.1 Dataset. We conducted extensive experiments of S&R Foundation model on real-world datasets, including 7 industrial datasets of Alipay Search Ranking and Query Recommendation. The statistics are summarized in table 1. 𝑆 denotes the search dataset, in which users have explicit search query, such as Query-Item Relevance Prediction, Content Search Ranking, etc. And 𝑅 denotes the recommendation dataset, in which users don't have explicit intent of search query. There are also some tasks between Search and Recommendation, which we classify as S/R, such as Query Suggest CTR Prediction, in which users have explicit query, and at the same time the task is a CTR prediction task to make recommendation of query suggestions to users. 3.1.2 Comparison Methods. S&R Foundation Model We compared our proposed S&R Multi-Domain Foundation model with SOTA multi-domain and multi-task models, such as Shared Bottom MTL [3], Multi-Gate Mixture of Experts (MMoE) [15], PLE [19], etc. For ablation study, we designed separate experiments to evaluate different modules of the framework, including the UserQuery-Item encoding module, Aspect Gating Fusion module and Domain Adaptive Multi Task module. The experiment of S&R MultiDomain Foundation (MLP) denotes the concatenated user-queryitem representations are followed by multiple MLP layers. And the experiment of S&R Multi-Domain Foundation-MMoE-DA-JS denotes the representations are followed by a Domain Adaptive Layer (JS-Divergence) and MMoE multi-task module. Domain-Invariant Text Features and Aspect Gating Fusion To prove the effectiveness of adding domain-invariant text features in S&R Foundation model, we have conducted experiments and ablation studies on different query and item token encoding methods on Alipay Content Query Recommendation dataset of tasks 4 in table 1. In the baseline method, we intentionally leave out the token-embedding of text features and only use ID and sparse features. We also compared randomly initialized token embedding with An Unified Search and Recommendation Foundation Model for Cold-Start Scenario CIKM '23, October 21-25, 2023, Birmingham, United Kingdom",
  "Table 1: Statistics of Alipay Search Ranking and Query Recommendation Datasets.": "",
  "Table 2: Performance of S&R Multi-Domain Foundation Model.": "",
  "Table 3: Comparison of Query and Item Token Encoding Methods after Fine-tuning Task 4.": "S&R Foundation-MLP S&R Foundation-MMoE-DA-MMD S&R Foundation-MMoE-DA-JS 30 20 10 0 10 20 30 10 0 10 20 task 1 task 2 task 3 task 4 task 5 task 6 task 7 60 40 20 0 20 60 40 20 0 20 40 task 1 task 2 task 3 task 4 task 5 task 6 task 7 40 Figure 2: Visualization of SR Foundation Model's Domain-Adaptive Layers 30 20 10 0 10 20 30 30 20 10 0 10 20 30 40 task 1 task 2 task 3 task 4 task 5 task 6 task 7",
  "Table 4: Comparison of Aspect Gating Fusion on Task 4.": "",
  "Table 5: Comparison of Cold Start Scenarios Task 4 and 6.": "CIKM '23, October 21-25, 2023, Birmingham, United Kingdom Yuqi Gong et al. 1 2 3 4 5 6 7 0.024 0.026 0.028 0.030 0.032 0.034 PVCTR Online A/B Testing Service Card Recommendation PVCTR Baseline SR Foundation-Finetune Days Figure 3: Online AB Testing PVCTR Performance of Service Card Recommendation. embedding restored from the pretrained S&R foundation model under different configurations. For the encoders, we compared mean pooling, randomly initialized Transformer, BERT, ChatGLM6B[8, 25], and ChatGLM+prompt, etc. In methods 10-12, we adopt ChatGLM-6B and ChatGLM2-6B to encode the text features of query and items. The implementation details are: we utilized the encoders of ChatGLM-6B and ChatGLM2-6B to convert the input text features and corresponding prompts into 4096-dimensional vectors, which are followed by 2 MLP dense layers and further reduced to 32-dimensional vectors. The prompt function used in our approach is defined as 𝑓 𝑝𝑟𝑜𝑚𝑝𝑡 ( 𝑋 ) = 'Extract keywords from sentence [X]'. To compare different Aspect Gating Fusion methods, e.g. MeanPooling, [CLS]-Gating, Domain-Gating, we conducted ablation studies and the results are listed in table 4. Supervised Fine-Tuning on Cold Start Scenarios For cold start scenarios, we compared the performance of supervised finetuning (SFT) the foundation model using downstream dataset, with the method of training single domain model on several tasks, including Service Card Recommendation (task 6) and Content Query Recommendation (task 4) as in table 5.",
  "3.2 Experimental Results": "3.2.1 S&R Multi-Domain Foundation model. To compare the performance of different multi-domain models, we report AUC performance on 7 search and recommendation datasets in table 2. All the experimented models share same input features, User-Query-Item Encoding module, and Domain-Gating as Aspect Gating Fusion strategy. The baseline for the multi-task learning (MTL) module is the shared bottom model. MMoE-DA-MMD and MMoE-DA-JS represent models that utilize Maximum Mean Discrepancy (MMD) and Jensen-Shannon Divergence (JS-Divergence) to constrain the distributions of domain adaptive layers respectively. The asterisk (*) denotes the best performance achieved in each task, and the absolute improvement represents the absolute improvement of MMoEDA-JS method compared to baseline. MMoE-DA-JS achieved best performance on 4 tasks: 4, 5, 6, 7 with AUC improvement of +0.0404, +0.0192, +0.0297, +0.0131 respectively. The domain-adaptive layer constrains the embedding representations from different domains in the common vector space. The t-SNE visualization of S&R Foundation model's domain-adaptive layers is depicted in Figure 2. The embedding depicted in the first subplot \"S&R Foundation MLP\" is scattered, and the embedding in the third subplot \"S&R FoundationMMoE-DA-JS\" is coherently aligned. 3.2.2 Domain-Invariant Text Features and Aspect Gating Fusion. We report the performance of different methods to encode domaininvariant text features and freeze-finetune split in table 3 on task 4 Content Query Recommendation. Our proposed method of restoring pretrained parameters from BERT BASE (12 layers Transformer) in S&R Foundation, freezing the parameters of the encoder and finetuning the remaining networks achieves the best AUC performance 0.7580, which is 0.0056 absolute gain over baseline model. Comparing different freeze-finetune split (methods 5-7), we can see that freezing pretrained parameters in level 𝐿 0 and 𝐿 1 (method 7) achieves better performance than other split methods (method 5/6), which is 0.0043 absolute gain in AUC. As for the ablation studies of Aspect Gating Fusion in table 4, the baseline is to simply mean pooling three aspects: ID, text and sparse features. We can see the Domain-Gating achieves best AUC performance 0.7524, which is 0.0139 absolute gain over mean-pooling method. 3.2.3 Supervised Finetuning in Cold Start Scenarios. To prove the effectiveness of finetuning our pretrained S&R Foundation model, we compared cold start performance of two scenarios, Service Card Recommendation (task 6) and Content Query Recommendation (task 4). They are new scenarios and we only collected a few samples in a short period of time. The samples are splitted as we leave out last one day's collected data for testing, and use the remaining data for fine-tuning the S&R Foundation. We also train the single domain model as the baseline. From table 5, we can see the fine-tuned S&R Foundation model achieves +0.0216 AUC improvement over single domain model on task 6 and +0.0279 AUC improvement on task 4. 3.2.4 Online AB Testing. To further prove the effectiveness of online performance in cold start scenario, we deployed the fine-tuned S&R Foundation model online in Service Card Recommendation scenario, and compared with baseline, which is the single domain DNN model. The results of the AB Testing from day 1 to day 7 are depicted in Figure 3. The key performance measurement of the cold start scenario is PVCTR (Page View Click Through Rate). And we observed that the fine-tuned S&R Foundation model achieved +17.54% relative gain in PVCTR over baseline. The online AB Testing results showed that our method achieved better performance than baseline consistently in cold start scenario.",
  "4 CONCLUSION": "In this paper, we study the problem of training search and recommendation tasks jointly as the S&R Multi-Domain Foundation model, and use domain adaptation techniques to benefit cold start scenario. Our proposed model learns user, query and item representations, applies LLM to encode domain invariant text features and Aspect Gating Fusion to merge ID, text and sparse features. We also conducted extensive experiments on finetuning the foundation models in cold start scenarios, which achieves better performance than the single domain model. The fine-tuned S&R Multi-Domain Foundation model has been successfully deployed online in Alipay's multiple search and recommendation scenarios. An Unified Search and Recommendation Foundation Model for Cold-Start Scenario CIKM '23, October 21-25, 2023, Birmingham, United Kingdom",
  "REFERENCES": "[1] Qingyao Ai, Yongfeng Zhang, Keping Bi, Xu Chen, and W. Bruce Croft. 2017. Learning a Hierarchical Embedding Model for Personalized Product Search. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (Shinjuku, Tokyo, Japan) (SIGIR '17) . Association for Computing Machinery, New York, NY, USA, 645-654. https: //doi.org/10.1145/3077136.3080813 [2] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual , Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.). [3] Rich Caruana. 1997. Multitask Learning. Mach. Learn. 28, 1 (1997), 41-75. [4] Zeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022. M6Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems. CoRR abs/2205.08084 (2022). [5] Zeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022. M6Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems. arXiv:2205.08084 [cs.IR] [6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers) , Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computational Linguistics, 4171-4186. [7] Xichen Ding, Jie Tang, Tracy Liu, Cheng Xu, Yaping Zhang, Feng Shi, Qixia Jiang, and Dan Shen. 2019. Infer Implicit Contexts in Real-Time Online-toOffline Recommendation. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery Data Mining (Anchorage, AK, USA) (KDD '19) . Association for Computing Machinery, New York, NY, USA, 2336-2346. https://doi.org/10.1145/3292500.3330716 [8] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM: General Language Model Pretraining with Autoregressive Blank Infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . 320-335. [9] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky. 2016. Domain-Adversarial Training of Neural Networks. J. Mach. Learn. Res. 17, 1 (jan 2016), 2096-2030. [10] Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Schölkopf, and Alexander Smola. 2012. A Kernel Two-Sample Test. J. Mach. Learn. Res. 13, null (mar 2012), 723-773. [11] Zhaoxin Huan, Ang Li, Xiaolu Zhang, Xu Min, Jieyu Yang, Yong He, and Jun Zhou. 2023. SAMD: An Industrial Framework for Heterogeneous MultiScenario Recommendation. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (Long Beach, CA, USA) (KDD '23) . Association for Computing Machinery, New York, NY, USA, 4175-4184. https://doi.org/10.1145/3580305.3599955 [12] Weize Kong, Swaraj Khadanga, Cheng Li, Shaleen Kumar Gupta, Mingyang Zhang, Wensong Xu, and Michael Bendersky. 2022. Multi-Aspect Dense Retrieval. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (Washington DC, USA) (KDD '22) . Association for Computing Machinery, New York, NY, USA, 3178-3186. https://doi.org/10.1145/3534678. 3539137 [13] Ningning Li, Qunwei Li, Xichen Ding, Shaohu Chen, and Wenliang Zhong. 2022. Prototypical Contrastive Learning and Adaptive Interest Selection for Candidate Generation in Recommendations. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management (Atlanta, GA, USA) (CIKM '22) . Association for Computing Machinery, New York, NY, USA, 4183-4187. https://doi.org/10.1145/3511808.3557674 [14] Jiongnan Liu, Zhicheng Dou, Qiannan Zhu, and Ji-Rong Wen. 2022. A CategoryAware Multi-Interest Model for Personalized Product Search. In Proceedings of the ACM Web Conference 2022 (Virtual Event, Lyon, France) (WWW '22) . Association for Computing Machinery, New York, NY, USA, 360-368. https: //doi.org/10.1145/3485447.3511964 [15] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H. Chi. 2018. Modeling Task Relationships in Multi-task Learning with Multi-gate Mixtureof-Experts. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2018, London, UK, August 19-23, 2018 . ACM, 1930-1939. [16] Xiang-Rong Sheng, Liqin Zhao, Guorui Zhou, Xinyao Ding, Binding Dai, Qiang Luo, Siran Yang, Jingshan Lv, Chi Zhang, Hongbo Deng, and Xiaoqiang Zhu. 2021. One Model to Serve All: Star Topology Adaptive Recommender for MultiDomain CTR Prediction. In CIKM '21: The 30th ACM International Conference on Information and Knowledge Management, Virtual Event, Queensland, Australia, November 1 - 5, 2021 , Gianluca Demartini, Guido Zuccon, J. Shane Culpepper, Zi Huang, and Hanghang Tong (Eds.). ACM, 4104-4113. [17] Zihua Si, Xueran Han, Xiao Zhang, Jun Xu, Yue Yin, Yang Song, and Ji-Rong Wen. 2022. A Model-Agnostic Causal Learning Framework for Recommendation Using Search Data. In Proceedings of the ACM Web Conference 2022 (Virtual Event, Lyon, France) (WWW '22) . Association for Computing Machinery, New York, NY, USA, 224-233. https://doi.org/10.1145/3485447.3511951 [18] Zihua Si, Zhongxiang Sun, Xiao Zhang, Jun Xu, Xiaoxue Zang, Yang Song, Kun Gai, and Ji-Rong Wen. 2023. When Search Meets Recommendation: Learning Disentangled Search Representation for Recommendation. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (Taipei, Taiwan) (SIGIR '23) . Association for Computing Machinery, New York, NY, USA, 1313-1323. https://doi.org/10.1145/3539618.3591786 [19] Hongyan Tang, Junning Liu, Ming Zhao, and Xudong Gong. 2020. Progressive Layered Extraction (PLE): A Novel Multi-Task Learning (MTL) Model for Personalized Recommendations. In RecSys 2020: Fourteenth ACM Conference on Recommender Systems, Virtual Event, Brazil, September 22-26, 2020 . ACM, 269-278. [20] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. LLaMA: Open and Efficient Foundation Language Models. CoRR abs/2302.13971 (2023). [21] Jun Xu, Xiangnan He, and Hang Li. 2018. Deep Learning for Matching in Search and Recommendation. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval (Ann Arbor, MI, USA) (SIGIR '18) . Association for Computing Machinery, New York, NY, USA, 1365-1368. https://doi.org/10.1145/3209978.3210181 [22] Jing Yao, Zhicheng Dou, Ruobing Xie, Yanxiong Lu, Zhiping Wang, and Ji-Rong Wen. 2021. USER: A Unified Information Search and Recommendation Model Based on Integrated Behavior Sequence. In Proceedings of the 30th ACM International Conference on Information and Knowledge Management (Virtual Event, Queensland, Australia) (CIKM '21) . Association for Computing Machinery, New York, NY, USA, 2373-2382. https://doi.org/10.1145/3459637.3482489 [23] Hamed Zamani and W. Bruce Croft. 2018. Joint Modeling and Optimization of Search and Recommendation. In Proceedings of the First Biennial Conference on Design of Experimental Search & Information Retrieval Systems, Bertinoro, Italy, August 28-31, 2018 (CEUR Workshop Proceedings, Vol. 2167) , Omar Alonso and Gianmaria Silvello (Eds.). CEUR-WS.org, 36-41. [24] Hamed Zamani and W. Bruce Croft. 2020. Learning a Joint Search and Recommendation Model from User-Item Interactions. In WSDM '20: The Thirteenth ACM International Conference on Web Search and Data Mining, Houston, TX, USA, February 3-7, 2020 , James Caverlee, Xia (Ben) Hu, Mounia Lalmas, and Wei Wang (Eds.). ACM, 717-725. [25] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414 (2022). [26] Kai Zhao, Yukun Zheng, Tao Zhuang, Xiang Li, and Xiaoyi Zeng. 2022. Joint Learning of E-Commerce Search and Recommendation with a Unified Graph Neural Network. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining (Virtual Event, AZ, USA) (WSDM '22) . Association for Computing Machinery, New York, NY, USA, 1461-1469. https://doi.org/10. 1145/3488560.3498414 [27] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A Survey of Large Language Models. CoRR abs/2303.18223 (2023).",
  "keywords_parsed": [
    "search and recommendation",
    "LLM",
    "multi-domain recommendation"
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "Learning a Hierarchical Embedding Model for Personalized Product Search"
    },
    {
      "ref_id": "b2",
      "title": "Language Models are Few-Shot Learners"
    },
    {
      "ref_id": "b3",
      "title": "Multitask Learning"
    },
    {
      "ref_id": "b4",
      "title": "M6Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems"
    },
    {
      "ref_id": "b5",
      "title": "M6Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems"
    },
    {
      "ref_id": "b6",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
    },
    {
      "ref_id": "b7",
      "title": "Infer Implicit Contexts in Real-Time Online-to-Offline Recommendation"
    },
    {
      "ref_id": "b8",
      "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling"
    },
    {
      "ref_id": "b9",
      "title": "Domain-Adversarial Training of Neural Networks"
    },
    {
      "ref_id": "b10",
      "title": "A Kernel Two-Sample Test"
    },
    {
      "ref_id": "b11",
      "title": "SAMD: An Industrial Framework for Heterogeneous MultiScenario Recommendation"
    },
    {
      "ref_id": "b12",
      "title": "Multi-Aspect Dense Retrieval"
    },
    {
      "ref_id": "b13",
      "title": "Prototypical Contrastive Learning and Adaptive Interest Selection for Candidate Generation in Recommendations"
    },
    {
      "ref_id": "b14",
      "title": "A Category-Aware Multi-Interest Model for Personalized Product Search"
    },
    {
      "ref_id": "b15",
      "title": "Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts"
    },
    {
      "ref_id": "b16",
      "title": "One Model to Serve All: Star Topology Adaptive Recommender for Multi-Domain CTR Prediction"
    },
    {
      "ref_id": "b17",
      "title": "A Model-Agnostic Causal Learning Framework for Recommendation Using Search Data"
    },
    {
      "ref_id": "b18",
      "title": "When Search Meets Recommendation: Learning Disentangled Search Representation for Recommendation"
    },
    {
      "ref_id": "b19",
      "title": "Progressive Layered Extraction (PLE): A Novel Multi-Task Learning (MTL) Model for Personalized Recommendations"
    },
    {
      "ref_id": "b20",
      "title": "LLaMA: Open and Efficient Foundation Language Models"
    },
    {
      "ref_id": "b21",
      "title": "Deep Learning for Matching in Search and Recommendation"
    },
    {
      "ref_id": "b22",
      "title": "USER: A Unified Information Search and Recommendation Model Based on Integrated Behavior Sequence"
    },
    {
      "ref_id": "b23",
      "title": "Joint Modeling and Optimization of Search and Recommendation"
    },
    {
      "ref_id": "b24",
      "title": "Learning a Joint Search and Recommendation Model from User-Item Interactions"
    },
    {
      "ref_id": "b25",
      "title": "Glm-130b: An open bilingual pre-trained model"
    },
    {
      "ref_id": "b26",
      "title": "Joint Learning of E-Commerce Search and Recommendation with a Unified Graph Neural Network"
    },
    {
      "ref_id": "b27",
      "title": "A Survey of Large Language Models"
    }
  ]
}