{"ArchRepair : Block-Level Architecture-Oriented Repairing for Deep Neural Networks": "Hua Qi 1 glyph[star] , Zhijie Wang 2 glyph[star] , Qing Guo 3 glyph[star]glyph[star] , Jianlang Chen 1 , Felix Juefei-Xu 4 , Lei Ma 1 , 5 , 2 , and Jianjun Zhao 1 1 Kyushu University, Japan 2 University of Alberta, Canada 3 Nanyang Technological University, Singapore 4 Alibaba Group, USA 5 Alberta Machine Intelligence Institute (Amii), Canada Abstract. Over the past few years, deep neural networks (DNNs) have achieved tremendous success and have been continuously applied in many application domains. However, during the practical deployment in the industrial tasks, DNNs are found to be erroneous-prone due to various reasons such as overfitting, lacking robustness to real-world corruptions during practical usage. To address these challenges, many recent attempts have been made to repair DNNs for version updates under practical operational contexts by updating weights ( i.e ., network parameters) through retraining, fine-tuning, or direct weight fixing at a neural level. Nevertheless, existing solutions often neglect the effects of neural network architecture and weight relationships across neurons and layers. In this work, as the first attempt, we initiate to repair DNNs by jointly optimizing the architecture and weights at a higher ( i.e ., block) level. We first perform empirical studies to investigate the limitation of whole network-level and layer-level repairing, which motivates us to explore a novel repairing direction for DNN repair at the block level. To this end, we need to further consider techniques to address two key technical challenges, i.e ., block localization , where we should localize the targeted block that we need to fix; and how to perform joint architecture and weight repairing . Specifically, we first propose adversarial-aware spectrum analysis for vulnerable block localization that considers the neurons' status and weights' gradients in blocks during the forward and backward processes, which enables more accurate candidate block localization for repairing even under a few examples. Then, we further propose the architecture-oriented search-based repairing that relaxes the targeted block to a continuous repairing search space at higher deep feature levels. By jointly optimizing the architecture and weights in that space, we can identify a much better block architecture. We implement our proposed repairing techniques as a tool, named ArchRepair , and conduct extensive experiments to validate the proposed method. The results show that our method can not only repair but also enhance accuracy & robustness, outperforming the state-of-the-art DNN repair techniques. Keywords: Deep Learning\u00b7 DNN Repair\u00b7 Network Architecture Search", "1 Introduction": "Modern high-capacity deep neural networks (DNNs) have shown astounding performance in many automated computer vision tasks ranging from complex scene understanding for autonomous driving [58,8,5,47,37,34], to accurate DeepFake media detection [28,10]; from challenging medical imagery grading and diagnosis [63,13,7,54], to billion-scale consumer applications such as the face authentication for mobile payment, etc . Many of the tasks are safety- and mission-critical and the reliability of the deployed DNNs are of utmost importance. However, over the years, we have come to realize that the existence of unintentional (natural degradation corruptions) and intentional (adversarial perturbations) examples such as [6,19,16,55,14,20,21,65,59,35,54,15,26,7] is a stark reminder that DNNs are vulnerable. Compared to existing DNN repair work ( e.g ., [42,66,53,17,49,64]), this work takes the DNN repairing to a whole new level, quite literally, where we are performing block-level architecture-oriented repairing as opposed to networklevel, layer-level, and neuron-level repairing. As we will show in the following sections that block-level repairing, being a midpoint sweet spot in terms of network module granularity, offers a good trade-off between network accuracy and time consumption for that just repairing some specific weights in a layer neglects the relationship between different layers while repairing the whole network weights leads to high cost. In addition, block-level repairing allows us to locally adjust not only the weights but also the network architecture within the block very effectively and efficiently. To tackle the DNN's vulnerability issues, many researchers have resorted to DNN repairing that aims at fixing the faulty DNN weights with the guidance of some specific repairing optimization criteria. An analogy to this is the traditional software repairing in the software engineering literature [18]. However, generalpurpose DNN repairing may not always be feasible in practice, due to (1) the difficulty of generalizing DNNs to any arbitrary unseen scenarios, and (2) the difficulty of generalizing DNNs to seen scenarios but with unpredictable, volatile, and ever-changing deployed environment. For these reasons, a more practical DNN repairing strategy is to work under some assumptions of practical contexts and to perform task-specific and environment-aware DNN repairing where the model gap is closed up for a certain scenario/environment, or a set of scenarios/environments. To this end, as the first attempt, we repair DNNs by jointly optimizing the architecture and weights at the block level in this work. The modern block structure stems from the philosophy of VGG nets [52] and is generalized to a common designing strategy in the state-of-the-art architectures [23] ( e.g ., ResNet) and optimization method [36]. To validate its importance for block-level repairing, we first study the drawbacks of network-level and layer-level repairing, which motivates us to explore a novel research granularity and repairing direction. Eventually, we identified that block-level architecture-oriented DNN repair is a promising direction. In order to achieve this, we need to address two challenges, i.e ., block localization and joint architecture and weight repairing . For the first challenge, we propose the adversarial-aware spectrum analysis for vulnerable block localization that considers the neuron suspiciousness and weights' gradients in blocks during the forward and backward processes when evaluating a series of examples. This method enables more precise block localization even under few-shot examples. In terms of the second challenge, we propose the architectureoriented search-based repairing that relaxes the targeted block to a continuous search space. The space consists of several nodes and edges where the node represents deep features and the edge is an operation to connect two nodes. By jointly optimizing the architecture and weights in that space, our method is able to find a much better block architecture for a specific repairing target. We conduct extensive experiments to validate the proposed repairing method and find that our method can not only enhance the accuracy but also the robustness across various corruptions. The different DNN models repaired with our technique perform better than the original one on both clean and corrupted data, with an average 3.939% improvement on clean data and 7.79% improvement on corrupted data, establishing vigorous general repairing capability on most of the DNN architectures. Overall, the contribution of this paper is summarized as follows: -We propose block-level architecture-oriented repairing for DNN repair. The intuition of block structure design in modern DNNs provides a suitable granularity of DNN repair at the block-level [23]. In addition, we also show that jointly optimizing architecture and weights further brings the advantage of DNN repair over repairing DNN by only updating weights, which is demonstrated by our comparative evaluation in the experimental section. -Technically, we originally propose the adversarial-aware spectrum analysisbased block localization and architecture-oriented search-based repairing method, both of which are novel for DNN repair. The first one enables us to localize a vulnerable block accurately even with only a few examples. The latter formulates the repairing problem as the joint optimization of both the architecture and weights at the block level. -In terms of the novelty and potential impacts , existing DNN repair methods [42,66,53,17,12,49] mostly focus on only repairing DNN via updating its weights while ignoring inherent DNN architecture design ( e.g ., block structure and relationships between different layers), which could also impact the DNN behavior, whereas only repairing the weights could not address such an issue. Therefore, compared with existing work, this paper initiates a new and wide direction for DNN repair by taking relationships of DNN architecture design as well as layers and weights into consideration. -We implement our repairing techniques in the tool ArchRepair and perform extensive evaluation against 6 state-of-the-art DNN repair techniques under 4 DNNs with different architectures on two different datasets. The results demonstrate the advantage of ArchRepair in achieving SOTA repairing performance in terms of both accuracy and robustness. To the best of our knowledge, this is the very first attempt to consider the DNN repairing problem at the block-level that repairs both network weights and architecture jointly. The results of this paper demonstrate the limitation of repairing DNN by only updating the weights, and show that other important DNN development elements such as architecture that encodes more advanced relationships of neurons and layers should also be taken into consideration during the design of DNN repair techniques.", "2 DNN Repairing and Motivation": "In this section, we review existing repairing methods in DNN and motivate our method. In Sec. 2.1, we thoroughly analyze previous DNN repair techniques from the viewpoint of different repairing targets, e.g ., the parameters ( i.e ., weights) of the whole network, layers, or neurons. To this end, we formulate the core mechanism and compare their strengths and weaknesses, which inspires and motivates us to develop the block-level repairing method. To validate our motivation, we perform a preliminary study in Sec. 2.2.", "2.1 DNN repair Solutions": "In the standard training process, given a training dataset, we can train a DNN denoted as \u03c6 ( W , A ) where A represents the network architecture related parameters determining what operations ( e.g ., convolution layer, pooling layer, etc .) are used in the architecture, and W is the respective weights ( i.e ., parameters of different operations). Generally, the architecture A is pre-defined and fixed during the training and testing processes. The variable W consists of weights for different layers. Although existing DNNs ( e.g ., ResNet [23]) have achieved significantly high accuracy on popular datasets, incorrect behaviors are always found in these models when we deploy them in the real world or test them on challenging datasets. There are a series of works that study how to repair these DNNs to be generalizable to misclassified examples, challenging corruptions, or bias errors [53,49,64,56]. In general, we can formulate the existing repairing methods as where W \u2217 is a subset of W and \u02c6 W \u2217 is the fixed counterpart of W \u2217 . The dataset D repair contains the examples for repairing guidance. Different works may set different D repair according to the repairing scenarios. For example, Yu et al . [64] sets D repair as the combination of the augmented training dataset. We will show that our method can address different repairing scenarios. Intuitively, Eq. (1) is to find the weights we need to fix in the DNN, and Eq. (2) with a task-related objective function J ( \u00b7 ) is to fix the selected weights W \u2217 and produce a new one \u02c6 W \u2217 . The above formulation can represent a series of existing repairing methods. For example, when we try to fix all weights of a DNN ( i.e ., W \u2217 = W ) and set the objective function J ( \u00b7 ) as the task-related loss function ( e.g ., cross-entropy function for image classification) with different data augmentation techniques on collected failure cases as D repair to retrain the weights, we actually get the methods proposed by [49] and [64]. In addition, when we employ the gradient loss of weights and forward impact to localize the targeted weights and use a fitness function to fix localized weights, the formulation becomes the method [53]. Nevertheless, with the general formulation in Eq. (1) and Eq. (2), we can see that existing methods have the following limitations: -Existing works only fix the targeted DNN either at the network-level ( i.e ., fixing all weights of the DNN) or at the neuron-level ( i.e ., only fixing partial weights of the DNN), and ignore the effects of the architecture A . -Only repairing some specific weights in a layer could easily neglect the relationship between different layers while repairing the whole network's weights leads to high cost. Note that, the state-of-the-art DNNs ( e.g ., ResNet [23]) are made up of several blocks where each block is built with stacked convolutional and activation layers. Such block-like architecture is mainly inspired by the philosophy of VGG nets [52] and its effectiveness has been demonstrated in wide applications. Therefore in this work, we focus on DNN repairing at the block-level. In particular, we consider both the architecture and weights repairing of a specific block.", "2.2 Empirical Study and Motivation": "First, we perform a preliminary experiment to discuss the effectiveness of the repairing methods at different levels. In this experiment, we choose 3 variants of ResNet [23] (specifically, ResNet-18, ResNet-50, and ResNet-101) as the targeted DNNs \u03c6 , and we select CIFAR-10 dataset as the experimental environment. We repair the DNN at four levels, i.e ., Neuron-level ( i.e ., only fixing weights of one neuron ), Layer-level ( i.e ., only fixing the weights of one layer), Block-level ( i.e ., fixing the weights of a block) and the Network-level ( i.e ., fixing all weights of the DNN). Inspired by recent work [53], we choose the neuron (or layer/block) with the greatest gradient (mean gradient for layer and block) as our target to fix. Note that as the previous work have shown that repairing DNN with only a few failure cases is meaningful and important [49,64], we only randomly select 100 failure cases from the testing dataset to calculate the gradients and choose such neuron (or layer/block). Then, we adjust the weights of the chosen neuron/layer/block by gradient descent w.r.t. the loss function ( e.g ., cross-entropy loss for image classification). To compare their effectiveness, we apply all methods on the same training dataset of CIFAR-10 and Tiny-ImageNet, then measure the accuracy on the respective testing dataset. We also record the execution time of the total repairing phase (100 epochs) as indicator of time cost. We show the repairing result in Table 1. According to Table 1, the network-level repairing achieves the highest accuracy on ResNet-18 and ResNet-101 when repairing CIFAR-10 dataset, and all 3 variants of ResNet when repairing Tiny-ImageNet dataset, but also leads to the highest time cost under every configuration. Among 3 other levels of repairing methods, the block-level repairing achieves the highest accuracy improvement without having drastic increment on time cost ( i.e ., the run-time increment comparing with neuron-level and layer-level is less than 500 seconds on 100 epochs across all 3 ResNets) when repairing on both CIFAR-10 and Tiny-ImageNet. Overall, the network-level repairing is significantly effective on the accuracy improvement but leads to a high time cost. Nevertheless, the block-level repairing achieves impressive accuracy enhancement with much less execution time comparing to network-level method ( e.g ., about 2 \u00d7 less on ResNet-18), making it a good trade-off between effectiveness and efficiency. This fact inspires and motivates us to further investigate the block-level repairing method.", "3 Block-level Architecture and Weights Repairing": "In this section, we first provide an overview of our method in the Sec. 3.1 by presenting our intuitive idea and the main pipeline containing two key modules, i.e ., Vulnerable Block Localization and Architecture-oriented Search-based Repairing . After that, we detail the first module in Sec. 3.2 and the second module in Sec. 3.3, respectively. The first module is to locate the vulnerable block in a deployed DNN, while the second module is to repair the architecture and weights of the localized block by formulating it as an architecture searching problem.", "3.1 Overview": "Given a deployed DNN \u03c6 ( W , A ) , the weights and architecture usually consist of several blocks, each of which is built by stacking basic operations, e.g ., convolutional layer. Then, we represent the weights and architecture with B blocks, i.e ., W = {W i b } B i =1 and A = {A i b } B i =1 , where the weights or architecture of each block are made up by one or multiple layers. For example, when we consider the ResNet-18 [23], we can say that it has six blocks (See Table 2). The first block contains only one convolution layer with the kernel size of 7 \u00d7 7 \u00d7 64 and the stride of 2. The second to the fifth blocks have two convolutional layers and the last block contains a fully connected layer and a softmax layer. Then, we can reformulate Eq. (1) and Eq. (2) for the proposed block-level repairing by where Eq. (3) is to locate the block ( i.e ., ( W \u2217 b , A \u2217 b ) ) that should be fixed through the proposed adversarial-aware block localization, and Eq. (4) is to repair the localized block by formulating it as a network architecture searching problem. Clearly, compared with the general repairing method ( i.e ., Eq. (1) and Eq. (2)), the proposed method focuses on fixing the weights and architecture at the block level. We detail the vulnerable block localization in Sec. 3.2 and architecture search-based repairing in Sec. 3.3. There are two main solutions for vulnerable neurons localization [53,12]. The first one employs the neuron spectrum analysis during the forward process of DNN on a testing dataset. It calculates the spectrum of all neurons ( e.g ., activated/non-activated times of neurons for correctly classified examples and activated/non-activated times of neurons for misclassified examples). These attributes are used to measure the suspiciousness of all neurons. The general principle is that a neuron is more suspicious when the neuron is more often activated under the misclassified examples than that under the correctly classified 0.1 0.5 1 5 10 Number of Failure Examples (K) 0.000 0.005 0.010 0.015 Gi Block 1 Block 2 Block 3 Block 4 examples [12]. This solution is able to localize the vulnerable neurons accurately but requires a large testing dataset, which is not suitable for the scenario where a few examples are available for repairing. The second solution is to actively localize the vulnerable neurons by performing backpropagation on the misclassified examples and calculating the gradients of neurons w.r.t. the loss function. The neurons with large gradients are responsible for the misclassification [53]. This solution is able to localize the vulnerable neuron with fewer examples but ignores the effects of correctly classified examples. As shown in Fig. 1, with different failure examples, the gradients of different convolutional blocks in ResNet18 may have similar values, which demonstrates that the gradient-based localization is not sensitive to the variance of the number of failure examples. Overall, existing methods only focus on localizing vulnerable neurons while ignoring the blocks in DNNs. In addition, they have their respective defects. In this work, we propose a novel localization method that aims to find the most vulnerable block in the DNN, which can lead to the buggy behavior of a deployed DNN. To take the respective advantages of existing works and avoid their defects, we propose adversarial-aware spectrum analysis to localize the vulnerable block.", "3.2 Adversarial-aware Specturm Analysis for Vulnerable Block Localization": "Neuron spectrum analysis Given a dataset D repair for repairing and the targeted DNN \u03c6 ( W , A ) , we calculate the spectrum attributes of the j th neuron in W by counting the times of activation and non-activation for the neuron under the correctly classified examples and denote them as N j ac and N j nc , respectively. Similarly, we can count the times of activation and non-activation for the same neuron under the misclassified examples and name them as N j am and N j nm , respectively. Then, we calculate a suspiciousness score for each neuron via the Tarantula measure [27],", "Algorithm 1: Vulnerable block localization": "Input: A DNN \u03c6 ( W , A ) and datasets D repair and D repair fail Output: W \u2217 b , A \u2217 b 1 Calculate suspiciousness scores S of all neurons via Eq. (5); 2 Calculate the gradients of all neurons on D repair fail and get G ; 3 Update the suspiciousness scores S and get \u02c6 S ; 4 Identify the vulnerable neurons via a threshold glyph[epsilon1] ; 5 Localize the vulnerable block with maximum number of vulnerable neurons; where s j determines the suspiciousness of the j th neuron and the higher s j means the j th neuron is more vulnerable. Adversarial-aware block spectrum analysis With the above neuron spectrum analysis, we can obtain the suspiciousness scores for all neurons and the suspiciousness set S = { s j } . Nevertheless, these suspiciousness scores depend on the statistical analysis and are not related to the objective directly, which leads to less effective localization. To alleviate the issue, we propose to refine the suspiciousness scores with adversarial information under the guidance of the loss function ( e.g ., cross-entropy function for classification). Specifically, we select the failure examples in D repair and construct a subset denoted as D repair fail . For each example in D repair fail , we can calculate the gradient of all neurons w.r.t. the loss function. Then, we average the gradients of a neuron on all examples and get a set G = { g j } where g j is the averaging gradient of the j th neuron on all examples in D repair fail . Intuitively, the larger gradient means that the corresponding neuron may significantly contribute to misclassification and should be tuned to minimize the loss. For the i th block, we denote its gradient as the average of the gradients of all neurons in that block, i.e ., G i = 1 |W i b | \u2211 w j \u2208W i b g j . We also calculate the averaging gradient across all neurons, i.e ., G = 1 B \u2211 B i =1 G i . Then, we use these gradients to reweight the suspiciousness scores of all neurons. The principle behind this strategy is that the suspiciousness score of the j th neuron decreases when its relative gradient is small. As a result, we can update the suspiciousness set S and get \u02c6 S = { \u02c6 s j } . A block in the DNN consists of a series of neurons and we collect the updated suspiciousness scores of the neurons in the i th block to the set \u02c6 S i \u2208 \u02c6 S . There are B suspiciousness sets and \u02c6 S = { \u02c6 S i } B i =1 . After that, we use a threshold ( i.e ., glyph[epsilon1] ) to select the vulnerable neurons, that is, the neuron with \u02c6 s j > glyph[epsilon1] is identified as the vulnerable neuron. Then, we can count the number of vulnerable neurons in each \u02c6 S i and the block with the most vulnerable neurons is identified as the targeted block we would repair. 0.2 1 2 10 Number of Test Inputs (K) (a) 0 5 10 15 20 25 30 35 40 45 0.2 1 2 10 Number of Test Inputs (K) (b) 0 5 10 15 20 25 30 35 40 45 Block 1 Block 2 Block 3 Block 4 Number of Suspicious Neurons We summarize the whole process of the block localization in Algorithm 1. To validate its advantages, we conduct an experiment to compare the effectiveness and stability of the blocks positioned from S and \u02c6 S , respectively. To compare the stability of the method, we changed the size of the dataset D repair fail . We observe that as the size of the dataset changes, the suspicious neurons on each block obtained by S vary significantly while those obtained by \u02c6 S are much more stable and lead to unanimous conclusions. As shown in Fig. 2, according to the experiments on ResNet-18, by the number of suspicious neurons contained in the block, S and \u02c6 S estimated that 'block 1' and 'block 4' are the most vulnerable, respectively. We observed similar results when the threshold glyph[epsilon1] are set to other values ( e.g ., glyph[epsilon1] 10 , glyph[epsilon1] 20 , glyph[epsilon1] 30 , glyph[epsilon1] 40 , glyph[epsilon1] 100 ). We also conduct detailed quantitative analysis and discussion in Sec. 5.3, presenting that repairing the most vulnerable block, i.e ., 'block 4', achieves much higher improvement.", "3.3 Architecture-oriented Search-based Repairing": "After localizing the targeted block, how to break the old architecture's bottleneck and fix it to become competent in the tasks is another challenge. To this end, we formulate the very first block-level architecture and weights repairing as the network architecture search task. Given a deployed DNN with pre-trained weights and fixed architecture ( i.e ., \u03c6 ( W , A ) ), we first relax the targeted block ( i.e ., \u03c6 ( W \u2217 b , A \u2217 b ) ) to a directed acyclic graph like the cell structure in the differentiable architecture search (DARTS) [36], which is composed of an ordered sequence of nodes that are connected by edges. Intuitively, the node corresponds to the deep feature while the edge denotes the operation layer like convolutional layer. Our goal is to optimize the edges, i.e ., to determine which two nodes should be connected and which operation should be selected for that connection. To this end, the key issues are to define the architecture search space and optimization strategy. OUTPUT INPUT 114 7 8 93 109 8 13 92 113 13 9 87 111 5 11 95 113 4 9 96 Nnc Nam Nac Nnm 114 7 8 93 109 8 13 92 113 13 9 87 111 5 11 95 113 4 9 96 Nnc Nam Nac Nnm 114 7 8 93 109 8 13 92 113 13 9 87 111 5 11 95 113 4 9 96 Nnc Nam Nac Nnm 114 7 8 93 109 8 13 92 113 13 9 87 111 5 11 95 113 4 9 96 Nnc Nam Nac Nnm Neuron Spectrum Analysis Adversarial Informa \ufffd on Embedding Suspiciousness Measure Neuron Suspiciousness Updated Suspiciousness Ranking Block5 16 Block2 8 Block4 5 Block3 3 Suspiciousness-based Block Ranking Vulnerable Block Architecture & Weights Repairing opera \ufffdo ns \u2026 opera \ufffdo ns \u2026 Op \ufffd mized Block Architecture in Con \ufffd nuous Space Discre \ufffd zing the Op \ufffd mized Architecture Deployed DNN Repaired DNN Vulnerable Block Localiza \ufffdon Architecture-oriented Search-based Repairing Relaxing candidate opera \ufffdo ns to con \ufffd nuous search space, represent by matrix DNN DNN Loss Func \ufffd on Opera \ufffd on Candidates RelU RelU RelU RelU Drepair Drepair Drepair fail Relu Architecture search space for the targeted block To better illustrate the process of architecture search, we take the ResNet as an example. Given a block in ResNet containing K operation layers, we reformulate it as a directed acyclic graph that has K +1 nodes { X k } K k =1 and allow each node to accept the outputs from all previous nodes instead of following the sequential order. As shown in Fig. 3, we present an example of the graph representation of the targeted block via nodes and edges. Specifically, we denote the edge for connecting the i th and j th nodes as e ( i,j ) and the node X j can be calculated by where e ( i,j ) ( X i ) is an edge taking the node X i as the input. Then, we define an operation set O containing six candidate operations as presented in Table 3, each of which can be set as the edge. For example, when we select 'None' for e ( i,j ) , the two nodes X i and X j should not be connected. Note that, the raw sequentially ordered block of ResNet is a special case in the defined search space and we can naturally inherent the raw weights and architecture setup as the initialization for the following optimization. Architecture and weights optimization The optimization goal is to select a suitable operation for each edge from the operation set. To this end, we relax the selection as a continuous process by regarding the edge connecting the node i and j as a weighted combination of the outputs of all candidate operations where the parameter \u03b1 o ( i,j ) determines the combination weight of using the operation o for connecting the i th and j th nodes. As a result, we can define the architecture parameters for the edge e ( i,j ) as a vector a ( i,j ) = [ \u03b1 o ( i,j ) | o \u2208 O ] assigning each operation in the O a combination weight. Moreover, for the whole block, we denote its architecture as A \u2217 b = { a ( i,j ) } and respective parameters for all candidate operations as W \u2217 b = { w ( i,j ) } . Then, we can specify the repairing process in Eq. (4) by optimizing the weights ( i.e ., W \u2217 b ) and architecture parameters ( i.e ., A \u2217 b ) on the training dataset and validation dataset, alternatively, that is, we have where J ( \u00b7 ) is specified as the cross-entropy loss function for the image classification task. During the training process, we initialize the block architecture A \u2217 b as the raw block architecture of the targeted DNN, and update the architecture and weights, alternatively. We will detail the repairing process in Sec. 3.4. After getting the optimized architecture ( i.e ., \u02c6 A \u2217 b ) in the continuous search space, we set the operation with maximum combination weight as the edge, i.e ., e ( i,j ) = arg max o \u2208O \u03b1 o ( i,j ) . Then, we retrain the weights \u02c6 W \u2217 b with fixed block architecture.", "3.4 Our Repairing Algorithm": "Fig. 3 displays the whole workflow of ArchRepair . Given a deployed DNN, we first employ the proposed vulnerable block localization to determine the block we aim to fix. Specifically, we use the D repair dataset and the neuron spectrum analysis to get the suspiciousness scores of all neurons, i.e ., S = { s j } . Meanwhile, we use the failure examples in D repair ( i.e ., D repair fail ) to get the gradients of all neurons w.r.t. the loss function ( i.e ., G = { g j } ). Then, we use Eq. (6) and G = { g j } to reweight S = { s j } , thus get \u02c6 S = { \u02c6 s j } . After that, we can calculate the number of vulnerable neurons through a threshold glyph[epsilon1] , that is ,when the suspiciousness score of a neuron is larger than glyph[epsilon1] , the neuron is identified as a vulnerable case. Finally, the block with the largest number of vulnerable cases is selected as the targeted block we want to repair. During the architecture search-based repairing, we reformulate the targeted block as a directed acyclic graph where the deep features are nodes and operations are edges. Then, we relax each edge as a combination of six operations ( i.e ., Eq. (8)) where the combination weights correspond to the architecture parameters A \u2217 b = { a ( i,j ) } . We use the dataset D repair to conduct the architecture and weights optimization via Eq. (9) and Eq. (10) where the original architecture and weights are inherited and serve as the optimization initialization. Hence given the optimized block architecture in the continuous space ( i.e ., \u02c6 A \u2217 b ), we discretize it to the final architecture by preserving the operation with the maximum combination weight and removing other operations. Finally, we use the D repair to fine-tune the weights by fixing the optimized architecture for the repaired DNN.", "4 Experimental Design and Settings": "In this section, we conduct extensive experiments to validate the proposed methods and compare with the state-of-the-art DNN repair techniques, to investigate the following research questions: - RQ1. Does ArchRepair outperform the state-of-the-art (SOTA) DNN repair techniques with better repairing effects? - RQ2. Could ArchRepair repair DNNs on certain failure patterns without sacrificing robustness on clean data and other failure patterns? - RQ3. Is our proposed localization method effective in identifying vulnerable neuron blocks? - RQ4. How do different components of our proposed method impact the overall repairing performance? RQ1 intends to evaluate the overall repairing capability of ArchRepair and to compare it to SOTA DNN repair techniques as baselines. RQ2 aims at exploring the potential of our method in repairing DNN on corrupted data, which are common robustness issues during DNN practical usage in the operational environments. RQ3 intends to examine whether the proposed localization method can precisely locate vulnerable blocks. RQ4 is to explore the contribution that each of ArchRepair 's key components makes on the overall performance of DNN repair.", "4.1 Experimental Setups": "To answer the research questions above, we design our evaluation from multiple perspectives listed in the following. According to above situations, we consider two repairing scenarios that commonly occur in practice: Subject Datasets and Repairing Scenarios. Given a deployed DNN trained on a training dataset D t , we can evaluate it on a testing dataset D v . In the real world, there are a lot of scenes that cannot be covered by D v and the DNN's performance may decrease significantly after the DNN is deployed in its operational environment. For example, there are common corruptions ( i.e ., noise patterns) in the real world that can affect the DNN significantly [24]: Gaussian noise (GN), shot noise (SN), impulse noise (IN), defocus blur (DB), Gaussian blur (GB), motion blur (MB), zoom blur (ZB), snow (SNW), frost (FRO), fog (FOG), brightness (BR), contrast (CTR), elastic transform (ET), pixelate (PIX), and JPEG compression (JPEG). - Repairing the accuracy drift on testing dataset. When we evaluate the DNN on the testing dataset D v , we can collect a few failure examples ( i.e ., 1,000 examples) denoted as D v fail . Then, we set D repair = D v fail \u222a D t and use the proposed or baseline repairing methods to enhance the deployed DNNs. We evaluate the accuracy on the testing dataset where D v fail is excluded ( i.e ., D v \\ D v fail ). Note that, the context of repairing DNN with only a few testing data is meaningful and important, which is adopted by recent works [49,64]. In addition, there could be many practical scenarios in which collecting buggy example is very difficult or at very high cost, with only a few buggy examples collected entirely. Hence, we follow the common choice in recent works [49,64] to select only 1,000 failure examples from testing data. - Repairing the robustness on corrupted datasets. When we evaluate the DNN on a corrupted testing dataset D c , we can also collect a few failure examples ( i.e ., 1,000 examples) denoted as D c fail and set D repair = D c fail \u222a D t . The repairing goal is to enhance the accuracy on D c \\D c fail and other corrupted datasets while maintaining the accuracy on the clean testing dataset ( i.e ., D v \\ D v fail ). We choose CIFAR-10 [30] and Tiny-ImageNet [33] as the evaluation datasets. They are commonly used datasets in recent DNN repair studies, which enables us for comparative studies in a relatively fair way. Each dataset contains their respective training dataset D t and testing dataset D v . CIFAR-10 contains a total of 60,000 images in 10 categories, in which 50,000 images are for D t and the other 10,000 are for D v . Tiny-ImageNet has a training dataset D t with the size of 100,000 images, and a testing dataset D v with the size of 10,000 images. Therefore, we have corrupted testing datasets {D c i } where i = 1 , 2 , . . . , 15 corresponding to the above fifteen corruptions [24]. DNN architectures. We select four different architectures of DNN, i.e ., ResNet-18, ResNet-50, ResNet-101 [23], and DenseNet-121 [25]. Given that ArchRepair is a block-based repairing method, the block-like architecture, ResNet, turns out to be a perfect research subject. For a broad comparison, we also choose a non-block-like architecture, DenseNet-121, to examine the repairing capability of ArchRepair 6 . For each architecture, we first pre-train them with the original training dataset D t (from CIFAR-10 or Tiny-ImageNet), the model with the highest accuracy in testing dataset D v (from CIFAR-10 or Tiny-ImageNet) will be saved as pre-trained model \u03c6 \u03b8 . As the original ResNet and DenseNet are not designed for CIFAR-10 and Tiny-ImageNet datasets, we use the unofficial architecture code offered by a popular GitHub project 7 , which has more than 4.1K stars. Hyper-parameters. In terms of the training setup, we employ stochastic gradient descent (SGD) as the optimizer, setting batch size as 128, the initial learning rate as 0.1 and the weight decay as 0.0005. We use cross-entropy loss as the loss function. The maximum number of epochs is 500, and an early-stop function will terminate the training phase when the validation loss no longer decreases in 10 epochs. Baselines. To demonstrate the repairing capability of the proposed ArchRepair , we select 6 SOTA DNN repair methods from two different categories as baselines: neuron-level repairing methods and network-level repairing methods. The neuron-level repairing methods focus on repairing certain neurons' weight in order to repair the DNNs, representative methods from this category are MODE [42], Apricot [66], and Arachne [53]. While network-level repairing methods mainly repair DNNs by using augmented datasets to fine-tune the whole network, where SENSEI [17], Few-Shot [49], and DeepRepair [64] are the most popular ones. For a fair comparison, we employ the same settings on all six repairing methods and ArchRepair . In order to fully evaluate the effectiveness of proposed method, we apply all methods (six baselines and ArchRepair ) to fix 4 different DNN architectures on large-scale datasets, including the clean version and 15 corrupted version from CIFAR-10 and Tiny-ImageNet, to assess the repairing capability. Other configurations. We implement ArchRepair in Python 3.9 based on PyTorch framework. All the experiments were performed on a same server with a 12-core 3.60GHz Xeon CPU E5-1650, 128GB RAM and four NVIDIA GeForce RTX 3090 GPUs (24GB memory of each). The opreation system is Ubuntu 18.04. In summary, for each baseline method and ArchRepair , our evaluation consists of 64 configurations (4 DNN architectures \u00d7 16 versions of a dataset 8 ) on both CIFAR-10 and Tiny-ImageNet. For CIFAR-10 dataset, an execution of training and repairing a model under one specific configuration costs about 12 hours on average (the maximum one is about 50 hours); while for Tiny-ImageNet dataset, an execution of training and repairing a model takes about 18 hours on average (the maximum one is about 64 hours). Overall, the total execution time of our experiments is more than 2 months.", "5 Experimental Results": "In this section, we summarize the high-level results and findings for answering our research questions.", "5.1 RQ1: Does ArchRepair outperform the state-of-the-arts (SOTA) DNN repair techniques?": "To answer RQ1, we train 4 DNNs ( i.e ., ResNet-18, ResNet-50, ResNet-101, and DenseNet-101) on CIFAR-10's and Tiny-ImageNet's training dataset ( i.e ., D t ) and evaluate them on testing datasets ( i.e ., D v ) respectively. To evaluate the performance of our method ( i.e ., ArchRepair ), we apply six different SOTA methods as well as ArchRepair to repair these 4 DNNs. The evaluation results of repairing are summarized in Table 4. In general, ArchRepair exhibits significant advantages over all baseline methods on the 4 DNNs, demonstrating its effectiveness and generalization ability of the proposed method. In particular, comparing with the state-of-the-art DNN repair methods ( i.e ., neuron-level repairing method Arachne [53], and network-level repairing method DeepRepair [64]), ArchRepair achieves much higher accuracy on all 4 DNNs on CIFAR-10 dataset. On the more challenging dataset, Tiny-ImageNet, ArchRepair still achieves much higher accuracy on 2 out of 4 DNNs. Note that on DenseNet-121, all the repairing methods failed to repair, i.e ., didn't improve the performance comparing to the original network. One possible explanation is that the original DenseNet-121's performance has almost reached the upper-bound of the classification accuracy on Tiny-ImageNet (highest accuracy among 4 different DNNs), hence there might not be much room for improvement in terms of the accuracy. Furthermore, to understand the influence of repairing on DNN's robustness, we evaluate the repaired DNNs' performance on corruption datasets ( i.e ., CIFAR10-C [24] and Tiny-ImageNet-C [24]). The CIFAR-10-C and Tiny-ImageNet-C clean GN SN IN DB GB MB ZB SNW FRO FOG BR CTR ET PIX JPEG ResNet-18 50 60 70 80 90 Acc.(%) clean GN SN IN DB GB MB ZB SNW FRO FOG BR CTR ET PIX JPEG ResNet-50 50 60 70 80 90 Acc.(%) clean GN SN IN DB GB MB ZB SNW FRO FOG BR CTR ET PIX JPEG ResNet-101 50 60 70 80 90 Acc.(%) clean GN SN IN DB GB MB ZB SNW FRO FOG BR CTR ET PIX JPEG DenseNet-121 50 60 70 80 90 Acc.(%) Original Arachne Apricot SENSEI DeepRepair ArchRepair contain over 15 types of natural corruption datasets, and we show the results on CIFAR-10-C in Fig. 4 and Tiny-ImageNet-C in Fig. 5. Obviously in Fig. 4, ArchRepair achieves the highest accuracy on a majority of corruption datasets across three variants of ResNet (8/15, 9/15, and 7/15 on ResNet-18, ResNet-50, and ResNet-101, respectively) besides the best performance on clean dataset. Even on DenseNet-121, which is not a block-like DNN, ArchRepair also achieves promising performance compared with SOTA method Apricot [66]. The performance of ArchRepair are also significant on Tiny-ImageNet-C. As we've mentioned before, Tiny-ImageNet is way more challenging. Nevertheless, ArchRepair still outperforms baselines in terms of the robustness on a majority of corruption datasets across three variants of ResNet (9/15, 9/15, and 7/15 on ResNet-18, ResNet-50, and ResNet-101, respectively) as well as the non-block-like DNN DenseNet-121 (8/15). This fact confirms that ArchRepair doesn't harm the DNN's robustness, and on the contrary, it can even sometimes improve DNN's generalization ability towards classifying corrupted data. clean GN SN IN DB GB MB ZB SNW FRO FOG BR CTR ET PIX JPEG ResNet-18 0 10 20 30 40 50 Acc.(%) clean GN SN IN DB GB MB ZB SNW FRO FOG BR CTR ET PIX JPEG ResNet-50 0 10 20 30 40 50 Acc.(%) clean GN SN IN DB GB MB ZB SNW FRO FOG BR CTR ET PIX JPEG ResNet-101 0 10 20 30 40 50 Acc.(%) clean GN SN IN DB GB MB ZB SNW FRO FOG BR CTR ET PIX JPEG DenseNet-121 0 10 20 30 40 50 Acc.(%) Original Arachne Apricot SENSEI DeepRepair ArchRepair Answer to RQ1: According to the experimental results on clean dataset, ArchRepair outperforms the SOTA repairing method on all 4 DNNs with different architetures ( i.e ., ResNet-18, ResNet-50, ResNet-101, and DenseNet121). Moreover, the experimental results on corruption datasets also support that ArchRepair can repair a DNN without harming its robustness.", "5.2 RQ2: Can ArchRepair fix DNN on a certain failure pattern without sacrificing robustness on clean data and other failure patterns?": "In Sec. 5.1, our investigation results demonstrated that ArchRepair will not affect DNN's robustness when repairing on the clean dataset. Hence in this section, we continue to validate whether our method harms DNN's robustness when repairing a specific failure pattern. We first verify the repairing capability of ArchRepair . We repair a deployed DNN ( i.e ., ResNet-18) on each of the corruption datasets from CIFAR-10-C and Tiny-ImageNet-C, and compare the performance with the other repairing methods, where the results are summarized in Table 5. Comparing the experimental results on the corruption dataset, we see that all repairing methods have the capability to repair the failure patterns, except shot noise (SN) on Tiny-ImageNetC (all repairing methods fail to repair this corruption pattern). Among these repairing techniques, our method ArchRepair has the highest accuracy on 8 out of 15 the corruption datasets on CIFAR-10-C dataset, and 9 out of 15 the corruption datasets on Tiny-ImageNet-C, respectively, demonstrating that ArchRepair exhibits the advantages in repairing failure patterns. To validate whether our method has harmed DNN's robustness, we also evaluate the performance of repaired DNNs on the other corruption datasets. The evaluation results on CIFAR-10 and Tiny-ImageNet are shown in Fig. 6 and Fig. 7, respectively. Comparing the accuracy difference on CIFAR-10-C (see Fig. 6), we observe that the DNNs repaired by ArchRepair ( i.e ., the red bar) have higher accuracies on both clean and corruption datasets than the original DNN ( i.e ., the gray bar, which is lower than others in most of the cases), indicating that repairing method will not harm the DNN's robustness when having fixed certain corruption patterns. This is also verified by the results on Tiny-ImageNet-C (see Fig. 7), where repairing on a certain corruption pattern will not affect the DNN's robustness on clean dataset and other corruption patterns, instead, it can even significantly enhance the robustness in some cases ( e.g ., when repairing on Fog corruption, the performance on other corruptions is also improved). Answer to RQ2: ArchRepair can successfully fix a certain corruption pattern on a deployed DNN ( i.e ., ResNet-18), outperforming the existing 4 DNN repair methods. In addition, ArchRepair 's repairing doesn't harm DNN's robustness on clean dataset and other failure patterns. clean GN SN IN DB GB MB ZB SNW FRO FOG BR CTR ET PIX JPEG Gaussian Noise 50 60 70 80 90 Acc.(%) clean GN SN IN DB GB MB ZB SNW FRO FOG BR CTR ET PIX JPEG Shot Noise 50 60 70 80 90 Acc.(%) clean GN SN IN DB GB MB ZB SNW FRO FOG BR CTR ET PIX JPEG Impulse Noise 50 60 70 80 90 Acc.(%) clean GN SN IN DB GB MB ZB SNW FRO FOG BR CTR ET PIX JPEG Defocus Blur 50 60 70 80 90 Acc.(%) clean GN SN IN DB GB MB ZB SNW FRO FOG BR CTR ET PIX JPEG Glass Blur 50 60 70 80 90 Acc.(%) clean GN SN IN DB GB MB ZB SNW FRO FOG BR CTR ET PIX JPEG Motion Blur 50 60 70 80 90 Acc.(%) clean GN SN IN DB GB MB ZB SNW FRO FOG BR CTR ET PIX JPEG Zoom Blur 50 60 70 80 90 Acc.(%) clean GN SN IN DB GB MB ZB SNW FRO FOG BR CTR ET PIX JPEG 50 60 70 80 90 Acc.(%) Snow clean GN SN IN DB GB MB ZB SNW FRO FOG BR CTR ET PIX JPEG Frost 50 60 70 80 90 Acc.(%) clean GN SN IN DB GB MB ZB SNW FRO FOG BR CTR ET PIX JPEG Fog 50 60 70 80 90 Acc.(%) clean GN SN IN DB GB MB ZB SNW FRO FOG BR CTR ET PIX JPEG Brightness 50 60 70 80 90 Acc.(%) clean GN SN IN DB GB MB ZB SNW FRO FOG BR CTR ET PIX JPEG Contrast 50 60 70 80 90 Acc.(%) clean GN SN IN DB GB MB ZB SNW FRO FOG BR CTR ET PIX JPEG Elastic Transform 50 60 70 80 90 Acc.(%) clean GN SN IN DB GB MB ZB SNW FRO FOG BR CTR ET PIX JPEG Pixelate 50 60 70 80 90 Acc.(%) clean GN SN IN DB GB MB ZB SNW FRO FOG BR CTR ET PIX JPEG JPEG Compression 50 60 70 80 90 Acc.(%) Original Arachne Apricot SENSEI DeepRepair ArchRepair glyph[negationslash] clean GN SN IN DB GB MB ZB SNW FRO FOG BR CTR ET PIX JPEG Gaussian Noise 0 10 20 30 40 50 Acc.(%) clean GN SN IN DB GB MB ZB SNW FRO FOG BR CTR ET PIX JPEG Shot Noise 0 10 20 30 40 50 Acc.(%) clean GN SN IN DB GB MB ZB SNW FRO FOG BR CTR ET PIX JPEG Impulse Noise 0 10 20 30 40 50 Acc.(%) clean GN SN IN DB GB MB ZB SNW FRO FOG BR CTR ET PIX JPEG Defocus Blur 0 10 20 30 40 50 Acc.(%) clean GN SN IN DB GB MB ZB SNW FRO FOG BR CTR ET PIX JPEG Glass Blur 0 10 20 30 40 50 Acc.(%) clean GN SN IN DB GB MB ZB SNW FRO FOG BR CTR ET PIX JPEG Motion Blur 0 10 20 30 40 50 Acc.(%) clean GN SN IN DB GB MB ZB SNW FRO FOG BR CTR ET PIX JPEG Zoom Blur 0 10 20 30 40 50 Acc.(%) clean GN SN IN DB GB MB ZB SNW FRO FOG BR CTR ET PIX JPEG 0 10 20 30 40 50 Acc.(%) Snow clean GN SN IN DB GB MB ZB SNW FRO FOG BR CTR ET PIX JPEG Frost 0 10 20 30 40 50 Acc.(%) clean GN SN IN DB GB MB ZB SNW FRO FOG BR CTR ET PIX JPEG Fog 0 10 20 30 40 50 Acc.(%) clean GN SN IN DB GB MB ZB SNW FRO FOG BR CTR ET PIX JPEG Brightness 0 10 20 30 40 50 Acc.(%) clean GN SN IN DB GB MB ZB SNW FRO FOG BR CTR ET PIX JPEG Contrast 0 10 20 30 40 50 Acc.(%) clean GN SN IN DB GB MB ZB SNW FRO FOG BR CTR ET PIX JPEG Elastic Transform 0 10 20 30 40 50 Acc.(%) clean GN SN IN DB GB MB ZB SNW FRO FOG BR CTR ET PIX JPEG Pixelate 0 10 20 30 40 50 Acc.(%) clean GN SN IN DB GB MB ZB SNW FRO FOG BR CTR ET PIX JPEG JPEG Compression 0 10 20 30 40 50 Acc.(%) Original Arachne Apricot SENSEI DeepRepair ArchRepair glyph[negationslash]", "5.3 RQ3: Is our proposed localization effective in identifying vulnerable block candidates?": "To verify the effectiveness of our localization method, we conduct an experiment by applying the repairing method on all 4 blocks of ResNet-18 & ResNet-50, and comparing the accuracy on the clean datasets D v of both CIFAR-10 and Tiny-ImageNet with their block suspiciousness S B ( i.e ., the number of suspicious neurons in correspond block). We calculate the block suspiciousness under 8 different thresholds glyph[epsilon1] i 9 ( i \u2208 { 10 , 20 , 30 , 40 , 50 , 75 , 100 , 150 } ) to evaluate how the threshold glyph[epsilon1] i affects the block suspiciousness. The experimental results are summarized in Table 6. It's worth mentioning that for a simpler DNN architecture, i.e ., ResNet-18, the vulnerable candidate block can be located more accurately when the threshold glyph[epsilon1] i is small. As the threshold glyph[epsilon1] i increases, the block suspiciousness S B on other blocks becomes larger, making the localization method difficult to identify the As shown in Table 6, the block suspiciousness S B of Block 4 in ResNet18 and Block 3 in ResNet-50 are always the highest on both CIFAR-10 and Tiny-ImageNet datasets, no matter what value the threshold glyph[epsilon1] i is. It matches the performance of repaired DNNs, where the DNN repaired on Block 4 in ResNet-18 and Block 3 in ResNet-50 has the highest accuracy, respectively. This demonstrates that our localization method can correctly locate the most vulnerable block. vulnerable block. While for ResNet-50 (a relatively complex DNN), no matter what value the threshold glyph[epsilon1] i is, the localization result are always significantly accurate (with a much higher suspiciousness S B comparing with other blocks). Answer to RQ3: ArchRepair can always locate the most vulnerable block regardless the settings of threshold glyph[epsilon1] i on different DNNs' architectures ( e.g ., ResNet-18 and ResNet-50).", "5.4 RQ4: How different components of ArchRepair impact its overall performance?": "To demonstrate the effectiveness of our ArchRepair and investigate how each component has contribute to its overall performance, we conduct an ablation study by repairing 4 pre-trained models ( i.e ., ResNet-18, ResNet-50, ResNet-101, and DenseNet-121) with two variants of our method on both CIFAR-10 and Tiny-ImageNet datasets. Table 7 summarizes the evaluation results. The first one performs ArchRepair on one single layer of the DNN, and we denote these variants as 'Layer-lv' in Table 7. The second one is our full (complete) version that applies ArchRepair at the block level, we denote this variant as 'Block-lv' in Table 7. Comparing with the original DNNs, the performance of 'Layer-lv' is acceptable on CIFAR-10 dataset, as it slightly improves the behaviors on three DNNs ( i.e ., ResNet-18, ResNet-50, and DenseNet-121) and only decreases slightly on ResNet101. The 'Block-lv' achieves better performance on all of the four DNNs on CIFAR-10, and these results indicate that ArchRepair 's repairing capability is effective at both levels. The performance on 'Block-lv' is better than the 'Layer-lv' on all the four DNNs on two different datasets, especially on the more challenging dataset Tiny-ImageNet, where 'Layer-lv' only shows small improvement on ResNet-18 while 'Block-lv' has significant improvement on all three variants of ResNet. This demonstrate that repairing on one specific layer cannot fully unleash ArchRepair 's potential while repairing on a block enables to take the advantage of all components of ArchRepair . Note that even though both 'Block-lv' and 'Layer-lv' fail to repair DenseNet-121 on Tiny-ImageNet (as well as all the SOTA baseline methods, see evaluation results in Table 4), 'Block-lv' still performs better than 'Layer-lv'. Answer to RQ4: Block-level repairing is more effective than layer-level one towards fully releasing ArchRepair 's repairing capability. In addition, adjusting the network's architecture and weights simultaneously is more effective than only adjusting the weights, especially for block-level repairing, demonstrating that jointly repairing the block architecture and weights is a promising research direction for DNN repair.", "5.5 Threat to validity": "The threats to the validity could be from the following aspects: 1) The selected dataset and the used model architectures could be a threat. To mitigate it, we selected the popular datasets as well as diverse architectures to evaluate our method. 2) The selection of the corruption dataset could be biased, i.e ., our method may not generalize well on other corruptions. We actually selected the 15 commonly used natural corruptions in the standard benchmarks of previous work [24]. 3) A Further threat is from the implementation of our method as well as the usage of the existing baselines. To mitigate the threat, we carefully follow the configuration as stated in the original papers or implementations, respectively. Moreover, our co-authors carefully test and review our code and the configuration of other tools. Furthermore, to be comprehensive for better understanding the position of ArchRepair , we perform a large scale comparative study against 6 SOTA DNN repair techniques. The results confirm DNN repair could be even more promising and there are still opportunities ahead when going beyond focusing on repairing DNN weights only.", "6 Related Work": "", "6.1 DNN Testing": "DNN testing is an important and relevant technique to DNN repair, aiming to detect potential buggy issues of a DNN. Some recent work focus on testing criteria design. For example, DeepXplore [45] proposes the neuron coverage based on the number of activated neurons on given testing data, where the neuron coverage represents the adequacy of the testing data. Similarly, DeepGauge [40] proposes multi-granularity testing criteria, which are based on neural behaviors. Different from previous work focusing on single neuron's behaviors, DeepCT [39] considers the interactions between the different neurons, and Kim et al . [29] propose the coverage criteria to measure the surprise of the inputs. Some researchers [51,22] also point out that the neuron coverage might fail if most of the neurons are activated by a few test cases, and more further research is still needed along this line. These testing criteria lay the foundation for testing generation techniques to detect defects in DNNs. DeepTest [57] generates test cases based on the guidance of neuron coverage. TensorFuzz [44] proposes a distance-based coverageguided fuzzing techniques to test DNNs. Similarly, DeepHunter [61] proposes another coverage-guided testing technique by integrating the coverage criteria from DeepGauge. Readers can also see [41]. DeepStellar [11] employs the coverage criteria and fuzzing technique for testing the recurrent neural network. More discussions on the progress of deep learning testing can be referred to the recent survey [67,38]. Different from these testing techniques, our work mainly focuses on repairing DNNs and enhance their robustness and generalization ability, which can be considered as the downstream tasks of DNN testing.", "6.2 Fault Localization on Deep Neuron Network": "Fault localization aims to locate the root causing of software failures. Similar approaches have been widely studied for traditional software, which focus on developing faults identification methods such as spectral-based [27,1,31,32,43,68,46], model-based [4,50], slice-based [2], and semantic fault localization [9]. Several works recently introduce fault localization on DNNs to find vulnerable neurons and repair their weights. Representative techniques include sensitivity-based fault localization [53] and spectrum-based fault localization [12]. Eniser et al . [12] try to identify suspicious neurons responsible for unsatisfactory DNN performance, which is an early attempt to introduce fault localization technique on DNNs with promising results. However, these methods only consider a fixed DNN architecture and neuron-aware buggy behaviors, which is less flexible for real-world applications. Our work repairs DNN at a higher level ( i.e ., block level) by localizing the vulnerable block and jointly repairing the block architecture and weights, which is novel and havn't been investigated before.", "6.3 DNN Repair": "So far, there are several attempts for repairing DNN models. Inspired by software debugging, Ma et al . [42] propose a novel model debugging technique for neural network models, which is denoted as MODE. MODE first performs state differential analysis on hidden layers to identify the faulty neurons that are responsible for the misclassification. Then, an input selection algorithm is used to select new input samples to retrain the faulty neurons. Zhang et al . [66] propose a weight-adjustment approach called Apricot to fix the DNN. Apricot first generates a set of reduced DNNs from the original model and trains them with a random subset of the original training dataset, respectively. For each failure example, Apricot separates reduced DNN models into two partitions, one successfully predicts the label and the other not, and takes the mean of the corresponding weight assignments of two partitions. After that, Apricot automatically adjusts the weight with these mean values. Further, Sohn et al . [53] propose a search-based repair technique for DNNs, called Arachne. Unlike other techniques, Arachne directly manipulates the neuron weights without retraining. Arachne first uses positive and negative input data to retain correct behavior and generate a patch, respectively. Then uses Particle Swarm Optimization (PSO) to search and locate faulty neurons, and uses the result of PSO candidate to update neurons' weight, and further calculates fitness value based on the outcomes. Our repairing method is orthogonal to data-augmentation based methods such as SENSEI [17] and DeepRepair [64], where we focus on repairing DNN from the architecture and weight perspective. Our method also goes one step further beyond the weight level ( e.g ., MODE [42], Apricot [66], and Arachne [53]), and considers at a higher granularity by jointly repairing architecture and weights at block level, which is demonstrated to be a promising direction for DNN repairing. Recently, Gao et al . [17] have proposed a new algorithm called SENSEI, which uses guided test generation techniques to address the data augmentation problem for robust generalization of DNNs under natural environmental variations. Firstly, SENSEI uses a genetic search on a space of the natural environmental variants of each training input data to identify the worst variant for augmentation on each epoch. Besides, SENSEI uses a heuristic technique named selective augmentation, which allows skipping augmentation in certain epochs based on an analysis of the DNN's current robustness. Another recent attempt for DNN repair is DeepRepair [64], a method to repair the DNN on the image classification task. DeepRepair uses a style-guided data augmentation for DNN repairing to introduce the unknown failure patterns into the training data to retrain the model and applies clustering-based failure data generation to improve the effectiveness of data augmentation.", "6.4 Neural Architecture Search": "Neural architecture search (NAS) could be another relevant line of our work, aiming to automatically design an architecture instead of handcrafting one. Typical NAS includes evolution-based [48,60], and reinforcement-learning-based [3] methods. However, the resources RL or evolution-based methods leveraged are often very expensive and still unaffordable in practice. More recently, DARTS [36] relaxes the search space to make it continuous so that the search processes can be performed based on the gradient. Differentiable NAS approaches can significantly reduce the computational cost. Our search method is based on PC-DARTS [62], a stability improved variant of DARTS by introducing a partially connected mechanism. The purpose of repairing and NAS is very different. The former intends to fix the buggy behaviors that follow some patterns with generalization capability, while NAS is to design general architecture automatically for better performance ( e.g ., energy efficiency). In this paper, we formulate the block-level joint architecture and weight repairing as a NAS problem, which demonstrates the possibilities and chances for DNN repair along this direction.", "7 Conclusion": "In this work, we have proposed ArchRepair , an architecture-oriented DNN repair at block level, which offers a good trade-off between repaired network accuracy and time consumption, compared to neuron-level, layer-level, and network-level (data augmentation) repairing. To achieve this, two key problems are identified and solved sequentially, i.e ., block localization , and joint architecture and weights repairing . By jointly repairing both architecture and weights on the candidate block for repairing, ArchRepair is able to achieve better repairing performance compared with 6 SOTA techniques. Our extensive evaluation have also demonstrated that our method could not only enhance the accuracy but also the robustness across various corruption patterns while being cost-effective. To the best of our knowledge, this is the very first attempt about DNN repair by considering adjusting both the architecture and weights at the 'block-level'. Our research also initiates a promising direction for further DNN repair research, towards addressing the current urgent industrial demands for reliable and trustworthy DNN deployment in diverse real-world environments.", "References": "1. Abreu, R., Zoeteweij, P., Golsteijn, R., van Gemund, A.J.C.: A practical evaluation of spectrum-based fault localization. Journal of Systems and Software 82 (11), 1780-1792 (Nov 2009) 2. Alves, E., Gligoric, M., Jagannath, V., d'Amorim, M.: Fault-localization using dynamic slicing and change impact analysis. In: 2011 26th IEEE/ACM International Conference on Automated Software Engineering (ASE 2011). pp. 520-523 (Nov 2011), iSSN: 1938-4300 3. Baker, B., Gupta, O., Naik, N., Raskar, R.: Designing Neural Network Architectures using Reinforcement Learning. arXiv:1611.02167 [cs] (Mar 2017), arXiv: 1611.02167 4. Birch, G., Fischer, B., Poppleton, M.: Fast test suite-driven model-based fault localisation with application to pinpointing defects in student programs. Software & Systems Modeling 18 (1), 445-471 (Feb 2019) 5. Chen, Y., Rong, F., Duggal, S., Wang, S., Yan, X., Manivasagam, S., Xue, S., Yumer, E., Urtasun, R.: Geosim: Realistic video simulation via geometry-aware composition for self-driving. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 7230-7240 (June 2021) 6. Cheng, Y., Guo, Q., Juefei-Xu, F., Xie, X., Lin, S.W., Lin, W., Feng, W., Liu, Y.: Pasadena: Perceptually Aware and Stealthy Adversarial Denoise Attack. IEEE Transactions on Multimedia (TMM) (2021) 7. Cheng, Y., Juefei-Xu, F., Guo, Q., Fu, H., Xie, X., Lin, S.W., Lin, W., Liu, Y.: Adversarial exposure attack on diabetic retinopathy imagery. arXiv preprint arXiv:2009.09231 (2020) 8. Choi, C., Choi, J.H., Li, J., Malla, S.: Shared cross-modal trajectory prediction for autonomous driving. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 244-253 (June 2021) 9. Christakis, M., Heizmann, M., Mansur, M.N., Schilling, C., W\u00fcstholz, V.: Semantic Fault Localization and Suspiciousness Ranking. In: Vojnar, T., Zhang, L. (eds.) Tools and Algorithms for the Construction and Analysis of Systems. pp. 226-243. Lecture Notes in Computer Science, Springer International Publishing, Cham (2019) 10. Dolhansky, B., Bitton, J., Pflaum, B., Lu, J., Howes, R., Wang, M., Canton Ferrer, C.: The deepfake detection challenge dataset. arXiv e-prints pp. arXiv-2006 (2020) 11. Du, X., Xie, X., Li, Y., Ma, L., Liu, Y., Zhao, J.: Deepstellar: Model-based quantitative analysis of stateful deep learning systems. In: Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. p. 477-487. ESEC/FSE 2019, Association for Computing Machinery, New York, NY, USA (2019) 12. Eniser, H.F., Gerasimou, S., Sen, A.: Deepfault: Fault localization for deep neural networks. In: H\u00e4hnle, R., van der Aalst, W. (eds.) Fundamental Approaches to Software Engineering. p. v. Springer International Publishing, Cham (2019) 13. Fan, D.P., Zhou, T., Ji, G.P., Zhou, Y., Chen, G., Fu, H., Shen, J., Shao, L.: Inf-net: Automatic covid-19 lung infection segmentation from ct images. IEEE Transactions on Medical Imaging 39 (8), 2626-2637 (2020) 14. Gao, R., Guo, Q., Juefei-Xu, F., Yu, H., Feng, W.: AdvHaze: Adversarial Haze Attack. arXiv preprint arXiv:2104.13673 (2021) 15. Gao, R., Guo, Q., Juefei-Xu, F., Yu, H., Ren, X., Feng, W., Wang, S.: Making images undiscoverable from co-saliency detection. arXiv preprint arXiv:2009.09258 (2020) 16. Gao, R., Guo, Q., Zhang, Q., Juefei-Xu, F., Yu, H., Feng, W.: Adversarial Relighting against Face Recognition. arXiv preprint arXiv:2108.07920 (2021) 17. Gao, X., Saha, R.K., Prasad, M.R., Roychoudhury, A.: Fuzz testing based data augmentation to improve robustness of deep neural networks. In: Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering. p. 1147-1158. ICSE '20, Association for Computing Machinery, New York, NY, USA (2020) 18. Gazzola, L., Micucci, D., Mariani, L.: Automatic software repair: A survey. IEEE Transactions on Software Engineering 45 (1), 34-67 (2017) 19. Guo, Q., Cheng, Z., Juefei-Xu, F., Ma, L., Xie, X., Liu, Y., Zhao, J.: Learning to Adversarially Blur Visual Object Tracking. In: Proceedings of the IEEE International Conference on Computer Vision (ICCV). IEEE (October 2021) 20. Guo, Q., Juefei-Xu, F., Xie, X., Ma, L., Wang, J., Yu, B., Feng, W., Liu, Y.: Watch out! Motion is Blurring the Vision of Your Deep Neural Networks. In: Advances in Neural Information Processing Systems (NeurIPS) (2020) 21. Guo, Q., Xie, X., Juefei-Xu, F., Ma, L., Li, Z., Xue, W., Feng, W., Liu, Y.: SPARK: Spatial-aware Online Incremental Attack Against Visual Tracking. In: Proceedings of the European Conference on Computer Vision (ECCV) (Aug 2020) 22. Harel-Canada, F., Wang, L., Gulzar, M.A., Gu, Q., Kim, M.: Is neuron coverage a meaningful measure for testing deep neural networks? In: Proceedings of the 28th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. ACM (2020) 23. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 770-778. IEEE Computer Society, Los Alamitos, CA, USA (jun 2016) 24. Hendrycks, D., Dietterich, T.: Benchmarking neural network robustness to common corruptions and perturbations. Proceedings of the International Conference on Learning Representations (2019) 25. Huang, G., Liu, Z., van der Maaten, L., Weinberger, K.Q.: Densely connected convolutional networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2017) 26. Huang, Y., Juefei-Xu, F., Guo, Q., Miao, W., Liu, Y., Pu, G.: Advbokeh: Learning to adversarially defocus blur. arXiv preprint (2021) 27. Jones, J.A., Harrold, M.J.: Empirical evaluation of the tarantula automatic faultlocalization technique. In: Proceedings of the 20th IEEE/ACM International Con- ference on Automated Software Engineering. p. 273-282. ASE '05, Association for Computing Machinery, New York, NY, USA (2005) 28. Juefei-Xu, F., Wang, R., Huang, Y., Guo, Q., Ma, L., Liu, Y.: Countering malicious deepfakes: Survey, battleground, and horizon. arXiv preprint arXiv:2103.00218 (2021) 29. Kim, J., Feldt, R., Yoo, S.: Guiding deep learning system testing using surprise adequacy. In: 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE). pp. 1039-1049. IEEE (2019) 30. Krizhevsky, A.: Learning multiple layers of features from tiny images. University of Toronto pp. 32-33 (2009) 31. Landsberg, D., Chockler, H., Kroening, D., Lewis, M.: Evaluation of Measures for Statistical Fault Localisation and an Optimising Scheme. In: Egyed, A., Schaefer, I. (eds.) Fundamental Approaches to Software Engineering. pp. 115-129. Lecture Notes in Computer Science, Springer, Berlin, Heidelberg (2015) 32. Landsberg, D., Sun, Y., Kroening, D.: Optimising Spectrum Based Fault Localisation for Single Fault Programs Using Specifications. In: Russo, A., Sch\u00fcrr, A. (eds.) Fundamental Approaches to Software Engineering. pp. 246-263. Lecture Notes in Computer Science, Springer International Publishing, Cham (2018) 33. Le, Y., Yang, X.: Tiny imagenet visual recognition challenge. In: Stanford CS 231N (2015) 34. Li, Y., Wen, C., Juefei-Xu, F., Feng, C.: Fooling lidar perception via adversarial trajectory perturbation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (October 2021) 35. Li, Y., Wen, C., Juefei-Xu, F., Feng, C.: Fooling LiDAR Perception via Adversarial Trajectory Perturbation. In: Proceedings of the IEEE International Conference on Computer Vision (ICCV). IEEE (October 2021) 36. Liu, H., Simonyan, K., Yang, Y.: Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055 (2018) 37. Luo, C., Yang, X., Yuille, A.: Self-supervised pillar motion learning for autonomous driving. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 3183-3192 (June 2021) 38. Ma, L., Juefei-Xu, F., Xue, M., Hu, Q., Chen, S., Li, B., Liu, Y., Zhao, J., Yin, J., See, S.: Secure Deep Learning Engineering: A Software Quality Assurance Perspective. arXiv preprint arXiv:1810.04538 (2018) 39. Ma, L., Juefei-Xu, F., Xue, M., Li, B., Li, L., Liu, Y., Zhao, J.: Deepct: Tomographic combinatorial testing for deep learning systems. In: 2019 IEEE 26th International Conference on Software Analysis, Evolution and Reengineering (SANER). pp. 614-618. IEEE (2019) 40. Ma, L., Juefei-Xu, F., Zhang, F., Sun, J., Xue, M., Li, B., Chen, C., Su, T., Li, L., Liu, Y., et al.: Deepgauge: Multi-granularity testing criteria for deep learning systems. In: Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering. pp. 120-131 (2018) 41. Ma, L., Zhang, F., Sun, J., Xue, M., Li, B., Juefei-Xu, F., Xie, C., Li, L., Liu, Y., Zhao, J., et al.: Deepmutation: Mutation testing of deep learning systems. In: 2018 IEEE 29th International Symposium on Software Reliability Engineering (ISSRE). pp. 100-111. IEEE (2018) 42. Ma, S., Liu, Y., Lee, W.C., Zhang, X., Grama, A.: Mode: automated neural network model debugging via state differential analysis and input selection. In: Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. pp. 175-186 (2018) 43. Naish, L., Lee, H.J., Ramamohanarao, K.: A model for spectra-based software diagnosis. ACM Transactions on Software Engineering and Methodology 20 (3), 11:1-11:32 (Aug 2011) 44. Odena, A., Goodfellow, I.: Tensorfuzz: Debugging neural networks with coverageguided fuzzing. In: Proceedings of the Thirty-sixth International Conference on Machine Learning (2019) 45. Pei, K., Cao, Y., Yang, J., Jana, S.: Deepxplore: Automated whitebox testing of deep learning systems. In: proceedings of the 26th Symposium on Operating Systems Principles. pp. 1-18 (2017) 46. Perez, A., Abreu, R., van Deursen, A.: A Test-Suite Diagnosability Metric for Spectrum-Based Fault Localization Approaches. In: 2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE). pp. 654-664 (May 2017), iSSN: 1558-1225 47. Prakash, A., Chitta, K., Geiger, A.: Multi-modal fusion transformer for end-to-end autonomous driving. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 7077-7087 (June 2021) 48. Real, E., Aggarwal, A., Huang, Y., Le, Q.V.: Regularized Evolution for Image Classifier Architecture Search. arXiv:1802.01548 [cs] (Feb 2019), arXiv: 1802.01548 49. Ren, X., Yu, B., Qi, H., Juefei-Xu, F., Li, Z., Xue, W., Ma, L., Zhao, J.: Few-shot guided mix for dnn repairing. In: 2020 IEEE International Conference on Software Maintenance and Evolution (ICSME). pp. 717-721 (2020) 50. S. Alves, E.H.d., Cordeiro, L.C., L. Filho, E.B.d.: A method to localize faults in concurrent C programs. Journal of Systems and Software 132 , 336-352 (Oct 2017) 51. Sekhon, J., Fleming, C.: Towards improved testing for deep learning. In: 2019 IEEE/ACM 41st International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER). pp. 85-88. IEEE (2019) 52. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. In: Bengio, Y., LeCun, Y. (eds.) 3rd International Conference on Learning Representations, ICLR 2015 (2015) 53. Sohn, J., Kang, S., Yoo, S.: Search based repair of deep neural networks. arXiv preprint arXiv:1912.12463 (2019) 54. Tian, B., Guo, Q., Juefei-Xu, F., Chan, W.L., Cheng, Y., Li, X., Xie, X., Qin, S.: Bias Field Poses a Threat to DNN-Based X-Ray Recognition. In: IEEE International Conference on Multimedia and Expo (ICME) (2021) 55. Tian, B., Juefei-Xu, F., Guo, Q., Xie, X., Li, X., Liu, Y.: AVA: Adversarial Vignetting Attack against Visual Recognition. In: Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI) (2021) 56. Tian, Y.: Repairing confusion and bias errors for DNN-based image classifiers. In: Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. pp. 16991700 (2020) 57. Tian, Y., Pei, K., Jana, S., Ray, B.: Deeptest: Automated testing of deep-neuralnetwork-driven autonomous cars. In: Proceedings of the 40th international conference on software engineering. pp. 303-314 (2018) 58. Wang, J., Pun, A., Tu, J., Manivasagam, S., Sadat, A., Casas, S., Ren, M., Urtasun, R.: Advsim: Generating safety-critical scenarios for self-driving vehicles. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 9909-9918 (June 2021) 59. Wang, R., Juefei-Xu, F., Guo, Q., Huang, Y., Xie, X., Ma, L., Liu, Y.: Amora: Black-box Adversarial Morphing Attack. In: Proceedings of the ACM International Conference on Multimedia (ACM MM) (2020) 60. Xie, L., Yuille, A.: Genetic CNN. arXiv:1703.01513 [cs] (Mar 2017), arXiv: 1703.01513 61. Xie, X., Ma, L., Juefei-Xu, F., Xue, M., Chen, H., Liu, Y., Zhao, J., Li, B., Yin, J., See, S.: Deephunter: A coverage-guided fuzz testing framework for deep neural networks. In: Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis. pp. 146-157 (2019) 62. Xu, Y., Xie, L., Zhang, X., Chen, X., Qi, G.J., Tian, Q., Xiong, H.: PCDARTS: Partial Channel Connections for Memory-Efficient Architecture Search. arXiv:1907.05737 [cs] (Apr 2020), arXiv: 1907.05737 63. Yim, J., Chopra, R., Spitz, T., Winkens, J., Obika, A., Kelly, C., Askham, H., Lukic, M., Huemer, J., Fasler, K., et al.: Predicting conversion to wet age-related macular degeneration using deep learning. Nature Medicine 26 (6), 892-899 (2020) 64. Yu, B., Qi, H., Guo, Q., Juefei-Xu, F., Xie, X., Ma, L., Zhao, J.: Deeprepair: Styleguided repairing for deep neural networks in the real-world operational environment. IEEE Transactions on Reliability pp. 1-16 (2021) 65. Zhai, L., Juefei-Xu, F., Guo, Q., Xie, X., Ma, L., Feng, W., Qin, S., Liu, Y.: It's raining cats or dogs? adversarial rain attack on dnn perception. arXiv preprint arXiv:2009.09205 (2020) 66. Zhang, H., Chan, W.: Apricot: A weight-adaptation approach to fixing deep learning models. In: 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE). pp. 376-387 (2019) 67. Zhang, J.M., Harman, M., Ma, L., Liu, Y.: Machine learning testing: Survey, landscapes and horizons. IEEE Transactions on Software Engineering (2020) 68. Zhang, L., Yan, L., Zhang, Z., Zhang, J., Chan, W.K., Zheng, Z.: A theoretical analysis on cloning the failed test cases to improve spectrum-based fault localization. Journal of Systems and Software 129 , 35-57 (Jul 2017)"}
