{
  "Explainable CTR Prediction via LLM Reasoning": "Xiaohan Yu Huawei Cloud BU Beijing, China yuxiaohan5@huawei.com",
  "Li Zhang": "Institute of Finance Technology, UCL Civil, Environmental and Geomatic Engineering, UCL United Kingdom ucesl07@ucl.ac.uk Chong Chen ∗ Huawei Cloud BU Beijing, China chenchong55@huawei.com",
  "ABSTRACT": "",
  "1 INTRODUCTION": "Recommendation Systems have become integral to modern user experiences, but lack transparency in their decision-making processes. Existing explainable recommendation methods are hindered by reliance on a post-hoc paradigm, wherein explanation generators are trained independently of the underlying recommender models. This paradigm necessitates substantial human effort in data construction and raises concerns about explanation reliability. In this paper, we present ExpCTR, a novel framework that integrates large language model based explanation generation directly into the CTR prediction process. Inspired by recent advances in reinforcement learning, we employ two carefully designed reward mechanisms, LC alignment, which ensures explanations reflect user intentions, and IC alignment, which maintains consistency with traditional IDbased CTR models. Our approach incorporates an efficient training paradigm with LoRA and a three-stage iterative process. ExpCTR circumvents the need for extensive explanation datasets while fostering synergy between CTR prediction and explanation generation. Experimental results demonstrate that ExpCTR significantly enhances both recommendation accuracy and interpretability across three real-world datasets.",
  "CCS CONCEPTS": "· Do Not Use This Code → Generate the Correct Terms for Your Paper ; Generate the Correct Terms for Your Paper ; Generate the Correct Terms for Your Paper; Generate the Correct Terms for Your Paper.",
  "KEYWORDS": "Large Language Models, Explainability, Recommendation System",
  "ACMReference Format:": "Xiaohan Yu, Li Zhang, and Chong Chen. 2025. Explainable CTR Prediction via LLM Reasoning. In Proceedings of The 18th ACM International Conference on Web Search and Data Mining (WSDM). ACM,NewYork,NY,USA,10pages. https://doi.org/XXXXXXX.XXXXXXX ∗ Corresponding authors Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WSDM, March 10-14, 2025, Hannover, Germany © 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/18/06 https://doi.org/XXXXXXX.XXXXXXX Recommendation Systems (RS) have become a cornerstone of modern user experiences, empowering users to discover relevant and personalized items or contents [14]. Collaborative methods [9, 25, 29] have been dominant in this field, leveraging user-item interaction data for future predictions. While these methods, ranging from simple collaborative approaches to deep neural networks, have demonstrated remarkable efficacy in predicting user engagement, particularly in tasks such as click-through rate (CTR) prediction, they often operate as \"black boxes\", offering recommendations without explaining the underlying rationale [44]. The imperative for transparency and accountability has given rise to the burgeoning of explainable recommendation, which moves beyond mere suggestions by providing justifications. Such explanations provide numerous benefits: building user trust and satisfaction, enhancing persuasiveness, and enabling effective debugging and refinement [34]. Currently, the prevailing approach to explainable recommendation relies on a post-hoc paradigm, where explanations are generated independently of the recommendation model after its predictions are made. These methods necessitate substantial human effort to curate external training datasets through customer review processing or handcrafted rules to produce human-readable explanations. Recently, Large Language Models (LLMs) have emerged as a powerful tool in natural language processing, demonstrating exceptional reasoning capabilities. Their potential to generate humanreadable explanations for complex tasks is particularly promising for explainable recommendation. Studies such as PETER [18] and RecExplainer [16] have explored integrating item and user latent representations into pre-trained language models, harnessing collaborative information to enhance explanation generation. Other researchers probe the innate reasoning capabilities of LLMs for recommendation tasks using in-context learning techniques [24]. Chat-Rec [7] has showcased the potential of LLMs for improving explainability in multi-round conversational contexts. Despite these promising developments, these approaches still rely on post-hoc explanations. They either over-rely on enhancing existing methods by substituting traditional language models with transformerbased LLMs or utilize basic zero-shot generation capabilities. Consequently, research on explainable recommendation with LLMs remains in its infancy. As illustrated in Figure 1, several critical challenges persist: · Resource intensity. Developing high-quality training datasets for explanation generators is resource-intensive, demanding substantial human effort. While customer reviews present a potential source of pseudo-explanations, they necessitate meticulous curation, extraction, and reformulation to yield training samples. WSDM, March 10-14, 2025, Hannover, Germany Xiaohan yu, Li Zhang, Chong Chen Interaction Module User-Item Interaction Use Profile Figure 1: Comparison of current post-hoc paradigm methods and ours. Explanation Generator CTR Model Pseudo Explanations User-Item Interaction Explanation Generator CTR Model Interaction User-Item Interaction Post-hoc ExpCTR Alternatively, methods like Chat-Rec necessitate extensive human involvement through interactive dialogues. · Explanation quality unreliability: The post-hoc paradigm introduces potential discrepancies between the generated explanations and the underlying operations of recommender systems. Current methodologies typically employ a unidirectional information flow, where latent representations or prediction results are passed from the recommender model to a separate explanation generator 1. This unidirectional process lacks mechanisms for quality assessment or feedback from the generated explanations to the existing recommender system. Consequently, there is no assurance that the produced explanations accurately reflect the recommender's internal decision-making process. In light of the aforementioned challenges, we propose ExpCTR, a novel approach that aims to operate in a data-free manner, while fostering synergy between CTR prediction and LLM-based explanation generation. Our method seamlessly integrates LLM-driven explanation generation with the CTR prediction process. Drawing inspiration from recent advancements in reinforcement learning [26], we employ real-world feedback signals to refine the LLM's reasoning capabilities to better align with the objectives of CTR prediction. ExpCTR involves a carefully crafted prompt template, tailored to fully elicit the LLM's reasoning capabilities through a chain-ofthought prompting strategy. Subsequently, we utilize a proximal policy optimization (PPO) algorithm that incorporates two distinct reward mechanisms: (1) LC alignment reward, which ensures that the produced explanations accurately reflect user intentions and preferences, as assessed by an LLM-based CTR predictor. (2) IC alignment reward, which treats the explanations as a textual input feature for a traditional ID-based CTR model, ensuring that the explanations are consistent with the model's internal mechanisms and predicted outcomes. These two rewards collectively incentivize the LLMs to generate explanations that are both human-centric and recommender-aligned. To accommodate the reward designs, we devise a specific training paradigm that leverages LoRA for LLM lightweight fine-tuning. This paradigm is based on a three-stage iterative process, consisting of aligning with user interactions with LC alignment reward, training a CTR model with textual features, and aligning with the recommender system's internal mechanisms with IC alignment reward. These stages are iteratively repeated to progressively improve the ExpCTR's performance. Our approach effectively circumvents the need for extensive explanation data construction and fosters collaboration between LLM-driven explainability and accurate CTR prediction. By deepening the understanding of user preferences and the recommendation mechanism, ExpCTR shows the potential to significantly enhance both interpretability and recommendation effectiveness. Our key contributions can be summarized as follows: · We introduce ExpCTR, an innovative framework that enhances the reasoning capabilities of LLMs to generate precise explanations that are closely aligned with CTR models. This approach simultaneously improves CTR prediction performance and RS interpretability. To the best of our knowledge, this represents the first attempt to leverage LLMs for this dual purpose without dependence on extensive data resources. · Wedevelop a reinforcement learning based approach to efficiently fine-tune LLMs using LoRA. Our approach integrates two meticulously designed reward mechanisms within a tailored three-stage training paradigm. · We conduct a comparative analysis of ExpCTR against several state-of-the-art CTR prediction methods and evaluate the quality of the generated explanations, demonstrating the effectiveness of our method.",
  "2 RELATED WORK": "Explainable recommendation (ER) extends traditional recommendation systems by addressing the \"why\" behind suggested items. ER provides not only item recommendations but also justifications clarifying the rationale for those suggestions [45]. Current methods can be broadly classified into two categories, model-intrinsic and post-hoc. Model-intrinsic methods aim for inherent explainability by leveraging interpretable algorithms [44]. Conversely, post-hoc approaches leverage black-box models for recommendation, followed by a separate explanation model that deciphers the reasoning behind the recommendations. The rise of deep neural networks has propelled post-hoc methods to the forefront, transforming explainable recommendation into a natural language generation task. Early works rely on pre-defined templates or association rules [6, 35]. Later advancements adopt Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) architectures for generating textual explanations [19, 43]. With the advent of the Transformer architecture, researchers have explored their potential for explanation generation [18]. [40] incorporates reinforcement learning techniques to address potential issues like hallucinations. Despite these advancements, these approaches rely on generators trained independently with carefully curated explanation datasets. Given the scarcity of user-item-explanation triplets in real-world RS, substantial efforts have been dedicated to constructing high-quality explanation datasets. Techniques such as word overlap analysis [18], LSH-based near-duplicate detection [17] and a combination of manual and automatic reformulation on dialogue datasets [8] have been employed to this end. Recently, the burgeoning field of LLMs has spurred research on LLM-based explainable recommendation, which still predominantly employs post-hoc approaches. For instance, [7] generates explanations in a zero-shot manner within a conversational scenario. [24] Explainable CTR Prediction via LLM Reasoning WSDM, March 10-14, 2025, Hannover, Germany probes the innate reasoning capabilities of LLMs for recommendation tasks using in-context learning techniques. However, these approaches heavily rely on LLM's intrinsic reasoning capabilities, with the recommender system remaining unaware of the generated explanations, let alone assessing their quality. This raises concerns about their effectiveness and the accuracy of the produced justifications in reflecting the true reasoning behind recommendations. This paper aims to address these limitations by proposing a novel approach that ensures coherent and reliable explanations directly integrated within the recommendation process.",
  "3 PRELIMINARY": "",
  "3.1 Problem Definition": "Let U = { 𝑢 1 , 𝑢 2 , . . . , 𝑢 𝑛 } denote a set of 𝑛 users and I = { 𝑖 1 , 𝑖 2 , . . . , 𝑖 𝑚 } a set of 𝑚 items. The user-item interaction data D is represented by a binary interaction matrix R ∈ { 0 , 1 } 𝑛 × 𝑚 , where R 𝑢,𝑖 indicates whether user 𝑢 has interacted with item 𝑖 . A value of 1 signifies explicit feedback (e.g., watching videos, clicking) and 0 otherwise. Each interaction is associated with a textual review 𝑒 𝑢,𝑖 . The objective of explainable recommendation is to jointly predict future user interactions and generate explanations for these predictions. We formulate this as a probabilistic model:  where Z represents the set of explanations for all user-item pairs and ˆ 𝑦 denotes the predicted interaction scores.",
  "3.2 Theoretical Basis of ExpCTR": "To generate post-hoc explanations, we decompose the joint probability as follows:  Wefirst train a CTR model 𝑓 : U×I → R . This model learns latent representations h 𝑖,𝑗 from user-item interaction and side information (e.g., user demographics, item features). The optimization process is formulated as:  where 𝑦 denotes the ground truth interactions for user-item pairs. We define a generator 𝑔 : U×I× R →V that explains why user 𝑢 might interact positively or negatively with item 𝑖 . This model generates explanations conditioned on the predicted result ˆ 𝑦 . The generator is optimized as:  where 𝑒 𝑢,𝑖 denotes the processed customer reviews used as explanation samples [2, 17, 18]. Post-hoc methodologies exhibit a critical dependency on curated training datasets, which fundamentally shapes the conditional distribution 𝑃 (Z| ˆ 𝑦, D) . Another limitation lies in that the generated explanations exert no influence on the CTR model, thereby failing to guarantee that the explanations faithfully reflect the underlying mechanisms of the CTR model. To address this, we integrate CTR prediction and explanation generation within a unified framework, leveraging LLMs, which can be mathematically expressed as:  Weemploy a LLM to generate explanations, circumventing the need for constructing a high-quality explanation dataset - a laborious and costly task. The CTR model, 𝑃 ( ˆ 𝑦 | Z , D) , depends on the generated explanations Z , thus establishing a direct link between the explanations and their impact on CTR predictions. This approach represents a significant departure from traditional post-hoc methods, as the generated explanations are not simply after-the-fact rationalizations but integral components of the recommendation decision-making process. Concretely, we adapt the CTR model to incorporate the generated explanations as features, denoted by 𝑓 : U × I × V → R . The prediction is then computed as follows:",
  "4 METHODOLOGY": "Figure 2 depicts the overall architecture of ExpCTR. It consists of three primary components: (1) Explanation Generation leverages a LLM to produce textual explanations for recommendations. (2) Reward Design utilizes CTR prediction processes to provide quality assessments for the generated explanations that serve as reward signals. (3) Training Paradigm introduces Lora lightweight finetuning techniques along with an iterative training process.",
  "4.1 Explanation Generation": "Traditional recommendation systems rely on implicit representations of users and items and suffer from a lack of interpretability [5]. However, recent advancements in LLMs have demonstrated extensive world knowledge and advanced reasoning capabilities [23, 27, 38]. These capabilities offer a promising avenue for humaninterpretable explanation generation. To harness this potential, we design a prompt template to guide the LLM to generate effective explanations. The prompt leverages user historical interaction data and frames the LLM as a helpful recommendation assistant:",
  "Prompt Template for Explanation Generation": "You are a helpful online recommendation assistant with access to a vast database of books and reader reviews. A customer has provided you with information about their reading preferences: The liked books: <item_1_1> . . . The disliked books: <item_2_1> . . . Considering the preference of the customer, predict how the customer will consider <Target Item> and give a reason. The answer should be within one sentence. The prompt template, originally developed for a book recommendation task [41, 42], can be easily adapted to different recommendation scenarios with minor adjustments. Items (e.g., <item_1_1> . . . ) WSDM, March 10-14, 2025, Hannover, Germany Xiaohan yu, Li Zhang, Chong Chen Figure 2: The overall architecture of ExpCTR. User-Item Interaction User Profile Explanation Generation Prompt LC Alignment Reward CTR Prediction Prompt IC Alignment Reward LLM Adapter LLM Generated Explanation Stage 1 Stage 2 & Stage 3 CTR Model Explanation Encoder CTR Model Zero Tensors LC  Alignment Training IC Alignment  Training hi,j are represented by their titles, while users are characterized by the titles of items they have interacted with. To further refine user profiles, we categorize these historical items into liked and disliked categories, using interaction signals such as ratings as indicators. A threshold-based function is employed to classify each item in a user's interaction sequence. Items rated above the threshold are considered 'liked', while those below are deemed 'disliked'. The threshold is a hyperparameter adjusted based on dataset characteristics. where 𝑥 denotes the prompt template for explanation generation, as detailed in Section 4.1. 𝜋 init is the initial LLM and 𝜋 RL 𝜙 represents the fine-tuned explanation generation language model to be optimized. 𝛽 is the KL penalty and 𝑅 (Z) is our reward function. This structured prompt empowers the LLM to effectively utilize its knowledge base to infer more nuanced user preferences from the liked and disliked items and generate rationales for why a user might like or dislike a particular target item. By applying this methodology to all user-item interaction pairs D , we obtain a set of explanations Z , where each explanation Z 𝑢,𝑖 ∈ Z corresponds to a specific user-item pair.",
  "4.2 Reward Design": "We optimize the LLM through quality assessment of the generated explanations. Inspired by InstructGPT [26], we leverage a reinforcement learning paradigm to achieve this objective with a well-designed reward function that incentivizes the LLM to generate informative explanations for CTR prediction and accurately represent the underlying user motivations behind their interactions. 4.2.1 Proximal Policy Optimization. Following [26], we adopt the proximal policy optimization (PPO) [30] algorithm for the reinforcement learning process. Given a prompt and response (explanation), the LLM produces a reward determined by a reward function, concluding the episode. The objective function for PPO training is formulated as:    The core concept behind our reward function design is to leverage real-world CTR prediction feedback to enhance explanation quality. We aim to incentivize explanations that accurately reflect user intent and preferences. Crucially, these explanations must also align with the underlying mechanics and predicted outcomes of the CTR models employed by the recommender system. To accomplish this, we decompose the reward function into two key components: Explanation and LLM-CTR Alignment, and Explanation and ID-CTR Alignment. These components will be elaborated in the following sections. 4.2.2 Explanation and LLM-CTR Alignment (LC Alignment). This component evaluates the effectiveness of the LLM's generated explanation towards accurately inferring the intended user behavior. A high LC alignment reward signifies that the explanation successfully conveys the underlying factors influencing user interaction. We operationalize LC alignment reward by leveraging recent advancements in CTR prediction with LLMs. We frame the task as a binary classification problem, where the LLM predicts whether a user will like a given item (e.g., book) based on their rationales (the generated explanation by LLM in Section 4.1). Specifically, we design a prompt template to guide the LLM towards predicting CTR. This template provides context for the LLM, including the user's thoughts about the item and a binary response option (\"Yes\" or \"No\") indicating their decision: Explainable CTR Prediction via LLM Reasoning WSDM, March 10-14, 2025, Hannover, Germany",
  "Prompt Template for CTR Prediction": "Given how the user thinks about a book, identify whether the user will like the target book by answering \"Yes.\" or \"No.\". The user thought: <reason> Whether the user will like the target book target: In this template, <reason> is replaced with the explanation Z from Section 4.1. Formally, we define the predicted CTR score for a user-item pair ( 𝑢, 𝑖 ) as:  where 𝑝 ( 𝑡 0 = V 𝑝𝑜𝑠 ) is the probability of the first generated token 𝑡 0 by LLM that equals V 𝑝𝑜𝑠 . 𝑇 is the temperature for softmax function and V 𝑝𝑜𝑠 = { ' 𝑌𝑒𝑠 ' } , V 𝑛𝑒𝑔 = { ' 𝑁𝑜 ' } . A closer alignment between the CTR prediction and the groundtruth label indicates a more precise explanation, demonstrating the ability to capture the actual factors influencing user's decisions. We formulate the LC alignment reward as:  However, directly using this reward function might lead to unstable gradients due to potential variations in reward scales across different batches [47]. We introduce a normalization and clipping procedure to ensure that reward values are appropriately scaled and bounded:  where mean ( 𝑅 𝐿𝐶 (Z 𝑢𝑖 )) and std ( 𝑅 𝐿𝐶 (Z 𝑢𝑖 )) denotes the mean and standard deviation of the rewards across a batch. The clip function constrains the normalized reward within a predefined bound 𝛿 . 4.2.3 Explanation and ID-CTR Alignment (IC Alignment). The congruence between generated explanations and the CTR model is quantitatively assessed by evaluating their contribution to CTR predictions. A positive reward value potentially signifies that the explanation provides substantial insights into the underlying patterns driving CTR. To rigorously evaluate this alignment, we integrate the generated explanations directly into the existing CTR prediction architecture. This integration serves a dual purpose: evaluating explanatory quality and potentially enhancing predictive accuracy by leveraging latent information within the explanations. Our approach first obtains a dense textual representation for each generated explanation Z 𝑢,𝑖 by employing a pre-trained language model (PLM), 𝑓 𝑒𝑛𝑐𝑜𝑑𝑒𝑟 : V → R 𝑑 , to map the explanation text into a unified semantic space. This enables the capture of underlying meaning and relationships within the explanation. 𝑓 𝑒𝑛𝑐𝑜𝑑𝑒𝑟 can be any frozen pre-trained language model, such as BERT [4], BGE [39] and we derive the dense representation for the explanation as follows:  where z 𝑢,𝑖 denotes the mean pooling hidden representations from the last layer in PLM. Subsequently, we integrate these textual representations with an original ID-based CTR model architecture. This integration facilitates the learning of a joint representation that combines user and item information with the insights provided by the explanation. We propose a simple yet effective concatenation operation to achieve this integration. Specifically, the textual representation z 𝑢,𝑖 is concatenated with the hidden representation h 𝑢,𝑖 (defined in Section 3.2) and fed into the existing CTR model to predict the CTR score as follows:  where 𝑓 (as in Equation 6) can be any original ID-based model architecture, such as DeepFM [22]. To evaluate the impact of semantic representations of LLM's explanations, we compare the performance of the CTR model with and without these explanations and quantify the differences in CTR predictions. A notable performance improvement when explanations are incorporated indicates that the introduced semantic features contribute positively. This implies a better-aligned explanation, justifying a higher reward. This evaluation is formalized as follows:  where ˜ 𝑠 𝑟 𝑢,𝑖 indicates the CTR prediction score obtained without using explanations as input features, by setting the representations z 𝑢,𝑖 to zero vectors. The IC alignment reward is normalized and clipped, as in Equation 10, resulting in 𝑅 𝑛𝑜𝑟𝑚 𝐼𝐶 (Z 𝑢,𝑖 ) . By synergizing the effects of these two reward components during the LLM training phase, we aim to ensure that the generated explanations not only accurately capture user rationales behind their behavior but also contribute meaningfully to the performance of CTR models. This approach promotes explanations that are both faithful and informative, ultimately leading to a more robust and interpretable recommendation system.",
  "4.3 Training Paradigm": "4.3.1 Light Weight Tuning. To mitigate the computational training burden associated with three independent LLMs - the initial LLM 𝜋 𝑖𝑛𝑖𝑡 , explanation generator 𝜋 RL 𝜙 and the LC alignment reward model, we adopt a lightweight tuning approach. Recent findings [11, 12, 20] suggest that LLMs can be effectively compressed without significant performance degradation, owing to the inherently lowerdimensional nature of the information they encode. Leveraging this insight, we employ Low-Rank Adapters (Lora) [12] to optimize our training process which introduces trainable low-rank matrices into each transformer layer, allowing for efficient parameterization while preserving model performance. Specifically, we employ a base LLM as both the initial model and the frozen LC alignment reward model. The explanation generator is instantiated as the base LLM with Lora. Crucially, this strategy drastically reduces the parameters. By consolidating computations into a single LLM with a minimal number of trainable parameters in Lora, we achieve substantial computational efficiency without compromising model quality. 4.3.2 Iterative Training. This section outlines the iterative training methodology employed to optimize the explanation generation WSDM, March 10-14, 2025, Hannover, Germany Xiaohan yu, Li Zhang, Chong Chen model, 𝜋 RL 𝜙 , considering both LC alignment and IC alignment rewards. Our approach involves a three-stage iterative training process, alternating between component-specific optimization phases. Stage 1. LC Alignment. We commence by utilizing a frozen language model 𝜋 𝑖𝑛𝑖𝑡 to compute LC rewards. The explanation generation model 𝜋 RL 𝜙 is then optimized using the LC alignment reward:  This phase establishes a foundational understanding of \"correct\" explanations, aligning the model with user preferences and intentions, and producing factually sound explanations. This stage persists for a predetermined number of iterations, during which we continuously accumulate fresh explanations for each user-item pair. Stage 2. CTR Model Training with Textual Features. Following Stage 1, we accumulate a corpus of generated explanations. These explanations are integrated as textual features with the original ID dataset for training the CTR model 𝑓 :  Stage 3. IC Alignment. In the final stage, we further refine the explanation generation model by incorporating the IC alignment reward:  Stages 2 and 3 are then repeated for a predefined number of iterations, allowing for continuous model refinement and performance improvement. Upon the completion of the training process, we obtain a robustly trained explanation generation model 𝜋 RL 𝜙 and a CTR model 𝑓 that effectively leverages textual features for prediction. This unified training approach prioritizes that generated explanations are informative and likely to resonate with both users and the CTR model, avoiding the pitfall of producing generic or uninformative content.",
  "5 EXPERIMENT": "In this section, we detail the experimental setup to evaluate the performance of ExpCTR. We aim to address the following research questions through a series of rigorous experiments and analyses: · RQ1: How does ExpCTR compare to existing state-of-the-art approaches in terms of generating explanations for recommendation decisions and improving CTR prediction? · RQ2: How effective is the integration of the PPO algorithm in ExpCTR? · RQ3: How does the quality of the explanations produced by our framework measure up?",
  "5.1 Experimental Setting": "5.1.1 Datasets. To comprehensively evaluate the effectiveness and generalizability of our proposed framework, we leverage three publicly available, large-scale datasets: BookCrossing 1 , MovieLens20M 2 , and Amazon Books 3 . Following [1], we employ a stratified 1 https://www.kaggle.com/datasets/somnambwl/bookcrossing-dataset 2 https://grouplens.org/datasets/movielens/20m/ 3 https://jmcauley.ucsd.edu/data/amazon/ randomsampling approach for each user within each dataset. Specifically, we randomly select one item a user interacted with as the target item for prediction. The remaining interacted items, up to a maximum of 10 items chronologically preceding the target item, are considered the user's historical interactions. Then, we partition the constructed data samples into training, validation, and testing sets with a ratio of 8:1:1. For datasets containing rating scores, we binarize the ratings using a threshold where ratings above the threshold are considered positive interactions (items the user liked), while ratings below are considered negative interactions. Specifically, the threshold for the ML-20M and Amazon Books datasets is 4, and 5 for the BookCrossing dataset [31, 48]. Finally, Amazon Books and ML20M comprise 16,000/2,000/2,000 while BookCrossing comprises 32,000/4,000/4,000 data samples. 5.1.2 Compared Methods. For CTR evaluations of ExpCTR, we leverage two distinct scoring mechanisms: LLM scores derived from the LC alignment module, designated as \"ExpCTR-LLM\", and CTR scores with explanations as textual features from the IC alignment module, referred to as \"ExpCTR-Aug\". ExpCTR-LLM reflects the effectiveness of the generated explanations in capturing and articulating user preferences and rationales for future interactions, which results in better outcomes under an LLM scorer. Conversely, a superior ExpCTR-Aug score suggests that the explanation aligns well with the internal workings of ID-based CTR models and provides supplementary information that enhances performance. This dual evaluation approach provides an indirect yet effective method for assessing explanation quality. We compare ExpCTR against diverse established baseline models, encompassing both ID-based and LLM-based recommendation methods: · ID-based methods: Factorization Machines (FM) [29] captures pairwise feature interactions for recommendation tasks. Deep learning models, including DSSM [13], DeepFM [22], AutoInt [31], PNN [28], Fi-GNN [21], DCN [36], DCNV2 [37], utilize multi-layer perceptrons, self-attention mechanisms, and graph neural networks to effectively capture both low-order and highorder feature interactions to enhancing recommendation accuracy. DIN [49] and DIEN [48] leverage attention mechanisms to extract user dynamic interests from their historical behavior sequences. Caser [33], GRU4Rec [10], SASRec [15] and BERT4Rec [32] are sequential-based recommendation models that employ Convolutional Neural Networks (CNNs), Gated Recurrent Units (GRUs), and transformer-encoder architectures for robust user behavior modeling, respectively, leading to more accurate recommendations. · LLM-based methods: In-Context Learning (ICL) for Recommendation [3] leverages an LLM for recommendations by directly posing queries to the LLM. TALLRec [1] adapts LLMs to recommendation scenarios through instruction tuning. 5.1.3 Metrics. To assess the effectiveness of ExpCTR, we utilize multiple regular CTR prediction metrics [22, 49]. Specifically, we evaluate performance using the Area Under the ROC Curve (AUC), binary cross-entropy loss (Log Loss), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE). 5.1.4 Implementation Details. In our experimental setup, we employ LLaMA-3-7b as the foundational model for both explanation Explainable CTR Prediction via LLM Reasoning WSDM, March 10-14, 2025, Hannover, Germany Table 1: Overall performance of different recommendation approaches on three benchmark datasets. The best results within the baseline methods and all methods are highlighted with underlined and boldface. ExpCTR-LLM and ExpCTR-Aug denote the CTR performance of the LC alignment and IC alignment reward model, respectively. generation and LC alignment reward computation. For explanations encoding, we employ BGE-small [39]. The IC alignment reward is built upon the DeepFM and implemented through the open-source project Recbole [46]. Our optimization incorporates a learning rate of 1 × 10 -5 , with a KL penalty of 0.05. The reward clip threshold 𝛿 is set to 1.0. The iterative training paradigm consists of two epochs per iteration. For TALLRec, we leverage the entire training dataset and apply a learning rate of 1 × 10 -4 . All experiments are conducted on a single machine equipped with NVIDIA A800 GPUs. reward function further extending and motivating the potential of LLMs in recommendation scenarios.",
  "5.2 Performance Comparison (RQ1)": "Table 1 presents a comparative analysis of our proposed method with existing ID-based CTR methods and LLM-based methods. The results yield several noteworthy observations: · Baseline models such as ICL and TALLRec demonstrate strong performance across all datasets, particularly when compared to ID-based methods. This suggests that the LLMs possess a robust foundational capability for reasoning and comprehension. Nevertheless, ExpCTR-LLM consistently surpasses these two LLM-based CTR models on all metrics and datasets. This empirical evidence indicates that our generated explanations accurately reflect and describe user behavior patterns, leading to significant performance improvements over ICL, which uses the same frozen LLM scorer, and TALLRec, which is finetuned directly under the CTR prediction task. These findings highlight ExpCTR's capability to leverage the intrinsic reasoning capabilities of LLMs effectively. Additionally, the consistent performance gains underscore the efficacy of our proposed framework, with the integration of reinforcement learning and the LC alignment · ExpCTR-Augemergesasasubstantial advancement over ExpCTRLLM, demonstrating superior performance across all evaluated datasets. This result highlights the pivotal role of the IC alignment reward in augmenting model efficacy. The explanations generated by ExpCTR-Aug offer profound insights into ID-based CTR models, leading to considerable performance improvements compared to the DeepFM baseline, with observed gains of 18.2%, 11.9%, and 17.8% in AUC across the respective datasets. These results underscore the dual advantages of ExpCTR that it not only enhances the interpretability of the recommendation system but also delivers substantial improvements in recommender system accuracy.",
  "5.3 In-depth Analysis of PPO (RQ2)": "5.3.1 PPO Reward Analysis. To investigate the efficacy and learning dynamics of the training paradigm in ExpCTR, we conduct a comprehensive analysis of the reward trajectories during the iterative training process. Specifically, we examined the evolution of LC alignment and IC alignment rewards on ML-20M and BookCrossing datasets. The results of this analysis are presented in Figure 3. Our analysis reveals a consistent upward trend in LC alignment rewards across both datasets, which stabilizes during the final stages of training and coincides with the performance enhancement of ExpCTR-LLM. Notably, the ML-20M dataset exhibits a more pronounced increase, ultimately reaching a higher plateau. This observation aligns with the superior performance obtained on the ML-20M dataset (9.1% improvement in AUC over ICL). These WSDM, March 10-14, 2025, Hannover, Germany Xiaohan yu, Li Zhang, Chong Chen 0 100 200 300 400 500 600 700 Steps 0.10 0.05 0.00 0.05 0.10 0.15 Rewards (a) LC Alignment Reward on ML-20M 0 100 200 300 400 500 600 700 Steps 0.15 0.10 0.05 0.00 0.05 Rewards (b) IC Alignment Reward on ML-20M 0 100 200 300 400 500 600 700 Steps 0.100 0.125 0.150 0.175 0.200 0.225 0.250 0.275 Rewards (c) LC Alignment Reward on BookCrossing 0 100 200 300 400 500 600 700 Steps 0.175 0.150 0.125 0.100 0.075 0.050 0.025 Rewards (d) IC Alignment Reward on BookCrossing Figure 3: Trends of LC alignment and IC alignment rewards across iterative training on ML-20M and BookCrossing datasets. 0.01 0.05 0.1 0.5 KL Penalty 0.60 0.62 0.64 0.66 0.68 0.70 Performance BookCrossing AUC BookCrossing LogLoss Amazon Books AUC Amazon Books LogLoss (a) KL Penalty 𝛽 0.5 1.0 2.0 5.0 Reward Normalization Bound 0.58 0.60 0.62 0.64 0.66 0.68 0.70 Performance BookCrossing AUC BookCrossing LogLoss Amazon Books AUC Amazon Books LogLoss (b) Reward Normalization Bound 𝛿 Figure 4: Hyperparameter sensitivity of ExpCTR in KL penalty 𝛽 and reward normalization bound 𝛿 . observations suggest that our LC alignment reward mechanism effectively steers LLMs towards generating explanations that are increasingly congruent with user behavior patterns and exhibit a strong correlation with subsequent user interactions. In contrast, the IC alignment stage is characterized by a more fluctuating curve across both datasets, with the alignment stabilizing more rapidly compared to LC alignment. This corresponds to the relatively modest improvement observed in ExpLLM-Aug over ExpCTR-LLM. The BookCrossing dataset experiences a slight decline followed by steady growth, reflecting the refinement process of IC alignment for LLM-based explanation generation. This empirical evidence underscores the effectiveness of our training paradigm in fostering the development of a more user-centric and contextually relevant recommendation system. 5.3.2 Hyperparameter Sensitivity Analysis. We assess the sensitivity of hyperparameters in PPO training, specifically focusing on the KL penalty 𝛽 and the reward normalization bound 𝛿 , both of which are crucial for effective PPO training [30]. Figure 4 shows the performance variations across different hyperparameter settings, with Table 2: Explanation generated by ICL and ExpCTR on Amazon Books dataset. 𝛽 ranging from [0.01, 0.05, 0.1, 0.5] and 𝛿 ranging from [0.5, 1.0, 2.0, 5.0], evaluated on the BookCrossing and Amazon Books datasets. For the KL penalty 𝛽 , its effect on ExpCTR's performance is notable across both datasets. Specifically, extreme values of 𝛽 -either too high or too low-detract from the model's capabilities, with a setting of 0.05 typically yielding the most competitive results. In contrast, the reward normalization bound 𝛿 shows significant performance variability on the BookCrossing dataset, while remaining stable on the Amazon Books dataset and we choose 𝛿 = 1 . 0 for both datasets.",
  "5.4 Case Study (RQ3)": "To elucidate the efficacy of ExpCTR in generating improved explanations, we present a comparative analysis of explanations produced by ICL and our proposed approach. Table 2 illustrates representative examples, accompanied by actual user reviews to provide real-world context for our analysis. In the first case, we observe that the user's attitude towards the targeted book is fundamentally positive. The user's comment, \"The right book\" , indicates approval, while the phrase \"class have already started\" represents an extraneous factor beyond the scope of the recommender system. This positive sentiment aligns with the high CTR prediction of 0.8843. ICL gives a negative attitude ( \"neutral or indifferent opinion\" ). However, ExpCTR successfully captures this alignment towards the CTR model, generating a recommendation explanation that accurately reflects the user's probable affinity for the book ( \"similar to the themes of history and mythology in their preferred books\" ). The second case demonstrates a more nuanced Explainable CTR Prediction via LLM Reasoning WSDM, March 10-14, 2025, Hannover, Germany improvement. The ICL approach erroneously infers a negative attitude ( \"likely dislike\" ), contradicting the user's actual 5.0 rating. In contrast, ExpCTR, enhanced by LC alignment reward training, correctly identifies the positive interaction potential ( \"likely enjoy\" ). Furthermore, the explanation generated by our model corresponds closely to the user's actual thoughts, accurately identifying the book's themes of \"business, self-improvement, marketing\" .",
  "6 CONCLUSION": "In the paper, we present ExpCTR to address the limitations of current post-hoc explainable recommendation methods. By integrating LLM-based explanation generation into the CTR prediction process, ExpCTR eliminates the need for extensive data preparation and mitigates reliability issues. Our approach leverages reinforcement learning to align LLM reasoning with both user preferences and the recommender system's internal workings. We believe that ExpCTR represents a significant step forward in the field of explainable recommendations and opens up new avenues for future research.",
  "REFERENCES": "[1] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. Tallrec: An effective and efficient tuning framework to align large language model with recommendation. In Proceedings of the 17th ACM Conference on Recommender Systems . 1007-1014. [2] Zhongxia Chen, Xiting Wang, Xing Xie, Tong Wu, Guoqing Bu, Yining Wang, and Enhong Chen. 2019. Co-attentive multi-task learning for explainable recommendation.. In IJCAI , Vol. 2019. 2137-2143. [3] Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si, Chen Xu, Zhongxiang Sun, Xiao Zhang, and Jun Xu. 2023. Uncovering chatgpt's capabilities in recommender systems. In Proceedings of the 17th ACM Conference on Recommender Systems . 1126-1132. [4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018). [5] Maurizio Ferrari Dacrema, Paolo Cremonesi, and Dietmar Jannach. 2019. Are we really making much progress? A worrying analysis of recent neural recommendation approaches. In Proceedings of the 13th ACM conference on recommender systems . 101-109. [6] Jingyue Gao, Xiting Wang, Yasha Wang, and Xing Xie. 2019. Explainable recommendation through attentive multi-view learning. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 33. 3622-3629. [7] Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, and Jiawei Zhang. 2023. Chat-rec: Towards interactive and explainable llms-augmented recommender system. arXiv preprint arXiv:2303.14524 (2023). [8] Shuyu Guo, Shuo Zhang, Weiwei Sun, Pengjie Ren, Zhumin Chen, and Zhaochun Ren. 2023. Towards explainable conversational recommender systems. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval . 2786-2795. [9] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural collaborative filtering. In Proceedings of the 26th international conference on world wide web . 173-182. [10] Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2016. Session-based Recommendations with Recurrent Neural Networks. In Proceedings of the 4th International Conference on Learning Representations . [11] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. In International conference on machine learning . PMLR, 2790-2799. [12] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021). [13] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. 2013. Learning deep structured semantic models for web search using clickthrough data. In Proceedings of the 22nd ACM international conference on Information & Knowledge Management . 2333-2338. [14] Dietmar Jannach, Markus Zanker, Alexander Felfernig, and Gerhard Friedrich. 2010. Recommender systems: an introduction . Cambridge University Press. [15] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recommendation. In 2018 IEEE international conference on data mining (ICDM) . IEEE, 197-206. [16] Yuxuan Lei, Jianxun Lian, Jing Yao, Xu Huang, Defu Lian, and Xing Xie. 2023. Recexplainer: Aligning large language models for recommendation model interpretability. arXiv preprint arXiv:2311.10947 (2023). [17] Lei Li, Yongfeng Zhang, and Li Chen. 2021. Extra: Explanation ranking datasets for explainable recommendation. In Proceedings of the 44th International ACM SIGIR conference on Research and Development in Information Retrieval . 24632469. [18] Lei Li, Yongfeng Zhang, and Li Chen. 2021. Personalized transformer for explainable recommendation. arXiv preprint arXiv:2105.11601 (2021). [19] Piji Li, Zihao Wang, Zhaochun Ren, Lidong Bing, and Wai Lam. 2017. Neural rating regression with abstractive tips generation for recommendation. In Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval . 345-354. [20] Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190 (2021). [21] Zekun Li, Zeyu Cui, Shu Wu, Xiaoyu Zhang, and Liang Wang. 2019. Fi-gnn: Modeling feature interactions via graph neural networks for ctr prediction. In Proceedings of the 28th ACM international conference on information and knowledge management . 539-548. [22] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong Sun. 2018. xdeepfm: Combining explicit and implicit feature interactions for recommender systems. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1754-1763. [23] Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, and Yue Zhang. 2023. Evaluating the logical reasoning ability of chatgpt and gpt-4. arXiv preprint arXiv:2304.03439 (2023). [24] Junling Liu, Chao Liu, Peilin Zhou, Renjie Lv, Kang Zhou, and Yan Zhang. 2023. Is chatgpt a good recommender? a preliminary study. arXiv preprint arXiv:2304.10149 (2023). [25] Andriy Mnih and Russ R Salakhutdinov. 2007. Probabilistic matrix factorization. Advances in neural information processing systems 20 (2007). [26] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems 35 (2022), 27730-27744. [27] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277 (2023). [28] Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, and Jun Wang. 2016. Product-based neural networks for user response prediction. In 2016 IEEE 16th international conference on data mining (ICDM) . IEEE, 1149-1154. [29] Steffen Rendle. 2010. Factorization machines. In 2010 IEEE International conference on data mining . IEEE, 995-1000. [30] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 (2017). [31] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, and Jian Tang. 2019. Autoint: Automatic feature interaction learning via selfattentive neural networks. In Proceedings of the 28th ACM international conference on information and knowledge management . 1161-1170. [32] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer. In Proceedings of the 28th ACM international conference on information and knowledge management . 1441-1450. [33] Jiaxi Tang, Ke Wang, Liqiang Zhang, Shuai Li, Jiajie Yan, and Zheng Zhang. 2016. Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 427-436. [34] Nava Tintarev. 2007. Explanations of recommendations. In Proceedings of the 2007 ACM conference on Recommender systems . 203-206. [35] Nan Wang, Hongning Wang, Yiling Jia, and Yue Yin. 2018. Explainable recommendation via multi-task learning in opinionated text data. In The 41st international ACMSIGIRconference on research & development in information retrieval . 165-174. [36] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network for ad click predictions. In Proceedings of the ADKDD'17 . 1-7. [37] Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong, and Ed Chi. 2021. Dcn v2: Improved deep & cross network and practical lessons for web-scale learning to rank systems. In Proceedings of the web conference 2021 . 1785-1797. [38] Sean Wu, Michael Koo, Lesley Blum, Andy Black, Liyo Kao, Fabien Scalzo, and Ira Kurtz. 2023. A comparative study of open-source large language models, gpt-4 and claude 2: Multiple-choice test taking in nephrology. arXiv preprint arXiv:2308.04709 (2023). [39] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023. C-Pack: Packaged Resources To Advance General Chinese Embedding. arXiv:2309.07597 [cs.CL] [40] Mengyuan Yang, Mengying Zhu, Yan Wang, Linxun Chen, Yilei Zhao, Xiuyuan Wang, Bing Han, Xiaolin Zheng, and Jianwei Yin. 2024. Fine-Tuning Large Language Model Based Explainable Recommendation with Explainable Quality WSDM, March 10-14, 2025, Hannover, Germany Xiaohan yu, Li Zhang, Chong Chen Reward. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 38. 9250-9259. [41] Xiaohan Yu, Li Zhang, Xin Zhao, and Yue Wang. 2024. Break the ID-Language Barrier: An Adaption Framework for Sequential Recommendation. arXiv preprint arXiv:2411.18262 (2024). [42] Xiaohan Yu, Li Zhang, Xin Zhao, Yue Wang, and Zhongrui Ma. 2024. RA-Rec: An Efficient ID Representation Alignment Framework for LLM-based Recommendation. arXiv preprint arXiv:2402.04527 (2024). [43] Jingsen Zhang, Xu Chen, Jiakai Tang, Weiqi Shao, Quanyu Dai, Zhenhua Dong, and Rui Zhang. 2023. Recommendation with causality enhanced natural language explanations. In Proceedings of the ACM Web Conference 2023 . 876-886. [44] Yongfeng Zhang, Xu Chen, et al. 2020. Explainable recommendation: A survey and new perspectives. Foundations and Trends® in Information Retrieval 14, 1 (2020), 1-101. [45] Yongfeng Zhang, Xu Chen, et al. 2020. Explainable recommendation: A survey and new perspectives. Foundations and Trends® in Information Retrieval 14, 1 (2020), 1-101. [46] Wayne Xin Zhao, Shanlei Mu, Yupeng Hou, Zihan Lin, Yushuo Chen, Xingyu Pan, Kaiyuan Li, Yujie Lu, Hui Wang, Changxin Tian, Yingqian Min, Zhichao Feng, Xinyan Fan, Xu Chen, Pengfei Wang, Wendi Ji, Yaliang Li, Xiaoling Wang, and Ji-Rong Wen. 2021. RecBole: Towards a Unified, Comprehensive and Efficient Framework for Recommendation Algorithms. In CIKM . ACM, 4653-4664. [47] Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, et al. 2023. Secrets of rlhf in large language models part i: Ppo. arXiv preprint arXiv:2307.04964 (2023). [48] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Deep interest evolution network for click-through rate prediction. In Proceedings of the AAAI conference on artificial intelligence , Vol. 33. 5941-5948. [49] Guorui Zhou, Chengru Song, Xiaoqiang Zhu, Xiao Ma, Yanghui Yan, Han Jin, Han Li, and Kun Gai. 2018. Deep Interest Network for Click-Through Rate Prediction. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 1059-1068.",
  "keywords_parsed": [
    "Large Language Models",
    " Explainability",
    " Recommendation System"
  ]
}