{"AutoML for Deep Recommender Systems: A Survey": "RUIQI ZHENG \u2217 , The University of Queensland, Australia LIANG QU \u2217 , The University of Queensland, Australia BIN CUI, Peking University, China YUHUI SHI \u2020 , Southern University of Science and Technology, China HONGZHI YIN \u2020 , University of Queensland, Australia Recommender systems play a significant role in information filtering and have been utilized in different scenarios, such as e-commerce and social media. With the prosperity of deep learning, deep recommender systems show superior performance by capturing non-linear information and item-user relationships. However, the design of deep recommender systems heavily relies on human experiences and expert knowledge. To tackle this problem, Automated Machine Learning (AutoML) is introduced to automatically search for the proper candidates for different parts of deep recommender systems. This survey performs a comprehensive review of the literature in this field. Firstly, we propose an abstract concept for AutoML for deep recommender systems (AutoRecSys) that describes its building blocks and distinguishes it from conventional AutoML techniques and recommender systems. Secondly, we present a taxonomy as a classification framework containing feature selection search, embedding dimension search, feature interaction search, model architecture search, and other components search. Furthermore, we put a particular emphasis on the search space and search strategy, as they are the common thread to connect all methods within each category and enable practitioners to analyze and compare various approaches. Finally, we propose four future promising research directions that will lead this line of research.", "CCS Concepts: \u00b7 Information systems \u2192 Recommender systems .": "Additional Key Words and Phrases: AutoML, survey, taxonomy", "ACMReference Format:": "Ruiqi Zheng, Liang Qu, Bin Cui, Yuhui Shi, and Hongzhi Yin. 2021. AutoML for Deep Recommender Systems: A Survey. ACM Transactions on Information Systems 1, 1, Article 1 (January 2021), 39 pages. https://doi.org/", "XXXXXXX.XXXXXXX": "\u2217 Both authors contributed equally to this research. \u2020 Corresponding authors. This work was supported by the Australian Research Council Future Fellowship (Grant No. FT210100624), the Discovery Project (Grant No. DP190101985), the National Natural Science Foundation of China (Grant No. 61761136008), the Shenzhen Fundamental Research Program (Grant No. JCYJ20200109141235597), the Guangdong Basic and Applied Basic Research Foundation (Grant No. 2021A1515110024), the Shenzhen Peacock Plan (Grant No. KQTD2016112514355531), the Program for Guangdong Introducing Innovative and Entrepreneurial Teams (Grant No. 2017ZT07X386). Authors' addresses: Ruiqi Zheng, The University of Queensland, Brisbane, Queensland, Australia, 4072, ruiqi.zheng@uq. net.au; Liang Qu, The University of Queensland, Brisbane, Queensland, Australia, 4072, l.qu1@uq.net.au; Bin Cui, Peking University, 5 Yiheyuan Rd, Haidian District, Beijing, China, bin.cui@pku.edu.cn; Yuhui Shi, Southern University of Science and Technology, Shenzhen, China, shiyh@sustech.edu.cn; Hongzhi Yin, University of Queensland, St Lucia QLD 4072, Brisbane, Queensland, Australia, 4072, h.yin1@uq.edu.au. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \u00a9 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM. 1046-8188/2021/1-ART1 $15.00 https://doi.org/XXXXXXX.XXXXXXX ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021. 1:2 Ruiqi Zheng, et al. Recommendation Scenarios Conventional AutoML Data ( x ( 1 , y 1 ) x n , y n ) Automated Design for Input Components Automated Design for Learning Components Fig. 1. Illustration of AutoML for recommender systems and conventional AutoML. AutoML for Recommender Systems Conventional AutoML Architecture Candidates Embedding Table Automated Design for Input Components Training Data Training Data Desired Model Desired Recommender System Automated Design for Representation Learning Components Automated Design for Representation Learning Components Activation Function Interaction Function Weight Connection Normal Connection Embedding Lookup", "Embedding Alignment 1 INTRODUCTION": "Interaction Layer Prediction Layer Implicit Interaction Explicit Interaction Aligned Embedding Layer The amount of information has increased tremendously due to the fast expansion of the internet. Users find it challenging to choose what interests them among many options due to the abundance of information. Recommender systems [68, 129] have been utilized in different scenarios, such as e-commerce [55, 138] and social media [19, 120], to improve the user experience. Users count on recommender systems to help them deal with information overload problems and find what they are interested in among the immense sea of options. An effective recommender system predicts users' preferences based on users' previous engagements [8, 57, 112]. x f 0 0 0 0 0 1 1 0 x i 0 0 0 0 0 1 x 2 0 0 0 0 0 0 1 1 0 1 1 0 x 1 Input Layer MD Embedding Layer e f e 1 e 2 e i Over the last several years, the primary model framework of recommender systems has been developed from neighborhood techniques [2, 58, 93] to representation learning [19, 50, 51, 94, 107]. Item-based neighborhood approaches [2, 58, 93] proactively recommend items that are similar to consumers' previous interacted items. Neighborhood techniques have been proven effective in real-world applications due to interpretability and simplicity. In comparison, representationbased methods represent the users and items in the latent embedding space. As the most classic representation-based methods, matrix factorization methods [50, 51] are designed to handle the data sparsity problem with dimensionality reduction. With the prosperity of deep learning [53], deep neural networks (DNN) generate more complicated and informative representations. Theoretically, one single-layer perceptron can mimic any function with enough computation source, and data [69]. Deep recommender systems that integrate deep learning techniques into recommender systems have been proposed to capture the non-linear information and item-user relationships [78, 104]. Therefore, they have become favored in the industrial and academic world. Deep recommender systems [15, 19] typically have four components. The input layer generates the binary features from raw data. The embedding layer maps the binary features into low-dimensional feature space. The interaction layer finds the powerful feature interaction that benefits the model's performance, and the prediction layer generates the model's prediction. Section 2 will introduce the mathematical form of these four components in detail. Although deep recommender systems show promising and encouraging results, they heavily require human experiences, and the lack of careful design for different components leads to suboptimal performance. For example, in the embedding layer, most existing methods [19, 29] simply Desired Recommender System ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021. AutoML for Deep Recommender Systems: A Survey 1:3 assign a uniform embedding dimension for all features, which suffers from the issues such as resource consumption, computation cost, and model representation ability. In the interaction layer, all the 2 \ud835\udc5b\ud835\udc51 order feature interactions are calculated [15, 29, 86, 89], which introduces excessive noise to the model and complicates the training procedure. Automatically designed methods for different components of the deep recommender systems are urgently needed to alleviate humans from complicated and time-consuming work. Recently, Automated Machine Learning (AutoML) [124] has emerged as a promising way to automate some components or the whole machine learning pipeline. Compared with conventional recommender systems, which require experts to develop a specific model, AutoML for deep recommender systems (AutoRecSys) outputs the well-performed deep recommender systems in a data-oriented and task-specific manner by automatically designing different opponents and alleviating human effort. It is more capable of discovering a well-performed model when encountering various application scenarios and outperforming traditional methods. It focuses on the challenges brought by the design of compact search space and efficient search strategy rather than developing one single recommender system model. As shown in Fig. 1, AutoML automatically designs the representation components, such as pooling, convolution, and the number of layers, in computer version applications [92, 122]. However, AutoRecSys is not simply an application of AutoML techniques but is faced with unique challenges [17]. Most existing AutoML methods primarily concentrate on the automatic design of representation learning components, and input components have received little attention because the majority of research is conducted on image understanding issues [62, 82, 118, 143], where the pixels of the image as the input component do not require creating features from the data since they are already in floating-point form. However, for deep recommender systems, the input component like the embedding matrix is the primary factor of memory consumption [108] in comparison with other parameters such as biases and weights. How to properly learn the features from the raw data dramatically influences other components and is crucial to the final model performance. AutoML does not reveal universal or principled approaches to learning features from data and only makes limited progress in this direction [124]. In industry, AutoRecSys has been deployed in large-scale real-world applications to provide discriminative and informative recommendation results. For example, Huawei Noah's Ark Lab implements AutoFIS [60] to automatically search for beneficial feature interactions [60] and illustrates the significant improvements in the Huawei App Store recommender task by a 10-day online A/B test. Given the significant growth rate of AutoRecSys, we believe it is essential to synthesize and describe representative techniques within a uniform and comprehensible paradigm. To the extent of our knowledge, the most relevant survey about automated machine learning for deep recommender systems published formally is a short paper [8]. Our work has the following difference from the above one: (1) Our survey includes more representative AutoRecSys methods from top venues, including MDE(ISIT'2021), SSEDS (SIGIR'2022), \ud835\udc3f 0-SIGN (AAAI'2021), HIRS (KDD'2022), NASR (WWW'2022), OptInter (ICDE'2022). (2) Our work is the first survey to comprehensively review AutoRecSys and present a taxonomy, which appeared on Arxiv on March 25, 2022. (3) Our work includes search space complexity and experiments, which horizontally compare AutoRecSys methods mathematically and empirically. (4) We summarize the core steps of AutoRecSys, and elaborate our analysis on the strengths and defects of AutoRecSys methods instead of generally introducing every model. The contributions of this survey paper are threefold: \u00b7 Wepropose an abstract concept AutoMLforDeepRecommenderSystems(AutoRecSys) that clarifies its procedures and differences from conventional AutoML and conventional ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021. 1:4 Ruiqi Zheng, et al. Fig. 2. Categorization for AutoRecSys methods. AutoRecSys Auto-FIS BP-FIS (SIGIR'2019) [14] AutoFIS (KDD'2020) [60] AutoGroup (SIGIR'2020) [59] FIVES (KDD'2021) [119] FROFIT (NeurIPS'2021) [24] \ud835\udc3f 0-SIGN (AAAI'2021) [99] HIRS (KDD '2022) [100] Auto-MAS AutoCTR (KDD'2020) [97] AMEIR (IJCAI'2021) [132] AutoIAS (CIKM'2021) [113] NASR (WWW'2022) [16] AUTO-OCS Loss Function Search AutoLoss (KDD'2021) [133] Feature Interaction Function Search SIF (WWW'2020) [123] OptInter (ICDE'2022) [71] AutoFeature (CIKM'2020) [45] AutoPI (SIGIR'2021) [74] Auto-EDS Embedding Prunning PEP (ICLR'2021) [65] AMTL (CIKM'2021) [121] RULE (KDD'2021) [12] DeepLight (WSDM'2021) [21] SSEDS (SIGIR'2022) [85] HPO NIS (KDD'2020) [42] DNIS (ArXiv'2020) [17] AutoEmb (ICDM'2021) [135] AutoDim (WWW'2021) [134] ESAPN (SIGIR'2020) [63] Heuristic MDE (ISIT'2021) [26] Auto-FSS AutoField (WWW'2022) [112] AdaFS (KDD'2022) [57] GLIDER(ICLR'2020) [105] AutoCross (KDD'2019) [70] recommender systems. It states that AutoML for deep recommender systems outputs the well-performed deep recommender systems in a data-oriented and task-specific manner by automatically designing different opponents and alleviating human effort. To our best knowledge, this is the first survey that proposes the abstract concept and systematically reviews the literature of AutoRecSys. \u00b7 The second contribution is the introduction of a taxonomy that classifies AutoML methods for recommendation systems. It contains feature selection search, embedding dimension search, feature interaction search, model architecture search, and other components search, as shown in Fig. 2. Moreover, we put a specific emphasis on the search space and search strategy, as they are the common thread to connect all methods within each category and enable practitioners to analyze and compare various approaches. \u00b7 We state our own opinions on existing works and discuss their potential drawbacks. Furthermore, we propose four future promising research directions leading this line of research. This survey paper aims to equip potential new users of AutoML for deep recommender systems with proven and practical techniques. As we plan to survey a broad range of techniques in this field, we cannot cover every methodical detail, and we do not claim to include all available research. Instead, we tend to analyze and summarize common grounds as well as the diversity of approaches. Thus, the prevailing research directions in AutoRecSys can be outlined. The rest of the paper is organized as follows. Section 2 describes how we categorized the approaches. Section 3 introduces the background of deep recommender systems and frequentlyused skills in AutoML for deep recommender systems inspired by Neural Architecture Search (NAS). The five categories in taxonomy: automated feature selection search, automated embedding dimension search, automated feature interaction search, automated model architecture search, and automated other components search, are presented from Section 4 to Section 8. In Section 9, horizontal comparison and empirical analysis for AutoRecSys are performed. In Section 10, future directions are discussed, followed by the conclusions in Section 11.", "2 CLASSIFICATION OF APPROACHES": "To understand how the concept of AutoML for deep recommender systems is implemented, we developed a comprehensive classification of existing methodologies. We do not claim to include all available research since our goals are to analyze different methods and determine their similarities or distinctions. The representative methods are selected from top computer science journals or ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021. AutoML for Deep Recommender Systems: A Survey 1:5 Fig. 3. Annual publications in the field of AutoRecSys. Numbers are based on our survey. We conducted the survey in October 2022. Articles presented in late 2022 most likely had not been published and thus were not discovered through our search. 0 5 10 15 20 25 30 35 Publications (Cumulated) Publications (New) 2019 2020 2021 2022 Cumulated 2 11 25 31 New 2 9 14 6 conferences, such as KDD and WWW. The count of papers published per year is illustrated in Fig. 3. In this section, we describe the analysis questions, which determine our classification, and then our literature surveying procedure.", "2.1 Analysis Questions": "Our guiding question is how can the different components of the deep recommender systems be automatically designed to fit various scenarios and data. Under the guiding question, our survey paper focuses on the following three questions. (1) Which components of the models are automatically designed? (2) How is the compact search space designed so that it is general enough to include popular human-crafted models and not too general to prevent the success of the search for the new models? (3) How is the search algorithm designed so that exploration and exploitation can be balanced to enhance search efficiency and effectiveness?", "2.2 Literature Surveying Procedure": "To comprehensively answer the above analysis questions, we reviewed a wide range of papers discussing AutoML for deep recommender systems. A comparative and iterative literature review process is implemented. In the first round, a set of publications is inspected and summarized based on their answers to our analysis questions. We found that many papers automatically design the same component and encounter similar challenges. Therefore we classified those papers in the same category. The second round focuses on similar challenges within the same category and analyses the proposed methods to deal with similar challenges. In the third cycle, we sorted publications and enlarged our set of publications. Consequently, there is an extensive literature review where a distilled taxonomy includes various papers.", "3 BACKGROUNDS": "", "3.1 Deep Recommender Systems": "The frequently used notations are listed in Table 1. Deep learning has been widely applied to recommender systems because it captures the non-linear information and item-user relationships. Deep ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021. 1:6 Ruiqi Zheng, et al. Fig. 4. Illustration of deep recommender systems. Interaction Layer Prediction Layer Implicit Interaction Explicit Interaction Embedding Layer x f 0 0 0 0 0 1 1 0 x i 0 0 0 0 0 1 x 2 0 0 0 0 0 0 1 1 0 1 1 0 x 1 Input Layer Activation Function Interaction Function Weight Connection Normal Connection Embedding Lookup recommender systems [74] typically have four layers, as shown in Fig. 4: input layer, embedding layer, interaction layer, and prediction layer. We will give an introduction to the above components. 3.1.1 Input Layer. The input data usually contains three types of information, namely, the user profile (user ID, age, city, etc.), the item profile (category, item ID, etc.), and the context information (position, weekday, etc.) [139]. The input data for deep recommender systems is commonly in tabular format, i.e., numerical, categorical, or multi-valued features of multiple fields, as opposed to other forms of information like texts or images. The sample size of the tabular data is typically immense, with a highly sparse feature space [15]. It is common to apply one-hot or multi-hot encoding to map the raw features as binary features into high-dimensional feature space. For categorical feature field \ud835\udc56 , binary feature x \ud835\udc56 is obtained by one-hot encoding. For numeric feature field, the numeric values are bucketed into discrete features manually (e.g. [0, 0, 0, 1] for \ud835\udc4e\ud835\udc54\ud835\udc52 \u2208 [ 0 , 14 ] ) or by training decision trees (e.g., GBDT [35]), and encoded as categorical feature field. Multi-valued fields are encoded by multi-hot encoding. The concatenation of binary features consists of a user-item interaction data instance x = [ x 1 , x 2 , \u00b7 \u00b7 \u00b7 , x \ud835\udc53 ] [133]:  where \ud835\udc53 is the number of feature fields and x \ud835\udc56 is the binary vector for \ud835\udc56 \ud835\udc61\u210e feature field. 3.1.2 Embedding Layer. The binary vectors are usually high-dimensional and sparse, which can be transformed into low-dimensional and dense vectors by feature embeddings. The embedding process is presented as follows: For the binary feature x \ud835\udc56 generated from categorical or numeric feature field, feature embedding e \ud835\udc56 is obtained by:  where E \ud835\udc56 \u2208 R \ud835\udc51 \u00d7 \ud835\udc5b \ud835\udc56 is the embedding matrix transforming the \ud835\udc56 \ud835\udc61\u210e binary feature to condensed feature embedding e \ud835\udc56 . For \ud835\udc56 \ud835\udc61\u210e feature field, \ud835\udc51 is the size of pre-defined low-dimensional embedding and \ud835\udc5b \ud835\udc56 is the number of distinctive feature values. For multi-valued feature field \u210e , E \u210e is the embedding matrix. The embedding is obtained by:  ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021. AutoML for Deep Recommender Systems: A Survey 1:7 where every feature is represented as a sequence, x \u210e \ud835\udc62 \u210e as the one-hot encoded binary vector, and \ud835\udc62 \u210e is the maximal length of the sequence. The embedding e \u210e \u2208 R \ud835\udc51 \u00d7 \ud835\udc62 \u210e can be aggregated to a \ud835\udc51 dimensional vector by mean or sum pooling. The output of embedding layer is the concatenation of all feature embeddings:  3.1.3 Interaction Layer. After the raw features are mapped in the low-dimensional space, an interaction layer is proposed to capture the feature interaction information between different feature fields. There are two types of feature interactions: explicit feature interactions and implicit feature interactions. Explicit feature interactions implement interaction functions among specific features, which are interpretable, and people acknowledge which features play an essential role in the model performance. Implicit feature interactions use multi-layer perceptron (MLP) to learn the non-linear information from all the feature embeddings. Based on the convention definition [56], and the mathematical definition proposed in [71], the result of explicit \ud835\udc5d \ud835\udc61\u210e order ( 1 \u2264 \ud835\udc5d \u2264 \ud835\udc53 ) of feature interaction is obtained by the feature embedding group P = { e \ud835\udc56 } \ud835\udc56 = \ud835\udc50 1 ,\ud835\udc50 2 , \u00b7\u00b7\u00b7 ,\ud835\udc50 \ud835\udc5d :  where every \ud835\udc52 \ud835\udc56 in P is searched from the concatenation of all feature embeddings \ud835\udc38 , and \ud835\udc5c ( \ud835\udc5d -1 ) (\u00b7) is a feature interaction function, commonly designed by human experts. For example, Factorization Machines (FM) [11, 89] implements the inner product of feature embeddings to explicitly model the 2 \ud835\udc5b\ud835\udc51 order feature interactions, and define 1 \ud835\udc60\ud835\udc61 order interaction as the binary vector x \ud835\udc56 . In this scenario, the output of the interaction layer \ud835\udc8d \ud835\udc39\ud835\udc3c will be the output of FM:  where \ud835\udc98 is the weight for binary vector x , e \ud835\udc56 is the low dimensional feature embedding of \ud835\udc56 \ud835\udc61\u210e field, and < e \ud835\udc56 , e \ud835\udc57 > is the inner product of vector e \ud835\udc56 and vector e \ud835\udc57 . In theory, FM can explicitly model any order of feature interactions by the inner product of the corresponding feature embeddings. However, high order ( \ud835\udc5d \ud835\udc61\u210e order with \ud835\udc5d \u2265 3) feature interaction introduces exponentially grown computation with respect to \ud835\udc5d . Multi-layer perceptron (MLP) can both learn the implicit feature interactions and integrate different orders of feature interaction and various types of embeddings, by extracting non-linear information with fully-connected layers and activation functions. The output of every layer \ud835\udc89 \ud835\udc59 + 1 is:  where \ud835\udf0e (\u00b7) is the activation function, \ud835\udc7e \ud835\udc59 is the weight, \ud835\udc89 \ud835\udc59 is the outputs of the previous layers, and \ud835\udc83 \ud835\udc59 is the bias. In many hand-crafted models, the output of MLP is combined with other embeddings before being taken as the output of the multi-interaction ensemble layer. DeepFM [29] concatenates the output of 2 \ud835\udc5b\ud835\udc51 order feature interaction \ud835\udc8d \ud835\udc39\ud835\udc40 and the output of MLP on the embedding matrix, denoted as \ud835\udc8d \ud835\udc39\ud835\udc3c = \ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61 ( \ud835\udc8d \ud835\udc39\ud835\udc40 , \ud835\udc40\ud835\udc3f\ud835\udc43 ( e )) . IPNN [86] feeds explicit feature interactions and embedding matrix to MLP, and the output of feature interaction layer is denoted as \ud835\udc8d \ud835\udc39\ud835\udc3c = \ud835\udc40\ud835\udc3f\ud835\udc43 ( \ud835\udc8d \ud835\udc39\ud835\udc40 , e )) . 3.1.4 Prediction Layer. The prediction layer yields the prediction \u02c6 \ud835\udc66 based on the output of the feature interaction layer:  ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021. 1:8 Ruiqi Zheng, et al. where \ud835\udc7e \ud835\udc59 is the weight, and \ud835\udc83 \ud835\udc59 is the bias. The specific recommendation task drives the choice of activation function \ud835\udf0e (\u00b7) . For instance, \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5a\ud835\udc5c\ud835\udc56\ud835\udc51 is preferred for the binary classification task [29], whereas \ud835\udc60\ud835\udc5c\ud835\udc53 \ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65 is selected for multi-class classification [98]. The loss is calculated for the backpropagation steps based on the ground truth label \ud835\udc66 :  where \u2113 is the loss function, such as cross-entropy and mean-squared-error, usually determined by human experts [133].", "3.2 Neural Architecture Search (NAS)": "Neural Architecture Search (NAS) [22, 82] is proposed to search for the data-oriented and taskspecific ideal deep learning architecture from the search space, alleviating considerable human effort in the architecture design procedure. Most works explore well-performed convolutional neural network architectures for different tasks like image classification [10] and Natural language processing [48]. The NAS methods are determined by two significant factors: search space and search strategy. The set of all possible architectures is search space, which is broad enough to encompass existing well-performed architectures but still maintains a reasonable size to prevent increasing search costs. Within the search space, the search strategy efficiently searches for the preferred architecture and is expected to balance exploration and exploitation. There are primarily three kinds of methodologies in the NAS field. (1) Sample-based approaches [6] explore new architecture by selecting from search space or mutating existing promising ones. (2) Reinforcement learning-based approaches [61, 88] implement a recurrent neural network as a policy controller to produce a sequence of actions to determine the architecture design. (3) Gradient-based approaches [62, 118] convert the discrete search space to continuous and optimize the search architecture with the gradient descent calculated from the performance on the validation set. Differentiable Architecture Search (DARTS) employs continuous relaxation to search over the non-differentiable and discrete search space as a favored gradient descent-based NAS method. 3.2.1 Continuous Relaxation. Let \u02c6 O = { \u02c6 \ud835\udc5c \ud835\udc57 (\u00b7)} represents the set of operations (e.g., the set of feature interaction candidate functions). To convert the search space continuous, a weight vector \ud835\udf36 ( \ud835\udc56 ) = [ \ud835\udefc ( \ud835\udc56 ) 1 , \u00b7 \u00b7 \u00b7 , \ud835\udefc ( \ud835\udc56 ) | \u02c6 O| ] indicates the contribution of individual operation to the model performance, where 1 \u2264 \ud835\udc56 \u2264 \ud835\udc3c , and \ud835\udc3c is the number of positions waiting to be put a selected operation. The original one operation \u02c6 \ud835\udc5c ( \ud835\udc56 ) (\u00b7) at position \ud835\udc56 is replaced by the mixed operation \u00af \ud835\udc5c ( \ud835\udc56 ) (\u00b7) with a softmax over all candidates [73]:  The task of searching for a well-performed discrete architecture with needed operations at \ud835\udc3c positions is altered to jointly learn the architecture set A = { \ud835\udf36 ( \ud835\udc56 ) } 1 \u2264 \ud835\udc56 \u2264 \ud835\udc3c , and the weight set W of all operators. After learning procedure, the operator at \ud835\udc56 \ud835\udc61\u210e position of the outputted discrete architecture is the one with the largest weight in the vector \ud835\udf36 ( \ud835\udc56 ) :  3.2.2 Gumbel-Softmax Operation. Like the above continuous relaxation, the Gumbel-Softmax operation substitutes a differentiable sample for the original non-differentiable categorical variable with a Gumbel-Softmax distribution [41]. Thus stochastic neural networks can perform backpropagation ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021. AutoML for Deep Recommender Systems: A Survey 1:9 through examples. Given the continuous distribution \ud835\udf36 = [ \ud835\udefc 1 , \u00b7 \u00b7 \u00b7 , \ud835\udefc \ud835\udc5b ] over the candidates, a hard selection \ud835\udc67 is drawn by the Gumbel-Max trick [28]:  where { \ud835\udc54 \ud835\udc56 } 1 \u2264 \ud835\udc56 \u2264 \ud835\udc5b are independent and identically distributed (i.i.d) noise samples drawn from -log (-log ( \ud835\udc62 \ud835\udc56 )) and \ud835\udc62 \ud835\udc56 \u223c \ud835\udc48\ud835\udc5b\ud835\udc56\ud835\udc53 \ud835\udc5c\ud835\udc5f\ud835\udc5a ( 0 , 1 ) . However, the sampling is non-differentiable due to arg max operation. The softmax function replaces arg max as a continuous and differentiable approximation. Gumbel-Softmax generates \ud835\udc5d \ud835\udc56 , the probability of selecting the \ud835\udc56 \ud835\udc61\u210e candidate as:  where \ud835\udf0f is the temperature parameter that controls the smoothness of the operation. The GumbelSoftmax operation's output turns into a one-hot vector as \ud835\udc61\ud835\udc4e\ud835\udc62 gets closer to zero. 3.2.3 Bi-level Optimization. Similar to sample-based NAS [6] and reinforcement learning-based NAS [61, 88] using the model performance over the validation set as the fitness or reward, DARTS [62] utilizes the validation set to guide the learning procedure of A and W by optimizing the validation loss and training loss in a gradient descent manner. The output of NAS A \u2217 is acquired by minimizing the loss on the validation set, while the weight set W \u2217 is attained by minimizing the loss on the training set, which can be formulated as a bi-level optimization problem [1, 18]:   where L \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b and L \ud835\udc63\ud835\udc4e\ud835\udc59 represent the validation loss and training loss. A is the upper-level parameter and W is the lower-level parameter. 3.2.4 Architecture Gradient Approximation. After constructing the NAS problem as a bi-level optimization, DARTS introduces a straightforward approximation strategy to overcome the costly internal variable optimization in equation 13. Instead of solving the internal optimization entirely by training to convergence, W(A) is approximated by varying W with one training step only [62]. Thus this trick can also be called a one-step approximation:   where \ud835\udf09 represents the learning rate for one step in the internal variable optimization, and W indicates the contemporary weights acquired by the optimization. If W reaches the local optimum for the internal optimization, \u2207WL \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b (W , A) = 0 and equation 14 degenerates to:  After the approximate architecture gradient is applied with the chain rule, it becomes:  where W \u2032 = W\ud835\udf09 \u2207 \ud835\udc4a L \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b (W , A) indicates the weights of the one-step forward model. \ud835\udf09 = 0 accelerates the optimization procedure by neglecting the second order derivative, which is called ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021. 1:10 Ruiqi Zheng, et al. first order approximation. Second order approximation refers to the scenario where \ud835\udf09 > 0. The choice of different approximation methods is the trade-off between accuracy and efficiency. Table 1. Frequently used notations.", "4 AUTOMATED FEATURE SELECTION SEARCH (AUTO-FSS)": "As mentioned above, the input for deep recommender systems is binary vectors of feature fields. Most existing works [29, 34] collect and use as many features as possible, regardless of whether the features are helpful for recommendation or not. This paradigm frequently calls for extra computations cost for feature embedding learning, additional inference time, and sub-optimal performance caused by the redundant or irrelevant features [77]. Therefore, selecting a subset of principal feature fields for deep recommender systems is highly demanded. The traditional feature selection methods such as hand-crafted by human experts, grid search [27, 37], or filter methods [128] cannot be seamlessly integrated with deep recommender systems. For instance, filter methods omit the connection between subsequent models and feature selection. Therefore, automatically selecting features specifically as the input of recommendation models has attracted much attention in recent years, which is termed the automated feature selection search (Auto-FSS) in this paper. Auto-FSS faces the following two challenges. (1) Huge search space: Real-world recommender systems possess a vast number of unique features (e.g., more than a billion user IDs on YouTube) [19]. These features and their cross features that are generated by operations over the unique features define the huge search space. Efficiently performing a search on the huge search ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021. AutoML for Deep Recommender Systems: A Survey 1:11 space is a challenging problem. (2) Dynamic feature significance: In the recommendation tasks, the significance of a particular feature field may vary widely for different user-item interaction instances. Discovering the same useful feature subset for all instances limits the recommendation performance. It is worth mentioning that we only survey Auto-FSS for deep recommender systems as shown in Table 2, focusing on the above challenges since our main scope is AutoRecSys. Table 2. Summary of Auto-FSS methods. AutoField [112] combines the feature selection process with the downstream recommendation tasks by implementing \ud835\udc39 two-dimensional controller vectors [ \ud835\udefc 1 \ud835\udc53 , \ud835\udefc 0 \ud835\udc53 ] to learn the contribution of each feature to the CTR prediction, where \ud835\udefc 0 \ud835\udc53 denotes the possibility of neglecting a feature filed, and \ud835\udefc 1 \ud835\udc53 represents the possibility of choosing a feature filed, \ud835\udefc 1 \ud835\udc53 + \ud835\udefc 0 \ud835\udc53 = 1, and 1 \u2264 \ud835\udc53 \u2264 \ud835\udc39 . After parameters of controller vectors and deep recommender models are jointly learned by DARTS in a bi-level optimization manner, the Gumbel-Max trick simulates the hard feature selection process according to the controller parameters. Following the idea of the controller in AutoField, AdaFS [57] provides an adaptive feature selection method for dynamic data instances rather than a static set. It utilizes a controller network with several fully-connected layers to output weights { \ud835\udefc \ud835\udc5a \ud835\udc53 } 1 \u2264 \ud835\udc53 \u2264 \ud835\udc39 , revealing the importance of different feature fields for a data instance. Before the feature embedding is fed to the MLP as the input, BatchNorm [40] is implemented to make the feature embedding e \ud835\udc53 with various magnitude comparable:  where \ud835\udc5a represents a index for \ud835\udc5a \ud835\udc61\u210e data instance in the batch, \ud835\udc40\ud835\udc52\ud835\udc4e\ud835\udc5b \ud835\udc53 B is a mini-batch mean, \ud835\udc49\ud835\udc4e\ud835\udc5f \ud835\udc53 B represents a variance, and \ud835\udf16 is a small constant. In addition to selecting relevant features from raw feature sets, some literature finds and produces useful combinatorial features (i.e., cross features), such as statistical features. GLIDER [105] utilizes gradient-based Neural Interaction Detection (NID) [106] to detect statistical feature interactions that span globally across multiple data instances from a source recommender model with a perturbation model called LIME [91]. Then, GLIDER explicitly encodes the generated features (i.e., searched global interaction) into a target recommendation model. The target recommendation model can be any classical recommender system, and the generated features are explored in 2 2 \ud835\udc53 search space. AutoCross [70] enables feature selection from generated cross features, and performs the search in a tree-structured search space by implementing greedy beam search. It is different from feature interactions, as the output of AutoCorss is the useful feature sets, which can be fed to different recommendation models such as Wide&Deep [15]. The search space is tree-structured with the original feature set F as the root and other nodes as the feature interaction set. If one node contains ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021. 1:12 Ruiqi Zheng, et al. \ud835\udc53 \u2032 feature interactions, including original features, the number of children of that node is \ud835\udc53 \u2032 ( \ud835\udc53 \u2032 -1 ) 2 . Every child node is the parent node set, adding one feature interaction from the parent node set. The size of the search space is \ud835\udc42 (( \ud835\udc39 2 2 ) \ud835\udc5b ) , growing exponentially with the maximum number of generated feature interactions \ud835\udc5b . To deal with the immense search space, the beam search, as a greedy strategy, is implemented in the tree-structured search space to traverse it from the root efficiently, by only exploring the most promising child node after evaluating all the children feature interaction sets for one node. In this greedy manner, it expands linearly with parameter \ud835\udc5b , since only \ud835\udc5b\ud835\udc39 2 nodes will be evaluated in the ( \ud835\udc39 2 2 ) \ud835\udc5b search space. In the feature set evaluation stage, field-wise logistic regression is applied to accelerate the process. It approximates the actual performance with mini-batch gradient descent. The model prediction is:  where x \ud835\udc5b\ud835\udc52\ud835\udc64 and x \ud835\udc50 represent newly added interactive feature and features in the current set, respectively. Only the weight of the newly added feature interaction \ud835\udc98 \ud835\udc5b\ud835\udc52\ud835\udc64 will be learned in the model training stage, while weights of current feature interactions \ud835\udc98 \ud835\udc50 stay fixed. In the data processing stage, AutoCross proposes multi-granularity discretization, which discretizes one numerical feature with several levels of granularity rather than pre-defined granularity. It evaluates different discretized features, and only the best one is kept. We analytically compare the above Auto-FSS methods as follows. AutoField and AdaFs search in the raw feature level within compact search spaces, while others search in the high-order generated cross feature level. There is no absolute winner among these two categories. Searching for raw features enables the task-specific subsequent recommender systems to discover the interactions and correlations within the original input features. It works well when the recommendation systems are expressive and have adequate computation resources. When the data distribution is rapidly changing in the scenario, or the significance of the high-order cross feature alters, the former is preferred since the downstream recommendation system can be finetuned, and the re-search procedure for the latter is time-consuming. Within the raw feature search category, AdaFs selects different features for different data instances to address the second challenge, while AutoField outputs constant feature sets. Within the generated feature search category, AutoCross searches the explicit high-order feature interactions and can be fully deployed on the distributed systems to fit industrial needs. Both traditional recommender systems and deep neural models can be implemented after AutoCross constructs the feature interaction set. The drawback is also apparent. Exploring the high-order interaction feature space in a trial-and-error manner to prune the search pace leads to sub-optimal results.", "5 AUTOMATED EMBEDDING DIMENSION SEARCH (AUTO-EDS)": "As mentioned above, the typical inputs of recommender systems involve many feature fields, and each field consists of a certain number of unique features ranging from a few to hundreds of millions. Since these original features are generally encoded as high-dimensional and sparse vectors, most modern deep recommender systems [15, 19, 29, 89] map them into the low-dimensional and dense feature representations (a.k.a., embeddings) for better capturing the implicit feature information. However, most of these methods assign a uniform embedding dimension for all features, suffering from the following issues. (1) Resources consumption: The huge number of parameters in the embedding matrix consume a large amount of storage and computational resources of the model. (2) Unappealing performance: The features generally follow a long-tail distribution in recommender ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021. AutoML for Deep Recommender Systems: A Survey 1:13 Fig. 5. The framework of Auto-EDS. x f 0 0 0 0 0 1 1 0 x i 0 0 0 0 0 1 x 2 0 0 0 0 0 0 1 1 0 1 1 0 x 1 Input Layer Interaction Layer MD Embedding Layer Prediction Layer Activation Function Interaction Function Weight Connection Normal Connection Embedding Lookup e f e 1 e 2 e i Implicit Interaction Explicit Interaction Aligned Embedding Layer Embedding Alignment systems [80], so setting the same dimension for head features and tail features may lead to suboptimal performance. In particular, the high-frequency features need more parameters to express their rich information, and over-parameterizing the low-frequency features could cause overfitting due to the limited training data. Thus, assigning embedding dimensions to each feature (field) automatically has attracted much attention in recent years, which is termed the automated embedding dimension search (Auto-EDS) in this paper. Concretely, as shown in Fig. 5, we introduce an overview of Auto-EDS architecture. The key component of Auto-EDS is the mixed dimension (MD) embedding layer, which consists of variable embedding sizes for each feature (fields). Then these MD embeddings are aligned into the same dimension via alignment operators (e.g., projection matrices [26]) in the aligned embedding layer in order to satisfy the further operations (e.g., the dot product) in the interaction layer. Existing methods in this research line could be categorized into heuristic, hyper-parameter optimization (HPO), and embedding pruning methods. We summarize these methods in Table 3.", "5.1 Heuristic Methods": "The heuristic methods generally assign embedding dimensions for each feature (field) based on the pre-defined human rules. For example, MDE [26] introduces a mixed dimension (MD) embedding layer which assigns the embedding dimension based on the feature's popularity. In particular, the MDlayer consists of \ud835\udc39 blocks corresponding to \ud835\udc39 feature fields for the CTR task, and each block is defined by the embedding matrix E \ud835\udc56 \u2208 R \ud835\udc5b \ud835\udc56 \u00d7 \ud835\udc51 \ud835\udc56 and the projection matrix P \ud835\udc56 \u2208 R \ud835\udc51 \ud835\udc56 \u00d7 \u02c6 \ud835\udc51 , respectively. The former stores embedding vectors for \ud835\udc56 \ud835\udc61\u210e block (field) with dimension \ud835\udc51 \ud835\udc56 , and \ud835\udc5b \ud835\udc56 is the number of unique features in the field. The latter \ud835\udc77 \ud835\udc56 aligns the dimension into a base dimension \u02c6 \ud835\udc51 \u2265 \ud835\udc51 \ud835\udc56 for the further feature operations (e.g., the inner product) requiring consistent embedding dimensions. Thus, the question is how to assign mixed embedding dimensions d = [ \ud835\udc51 1 , ..., \ud835\udc51 \ud835\udc39 ] for \ud835\udc39 blocks. To this end, MDE first defines the block-level probability vector p = [ 1 \ud835\udc5b 1 , ..., 1 \ud835\udc5b \ud835\udc39 ] , and then the mixed embedding dimensions d could be obtained as follows:  ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021. 1:14 Ruiqi Zheng, et al. where \ud835\udefc is the hyperparameter controlling the degree of popularity influencing the embedding dimension. Although such heuristic methods provided a simple scheme for assigning mixed embedding dimensions, the assumptions, i.e., the spectral decay following the power law, behind it were not guaranteed always to be satisfied, thus limiting its generalization on complex tasks.", "5.2 HPO Methods": "Inspired by the recent success of neural architecture search (NAS) [22, 142] for automatically searching architectures for deep neural networks, another research line in Auto-EDS is to consider it as the hyper-parameter optimization (HPO) problem that searches embedding dimensions from a pre-defined candidate dimension set. For example, NIS [42] (Neural Input Search) is the first HPO-based Auto-EDS work to automatically learn embedding dimensions and vocabulary sizes for each unique feature. In particular, it first sorts the \ud835\udc63 unique features in decreasing order based on their frequency in the dataset resulting in a E \u2208 R \ud835\udc63 \u00d7 \u02c6 \ud835\udc51 embedding matrix, where \u02c6 \ud835\udc51 is the pre-defined maximum embedding dimension. Then, the E is divided into a grid of \ud835\udc46 \u00d7 \ud835\udc47 submatrices (embedding blocks) with \ud835\udc46 > 1 and \ud835\udc47 > 1, and the ( \ud835\udc60, \ud835\udc61 ) \ud835\udc61\u210e submatrix is of size \u02c6 \ud835\udc63 \ud835\udc60 \u00d7 \u02c6 \ud835\udc51 \ud835\udc61 such that \ud835\udc63 = \u02dd \ud835\udc46 \ud835\udc60 = 1 \u02c6 \ud835\udc63 \ud835\udc60 and \ud835\udc51 = \u02dd \ud835\udc47 \ud835\udc61 = 1 \u02c6 \ud835\udc51 \ud835\udc61 . In order to learn a MD embeddings, inspired by ENAS [82], a controller is employed to make a sequence of \ud835\udc47 choices, and each choice is an \u02c6 \ud835\udc60 \ud835\udc61 = { 1 , ..., \ud835\udc46 } \u222a { 0 } , where \u02c6 \ud835\udc60 \ud835\udc61 = 0 means that the \u02c6 \ud835\udc51 \ud835\udc61 -dimensional embedding is removed. In this way, the search space size is ( \ud835\udc46 + 1 ) \ud835\udc47 , and the reward of the controller is defined by the combination of the quality reward (i.e., the metric values of the model evaluation on the validation set.) and the memory cost. Finally, the model is first trained by a warm-up phase to ensure that embedding blocks are expressive; after that, the main model (i.e., the recommendation model) and the controller are trained alternatingly by A3C [76]. Inspired by the differentiable NAS (DARTS) [62], DNIS [17] proposes a differential NIS framework to improve the search efficiency. Specifically, unlike NIS, which searches for embedding dimensions from pre-defined discrete dimension sets, DNIS argues that this restriction will hurt the flexibility of dimension selection and relax the search space to be continuous via the soft selection layer. In particular, for \ud835\udc63 unique features, it uses \ud835\udc63 binary dimension index vectors \u02c6 d to maintain the ordered locations of the feature's existing dimensions from the pre-defined dimension set { 1 , \u00b7 \u00b7 \u00b7 , \ud835\udc51 } , and then uses the similar feature sorting method as NIS to divide \ud835\udc63 features into \ud835\udc46 blocks such that features within the same block share the same binary dimension index vector. Thus, the total search space is 2 \ud835\udc46\ud835\udc51 . To learn \u02c6 d efficiently, the \u02c6 d is relaxed to be a continuous variable \u00af d within the range of [ 0 , 1 ] , named the soft selection layer, which is inserted between the embedding layer and the interaction layer. In this way, the relaxed dimension vectors could be jointly optimized with the embedding matrix by gradient descent. Furthermore, the gradient normalization operation is utilized to avoid numerical overflow. After training, the final output embedding matrix \u02c6 \ud835\udc38 is obtained as follows:  where \u25e6 is the element-wise product. Finally, to obtain the MD embeddings, we could prune the \u02c6 E by a pre-defined threshold. AutoEmb [135] proposes an AutoML-based end-to-end framework in streaming recommendations that could automatically and dynamically select embedding dimensions for users/items based on their popularity changes. Specifically, each user and item are defined in the \ud835\udc64 embedding spaces with embeddings { e 1 \u2208 R \ud835\udc51 1 , \u00b7 \u00b7 \u00b7 , e \ud835\udc64 \u2208 R \ud835\udc51 \ud835\udc64 } , { h 1 \u2208 R \ud835\udc51 1 , \u00b7 \u00b7 \u00b7 , h \ud835\udc64 \u2208 R \ud835\udc51 \ud835\udc64 } , respectively, where \ud835\udc51 1 < \u00b7 \u00b7 \u00b7 < \ud835\udc51 \ud835\udc64 are the corresponding embedding dimensions. Then, embeddings in different spaces ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021. AutoML for Deep Recommender Systems: A Survey 1:15 are unified into the same space via the linear transform as follows:  where W \ud835\udc56 \u2208 R \ud835\udc51 \ud835\udc64 \u00d7 \ud835\udc51 \ud835\udc56 and \ud835\udc4f \ud835\udc56 \u2208 R \ud835\udc51 \ud835\udc64 are the weight matrix and bias vector, respectively. In order to make the magnitude of the transformed embeddings comparable, the Batch-Norm [40] with the Tanh activation function [44] is utilised to normalize the transform embeddings resulting in the magnitude-comparable user embedding vectors { \u00af e 1 \u2208 R \ud835\udc51 \ud835\udc64 , \u00b7 \u00b7 \u00b7 , \u00af e \ud835\udc64 \u2208 R \ud835\udc51 \ud835\udc64 } and item embedding vectors { \u00af h 1 \u2208 R \ud835\udc51 \ud835\udc64 , ..., \u00af h \ud835\udc64 \u2208 R \ud835\udc51 \ud835\udc64 } . Thus, the search space is 2 ( \ud835\udc48 + \ud835\udc3c ) \ud835\udc64 , where \ud835\udc48 and \ud835\udc3c are the numbers of users and items, respectively. Furthermore, two MLP-based controller networks are used to choose dimensions from the above candidates for users and items based on their popularity and contextual information separately. In order to make the whole framework end-to-end differentiable, the soft selection layer that employs weighted sum of { \u00af e 1 \u2208 R \ud835\udc51 \ud835\udc64 , \u00b7 \u00b7 \u00b7 , \u00af e \ud835\udc64 \u2208 R \ud835\udc51 \ud835\udc64 } is utilised to obtain the final representations \u00af e \u2217 and \u00af h \u2217 of users and items as follow:  where \ud835\udefc \ud835\udc56 and \ud835\udefd \ud835\udc56 are the weights for user representation and item representation,respectively. Finally, to jointly optimize parameters in embedding matrix and parameters in controllers, it introduces a variant of DARTS that leverage the first order approximation as equation 16. AutoDim [134] extends the AutoEmb to the field-aware embedding dimension search, which aims to automatically assign embedding dimensions for different feature fields instead of users/items. In particular, similar to AutoEmb, each feature field is defined in the \ud835\udc64 embedding space and then unified and normalized into the same space. The major difference is that it utilizes the Gumbelsoftmax operation [41] with architecture weights to select embedding dimensions, which could also deal with the end-to-end indifferentiable issue due to the hard dimension selection. Finally, the DARTS-based optimization algorithm is also employed to optimize the embedding matrix and the architectural weights jointly. Table 3. Summary of Auto-EDS methods. ESAPN [63] aims to search embedding dimensions for users and items dynamically based on their popularity by an automated reinforcement learning agent. However, unlike AutoEmb using a soft selection strategy, ESAPN performs a hard selection on candidate embedding dimensions, which could effectively reduce the storage space. Specifically, it consists of a deep recommendation model performing personalized recommendations and two policy networks learning embedding ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021. 1:16 Ruiqi Zheng, et al. dimensions for users and items from a discrete candidate dimension set. The deep recommendation component is similar to AutoEmb which the embeddings of users/items are defined in \ud835\udc64 various embedding spaces and then are transformed into the largest dimension via linear transformations and batch normalization. For policy networks which are multi-layer perceptrons with multiple hidden layers, they take the frequency and current embedding dimension of users/items as inputs (i.e., states for reinforcement learning agent) and output two possible actions: enlarging the current dimension to the next larger dimension or unchanging the current dimension. The design of such actions is because they assume that users/items with a higher frequency have larger embedding dimensions. Furthermore, the reward of policy networks is defined based on the difference between the current prediction loss and the previous losses. Finally, inspired by ENAS [82], the recommendation model and policy networks are optimized in an alternative fashion, which optimizes policy networks using the sampled validation data and uses training data to optimize the recommendation model. Although HPO-based Auto-EDS methods could effectively learn MD embeddings in different levels (i.e., features, feature fields, and users/items), such kinds of methods still suffer from several issues. (1) Resources consumption : to maintain embeddings on different embedding spaces [63, 134, 135], they have to maintain additional embedding matrixes with different dimensions, which consumes a huge amount of storage space. (2) Expensive optimization : the model overall model optimization is time-consuming due to optimizing the extra parameters in controllers [42, 63]. (3) Severe assumption : the assumption that the high-frequency features (users/items) should be assigned with the larger embedding dimensions [42, 63] might not always be satisfied due to the complex situations in recommendations.", "5.3 Embedding Pruning Methods": "Another research line in this field considers the Auto-EDS as the embedding pruning problem, which performs embedding pruning over the whole embedding matrix using different pruning strategies such that the MD embeddings are automatically obtained. Thus, the key idea of such kinds of methods is to build memory-efficient models by identifying and removing the redundant parameters in the embedding matrix and keeping the recommendation as accurate as possible. For example, PEP [65] introduces the learnable thresholds to identify the importance of parameters in the embedding matrix. In particular, inspired by Soft Threshold Reparameterization [52], it directly performs adaptively pruning as follows:  where \u02c6 E and E are the re-parameterized embedding matrix and original embedding matrix, respectively. \ud835\udc4e\ud835\udc4f\ud835\udc60 (\u00b7) is the absolute operation. \ud835\udc60 is learnable threshold(s) that could be updated by gradient descent, and \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5b (\u00b7) is sign function. Furthermore, the sub-gradient is utilized to solve the non-differentiability of equation (23) as below:  where \ud835\udf02 \ud835\udc61 denotes \ud835\udc61 -th step learning rate, and \u25e6 is element-wise product. L and D are the loss function (e.g., the cross-entropy loss) and the dataset, respectively. L {\u00b7} is the indicator function. In this way, the threshold(s) \ud835\udc60 and embedding matrix E could be jointly trained by gradient descent. After that, it could mask those dropped parameters in E to obtain a pruned embedding matrix which could be utilized to re-train the based model according to the Lottery Tickey Hypothesis [23]. ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021. AutoML for Deep Recommender Systems: A Survey 1:17 ATML [121] proposes to use an Adaptively-Masked Twins-based layer (i.e., AMTL) behind the original embedding layer to learn a mask vector which is utilized to mask those redundant dimensions in the embedding matrix. Specifically, to leverage the feature frequency knowledge, the AMTL takes the feature frequency vectors as input and then uses two branches (i.e., h-AML and l-AML) to handle high-frequency and low-frequency samples, respectively, so that the parameters in l-AML will not be dominated by the high-frequency samples. Moreover, it introduces a soft decision strategy to determine the high- or low- frequency samples, which uses a weighted sum of outputs of h-AML and l-AML as follows:  where \ud835\udc5c\ud835\udc62\ud835\udc61\ud835\udc5d\ud835\udc62\ud835\udc61 ( \u210e -\ud835\udc34\ud835\udc40\ud835\udc3f ) \ud835\udc3f and \ud835\udc5c\ud835\udc62\ud835\udc61\ud835\udc5d\ud835\udc62\ud835\udc61 ( \ud835\udc59 -\ud835\udc34\ud835\udc40\ud835\udc3f ) \ud835\udc3f are the \ud835\udc3f -th outputs of l-AML and h-AML, respectively, and \ud835\udefc \ud835\udc56 is the weight which is influenced by the feature frequency \ud835\udc53 \ud835\udc56 . Then, a temperated softmax function [36] is applied on \ud835\udc5c\ud835\udc62\ud835\udc61\ud835\udc5d\ud835\udc62\ud835\udc61 \ud835\udc34\ud835\udc40\ud835\udc47\ud835\udc3f \ud835\udc3f to obtain the probability to select different embedding dimensions. RULE [12] proposes an on-device recommendation paradigm with elastic embeddings dealing with various memory budgets for devices. The paradigm is divided into learning full embeddings and searching elastic embeddings for items with an evolutionary algorithm. As argued by the authors, the learning time of embedding is far beyond the searching time, leading to a once-forall paradigm, which adapts the learned embedding table to local on-device recommenders with heterogeneous resource budgets. In the learning phase, Bayesian personalized ranking (BPR) [90] loss is applied to optimize the full embedding and regularization terms maintaining the diversity of the learned embedding blocks, benefiting the subsequent search procedure. In the deployment phase, the evolutionary algorithm [87] with a single-layer feed-forward network estimator is adopted to search the desired embedding blocks under the memory budgets. The search space is ( 2 \ud835\udc5b 1 -1 ) |I| \ud835\udc5b 2 , where the hyper-parameters \ud835\udc5b 1, \ud835\udc5b 2 represent the number of embedding blocks for an item, and the number of item groups respectively. The embedding pruning is done by maintaining the searched embeddings by evolutionary algorithms. Deeplight [21] proposes to prune both parameters in the embedding layer and DNN layer to solve the high-latency issues in CTR prediction. The weight matrices of the DNN component are pruned to remove the connections. Thus, the sparse DNN component with less computation complexity contributes to the training acceleration. The field pair interaction matrix is pruned as field pair selection. Moreover, the elements in the embedding vectors are pruned to be sparse embedding vectors. Considering the majority of the parameters in deep learning models for click prediction are feature embeddings, Deeplight is classified into Auto-EDS in our taxonomy. The model is trained for a few epochs, and weights with the smallest values are removed based on adaptive sparse rate. The search strategy can be summarized as the variant of the greedy algorithm with weak sub-modular optimization [20]. SSEDS [85] proposes a single-shot embedding pruning method named SSEDS. In particular, it first pre-trains a traditional CTR model with unified embedding dimensions, and then utilizes the proposed criterion which could measure the importance of embedding dimensions only in one forward-backward pass to obtain the salience scores of each dimension. In this way, the redundant dimensions could be removed based on the dimension importance ranking and the parameter budget. Furthermore, since the obtained mixed-dimensional embeddings could not be directly applied to traditional CTR models due to some feature interaction operations (e.g., the dot product) requiring all embeddings with the same dimension, SSEDS utilizes the additional transform matrices to align all dimensions and re-trains the slim model. ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021. 1:18 Ruiqi Zheng, et al. Fig. 6. The framework of Auto-FIS. Interaction Layer Prediction Layer Embedding Layer x f 0 0 0 0 0 1 1 0 x i 0 0 0 0 0 1 x 2 0 0 0 0 0 0 1 1 0 1 1 0 x 1 Input Layer Discarded Connection After Search Activation Function Interaction Function Weight Connection Embedding Lookup All Order Feature Interactions Although the embedding pruning-based Auto-EDS methods could build the memory-efficient model via selectively reducing parameters in the embedding matrix, these methods generally require an iterative optimization procedure for both parameters in the embedding matrix and additional parameters used to prune, which is time-consuming.", "6 AUTOMATED FEATURE INTERACTION SEARCH (AUTO-FIS)": "Feature interactions combine individual features of users, items, and other context information. For instance, the user's age and generic of the downloaded application can be combined together as the 2 \ud835\udc5b\ud835\udc51 order feature interactions and contribute to the users' preference prediction in the application recommendation scenario. The order implements the number of combined features. The effectiveness of 2 \ud835\udc5b\ud835\udc51 order feature interactions has been proved in Factorization Machines (FMs) [89] and their variants [81][43]. Meanwhile, the high-order ( \ud835\udc5d \ud835\udc61\u210e order with \ud835\udc5d \u2265 3) feature interactions is approximated by Higher-order Factorization Machine (HOFM) [4]. With the prosperity of deep neural networks, many deep learning recommender models have been proposed due to their better performance than traditional models. Product-based Neural Network (PNN) [86] extracts the high-order feature interactions and discards the lower ones. Wide&Deep [15] and DeepFM [29] learn the low-order and high-order feature interactions by a shallow component and a deep component and state that both the low-order and high order feature interactions play a significant role in context-aware recommender systems. However, enumerating all high-order feature interactions is time and space-consuming. When there are \ud835\udc53 features in total, the number of \ud835\udc5d \ud835\udc61\u210e order interaction terms is GLYPH<0> \ud835\udc53 \ud835\udc5d GLYPH<1> . Even for 2 \ud835\udc5b\ud835\udc51 order feature interactions, simply listing all combinations may introduce useless interactions as noise and disturb the model performance. Therefore, how to keep the necessary feature interactions and filter out the useless ones arouse people's interest. We summarize these methods in Table 4. Many automated feature interaction search methods are proposed to deal with the following three challenges mainly. (1) The desired beneficial feature interaction sets are discrete. (2) Both lower and higher order feature interactions are highly correlated. (3) The priorities of the low- and high-order interactions in the search procedure should be considered. Concretely, as shown in Fig. 6, we introduce an overview of Automated Feature Interaction Search (Auto-FIS) architecture. The key components are the set containing all possible order feature interactions and the discarded connections, which are determined by the search procedure. ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021. AutoML for Deep Recommender Systems: A Survey 1:19 Sparse Factorization Machines (SFMs) [131] determine the relevant user features and item features based on their contributions to the prediction model by sparsity regularization [102]. If one feature does not contribute to the predictive modeling, entire feature interactions related to it will be deactivated. If some significant high-order feature interactions play a role in the prediction only as a whole rather than individuals, SFMs may discard them even it has a recovery procedure to cover features that are relevant to user-item prediction. Moreover, the feature interaction selection strategy of SFMs is identical for all users. Same interactions may play a more important role to one user than others. The most intuitive solution is to build distant SFMs for every user, which does not preserve the benefit of collaborative filtering. Thus, Bayesian Personalized Feature Interaction Selection ( BP-FIS ) [14] is proposed to adaptively select interactions for individual users by Bayesian Variable Selection [75]. The prediction is modified as below:  where \ud835\udc4f \ud835\udc62 is the bias for user \ud835\udc62 . The single feature interaction weights W = { \ud835\udc64 \ud835\udc56 } \u222a { \ud835\udc64 \ud835\udc56 \ud835\udc57 } are learned for all users, while personalized feature interaction selection variables S = { \ud835\udc60 \ud835\udc62\ud835\udc56 } \u222a { \ud835\udc60 \ud835\udc62\ud835\udc56\ud835\udc57 } indicate the 1 \ud835\udc60\ud835\udc61 order and 2 \ud835\udc5b\ud835\udc51 order feature interaction selection for individual user \ud835\udc62 . |U| is the number of users. Due to the immense search space \ud835\udc42 (|U| \u00b7 \ud835\udc39 2 ) , BP-FIS proposes Hereditary Spike-and-Slab Prior (HSSP) based on Spike-and-Slab Priors (SSPs) [3] to attain the heredity property of feature interaction. Strong heredity indicates that if the 1 \ud835\udc60\ud835\udc61 order interactions x \ud835\udc56 and x \ud835\udc57 are chosen, their combination, i.e., the 2 \ud835\udc5b\ud835\udc51 order interaction < e \ud835\udc56 , e \ud835\udc57 > would be selected, while weak heredity indicates the selection possibility of their combination to be \ud835\udf0b 2 when only one of the 1 \ud835\udc60\ud835\udc61 order interactions is selected. To be specific, the HSSP is stated as:  where \ud835\udf0b 1 , \ud835\udf0b 2 \u2208 { 0 , 1 } are constant values. Inspired by Variational Auto-Encoder (VAE) [47], Stochastic Gradient Variational Bayes (SGVB) estimator is proposed to approximate posteriors of the latent variables and optimize the model. BP-FIS can be combined with both linear FM and neural FM. AutoFIS [60] is proposed to learn the feature interactions in recommender system models by adding an attention gate to every potential feature interaction, rather than simply enumerating all the two-order feature interactions like Factorization Machines. There are two stages for AutoFIS: the search stage and the re-train stage. In the search stage, equation 28 shows the interaction layer of the factorization model in AutoFIS.  Instead of searching the desired discrete combination set from \ud835\udc42 ( 2 \ud835\udc39 2 ) search space for two-order feature interactions, the wight \ud835\udefc ( \ud835\udc56,\ud835\udc57 ) represents the importance of the interaction between vector \ud835\udc52 \ud835\udc56 and vector \ud835\udc52 \ud835\udc57 , and determinates whether preserve or delete this interaction. After feeding the output of the interaction layer as the input of multi-layer perceptron (MLP), the user's preference ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021. 1:20 Ruiqi Zheng, et al. for item can be calculated via:  Architecture parameters \ud835\udf36 = { \ud835\udefc ( \ud835\udc56,\ud835\udc57 ) } \ud835\udc56 = 1 , \u00b7\u00b7\u00b7 ,\ud835\udc5a,\ud835\udc56 < \ud835\udc57 \u2264 \ud835\udc53 reveal the contribution of every feature interaction towards the final prediction. Furthermore, the architecture parameters \ud835\udf36 are trained by generalized regularized dual averaging (GRDA) optimizer [7], while other parameters (such as w in equation 28 ) are updated by Adam optimizer [46]. These two optimizations are conducted jointly in one gradient descent step, unlike the bi-level optimization algorithm in DARTS [62]. In the re-train stage, architecture parameters \ud835\udf36 are fixed as \ud835\udf36 \u2217 after search stage. Feature interactions that benefit the final prediction have higher \ud835\udefc \u2217 ( \ud835\udc56,\ud835\udc57 ) values. The feature interaction layer in equation 28 is modified as:  where \ud835\udefe ( \ud835\udc56,\ud835\udc57 ) depicting the gate status is set as 0 when \ud835\udf36 \u2217 = 0, otherwise 1. With attention unit \ud835\udf36 \u2217 , the model is further trained by relative important interactions, and all the parameters are learned by Adam optimizer [46]. Unlike AutoFIS simply focusing on two-order feature interaction selections, AutoGroup [59] considers the high-order feature interaction search as a structural optimization problem to identify the useful high-order interactions. The search space is \ud835\udc42 ( 2 \ud835\udc53 \ud835\udc3e ) , where pre-defined \ud835\udc3e represents the maximum order of feature interactions. Firstly, for \ud835\udc5d \ud835\udc61\u210e order of feature interactions, it selects \ud835\udc5b \ud835\udc5d feature sets. F \ud835\udc5d \ud835\udc57 represents the \ud835\udc57 \ud835\udc61\u210e feature set for \ud835\udc5d \ud835\udc61\u210e order. Every feature fi is possible to be included in F \ud835\udc5d \ud835\udc57 indicated by the structural parameter \ud835\udefc \ud835\udc5d \ud835\udc56,\ud835\udc57 . After the automatic feature grouping stage, the representation of a feature set F \ud835\udc5d \ud835\udc57 is defined as the weighted sum of feature embedding within it:  where e \ud835\udc56 is the embedding of feature f \ud835\udc56 and \ud835\udc64 \ud835\udc5d \ud835\udc56 is trainable weight parameter. Inspired by FM [89], the time complexity is reduced from \ud835\udc42 ( \ud835\udc39 \ud835\udc5d ) to \ud835\udc42 ( \ud835\udc39 ) , owning to feature interaction calculation method within one feature set F \ud835\udc5d \ud835\udc57 .  where ( g \ud835\udc5d \ud835\udc57 ) \ud835\udc5d represents the sum of all the embedding components of the embedding generated by \ud835\udc5d times the element-wise product of g \ud835\udc5d \ud835\udc57 with itself. There are \u02dd \ud835\udc3e \ud835\udc56 = 1 \ud835\udc5b \ud835\udc56 interaction results in total. All the interactions are concatenated and fed into an MLP. The prediction is calculated by:  AutoGroup optimizes the structural parameters and network weights (e.g., embedding parameters and MLP parameters) alternatively by gradient descent, similar to DARTS [62], since two kinds of trainable parameters are highly dependent on each other. With the prosperity of Graph Neural Network (GNN), \ud835\udc3f 0 -SIGN [99] implements GNN techniques to tackle the feature interaction search problem. Given a Graph G = (N , E) , where \ud835\udc5b \ud835\udc56 \u2208 N ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021. AutoML for Deep Recommender Systems: A Survey 1:21 Table 4. Summary of Auto-FIS methods. represents feature f \ud835\udc56 , edge \ud835\udc52 \ud835\udc56,\ud835\udc57 \u2208 { 1 , 0 } from the edge set E = { \ud835\udc52 \ud835\udc56,\ud835\udc57 } \ud835\udc56,\ud835\udc57 = 1 , \u00b7\u00b7\u00b7 ,\ud835\udc53 represents whether select the feature interaction between feature f \ud835\udc56 and feature f \ud835\udc57 . Initially, the edge set is empty E = \u2205 , and the task of searching for beneficial feature interactions is converted to the edge prediction in graph G . Therefore, the complexity of the search space is \ud835\udc42 ( 2 \ud835\udc39 2 ) . \ud835\udc3f 0-SIGN implements an MLP to predict the edge:  where the feature embedding e \ud835\udc56 and e \ud835\udc57 are obtained by equation 1 and 2. Set E \u2032 including all detected edges \ud835\udc52 \u2032 \ud835\udc56,\ud835\udc57 , is performed with an \ud835\udc3f 0 activation regularization to minimize the number of searched beneficial interactions. After determining the edge set \ud835\udc52 \u2032 \ud835\udc56,\ud835\udc57 , the embedding e ( 1 ) \ud835\udc56 = e \ud835\udc56 is updated iteratively \ud835\udc61 times by a linear aggregation function \ud835\udf13 (\u00b7) (e.g. element-wise summation/mean):  where \ud835\udc61 is the iteration index, and \ud835\udc60 ( \ud835\udc61 -1 ) \ud835\udc56 is the set of statistical interaction analysis outcomes between e ( \ud835\udc61 -1 ) \ud835\udc56 (i.e. the embedding of node \ud835\udc56 at ( \ud835\udc61 -1 ) \ud835\udc61\u210e iteration) and embeddings of its neighbours. The final prediction \u02c6 \ud835\udc66 is calculated as:  where the linear function \ud835\udc54 (\u00b7) (e.g., weighted sum function) converts the node embedding to a scalar value, and \ud835\udc4f is the bias term. While \ud835\udc3f 0-SIGN can only search for the 2 \ud835\udc5b\ud835\udc51 -order feature interaction, HIRS [99] extends it to arbitrary order of feature interaction by introducing the hyper-edge set { \ud835\udc52\ud835\udc51\ud835\udc54\ud835\udc52 \ud835\udc57 } \ud835\udc5b 1 \ud835\udc57 = 1 , where hyperparameter \ud835\udc5b 1 represents the number of intended search feature interaction. Each hyper-edge is \ud835\udc39 -dimensional binary vector \ud835\udc52\ud835\udc51\ud835\udc54\ud835\udc52 \ud835\udc56 \ud835\udc57 \u2208 { 0 , 1 } , where \ud835\udc52\ud835\udc51\ud835\udc54\ud835\udc52 \ud835\udc56 \ud835\udc57 = 1 represents that the \ud835\udc57 \ud835\udc61\u210e hyper-edge links to feature f \ud835\udc56 . The hyper-edge prediction module contains a multi-layer perceptron, looking through the search space with complexity \ud835\udc42 ( \ud835\udc5b 1 \u00b7 2 \ud835\udc39 ) . Under the same settings of graph G , FIVES [119] extends the interaction search to high-order with a graph neural network and an adjacency tensor. Adjacency tensor A \u2208 { 0 , 1 } \ud835\udc3e \u00d7 \ud835\udc39 \u00d7 \ud835\udc39 indicates the interactions at every order \ud835\udc58 \u2264 \ud835\udc3e . e ( \ud835\udc58 ) \ud835\udc56 is the representation for node \ud835\udc56 at order \ud835\udc58 , and e ( 1 ) \ud835\udc56 = e \ud835\udc56 . Based on the proposition that the interaction of uninformative ones is unlikely to build an informative feature, when { A ( \ud835\udc58 ) \ud835\udc56,\ud835\udc57 } \ud835\udc56,\ud835\udc57 = 1 , \u00b7\u00b7\u00b7 ,\ud835\udc53 is active, e ( \ud835\udc58 ) \ud835\udc56 is calculated as:  ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021. 1:22 Ruiqi Zheng, et al. where \"MEAN\" is the aggregator, \u25e6 is the element-wise product, and W \ud835\udc57 represents the transformation matrix for node \ud835\udc57 . Under the assumption that ( W \ud835\udc57 \ud835\udc5b ( 1 ) \ud835\udc57 ) \u25e6 \ud835\udc5b ( 1 ) \ud835\udc56 can express feature interaction f \ud835\udc56 \u2297 f \ud835\udc57 , the \ud835\udc58 \ud835\udc61\u210e node representation e ( \ud835\udc58 ) = [ e ( \ud835\udc58 ) 1 , \u00b7 \u00b7 \u00b7 e ( \ud835\udc58 ) \ud835\udc53 ] matches the \ud835\udc58 \ud835\udc61\u210e order feature interactions, and adjacency tensor A determines which features to be selected. Thus, the search space is \ud835\udc42 ( 2 \ud835\udc3e\ud835\udc39 2 ) . The search problem of FIVES is a Bi-level optimization where A is the upper-level variable, and other model variables are the lower-level ones, Following the settings in AutoFIS, where gradient descent NAS method with relaxation turns the search space into symmetric parameter matrix \ud835\udc7e \u2208 R \ud835\udc39 \u00d7 \ud835\udc39 , PROFIT [24] finds that parameter matrix \ud835\udc7e has several dominant singular values, and exhibits a low-rank property. Therefore, PROFIT proposes a distilled search space \ud835\udc68 by symmetric CP decomposition [49], extending to \ud835\udc58 \ud835\udc61\u210e order interactions:  where approximated vector \ud835\udf37 \ud835\udc5f \u2208 R 1 \u00d7 \ud835\udc39 is updated by gradient descent, and the distilled search space is based on low-rank approximation with a positive integer \ud835\udc45 \u226a \ud835\udc39 . The authors state that the complexity is \ud835\udc42 ( \ud835\udc45\ud835\udc39\ud835\udc3e ) . Following the idea that different orders of feature interactions are highly correlated, PROFIT proposes a progressive gradient descent to learn the high-order after the low one. Specifically, when learning the \ud835\udc58 \ud835\udc61\u210e order interaction, the architecture parameters { \ud835\udf37 \ud835\udc8a } \ud835\udc56 -1 ,...,\ud835\udc58 -1 are fixed. BP-FIS has two major drawbacks. (1) BP-FIS limits the interactions up to two orders. (2) Selection for every user may be a waste of resources. Group-level penalization could be an efficient manner to control the size of the search space. AutoFIS, AutoGroup, and AutoCross adopt NAS on the continuous search space with the help of continuous relaxation. The search problem is modified from choosing one interaction to calculating the weight of that interaction. The complexity of search space is reduced from the original search space to the number of the weight parameters \u02dd \ud835\udc3e \ud835\udc56 = 1 GLYPH<0> \ud835\udc39 \ud835\udc56 GLYPH<1> . However, they still encounter several issues. (1) Slow optimization procedure: The number of weight parameters is significantly greater than existing gradient-based NAS methods [62, 118], which usually have less than one hundred parameters. (2) Ignorance on the order-priority property: They neglect the relations between different order interactions. Both implementing GNN techniques to search for beneficial feature interactions, \ud835\udc3f 0-SIGN focuses on 2 \ud835\udc5b\ud835\udc51 order feature interactions. In contrast, FIVES extends to high-order under the proposition that it is improbable to produce the instructive feature interactions from the uninformative interactions, which may miss particular significant interactions in some scenarios. Although the search space of PROFIT has been dramatically reduced, the choice of hyper-parameter \ud835\udc45 influences the final performance.", "7 AUTOMATED MODEL ARCHITECTURE SEARCH (AUTO-MAS)": "Numerous classical approaches [56, 109] have demonstrated the significance and effectiveness of feature interactions in coping with high-cardinality feature attributes and large-scale datasets. They discover the explicit low-order feature interactions and combine them with explicit or implicit high-order feature interactions. For instance, Wide&Deep [15] and DeepFM [29] learn the explicit low-order and implicit high-order feature interactions by a shallow and a deep component. The section on feature interaction search methods systematically reviews various techniques to identify ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021. AutoML for Deep Recommender Systems: A Survey 1:23 Table 5. Summary of Auto-MAS methods. beneficial feature interactions. However, existing methods search for one particular layer and leave other components of the deep recommender systems hand-crafted, causing three problems. (1) An integrated recommender system model cannot be directly obtained by the above-mentioned automated search methods, and domain experts are needed to design other components manually. (2) An whole well-performed architecture is less likely to be acquired due to other hand-crafted components, even the searched one finding the best candidate. (3) The generalization capacity is decreased by the limited search space for one specific layer rather than the whole architecture. Moreover, the design of MLP layers in classical approaches may not be optimal both in efficiency and effectiveness. Diamond MLP frameworks may outperform rectangular, and triangle frameworks [130], which aroused people's interest in the MLP design. Experts cannot attempt all the potential design architecture. Therefore, automated model architecture search methods are employed to mitigate human efforts and search for an automatically designed task-specific architecture that organically combines informative embeddings and various feature interactions. There are mainly three challenges. (1) Accurate search space: The search space of automated model architecture search methods should be carefully designed, which includes popular and effective human-crafted architectures. In the meantime, the search space cannot be exceedingly general. (2) Search Efficiency: Automated model architecture search methods should be efficient, especially for the design of the recommender system architecture. A practical model encounters a billion-level of user-item data in the industry, for example, the private dataset in AutoFIS [60] from Huawei App Store. (3) Ability to distinguish: Recommender systems with diverse architecture may lead to similar performance on the validation dataset since minor improvements to the experiment contribute to significant refinements in practice. Automated model architecture search methods should be sensitive to slight progress. We summarize these methods in Table 5. AutoCTR [97] proposes a two-level hierarchical search space, which includes the popular human-crafted deep recommender system architectures by abstracting different feature interaction methods into virtual blocks. The inner search space contains the choice of virtual blocks and detailed hyperparameters inside each of those blocks. Based on the existing literature, the virtual blocks can be selected from FM, MLP, and dot products. Instead of simply stacking the blocks sequentially, AutoCTR implements a direct acyclic graph (DAG) to connect different feature interaction blocks organically, rather than directly stacking different components sequentially. A block may receive outputs from any preceding block, including numerous unexplored architectures. The outer search space contains all the possible block combination choices. Let \ud835\udc5b represent the number of virtual blocks, controlling the complexity of the architectures. There are four basic candidate operators for blocks, and \ud835\udc5b ( \ud835\udc5b -1 ) 2 possible connections for \ud835\udc5b blocks. Therefore the search space is \ud835\udc42 ( 4 \ud835\udc5b \u00b7 2 \ud835\udc5b ( \ud835\udc5b -1 ) 2 ) . A multi-objective evolutionary search is used to explore the two-level hierarchical search space. The first procedure is survivor selection. Only top\ud835\udc5d architectures survive according to the metric \ud835\udc54 : ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021. 1:24 Ruiqi Zheng, et al.  where \ud835\udc5e is the hyperparameter larger than \ud835\udc5d , and the indicator function L (\u00b7) filters out architectures older than q. Three objectives of the architecture \ud835\udc34 : age \ud835\udc4e \ud835\udc34 , performance \ud835\udc5f \ud835\udc34 , and complexity \ud835\udc50 \ud835\udc34 are balanced by parameters \ud835\udf07 1, \ud835\udf07 2, and \ud835\udf07 3. To tackle the third challenge mentioned above, the probability of being selected as the parents in the second procedure is denoted as:  where hyperparameter \ud835\udf06 balances exploitation and exploration. In the last procedure, a learningto-rank strategy is adopted to guide the mutations by the gradient boosted tree learner and a pairwise ranking loss named LambdaRank [5]. AMEIR [132] divides the recommendation models into three stages: behavior modeling, feature interaction, and MLP aggregation. Behavior modeling networks encapsulate users' specific and dynamic interests based on the sequential input features. Behavior modeling networks have three components: normalization, specific layer, and activation function, respectively selected from three candidate sets: {layer normalization, None}, {convolutional, recurrent, pooling, attention}, {ReLU, GeLU, Swish, Identity}. The search subspace of feature interactions is \ud835\udc42 ( 2 \ud835\udc39 \ud835\udc58 ) , containing all possible combinations of \ud835\udc58 \ud835\udc61\u210e order feature interactions. For the MLP aggregation stage, the number of hidden units has ten candidate values, and the activation function is chosen from {ReLU, Swish, Identity, Dice}. Therefore the complexity of the overall search is \ud835\udc42 ( 32 \ud835\udc5b 1 \u00b7 2 \ud835\udc39 \ud835\udc58 \u00b7 40 \ud835\udc5b 2 ) , where \ud835\udc5b 1 and \ud835\udc5b 2 represent the number of layers for behavior modeling and MLP respectively. A one-shot random search is implemented to incorporate industrial requirements rather than a one-shot weight-sharing paradigm. It randomly samples child models from the search space and reserves the one with the best performance on the validation set. By gradually increasing the order of interactions, effective feature interactions are discovered. Beneficial interaction set with the fixed size is initialized with feature matrix e and updated by interacting with e . New interactions with the highest validation fitness are retained in the interaction set. AutoIAS [113] provides a more fine-grained search space including six components S \ud835\udc56 with \ud835\udc5b \ud835\udc56 number of candidates, where \ud835\udc56 \u2208 { 1 , \u00b7 \u00b7 \u00b7 , 6 } . First component S 1 represents the embedding size for each binary feature vector { x \ud835\udc56 } \ud835\udc56 = 1 , \u00b7\u00b7\u00b7 ,\ud835\udc53 . S 2 determines the unified projection embedding size before interaction for every 2 \ud835\udc5b\ud835\udc51 order interaction pair, and the third component S 3 decides the feature interaction function. S 4 inserts the result of one interaction \ud835\udc5c ( e \ud835\udc56 , e \ud835\udc57 ) into the \ud835\udc59 \ud835\udc61\u210e layer of MLP to mix the low- and high-order of feature interactions. The input of \ud835\udc59 \ud835\udc61\u210e layer is the concatenation between the interaction result and the output of ( \ud835\udc59 -1 ) \ud835\udc61\u210e layer. The output of \ud835\udc59 \ud835\udc61\u210e layer is calculated as:  where \ud835\udc59 \u2208 { 1 , \u00b7\u00b7\u00b7 , \ud835\udc3f } . S 6 decides the number of layers in MLP. AutoIAS implements architecture generator network to search through the immense search space with complexity \ud835\udc42 ( \ud835\udc39 \ud835\udc5b 1 \u00b7( 2 \ud835\udc39 2 ) \ud835\udc5b 2 + \ud835\udc5b 3 + 1 + \ud835\udc5b 5 \u00b7 \ud835\udc5b 6 ) . The Deep Neural Network (DNN) models the dependencies among different components by taking in previous components' states and generating the current component's selection probability on the candidate set. The performance prediction of a particular architecture is fastened by the ordinal parameter sharing on the supernet whose embedding size for different components is the largest one over the corresponding candidate set. ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021. AutoML for Deep Recommender Systems: A Survey 1:25 Fig. 7. The framework of Auto-OCS. Activation Function Interaction Function Weight Connection Normal Connection Embedding Lookup Loss Function Candidates Feature Interaction Function Candidates Interaction Layer Prediction Layer Implicit Interaction Explicit Interaction Embedding Layer x f 0 0 0 0 0 1 1 0 x i 0 0 0 0 0 1 x 2 0 0 0 0 0 0 1 1 0 1 1 0 x 1 Input Layer Loss Function Search Feature Interaction Function Search Unlike the above methods, NASR [16] is an Auto-MAS model, searching hybrid architectures for the sequential recommendation. To alleviate the dilemma where increasing the number of depth layers leads to difficult training for neural networks [32], NASR adds one trainable parameter \ud835\udf06 \ud835\udc3f to the residual connection:  where h \ud835\udc62 \ud835\udc59 + 1 and h \ud835\udc62 \ud835\udc59 indicate the input and output for \ud835\udc59 + 1-th residual block. \ud835\udc40\ud835\udc4e\ud835\udc5d \ud835\udc59 + 1 is the learnable residual mapping, and W \ud835\udc59 + 1 represent all the parameters for \ud835\udc59 + 1-th block. The trainable parameter \ud835\udf06 \ud835\udc59 boosts convergence and improves sequential recommendation performance. The search space consists of \ud835\udc5b 1 deep layers. For each layer, there are four candidates: two transformer block variants, and two temporal convolutional network variants. A greedy search strategy is utilized, accompanied by an unsupervised evaluation metric, which estimates the performance for each candidate layer. One of the biggest challenges in automated model architecture search is the immense search space. All three methods suffer from this issue and propose distinct solutions to deal with it. AutoCTR implements multi-objective evolutionary search rather than gradient descent methods to parallelly explore the vast search space. AMEIR employs a one-shot random search to accelerate the search process, while AutoIAS utilizes DNN to model the dependencies among different components. Although the automated model architecture search is appealing and directly outputs well-performed recommender systems, simply stacking all the possible candidates leads to huge search space and is not practical in real-world applications. The inner relation between different components should be discovered, or some rule should be implemented to shrink the search space and serve as a search strategy guideline.", "8 AUTOMATED OTHER COMPONENTS SEARCH (AUTO-OCS)": "", "8.1 Loss Function Search": "Deep neural networks (DNN) show promising results in recommender systems. One pivotal part of DNN training is the backpropagation, calculating the gradient based on the pre-defined loss function. However, choosing distinct loss functions is not universal and does not guarantee good performance. More appropriate gradients with a carefully-designed loss function may contribute to a better deep model. ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021. 1:26 Ruiqi Zheng, et al. People manually designed loss functions for specific tasks and purposes. A loss function with a higher value at the boundary position is proposed to improve the boundary metrics and satisfy the scenario when the boundary region is more significant [83]. A large-margin softmax loss function in the image processing field is proposed to replace the common softmax loss function for desired feature discrimination learning [66]. Despite the effectiveness of manually-designed loss functions, the exhausting design process requires human experts and a heavy workload. As shown in Fig. 7, the automated loss search method selects the proper loss function concerning different tasks and goals from the set of loss function candidates. Given a set of loss function candidates { \u2113 \ud835\udc56 } \ud835\udc56 = 1 , \u00b7\u00b7\u00b7 ,\ud835\udc5b with size \ud835\udc5b , the complexity of search space is \ud835\udc42 ( \ud835\udc5b ) . Stochastic loss function (SLF) [64] calculates the overall loss value as follows:  where a set of weights { \ud835\udefc \ud835\udc56 } \ud835\udc56 = 1 , \u00b7\u00b7\u00b7 ,\ud835\udc5b represents the contributions of individual loss functions, while \ud835\udc66 and \u02c6 \ud835\udc66 represent the ground truth and prediction, respectively. However, this soft fusing strategy cannot prohibit the sub-optimal loss function from depreciating the loss value L . To tackle this problem, AutoLoss [133] simulates the hard selection with Gumbel-softmax operation on the weight set { \ud835\udefc \ud835\udc56 } \ud835\udc56 = 1 , \u00b7\u00b7\u00b7 ,\ud835\udc5b . Moreover, diverse user-item interaction examples exhibit different convergence behaviors. The weight set for every example cannot be initialized by the same static probability. AutoLoss uses a controller network with several fully-connected layers like equation 6, taking the pair ( \ud835\udc66, \u02c6 \ud835\udc66 ) as input and outputting the weight set. Therefore, the weight set { \ud835\udefc \ud835\udc56 } \ud835\udc56 = 1 , \u00b7\u00b7\u00b7 ,\ud835\udc5b is adaptively produced for different examples depending on distinct convergence behavior patterns. Although seldom works implement loss function search in recommendation scenario. Automated loss function search itself is not a brand new field direction. In the image semantic segmentation application, AUTO SEG-LOSS [54] searches particular surrogate losses and enhances the model performance on distinct metrics. In the face recognition task, a search space and a reward-guided search method [110] are presented to acquire the best loss function candidate. Those papers validate the effectiveness of the automated loss function search. Perhaps some methodologies can be transferred to recommendation tasks, or new methods can be explicitly proposed for recommendation scenarios. The training efficiency of recommender system models can be further improved. Table 6. Summary of Auto-OCS methods.", "8.2 Feature Interaction Function Search": "The effectiveness of feature interaction has been addressed in recent techniques, and various search methods for beneficial interactions are introduced above. However, most literature uses the same feature interaction function to model all the feature interactions while neglecting their ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021. AutoML for Deep Recommender Systems: A Survey 1:27 distinction. Wide&Deep [15] implements a shallow component and MLP to model low- and highorder interactions. Attention networks distinguish the contributions of 2 \ud835\udc5b\ud835\udc51 order interaction in AFM [117] and DIN [136]. Only simple inner product operations are employed in PNN [86] and DeepFM [29]. PIN [130] presents a net-in-net architecture to model the pairwise interactions, accompanied by a product-based neural network. All the literature mentioned above implements the same network architecture or inner product to learn interactions regardless of the input data change, which leads to sub-optimal performance. Therefore, feature interaction search methods are needed to search for suitable interaction functions according to different datasets and tasks as shown in Fig. 7. We summarize these methods in Table 6. Simple neural interaction functions ( SIF ) [123] has been proposed to automatically select interaction function for collaborative filtering (CF) [51]. Within the search space, interaction function \ud835\udc53 between user \ud835\udc56 and item \ud835\udc57 is designed as \ud835\udc53 ( \ud835\udc62 \ud835\udc56 , \ud835\udc63 \ud835\udc57 ) = \ud835\udc5c ( \ud835\udc54 ( e \ud835\udc56 ) , \ud835\udc54 ( e \ud835\udc57 )) . \ud835\udc54 (\u00b7) is the simple non-linear element-wise operation (small MLP with fixed architecture) and \ud835\udc5c (\u00b7) is the vector-wise operations selected from five candidates. Let \ud835\udc5b denote the number of selected operations. The search space is \ud835\udc42 ( GLYPH<0> 5 \ud835\udc5b GLYPH<1> ) , including popular human-designed interaction functions. Moreover, inspired by an efficient NAS method based on proximal iterations (NASP) [125], SIF implements a one-shot search algorithm jointly searching interaction functions and updating the learning parameters through stochastic gradient descent. Unlike SIF searches proper functions for all feature interactions. OptInter [71] divides the potential feature function into three categories for every 2 \ud835\udc5b\ud835\udc51 order feature interaction: elementwise product, MLP, and memorized methods, which regard interactions as a new feature and assign trainable weights. There are three function candidates for each interaction, and \ud835\udc42 ( \ud835\udc39 2 ) 2 \ud835\udc5b\ud835\udc51 order feature interactions exist. Therefore, the complexity of the search space is \ud835\udc42 ( 3 \ud835\udc39 2 ) . The element-wise product for e \ud835\udc56 and e \ud835\udc57 is represented as:  Gradient descent-based NAS search method with Gumbel-softmax operation is implemented to make the discrete search space continuous, and the architecture parameters can be learned by Adam optimizer [46]. AutoPI [74] introduces the computational graph to search space. The computational graph is a directed acyclic graph (DAG) comprising input nodes, \ud835\udc5b intermediate nodes, and the output node in an ordered sequence. \ud835\udc5b is the pre-defined parameter that controls the complexity of the computational graph. Every node \ud835\udc56 contains a feature matrix E ( \ud835\udc56 ) \u2208 R \ud835\udc39 \u00d7 \ud835\udc60 stacked by \ud835\udc39 feature embeddings with size \ud835\udc60 , and every directed edge \ud835\udc52 \ud835\udc56,\ud835\udc57 between node \ud835\udc56 and node \ud835\udc57 represents an interaction function \ud835\udc5c \ud835\udc56,\ud835\udc57 (\u00b7) , which transforms the feature matrix E ( \ud835\udc56 ) . Every intermediate node is calculated using all of its predecessors' values:  There are \ud835\udc5b ( \ud835\udc5b -1 ) 2 edges, and for each edge, one interaction function is searched from an interaction function set O , including six candidates. Therefore, the search space is \ud835\udc42 ( 6 \ud835\udc5b 2 ) . Six feature interaction functions candidates are Skip-connection, SENET layer [39], Self-attention [98], FM [89], Singlelayer Perceptron, and 1d Convolution. Skip-connection outputs the same feature matrix e \u2032 = e \u2208 R \ud835\udc39 \u00d7 \ud835\udc60 as the input. Single-layer Perceptron uses a linear transformation to transform a flattened feature into a feature matrix e \u2032 = e \u00b7 \ud835\udc7e \u2208 R \ud835\udc39 \u00d7 \ud835\udc60 , and 1d Convolution outputs a feature matrix e \u2032 \u2208 R \ud835\udc39 \u00d7 \ud835\udc60 through \ud835\udc5a kernel matrices { \ud835\udc6a \ud835\udc8a } \ud835\udc56 = 1 , \u00b7\u00b7\u00b7 ,\ud835\udc53 \u2208 R \ud835\udc39 \u00d7 1 \u00d7 1 . ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021. 1:28 Ruiqi Zheng, et al. In the search stage, inspired by DARTS [62], a gradient descent NAS method converts the combinatorial search problem to a bi-level optimization with continuous relaxation. Node-level parameters \ud835\udf36 = { \ud835\udefc \ud835\udc56,\ud835\udc57 } \ud835\udc56 < \ud835\udc57 show the weights of different interaction functions, where \ud835\udefc \ud835\udc56,\ud835\udc57 is a vector with size |O| . Since the value of one node is determined by all its predecessors, edge-level weights \ud835\udf37 = { \ud835\udefd \ud835\udc56,\ud835\udc57 } \ud835\udc56 < \ud835\udc57 represent the contributions of the node \ud835\udc56 to the node \ud835\udc57 , where \ud835\udefd \ud835\udc56,\ud835\udc57 is a scalar. The bi-level optimization is formulated with \ud835\udefc, \ud835\udefd as the upper-level parameters and other weights as the lower-level parameters. The one-step approximation [62] is implemented to tackle the expensive inner optimization problem by approximating the architecture gradient. AutoFeature [45] extends the search of proper interaction function for every interaction to high-order with a distinct DAG sub-network structure. The DAG contains a pre-defined number of operations selected from five interaction functions: pointwise addition, Hadamard product, concatenation, generalized product, and none. The search space is \ud835\udc42 ( 5 \ud835\udc59 \ud835\udc59 \ud835\udc53 \ud835\udc3e ) , where \ud835\udc59 represents the number of operations in the sub-network, \ud835\udc53 represents the number of features, and \ud835\udc3e represents the interaction order. Due to the immense search space, a tree of Naive Bayes classifiers (NBTree) with thresholds indicating the 90 \ud835\udc61\u210e percentile of explored space partitions the search space of architectures recursively. As for the search strategy, the likelihood of selecting from a node is proportionate to the number of samplings formerly selected from that node. SIF [123] chooses the proper feature interaction function for all 2 \ud835\udc5b\ud835\udc51 order interactions while neglecting that different feature interactions may require different methodologies to model. Based on SIF, AutoFeature [45] searches individual functions for every interaction and extends the order of interactions, including most interaction modeling techniques, but high-order interaction and complex DAG components lead to immense search space. Besides, the search strategy of AutoFeature [45] can be trapped in the local optimal. OptInter [71] balances the efficiency and the generalization of the search space, searching distinct functions for every interaction limited to 2 \ud835\udc5b\ud835\udc51 order, and introduces memorized methods as the candidate of feature interaction functions. Taking feature interactions as new features benefits the model performance since this action makes the correlated patterns of some interactions with strong signals more accessible to be captured. It is worth mentioning that the abuse of memorized methods may depreciate the overall performance due to the overfitting problem accompanied by sparse new features.", "9 HORIZONTAL COMPARISON FOR AUTORECSYS": "The empirical analysis aims to help practitioners investigate the bottlenecks and strengths of current AutoRecSys models from different aspects (e.g., number of parameters, training time, etc.). Then researchers can evaluate the applicability of the existing method to their unique problems or scenarios. Therefore, we select the representative AutoRecSys methods to perform the empirical analyses for two tasks: click-through prediction (CTR) and Top-K recommendation.", "9.1 Experiment details and hyper-parameter setting": "Following the experiment settings in the original papers, we employ commonly-used metrics, Recall@10 (i.e., recall at rank 10) and NDCG@10 (i.e., normalized discounted cumulative gain at rank 10) for Top-K recommendation, and Area Under the ROC Curve (AUC) for click-through prediction (CTR). To gauge the complexity of the model space, we additionally count the number of model parameters, represented as # \ud835\udc43\ud835\udc4e\ud835\udc5f\ud835\udc4e\ud835\udc5a\ud835\udc60 , and measure the training time in seconds. All the methods are implemented by the codes provided by the authors, and the hyper-parameters are set relying on the authors' suggestions. DeepFM [29] is set as the base model. Reduce-LR-on-Plateau scheduler and early stopping [140] are employed on all methods. For a fair comparison, the same machine with 32G memory and GeForce RTX 2080Ti is utilized for the experiment. ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021. AutoML for Deep Recommender Systems: A Survey 1:29", "9.2 Datasets": "For the CTR task, AutoRecSys are horizontally compared on Criteo, and Avazu [85] by selecting three methods: MDE, PEP, and DeepLight. For Top-K recommendation, AutoFIS, \ud835\udc3f 0-SIGN, and HIRS as representative methods are evaluated on MovieLens 1M [31], and Book-crossing [141]. All four datasets are widely used in surveyed papers and related publications [21, 26, 60]. Data pre-processing and dataset splitting strictly follow the setting in [65, 100]. \u00b7 Criteo : It is a real-world industry dataset for CTR prediction, which consists of 45 million users' click records on ads over one month. Each click record contains 13 numerical feature fields and 26 categorical feature fields. \u00b7 Avazu : It consists of 40 million users' click records on ads over 11 days, and each record contains 22 categorical features. \u00b7 MovieLens 1M : It contains users' ratings on movies. Each data sample contains a user and a movie with their corresponding attributes as features. \u00b7 Book-crossing : It contains users' implicit and explicit ratings of books. Each data sample contains a user and a book with their corresponding features. Table 7. Results of representative models on CTR and Top-K Recommendation tasks.", "9.3 Results and Analysis": "Results of representative models on CTR and Top-K Recommendation tasks are shown in Table 7. The best result under each metric is shown in bold. We acquire several useful insights from the horizontal comparison for AutoRecSys. \u00b7 For CTR task, the embedding pruning-based AutoEDS methods PEP and Deeplight can outperform the heuristic-based method MDE. The possible explanation is that, unlike heuristic approaches, pruning-based AutoEDS methods estimate the importance of various dimensions at a lower level (i.e., the embedding level) as opposed to a higher level (i.e., the dimension level). \u00b7 For the Top-K recommendation task, GNN-based models \ud835\udc3f 0-SIGN outperform AutoFIS, demonstrating the power of GNNs for interaction modeling. The model that takes into account higher-order feature interactions (i.e., HIRS) beats work that takes into account pairwise interactions. Therefore, incorporating high-order feature interactions helps to improve prediction performance. \u00b7 The results of some methods are not satisfying compared with the reported scores in their papers. They are mostly caused by the fact that various data pre-processing and splitting methods are commonly used, even on the same datasets. This indicates that uniform data splitting and preparation are urgently expected in order to directly compare the outcomes ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021. 1:30 Ruiqi Zheng, et al. of different models and save researchers in this community from repeatedly realizing the baselines on their own. \u00b7 Model efficiency is a significant aspect in real-world recommendation tasks, where the industrial recommender systems must be updated often (for example, once per hour) due to the continuous changes in feature distribution [85]. The training time for each model is usually omitted in the original paper, but plays an essential role in the industry. Excessive training time prevents them from deployment in daily lives. Some models with slow training procedures may be on account of code implementations. Based on the time in Table 7, HIRS achieves decent recommendation performance while retaining the least training time. \u00b7 Memory consumption is a vital metric when practitioners pay attention to memory-efficient recommender systems [42, 95] because there has been a recent spike in the migration of data and models from cloud servers side to edge devices [96] to preserve privacy and timeliness. In this circumstance, edge devices (e.g., smartphones) have limited resources. If a little recommendation performance drop is acceptable, models with a small number of parameters (such as \ud835\udc3f 0-SIGN) would be more favored.", "10 FUTURE DIRECTIONS": "Feature Cold Start Although existing methods could efficiently and adaptively assign feature dimensions to features (fields), new features (fields) may be added in real time in practical recommender systems. How to efficiently assign embedding dimensions to these new features is still an open question. For example, IncCTR [111] sets a unified embedding dimension for all features and initializes feature embeddings. It is worth mentioning that feature cold start problem is not the unique direction for Auto-EDS, but influences many categories in our AutoRecSys taxonomy. For instance, Auto-FSS should consider not only the data distributions but also the users' interest shifts and newly emerged features since new contents and new labels are created from individuals or companies daily [13]. How to quickly evaluate the new incoming features based on the existing feature selection model still needs more discussions and experiments from the community. Long-tail Features Generally, most existing Auto-EDS methods assume that high-frequency users/items should have a larger embedding dimension than low-frequency users/items. This is because low-frequency users/items have lesser training data. However, PEP [65] illustrates that simply assigning larger dimensions for high-frequency features is sub-optimal. Thus, the dimension assignment for those long-tail users/items is still a challenging problem due to their sparse data. Theory analysis Most existing AutoML methods for deep recommender systems show competitive recommendation performance and promising results in finding the model's suitable component. However, seldom works provide solid theory analysis to guarantee the effectiveness of the search strategy. MED [26] provides a close solution based on strict assumptions, which is not practical in a real-world application. \ud835\udc3f 0-SIGN [99] ensures the success of valuable interaction search by revealing its relation with the information bottleneck principle [103] and spike-and-slab distribution [75]. The gap between the theory and application scenarios should be bridged so that the concrete theory analysis could provide prior knowledge to guide the design of search space and search strategy. AutoML for on-device recommender systems Most existing AutoML for deep recommender systems focuses on models deployed on a centralized cloud server, which is universal. These centralized deep recommender systems introduce privacy issues when the information of users is shared with the could server and other users. Therefore on-device recommender systems have aroused people's interest, where the recommender systems are deployed on the users' devices rather than the could. There are mainly two challenges. (1) Issue of heterology: They assume all the devices implement the same architectures of the recommender systems or neglect the difference between devices, such as memory size, computation ability, and latency. (2) Issue of limited resources: ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021. AutoML for Deep Recommender Systems: A Survey 1:31 Unlike the centralized recommender system, implementing millions of parameters in the model, it is impractical to deploy the immense recommender systems on the devices. AutoML for on-device recommender systems automatically designs heterogeneous recommender systems for heterogeneous devices with several restrictions. To deal with one of the limitations: memory budget, RULE [12] is proposed to learn the diversified embedding blocks and customize elastic item embeddings for various devices with different memory constraints. AutoML for on-device recommender systems is challenging and distinctive from traditional AutoML or on-device recommender systems, where it emphasizes the automated input components and heterology. AutoML for Various Important Recommendation Tasks. Different recommendation tasks such as social recommendation [126, 127], sequential recommendation [30, 116], POI recommendation [67], and multi-modality recommendations [101] have different data inputs. However, the recommender models designed for each specific task have seldom been integrated with automated machine learning to save experts from overloaded model architecture design. For instance, the fusion function selection for multi-modality RecSys is non-trivial and requires domain experts, especially for heterogeneous sources [38]. AutoSTG [79], which made the first attempt to combine automated neural architecture search with the spatio-temporal graph prediction, could provide helpful insight into AutoML for POI recommendation. Applying AutoML to various recommendation tasks and conquering the unique challenges in each recommendation scenario with specific input data remain open questions. AutoML for GNNs-based Recommendations. Many GNN-based recommendation models [30, 33, 84] have emerged recently due to the fast development of GNNs. Despite the success, many parameters of the graph neural architectures need to be tuned by heavy manual work and domain knowledge. Therefore, some recent works, such as GraphNAS [25], and Auto-GNN [137], integrate AutoML with GNN to automatically design GNN models. However, in recommendation scenarios, the graph construction plays an essential role in the final performance [114]. More work should be conducted on choosing different graph construction (e.g., adding edges between two consecutive items [115], adjusting the current sequence graph [72]) with AutoML. AutoGSR [9] only searches for the proper graph and layer aggregators, not unique for recommendation tasks, to construct a well-performed model. The search space of AutoML for GNN RecSys should be compact and extensive to cover the handcraft and unknown GNN-based recommendation models.", "11 CONCLUSION": "Over the past few years, deep recommender systems have become powerful and practical tools for both the academic world and industry applications, and AutoML has emerged as a promising way to automate the design of some components or the entirety of the machine learning pipeline. This survey has conducted a comprehensive review of AutoML methodologies for deep recommender systems and provided a new taxonomy to classify those methods into five categories according to the encountered issues. Finally, we have proposed six potential future research directions.", "REFERENCES": "[1] G Anandalingam and Terry L Friesz. 1992. Hierarchical optimization: An introduction. Annals of Operations Research 34, 1 (1992), 1-11. [2] Robert M Bell and Yehuda Koren. 2007. Scalable collaborative filtering with jointly derived neighborhood interpolation weights. In Seventh IEEE international conference on data mining (ICDM 2007) . IEEE, 43-52. [3] JM Bernardo, MJ Bayarri, JO Berger, AP Dawid, D Heckerman, AFM Smith, and M West. 2003. Bayesian factor regression models in the 'large p, small n' paradigm. Bayesian statistics 7 (2003), 733-742. [4] Mathieu Blondel, Akinori Fujino, Naonori Ueda, and Masakazu Ishihata. 2016. Higher-order factorization machines. In Advances in Neural Information Processing Systems , Vol. 29. 3351-3359. ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021. 1:32 Ruiqi Zheng, et al. ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021. AutoML for Deep Recommender Systems: A Survey 1:33 [23] Jonathan Frankle and Michael Carbin. 2018. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635 (2018). [24] Chen Gao, Yinfeng Li, Quanming Yao, Depeng Jin, and Yong Li. 2021. Progressive Feature Interaction Search for Deep Sparse Network. In Advances in Neural Information Processing Systems , M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (Eds.), Vol. 34. Curran Associates, Inc., 392-403. https://proceedings.neurips.cc/ paper/2021/file/03b2ceb73723f8b53cd533e4fba898ee-Paper.pdf [25] Yang Gao, Hong Yang, Peng Zhang, Chuan Zhou, and Yue Hu. 2019. Graphnas: Graph neural architecture search with reinforcement learning. arXiv preprint arXiv:1904.09981 (2019). [26] A.A. Ginart, Maxim Naumov, Dheevatsa Mudigere, Jiyan Yang, and James Zou. 2021. Mixed Dimension Embeddings with Application to Memory-Efficient Recommendation Systems. In 2021 IEEE International Symposium on Information Theory (ISIT) . 2786-2791. https://doi.org/10.1109/ISIT45174.2021.9517710 [27] Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (Proceedings of Machine Learning Research, Vol. 9) , Yee Whye Teh and Mike Titterington (Eds.). PMLR, Chia Laguna Resort, Sardinia, Italy, 249-256. https://proceedings.mlr.press/v9/glorot10a.html [28] Emil Julius Gumbel. 1954. Statistical theory of extreme values and some practical applications: a series of lectures . Vol. 33. US Government Printing Office. [29] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: A Factorization-Machine Based Neural Network for CTR Prediction. In Proceedings of the 26th International Joint Conference on Artificial Intelligence (Melbourne, Australia) (IJCAI'17) . AAAI Press, 1725-1731. [30] Lei Guo, Li Tang, Tong Chen, Lei Zhu, Quoc Viet Hung Nguyen, and Hongzhi Yin. 2021. DA-GCN: a domain-aware attentive graph convolution network for shared-account cross-domain sequential recommendation. arXiv preprint arXiv:2105.03300 (2021). [31] F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Trans. Interact. Intell. Syst. 5, 4, Article 19 (dec 2015), 19 pages. https://doi.org/10.1145/2827872 [32] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition(CVPR'16) . 770-778. [33] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, YongDong Zhang, and Meng Wang. 2020. LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, China) (SIGIR '20) . Association for Computing Machinery, New York, NY, USA, 639-648. https://doi.org/10.1145/3397271.3401063 [34] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural Collaborative Filtering. In Proceedings of the 26th International Conference on World Wide Web (Perth, Australia) (WWW '17) . International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, CHE, 173-182. https://doi.org/10.1145/3038912.3052569 [35] Xinran He, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Yanxin Shi, Antoine Atallah, Ralf Herbrich, Stuart Bowers, and Joaquin Qui\u00f1onero Candela. 2014. Practical Lessons from Predicting Clicks on Ads at Facebook. In Proceedings of the Eighth International Workshop on Data Mining for Online Advertising (New York, NY, USA) (ADKDD'14) . Association for Computing Machinery, New York, NY, USA, 1-9. https://doi.org/10.1145/2648584.2648589 [36] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 2, 7 (2015). [37] Geoffrey E. Hinton. 2012. A Practical Guide to Training Restricted Boltzmann Machines . Springer Berlin Heidelberg, Berlin, Heidelberg, 599-619. https://doi.org/10.1007/978-3-642-35289-8_32 [38] Duc Hoang, Haotao Wang, Handong Zhao, Ryan Rossi, Sungchul Kim, Kanak Mahadik, and Zhangyang Wang. 2022. AutoMARS: Searching to Compress Multi-Modality Recommendation Systems. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management (Atlanta, GA, USA) (CIKM '22) . Association for Computing Machinery, New York, NY, USA, 727-736. https://doi.org/10.1145/3511808.3557242 [39] Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. 2019. FiBiNET: Combining Feature Importance and Bilinear Feature Interaction for Click-through Rate Prediction. In Proceedings of the 13th ACM Conference on Recommender Systems (Copenhagen, Denmark) (RecSys '19) . Association for Computing Machinery, New York, NY, USA, 169-177. https://doi.org/10.1145/3298689.3347043 [40] Sergey Ioffe and Christian Szegedy. 2015. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. In Proceedings of the 32nd International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 37) , Francis Bach and David Blei (Eds.). PMLR, Lille, France, 448-456. https: //proceedings.mlr.press/v37/ioffe15.html [41] Eric Jang, Shixiang Gu, and Ben Poole. 2016. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144 (2016). ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021. 1:34 Ruiqi Zheng, et al. [42] Manas R. Joglekar, Cong Li, Mei Chen, Taibai Xu, Xiaoming Wang, Jay K. Adams, Pranav Khaitan, Jiahui Liu, and Quoc V. Le. 2020. Neural Input Search for Large Scale Recommendation Models. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (Virtual Event, CA, USA) (KDD '20) . Association for Computing Machinery, New York, NY, USA, 2387-2397. https://doi.org/10.1145/3394486.3403288 [43] Yuchin Juan, Yong Zhuang, Wei-Sheng Chin, and Chih-Jen Lin. 2016. Field-Aware Factorization Machines for CTR Prediction. In Proceedings of the 10th ACM Conference on Recommender Systems (Boston, Massachusetts, USA) (RecSys '16) . Association for Computing Machinery, New York, NY, USA, 43-50. https://doi.org/10.1145/2959100.2959134 [44] Bekir Karlik and A Vehbi Olgac. 2011. Performance analysis of various activation functions in generalized MLP architectures of neural networks. International Journal of Artificial Intelligence and Expert Systems 1, 4 (2011), 111-122. [45] Farhan Khawar, Xu Hang, Ruiming Tang, Bin Liu, Zhenguo Li, and Xiuqiang He. 2020. AutoFeature: Searching for Feature Interactions and Their Architectures for Click-through Rate Prediction. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management (Virtual Event, Ireland) (CIKM '20) . Association for Computing Machinery, New York, NY, USA, 625-634. https://doi.org/10.1145/3340531.3411912 [46] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014). [47] Diederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 (2013). [48] Nikita Klyuchnikov, Ilya Trofimov, Ekaterina Artemova, Mikhail Salnikov, Maxim Fedorov, and Evgeny Burnaev. 2020. NAS-Bench-NLP: neural architecture search benchmark for natural language processing. arXiv preprint arXiv:2006.07116 (2020). [49] Tamara G Kolda and Brett W Bader. 2009. Tensor decompositions and applications. SIAM review 51, 3 (2009), 455-500. [50] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization techniques for recommender systems. Computer 42, 8 (2009), 30-37. [51] Yehuda Koren, Steffen Rendle, and Robert Bell. 2022. Advances in Collaborative Filtering . Springer US, New York, NY, 91-142. https://doi.org/10.1007/978-1-0716-2197-4_3 [52] Aditya Kusupati, Vivek Ramanujan, Raghav Somani, Mitchell Wortsman, Prateek Jain, Sham Kakade, and Ali Farhadi. 2020. Soft Threshold Weight Reparameterization for Learnable Sparsity. In Proceedings of the 37th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 119) , Hal Daum\u00e9 III and Aarti Singh (Eds.). PMLR, 5544-5555. https://proceedings.mlr.press/v119/kusupati20a.html [53] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learning. nature 521, 7553 (2015), 436-444. [54] Hao Li, Chenxin Tao, Xizhou Zhu, Xiaogang Wang, Gao Huang, and Jifeng Dai. 2020. Auto seg-loss: Searching metric surrogates for semantic segmentation. arXiv preprint arXiv:2010.07930 (2020). [55] Seth Siyuan Li and Elena Karahanna. 2015. Online recommendation systems in a B2C E-commerce context: a review and future directions. Journal of the Association for Information Systems 16, 2 (2015), 2. [56] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong Sun. 2018. XDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (London, United Kingdom) (KDD '18) . Association for Computing Machinery, New York, NY, USA, 1754-1763. https://doi.org/10.1145/3219819.3220023 [57] Weilin Lin, Xiangyu Zhao, Yejing Wang, Tong Xu, and Xian Wu. 2022. AdaFS: Adaptive Feature Selection in Deep Recommender System. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (Washington DC, USA) (KDD '22) . Association for Computing Machinery, New York, NY, USA, 3309-3317. https://doi.org/10.1145/3534678.3539204 [58] Greg Linden, Brent Smith, and Jeremy York. 2003. Amazon. com recommendations: Item-to-item collaborative filtering. IEEE Internet computing 7, 1 (2003), 76-80. [59] Bin Liu, Niannan Xue, Huifeng Guo, Ruiming Tang, Stefanos Zafeiriou, Xiuqiang He, and Zhenguo Li. 2020. AutoGroup: Automatic Feature Grouping for Modelling Explicit High-Order Feature Interactions in CTR Prediction. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, China) (SIGIR '20) . Association for Computing Machinery, New York, NY, USA, 199-208. https://doi.org/10.1145/ 3397271.3401082 [60] Bin Liu, Chenxu Zhu, Guilin Li, Weinan Zhang, Jincai Lai, Ruiming Tang, Xiuqiang He, Zhenguo Li, and Yong Yu. 2020. AutoFIS: Automatic Feature Interaction Selection in Factorization Models for Click-Through Rate Prediction. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (Virtual Event, CA, USA) (KDD '20) . Association for Computing Machinery, New York, NY, USA, 2636-2645. https://doi.org/10. 1145/3394486.3403314 [61] Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, and Koray Kavukcuoglu. 2017. Hierarchical representations for efficient architecture search. arXiv preprint arXiv:1711.00436 (2017). [62] Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2018. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055 (2018). ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021. AutoML for Deep Recommender Systems: A Survey 1:35 [63] Haochen Liu, Xiangyu Zhao, Chong Wang, Xiaobing Liu, and Jiliang Tang. 2020. Automated Embedding Size Search in Deep Recommender Systems. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, China) (SIGIR '20) . Association for Computing Machinery, New York, NY, USA, 2307-2316. https://doi.org/10.1145/3397271.3401436 [64] Qingliang Liu and Jinmei Lai. 2020. Stochastic Loss Function. Proceedings of the AAAI Conference on Artificial Intelligence 34, 04 (Apr. 2020), 4884-4891. https://doi.org/10.1609/aaai.v34i04.5925 [65] Siyi Liu, Chen Gao, Yihong Chen, Depeng Jin, and Yong Li. 2021. Learnable Embedding Sizes for Recommender Systems. arXiv preprint arXiv:2101.07577 (2021). [66] Weiyang Liu, Yandong Wen, Zhiding Yu, and Meng Yang. 2016. Large-Margin Softmax Loss for Convolutional Neural Networks. In Proceedings of The 33rd International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 48) , Maria Florina Balcan and Kilian Q. Weinberger (Eds.). PMLR, New York, New York, USA, 507-516. https://proceedings.mlr.press/v48/liud16.html [67] Jing Long, Tong Chen, Nguyen Quoc Viet Hung, and Hongzhi Yin. 2022. Decentralized Collaborative Learning Framework for Next POI Recommendation. arXiv preprint arXiv:2204.06516 (2022). [68] Linyuan L\u00fc, Mat\u00fa\u0161 Medo, Chi Ho Yeung, Yi-Cheng Zhang, Zi-Ke Zhang, and Tao Zhou. 2012. Recommender systems. Physics reports 519, 1 (2012), 1-49. [69] Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. 2017. The Expressive Power of Neural Networks: A View from the Width. In Advances in Neural Information Processing Systems , I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.), Vol. 30. Curran Associates, Inc., 6231-6239. https://proceedings.neurips.cc/paper/2017/file/32cbf687880eb1674a07bf717761dd3a-Paper.pdf [70] Yuanfei Luo, Mengshuo Wang, Hao Zhou, Quanming Yao, Wei-Wei Tu, Yuqiang Chen, Wenyuan Dai, and Qiang Yang. 2019. AutoCross: Automatic Feature Crossing for Tabular Data in Real-World Applications. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (Anchorage, AK, USA) (KDD '19) . Association for Computing Machinery, New York, NY, USA, 1936-1945. https://doi.org/10.1145/3292500.3330679 [71] Fuyuan Lyu, Xing Tang, Huifeng Guo, Ruiming Tang, Xiuqiang He, Rui Zhang, and Xue Liu. 2021. Memorize, Factorize, or be Na \\ \" ive: Learning Optimal Feature Interaction Methods for CTR Prediction. arXiv preprint arXiv:2108.01265 (2021). [72] Chen Ma, Liheng Ma, Yingxue Zhang, Jianing Sun, Xue Liu, and Mark Coates. 2020. Memory augmented graph neural networks for sequential recommendation. In Proceedings of the AAAI conference on artificial intelligence , Vol. 34. 5045-5052. [73] Chris J Maddison, Andriy Mnih, and Yee Whye Teh. 2016. The concrete distribution: A continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712 (2016). [74] Ze Meng, Jinnian Zhang, Yumeng Li, Jiancheng Li, Tanchao Zhu, and Lifeng Sun. 2021. A General Method For Automatic Discovery of Powerful Interactions In Click-Through Rate Prediction. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, Canada) (SIGIR '21) . Association for Computing Machinery, New York, NY, USA, 1298-1307. https://doi.org/10.1145/3404835.3462842 [75] Toby J Mitchell and John J Beauchamp. 1988. Bayesian variable selection in linear regression. Journal of the american statistical association 83, 404 (1988), 1023-1032. [76] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asynchronous Methods for Deep Reinforcement Learning. In Proceedings of The 33rd International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 48) , Maria Florina Balcan and Kilian Q. Weinberger (Eds.). PMLR, New York, New York, USA, 1928-1937. https://proceedings.mlr.press/ v48/mniha16.html [77] Boaz Nadler and Ronald R Coifman. 2005. The prediction error in CLS and PLS: the importance of feature selection prior to multivariate calibration. Journal of Chemometrics: A Journal of the Chemometrics Society 19, 2 (2005), 107-118. [78] Shumpei Okura, Yukihiro Tagami, Shingo Ono, and Akira Tajima. 2017. Embedding-Based News Recommendation for Millions of Users. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (Halifax, NS, Canada) (KDD '17) . Association for Computing Machinery, New York, NY, USA, 1933-1942. https://doi.org/10.1145/3097983.3098108 [79] Zheyi Pan, Songyu Ke, Xiaodu Yang, Yuxuan Liang, Yong Yu, Junbo Zhang, and Yu Zheng. 2021. AutoSTG: Neural Architecture Search for Predictions of Spatio-Temporal Graph. In Proceedings of the Web Conference 2021 (Ljubljana, Slovenia) (WWW'21) . Association for Computing Machinery, New York, NY, USA, 1846-1855. https://doi.org/10. 1145/3442381.3449816 [80] Yoon-Joo Park and Alexander Tuzhilin. 2008. The Long Tail of Recommender Systems and How to Leverage It. In Proceedings of the 2008 ACM Conference on Recommender Systems (Lausanne, Switzerland) (RecSys '08) . Association for Computing Machinery, New York, NY, USA, 11-18. https://doi.org/10.1145/1454008.1454012 ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021. 1:36 Ruiqi Zheng, et al. [99] Yixin Su, Rui Zhang, Sarah Erfani, and Zhenghua Xu. 2021. Detecting Beneficial Feature Interactions for Recommender Systems. Proceedings of the AAAI Conference on Artificial Intelligence 35, 5 (May 2021), 4357-4365. https://doi.org/10. ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021. AutoML for Deep Recommender Systems: A Survey 1:37 ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021. 1:38 Ruiqi Zheng, et al. ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021. AutoML for Deep Recommender Systems: A Survey 1:39 [139] Jieming Zhu, Jinyang Liu, Shuai Yang, Qi Zhang, and Xiuqiang He. 2020. Fuxictr: An open benchmark for click-through rate prediction. arXiv preprint arXiv:2009.05794 (2020). [140] Jieming Zhu, Jinyang Liu, Shuai Yang, Qi Zhang, and Xiuqiang He. 2021. Open Benchmarking for Click-Through Rate Prediction. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management (Virtual Event, Queensland, Australia) (CIKM '21) . Association for Computing Machinery, New York, NY, USA, 2759-2769. https://doi.org/10.1145/3459637.3482486 [141] Cai-Nicolas Ziegler, Sean M. McNee, Joseph A. Konstan, and Georg Lausen. 2005. Improving Recommendation Lists through Topic Diversification. In Proceedings of the 14th International Conference on World Wide Web (Chiba, Japan) (WWW'05) . Association for Computing Machinery, New York, NY, USA, 22-32. https://doi.org/10.1145/1060745. 1060754 [142] Barret Zoph and Quoc V Le. 2016. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578 (2016). [143] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. 2018. Learning transferable architectures for scalable image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition(CVPR'18) . 8697-8710. ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2021."}
