{
  "Multimodal Pretraining, Adaptation, and Generation for Recommendation: A Survey": "Qijiong Liu ∗† The HK PolyU Hong Kong, China liu@qijiong.work Jieming Zhu ∗ Huawei Noah's Ark Lab Shenzhen, China jiemingzhu@ieee.org Yanting Yang † Zhejiang Unversity Hangzhou, China yantingyang@zju.edu.cn Quanyu Dai Huawei Noah's Ark Lab Shenzhen, China daiquanyu@huawei.com",
  "Zhaocheng Du": "Huawei Noah's Ark Lab Shenzhen, China zhaochengdu@huawei.com Xiao-Ming Wu The HK PolyU Hong Kong, China xiao-ming.wu@polyu.edu.hk Zhou Zhao Zhejiang Unversity Hangzhou, China zhaozhou@zju.edu.cn",
  "Rui Zhang": "Huazhong University of Science and Technology, China rayteam@yeah.net",
  "Zhenhua Dong": "Huawei Noah's Ark Lab Shenzhen, China dongzhenhua@huawei.com",
  "ABSTRACT": "Personalized recommendation serves as a ubiquitous channel for users to discover information tailored to their interests. However, traditional recommendation models primarily rely on unique IDs and categorical features for user-item matching, potentially overlooking the nuanced essence of raw item contents across multiple modalities such as text, image, audio, and video. This underutilization of multimodal data poses a limitation to recommender systems, especially in multimedia services like news, music, and short-video platforms. The recent advancements in large multimodal models offer new opportunities and challenges in developing content-aware recommender systems. This survey seeks to provide a comprehensive exploration of the latest advancements and future trajectories in multimodal pretraining, adaptation, and generation techniques, as well as their applications in enhancing recommender systems. Furthermore, we discuss current open challenges and opportunities for future research in this dynamic domain. We believe that this survey, alongside the curated resources 1 , will provide valuable insights to inspire further advancements in this evolving landscape.",
  "KEYWORDS": "Recommender Systems, Multimodal Pretraining, Multimodal Adaptation, Multimodal Generation",
  "ACMReference Format:": "Qijiong Liu, Jieming Zhu, Yanting Yang, Quanyu Dai, Zhaocheng Du, XiaoMing Wu, Zhou Zhao, Rui Zhang, and Zhenhua Dong. 2024. Multimodal Pretraining, Adaptation, and Generation for Recommendation: A Survey. In ∗ Equal contribution. Correspondence to: Jieming Zhu. 1 Github repository: https://mmrec.github.io/survey † The work was done when the authors were visiting at Huawei Noah's Ark Lab. KDD '24, August 25-29, 2024, Barcelona, Spain © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. This is the author's version of the work. It is posted here for your personal use. Not for redistribution. The definitive Version of Record was published in Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '24), August 25-29, 2024, Barcelona, Spain , https://doi.org/10.1145/3637528.3671473. Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '24), August 25-29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3637528.3671473",
  "1 INTRODUCTION": "Recommender systems have been widely employed in various online applications, including e-commerce websites, advertising systems, streaming services, and social media platforms, to deliver personalized recommendations to users. Their primary goal is to enhance user experience, boost user engagement, and facilitate the discovery of items tailored to individual interests. However, traditional recommendation models primarily rely on unique IDs (e.g., user/item IDs) and categorical features (e.g., tags) for user-item matching [162], potentially overlooking the nuanced essence of raw item contents across multiple modalities such as text, image, audio, and video [149]. This underutilization of multimodal data poses a limitation to recommender systems, especially in multimedia services like news, music, and short-video platforms [14]. To tackle this limitation, researchers have extensively investigated multimodal recommendation techniques for over a decade, resulting in a large body of research work that explores the integration of multimodal item features into recommendation models. For a comprehensive review, interested readers can refer to recent surveys [14, 73, 84, 157]. These surveys primarily delve into techniques such as multimodal feature extraction [84], feature representation [14], feature interaction [73], feature alignment [14], feature enhancement [73], and multimodal fusion [157] for recommendation models. However, the majority of these approaches rely on extracted multimodal feature embeddings, leaving other aspects of multimodal pretraining and generation relatively unexplored. Nowadays, pretrained large models have gained significant popularity in the domains of natural language processing (NLP), computer vision (CV), and multimodal systems (MM). The emergence of language models like the GPT [7] and Llama [116] series has ushered in a new era of capabilities for understanding and generating language, while the CV field has witnessed breakthroughs KDD '24, August 25-29, 2024, Barcelona, Spain Qijiong Liu et al. with models such as ViT [21] and DINOv2 [90]. Leveraging these successes in unimodal domains, the multimodal community has concentrated on on aligning content across different modalities, such as CLIP [92], CLAP [24] and BLIP-2 [54]. Notably, the recent introduction of groundbreaking technologies, such as ChatGPT [88], SD [99], and Sora [80], has further advanced the generation capabilities of pretrained large models to unprecedented levels. These recent advancements in pretrained large multimodal models offer new opportunities and challenges in developing content-aware recommender systems. Morespecifically, Section 2 introduces the task of multimodal pretraining for recommendation, emphasizing methods to enhance indomain multimodal pretraining using domain-specific data. Section 3 examines multimodal adaptation for recommendation, elucidating how pretrained multimodal models can be adapted to downstream recommendation tasks through techniques such as representation transfer, model finetuning, adapter tuning, and prompt tuning. Section 4 delves into the emerging topic of multimodal generation for recommendation, with a focus on the application of AI-generated content (AIGC) techniques in recommendation contexts. In Section 5, we outline a range of common applications that necessitate multimodal recommendation, followed by a discussion of open challenges and opportunities for future research in Section 6. Finally, we conclude the survey in Section 7. We hope that this survey, along with the curated resources, will inspire further research efforts to advance this evolving landscape. In this survey, our objective is to provide a comprehensive overview of multimodal recommendation techniques from a new perspective, focusing on leveraging pretrained multimodal models. We explore the latest advancements and future trajectories in multimodal pretraining, adaptation, and generation techniques, along with their applications to recommender systems. Different from prior works, our survey capitalizes on recent progress in multimodal language models [54, 89], prompt and adapter tuning [60, 159], and generation techniques such as stable diffusion [99]. Additionally, we delve into the most recent practical developments and remaining open challenges in applying pretrained multimodal models for recommendation tasks.",
  "2 MULTIMODAL PRETRAINING FOR RECOMMENDATION": "In contrast to supervised learning directly on domain-specific data, self-supervised pretraining learns from a large-scale unlabeled corpus and then adapts the pretrained model to downstream tasks. This approach allows for the acquisition of rich external knowledge in pretraining data, thus leading to the widespread recognition of its effectiveness. In this section, we will first provide a review of major pretraining paradigms and then introduce how they are utilized in the recommendation domain. Figure 1a presents an overview of multimodal pretraining techniques.",
  "2.1 Self-supervised Pretraining Paradigms": "We broadly categorize self-supervised pretraining paradigms into three types according to their pretraining tasks. Reconstructive Paradigm . This pretraining paradigm aims to teach models to reconstruct raw inputs within the information bottleneck framework. Examples include mask prediction methods for partial reconstruction and autoencoder methods for complete reconstruction. Mask prediction methods were initially introduced in BERT [18], where input tokens are randomly masked, prompting the model to learn to predict them based on the surrounding context. In contrast, autoencoder methods (e.g., AE [5], VAE [50]) encode input data into a concise latent space and subsequently learn to fully recover the input from this latent representation. These methods have found extensive use in self-supervised pretraining across various domains such as text [52], vision [21, 36, 119], audio [150], and multimodal data [97, 99]. Following their success, researchers have applied the reconstructive pretraining paradigm to recommendation tasks. For instance, methods like mask item prediction in Bert4Rec [112], mask token prediction in Recformer [55], autoencoder-based item tokenization [72, 96], and masked node feature reconstruction in PMGT [79] have emerged. Despite significant progress, relying solely on this reconstructive paradigm may not capture proximity information from user-item interactions effectively. Consequently, these methods are typically complemented with a contrastive learning paradigm in practice. Autoregressive Paradigm . This paradigm has recently achieved remarkable success, particularly with the rise of large language models (LLMs) such as the GPT family [7, 89, 93, 94]. It generates sequence data token by token in an autoregressive manner, where each token is predicted based on previous observations. In other words, this approach operates in a unidirectional, left-to-right generation framework, which is different from the reconstructive paradigm that employs bidirectional context to predict masked tokens. It has also gained rapid adoption in the CV domain [98] and multimodal domain [151]. In the realm of recommender systems, user behavior sequences naturally lend themselves to sequential processing, fostering the development of numerous autoregressive sequential recommendation models such as SASRec [48]. Recent studies, such as P5 [31] and VIP5 [32], have explored the integration of LLMs or pretrained multimodal models into recommendation tasks. Concurrently, generative recommendation, which frames recommendation as autoregressive sequence generation, has emerged as a burgeoning area of research [96, 123, 125]. Contrastive Paradigm . This pretraining focuses on pairwise similarity, distinguishing between similar and dissimilar data samples by maximizing distances between negative pairs and minimizing them for positive pairs within a representation space. It has proven effective in enhancing the quality of representations across different domains. Examples such as SimCSE [28] for text, SimCLR [11] for images, CLMR [110] for music, and CLIP [92] for multimodal representations highlight its versatility and applicability. Given its ability to capture pairwise similarities, this paradigm finds extensive use in aligning user-item preferences. Applications like MGCL [69], MMSSL [126], MMCPR [82], MSM4SR [152], and MISSRec [121] exemplify the utilization of contrastive learning for enhancing multimodal pretraining in recommender systems.",
  "2.2 Content-aware Pretraining for Recommendation": "Content-aware recommender systems strive to incorporate the semantic content of items to improve recommendation accuracy. Multimodal Pretraining, Adaptation, and Generation for Recommendation: A Survey KDD '24, August 25-29, 2024, Barcelona, Spain Figure 1: An overview of multimodal pretraining, adaptation, and generation tasks for recommendation. Sequence Graph Interaction General-domain Pretraining In-domain Pretraining Reconstructive Task Autoregressive Task Contrastive Task Post-pretrain Text Image Video Text Image Video External Data Domain Data (a) Multimodal Pretraining. Representation Transfer Model Finetuning Adapter Tuning Prompt Tuning Recommendation Model Recommend Item Identier LLM ViT CLIP Pretrained Models (b) Multimodal Adaptation. Text Generation Image Generation Video Generation Generate Item Content text image video Sora LLM SD Generative Models Personalized Generation (c) Multimodal Generation. Consequently, numerous studies have explored content-enhanced pretraining methods for recommendation systems. In this section, we categorize existing research based on the modalities employed for pretraining and discuss their application both generally and within recommendation systems. ads, movies, and videos. In the CV domain, the evolution of visionbased pretraining has transitioned from CNN-based architectures like ResNet [37] to transformer architectures such as ViT [21] and DINOv2 [90], enabling the extraction of versatile visual features. These pretrained models have significantly advanced vision-aware recommendation systems. Researchers like Liu et al.[68] and Chen et al.[12] leverage pretrained CNN encoders with category priors to extract image features for industrial recommendation tasks, finetuning image encoders alongside recommendation models. Similarly, Wang et al.[121] and Wei et al.[125] utilize pretrained transformer encoders, such as ViT [92], to encode images and sequences of user behaviors. Vision foundation models continue to evolve rapidly, promising future applications in recommendation tasks. Looking ahead, there is a growing interest in exploring the use of newly pretrained models for recommendation tasks. For instance, leveraging pretrained video transformers like Video-LLaVA [62] remains an unexplored area in building video recommendation models. Audio-based Pretraining . Music recommendation represents a prominent scenario heavily reliant on audio modalities to capture content semantics. Analogous to the NLP domain, various pretraining techniques have been employed to enhance audio representations, including Wav2Vec [101], MusicBert [150], MART [145], and MERT [61]. In the context of music recommendation, researchers explore leveraging these audio pretraining methods by utilizing user-item interactions as supervision signals to finetune music representations. For example, Chen et al.[10] propose learning user-audio embeddings from track data and user interests through contrastive learning techniques. Furthermore, Huang et al.[43] integrate pairwise textual and audio features into a convolutional model to jointly learn content embeddings in a similarity metric. Interested readers can find additional examples in a comprehensive review paper [15]. Text-based Pretraining . Texts are among the most prevalent forms of content in recommender systems, applied in contexts such as news recommendation and review-based recommendation. Within the domain of natural language processing (NLP), pretrained language models like BERT [18] and T5 [95] have been developed to capture context-aware representations of text. These models typically follow a pretraining-finetuning paradigm tailored to specific tasks. Recently, large language models (LLMs) such as ChatGPT [88] and LLaMa [115] have demonstrated significant capabilities in language-related tasks, leveraging techniques such as prompting and in-context learning. Building on their success, textenhanced pretraining has gained traction in recommender systems. Notable examples include MINER [56] for news recommendation, Recformer [55] for sequential recommendation, UniSRec [38] for cross-domain recommendation, and P5 [31] for LLM-based interactive recommendation. Vision-based Pretraining . Images and videos constitute the primary visual data in multimedia recommendation scenarios as Unlike single-modal techniques, multimodal models must capture both commonalities and complementary information across multimodal data sources through techniques like cross-modal alignment and fusion. In recent years, multimodal pretraining has seen rapid development, resulting in a plethora of pretrained models, including single-stream models (e.g., VL-BERT [111]), dual-stream models (e.g., CLIP [92]), and hybrid models (e.g., FLAVA [106] and CoCa [147]). Recent research has also focused on achieving unified representations of multimodal data, exemplified by models like ImageBind [33], MetaTransformer [154], and UnifiedIO-2 [83]. Another emerging trend involves integrating multimodal encoders with large language models, resulting in multimodal large language models such as BLIP-2 [54], Flamingo [1], and Llava [67]. These Multimodal Pretraining . In current literature, most studies tend to focus on modeling the primary modality of content, such as text for news recommendation [56], audio for music recommendation [10], and images for e-commerce recommendation [68]. However, multimedia content inherently involves multiple modalities. For instance, news articles often include titles, descriptions, and accompanying images. Similarly, video recommendation involves handling visual frames, audio signals, and subtitles. KDD '24, August 25-29, 2024, Barcelona, Spain Qijiong Liu et al. advancements offer promising opportunities for building modern multimodal recommender systems. Initial efforts in this direction include models like MISSRec [121], Rec-GPT4V [77], MMSSL [126], MSM4SR [152], and AlignRec [81]. However, current research primarily focuses on utilizing off-theshelf pretrained multimodal encoders and integrating them with sequence-based or graph-based pretraining techniques for handling recommendation data. There remains a gap in how to specifically pretrain domain-specific multimodal models tailored for recommendation tasks. Pioneering efforts have been made in the e-commerce domain, as evidenced by models such as M5Product [20], K3M [165], ECLIP [47], and CommerceMM [148]. Future research is expected to further explore and advance in this direction.",
  "3 MULTIMODAL ADAPTION FOR RECOMMENDATION": "While most existing pretrained models are trained on general data corpora, adapting them for recommender systems requires strategic methods to fully utilize their learned knowledge. This section summarizes four major adaptation techniques: representation transfer, model finetuning, adapter tuning, and prompt tuning. Each technique provides a distinct approach to harnessing the benefits of a pretrained model. Figure 1b presents an overview of multimodal adaptation techniques.",
  "3.1 Representation Transfer": "Representation transfer is one of the most commonly used adaptation techniques for transferring pretrained knowledge to recommendation models. Specifically, item representations are extracted from frozen pretrained models and used as additional features alongside ID embeddings. These representations provide supplementary general information to recommender systems, addressing the cold-start problem where new or infrequently interacted items may have inadequate ID embeddings derived from limited interactions [74]. Approaches based on representation transfer have been extensively studied and proven effective across various domains, e.g., text-based recommendation [132], vision-based recommendation [4, 102], multimodal recommendation [22, 41, 64, 128, 140]. For multimodal scenarios specifically, significant efforts have been directed towards fusing representations from multiple modalities. This includes techniques such as early fusion [41, 75, 79, 161], intermediate fusion [22, 134, 140], and late fusion [114, 130]. Another research focus involves aligning multimodal representations within user behavior spaces using methods like content-ID alignment [78, 129], item-to-item matching [44], user sequence modeling [38], and graph neural networks [131]. However, straightforward and efficient representation transfer may encounter a significant domain generalization gap due to misalignment between the semantic space and the behavior space, which may not consistently lead to performance improvements in practice. Model finetuning offers a direct solution to address this issue. Furthermore, as noted by KDSR [41], there is a risk of forgetting modality features in representations, leading them to resemble those trained without incorporating such features. One viable approach involves integrating explicit constraints [41], while another potential solution introduces semantic tokenization techniques [46, 72, 96, 107], which quantize item content representations into discrete tokens.",
  "3.2 Model Finetuning": "Model finetuning refers to the process of further training a pretrained model on task-specific data. Its goal is to adapt the model parameters to effectively capture domain-specific nuances, thereby improving its performance on the specific downstream task. This pretraining-finetuning paradigm has proven successful in various practical applications. Specifically, finetuning can involve aligning the semantic space of pretrained models with the behavior space of recommendation models. Depending on the application of pretrained models, they can extract item representations [56, 141], user representations [113], or both [55, 74, 121]. Moreover, based on the types of downstream tasks, current research can be classified into representation-based matching tasks [56, 121, 133] and scoring-based ranking tasks [68, 141, 153]. However, end-to-end finetuning of pretrained models with recommendation data faces challenges related to training efficiency. Recommendation tasks often require processing millions or even billions of samples daily. Fully finetuning a pretrained model substantially amplifies training overhead, which poses practical limitations in large-scale recommender systems, particularly given the scale of large language and multimodal models [54, 115]. Moreover, finetuning with large volumes of data easily leads to the issue of catastrophic forgetting, where previously learned knowledge rapidly deteriorates during continual training.",
  "3.3 Adapter Tuning": "Toreduce training overhead with pretrained large models, parameterefficient finetuning (PEFT) methods have been developed. One prominent approach is through parameter-efficient adapters like LoRAs [40], which integrate compact, task-specific modules directly into pretrained models. This strategy effectively reduces the number of parameters needed for finetuning and facilitates rapid model adaptation. Widely recognized for its efficacy across various domains, PEFT techniques have gained significant traction in recommendation systems [27, 32, 38, 71, 135]. For example, the ONCE framework [71] leverages the pretrained Llama model [115] with LoRAs as item encoders to enhance content-aware recommendation. Similarly, UniSRec [38] employs an MOE-based adapter with the BERT model to improve semantic representations of items across diverse domains. In the realm of multimodal recommendation, TransRec [27] and VIP5 [32] have introduced layerwise adapters. M3SRec utilizes modality-specific MOE adapters [6], while EM3 employs multimodal fusion adapters [16] during finetuning. Nonetheless, the ongoing challenge lies in designing adapters that effectively balance both effectiveness and efficiency, which remains an active area of research.",
  "3.4 Prompt Tuning": "With the emergence of large language models, prompting has become a pivotal technique in harnessing their capabilities to generate desired outputs or perform specific tasks [70]. Instead of using handcrafted prompts, prompt tuning aims to learn task-adaptable Multimodal Pretraining, Adaptation, and Generation for Recommendation: A Survey KDD '24, August 25-29, 2024, Barcelona, Spain prompts from task-specific data while keeping the model parameters frozen. As a result, prompt tuning can avoid catastrophic forgetting and enable fast adaptation with only prompt tokens as tunable parameters. Depending on whether prompts are optimized in a discrete token space, they can be further categorized into hard prompt tuning, such as AutoPrompt [105], and soft prompt tuning, like Prefix-Tuning [60]. Prompt tuning has been successful in visual learning [159] and multimodal learning [23, 49], enhancing model performance. For multimodal recommendation tasks, prompt tuning has emerged as a novel technique to adapt pretrained models. Recent studies such as RecPrompt [66], ProLLM4Rec [138], Prompt4NR [155], and PBNR [59] employ prompting methods to customize large language models (LLMs) for news recommendation tasks. Additionally, DeepMP [125], VIP5 [32], and PromptMM [127] employ prompt tuning to integrate and adapt multimodal content knowledge to enhance recommendation. Despite recent advancements, this area of research remains underexplored. An interesting direction is the development of personalized multimodal prompting techniques to advance multimodal recommendation systems.",
  "4 MULTIMODAL GENERATION FOR RECOMMENDATION": "With recent advancements in generative models, AI-generated content (AIGC) has gained significant popularity across diverse applications. In this section, we explore potential research avenues for employing AIGC techniques within recommender systems. Figure 1c presents an overview of multimodal generation techniques.",
  "4.1 Text Generation": "With the support of powerful large language models (LLMs), text generation has become a mature capability and is now being applied in various tasks within the recommendation domain [86]. · Keyword Generation : Keyword tagging plays a pivotal role in content understanding for ads targeting and recommendation. Previous techniques mostly rely on explicit keyword extraction from textual content, potentially missing important keywords absent from the text. Consequently, keyword generation techniques have been widely applied to enhance the keyword tagging process [53, 108]. · Marketing Copy Generation : Marketing copy refers to the text used to promote a product and motivate consumers to purchase. It plays a vital role in capturing users' interest and enhancing engagement. Recent efforts have focused on automatic marketing copywriting based on LLMs [85, 158] · News Headline Generation : The demand for personalized and engaging news content has fueled the exploration of news headline generation. Conventionally, headline generation is framed as a text summarization task, condensing input text or multimodal content into a title [19, 51]. However, typical news headlines may lack appeal or relevance to specific users, prompting the need for personalized approaches. Consequently, personalized headline generation has emerged as a compelling research topic, focusing on generating titles tailored to individual users' reading preferences and available news content [35, 100]. · Explanation Generation : In interactive scenarios, the demand for explainable recommendations is growing significantly. This involves generating natural language explanations to justify the recommendation of items to individual users, thereby enhancing user understanding and trust in the system [57, 156]. · Dialogue Generation : Dialogue generation [142] is essential in conversational recommender systems, encompassing the generation of responses that describe recommended items [26]. Moreover, it entails generating questions to guide users towards further rounds of conversation and interaction [124]. While these tasks benefit from powerful LLMs, two critical challenges persist in text generation for recommendation: 1) Controllable Generation: Industrial applications necessitate precise control over generated texts to ensure correctness of product descriptions, use unique selling propositions, or adhere to specific writing styles [160]. 2) Knowledge-Enhanced Generation: Existing LLMs often lack explicit awareness of domain-specific knowledge, such as product entities, categories, and selling points. Recent research has concentrated on integrating domain-specific knowledge bases to achieve more satisfactory results [117, 139].",
  "4.2 Image and Video Generation": "Text-to-image generation has achieved remarkable success with the prevalence of diffusion models (e.g., SD [99]). In this section, we delve into their potential applications in e-commerce and advertising. Unlike natural image generation, generating product images and ad banners involves dealing with complex layouts, encompassing various elements such as products, logos, and textual descriptions. Consequently, unique challenges arise in designing a coherent layout and effectively integrating text with appropriate fonts and colors to create visually appealing posters. More recently, video generation has made significant strides. Sora [80] emerges as a groundbreaking technology showcasing immense potential for generating advertising videos for products. In this context, Gong et al. [34] introduce AtomoVideo, a high-fidelity image-to-video generation solution that effectively transforms product images into engaging promotional videos for advertising purposes. Additionally, Liu et al. [65] have devised a system capable of automatically generating visual storylines from a given set of visual materials, producing compelling promotional videos tailored for e-commerce. Furthermore, Wang et al. [122] have developed an integrated approach, merging text-to-image models, video motion Specifically, Inoue et al. [45] propose LayoutDM, a model designed to effectively handle structured layout data and facilitate the discrete diffusion process. Hsu et al. [39] enable content-aware layout generation (namely PosterLayout) by arranging predefined spatial elements on a given canvas. Lin et al. [63] develop AutoPoster, a highly automated and content-aware system for generating advertising posters. Concurrently, some studies explore text design for poster generation. For example, Gao et al. [29] introduce TextPainter, a novel multimodal approach that leverages contextual visual information and corresponding text semantics to generate text images. Tuo et al. [118] propose a diffusion-based multilingual visual text generation and editing model, AnyText, which addresses how to render accurate and coherent text in the image. KDD '24, August 25-29, 2024, Barcelona, Spain Qijiong Liu et al. generators, reference image embedding modules, and frame interpolation modules into an end-to-end video generation pipeline, which is valuable for micro-video recommendation platforms. We believe that this field is rapidly expanding, enabling the advancement of AIGC-based recommendation and advertising applications.",
  "4.3 Personalized Generation": "With the rise of AIGC, there is a notable shift towards personalized generation, aiming to enhance the customization and personalization of generated content. This trend holds particular significance in recommendation scenarios, where personalized content can better cater to users' interests. Pioneering work has been undertaken in various domains, including personalized news headline generation [2, 3, 8, 100], personalized product description generation in e-commerce [17], personalized answer generation [17], personalized image generation with identity preservation [21], and personalized multimodal generation [104]. Integrating recommender systems with personalized generation techniques shows promise for developing next-generation recommender systems.",
  "5 APPLICATIONS": "In this section, we summarize some common application domains that require multimodal recommendation techniques. · E-commerce Recommendation . E-commerce represents one of the most extensively studied application domains in recommender systems research, aimed at assisting users in discovering items they are likely to purchase. The abundance of multimodal data in e-commerce, including product titles, descriptions, images, and reviews, poses a challenge in integrating different modalities with user interaction data to enhance recommendation quality. To address this challenge, numerous research efforts have been undertaken. Notable examples include works by Alibaba [30, 58, 141], JD.com [68, 136], and Pinterest [4]. · News Recommendation . Personalized news recommendation is a crucial technique for assisting users in discovering news of interest. To enhance recommendation accuracy and diversity, recommender systems must comprehend news content and extract semantic information from a user's reading history. This often involves learning semantic representations of news titles, abstracts, body text, and cover images. Recent research has focused on modeling features from multiple modalities, as exemplified by MM-Rec [134] and IMRec [140]. · Advertisement Recommendation . Online advertising serves as a primary revenue source for many web applications. Advertising creatives play a pivotal role in this ecosystem, spanning various formats such as images, titles, and videos. Aesthetic creatives have the potential to engage potential users and enhance the click-through rate (CTR) of products [9]. There is also a pressing need to understand ad creatives better to effectively align advertisements with users' interests [143, 144]. · Video Recommendation . With the surge in popularity of microvideo platforms, video recommendation has garnered significant attention within the community. Videos encapsulate a multitude of modalities, including titles, thumbnail images, frames, audio tracks, transcripts, and more. Current research efforts have been concentrated on integrating and adapting multimodal information within micro-video recommendation models. [131, 146]. Notably, Ni et al. [87] have recently introduced a comprehensive micro-video recommendation dataset, enriched with abundant multimodal side information, to foster further research in this domain. · Fashion Recommendation . With the visual and aesthetic nature of fashion products, fashion recommendation has emerged as a distinct vertical domain. Unlike traditional recommender systems, fashion recommendation not only suggests individual items but also outfits that complement multiple items. Multimodal understanding capabilities play a pivotal role in this area, including tasks such as localizing fashion items from images, identifying their attributes, and computing compatibility scores for multiple items [13, 109]. Moreover, pioneering work [164] has developed text-to-image diffusion models that allow users to virtually try on clothes. These techniques are expected to enhance the personalization of fashion recommendation and elevate user experience to the next level. · Music Recommendation . The realm of music streaming services represents another prominent domain that necessitates multimodal recommendation techniques. Within this sphere, a diverse array of multimodal data is involved, including music audio, scores, lyrics, tags, and reviews. Leveraging these various types of music data has proven effective in crafting more personalized recommendations aimed at engaging users; notable examples can be found in [10, 43]. Additionally, Shen et al. [103] propose that incorporating multimodal information from users' social media can offer insights into their personalities, emotions, and mental well-being, thereby enhancing the accuracy of music recommendation. · LBS Recommendation . Location-based services (LBS) have become ubiquitous, offering a wide range of services including taxi travel, food delivery, and restaurant recommendation. In these contexts, users can share their Points of Interest (POI) check-ins, photos, opinions, and comments, which encompass a rich array of multimodal spatio-temporal data. Integrating this multimodal information and understanding spatio-temporal correlations among locations enables more accurate modeling of user preferences. Notable examples can be found in [76, 91].",
  "6 CHALLENGES AND OPPORTUNITIES": "In this section, we discuss the persistent challenges and emerging opportunities for future research. · Multimodal Information Fusion . Multimodal fusion has been extensively explored in research. Within recommender systems, current studies primarily concentrate on fusing and adapting multimodal feature embeddings of items to recommendation models [157]. However, multimodal information for recommendation inherently adopts a hierarchical structure, ranging from user behavior sequences to individual items, each comprising multiple modalities and further subdivided into semantic tokens and objects. Additionally, the impact of information from diverse modalities and regions can vary significantly among different Multimodal Pretraining, Adaptation, and Generation for Recommendation: A Survey KDD '24, August 25-29, 2024, Barcelona, Spain users. As a result, the challenge lies in effectively fusing multimodal information in a hierarchical and personalized manner to optimize recommendations. · MultimodalFoundationModelsforRecommendation. While large language models and large multimodal models have emerged as foundation models in the NLP and CV domains, there exists a compelling opportunity to extend this exploration into the recommendation domain. An ideal recommendation foundation model should demonstrate robust in-context learning capabilities while maintaining generalizability across diverse tasks and domains [42]. Potential avenues for exploration include adapting existing multimodal LLMs for recommendation tasks (e.g., [32]), or conducting the pretraining of a multimodal generative model from scratch using large-scale multimodal multi-domain recommendation data. · Multimodal Multi-domain Recommendation. Multimodal information provides rich semantic insights into item content. Despite considerable research into multimodal recommendation and cross-domain recommendation, effectively leveraging multimodal information to bridge the information gap across domains remains an open challenge [113]. For instance, recommending music based on a user's reading habits entails semantic alignment across modalities (audio vs. text) and domains (music vs. books). · AIGC for Recommendation. The integration of AIGC represents a notable advancement in recommender systems, offering an opportunity to significantly enhance user personalization, engagement, and overall experience. This encompasses personalized news headlines, tailored advertising creatives, and explanatory content across diverse recommendation contexts. This field is rapidly expanding, with the primary challenge lying in achieving a comprehensive understanding of both content and users, facilitating controllable generation, and ensuring accurate formatting to optimize the user experience. Additionally, it is imperative to address potential ethical and privacy concerns arising from the use of AIGC. · Efficiency of Training and Inference. Recommendation tasks typically have stringent latency requirements to meet real-time service demands. Therefore, ensuring training and inference efficiency becomes imperative when applying multimodal pretraining and generation techniques in practice. There is a high demand for the development of efficient strategies to leverage the capabilities of multimodal models. Pioneer efforts in this direction include speeding up training by merging item sets to avoid redundant encoding operations [137] and enhancing inference speed [12, 74] through caching item and user representations. · MultimodalRecommendationAgent. LLM-based agents [120] have demonstrated exceptional proficiency in automating tasks through extensive knowledge and strong reasoning capabilities. The integration of these agents has introduced innovative prospects in the field of recommendation, particularly in conversational recommendation [25]. This entails directly engaging users in the task completion process, thereby enhancing the user experience and the effectiveness of recommender systems. As a concrete example, integrating conversation and virtual try-on generation [164] capabilities may present new opportunities for fashion recommendation.",
  "7 CONCLUSION": "Multimodal recommendation, an immensely promising field, has garnered significant attention in recent years, fueled by advancements in both multimodal machine learning and the recommendation system community. The advent of large multimodal models has transformed the multimodal recommendation landscape, endowing it with enhanced capabilities for comprehension and content generation. This paper provides a systematical overview of the current multimodal recommendation framework, focusing on key aspects such as multimodal pretraining, adaptation, and generation. Additionally, we delve into its applications, challenges, and future prospects. Our aim is to offer this survey as a resourceful guide to aid subsequent research in the field.",
  "ACKNOWLEDGMENTS": "We thank Dr. Xin Zhou and Dr. Chuhan Wu for the discussion and contribution to the tutorial materials of multimodal pretraining and generation for recommendation presented at WWW 2024 [163].",
  "REFERENCES": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. 2022. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems (NeurIPS) (2022), 23716-23736. [3] Xiang Ao, Xiting Wang, Ling Luo, Ying Qiao, Qing He, and Xing Xie. 2021. PENS: A Dataset and Generic Framework for Personalized News Headline Generation. In Proceedings of ACL/IJCNLP . 82-92. [2] Xiang Ao, Ling Luo, Xiting Wang, Zhao Yang, Jiun-Hung Chen, Ying Qiao, Qing He, and Xing Xie. 2023. Put Your Voice on Stage: Personalized Headline Generation for News Articles. TKDD 18, 3 (2023). [4] Paul Baltescu, Haoyu Chen, Nikil Pancha, Andrew Zhai, Jure Leskovec, and Charles Rosenberg. 2022. Itemsage: Learning product embeddings for shopping recommendations at pinterest. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD) . 2703-2711. [6] Shuqing Bian, Xingyu Pan, Wayne Xin Zhao, Jinpeng Wang, Chuyuan Wang, and Ji-Rong Wen. 2023. Multi-modal Mixture of Experts Represetation Learning for Sequential Recommendation. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management (CIKM) . 110-119. [5] Dor Bank, Noam Koenigstein, and Raja Giryes. 2020. Autoencoders. CoRR abs/2003.05991 (2020). [7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in Neural Information Processing Systems (NeurIPS) (2020), 1877-1901. [9] Jin Chen, Ju Xu, Gangwei Jiang, Tiezheng Ge, Zhiqiang Zhang, Defu Lian, and Kai Zheng. 2021. Automated Creative Optimization for E-Commerce Advertising. In The ACM Web Conference (WWW) . 2304-2313. [8] Pengshan Cai, Kaiqiang Song, Sangwoo Cho, Hongwei Wang, Xiaoyang Wang, HongYu,Fei Liu, and Dong Yu. 2023. Generating User-Engaging News Headlines. In Proceedings of ACL . 3265-3280. [10] Ke Chen, Beici Liang, Xiaoshuan Ma, and Minwei Gu. 2021. Learning audio embeddings with user listening data for content-based music recommendation. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) . IEEE, 3015-3019. [12] Xin Chen, Qingtao Tang, Ke Hu, Yue Xu, Shihang Qiu, Jia Cheng, and Jun Lei. 2022. Hybrid CNN Based Attention with Category Prior for User Image Behavior Modeling. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR) . 2336-2340. [11] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. 2020. A Simple Framework for Contrastive Learning of Visual Representations. In Proceedings of the 37th International Conference on Machine Learning (ICML) . 1597-1607. [13] Yashar Deldjoo, Fatemeh Nazary, Arnau Ramisa, Julian J. McAuley, Giovanni Pellegrini, Alejandro Bellogín, and Tommaso Di Noia. 2024. A Review of Modern Fashion Recommender Systems. ACM Comput. Surv. 56, 4 (2024), 87:1-87:37. [15] Yashar Deldjoo, Markus Schedl, and Peter Knees. 2021. Content-driven Music Recommendation: Evolution, State of the Art, and Challenges. CoRR [14] Yashar Deldjoo, Markus Schedl, Paolo Cremonesi, and Gabriella Pasi. 2020. Recommender systems leveraging multimedia content. Comput. Surveys 53, 5 (2020), 1-38. KDD '24, August 25-29, 2024, Barcelona, Spain Qijiong Liu et al. Multimodal Pretraining, Adaptation, and Generation for Recommendation: A Survey KDD '24, August 25-29, 2024, Barcelona, Spain KDD '24, August 25-29, 2024, Barcelona, Spain Qijiong Liu et al. [126] Wei Wei, Chao Huang, Lianghao Xia, and Chuxu Zhang. 2023. Multi-modal self-supervised learning for recommendation. In Proceedings of the ACM Web Multimodal Pretraining, Adaptation, and Generation for Recommendation: A Survey KDD '24, August 25-29, 2024, Barcelona, Spain",
  "keywords_parsed": [
    "Recommender Systems",
    "Multimodal Pretraining",
    "Multimodal Adaptation",
    "Multimodal Generation"
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "Flamingo: a visual language model for few-shot learning"
    },
    {
      "ref_id": "b3",
      "title": "PENS: A Dataset and Generic Framework for Personalized News Headline Generation"
    },
    {
      "ref_id": "b2",
      "title": "Put Your Voice on Stage: Personalized Headline Generation for News Articles"
    },
    {
      "ref_id": "b4",
      "title": "Itemsage: Learning product embeddings for shopping recommendations at pinterest"
    },
    {
      "ref_id": "b6",
      "title": "Multi-modal Mixture of Experts Representation Learning for Sequential Recommendation"
    },
    {
      "ref_id": "b5",
      "title": "Autoencoders"
    },
    {
      "ref_id": "b7",
      "title": "Language models are few-shot learners"
    },
    {
      "ref_id": "b9",
      "title": "Automated Creative Optimization for E-Commerce Advertising"
    },
    {
      "ref_id": "b8",
      "title": "Generating User-Engaging News Headlines"
    },
    {
      "ref_id": "b10",
      "title": "Learning audio embeddings with user listening data for content-based music recommendation"
    },
    {
      "ref_id": "b12",
      "title": "Hybrid CNN Based Attention with Category Prior for User Image Behavior Modeling"
    },
    {
      "ref_id": "b11",
      "title": "A Simple Framework for Contrastive Learning of Visual Representations"
    },
    {
      "ref_id": "b13",
      "title": "A Review of Modern Fashion Recommender Systems"
    },
    {
      "ref_id": "b15",
      "title": "Content-driven Music Recommendation: Evolution, State of the Art, and Challenges"
    },
    {
      "ref_id": "b14",
      "title": "Recommender systems leveraging multimedia content"
    },
    {
      "ref_id": "b126",
      "title": "Multi-modal self-supervised learning for recommendation"
    }
  ]
}