{
  "Hybrid Contrastive Constraints for Multi-Scenario Ad Ranking": "Shanlei Mu mushanlei.msl@alibaba-inc.com Alibaba Group",
  "Penghui Wei": "wph242967@alibaba-inc.com Alibaba Group Wayne Xin Zhao batmanfly@gmail.com Renmin University of China Shaoguo Liu shaoguo.lsg@alibaba-inc.com Alibaba Group",
  "Liang Wang": "liangbo.wl@alibaba-inc.com Alibaba Group",
  "ABSTRACT": "Multi-scenario ad ranking aims at leveraging the data from multiple domains or channels for training a unified ranking model to improve the performance at each individual scenario. Although the research on this task has made important progress, it still lacks the consideration of cross-scenario relations, thus leading to limitation in learning capability and difficulty in interrelation modeling. In this paper, we propose a H ybrid C ontrastive C onstrained approach ( HC 2 ) for multi-scenario ad ranking. To enhance the modeling of data interrelation, we elaborately design a hybrid contrastive learning approach to capture commonalities and differences among multiple scenarios. The core of our approach consists of two elaborated contrastive losses, namely generalized and individual contrastive loss, which aim at capturing common knowledge and scenario-specific knowledge, respectively. To adapt contrastive learning to the complex multi-scenario setting, we propose a series of important improvements. For generalized contrastive loss, we enhance contrastive learning by extending the contrastive samples (label-aware and diffusion noise enhanced contrastive samples) and reweighting the contrastive samples (reciprocal similarity weighting). For individual contrastive loss, we use the strategies of dropout-based augmentation and cross-scenario encoding for generating meaningful positive and negative contrastive samples, respectively. Extensive experiments on both offline evaluation and online test have demonstrated the effectiveness of the proposed HC 2 by comparing it with a number of competitive baselines.",
  "CCS CONCEPTS": "¬∑ Information systems ‚Üí Online advertising .",
  "KEYWORDS": "Online Advertising, Advertisement Ranking, Multi-Scenario Ad Ranking, Contrastive Learning",
  "ACMReference Format:": "Shanlei Mu, Penghui Wei, Wayne Xin Zhao, Shaoguo Liu, Liang Wang, Bo Zheng. 2023. Hybrid Contrastive Constraints for Multi-Scenario Ad Ranking. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference'17, July 2017, Washington, DC, USA ¬© 2023 Association for Computing Machinery. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn Bo Zheng bozheng@alibaba-inc.com Alibaba Group In Proceedings of ACM Conference (Conference'17). ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn",
  "1 INTRODUCTION": "Online advertising is an important e-commerce marketing strategy for targeting potential consumers for product or service [23]. To conduct this strategy, advertisement ranking [2] has become a widely studied task [6, 12], which aims at predicting the probability of user behavior (such as click and conversion) given a pair of ‚ü® ùë¢ùë†ùëíùëü, ùëéùëë ‚ü© . In practice, an ad ranking system usually has to deal with multiple business domains ( e.g., electronic or home products) or channels ( e.g., mobile app or search engine), referred to as multi-scenario ad ranking [5, 8, 26, 27, 32]. For multi-scenario ad ranking, a unified model is to be trained by leveraging various kinds of user feedback data from different scenarios. As a major merit, multi-scenario signals can alleviate the data sparsity issue at each individual scenario, thus leading to an improved overall performance. Although existing methods have been shown effective to some extent, two key issues are still not well resolved. (1) Limitation in learning capability : typically, the shared-specific architecture is only optimized based on the supervision signals from multiple scenarios. Task-specific losses are highly limited by the availability and quality of training data, and it would be less effective to optimize the composite architecture integrating multi-scenario components. (2) Difficulty in interrelation modeling : since we cannot obtain direct supervision signals for learning multi-scenario relations, it is difficult to accurately capture the commonalities and differences across scenarios. Especially, data interrelation is very complex across scenarios, showing diverse correlating patterns in varying samples or contexts. Traditional optimization methods based on task loss cannot well capture such interrelation in multi-scenario ad ranking. Technically speaking, the key of multi-scenario ranking lies in the modeling of multi-scenario interrelations, capturing the commonalities as well as discriminating the differences across scenarios. For this purpose, shared-specific architecture ( e.g., SharedBottom [3] and MMoE [20]) has been widely adopted to develop neural network ranking models [15, 17, 26, 28, 42], where scenario-shared and scenario-specific components are combined in the backbone. Typically, scenario-shared components are optimized using all scenarios' data for capturing common knowledge among multiple scenarios, while scenario-specific components are optimized using the data of the corresponding scenario for learning scenario-specific knowledge. Besides, some parameter generation models have been also proposed [26, 35, 38], where a shared network is used to dynamically generate scenario-specific parameters, and they can also be viewed as a type of shared-specific architecture. Conference'17, July 2017, Washington, DC, USA Mu, et al. To address the above issues, we borrow the idea of contrastive learning [4, 10, 40] for improving multi-scenario ad ranking. Contrastive learning aims to enhance neural network's expressive ability by contrasting positive and negative samples' representations from different views, achieving great success in many domains. By leveraging either explicit or implicit data correlation patterns, it can derive rich self-supervision signals for enhancing the model learning capacity ( corresponding to the first issue ). Meanwhile, it is conducted at the sample level, so that we can devise flexible, tailored contrastive losses for achieving specific goals ( corresponding to the second issue ). Combining the two merits, we can develop more effective approaches for multi-scenario ad ranking. Our major contributions are summarized as follows: To this end, in this paper, we propose a H ybrid C ontrastive C onstrained approach HC 2 for multi-scenario ad ranking. To effectively model the data interrelation among multiple scenarios, HC 2 designs a hybrid of generalized and individual contrastive losses for capturing common and scenario-specific knowledge, respectively. To adapt contrastive learning to the complex multi-scenario task, we introduce two important technical improvements based on a shared-specific architecture. For generalized contrastive loss , we enhance contrastive learning by extending the contrastive samples and reweighting the contrastive samples. To extend the contrastive samples, we propose to construct label-aware contrastive samples (using both coarse- and fine-grained samples under the guidance of prediction labels) and diffusion noise enhanced contrastive samples (incorporating stochastic noise to enhance the generalization ability). In this way, we can derive more sufficient contrastive samples tailored for capturing common knowledge across multiple scenarios. Further, we propose a reciprocal similarity weighting mechanism to reduce the influence of false negative/positive , which is an important issue in contrastive learning. The basic idea is to adaptively set the weight of a contrastive pair during training according to their similarity. For individual contrastive loss , we construct scenario-aware contrastive samples by leveraging across-scenario interrelation to capture scenario-specific knowledge. Specifically, we propose to use the strategies of dropout-based augmentation and cross-scenario encoding for generating meaningful positive and negative contrastive samples, respectively. Putting them all together, such a hybrid of generalized and individual contrastive losses can lead to significant performance improvement for multi-scenario ad ranking. ¬∑ To the best of our knowledge, this is the first work that deeply explores the modeling of multi-scenario interrelations in a contrastive learning way. Our approach can be generally applied to various shared-specific architectures. ¬∑ Experimental results on both public and production datasets demonstrate that HC 2 achieves much better performance than competitors for multi-scenario ad ranking in various settings. Besides, online test by deploying our approach on a real ad ranking system also verifies the effectiveness in increased CTR, CVR and GMV. ¬∑ To better capture both common and scenario-specific knowledge, we design novel generalized and individual contrastive losses to improve the multi-scenario ad ranking. Further, we introduce a series of specific strategies to adapt contrastive learning to the complex multi-scenario ranking task.",
  "2 PRELIMINARIES": "We first formulate the multi-scenario ad ranking problem. Let X and Y denote the feature space and the label space respectively. The feature space consists of user features, ad features and context features, and the label space represents user behavior denoted as binary labels such as { click , non-click } . Consider ùêæ different scenarios {S 1 , ¬∑ ¬∑ ¬∑ , S ùêæ } , where the instance set of scenario ùëò is represented as S ùëò = {( ùë• ùëò ùëñ , ùë¶ ùëò ùëñ )} | S ùëò | ùëñ = 1 . Here ùë• ùëò ùëñ ‚àà X is the feature representation of the ùëñ -th instance, and ùë¶ ùëò ùëñ ‚àà { 0 , 1 } is the binary label of user behavior. Multi-scenario ad ranking aims to leverage all scenarios' samples to train a model ÀÜ ùë¶ ùëò ùëñ = ùëù Œò ( ùë• ùëò ùëñ ) (parameterized by Œò ) that can accurately predict the user behavior label ùë¶ ùëò ùëñ . The cross-entropy loss is usually used to optimize the model parameters Œò , and the standard loss function of multi-scenario ad ranking can be formulated as follows:  where ‚Ñì (¬∑) is the cross-entropy loss.",
  "3 METHODOLOGY": "In this section, we present the H ybrid C ontrastive C onstrained multi-scenario ad ranking approach, named HC 2 . As shown in Figure 1, our approach is built on a shared-specific backbone, and introduces two major contrastive learning losses, namely generalized contrastive loss and individual contrastive loss . The generalized contrastive loss is applied to the scenario-shared component for capturing the common knowledge from multiple scenarios. The individual contrastive loss is applied to the scenario-specific component for learning the scenario-specific knowledge. These two contrastive losses can effectively explore multi-scenario interrelation for improving ad ranking in multiple scenarios. Next, we describe each part in detail.",
  "3.1 Backbone of HC 2": "Our approach HC 2 is designed as a general solution to improve the training of shared-specific architectures [3, 17, 20, 26, 38] in multi-scenario ad ranking. Without loss of generality, we use the representative SharedBottom [3] architecture as the backbone to describe our approach. It consists of four main parts [3]: feature embedding layer, scenario-shared neural network, scenario-specific neural network and output layer. The feature embedding layer is shared for all the scenarios, which is used to transform the input features ùë• ùëò ùëñ into the embeddings ùíÜ ùëò ùëñ . The scenario-shared neural network is also used for all the scenarios, which is an ùêø -layered fullyconnected neural network ùëì (¬∑) , taking as input the embeddings ùíÜ ùëò ùëñ and generating the shared hidden representations ùíõ ùëò ùëñ as follows:  Thescenario-specific neural network is an ùêø ‚Ä≤ -layered fully-connected neural network ùëî ( ùëò ) (¬∑) , which is maintained specifically for each scenario ùëÜ ùëò . It tasks the shared hidden representations ùíõ ùëò ùëñ as input and generates the scenario-specific hidden representations ùíâ ùëò ùëñ as:  Hybrid Contrastive Constraints for Multi-Scenario Ad Ranking Conference'17, July 2017, Washington, DC, USA (b) Generalized Contrastive Loss for Capturing Common Knowledge Generalized Contrastive Loss Shared anchor positive negative // stop-gradient feature embedding same label different scenario different label different scenario // inverse similarity score diffusion noise Pull Together Push Apart Shared Feature Embedding Layer Scenario- specific Scenario- specific input feature Output Layer Individual Contrastive Loss Scenario- specific Scenario- specific shared hidden representations Add Dropout anchor positive hard negative negative at step label-aware contrastive samples scenario-aware contrastive samples (a) Backbone of HC 2 (c) Individual Contrastive Loss for Capturing Scenario-Specific Knowledge Figure 1: The overall framework of our approach. Finally, the scenario-specific hidden representations ùíâ ùëò ùëñ are fed to the output layer to generate the predicted user behavior label ÀÜ ùë¶ ùëò ùëñ . pairs equally, we further propose a reciprocal similarity weighting mechanism (Section 3.2.3) to reduce the influence of false contrastive samples. Next, we discuss the two improvements in detail.",
  "3.2 Generalized Contrastive Loss for Capturing Common Knowledge": "Existing multi-scenario ad ranking methods mainly rely on the shared component to capture the common knowledge from multiple scenarios, while it is only optimized by the end-to-end feature-tolabel mapping. For the shared component, the lack of targeting guidance or constraint in the training process makes it difficult to capture the common knowledge thoroughly. To address this issue, we devise an improved contrastive learning way to help the shared component better capture the common knowledge. 3.2.1 Overall Contrastive Loss. Our generalized contrastive loss is adapted based on the classic contrastive learning [4, 10, 40] by introducing two major improvements: (1) extending the contrastive samples, and (2) incorporating the contrastive weights, which is formulated as follows:  where ùíõ ùëñ , ùíõ ùëñ + and ùíõ ùëñ -denote the representations for the anchor sample, the positive sample and the negative sample respectively, N denotes the set of negative samples, ùúè is a temperature coefficient, and ùë† ùëñ,ùëñ ‚Ä≤ a contrastive weight to be set. For ease of discussion, we drop the scenario index ùëò in Eq. (4). In a standard contrastive learning approach [4], positives are usually generated by leveraging explicit associations or data augmentation, while negatives are usually obtained by random sampling. To construct more meaningful contrastive samples, we incorporate label-aware contrastive samples and diffusion noise enhanced samples (Section 3.2.2). As another improvement, the original loss considers all the contrastive 3.2.2 Contrastive Samples Construction. In contrastive learning, it is key to contrast high-quality positive and negative samples. Next, we extend the original contrastive samples in two new ways. Label-aware Contrastive Samples . Firstly, we propose to use the label-aware contrastive samples, which utilize the label guidance to construct contrastive samples for the generalized contrastive loss. We consider both coarse- and fine-grained contrastive samples: ¬∑ Fine-grained extension : With the guidance of user behavior labels, we can only explore coarse-grained information to generate contrastive samples. Intuitively, the representations of similar users or ads in different scenarios are more informative to reflect common knowledge across multiple scenarios. Thus, we further select informative samples based on the similarity with the anchor samples. Note, the label of a sample still determines its role in contrastive learning ( positive or negative ), while similarity is only used to select the informative samples (only keeping the similar samples for both positives and negatives). In our implementation, we use the ùêæ -means algorithm in Faiss [16] to select the candidates in ¬∑ Coarse-grained extension : Specifically, given the anchor sample ùë• ùëò ùëñ from scenario S ùëò , the sample from other scenarios with the same label as ùë• ùëò ùëñ is selected as positive sample ùë• ùëò ‚Ä≤ ùëñ + , and sample from other scenarios with an opposite label is selected as negative sample ùë• ùëò ‚Ä≤ ùëñ -. By pulling the shared hidden representations of ùë• ùëò ùëñ and ùë• ùëò ‚Ä≤ ùëñ + closer and pushing the shared hidden representations of ùë• ùëò ùëñ and ùë• ùëò ‚Ä≤ ùëñ -away, the generalized contrastive loss encourages the shared component to better capture the common knowledge belonging to the same label between multiple scenarios. Conference'17, July 2017, Washington, DC, USA Mu, et al. an offline way, and adopt the memory bank contrastive sampling strategy [13] for enhancing the sample diversity. Diffusion Noise Enhanced Negative Samples . In the above, the samples in the generalized contrastive loss are limited to locally observed samples from the training set. Due to the anisotropy problem [7] in contrastive learning, the representations of sampled negatives are from the narrow hidden representation cone spanned by the shared component, which cannot fully reflect the overall semantics of the hidden representation space. Considering this issue, we are inspired by the recent progress in the diffusion model [14], and propose to incorporate diffusion noise to generate more generalized, diverse negative samples. Specifically, given the negative sample ùíõ ùëñ -, we follow a standard forward diffusion process by iteratively adding small Gaussian noise to the sample, producing a sequence of noisy samples ùíõ ( 1 ) ùëñ -, ùíõ ( 2 ) ùëñ -, ¬∑ ¬∑ ¬∑ , ùíõ ( ùëá ) ùëñ -in ùëá steps. The step sizes are controlled by a variance schedule { ùõΩ ùë° ‚àà ( 0 , 1 )} ùëá ùë° = 1 , and the forward diffusion process can be formulated as following:  where N(¬∑) is the Gaussion distribution with ùúá = ‚àöÔ∏Å 1 -ùõΩ ùë° ùíõ ( ùë° -1 ) ùëñ -and ùúé 2 = ùõΩ ùë° . The noised negative sample at step ùë° only depends on ùíõ ( ùë° -1 ) ùëñ -and ùõΩ ùë° , so the whole forward diffusion process can be viewed as a Markov chain. Further, it has been shown that the sample at the ùë° -th diffusion step can be obtained in a simple analytic form [14]:  where ùíõ ùëñ -is the original sample, ùíé is a Gaussian distribution with ùúá = 0 and ùúé 2 = I , and ùõº ùë° = 1 -ùõΩ ùë° and ùõº ùë° = Àõ ùë° ùëñ = 1 ùõº ùëñ . With this formula, we can directly obtain the sample at an arbitrary time step ùë° . To generate new negative samples, we first sample different time steps ( ùë° ), and then employ Eq. (6) to obtain the corresponding samples. These new samples are more diverse and can span a broader space, thus improving the generalizability of the underlying model. Note that we do not apply the similar methods to positive samples, since the diffusion process will largely change the original sample, making it dissimilar to the anchor sample. 3.2.3 Reciprocal Similarity Weighting. In contrastive learning, the constructed contrastive samples are likely to be false positive or negative. In our setting, though two positive samples from different scenarios have the same label, their feature characteristics might be highly different, which would lead to the false positive issue. Similarly, false negative might appear when a sample considered as ' negative ' has a larger similarity to the anchor sample. To alleviate this issue, we design an adaptive weighting method for contrastive pairs based on their reciprocal similarity. The basic idea is that a positive contrastive pair should have a large similarity, while a negative contrastive pair should have a small similarity. If the cases become opposite, the corresponding pairs are likely to be false positive/negative pairs. Based on this idea, for a pair of contrastive samples ùë• ùëñ and ùë• ùëó , we calculate their reciprocal similarity score and employ it to set the contrastive weights ùë† ùëñ,ùëó in Eq. (4):  where ùíÜ ùëñ and ùíÜ ùëó are the feature embedding vectors for samples ùë• ùëñ and ùë• ùëó , consisting of all the context features. Though simple, this weighting strategy can adaptively assign specific weight to each individual contrastive pair, and these weights are also dynamically updated in the training process. As we can see from the loss in Eq. (4), when the false positive sample appears, the reciprocal similarity score ùë† ùëñ,ùëñ + would be a much larger value than it should be, such a score will reduce its effect when minimizing the generalized contrastive loss. Similarly, when the false negative sample appears, the reciprocal similarity score ùë† ùëñ,ùëñ -would be a much smaller value, so the contrastive pair will also contribute less in the optimization objective.",
  "3.3 Individual Contrastive Loss for Capturing Scenario-Specific Knowledge": "Besides learning common knowledge across scenarios, it is also important to capture scenario-specific knowledge for improving the performance of ad ranking at each individual scenario. Existing methods usually directly optimize the scenario-specific component based on the feedback data from corresponding scenario. By leveraging across-scenario information, we propose the individual contrastive loss for improving the learning of scenario-specific component. 3.3.1 Scenario-aware Contrastive Samples. Theindividual contrastive loss aims to enhance the capacity of scenario-specific component to model the characteristics of the corresponding scenario data, so as to better capture the scenario-specific knowledge. To achieve this, we contrast the hidden representations ( i.e., ùíâ ùëò ùëñ in Eq. (3)) generated by different scenario-specific components to reduce the relatedness between different scenarios. Positive Contrastive Samples. In order to better capture scenariospecific knowledge, we consider augmenting the original samples by constructing the positive contrastive sample Àú ùíâ ùëò ùëñ . In contrastive learning, data augmentation [4, 10, 33] has been widely used to generate positive samples, which also applies to advertisement ranking [37]. While, the method in [37] relies on feature correlations to generate high-quality positive contrastive samples. Inspired by the recent progress in natural language processing [9] and sequential recommendation [24], we propose to add the dropout noise to conduct data augmentation. Specifically, we insert a dropout module in each layer of the scenario-specific component to generate the positive contrastive sample Àú ùíâ ùëò ùëñ . The dropout strategy in the continuous representation space will generate the augmented vectors ( Àú ùíâ ùëò ùëñ ), which are different from ùíâ ùëò ùëñ but semantically similar. Since data augmentation is applied to the positive samples, it can enhance the modeling of the scenario-specific knowledge. Negative Contrastive Samples. For the negative samples, we first select the samples from other scenarios, and then obtain their corresponding scenario-specific representations ùíâ ùëò ‚Ä≤ ùëñ -, where ùëò ‚Ä≤ ‚â† ùëò . By pushing ùíâ ùëò ùëñ and ùíâ ùëò ‚Ä≤ ùëñ -away, we aim to enlarge the difference between scenario S ùëò and scenario S ùëò ‚Ä≤ , so that the scenario-specific component can focus on capturing scenario-specific knowledge. Besides, in order to conduct more informative contrastive learning, we further propose a cross-scenario encoding strategy to generate Hybrid Contrastive Constraints for Multi-Scenario Ad Ranking Conference'17, July 2017, Washington, DC, USA Table 1: Comparison between the existing representative multi-scenario ad ranking methods and HC 2 . \" indicates the method improves the multi-scenario ad ranking from the corresponding view and % indicates not. hard negative samples in the individual contrastive loss. The basic idea is that we first sample a within-scenario negative ùë• ùëò ùëñ -for the anchor sample ùë• ùëò ùëñ , and then feed its shared representation ùíõ ùëò ùëñ -to into the scenario-specific encoder of another scenario. In this way, the hard negative contrastive sample can be generated as follows:  where ùëò and ùëò ‚Ä≤ denote two different scenarios and it is why we call it cross-scenario encoding . Finally, we have the following individual contrastive loss:  where ùíâ ùëò ùëñ and Àú ùíâ ùëò ùëñ denote the original and augmented representations, respectively, and ùíâ ùëò ‚Ä≤ ùëñ -and ùíâ ( ùëò ‚Ä≤ ) ùëñ -denotes the negative representations obtained by sampling from other scenarios and crossscenario encoding (Eq. (8)), respectively. Note that here we do not use the reciprocal similarity weighting mechanism (Section 3.2.3), since such a construction way (data augmentation and cross-scenario encoding) is less likely to produce false samples.",
  "3.4 Model Optimization and Discussion": "In this section, we first introduce how to optimize our approach, and then make some discussions about our approach. 3.4.1 Model Optimization. In this part, we introduce how to combine the generalized contrastive loss and individual contrastive loss to optimize the model. We jointly optimize the main loss (Eq. (1)), generalized contrastive loss (Eq. (4)) and individual contrastive loss (Eq. (9)) in an end-to-end manner:  where ùúÜ 1 and ùúÜ 2 are the balancing coefficients, which control the influence of generalized contrastive loss and individual contrastive loss, respectively. Note that we freeze the gradients from this weight term during training, since it only involves feature embeddings, without containing other parameters. 3.4.2 Discussion. In this part, we further discuss our approach in three aspects, namely the application to other backbones, the differences with existing methods, and complexity analysis. Applications to Other Backbones. The proposed approach can be generally applied to various shared-specific architectures . Existing multi-scenario ad ranking methods with shared-specific architecture can be divided into two major categories. (2) Models with the generated parameters [26, 38]: a shared component is used to dynamically generate scenario-specific parameters. For this kind of method, the application of individual contrastive loss to scenario-specific component is same as the static models. The shared component is usually used to generate scenario-specific parameters, while we can feed the feature embedding to the shared component for generating the hidden representations and then apply the generalized contrastive loss to it. In Section 4.4.1, the experimental results also demonstrate that the proposed approach can consistently improve the performance of these backbones. (1) Models with the static parameters [3, 17, 20, 28]: the shared and scenario-specific components are both with static parameters. For this kind of method, the two contrastive losses in our approach can be directly applied to the hidden representations generated by the corresponding components (same as the SharedBottom [3]). Differences with Previous Work. For multi-scenario ad ranking, existing methods [3, 5, 17, 20, 25, 26, 42] mainly focus on designing various scenario-shared and scenario-specific network structures to capture the commonalities and differences among multiple scenarios. Different from them, our approach focuses on the optimization objective of the two components. For this, we elaborate the novel self-supervised generalized contrastive loss and individual contrastive loss borrowing the idea of contrastive learning [4]. Based on the shared-specific architecture, the two contrastive losses help the shared component and scenario-specific component better capture the common and scenario-specific knowledge. From the data view, existing methods only leverage the scenario-specific features, and treat the samples from various scenarios independently which omits the relationships among multiple scenarios. In contrast, our approach constructs label-aware and scenario-aware contrastive samples across the different scenarios to effectively model the data interrelation among multiple scenarios. The comparison of these approaches is presented in Table 1. Complexity Analysis. Our approach proposes the generalized contrastive loss and individual contrastive loss to improve the multiscenario ad ranking, which adopts the same architecture as previous studies [3, 17, 20, 26, 38], thus having similar costs in the backbone. Given a batch of training data, the time complexity of the backbone in the training stage can be roughly estimated as O( ùêµ ¬∑ ùëë ¬∑ Àú ùëë ) , where ùêµ is the batch size, ùëë is the feature embedding size, and Àú ùëë is the dimension of the hidden representations. The extra time cost of our approach in the training stage is from the two contrastive losses. It can be roughly estimated as O( ùêµ ¬∑ Àú ùëë 2 + ùêµ ¬∑ Àú ùëë ) . In our settings, we set Àú ùëë ‚â™ ùëë (Section 4.1.3), so the two contrastive losses increase small time cost in the training stage. As for the inference stage, which actually influences the online serving, there is no extra time cost compared with other multi-scenario ad ranking methods. It is also verified by the online experiments conducted in Section 4.5: our approach does not incur increased response time (RT) for online serving. For space complexity, our approach does not introduce any additional parameters. The amount of parameters only depends on Conference'17, July 2017, Washington, DC, USA Mu, et al. the corresponding backbone model. In a word, the proposed HC 2 is an efficient and effective contrastive learning approach to improve multi-scenario ad ranking.",
  "4 EXPERIMENTS": "",
  "4.1 Experimental Setup": "We first describe the experimental setup, including evaluation datasets, comparison methods, implementation details and evaluation settings. 4.1.1 Datasets. To evaluate the effectiveness of the proposed approach, we use a public multi-scenario dataset AliExpress [17], and a large-scale production dataset Ali-ads from a leading advertising platform. The public dataset AliExpress is collected from the AliExpress search system, which contains users' click and purchase behaviors from five different countries. The five countries are Netherlands (NL), French (FR), America (US), Spain (ES) and Russia (RU), respectively. Following the previous work [17], we regard each country as an advertising scenario. The production dataset is collected from the Alibaba advertising platform, named Ali-ads dataset. Specifically, we collect 2-week consecutive user feedback logs from five different marketing scenarios: S1, S2, S3, S4 and S5. For the public dataset, we follow the original dataset [17] to split the training set and test set. For the production dataset, we keep the last day's data as the test set, and the remains are training set. The statistics of the datasets are summarized in Table 2. Table 2: Statistics of the datasets. 4.1.2 Comparison Methods. In order to verify the effectiveness of the proposed approach, we mainly consider the following methods: ¬∑ CFM [37]: It proposes a contrastive learning framework to improve single scenario item recommendations. ¬∑ BaseDNN : It adopts the standard DNN structure to train the data from the single scenario for advertisement ranking. ¬∑ MultiDNN : It adopts the standard DNN structure to train the data from multi scenarios for advertisement ranking. ¬∑ MMoE [20]: It designs the shared Multi-gate Mixture-of-Experts (MoE) structure to model task relationships and capture common representations from multi scenarios. ¬∑ SharedBottom [3]: It adopts the multi-task learning framework that shares the parameters of the bottom layer and designs scenario-specific parameters for each scenario. ¬∑ HMoE [17]: It proposes the hybrid of implicit and explicit Mixture-of-Experts approach which learns the scenario relationships from the feature space implicitly and the label space explicitly. ¬∑ M2M [38]: It uses a shared network to learn the commonalities between different scenarios, then designs a meta unit to generate specific parameters to capture scenario-specific knowledge. ¬∑ STAR [26]: It shares a centered network for all the scenarios and generates scenario-specific network for each scenario. ¬∑ FOREC [1]: It's a meta-learning based method. It first treats each scenario as a meta-task to pre-train the model parameters, and then fine-tune it on the specific scenario data. Among them, BaseDNN and CFM are single-scenario ad ranking methods, and others are multi-scenario ad ranking methods. Unless otherwise specified, our approach uses the SharedBottom as the backbone to compare with other baselines. 4.1.3 Implementation Details. To ensure a fair comparison, we optimize all the methods (baselines and the proposed approach) with Adam optimizer, where the learning rate is set to be 0.001, and the batch size is set to be 2048. The feature embedding layer is same for all the methods, with embedding size 632 (public AliExpress dataset) and 4424 (production Ads dataset). The default Xavier initializer is used to initialize the model parameters. For the baseline methods, we carefully tune their hyperparameters. 4.1.4 Evaluation Settings. To evaluate the performance, we adopt the CTR and CTCVR prediction tasks, which are classical advertisement ranking tasks [6, 21]. For the CTR prediction, we treat the click behaviors as positive samples. For the CTCVR prediction, we treat the conversion behaviors as positive samples. For the evaluation metrics, we adopt the area under the ROC curve (AUC) metric, which reflects the model's ranking ability on candidates, and is widely used in CTR and CTCVR prediction task [6, 21].",
  "4.2 Performance Comparison": "We compare the proposed approach with the aforementioned baselines on the two adopted datasets, and the results are summarized in Table 3. From it, we have the following observations: Compared with MultiDNN, other multi-scenario ad ranking methods ( e.g., SharedBottom, HMoE and STAR) generally achieve better performance. Because the designed scenario-shared components and scenario-specific components in these methods can better capture commonalities and characteristics between different scenarios. These results show the importance of fully using the commonalities between different scenarios and avoiding the negative impact of data conflict issues between different scenarios. Compared with single-scenario ad ranking methods BaseDNN and CFM, multi-scenario ad ranking methods achieve better results on most scenarios. It is because multi-scenario ad ranking methods can leverage richer information from multi-scenario data than only single scenario, so as to make more accurate ranking results. This demonstrates the effectiveness of utilizing multiple scenario datasets to improve the advertisement ranking performance. Finally, by comparing the proposed approach HC 2 with all the baselines, it is clear that HC 2 consistently performs better than them by a large margin on both CTR and CTCVR tasks. Besides, our approach performs much better than the baselines on the relatively Hybrid Contrastive Constraints for Multi-Scenario Ad Ranking Conference'17, July 2017, Washington, DC, USA Table 3: Performance comparison of different methods on the two datasets. The best performance and the runner-up performance are denoted in bold and underlined fonts respectively. sparse scenarios, e.g., NL and US on the AliExpress dataset, S4 and S5 on the Ali-ads dataset. It is because the proposed approach can better model the data interrelation among multiple scenarios, and the two elaborated contrastive losses can effectively help the model capture the common and scenario-specific behavior knowledge from multi-scenario dataset. Table 4: Ablation study of the proposed approach on the AliExpress dataset (AUCCTR).",
  "4.3 Ablation Study": "In this part, we continue to analyze how each of the proposed techniques or components affects the final performance. We prepare four variants of the proposed approach HC 2 for comparisons, including (1) ùë§ / ùëú g-loss without the generalized contrastive loss (Eq. (4)), (2) ùë§ / ùëú noise without the diffusion noise in generalized contrastive loss (Eq. (6)), (3) ùë§ / ùëú weight without the reciprocal similarity weighting in generalized contrastive loss (Eq. (7)), (4) ùë§ / ùëú s-loss without the individual contrastive loss (Eq. (9)). Besides, we add the backbone model SharedBottom for reference. The experimental results of the proposed approach HC 2 and its variants are reported in Table 4. From it, we can observe that removing each of the components leads to the performance decrease while the four variants all perform better than the backbone model SharedBottom. The performance of the variant ùë§ / ùëú g-loss is much worse than the variant ùë§ / ùëú s-loss, showing the generalized contrastive loss is more important than the individual contrastive loss, and the contrastive constraints for common behavior knowledge can bring more performance improvement. The result of variant ùë§ / ùëú noise and ùë§ / ùëú weight show they can further improve the performance by alleviating false positive/negative samples issue and enhancing informativeness of negative samples. All these results show that the designed components in HC 2 are essential for improving the multi-scenario ad ranking performance.",
  "4.4 Further Analysis": "In this section, we further perform a series of detailed analyses on the proposed HC 2 to confirm its effectiveness. 4.4.1 Applying HC 2 on Other Backbones. As mentioned above, our approach HC 2 can generally be applied to other multi-scenario ad ranking methods. Thus, in this part, we conduct an experiment to examine whether our method can bring improvements to other backbones. Specifically, we apply our approach on the multi-scenario ad ranking methods MMoE [20], HMoE [17], STAR [26] and M2M [38]. The results are reported in Figure 2. From this figure, we can observe that the proposed approach can consistently improve the performance of these backbones, which further verifies the effectiveness of the proposed method. Besides, the improvement on STAR and M2M is not as remarkable as the improvement on MMoE Conference'17, July 2017, Washington, DC, USA Mu, et al. Figure 2: Performance (AUCCTR) comparison of different backbone models enhanced by our approach on the AliExpress datasets. MMoE HMoE STAR M2M 0.720 0.725 0.730 0.735 AUC-CTR Original +Our (a) AliExpress-NL MMoE HMoE STAR M2M 0.720 0.725 0.730 0.735 0.740 0.745 AUC-CTR Original +Our (b) AliExpress-ES Figure 3: Performance comparison w.r.t. the amount of training data on the AliExpress dataset. 100% 80% 60% 40% 20% 0.72 0.73 0.74 AUC-CTR SharedBottom Our (a) AliExpress-NL 100% 80% 60% 40% 20% 0.720 0.725 0.730 0.735 0.740 0.745 AUC-CTR SharedBottom Our (b) AliExpress-ES and HMoE. A possible reason is that STAR and M2M are generative parameters methods, the shared component is used to generate scenario-specific parameters. So the generalized contrastive loss contributes less to these backbones than other methods. But it can still bring performance improvements. 4.4.2 Performance Comparison w.r.t. the Amount of Training Data. Conventional advertisement ranking methods require a considerable amount of training data, thus they are likely to suffer from the data sparsity in real-world applications. In this section, we examine how the proposed approach performs w.r.t. the amount of training data. For this purpose, we use different proportions of the full dataset as the training data, i.e., 100%, 80%, 60%, 40% and 20%. Then we compare the performance of HC 2 and SharedBottom trained on these five groups, and report the results in Figure 3. From this figure, we can find that the performance of HC 2 is consistently better than SharedBottom. Meanwhile, as the amount of training data decreases, the performance gain brought by HC 2 increases. This is because the proposed approach can leverage the instance-level relationship between multiple scenarios to fully use the multi-scenario data. Even if the training data is limited, it can still perform well. 4.4.3 Impact of the Contrastive Loss Weights ùúÜ 1 and ùúÜ 2 . In Eq. (10), weincorporate two coefficients ùúÜ 1 and ùúÜ 2 to control the combination weights of the two contrastive losses. The coefficient ùúÜ 1 controls the impact of generalized contrastive loss for common knowledge. The coefficient ùúÜ 2 controls the impact of individual contrastive loss for scenario-specific knowledge. Here, we examine how ùúÜ 1 and ùúÜ 2 affect the final ranking performance. At each time, we fix one coefficient as its optimal setting and tune the other coefficient. Figure 4(b) presents the results of varying the weight ùúÜ 1. As ùúÜ 1 increases within a certain range, the performance improves first. 0 1e-4 5e-4 0.001 0.005 0.01 0.05 0.1 0.5 1.0 0.720 0.725 0.730 0.735 0.740 0.745 AUC-CTR BaseDNN SharedBottom Our (a) ùúÜ 1 on the AliExpress dataset. 0 1e-4 5e-4 0.001 0.005 0.01 0.05 0.1 0.5 1.0 0.720 0.725 0.730 0.735 0.740 0.745 AUC-CTR BaseDNN SharedBottom Our (b) ùúÜ 2 on the AliExpress dataset. Figure 4: Impact of the contrastive loss weights ùúÜ 1 and ùúÜ 2 on the AliExpress dataset. (a) SharedBottom Figure 5: Visualization of the hidden representations from the shared component on the AliExpress dataset. (b) HC 2 This is because the generalized contrastive loss can effectively leverage the label information between samples in different scenarios to capture common behavior knowledge and further improve the performance. As ùúÜ 1 continues to increase, the performance starts to decrease. That is because a too large value for ùúÜ 1 can dominate the overall loss. The results of varying the weight ùúÜ 2 are shown in Figure 4(a). Adding the individual contrastive loss brings the better results, and the individual contrastive loss is more robust to ùúÜ 2. 4.4.4 Visualizing the Distribution of Representations. A key contribution of the proposed approach HC 2 is to integrate the generalized contrastive loss in the multi-scenario ad ranking methods. And it brings a big performance improvement which has been shown in Section 4.3. To better understand the benefits brought by the generalized contrastive loss, we visualize the learned hidden representations of the shared component in Figure 5 to show how the proposed approach affects the shared component. Specifically, we use T-SNE [29] to reduce the dimension of the hidden representations to the two-dimensional space and then plot them. From this figure, we can observe that the representations learned by SharedBottom fall into a narrow space, while the representations learned by HC 2 clearly exhibit a more uniform distribution. This can be explained by the concept of uniformity in representation learning, which has been widely studied in previous works [31]. The uniformity reflects the ability of representations to preserve information. If the representations are roughly uniformly distributed on the unit hypersphere, they can preserve as much information of the data as possible. We speculate that the shared hidden representations learned by our approach HC 2 can preserve more common Hybrid Contrastive Constraints for Multi-Scenario Ad Ranking Conference'17, July 2017, Washington, DC, USA knowledge from the multiple scenarios. This demonstrates the proposed generalized contrastive loss can help the shared component capture common knowledge effectively. Table 5: Results of online A/B tests on Alibaba advertising platform (relative improvements).",
  "4.5 Online Experiments": "To further verify the effectiveness of HC 2 , we deploy it on the function of search engine marketing on the Alibaba advertising platform for online A/B test. The search engine marketing aims to advertise for different mobile apps and search engines. We regard the different mobile apps and search engines as different advertising scenarios. To make a fair comparison, we follow the same configuration with the best CTR and CVR model deployed online, such as feature set and model size. The core online metrics include CTR (click-through rate, i.e., # ùëêùëôùëñùëêùëò # ùëñùëöùëùùëüùëíùë†ùë†ùëñùëúùëõ ), CVR (conversion rate, i.e., # ùëêùëúùëõùë£ùëíùëüùë†ùëñùëúùëõ # ùëñùëöùëùùëüùëíùë†ùë†ùëñùëúùëõ ) and GMV (gross merchandise volume). Table 5 reports the relative improvements in the different scenarios and the overall performance. From it, we can have the following observations. Firstly, for the overall performance, HC 2 achieves + 2 . 18 % lift on CTR, + 2 . 51 % lift on CVR, and + 3 . 72 % lift on GMV, showing HC 2 improves the important online metrics and promotes the performance of advertising system. Secondly, HC 2 performs much better than the base model on the relatively sparse scenario ( e.g., S3, S4 and S5). Because the proposed approach can better leverage the data interrelation among multiple scenarios. Besides, we also record the metric of RT (response time), which reflects the efficiency of the online serving. The average RT of our approach is 11 . 7 ms, which equals to the base model, showing the proposed approach is an efficient contrastive learning approach to improve multi-scenario ad ranking.",
  "5 RELATED WORK": "In this section, we review the related works in two aspects, namely multi-scenario advertisement ranking and contrastive learning. Multi-Scenario Advertisement Ranking. Multi-scenario advertisement ranking utilizes the data from multi-scenarios to improve the advertisement ranking, which aims to achieve better performance than single-scenario modeling. Most existing works [5, 17, 20, 22, 25, 26, 28, 39] usually adopt the multi-task learning (MTL) framework to learn from the multi-scenario data together. They design scenario-shared structure to learn the common knowledge between different scenarios and scenario-specific structure to learn the scenario-specific knowledge. For instance, MMoE [20] adopts the Mixture-of-Experts (MoE) structure to capture the relationships between different scenarios. HMoE [17] further exploits task relationships in the label space based on the MoE structure. Recently, some works [26, 35, 38, 41] propose to use a shared auxiliary network to dynamically generate scenario-specific parameters. Besides, some works [1, 11, 19, 36] adopt the pre-training or meta-learning strategy to train a shared model for all scenarios and then use the shared model to generate scenario-specific model for each scenario. Different from these works, our work mainly focuses on helping the designed scenario-shared and scenario-specific components better learn the commonalities and differences between different scenarios via contrastive learning. Indeed, our approach can be considered as a general improvement way to enhance existing methods. Contrastive Learning. Contrastive learning has achieved great success in the field of computer vision (CV) [4], natural language processing (NLP) [10], and information retrieval (IR) [33, 40]. It aims to learn quality discriminative representations by contrasting positive and negative samples from different views. In the field of information retrieval, contrastive learning is usually applied to enhancing the task-specific representations and alleviating data sparsity issues for single-scenario recommendation task [18, 33, 37, 40]. For example, S 3 -Rec [40] and DuoRec [24] propose to utilize contrastive learning to enhance the sequence representations for sequential recommendation task. SGL [33] and NCL [18] propose to utilize contrastive learning to enhance the graph representations for graph-based collaborative filtering. CFM [37] propose to improve two tower model with contrastive learning for large-scale item recommendations. In addition, several attempts apply contrastive learning on cross-domain recommendation task [30, 34]. These works leverage contrastive learning to transfer user preference from the source domain to the target domain. Different from these, our approach design two different contrastive loss to help the model better learn the common and scenario-specific knowledge for multiscenario advertisement ranking.",
  "6 CONCLUSION": "In this paper, we propose a novel hybrid contrastive constrained approach (HC 2 ) for multi-scenario ad ranking. Not only relying on the task-specific loss, we utilize contrastive learning to model the across-scenario relation. The major technical contributions lie in the two elaborated contrastive losses, namely generalized and individual contrastive losses, which aims at capturing common knowledge and scenario-specific knowledge. To adapt contrastive learning to the multi-scenario setting, we design a series of specific strategies in improving both contrastive samples and contrastive weights. By effectively modeling the data interrelation across scenarios, our approach can be generally applied to various multi-scenario ad ranking tasks. Extensive experiments on both offline evaluation and online test have demonstrated the effectiveness. Currently, our approach HC 2 is only designed for multi-scenario ad ranking. Indeed, our approach in essence can be generally extended to more recommendation and advertising tasks, such as multi-domain item recommendation, multi-task ad ranking. As future work, we will consider adapting our approach to these fields. Conference'17, July 2017, Washington, DC, USA Mu, et al.",
  "REFERENCES": "[1] Hamed R. Bonab, Mohammad Aliannejadi, Ali Vardasbi, Evangelos Kanoulas, and James Allan. 2021. Cross-Market Product Recommendation. In CIKM . 110-119. [3] Rich Caruana. 1998. Multitask Learning. In Learning to Learn . 95-133. [2] L√©on Bottou, Jonas Peters, Joaquin Qui√±onero Candela, Denis Xavier Charles, Max Chickering, Elon Portugaly, Dipankar Ray, Patrice Y. Simard, and Ed Snelson. 2013. Counterfactual reasoning and learning systems: the example of computational advertising. JMLR 14, 1 (2013), 3207-3260. [4] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. 2020. A Simple Framework for Contrastive Learning of Visual Representations. In ICML , Vol. 119. 1597-1607. [6] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and Hemal Shah. 2016. Wide & Deep Learning for Recommender Systems. In RecSys . 7-10. [5] Yuting Chen, Yanshi Wang, Yabo Ni, Anxiang Zeng, and Lanfen Lin. 2020. Scenario-aware and Mutual-based approach for Multi-scenario Recommendation in E-Commerce. In ICDM workshop . 127-135. [7] Kawin Ethayarajh. 2019. How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings. In EMNLP , Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (Eds.). 55-65. [9] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple Contrastive Learning of Sentence Embeddings. In EMNLP . 6894-6910. [8] Jun Feng, Heng Li, Minlie Huang, Shichen Liu, Wenwu Ou, Zhirong Wang, and Xiaoyan Zhu. 2018. Learning to Collaborate: Multi-Scenario Ranking via MultiAgent Reinforcement Learning. In WWW . 1939-1948. [10] John M. Giorgi, Osvald Nitski, Bo Wang, and Gary D. Bader. 2021. DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations. In ACL . 879-895. [12] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: A Factorization-Machine based Neural Network for CTR Prediction. In IJCAI . 1725-1731. [11] Yulong Gu, Wentian Bao, Dan Ou, Xiang Li, Baoliang Cui, Biyu Ma, Haikuan Huang, Qingwen Liu, and Xiaoyi Zeng. 2021. Self-Supervised Learning on Users' Spontaneous Behaviors for Multi-Scenario Ranking in E-commerce. In CIKM . 3828-3837. [13] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. 2020. Momentum Contrast for Unsupervised Visual Representation Learning. In CVPR . 9726-9735. [15] Yuchen Jiang, Qi Li, Han Zhu, Jinbei Yu, Jin Li, Ziru Xu, Huihui Dong, and Bo Zheng. 2022. Adaptive Domain Interest Network for Multi-domain Recommendation. In CIKM . 3212-3221. [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising Diffusion Probabilistic Models. In NeurIPS . [16] Jeff Johnson, Matthijs Douze, and Herv√© J√©gou. 2019. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data 7, 3 (2019), 535-547. [18] Zihan Lin, Changxin Tian, Yupeng Hou, and Wayne Xin Zhao. 2022. Improving Graph Collaborative Filtering with Neighborhood-enriched Contrastive Learning. In WWW . 2320-2329. [17] Pengcheng Li, Runze Li, Qing Da, Anxiang Zeng, and Lijun Zhang. 2020. Improving Multi-Scenario Learning to Rank in E-commerce by Exploiting Task Relationships in the Label Space. In CIKM . 2605-2612. [19] Linhao Luo, Yumeng Li, Buyu Gao, Shuai Tang, Sinan Wang, Jiancheng Li, Tanchao Zhu, Jiancai Liu, Zhao Li, Binqiang Zhao, Ziyang Zheng, and Shirui Pan. 2022. MAMDR: A Model Agnostic Learning Method for Multi-Domain Recommendation. CoRR abs/2202.12524 (2022). [21] Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu, and Kun Gai. 2018. Entire Space Multi-Task Model: An Effective Approach for Estimating Post-Click Conversion Rate. In SIGIR . 1137-1140. [20] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H. Chi. 2018. Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-ofExperts. In KDD . 1930-1939. [22] Xichuan Niu, Bofang Li, Chenliang Li, Jun Tan, Rong Xiao, and Hongbo Deng. 2021. Heterogeneous Graph Augmented Multi-Scenario Sharing Recommendation with Tree-Guided Expert Networks. In WSDM . 1038-1046. [24] Ruihong Qiu, Zi Huang, Hongzhi Yin, and Zijian Wang. 2022. Contrastive Learning for Representation Degeneration Problem in Sequential Recommendation. In WSDM . 813-823. [23] Louisa Ha Ph.D. 2008. Online Advertising Research in Advertising Journals: A Review. Journal of Current Issues & Research in Advertising 30, 1 (2008), 31-48. [25] Qijie Shen, Wanjie Tao, Jing Zhang, Hong Wen, Zulong Chen, and Quan Lu. 2021. SAR-Net: A Scenario-Aware Ranking Network for Personalized Fair Recommendation in Hundreds of Travel Scenarios. In CIKM . 4094-4103. [27] Shulong Tan, Meifang Li, Weijie Zhao, Yandan Zheng, Xin Pei, and Ping Li. 2021. Multi-Task and Multi-Scene Unified Ranking Model for Online Advertising. In Big Data . 2046-2051. [26] Xiang-Rong Sheng, Liqin Zhao, Guorui Zhou, Xinyao Ding, Binding Dai, Qiang Luo, Siran Yang, Jingshan Lv, Chi Zhang, Hongbo Deng, and Xiaoqiang Zhu. 2021. One Model to Serve All: Star Topology Adaptive Recommender for Multi-Domain CTR Prediction. In CIKM . 4104-4113. [28] Hongyan Tang, Junning Liu, Ming Zhao, and Xudong Gong. 2020. Progressive Layered Extraction (PLE): A Novel Multi-Task Learning (MTL) Model for Personalized Recommendations. In RecSys . 269-278. [30] Chen Wang, Yueqing Liang, Zhiwei Liu, Tao Zhang, and Philip S. Yu. 2021. Pretraining Graph Neural Network for Cross Domain Recommendation. In CogMI . 140-145. [29] Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing Data using t-SNE. Journal of Machine Learning Research (2008), 2579-2605. [31] Tongzhou Wang and Phillip Isola. 2020. Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere. In ICML , Vol. 119. 9929-9939. [33] Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian, and Xing Xie. 2021. Self-supervised Graph Learning for Recommendation. In SIGIR . 726-735. [32] Yichao Wang, Huifeng Guo, Bo Chen, Weiwen Liu, Zhirong Liu, Qi Zhang, Zhicheng He, Hongkun Zheng, Weiwei Yao, Muyu Zhang, Zhenhua Dong, and Ruiming Tang. 2022. CausalInt: Causal Inspired Intervention for Multi-Scenario Recommendation. In KDD . 4090-4099. [34] Ruobing Xie, Qi Liu, Liangdong Wang, Shukai Liu, Bo Zhang, and Leyu Lin. 2021. Contrastive Cross-domain Recommendation in Matching. (2021). arXiv:2112.00999 https://arxiv.org/abs/2112.00999 [36] Xuanhua Yang, Xiaoyu Peng, Penghui Wei, Shaoguo Liu, Liang Wang, and Bo Zheng. 2022. AdaSparse: Learning Adaptively Sparse Structures for Multi-Domain Click-Through Rate Prediction. In CIKM . 4635-4639. [35] Bencheng Yan, Pengjie Wang, Kai Zhang, Feng Li, Jian Xu, and Bo Zheng. 2022. APG: Adaptive Parameter Generation Network for Click-Through Rate Prediction. CoRR abs/2203.16218 (2022). [37] Tiansheng Yao, Xinyang Yi, Derek Zhiyuan Cheng, Felix X. Yu, Ting Chen, Aditya Krishna Menon, Lichan Hong, Ed H. Chi, Steve Tjoa, Jieqi (Jay) Kang, and Evan Ettinger. 2021. Self-supervised Learning for Large-scale Item Recommendations. In CIKM . 4321-4330. [39] Yuanliang Zhang, Xiaofeng Wang, Jinxin Hu, Ke Gao, Chenyi Lei, and Fei Fang. 2022. Scenario-Adaptive and Self-Supervised Model for Multi-Scenario Personalized Recommendation. In CIKM . 3674-3683. [38] Qianqian Zhang, Xinru Liao, Quan Liu, Jian Xu, and Bo Zheng. 2022. Leaving No One Behind: A Multi-Scenario Multi-Task Meta Learning Approach for Advertiser Modeling. In WSDM . 1368-1376. [40] Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, and Ji-Rong Wen. 2020. S3-Rec: Self-Supervised Learning for Sequential Recommendation with Mutual Information Maximization. In CIKM . 1893-1902. [42] Xinyu Zou, Zhi Hu, Yiming Zhao, Xuchu Ding, Zhongyi Liu, Chenliang Li, and Aixin Sun. 2022. Automatic Expert Selection for Multi-Scenario and Multi-Task Search. In SIGIR . 1535-1544. [41] Yongchun Zhu, Zhenwei Tang, Yudan Liu, Fuzhen Zhuang, Ruobing Xie, Xu Zhang, Leyu Lin, and Qing He. 2022. Personalized Transfer of User Preferences for Cross-domain Recommendation. In WSDM . 1507-1515.",
  "keywords_parsed": [
    "Online Advertising",
    "Advertisement Ranking",
    "Multi-Scenario Ad Ranking",
    "Contrastive Learning"
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "Cross-Market Product Recommendation"
    },
    {
      "ref_id": "b3",
      "title": "Multitask Learning"
    },
    {
      "ref_id": "b2",
      "title": "Counterfactual reasoning and learning systems: the example of computational advertising"
    },
    {
      "ref_id": "b4",
      "title": "A Simple Framework for Contrastive Learning of Visual Representations"
    },
    {
      "ref_id": "b6",
      "title": "Wide & Deep Learning for Recommender Systems"
    },
    {
      "ref_id": "b5",
      "title": "Scenario-aware and Mutual-based approach for Multi-scenario Recommendation in E-Commerce"
    },
    {
      "ref_id": "b7",
      "title": "How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings"
    },
    {
      "ref_id": "b9",
      "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings"
    },
    {
      "ref_id": "b8",
      "title": "Learning to Collaborate: Multi-Scenario Ranking via MultiAgent Reinforcement Learning"
    },
    {
      "ref_id": "b10",
      "title": "DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations"
    },
    {
      "ref_id": "b12",
      "title": "DeepFM: A Factorization-Machine based Neural Network for CTR Prediction"
    },
    {
      "ref_id": "b11",
      "title": "Self-Supervised Learning on Users' Spontaneous Behaviors for Multi-Scenario Ranking in E-commerce"
    },
    {
      "ref_id": "b13",
      "title": "Momentum Contrast for Unsupervised Visual Representation Learning"
    },
    {
      "ref_id": "b15",
      "title": "Adaptive Domain Interest Network for Multi-domain Recommendation"
    },
    {
      "ref_id": "b14",
      "title": "Denoising Diffusion Probabilistic Models"
    },
    {
      "ref_id": "b16",
      "title": "Billion-scale similarity search with GPUs"
    },
    {
      "ref_id": "b18",
      "title": "Improving Graph Collaborative Filtering with Neighborhood-enriched Contrastive Learning"
    },
    {
      "ref_id": "b17",
      "title": "Improving Multi-Scenario Learning to Rank in E-commerce by Exploiting Task Relationships in the Label Space"
    },
    {
      "ref_id": "b19",
      "title": "MAMDR: A Model Agnostic Learning Method for Multi-Domain Recommendation"
    },
    {
      "ref_id": "b21",
      "title": "Entire Space Multi-Task Model: An Effective Approach for Estimating Post-Click Conversion Rate"
    },
    {
      "ref_id": "b20",
      "title": "Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-ofExperts"
    },
    {
      "ref_id": "b22",
      "title": "Heterogeneous Graph Augmented Multi-Scenario Sharing Recommendation with Tree-Guided Expert Networks"
    },
    {
      "ref_id": "b24",
      "title": "Contrastive Learning for Representation Degeneration Problem in Sequential Recommendation"
    },
    {
      "ref_id": "b23",
      "title": "Online Advertising Research in Advertising Journals: A Review"
    },
    {
      "ref_id": "b25",
      "title": "SAR-Net: A Scenario-Aware Ranking Network for Personalized Fair Recommendation in Hundreds of Travel Scenarios"
    },
    {
      "ref_id": "b27",
      "title": "Multi-Task and Multi-Scene Unified Ranking Model for Online Advertising"
    },
    {
      "ref_id": "b26",
      "title": "One Model to Serve All: Star Topology Adaptive Recommender for Multi-Domain CTR Prediction"
    },
    {
      "ref_id": "b28",
      "title": "Progressive Layered Extraction (PLE): A Novel Multi-Task Learning (MTL) Model for Personalized Recommendations"
    },
    {
      "ref_id": "b30",
      "title": "Pretraining Graph Neural Network for Cross Domain Recommendation"
    },
    {
      "ref_id": "b29",
      "title": "Visualizing Data using t-SNE"
    },
    {
      "ref_id": "b31",
      "title": "Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere"
    },
    {
      "ref_id": "b33",
      "title": "Self-supervised Graph Learning for Recommendation"
    },
    {
      "ref_id": "b32",
      "title": "CausalInt: Causal Inspired Intervention for Multi-Scenario Recommendation"
    },
    {
      "ref_id": "b34",
      "title": "Contrastive Cross-domain Recommendation in Matching"
    },
    {
      "ref_id": "b36",
      "title": "AdaSparse: Learning Adaptively Sparse Structures for Multi-Domain Click-Through Rate Prediction"
    },
    {
      "ref_id": "b35",
      "title": "APG: Adaptive Parameter Generation Network for Click-Through Rate Prediction"
    },
    {
      "ref_id": "b37",
      "title": "Self-supervised Learning for Large-scale Item Recommendations"
    },
    {
      "ref_id": "b39",
      "title": "Scenario-Adaptive and Self-Supervised Model for Multi-Scenario Personalized Recommendation"
    },
    {
      "ref_id": "b38",
      "title": "Leaving No One Behind: A Multi-Scenario Multi-Task Meta Learning Approach for Advertiser Modeling"
    },
    {
      "ref_id": "b40",
      "title": "S3-Rec: Self-Supervised Learning for Sequential Recommendation with Mutual Information Maximization"
    },
    {
      "ref_id": "b42",
      "title": "Automatic Expert Selection for Multi-Scenario and Multi-Task Search"
    },
    {
      "ref_id": "b41",
      "title": "Personalized Transfer of User Preferences for Cross-domain Recommendation"
    }
  ]
}