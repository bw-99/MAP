{"Pre-train and Fine-tune: Recommenders as Large Models": "", "Zhenhao Jiang \u2217": "", "Chenghao Chen \u2217": "222041010@link.cuhk.edu.cn The Chinese University of Hongkong, Shenzhen Shenzhen, Guangdong, China chenchenghao.cch@alibaba-inc.com Alibaba Group Hangzhou, Zhejiang, China Hao Feng zhisu.fh@alibaba-inc.com Alibaba Group Hangzhou, Zhejiang, China Yu Yang huke.yy@alibaba-inc.com Alibaba Group Hangzhou, Zhejiang, China Jin Liu nanjia.lj@alibaba-inc.com Alibaba Group Hangzhou, Zhejiang, China Jia Jia jj229618@alibaba-inc.com Alibaba Group Hangzhou, Zhejiang, China Jie Zhang shenxu.zj@alibaba-inc.com Alibaba Group Hangzhou, Zhejiang, China Ning Hu Alibaba Group Hangzhou, Zhejiang, China", "Abstract": "In reality, users have different interests in different periods, regions, scenes, etc . Such changes in interest are so drastic that they are difficult to be captured by recommenders. Existing multi-domain learning can alleviate this problem. However, the structure of the industrial recommendation system is complex, the amount of data is huge, and the training cost is extremely high, so it is difficult to modify the structure of the industrial recommender and re-train it. To fill this gap, we consider recommenders as large pre-trained models and fine-tune them. We first propose the theory of the information bottleneck for fine-tuning and present an explanation for the fine-tuning technique in recommenders. To tailor for recommendation, we design an information-aware adaptive kernel (IAK) technique to fine-tune the pre-trained recommender. Specifically, we define fine-tuning as two phases: knowledge compression and knowledge matching and let the training stage of IAK explicitly approximate these two phases. Our proposed approach designed from the essence of fine-tuning is well interpretable. Extensive online and offline experiments show the superiority of our proposed method. Besides, we also share unique and important lessons we learned when deploying the method in a large-scale online platform. We also present the potential issues of fine-tuning techniques in recommendation systems and the corresponding solutions. The recommender with IAK technique has been deployed on the homepage of a billion-scale online food platform for several months and has yielded considerable profits in our business. \u2217 Both authors contributed equally to this research. This work was done during Zhenhao Jiang's internship at Alibaba Group. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). WWWCompanion '25, Sydney, NSW, Australia. \u00a9 2025 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-1331-6/25/04 https://doi.org/10.1145/XXXXXX.XXXXXX", "CCS Concepts": "\u00b7 Information systems \u2192 Recommender systems ; \u00b7 Mathematics of computing \u2192 Information theory .", "Keywords": "Recommendation System, Information Bottleneck, Fine-tuning Technique", "ACMReference Format:": "Zhenhao Jiang, Chenghao Chen, Hao Feng, Yu Yang, Jin Liu, Jie Zhang, Jia Jia, and Ning Hu. 2025. Pre-train and Fine-tune: Recommenders as Large Models. In Companion Proceedings of the ACM Web Conference 2025 (WWW Companion '25), April 28-May 2, 2025, Sydney, NSW, Australia. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/XXXXXX.XXXXXX", "1 Introduction": "Shipping billions of items to meet users' tastes is the basic functionality of recommendation systems [27-29], which is an important part of online information distribution platforms such as news feeds, e-commerce, social media, etc. At present, the mainstream recommender systems, based on deep learning [16, 20], use an end-to-end training strategy to mine user interest and rank items according to user preference [6]. With the development of business, there are more and more topics/conditions in the domain of recommender algorithms. Multi-scene [15, 44], multi-region [12, 36], and multi-period [1, 42] are three representatives becoming the research frontier. Multi-scene describes that online recommender service consists of many scenes, such as home page, channel page, in-shop related page, etc . Users may click back and forth on different scenes with distinct behaviors. Therefore, it is necessary to explicitly model user-item interaction behavior in different scenes. Multi-region mainly includes multi-country and multi-city, which describes the differences in life habits and preferences of users among different regions, as well as the differences in the supply of items. For example, traditionally, turkey is for Thanksgiving in America while zongzi is for the Dragon Boat Festival in China. Multi-period describes that users have different behaviors at different times. For example, morning vs. evening on takeaway platforms, WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia. Zhenhao Jiang et al. and holiday vs. workday on short video platforms. Generally speaking, we can consider multi-scene, multi-region, and multi-period as belonging to the scope of multi-domain learning. There are several early studies [35, 53] that address the problems with the multi-domain learning paradigm that employs one model to serve all domains. However, if a new task is added, the entire model needs to be retrained, which is very inconvenient and increases the training overhead. Remark: Large-scale online platforms generate data volumes of several terabytes (Tb) per day, and recommender algorithms are incrementally learning with newly generated data at regular intervals rather than training from scratch. Therefore, when the model structure is massively modified, it means that a large number of parameters need to be trained from scratch, and more historical data (thousands of Tb) may be needed to train the newly modified structure, which greatly increases the training cost that is unacceptable in industrial recommender. Under different conditions/domains, users' interests and behaviors change. Therefore, the recommendation tasks under different conditions/domains can be regarded as downstream tasks, and the pre-training and fine-tuning methods can be used to conduct more tailored modeling. Compared to multi-domain learning, fine-tuning requires only minor changes to the original model. Adding a new task also only requires fine-tuning on that task and has no effect on the original model [17]. Therefore, we consider treating our recommender as a large model and fine-tuning it on different domains. Recently, large language models (LLMs) [3, 7] have taken the world by storm for their amazing performance in the field of natural language processing (NLP). Similarly, taking the multi-period task in a takeaway platform as an instance, the large-scale industrial recommender is trained on the full dataset covering all time, which learns the general business knowledge and the common preferences of users. However, due to the huge natural gap between different periods ( e.g. breakfast vs. dinner), it is difficult for the model to finely trace the shift in user preference at different periods. Fortunately, fine-tuning on the dinner dataset can enable the model to learn user's preference for dinner more accurately and the user preference learned on the breakfast dataset can assist in the dinner decision as well. In addition, fine-tuning has the following advantages over multi-domain learning: 1) model-agnostic : it can be used to extend any industrial-scale recommender, and 2) low training cost : on the basis of an industrial-scale recommender, fine-tuning the model with one-week-data in the corresponding domain/condition can achieve good performance. In this paper, we first present a preliminary explanation of finetuning in recommender systems. To build a tailored pre-training and fine-tuning (PF) framework for recommendation algorithms, we select our Online Large Recommender (OLR) deployed on an online food platform serving hundreds of millions of users as the pre-trained model. We propose a plug-and-play Information-Aware Adaptive Kernel (IAK) strategy to fine-tune the large pre-trained model. Specifically, the OLR mainly includes five parts: 1) embedding layer [26], 2) short-term and long-term sequence module [33], 3) MMoE-like main net (MMoE is a multi-task learning framework [31]), 4) debias module [5], and 5) stacked logits layer. Because the structure of OLR cannot be disclosed in detail due to trade secret, we will also use IAK on the commonly used SOTA baselines to verify the effectiveness of our proposed fine-tuning technique on the recommendation system. IAK is an encoder-decoder structure that can adaptively learn prior knowledge related to downstream tasks with the help of the information bottleneck. IAK compresses general business knowledge at first, then learns specific knowledge on given downstream tasks (In this paper, we consider learning on the given domain/condition as a downstream task.) and forgets the part of business knowledge that is irrelevant to the given task. This approach can draw users more accurately and give proper recommendation results in different domains/conditions flexibly. In addition, we also report two potential issues of fine-tuning in recommendation systems: 1) pseudo cold start issue, 2) user/item overlapping issue, and give preliminary discussions on the concerns. Our contributions are summarized as follows: \u00b7 We first present a theoretical explanation of the fine-tuning technique in recommender systems from the viewpoint of the information bottleneck. This makes a significant contribution to interpretability and points out a new topic for future research. \u00b7 We share unique and important lessons we have learned when deploying the presented solution on a billion-user scale online food platform. \u00b7 We first report two hidden issues of the fine-tuning technique in recommender systems and provide explorations and discussions. Extensive offline and online experiments on multiple real-world datasets show the superiority of our proposed approaches.", "2 related works": "There are mainly three issues related to this work: multi-domain learning, fine-tuning technique and multi-task learning. Multi-Domain Learning: Multi-domain recommendation [30, 37, 56] aims to improve recommendation performance in each domain using knowledge transferred from other domains. SAR-Net [40] employs the paradigm of a mixture of experts to give recommendation results and accommodates an attention mechanism [9] to extract the relationship of the user's interests in different domains. STAR [41] presents a star topology including shared parameters and domain-specific parameters, which can realize using one model to serve all domains in CTR prediction. AFT [14] learns the feature translations between different domains under a generative adversarial network [13] framework. It explicitly models the relationships between items, domains and users' representations in general and specific domains. ADIN [18] is able to adaptively treat the commonalities and diversities across scenarios, tracing the user's interest in different domains with shared networks and specific networks. HKGCL [24] handles each relevant domain as a hierarchy in the interaction network, and the hierarchical knowledge graph is aggregated based on the LightGCN aggregation strategy to learn knowledge representations for users and items. Fine-Tuning in Recommendation: Although fine-tuning techniques are widely used in NLP, there are few studies on fine-tuning in recommendation. [59] employs a simple fine-tuning technique to address mainstream bias [23] in recommendation. C 2 -CRS [58] employs user's feedback to fine-tune a conversational recommender system [43]. Pre-train and Fine-tune: Recommenders as Large Models WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia. Multi-Task Learning: Multi-task learning [54, 55] is a learning structure that allows for the simultaneous learning of multiple related tasks. Share-Bottom [4] network designs a shared network at the bottom to learn the similarities and multiple task-specific towers at the top to give the final results. ESMM [32] proposes an entire space estimation method for conversion rate prediction and considers the decision path of 'impression->click->conversion\". MMoE [31] consists of multiple gates to mix the results of multiple experts and learn the correlations and differences in different tasks. PLE [45] presents a stacked structure with shared experts and specific experts which can decouple the parameters for different tasks to improve the performance. In contrast to the existing studies, this paper proposes a pretrained multi-task learning model and utilizes fine-tuning technique to solve multi-domain learning issues. Additionally, this paper provides theoretical exploration and practical guidance through mathematical derivation and extensive experiments.", "3 preliminary": "In this paper, we use the ideology of information bottleneck [39, 47] to explain the fine-tuning technology. We first introduce information bottleneck in this section. Information bottleneck theory is originally developed from the rate-distortion theory of data compression in information theory [2, 52], which discusses the problem of how to retain as much information as possible when compressing data. To open the black box of deep neural network, information bottleneck for deep learning is proposed as follows [38, 48].  where \ud835\udc4b is input data, \ud835\udc4c is input label, \u02c6 \ud835\udc4b is minimum sufficient representation of \ud835\udc4b , \ud835\udefd is a weight, and mutual information \ud835\udc3c is defined as:  Further, a variational approximation form with neural network is proposed to make information bottleneck become more suitable for deep learning analysis.  where \ud835\udc41 is the number of data, \ud835\udc5e ( \ud835\udc4c \ud835\udc56 | \u02c6 \ud835\udc4b ) and \ud835\udc5f ( \u02c6 \ud835\udc4b ) are variational approximation to \ud835\udc5d ( \ud835\udc4c \ud835\udc56 | \u02c6 \ud835\udc4b ) and \ud835\udc5d ( \u02c6 \ud835\udc4b ) , and \ud835\udc37 \ud835\udc3e\ud835\udc3f is KL-divergence formulated as:  Information bottleneck for deep learning describes how, during the training process, the neural network retains label information and compresses the data as much as possible to learn the representation from the massive data. For example, the feature map generated by convolutional layers in a convolutional neural network greatly compresses the information of the original data, but retains important features that can identify its labels. The user interest vector extracted by the user historical behavior sequence extraction module in the sequence recommendation algorithm greatly compresses the user behavior information, but retains the main interest of the user. There are also some studies on the information bottleneck for machine learning [11, 51]. However, the information bottleneck for fine-tuning technique was not considered by the early research.", "4 Explanation of Fine-tuning in Recommender 4.1 Information Bottleneck for Fine-tuning": "Here, we present a new explanation of fine-tuning from the perspective of information bottleneck. The original information bottleneck describes a minimum sufficient representation of samples that contains specific information. OLR is pre-trained on all domain data on a large scale to fully learn the general business knowledge, but it is difficult to deal with the impact of sudden changes in user interest caused by changes in conditions in downstream tasks ( e.g. human habits determine that there is a huge gap between user-item distribution of breakfast and that of dinner). Thus, each downstream task has its hidden characteristics which is the learning objective of fine-tuning. This means that some of the general business knowledge learned by the pre-trained model is helpful in understanding the downstream task, and some is not. Therefore, the knowledge or information learned in the pre-trained model needs to be compressed, and the information related to the downstream task should be retained as much as possible. Theorem 1 (Information Bottleneck for Fine-tuning). Given the general knowledge in the pre-trained model \ud835\udc3a and the knowledge in the given downstream task \ud835\udc47 , the optimization objective of Information Bottleneck for Fine-tuning is to find the minimum sufficient representation \u02c6 \ud835\udc3a :", "4.2 Two Phases in Fine-tuning": "It is generally believed that fine-tuning is a process of knowledge transfer [34, 46]. According to the above discussion, this process can be divided into two parts: knowledge compression and knowledge matching. Knowledge compression describes the process of extracting knowledge from general knowledge that is relevant to the downstream task. Knowledge matching describes the process of supplementing missing knowledge with downstream task data and integrating old and new knowledge. Theorem 2 (Knowledge Compression). Given the general knowledge in the pre-trained model \ud835\udc3a and the knowledge in the given downstream task \ud835\udc47 , the optimization objective of Knowledge Compression can be expressed as:  Theorem3 (Knowledge Matching). Given the compressed general knowledge \u00af \ud835\udc3a , the knowledge in the given downstream task \ud835\udc47 , and the learner \ud835\udc53 , the optimization objective of Knowledge Matching can be expressed as follows:  WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia. Zhenhao Jiang et al.", "4.3 Approximated Upper Bound": "In light of the above discussion, we calculate the upper bound of the information bottleneck for fine-tuning. In (5), the first term can be expressed as follows.  where \ud835\udc3b means entropy presented as follows.  Suppose the information entropy of \u02c6 \ud835\udc3a is fixed. The objective becomes to minimize the cross-entropy between \u02c6 \ud835\udc3a and \ud835\udc47 . Alternatively, we can optimize the cross-entropy loss of estimation and label directly:  and \u02c6 where \ud835\udc4c is label in \ud835\udc47 \ud835\udc4c is estimation given by fine-tuned model. For the second term, based on the non-negative nature of KL divergence, the following derivation is valid:  1 where \ud835\udc5f ( \u02c6 \ud835\udc3a ) is the variational approximation to \ud835\udc5d ( \u02c6 \ud835\udc3a ) . Thus, the upper bound of (5) is as follows.  where knowledge \ud835\udc3a and \u02c6 \ud835\udc3a can be approximated with training samples. More details are presented in Appendix A.1.", "5 proposed method": "", "5.1 Problem Formulation": "In this paper, we mainly focus on multi-scene, multi-region and multi-period in food recommendations. Let \ud835\udc46, \ud835\udc45, \ud835\udc43 denote three topics, respectively. Given a set of downstream tasks \ud835\udc37 = \ud835\udc46 \u222a \ud835\udc45 \u222a \ud835\udc43 = { \ud835\udc60 1 , ..., \ud835\udc60 \ud835\udc56 } \u222a { \ud835\udc5f 1 , ..., \ud835\udc5f \ud835\udc57 } \u222a { \ud835\udc5d 1 , ..., \ud835\udc5d \ud835\udc58 } where \ud835\udc60 \ud835\udc56 denotes the i-th scene, \ud835\udc5f \ud835\udc57 denotes the j-th city, and \ud835\udc5d \ud835\udc58 denotes the k-th mealtime. Here, a task is defined as modeling user interest in the given domain/condition. Therefore, there are \ud835\udc56 + \ud835\udc57 + \ud835\udc58 downstream tasks, the tasks under the same topic are independent, and the tasks under different topics are orthogonal. \ud835\udc3a is the general knowledge and \ud835\udc47 \ud835\udc5a is specific knowledge in the m-th task. The challenge is how to effectively extract the knowledge related to \ud835\udc47 \ud835\udc5a from \ud835\udc3a to assist the model in decision-making on the m-th task, in addition, to accomplish the above goal while compressing business knowledge of the large model as much as possible. This can be formulated as follows.  where F \ud835\udf03 is the learner with parameter \ud835\udf03 and | \ud835\udc3a | expresses the information of business knowledge. 1 It is difficult to calculate \ud835\udc5d ( \u02c6 \ud835\udc3a ) directly.", "5.2 Online Large Recommender": "Here, we give a brief introduction to the OLR, which serves billions of recommendation requests per day. 2 OLR is a multi-task schema flow trained on the comprehensive domain dataset, allowing OLR to extract general business knowledge and track user and item profiles. First, all ID features and discretized continuous features are input into an embedding layer to generate dense low-dimensional vectors. Once the data is embedded, the historical behavior related features are fed into short-term and long-term sequence modules to draw the interest evolution and user-item interaction, ensuring personalized mining. Then, the extracted user interest vectors are merged with other features and fed into an MMoE-like main net to learn the underlying relationship between CTR and CTCVR. In addition, a debias module is employed to avoid position bias, user bias, .etc , to assist OLR in providing a ground-truth result. Finally, the stacked logits layer uses all features and knowledge to perform estimation. The number of parameters of OLR is around one billion and it is pre-trained on hundreds of billions of samples.", "5.3 Information-Aware Adaptive Kernel": "Although we have provided an upper bound for the information bottleneck for fine-tuning, it is not easy to apply it to model optimization. Thus, we further relaxed the upper bound to simplify it.  Since we have to minimize \u02c6 \ud835\udc3a , minimizing its upper bound is a good alternative. More details are presented in Appendix A.2.  IAK is designed as an encoder-decoder structure. In this paper, we employ MLPs as the encoder and decoder. The encoder is mainly responsible for knowledge compression and both the encoder and decoder are responsible for knowledge matching. We consider the output of the encoder to represent the compressed business knowledge of the large model. Gaussian Approximation It is still difficult to solve -\ud835\udefd log \ud835\udc5f ( \u02c6 \ud835\udc3a ) directly, so we propose an alternative. Starting from the original meaning of the information bottleneck, the second term's requirement is to compress the original information as much as possible. We can address this problem from the distribution of parameters. If the parameter distribution of the well-trained model is similar to the initial parameter distribution, it indicates that the input sample has little influence on the model, that is, the knowledge compression rate is large. Here we employ a standard Gaussian distribution to initialize the encoder's parameters \ud835\udc64 0:  In order to optimize the second term, we consider a parametric Gaussian approximation:  where \ud835\udf07 and \ud835\udf0e are mean and variance of parameters of the encoder, and \ud835\udc64 is the well-trained parameters. 2 The model involves trade secrets and cannot be disclosed in detail. Pre-train and Fine-tune: Recommenders as Large Models WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia. The distance D between the distribution of the trained parameter and that of the original parameter can be formulated as the following equation.  Based on the above assumptions, the objective can be simplified as follows. More details are given in Appendix B.  Here, we make the following assumption. Assumption1 . The distance between the distribution of the trained parameter and that of the original parameter is positively correlated with the information contained in the parameter. During the training stage, the parameters of the large pre-trained model are frozen to maintain the general business knowledge and we only train IAKs on the downstream datasets.", "5.4 Lessons Learned from the real recommender": "In a real recommendation system, deploying a solution on the server presents many limitations and challenges. Here, we share two important lessons learned in our real business to help practitioners use IAK technique in their recommenders. 5.4.1 Parallel Inference and Domain Activation. For online inference, each model needs to be deployed independently and consumes server resources. Therefore, we cannot deploy the corresponding IAK on each domain. We propose a parallel inference and domain activation strategy to address this problem. All IAKs for different domains are deployed simultaneously. For any recommendation request, all IAKs will infer in parallel, and then select the output of the corresponding IAK according to the domain ID, which can be formulated as:  where F represents the IAK, \ud835\udc41 \ud835\udc51 is the number of domains, F \ud835\udc56 represents the IAK for the i-th domain, \ud835\udc51 is the domain ID of the current recommendation request, and I is the indicator function. 5.4.2 Dynamic Batch Aware Training. Although IAK is easy to use, training multiple IAKs for different downstream tasks is cumbersome. Moreover, due to the varying number of samples in different downstream tasks, it is difficult to combine multiple downstream tasks for training. This may lead to a lack of data for some downstream tasks in a training batch, resulting in large fluctuations in the corresponding IAK's gradient, which affects convergence. Therefore, the learning rate should change with the number of samples and the magnitude of the gradient.  where \ud835\udf06 is the original learning rate, \ud835\udc41 \ud835\udc35 is the number of samples in a batch for IAKs, \ud835\udc4a is the reciprocal of the gradient for IAKs and \u2299 denotes element-wise product. We still recommend choosing tasks of a similar magnitude to train together, or training them one-by-one.", "6 discussion on the hidden issues": "In this section, we report two concerns about the fine-tuning technique in recommender systems. These issues are significant for understanding the role of fine-tuning in the field of recommendation systems. Pseudo cold start issue. In practice, the amount of data used for fine-tuning is much smaller than that used for pre-training, and obviously the historical records of user-item interactions in some domains are lost in the downstream dataset. For example, we select the last seven days of scene A data to fine-tune OLR, but user B has not entered scene A in the last seven days. The IAK module does not learn the user's behavior in scene A from the downstream dataset. When the model is deployed on the server, the user B enters scene A, which the IAK module encounters for the first time, creating a pseudo cold start issue 3 . Models can only estimate based on the pre-trained general knowledge. User/item overlapping issue. Traditional fine-tuning methods completely isolate samples for different domains. Models serving scene A are fine-tuned using only scene A's samples. But that doesn't make sense in the realm of recommendation systems. In a recommendation system, samples from different domains are highly correlated. Taking multi-period as an example, there is not much difference in the distribution of items for lunch and dinner. So the information of an item at noon is helpful for understanding the characteristics of the item at night. It's not appropriate to think of them as independent tasks.", "7 experiments": "We conducted extensive experiments to evaluate the performance of IAK, and the following research questions (RQs) were answered: \u00b7 RQ1 Does OLR outperform state-of-the-art baselines in multiple conditions? \u00b7 RQ2 Does IAK fine-tuning technique further improve the performance of OLR? \u00b7 RQ3 Does IAK fine-tuning technique further improve the performance of baselines? \u00b7 RQ4 Can IAK address multiple topics at the same time? \u00b7 RQ5 How do critical components affect the performance of IAK? \u00b7 RQ6 How does the issue in Section 6 affect IAK's performance? \u00b7 RQ7 Does OLR+IAK work in real large-scale online recommendation scenarios? \u00b7 RQ8 How does IAK affect recommendation results?", "7.1 Experimental Settings": "Here we introduce our experimental settings. More details are given in Appendix C. 7.1.1 Datasets. We collected 11 offline datasets from the user's behavior logger between July 20, 2023 and July 27, 2023 on a largescale online platform serving over one billion recommendation requests per day. The datasets can be classified into three groups 3 Unlike the traditional cold start problem [25], this user is not an absolutely new user, but a new user for a certain period of time in a specific scene. WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia. Zhenhao Jiang et al. according to topics: 1) multi-region, 2) multi-period, and 3) multiscene. The statistics of datasets are summarized in Table 1. 7.1.2 Baselines. In most large-scale practices, the core goals of recommenders are the estimation of click-through rate (CTR) and click-through conversion rate (CTCVR) [19, 50]. Therefore, the discussion in this paper is based on the multi-task learning framework. The representative state-of-the-art approaches are listed as follows. Shared Bottom (SB) [4] is one of the most classic multi-task models. The bottom learns the underlying relationships among different tasks and each task corresponds to an output tower. ESMM [32] estimates CTR and CTCVR via explicitly modeling the user's decision path of \"impression-click-conversion\" with the chain rule of conditional probability. MMoE [31] employs multiple experts and gates to leverage different tasks. It uses gates to adapt the weights of experts on different tasks. 7.1.3 Metrics. To evaluate the performance of the proposed approach, we select two widely used metrics for offline testing, i.e. , CTR-AUC and CTCVR-AUC for CTR and CTCVR estimation, respectively.", "7.2 RQ1&RQ2: Comparison with Baselines": "In this subsection, we discuss the results of baselines vs. our models and OLR vs. OLR+IAK to answer RQ1 and RQ2. Table 2 and Table 3 indicate that OLR outperforms all state-ofthe-art (SOTA) baselines in terms of CTR-AUC and CTCVR-AUC on 11 real-world datasets. The best results are shown in Bold and the second best results are shown in Italic . All the best results and the second best results come from our methods. Compared to the baselines, OLR+IAK achieves a huge improvement on average, i.e. , CTR-AUC +1.09% and CTCVR-AUC +0.70% for multi-scene, CTR-AUC +1.28% and CTCVR-AUC +0.75% for multi-period, and CTR-AUC +0.68% and CTCVR-AUC +0.39% for multi-region. The improvement verifies the superiority of our proposed approaches. Considering ZS-OLR and baselines, MMoE achieves the best performance among the baselines. However, our OLR still makes a great improvement on the basis of MMoE. This shows that OLR has fully learned the general business knowledge and can give accurate recommendations in various downstream tasks without fine-tuning. For ZS-OLR and OLR+IAK, OLR+IAK outperforms ZS-OLR on all datasets except comparison on Scene2 in terms of CTR-AUC. Compared to ZS-OLR, OLR+IAK achieves a significant improvement on average, i.e. , CTR-AUC +0.26% and CTCVR-AUC +0.49% for multi-scene, CTR-AUC +0.66% and CTCVR-AUC +0.26% for multi-period, and CTR-AUC +0.35% and CTCVR-AUC +0.11% for multi-region. This indicates that IAK can further improve the recommendation performance of OLR on multiple domains. Such a big improvement on a high benchmark is astounding and proves that IAK is meaningful and promising.", "7.3 RQ3: Control Test": "To demonstrate the universality of IAK, we use it to fine-tune the baseline models. Fig. 1 visualize the results of the control test. We can see that IAK fine-tuning technique consistently improves the performance of Figure 1: Heatmap of results of control test. (a) Results of control test in terms of CTR-AUC. (b) Results of control test in terms of CTCVR-AUC. ESMM MMoE Scenel Scene2 Timel Timez Time3 Regionl   Region2   Region3 Regions   Regiong 0,50 ESMM 0.25 MMoE Scenel Scene2 Timel Timez Time3 Regionl Region2 Region3 the baseline models. This proves that IAK is a universal module that can be employed in any model. Additionally, IAK performs better in Scene1, Scene2, Time2, and Region4 which is consistent with the results of fine-tuning OLR. In light of the results of data analysis, there is a large gap between the data distribution of the better performing domains and the overall data distribution. In these domains, the user's interest shifts severely and discontinuously, making it difficult for the model to capture the user's intent. Fine-tuning alleviates this problem by allowing the model to learn domain-specific sample distributions that can explicitly trace the sudden change of the user's interest. As a result, models can perform better on these domains.", "7.4 RQ4: Cross-Topic Test": "In this subsection, we verify the performance of IAK in crosstopic conditions. Here, we combine domains under different topics in pairs to form new domains. Table 4 presents the results of cross-topic tests. On average, OLR+IAK achieves CTR-AUC +0.37%, CTCVR-AUC +0.42% in Scene-Region, CTR-AUC +0.55%, CTCVRAUC +0.61% in Scene-Time, and CTR-AUC +0.63%, CTCVR-AUC +0.31% in Region-Time. The results prove the superiority of IAK. Cross-topic testing is meaningful for the real recommendation system. In reality, conditions under different topics occur at the same time, and the ability of cross-topic modeling can depict users' interests more accurately and further improve the recommendation performance.", "7.5 RQ5: Parameter Sensitivity Test": "Since IAK is an encoder-decoder structure, the encoding dimension \ud835\udc51 \ud835\udc52 is an important parameter that affects the IAK structure. Fig. 2 shows that the performance of the model increases with the increase of \ud835\udc51 \ud835\udc52 , while the growth speed is slower and slower. This indicates that there is an upper bound on the number of parameters required to complete the knowledge transfer in IAK. Based on the experiments, we recommend setting \ud835\udc51 \ud835\udc52 to 50.", "7.6 RQ6: Exploration Study": "In this subsection, we implement experiments to preliminarily explore the answers to the concerns proposed in Section 6. More details are presented in Appendix D. To evaluate the pseudo cold start issue, we test the performance of IAK trained on samples in different time spans. In Fig. 3a and 3b, as the time span increases, the performance of the model grows Pre-train and Fine-tune: Recommenders as Large Models WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia. Table 1: The basic statistics of datasets. '#\" denotes 'the number of\". K refers to thousand and M refers to million. Table 2: Comparison with SOTA baselines in terms of CTRAUC. ZS-OLR means zero shot OLR. Figure 2: Results of parameter sensitivity test. Table 3: Comparison with SOTA baselines in terms of CTCVRAUC. ZS-OLR means zero shot OLR. more and more slowly. Even though an increase in time span can further improve model performance, it is not reasonable to increase the time span all the time. Besides, training data over a week-long span is recommended for fine-tuning. To evaluate the user/item overlapping issue, we try not to completely isolate samples from different domains. For Scene1, we use Scene1 samples with 70% weight and Scene2 samples with 30% weight to train IAK for Scene1. The weight setting is based on the proportion of users in real business. Fig. 3c shows that fine tuning on completely isolated samples can not achieve the best Table 4: Cross-topic test. The results are the relative increase of OLR+IAK compared to ZS-OLR. performance, and the proposed weighted sample training can further improve the performance of the model by introducing samples from other related domains. WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia. Zhenhao Jiang et al. (a) Results of exploration of the pseudo cold start issue in terms of CTR-AUC. (b) Results of exploration of the pseudo cold start issue in terms of CTCVR-AUC. (c) Results of exploration of user/item overlapping issue. CTR-AUC CTR-AUC Sccnc2 CTCVR-AUC CTCVR-AUC on Sccncz Figure 3: Results of exploration study.", "7.7 RQ7: Online A/B Test": "To evaluate the performance of OLR+IAK in large-scale online recommender, we conducted three online A/B tests (one single IAK test and two multiple IAKs tests) by deploying it to the recommendation scenario on the homepage of a billion-scale online platform. The online base model is ZS-OLR. Here, we selected three businessrelated metrics: Number of Orders (NO), Order Rate (OR) and Net GMV (NG, a measure of net profit) to evaluate the performance in an online environment. 7.7.1 Single IAK Test. From August 2, 2023 to August 25, 2023, we conducted a 23-day online A/B test in Region4 serving 2% of users. On average, OLR+IAK improved NO by 1.86% OR by 1.14% and NG by 1.62% in Region4. This proves the effectiveness of IAK technique in large-scale online recommenders in a given domain. 7.7.2 Multiple IAKs Test. To verify the influence of IAK technique on the global domain of recommendation, we conducted experiments in two dimensions: multi-scene and multi-region. From July 25, 2023 to August 25, 2023, we conducted a 30-day online A/B test in both Scene1 and Scene2 serving 8% of users. On average, OLR+IAK improved NO by 0.61% OR by 0.46% and NG by 0.74% in the global domain. From August 18, 2023 to August 25, 2023, we conducted a seven-day online A/B test in all regions serving 2% of users. On average, OLR+IAK improved NO by 0.58% OR by 0.62% and NG by 0.74% in the global domain. The experiments Figure 4: Some hot items in Region4, the vertical axis represents the number of impressions of the items, and the items surrounded by green squares are the specialties of Region4. 1299 965 791 665 581 509 489 8 453 418 353 281 show the superiority of IAK technique in large-scale online recommenders. Finally, we chose to deploy the multi-scene dimensional multi-IAK OLR to the homepage of our platform serving billions of recommendation requests per day.", "7.8 RQ8: Case Study": "In this subsection, we present a representative case to visually illustrate the importance of fine-tuning. Fig. 4 demonstrates the number of impressions of 11 hot items in Region4. The data is collected from the real online environment deploying OLR+IAK serving 2% of users in Region4 in seven days. Among the items, five items enclosed by green squares belong to Cantonese cuisine, a specialty of Region4. Users in Region4 are divided into two classes: permanent residents and outsiders. Due to eating habits and living habits, local residents usually prefer local specialties. Outsiders, especially travelers, also prefer local specialties that are regarded as a part of their trip. Besides, barbecue also accounts for a large proportion, which is a popular food in the world. This case demonstrates that IAK technique can mine the characteristics of different domains to make the recommendation results more in line with the user's interest distribution in the downstream task. We can understand the user's interest much better in different conditions in this manner.", "8 conclusion": "In this paper, we propose a fine-tuning technique called IAK for recommenders to address the multi-domain learning issue. To make it suitable for real large-scale recommendation systems, we specifically design deployment and training strategies. Additionally, we first present a theoretical explanation for fine-tuning in recommendation that significantly improves the interpretability of our proposed methodologies. IAK can be flexibly applied to any recommendation model structure without affecting the recommendation performance and overall framework of the original model. Extensive online and offline experiments test the superiority of our proposed approaches. The IAK technique is deployed on the homepage of Pre-train and Fine-tune: Recommenders as Large Models a real online platform and yields significant financial benefits. Future work may include exploring a better method to mix different domains in different topics.", "Acknowledgments": "Thanks to Alibaba Group for supporting this study.", "References": "[1] Sajad Ahmadian, Nima Joorabloo, Mahdi Jalili, and Milad Ahmadian. 2022. Alleviating data sparsity problem in time-aware recommender systems using a reliable rating profile enrichment approach. Expert Systems with Applications 187 (2022), 115849. [2] Robert B Ash. 2012. Information theory . Courier Corporation. [3] Som S Biswas. 2023. Potential use of chat gpt in global warming. Annals of biomedical engineering 51, 6 (2023), 1126-1127. [4] Rich Caruana. 1997. Multitask learning. Machine learning 28 (1997), 41-75. [5] Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan He. 2023. Bias and debias in recommender system: A survey and future directions. ACM Transactions on Information Systems 41, 3 (2023), 1-39. [6] Qiwei Chen, Changhua Pei, Shanshan Lv, Chao Li, Junfeng Ge, and Wenwu Ou. 2021. End-to-end user behavior retrieval in click-through rateprediction model. arXiv preprint arXiv:2108.04468 (2021). [7] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences. Advances in neural information processing systems 30 (2017). [8] Alexandre D\u00e9fossez, L\u00e9on Bottou, Francis Bach, and Nicolas Usunier. 2020. A simple convergence proof of adam and adagrad. arXiv preprint arXiv:2003.02395 (2020). [9] Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. 2021. Attention is not all you need: Pure attention loses rank doubly exponentially with depth. In International Conference on Machine Learning . PMLR, 2793-2803. [10] Boya Du, Shaochuan Lin, Jiong Gao, Xiyu Ji, Mengya Wang, Taotao Zhou, Hengxu He, Jia Jia, and Ning Hu. 2023. BASM: A Bottom-up Adaptive Spatiotemporal Model for Online Food Ordering Service. In 2023 IEEE 39th International Confer- ence on Data Engineering (ICDE) . IEEE, 3549-3562. [11] Ziv Goldfeld and Yury Polyanskiy. 2020. The information bottleneck problem and its applications in machine learning. IEEE Journal on Selected Areas in Information Theory 1, 1 (2020), 19-38. [12] Elizabeth G\u00f3mez, Ludovico Boratto, and Maria Salam\u00f3. 2022. Provider fairness across continents in collaborative recommender systems. Information Processing & Management 59, 1 (2022), 102719. [13] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. Advances in neural information processing systems 27 (2014). [14] Xiaobo Hao, Yudan Liu, Ruobing Xie, Kaikai Ge, Linyao Tang, Xu Zhang, and Leyu Lin. 2021. Adversarial feature translation for multi-domain recommendation. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining . 2964-2973. [15] Junyou He, Guibao Mei, Feng Xing, Xiaorui Yang, Yongjun Bao, and Weipeng Yan. 2020. Dadnn: multi-scene ctr prediction via domain-aware deep neural network. arXiv preprint arXiv:2011.11938 (2020). [16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition . 770-778. [17] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021). [18] Yuchen Jiang, Qi Li, Han Zhu, Jinbei Yu, Jin Li, Ziru Xu, Huihui Dong, and Bo Zheng. 2022. Adaptive domain interest network for multi-domain recommendation. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management . 3212-3221. [19] Zhenhao Jiang, Biao Zeng, Hao Feng, Jin Liu, Jicong Fan, Jie Zhang, Jia Jia, Ning Hu, Xingyu Chen, and Xuguang Lan. 2023. ESMC: Entire Space Multi-Task Model for Post-Click Conversion Rate via Parameter Constraint. arXiv preprint arXiv:2307.09193 (2023). [20] Zhenhao Jiang, Biao Zeng, Hao Feng, Jin Liu, Jie Zhang, Jia Jia, and Ning Hu. 2023. Mobile Supply: The Last Piece of Jigsaw of Recommender System. arXiv preprint arXiv:2308.03855 (2023). [21] Xuan Nhat Lam, Thuc Vu, Trong Duc Le, and Anh Duc Duong. 2008. Addressing cold-start problem in recommendation systems. In Proceedings of the 2nd international conference on Ubiquitous information management and communication . 208-211. [22] Hao Li, Charless Fowlkes, Hao Yang, Onkar Dabeer, Zhuowen Tu, and Stefano Soatto. 2023. Guided recommendation for model fine-tuning. In Proceedings of WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia. the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 3633-3642. [23] Roger Zhe Li, Juli\u00e1n Urbano, and Alan Hanjalic. 2023. Mitigating Mainstream Bias in Recommendation via Cost-sensitive Learning. In Proceedings of the 2023 ACM SIGIR International Conference on Theory of Information Retrieval . 135-142. [24] Yakun Li, Lei Hou, Dongmei Li, and Juanzi Li. 2023. HKGCL: Hierarchical graph contrastive learning for multi-domain recommendation over knowledge graph. Expert Systems with Applications 233 (2023), 120963. [25] Blerina Lika, Kostas Kolomvatsos, and Stathes Hadjiefthymiades. 2014. Facing the cold start problem in recommender systems. Expert systems with applications 41, 4 (2014), 2065-2073. [26] Haochen Liu, Xiangyu Zhao, Chong Wang, Xiaobing Liu, and Jiliang Tang. 2020. Automated embedding size search in deep recommender systems. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . 2307-2316. [27] Junling Liu, Chao Liu, Renjie Lv, Kang Zhou, and Yan Zhang. 2023. Is chatgpt a good recommender? a preliminary study. arXiv preprint arXiv:2304.10149 (2023). [28] Jie Lu, Dianshuang Wu, Mingsong Mao, Wei Wang, and Guangquan Zhang. 2015. Recommender system application developments: a survey. Decision support systems 74 (2015), 12-32. [29] Linyuan L\u00fc, Mat\u00fa\u0161 Medo, Chi Ho Yeung, Yi-Cheng Zhang, Zi-Ke Zhang, and Tao Zhou. 2012. Recommender systems. Physics reports 519, 1 (2012), 1-49. [30] Linhao Luo, Yumeng Li, Buyu Gao, Shuai Tang, Sinan Wang, Jiancheng Li, Tanchao Zhu, Jiancai Liu, Zhao Li, and Shirui Pan. 2022. MAMDR: a model agnostic learning method for multi-domain recommendation. arXiv preprint arXiv:2202.12524 (2022). [31] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. 2018. Modeling task relationships in multi-task learning with multi-gate mixture-ofexperts. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1930-1939. [32] Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu, and Kun Gai. 2018. Entire space multi-task model: An effective approach for estimating post-click conversion rate. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval . 1137-1140. [33] Jarana Manotumruksa and Emine Yilmaz. 2020. Sequential-based adversarial optimisation for personalised top-n item recommendation. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . 2045-2048. [34] Alan Tan Wei Min, Ramon Sagarna, Abhishek Gupta, Yew-Soon Ong, and Chi Keong Goh. 2017. Knowledge transfer through machine learning in aircraft design. IEEE Computational Intelligence Magazine 12, 4 (2017), 48-60. [35] Wentao Ning, Xiao Yan, Weiwen Liu, Reynold Cheng, Rui Zhang, and Bo Tang. 2023. Multi-domain Recommendation with Embedding Disentangling and Domain Alignment. arXiv preprint arXiv:2308.05508 (2023). [36] Pyeonghwa Oh and Byung-Yeon Hwang. 2017. Real-time Spatial Recommendation System based on Sentiment Analysis of Twitter. Journal of Society for e-Business Studies 21, 3 (2017). [37] Fulan Qian, Kaili Qin, Hai Chen, Jie Chen, Shu Zhao, and Yanping Zhang. 2023. Adaptive social recommendation combined with the multi-domain influence. Information Systems 113 (2023), 102145. [38] Andrew M Saxe, Yamini Bansal, Joel Dapello, Madhu Advani, Artemy Kolchinsky, Brendan D Tracey, and David D Cox. 2019. On the information bottleneck theory of deep learning. Journal of Statistical Mechanics: Theory and Experiment 2019, 12 (2019), 124020. [39] Ohad Shamir, Sivan Sabato, and Naftali Tishby. 2010. Learning and generalization with the information bottleneck. Theoretical Computer Science 411, 29-30 (2010), 2696-2711. [40] Qijie Shen, Wanjie Tao, Jing Zhang, Hong Wen, Zulong Chen, and Quan Lu. 2021. Sar-net: a scenario-aware ranking network for personalized fair recommendation in hundreds of travel scenarios. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management . 4094-4103. [41] Xiang-Rong Sheng, Liqin Zhao, Guorui Zhou, Xinyao Ding, Binding Dai, Qiang Luo, Siran Yang, Jingshan Lv, Chi Zhang, Hongbo Deng, et al. 2021. One model to serve all: Star topology adaptive recommender for multi-domain ctr prediction. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management . 4104-4113. [42] Jagendra Singh, Mohammad Sajid, Chandra Shekhar Yadav, Shashank Sheshar Singh, and Manthan Saini. 2022. A Novel Deep Neural-based Music Recommendation Method considering User and Song Data. In 2022 6th International Conference on Trends in Electronics and Informatics (ICOEI) . IEEE, 1-7. [43] Yueming Sun and Yi Zhang. 2018. Conversational recommender system. In The 41st international acm sigir conference on research & development in information retrieval . 235-244. [44] Shulong Tan, Meifang Li, Weijie Zhao, Yandan Zheng, Xin Pei, and Ping Li. 2021. Multi-Task and Multi-Scene Unified Ranking Model for Online Advertising. In 2021 IEEE International Conference on Big Data (Big Data) . IEEE, 2046-2051. [45] Hongyan Tang, Junning Liu, Ming Zhao, and Xudong Gong. 2020. Progressive layered extraction (ple): A novel multi-task learning (mtl) model for personalized recommendations. In Proceedings of the 14th ACM Conference on Recommender Zhenhao Jiang et al. WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia. Systems . 269-278. [46] Genevieve N Thompson, Carole A Estabrooks, and Lesley F Degner. 2006. Clarifying the concepts in knowledge transfer: a literature review. Journal of advanced nursing 53, 6 (2006), 691-701. [47] Naftali Tishby, Fernando C Pereira, and William Bialek. 2000. The information bottleneck method. arXiv preprint physics/0004057 (2000). [48] Naftali Tishby and Noga Zaslavsky. 2015. Deep learning and the information bottleneck principle. In 2015 ieee information theory workshop (itw) . IEEE, 1-5. [49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017). [50] Hong Wen, Jing Zhang, Yuan Wang, Fuyu Lv, Wentian Bao, Quan Lin, and Keping Yang. 2020. Entire space multi-task modeling via post-click behavior decomposition for conversion rate prediction. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval . 2377-2386. [51] Tailin Wu, Ian Fischer, Isaac L Chuang, and Max Tegmark. 2020. Learnability for the information bottleneck. In Uncertainty in Artificial Intelligence . PMLR, 1050-1060. [52] Tailin Wu, Hongyu Ren, Pan Li, and Jure Leskovec. 2020. Graph information bottleneck. Advances in Neural Information Processing Systems 33 (2020), 2043720448. [53] Yingyi Zhang, Xianneng Li, Yahe Yu, Jian Tang, Huanfang Deng, Junya Lu, Yeyin Zhang, Qiancheng Jiang, Yunsen Xian, Liqian Yu, et al. 2023. Meta-Generator Enhanced Multi-Domain Recommendation. In Companion Proceedings of the ACM Web Conference 2023 . 485-489. [54] Yu Zhang and Qiang Yang. 2018. An overview of multi-task learning. National Science Review 5, 1 (2018), 30-43. [55] Yu Zhang and Qiang Yang. 2021. A survey on multi-task learning. IEEE Transactions on Knowledge and Data Engineering 34, 12 (2021), 5586-5609. [56] Zihan Zhang, Xiaoming Jin, Lianghao Li, Guiguang Ding, and Qiang Yang. 2016. Multi-domain active learning for recommendation. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 30. [57] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Deep interest evolution network for click-through rate prediction. In Proceedings of the AAAI conference on artificial intelligence , Vol. 33. 5941-5948. [58] Yuanhang Zhou, Kun Zhou, Wayne Xin Zhao, Cheng Wang, Peng Jiang, and He Hu. 2022. C 2 -CRS: Coarse-to-Fine Contrastive Learning for Conversational Recommender System. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining . 1488-1496. [59] Ziwei Zhu and James Caverlee. 2022. Fighting mainstream bias in recommender systems via local fine tuning. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining . 1497-1506.", "A Mathematical Derivation": "", "A.1 Mathematical Derivation in Section 4.3": "For the second term, based on the non-negative nature of KL divergence, the following derivation is valid:  where \ud835\udc5f ( \u02c6 \ud835\udc3a ) is the variational approximation to \ud835\udc5d ( \u02c6 \ud835\udc3a ) . According to (2) and (11),  Thus, the upper bound of (5) is as follows. -\ud835\udc3c ( \u02c6 \ud835\udc3a ; \ud835\udc47 ) + \ud835\udefd\ud835\udc3c ( \u02c6 \ud835\udc3a ; \ud835\udc3a ) \u2248 \ud835\udc3b ( \u02c6 \ud835\udc4c | \ud835\udc4c ) + \ud835\udefd\ud835\udc3c ( \u02c6 \ud835\udc3a ; \ud835\udc3a )  where knowledge \ud835\udc3a and \u02c6 \ud835\udc3a can be approximated with training samples.", "A.2 Mathematical Derivation in Section 5.3": "", "B Gaussian Approximation": "In deep learning, we often need to solve for functional optimal. It is hard to do functional analysis in training models. The common method is to use differential (variational) approximation. Assuming that the unknown function obeys the Gaussian distribution, the analysis of the function is converted to the solution of the mean and variance of the Gaussian distribution. This process is called Gaussian approximation.  where \ud835\udc5e ( \ud835\udc65 ) is the unknown function, N is Gaussian distribution, \ud835\udf07 and \ud835\udf0e 2 are mean and variance of N . The IAK's initialization in this paper is normalized Gaussian initialization, and we use Gaussian approximation to represent the parameter distribution of the trained IAK. This allows the functional analysis problem to be transformed into a two-unknowns function optimization problem, which can be calculated simply by using gradient descent method.", "C Experimental Settings": "All models in this paper are implemented with Tensorflow 1.12 in Python 2.7 environment. All models are trained with a cheif-worker distributed framework with 1600 CPUs. To ensure fair comparison, all models are equipped with the same embedding table, short-term and long-term sequence modules, and debias module. The initial learning rate is set to 0.005 and the optimizer is set to AdagradDecay. Besides, batch size is set to 1024 and training epoch is set to 1. The activation function is set to LeakyReLU. All the DNNs in all models have three layers with the parameter set of [1024, 512, 256]. The embedding size is set to 8 for all categorical features. The dataset Pre-train and Fine-tune: Recommenders as Large Models is split into training set and test set based on chronological order and the ratio is 6:1. The weights of loss functions are all set to 1 in all multi-task learning models. The encoding dimension is set to [10, 30, 50, 80, 100]. All Attention modules used are multi-head attention with 8 heads. There are two experts in MMoE. WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia. For the online A/B test, the users in two groups strictly satisfy the statistical assumption and the users' feedback actually happened. Received 20 February 2007; revised 12 March 2009; accepted 5 June 2009"}
