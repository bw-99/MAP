{"Multi-Granularity Attention Model for Group Recommendation": "Jianye Ji * , Jiayan Pei * , Shaochuan Lin * , Taotao Zhou, Hengxu He, Jia Jia, Ning Hu Alibaba Group Hangzhou&Shanghai, China {jianyeji.jjy,jiayanpei.pjy,lin.lsc,taotao.zhou,hengxu.hhx,jj229618,huning.hu}@alibaba-inc.com", "ABSTRACT": "Group recommendation provides personalized recommendations to a group of users based on their shared interests, preferences, and characteristics. Current studies have explored different methods for integrating individual preferences and making collective decisions that benefit the group as a whole. However, most of them heavily rely on users with rich behavior and ignore latent preferences of users with relatively sparse behavior, leading to insufficient learning of individual interests. To address this challenge, we present the Multi-Granularity Attention Model (MGAM), a novel approach that utilizes multiple levels of granularity ( \ud835\udc56.\ud835\udc52. , subsets, groups, and supersets) to uncover group members' latent preferences and mitigate recommendation noise. Specially, we propose a Subset Preference Extraction module that enhances the representation of users' latent subset-level preferences by incorporating their previous interactions with items and utilizing a hierarchical mechanism. Additionally, our method introduces a Group Preference Extraction module and a Superset Preference Extraction module, which explore users' latent preferences on two levels: the group-level, which maintains users' original preferences, and the superset-level, which includes group-group exterior information. By incorporating the subset-level embedding, group-level embedding, and supersetlevel embedding, our proposed method effectively reduces group recommendation noise across multiple granularities and comprehensively learns individual interests. Extensive offline and online experiments have demonstrated the superiority of our method in terms of performance.", "ACMReference Format:": "Jianye Ji * , Jiayan Pei * , Shaochuan Lin * , Taotao Zhou, Hengxu He, Jia Jia, Ning Hu. 2023. Multi-Granularity Attention Model for Group Recommendation . In Proceedings of (Conference'23). ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn", "1 INTRODUCTION": "Group recommendation offers tailored and attractive suggestions to each member of the group, ensuring personalized satisfaction for all. Compared with individual recommendations, group recommendations provide a broader range of options by considering social * These authors contributed equally. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference'23, October 2023, Birmingham, UK \u00a9 2023 Association for Computing Machinery. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn Figure 1: Example showcasing the significance of subset granularity. User A's interactions with a candidate item carry significant weight compared to users with less interaction, but without considering the granularity of subsets. By utilizing the interaction history with other items to incorporate subset level granularity, we can arrive at a more informed and acceptable final decision for the entire group. Love film Group Horror film C A Horror film Drama film Comedy Romance film like dislike A Interaction history B C D B D 0.6 0.1 0.1 0.2 C A B D Subset Horror film 0.4 0.6 Horror film \u2026 \u2026 Candidate Final decision influence and collaborative decision-making, making them especially beneficial for group activities [11, 19] such as family dinners, group trips, and colleagues' parties. In recent research, group recommendation has garnered significant attention due to its widespread application. Prior studies [1-3, 20] have employed static strategies to aggregate users' profiles, resulting in limited consideration of diverse member preferences. To address this gap, neural attention-based methods [4, 5, 10, 11, 1417] have been proposed to dynamically adjust members' weights. However, these methods often suffer from overfitting and noise due to their heavy reliance on users with rich behavioral data. Moreover, assigning lower weights to users with fewer interactions may fail to capture their preferences, even if they have a rich history of interactions with other items. Graph-based methods [6, 9, 13, 18, 19, 21] have attempted to integrate intra- and inter-graph connections to alleviate this issue, but they still lack the ability to utilize comprehensive granularity for better representation of the final group embedding in multi-views. In this paper, we propose a solution to the above challenge of uncovering group members' latent preferences and mitigating recommendation noise through the M ultiG ranularity A ttention M odel ( MGAM ). The MGAM effectively leverages multi-granularity at three levels: subset, group, and superset, to achieve this goal. Specifically, we introduce the Subset Preference Extraction (SubPE) module to incorporate a new level of granularity, namely subset, to improve the representation of users' latent subset-level preferences. The SubPE module leverages the users' past interactions with other Jianye Ji * , Jiayan Pei * , Shaochuan Lin * , Taotao Zhou, Hengxu He, Jia Jia, Ning Hu Conference'23, October 2023, Birmingham, UK Subset Preference Extraction SubPE Group  Preference Extraction SupPE GPE Fusion Layer Prediction Layer Item Group Group Representation MGAM (a) Item Superset Preference Extraction (c) Group Graph SupPE G 2 u 1 u 2 u 3 u 4 u 5 G 1 u 6 u 7 u 8 G 3 G 4 G 1 G 2 G 3 G 4 \ud835\udc62 ! \ud835\udc62 \" \ud835\udc62 # Projection Softmax Users Item \ud835\udefc ! \ud835\udefc \" \ud835\udefc $ \u03a3 Group Preference Extraction \u2026 \u2026 GPE \ud835\udc46 ! \ud835\udc46 \" \ud835\udc46 % Group \u2026 \ud835\udc62 ! \ud835\udc62 \" \ud835\udc62 & Item User Attention Network Subset Attention Network \ud835\udc38\ud835\udc5a\ud835\udc4f ! ! \ud835\udc38\ud835\udc5a\ud835\udc4f ! \" \ud835\udc38\ud835\udc5a\ud835\udc4f ! # SubPE Superset Preference Extraction \u2026 (b) Subset Preference Extraction GCN (d) Figure 2: The overview of MGAM (a) and each substructure for preference extraction (b, c, d). items and employs a hierarchical mechanism to mitigate the reliance on users with abundant behavioral data on candidate items. Fig. 1 illustrates the role that subset-level granularity plays in our proposed method. Additionally, our approach involves utilizing the Group Preference Extraction (GPE) module to create a unique group-level preference representation that preserves initial user expression and aids decision-making. To reduce recommendation noise and gain a deeper understanding of group representation, we employ the Superset Preference Extraction (SupPE) module at the superset-level granularity to capture external preference information. Our fusion layer dynamically incorporates subsetlevel, group-level, and superset-level embeddings to uncover latent but real preferences of group members. Extensive experiments on public datasets demonstrate the superiority of our approach over state-of-the-art methods, and successful online A/B testing further proves its effectiveness.", "2 METHODOLOGY": "In this section, we introduce the definition of group recommendation task and detail the MGAM, with its overall architecture illustrated in Fig. 2. MGAM proposes the Subset Preference Extraction Module, the Group Preference Extraction Module, and the Superset Preference Extraction Module, which comprehensively solves the above problems from the subset granularity, the group granularity, and the superset granularity, respectively.", "2.1 Preliminaries": "In group recommendation, there are three sets of entities: a user set U = { \ud835\udc62 1 , \ud835\udc62 2 , ..., \ud835\udc62 \ud835\udc5b } , an item set V = { \ud835\udc63 1 , \ud835\udc63 2 , ..., \ud835\udc63 \ud835\udc5d } , and a group set G = { \ud835\udc54 1 , \ud835\udc54 2 , ..., \ud835\udc54 \ud835\udc5e } , where \ud835\udc5b , \ud835\udc5d , \ud835\udc5e denote the sizes of these three sets. The \ud835\udc61 -th group \ud835\udc54 \ud835\udc61 \u2208 G consists of a set of user members \ud835\udc54 \ud835\udc61 = { \ud835\udc62 1 , \ud835\udc62 2 , ..., \ud835\udc62 | \ud835\udc54\ud835\udc59 | } , where \ud835\udc62 | \ud835\udc54\ud835\udc59 | \u2208 U and | \ud835\udc54 \ud835\udc59 | is the size of \ud835\udc54 \ud835\udc61 . The aim of this task is to calculate the probability of an item being interacted with by a specific group \ud835\udc54 \ud835\udc61 ,  where the training data is represented as { \ud835\udc65 \ud835\udc56 , \ud835\udc66 \ud835\udc56 } \ud835\udc41 \ud835\udc56 = 1 \u2208 D , \ud835\udc56 refers to the index of the data and \ud835\udc41 is the total number of data. F represents the group recommendation model with training parameter \ud835\udf03 . The label \ud835\udc66 \ud835\udc56 \u2208 Y indicates whether group \ud835\udc54 \ud835\udc61 has interacted with item \ud835\udc63 \ud835\udc56 . \ud835\udc65 \ud835\udc56 \u2208 X is a sample consisting of \ud835\udc63 \ud835\udc56 , \ud835\udc54 \ud835\udc61 , I \ud835\udc62 \ud835\udc56 , and I \ud835\udc54 \ud835\udc61 , representing item, group, and user-item and group-item interactions respectively.", "2.2 Subset Preference Extraction (SubPE)": "Our proposal introduces a new level of granularity called subsets, which improves the representation of users' preferences. We achieve this using a hierarchical mechanism consisting of the User Attention Network and Subset Attention Network, which reduces reliance on users with extensive behavioral data on candidate items. Further details can be found in Fig. 2(b). 2.2.1 Subset Generation. Traditional group-based methods often suffer from overfitting and noise, as they heavily rely on users with substantial behavioral data. This can result in an inaccurate depiction of preferences for users with fewer interactions. To address this, we introduce subsets as a new level of granularity, clustering users within existing groups based on their interaction history into \ud835\udc40 subsets. This enables us to identify users with similar preferences and filter out noise and bias in decision-making, leading to a more robust and accurate understanding of user preferences. Thus, \ud835\udc54 \ud835\udc61 = { \ud835\udc60 1 , ..., \ud835\udc60 \ud835\udc56 , ..., \ud835\udc60 \ud835\udc40 } , where \ud835\udc60 \ud835\udc56 represents a specific subset. 2.2.2 Hierarchical Mechanism. To improve the representation of users' preferences, we employ a hierarchical mechanism that begins by consolidating similar preferences within each subset utilizing a User Attention Network. This is followed by a Subset Attention Network that reduces reliance on users with extensive behavioral data and improves weights for users with focused preferences. User Attention Network aggregates preference within each subset:  where \ud835\udc52 ( \ud835\udc62 \ud835\udc58 ) and \ud835\udc52 ( \ud835\udc63 \ud835\udc57 ) represent the embeddings of \ud835\udc62 \ud835\udc58 and \ud835\udc63 \ud835\udc57 , respectively. \ud835\udc57 is the index of an item. \ud835\udc5a is the size of subset \ud835\udc60 \ud835\udc56 , \ud835\udc62 \ud835\udc58 \u2208 \ud835\udc60 \ud835\udc56 , \ud835\udc58 also refers to the index. W \ud835\udc62 and b \ud835\udc62 are trainable parameters, and \ud835\udf19 is a ReLU activation function. \u210e \ud835\udc56 \ud835\udc60 is the preference embedding for subset \ud835\udc60 \ud835\udc56 . Subset Attention Network uses an attention mechanism like MoSAN [16] to capture interactions and variations between subsets, giving more weight to users with specific preferences and improving overall performance. This can be visually represented in Fig. 1. Finally, we obtain the aggregated SubPE embedding for group \ud835\udc54 \ud835\udc61 ,    where W \ud835\udc56 , W \ud835\udc56 , W \ud835\udc60 , b \ud835\udc56 and b \ud835\udc60 are trainable parameters. \ud835\udc4e \ud835\udc56 refers to the activation between any specific subset \ud835\udc60 \ud835\udc56 related to other subsets. \u00af \u210e \ud835\udc56 \ud835\udc60 is the embedding of the all subsets within the group, except \u210e \ud835\udc56 \ud835\udc60 . \u210e \ud835\udc54 \ud835\udc61 \ud835\udc60\ud835\udc62\ud835\udc4f\ud835\udc5d\ud835\udc52 is the subset-granularity embedding for group \ud835\udc54 \ud835\udc61 . Multi-Granularity Attention Model for Group Recommendation Conference'23, October 2023, Birmingham, UK", "2.3 Group Preference Extraction (GPE)": "To better preserve the initial user expression and facilitate decisionmaking, we utilize the Group Preference Extraction (GPE) module to generate a distinct representation of group-level preferences,  where \ud835\udc62 \ud835\udc58 \u2208 \ud835\udc54 \ud835\udc61 and \u210e \ud835\udc61 \ud835\udc54\ud835\udc5d\ud835\udc52 refers to the group-granularity embedding of the group \ud835\udc54 \ud835\udc61 . W \ud835\udc54 and b \ud835\udc54 are trainable parameters.", "2.4 Superset Preference Extraction (SupPE)": "WeusetheSuperset Preference Extraction (SupPE) module to gather external preference information at the superset-granularity, minimizing recommendation noise and gain a deeper understanding of group representation. This involves graph construction and external information extraction using SupPE. Graph Construction. Various groups may have members in common, leading to overlapping relationships among them. By utilizing these shared users, we can establish a graph structure that links different groups together. To construct the graph, we treat each group as a node, and if a user belongs to multiple groups, they act as an edge connecting the relevant nodes. Fig. 2(d) provides a visual demonstration of this concept. External information Extraction. Byfollowing the above approach, the graph can be constructed and external information regarding the behavioral preferences of users in other groups can be incorporated. This leads to a reduction in recommendation noise and a more comprehensive understanding of group representation. To obtain the superset-granularity embedding, follow these steps:    where our approach involves \ud835\udc59 layers and utilizes the initial group index ID embedding, denoted as \u210e 0 \ud835\udc54\ud835\udc59 , to aid in end-to-end training. We set \u210e 0 \ud835\udc4f\ud835\udc61 to be equal to \u210e \ud835\udc54 \ud835\udc61 \ud835\udc60\ud835\udc62\ud835\udc4f\ud835\udc5d\ud835\udc52 . The adjacency matrices for the global group and batch group are represented by A \ud835\udc54 and A \ud835\udc4f , respectively, while D \ud835\udc54 and D \ud835\udc4f are the diagonal node degree matrices of A \ud835\udc54 and A \ud835\udc4f . Global and batch groups are used to enhance embeddings and improve accuracy, while also increasing efficiency through regulation. W \ud835\udc59 \ud835\udc54 and W \ud835\udc59 \ud835\udc4f are trainable parameters.", "2.5 Fusion and Optimization": "Fusion Layer. The fusion layer is designed to dynamically incorporate preferences from the three levels of granularity embeddings (SubPE, GPE, and SupPE). Specifically, We employ a self-attention network to comprehensively uncover the latent preferences of users among various levels of granularity. This enables the network to flexibly adjust the weights between different embeddings, achieving a dynamic fusion of multi-level granularity preferences:   where \u210e \ud835\udc54 \ud835\udc61 is the concatenation embedding of the above three granularity embedding, and \ud835\udc51 is the embedding dimension of \u210e \ud835\udc54 \ud835\udc61 . Prediction Layer. Taking inspiration from AGREE[4], our final prediction is derived through,  where \u210e \ud835\udc54 \ud835\udc61 \ud835\udc53 \ud835\udc62\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b is the group embedding after dynamic fusion. \u2299 refers to Hadamard product and \ud835\udf0e is the sigmoid function. W and b are trainable parameters. Loss Function. We update the entire network by minimizing the following loss L with the prediction \u02c6 \ud835\udc66 \ud835\udc54 \ud835\udc61 and the label \ud835\udc66 \ud835\udc54 \ud835\udc61 :    where \ud835\udf06 1 is the hyper-parameter, \ud835\udc66 + \ud835\udc54\ud835\udc61 and \ud835\udc66 -\ud835\udc54\ud835\udc61 are the prediction score of the item that has the same and different labels with the anchor, and \ud835\udf02 is the margin. Table 1: Datasets Statistics. 'ID', 'Ml', 'Mt-NYC', and 'Mt-CA' stand for Industrial Dataset, Movielens-1M, Meetup-NYC, and Meetup-CA, respectively.", "3 EXPERIMENTS": "", "3.1 Experimental Settings": "3.1.1 Datasets & Metrics. Our experiments were conducted on four datasets, namely, Movielens-1M, Meetup-NYC, Meetup-CA, which are extensively used in real-world scenarios, and an industrial dataset. Details can be found in Table 1. For Movilens-1M, the way we generate groups is line with the approach in [2]. Events in Meetup datasets are groups with attendees as members and the venue as the item. The Industrial dataset collected from Ele.me, a major online food ordering platform in China, includes seven consecutive days of training data and one day of testing data. The detailed implementation of our industrial dataset is outlined in Section 3.2.5. In terms of metrics, we adopt Hit Ratio and Normalized Discounted Cumulative Gain at topK recommendations, known as HR@K and NDCG@K , respectively, are widely used in recommendation systems. Higher HR@K and NDCG@K indicate better performance. In analyzing industrial datasets, we employ the Area Under the Curve (AUC) metric in offline experiments and utilize Click-Through Rate (CTR) to measure online performance. These metrics are commonly employed in industrial systems. Conference'23, October 2023, Birmingham, UK Jianye Ji * , Jiayan Pei * , Shaochuan Lin * , Taotao Zhou, Hengxu He, Jia Jia, Ning Hu Table 2: Performance Comparison of Various Methods. 'SC'=Spectral Clustering, 'AC'= Agglomerative Clustering. Figure 3: Performance of different subsets' size on Movielens1M dataset (a), and Meetup-CA dataset (b). 0.383 0.398 0.413 0.428 0.443 0.458 0.3800 0.3950 0.4100 0.4250 0.4400 1 2 3 4 5 6 7 8 NDCG@5 HR@5 Subset Size HR@5 NDCG@5 0.1735 0.1750 0.1765 0.1780 0.1795 0.1810 0.7250 0.7285 0.7320 0.7355 0.7390 0.7425 0.7460 1 2 3 4 5 6 7 8 NDCG@5 HR@5 Subset Size HR@5 NDCG@5 (a) Movielens-1M (b) Meetup-CA 3.1.2 Comparison Method & Running Settings. To verify the effectiveness of MGAM, we select three memory-based methods (NCF AVG [8], NCF LM [8], NCF MS [8]) and five attention-based methods (NeuMF [7], AGREE [4], MoSAN [16], GRADI [10], GAME [9]) for comparison. All methods are implemented with Tensorflow1.14. Adam optimizer [12] with a learning rate of 0.001 optimizes the objective function. The cluster method is chosen in Section. 2.2.1 is K-Means. \ud835\udc40 was grid searched to 3 and 5 for Movielens-1M and Meetup, respectively. The layers of \ud835\udc59 is set to 2, \ud835\udf06 1 is 0.5 and the margin \ud835\udf02 is set to 1. All the experiments in our paper have been repeated 10 times, with the final output reflecting the average scores.", "3.2 Experiments Results": "3.2.1 Performance Comparison. The results of our experiments are listed in Table 2. We can identify that the majority of model-based methods outperform memory-based methods, which is a common trend. Our MGAM has attained a state-of-the-art performance in three datasets across various metrics. 3.2.2 Ablation Study. Table 2 also presents the outcome of the ablation study conducted on each module, which clearly shows that MGAM's performance is negatively impacted by removing any of the modules. This ablation study demonstrates that each granularity module (denoted as w/o SubPE, w/o GPE, and w/o SupPE) plays a distinct role in capturing the users' latent preferences. Especially, removing SubPE significantly decreases performance compared to", "Table 3: Offline and online performance on industrial dataset.": "other modules, indicating the superiority introduced by the subset granularity. 3.2.3 Impact of Subsets' Number. Fig. 3 presents the results from hyper-parameter tuning for the number of subsets within a group. Results were only reported for Movielens-1M and Meetup-CA due to similar results on Meetup-NYC. Higher \ud835\udc40 indicates more subsets in a group and fewer members in a subset. The optimal subset sizes for the two datasets are 3 and 5, as depicted in Fig. 3(a) and Fig. 3(b), respectively. 3.2.4 Analysis of Clustering Methods. Table 2 also shows variation studies with different clustering methods, including K-Means (MGAM), Spectral Clustering (MGAM w/ SC), and Agglomerative Clustering (MGAM w/ AC). Our findings suggest that our approach is better suited to be used with K-Means compared to Spectral Clustering and Agglomerative Clustering. This is likely due to K-Means clustering having several advantages over Agglomerative Clustering and Spectral Clustering, including scalability, simplicity, and convergence guarantees. 3.2.5 Industrial Implementation and Online Performance. In online food ordering services, an area grid is utilized as a means of area division. Specifically, for a designated area grid, individuals with purchasing histories within the past seven days are grouped since they exhibit similar area preferences. For such groups of users, we can employ group recommendation techniques to suggest product collections just prior to the product recall stage in industrial recommendation. By doing so, we can significantly improve product selection without the need for excessive human labor and enhance the efficiency of this stage. Ultimately, this approach can lead to improved personalized recommendation outcomes. To validate the performance of our proposed model, we conducted methods comparison in our industrial dataset. Table 3 shows that our MGAM significantly outperformed the base model (AGREE). Our online Multi-Granularity Attention Model for Group Recommendation Conference'23, October 2023, Birmingham, UK experiments also confirmed this, as we deployed MGAM and compared it with the AGREE. The CTR from the 5-day experiment consistently outperformed the base model, with an average improvement of 1.2%.", "4 CONCLUSIONS": "In this paper, we propose a Multi-Granularity Attention Model (MGAM) to uncover the latent preferences of users in group recommendation tasks, further improving the preference extraction and aggregation ability of the model. Specifically, we propose the Subset Preference Extraction module, the Group Preference Extraction module, and the Superset Extraction Preference module from 3 different granularities to uncover group members' latent preferences and further mitigate recommendation noise. Extensive offline experiments and online performance demonstrate the effectiveness and efficiency of MGAM.", "REFERENCES": "[1] Sihem Amer-Yahia, Senjuti Basu Roy, Ashish Chawlat, Gautam Das, and Cong Yu. 2009. Group recommendation: Semantics and efficiency. Proceedings of the VLDB Endowment 2, 1 (2009), 754-765. [2] Linas Baltrunas, Tadas Makcinskas, and Francesco Ricci. 2010. Group recommendations with rank aggregation and collaborative filtering. In Proceedings of the fourth ACM conference on Recommender systems . 119-126. [3] Ludovico Boratto and Salvatore Carta. 2014. Modeling the preferences of a group of users detected by clustering: A group recommendation case-study. In Proceedings of the 4th international conference on web intelligence, mining and semantics (WIMS14) . 1-7. [4] Da Cao, Xiangnan He, Lianhai Miao, Yahui An, Chao Yang, and Richang Hong. 2018. Attentive group recommendation. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval . 645-654. [5] Da Cao, Xiangnan He, Lianhai Miao, Guangyi Xiao, Hao Chen, and Jiao Xu. 2019. Social-enhanced attentive group recommendation. IEEE Transactions on Knowledge and Data Engineering (2019). [6] Lei Guo, Hongzhi Yin, Tong Chen, Xiangliang Zhang, and Kai Zheng. 2021. Hierarchical hyperedge embedding-based representation learning for group recommendation. ACM Transactions on Information Systems (TOIS) 40, 1 (2021), 1-27. [7] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural Collaborative Filtering. In Proceedings of the 26th International Conference on World Wide Web, WWW 2017, Perth, Australia, April 3-7, 2017 , Rick Barrett, Rick Cummings, Eugene Agichtein, and Evgeniy Gabrilovich (Eds.). ACM, 173-182. [8] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural collaborative filtering. In Proceedings of the 26th international conference on world wide web . 173-182. [9] Zhixiang He, Chi-Yin Chow, and Jia-Dong Zhang. 2020. GAME: Learning Graphical and Attentive Multi-view Embeddings for Occasional Group Recommendation. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . 649-658. [10] Z. He, C. Y. Chow, J. D. Zhang, and N. Li. 2019. GRADI: Towards Group Recommendation Using Attentive Dual Top-Down and Bottom-Up Influences. In 2019 IEEE International Conference on Big Data (Big Data) . [11] Zhenhua Huang, Xin Xu, Honghao Zhu, and MengChu Zhou. 2020. An efficient group recommendation model with multiattention-based neural networks. IEEE Transactions on Neural Networks and Learning Systems 31, 11 (2020), 4461-4474. [12] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014). [13] Ruxia Liang, Qian Zhang, Jianqiang Wang, and Jie Lu. 2022. A Hierarchical Attention Network for Cross-Domain Group Recommendation. IEEE Transactions on Neural Networks and Learning Systems (2022). [14] Hongtao Liu, Fangzhao Wu, Wenjun Wang, Xianchen Wang, Pengfei Jiao, Chuhan Wu, and Xing Xie. 2019. NRPA: neural recommendation with personalized attention. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval . 1233-1236. [15] Aravind Sankar, Yanhong Wu, Yuhang Wu, Wei Zhang, Hao Yang, and Hari Sundaram. 2020. Groupim: A mutual information maximization framework for neural group recommendation. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . 1279-1288. [16] Lucas Vinh Tran, Tuan-Anh Nguyen Pham, Yi Tay, Yiding Liu, Gao Cong, and Xiaoli Li. 2019. Interact and decide: Medley of sub-attention networks for effective group recommendation. In Proceedings of the 42nd International ACM SIGIR conference on research and development in information retrieval . 255-264. [17] Wen Wang, Wei Zhang, Jun Rao, Zhijie Qiu, Bo Zhang, Leyu Lin, and Hongyuan Zha. 2020. Group-Aware Long-and Short-Term Graph Representation Learning for Sequential Group Recommendation. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . 1449-1458. [18] Xixi Wu, Yun Xiong, Yao Zhang, Yizhu Jiao, Jiawei Zhang, Yangyong Zhu, and Philip S Yu. 2023. ConsRec: Learning Consensus Behind Interactions for Group Recommendation. arXiv preprint arXiv:2302.03555 (2023). [19] Hongzhi Yin, Qinyong Wang, Kai Zheng, Zhixu Li, Jiali Yang, and Xiaofang Zhou. 2019. Social influence-based group representation learning for group recommendation. In 2019 IEEE 35th International Conference on Data Engineering (ICDE) . IEEE, 566-577. [20] Zhiwen Yu, Xingshe Zhou, Yanbin Hao, and Jianhua Gu. 2006. TV program recommendation for multiple viewers based on user profile merging. User modeling and user-adapted interaction 16, 1 (2006), 63-82. [21] Junwei Zhang, Min Gao, Junliang Yu, Lei Guo, Jundong Li, and Hongzhi Yin. 2021. Double-Scale Self-Supervised Hypergraph Learning for Group Recommendation. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management . 2557-2567."}
