{
  "Specify Robust Causal Representation from Mixed Observations": "Mengyue Yang University College London London, United Kingdom mengyue.yang.20@ucl.ac.uk Xinyu Cai Nanyang Technological University Singapore, Singapore xinyu.cai@ntu.edu.sg Furui Liu ∗ Zhejiang Lab Hangzhou, China liufurui@zhejianglab.com Weinan Zhang Shanghai Jiao Tong University Shanghai, China wnzhang@apex.sjtu.edu.cn Jun Wang University College London London, United Kingdom j.wang@cs.ucl.ac.uk",
  "ABSTRACT": "Learning representations purely from observations concerns the problem of learning a low-dimensional, compact representation which is beneficial to prediction models. Under the hypothesis that the intrinsic latent factors follow some casual generative models, we argue that by learning a causal representation, which is the minimal sufficient causes of the whole system, we can improve the robustness and generalization performance of machine learning models. In this paper, we develop a learning method to learn such representation from observational data by regularizing the learning procedure with mutual information measures, according to the hypothetical factored causal graph. We theoretically and empirically show that the models trained with the learned causal representations are more robust under adversarial attacks and distribution shifts compared with baselines. The supplementary materials are available at https://github.com/ymy4323460/CaRI/.",
  "CCS CONCEPTS": "· Computing methodologies → Machine learning algorithms .",
  "KEYWORDS": "Causal representation learning, Robustness learning, Learning theory for Causal representation learning.",
  "ACMReference Format:": "Mengyue Yang, Xinyu Cai, Furui Liu, Weinan Zhang, and Jun Wang . 2023. Specify Robust Causal Representation from Mixed Observations. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '23), August 6-10, 2023, Long Beach, CA, USA. ACM, New York, NY, USA, 19 pages. https://doi.org/10.1145/3580305.3599512",
  "1 INTRODUCTION": "Causal representation learning is an effective approach for extracting invariant, cross-domain stable causal information, which is believed to be able to improve sample efficiency by understanding the underlying generative mechanism from observational data ∗ Corresponding Author This work is licensed under a Creative Commons Attribution International 4.0 License. KDD '23, August 6-10, 2023, Long Beach, CA, USA © 2023 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-0103-0/23/08. https://doi.org/10.1145/3580305.3599512 [3, 31]. Causal representation learning is widely applied in many real-world applications like recommendation systems, search engines etc.[23, 36, 40, 47]. Recently, multiple approaches were proposed to learn the invariant causal representations, which are supposed to encode underlying causal generative systems describing the data, based on the problem-specific priors. The usual theory to implement it is called Independent Causal Machine (ICM) [25] principle, which can be applied to identify the cause information when all factors are observable. However, when the variables are unobservable in general and complex systems, this method usually does not work. Given that most methods employ a generative model, the main reason for such failure is due to the observation data (e.g. human images) is entangled by causal variables. To tackle this problem, previous works learned latent representations to capture the causal properties, e.g., causal disentanglement methods [34, 45] and invariant causal representation learning method [2, 18]. However, additional information like causal variable labels and domain information should be provided, which is usually unavailable in real-world systems. In this paper, we aim at disentangling the causal variables from an information theoretical view without providing additional supervision signals. Supposing that the factors are casually structured, we formalize a causal system as in Fig.1 (a), which is commonly accepted by the causality community [41, 43]. Given the label 𝑌 , the 𝑑 -dimensional observational data X is consist of causal factors including the parents paY , non-descendants ndY , descendants dcY of Y . The causal information paY enables the model a better generalization and robustness for prediction tasks. We consider the natural data generative process as an information propagation along the causal graph and try to find out paY from X . Based on the causal modelling, we propose to learn latent representations which maintain the most necessary causal information for the prediction task, named minimal sufficient causal information of a system. More specifically, we define the minimal sufficient cause (MSC) Z as a proxy of the parents in factor space as shown in Fig. 1 (b). MSCs are variables that are specially positioned in the system, blocking the path from the causes and non-descendants to 𝑌 . In this paper, we implement it by an information-theoretical approach, reducing the traditional two-step procedure i.e. causal disentanglement and information minimizing, to an optimization problem that can directly learn a latent causal representation with minimal sufficiency from observations. Specifically, the proposed optimization problem is a bi-level optimization problem minimizing 𝐼 ( Z ; paY , ndY ) , with KDD '23, August 6-10, 2023, Long Beach, CA, USA Mengyue Yang, Xinyu Cai, Furui Liu, Weinan Zhang, & Jun Wang maximizing mutual information 𝐼 ( Z ; 𝑌 ) as a constraint. Based on this, we propose an intervention effect to accurately specify the causal information paY . We name this method as CaRI (learning Cause Representation by Information-theoretic approach) and we further extend the method under robustness learning framework. Moreover, we theoretically analyze the sample efficiency of CaRI by giving a generalization error bound with respect to sample size. Experiments on synthetic and real-world datasets show the effectiveness of the proposed method. The main contribution of this paper are summarized below: · We define minimal sufficient causes (MSC) in causal system by the formalization of an explicit causal graphical model to describe the data generative process of the real-world system and propose an information-theoretical approach to learn MSC from observational data. · We theoretically analyze the sample efficiency of the learning approach by giving a generalization error bound w.r.t sample size. The theorem depicts a quantitative link between the amount of causal information contained in the learned representation and the sample complexity of the model on downstream tasks. · We empirically verify that CaRI is able to generalize well distribution shift respectively and robust against adversarial attack.",
  "2 RELATED WORKS": "Causal Representation Learning is a set of approaches to finding generalizable representations by extracting and utilizing causal information from observational data. They usually aim at finding causal structure and causal variables behind observations. From several different perspectives, a bunch of methods have been proposed in the literature. Causal Structure Learning. To assess the connection between causally related variables in real-world systems, a bunch of traditional methods use the Markov condition and conditional independence between cause and mechanism principle (ICM) to discover the causal structure or distinguish causes from effect [22]. Several works focus on the asymmetry between cause and effect. [9, 13, 38, 39], and similar ideas are utilized by [25, 39]. The series of works always assume that all the variable is observable. In contrast with these works, our proposed method is applicable to scenarios where the observed data is generated by hidden causal factors. Invariant Representation Learning Cross Multidomain. Some pioneering work [35, 43, 48] considers the heterogeneity across multiple domains under the out-of-distribution settings [11, 17, 18, 20, 21, 27, 30, 46]. They learn causal representations from observational data by enforcing invariant causal mechanisms between the causal representation and the task labels across multidomains. Similar to these works, we target obtaining invariant latent causal information but do not assume that the datasets are collected from multi-domains. Causal Disentanglement Representation Learning. Causal representation learning helps to reduce the dimension of the original high-dimensional input. Several works leverage structural causal models to describe causal relationships inside the entangled observational data [34, 43, 45] and learn to disentangle causal concepts from original inputs. Different from aforementioned works, the proposed method in this paper considers the causal information from the perspective of information theory [4, 7]. We put our attention on minimal causal information, which can be regarded as a compact representation of the whole underlying causal system. We also theoretically analyze the generalization ability from PAC learning frameworks [32, 33] and explain why the causal representation can achieve better generalization ability from the perspective of sample complexity.",
  "3 PROBLEM DEFINITION": "",
  "3.1 Notations": "Considering the causal scenario in Fig.1 (a), the observation data can be generated by the concepts in hidden space which contain multiple hidden causal variables. Denote X ∈ X as 𝑑 -dimensional observational data like context information or features in real-world systems, and 𝑌 ∈ Y as the labels of downstream tasks. Each pair of sample ( x ,𝑦 ) is drawn i.i.d. from joint distribution 𝑝 ( x , 𝑦 ) . We use paY ∈ R 𝑝 1 to denote the variables including parent nodes of 𝑌 in the causal graph, while 𝝐 is the vector of independent noise with probability densities of 𝑝 𝝐 = N( 0 , 𝛽𝐼 ) . Similarly, dcY ∈ R 𝑝 2 and ndY ∈ R 𝑝 3 denote the descendant and non-descendant nodes of Y , respectively. In our method, we introduce minimal sufficient parents, denoted by Z ∈ Z of the system. Note that all the causal factors are assumed to be embedded in factors space, the observed data only contains ( X , 𝑌 ) , where X = h ( paY , ndY , dcY ) , h ∈ H where h : R 𝑝 1 + 𝑝 2 + 𝑝 3 → R 𝑑 is a deterministic function. In causal systems, the causes of prediction tasks are stable and robust, this means that when intervening on the parents, the causal effect is propagated to its child but not vice versa. All other correlated variables ndY , dcY in the causal system are regarded as spurious-correlated variables.",
  "3.2 Minimal Sufficient Causes (MSC)": "In our paper, we claim that not all the cause information is useful for prediction tasks. For example, considering a case of burning fire in a room, it is the presence of oxygen which explain the fire, but the match struck is definitely the necessary cause of fire. This real-world example is selected from section 9 in [26]. From the perspective of finding the most useful causes from observational data, we introduce the minimal sufficient cause variable Z into the causal system. As Fig. 1 (b) shows, the minimal sufficient causes Z are regarded as the proxy of parent variables. We define minimal sufficient causes in detail as below. Definition 1. Assuming that the causal graph (Fig. 1 (b)) with Minimal Sufficient Causes holds, the Minimal Sufficient Cause blocks the path between [ paY , ndY ] and 𝑌 , and the following conditional independence condition holds:  Our goal is to identify the minimal sufficient information Z in hidden factors space. The minimum sufficient causal variable Z in a causal system is stable information for predicting 𝑦 . From the perspective of sufficient causes, we define it from a probabilistic view, which is inspired by the minimal sufficient statistics [16]. Specify Robust Causal Representation from Mixed Observations KDD '23, August 6-10, 2023, Long Beach, CA, USA 𝑝𝑎 ! 𝑌 𝑑𝑐 ! 𝑛𝑑 ! 𝑋 𝐹𝑎𝑐𝑡𝑜𝑟𝑠\t𝑆𝑝𝑎𝑐𝑒 𝜖 𝑝𝑎 ! 𝑌 𝑑𝑐 ! 𝑛𝑑 ! 𝑋 𝐹𝑎𝑐𝑡𝑜𝑟𝑠\t𝑆𝑝𝑎𝑐𝑒 𝑂𝑏𝑠𝑒𝑟𝑣𝑒𝑠 𝜖 𝑍 𝑀𝑖𝑛𝑖𝑚𝑎𝑙\t𝑆𝑢𝑓𝑓𝑖𝑐𝑖𝑒𝑛𝑡\t𝐶𝑎𝑢𝑠𝑒𝑠 𝜙 a \t𝑔𝑒𝑛𝑒𝑟𝑎𝑙\t𝑐𝑎𝑢𝑠𝑎𝑙\t𝑠𝑦𝑠𝑡𝑒𝑚 𝑏 \t𝑐𝑎𝑢𝑠𝑎𝑙\t𝑠𝑦𝑠𝑡𝑒𝑚\t𝑤𝑖𝑡ℎ\t𝑀𝑖𝑛𝑖𝑚𝑎𝑙\t𝑆𝑢𝑓𝑓𝑖𝑐𝑖𝑒𝑛𝑡\t𝐶𝑎𝑢𝑠𝑒𝑠 Figure 1: The figure demonstrates a case of a causal system (a) and its extension of introducing minimal sufficient causes (b). Definition 2. (Sufficient Causes). Let paY , ndY , Z , 𝑌 be random variables. Z is sufficient cause of 𝑌 shown in Fig. 1 (b) if and only if utilize the information-theoretic ways since it can naturally combine Definition 2 and 3 by considering the information contained in MSC.  The definition of sufficient causes are the variables that are able to 'produce' the causal system. From the perspective of minimal, we define a variable which can generate the whole the system with the minimum information. That is, all the variables of prediction task can be inferred if minimal sufficient causes are given. Definition 3. (Minimal Sufficient Causes). The sufficient cause Z ∗ is minimal if and only if for any sufficient cause Z , there exists a deterministic function f such that Z ∗ = f ( Z ) almost everywhere w.r.t. X . Definition 3 shows that the Minimal Sufficient Causes Z in the causal system is the variable containing minimal information from all parents.",
  "3.3 Learning MSC as Causal Representation from Observational Data": "This paper focuses on causal representation learning, which aims at finding a low-dimensional representation of observation benefiting for predicting 𝑌 . Fig. 1 (a)(b) shows the causal system behind a prediction task, which uses observational data X to predict the target 𝑌 . The method is treated as a two stage process, and the first stage is to extract the representation from observational data. Let Z = 𝜙 ( X ) denote representation extracted from original observation X , where 𝜙 : X → Z is the representation extraction function. The next stage is to use the representation to predict 𝑌 . Now that we have formally defined Minimal Sufficiency, the basic objective is defined as learning a representation where all the information from minimal sufficient causes is included. The process is to model a flow of representation learning method and downstream prediction by satisfying Definition 2 3. The objective from Definition 2 is easy to be evaluated by common statistic methods, like independent testing by mutual information. However, it is very hard to get the minimal variable in Definition 3. To evaluate the objectives in Definition 2 3 in a unified framework, we",
  "4 LEARNING MINIMAL SUFFICIENT CAUSAL REPRESENTATIONS": "In this section, we present a method to learn the minimal sufficient parent's information Z from observational data X . The difficulty lies in distinguishing minimal sufficient cause Z from X , when we only observe X . We first analyze the information propagation among different causal variables under two typical causal graphs in hidden factors space, based on which we propose an objective function with mutual information constraints. Next, we extend our method by introducing do-operation, which can enhance the ability to distinguish causes if such information is not embedded in the observational data.",
  "4.1 Information-theoretic property of MSC in factor space": "Animportant fact is that in Fig.1 (b), the minimal sufficient causes in observational data X dominate the generative process of the causal system defined in Fig.1 (b). If there exists a mapping from X to Z , it is a function that finds the minimal sufficient causes inside the causal system. We develop an algorithm to learn representations based on such hypothetical structure Fig. 1 (b). Based on the definition of Z , denoted by 𝐼 (· , ·) the mutual information, we obtain the following Theorem (The proof is provided in supplementary material). Theorem 4.1. Let Z ∈ Z , Z = 𝜙 ( X ) , X = h ( paY , ndY , dcY ) and h ∈ H is an invertible function, Z is a minimal sufficient cause of the causal system demonstrated in Fig. 1 (b) if and only if Z is an optimal solution of following objective  Theorem 4.1 shows that we can identify the MSC by solving the min-max optimization problem. In real-world applications, the information of ndY and dcY may not be revealed, and the above KDD '23, August 6-10, 2023, Long Beach, CA, USA Mengyue Yang, Xinyu Cai, Furui Liu, Weinan Zhang, & Jun Wang objective function cannot be optimized directly. To get a tractable form, in the next section, we extend our optimization objective to observational space. We extend Eq. 3 to a tractable objective by scaling the mutual information terms in Eq. 3. The way is to link the unrevealed variables ndY , dcY to observation X . The following lemma can help us scale Eq. 3. Lemma 4.2. Suppose the features and labels are X , 𝑌 respectively, where X deterministically consists of the minimal sufficient parents, descendants and non-descendant as X = h ( paY , ndY , dcY ) . The following inequality holds if and only if h is an invertible deterministic function  Proposition 4.3. Let Z ′ , Z ∈ Z , Z = 𝜙 ( X ) , X = h ( paY , ndY , dcY ) , ℎ ∈ H h (·) is invertible function. When all the functions (lines in Fig. 1) between paY , ndY , dcY are invertible Z is the minimal sufficient cause of the causal system demonstrated in Fig. 1 (b) if and only if Z equals to the optimal solution of following objective  From Theorem 4.3 we can substitute the terms including ndY and paY by tractable mutual information term. For Eq. 5 in Proposition 4.3, by defining 𝑚 ( 𝑌 ) = max Z ′ 𝐼 ( Z ′ ; 𝑌 ) and then reformulating Z ∈ arg max Z ′ 𝐼 ( Z ′ ; 𝑌 ) as 𝐼 ( Z ; 𝑌 ) ≥ 𝑚 ( 𝑌 ) , with plugging in Z = 𝜙 ( X ) the optimization problem in Eq. 3 can be equivalently formulated as minimizing the following Lagrangian L( 𝜙, 𝜆 ) such that  where L( 𝜙, 𝜆 ) is denoted as 𝛿 ( 𝜙 ) for conciseness and 𝜆 is a hyperparameter which is manually selected in practice. The object coincides with Information Bottleneck (IB) objective function [33]. The difference is that IB is deduced from Rate Distortion Theorem in information theory, and it holds under the structure of the Markov Chain instead of a causal graph (i.e. Fig. 1). In this paper, the IB setting is generalized into causal space, by bridging minimal sufficient causes with root cause variables in the hypothetical causal graph. The detailed proof of Theorem 4.1 and Proposition 4.3 in supplementary materials show the differences between our proposed method and IB.",
  "4.2 Distinguishing components by intervention effect": "The previous section illustrates a method to find Z on the factor and observation level. Note that the objective function 𝛿 ( 𝜙 ) (Eq. 6) given from Proposition 4.3 can find the minimal sufficient causes under the strong assumption. In real-world applications, if we use the information-theoretic objective, it is very hard to distinguish causes pay and spurious variable dcY , ndy of 𝑌 from the objective. To release this problem, we introduce an intervention operation, denoted by 𝑑𝑜 ( 𝑋 = 𝑥 ) [26] into our method. Intervention in causality means the system operates under the condition that certain variables are controlled by external forces. In hidden factor space, one of the differences between Z and dcY , ndy is that if we intervene on the value of Z , the causal effect will be delivered to its child 𝑌 , but the causal effect to 𝑌 from the intervention conducted on child node dcY will be blocked. From such, let ¯ x means the intervened value which not equals to x , the following inequality describes intervention effect holds  Instead of conducting interventions on the parental variables in a real-world environment, we create a representation space Z where it supports simulation of the interventional manipulation on parents by intervening Z in the learned model. The functional interventional distributions 𝑃 ( 𝑌 = 𝑦 | 𝑑𝑜 ( Z = 𝜙 ( ˆ x ))) can be identified from purely observational data X and 𝑌 ( [26, 28, 43]),  Therefore in the representation space, we can directly maximize the intervention effect on the intervention space Z to satisfy Eq. 7. To make the intervention effect easier to be evaluated in the mutual information process, we introduce an intervention variable ¯ Z ∈ |Z| and build an intervention network shown in Fig. 2, in which we first infer the representation z from observational data x , based on which we can obtain the intervened value ¯ z ≠ z . Then we optimize the parameters in the model by maximizing the intervention effect term defined in mutual information language by  Integrating intervention effect and the objective function Eq. 6, the final objective is defined as below. The additional term 𝐼 ( ¯ Z ,𝑌 ) is the key to evaluating the intervention effect.  To intuitively understand the final objective Eq. 10, we divide it into positive and negative parts. The first positive term aims at finding minimal causes, and it helps retain information from the prediction task and drops redundant information from the original input. For the negative term, it is used to distinguish causes from all correlated variables by decreasing the information overlapping between 𝑌 and intervened representation ¯ Z .",
  "5 PRACTICAL ALGORITHMS": "In this section, we provide the details of how to evaluate the mutual information term in Eq. 10 and the alternative robust training process of our method. Specify Robust Causal Representation from Mixed Observations KDD '23, August 6-10, 2023, Long Beach, CA, USA",
  "5.1 Implementation of 𝐿 ( 𝜙 )": "In this paper, all objective functions are defined under mutual information formulation. We evaluate Eq.10 in two parts. The first positive part (Eq.10 (1)) is evaluated by the following parameterized objective, the variational estimation of mutual information [1]:   For the negative term described in Eq. 10, the minimization process requires the upper bound of it [8]. The upper bound is formed below:  Note that the expectation on second term in Eq. 12 requires marginal distribution 𝑝 ( 𝑦 ) 𝑝 ( ¯ z ) rather than joint distribution, therefore we independently sample 𝑦 in practice. The intervened network (Fig. 2) helps us calculate the value of ¯ Z along two steps. The first step, we build a neural network to generate the transformation vector T ∈ R 𝑡 from observational data X , where ( t = k ( x ) ) and k : X → R 𝑡 is a deterministic function modeled by neural network. The second step, the density of intervened ¯ Z is calculated by 𝑝 ( ¯ z | z , t ) = 𝛿 ( ¯ z = z + t ) , where 𝛿 (·) is Dirac delta function. In experiments, if t is close to 0, it will decline performance of our method since original z is close to the intervened one ¯ z . To avoid this problem, we add an additional constraint min k | t 2 -b | 2 , where 𝑏 is a hyperparameter, in our experiments, we set 𝑏 = 0 . 8.",
  "5.2 Robust Learning under Adversarial Attack": "To enhance the robustness against potential exogenous variables or noises 𝝐 and guarantee the robustness of the proposed method, we extend our method by incorporating adversarial learning. Considering the causal generative process as Y = 𝑓 ( paY , 𝝐 ) , the 𝝐 is regarded as a random noise perturbing the pay inside a ball with finite diameter. We treat the inference approach as the process of adversarial attack [5, 6, 42] and define the influence of exogenous variables as   where B( z , 𝛽 ) is Wasserstein ball, in which the 𝑝 -th Wasserstein distance [24] W 𝑝 1 between 𝑧 and z ′ is smaller than 𝛽 . z ′ and ¯ z ′ integrate both intervention and exogenous information. We further define intervention robustness (IR) to measure the worst intervention results of the intervention term in Eq.10. IR aims at finding the worst perturbation of z and ¯ z , which is formally defined below, Definition 4. (Intervention Robustness) Let ¯ Z ′ denote intervened variables on Z = 𝜙 ( X ) , ∀ z ′ ∈ B( z , 𝛽 ) , ¯ z ′ ∈ B( ¯ z , 𝛽 ) , 𝐷 and 𝐷 ′ denote datasets sample from 𝑝 ( z ′ , y ) and 𝑝 ( ¯ z ′ , y ) , the intervention robustness is defined as  1 W 𝑝 ( 𝜇, 𝜈 ) = GLYPH<16> inf 𝛾 ∈ Γ ( 𝜇,𝜈 ) ∫ Z×Z Δ ( 𝒛 , 𝒛 ′ ) 𝑝 𝑑𝛾 ( 𝑧, 𝑧 ′ ) GLYPH<17> 1 / 𝑝 , Γ ( 𝜇, 𝜈 is the collection of all probability measures on Z × Z Remark. The intervention robustness defines the worst intervention effect influenced by exogenous 𝝐 . For the representation z , the term min z ′ ∈B( z ,𝛽 ) 𝐼 ( 𝑌 ; Z ′ ) aims at find the perturbed z ′ around z with lowest mutual information 𝐼 ( 𝑌 ; Z ′ ) . For the transformed variable ¯ z , IV aims to find worst mutual information max ¯ z ′ ∈B( ¯ z ,𝛽 ) 𝐼 ( 𝑌 ; ¯ Z ′ ) . Combining two worst mutual information together, the IR term aims at finding the worst intervention effect perturbed by 𝝐 . Combining the IR term with the original objective 𝐿 ( 𝜙 ) , we get the final objective function optimized by minmax approach. Equivalently, we only need to optimize 𝐼 ( Z ′ ; 𝑌 ) rather than 𝐼 ( Z ; 𝑌 ) + 𝐼 ( Z ′ ; 𝑌 ) since if the worst case 𝐼 ( Z ′ ; 𝑌 ) is satisfied, 𝐼 ( Z ; 𝑌 ) is satisfied. The robust optimization objective function is 𝐿 rb ( 𝜙 ) , where  𝐿 = rb ( 𝜙 ) The inequality in above objective is due to  The robust method is learned by the minimax procedure. Literally, the minimization procedure helps to avoid the worst-case led by exogenous variable 𝝐 , because it maximizes the intervention robustness by adjusting the parameter of feature extractor 𝜙 . The optimization objective of the robust method can extract minimal sufficient causal representation from observation data with high robustness ability. Wetrain and evaluate the robust method by the adversarial attack on representation space. We use PGD attack [19] with ∞ -norm and 2-norm to get intervened z ′ and ¯ z ′ . We set 𝑝 𝜽 ( z ) as N( 𝑦, 1 ) to avoid trivial representations. Then we use negative cross entropy to approximate mutual information. More implementation details are shown in the supplementary material.",
  "6 WHYCAUSAL REPRESENTATION CAN ENHANCE GENERALIZATION ABILITY": "In this section, we theoretically analyze the generalization property of causal representation by learning theory framework [32]. Learning theory contains a set of methodologies to show the upper bound of the gap between risk/error on training data and all possible data from the data distribution. These methods justify a generalization problem that whether a model learned from a small data set can be generalized to any unseen test data from data distribution. Instead of estimating the risk bound, we start from the perspective of information theory and follow the framework of information bottleneck [33]. We provide a finite sample bound of the difference between ground truth and estimated one, which measures the generalization ability. The bound the relationship between 𝐼 ( Z ; 𝑌 ) and its estimation ˆ 𝐼 ( Z ; 𝑌 ) .",
  "6.1 The Generalization Error Bound of i.i.d. Data": "Here, we provide theoretical justification with the following theory (The proof is provided in supplementary material): KDD '23, August 6-10, 2023, Long Beach, CA, USA Mengyue Yang, Xinyu Cai, Furui Liu, Weinan Zhang, & Jun Wang Theorem 6.1. Let Z = 𝜙 ( X ) where 𝜙 : X → Z be a fixed arbitrary function, determined by a known conditional probability distribution 𝑝 ( z | x ) . Let 𝑚 be sample size and 𝐶 is a constant. For any confidence parameter 0 < 𝛿 < 1 , it holds with a probability of at least 1 -𝛿 , that 1. General case (The learned representation 𝑍 contains correlated information)  where 𝑚 ≥ 𝐶 4 log (|Y|/ 𝛿 )|Z| e 2 2. Ideal case (The learned representation Z contains information of causes)  where 𝑚 ≥ 𝐶 log (|Y|/ 𝛿 ) 𝛽 e 2 Remark. The theorem provides a generalization bound under finite sample settings. It shows that when representation Z fully contains parent information paY , we achieve a sample complexity bound as 𝑚 ≥ 𝐶 log (|Y|/ 𝛿 ) 𝛽 e 2 , where 𝛽 is the variance of 𝝐 . The minimum number of samples needed reduces from |Z| to 𝛽 , which is a tighter bound since in most of cases we assume |Z| ≫ 𝛽 . This shows that z = paY gives the reduced sample complexity and tightened generalization bound. The theorem also serves as a general solution to causality prediction problems, supporting the claim that a better prediction is achieved with causal variables, compared to that with correlated variables.",
  "6.2 The Generalization Error Bound when Distribution Shift Happens": "We also show additional generalization results. For the scenario of distribution sift, we define the mutual information on source domain as 𝐼 S ( Z , 𝑌 ) and mutual information on target domain as 𝐼 T ( Z , 𝑌 ) . Denote joint distribution in source and target domain as S( z , 𝑦 ) = 𝑝 S ( z , 𝑦 ) and T( z , 𝑦 ) = 𝑝 T ( z , 𝑦 ) , separately. Assumption1. The causal mechanism 𝑝 ( y | z ) and causal representation 𝑝 ( z ) are stable under distribution shift such that 𝑝 S ( y | z = 𝜙 ( x )) = 𝑝 T ( y | z = 𝜙 ( x )) and 𝑝 S ( z = 𝜙 ( x )) = 𝑝 T ( z = 𝜙 ( x )) , if Z is sufficient cause of 𝑌 . When the invariant assumption holds, we can connect 𝐼 T ( 𝑌 ; Z ) and ˆ 𝐼 T ( 𝑌 ; Z ) by following theorem. Theorem 6.2. Let Z = 𝜙 ( X ) where 𝜙 : X → Z be a fixed arbitrary function, determined by a known conditional probability distribution 𝑝 ( z | x ) . Let 𝑚 be sample size and 𝐶 is a constant. In domain adaptation scenario, defining 𝐷 𝐾𝐿 (S||T) > 0 as the Kullback-Leibler divergence between source domain and target domain. For any confidence parameter 0 < 𝛿 < 1 , it holds with a probability of at least 1 -𝛿 , that 1. General case (The learned representation 𝑍 contains correlated information)  2. Ideal case (The learned representation Z contains information of sufficient causes of 𝑌 , Assumption 1 holds)   Remark. The theorem shows that in a domain adaptation scenario, causal representation can help to achieve better generalization ability. We bound the risk of mutual information evaluation on the target domain by the bound on the source domain. It is because, in the training process, the information from the target domain is not observable. From the bounds of | 𝐼 T ( 𝑌 ; Z ) -ˆ 𝐼 T ( 𝑌 ; Z )| shown in the general case and ideal case, we can see that the generalization error bound of the ideal case is smaller than that of the general case, with a margin quantified by a positive term 𝐷 𝐾𝐿 (S||T) > 0. These theoretical results support that the causal representation can achieve better generalization ability under distribution shift.",
  "7 EXPERIMENTS": "In this section, we conduct extensive experiments to verify the effectiveness of our framework. In the following, we begin with the experiment setup, and then report and analyze the results.",
  "7.1 Datasets": "Our experiments are based on one synthetic and four real-world benchmarks. With the synthetic dataset, we evaluate our method in a controlled manner under the selected dataset. We follow the causal graph defined in Fig.1 (a) to build our synthetic simulator, on which we compare the representation learnt by our method with the ground truth under different 𝛽 degrees.",
  "7.2 Synthetic Datasets": "The synthetic data is generated following the general causal graph Fig.1. We build the simulator using nonlinear functions refering to [44, 49]. We simulate 500 data for each settings. Let 𝜅 1 (·) and 𝜅 2 (·) as piecewise functions, and 𝜅 1 ( 𝑥 ) = 𝑥 -0 . 5 if 𝑥 > 0, otherwise 𝜅 1 ( 𝑥 ) = 0, 𝜅 2 ( 𝑥 ) = 𝑥 if 𝑥 > 0, otherwise 𝜅 2 ( 𝑥 ) = 0 and 𝜅 3 ( 𝑥 ) = 𝑥 + 0 . 5 if 𝑥 < 0, otherwise 𝜅 3 ( 𝑥 ) = 0. . For the fair evaluation, we set the same dimension for paY , ndY , dcY that 𝑑 1 = 𝑑 2 = 𝑑 3 = 5. Specify Robust Causal Representation from Mixed Observations KDD '23, August 6-10, 2023, Long Beach, CA, USA Table 1: Overall Results on Yahoo!R3-OOD, Yahoo!R3-i.i.d. and PCIC The nonlinear systems are: paY ∼ 𝑈 (- 1 , 1 ) , 𝝐 1 = 𝝐 2 = 𝝐 3 ∼ N( 0 . 3 , 𝛽𝐼 ) nd 1 = 𝒂 𝑇 𝜅 1 ( 𝜅 2 ([ paY , 𝝐 2 ])) + 𝑞, nd 2 = 𝒂 𝑇 𝜅 3 ( 𝜅 2 ([- paY , - 𝝐 2 ])) + 𝑞, ndY = 𝜎 ( nd 1 + nd 1 · nd 2 ) y 1 = 𝒂 𝑇 𝜅 1 ( 𝜅 2 ([ paY , 𝝐 1 ])) + 𝑞, y 2 = 𝒂 𝑇 𝜅 3 ( 𝜅 2 ([- paY , - 𝝐 1 ])) + 𝑞, ndY = I ( 𝜎 ( y 1 + y 1 · y 2 )) dc 1 = 𝒂 𝑇 𝜅 1 ( 𝜅 2 ([ 𝑦, 𝝐 3 ])) + 𝑞, dc 2 = 𝒂 𝑇 𝜅 3 ( 𝜅 2 ([- 𝑦, - 𝝐 3 ])) + 𝑞, dcY = 𝜎 ( dc 1 + dc 1 · dc 2 ) X = [ paY , ndY , dcY ] where 𝑞 = 0 . 3, I ( 𝑥 ) is an indicator function, which is 1 if 𝑥 > 0, and 0 otherwise. From synthetic data, we analyze whether CaRI has the ability to identify the paY from mixed observational X .",
  "7.3 Real-word benchmarks": "We also evaluate our method on real-world benchmarks for the recommendation system. Yahoo! R3 2 is an online music recommendation dataset, which contains the user survey data and ratings for randomly selected songs. The dataset contains two parts: the uniform (OOD) set and the nonuniform (i.i.d.) set. The non-uniform (OOD) set contains 2 https://webscope.sandbox.yahoo.com/catalog.php?datatype=r samples of users deliberately selected and rates the songs by preference, which can be considered as a stochastic logging policy. For the uniform (i.i.d.) set, users were asked to rate 10 songs randomly selected by the system. The dataset contains 14,877 users and 1,000 items. The density degree is 0.812%, which means that the dataset only records 0.812% of rating pairs. PCIC The dataset is collected from a survey by questionnaires about the rate and reason why the audience like or dislike the movie. Movie features are collected from movie review pages. The training data is a biased dataset consisting of 1000 users asked to rate the movies they care from 1720 movies. The validation and test set is the user preference on uniformly exposed movies. The density degree is set to be 0.241%. For evaluation, Yahoo! R3 and Coat dataset both have two validation (include test) datasets. The i.i.d. set is 1/3 of data from a nonuniform logging policy, and the OOD set consists of the data generated under a uniform policy. For the PCIC dataset, we train our method on non-uniform datasets and perform evaluations on uniform datasets. CelebA-anno The dataset contains more than 200K celebrity images, each with 40 attribute annotations. Following the previous work [15], we select 9 attribute annotations, which include Young, Male, Eyeglasses, Bald, Mustache, Smiling, Wearing Lipstick, and Mouth Open. Our task is to predict Smiling. paY including {Young, Male}, ndY including {Eyeglasses, Bald, Mustache, Wearing Lipstick} and cdY including {Mouth Open}. From this dataset, we evaluate the ability to distinguish paY from X (Results on CelebA-anno are provided in supplementary materials). KDD '23, August 6-10, 2023, Long Beach, CA, USA Mengyue Yang, Xinyu Cai, Furui Liu, Weinan Zhang, & Jun Wang Table 2: Overall Results on Coat dataset. Coat Shopping Dataset 3 is a commonly used dataset collected from web-shop ratings on clothing. The self-selected ratings are the i.i.d. set and the uniformly selected ratings are the OOD set. In the training dataset, users were asked to rate 24 coats selected by themselves from 300 item sets. The test dataset collects the user rates on 16 random items from 300 item sets. Just as Yahoo! R3, the training dataset is a non-uniform dataset and the test dataset is a uniform dataset. The dataset provides side information on both users and item sets. The feature dimension of the user/item pair is 14/33. Compared Method . For all the compared methods, we use the same model architecture, with different training strategies. The model consists of representation learning module z = 𝜙 ( x ) and the downstream prediction module ˆ y = 𝑔 ( z ) , with each module implemented by neural networks. Base model has no additional constraints on representation, and the optimization is to minimize the cross-entropy between 𝑦 and learned ˆ 𝑦 . We involve a recently proposed variational estimation with information bottleneck ( IB ) [1], extend the condition VAE (CVAE [37]) by robust training process as r-CVAE , whose objective function is similar with CaRI but without a negative term (Eq.10 (2)). We conduct ablation studies by comparing our proposed method CaRI with the r-CVAE to evaluate the effectiveness of negative term. We evaluate our method on two main aspects: (i) Generalization of the model under distribution shifts and (ii) Robustness under adversarial attack on representation space. For (i), we evaluate our method on OOD and i.i.d. setting on Yahoo! R3 and Coat. For (ii), the standard mode of adversarial attack ( 𝛽 = 0) means that we do not perturb original z . In robust mode, we set 𝛽 = { 0 . 1 , 0 . 2 , 0 . 1 , 0 . 3 , 0 . 3 } for PCIC, Yahoo! R3, Coat, Synthetic and CelebA-anno respectively. Metrics . We use the common evaluation metrics AUC/ACC [12, 29] on CTR prediction and their variants called adv-ACC/ adv-AUC [19] on adversarially perturbed evaluation dataset. Moreover, we 3 https://www.cs.cornell.edu/ schnabts/mnar/ consider Distance Correlation metrics [14] to evaluate the similarity between learned representation and parental information paY .",
  "7.4 Implementation": "Architecture and Setups : The model consists of two parts, the representation learning part and the downstream prediction part. For the representation learning part, we first use encode function 𝜙 (·) to get representation z and get the intervened ˆ z . Then we perturb the learned z and ¯ z by PGD attack [19] procedure to find the worst case corresponding to the worst downstream loss. We use PGD attack with ∞ -norm ( 𝑝 = ∞ ) and 2-norm ( 𝑝 = 2) in our implementation. Finally we put z ′ and ˆ z ′ into the downstream prediction model 𝑔 (·) to calculate 𝑦 . The likelihood in Eq.11 is estimated by cross-entropy loss. Note that the perturbation approach would block the gradient propagation between the representation learning process and downstream prediction in some implementation ways. Thus we use the conditional Gaussian prior 𝑝 𝜃 ( z ) = N( 𝑦 1 , I ) rather than standard Gaussian distribution 𝑝 𝜃 ( z ) = N( 0 , I ) to calculate KL term. If gradient propagation is blocked, by using conditional prior, the learning process of representation z and exogenous 𝝐 embedded in z ′ will not be influenced. The form of conditional Gaussian prior is more general 𝑝 𝜃 ( z ) = N( 𝜁 ( 𝑦 ) , I ) , where 𝜁 (·) could be any non-trivial function like linear function even neural network. Hyperparameter The hyper-parameters are determined by grid search. Specifically, the learning rate and batch size are tuned in the ranges of [ 10 -1 , 10 -2 , 10 -3 , 10 -4 ] and [ 64 , 128 , 256 , 512 , 1024 ] , respectively. The weighting parameter 𝜆 is tuned in [ 1 , 100 ] . Perturbation degrees are set to be 𝛽 = { 0 . 1 , 0 . 2 , 0 . 1 , 0 . 3 } for Coat, Yahoo!R3 and PCIC separately. The representation dimension is empirically set as 64. All the experiments are conducted based on a server with a 16-core CPU, 128g memories and an RTX 5000 GPU.",
  "7.5 Overall Effectiveness": "Table 1 shows the overall results on Yahoo! R3 and PCIC. From Yahoo! R3 dataset, which contains both i.i.d. and OOD validation Specify Robust Causal Representation from Mixed Observations KDD '23, August 6-10, 2023, Long Beach, CA, USA Figure 2: Representation learning results on synthetic dataset over different range of 𝛽 , where 𝑝 = 2 under robust training. 0.1 0.3 0.5 0.7 1 3 0.2 0.4 0.6 0.8 Distance Correlation Variable = pa 0.1 0.3 0.5 0.7 1 3 Variable = nd 0.1 0.3 0.5 0.7 1 3 Variable = dc Model CaRI Base and test sets, we find that our method enjoys better generalization. In Yahoo! R3 OOD, our method increases the performance by 1.9%, and 8.1%, in terms of ACC and adv-ACC, compared with the base method. The performance of r-CVAE is close to CaRI, since it is a modified version of our method, which only includes the positive term in Eq.15 but removes the negative term. The difference between the performances of CaRI and r-CVAE shows the effectiveness of the negative term in the objective function of CaRI. In PCIC dataset, standard and robust modes of CaRI achieve the best AUC at 64.47%, and 63.9% respectively, which validates the effectiveness of our idea. In the robust training mode, our method achieves the best performance in adversarial metrics. In the PCIC dataset, our method reaches 62.25%, which increases by 8.37% against the base method on adv-AUC. Robust training of CaRI is also better than the standard training, winning with a margin of around 1.42%. we present the additional test results and analysis in this section. Table 2 shows overall experimental results on Coat, The table contains both i.i.d. and OOD settings. Based on this we find that in most cases, our method achieves better performance in terms of AUC and ACC, compared to base methods. The overall results show that the robust learning process with exogenous variables involved enhances the adversarial performance on perturbed samples. On the other hand, in standard training mode, CaRI achieves better adversarial performance than baselines including base method and IB. We find that standard training of CaRI on PCIC has an AUC of 64.47%, which is better than the performance under robust training (63.9%). But contrary conclusions are drawn on adversarial performance. The result supports that the causal representation we learned is more robust. The performance of the base method in robust training mode is worst in most of cases, indicating that the robust training process will largely influence the learning of the model and ruin the prediction model. Although the robust training deteriorates the performance of on normal dataset, it will help to identify the causal representation, which benefits downstream prediction under adversarial attack. The robust learning process with exogenous variables involved enhances the adversarial performance on perturbed samples. On the other hand, in standard training mode, CaRI achieves better adversarial performance than all baselines. The result supports that the causal representation our method learned is more robust.",
  "7.6 Representation Analysis": "In this section, we study whether our method CaRI helps to identify the parental information from observational data. Fig.2 demonstrates the ability of the model to learn causal representations under different 𝛽 degrees on a synthetic dataset. The figure shows the distance correlation between the learned representation z and different parts of observational data, namely ( paY , ndY , dcY ). From Fig.2 (left), we find that our method learns a representation that is with the highest similarity, in comparison with the base method under different values of 𝛽 . It is evidence that our method successfully identifies the parental information from mixed observational data. The information from ndY and dcY are not considered as important as parental information from CaRI, and the distance correlation metric corresponding to this part is slightly lower. We also find that the metric under CaRI gets lower variance, which shows the stable performance of CaRI. On the contrary, the distance correlation metric of the base method is with high variance, which indicates the possible incapability of the base method on extracting the parental information from observations.",
  "8 CONCLUSIONS": "In this paper, we deal with the problem of learning causal representations from observational data, which comes with satisfactory generalization ability. Assuming that the underlying latent factors follow some causal generative models, we argue that learning a minimum sufficient cause of the system is the optimal solution. By analyzing the information theoretical property of our hypothetical graphical model, we propose a causality-inspired representation learning method by optimizing a function with regularized mutual information constraints. It achieves effective learning with guaranteed sample complexity reduction under certain assumptions. Extensive experiments on real-world dataset show the effectiveness of our algorithm, verifying our claim of robustness of the representation with respect to downstream tasks.",
  "9 ACKNOWLEDGEMENTS": "The work was supported by National Key R&D Program of China (2022YFB4501500, 2022YFB4501504). We thank Jianhong Wang for the fruitful discussion on some of the results and for proofreading this paper and thank Xu Chen for proofreading. We thank the anonymous reviewers for their feedback. KDD '23, August 6-10, 2023, Long Beach, CA, USA Mengyue Yang, Xinyu Cai, Furui Liu, Weinan Zhang, & Jun Wang",
  "REFERENCES": "[1] Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. 2016. Deep variational information bottleneck. arXiv preprint arXiv:1612.00410 (2016). [2] Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. 2019. Invariant risk minimization. arXiv preprint arXiv:1907.02893 (2019). [3] Nihat Ay and Daniel Polani. 2008. Information Flows in Causal Networks. Adv. Complex Syst. 11, 1 (2008), 17-41. https://doi.org/10.1142/S0219525908001465 [4] Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, R. Devon Hjelm, and Aaron C. Courville. 2018. Mutual Information Neural Estimation. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018 (Proceedings of Machine Learning Research) , Jennifer G. Dy and Andreas Krause (Eds.), Vol. 80. PMLR, 530-539. http://proceedings.mlr.press/v80/belghazi18a. html [5] Aharon Ben-Tal, Laurent El Ghaoui, and Arkadi Nemirovski. 2009. Robust optimization . Princeton university press. [6] Battista Biggio and Fabio Roli. 2018. Wild patterns: Ten years after the rise of adversarial machine learning. Pattern Recognition 84 (2018), 317-331. [7] Pengyu Cheng, Weituo Hao, Shuyang Dai, Jiachang Liu, Zhe Gan, and Lawrence Carin. 2020. CLUB: A Contrastive Log-ratio Upper Bound of Mutual Information. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event (Proceedings of Machine Learning Research) , Vol. 119. PMLR, 1779-1788. http://proceedings.mlr.press/v119/cheng20b.html [8] Pengyu Cheng, Weituo Hao, Shuyang Dai, Jiachang Liu, Zhe Gan, and Lawrence Carin. 2020. Club: A contrastive log-ratio upper bound of mutual information. In International conference on machine learning . PMLR, 1779-1788. [9] Thomas M Cover. 1999. Elements of information theory . John Wiley & Sons. [10] Ronald A Fisher. 1922. On the mathematical foundations of theoretical statistics. Philosophical transactions of the Royal Society of London. Series A, containing papers of a mathematical or physical character 222, 594-604 (1922), 309-368. [11] Mingming Gong, Kun Zhang, Tongliang Liu, Dacheng Tao, Clark Glymour, and Bernhard Schölkopf. 2016. Domain adaptation with conditional transferable components. In International conference on machine learning . PMLR, 2839-2848. [12] Asela Gunawardana and Guy Shani. 2009. A survey of accuracy evaluation metrics of recommendation tasks. Journal of Machine Learning Research 10, 12 (2009). [13] Dominik Janzing and Bernhard Schölkopf. 2010. Causal inference using the algorithmic Markov condition. IEEE Transactions on Information Theory 56, 10 (2010), 5168-5194. [14] Terry Jones, Stephanie Forrest, et al. 1995. Fitness Distance Correlation as a Measure of Problem Difficulty for Genetic Algorithms.. In ICGA , Vol. 95. 184-192. [15] Murat Kocaoglu, Christopher Snyder, Alexandros G Dimakis, and Sriram Vishwanath. 2017. Causalgan: Learning causal implicit generative models with adversarial training. arXiv preprint arXiv:1709.02023 (2017). [16] Erich Leo Lehmann and Henry Scheffé. 2012. Completeness, similar regions, and unbiased estimation-Part I. In Selected Works of EL Lehmann . Springer, 233-268. [17] Ya Li, Mingming Gong, Xinmei Tian, Tongliang Liu, and Dacheng Tao. 2018. Domain generalization via conditional invariant representations. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 32. [18] Chaochao Lu, Yuhuai Wu, Jośe Miguel Hernández-Lobato, and Bernhard Schölkopf. 2021. Nonlinear invariant risk minimization: A causal approach. arXiv preprint arXiv:2102.12353 (2021). [19] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. 2017. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 (2017). [20] Sara Magliacane, Thijs van Ommen, Tom Claassen, Stephan Bongers, Philip Versteeg, and Joris M Mooij. 2017. Domain adaptation by using causal inference to predict invariant conditional distributions. arXiv preprint arXiv:1707.06422 (2017). [21] Nicolai Meinshausen. 2018. Causality from a distributional robustness point of view. In 2018 IEEE Data Science Workshop (DSW) . IEEE, 6-10. [22] Joris M Mooij, Jonas Peters, Dominik Janzing, Jakob Zscheischler, and Bernhard Schölkopf. 2016. Distinguishing cause from effect using observational data: methods and benchmarks. The Journal of Machine Learning Research 17, 1 (2016), 1103-1204. [23] Shumpei Okura, Yukihiro Tagami, Shingo Ono, and Akira Tajima. 2017. Embedding-based news recommendation for millions of users. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . 1933-1942. [24] Victor M Panaretos and Yoav Zemel. 2019. Statistical aspects of Wasserstein distances. Annual review of statistics and its application 6 (2019), 405-431. [25] Giambattista Parascandolo, Niki Kilbertus, Mateo Rojas-Carulla, and Bernhard Schölkopf. 2018. Learning independent causal mechanisms. In International Conference on Machine Learning . PMLR, 4036-4044. [26] Judea Pearl. 2009. Causality . Cambridge university press. [27] Jonas Peters, Peter Bühlmann, and Nicolai Meinshausen. 2016. Causal inference by using invariant prediction: identification and confidence intervals. Journal of the Royal Statistical Society. Series B (Statistical Methodology) (2016), 947-1012. [28] Aahlad Puli, Adler Perotte, and Rajesh Ranganath. 2020. Causal estimation with functional confounders. Advances in neural information processing systems 33 (2020), 5115-5125. [29] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2012. BPR: Bayesian personalized ranking from implicit feedback. arXiv preprint arXiv:1205.2618 (2012). [30] Mateo Rojas-Carulla, Bernhard Schölkopf, Richard Turner, and Jonas Peters. 2018. Invariant models for causal transfer learning. The Journal of Machine Learning Research 19, 1 (2018), 1309-1342. [31] Bernhard Schölkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal, and Yoshua Bengio. 2021. Toward causal representation learning. Proc. IEEE 109, 5 (2021), 612-634. [32] Shai Shalev-Shwartz and Shai Ben-David. 2014. Understanding machine learning: From theory to algorithms . Cambridge university press. [33] Ohad Shamir, Sivan Sabato, and Naftali Tishby. 2010. Learning and generalization with the information bottleneck. Theoretical Computer Science 411, 29-30 (2010), 2696-2711. [34] Xinwei Shen, Furui Liu, Hanze Dong, Qing Lian, Zhitang Chen, and Tong Zhang. 2020. Disentangled generative causal representation learning. arXiv preprint arXiv:2010.02637 (2020). [35] Zheyan Shen, Jiashuo Liu, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, and Peng Cui. 2021. Towards Out-Of-Distribution Generalization: A Survey. arXiv preprint arXiv:2108.13624 (2021). [36] Chuan Shi, Binbin Hu, Wayne Xin Zhao, and S Yu Philip. 2018. Heterogeneous information network embedding for recommendation. IEEE Transactions on Knowledge and Data Engineering 31, 2 (2018), 357-370. [37] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. 2015. Learning structured output representation using deep conditional generative models. Advances in neural information processing systems 28 (2015), 3483-3491. [38] Sumedh A Sontakke, Arash Mehrjou, Laurent Itti, and Bernhard Schölkopf. 2021. Causal curiosity: RL agents discovering self-supervised experiments for causal representation learning. In International Conference on Machine Learning . PMLR, 9848-9858. [39] Bastian Steudel, Dominik Janzing, and Bernhard Schölkopf. 2010. Causal Markov condition for submodular information measures. arXiv preprint arXiv:1002.4020 (2010). [40] Zhu Sun, Jie Yang, Jie Zhang, Alessandro Bozzon, Long-Kai Huang, and Chi Xu. 2018. Recurrent knowledge graph embedding for effective recommendation. In Proceedings of the 12th ACM Conference on Recommender Systems . 297-305. [41] Raphael Suter, Djordje Miladinovic, Bernhard Schölkopf, and Stefan Bauer. 2019. Robustly disentangled causal mechanisms: Validating deep representations for interventional robustness. In International Conference on Machine Learning . PMLR, 6056-6065. [42] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. 2013. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199 (2013). [43] Yixin Wang and Michael I Jordan. 2021. Desiderata for Representation Learning: A Causal Perspective. arXiv preprint arXiv:2109.03795 (2021). [44] Mengyue Yang, Quanyu Dai, Zhenhua Dong, Xu Chen, Xiuqiang He, and Jun Wang. 2021. Top-N Recommendation with Counterfactual User Preference Simulation. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management . 2342-2351. [45] Mengyue Yang, Furui Liu, Zhitang Chen, Xinwei Shen, Jianye Hao, and Jun Wang. 2021. CausalVAE: disentangled representation learning via neural structural causal models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 9593-9602. [46] Kun Zhang, Mingming Gong, and Bernhard Schölkopf. 2015. Multi-source domain adaptation: A causal view. In Twenty-ninth AAAI conference on artificial intelligence . [47] Yongfeng Zhang, Qingyao Ai, Xu Chen, and W Bruce Croft. 2017. Joint representation learning for top-n recommendation with heterogeneous information sources. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management . 1449-1458. [48] Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. 2021. Domain generalization: A survey. arXiv preprint arXiv:2103.02503 (2021). [49] Hao Zou, Kun Kuang, Boqi Chen, Peixuan Chen, and Peng Cui. 2019. Focused Context Balancing for Robust Offline Policy Evaluation. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2019, Anchorage, AK, USA, August 4-8, 2019 , Ankur Teredesai, Vipin Kumar, Ying Li, Rómer Rosales, Evimaria Terzi, and George Karypis (Eds.). ACM, 696-704. https://doi.org/10.1145/3292500.3330852 Specify Robust Causal Representation from Mixed Observations KDD '23, August 6-10, 2023, Long Beach, CA, USA",
  "9.1 Proof of Theorem 4.1": "The proof directly follows the following two lemmas. We denote the set of probabilistic functions of X into an arbitrary target space as F( X ) , and as S( 𝑌 ) the set of sufficient statistics for 𝑌 . Since h (·) is a function of combination , we have 𝐼 ( 𝑌 ; paY , ndY , dcY ) = 𝐼 ( 𝑌 ; X ) Lemma 9.1. Let Z be a probabilistic function of X and X = h ( paY , ndY , dcY ) , where h is function of combination. Then Z is a sufficient causes for 𝑌 if and only if  Proof. Lemma is an extension of Lemma 12 in [33]. The differences lie on that X is consist of paY , ndY , dcY and we focus on the sufficient causes defined in Definition 2. Firstly for the sufficient condition, for every Z ′ which is a probabilistic function of X , we have Markov Chain 𝑌 -X -Z ′ , so from data processing inequality [9] we have 𝐼 ( 𝑌 ; X ) ≥ 𝐼 ( 𝑌 ; Z ′ ) . Therefore we have 𝐼 ( 𝑌 ; Z ) = max Z ′ ∈F( X ) 𝐼 ( 𝑌, Z ′ ) . We also have Markov Chain 𝑌 --, for the data processing inequality we have 𝐼 ( 𝑌 ; ) ≤ 𝐼 ( 𝑌 ; ) . Thus 𝐼 ( 𝑌 ; ) max 𝐼 ( 𝑌, ′ ) is held. Then for necessary condition, assume that we have a Markov Chain 𝑌 -X -Z . According to data processing inequality, the 𝐼 ( 𝑌 ; Z ) = 𝐼 ( 𝑌 ; X ) holds if and only if 𝐼 ( 𝑌 ; X | Z ) = 0. Since X is consist of paY , ndY , dcY , the 𝐼 ( 𝑌 ; paY , ndY | Z ) = 0. in other word, 𝑌 and paY , ndY independent by Z , hence Z is a sufficient causes satisfied Definition 2. Z X X Z Z = Z ′ ∈F( X ) Z are conditionally □ Lemma 9.2. Let Z be sufficient statistics of 𝑌 and X = h ( paY , ndY , dcY ) , where h is function of combination. Then Z is minimal sufficient causes for Y if and only if  Proof. Firstly, for the sufficient condition, let Z be a minimal sufficient causes, and Z ′ be some sufficient causes. Because there is a function Z = 𝑓 ( Z ′ ) from Definition 3, it has Markov Chain ( ndY , paY ) -𝑌 -Z ′ -Z , and we get 𝐼 ( ndY , paY ; Z ) ≤ 𝐼 ( ndY , paY ; Z ′ ) . So that 𝐼 ( ndY , paY ; Z ) = min Z ′ ∈S( 𝑌 ) 𝐼 ( ndY , paY ; Z ′ ) holds. For the necessary condition, assume that Z is not minimal, then there exist another sufficient statistics V allows 𝐼 ( ndY , paY ; Z ) > 𝐼 ( ndY , paY ; V ) and let V : X → Z is a function of X such that ∀ x , V ( x ) ∈ { z | z ∼ Z ( x )} . Inspired by Fisher-Neyman factorization theorem [10], we can factorize 𝑝 ( x ) as below  In above equation since dcY is decided by paY and 𝑌 . We can drop the dcy by defining 𝑙 3 Z ( dcy | pay ) ≜ 𝑙 3 V ( dcy | pay ) and we can rewrite the sufficient causes condition as below  We define a equivalence relation ∼ by  There exists a sufficient cause Z ′ such that Z is not a function of Z ′ . The following process proof that V is also sufficient cause of 𝑌 :  Then  Since above equation holds, V have factorization formulation of sufficient statistics, V is also a sufficient statistic. Let x1 , x2 such that Z ′ ( x1 ) = Z ′ ( x2 ) , then 𝑙 2 Z ′ ( Z ′ ( x1 ) , 𝑦 ) = 𝑙 2 Z ′ ( Z ′ ( x2 ) , 𝑦 )  KDD '23, August 6-10, 2023, Long Beach, CA, USA Mengyue Yang, Xinyu Cai, Furui Liu, Weinan Zhang, & Jun Wang From the above equation we can get Z ( x1 ) = Z ( x2 ) , then we have V ( x1 ) = V ( x2 ) because V is function of Z ′ . Since Z is sufficient cause of 𝑌 , and paY , ndY ⊥ 𝑌 | Z in Definition 1 holds. There exists Markov Chains X -Z -V and ( paY , ndY ) -Z -V . From data processing inequality, 𝐼 ( ndY , paY ; Z ) ≥ 𝐼 ( ndY , paY ; V ) . The term 𝐼 ( ndY , paY ; Z ) can be decomposed as below  Since Z ′ is not the function of Z , thus 𝐼 ( ndY , paY ; Z | Z ′ ) > 0, therefore we have 𝐼 ( ndY , paY ; Z ) > 𝐼 ( ndY , paY ; V ) . Thus Eq. 19 does not hold if Z is not minimal. The proof completes. □",
  "9.2 Proof of Proposition 4.3": "Proof. Under the assumption that Z block the path between X and dcY , X and dcY are conditional independent by variable Z . X = ℎ ( paY , ndY , dcY ) = ℎ ( paY , ndY , Z ) = ℎ ( paY , ndY ) . Since all generative function of factors are invertible, we can replace ( ndY , dcY ) in Markov Chain shown in the proof of Theorem 4.1 by variable X . Therefore, 𝑝 ( 𝑦 | z , pay , ndy ) = 𝑝 ( 𝑦 | z ) is held if and only if 𝑝 ( 𝑦 | z , x ) = 𝑝 ( 𝑦 | z ) holds. Thus, under the the assumption that Z block the path between X and dcY and ℎ is a linear invertible function, the optimization processes defined in Proposition 4.3 and Theorem 4.1 are equivalence. □",
  "9.3 Proof of Theorem 6.1": "The proof follows [33] Theorem 3. The sketch of proof contains two steps: (i) we decompose the original objective | 𝐼 ( 𝑌 ; Z ) -ˆ 𝐼 ( 𝑌 ; Z )| into two parts. (ii) for each part, we deduce the deterministic finite sample bound by concentration of measure arguments on L2 norms of random vector. Let 𝐻 ( 𝑋 ) denote the entropy of 𝑋 , we have  Let 𝜁 ( 𝑥 ) denote a continuous, monotonically increasing and concave function.   for the term | 𝐻 ( 𝑌 | 𝑍 ) -ˆ 𝐻 ( 𝑌 | 𝑍 )|  For the first summand in this bound, we introduce variable 𝝐 to help decompose 𝑝 ( 𝑦 | z ) , where 𝝐 is independent with the parents pay (i.e. 𝝐 ⊥ pay )  where 1 𝑚 𝑉 ( 𝑥 ) denote the variance of vector 𝑥 . For the second summand in Eq.26.  Specify Robust Causal Representation from Mixed Observations KDD '23, August 6-10, 2023, Long Beach, CA, USA For the summand | 𝐻 ( 𝑌 ) -ˆ 𝐻 ( 𝑌 )| :  Combining above bounds:  Let 𝝆 be a distribution vector of arbitrary cardinality, and let ˆ 𝝆 be an empirical estimation of 𝝆 based on a sample of size 𝑚 . Then the error ∥ 𝝆 -ˆ 𝝆 ∥ will be bounded with a probability of at least 1 -𝛿  Following the proof of Theorem 3 in [33], to make sure the bounds hold over |Y| + 2 | quantities, we replace 𝛿 in Eq.31 by 𝛿 /(|Y| + 2 | , than substitute ∥ p ( z , 𝑦 | 𝝐 ) -ˆ p ( z , 𝑦 | 𝝐 )∥ ∥ ˆ p ( 𝑦 | z , 𝝐 ) -p ( 𝑦 | z , 𝝐 )∥ , ∥ p ( z ) -ˆ p ( z )∥ , by Eq.31.  There exist a constant 𝐶 , where 2 + √︁ 2 log ((|Y| + 2 )/ 𝛿 ) ≤ √︁ 𝐶 log ((|Y|)/ 𝛿 ) . From the fact that variance of any random variable bounded in [0, 1] is at most 1/4, we analyze the bound under two different cases: In general case ( z = 𝜙 ( x ) is arbitrary representation of x ),  let 𝑚 denote the number of sample, we get a lower bound of 𝑚 , which is also known as sample complexity.  In ideal case ( z is sufficient cause of x ) in that case z is independent with the exogenous noise 𝝐 , z ⊥ 𝝐 :     Then, from the fact that ([33]): KDD '23, August 6-10, 2023, Long Beach, CA, USA Mengyue Yang, Xinyu Cai, Furui Liu, Weinan Zhang, & Jun Wang We can get the upper bound of second summand in Eq.43 as follows  In general case : In ideal case :  For the first summand in Eq.43, we follow the fact ([33] Theorem 3) that:  Finally we accomplish the proof of Theorem 6.1.",
  "9.4 Extension of Theorem 6.1 for distribution shift": "Proof. The risk under target domain is defined as | 𝐼 T ( 𝑌 ; Z ) -ˆ 𝐼 T ( 𝑌 ; Z )| , the proof is start by the following equation shown in the proof of Theorem 6.1.  We will then bound the term 𝑉 ( ˆ H T ( 𝑌 | z )) by the variance of entropy on source data. From the definition of function 𝑉 , we have  Supposing that only source data S( X , 𝑌 ) is available, for the term ˆ 𝐼 T ( Z ; 𝑌 ) evaluated on target dataset, we change the measure by important sampling and Jensen's inequality. The way helps bound ˆ 𝐼 T ( Z ; 𝑌 ) by the evaluation on source domain. Denoting 𝐷 𝐾𝐿 ( 𝑃 | | 𝑄 ) by the KullbackLeibler divergence between distribution 𝑃 and 𝑄 . S( z , 𝑦 ) and T( z , 𝑦 ) are the distribution of 𝑝 ( z , 𝑌 ) on source domain and target domain separately.  Substituting ˆ 𝐼 T ( Z , 𝑌 ) into Eq. 44 and Eq. 43. Since |Z| > 1, let 𝐷 = 2 min z 𝑝 ( z ) and 𝐼 S = E S( z ,𝑦 ) ˆ 𝑝 ( z ,𝑦 ) ˆ 𝑝 ( z ) ˆ 𝑝 ( 𝑦 ) . We can get the bounds under two different cases:  Specify Robust Causal Representation from Mixed Observations KDD '23, August 6-10, 2023, Long Beach, CA, USA In general case , since since Z is arbitrary representation of X , we get 𝐷 𝐾𝐿 (T ( z , 𝑦 )||S( z , 𝑦 )) > 0. We cannot drop the 𝐷 𝐾𝐿 term. Thus we have the bound:  In ideal case , since Z is sufficient cause of 𝑌 , we get 𝐷 𝐾𝐿 (T ( z , 𝑦 )||S( z , 𝑦 )) = 0 from Assumption 1.  □",
  "10 EXPERIMENTAL DETAILS": "",
  "10.1 Model Architecture and Implementation Details": "The hyper-parameters are determined by grid search. Specifically, the learning rate and batch size are tuned in the ranges of [ 10 -4 , 10 -1 ] and [ 64 , 128 , 256 , 512 , 1024 ] , respectively. The weighting parameter 𝜆 is tuned in [ 0 . 001 ] . Perturbation degrees are set to be 𝛽 = { 0 . 1 , 0 . 2 , 0 . 1 , 0 . 3 } for Coat, Yahoo!R3, PCIC and CPC separately. The representation dimension is empirically set as 64. All the experiments are conducted based on a server with a 16-core CPU, 128g memories and an RTX 5000 GPU. The deep model architecture is shown as follows: (1)Representation learning method 𝜙 ( x ) : If the dataset is Yahoo!R3 or PCIC, in which only the user id and item id are the input, we first use an embedding layer. The representation function architecture is: · Concat(Embedding(user id, 32), Embedding(item id, 32)) · Linear(64, 64), ELU() · Linear(64, representation dim), ELU() Then for the dataset Coat and CPC, the feature dimension is 29 and 47 separately. It do not use embedding layer at first. The representation function architecture is. · Linear(64, 64), ELU() · Linear(64, representation dim), ELU() (2)Downstream Prediction Model 𝑔 ( z ) : · Linear(representation dim, 64), ELU() · Linear(64, 2)",
  "𝑅𝑒𝑝𝑟𝑒𝑠𝑒𝑛𝑡𝑎𝑡𝑖𝑜𝑛\t𝐿𝑒𝑎𝑟𝑛𝑖𝑛𝑔": "𝐷𝑜𝑤𝑛𝑠𝑡𝑟𝑒𝑎𝑚\t𝑃𝑟𝑒𝑑𝑖𝑐𝑡𝑖𝑜𝑛 𝑥 𝑧 𝑧̅ 𝑒𝑛𝑐𝑜𝑑𝑒:𝜙 𝑖𝑛𝑡𝑒𝑟𝑣𝑒𝑛𝑒: ±𝑏 𝑃𝐺𝐷\t𝑎𝑡𝑡𝑎𝑐𝑘 𝑧′ 𝑧̅′ 𝑦 ' min𝐾𝐿(𝑞(𝑧)||𝑝 𝑧 ) min𝑙 !\" (𝑦, 𝑦 B) 𝑑𝑒𝑐𝑜𝑑𝑒:𝑔 𝑦 ' 𝑚𝑖𝑛 -𝑙 !\" (𝑦,𝑦 B) 𝑑𝑒𝑐𝑜𝑑𝑒:𝑔 𝑟𝑎𝑛𝑑𝑜𝑚\t𝑝𝑒𝑟𝑡𝑢𝑟𝑏 Figure 3: The figure demonstrates the model architecture of CaRI KDD '23, August 6-10, 2023, Long Beach, CA, USA Mengyue Yang, Xinyu Cai, Furui Liu, Weinan Zhang, & Jun Wang",
  "11 ADDITIONAL RESULTS": "Due to the page limit in main text, we present the additional test results and analysis in this section. Table 3 overall results with standard error via 5 tims runs. Fig. 5 and 6 compare the distance correlation metric given by the training under standard and robust mode. It shows that our method performs consistently better compared with base methods in both modes, with a higher distance correlation, under smaller variance. The gap is obvious especially in the learning of parental information, which is the main focus of our approaches. Fig. 7 and 8 record the results along optimization process and until convergence, under different settings of the pertubation degree 𝛽 , considering the dataset CelebA-anno. The annotation smile is used as the label to be prediced, and other features are the source data. It shows that when the optimization process is not finished, both approaches have similar performance, with unstability evidenced by large variance of the DC metric. However, our method outperforms the baseline when the optimization converges, owning a higher DC with smaller variations. The results also show that 𝛽 is an important factor for training the model. Larger 𝛽 often leads to higher variance of the training of the model. Fig.4 demonstrates how robust training degree ( 𝛽 = { 0 . 1 , 0 . 3 , 0 . 5 , 0 . 7 , 1 . 0 } ) influences the downstream prediction under adversarial settings. We conduct the experiments on the attacked real-world dataset by PGD attacker. From Fig.4, we find that our method is better than base method, because the base model's ability on standard prediction is broken by adversarial training. When 𝛽 is small, our method behaves closely to the r-CVAE in all the datasets. When 𝛽 gets larger, the difference between performance of CaRI and that of r-CVAE continuously enlarges in Yahoo!R3. In PCIC, the gap becomes the largest among all when 𝛽 = 0 . 5, and narrows down to 0 when 𝛽 = 0 . 7. This is because in our framework, we explicitly deploy a model to achieve more robust representations, while others fail. Figure 4: Results under different adversarial perturbations 𝛽 on three datasets. Axis-x is the attack degree 𝛽 . Axis-y is the adv-AUC under attacked test datasets. 0.2 0.4 0.6 0.8 1.0 0.45 0.50 0.55 0.60 0.65 0.70 0.75 data = Yahoo!R3-IID 0.2 0.4 0.6 0.8 1.0 data = Yahoo!R3-OOD 0.2 0.4 0.6 0.8 1.0 data = PCIC train_mode s-BASE r-CVAE r-BASE CaR I norm 2.0 inf",
  "11.1 Future Works": "For future works, one promising direction is to involve the concept of Kolmogorov complexity in information theory. Different to mutual information and information entropy, Kolmogorov complexity is an asymmetric notion. Based on such a concept, we can develop a causal representation learning method without introducing an intervention network. Another direction is that our proposed method can be generalized to a mixture of anti-causal and causal learning frameworks where observation data contains both parents and descendants of outcome label 𝑌 . The information-theoretic-based sample complexity theorem can inspire the generalization error/risk analysis on causal representation learning and causal structure learning. Lastly, this paper is based on the assumption of the given causal graph Fig. 1. In the future, it is interesting to extend our method to more complex scenarios like sequential prediction, reinforcement learning etc. Specify Robust Causal Representation from Mixed Observations KDD '23, August 6-10, 2023, Long Beach, CA, USA Table 3: Additional overall results with standard error. KDD '23, August 6-10, 2023, Long Beach, CA, USA Mengyue Yang, Xinyu Cai, Furui Liu, Weinan Zhang, & Jun Wang Figure 5: Identify results on synthetic dataset over different range of 𝛽 under robust training. 0.2 0.4 0.6 0.8 Distance Correlation norm = inf | Variable = pa norm = inf | Variable = nd norm = inf | Variable = dc 0.1 0.3 0.5 0.7 1 3 0.2 0.4 0.6 0.8 Distance Correlation norm = 2 | Variable = pa 0.1 0.3 0.5 0.7 1 3 norm = 2 | Variable = nd 0.1 0.3 0.5 0.7 1 3 norm = 2 | Variable = dc Model CaRI Base Figure 6: Identify results on synthetic dataset over different range of 𝛽 under standard training. 0.2 0.4 0.6 0.8 Distance Correlation norm = inf | Variable = pa norm = inf | Variable = nd norm = inf | Variable = dc 0.1 0.3 0.5 0.7 1 3 0.2 0.4 0.6 0.8 Distance Correlation norm = 2 | Variable = pa 0.1 0.3 0.5 0.7 1 3 norm = 2 | Variable = nd 0.1 0.3 0.5 0.7 1 3 norm = 2 | Variable = dc Model CaRI Base Specify Robust Causal Representation from Mixed Observations KDD '23, August 6-10, 2023, Long Beach, CA, USA Figure 7: Identify results on CelebA-anno dataset over different range of 𝛽 during early optimization step. 0.0 0.2 0.4 0.6 0.8 1.0 Distance Correlation norm = inf | = 0.0 norm = inf | = 0.1 norm = inf | = 0.3 norm = inf | = 0.7 pa nd Variable 0.0 0.2 0.4 0.6 0.8 1.0 Distance Correlation norm = 2 | = 0.0 pa nd Variable norm = 2 | = 0.1 pa nd Variable norm = 2 | = 0.3 pa nd Variable norm = 2 | = 0.7 Model CaRI Base Figure 8: Identify results on CelebA-anno dataset over different range of 𝛽 after converging. 0.0 0.1 0.2 0.3 0.4 Distance Correlation norm = inf | = 0.0 norm = inf | = 0.1 norm = inf | = 0.3 norm = inf | = 0.7 pa nd Variable 0.0 0.1 0.2 0.3 0.4 Distance Correlation norm = 2 | = 0.0 pa nd Variable norm = 2 | = 0.1 pa nd Variable norm = 2 | = 0.3 pa nd Variable norm = 2 | = 0.7 Model CaRI Base",
  "keywords_parsed": [
    "Causal representation learning",
    "Robustness learning",
    "Learning theory for Causal representation learning."
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "Deep variational information bottleneck"
    },
    {
      "ref_id": "b2",
      "title": "Invariant risk minimization"
    },
    {
      "ref_id": "b3",
      "title": "Information Flows in Causal Networks"
    },
    {
      "ref_id": "b4",
      "title": "Mutual Information Neural Estimation"
    },
    {
      "ref_id": "b5",
      "title": "Robust optimization"
    },
    {
      "ref_id": "b6",
      "title": "Wild patterns: Ten years after the rise of adversarial machine learning"
    },
    {
      "ref_id": "b7",
      "title": "CLUB: A Contrastive Log-ratio Upper Bound of Mutual Information"
    },
    {
      "ref_id": "b8",
      "title": "Club: A contrastive log-ratio upper bound of mutual information"
    },
    {
      "ref_id": "b9",
      "title": "Elements of information theory"
    },
    {
      "ref_id": "b10",
      "title": "On the mathematical foundations of theoretical statistics"
    },
    {
      "ref_id": "b11",
      "title": "Domain adaptation with conditional transferable components"
    },
    {
      "ref_id": "b12",
      "title": "A survey of accuracy evaluation metrics of recommendation tasks"
    },
    {
      "ref_id": "b13",
      "title": "Causal inference using the algorithmic Markov condition"
    },
    {
      "ref_id": "b14",
      "title": "Fitness Distance Correlation as a Measure of Problem Difficulty for Genetic Algorithms"
    },
    {
      "ref_id": "b15",
      "title": "Causalgan: Learning causal implicit generative models with adversarial training"
    },
    {
      "ref_id": "b16",
      "title": "Completeness, similar regions, and unbiased estimation-Part I"
    },
    {
      "ref_id": "b17",
      "title": "Domain generalization via conditional invariant representations"
    },
    {
      "ref_id": "b18",
      "title": "Nonlinear invariant risk minimization: A causal approach"
    },
    {
      "ref_id": "b19",
      "title": "Towards deep learning models resistant to adversarial attacks"
    },
    {
      "ref_id": "b20",
      "title": "Domain adaptation by using causal inference to predict invariant conditional distributions"
    },
    {
      "ref_id": "b21",
      "title": "Causality from a distributional robustness point of view"
    },
    {
      "ref_id": "b22",
      "title": "Distinguishing cause from effect using observational data: methods and benchmarks"
    },
    {
      "ref_id": "b23",
      "title": "Embedding-based news recommendation for millions of users"
    },
    {
      "ref_id": "b24",
      "title": "Statistical aspects of Wasserstein distances"
    },
    {
      "ref_id": "b25",
      "title": "Learning independent causal mechanisms"
    },
    {
      "ref_id": "b26",
      "title": "Causality"
    },
    {
      "ref_id": "b27",
      "title": "Causal inference by using invariant prediction: identification and confidence intervals"
    },
    {
      "ref_id": "b28",
      "title": "Causal estimation with functional confounders"
    },
    {
      "ref_id": "b29",
      "title": "BPR: Bayesian personalized ranking from implicit feedback"
    },
    {
      "ref_id": "b30",
      "title": "Invariant models for causal transfer learning"
    },
    {
      "ref_id": "b31",
      "title": "Toward causal representation learning"
    },
    {
      "ref_id": "b32",
      "title": "Understanding machine learning: From theory to algorithms"
    },
    {
      "ref_id": "b33",
      "title": "Learning and generalization with the information bottleneck"
    },
    {
      "ref_id": "b34",
      "title": "Disentangled generative causal representation learning"
    },
    {
      "ref_id": "b35",
      "title": "Towards Out-Of-Distribution Generalization: A Survey"
    },
    {
      "ref_id": "b36",
      "title": "Heterogeneous information network embedding for recommendation"
    },
    {
      "ref_id": "b37",
      "title": "Learning structured output representation using deep conditional generative models"
    },
    {
      "ref_id": "b38",
      "title": "Causal curiosity: RL agents discovering self-supervised experiments for causal representation learning"
    },
    {
      "ref_id": "b39",
      "title": "Causal Markov condition for submodular information measures"
    },
    {
      "ref_id": "b40",
      "title": "Recurrent knowledge graph embedding for effective recommendation"
    },
    {
      "ref_id": "b41",
      "title": "Robustly disentangled causal mechanisms: Validating deep representations for interventional robustness"
    },
    {
      "ref_id": "b42",
      "title": "Intriguing properties of neural networks"
    },
    {
      "ref_id": "b43",
      "title": "Desiderata for Representation Learning: A Causal Perspective"
    },
    {
      "ref_id": "b44",
      "title": "Top-N Recommendation with Counterfactual User Preference Simulation"
    },
    {
      "ref_id": "b45",
      "title": "CausalVAE: disentangled representation learning via neural structural causal models"
    },
    {
      "ref_id": "b46",
      "title": "Multi-source domain adaptation: A causal view"
    },
    {
      "ref_id": "b47",
      "title": "Joint representation learning for top-n recommendation with heterogeneous information sources"
    },
    {
      "ref_id": "b48",
      "title": "Domain generalization: A survey"
    },
    {
      "ref_id": "b49",
      "title": "Focused Context Balancing for Robust Offline Policy Evaluation"
    }
  ]
}