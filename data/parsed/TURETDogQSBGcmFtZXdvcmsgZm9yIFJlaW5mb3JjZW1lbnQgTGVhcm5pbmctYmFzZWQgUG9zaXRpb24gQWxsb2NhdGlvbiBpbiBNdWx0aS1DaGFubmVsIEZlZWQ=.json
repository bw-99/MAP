{"MDDL: A Framework for Reinforcement Learning-based Position Allocation in Multi-Channel Feed": "Xiaowen Shi \u2217 Meituan Beijing, China shixiaowen03@meituan.com Ze Wang \u2217\u2020 Meituan Beijing, China wangze18@meituan.com Yuanying Cai \u2021 IIIS, Tsinghua University Beijing, China yuanying.cc@gmail.com Xiaoxu Wu Meituan Beijing, China wuxiaoxu04@meituan.com Fan Yang Meituan Beijing, China yangfan129@meituan.com Guogang Liao Meituan Beijing, China liaoguogang@meituan.com Yongkang Wang Meituan Beijing, China wangyongkang03@meituan.com Xingxing Wang Meituan Beijing, China wangxingxing04@meituan.com Dong Wang Meituan Beijing, China wangdong07@meituan.com", "ABSTRACT": "Nowadays, the mainstream approach in position allocation system is to utilize a reinforcement learning model to allocate appropriate locations for items in various channels and then mix them into the feed. There are two types of data employed to train reinforcement learning (RL) model for position allocation, named strategy data and random data. Strategy data is collected from the current online model, it suffers from an imbalanced distribution of stateaction pairs, resulting in severe overestimation problems during training. On the other hand, random data offers a more uniform distribution of state-action pairs, but is challenging to obtain in industrial scenarios as it could negatively impact platform revenue and user experience due to random exploration. As the two types of data have different distributions, designing an effective strategy to leverage both types of data to enhance the efficacy of the RL model training has become a highly challenging problem. In this study, we propose a framework named M ultiD istribution D ata L earning ( MDDL ) to address the challenge of effectively utilizing both strategy and random data for training RL models on mixed multi-distribution data. Specifically, MDDL incorporates a novel imitation learning signal to mitigate overestimation problems in strategy data and maximizes the RL signal for random data to facilitate effective learning. In our experiments, we evaluated the proposed MDDL framework in a real-world position allocation system and demonstrated its superior performance compared to the \u2217 Equal contribution. Listing order is random. \u2020 Corresponding author. \u2021 This work was done when Yuanying Cai was an intern in Meituan. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '23, July 23-27, 2023, Taipei, Taiwan. \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn previous baseline. MDDL has been fully deployed on the Meituan food delivery platform and currently serves over 300 million users.", "KEYWORDS": "Reinforcement Learning; Multi-Distribution Data Learning; Position Allocation", "1 INTRODUCTION": "In the e-commerce scenario, a diverse array of items from different channels are aggregated and presented to users in feed [17, 20]. As shown in Figure 1, the Meituan food delivery platform mainly provides two formats for presenting items, specifically graphic-text and video formats. Determining an optimal allocation of graphic-text and video positions that accommodates users' individual preferences while maximizing platform benefits has emerged as a critical research question. There exist two primary strategies for determining the display positions of items, namely the fixed position strategy [1, 8, 14] and the dynamic position strategy [9, 10, 18, 19, 21-23]. The fixed position strategy involves allocating video items to predetermined positions. This strategy fails to account for personalized user information and is thus suboptimal. The dynamic allocation strategy, on the other hand, approaches the problem as a Markov Decision Process (MDP) [15], which is subsequently solved via reinforcement learning (RL) [10, 21, 23]. For instance, Xie et al. [21] propose a hierarchical RL framework that first determines the channel for each position before selecting a specific item to display. Liao et al. [10] propose CrossDQN, which treats the combination of positions as an action in order to determine the allocation of products within a single screen at a time. This approach fully models the mutual influence between adjacent items. Previous research has predominantly centered around the design of reinforcement learning models to achieve superior performance. However, the quality of data is equally significant for reinforcement learning, yet has not been well explored in prior works. Two types of data can generally be utilized for training reinforcement learning models. The first is called strategy data, which comprises Figure 1: Structure of the position allocation system on Meituan food delivery platform. video channel graphic-text channel Matching & Ranking Mixing MDDL Matching & Ranking top10 100+ 100+ 1000+ 1000+ PIZZA cales 1002 Average Y39 HAMBURGER sales 3008 Averafe * 34 PIZZA sales 1002 Average * 38 HAMBURGER 3008 Average Y34 data collected by the state-of-the-art model that operates online. Although strategy data is adequate and enables the utilization of various features, the distribution of state-action pairs is highly imbalanced (see Figure 2). For instance, samples collected from users who prefer videos are mostly of high video proportions. This skewed distribution can result in severe overestimation problems during the reinforcement learning training process[6]. Particularly when utilizing advanced models like CrossDQN, the overestimation problems are further exacerbated by the high-dimensional state-action space[16]. Even when employing techniques such as DoubleDQN [16], MaxminDQN [7], and other methods [2, 3, 11] to mitigate overestimation, it is still difficult to obtain a policy with better performance. The second type of data is random data collected through a random strategy. While the distribution of state-action pairs in random data is relatively uniform, the random strategy can have a detrimental impact on platform benefits. Moreover, random data can only be obtained through limited traffic in industrial scenarios, thereby constraining the utilization of effective features such as sparse ID features, which also hinders the performance of the RL model. Therefore, determining how to effectively utilize both types of data to enhance the efficacy of the RL model remains a highly challenging problem. To this end, we propose a framework named M ultiD istribution D ata L earning ( MDDL ), which is able to effectively utilize two kinds of learning signals to train RL agents on mixed multi-distribution data. For strategy data, which is collected from the approximate expert strategy that runs well online, we impose the novel imitation learning signal [13] to reduce the impact of overestimation problems. For random data, we make full use of the RL signal to enable agent to learn effectively. While this data has a relatively uniform distribution of state-action pairs and is less prone to overestimation problems, it is limited in terms of its ability to fully utilize effective features such as sparse ID features. By incorporating both signals, MDDL is able to strike a balance between exploration and exploitation, enabling the RL agent to learn more effectively from both types of data. Our proposed framework has the potential to improve the performance of RL agents in industrial scenarios where large amounts of mixed multi-distribution data are available. The contribution of our work can be summarized as follows: Figure 2: Distribution of actions on a certain type of state. 0.16 0.14 0.12 0.08 0.06 0.04 0.02 au1 a1s random data strategy data \u00b7 A novel multi-distribution data learning framework . The MDDL framework we proposed can effectively learn on multidistribution data and perform better for position allocation problems. To the best of our knowledge, this is the first multi-distribution data learning framework for industrial position allocation. \u00b7 Detailed industrial experiments . Our method significantly outperforms the all baselines in both offline experiments and online A/B tests. For now, our framework has been fully employed on Meituan food delivery platform. \u00b7 Completeevaluationindicators for overestimation . We provides a comprehensive approach to evaluate and locate overestimation problems in RL model training for industrial scenarios.", "2 PROBLEM FORMULATION": "In our scenario, we mix items from video and graphic-text channel 1 and present \ud835\udc3e slots in one screen and handle the position allocation for each screen in the feed of a request sequentially. The position allocation problem is formulated as a Markov Decision Process (MDP) [15] ( S , A , \ud835\udc5f , \ud835\udc43 , \ud835\udefe ), the elements of which are defined as follows: \u00b7 State space S . A state \ud835\udc60 \u2208 S contains various information about candidate items available at step \ud835\udc61 from the video channel and the graphic-text channel , such as their IDs. \u00b7 Action space A . An action \ud835\udc4e \u2208 A is the decision of displaying a video or a graphic-text at each position on the current screen, which is formulated as follows:   \u00b7 Reward \ud835\udc5f . After the system takes an action in one state, a user browses the mixed list and gives a feedback. We use the Gross Merchandise Volume (GMV) on the screen as \ud835\udc5f . \u00b7 Transition probability \ud835\udc43 . \ud835\udc43 is the state transition probability from \ud835\udc60 \ud835\udc61 to \ud835\udc60 \ud835\udc61 + 1 after taking the action \ud835\udc4e \ud835\udc61 . When user pulls down, the state \ud835\udc60 \ud835\udc61 transits to the state of next screen \ud835\udc60 \ud835\udc61 + 1. The items selected to present by \ud835\udc4e \ud835\udc61 will be removed from the state on next step \ud835\udc60 \ud835\udc61 + 1. If user no longer pulls down, the transition terminates. 1 We set the number of channels to 2 for ease of illustration, but our method can be readily extended to scenarios where the number of channels is larger than 2. \u00b7 Discount factor \ud835\udefe . The discount factor \ud835\udefe \u2208 [ 0 , 1 ] balances the short-term and long-term rewards. We use D \ud835\udc5a = {( \ud835\udc60 \ud835\udc61 , \ud835\udc4e \ud835\udc61 , \ud835\udc5f \ud835\udc61 , \ud835\udc60 \ud835\udc61 + 1 )} to denote the data collected by model policy (hereinafter referred to as strategy data) and use D \ud835\udc5f = {( \ud835\udc60 \ud835\udc61 , \ud835\udc4e \ud835\udc61 , \ud835\udc5f \ud835\udc61 , \ud835\udc60 \ud835\udc61 + 1 )} to denote the data collected by random policy (hereinafter referred to as random data). Given the MDP formulated as above, the objective is to find a position allocation policy \ud835\udf0b : S \u2192 A on D \ud835\udc5a \u222aD \ud835\udc5f to maximize the cumulative reward.", "3 METHODOLOGY": "In this section, we will detail how to use data collected by model policy and random policy to train reinforcement learning strategies in industrial scenarios. To train on D \ud835\udc5a , we define the similarity between different actions using the weighted exposure ratio (WER) and introduce the imitation learning signals to help the agent learn. For random data D \ud835\udc5f , we train the agent using reinforcement learning signals based on the Bellman equation. Next, we will introduce the concept of WER first and then explain how to use the two signals for training.", "3.1 Weighted Exposure Ratio (WER)": "The task of imposing an imitation learning signal in offline reinforcement learning (RL) requires defining the similarity between actions. However, most recent methods that incorporate imitation learning signals in offline RL are designed for continuous control tasks [4, 5, 13], where the \u2113 2 distances between actions are sufficient to measure similarity. In contrast, our work focuses on the position allocation problem, where the action space is discrete. This presents a new challenge in defining the imitation learning signal, as it is not reasonable to directly calculate the distance between two actions represented by 0-1 vectors. As an illustration, consider three different actions: \ud835\udc4e 1 = ( 1 , 1 , 1 , 0 , 0 ) , \ud835\udc4e 2 = ( 0 , 0 , 1 , 0 , 0 ) , and \ud835\udc4e 3 = ( 1 , 1 , 1 , 1 , 1 ) . The distance between \ud835\udc4e 1 and \ud835\udc4e 2 is the same as the distance between \ud835\udc4e 1 and \ud835\udc4e 3, while the distance between \ud835\udc4e 2 and \ud835\udc4e 3 is far greater. The main reason why it is unreasonable to directly calculate the distance between actions is that it fails to consider the differences in the position and quantity of videos being exposed. To overcome this challenge, we propose a new concept called the weighted exposure ratio (WER) to measure the similarity between actions. The WER converts each action into a specific number, which can be used to calculate the distance between different actions. Specifically, the WER of action \ud835\udc4e \ud835\udc61 \ud835\udc56 at step \ud835\udc61 is calculated as follows:  where \ud835\udc5d \ud835\udc57 is the exposure probability for the \ud835\udc57 -th position in feed, CTR \ud835\udc57 it the click-through rate for the \ud835\udc57 -th position in feed, and \ud835\udc3c \ud835\udc56 ; \ud835\udc58 is an indicator where \ud835\udc3c \ud835\udc56 ; \ud835\udc58 = 1 means a video is inserted into the \ud835\udc58 -th position in current action \ud835\udc4e \ud835\udc61 \ud835\udc56 .", "3.2 Model Training": "3.2.1 Strategy Data. As for strategy data, we use imitation learning for training. The learning goal is to make the action selected by the training model as consistent as possible with the original action in the sample. Given a batch of transitions \ud835\udc35 , the loss for imitation learning is calculated as follows:  However, the argmax function is not differentiable. Therefore, we use a soft version of argmax instead, i.e., we use  where \ud835\udc41 is the size of A , \ud835\udc4d = \u02dd \ud835\udc41 \ud835\udc57 = 1 exp [ \ud835\udefd\ud835\udc44 ( \ud835\udc60 \ud835\udc61 , \ud835\udc4e \ud835\udc57 )] is the normalization factor and \ud835\udefd is the temperature coefficient. The larger the \ud835\udefd is, the closer the result is to argmax. 3.2.2 Random Data. As for random data, we use the RL signal based on the Bellman equation [12], calculated as follows:  3.2.3 Offline Training. For each iteration, we sample a batch of transitions from D \ud835\udc5a \u222a D \ud835\udc5f , and update the parameter through the following loss:  where \ud835\udefc 1 and \ud835\udefc 2 are the coefficients to balance the two losses on mixed dataset D \ud835\udc5a \u222a D \ud835\udc5f .", "4 EXPERIMENTS": "In this section, we will verify the effect of our proposed method through extensive offline experiments and online A/B test. In offline experiments, we use crossDQN as the baseline model to compare the model performance and the degree of overestimation on different types of data, and analyze the influence of different hyperparameters. In online A/B test, we will compare our proposed method with the previous strategy deployed on the Meituan platform.", "4.1 Experimental Settings": "4.1.1 Dataset. We collect the dataset on the Meituan platform during October 2022. We use the data from 20221010 to 20221023 as the training set D \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b and the data from 20221024 to 20221030 as the testing set D \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 . It is worth noting that the strategy data is collected from the online model which is trained through random data during September. We present the detailed statistics of the dataset in Table 1. In order to better reflect the distribution of the dataset, on the one hand, we convert the state representation into two dimensions through the t-SNE method, and cluster into 10 categories through the K-means. There are 320 types of state-action in total. 4.1.2 Evaluation Metrics. We evaluate the performance using an offline estimator. Through extended engineering, the offline estimator models the user preference and aligns well with the online service. For offline experiments, we evaluate the method with revenue indicator and overestimation indicators. \u00b7 Reward . The reward is the total cumulative discount rewards predicted by offline estimator models. \u00b7 Average of Overestimation Degree(AVG-OD) . The average degree of overestimation on each state-action type. The overestimation degree can be expressed as the difference between the estimated Q value and the actual cumulative rewards.  \u00b7 Standard Deviation of Overestimation Degree(STD-OD) . The standard deviation of overestimation degree on different stateaction type. If the overestimation degree on all state-action types is the same, the STD-OD is small and will not affect the policy's choice of the optimal action. If the overestimation degree is very different and the STD-OD is large, it will seriously harmful the effect of the policy.  4.1.3 Hyperparameters. We implement CrossDQN as our baseline model. The learning rate is 10 -3 , the optimizer is Adam and the batch size is 8, 192. \ud835\udefc is set to 1.0, \ud835\udefd is set to 0.5 and temperature coefficient is 10.", "4.2 Offline Experiment": "4.2.1 Baselines. In this section, we compare the model performance and the degree of overestimation on following types of data and training strategies: \u00b7 Random Data & RL . Only use random data and RL singal. \u00b7 Strategy Data & IL . Only Use strategy data and IL singal. \u00b7 Strategy Data & RL . Only use strategy data and RL singal. \u00b7 Mixed Data & RL . Use strategy and random data and RL singal. 4.2.2 Performance Comparison. Table 2 summarizes the results of offline experiments. All experiments were repeated 5 times and we have the following observations from the experimental results: i) Due to the unbalanced data distribution, using RL signals to train strategy data or mixed data cannot get good performance on revenue indicator, and overestimation indicators indicate serious overestimation on the trained model. ii) Using RL signals to train random data, the overestimation is mitigated and achieve better performance on revenue indicator. iii) Imposing imitation learning signal to train the strategy data, the estimated Q-value is even smaller than the actual cumulative rewards due to the direct learning of the online policy and the lack of the RL signal. As mentioned above, since the onlien policy used to collect strategy data is trained Table 1: Statistics of the dataset. Table 2: The results on different types of data and training strategies. Each experiment are presented in the form of mean\u00b1standard deviation. The improvement means the improvements of MDDL across the best baselines. from random data, its performance on revenue indicator is close to that of using RL signals to train random data. iiii) Our proposed method outperforms all other methods which shows that two kinds of data can be fully utilized to learn better reinforcement learning policy by applying different learning signals. 4.2.3 Hyperparameter Analysis. We keep \ud835\udefc 1 =1 to analyze the sensitivity of two hyperparameters: \ud835\udefc 2, \ud835\udefd . Specifically, \ud835\udefc 2 is the weight of the imitation learning loss and \ud835\udefd is temperature coefficient on softmax function. i) As \ud835\udefc 2 increases within a certain range, it's easier to converge to a policy similar to the base model, and to explore a better strategy. When \ud835\udefc 2 exceeds a certain level, the role played by the exploration data is gradually reduced, and the performance appears to be attenuated to a certain extent. ii) With the increase of \ud835\udefd , the imitation learning signal becomes more and more accurate, which can bring about a certain improvement.", "4.3 Online Results": "We compare our method with baseline method which all deployed on Meituan food delivery platform through online A/B test. Specifically, we conduct online A/B test with 10% of whole production traffic from November 09, 2022 to November 15, 2022 (one week). As a result, we gets CTR and GMV increase by 5.46% and 5.83% respectively while the overestimation degree is nearly equal. Now, MDDL has been deployed online and serves the main traffic, and contributes to significant business growth.", "5 CONCLUSION": "In this paper, we mainly focus on the data aspect which is less researched but equally significant to train RL agent for position allocation problem in feed. We propose MDDL to effectively train RL agents on mixed multi-distribution data by utilizing two kinds of learning signals. For strategy data which is collected from the approximate expert strategy but is highly imbalanced, we impose the novel imitation learning signal to reduce the impact of overestimation. For random data which is small scale but relatively uniform, we make full use of the RL signal to enable agent to learn effectively. Practically, both offline experiments and online A/B test have demonstrated the superior performance of our solution.", "REFERENCES": "[1] Chi Chen, Hui Chen, Kangzhi Zhao, Junsheng Zhou, Li He, Hongbo Deng, Jian Xu, Bo Zheng, Yong Zhang, and Chunxiao Xing. 2022. EXTR: Click-Through Rate Prediction with Externalities in E-Commerce Sponsored Search. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 2732-2740. [2] Xinyue Chen, Che Wang, Zijian Zhou, and Keith Ross. 2021. Randomized ensembled double q-learning: Learning fast without a model. arXiv preprint arXiv:2101.05982 (2021). [3] Andrea Cini, Carlo D'Eramo, Jan Peters, and Cesare Alippi. 2020. Deep reinforcement learning with weighted Q-Learning. arXiv preprint arXiv:2003.09280 (2020). [4] Jonathan Ho and Stefano Ermon. 2016. Generative adversarial imitation learning. Advances in neural information processing systems 29 (2016). [5] Jiang Hua, Liangcai Zeng, Gongfa Li, and Zhaojie Ju. 2021. Learning for a robot: Deep reinforcement learning, imitation learning, transfer learning. Sensors 21, 4 (2021), 1278. [6] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. 2020. Conservative q-learning for offline reinforcement learning. Advances in Neural Information Processing Systems 33 (2020), 1179-1191. [7] Qingfeng Lan, Yangchen Pan, Alona Fyshe, and Martha White. 2020. Maxmin q-learning: Controlling the estimation bias of q-learning. arXiv preprint arXiv:2002.06487 (2020). [8] Xiang Li, Chao Wang, Bin Tong, Jiwei Tan, Xiaoyi Zeng, and Tao Zhuang. 2020. Deep Time-Aware Item Evolution Network for Click-Through Rate Prediction. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management . 785-794. [9] Guogang Liao, Xiaowen Shi, Ze Wang, Xiaoxu Wu, Chuheng Zhang, Yongkang Wang, Xingxing Wang, and Dong Wang. 2022. Deep Page-Level Interest Network in Reinforcement Learning for Ads Allocation. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval . 2292-2296. [10] Guogang Liao, Ze Wang, Xiaoxu Wu, Xiaowen Shi, Chuheng Zhang, Yongkang Wang, Xingxing Wang, and Dong Wang. 2022. Cross dqn: Cross deep q network for ads allocation in feed. In Proceedings of the ACM Web Conference 2022 . 401409. [11] Lingheng Meng, Rob Gorbet, and Dana Kuli\u0107. 2021. The effect of multi-step methods on overestimation in deep reinforcement learning. In 2020 25th International Conference on Pattern Recognition (ICPR) . IEEE, 347-353. [12] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. 2015. Human-level control through deep reinforcement learning. nature 518, 7540 (2015), 529-533. [13] Takayuki Osa, Joni Pajarinen, Gerhard Neumann, J Andrew Bagnell, Pieter Abbeel, Jan Peters, et al. 2018. An algorithmic perspective on imitation learning. Foundations and Trends\u00ae in Robotics 7, 1-2 (2018), 1-179. [14] Wentao Ouyang, Xiuwu Zhang, Lei Zhao, Jinmei Luo, Yu Zhang, Heng Zou, Zhaojie Liu, and Yanlong Du. 2020. MiNet: Mixed Interest Network for Cross-Domain Click-Through Rate Prediction. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management . 2669-2676. [15] Richard S Sutton, Andrew G Barto, et al. 1998. Introduction to reinforcement learning . Vol. 135. MIT press Cambridge. [16] Hado Van Hasselt, Arthur Guez, and David Silver. 2016. Deep reinforcement learning with double q-learning. In Proceedings of the AAAI conference on artificial intelligence , Vol. 30. [17] Xiangmeng Wang, Qian Li, Dianer Yu, and Guandong Xu. 2022. Off-policy Learning over Heterogeneous Information for Recommendation. In Proceedings of the ACM Web Conference 2022 . 2348-2359. [18] Ze Wang, Guogang Liao, Xiaowen Shi, Xiaoxu Wu, Chuheng Zhang, Yongkang Wang, Xingxing Wang, and Dong Wang. 2022. Learning List-wise Representation in Reinforcement Learning for Ads Allocation with Multiple Auxiliary Tasks. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management . 3555-3564. [19] Ze Wang, Guogang Liao, Xiaowen Shi, Xiaoxu Wu, Chuheng Zhang, Bingqi Zhu, Yongkang Wang, Xingxing Wang, and Dong Wang. 2022. Hybrid Transfer in Deep Reinforcement Learning for Ads Allocation. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management . 4560-4564. [20] Wei Xia, Weiwen Liu, Yifan Liu, and Ruiming Tang. 2022. Balancing Utility and Exposure Fairness for Integrated Ranking with Reinforcement Learning. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management . 4590-4594. [21] Ruobing Xie, Shaoliang Zhang, Rui Wang, Feng Xia, and Leyu Lin. 2021. Hierarchical Reinforcement Learning for Integrated Recommendation. In Proceedings of AAAI . [22] Xiangyu Zhao, Changsheng Gu, Haoshenglun Zhang, Xiwang Yang, Xiaobing Liu, Hui Liu, and Jiliang Tang. 2021. DEAR: Deep Reinforcement Learning for Online Advertising Impression in Recommender Systems. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 35. 750-758. [23] Xiangyu Zhao, Xudong Zheng, Xiwang Yang, Xiaobing Liu, and Jiliang Tang. 2020. Jointly learning to recommend and advertise. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 3319-3327."}
