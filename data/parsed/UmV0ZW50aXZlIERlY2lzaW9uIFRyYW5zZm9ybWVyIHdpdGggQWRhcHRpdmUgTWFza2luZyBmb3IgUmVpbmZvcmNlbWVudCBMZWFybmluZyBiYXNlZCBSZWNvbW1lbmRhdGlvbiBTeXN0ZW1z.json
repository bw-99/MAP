{"Retentive Decision Transformer with Adaptive Masking for Reinforcement Learning based Recommendation Systems": "Siyu Wang The University of New South Wales Sydney, Australia siyu.wang5@student.unsw.edu.au Xiaocong Chen Data61, CSIRO Eveleigh, Australia xiaocong.chen@data61.csiro.au Lina Yao Data61, CSIRO Eveleigh, Australia The University of New South Wales Sydney, Australia lina.yao@unsw.edu.au", "ABSTRACT": "Reinforcement Learning-based Recommender Systems (RLRS) have shown promise across a spectrum of applications, from e-commerce platforms to streaming services. Yet, they grapple with challenges, notably in crafting reward functions and harnessing large preexisting datasets within the RL framework. Recent advancements in offline RLRS provide a solution for how to address these two challenges. However, existing methods mainly rely on the transformer architecture, which, as sequence lengths increase, can introduce challenges associated with computational resources and training costs. Additionally, the prevalent methods employ fixed-length input trajectories, restricting their capacity to capture evolving user preferences. In this study, we introduce a new offline RLRS method to deal with the above problems. We reinterpret the RLRS challenge by modeling sequential decision-making as an inference task, leveraging adaptive masking configurations. This adaptive approach selectively masks input tokens, transforming the recommendation task into an inference challenge based on varying token subsets, thereby enhancing the agent's ability to infer across diverse trajectory lengths. Furthermore, we incorporate a multi-scale segmented retention mechanism that facilitates efficient modeling of long sequences, significantly enhancing computational efficiency. Our experimental analysis, conducted on both online simulator and offline datasets, clearly demonstrates the advantages of our proposed method. rewards. Over time, these agents refine their policies to maximize long-term rewards, such as enhancing user satisfaction or engagement. Recently, Chen et al. [9] suggest that the offline RLRS would be a better solution than RLRS. The offline RLRS empowers the RL agent to learn from the pre-collected datasets instead of learning from interaction. With the offline RLRS, numerous datasets can be used to train the RL agent, which can significantly improve the training efficiency and performance of RLRS. As a typical example, Wang et al. [29] propose CDT4Rec that incorporates the offline RL into RS. Although this method demonstrated potential, subsequent ablation studies underscored the substantial impact that input trajectory length-referred to as context length-has on the model's performance.", "CCS CONCEPTS": "\u00b7 Information systems \u2192 Recommender systems ; \u00b7 Computing methodologies \u2192 Reinforcement learning .", "KEYWORDS": "RecommenderSystems, Deep Learning, Offline Reinforcement Learning, Transformer", "1 INTRODUCTION": "Reinforcement Learning (RL)-based Recommender Systems (RS) have emerged as powerful tools across diverse applications, from e-commerce and advertising to streaming services. Their strength lies in their ability to adapt to the dynamic nature of user interests in real-world scenarios [10]. In RLRS, agents interact with environments, recommending items and receiving feedback in the form of In the realm of RS, a user's trajectory comprises a chronological sequence of their interactions and behaviors, offering valuable insight into how their preferences evolve over time. This trajectory plays a pivotal role in capturing the changing trends and shifts in what a user might find appealing or engaging. However, given the dynamic nature of user interests in RS, it becomes clear that the historical significance of these trajectories varies among users. For some, recent interactions may be the most indicative of their current interests, while for others, a longer history may provide a clearer view of their enduring preferences. Considering this variability, there is a pressing need for RS models capable of intelligently and adaptively handling trajectories of different lengths. We address this problem by designing a novel adaptive causal masking mechanism. This adaptive capability is vital as it empowers the recommendation system to seamlessly switch between using recent interactions (for short-term insights) and incorporating broader historical patterns (for long-term insights). By doing so, the model can make more informed and nuanced recommendations, thereby enhancing its predictive accuracy and the relevance of its suggestions. Another challenge of employing transformer-based offline RL in Recommender Systems, is the inherent complexity of Transformers. This complexity tends to escalate with increasing sequence lengths, leading to significant challenges in terms of memory usage, latency, and training expenses [2, 20, 22, 26]. Such challenges render them less practical for deployment in real-world scenarios, particularly within large-scale systems. To address this, we draw inspiration from RetNet [26], which facilitates efficient long-sequence modeling at a reduced inference cost. Building upon this, we introduce a novel framework: the Retentive Decision Transformer with Adaptive Masking for Offline Reinforcement Learning in Recommender Systems (MaskRDT). Emb. Emb. Emb. Positional encoding Gt-1 at-1 Emb. Emb. Emb. Positional encoding Gt at Emb. Emb. Emb. Positional encoding \u00c9 C \u00c9 \u2026 Mask at-1 \u00c9 Mask Mask Gt-C+1 st-C+1 at-C+1 m L x RetNet Decision Block Causal Decoder at r t \u00c9 \u00c9 st-1 st-1 st \u00c9 Mask Mask st Segment 1 Segment S \u00c9 Parallel Retention Parallel Retention Recurrent Retention Feed Forward \u00c9 Mask Mask \u00c9 l-1 st l Gt-m+1 st-m+1 at-m+1 l-1 l-1 l-1 Gt-m+1 st-m+1 at-m+1 l l l s a G L L L Pred. \u03a8 t \u03a8 t N e r t N g at Pred. s a RetNet Decision Block Causal Decoder \u00c9 Causal Decoder at-1 r t-1 Causal Decoder st-C+1 Gt-C+1 at-C+1 In our MaskRDT framework, we reframe the RLRS challenge by treating sequential decision-making as an inference task with specific masking configurations. By strategically masking specific tokens within input trajectories, we dictate which portions of the user's history are visible to the model and which predictions it should generate. This method ensures that the model receives selective information about a user's past interactions and behaviors, guiding it to predict items for recommendation. A cornerstone of our approach is the innovative Adaptive Causal Masking. This technique introduces variability in the lengths of trajectories fed to the model, exposing the agent to a diverse range of trajectory segments. This adaptive strategy alternates between longer and shorter sequences, providing the agent with a nuanced and dynamic view of user behaviors. To bolster the efficiency of our model, we've integrated a multi-scale segmented retention mechanism, serving as an adept alternative to conventional multi-head attention. This design choice ensures long-sequence modeling is not only efficient but also resource-conscious. It achieves this by encoding each segmentation in parallel for swift computation, while different segmentations are encoded recurrently to reduce the training cost. The key contributions of this study are as follows: \u00b7 We model the offline RLRS challenge as an inference task using a unique masking configuration. \u00b7 Our innovative adaptive causal masking configuration allows the model to handle variable token lengths during training, enhancing its inference capabilities across diverse time frames. \u00b7 By integrating the causal retention network with masking, we achieve efficient long-sequence modeling while minimizing training costs. \u00b7 We empirically demonstrate the effectiveness of MaskRDT through comprehensive experiments on various datasets and in an online simulator.", "2 PROBLEM FORMULATION": "Given a set of users U = \ud835\udc62 0 , \ud835\udc62 1 , ..., \ud835\udc62 \ud835\udc5b , a set of items I = \ud835\udc56 0 , \ud835\udc56 1 , ..., \ud835\udc56 \ud835\udc5a , and historical interaction trajectories of users over a sequence of time steps \ud835\udc61 = 1 , ..., \ud835\udc47 , the goal of an RL-based Recommender System is to leverage historical interaction data to learn an effective policy \ud835\udf0b that recommends items to users in a way that maximizes their satisfaction and overall engagement. By transforming this scenario into an offline RL framework, we can employ the Markov Decision Process (MDP) paradigm [27]. The core components of this MDP, tailored for recommendation, are: \u00b7 State Space ( S ): The state space represents the information and historical interaction context of users. In time step \ud835\udc61 , the state \ud835\udc60 \ud835\udc61 \u2208 S captures user features, preferences, and their historical interactions. \u00b7 Action Space ( A ): The action space represents the choices available to the recommender agent in each state. A( \ud835\udc60 \ud835\udc61 ) denotes the set of actions possible in state \ud835\udc60 \ud835\udc61 , where an action \ud835\udc4e \ud835\udc61 corresponds to recommending an item to a user. \u00b7 Transition Probability ( P ): The transition probability \ud835\udc5d ( \ud835\udc60 \ud835\udc61 + 1 | \ud835\udc60 \ud835\udc61 , \ud835\udc4e \ud835\udc61 ) \u2208 P defines the likelihood of transitioning from state \ud835\udc60 \ud835\udc61 to \ud835\udc60 \ud835\udc61 + 1 when action \ud835\udc4e \ud835\udc61 is taken. \u00b7 Reward Function ( R ): The reward function R( \ud835\udc60, \ud835\udc4e ) \u2192 R quantifies the immediate benefit of taking action \ud835\udc4e in state \ud835\udc60 . In the context of the recommender system, the reward \ud835\udc5f \ud835\udc61 is based on the feedback received from users on recommended items. \u00b7 Discount Factor ( \ud835\udefe ): The discount factor \ud835\udefe \u2208 [ 0 , 1 ] determines the weight of future rewards compared to immediate rewards in the agent's decision-making process. An RL agent's objective is to learn a policy \ud835\udf0b , which is a mapping from states to actions, that maximizes the expected cumulative reward over trajectories: where \ud835\udf0f = ( \ud835\udc60 0 , \ud835\udc4e 0 , \ud835\udc60 1 , \ud835\udc4e 1 , ..., \ud835\udc60 \ud835\udc47 , \ud835\udc4e \ud835\udc47 ) represents a trajectory under policy \ud835\udf0b . In Offline RL, the focus is on improving the agent's policy using only a static dataset D of historical transitions, without further online interaction. The dataset D contains tuples ( \ud835\udc60 \ud835\udc62 \ud835\udc61 , \ud835\udc4e \ud835\udc62 \ud835\udc61 , \ud835\udc60 \ud835\udc62 \ud835\udc61 + 1 , \ud835\udc5f \ud835\udc62 \ud835\udc61 ) , where each tuple corresponds to a user \ud835\udc62 at time step \ud835\udc61 . Formally, the problem can be summarized as learning a policy \ud835\udf0b that maximizes the expected cumulative reward using the provided dataset D , in which each transition tuple is sampled according to the distribution \ud835\udf0b \ud835\udefd . By solving this Offline RL-based Recommender System problem, the aim is to make accurate and effective recommendations to users based on historical interactions, without the need for real-time exploration and interaction with the environment.", "3 METHODOLOGY": "", "3.1 Structuring Trajectories in RL as Sequences": "RL-based RS traditionally interprets trajectories as sequences of state-action tuples as described in section 2, capturing an agent's decisions and the associated outcomes. However, following the perspective presented in [5, 16], we recast these trajectories as sequences of tokens. This refined representation emphasizes three critical tokens: states, actions, and returns-to-go (RTG), with the following trajectory representation: where \u02c6 \ud835\udc3a \ud835\udc61 = \u02dd \ud835\udc47 \ud835\udc58 = \ud835\udc61 \ud835\udefe \ud835\udc58 -\ud835\udc61 \ud835\udc5f \ud835\udc58 embodies the RTG at the time \ud835\udc61 , highlighting the cumulative anticipated value for an agent. By its definition, RTG serves as a foresight-driven metric, quantifying the aggregated discounted rewards an agent anticipates receiving in the future. Such a metric embeds the agent's immediate decisions and future-oriented strategies, acting as a crucial linkage between the two. Hence, RTG's inclusion in our sequence modeling technique empowers the model with a prophetic viewpoint, effectively marrying the agent's short-term maneuvers with its long-term aspirations.", "3.2 Adaptive Causal Masking": "In offline RL, trajectory sequences can be interpreted within the context of a sequential decision-making paradigm. Within this framework, we can consider that specific input tokens undergo masking, thereby transforming the task into an inference challenge based on particular token subsets. For a specified time \ud835\udc61 , the masking configuration exposes only the tokens \ud835\udc60 0: \ud835\udc61 , \ud835\udc4e 0: \ud835\udc61 -1, and \u02c6 \ud835\udc3a 0. Consequently, the model is tasked with inferring the action \ud835\udc4e \ud835\udc61 , conditioned on these revealed tokens, as denoted by \ud835\udc43 ( \ud835\udc4e \ud835\udc61 | \ud835\udc60 0: \ud835\udc61 , \ud835\udc4e 0: \ud835\udc61 -1 , \u02c6 \ud835\udc3a 0 ) . Let \ud835\udc36 \u2265 1 define the context length, which corresponds to the most recent \ud835\udc36 timesteps provided to the transformer. This results in trajectory segments, \ud835\udf0f \ud835\udc61 -\ud835\udc36 + 1: \ud835\udc61 , each of length \ud835\udc36 . Within these segments, \ud835\udc60 \ud835\udc61 -\ud835\udc36 + 1: \ud835\udc61 and \ud835\udc4e \ud835\udc61 -\ud835\udc36 + 1: \ud835\udc61 denote sequences of the preceding \ud835\udc36 states and actions at time \ud835\udc61 , respectively, while \ud835\udc3a \ud835\udc61 -\ud835\udc36 + 1: \ud835\udc61 signifies the RTG values over the same interval. Contrary to conventional methods that incorporate an RTG token at each timestep, our methodology emphasizes solely on the initial RTG token within a given context window, masking the subsequent RTGs. This minimalistic strategy, which only leverages the first RTG token, has proven effective for our inference tasks. To enhance the agent's inference capabilities, we incorporate variability in the token length fed to the model. This variability exposes the agent to trajectory segments of diverse lengths, alternating between longer and shorter sequences. For a given segment \ud835\udf0f \ud835\udc61 -\ud835\udc36 + 1: \ud835\udc61 , the masking configuration at time \ud835\udc61 is dictated by \ud835\udc5a , uniformly selected from [0, \ud835\udc36 ]. Consequently, the model is presented with tokens \ud835\udc60 \ud835\udc61 -\ud835\udc5a + 1: \ud835\udc61 , \ud835\udc4e \ud835\udc61 -\ud835\udc5a + 1: \ud835\udc61 -1, and \u02c6 \ud835\udc3a \ud835\udc61 -\ud835\udc5a + 1, while the remaining tokens are masked. And the model is tasked with predicting the action \ud835\udc4e \ud835\udc61 for the timestep \ud835\udc61 . This adaptive causal masking approach serves a dual purpose. For smaller values of \ud835\udc5a , the agent is presented with a constrained context, predominantly relying on recent states and actions. This setup fine-tunes the agent's capacity to anticipate near-future events, nurturing its short-term inference skills. On the other hand, as \ud835\udc5a approaches \ud835\udc36 , the agent is immersed in an expansive context that spans a more extended historical sequence of states and actions. This broader perspective refines the agent's capacity for long-term inference. Through this spectrum of context lengths, the agent cultivates a versatile inference aptitude, enhancing its decision-making acumen across a range of scenarios.", "3.3 Segmented Retention Mechanism": "Given a sequence trajectory, the representation of states, actions, and rewards is delineated as separate tokens. This delineation triples the sequence length for a trajectory segment of length \ud835\udc36 , resulting in a length of 3 \ud835\udc36 . Such an expansion not only amplifies computational demands but also accentuates the inherent computational bottleneck of self-attention, which inherently scales quadratically with sequence length. To address this challenge, we draw inspiration from [26] and introduce the multi-scale segmented retention mechanism. This segmented retention mechanism is specifically designed to replace the conventional masked multi-head attention mechanism, thereby enhancing training efficiency, especially for proficient long-sequence modeling. Let \ud835\udc51 \u210e denote the size of the hidden states and \ud835\udc3b \u2208 R \ud835\udc36 \u00d7 \ud835\udc51 \u210e represents the hidden states for the trajectory segment \ud835\udf0f \ud835\udc61 -\ud835\udc36 + 1: \ud835\udc61 . We define matrices \ud835\udc44, \ud835\udc3e,\ud835\udc49 \u2208 R \ud835\udc36 \u00d7 \ud835\udc51 as the query, key, and value matrices, respectively, where \ud835\udc51 is the embedding dimension. The hidden state is projected into a one-dimensional function as: where \ud835\udc64 \ud835\udc63 is the associated weight vector. Central to the retention mechanism is the recurrent state \ud835\udc67 \ud835\udc5b , which captures accumulated information up to timestep \ud835\udc5b . This state is pivotal in computing the output \ud835\udc5f\ud835\udc52\ud835\udc61 ( \ud835\udc5b ) of the mechanism, given by: where matrix \ud835\udc34 can be diagonalized as \ud835\udc34 = \u039b ( \ud835\udefc\ud835\udc52 \ud835\udc56\ud835\udefd ) \u039b -1 , with \ud835\udefc and \ud835\udefd being vectors in \ud835\udc45 \ud835\udc51 . The output of the retention mechanism at time \ud835\udc5b is then expressed as: By integrating \u039b into \ud835\udc4a \ud835\udc44 and \ud835\udc4a \ud835\udc3e , where \ud835\udc4a \ud835\udc44 , \ud835\udc4a \ud835\udc3e \u2208 \ud835\udc45 \ud835\udc51 \u210e \u00d7 \ud835\udc51 are the learnable parameter matrices for the Query and Key projections, respectively, the equation becomes: We can define: where \ud835\udf14 = \ud835\udc52 \ud835\udc56\ud835\udefd and \u00af \ud835\udf14 is the complex conjugate of \ud835\udf14 . The output of the retention layer in a Recurrent Neural Network (RNN) manner is: Considering \ud835\udefc as a scalar, the equation simplifies to an easily parallelizable form: where \u2217 indicates the conjugate transpose. The output of the retention layer in a parallel manner is: For handling extended sequences, we adopt a segmentation strategy. Specifically, input sequences are partitioned into segments, with parallel retention computations applied within each segment and recurrent retention computations bridging across segments. Let's assume the input sequences are divided into \ud835\udc46 segments, each of length \ud835\udc40 . The retention output for the \ud835\udc60 -th segment, denoted as \ud835\udc3b ( \ud835\udc60 ) (where \ud835\udc60 \u2208 [ 1 , \ud835\udc46 ] ), can be expressed as: where \ud835\udc4d seg \ud835\udc60 -1 represents a segment-adapted version of the recurrent representation of retention, defined as:", "3.4 Model Architecture": "Our framework employs the RetNet architecture [26], tailored for sequential modeling in offline RL for RS, incorporating adaptive causal causal masking. Comprising \ud835\udc3f stacked multi-input blocks, MaskRDT processes a trajectory segment \ud835\udf0f \ud835\udc61 -\ud835\udc36 + 1: \ud835\udc61 spanning the last \ud835\udc36 timesteps as input to the initial transformer block. We initiate by deriving the masked trajectory representation, masking 3 \ud835\udc36 tokens as delineated in section 3.2. 3.4.1 Embedding Layer. For masked RTGs, masked states, and masked actions, we employ a linear layer to derive their respective token embeddings, which is followed by layer normalization. To convey the time-horizon information within the trajectory segment \ud835\udf0f \ud835\udc61 -\ud835\udc36 + 1: \ud835\udc61 , we utilize absolute positional encoding for tokens, deviating from the conventional timestep encoding approach [5]. This methodology curtails the propensity for overfitting often associated with direct timestep data. Additionally, we have adjusted the return-to-go token to encompass both the return value and the current timestep, ensuring the preservation of vital trajectory-level timestep insights. 3.4.2 RetNet Decision Block. As depicted in Figure 1, our transformer is structured into \ud835\udc3f consistent blocks, indexed as \ud835\udc59 = 1 , ..., \ud835\udc3f from the bottom upwards. At each time step \ud835\udc61 , these blocks simultaneously generate hidden representations for the state, action, and RTG at every layer \ud835\udc59 . For the masked trajectory segment \ud835\udf0f \ud835\udc61 -\ud835\udc5a + 1: \ud835\udc61 , the hidden representations at layer \ud835\udc59 and time step \ud835\udc61 are represented as \ud835\udc3b \ud835\udc59 = ( \ud835\udc3b \ud835\udc59 1 , ....\ud835\udc3b \ud835\udc59 \ud835\udc47 ) \u22a4 \u2208 R \ud835\udc47 \u00d7 \ud835\udc51 \u210e . Each transformer block operates on three concurrent sequences of these representations. The foundational block ingests the output \ud835\udc3b 0 from the embedding layer. For layers \ud835\udc59 \u2265 2, the input is sourced from the output of its immediate predecessor, the ( \ud835\udc59 -1 ) block: Incorporated within each block is a Multi-Head Retention, which acts on the input tokens and is followed by a Position-wise FeedForward layer. Multi-Head Retention Mechanism. Consider \u210e = \ud835\udc51 \u210e / \ud835\udc51 , which denotes the number of attention heads. As detailed in Section section 3.3, each attention head produces a representation described by: Each attention head is assigned a unique \ud835\udefc value. The multi-head retention mechanism operates by applying \u210e attention functions in parallel to create a unified output representation. This output is a concatenated projection of the representations from all the heads: To ensure consistent scaling and normalization across different attention heads, we employ Group Normalization (GN) [31] on the outputs of each head. To enhance the non-linearity within the retention layers, we incorporate the Swish activation function [21]. The final multi-head retention output, denoted as \ud835\udc40\ud835\udc3b\ud835\udc45 ( \ud835\udc4b ) , is formulated as: where \ud835\udc4a \ud835\udc43 , \ud835\udc4a \ud835\udc42 \u2208 \ud835\udc45 \ud835\udc51 \u210e \u00d7 \ud835\udc51 \u210e and represent learnable parameters. Position-wise Feed-Forward Networks. Within each block, to enhance the representations derived from the retention layer and adapt them to the specific task at hand, we incorporate a Positionwise Feed-Forward Network (FFN). This FFN is delineated by two linear layers, interspersed with an activation function. The resulting output is formulated as: where the activation function employed is the Gaussian Error Linear Unit (GELU) [15]. 3.4.3 Causal Layer. The culmination of the \ud835\udc3f RetNet blocks yields an output represented as \ud835\udc3b \ud835\udc3f \ud835\udc61 = ( \ud835\udc60 \ud835\udc3f \ud835\udc61 -\ud835\udc5a + 1 , \ud835\udc4e \ud835\udc3f \ud835\udc61 -\ud835\udc5a + 1 , \ud835\udc3a \ud835\udc3f \ud835\udc61 -\ud835\udc5a + 1 , ..., \ud835\udc60 \ud835\udc3f \ud835\udc61 , -, -) , where the symbol '-' denotes masked tokens. This output is segmented into three distinct sets: RTG, states, and actions. This can be articulated as: Given that the action inference is contingent upon all unmasked tokens, the final representation is synthesized from these three hidden states. To achieve action prediction, we apply a linear layer followed by the GELU activation: In a parallel vein, the prediction for the subsequent state is derived from \ud835\udc60 \ud835\udc61 -\ud835\udc3e + 1: \ud835\udc61 and \u03a8 \ud835\udc4e \ud835\udc61 , as state transitions inherently depend on the preceding state and the current action: To mitigate overfitting, we introduce Dropout [24] post the linear layer's output. Both the action and state predictions are subsequently processed through fully-connected networks, termed the reward estimation network \ud835\udc41 \ud835\udc52 , to deduce the prospective reward \ud835\udc5f \ud835\udc61 . This potential reward, alongside the action prediction, is then channeled into the action generation network \ud835\udc41 \ud835\udc54 to produce the final anticipated action \ud835\udc4e \ud835\udc61 .", "3.5 Training Procedure": "Our training procedure is grounded on a dataset of recommendation trajectories. Initially, the Deep Deterministic Policy Gradient (DDPG) method [18] is employed to train an expert reinforcement learning (RL) agent. This expert agent then interacts with the environment to produce a collection of expert trajectories, which form our primary dataset. From this dataset, we sample mini-batches of sequences, each characterized by a context length of \ud835\udc36 . These sequences undergo a transformation to be suitable as network inputs. After this transformation, a masking configuration is applied to the input, which is then processed through the entirety of the network blocks. Within our model, \ud835\udf03 \ud835\udc52 represents the trainable parameters for the reward estimation network \ud835\udc41 \ud835\udc52 , \ud835\udf03 \ud835\udc54 denotes those for the action generation network \ud835\udc41 \ud835\udc54 , \ud835\udf03 \ud835\udc60 encapsulates the parameters for generating the state representation \u03a8 \ud835\udc60 \ud835\udc61 , and \ud835\udf03 \ud835\udc4e is reserved for generating the action representation \u03a8 \ud835\udc4e \ud835\udc61 . The reward estimation network \ud835\udc41 \ud835\udc52 , in conjunction with state and action predictions, is optimized by minimizing the factual reward loss: For the action generation network \ud835\udc41 \ud835\udc54 , the objective is to produce the final action by minimizing its cross-entropy loss: The overarching loss function amalgamates the losses mentioned above: where \ud835\udefd serves as a hyper-parameter, modulating the contributions of the individual losses.", "4 EXPERIMENTS": "In this section, we present experimental results addressing four primary research questions: \u00b7 RQ1 : How does the performance of MaskRDT stack up against traditional deep RL algorithms in both online recommendation and offline dataset environments? \u00b7 RQ2 : What is the impact of adaptive causal masking on performance across different context lengths? \u00b7 RQ3 : How does MaskRDT influence training cost? \u00b7 RQ4 : How is the performance of MaskRDT affected by varying dataset sizes? We focus our investigations on RQ2, RQ3 and RQ4 within online simulation settings, as they more accurately mirror real-world scenarios. Offline datasets, being static, don't capture the dynamic nature of users' interests.", "4.1 Experimental Setup: Datasets and Simulation Environments": "In this subsection, we detail the datasets and environments employed to evaluate the performance of our MaskRDT algorithm in comparison with other leading methods. Our model is implemented using PyTorch and all experiments are executed on a server equipped with two Intel Xeon CPU E5-2697 v2 CPUs, six NVIDIA TITAN X Pascal GPUs, two NVIDIA TITAN RTX GPUs, and 768 GB of RAM. 4.1.1 Dataset. We utilize six diverse, publicly available datasets from various recommendation domains for our offline evaluations, each exhibiting distinct levels of sparsity: \u00b7 GoodReads : A dataset from the book review platform GoodReads 1 [28], which includes varied user interactions with books such as ratings and reviews. \u00b7 LibraryThing : Originating from LibraryThing 2 , a digital service aiding users in book cataloging. This dataset assists in cataloging books and captures social networking features, making it suitable for studying social-based recommendation models. \u00b7 Netflix : A renowned dataset from the Netflix Prize Challenge 3 , is a collection of movie ratings used for recommendation system research. \u00b7 Amazon CD 4 : This dataset is a subset of product reviews from Amazon.com, specifically focusing on the \"CD\" category [19]. \u00b7 MovieLens : A standard dataset for benchmarking recommender systems. Our study employs two versions of MovieLens datasets: MovieLens-1M 5 and MovieLens-20M 6 , which differ in scale. To facilitate reinforcement learning interactions, we transform these offline datasets into simulated environments, drawing inspiration from prior works [8, 35]. We employ an LSTM-based state encoder to capture temporal dynamics within the data. 4.1.2 Online Simulator. We conduct experiments on the VirtualTB [23] online simulation platform. VirtualTB replicates the dynamics of an online retail environment, having been trained on real data from the Taobao platform, one of China's premier online retail platforms. The VirtualTaobao interacts with customers by first sampling a feature vector that includes the customer's profile and search query. It then retrieves a set of items related to the query and uses a model to assign a weight vector to these items' attributes. The system calculates the dot product of this weight vector with each item's attributes, selecting the top 10 items based on these values. These items are presented to the customer, who can either click on them, navigate to the next page (prompting the system to update customer features and reiterate the process), or exit the platform. 4.1.3 Evaluation Metric. In the simulated online environment, we employ the Click-Through Rate (CTR) as the primary metric for assessing the performance of Reinforcement Learning Recommender Systems (RLRS). The CTR is calculated using the formula: where \ud835\udc52\ud835\udc5d\ud835\udc56\ud835\udc60\ud835\udc5c\ud835\udc51\ud835\udc52 _ \ud835\udc5f\ud835\udc52\ud835\udc61\ud835\udc62\ud835\udc5f\ud835\udc5b represents the total reward accumulated in an episode, \ud835\udc52\ud835\udc5d\ud835\udc56\ud835\udc60\ud835\udc5c\ud835\udc51\ud835\udc52 _ \ud835\udc59\ud835\udc52\ud835\udc5b\ud835\udc54\ud835\udc61\u210e denotes the number of steps in the episode, and \ud835\udc5a\ud835\udc4e\ud835\udc65\ud835\udc56\ud835\udc5a\ud835\udc62\ud835\udc5a _ \ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc4e\ud835\udc5f\ud835\udc51 is the highest reward achievable in a single step. For offline datasets, our evaluation encompasses a range of metrics, including recall, precision, and normalized discounted cumulative gain (nDCG), which are among the most popular metrics utilized by RLRSs, due to the absence of metrics specifically developed for RLRSs [1].", "4.2 Baselines": "Most of the existing works are evaluating their methods on offline datasets, and very few works provide a public online simulator evaluation. As there are two types of experiments, we provide two sets of baselines to be used for different experimental settings. Firstly, we will introduce the baselines for the online simulator, which are probably the most popular benchmarks in reinforcement learning: \u00b7 Deep Deterministic Policy Gradient (DDPG) [18] is an off-policy method for environments with continuous action spaces. DDPG employs a target policy network to compute an action that approximates maximization to deal with continuous action spaces. \u00b7 Soft Actor Critic (SAC) [14] is an off-policy maximum entropy Deep Reinforcement Learning approach that optimizes a stochastic policy. It employs the clipped double-Q method and entropy regularisation that trains the policy to maximize a trade-off between expected return and entropy. \u00b7 Twin Delayed DDPG (TD3) [12] is an algorithm that improves baseline DDPG performance by incorporating three key tricks: learning two Q-functions instead of one, updating the policy less frequently, and adding noise to the target action. \u00b7 Decision Transformer (DT) [5] is an offline reinforcement learning algorithm that incorporates the transformer as the major network component to infer actions. Moreover, the following recommendation algorithms are used for offline evaluations which come from two different categories: transformerbased methods and reinforcement learning-based methods. \u00b7 SASRec [17] is a well-known baseline that uses the selfattention mechanism to make sequential recommendations. \u00b7 BERT4Rec [25] is a recent transformer based method for recommendation. It adopts BERT to build a recommender system. \u00b7 S3Rec [36] is BERT4Rec follow-up work that uses transformer architecture and self-supervised learning to maximize mutual information. \u00b7 KGRL [8] is a reinforcement learning-based method that utilizes the capability of Graph Convolutional Network (GCN) to process the knowledge graph information. \u00b7 TPGR [4] is a model that uses reinforcement learning and binary tree for large-scale interactive recommendations. \u00b7 PGPR [32] is a knowledge-aware model that employs reinforcement learning for explainable recommendations. \u00b7 CDT4Rec [29] is a casual decision transformer model for offline RLRS. We note that SASRec, BERT4Rec, and S3Rec are not suitable for the reinforcement learning evaluation procedure. In order to evaluate the performance of those models, we feed the trajectory representation \ud835\udf0f as an embedding into those models for training purposes and use the remaining trajectories for testing purposes.", "4.3 Overall Results in the Online Simulator (RQ1)": "This section initiates with a description of our methodology for conducting experiments within an online simulator, employing offline RL techniques. Our approach begins with the training of an expert agent through DDPG, which is subsequently deployed within the simulator to accumulate expert trajectories. It is imperative to note that the expert's interaction with the environment is intentionally restricted, with the goal of collectiong a specific number of random trajectories. These collected trajectories constitute the initial dataset, which is essential for the pre-training phase of offline RL methods. Subsequently, the offline RL algorithm is refined through interactions within the simulated environment. The effectiveness of our MaskRDT method, in comparison to baseline approaches, is delineated in Figure 2, which demonstrates the CTR performance across iterative timesteps in the VirtualTaobao simulation. The data reveals a marked improvement in performance, with MaskRDT outperforming nearly all competing algorithms around the 20,000-timestep threshold.As the simulation progresses to 100,000 timesteps, MaskRDT's CTR stabilizes at a plateau approximately 0.9, reflecting its potent recommendation capabilities. The variance in our model's CTR, illustrated by the shaded region around the MaskRDT trajectory, is narrower than that of the competing methods, underscoring our model's consistent learning and dependability. Whencomparedwithbaselines, it becomes apparent that CDT4Rec, while following a rising trend similar to MaskRDT, experiences a higher variance and falls short of MaskRDT's peak CTR. Other algorithms like SAC and TD3 achieve moderate success but do not reach the high consistency level displayed by MaskRDT. In contrast, DDPG and DT show significantly lower effectiveness in this simulated setting. In summary, within the VirtualTaobao online simulation, MaskRDT exhibits a pronounced superiority in maximizing CTR, suggesting its potential as a more efficient system for providing clickable recommendations to users in comparison to the evaluated baselines.", "4.4 Overall Results on Offline Dataset(RQ1)": "The performance of our MaskRDT method against baseline models across several offline datasets is summarized in Table 1. MaskRDT consistently superior its counterparts in measures of Recall, Precision, and nDCG across datasets such as GoodReads, Librarything, Amazon CD, Netflix, MovieLens-1M, and MovieLens-20M. Particularly, MaskRDT achieves notable high scores in the GoodReads and Librarything datasets with the highest nDCG values of 10.934% and 14.855% respectively. It continues this leading trend in Amazon CD and Netflix datasets, showing superior Recall and Precision. In the extensive MovieLens-1M and 20M datasets, MaskRDT maintains its dominance with the highest Recall and nDCG scores, emphasizing its robust recommendation capabilities. The results emphasize how effective and stable MaskRDT is in offline settings, showcasing its ability to learn from different context MaskRDT SAC CDTARec DDPG TD3 DT 1.0 0.8 0.2 20000 40000 60000 80000 100000 Timesteps lengths as needed. This adaptability is crucial in allowing our model to find a delicate balance between understanding recent user behavior trends and respecting long-standing preferences, ultimately boosting the relevance and precision of its recommendations.", "4.5 Ablation Study": "4.5.1 Efficacy of Adaptive Causal Masking (RQ2). We evaluated the efficacy of adaptive contextual masking by testing MaskRDT and CDT4Rec across various context lengths: [2, 4, 8, 16, 32, 64], as depicted in Figure 3. Our analysis reveals a pronounced divergence in performance trends between the two models as the context length increases. CDT4Rec shows promising results at shorter context lengths but its performance declines with longer contexts. MaskRDT, on the other hand, displays remarkable consistency across the spectrum of context lengths. It matches CDT4Rec at shorter lengths and starts to show superior performance at extended context lengths, particularly from 16 onwards. In Recommender Systems, the relevancy of historical data to current user interests varies. Shorter user interaction histories often align closely with present preferences, while older interactions may lose relevance due to the dynamic nature of user interests. CDT4Rec, with its fixed context length design, may inadvertently factor in less pertinent historical data as context lengthens, introducing noise and potentially degrading its predictive accuracy. Contrastingly, MaskRDT is engineered to adapt to varying context lengths, allowing it to adeptly balance recent and long-term user behaviors. This flexibility could account for the observed performance dip in CDT4Rec at longer context lengths, while MaskRDT enhances performance. This is likely because the increase in context length introduces a wider range of trajectory segment lengths, enriching MaskRDT's learning with a broader diversity of user behaviors. 4.5.2 Assessing the Training Efficiency of MaskRDT (RQ4). In our investigation into the training efficiency of MaskRDT, we compared the memory consumption and training time with two other models: DT and CDT4Rec. This examination was conducted across varying context lengths to ascertain the effectiveness of our model's architecture in terms of training costs. Section 4.5.1 presents a compelling case for the efficiency of MaskRDT. It reports lower memory usage and faster training times than its counterparts at all examined context lengths, as shown in Section 4.5.1. MaskRDT demonstrates the benefits of its network architecture, achieving notable training speed improvements which are crucial for practical applications where time is a valuable resource. The results confirm that MaskRDT's training efficiency gains do not come at the expense of performance. This equilibrium of speed and accuracy is critical for models intended for real-world implementation, where both factors are essential. MaskRDT stands out as a model that can deliver swift training cycles while maintaining high-quality recommendations, making it an attractive option for scalable recommender systems. 4.5.3 Dataset Size Influence on MaskRDT Performance (RQ4). In Figure 4, we explore how MaskRDT's performance is influenced by dataset size, where size is indicated by the number of user trajectories. This analysis directly addresses the question of how varying dataset sizes affect the efficacy of MaskRDT, with comparative insights drawn against DT and CDT4Rec. The data reveals that MaskRDT maintains a high level of performance across different dataset sizes, which not only indicates stability but also reflects the model's proficiency in learning from varying context lengths. Such capability to adapt to different trajectory lengths suggests that MaskRDT is adept at understanding and incorporating both short-term and long-term user behaviors into its recommendation process. While CDT4Rec shows robust performance, it occasionally experiences fluctuations, and DT tends to stabilize only when presented with larger datasets, such as the 30k trajectory set.", "5 RELATED WORK": "RL-based Recommender Systems. Reinforcement learning (RL) has recently emerged as a powerful tool in the domain of recommender systems [10]. Zhao et al. [34] introduced a RL-based pagewise recommendation framework using real-time user feedback. Bai et al. [3] proposed a model-based technique with generative adversarial training to learn user behaviors and update recommendation policies. Chen et al. [8] integrated knowledge graphs into RL to enhance decision-making. Chen et al. [11] employed generative inverse reinforcement learning for online recommendations, extracting a reward function from user behaviors. While a significant Context Lenglh MaskRDT CDT4Rec Timesteps Context Lengih MaskRDT CDTARec Timesteps Context Length MaskRDT Timesteps Context Length MaskRDT 20000 40000 60000 80000 100000 Irnesteps Context Length MaskRDT CDTARec 20000 40000 100000 Timesteps Context Length MaskRDT CDTARec 20000 40000 60000 80000 10k Trajectories MaskRDT CDTARec Timesteps 20k Trajectories MaskRDT CDTARec 20000 BOOOO Timesteps 30k Trajectones MaskRDT CDT Rec 40000 Timesteps portion of the literature has been dedicated to online RL-based recommender systems, there's a growing interest in offline RL approaches. Chen et al. [6] scaled an off-policy actor-critic algorithm for industrial recommendation systems, addressing offline evaluation challenges. Gao et al. [13] discussed the Matthew effect in offline RL-based systems, where popular items overshadow less popular ones due to frequent recommendations. Wang et al. [29] proposed to use causal transformers for offline RL recommender systems, addressing the challenges of reward function design and handling vast datasets. a self-attention mechanism to enrich item representations in user behavior sequences, considering the inherent sequential patterns, and demonstrated its efficacy on a real-world e-commerce platform. Lastly, Zhao et al. [33] presented the Decision Transformer (DT) optimized for user retention, utilizing a weighted contrastive learning approach to maximize knowledge extraction from samples and prioritize high-reward recommendations. Transformer in Recommender Systems. The transformative potential of transformer architectures has recently garnered significant attention in sequential recommendation systems. Sun et al. [25] introduced BERT4Rec, leveraging a bidirectional self-attention network to capture user behavior sequences for sequential recommendations. Wu et al. [30] developed a personalized transformer model, enhancing self-attentive neural architectures with SSE regularization for tailored recommendations. Chen et al. [7] employed In contrast to the aforementioned works, our proposed MaskRDT uniquely integrates adaptive causal masking with retentive networks, offering enhanced stability and performance across varying trajectory lengths in offline RL scenarios.", "6 CONCLUSION": "In this study, we presented the MaskRDT framework, a novel approach that integrates adaptive causal masking with retentive networks for offline RL in recommendation systems. By reinterpreting RLRS as an inference task and leveraging segmented retention mechanisms, we achieved computational efficiency and adaptability to diverse trajectory lengths. The causal mechanism further simplifies reward estimation based on user behaviors. While MaskRDT addresses many challenges in RLRS, future work could: 1) investigate optimal segment lengths specific to individual users, and 2) deepen our understanding of the causal implications of users' decisions, with the goal of refining the reward function estimation using accumulated trajectory data.", "REFERENCES": "[1] MMehdi Afsar, Trafford Crump, and Behrouz Far. 2022. Reinforcement learning based recommender systems: A survey. Comput. Surveys 55, 7 (2022), 1-38. [2] Pranav Agarwal, Aamer Abdul Rahman, Pierre-Luc St-Charles, Simon JD Prince, and Samira Ebrahimi Kahou. 2023. Transformers in Reinforcement Learning: A Survey. arXiv preprint arXiv:2307.05979 (2023). [3] Xueying Bai, Jian Guan, and Hongning Wang. 2019. A model-based reinforcement learning with adversarial training for online recommendation. Advances in Neural Information Processing Systems 32 (2019). [4] Haokun Chen, Xinyi Dai, Han Cai, Weinan Zhang, Xuejian Wang, Ruiming Tang, Yuzhou Zhang, and Yong Yu. 2019. Large-scale interactive recommendation with tree-structured policy gradient. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 33. 3312-3320. [5] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. 2021. Decision Transformer: Reinforcement Learning via Sequence Modeling. In Advances in Neural Information Processing Systems , M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (Eds.), Vol. 34. Curran Associates, Inc., 15084-15097. https://proceedings.neurips.cc/paper_files/paper/2021/file/ 7f489f642a0ddb10272b5c31057f0663-Paper.pdf [6] Minmin Chen, Can Xu, Vince Gatto, Devanshu Jain, Aviral Kumar, and Ed Chi. 2022. Off-policy actor-critic for recommender systems. In Proceedings of the 16th ACM Conference on Recommender Systems . 338-349. [7] Qiwei Chen, Huan Zhao, Wei Li, Pipei Huang, and Wenwu Ou. 2019. Behavior sequence transformer for e-commerce recommendation in alibaba. In Proceedings of the 1st International Workshop on Deep Learning Practice for High-Dimensional Sparse Data . 1-4. [8] Xiaocong Chen, Chaoran Huang, Lina Yao, Xianzhi Wang, Wei liu, and Wenjie Zhang. 2020. Knowledge-guided Deep Reinforcement Learning for Interactive Recommendation. In 2020 International Joint Conference on Neural Networks (IJCNN) . 1-8. https://doi.org/10.1109/IJCNN48605.2020.9207010 [9] Xiaocong Chen, Siyu Wang, Julian McAuley, Dietmar Jannach, and Lina Yao. 2023. On the Opportunities and Challenges of Offline Reinforcement Learning for Recommender Systems. arXiv preprint arXiv:2308.11336 (2023). [10] Xiaocong Chen, Lina Yao, Julian McAuley, Guanglin Zhou, and Xianzhi Wang. 2023. Deep reinforcement learning in recommender systems: A survey and new perspectives. Knowledge-Based Systems 264 (2023), 110335. [11] Xiaocong Chen, Lina Yao, Aixin Sun, Xianzhi Wang, Xiwei Xu, and Liming Zhu. 2021. Generative inverse deep reinforcement learning for online recommendation. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management . 201-210. [12] Scott Fujimoto, Herke van Hoof, and David Meger. 2018. Addressing Function Approximation Error in Actor-Critic Methods. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018 (Proceedings of Machine Learning Research, Vol. 80) . PMLR, 1582-1591. http://proceedings.mlr.press/v80/fujimoto18a.html [13] Chongming Gao, Kexin Huang, Jiawei Chen, Yuan Zhang, Biao Li, Peng Jiang, Shiqi Wang, Zhong Zhang, and Xiangnan He. 2023. Alleviating Matthew Effect of Offline Reinforcement Learning in Interactive Recommendation. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (Taipei, Taiwan) (SIGIR '23) . Association for Computing Machinery, New York, NY, USA, 238-248. https://doi.org/10.1145/3539618.3591636 [14] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018 (Proceedings of Machine Learning Research, Vol. 80) . PMLR, 1856-1865. http: //proceedings.mlr.press/v80/haarnoja18b.html [15] Dan Hendrycks and Kevin Gimpel. 2016. Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units. CoRR abs/1606.08415 (2016). arXiv:1606.08415 http://arxiv.org/abs/1606.08415 [16] Michael Janner, Qiyang Li, and Sergey Levine. 2021. Offline Reinforcement Learning as One Big Sequence Modeling Problem. In Advances in Neural Information Processing Systems , Vol. 34. Curran Associates, Inc., 1273-1286. https://proceedings.neurips.cc/paper_files/paper/2021/file/ 099fe6b0b444c23836c4a5d07346082b-Paper.pdf [17] Wang-Cheng Kang and Julian McAuley. 2018. Self-Attentive Sequential Recommendation. In 2018 IEEE International Conference on Data Mining (ICDM) . 197-206. https://doi.org/10.1109/ICDM.2018.00035 [18] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2016. Continuous control with deep reinforcement learning. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings . http://arxiv.org/abs/1509.02971 [19] Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019. Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP) . Association for Computational Linguistics, Hong Kong, China, 188-197. https://doi.org/10.18653/v1/D19-1018 [20] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. 2023. RWKV: Reinventing RNNs for the Transformer Era. arXiv preprint arXiv:2305.13048 (2023). [21] Prajit Ramachandran, Barret Zoph, and Quoc V Le. 2017. Searching for activation functions. arXiv preprint arXiv:1710.05941 (2017). [22] Noam Shazeer. 2019. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150 (2019). [23] Jing-Cheng Shi, Yang Yu, Qing Da, Shi-Yong Chen, and An-Xiang Zeng. 2019. Virtual-Taobao: Virtualizing Real-World Online Retail Environment for Reinforcement Learning. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 33. 4902-4909. [24] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Journal of Machine Learning Research 15, 56 (2014), 1929-1958. http://jmlr.org/papers/v15/srivastava14a.html [25] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer (CIKM '19) . Association for Computing Machinery, New York, NY, USA, 1441-1450. https://doi.org/10.1145/3357384.3357895 [26] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. 2023. Retentive network: A successor to transformer for large language models. arXiv preprint arXiv:2307.08621 (2023). [27] Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An introduction . MIT press. [28] Mengting Wan and Julian McAuley. 2018. Item Recommendation on Monotonic Behavior Chains. In Proceedings of the 12th ACM Conference on Recommender Systems (Vancouver, British Columbia, Canada) (RecSys '18) . Association for Computing Machinery, New York, NY, USA, 86-94. https://doi.org/10.1145/ 3240323.3240369 [29] Siyu Wang, Xiaocong Chen, Dietmar Jannach, and Lina Yao. 2023. Causal Decision Transformer for Recommender Systems via Offline Reinforcement Learning. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (Taipei, Taiwan) (SIGIR '23) . Association for Computing Machinery, New York, NY, USA, 1599-1608. https://doi.org/10. 1145/3539618.3591648 [30] Liwei Wu, Shuqing Li, Cho-Jui Hsieh, and James Sharpnack. 2020. SSE-PT: Sequential recommendation via personalized transformer. In Fourteenth ACM Conference on Recommender Systems . 328-337. [31] Yuxin Wu and Kaiming He. 2018. Group normalization. In Proceedings of the European conference on computer vision (ECCV) . 3-19. [32] Yikun Xian, Zuohui Fu, S Muthukrishnan, Gerard de Melo, and Yongfeng Zhang. 2019. Reinforcement Knowledge Graph Reasoning for Explainable Recommendation. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval . 285-294. [33] Kesen Zhao, Lixin Zou, Xiangyu Zhao, Maolin Wang, and Dawei Yin. 2023. User Retention-Oriented Recommendation with Decision Transformer. In Proceedings of the ACM Web Conference 2023 (Austin, TX, USA) (WWW'23) . Association for Computing Machinery, New York, NY, USA, 1141-1149. https://doi.org/10.1145/ 3543507.3583418 [34] Xiangyu Zhao, Long Xia, Liang Zhang, Zhuoye Ding, Dawei Yin, and Jiliang Tang. 2018. Deep reinforcement learning for page-wise recommendations. In Proceedings of the 12th ACM Conference on Recommender Systems . 95-103. [35] Xiangyu Zhao, Liang Zhang, Zhuoye Ding, Long Xia, Jiliang Tang, and Dawei Yin. 2018. Recommendations with negative feedback via pairwise deep reinforcement learning. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 1040-1048. [36] Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, and Ji-Rong Wen. 2020. S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management . 1893-1902."}
