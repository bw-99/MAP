{"Directed Acyclic Graph Factorization Machines for CTR Prediction via Knowledge Distillation": "", "Zhen Tian": "", "\u2217\u2021": "Gaoling School of Artificial Intelligence, Renmin University of China Beijing, China chenyuwuxinn@gamil.com Ting Bai Beijing University of Posts and Telecommunications Beijing, China baiting@bupt.edu.cn Kangyi Lin Weixin Open Platform, Tencent Guangzhou, China plancklin@tecent.com", "Zibin Zhang Zhiyuan Xu": "Weixin Open Platform, Tencent Guangzhou, China bingoozhang@tecent.com zhiyuanxu@tecent.com", "\u2020": "Ji-Rong Wen Gaoling School of Artificial Intelligence, Renmin University of China Beijing, China jrwen@ruc.edu.cn", "ABSTRACT": "With the growth of high-dimensional sparse data in web-scale recommender systems, the computational cost to learn high-order feature interaction in CTR prediction task largely increases, which limits the use of high-order interaction models in real industrial applications. Some recent knowledge distillation based methods transfer knowledge from complex teacher models to shallow student models for accelerating the online model inference. However, they suffer from the degradation of model accuracy in knowledge distillation process. It is challenging to balance the efficiency and effectiveness of the shallow student models. To address this problem, we propose a D irected A cyclic G raph F actorization M achine ( KDDAGFM ) to learn the high-order feature interactions from existing complex interaction models for CTR prediction via K nowledge D istillation. The proposed lightweight student model DAGFM can learn arbitrary explicit feature interactions from teacher networks, which achieves approximately lossless performance and is proved by a dynamic programming algorithm. Besides, an improved general model KD-DAGFM+ is shown to be effective in distilling both explicit and implicit feature interactions from any complex teacher model. Extensive experiments are conducted on four real-world datasets, including a large-scale industrial dataset from WeChat platform with billions of feature dimensions. KD-DAGFM achieves \u2217 Ting Bai (baiting@bupt.edu.cn) is the corresponding author. \u2021 Also with Beijing Key Laboratory of Intelligent Telecommunications Software and Multimedia. \u2020 Also with Beijing Key Laboratory of Big Data Management and Analysis Methods. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WSDM '23, February 27-March 3, 2023, Singapore, Singapore \u00a9 2023 Association for Computing Machinery. ACM ISBN 978-1-4503-9407-9/23/02...$15.00 https://doi.org/10.1145/3539597.3570384 Wayne Xin Zhao \u2020 Gaoling School of Artificial Intelligence, Renmin University of China Beijing, China batmanfly@gmail.com the best performance with less than 21 . 5% FLOPs of the state-ofthe-art method on both online and offline experiments, showing the superiority of DAGFM to deal with the industrial scale data in CTR prediction task 1 .", "CCS CONCEPTS": "\u00b7 Information systems \u2192 Recommender systems .", "KEYWORDS": "Graph Factorization Machine; Knowledge Distillation; CTR Prediction; Recommender Systems", "ACMReference Format:": "Zhen Tian, Ting Bai, Zibin Zhang, Zhiyuan Xu, Kangyi Lin, Ji-Rong Wen and Wayne Xin Zhao. 2023. Directed Acyclic Graph Factorization Machines for CTR Prediction via Knowledge Distillation. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining (WSDM '23), February 27-March 3, 2023, Singapore, Singapore. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3539597.3570384", "1 INTRODUCTION": "Click-Through Rate (CTR) prediction, which aims to predict the probability of a user clicking on an item, is a very critical task in recommender systems. The key to achieve good performance in CTR prediction is learning the effective high-order feature interactions, which has attracted great attention in recent years [3, 7, 13, 23]. One of the biggest challenges is the high computational cost to model the high-order interactions of raw features, because the number of feature combinations increases exponentially when the number of raw features increases. In real-world applications, raw features are usually highly sparse with millions of dimensions. For example, identifier feature such as the ID of user/item is very sparse after being encoded as an one-hot vector; so are the multi-filed vectors built from upstream tasks such as visual information. It is very time-consuming to calculate the high-order feature interactions on such sparse features with millions of dimensions, and there is a 1 Our implementation code is available at: https://github.com/RUCAIBox/DAGFM WSDM'23, February 27-March 3, 2023, Singapore, Singapore Zhen Tian et al. high risk of over-fitting problem for CTR prediction in real industry recommender systems. Hence, a lightweight recommendation algorithm for CTR prediction is needed to simplify the online inference process, which enables it to avoid the explosion of computational costs in real industry recommender systems. Some efforts have been made to solve this problem [31, 35, 37]. They utilize Knowledge Distillation (KD) technique to transfer knowledge from complex teacher models to shallow student models with reduced learning parameters, so as to speed up the model inference for dealing with the real-time massive data in recommender systems. However, the acceleration of KD model with reduced learning parameters is at the expense of the degradation of the model accuracy, and it is inevitable for KD model to balance the effectiveness and efficiency [35]. The KD method [37] achieves better performance than its teacher model, but at cost of an undiminished amount of parameters of the student model, which may limit its adoption in online inference process. To keep the low model complexity and meanwhile achieve better performance of KD student model, we propose a lightweight K nowledge D istillation based D irected A cyclic G raph F actorization M achine ( KD-DAGFM ) to predict the CTR in recommender systems. Our aim is to design a shadow student model with fewer learning parameters, but maintain the capability to distill the highorder feature interactions from existing complex interaction models. To achieve the approximately lossless knowledge distillation, we carefully design the student model: Directed Acyclic Graph Factorization Machine ( DAGFM ) to learn arbitrary explicit feature interactions. Specifically, each node in Directed Acyclic Graph (DAG) represents a feature field, and different nodes are connected by directed edges without cycles. The interaction of nodes, which represents the interaction of features, are modeled by the learnable weights of edges in DAG. With the specific interaction learning functions ( i.e., inner, outer and kernel), the feature interactions on DAG can be translated into explicit arbitrary-order interactions, which is demonstrated by a dynamic programming algorithm. Except for the explicit feature interactions, to distill the knowledge from implicit feature interaction models ( e.g., AutoInt [25] and FiBiNet [10]), we further propose an improved model KD-DAGFM+, in which an Multi-Layer Perceptron (MLP) component is integrated into the student model DAGFM. Experiments show that KD-DAGFM+ has the capability to distill both explicit and implicit feature interactions from any complex teacher models. The contributions are summarized as follows: \u00b7 We propose a lightweight distillation model KD-DAGFM to learn high-order explicit interactions from different complex teacher models, which could greatly reduce the computational costs ( i.e., FLOPs) by at least 10 times, making it possible to be applied into large-scale industry recommender systems. \u00b7 Wedesign a directed acyclic graph neural network - DAGFM as the student model and use three different interaction learning functions ( i.e., inner, outer and kernel functions) in the propagation process to capture arbitrary-order explicit feature interactions, which achieves approximately lossless performance of the student model and is demonstrated by a dynamic programming algorithm. \u00b7 KD-DAGFM achieves the best performance with less than 21 . 5% FLOPs of the state-of-the-art method on both online and offline experiments, showing the superiority of KDDAGFM to deal with the large scale industry data in CTR prediction task.", "2 PRELIMINARY": "We first give a preliminary introduction to the CTR prediction task, then introduce the explicit high-order feature interaction and the interaction learning function in CTR prediction task.", "2.1 CTR Prediction": "Click-Through Rate (CTR) prediction aims to predict the probability of the user clicking on an item. Specifically, given the \ud835\udc5a feature fields, we use \ud835\udc99 = { \ud835\udc99 1 , \ud835\udc99 2 , ..., \ud835\udc99 \ud835\udc5a } to represent the feature of users and items, where \ud835\udc99 \ud835\udc56 is the feature representation of the \ud835\udc56 -th feature. Label \ud835\udc66 \u2208 { 0 , 1 } represents each item is click or not, and is predicted based on the input feature \ud835\udc99 . The key to achieve good performance in CTR prediction task is learning the effective high-order feature interactions, which is a fundamental problem and has attracted great attention in research areas.", "2.2 High-order Feature Interactions": "Given the input feature \ud835\udc99 = { \ud835\udc99 1 , \ud835\udc99 2 , ..., \ud835\udc99 \ud835\udc5a } , the embedding features of \ud835\udc99 is represented as { \ud835\udc86 1 , \ud835\udc86 2 , .., \ud835\udc86 \ud835\udc5a } , where \ud835\udc5a is the number of feature fields, the explicit feature interactions can be formulated as:  where \ud835\udf19 denotes the feature interaction learning function. Take the 2-order feature interactions as example, formulated as:", "2.3 Interaction Learning Function": "The feature interaction learning function defines the way to compute the interactions among features. Different interaction models utilize different interaction learning functions. 2.3.1 Basic Inner Interaction Learning Function. The most commonly used interaction learning function in existing studies, i.e., FM [22], IM [33], HOFM [1], is the basic inner interaction learning function, and it can be formulated as:  where \u2299 is the element-wise product of two vectors. 2.3.2 Weighted Inner Interaction Learning Function. The above methods with basic inner function can not capture the distinct field information of features, leading to gradient coupled issue [21]. To address this problem, many field-aware variants such as FwFM [19] and FvFM [26] use weight parameters to model the feature interactions of different fields. The inner interaction learning function used in them can be formulated as:  where \ud835\udc98 \ud835\udc56,\ud835\udc57 is a weight vector between the fields \ud835\udc56 and \ud835\udc57 . WSDM'23, February 27-March 3, 2023, Singapore, Singapore Directed Acyclic Graph Factorization Machines for CTR Prediction via Knowledge Distillation \ud835\udc86\ud835\udfcf \ud835\udc86\ud835\udfd0 \ud835\udc86\ud835\udfd1 \ud835\udc86\ud835\udfd2 \ud835\udc95 = \ud835\udfcf \ud835\udc95 = \ud835\udfd0 \ud835\udc95 = \ud835\udfd1 \ud835\udc89\ud835\udfcf \ud835\udfcf \ud835\udc89\ud835\udfd0 \ud835\udfcf \ud835\udc89\ud835\udfd1 \ud835\udfcf \ud835\udc89\ud835\udfd2 \ud835\udfcf \ud835\udc8f\ud835\udfcf \ud835\udc8f\ud835\udfd0 \ud835\udc8f\ud835\udfd1 \ud835\udc8f\ud835\udfd2 \ud835\udc6f\ud835\udfcf \ud835\udc6f\ud835\udfd0 \ud835\udc6f\ud835\udfd1 \ud835\udc6f\ud835\udfd2 \u0de1 \u0ddc \ud835\udc66 Embedding Layer \ud835\udc99\ud835\udfcf \ud835\udc99\ud835\udfd0 \ud835\udc99\ud835\udfd1 \ud835\udc99\ud835\udfd2 Field 1 Field 2 Field 3 Field 4 Field 1 (a) The architecture of DAGFM. Field 2 Field 3 Field 4 \ud835\udc59 = 1 Linear Transformation Predicted CTR \ud835\udc53 S(\ud835\udc86) = \u0dcd \ud835\udc95,\ud835\udc8b \ud835\udf4b\ud835\udc8b (\ud835\udc86\ud835\udc8b\ud835\udfcf \ud835\udc86\ud835\udc8b\ud835\udfd0 \ud835\udc86\ud835\udc8b\ud835\udfd1 \u2026\ud835\udc86\ud835\udc8b\ud835\udc95 ) \ud835\udc53 \ud835\udc47(\ud835\udc86) = \u0dcd \ud835\udc95,\ud835\udc8b \ud835\udf4d\ud835\udc8b (\ud835\udc86\ud835\udc8b\ud835\udfcf \ud835\udc86\ud835\udc8b\ud835\udfd0 \ud835\udc86\ud835\udc8b\ud835\udfd1 \u2026\ud835\udc86\ud835\udc8b\ud835\udc95 ) \ud835\udc6f\ud835\udfcf \ud835\udc6f\ud835\udfd0 \ud835\udc6f\ud835\udfd1 \ud835\udc6f\ud835\udfd2 Teacher Network Student Network Embedding Layer Sigmoid Input Label \ud835\udc8f\ud835\udfcf \ud835\udc8f\ud835\udfd0 \ud835\udc8f\ud835\udfd1 \ud835\udc8f\ud835\udfd2 Logits Logits (b) The architecture of KD-DAGFM. Figure 1: Components and architecture of our proposed KD-DAGFM. 2.3.3 Kernel Interaction Learning Function. By extending the dimension of weight vector, recent studies [21, 26] propose a more powerful interaction learning function, i.e., kernel interaction learning function, defined as: high-dimensional sparse space to a lower-dimensional dense space, i.e., \ud835\udc86 \ud835\udc56 = \ud835\udc99 \ud835\udc56 \ud835\udc7e \ud835\udc56 , where \ud835\udc7e \ud835\udc56 \u2208 R | \ud835\udc99 \ud835\udc56 |\u00d7 \ud835\udc51 and \ud835\udc51 is the embedding size.  where \ud835\udc7e \ud835\udc56,\ud835\udc57 is the learning weight matrix. Despite the great improvements by incorporating field weights, the model complexity has largely increased. Enumerating all highorder feature interactions with distinct field weights is an NP-hard problem. Most existing high-order feature interaction models [16, 28] improve the model capability by increasing the parameters, but at the expense of high computational costs, making it impractical to be applied into large-scale industry recommendation scenarios.", "3 METHODOLOGY": "To improve model performance with low computational costs, we propose a Knowledge Distillation based Directed Acyclic Graph Factorization Machine (KD-DAGFM), in which a lightweight student model Directed Acyclic Graph Factorization Machine (DAGFM) is designed to learn the high-order feature interactions from existing complex teacher models.", "3.1 Directed Acyclic Graph Factorization Machine (DAGFM)": "To effectively and efficiently extract knowledge from complex highorder interaction models, we design a lightweight student model, named D irected A cyclic G raph F actorization M achine ( DAGFM ). The architecture of DAGFM is shown in Fig. 1 (a), we first introduce the construction of Directed Acyclic Graph (DAG) and the information interaction process, then introduce our proposed efficient outer interaction learning function. 3.1.1 The Construction of DAG. As shown in Fig. 1(a), given the input features \ud835\udc99 = { \ud835\udc99 1 , \ud835\udc99 2 , ..., \ud835\udc99 \ud835\udc5a } , where \ud835\udc5a is the number of total feature fields, and \ud835\udc99 \ud835\udc56 is the feature representation of the \ud835\udc56 -th feature. The embedding layer projects the input features from a The directed acyclic graph (DAG) is defined as G = (N , E) , where each node \ud835\udc5b \ud835\udc56 \u2208 N corresponds to a feature field and |N| = \ud835\udc5a . The initial state vector of each node \ud835\udc5b \ud835\udc56 is set to \ud835\udc86 \ud835\udc56 , i.e., \ud835\udc89 1 \ud835\udc56 = \ud835\udc86 \ud835\udc56 . The edge from node \ud835\udc5b \ud835\udc56 to \ud835\udc5b \ud835\udc57 ( \ud835\udc57 > \ud835\udc56 ) is directed, which controls the interaction weight between two features from different fields. 3.1.2 Learning Feature Interactions on DAG. Theinteractions among features can be modeled as the information propagation process on DAG in a recurrent fashion. At each propagation layer, each node will aggregate the state from its neighbors and combine it with the initial state \ud835\udc89 1 \ud835\udc56 of itself to update the state vector. Formally, the state vector of node \ud835\udc5b \ud835\udc56 at propagation layer \ud835\udc61 can be represented as:  where N( \ud835\udc56 ) = { \ud835\udc57 | \ud835\udc5b \ud835\udc57 \u2192 \ud835\udc5b \ud835\udc56 \u2208 E} , and \ud835\udf19 is the interaction learning function. To obtain the information of high-order feature interactions for CTR prediction, we first apply sum pooling of the hidden state vector for each node as \ud835\udc5d \ud835\udc61 \ud835\udc56 = \u02dd \ud835\udc51 \ud835\udc58 = 1 \u210e \ud835\udc61 \ud835\udc56,\ud835\udc58 at each interaction step, where \u210e \ud835\udc61 \ud835\udc56,\ud835\udc58 denotes the \ud835\udc58 -th element of \ud835\udc89 \ud835\udc61 \ud835\udc56 . Then we concatenate all node statesp at layer \ud835\udc61 as \ud835\udc91 \ud835\udc61 = [ \ud835\udc5d \ud835\udc61 1 , \ud835\udc5d \ud835\udc61 2 , ..., \ud835\udc5d \ud835\udc61 \ud835\udc5a ] , and the differentorder interactions of features on DAG can be represented as \ud835\udc91 = [ \ud835\udc91 1 ; \ud835\udc91 2 ; ... ; \ud835\udc91 \ud835\udc59 ] , where the number of propagation layers is \ud835\udc59 -1. Finally, the prediction function for CTR prediction can be formulated as:  where \ud835\udf0e is the sigmoid activation function, \ud835\udc98 is the transition vector and \ud835\udc4f is the bias. 3.1.3 Outer Interaction Learning Function. The feature interaction learning function \ud835\udf19 of DAGFM (see in Eq. 6) is flexible, we can utilize the most commonly used learning functions, i.e., the inner interaction learning function (see Eq. 4) or the kernel interaction learning function with more powerful model capability (see Eq. 5), to learn the high-order feature interactions. WSDM'23, February 27-March 3, 2023, Singapore, Singapore Zhen Tian et al. To reduce the computational complexity of kernel interaction learning function, we propose a simplified outer interaction learning function , in which the learning weight matrix in kernel interaction learning function (see Eq. 5) can be decomposed as:  where \ud835\udc91 \ud835\udc61 \ud835\udc57,\ud835\udc56 \u2208 R \ud835\udc51 and \ud835\udc92 \ud835\udc61 \ud835\udc57,\ud835\udc56 \u2208 R \ud835\udc51 are the decomposed vectors. Then the outer interaction learning function can be defined as:  Our proposed outer interaction learning function is decomposed from kernel interaction learning function (see Eq. 5) and has the same complexity with inner interaction learning function ( i.e., \ud835\udc42 ( \ud835\udc51 ) ), which is much less than kernel interaction learning function ( i.e., \ud835\udc42 ( \ud835\udc51 2 ) ), making it more efficient to capture the feature interactions.", "3.2 Arbitrary-Order Interaction Learning in DAGFM": "In this section, we demonstrate that the feature interactions learned in DAGFM align with different unique weighted paths in a dynamic programming (DP) algorithm, showing the ability of DAGFM to learn arbitrary-order feature interactions. We define the suffix of a feature interaction as the feature with the largest subscript. For example, for the interactions with feature \ud835\udc86 2 \ud835\udc86 3 and \ud835\udc86 3 \ud835\udc86 4 \ud835\udc86 5, the suffix are \ud835\udc86 3 and \ud835\udc86 5 respectively. Let \ud835\udc7a \ud835\udc61 \ud835\udc56 denote the set of all \ud835\udc61 -th order feature interactions suffixed with feature \ud835\udc86 \ud835\udc56 . Take 2-order feature interactions suffixed with \ud835\udc86 3 as example: \ud835\udc7a 2 3 = { \ud835\udc86 1 \ud835\udc86 3 , \ud835\udc86 2 \ud835\udc86 3 , \ud835\udc86 3 \ud835\udc86 3 } . The 1-order feature interaction suffixed with \ud835\udc86 \ud835\udc56 is itself, i.e., \ud835\udc7a 1 \ud835\udc56 = { \ud835\udc86 \ud835\udc56 } . In dynamic programming algorithm, the state variable \ud835\udc89 \ud835\udc61 \ud835\udc56 is defined as the sum of elements in \ud835\udc7a \ud835\udc61 \ud835\udc56 , i.e., \ud835\udc89 \ud835\udc61 \ud835\udc56 = \u02dd \ud835\udc8b \u2208 \ud835\udc7a \ud835\udc61 \ud835\udc56 \ud835\udc8b . In fact, all \ud835\udc61 + 1-th order interactions suffixed with \ud835\udc86 \ud835\udc56 can be expressed as the interaction between \ud835\udc86 \ud835\udc56 and \ud835\udc61 -th order interactions suffixed with \ud835\udc86 \ud835\udc57 ( \ud835\udc57 \u2264 \ud835\udc56 ). Therefore, the state transfer equation of \ud835\udc89 \ud835\udc61 \ud835\udc56 in DP algorithm can be formulated as:  In DAGFM, the feature interactions are propagated based on a directed acyclic graph, which means the neighbors N( \ud835\udc56 ) of node \ud835\udc5b \ud835\udc56 refer to the directly linked nodes \ud835\udc5b \ud835\udc57 , where \ud835\udc57 < \ud835\udc56 . Hence, the state vector of node \ud835\udc5b \ud835\udc56 in graph propagation process in Eq. 6 is equivalent to the transferred state in DP algorithm (see Eq. 10).  here the interaction learning function \ud835\udf19 is equal to the basic inner interaction learning function (see in Eq. 3). From Eq. 10 and Eq. 11, we can see that each feature interaction in DAGFM can align with a unique path in DP graph (see in Fig. 2): each \ud835\udc58 -order feature interaction corresponds to a unique path with length \ud835\udc58 -1 from the first layer. By summing all the state vectors in DP algorithm, all the paths can be traversed, indicating all the feature interactions can be modeled in DAGFM. Figure 2: The propagation graph of DAGFM. Each \ud835\udc58 -order feature interaction corresponds to a unique path with length \ud835\udc58 -1 in the dynamic programming algorithm. \ud835\udc5b1 4 \ud835\udc5b2 4 \ud835\udc5b3 4 \ud835\udc5b4 4 \ud835\udc5b1 3 \ud835\udc5b2 3 \ud835\udc5b3 3 \ud835\udc5b4 3 \ud835\udc5b1 2 \ud835\udc5b2 2 \ud835\udc5b3 2 \ud835\udc5b4 2 \ud835\udc5b1 1 \ud835\udc5b2 1 \ud835\udc5b3 1 \ud835\udc5b4 1 \ud835\udc95 = \ud835\udfcf \ud835\udc95 = \ud835\udfd0 \ud835\udc95 = \ud835\udfd1 male weekday Canada \ud835\udc5b1 1 male \ud835\udc5b2 2 \ud835\udc5b3 3 weekday x x Canada \ud835\udc98\ud835\udfcf,\ud835\udfd0 \ud835\udfcf \ud835\udc98\ud835\udfd0,\ud835\udfd1 \ud835\udfd0 \ud835\udc89\ud835\udfd1 \ud835\udfd1 \ud835\udc89\ud835\udfd0 \ud835\udfd0 \ud835\udc89\ud835\udfcf \ud835\udfcf For graph propagation processes with inner (see Eq. 4) and kernel (see Eq. 5) interaction learning functions, they can be equivalent to Eq. 11 by assigning the weights \ud835\udc98 \ud835\udc61 \ud835\udc57,\ud835\udc56 to be all-ones vector and identity matrix respectively. Besides, the weights can also be learned during training process, which enables DAGFM to model all types of feature interactions with different semantic information. For example, let nodes \ud835\udc5b 1 , \ud835\udc5b 2 , \ud835\udc5b 3 , \ud835\udc5b 4 represent male , weekday , Canada , sunny at first layer respectively. Feature interaction male \u00d7 weekday \u00d7 Canada corresponds to the path \ud835\udc5b 1 1 \u2192 \ud835\udc5b 2 2 \u2192 \ud835\udc5b 3 3 marked red color with the learning weights \ud835\udc98 1 1 , 2 \u2299 \ud835\udc98 2 2 , 3 . The edge weight \ud835\udc98 \ud835\udc61 \ud835\udc56,\ud835\udc57 controls the strength of the interaction between \ud835\udc56 -th feature and \ud835\udc57 -th feature at \ud835\udc61 -th order interactions. In the above example, \ud835\udc98 1 1 , 2 controls the semantic strength of male \u00d7 weekday in male \u00d7 weekday \u00d7 Canada .", "3.3 Knowledge Distillation Process": "The overall distillation framework of KD-DAGFM is shown in Fig. 1(b). In knowledge distillation process, the student network DAGFM learns the knowledge of feature interactions from teacher networks. To ensure the feature space in student and teacher networks be the same, we share the embedding layer of teacher network with student network. The loss function in knowledge distillation can be formulated as:  where \ud835\udc41 is the total number of training instances, \ud835\udc47 and \ud835\udc46 denote the learning function, \ud835\udf4d and \ud835\udf4b are the parameters in teacher and student networks respectively. Besides, we adopt cross entropy loss function for CTR prediction, which could be formulated as:  where \ud835\udc66 \ud835\udc56 is the true label , \u02c6 \ud835\udc66 \ud835\udc56 is predicted result. The final optimization objective of the knowledge distillation model KD-DAGFM is:  where \ud835\udefc, \ud835\udefd are hyper-parameters to balance the weights of two loss functions. After knowledge distillation, the student model can well learn the useful feature interactions from the teacher network. However, Directed Acyclic Graph Factorization Machines for CTR Prediction via Knowledge Distillation WSDM'23, February 27-March 3, 2023, Singapore, Singapore Table 1: The statistics of datasets. since the feature embedding of student is inherited from teacher which is not trained during distillation, it may result in the underfitting problem and make sub-optimal performance of the student model. To alleviate this problem, we fine-tune the student model for the final CTR prediction.", "4 EXPERIMENTS": "We conduct experiments to demonstrate the effect of our proposed knowledge distillation model KD-DAGFM. Four real world datasets are used in our experiments: Criteo 2 , Avazu 3 , MovieLens-1M 4 and WeChat. Criteo is the most popular CTR prediction benchmark dataset contains user logs over a period of 7 days. Avazu contains user logs over a period of 7 days, which was used in the Avazu CTR prediction competition. MovieLens-1M is the most popular dataset for recommendation systems research. WeChat is collected on WeChat platform, which contains more than one hundred million daily active users. Table 1 summarizes the dataset statistics information. We first verify the effectiveness of our proposed student model DAGFM and then compare the model performance of KD-DAGFM with the state-of-the-art interaction models for CTR prediction task.", "4.1 Effectiveness Analysis of the Student Model DAGFM": "In this section, we conduct experiments to demonstrate that our proposed student model DAGFM can achieve the approximately lossless performance of the complex teacher models. 4.1.1 The Teacher Networks. We choose the most commonly used complex models with competitive performance in CTR prediction task, i.e., xDeepFM [16] and DCNV2 [28]. Due to the fact that DAGFM is an explicit model and its distillation from deep implicit component may incorporate noise to degrade model performance, for fair comparison and speed up the knowledge distillation process, we only use the explicit feature interaction component CIN and CrossNet as the teacher models in xDeepFM and DCNV2 respectively. (results with different types of teachers, including both implicit and explicit teacher networks, are shown in Sec. 4.3.4.) 4.1.2 The Student Networks. Considering the high computational costs of deep methods may limit their adoption in large-scale recommendation scenarios, we use the most commonly used light models: FwFM [19], FmFM [26], and tiny MLP ( i.e., the MLP with only three layers: 128 \u00d7 128 \u00d7 128) as the student models for comparison. For DAGFM, we adopt the best combination of teachers and interaction learning functions, i.e., CIN with DAGFM-inner and 2 http://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset 3 http://www.kaggle.com/c/avazu-ctr-prediction 4 https://grouplens.org/datasets/movielens Table 2: Effectiveness comparisons of different student models. \ud835\udc59 is the depth, \ud835\udc5a is the number of feature fields, \ud835\udc51 is the embedding size, \ud835\udc3b is the dimension of hidden vectors. CrossNet with DAGFM-outer (more detailed experimental results are analyzed in Sec. 4.3.2). 4.1.3 Implementation Details. The experimental settings of the knowledge distillation process is identical to the settings of KDDAGFM in Section 4.2. The details of implementation are summarized in Section 4.2.2. Figure 3: The loss curves in knowledge distillation process of different student models. 0 100 200 300 step 10 -2 10 -1 10 0 KD loss Criteo 0 100 200 300 step 10 -3 10 -2 10 -1 10 0 KD loss Avazu DAGFM-inner FmFM FwFM Tiny MLP (a) CIN 0 100 200 300 step 10 -2 10 -1 10 0 KD loss Criteo 0 100 200 300 step 10 -2 10 -1 10 0 KD loss Avazu DAGFM-outer FmFM FwFM Tiny MLP (b) CrossNet 4.1.4 The Experimental Results of Different Student Models. The experimental results of different student models on Criteo and Avazu datasets are shown in Table 2, we can see that: (1) FwFM and FmFM perform the worst due to the limited 2-order feature interaction capability, which can not well fit the high-order information of the teacher network. (2) Tiny MLP learns high-order feature interactions with multilayer neural networks in an implicit manner, and its capability is limited by the shallow model architecture. WSDM'23, February 27-March 3, 2023, Singapore, Singapore Zhen Tian et al. Table 3: Performance comparisons. Note that a higher AUC or lower Logloss at 0.001-level is significant for CTR prediction. (3) DAGFM achieves competitive performance compared with the teacher network, showing its capability to transfer the highorder feature interaction information. \u00b7 FiGNN [14]: it uses GRU components to model implicit feature interactions in a fully-connected graph. (4) As the student network, the model complexity of DAGFM ( i.e., \ud835\udc5a 2 \ud835\udc51\ud835\udc59 ) is much lower than the teacher model CIN ( i.e., \ud835\udc5a\ud835\udc3b 2 \ud835\udc51\ud835\udc59 ) and CrossNet ( i.e., \ud835\udc5a 2 \ud835\udc51 2 \ud835\udc59 ) ( \ud835\udc51 is the size of embedding and \ud835\udc3b is the dimension of hidden vectors, which are much larger than the number of fields \ud835\udc5a ). For the student networks, the complexity of DAGFMismuchlowerthan FmFM and Tiny MLP and is comparable with FwFM. We also show the distillation loss in training process. As shown in Fig. 3: the deviation of DAGFM from the teacher network is very small (10 -3 ), which is almost 1000 times smaller than it in the tiny MLP model, 30 times and 300 times smaller than it in FmFM and FwFM respectively, showing the approximately lossless distillation capability of DAGFM.", "4.2 Experimental Results of KD-DAGFM": "We conduct experiments to show the effectiveness of our proposed knowledge distillation model KD-DAGFM in CTR prediction task. 4.2.1 Compared Methods. We compare KD-DAGFM with state-ofthe-art methods in CTR prediction task, including: \u00b7 FmFM [26]: it utilizes kernel product with matrix weights to capture the field information. \u00b7 FwFM [19]: it utilizes inner product with scalar weights to capture the field information. \u00b7 xDeepFM [16]: it utilizes inner product to generate multiple feature maps, which encode all pairwise interactions to model explicit interactions. \u00b7 DCNV2 [28]: it utilizes kernel product to model vector-wise feature interactions and achieves state-of-the-art performance. \u00b7 FiBiNet [10]: it uses SENet and bi-linear operation to improve pair-wise feature interactions. \u00b7 AutoInt [25]: it uses multi-head self-attentive mechanism to learn implicit feature interactions. AutoInt+ improves AutoInt by combining a feed-forward neural network. \u00b7 GraphFM [15]: it performs graph sampling and utilizes multihead attentive mechanism to model implicit feature interactions in a two-stage manner. \u00b7 ECKD [37]: it is a knowledge distillation based approach, and utilizes large-scale MLP as student network to distill the knowledge from several state-of-the-art teachers. The above approaches could cover different types in the area of CTR prediction. FmFM, FwFM, xDeepFM and DCNV2 model explicit feature interactions. The rest methods model implicit feature interactions. Specially, GraphFM and FiGNN are GNN based methods and ECKD is a knowledge distillation based approach. Our proposed KD-DAGFM is a knowledge distillation model based on the field-aware graph neural networks, and is able to capture the high-order explicit feature interactions. 4.2.2 Implementation Details. For each method, grid search is applied to find the optimal settings. All methods are implemented in Pytorch [20]. The dimension of feature embedding is 16. The learning rate is in [1e-3, 1e-4, 1e-5]. The \ud835\udc3f 2 penalty weight is 1e-5. The optimizer is Adam. The distillation loss weights \ud835\udefc is in [1e-1, 1, 10] and \ud835\udefd is in [10, 100, 1000]. The depth of CIN and CrossNet is in [2, 3, 4]. The layer size of CIN is in [100, 200, 400]. For DAGFM, the number of propagation layers is in [2, 3, 4]. For FwFM and FmFM, we adopt field-wise linear weight. For AutoInt+, the depth, number of head and attention size is 2, 2, 40 respectively. For ECKD, we choose AutoInt+, DCNV2 and xDeepFM as teachers and adopt \"soft label+pre-train\" scheme. For KD-DAGFM, we report the results with the simple CIN and CrossNet as teachers and the number of propagation layers of DAGFM is equal to the depth of teacher. 4.2.3 The Experimental Results of Different CTR Methods. Wepresent the results of different models for CTR prediction task in Table 3. We have the following observations: (1) Deep methods ( i.e., xDeepFM, DCNV2, AutoInt+) outperform the shallow explicit models ( i.e., FwFM, FmFM) on the public Directed Acyclic Graph Factorization Machines for CTR Prediction via Knowledge Distillation WSDM'23, February 27-March 3, 2023, Singapore, Singapore datasets, which indicates the importance of modeling high-order feature interactions. (2) Compared with the implicit methods ( i.e., AutoInt+, FiGNN, FiBiNet), explicit methods ( i.e., xDeepFM and DCNV2) achieve better performance on Criteo and MovieLens-1M datasets, which suggests the explicit feature interactions are important patterns for CTR prediction task. As for the datasets Avazu and WeChat, the performance of explicit methods loses the competition with implicit models, due to the over-fitting problem with high sparsity of features. (3) Most graph-based models ( i.e., FiGNN, DAGFM) except for GraphFM perform competitively on Criteo, Avazu and Movielens1M datasets, which suggests that the performance of graph-based model is also influenced by the different aggregation strategies and graph topology. (4) For the knowledge distillation model ECKD, by transferring knowledge from multiple teachers, it achieves better performance than graph-based methods, showing the effectiveness of knowledge distillation. (5) Our proposed model KD-DAGFM achieves the best performance with knowledge distillation and fine-tuning. KD-DAGFM with knowledge distillation performs better than DAGFM, showing the effectiveness of learning high-order feature interaction from complex teacher models. The fine-tuning of KD-DAGFM improves the model performance due to the alleviation of the under-fitting problem of the student's feature embedding. (6) Different interaction learning functions in KD-DAGFM have different effects: DAGFM-inner outperforms DAGFM-outer on Avazu and WeChat datasets, and loses the advantages on Criteo and Movielens datasets. (7) Comparing the number of parameters and FLOPs, we can see that FwFM is the most efficient algorithm due to its simplicity. The DNN based methods are much more time-consuming due to their complicated feature interacting units, which makes them impractical in large-scale recommendation scenarios. In contrast, DAGFM is an efficient model, in which the amount of parameters is comparable with that in the shallow linear methods. It could greatly reduce the computational costs (i.e., FLOPs) by more than 10 times compared with the teacher model, besides, the latency of DAGFMis much more smaller than the other models aside from the light model FwFM, making it possible to be applied into large-scale industry recommender systems. To summarize, our proposed KD-DAGFM could effectively learn the high-order feature interactions. The fewer parameters make it much more efficient to be applied into large-scale recommendation scenarios.", "4.3 Experimental Analysis of KD-DAGFM": "We analyze the factors that influence the model performance and conduct online A/B testing to show the superiority of KD-DAGFM in real industry recommender systems. Besides, an advanced knowledge distillation model KD-DAGFM+ is proposed, which improves its ability to be applied into a wider range of applications. 4.3.1 The Effect of Number of Propagation Layers. We explore the effects of the number of propagation layers, which implies the orders of feature interactions. We use CIN as the teacher network with 3 layers operation (4-order interactions), and the model performance on Criteo dataset is shown in Fig. 4. We can observe that as the number of layers increases, the model performance becomes better, showing the importance of learning the high-order feature interaction information for CTR prediction. The improvements of the model with 2 propagation layers (modeling 3-order interactions) are significant compared with the one layer model (modeling 2-order interactions), consequently becoming slight as the layers increase, showing the crucial of 2-order and 3-order interactions. Figure 4: The performance of KD-DAGFM with different number of propagation layers. 1 2 3 4 Depth 0.8060 0.8072 0.8084 0.8096 0.8108 0.8120 AUC 0.440 0.441 0.442 0.443 0.444 0.445 Log Loss (a) Performance 0 50 100 150 200 250 300 Step 10 -2 10 -1 KD loss Layer=1 Layer=2 Layer=3 Layer=4 (b) KD Loss 4.3.2 The Effect of Different Interaction Learning Functions. As the performance of knowledge distillation model is influenced by the interaction learning functions, we conduct experiments to explore the correlation between teacher models and the interaction learning functions in KD-DAGFM. As shown in Fig. 5, we can see that DAGFMwith our proposed outer learning function can be well suitable for different teacher models ( i.e., CIN and CrossNet with inner and kernel interaction learning function respectively), while the DAGFM with inner function performs worse with teacher model CrossNet. Our proposed outer function is decomposed from kernel function, which can be degenerated into inner function, making it more effective to transfer knowledge from the teacher models. Figure 5: The performance of KD-DAGFM with different interaction learning functions. CIN CrossNet 0.806 0.810 0.814 AUC CIN CrossNet 0.438 0.442 0.446 Log Loss DAGFM-inner DAGFM-outer DAGFM-kernel 4.3.3 Online A/B Testing. We conduct online experiments on a large scale industry recommender systems i.e., WeChat Official Account Platform 5 . We calculate the gain of the corresponding evaluation metrics: # Click Users: the number of users had clicked the recommended articles; CTR: the Click Through Rate. The SOTA model is an extended version of DCNV2 which achieves the best 5 https://mp.weixin.qq.com/ WSDM'23, February 27-March 3, 2023, Singapore, Singapore Zhen Tian et al. Table 4: Online A/B test results on WeChat Official Account Platform. Higher \u2191 is better for Click Users and CTR, while lower \u2193 is better for FLOPs and Latency. Table 5: Distillation performance of KD-DAGFM+. performance in online service. Our model KD-DAGFM learns from the CrossNet (as teacher network) with outer interaction learning function. From Table 4, we can see that: compared with the SOTA method, KD-DAGFM gains in both the number of click users and the CTR. More importantly, KD-DAGFM outperforms the SOTA method with only 21.5% computational costs and its inference speed is increased by nearly 3 times, showing the superiority of KDDAGFM to deal with the web-scale data in real industry recommender systems. 4.3.4 A General Distillation Model KD-DAGFM+. Considering the teacher models are complex, i.e., learning both explicit and implicit feature interactions, in order to effectively extract the knowledge from both explicit and implicit feature interactions, we propose an advanced distillation model KD-DAGFM+, in which an MLP component are attached after the student model DAGFM (the representations of all node states in DAG is concatenated to feed into the MLP). Four widely used methods i.e., xDeepFM, DCNV2, AutoInt+ and FiBiNet, are chosen as teacher networks, and the results are shown in Table 5. We can observe that KD-DAGFM+ achieves approximately lossless distillation performance and effectively improves performance during fine-tuning. Besides, comparing with the complex teacher networks, the inference latency of KD-DAGFM+ is reduced greatly.", "5 RELATED WORK": "Feature Interactions Learning. Learning feature interactions is a fundamental problem in CTR prediction tasks. FM [22] is the most basic and widely used model, which assigns a learnable embedding vector to each feature to capture the second-order feature interactions. Besides, HOFM [1] captures the high-order feature interactions; FwFM [19] and FmFM [26] utilize inner product and kernel product to capture field interaction respectively. To better model high order feature interactions, lots of deep learning based methods learn feature interactions through a well-designed component and incorporate a deep neural network. For example, DeepFM [5] imposes a factorization machine as explicit component; xDeepFM [16] and DCNV2 [28] propose the Compressed Interaction Network (CIN) and Cross Network to model the explicit feature interactions. Besides, recent research [2, 11, 17, 18, 24, 34, 36] trend focuses on learning effective feature interactions via AutoML. However, these deep models have extremely high computational complexity, making them impractical be applied to real-time large-scale recommendation scenarios. Graph Neural Networks. In recent years, Graph Neural Network is widely used in recommender systems. Many GNN-based studies [8, 29, 30, 32] capture the collaborative signal existing in the user-item bipartite graph and treats the recommendation task as a link prediction problem. In the area of GNN based CTR predictions, DG-ENN [6] exploits the strengths of graph representation with two carefully designed learning strategies to refine the feature embeddings; Fi-GNN [14] learns implicit feature interactions by graph propagation on a fully-connected graph; GraphFM [15] proposes a two-stage aggregation strategy to model high-order feature interactions; PCF-GNN [12] proposes a pre-trained model to generate the explicit semantic cross features. Interestingly, a recent study [4] points out that GNNs are claimed to align with dynamic programming. In this work, we propose a lightweight and directed acyclic graph based model to learn the explicit high-order feature interactions. Knowledge Distillation. Knowledge Distillation (KD) [9] is a method to extract knowledge from sophisticated models and compress it into a simple model, so that it can be deployed to practical applications. As KD has achieved great success in many research fields such as natural language processing [27], the KD based models [31, 35, 37] have attracted increasing attention in recommender systems. ECKD [37] is an ensemble learning approach via knowledge distillation to deliver the performance of MLP. It achieves better performance than its teacher at cost of an undiminished amount of parameters of its student. For model compression, the general defect of knowledge distillation is the degradation of model accuracy because the capacity gap between large teachers and small students always exists. The design of an effective student model is still a challenge.", "6 CONCLUSION": "In this paper, we propose a lightweight knowledge distillation model KD-DAGFM with a directed acyclic graph neural network for CTR predictions. The directed acyclic graph structure proposed in KDDAGFM makes it possible to align with a DP algorithm, which improves its capability to learn arbitrary feature interactions from the complex teacher networks and makes it achieve approximately lossless model performance. The information propagation in the student model DAGFM with different interaction learning function could greatly reduce the computational resource costs. KDDAGFM achieves the best performance on both online and offline experiments, showing the superiority of DAGFM to deal with the industrial scale data in CTR prediction task. Directed Acyclic Graph Factorization Machines for CTR Prediction via Knowledge Distillation WSDM'23, February 27-March 3, 2023, Singapore, Singapore", "ACKNOWLEDGMENTS": "This work is supported by the National Natural Science Foundation of China under Grant No. 62102038 and 62222215, and Weixin Open Platform under Grant No. S2021120.", "REFERENCES": "[1] Mathieu Blondel, Akinori Fujino, Naonori Ueda, and Masakazu Ishihata. 2016. Higher-order factorization machines. Advances in Neural Information Processing Systems 29 (2016). [2] Yifan Chen, Pengjie Ren, Yang Wang, and Maarten de Rijke. 2019. Bayesian personalized feature interaction selection for factorization machines. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval . 665-674. [3] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems . 7-10. [4] Andrew Dudzik and Petar Veli\u010dkovi\u0107. 2022. Graph Neural Networks are Dynamic Programmers. arXiv preprint arXiv:2203.15544 (2022). [5] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. arXiv preprint arXiv:1703.04247 (2017). [6] Wei Guo, Rong Su, Renhao Tan, Huifeng Guo, Yingxue Zhang, Zhirong Liu, Ruiming Tang, and Xiuqiang He. 2021. Dual Graph enhanced Embedding Neural Network for CTR Prediction. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining . 496-504. [7] Xiangnan He and Tat-Seng Chua. 2017. Neural factorization machines for sparse predictive analytics. In Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval . 355-364. [8] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for recommendation. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval . 639-648. [9] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 2, 7 (2015). [10] Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. 2019. FiBiNET: combining feature importance and bilinear feature interaction for click-through rate prediction. In Proceedings of the 13th ACM Conference on Recommender Systems . 169-177. [11] Farhan Khawar, Xu Hang, Ruiming Tang, Bin Liu, Zhenguo Li, and Xiuqiang He. 2020. Autofeature: Searching for feature interactions and their architectures for click-through rate prediction. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management . 625-634. [12] Feng Li, Bencheng Yan, Qingqing Long, Pengjie Wang, Wei Lin, Jian Xu, and Bo Zheng. 2021. Explicit Semantic Cross Feature Learning via Pre-trained Graph Neural Networks for CTR Prediction. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval . 2161-2165. [13] Zeyu Li, Wei Cheng, Yang Chen, Haifeng Chen, and Wei Wang. 2020. Interpretable click-through rate prediction through hierarchical attention. In Proceedings of the 13th International Conference on Web Search and Data Mining . 313-321. [14] Zekun Li, Zeyu Cui, Shu Wu, Xiaoyu Zhang, and Liang Wang. 2019. Fi-gnn: Modeling feature interactions via graph neural networks for ctr prediction. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management . 539-548. [15] Zekun Li, Shu Wu, Zeyu Cui, and Xiaoyu Zhang. 2021. GraphFM: Graph Factorization Machines for Feature Interaction Modeling. arXiv preprint arXiv:2105.11866 (2021). [16] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong Sun. 2018. xdeepfm: Combining explicit and implicit feature interactions for recommender systems. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1754-1763. [17] Bin Liu, Niannan Xue, Huifeng Guo, Ruiming Tang, Stefanos Zafeiriou, Xiuqiang He, and Zhenguo Li. 2020. AutoGroup: Automatic feature grouping for modelling explicit high-order feature interactions in CTR prediction. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . 199-208. [18] Bin Liu, Chenxu Zhu, Guilin Li, Weinan Zhang, Jincai Lai, Ruiming Tang, Xiuqiang He, Zhenguo Li, and Yong Yu. 2020. Autofis: Automatic feature interaction selection in factorization models for click-through rate prediction. In Proceedings"}
