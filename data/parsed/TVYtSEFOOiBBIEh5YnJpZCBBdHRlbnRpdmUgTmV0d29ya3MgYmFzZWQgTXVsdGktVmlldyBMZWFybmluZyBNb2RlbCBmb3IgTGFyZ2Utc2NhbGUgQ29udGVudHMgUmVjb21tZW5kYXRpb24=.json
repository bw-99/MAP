{"MV-HAN: A Hybrid Attentive Networks based Multi-View Learning Model for Large-scale Contents Recommendation": "Ge Fan Tencent Inc. Shenzhen, China ge.fan@outlook.com Kai Wang Tencent Inc. Shenzhen, China wangjinjie722@gmail.com", "ABSTRACT": "Industrial recommender systems usually employ multi-source data to improve the recommendation quality, while effectively sharing information between different data sources remain a challenge. In this paper, we introduce a novel M ulti -V iew Approach with H ybrid A ttentive N etworks (MV-HAN) for contents retrieval at the matching stage of recommender systems. The proposed model enables high-order feature interaction from various input features while effectively transferring knowledge between different types. By employing a well-placed parameters sharing strategy, the MV-HAN substantially improves the retrieval performance in sparse types. The designed MV-HAN inherits the efficiency advantages in the online service from the two-tower model, by mapping users and contents of different types into the same features space. This enables fast retrieval of similar contents with an approximate nearest neighbor algorithm. We conduct offline experiments on several industrial datasets, demonstrating that the proposed MV-HAN significantly outperforms baselines on the content retrieval tasks. Importantly, the MV-HAN is deployed in a real-world matching system. Online A/B test results show that the proposed method can significantly improve the quality of recommendations.", "CCS CONCEPTS": "\u00b7 Information systems \u2192 Recommendersystems ; Information retrieval .", "KEYWORDS": "Recommender Systems, Transfer Learning, Multi-View Learning, Deep Learning Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ASE '22, October 10-14, 2022, Rochester, MI, USA \u00a9 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-9475-8/22/10...$15.00 https://doi.org/10.1145/3551349.3559496 Chaoyun Zhang \u2217 Microsoft Research Beijing , China chaoyun.zhang@microsoft.com Junyang Chen \u2020 Shenzhen University Shenzhen , China junyangchen@szu.edu.cn", "ACMReference Format:": "Ge Fan, Chaoyun Zhang, Kai Wang, and Junyang Chen. 2022. MV-HAN: A Hybrid Attentive Networks based Multi-View Learning Model for Largescale Contents Recommendation. In 37th IEEE/ACM International Conference on Automated Software Engineering (ASE '22), October 10-14, 2022, Rochester, MI, USA. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3551349. 3559496", "1 INTRODUCTION": "Personalized recommender systems have been widely employed in web applications, such as e-commerce services, news recommendations, and video recommendations [1, 2, 5, 16]. The recommender systems improve user experience by filtering items in which users are interested. In the industrial scenario, scoring large-scale items effectively in real-time becomes challenging, as the system serves billion-scale users with billion-scale contents, e.g., QQ Kandian. There is a general practice that designs the whole system with a matching stage and a ranking stage, as shown in Figure 1. The matching stage aims at retrieving hundreds or thousands of satisfying items from billions of candidates. Next, the ranking stage generates a meticulous ranking list based on the selected items. Both matching and ranking stages play critical roles in the entire recommendation pipeline. In this work, we target on the matching stage. Inspired by recent success of deep learning achieved in other domains [7, 14, 17, 21, 22], many research applies deep neural networks (DNNs) to personalized recommender systems [1, 2, 4]. In particular, the two-tower model (TTM) is one of the most popular DNN-based methods deployed in the real-world matching stage [8, 9, 20]. TTM learns two mappings for users and item features via two independent DNN towers, encoding representations of users and items in the same space. This is beneficial to the online serving module, as similar items can be efficiently retrieved with an approximate nearest neighbor (ANN) algorithm [10]. However, though the TTM-based models are effective, there remain several challenges to be addressed. Namely, \u00b7 Information Sharing. Modern information recommender applications usually contain various types of content, such as short news, novels, images, and videos. Since distributions of features and labels vary from different content types, traditional industrial recommender systems serve different types with corresponding models. This limits the information sharing across different data sources. \u00b7 Contents Cold Start. Since many of the contents are created frequently by users in online applications, the cold start problem is exacerbated. Current recommender systems not only meet the cold start challenges for new content, but also new features and data types. \u00b7 Feature Interaction. Alarge number of models utilize vanilla multilayer perceptrons (MLPs) to learn the high-order feature interactions. However, MLPs are inefficient in dealing with multiplicative high-order feature interactions. \u00b7 Data Flow Asynchronism. In real-world recommender systems, machine learning models are updated frequently, so as to capture the evolution of users' interests agilely. Nevertheless, different contents in the data flow are processed with various pipelines, which causes asynchronism in terms of data updates. The modern recommendation model should be capable of handling asynchronous data flows at both training and inference stages. Though there exists some research attacking these issues (e.g., [6, 12, 15, 18]), only few of them address all problems simultaneously, and are deployed in real-world large-scale recommender systems. To tackle the above challenges, we propose Multi-View Hybrid Attentive Networks (MV-HAN), for the matching stage in industrial recommender systems. The MV-HAN extends the TTM by transferring information between different content types via several hybrid neural networks. The proposed method shares parameters of the bottom structures between different data types, which enables information sharing effectively. As such, the minor data types and features are both well-trained. Our MV-HAN is implemented with several multi-head self-attentive neural networks with residual connections, which promotes feature interactions. This enables knowledge to be transferred efficiently among different content types, and allows to automatically process multiple types of data from asynchronous pipelines. To summarize, this paper makes the following contributions: \u00b7 We proposed a generic modeling framework for the matching stage in the recommender systems by extending the two-tower model for transferring source information to the target type. \u00b7 We introduce an alternate training algorithm to optimize multiple objects concurrently, which enables the proposed MV-HAN to optimize easily between different datasets. \u00b7 We test our proposed model in offline experiments with top-N recommendations. Experiments show that MV-HAN outperforms state-of-the-art baselines and achieves up to 4 . 64% higher Hit Ratio (HR) score. \u00b7 We deploy the MV-HAN in a real-world recommender system. The online A/B test shows that the proposed model obtains significant improvements in all metrics. These results demonstrate the efficiency of our design. Matching Ranking Novels Million- scale Matching Ranking short contents Million- scale Matching Ranking Novels Million- scale short contents \u2026 \u2026 Ranking \u2026 Traditional Serving Pipeline Our Serving Pipeline Offline Training Pipeline Novels' Log Short Contents' Log \u2026 Data Cleaning Feature Engineering Model Updating Model Online Data Cleaning Feature Engineering Model Updating Model Online", "2 PROPOSED METHOD": "", "2.1 Problem Statement": "In this work, we focus on the matching stage in recommender systems. The issue in the matching stage is a typical Information Retrieval (IR) question, aiming at retrieving a set of content that users are interested in from massive content. We consider a user set U = [ \ud835\udc62 1 , \ud835\udc62 2 , ...., \ud835\udc62 \ud835\udc41 \ud835\udc62 ] , a source content set O \ud835\udc60 = [ \ud835\udc5c \ud835\udc60 1 , \ud835\udc5c \ud835\udc60 2 , ...., \ud835\udc5c \ud835\udc60 \ud835\udc41 \ud835\udc60 \ud835\udc63 ] and a target content set O \ud835\udc61 = [ \ud835\udc5c \ud835\udc61 1 , \ud835\udc5c \ud835\udc61 2 , ...., \ud835\udc5c \ud835\udc61 \ud835\udc41 \ud835\udc61 \ud835\udc63 ] , where \ud835\udc41 \ud835\udc62 , \ud835\udc41 \ud835\udc60 \ud835\udc63 and \ud835\udc41 \ud835\udc61 \ud835\udc63 denote the number of users, source contents and target contents respectively. The user-content interaction history can be defined as a matrix Y \u2208 R \ud835\udc41 \ud835\udc62 \u00d7( \ud835\udc41 \ud835\udc60 \ud835\udc63 + \ud835\udc41 \ud835\udc61 \ud835\udc63 ) , where \ud835\udc66 \ud835\udc56 \ud835\udc57 = 1 if the interaction between user \ud835\udc56 and content \ud835\udc57 is observed, and \ud835\udc66 \ud835\udc56 \ud835\udc57 = 0 otherwise. Our object is to learn a function \u02c6 \ud835\udc66 \ud835\udc56 \ud835\udc57 = \ud835\udc53 ( \ud835\udc62 \ud835\udc56 , \ud835\udc5c \ud835\udc57 | \u0398 ) to predict the score \u02c6 \ud835\udc66 \ud835\udc56 \ud835\udc57 of interaction \ud835\udc66 \ud835\udc56 \ud835\udc57 , where \u0398 denotes model parameters. In this way, we can rank relevant content by prediction scores.", "2.2 Multi-View Hybrid Neural Networks": "We show the overall architecture of the MV-HAN in Figure 2. The MV-HAN contains two model towers for users and contents respectively. This inherits from TTM to keep the embedding of users and contents independently and retrieves similar contents in the online serving stage efficiently. Each tower of the MV-HAN includes three major structures: embedding layers, feature extraction layers, and multi-view representation layers. The users' tower can learn better representations by sharing parameters even if the data of the target types is limited. Different from users' towers, the content tower merely shares parameters in embedding layers and feature extracting layers. This is to balance the learning process between different types of content to mitigate the cold start issue in the sparse types. In addition, feature extraction layers are implemented with multi-head self-attentive neural (MHSA) networks with residual connections, which extract high-order latent feature interactions effectively [15, 17]. The multi-view representation layers are several independent MLPs, capturing the difference between source and target contents. Finally, the MV-HAN predicts the final \u2026 \u2026 \u2026 Pooling Pooling Pooling Concatenate Multi-head Self-Attention User Vector \u2026 \u2026 \u2026 Pooling Pooling Pooling Concatenate Multi-head Self-Attention MLP MLP Source Vector Target Vector \u2026 \u2026 Source Target User Fie l d 1 User Field 2 User Filed Ku Content Field 1 Content Field 2 Content Field Ki Embedding Layers Feature Extraction Layers Repeat Repeat Multi-View Representation Layers Embedding Indexing Online Serving MLP scores by the users' and contents' representations. We formulate overall MV-HAN as follows: The loss for user \ud835\udc56 can be formulated as: Here \ud835\udc67 \ud835\udc62 \ud835\udc56 denotes the \ud835\udc56 -th user's representation, \ud835\udc67 \ud835\udc60 \ud835\udc57 , and \ud835\udc67 \ud835\udc61 \ud835\udc58 denote the \ud835\udc57 -th, and \ud835\udc58 -th content's representations of source and target data. \ud835\udc40\ud835\udc45\ud835\udc3f \ud835\udc62 , \ud835\udc40\ud835\udc45\ud835\udc3f \ud835\udc60 \ud835\udc5c , and \ud835\udc40\ud835\udc45\ud835\udc3f \ud835\udc61 \ud835\udc5c denote the mappings of multi-view representation layers with users, source contents, and target contents, which are implemented by several MLPs. \ud835\udc39\ud835\udc38\ud835\udc3f \ud835\udc62 and \ud835\udc39\ud835\udc38\ud835\udc3f \ud835\udc5c denote the mappings of feature extraction layers, which are implemented by multiple blocks via MHSA networks with residual connections. The \ud835\udc5d \ud835\udc56 , \ud835\udc5e \ud835\udc60 \ud835\udc56 , and \ud835\udc5e \ud835\udc61 \ud835\udc58 are formulated as: where \ud835\udc38\ud835\udc5a\ud835\udc4f \ud835\udc62 includes the embedding and concatenate function for users, and \ud835\udc38\ud835\udc5a\ud835\udc4f \ud835\udc5c for contents. We follow a common setting to predict the user-content interaction scores by the cosine function. Specifically: where \u02c6 \ud835\udc66 \ud835\udc60 \ud835\udc56 \ud835\udc57 and \u02c6 \ud835\udc66 \ud835\udc61 \ud835\udc56 \ud835\udc57 denote the prediction score of the source type and the target type.", "2.3 Optimization Objective": "The retrieval problem is essentially a classification problem, which estimates the probability distribution of \u02c6 \ud835\udc66 with a softmax function: Since \ud835\udc41 \ud835\udc57 is usually huge in industrial scenarios, it is time-consuming to include all contents in computing Eq. 5. To address this problem, for each user-content interaction, we randomly sample \ud835\udc5f negative cases in \ud835\udc66 \ud835\udc56 \ud835\udc57 = 0 for user \ud835\udc56 . Then we use Stochastic Gradient Decent (SGD) and its variants to optimize multiple objectives. We employ an alternative way to train the proposed method. Specifically, we put pairs with the same type in a batch, e.g., the source pair ( \ud835\udc62 \ud835\udc56 , \ud835\udc5c \ud835\udc60 \ud835\udc57 , \ud835\udc66 \ud835\udc60 \ud835\udc56 \ud835\udc57 ) , and update model parameters with this batch. Next, we train the model with data of another type to update model parameters.", "3 EXPERIMENT": "We conduct both online and offline experiments to evaluate the performance of the proposed MV-HAN, to answer the following research questions: \u00b7 RQ1: Can our proposed MV-HAN outperform the state-ofthe-art methods at the matching stage? \u00b7 RQ2: How do the key designed structures affect the performance of our proposed MV-HAN? \u00b7 RQ3: How does MV-HAN handle the online serving and perform for a real-world application? To answer RQ1, we conduct offline experiments on several industrial datasets to compare the MV-HAN with baselines. To answer RQ2, we conduct ablation studies to evaluate our purpose-built designs. To answer RQ3, we deploy MV-HAN in our real-world application and conduct an online A/B test to evaluate the performance of the proposed model.", "3.1 Offline Experiment": "3.1.1 Datasets. We collect large-scale industrial datasets from QQ Kandian with different types. We use the data in the first nine days for the train sets and the rest are used for the test set. The datasets include different data types collected in different sources, where we show their statistics of datasets in Table 1. Specifically: \u00b7 Articles is a mature type in our platform, including texts and images. We choose it as the source dataset since it contains rich information about users. \u00b7 Novels is a new type in QQ Kandian. We collect user behaviors from this type as the target dataset to evaluate the performance of methods. \u00b7 Short Contents (SC) is a relatively sparse type since short contents usually are released and taken down fast. The data of short contents are selected as a different target source. 3.1.2 Baselines Methods. We compare the proposed method with several existing recommendation methods used in the matching stage, including logistics regression (LR) [13], YoutubeDNN [2], Two-Towerbased Model (TTM) [20], Cross-domain Content-boosted Collaborative Filtering neural NETwork (CCCFNet) [11] and MultiView Deep Neural Network (MV-DNN) [3]. We train TTM on both source and target datasets to evaluate the performance of knowledge transfer. We mark this method as TTM_all. 3.1.3 Metrics. We employ two widely used metrics, Area Under the ROC Curve (AUC) and Hit Ratio (HR@50) [12, 19] to evaluate the performance of the MV-HAN. The AUC is used for evaluating the ranking performance of contents, and the HR is used for testing whether good contents are retrieved . Note that we use the relative AUC improvement [5, 23] for the evaluation in this paper. 3.1.4 Result Analysis. We show the results in Table 2. Observe that our proposed MV-HAN consistently outperforms all baseline methods on both datasets. Specifically, MV-HAN achieves up to 93.12% higher relative AUC and 113.45% higher relative HR. This verifies the strong ability of interested content retrieval for our model. In addition, though TTM_all and MV-DNN can utilize the source information to mitigate the cold start problem, the vanilla TTM outperforms those methods in the Novel dataset. This demonstrates that simply utilizing source data may dilute the knowledge of the target type. MV-HAN also achieves the best results in this dataset, which proves that the proposed method is more effective for transferring the latent information between different data types. 3.1.5 Ablation Studies. There are three key components in the proposed MV-HAH. We respectively conduct ablation studies to evaluate each of their effectiveness. Specifically, MV-HAN w/o SE is a variant of the MV-HAN, which uses exclusive embedding layers. MV-HAN w/o FE removes the shared feature extracting layers from the MV-HAN, while it employs independent networks for each type in the content tower. The MV-HAN \ud835\udc40\ud835\udc3f\ud835\udc43 replaces multi-head self-attentive neural networks with MLPs for feature extraction. We show the results in Table 3. Observe that the MV-HAN achieves the best performance in both datasets, which verifies the effectiveness of the purpose-built design. Specifically, compared to the MV-HAN w/o SE, the MV-HAN obtains substantial relative improvements in SC over the Novel dataset. This is because the short contents are updated more agilely than in novels. Thus, the model becomes underfitting over this data type. In contrast, compared to the MV-HAN w/o FE, MV-HAN achieves greater relative improvements in Novel than in the SC dataset. As the novel type includes the smallest number of training data, the corresponding tower may be trained insufficiently. In addition, compared to MV-HAN \ud835\udc40\ud835\udc3f\ud835\udc43 , the MV-HAN also achieves 1.07% \u223c 2.08% relative improvements in the two datasets. This demonstrates that the multi-head self-attentive neural networks have better feature extraction ability over MLPs.", "3.2 Online A/B Test": "In order to evaluate the performance of the proposed method in the real-world application, we deploy MV-HAN in the QQ Kandian short content recommendation and conducted live experiments to compare MV-HAN with a two-tower based method only trained by the short content data. The online evaluation metrics employed for evaluation are Click-Through-Rate (CTR) , the number of the", "Table 4: Relative changes for all metrics after employing the MV-HAN in online A/B Test.": "Daily Active User (DAU) , the number of the clicks and the Duration users used. Table 4 shows the relative changes after employing our method. Observe that the proposed MV-HAN obtains significant improvements in all metrics. Specifically, the MV-HAN achieves 6.95% and 10.12% relative improvement in terms of CTR and #Clicks , respectively. This is because MV-HAN can transfer information from other types to the short content to retrieve contents in which users are more interested. DAU and Duration are regarded as key metrics for respectively evaluating the short-term and long-term competitiveness of a platform, especially for user-generated content (UGC) applications. The MV-HAN gains relative improvement of 5.09% and 9.69% in DAU and Duration respectively. It shows that the MV-HAN is beneficial in keeping users for both the short term and the long term.", "4 CONCLUSION": "In this paper, we propose a novel model called MV-HAN for the matching stage in recommender systems. We design a hybrid neural structure configured with different models, including MLPs and multi-head self-attentive neural networks. The proposed method transfers the knowledge from the source types to the target types, which helps better representation learning for users and contents. Moreover, the MV-HAN shares parameters of the bottom networks to mitigate the cold start on the spare types. Offline experiment results on industrial datasets show that the proposed method outperforms different baselines, i.e., achieving up to 4.43% and 4.64% higher AUC and HR than the best results of baseline methods on the SC dataset. Online experiment results on real-world recommender systems show that the MV-HAN significantly improves the recommendation performance compared with baseline methods in all metrics. It verifies that the MV-HAN is able to handle multi-source asynchronous dataflows and extract information from different content types in real-world applications.", "ACKNOWLEDGEMENTS": "We thank all anonymous reviewers for their hardworking and suggestions. This work was supported in part by the National Natural Science Foundation of China under Grant No. 62102265, by the Open Research Fund from Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ) under Grant No. GML-KF-22-29, by the Natural Science Foundation of Guangdong Province of China under Grant No. 2022A1515011474.", "REFERENCES": "[1] Junyang Chen, Zhiguo Gong, Yuanman Li, Huanjian Zhang, Hongyong Yu, Junzhang Zhu, Ge Fan, Xiao-Ming Wu, and Kaishun Wu. 2022. Meta-path Based Neighbors for Behavioral Target Generalization in Sequential Recommendation. IEEE Transactions on Network Science and Engineering (2022). [2] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks for youtube recommendations. In Proceedings of the 10th ACM conference on recommender systems . 191-198. [3] Ali Mamdouh Elkahky, Yang Song, and Xiaodong He. 2015. A multi-view deep learning approach for cross domain user modeling in recommendation systems. In Proceedings of the 24th international conference on world wide web . 278-288. [4] Ge Fan, Biao Geng, Jianrong Tao, Kai Wang, Changjie Fan, and Wei Zeng. 2022. PPPNE: Personalized proximity preserved network embedding. Neurocomputing 472 (2022), 103-112. [5] Ge Fan, Chaoyun Zhang, Junyang Chen, Baopu Li, Zenglin Xu, Yingjie Li, Luyu Peng, and Zhiguo Gong. 2022. Field-aware Variational Autoencoders for Billionscale User Representation Learning. In Proceedings of the 38th International Conference on Data Engineering . [6] Ge Fan, Chaoyun Zhang, Junyang Chen, and Kaishun Wu. 2021. Predicting ratings in multi-criteria recommender systems via a collective factor model. In DeMal@ The Web Conference . 1-6. [7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition . 770-778. [8] Jui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip Pronin, Janani Padmanabhan, Giuseppe Ottaviano, and Linjun Yang. 2020. Embeddingbased retrieval in facebook search. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 2553-2561. [9] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. 2013. Learning deep structured semantic models for web search using clickthrough data. In Proceedings of the 22nd ACM international conference on Information & Knowledge Management . 2333-2338. [10] Jeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. 2019. Billion-scale similarity search with gpus. IEEE Transactions on Big Data 7, 3 (2019), 535-547. [11] Jianxun Lian, Fuzheng Zhang, Xing Xie, and Guangzhong Sun. 2017. CCCFNet: a content-boosted collaborative filtering neural network for cross domain recommender systems. In Proceedings of the 26th international conference on World Wide Web companion . 817-818. [12] Zinan Lin, Dugang Liu, Weike Pan, and Zhong Ming. 2021. Transfer Learning in Collaborative Recommendation for Bias Reduction. In Fifteenth ACM Conference on Recommender Systems . 736-740. [13] H Brendan McMahan, Gary Holt, David Sculley, Michael Young, Dietmar Ebner, Julian Grady, Lan Nie, Todd Phillips, Eugene Davydov, Daniel Golovin, et al. 2013. Ad click prediction: a view from the trenches. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining . 1222-1230. [14] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. Advances in neural information processing systems 26 (2013). [15] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, and Jian Tang. 2019. Autoint: Automatic feature interaction learning via selfattentive neural networks. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management . 1161-1170. [16] Mingze Sun, Daiyue Xue, Weipeng Wang, Qifu Hu, and Jianping Yu. 2021. GroupBased Deep Transfer Learning with Mixed Gate Control for Cross-Domain Recommendation. In 2021 International Joint Conference on Neural Networks . 1-8. [17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017). [18] Ruoxi Wang, Zhe Zhao, Xinyang Yi, Ji Yang, Derek Zhiyuan Cheng, Lichan Hong, Steve Tjoa, Jieqi Kang, Evan Ettinger, and H Chi. 2019. Improving Relevance Prediction with Transfer Learning in Large-scale Retrieval Systems. In Proceedings of the 1st Adaptive & Multitask Learning Workshop . [19] Ruobing Xie, Zhijie Qiu, Jun Rao, Yi Liu, Bo Zhang, and Leyu Lin. 2020. Internal and Contextual Attention Network for Cold-start Multi-channel Matching in Recommendation. In IJCAI . 2732-2738. [20] Xinyang Yi, Ji Yang, Lichan Hong, Derek Zhiyuan Cheng, Lukasz Heldt, Aditee Kumthekar, Zhe Zhao, Li Wei, and Ed Chi. 2019. Sampling-bias-corrected neural modeling for large corpus item recommendations. In Proceedings of the 13th ACM Conference on Recommender Systems . 269-277. [21] Chaoyun Zhang, Marco Fiore, Cezary Ziemlicki, and Paul Patras. 2020. Microscope: mobile service traffic decomposition for network slicing as a service. In Proceedings of the 26th Annual International Conference on Mobile Computing and Networking . 1-14. [22] Chaoyun Zhang, Kai Wang, Hao Chen, Ge Fan, Yingjie Li, Lifang Wu, and Bingchao Zheng. 2022. QuickSkill: Novice Skill Estimation in Online Multiplayer Games. arXiv preprint arXiv:2208.07704 (2022). [23] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through rate prediction. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1059-1068."}
