{"title": "\"It is there, and you need it, so why do you not use it?\" Achieving better adoption of AI systems by domain experts, in the case study of natural science research", "authors": "Auste Simkute; Ewa Luger; Michael Evans; Rhianne Jones", "pub_date": "", "abstract": "Artificial Intelligence (AI) is becoming ubiquitous in domains such as medicine and natural science research. However, when AI systems are implemented in practice, domain experts often refuse them. Low acceptance hinders effective human-AI collaboration, even when it is essential for progress. In natural science research, scientists' ineffective use of AI-enabled systems can impede them from analysing their data and advancing their research. We conducted an ethnographically informed study of 10 in-depth interviews with AI practitioners and natural scientists at the organisation facing low adoption of algorithmic systems. Results were consolidated into recommendations for better AI adoption: i) actively supporting experts during the initial stages of system use, ii) communicating the capabilities of a system in a user-relevant way, and iii) following predefined collaboration rules. We discuss the broader implications of our findings and expand on how our proposed requirements could support practitioners and experts across domains. CCS CONCEPTS \u2022Human-centered computing \u00e0 Human-computer interaction (HCI) \u00e0Empirical studies in HCI \u2022 Social and professional topics \u00e0 Professional topics \u00e0 Computing education \u00e0 Computing literacy \u2022 Applied computing \u00e0 Life and medical", "sections": [{"heading": "INTRODUCTION", "text": "Artificial Intelligence (AI) is increasingly important in domains such as public sector administration [22], medicine [54], and life sciences [64,84]. AI is intended to automate mundane, repetitive tasks and focus human expertise on higher-level and creative activities [34,68,82,83]. In many contexts, AI is a crucial solution to coping with the vast data available due to technological advances [26]. Complex AI systems and the availability of biological data have already contributed to significant achievements in fields such as Alzheimer's research [84], drug discovery [53] and disease diagnostics [40]. However, without AI being appropriately implemented, there is a risk of \"bottleneck\" situations in domains such as the natural sciences [65]. If scientists are constrained to manual data processing, valuable data is left unused, and significant scientific discoveries are slowed down or halted [47]. Domain experts also acknowledge the benefits AI systems offer [79], yet in practice, they remain reluctant to include AI in their workflows [16], they use AI systems ineffectively [33,46,76], or their performance worsens with the support of AI [15,16,24,81].\nThe Human-Computer Interaction (HCI) community has approached this issue of AI resistance by proposing more usable, accurate, and explainable technologies [30,59,60]. Increasing efforts have been directed at understanding how model transparency influences the adoption of AI systems in practical settings, for example, by studying physicians' interactions with AI [14]. These efforts are undoubtedly needed to develop ethical and user-centred technologies. However, domain experts prematurely reject and mismanage AI systems, even if they are explainable and outperform them [15,16]. However, despite the growing user-centred research focus, in practice, experts' needs continue to be misinterpreted in the design of these systems, leaving them frustrated with technologies that do not fit their needs, knowledge, and workflows [12,66]. We argue that more fundamental and underexplored barriers exist between experts and AI tools designed to support them. First, there is a lack of research examining how and why experts' needs are misinterpreted by those introducing the new AI technology. Second, outside of studies surveying organisations and looking into higher-level barriers to AI adoption (e.g., environmental, organisational) [1,50], few have explored the emotional, contextual, and collaborative obstacles that might have to be removed before proposing more usable, explainable technologies to domain experts. The AI-related experts' training needs are also underexplored.\nRather than exploring the technology features influencing AI acceptance, we focus on understanding i) the practical and contextual barriers that experts face when AI systems are introduced into their work practices, ii) the AI practitioners' role and their perceptions of the reasons for low AI adoption among experts and iii) how they approached this issue. We then iv) compare practitioners' and experts' perceptions and follow how inaccurate assumptions can stifle effective AI adoption. Lastly, we study v) how effective collaboration between domain experts and practitioners is reflected in experts' motivation to use AI systems. We report on an ethnographically informed study of 10 in-depth interviews with AI developers and natural scientists working at the same organisation, which faces slow adoption of AI systems and bottleneck issues, with more data being collected than possible to process manually.\nOur analysis reveals the miscommunications and frustrations between practitioners and experts and how they create barriers to AI adoption. We show that practitioners overestimate the technical knowledge of expert scientists and create an environment in which experts are expected to manage aspects such as software installation and command lines independently. However, the lack of support in performing these simple tasks put a huge emotional burden on experts, took their attention from their projects, and made them resent the AI system. Further, we show that experts are likely to reject an unfamiliar system if its capabilities are not communicated in a relevant way to their project but are willing to invest time in exploring them if they understand how AI aligns with their needs. However, practitioners focus on communicating the technical parameters of their systems, hoping that experts will discover how they can use them in their projects while experimenting with the system. Finally, we report positive cases showing that continuous and goal-driven collaboration can help resolve these miscommunications and develop mutual understanding between teams representing different disciplines.\nThe paper makes three contributions:\n1. We provide AI acceptance requirements that could help to overcome the obstacles to effective AI adoption: i) actively supporting experts during the initial stages of system use, ii) communicating the capabilities of a system in a user-relevant way, and iii) following predefined collaboration rules. 2. We present recommendations for effective collaboration to support the implementation of AI systems in workplaces. These findings could inform practices in various domains and be generalised beyond the case organisation. By following our recommendations, organisations could enable effective AI adoption, scaffold the development of usable technologies, and allow experts to benefit from the systems developed to support them and focus on using and developing their expertise. 3. We describe the emotional and practical consequences of the sudden increased pressure for domain experts to adopt AI tools in their work and the lack of transitional support they receive. We also draw attention to the misappreciation of experts' relatively limited computational skills. We expect these findings to inspire a shift in the research of AI applications in expert contexts.", "publication_ref": ["b21", "b53", "b63", "b83", "b33", "b67", "b81", "b82", "b25", "b83", "b52", "b39", "b64", "b46", "b78", "b15", "b32", "b45", "b75", "b14", "b15", "b23", "b80", "b29", "b58", "b59", "b13", "b14", "b15", "b11", "b65", "b0", "b49"], "figure_ref": [], "table_ref": []}, {"heading": "RELATED WORK", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "AI in experts' workflows", "text": "AI should support experts by automating time-consuming and mundane tasks and allowing them to focus on exercising their expert skills [82], but it often has the opposite effect [25,57,66]. Researchers studying automation have shown that introducing it has disrupted experts' workflow and changed how they approach their tasks [38], adding to experts' busy workloads and causing frustration [21]. Experts also report feeling a loss of control and agency when relying on automation or AI [37,72,80]. For example, experienced weather forecasters felt that automation made them less adaptive and passive, pigeonholing and disconnecting them from their preferred way of data analysis [69]. Feeling disengaged can force experts to return to more familiar but less effective methods, such as manual information search [43]. Experts often need to change their working methods to accommodate technologies that do not fit their workflows. Interviews with 28 workers facing high job demands using productivity assistant technologies showed that the view of users' work habits presented by the technology did not match their experienced reality. This mismatch required significant time and cognitive efforts for them to adapt [11].", "publication_ref": ["b81", "b24", "b56", "b65", "b37", "b20", "b36", "b71", "b79", "b68", "b42", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "AI adoption", "text": "Experts often distrust AI advice, even when it consistently outperforms human judgement (they show algorithmic aversion) [6,15,16]. This effect can be influenced by the specific features of the technology (e.g., explainability [3,8], accuracy metrics [83], perceived power [31]) and various individual and contextual factors, such as level of expertise [44,56].\nSeveral studies suggest that having some control over the AI outcome could mitigate users' algorithmic aversion [17,67,71]. When algorithmic aversion is not addressed, it can lead to decision-makers relying on less effective manual methods and failing to benefit from AI systems [43,78]. Barriers to AI adoption are less researched than algorithmic aversion. Automation studies show that experts are unlikely to rely on automation that lacks transparency, especially if its suggestions are misaligned with their judgement [77], threatens to degrade their skills [42] or jeopardises their agency [4,36,51]. Similar results were observed in AI use cases. For example, medics were unwilling to integrate AI tools into their workflows if they perceived them as threatening their clinical autonomy or if the clinical value of these tools was unclear [29], whereas intelligence workers either reduced frequency or altogether ceased using AI if they observed it underperform [18].\nSeveral studies explored the issues of AI adoption at the organisational level. Alsheibani et al. [1] conducted a questionnaire study involving 207 different-sized organisations about the main barriers to AI adoption. They reported environmental barriers, such as regulations, organisational obstacles related to a lack of top management support, and technical barriers, such as a lack of AI skills and employees' fear of change. Pan et al. [50] studied the challenges of AI adoption in recruitment. The authors emphasised reducing obstacles to AI adoption rather than just developing practical AI techniques for the future of AI technology. They also drew attention to AI practitioners' efforts to understand the difficulty of using AI technologies with little or no technical background. They suggested that reducing complexity and making AI tools user-friendly should be prioritised to increase AI adoption [50]. Factors such as the complexity of the task or organisational impacts are currently underexplored when trying to understand the AI adoption [45].", "publication_ref": ["b5", "b14", "b15", "b2", "b7", "b82", "b30", "b43", "b55", "b16", "b66", "b70", "b42", "b77", "b76", "b41", "b3", "b35", "b50", "b28", "b17", "b0", "b49", "b49", "b44"], "figure_ref": [], "table_ref": []}, {"heading": "Theories of technology acceptance", "text": "The two most widely accepted theories of the user adoption of technology are the Technology Acceptance Model (TAM) [73,75] and the Unified Theory of Acceptance and Use of Technology (UTAUT) model [39,75]. TAM focuses on the relationship between perceived usefulness and ease of use of new technology, predispositions towards the latest technology and behavioural intentions to use the technology. An individual's decision to engage or not engage in the behaviour of using a new technology is thus based on the expected outcome of using the technology and whether using a new technology is free of effort. The UTAUT model extends the TAM approach by proposing that technology acceptance depends on how widely others accept it by observing others within one's social interactions and experiences [62,75]. We argue that current research efforts exploring AI-expert interactions overlook the aspects of perceived usefulness and required effort. There has been little effort to examine practical hurdles and workflow changes that might prevent experts from using newly introduced AI systems. There is insufficient understanding of the barriers users face before and during the interaction with AI and how they perceive potential future benefits that could outweigh these barriers. In line with Pan and colleagues [50], we argue that the importance of burdens related to domain experts having little or no technical background is underexplored. Research shows that experts understand that they need AI to advance their work [65]. However, there seems to be a hurdle blocking users from adopting and effectively using AI systems. It is essential to understand the factors that impede users before trying to enhance the appeal of the technology.", "publication_ref": ["b72", "b74", "b38", "b74", "b61", "b74", "b49", "b64"], "figure_ref": [], "table_ref": []}, {"heading": "METHOD", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Procedure and Participants", "text": "The research was conducted in a large natural science research centre where the AI-enabled technologies are developed by the in-house team for the scientists within and outside the organisation. The organisation was chosen as a case study because i) AI systems are essential for experts' progress (as the volumes of data are too large to process manually), but the adoption of the AI-enabled software remains low; ii) the advances in data collection technologies are recent, and experts are new in having to rely on AI systems; iii) practitioners and experts work in the same building, and some projects require their collaboration.\nWe define Practitioners as software developers working on developing AI-enabled products and services. We define Experts as scientists who work with available technologies to answer scientific questions related to natural sciences. We recruited practitioner and expert participants using a snowball sampling [23]. Our inclusion criteria were participants working in the office and being available for in-person interviews. For expert participants, using AI-enabled software had to be a part of their project. We wanted to understand i) the key barriers preventing users from effectively adopting readily available AI-enabled software that they need for their projects; ii) how the views of experts and practitioners align when reflecting on the same processes, and how that relates to the low adoption of AI-enabled software; and finally, iii) how teams collaborate and what role this collaboration play in improving effective adoption of AI systems.", "publication_ref": ["b22"], "figure_ref": [], "table_ref": []}, {"heading": "Data collection", "text": "The data collection was guided by a contextual enquiry method [58]. The first author conducted in-depth semi-structured interviews with five practitioners (including the AI team lead and science director) and five expert scientists (in various stages of their PhD projects). During the interviews, experts were also asked to demonstrate how they would use the software. Some practitioners showed relevant material (guides) and software to illustrate their answers, but this was not a planned part of the interviews, and, therefore, the notes from observing this were used only as additional enrichment during the data analysis. Participants signed a consent form before starting the interviews. All interviews were conducted in person, in AI team members' and experts' familiar working environments. The interviewees were not incentivised for their participation. The data sampling was guided by the Grounded theory approach, using Theoretical sampling methods [5]. The interview with the AI lead and science director revealed potential communication issues during the software's introduction and showed that practitioners might have assumptions about why communication does not work and what experts expect-this led to including additional questions in subsequent interviews.\nOur interview protocol had three main parts. First, participants were asked about their roles, practices, and workflows to gain background information and context. The second part pertained to their general experiences and attitudes towards using and developing AI-enabled systems. Expert participants were asked how they chose and trusted these systems, how they felt they understood them, how they were first introduced to the new software, and what training they underwent. Practitioners were asked how they develop and introduce usable software to potential users. The third part of the interview was focused on collaboration experiences. Both classes of participants were asked to share experiences and expectations regarding collaboration between practitioners and experts. The interviews were semi-structured and guided by participants' answers.", "publication_ref": ["b57", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "Data analysis and positionality", "text": "Each interview lasted between 45 to 90 minutes. All interviews were conducted in English. The author recorded and transcribed the interviews and analysed notes from the observations. Data was analysed using the Grounded theory approach and advanced coding method [9]. Initial codes were used to fracture the data into categories that were then transformed into more abstract concepts and their dimensions. The relationships were identified between categories and the central core category, creating a storyline supported by explanatory connecting statements [10].\nBecause of a rich contextual understanding of the data they gained while residing in the organisation, the analysis was conducted by the first author, who collected the data. The author spent four days working in an open office alongside the AI team and experts, where they were part of informal conversations and observations that contextually enriched the collected data. The first author acknowledges that their experiences and positionality could have influenced the perspective and approaches regarding data collection and analysis.", "publication_ref": ["b8", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "RESULTS", "text": "In this section, we report the categories that emerged during the analysis, the interrelations between these categories and a core category of assumptions. We compare the answers of the practitioners and experts to emphasise the miscommunications between the teams and link them to the key frustrations. The letter P next to the participant number demotes the practitioners, and E refers to the experts. In the interviews, experts referred to AI-enabled systems as software, and this term is used in the following sections.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Using unfamiliar software", "text": "In presenting the challenges that experts faced when using new AI systems, we detail four key frictions occurring during the initial stages of use. Each friction is juxtaposed with the practitioners' associated assumptions and illustrates how these misapprehensions could lead to unresolved issues.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "4.1.1Barrier 1: Not knowing how to get the software to work.", "text": "When experts were asked to demonstrate the software, all observed experts struggled with the command line and had to follow available documentation online. They were frustrated and expressed how stressful the installation and initiation processes were without having programming knowledge: \"[\u2026] it is absolute hell to try to install in the first place. And if you do not have any programming background, you lost.\" (E05). Other experts said: \"[\u2026] I find the command line the most difficult thing ever.\" (E06). When asked how they know the command line, the experts said: \"Hopefully, someone has told me at some point, and I will have remembered it, but that is not always the case.\" (E06). Despite AI tools being intended to increase the productivity and efficiency of its users, interviewed experts admitted that trying to get the system to work was time-consuming and took their focus away from their projects: \"I have only started my project [\u2026]and a lot of it has been trying to get something to work, installation and stuff like that.\" (E05) The ones that were still looking for the right software were fearful about how long it will take: When asked if they spend more time dealing with software issues than their project, one of the experts said: \"Yes. Which I have heard is quite normal, especially for scientific software because it is all very specific.\" (E05) Not being able to start using the software stifled experts' progress: \"I just have loads of data that is sitting there that have not been [analysed], has a lot of biological information within it, but I have not been able to take any of it.\" (E06)\n\"[\u2026]", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "4.1.2Practitioners' assumption: \"They must know this much.\"", "text": "Practitioners admitted that computational knowledge is expected: \"You probably need some basic knowledge about Linux and command line and manipulation [\u2026].", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "\" (P04). Another practitioner said: \"[\u2026] You need to be able to use a command line, and you need to be able to think about image data which often has various sort of very specific qualities relating to the quality of the image or the resolution.\" (P09)", "text": "The practitioner seemed to be aware that experts might lack the skills but still overestimated them: \"You assume that these days a lot of people have some knowledge of machine learning if they are a scientist [\u2026] you assume a certain background, but you are probably often not right.\" (P09) Some technical aspects are seen as common sense by practitioners and not something they need to explain to experts: \"I have worked with somebody recently who was effectively a user [\u2026] and the level of computational knowledge that they had was incredibly limited to the point of things that I do not even consider. It is not even something you think about anymore.\" (P02) Tasks that were not directly related to software development (i.e., help with software installation or updating instructions) were seen by practitioners as secondary, something that someone else would do: \"Somebody in a group [scientist team] that wants to use [a software] has the technical expertise and maybe does the job of installing the software, and then other people in the team with less technical expertise use it.\" (P09) 4.1.3Barrier 2: Incomprehensible, outdated, or missing documentation.\nExperts emphasised the need for easy-to-follow and up-to-date installation instructions and documentation as key features that would encourage them to adopt a software: \"Very clear installation instructions like almost a step by step [\u2026] then also step by step what code you should be running and an explanation as to what each of the bits of the code is so that you not have to change it for your specific work.\" (E05) Another expert said: \"Easy to install, easy to understand [\u2026] having good documentation is probably the main thing.\" (E03) The same expert admitted that not having established documentation has been challenging: \"[\u2026]I think that was one of the hardest things I found, just understanding how to use the system and just so many different pieces of software to use. And the documentation, as I said before, there is no established [documentation].\" (E03) Another expert emphasised how important it is that instructions are prepared considering their level of computation knowledge. \"So many of these softwares are just absolutely horrendous to use.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "[\u2026]", "text": "A lot of the time, there are no instructions online on how to use it, or if there are, they are for people that are very specifically computer science.\" (E05).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "4.1.4Practitioners' assumption: It is available, so it must be useful.", "text": "Practitioners explained that documentation is by default made available to accompany new software: \"[\u2026] we always try to make all our software sustainable and to have good documentation [\u2026] so they can visit the documentation at their own time get into deeper end on how to use the software.\" (P01) When explaining how users would know how to get a new software working, they explained: \"[\u2026] it has a GitHub page, so it has a repository, and it also has, I think, online documentation, which is quite self-contained, just good.\" (P04) When talking about the software they developed, a practitioner said, \"There is the documentation, the manual, and then there is putting an issue on GitHub or sending us an email there was a forum that we had for a while.\" (P09) But after being asked if there is anything apart the paper guide for one of their software, the same practitioner said: \"Yeah, it is just that booklet, that kind of thing [\u2026] it is likely to be important for them to be able to run [software], especially to install.\" (P09) The practitioner referred to the physical guide booklet that one of the interviewed experts tried to use when they could not run the software: \"Luckily, I have remembered that somewhere there is this very handy guide. It is handy because it does explain some levels of how you use [the software]. Whether it tells me how to open it is another thing. Sometimes, I find this guide is just too simplistic. It leaves out a lot of information that I need.\" (E06). The expert did not find the necessary information in the booklet.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "4.1.5Barrier 3. Lack of active support when starting to use the software.", "text": "For experts, asking for help from the in-house AI team was the last option. When asked what they would do if something did not work, they said: \"Ask someone else here [among colleagues] first. There are good forums online [\u2026] I have been working with a postdoc here, and he has basically taught me all of this pipeline.\" (E03) Another expert said: \"I could give you the easy answer to it -I struggle. [\u2026] I think most of the time I try to find material that is out there that maybe other people have made to try and help the users to use the software and things like videos online.\" (E06) This expert had to rely on the Internet to find answers: \"[\u2026] I do not have anyone more senior to be able to turn to, to be like, Hey, I see that you have used this software before. Can you help me learn how to use it? [\u2026] I am just going to have to work it out by myself. I am just trying to look on the Internet and see if there are any tutorials on there and things like that, which is obviously challenging\" (E06). The expert was emotional about the lack of having somebody actively providing help at the start: \"[\u2026]I know that if someone was around to just teach me how to use it, we could then spend some time, sit down, work it out, practice with using it on some of my data sets. It would probably take a day or two, and then it would simplify the rest of my project [\u2026] I am having to struggle through trying to get something to work, which makes you feel stupid sometimes because you are trying to get the software to work and you feel like, Well, why I cannot get this to work? [\u2026]It can cause some real existential crises sometimes.\" (E06) This expert explained that it is intimidating for a scientist to admit that they do not know something so seemingly basic, so they do not reach out to the AI team with these questions: \"There is definitely an element of pride [\u2026] I would say there is an element of not wanting to admit that you do not understand something.\" (E06) Another expert also emphasised how challenging it was to find solutions independently: \"[\u2026] I was trying to go through literature as to how maybe people who have helped develop it a little bit were using it to try and figure out what code I was meant to be using. Um, no joy whatsoever.\" (E05) Receiving initial support, improved how experts approached the software and how they dealt with issues later: \"[practitioner] was trying to get involved in actually seeing how the code runs and everything and seeing how we were able to run it ourselves eventually. So [practitioner] does not have to every time we want to use something run it for us.\" (E10) Getting answers to simple questions led to learning about the software and being more confident and independent in using it: \"[practitioner] has been fantastic at not just telling me what to do but helping me to understand a bit deeper as to what I am supposed to do in the future [\u2026] he is helping me so that I am not as lost every single time I use it.\" (E05) 4.1.6Practitioners' assumption: If they need help, they will ask for it or find it.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "When asked about troubleshooting, one of the practitioners said: \"I have taught programming for quite a few years. We always tell students, Go see your friend, search if it's open source if it has a repository and GitHub [..].\" (P04", "text": "). This answer also shows that the practitioner compared experts to their programming students, showing a certain level of expectation they have for the expert users, assuming that the same type of material and training methods should suit both groups. When they did have to provide support, frustration was evident: \"I just sent them a very explanatory email of how exactly you have to do it [\u2026] what is the correct word about it, customer service or something.\" (P01) Practitioners felt frustrated by the way they were being approached by the experts: \"[\u2026] most of the time they are like, I do not know how to do this, please fix it. And there is no clear like, Please, can you do ABC? It is up to us to figure out ABC, and then we work with them again to say well, I manage to do ABC for you. What do you think? And like I actually wanted X, Y, Z.\" (P08) After introducing software, practitioners rarely reach out to experts to find out whether they are able to use it: \"We train them to use the software and then let them get on with it and then the process of working through. You know, it is just a case of emails that backwards and forwards, if they have questions\" (P02). And: \"No, I do not do it [reach out] actively. It is up to them.\" (P09).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Effective collaboration and communication as a solution", "text": "In this section, we reflect on collaboration between experts and practitioners. We present cases where collaboration led to removing some of the frictions described above, as well as factors that led to successful collaboration or hindered it. Two interviewed experts worked with the AI team as part of their projects and were more positive and involved than other experts. Practitioners also reflected on their experiences of collaborations as successes. These stories involved users becoming advocates for the software, getting involved and finding the confidence to approach each other.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "4.2.1Collaboration can help establish mutual understanding over time.", "text": "In one project, a group of practitioners and experts had to collaborate on developing the software in weekly meetings. The communication between the teams was not effective in the beginning: \"At the start, we could not communicate with each other at all.\" (E10). The expert said that they would simply not understand each other's languages and would have to have someone with both backgrounds translate. \"We were speaking in chemistry, and they were speaking in AI. And I genuinely felt like we were speaking different languages, and the meetings would go round in circles a lot.\" (E10). After around three months, they became better at communicating and having a shared goal helped. \"But we have gotten a lot better at it. [\u2026] We needed time where we all learned a bit more to be able to talk to each other. I think everyone would agree because you are trying to figure out how to make [software] better.\" (E10) One practitioner also reflected that working with experts helped to understand scientists better: \"I find it easier to talk to a biologist now that I have spent a lot of time with them.\n[\u2026] But if you are someone who is completely in your field and all the people you speak to on a daily basis are computer scientists, if you are trying to speak to a biologist, it is a bit difficult to bridge that gap.\" (P08)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "4.2.2Collaboration can help to communicate software capabilities.", "text": "Establishing mutual understanding through collaboration can help to communicate software capabilities to experts. Otherwise, experts might not find the software relevant enough to invest their time:", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "\"[\u2026] I do not want to put a lot of time into learning how to use something when I do not actually know if the software itself is good enough for what I need it to be able to do because that would just be a complete waste of my time.\" (E06) A practitioner said that capabilities had not been communicated effectively in the past: \"The scientists did not know what [software] could do. It was not explained very well because the way that it was presented in our science talks and stuff was very much like a computing thing [\u2026] the presentations will always like what does the [software] do rather than what does [software] do for you.\" (P08).", "text": "The same practitioner shared a success story of applying this practice: \"We took some data which people were actually working with and showed them what they could do, and they were like, Actually, this is really cool. Like, why did we not know about this before? And we were like, It has always been here. It just did not get across.\" (P08) Experts were willing to learn more about the software if they saw how, it could be useful to them: \"[\u2026] when you know that you want to use it, I find it interesting, and I know it is going to give me what I want.\" (E10) Experts sought information about the software relevance to their projects online: \"On the website, they were like [\u2026] here is an example of someone using this software for this specific dataset or this specific dataset, you could at least get an idea of whether it might be applicable to yours.\"(E06)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "4.2.3Continuous collaboration can help to align expectations.", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Collaborating is essential for effectively aligning expectations and finding middle-ground solutions: \"Mostly what those meetings consist of is all of us talking, right? Does that make sense as a chemist? [\u2026] From my point of view, it is a lot of just kind of hearing what [practitioner] is trying and try to make sense of whether that makes sense from the chemistry side of things.\" (E10) Effective collaboration was based on maintaining the relationship throughout the development process and frequently checking in with each other: \"There is this continuous communication regarding how the computer science attempts to solve certain issues and if this is okay with them [experts] [\u2026] it evolved around what we can do and what we cannot do [\u2026] there is a little back and forth to find a middle ground and something that can be done.\" (P01)", "text": "Continuous communication led practitioners to reflect on their assumptions about users' needs: \"[\u2026] now that we are trying it with them, sitting with them, we are finding that they do things differently than how we expected them to\" (P08)\nPractitioners emphasised the importance of communicating throughout the development process as it gives insights into the field and helps to consider details relevant to experts: \"It is very important [\u2026] to bring in somebody who is an expert and get some feedback from them because otherwise, you can spend a lot of time just talking about blobs, and you are not talking about what you need to be talking about.\" (P09). Another practitioner added that you need to understand the needs of the experts: \"I think it is really important to spend time with people in the domain that you are working with and understand what is important to them before we can help them do anything.\" (P08)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "4.2.4Collaboration increases experts' motivation to be more involved.", "text": "When practitioners had to collaborate with domain experts due to their projects, they actively provided feedback about the software to the AI team. The AI team lead reflected on one of the success stories: \"This user was basically happy to sit and use all the software that did not work [\u2026]That enabled us to really drive the software towards something that was practically useful.\" (P02) An expert who collaborated with developers was willing to invest time and effort in trying the software and helping to improve it: \"Having played around with it and seeing like what the problems I experience are, it is quite nice coming up with ideas, being like, Okay, if I fix this, this is going to make this so much easier for everyone.\" (E05) Collaborating with practitioners when testing the software led experts to be more accepting of software issues and proactive in trying to achieve desired results: \"I run two [", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "analysis], and they were not fantastic [\u2026] [AI team] are kind of working on figuring that out at the moment. So, instead of just sitting and doing nothing, I am going to try and see if I can get my [inputs] a bit more specific.\" (E10)", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "4.2.5Collaboration makes communication more casual.", "text": "One of the experts shared how the courage to ask grew over time, collaborating with the AI team. \"If I do not get it, I will just be like, What are you talking about? Whereas at the start, I was like, Oh, they expect me to know this. I think it is more of a personal thing.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "[\u2026] I was like, I do not want to waste everyone's time here trying to explain to me. [\u2026] I know now they would not have thought it was a waste of time in my mind.\" (E10)", "text": "The expert shared that constantly talking to practitioners made it easy to approach them: \"[\u2026] we talk so much, we spend so much time together, and I would just be like, Yeah, hi, remember me? Um, I have this problem.\" (E10) An expert who already had an established connection with practitioners was happy to contact them: \"I would normally email the software developers and be like, Hey, this is for some reason not working or isn't working as well as it should.\" (E05) They also gave feedback about errors and persisted with the choice of the software: \"I would much rather contact the development team, mostly because I know them now.\" (E05) Without having an established connection, it is difficult to give constructive feedback unless it is encouraged and/or initiated by the AI team: \"[\u2026]it is just always difficult to try and give out feedback in a sort of constructive way\". (E06)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "DISCUSSION", "text": "This study and its analysis develop an actionable strategy for tackling AI adoption issues among domain experts. Our findings have revealed practical barriers to AI adoption at the human level. These transcend the influence of software capabilities (e.g., confidence, explainability). Many of these barriers stem from inaccurate assumptions of users' needs and attitudes. We argue that without eliminating them, experts cannot explore the software and benefit from the features, including explainability. Currently, there is a lack of research trying to understand the aspects unrelated to the AI technical capabilities that stifle the adoption. Our study details the importance of the learning hurdles that users face when new systems are embedded in their workflows. Guiding users through practicalities (e.g., software installation), providing active support, and establishing ongoing collaborative efforts are fundamental to increasing AI uptake responsibly. This means that experts would not only make greater use of AI systems but would also be equipped to use them more meaningfully, with deeper understanding and motivation. Our observations also align with both TAM [73,74] and UTAUT [39,75] models, arguing that the factors influencing technology acceptance are driven by the balance between the perceived benefits and effort required. We showed that this balance can be achieved by clearly communicating the system's capabilities and reducing required effort by providing initial and ongoing collaborative support. The following subsections expand on the observed barriers and recommendations to overcome them.", "publication_ref": ["b72", "b73", "b38", "b74"], "figure_ref": [], "table_ref": []}, {"heading": "Barriers to AI adoptions during the introductory period", "text": "Experts struggled to bypass the initial steps of installing and running the software. They were more likely to reject the system if they experienced issues during this initial stage, especially if the software did not have comprehensible documentation and they did not receive support. Experts complained that practitioners expected them to be programmers and to just know, for example, what command line instructions to use. However, they did not seek help because, as scientists, they felt embarrassed to struggle with basic tasks. Practitioners, indeed, did not consider that experts might not have these basic skills. They saw tasks, such as software installation, as a matter of fact. When introducing their software to potential users, they used one-off training workshops, predominantly focused on explaining higher-level details, such as underlying mechanisms or the data processing pipeline and then processing some data. Although documentation and contact information were usually available, practitioners did not reflect on whether the available training or documentation was helpful. There was a lack of active support or follow up on experts' progress during the initial stage.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "5.1.1Recommendation: Offer active support during the initial stages.", "text": "We recommend providing support during the initial stage of AI adoption. Before the new technology is fully introduced to the experts' workflow, they should be given time to adjust to the new system. During this stage, experts should receive active support from assigned team members, who would guide them and help install and run the new system. This should also establish contact between an expert and a person/team to whom they could communicate their questions. Experts should be guided to the updated and comprehensible documentation, which would be established in collaboration with them. Providing support when implementing new technologies could positively influence user attitudes towards technology and their self-efficacy [41]. Experts should actively seek practitioners' feedback and clearly communicate theirs during this stage. During this stage, communication between experts and practitioners could improve how technology fits within experts' workflow [49]. It could also reduce the stress of adjusting to the workflow-related changes. Park and colleagues [52] showed that discussing practical solutions to their AI problems with the management team provided emotional support, lowered social burdens, and demonstrated that managers cared for them. Experts should also be allowed to practice using the new system. Practice sessions can enable a learning-before-doing approach, enhance later performance [55], and increase motivation to use and experiment with the new technology independently [19]. Allowing users to test and reflect on the new technology has also increased its acceptance. A study exploring the learning difficulties of novice technology users revealed that they mostly valued the opportunities to experiment and explore the system freely and safely [7]. An ethnographic study of technology implementation in different cardiac surgery departments reported that sites that implemented trials for technology use followed by reflection sessions improved the chances of successful implementation and formed a learning cycle [19].", "publication_ref": ["b40", "b48", "b51", "b54", "b18", "b6", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Barriers to investing time to learn and explore AI systems", "text": "Practitioners argued that users were unwilling to participate in software design and development stages and just wanted a final result or expected AI to just do its magic. This felt unfair to practitioners: \"[\u2026] we are expected to learn biology, but they [experts] are never expected to learn the computing side of things\". Experts were actually willing to invest time and effort in learning and improving the software if they knew the system would likely be helpful for their project. Experts prioritised systems that had any evidence of being beneficial for their specific project. For example, they searched for videos on Twitter, with scientists demonstrating software and explaining its features in the language and context understandable to them. Otherwise, they felt they were risking wasting their time. However, the capabilities of the systems were not communicated effectively to them. Practitioners would present the technical aspects of the software without framing its capabilities in a way that would be relevant to experts. Practitioners also assumed that experts would discover software benefits independently and then communicate them to their peers.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "5.2.1Recommendation: Clearly communicate the capabilities of a new system.", "text": "When introducing a new AI system, clearly communicate how experts could benefit from it. Communicate not what the system can do but what the system can do for them as experts and for their projects. Use domain-specific language and avoid technical jargon. Research in digital skill education showed that improving users' involvement and facilitating their learning requires the identification of specific functions of the technology for each member based on their expertise [13]. Our interviewed experts were willing to invest their time and effort if they could see how they could benefit from the system. This aligns with research showing that the relevance of the learning material and tasks is an important motivator to seek more knowledge [35]. Showcase the capabilities of the system by using practical demonstrations. One of the most convincing factors for the adoption of a new system was seeing other experts use it on a similar dataset or project. For example, one of the experts decided if they would invest time in trying the software and learning about it if they saw another scientist demonstrating it on Twitter. We recommend including practical demonstrations (not for teaching purposes) tailored to the experts during training workshops. If this is not possible, we recommend preparing short videos in collaboration with domain experts that would be made available online or internally to show how it could be used and benefit them. Begin talking about the potential capabilities of the software with experts before it is fully developed. It can help to see if experts' and practitioners' expectations align, better understand experts' needs, and avoid developing unusable systems. It could also help experts feel more in control and less averse to the technology [17,64].", "publication_ref": ["b12", "b34", "b16", "b63"], "figure_ref": [], "table_ref": []}, {"heading": "Barriers to AI adoption due to effective collaboration between experts and practitioners", "text": "Our study revealed the importance of ongoing collaborative efforts for responsible AI adoption. Infrequent communication without a pre-defined plan and set meetings did not lead to established collaboration, even when teams worked in the same building. Without it, members of both groups were increasingly frustrated because of the expectations projected onto them. Our results showed that consistent and planned collaboration could resolve these frustrations. When experts and practitioners had to work together, they established mutual understanding, aligned their expectations, and learned how to ask questions and explain information that would be understandable and meaningful to the other party. After collaboration, experts were motivated to learn and troubleshoot independently and invest time in taking steps to improve the software. When two teams worked together because of the nature of the projects, experts actively got involved, and even when the software did not perform as they expected immediately, they knew that it was still in the stage of development. Instead of rejecting it, they gave experts feedback that could help improve the system. Expert engagement can help define expectations for system performance and identify contextual factors that are likely to change when the system is implemented in practice [18].", "publication_ref": ["b17"], "figure_ref": [], "table_ref": []}, {"heading": "5.3.1Recommendation: Plan the collaboration.", "text": "Implement the plan for collaboration between the experts and practitioners (or other stakeholders). For example, start with agreeing on the type (online, in-person), the frequency and the minimum timeframe for collaboration. The collaboration does not have to be extensive in the time spent working together, but it must be planned and continuous. Having collaboration planned could take the burden off someone having to initiate meetings. It could also change the dynamic from experts only contacting practitioners to reporting problems or at the stage where they already feel frustrated. It could also help to continue when collaborative efforts are not immediately beneficial. Our results showed that establishing an understanding between multidisciplinary teams required continuing collaboration even when it felt ineffective. Poorly planned practices have been shown to complicate effective collaboration [63]. Having a collaboration plan improved engagement from both teams. Improving interdisciplinary communication skills is essential for effectively providing and receiving feedback [27,85]. Effective user feedback is necessary for generating, refining, and fine-tuning design options and alternatives [27]. Feedback is critical to framing arguments and balancing multiple perspectives and considerations in the design process [27]. Collaboration practices, such as team discussions, should also be applied during the initial stages of system use with the goal of learning. Learning through discussion is an effective way of acquiring new information. Collaborating can create a dynamic learning system with humans in the loop [70].\nDefine collaborative goals for different stages of the process. The ability to work effectively in teams is driven by having a common goal and assuming shared responsibility for completing tasks [48]. For example, during the planning or design stages, collaborate on aligning expectations and trying to understand experts and their workflows better. Our study showed that starting collaborative efforts before the software was developed helped meet users' needs better. Early efforts also made users more confident, independent, and persistent when testing the new system, troubleshooting it, and communicating issues and feedback. Collaboration could also be applied in the mid-stages of software development, with a goal to demonstrate and discuss a preliminary version of the project. Involving users through collaboration is necessary for effective AI development [32]. Our interviewed expert, involved in a collaboration, volunteered to test a Beta version of a new product. The argument that experts' needs should guide the development of the technology is in line with usercentred HCI research goals [28]. Including users in the design process is also the basis of the participatory design methods [86], which have been effective in various domains [2,61].", "publication_ref": ["b62", "b26", "b84", "b26", "b26", "b69", "b47", "b31", "b27", "b85", "b1", "b60"], "figure_ref": [], "table_ref": []}, {"heading": "Further Implications", "text": "Our study revealed that the growing pressure and necessity to use AI systems can put a significant emotional burden on experts. Introducing new technologies to experts' workflows without appropriate support or preparation negatively affected their emotional well-being and reduced the time they could spend on their projects. The interviewed scientists were experts who had access to all the best equipment, creating the potential for making ground-breaking scientific discoveries in life sciences. However, they were spending a lot of time figuring out how to use software, unable to process available data. Experts admitted that this process affected their wellbeing, quality of work and confidence. Addressing our recognised issues could reduce this stress, allowing experts to focus on their expert tasks and progress with their work. Our study also emphasised an issue of lack of appreciation for experts' support needs with practical aspects, such as command lines and installation processes. Preparation and ongoing assistance should be prioritised when introducing new AI systems.\nOtherwise, there is a risk that the application of expert skills will be limited across domains that could greatly benefit from adopting AI.\nApplying our suggestions could help AI developers calibrate experts' expectations, motivate them to learn about the AI system, advocate it to their peers, effectively communicate their feedback, and be more independent users. The interviewed practitioners referred to the lack of resources, such as time, that prevented them from providing more support for experts. However, practitioners admitted that their approach was ineffective, focusing too much on the technology and presenting it to experts only when it was developed. This led to cases where their software was unused and had to be abandoned. We suggest that initial time can be found by reprioritising tasks and that it will pay off later, as practitioners would have to spend less time troubleshooting and advocating for their products. Following our steps towards better collaboration could also facilitate research approaches, such as participatory design, requiring meaningful user engagement and collaborative efforts between stakeholders.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations and Future Directions", "text": "Our study focused on a specific domain and two teams working within the same organisation and environment. A study involving more participants with varying levels of task and domain expertise is necessary to explore further the practical barriers to AI adoption arising at various stages. These findings provide meaningful insights regarding AI adoption among experts that could be generalised across different areas and domains. However, further studies are needed to test our recommendations in domains of varying risk, time pressure, and the extent to which human experts rely on AI.\nThis study revealed that the practitioners' lack of appreciation for experts' limited computational knowledge prevented experts from effectively adopting AI tools. Experts often do not have a choice but to use AI to exercise their expertise. The pressure to use new technologies will only increase with further advances in AI and data collection methods. Researchers should think of ways to support and motivate experts to learn about AI. But they should also explore ways to educate developers to communicate information about their developed systems effectively and support users more proactively. Future studies should examine ways to support mutual learning processes through collaboration.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "CONCLUSION", "text": "Research analysing AI adoption often explores users' actions and feelings towards AI outputs or how certain software features affect users' trust and reliance on AI. The practical hurdles users face when these systems are embedded in their workflows are significantly less explored. Our study has revealed that guiding users through mundane practicalities (e.g., software installation), providing active support, and establishing ongoing collaborative efforts are fundamental to increasing AI uptake. The paper has presented steps likely to help remove the key barriers separating experts from the systems they need and want to use. We propose i) actively supporting experts during the initial stages of software use, ii) clearly communicating the software's capabilities in a way that is relevant to the experts' language, and iii) following predefined collaboration rules. We argue that following these recommendations could make experts more accepting of AI systems and increase their motivation to understand the technology they use, advocate it to their peers, and be more independent and confident using it. Moreover, it could help them to be more specific in communicating their feedback to practitioners. Overall, supporting experts more effectively and promoting between-team collaboration could make them more conscious and responsible AI users, allowing them to benefit more from new technologies. Our findings could facilitate research approaches, such as participatory design, requiring meaningful user engagement and stakeholder collaborative efforts to improve AI technology's contextual fit and usability.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Factors Inhibiting the Adoption of Artificial Intelligence at organizational-level: A Preliminary Investigation", "journal": "", "year": "", "authors": "Abdallah Sulaiman; Dr Alsheibani; Yen Cheung; Chris Dr;  Messom"}, {"ref_id": "b1", "title": "Stranger Danger! Social Media App Features Co-designed with Children to Keep Them Safe Online", "journal": "Association for Computing Machinery", "year": "2019-06-12", "authors": "Karla Badillo-Urquiola; Diva Smriti; Brenna Mcnally; Evan Golub; Elizabeth Bonsignore; Pamela J Wisniewski"}, {"ref_id": "b2", "title": "Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI", "journal": "Inf. Fusion", "year": "2020-06", "authors": "Alejandro Barredo Arrieta; Natalia D\u00edaz-Rodr\u00edguez; Javier Del Ser; Adrien Bennetot; Siham Tabik; Alberto Barbado; Salvador Garcia; Sergio Gil-Lopez; Daniel Molina; Richard Benjamins; Raja Chatila; Francisco Herrera"}, {"ref_id": "b3", "title": "Tipping point: The narrow path between automation acceptance and rejection in air traffic management", "journal": "Saf. Sci", "year": "2012-02", "authors": "Marek Bekier; R C Brett; Ann Molesworth;  Williamson"}, {"ref_id": "b4", "title": "Grounded Theory: A Practical Guide", "journal": "SAGE", "year": "2022", "authors": "Melanie Birks; Jane Mills"}, {"ref_id": "b5", "title": "Humans rely more on algorithms than social influence as a task becomes more difficult", "journal": "Sci. Rep", "year": "2021-04", "authors": "Eric Bogert; Aaron Schecter; Richard T Watson"}, {"ref_id": "b6", "title": "Decision-making framework with double-loop learning through interpretable black-box machine learning models", "journal": "Ind. Manag. Data Syst", "year": "2017", "authors": "Marko Bohanec; Marko Robnik-Sikonja; Mirjana Kljajic; Borstnar "}, {"ref_id": "b7", "title": "What Do People Really Want When They Say They Want \"Explainable AI?", "journal": "ACM", "year": "2020-04-25", "authors": "Andrea Brennen"}, {"ref_id": "b8", "title": "Grounded theory research: A design framework for novice researchers", "journal": "SAGE Open Med", "year": "2019-01", "authors": "Ylona Chun Tie; Melanie Birks; Karen Francis"}, {"ref_id": "b9", "title": "Qo Techniques and Procedures for Developing Grounded Theory", "journal": "", "year": "", "authors": "Juliet Corbin; Anselm Strauss"}, {"ref_id": "b10", "title": "Partnering with AI: the case of digital productivity assistants", "journal": "J. R. Soc. N. Z", "year": "2023-01", "authors": "Jocelyn Cranefield; Michael Winikoff; Yi-Te Chiu; Yevgeniya Li; Cathal Doyle; Alex Richter"}, {"ref_id": "b11", "title": "A Case for Humans-in-the-Loop: Decisions in the Presence of Erroneous Algorithmic Scores", "journal": "ACM", "year": "2020-04-21", "authors": "Maria De-Arteaga; Riccardo Fogliato; Alexandra Chouldechova"}, {"ref_id": "b12", "title": "The digital divide shifts to differences in usage", "journal": "New Media Soc", "year": "2014-05", "authors": "Alexander Jam Van Deursen; Jan Agm Van Dijk"}, {"ref_id": "b13", "title": "Human-centered explainability for life sciences, healthcare, and medical informatics", "journal": "Patterns", "year": "2022-05", "authors": "Sanjoy Dey; Prithwish Chakraborty; Bum Chul Kwon; Amit Dhurandhar; Mohamed Ghalwash; Fernando J Suarez Saiz; Kenney Ng; Daby Sow; R Kush; Pablo Varshney;  Meyer"}, {"ref_id": "b14", "title": "Lay Perceptions of Selection Decision Aids in US and Non-US Samples", "journal": "Int. J. Sel. Assess", "year": "2011", "authors": "Dalia L Diab; Shuang-Yueh Pui; Maya Yankelevich; Scott Highhouse"}, {"ref_id": "b15", "title": "Algorithm aversion: People erroneously avoid algorithms after seeing them err", "journal": "J. Exp. Psychol. Gen", "year": "2015", "authors": "Berkeley J Dietvorst; Joseph P Simmons; Cade Massey"}, {"ref_id": "b16", "title": "Overcoming Algorithm Aversion", "journal": "", "year": "", "authors": "J Berkeley; Joseph P Dietvorst; Cade Simmons;  Massey"}, {"ref_id": "b17", "title": "Adaptations to Trust Incidents with Artificial Intelligence", "journal": "Proc. Hum. Factors Ergon. Soc. Annu. Meet", "year": "2022-09", "authors": "Stephen L Dorton; Samantha B Harper; Kelly J Neville"}, {"ref_id": "b18", "title": "Disrupted Routines: Team Learning and New Technology Implementation in Hospitals", "journal": "Adm. Sci. Q", "year": "2001-12", "authors": "Amy C Edmondson; Richard M Bohmer; Gary P Pisano"}, {"ref_id": "b19", "title": "Charting the Sociotechnical Gap in Explainable AI: A Framework to Address the Gap in XAI", "journal": "Proc. ACM Hum.-Comput. Interact", "year": "2023-04", "authors": "Koustuv Upol Ehsan; Munmun De Saha; Mark O Choudhury;  Riedl"}, {"ref_id": "b20", "title": "Many miles to go \u2026\": a systematic review of the implementation of patient decision support interventions into routine clinical practice", "journal": "BMC Med. Inform. Decis. Mak", "year": "2013-11", "authors": "Glyn Elwyn; Isabelle Scholl; Caroline Tietbohl; Mala Mann; Adrian Gk Edwards; Catharine Clay; France L\u00e9gar\u00e9; Trudy Van Der Weijden; Carmen L Lewis; Richard M Wexler; Dominick L Frosch"}, {"ref_id": "b21", "title": "Perspectives from Practice: Algorithmic Decision-Making in Public Employment Services", "journal": "Association for Computing Machinery", "year": "2021-10-23", "authors": " Asbj\u00f8rn ; Fl\u00fcgge"}, {"ref_id": "b22", "title": "Snowball Sampling", "journal": "Ann. Math. Stat", "year": "1961", "authors": "Leo A Goodman"}, {"ref_id": "b23", "title": "Disparate Interactions: An Algorithm-in-the-Loop Analysis of Fairness in Risk Assessments", "journal": "ACM", "year": "2019-01-29", "authors": "Ben Green; Yiling Chen"}, {"ref_id": "b24", "title": "The Principles and Limits of Algorithm-in-the-Loop Decision Making", "journal": "Proc. ACM Hum.-Comput. Interact", "year": "2019-11", "authors": "Ben Green; Yiling Chen"}, {"ref_id": "b25", "title": "Man against machine: diagnostic performance of a deep learning convolutional neural network for dermoscopic melanoma recognition in comparison to 58 dermatologists", "journal": "Ann. Oncol", "year": "2018-08", "authors": "H A Haenssle; C Fink; R Schneiderbauer; F Toberer; T Buhl; A Blum; A Kalloo; A Ben Hadj Hassen; L Thomas; A Enk; L Uhlmann; Christina Alt; Monika Arenbergerova; Renato Bakos; Anne Baltzer; Ines Bertlich; Andreas Blum; Therezia Bokor-Billmann; Jonathan Bowling; Naira Braghiroli; Ralph Braun; Kristina Buder-Bakhaya; Timo Buhl; Horacio Cabo; Leo Cabrijan; Naciye Cevic; Anna Classen; David Deltgen; Christine Fink; Ivelina Georgieva; Lara-Elena Hakim-Meibodi; Susanne Hanner; Franziska Hartmann; Julia Hartmann; Georg Haus; Elti Hoxha; Raimonds Karls; Hiroshi Koga; J\u00fcrgen Kreusch; Aimilios Lallas; Pawel Majenka; Ash Marghoob; Cesare Massone; Lali Mekokishvili; Dominik Mestel; Volker Meyer; Anna Neuberger; Kari Nielsen; Margaret Oliviero; Riccardo Pampena; John Paoli; Erika Pawlik; Barbar Rao; Adriana Rendon; Teresa Russo; Ahmed Sadek; Kinga Samhaber; Roland Schneiderbauer; Anissa Schweizer; Ferdinand Toberer; Lukas Trennheuser; Lyobomira Vlahova; Alexander Wald; Julia Winkler; Priscila W\u00f6lbing; Iris Zalaudek"}, {"ref_id": "b26", "title": "How to give and receive feedback effectively", "journal": "Breathe", "year": "2017-12", "authors": "Georgia Hardavella; Ane Aamli-Gaagnat; Neil Saad; Ilona Rousalova; Katherina B Sreter"}, {"ref_id": "b27", "title": "Human-Centered AI Design in Reality: A Study of Developer Companies' Practices: A study of Developer Companies' Practices", "journal": "ACM", "year": "2022-10-08", "authors": "Maria Hartikainen; Kaisa V\u00e4\u00e4n\u00e4nen; Anu Lehti\u00f6; Saara Ala-Luopa; Thomas Olsson"}, {"ref_id": "b28", "title": "Human-machine teaming is key to AI adoption: clinicians' experiences with a deployed machine learning system", "journal": "Npj Digit. Med", "year": "2022-07", "authors": "Katharine E Henry; Rachel Kornfield; Anirudh Sridharan; Robert C Linton; Catherine Groh; Tony Wang; Albert Wu; Bilge Mutlu; Suchi Saria"}, {"ref_id": "b29", "title": "What do we need to build explainable AI systems for the medical domain", "journal": "", "year": "2017-09-01", "authors": "Andreas Holzinger; Chris Biemann; Constantinos S Pattichis; Douglas B Kell"}, {"ref_id": "b30", "title": "Who is the Expert? Reconciling Algorithm Aversion and Algorithm Appreciation in AI-Supported Decision Making", "journal": "Proc. ACM Hum.-Comput. Interact", "year": "2021-10", "authors": "Tsung-Yu Yoyo; Malte F Hou;  Jung"}, {"ref_id": "b31", "title": "Where is the Human?: Bridging the Gap Between AI and HCI", "journal": "ACM", "year": "2019-05-02", "authors": "Kori Inkpen; Stevie Chancellor; Munmun De Choudhury; Michael Veale; Eric P S Baumer"}, {"ref_id": "b32", "title": "How machine-learning recommendations influence clinician treatment selections: the example of antidepressant selection", "journal": "Transl. Psychiatry", "year": "2021-02", "authors": "Maia Jacobs; Melanie F Pradier; Thomas H Mccoy; Roy H Perlis; Finale Doshi-Velez; Krzysztof Z Gajos"}, {"ref_id": "b33", "title": "History and future of human-automation interaction", "journal": "Int. J. Hum.-Comput. Stud", "year": "2019-11", "authors": "Christian P Janssen; Stella F Donker; Duncan P Brumby; Andrew L Kun"}, {"ref_id": "b34", "title": "COMPUTER-BASED PRACTICAL WORK AT A DISTANCE: A CASE STUDY", "journal": "Elsevier", "year": "1994", "authors": "Ann Jones; Marian Petre"}, {"ref_id": "b35", "title": "The effects of level of automation and adaptive automation on human performance, situation awareness and workload in a dynamic control task", "journal": "Theor. Issues Ergon. Sci", "year": "2004-03", "authors": "David B Kaber; Mica R Endsley"}, {"ref_id": "b36", "title": "Making Sense of Sensemaking 2: A Macrocognitive Model", "journal": "Intell. Syst. IEEE", "year": "2006-10", "authors": "Gary Klein; Brian Moon; Robert Hoffman"}, {"ref_id": "b37", "title": "A data-frame theory of sensemaking", "journal": "", "year": "2007-01", "authors": "J K Gary Klein; E L Phillips; Deborah Rall;  Peluso"}, {"ref_id": "b38", "title": "Toward a unified theory of consumer acceptance technology", "journal": "Psychol. Mark", "year": "2007", "authors": "Songpol Kulviwat; Gordon C Bruner; I I ; Anand Kumar; Suzanne A Nasco; Terry Clark"}, {"ref_id": "b39", "title": "Artificial intelligence in disease diagnosis: a systematic literature review, synthesizing framework and future research agenda", "journal": "J. Ambient Intell. Humaniz. Comput", "year": "2023-07", "authors": "Yogesh Kumar; Apeksha Koul; Ruchi Singla; Muhammad Fazal; Ijaz "}, {"ref_id": "b40", "title": "Enhancing computer self-efficacy and attitudes in multi-ethnic older adults: a randomised controlled study", "journal": "Ageing Soc", "year": "2011-08", "authors": "Luciana Lagan\u00e1; Taylor Oliver; Andrew Ainsworth; Marc Edwards"}, {"ref_id": "b41", "title": "In automation we trust\" -Australian air traffic controller perspectives of increasing automation in air traffic management", "journal": "Transp. Policy", "year": "2022-09", "authors": "Kieran Langford; Tarryn Kille; Seung-Yong Lee; Yahua Zhang; Paul R Bates"}, {"ref_id": "b42", "title": "A Human-Centered Approach to Algorithmic Services: Considerations for Fair and Motivating Smart Community Service Management that Allocates Donations to Non-Profit Organizations", "journal": "Association for Computing Machinery", "year": "2017-05-02", "authors": "Min Kyung; Lee ; Ji Tae Kim; Leah Lizarondo"}, {"ref_id": "b43", "title": "Algorithm appreciation: People prefer algorithmic to human judgment", "journal": "Organ. Behav. Hum. Decis. Process", "year": "2019-03", "authors": "Jennifer M Logg; Julia A Minson; Don A Moore"}, {"ref_id": "b44", "title": "What influences algorithmic decision-making? A systematic literature review on algorithm aversion", "journal": "Technol. Forecast. Soc. Change", "year": "2022-02", "authors": "A K M Hasan Mahmud; Syed Ishtiaque Islam; Kari Ahmed;  Smolander"}, {"ref_id": "b45", "title": "Adopting evidencebased practice in clinical decision making: nurses' perceptions, knowledge, and barriers", "journal": "J. Med. Libr. Assoc. JMLA", "year": "2011-07", "authors": "Shaheen Majid; Schubert Foo; Brendan Luyt; Xue Zhang; Yin-Leng Theng; Yun-Ke Chang; Intan A Mokhtar"}, {"ref_id": "b46", "title": "The big challenges of big data", "journal": "Nature", "year": "2013-06", "authors": "Vivien Marx"}, {"ref_id": "b47", "title": "What 21st Century Learning? A review and a synthesis", "journal": "", "year": "2011-03-07", "authors": "Punya Mishra; Kristen Kereluik"}, {"ref_id": "b48", "title": "Using Technology and Constituting Structures: A Practice Lens for Studying Technology in Organizations", "journal": "Organ. Sci", "year": "2000-08", "authors": "Wanda J Orlikowski"}, {"ref_id": "b49", "title": "The adoption of artificial intelligence in employee recruitment: The influence of contextual factors", "journal": "Int. J. Hum. Resour. Manag", "year": "2022-03", "authors": "Fabian Yuan Pan; Ni Froese; Yunyang Liu; Maolin Hu;  Ye"}, {"ref_id": "b50", "title": "Humans and Automation: Use, Misuse, Disuse, Abuse", "journal": "Hum. Factors J. Hum. Factors Ergon. Soc", "year": "1997-06", "authors": "Raja Parasuraman; Victor Riley"}, {"ref_id": "b51", "title": "Human-AI Interaction in Human Resource Management: Understanding Why Employees Resist Algorithmic Evaluation at Workplaces and How to Mitigate Burdens", "journal": "Association for Computing Machinery", "year": "2021-05-07", "authors": "Hyanghee Park; Daehwan Ahn; Kartik Hosanagar; Joonhwan Lee"}, {"ref_id": "b52", "title": "Artificial intelligence in drug discovery and development", "journal": "Drug Discov. Today", "year": "2021-01", "authors": "Debleena Paul; Gaurav Sanap; Snehal Shenoy; Dnyaneshwar Kalyane; Kiran Kalia; Rakesh K Tekade"}, {"ref_id": "b53", "title": "Incorporating Explainable Artificial Intelligence (XAI) to aid the Understanding of Machine Learning in the Healthcare Domain", "journal": "", "year": "2020", "authors": "Urja Pawar; O' Donna; Susan Shea;  Rea; O' Ruairi;  Reilly"}, {"ref_id": "b54", "title": "Learning-before-doing in the development of new process technology", "journal": "Res. Policy", "year": "1996-10", "authors": "P Gary;  Pisano"}, {"ref_id": "b55", "title": "How to Discriminate between Computer-Aided and Computer-Hindered Decisions: A Case Study in Mammography", "journal": "Med. Decis. Making", "year": "2013-01", "authors": "Andrey A Povyakalo; Eugenio Alberdi; Lorenzo Strigini; Peter Ayton"}, {"ref_id": "b56", "title": "The Algorithmic Automation Problem: Prediction, Triage, and Human Effort", "journal": "", "year": "2019-09-04", "authors": "Maithra Raghu; Katy Blumer; Greg Corrado; Jon Kleinberg; Ziad Obermeyer; Sendhil Mullainathan"}, {"ref_id": "b57", "title": "Using contextual inquiry to learn about your audiences", "journal": "ACM SIGDOC Asterisk J. Comput. Doc", "year": "1996-02", "authors": "Mary Elizabeth; Raven ; Alicia Flanders"}, {"ref_id": "b58", "title": "Beyond Accuracy: Behavioral Testing of NLP models with CheckList", "journal": "", "year": "2020", "authors": "Marco Tulio Ribeiro; Tongshuang Wu; Carlos Guestrin; Sameer Singh"}, {"ref_id": "b59", "title": "", "journal": "Towards Explainable Artificial Intelligence", "year": "2019", "authors": "Wojciech Samek; Klaus-Robert M\u00fcller"}, {"ref_id": "b60", "title": "A Human-Centered Review of Algorithms used within the U.S. Child Welfare System", "journal": "ACM", "year": "2020-04-21", "authors": "Devansh Saxena; Karla Badillo-Urquiola; Pamela J Wisniewski; Shion Guha"}, {"ref_id": "b61", "title": "Social cognitive theory", "journal": "American Psychological Association", "year": "2012", "authors": "H Dale;  Schunk"}, {"ref_id": "b62", "title": "Machines as teammates: A research agenda on AI in team collaboration", "journal": "Inf. Manage", "year": "2020-03", "authors": "Isabella Seeber; Eva Bittner; Robert O Briggs; Triparna De Vreede; Gert-Jan De Vreede; Aaron Elkins; Ronald Maier; Alexander B Merz; Sarah Oeste-Rei\u00df; Nils Randrup; Gerhard Schwabe; Matthias S\u00f6llner"}, {"ref_id": "b63", "title": "Artificial intelligence in drug discovery", "journal": "Future Med. Chem", "year": "2018-09", "authors": "Mohamed Matthew A Sellwood; Marwin Hs Ahmed; Nathan Segler;  Brown"}, {"ref_id": "b64", "title": "The Lean Data Scientist: Recent Advances Toward Overcoming the Data Bottleneck", "journal": "Commun. ACM", "year": "2023-02", "authors": "Chen Shani; Jonathan Zarecki; Dafna Shahaf"}, {"ref_id": "b65", "title": "Explainability for experts: A design framework for making algorithms supporting expert decisions more explainable", "journal": "J. Responsible Technol", "year": "2021-11", "authors": "Auste Simkute; Ewa Luger; Bronwyn Jones; Michael Evans; Rhianne Jones"}, {"ref_id": "b66", "title": "Ignore, Trust, or Negotiate: Understanding Clinician Acceptance of AI-Based Treatment Recommendations in Health Care", "journal": "ACM", "year": "2023-04-19", "authors": "Leigh A Venkatesh Sivaraman; Joel Bukowski; Jeremy M Levin; Adam Kahn;  Perer"}, {"ref_id": "b67", "title": "How and where is artificial intelligence in the public sector going? A literature review and research agenda", "journal": "Gov. Inf. Q", "year": "2019-10", "authors": "Weslei Gomes De Sousa; Elis ; Regina Pereira De Melo; Paulo ; Henrique De Souza Bermejo; Rafael Ara\u00fajo Sousa Farias; Adalmir Oliveira Gomes"}, {"ref_id": "b68", "title": "Maintaining the Role of Humans in the Forecast Process: Analyzing the Psyche of Expert Forecasters", "journal": "Bull. Am. Meteorol. Soc", "year": "2007-12", "authors": "Neil A Stuart; David M Schultz; Gary Klein"}, {"ref_id": "b69", "title": "Robotic Process Automation: Contemporary themes and challenges", "journal": "Comput. Ind", "year": "2020-02", "authors": "Rehan Syed; Suriadi Suriadi; Michael Adams; Wasana Bandara; J J Sander; Chun Leemans;  Ouyang; H M Arthur; Inge Ter Hofstede;  Van De; Moe Weerd; Hajo A Thandar Wynn;  Reijers"}, {"ref_id": "b70", "title": "The Illusion of Control: Placebo Effects of Control Settings", "journal": "ACM", "year": "2018-04-19", "authors": "Kristen Vaccaro; Dylan Huang; Motahhare Eslami; Christian Sandvig; Kevin Hamilton; Karrie Karahalios"}, {"ref_id": "b71", "title": "A systematic review of human-AI interaction in autonomous ship systems", "journal": "Saf. Sci", "year": "2022-08", "authors": "Erik Veitch; Ole Andreas; Alsos "}, {"ref_id": "b72", "title": "Technology Acceptance Model 3 and a Research Agenda on Interventions", "journal": "Decis. Sci", "year": "2008", "authors": "Viswanath Venkatesh; Hillol Bala"}, {"ref_id": "b73", "title": "A Theoretical Extension of the Technology Acceptance Model: Four Longitudinal Field Studies", "journal": "Manag. Sci", "year": "2000", "authors": "Viswanath Venkatesh; Fred D Davis"}, {"ref_id": "b74", "title": "Consumer Acceptance and Use of Information Technology: Extending the Unified Theory of Acceptance and Use of Technology", "journal": "MIS Q", "year": "2012", "authors": "Viswanath Venkatesh; Y L James; Xin Thong;  Xu"}, {"ref_id": "b75", "title": "Are Explanations Helpful? A Comparative Study of the Effects of Explanations in AI-Assisted Decision-Making", "journal": "ACM", "year": "2021-04-14", "authors": "Xinru Wang; Ming Yin"}, {"ref_id": "b76", "title": "Strategic Conformance: Overcoming Acceptance Issues of Decision Aiding Automation?", "journal": "IEEE Trans. Hum.-Mach. Syst", "year": "2016-02", "authors": "Carl Westin; Clark Borst; Brian Hilburn"}, {"ref_id": "b77", "title": "Expert Systems versus Systems for Experts", "journal": "", "year": "1995", "authors": "Jack Whalen"}, {"ref_id": "b78", "title": "Extrafollicular B cell responses correlate with neutralizing antibodies and morbidity in COVID-19", "journal": "Nat. Immunol", "year": "2020-12", "authors": "C Matthew; Richard P Woodruff;  Ramonell; C Doan; Kevin S Nguyen; Ankur Singh Cashman; Natalie S Saini; Ariel M Haddad; Shuya Ley; J Christina Kyu; Tugba Howell; Saeyun Ozturk; Naveenchandra Lee; James Suryadevara; Regina Brett Case; Weirong Bugrovsky; Jacob Chen; Andrea Estrada; Andrew Morrison-Porter; Fabliha A Derrico; Monika Anam; Henry M Sharma; Sang N Wu; Scott A Le; Christopher M Jenks; Bashar Tipton; John L Staitieh; Eliver Daiss; Michael S Ghosn; Robert H Diamond; James E Carnahan; William T Crowe; F Hu; Ignacio Eun-Hyung Lee;  Sanz"}, {"ref_id": "b79", "title": "What Does It Mean to Explain? A User-Centered Study on AI Explainability", "journal": "Cham. Springer International Publishing", "year": "2021", "authors": "Lingxue Yang; Hongrun Wang; L\u00e9a A Deleris"}, {"ref_id": "b80", "title": "Understanding the Effect of Accuracy on Trust in Machine Learning Models", "journal": "ACM", "year": "2019-01-12", "authors": "Ming Yin; Jennifer Wortman Vaughan; Hanna Wallach"}, {"ref_id": "b81", "title": "Viewpoint: Human-in-the-loop Artificial Intelligence", "journal": "J. Artif. Intell. Res", "year": "2019-02", "authors": "Fabio Massimo; Zanzotto "}, {"ref_id": "b82", "title": "Effect of Confidence and Explanation on Accuracy and Trust Calibration in AI-Assisted Decision Making", "journal": "", "year": "2020-01-27", "authors": "Yunfeng Zhang; Q Vera; Rachel K E Liao;  Bellamy"}, {"ref_id": "b83", "title": "The Application of Artificial Intelligence in Alzheimer's Research", "journal": "Tsinghua Sci. Technol", "year": "2024-02", "authors": "Qing Zhao; Hanrui Xu; Jianqiang Li; Faheem Akhtar Rajput; Liyan Qiao"}, {"ref_id": "b84", "title": "Interdisciplinary collaboration between nursing and engineering in health care: A scoping review", "journal": "Int. J. Nurs. Stud", "year": "2021-05", "authors": "Ying Zhou; Zheng Li; Yingxin Li"}, {"ref_id": "b85", "title": "Participatory Design of AI Systems: Opportunities and Challenges Across Diverse Users, Relationships, and Application Domains", "journal": "ACM", "year": "2022-04-27", "authors": "Douglas Zytko; Pamela J Wisniewski; Shion Guha; P S Eric; Min Kyung Baumer;  Lee"}], "figures": [], "formulas": [{"formula_id": "formula_0", "formula_text": "\"[\u2026]", "formula_coordinates": [6.0, 390.63, 389.72, 20.06, 8.01]}], "doi": "10.1145/3311927.3323133"}
