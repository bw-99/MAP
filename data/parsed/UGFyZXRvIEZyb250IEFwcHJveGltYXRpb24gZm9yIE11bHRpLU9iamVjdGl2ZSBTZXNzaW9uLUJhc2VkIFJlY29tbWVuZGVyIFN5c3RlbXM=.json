{"Pareto Front Approximation for Multi-Objective Session-Based Recommender Systems": "Timo Wilm timo.wilm@otto.de OTTO (GmbH & Co KG) Hamburg, Germany Philipp Normann philipp.normann@otto.de OTTO (GmbH & Co KG) Hamburg, Germany Felix Stepprath felix.stepprath@otto.de OTTO (GmbH & Co KG) Hamburg, Germany", "ABSTRACT": "This work introduces MultiTRON , an approach that adapts Pareto front approximation techniques to multi-objective session-based recommender systems using a transformer neural network. Our approach optimizes trade-offs between key metrics such as clickthrough and conversion rates by training on sampled preference vectors. A significant advantage is that after training, a single model can access the entire Pareto front, allowing it to be tailored to meet the specific requirements of different stakeholders by adjusting an additional input vector that weights the objectives. We validate the model's performance through extensive offline and online evaluation. For broader application and research, the source code 1 is made available. The results confirm the model's ability to manage multiple recommendation objectives effectively, offering a flexible tool for diverse business needs.", "CCS CONCEPTS": "\u00b7 Applied computing \u2192 Online shopping ; \u00b7 Information systems \u2192 Recommender systems ; \u00b7 Computing methodologies \u2192 Multi-task learning ; Neural networks .", "KEYWORDS": "session-based recommender systems, multi-objective, pareto front", "ACMReference Format:": "Timo Wilm, Philipp Normann, and Felix Stepprath. 2024. Pareto Front Approximation for Multi-Objective Session-Based Recommender Systems. In 18th ACM Conference on Recommender Systems (RecSys '24), October 14-18, 2024, Bari, Italy. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/ 3640457.3688048", "1 INTRODUCTION": "Large e-commerce platforms such as OTTO face the complex task of optimizing diverse revenue streams through personalized recommendations. These systems cater to various business needs, such as sponsored product advertisements, which generate revenue per click, and organic recommendations to maximize conversions. The challenge lies in balancing these goals, as marketing teams prioritize high visibility and click-through rates for sponsored content, 1 https://github.com/otto-de/MultiTRON Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). RecSys '24, October 14-18, 2024, Bari, Italy \u00a9 2024 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-0505-2/24/10 https://doi.org/10.1145/3640457.3688048 whereas sales departments focus on enhancing conversion rates and customer loyalty through organic product listings. To address these conflicting objectives, multi-objective recommender systems offer a framework capable of optimizing across different metrics within a unified system [1, 12, 16, 19, 20, 29]. This study adapts Pareto front approximation techniques, previously successful in other domains [18, 21, 24, 25], to session-based recommender systems. By training on sampled preference vectors, our model can access the entire Pareto front at inference time, providing an efficient tool to meet the diverse business goals of stakeholders.", "2 RELATED WORK": "Recent research in multi-objective optimization for recommender systems has focused on balancing different goals such as accuracy, revenue, and fairness [7, 17, 27]. Traditional methods often involve training multiple models, using different loss scalarizations, constraints or initializations [16, 20, 23, 29]. However, these approaches become impractical as each point on the Pareto front requires a separate model, leading to high computational costs during training and inference, particularly with large datasets. To address these challenges, Pareto Front Learning (PFL) and Pareto Front Approximation (PFA) have emerged as scalable alternatives [11, 18]. These techniques allow a single model to approximate the entire Pareto front, providing flexibility to adjust to different objectives post-training. For example, Dosovitskiy and Djolonga [6] introduced a method that trains a deep neural network on a distribution of losses conditioned by an additional input vector, effectively integrating multiple objectives within a single model. Further advancements include Pareto hypernetworks (PHNs), which generate the model parameters based on preference vectors [11, 21, 25]. Additionally, Exact Pareto Optimal (EPO) Search has been combined with PHNs to ensure convergence to an exact Pareto optimal solution when one exists, or to the closest possible solution otherwise [19, 21]. This approach requires solving a linear program after each forward pass, leading to slower training. Although PHNs are efficient for exploring Pareto fronts, they face scalability challenges when applied to models with extensive parameters, such as those in recommender systems with large item sets. To overcome this limitation, Ruchte and Grabocka [24] proposed incorporating preference vectors directly into the model as input features, thereby eliminating the need for PHNs. The Transformer architecture has gained traction in sequential recommender systems, with models like TRON [26] demonstrating strong performance in session-based tasks. TRON's effectiveness in single-objective optimization motivates its extension to multiobjective settings, where its inherent flexibility can be leveraged for Pareto front approximation. RecSys '24, October 14-18, 2024, Bari, Italy Timo Wilm, Philipp Normann, and Felix Stepprath", "3 CONTRIBUTIONS": "We introduce MultiTRON , a novel extension of Pareto front approximation techniques tailored for multi-objective session-based recommender systems. Building on the Transformer-based TRON model [26], MultiTRON leverages sampled preference vectors and a scalarization approach with a customized regularization term to balance competing objectives, such as click-through and conversion rates. Our primary contributions include: (1) Scalable Pareto Front Approximation: We extend existing Pareto front approximation methods used in other domains by integrating them into a session-based Transformer model, enabling the efficient exploration of trade-offs between multiple objectives without the need for separate models for each point on the Pareto front. (2) Regularization for Improved Coverage: We propose a regularization technique, inspired by the non-uniformity loss introduced by Mahapatra and Rajan [19], which enhances the diversity of solutions along the Pareto front, ensuring better coverage and minimizing the risk of a collapsing front, while maintaining efficient training times. (3) Comprehensive Evaluation: MultiTRON is evaluated on a range of RecSys datasets, demonstrating its effectiveness in achieving competitive performance across multiple objectives. Additionally, we validate our approach through extensive online A/B testing, confirming the practical applicability in real-world e-commerce environments. (4) Open-Source Implementation: To facilitate further research and practical application of Pareto front approximation in session-based recommender systems, we provide an open-source implementation of MultiTRON. To the best of our knowledge, MultiTRON is the first model adapting PFA techniques to session-based recommender systems, providing a scalable and flexible solution that meets the diverse needs of modern e-commerce platforms.", "4 METHODS": "Multi-objective session-based recommender systems predict the next item interaction based on prior user activities. Each user session consists of user-item interactions \ud835\udc60 \ud835\udc5f\ud835\udc4e\ud835\udc64 = [ \ud835\udc56 \ud835\udc4e 1 1 , \ud835\udc56 \ud835\udc4e 2 2 , . . . , \ud835\udc56 \ud835\udc4e \ud835\udc47 \ud835\udc47 ] , where \ud835\udc47 is the session length, and \ud835\udc56 \ud835\udc4e \ud835\udc61 \ud835\udc61 represents the action taken on item \ud835\udc56 at time \ud835\udc61 . Actions include clicking or ordering, typically with orders following clicks. Sessions are modelled as \ud835\udc60 : = [( \ud835\udc50 1 , \ud835\udc5c 1 ) , ( \ud835\udc50 2 , \ud835\udc5c 2 ) , . . . , ( \ud835\udc50 \ud835\udc47 -1 , \ud835\udc5c \ud835\udc47 -1 )] , where \ud835\udc50 \ud835\udc61 is the clicked item at time \ud835\udc61 , and \ud835\udc5c \ud835\udc61 indicates if the item was ordered up to time \ud835\udc47 .", "4.1 Recommender Model and Loss Functions": "Our multi-objective recommender model R leverages past clicks to predict scores \ud835\udc5f \ud835\udc56 \ud835\udc61 for potential item interactions, optimizing the trade-off between click L \ud835\udc50 ( \ud835\udc50 \ud835\udc61 , \ud835\udc5f \ud835\udc56 \ud835\udc61 ) and order L \ud835\udc5c ( \ud835\udc5c \ud835\udc61 , \ud835\udc5f \ud835\udc56 \ud835\udc61 ) losses. Standard scalarization methods [18, 24] use a fixed preference vector \ud835\udf0b : = [ \ud835\udf0b \ud835\udc50 , \ud835\udf0b \ud835\udc5c ] , with \ud835\udf0b \ud835\udc50 + \ud835\udf0b \ud835\udc5c = 1, and minimize:  This approach does not scale for large datasets because each point on the Pareto front requires a separate model. Table 1: Statistics of the datasets used in our experiments.", "4.2 Pareto Front Approximation": "In Pareto front approximation, sampling \ud835\udf0b \u223c \ud835\udc37\ud835\udc56\ud835\udc5f ( \ud835\udefd ) from a Dirichlet distribution with parameter \ud835\udefd \u2208 R 2 > 0 during training and adding it to the input yields a model M(\u00b7 , \ud835\udf0b ) conditioned on \ud835\udf0b during inference [6, 21, 24, 25]. We adapt this approach to sequential recommender models R(\u00b7 , \ud835\udf0b ) and conclude from [6] that if R \u2217 minimizes  \u00ab \u2039 then R \u2217 minimizes Equation 1 almost surely w.r.t P \ud835\udf0b .", "4.3 Regularization and Pareto Front Coverage": "To address the limitation of narrow Pareto fronts [24], we also leverage the non-uniformity term from [19], defined as:  where \u02c6 \ud835\udf0b \ud835\udc58 : = \ud835\udf0b \ud835\udc58 L \ud835\udc58 \ud835\udf0b \ud835\udc50 L \ud835\udc50 + \ud835\udf0b \ud835\udc5c L \ud835\udc5c , 1 / 2 = GLYPH<2> 1 2 , 1 2 GLYPH<3> , and KL is the Kullback-Leibler divergence. The function \ud835\udc54 maps \u02c6 \ud835\udf0b to a vector of probabilities summing to 1. For instance, \ud835\udc54 could be chosen as the identity or the softmax function. By adding this regularization term in Equation 3 to the primary loss function in Equation 2, we avoid the need for solving a linear program after each forward pass, thereby maintaining efficient training speeds. This approach yields a Pareto front that approximately intersects with the inverse preference vector \ud835\udf0b -1 = h 1 \ud835\udc54 ( \ud835\udf0b \ud835\udc50 ) , 1 \ud835\udc54 ( \ud835\udf0b \ud835\udc5c ) i at the point [L \u2217 \ud835\udc50 (\u00b7 , \ud835\udf0b ) , L \u2217 \ud835\udc5c (\u00b7 , \ud835\udf0b )] [19].", "4.4 Overall Loss Function": "The overall loss is formulated as:  \u00ab \u2039 with \ud835\udf06 \u2265 0 as a regularization parameter. Our model MultiTRON minimizes the loss in Equation 4 to approximate the Pareto front.", "5 EXPERIMENTAL SETUP": "In our experiments, we evaluate the proposed approach using three benchmark datasets of varying complexity: Diginetica [5], Yoochoose [2], and OTTO [22]. These datasets differ in terms of the number of events and the diversity of item sets. The experiments focus on click and order events, ensuring a minimum item support of five and a session length of at least two clicks for all datasets [10]. Pareto Front Approximation for Multi-Objective Session-Based Recommender Systems RecSys '24, October 14-18, 2024, Bari, Italy Table 2: Hypervolumes for \ud835\udefd = [ 1 2 , 1 2 ] and different values of \ud835\udf06 for each dataset. The models are trained on Diginetica for 20 epochs and Yoochoose and OTTO for 10 epochs. We adopt a temporal train/test split approach for training and testing the models. Specifically, the entire last day from the Yoochoose dataset and the entire last week from the Diginetica and OTTO datasets are designated as test datasets [14], with the remainder used for training. Table 1 presents an overview of these datasets. All models are trained on an NVIDIA Tesla V100 GPU with a batch size of 256. MultiTRON uses the session-based Transformer architecture TRON [26], configured with three layers and a learning rate of 10 -4 . The loss functions used are binary cross-entropy loss for the order task ( L \ud835\udc5c ) and sampled softmax loss for the click task ( L \ud835\udc50 ) [26, 28]. We select \ud835\udc54 as the softmax function because it has smaller gradients than the identity and, in our experiments, provided more stable convergence to the Pareto front. The Dirichlet parameter \ud835\udefd = [ 1 2 , 1 2 ] is fixed, leading to \ud835\udf0b \u223c \ud835\udc37\ud835\udc56\ud835\udc5f ([ 1 2 , 1 2 ]) . The regularization parameter \ud835\udf06 is tuned between 0.02 and 1.0 for each dataset. For offline evaluation, the Hypervolume Indicator (HV) is employed [8], with reference points based on the nadir points [4] of each dataset: \ud835\udc5f \ud835\udc37 = [ 3 . 86 , 1 . 12 ] , \ud835\udc5f \ud835\udc4c = [ 4 . 03 , 0 . 17 ] , \ud835\udc5f \ud835\udc42 = [ 3 . 91 , 1 . 02 ] .", "6 EVALUATION": "Table 2 presents the results of our offline evaluation . We found that larger values of \ud835\udf06 led to increased hypervolumes, especially on more complex datasets. The Pareto fronts demonstrating the highest hypervolumes for each dataset are depicted in Figure 1. Incorporating the sampling parameter \ud835\udf0b into the model does not adversely impact training speed or increase the number of epochs required compared to training models optimized for single objectives with the same architecture. Given the extensive study of the click task in previous research [3, 9, 10, 13, 15, 26], we also provide the Recall@20 for \ud835\udf0b = [ 1 , 0 ] , which resulted in scores of 0.529 for Diginetica, 0.724 for Yoochoose, and 0.485 for OTTO. Our previous work [26] utilizing the same TRON architecture trained on solely the click task yielded Recall@20 scores of 0.541 ( -2 . 2%), 0.732 ( -1 . 1%), and 0.472 ( + 2 . 8%) for these datasets. These results demonstrate that MultiTRON performs comparably to single-objective click task models that use the same backbone. For the online evaluation , we utilized a model trained on OTTO's private data collected in May 2024. A live A/B test was conducted the following week, with four groups, each assigned different \ud835\udf0b values. The test results confirmed that the offline trade-off between -L \ud835\udc50 and -L \ud835\udc5c translates into real-world trade-offs between click-through rates (CTR) and conversion rates (CVR). Specifically, higher -L \ud835\udc5c values correlated with increased CVR, while higher -L \ud835\udc50 values correlated with increased CTR, as detailed in Figure 2. Figure 1: The best performing Pareto fronts from the offline evaluation on all three datasets showing the trade-off between L \ud835\udc50 and L \ud835\udc5c for 26 increasing values of \ud835\udf0b \ud835\udc5c . 3.66 3.74 3.83 c 0.09 0.40 0.72 o Diginetica 2.6 2.8 3.0 c 0.11 0.13 0.15 Yoochoose 2.0 3.0 3.9 c 0.18 0.60 1.02 OTTO 0.00 0.25 0.50 0.75 1.00 o Figure 2: Evaluation results from the offline dataset (left) and live A/B test (right). It demonstrates that points on the predicted offline front translate into real-world trade-offs of CTR and CVR. The colored points correspond to differing values of \ud835\udf0b \ud835\udc5c , representing the four different A/B test groups. -2.9 -2.8 -2.7 -2.6 -2.4 -2.3 c -0.35 -0.31 -0.27 -0.23 -0.19 o Offline Front 0.0 1.7 3.3 5.0 6.6 8.3 9.9 CTR uplift in % 0.0 1.1 2.3 3.4 4.5 5.7 6.8 CVR uplift in % Live Front 0.0 0.2 0.4 0.6 0.8 1.0 o", "7 CONCLUSION": "In this work, we successfully applied a Pareto front approximation technique to multi-objective session-based recommender systems. MultiTRON enables a single model to access the entire Pareto front, offering a scalable solution for balancing competing objectives without the need for multiple models. We also introduce a regularization term, to improve Pareto front coverage and convergence. The practical relevance of MultiTRON was demonstrated through offline evaluation on three benchmark datasets, as well as an online realworld A/B test. The ability of the offline calculated Pareto front to translate into real-world trade-offs of CTR and CVR in a live setting validates the model's effectiveness in commercial environments.", "8 SPEAKER BIO": "Timo Wilm and Philipp Normann are Senior Data Scientists specializing in the design and integration of deep learning models. Felix Stepprath is a Digital Analyst specializing in advanced analytics. All three are members of OTTO's recommendation team.", "REFERENCES": "[1] Himan Abdollahpouri, Gediminas Adomavicius, Robin Burke, Ido Guy, Dietmar Jannach, Toshihiro Kamishima, Jan Krasnodebski, and Luiz Pizzato. 2020. Multistakeholder recommendation: Survey and research directions. User Modeling and User-Adapted Interaction 30, 1 (March 2020), 127-158. https://doi.org/10.1007/ s11257-019-09256-1 RecSys '24, October 14-18, 2024, Bari, Italy Timo Wilm, Philipp Normann, and Felix Stepprath [2] David Ben-Shimon, Alexander Tsikinovsky, Michael Friedmann, Bracha Shapira, Lior Rokach, and Johannes Hoerle. 2015. RecSys Challenge 2015 and the YOOCHOOSE Dataset. In Proceedings of the 9th ACM Conference on Recommender Systems . ACM, Vienna Austria, 357-358. https://doi.org/10.1145/2792838.2798723 [3] Gabriel De Souza Pereira Moreira, Sara Rabhi, Jeong Min Lee, Ronay Ak, and Even Oldridge. 2021. Transformers4Rec: Bridging the Gap between NLP and Sequential / Session-Based Recommendation. In Fifteenth ACM Conference on Recommender Systems . ACM, Amsterdam Netherlands, 143-153. https://doi.org/ 10.1145/3460231.3474255 [4] Kalyanmoy Deb, Kaisa Miettinen, and Shamik Chaudhuri. 2010. Toward an Estimation of Nadir Objective Vector Using a Hybrid of Evolutionary and Local Search Approaches. IEEE Transactions on Evolutionary Computation 14, 6 (Dec. 2010), 821-841. https://doi.org/10.1109/TEVC.2010.2041667 [5] DIGINETICA. 2016. CIKM Cup 2016 Track 2: Personalized E-Commerce Search Challenge. https://competitions.codalab.org/competitions/11161 [6] Alexey Dosovitskiy and Josip Djolonga. 2020. You Only Train Once: LossConditional Training of Deep Networks. In International Conference on Learning Representations . https://api.semanticscholar.org/CorpusID:214278158 [7] Yingqiang Ge, Xiaoting Zhao, Lucia Yu, Saurabh Paul, Diane Hu, Chu-Cheng Hsieh, and Yongfeng Zhang. 2022. Toward Pareto Efficient Fairness-Utility Trade-off in Recommendation through Reinforcement Learning. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining (WSDM'22) . Association for Computing Machinery, New York, NY, USA, 316-324. https://doi.org/10.1145/3488560.3498487 [8] Andreia P. Guerreiro, Carlos M. Fonseca, and Lu\u00eds Paquete. 2022. The Hypervolume Indicator: Computational Problems and Algorithms. Comput. Surveys 54, 6 (July 2022), 1-42. https://doi.org/10.1145/3453474 [9] Bal\u00e1zs Hidasi and Alexandros Karatzoglou. 2018. Recurrent Neural Networks with Top-k Gains for Session-based Recommendations. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management . 843-852. https://doi.org/10.1145/3269206.3271761 [10] Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2016. Session-based Recommendations with Recurrent Neural Networks. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings , Yoshua Bengio and Yann LeCun (Eds.). http://arxiv.org/abs/1511.06939 [11] Long P. Hoang, Dung D. Le, Tran Anh Tuan, and Tran Ngoc Thang. 2023. Improving Pareto Front Learning via Multi-Sample Hypernetworks. Proceedings of the AAAI Conference on Artificial Intelligence 37, 7 (June 2023), 7875-7883. https://doi.org/10.1609/aaai.v37i7.25953 [12] Jipeng Jin, Zhaoxiang Zhang, Zhiheng Li, Xiaofeng Gao, Xiongwen Yang, Lei Xiao, and Jie Jiang. 2023. Pareto-based Multi-Objective Recommender System with Forgetting Curve. https://doi.org/10.48550/ARXIV.2312.16868 Version Number: 2. [13] W.KangandJ. McAuley. 2018. Self-Attentive Sequential Recommendation. In 2018 IEEE International Conference on Data Mining (ICDM) . IEEE Computer Society, Los Alamitos, CA, USA, 197-206. https://doi.org/10.1109/ICDM.2018.00035 [14] Walid Krichene and Steffen Rendle. 2020. On Sampled Metrics for Item Recommendation. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . ACM, Virtual Event CA USA, 1748-1757. https://doi.org/10.1145/3394486.3403226 [15] Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Tao Lian, and Jun Ma. 2017. Neural Attentive Session-based Recommendation. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management . ACM, Singapore"}
