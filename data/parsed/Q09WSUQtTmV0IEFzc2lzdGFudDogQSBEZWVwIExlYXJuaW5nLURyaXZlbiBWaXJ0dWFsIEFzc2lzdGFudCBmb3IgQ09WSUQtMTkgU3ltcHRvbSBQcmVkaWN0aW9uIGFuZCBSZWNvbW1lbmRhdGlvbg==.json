{"title": "COVID-Net Assistant: A Deep Learning-Driven Virtual Assistant for COVID-19 Symptom Prediction and Recommendation", "authors": "Pengyuan Shi; Yuetong Wang; \u2020 Saad; Alexander Wong; David R Cheriton", "pub_date": "2022-11-22", "abstract": "As the COVID-19 pandemic continues to put a significant burden on healthcare systems worldwide, there has been growing interest in finding inexpensive symptom pre-screening and recommendation methods to assist in efficiently using available medical resources such as PCR tests. In this study, we introduce the design of COVID-Net Assistant, an efficient virtual assistant designed to provide symptom prediction and recommendations for COVID-19 by analyzing users' cough recordings through deep convolutional neural networks. We explore a variety of highly customized, lightweight convolutional neural network architectures generated via machine-driven design exploration (which we refer to as COVID-Net Assistant neural networks) on the Covid19-Cough benchmark dataset. The Covid19-Cough dataset comprises 682 cough recordings from a COVID-19 positive cohort and 642 from a COVID-19 negative cohort. Among the 682 cough recordings labeled positive, 382 recordings were verified by PCR test. Our experimental results show promising, with the COVID-Net Assistant neural networks demonstrating robust predictive performance, achieving AUC scores of over 0.93, with the best score over 0.95 while being fast and efficient in inference. The COVID-Net Assistant models are made available in an open source manner through the COVID-Net open initiative and, while not a production-ready solution, we hope their availability acts as a good resource for clinical scientists, machine learning researchers, as well as citizen scientists to develop innovative solutions.", "sections": [{"heading": "Introduction", "text": "The coronavirus 2019 (COVID-19) pandemic, caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), continues to impact human society worldwide significantly. Real-time reverse transcription polymerase chain reaction (RT-PCR) testing remains the most reliable standard screening tool for detecting COVID-19. While RT-PCR testing is the primary screening tool against COVID-19, it typically requires two to three days to complete and significantly burdens the healthcare system. Another widely used screening test is the antigen test. They are rapid tests that produce results in 15-30 minutes. While medical screening tests are becoming more accessible and widespread, many countries do not have widespread access, especially in some developing countries.\nEarlier works show deep learning is successful on particular COVID-19 classification tasks [1,2] with deep convolutional neural networks (CNNs). Deep learning algorithms involve using a dataset to train a tailored parameterized model to help detect symptoms and scientific evidence. The success of deep learning led researchers to explore new diagnosis methods via visual information, such as Figure 1: Overview of COVID-Net Assistant Workflow chest X-rays and CT images. However, a successful deep learning algorithm usually leverages a large dataset with high-quality labels, which is expensive in medical studies. Nevertheless, such visual information is challenging to retrieve in a short time. A more affordable and efficient pre-screening method is beneficial to the use of medical resources.\nCoughing is one of the primary symptoms of COVID-19 and is also the consequence of other diseases. Due to the difference in infection sources, cough sounds may differ in patterns hardly detectable by humans. With signal processing techniques, imperceptible patterns can be learned and detected by deep learning techniques. One of the signal processing techniques is Mel-frequency cepstral coefficients (MFCCs), which work well with CNNs.\nMotivated by providing an affordable and accessible tool to provide early recommendations and leverage the use of diagnostic tests in the fight against the COVID-19 pandemic, in this work, we present a design of a virtual assistant -COVID-Net Assistant. It can sit on a mobile device or website to provide early COVID-19 recommendations based on cough recordings due to the feasibility of retrieval. Figure 1 shows the workflow of COVID-Net Assistant: 1) An individual records or uploads cough audio through the application; 2) The audio gets processed by an efficient deep convolutions neural network to predict whether the individual may have signs of COVID-19; 3) If the network predicts signs of COVID-19 based on the audio, it will give the user a recommendation that they may have signs and seek medical advice or further testing using COVID-19 tests such as PCR and antigen tests.\nWe hope the open-source nature of the COVID-Net Assistantfoot_0 encourages further innovation as part of the COVID-Net global open initiative 2", "publication_ref": ["b0", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Data", "text": "We found three public cough datasets that frequently used in relevant research: the COUGHVID [3] dataset, Coswara [4] dataset, and Covid19-Cough [5] dataset. The positive samples in COUGHVID and Coswara datasets are all self-reported, which does not guarantee the correctness of labels. In our study, we require strong labels to indicate the status of COVID-19 infection; therefore, we chose Covid19-Cough as the main dataset to move forward. This Covid19-Cough dataset consists of 1324 raw audio samples with 682 cases labeled positive, and 382 of them are confirmed by PCR test. For the audio pieces labeled negative, no verification information is provided. The audio was collected through a call center or telegram. We excluded two malformed audio files in our study, leaving us with 1322 pieces of audio, with 681 labeled positive and 381 positive samples verified.\nIn our study, we trained deep learning models with two dataset splits. The first split, namely Verified-Only, excludes all unverified positive samples as we aimed to experiment with high-quality labels. The other split, namely All-Data, contains a total of 1322 valid samples from the Covid19-Cough dataset. We used 60%, 20%, and 20% of the audio for each dataset split as the training, validation, and testing data, respectively. Table 1 shows the exact label distribution in each dataset split. 3 Methodology", "publication_ref": ["b2", "b3", "b4"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Data Processing", "text": "To leverage the power of deep convolutional neural networks (CNNs), we processed the raw audio using the librosa [6] library to extract Mel-frequency cepstral coefficients (MFCC) features. MFCC is widely used in audio processing and is used in related work [7,8]. The complete data processing pipeline comprises the following steps:\n1. Load raw audio using librosa [6].\n2. Apply data augmentation to raw audio.\n3. Extract MFCC features from augmented audio.\nThe raw audio files are loaded with a sample rate of 48000 Hz. We applied data augmentation to alleviate the scarcity of data and the over-fitting issue, as we will address later. We found that data augmentation helps to scale the data and improve model performance. We used audiomentations 3 , another python library, to apply the augmentations. For every piece of training audio, another five augmented pieces are generated with random trimming, shifting, Gaussian noises, and pitch shifting.\nAfterward, MFCC features are extracted using librosa [6]. By using MFCC, we hope it can reveal strong signals in the data and filter out noisy signals. To extract the MFCC features from audio, the time domain input is first mapped to the frequency domain by taking the discrete Fourier transform. Then a mel-scaling, defined by Eq. 1, is applied to the frequency domain data to map the frequencies to conceptually equally distanced pitches as perceived by humans.\nm = 2595 * log(1 + f 100 )(1)\nFinally, a discrete cosine transform is applied to the log of the previous result to obtain the coefficients.\nIn our work, we used 32 MFCC coefficients, 32 mel bands, and an FFT window size of 2048. The MFCC feature extraction will transform an audio into an image-like data with shape 32 \u00d7 328 \u00d7 1 to be used as the input to deep convolutional neural networks. Compared to related works [7], we used more MFCC coefficients for two reasons. First, we believe strong signals in cough sounds might be hidden in information imperceptible by humans, encoded by higher MFCC coefficients. Second, we believe that by using generative synthesis [9], we will be able to train and optimize the models that are aware of which part of the data is essential.", "publication_ref": ["b5", "b6", "b7", "b5", "b5", "b6", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Machine-driven Design Exploration", "text": "Inspired from earlier works of COVID-Nets [1,2], the final architecture designs of deep neural networks in COVID-Net Assistant were discovered automatically via a machine-driven design exploration process using generative synthesis [9]. The architecture exploration process identified the optimal macroarchitecture and microarchitecture designs of the tailored model architecture. With userdefined performance metrics, dataset, and seed architecture, the optimal architecture is determined via an iterative constrained optimization process based on a universal performance function (e.g., [10]) and a set of quantitative constraints. The architectures discovered via generative synthesis are highly customized designs that compromise complexity and representational power, that is outperforming manual design with greater flexibility and granularity [1].", "publication_ref": ["b0", "b1", "b8", "b9", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Model Architectures", "text": "To leverage the power of generative synthesis [9], we built the seed architectures by experimenting with three fundamental convolution-based cells, which are prevalent in building light-weight computer vision models: 1) COVID-Net Assistant CNN built with standard convolutions; 2) COVID-Net Assistant Res-CNN build with residual blocks [11]; 3) COVID-Net Assistant DW-CNN build with depth-wise separable convolutions [12]. In particular, residual blocks bring benefits in building deep networks, as it uses identity mapping to tackle the vanishing gradient problem [13]. Depth-wise separable convolutions increase the representation power of a model while reducing the number of parameters and computation [14].\nDue to the limited training data, we found that the originally proposed ResNet [11] and Mo-bileNetV1 [12] architectures can easily over-fit the training set, and result in a poor performance in the test set. To tackle the over-fitting issue, we built models with dropout layers and fewer parameters. In our experiments, we built multiple seed designs with the aforementioned fundamental cells, as shown in Table 2. For each model architecture built with residual blocks (Res) and depth-wise separable convolutions (DW), we built three variants (S, M, L) with increasing complexity. The complexity was mainly raised via larger filter size, more layers, and the value of strides in stage 1. After Stage 1, we added spatial dropout layers after convolutional layers with a stride of 2. Each feature extractor variant in Table 2 is followed by an average pooling layer and a fully connected classification head with a Sigmoid output activation function. ", "publication_ref": ["b8", "b10", "b11", "b12", "b13", "b10", "b11"], "figure_ref": [], "table_ref": ["tab_1", "tab_1"]}, {"heading": "Training Policy", "text": "All seed designs of proposed COVID-Net Assistant neural networks were trained using the Adam optimizer with a binary cross entropy loss function. In our training configuration, the learning rate would be decayed by a factor of 0.75 if the validation loss has not improved for two epochs, with an initial value of 0.0002 (ReduceLROnPlateau). The training would be terminated early if validation loss stopped improving for 10 epochs with 150 maximum training epochs (EarlyStopping). We constructed, trained, and evaluated the models using TensorFlow Keras. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "We set up experiments to compare the representation power and efficiency of COVID-Net Assistant models generated via generative synthesis [9], based on different seed designs introduced in section 3.3. Specifically, we evaluated the Area under the Receiver Operating Characteristic Curve (AUC) as the primary performance metric. To express the theoretical capacity of models, we also calculated the total number of parameters and floating point operations (FLOPs) of the final architectures of COVID-Net Assistant models.\nTable 3 shows the performance and capacity of each proposed COVID-Net Assistant deep neural network generated via generative synthesis differentiated by seed designs. From Table 3, we noticed that COVID-Net Assistant CNN has dominant performance on the Verified-Only split, with the second smallest model in terms of the number of parameters. In addition, there is a slight increase in AUC by increasing the complexity of a residual block architected seed design (Res-CNN). In contrast, increasing the complexity of a depth-wise separable convolution architected seed design (DW-CNN) will cause a slight drop in the AUC score. The variants of COVID-Net Assistant DW-CNN have relatively small capacities, especially for models generated with seed design DW-CNN-S/M, whose numbers of FLOPs are less than 1/8 of other models. We also noticed a difference in the capacity levels of final architectures while the models were trained on different dataset splits with the same seed design. For example, the generated COVID-Net Res-CNN-S trained on All-Data Split has FLOPs fewer than half of the one trained on Verified-Only Split, while the number of parameters is only 3% fewer. From our investigation, the difference is mainly in the number of filters in the first two stages described in Table 2, which are the most computationally expensive stages. We also noticed that a more complex seed design could result in a model with fewer FLOPs via generative synthesis. For instance, the final architecture of COVID-Net Assistant Res-CNN-M has 7% fewer FLOPs than COVID-Net Assistant Res-CNN-S, trained on the Verified-Only split, while Res-CNN-S has much fewer parameters.\nWe noticed a considerable performance difference (over 0.1 AUC) on the test sets between the model trained on All-Data split and Verified-Only split. We hypothesized that the root cause is the more extensive noise of the unverified positive samples in All-Data split. To verify our hypothesis, we tested the models on the same test set in All-Data split but filtered -the unverified positive samples  4 shows the AUC scores on the filtered test set. As a result, the scores are close to the ones evaluated on Verfied-Only split, which is more than 10 points higher than the scores on the unfiltered test set, referred to Table 3. The observation implies that the unverified positive samples are noisy and may negatively impact the models trained via standard supervised learning.\nMotivated by deploying the models on different platforms, such as mobile and web applications, we evaluated the model's forward pass latency on different platforms, reported in Table 5. Particularly, we computed the trimmed mean of the latency in 1000 forward propagation on 1) a workstation (WORKSTN) with x86_64 architecture and Intel Xeon Gold 6230 CPU, 80 cores, 3.9 GHz, 400GB RAM; 2) a single-board computer (SBC) with aarch64 architecture and Cortex-A72 CPU, 4 cores, 2.0 GHz, 4GB RAM. According to Table 5, a model's latency on the WORKSTN is not necessarily proportional to the latency on the SBC. For instance, the COVID-Net Assistant CNN trained on Verified-Only split can perform the fastest inference on the WORKSTN, but on the SBC, the latency of COVID-Net Assistant DW-CNN-S is almost half of its. The models infer fastest on the SBC are COVID-Net Assistant DW-CNN-S/M, whose FLOPs are strictly fewer than other models. However, for lightweight models, The impact of fewer Flops on a workstation with abundant computational resources is not significant because of the large amount of parallel computation that can be performed in a deep convolutional neural network.", "publication_ref": ["b8"], "figure_ref": [], "table_ref": ["tab_2", "tab_2", "tab_1", "tab_3", "tab_2", "tab_4", "tab_4"]}, {"heading": "Conclusion and Future Work", "text": "In this study, we proposed the initiative of COVID-Net Assistant, a deep learning-driven system that performs pre-screening of COVID-19 conditions by processing cough audio. The goal of COVID-Net Assistant is not to make a diagnosis but to provide an early recommendation to users on whether they may have signs of COVID-19 and to seek medical advice or further COVID-19 tests. The study is purely research and should not be leveraged for medical advice or diagnosis. However, With effective pre-screening techniques and recommendation systems, limited medical sources like RT-PCR tests will gain more usability in the fight against the COVID-19 pandemic.\nOur results show that deep learning algorithms can do well on a dataset with verified positive samples. We proposed multiple deep convolutional neural network architectures generated via generative synthesis [9] with different seed architecture designs. We found the standard convolution based architecture, namely COVID-Net Assistant CNN, can achieve dominant representation power on the dataset with less noise; the lightweight depth-wise separable convolution based architecture, namely COVID-Net Assistant DW-CNN-S, can achieve the highest efficiency on low-end devices such as a single-board computer; and the residual block based architecture, namely COVID-Net Assistant Res-CNN-M, can achieve a great balance between representation power and efficiency, while the neural networks are not deep enough to fully leverage the power of residual connections.\nOne of the challenges is that the audio samples in the dataset are scarce. Due to the limited numbers of test data, the AUC scores with a slight difference are hardly comparable. More training data is necessary to generalize the model's target missions and deploy them in practice. In fact, deep learning algorithms have shown promising results in many diagnostic tasks with a massive amount of labeled training data. However, high-quality labeled data in medical diagnosis is expensive to obtain and is always a challenge in related literature. Fortunately, recent studies have used semi-supervised learning approaches to leverage the unlabeled data or weak labels (e.g., [15]). As one of our future research directions, we will leverage transfer learning and semi-supervised learning algorithms to train the models on larger, crowdsourced datasets and leverage the unverified labels, such as combining the samples in COUGHVID [3] dataset and Coswara [4] dataset with Covid19-Cough dataset [5] currently in use.\nThe promising AUC scores on the Verified-Only dataset split also imply a potential pattern in coughing sounds by COVID-19 infection differentiated from other diseases. Hence, another research direction is leveraging explainable AI techniques to study the region of interest used in neural networks' prediction. This could further help us study and understand the performance of generative synthesis. We hope our research can motivate further study and applications or even research in biological reasoning.", "publication_ref": ["b8", "b14", "b2", "b3", "b4"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Covid-net: a tailored deep convolutional neural network design for detection of covid-19 cases from chest x-ray images", "journal": "Scientific Reports", "year": "2020-11", "authors": "L Wang; Z Q Lin; A Wong"}, {"ref_id": "b1", "title": "Covidnet-ct: A tailored deep convolutional neural network design for detection of covid-19 cases from chest ct images", "journal": "Frontiers in Medicine", "year": "2020", "authors": "H Gunraj; L Wang; A Wong"}, {"ref_id": "b2", "title": "The coughvid crowdsourcing dataset, a corpus for the study of large-scale cough analysis algorithms", "journal": "Scientific Data", "year": "2021-06", "authors": "L Orlandic; T Teijeiro; D Atienza"}, {"ref_id": "b3", "title": "Coswara -a database of breathing, cough, and voice sounds for covid-19 diagnosis", "journal": "", "year": "2020", "authors": "N Sharma; P Krishnan; R Kumar; S Ramoji; S Chetupalli; N R ; P Ghosh; S Ganapathy"}, {"ref_id": "b4", "title": "Dataset of recordings of induced cough", "journal": "", "year": "2021", "authors": "P \" Fkthecovid"}, {"ref_id": "b5", "title": "librosa: Audio and music signal analysis in python", "journal": "", "year": "2015", "authors": "B Mcfee; C Raffel; D Liang; D P W Ellis; M Mcvicar; E Battenberg; O Nieto"}, {"ref_id": "b6", "title": "Covid-19 diagnosis from cough acoustics using convnets and data augmentation", "journal": "", "year": "2021", "authors": "S K Mahanta; D Kaushik; H Van Truong; S Jain; K Guha"}, {"ref_id": "b7", "title": "Cough classification for covid-19 based on audio mfcc features using convolutional neural networks", "journal": "", "year": "2020", "authors": "V Bansal; G Pahwa; N Kannan"}, {"ref_id": "b8", "title": "Gensynth: A generative synthesis approach to learning generative machines to generate efficient neural networks", "journal": "Electronics Letters", "year": "2019", "authors": "A Wong; M J Shafiee; B Chwyl; F Li"}, {"ref_id": "b9", "title": "Netscore: Towards universal metrics for large-scale performance analysis of deep neural networks for practical usage", "journal": "", "year": "2018", "authors": "A Wong"}, {"ref_id": "b10", "title": "Deep residual learning for image recognition", "journal": "", "year": "2016", "authors": "K He; X Zhang; S Ren; J Sun"}, {"ref_id": "b11", "title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications", "journal": "", "year": "2017", "authors": "A Howard; M Zhu; B Chen; D Kalenichenko; W Wang; T Weyand; M Andreetto; H Adam"}, {"ref_id": "b12", "title": "Residual networks behave like ensembles of relatively shallow networks", "journal": "Curran Associates Inc", "year": "2016", "authors": "A Veit; M Wilber; S Belongie"}, {"ref_id": "b13", "title": "Depthwise separable convolutions for neural machine translation", "journal": "", "year": "2018", "authors": "L Kaiser; A N Gomez; F Chollet"}, {"ref_id": "b14", "title": "A semi-supervised learning approach for COVID-19 detection chest CT scans", "journal": "Neurocomputing", "year": "2022-06", "authors": "Y Zhang; L Su; Z Liu; W Tan; Y Jiang; C Cheng"}], "figures": [{"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Dataset Split and Label Distribution.", "figure_data": "Dataset SplitSub-splitPositive NegativeAll-DataTrain424369Validation 138126Test119146Verified-Only Train238375Validation 73131Test70135"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Seed Designs of COVID-Net Assistant Models, Annotation Follows: \"filter size, number of filters, stride\"", "figure_data": "Seed DesignStage 1Stage 2Stage 3Stage 4Conv 3x3, 32Conv 3x3, 64CNNConv 3x3, 32Conv 3x3, 64Batch Normalization-Maxpool 2x2Maxpool 2x2Res-CNN-S Conv 3x3, 64, s2Res 3x3, 64, s2, d/o Res 3x3, 64--Res-CNN-M Conv 3x3, 64, s2Res 3x3, 64, s2, d/o Res 3x3, 64Res 3x3, 64, s2, d/o Res 3x3, 64-Res-CNN-L Conv 7x7, 64, s2Res 5x5, 64, s2, d/o Res 5x5, 64Res 3x3, 64, s2, d/o Res 3x3, 64Res 3x3, 64, s2, d/o Res 3x3, 64DW-CNN-S Conv 3x3, 32, s2DW 3x3, 64, s2, d/o DW 3x3, 64DW 3x3, 128-DW-CNN-MConv 3x3, 32DW 3x3, 64, s2, d/o DW 3x3, 128, s2, d/o DW 3x3, 256, s2, d/o DW 3x3, 64 DW 3x3, 128 DW 3x3, 256DW-CNN-LConv 9x9, 32DW 9x9, 64, s2, d/o DW 7x7, 128, s2, d/o DW 3x3, 256, s2, d/o DW 9x9, 64 DW 7x7, 128 DW 3x3, 256"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Theoretical Evaluation Results of COVID-Net Assistant Models generated via Generative Synthesis based on Seed Designs, Format Follows \"All-Data Split / Verified-Only Split\", best result in bold", "figure_data": "Seed DesignAUCParams (K)FLOPs (M)CNN0.7815 / 0.95086.6 / 4.663.3 / 45.8Res-CNN-S 0.7828 / 0.934974.3 / 76.325.2 / 59.7Res-CNN-M 0.7887 / 0.9394 161.4 / 228.8 39.7 / 55.6Res-CNN-L 0.8039 / 0.9388 254.8 / 229.0176 / 66.7DW-CNN-S 0.7860 / 0.93303.5 / 3.93.9 / 4.5DW-CNN-M 0.7832 / 0.930425.2 / 23.76.2 / 6.5DW-CNN-L 0.7692 / 0.930534.1 / 35.271.0 / 71.1"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "AUC Score of COVID-Net Assistant Models generated via Generative Synthesis based on Seed Designs Trained Split on the Filtered Test Set, best result in bold", "figure_data": "Seed DesignAUCCNN0.9411Res-CNN-S 0.9189Res-CNN-M 0.9453Res-CNN-L 0.9353DW-CNN-S 0.9439DW-CNN-M 0.9015DW-CNN-L 0.9362"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Latency Evaluation Results of COVID-Net Assistant Models generated via Generative Synthesis based on Seed Designs, Format Follows \"All-Data Split / Verified-Only Split\", best result in bold", "figure_data": "Seed Design Latency WORKSTN (ms) Latency SBC (ms)CNN2.4 / 1.7210.7 / 9.44Res-CNN-S2.29 / 2.228.76 / 10.6Res-CNN-M2.64 / 2.6111.2 / 12.1Res-CNN-L3.24 / 3.0521.8 / 13.6DW-CNN-S2.34 / 2.314.94 / 5.39DW-CNN-M3.06 / 3.186.74 / 6.98DW-CNN-L14.0 / 14.135.1 / 35.2were rejected, left with 63 verified positive samples and 146 negative samples in the filtered test set.Table"}], "formulas": [{"formula_id": "formula_0", "formula_text": "m = 2595 * log(1 + f 100 )(1)", "formula_coordinates": [3.0, 252.76, 507.26, 251.24, 22.31]}], "doi": "10.1038/s41598-020-76550-z"}
