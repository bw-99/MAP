{"TransAct: Transformer-based Realtime User Action Model for Recommendation at Pinterest": "Xue Xia xxia@pinterest.com Pinterest San Francisco, CA, USA Nikil Pancha npancha@pinterest.com Pinterest San Francisco, CA, USA Dhruvil Deven Badani dbadani@pinterest.com Pinterest San Francisco, CA, USA Saurabh Vishwas Joshi sjoshi@pinterest.com Pinterest San Francisco, CA, USA Pong Eksombatchai pong@pinterest.com Pinterest San Francisco, CA, USA Po-Wei Wang poweiwang@pinterest.com Pinterest San Francisco, CA, USA Nazanin Farahpour nfarahpour@pinterest.com Pinterest San Francisco, CA, USA", "Andrew Zhai \u2217": "andrew@aideate.ai Pinterest San Francisco, CA, USA", "ABSTRACT": "Sequential models that encode user activity for next action prediction have become a popular design choice for building web-scale personalized recommendation systems. Traditional methods of sequential recommendation either utilize end-to-end learning on realtime user actions, or learn user representations separately in an offline batch-generated manner. This paper (1) presents Pinterest's ranking architecture for Homefeed, our personalized recommendation product and the largest engagement surface; (2) proposes TransAct, a sequential model that extracts users' short-term preferences from their realtime activities; (3) describes our hybrid approach to ranking, which combines end-to-end sequential modeling via TransAct with batch-generated user embeddings. The hybrid approach allows us to combine the advantages of responsiveness from learning directly on realtime user activity with the cost-effectiveness of batch user representations learned over a longer time period. We describe the results of ablation studies, the challenges we faced during productionization, and the outcome of an online A/B experiment, which validates the effectiveness of our hybrid ranking model. We further demonstrate the effectiveness of TransAct on other surfaces such as contextual recommendations and search. KDD '23, August 6-10, 2023, Long Beach, CA, USA https://doi.org/10.1145/3580305.3599918 Our model has been deployed to production in Homefeed, Related Pins, Notifications, and Search at Pinterest.", "CCS CONCEPTS": "\u00b7 Information systems \u2192 Web searching and information discovery ; Content ranking ; Personalization ;", "KEYWORDS": "Personalization, Recommender Systems, Sequential Recommendation, User Interest Modeling", "ACMReference Format:": "Xue Xia, Pong Eksombatchai, Nikil Pancha, Dhruvil Deven Badani, PoWei Wang, Neng Gu, Saurabh Vishwas Joshi, Nazanin Farahpour, Zhiyuan Zhang, and Andrew Zhai. 2023. TransAct: Transformer-based Realtime User Action Model for Recommendation at Pinterest. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '23), August 6-10, 2023, Long Beach, CA, USA. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3580305.3599918", "1 INTRODUCTION": "The proliferation of online content in recent years has created an overwhelming amount of information for users to navigate. To address this issue, recommender systems are employed in various industries to help users find relevant items from a vast selection, including products, images, videos, and music. By providing personalized recommendations, businesses and organizations can better serve their users and keep them engaged with the platform. Therefore, recommender systems are vital for businesses as they drive growth by boosting engagement, sales, and revenue. As one of the largest content sharing and social media platforms, Pinterest hosts billions of pins with rich contextual and visual information, and brings inspiration to over 400 million users. Neng Gu ngu@pinterest.com Pinterest San Francisco, CA, USA Zhiyuan Zhang zhiyuan@pinterest.com Pinterest San Francisco, CA, USA n Upon visiting Pinterest, users are immediately presented with the Homefeed page as shown in Figure 1, which serves as the primary source of inspiration and accounts for the majority of overall user engagement on the platform. The Homefeed page is powered by a 3-stage recommender system that retrieves, ranks, and blends content based on user interests and activities. At the retrieval stage, we filter billions of pins created on Pinterest to thousands, based on a variety of factors such as user interests, followed boards, etc. Then we use a pointwise ranking model to rank candidate pins by predicting their personalized relevance to users. Finally, the ranked result is adjusted using a blending layer to meet business requirements. Realtime recommendation is crucial because it provides a quick and up-to-date recommendation to users, improving their overall experience and satisfaction. The integration of realtime data, such as recent user actions, results in more accurate recommendations and increases the probability of users discovering relevant items [4, 21]. Longer user action sequences result in improved user representation and hence better recommendation performance. However, using long sequences in ranking poses challenges to infrastructure, as they require significant computational resources and can result in increased latency. To address this challenge, some approaches have utilized hashing and nearest neighbor search in long user sequences [21]. Other work encodes users' past actions over an extended time frame to a user embedding [20] to represent longterm user interests. User embedding features are often generated as batch features (e.g. generated daily), which are cost-effective to serve across multiple applications with low latency. The limitation of existing sequential recommendation is that they either only use realtime user actions, or only use a batch user representation learned from long-term user action history. We introduce a novel realtime-batch hybrid ranking approach that combines both realtime user action signals and batch user representations. To capture the realtime actions of users, we present TransAct - a new transformer-based module designed to encode recent user action sequences and comprehend users' immediate preferences. For user actions that occur over an extended period of time, we transform them into a batch user representation [20]. By combining the expressive power of TransAct with batch user embeddings, the hybrid ranking model offers users realtime feedback on their recent actions, while also accounting for their long-term interests. The realtime component and batch component complement each other for recommendation accuracy. This leads to an overall improvement in the user experience on the Homefeed page. The major contributions of this paper are summarized as follows: \u00b7 Wedescribe Pinnability, the architecture of Pinterest's Homefeed production ranking system. The Homefeed personalized recommendation product accounts for the majority of the overall user engagement on Pinterest. \u00b7 We propose TransAct, a transformer-based realtime user action sequential model that effectively captures users' shortterm interests from their recent actions. We demonstrate that combining TransAct with daily-generated user representations [20] to a hybrid model leads to the best performance in Pinnability. This design choice is justified through a comprehensive ablation study. Our code implementation is publicly available 1 . \u00b7 Wedescribe the serving optimization implemented in Pinnability to make feasible the computational complexity increase of 65 times when introducing TransAct to the Pinnability model. Specifically, optimizations are done to enable GPU serving of our prior CPU-based model. \u00b7 We describe online A/B experiments on a real-world recommendation system using TransAct. We demonstrate some practical issues in the online environment, such as recommendation diversity drop and engagement decay, and propose solutions to address these issues. The remainder of this paper is organized as follows: Related work is reviewed in Section 2. Section 3 describes the design of TransAct and the details of bringing it to production. Experiment results are reported in Section 4. We discuss some findings beyond experiments in Section 5. Finally, we conclude our work in Section 6.", "2 RELATED WORK": "", "2.1 Recommender System": "Collaborative filtering (CF) [12, 18, 24] makes recommendations based on the assumption that a user will prefer an item that other similar users prefer. It uses the user behavior history to compute the similarity between users and items and recommend items based on similarity. This approach suffers from the sparsity of the user-item matrix and cannot handle users who have never interacted with any items. Factorization machines [22, 23], on the other hand, are able to handle sparse matrices. More recently, deep learning (DL) has been used in click-through rate (CTR) prediction tasks. For example, Google uses Wide & Deep [5] models for application recommendation. The wide component achieves memorization by capturing the interaction between features, while the deep component helps with generalization by learning the embedding of categorical features using a feed forward network. DeepFM [7] makes improvements by learning both low-order and high-order feature interactions automatically. DCN [34] and its upgraded version DCN v2 [35] both aim to automatically model the explicit feature crosses. The aforementioned recommender systems do not work well in capturing the shortterm interests of users since only the static features of users are utilized. These methods also tend to ignore the sequential relationship within the action history of a user, resulting in an inadequate representation of user preferences.", "2.2 Sequential Recommendation": "To address this problem, sequential recommendation has been widely studied in both academia and the industry. A sequential recommendation system uses a behavior history of users as input and applies recommendation algorithms to suggest appropriate items to users. Sequential recommendation models are able to capture users' long-term preferences over an extended period of time, similar to traditional recommendation methods. Additionally, they also have the added benefit of being able to account for users' evolving interests, which enables higher quality recommendations. Sequential recommendation is often viewed as a next item prediction task, where the goal is to predict a user's next action based on their past action sequence. We are inspired by the previous sequential recommendation method [4] in terms of encoding users' past action into a dense representation. Some early sequential recommendation systems use machine learning techniques, such as Markov Chain [8] and session-based K nearest neighbors (KNN) [11] to model the temporal dependencies among interactions in users' action history. These models are criticized for not being able to fully capture the long-term patterns of users by simply combining information from different sessions. Recently, deep learning techniques such as recurrent neural networks (RNN) [25] have shown great success in natural language processing and have become increasingly popular in sequential recommendation. As a result, many DL-based sequential models [6, 9, 30, 42] have achieved outstanding performance using RNNs. Convolutional neural networks (CNNs) [40] are widely used for processing time-series data and image data. In the context of sequential recommendation, CNN-based models can effectively learn dependency within a set of items users recently interacted with, and make recommendations accordingly [31, 32]. Attention mechanism is originated from the neural machine translation task, which models the importance of different parts of the input sentences on the output words [2]. Self-attention is a mechanism known to weigh the importance of different parts of an input sequence [33]. There have been more recommender systems that use attention [43] and self-attention [4, 13, 16, 27, 39]. Many previous works [13, 16, 27] only perform offline evaluations using public datasets. However, the online environment is more challenging and unpredictable. Our method is not directly comparable to these works due to differences in the problem formulation. Our approach resembles a Click-through Rate (CTR) prediction task. Deep Interest Network (DIN) uses an attention mechanism to model the dependency within users' past actions in CTR prediction tasks. Alibaba's Behavior Sequence Transformer (BST) [4] is the improved version of DIN and is closely related to our work. They propose to use Transformer to capture the user interest from user actions, emphasizing the importance of the action order. However, we found that positional information does not add much value. We find other designs like better early fusion and action type embedding are effective when dealing with sequence features.", "3 METHODOLOGY": "In this section, we introduce TransAct, our realtime-batch hybrid ranking model. We will start with an overview of the Pinterest Homefeed ranking model, Pinnability. We then describe how to use TrancAct to encode the realtime user action sequence features in Pinnability for the ranking task.", "3.1 Preliminary: Homefeed Ranking Model": "In Homefeed ranking, we model the recommendation task as a pointwise multi-task prediction problem, which can be defined as follows: given a user \ud835\udc62 and a pin \ud835\udc5d , we build a function to predict the probabilities of user \ud835\udc62 performing different actions on the candidate pin \ud835\udc5d . The set of different actions contains both positive and negative actions, e.g. click, repin 2 and hide. We build Pinnability , Pinterest's Homefeed ranking model, to approach the above problem. The high-level architecture is a Wide and Deep learning (WDL) model [5]. The Pinnability model utilizes various types of input signals, such as user signals, pin signals, and context signals. These inputs can come in different formats, including categorical, numerical, and embedding features. We use embedding layers to project categorical features to dense features, and perform batch normalization on numerical features. We then apply a feature cross using a full-rank DCN V2 [35] to explicitly model feature interactions. At last, we use fully connected layers with a set of output action heads \ud835\udc6f = { \u210e 1 , \u210e 2 , . . . , \u210e \ud835\udc58 } to predict the user actions on the candidate pin \ud835\udc5d . Each head maps to one action. As shown in Figure 2, our model is a realtime-batch hybrid model that encodes the user action history features by both realtime (TransAct) and batch (PinnerFormer) approaches and optimizes for the ranking task [37]. TransAct Embedding Layer DCN V2 MLP Static features context features user features pin features \u2026 action type pin embedding timestamp head 1 head 2 head k Concatenate id features dense features PinnerFormer non-realtime user actions realtime user actions \u2026 \u2026 Each training sample is ( \ud835\udc99 , \ud835\udc9a ) , where \ud835\udc99 represents a set of features, and \ud835\udc9a \u2208 { 0 , 1 } | \ud835\udc6f | . Each entry in \ud835\udc9a corresponds to the label of an action head in \ud835\udc6f . The loss function of Pinnability is a weighted cross-entropy loss, designed to optimize for multi-label classification tasks. We formulate the loss function as: where \ud835\udc53 ( \ud835\udc99 ) \u2208 ( 0 , 1 ) \ud835\udc3b , and \ud835\udc53 ( \ud835\udc99 ) \u210e is the output probability of head \u210e . \ud835\udc66 \u210e \u2208 { 0 , 1 } is the ground truth on head \u210e . Aweight \ud835\udc64 \u210e is applied on the cross entropy of each head's output \ud835\udc53 ( \ud835\udc99 ) \u210e . \ud835\udc64 \u210e is calculated using the ground truth \ud835\udc9a and a label weight matrix \ud835\udc74 \u2208 R | \ud835\udc3b | \u2217| \ud835\udc3b | as follows: The label weight matrix \ud835\udc74 acts as a controlling factor for the contribution of each action to the loss term of each head 3 . Note that if \ud835\udc74 is a diagonal matrix, Eq (1) reduces to a standard multi-head binary cross entropy loss. But selecting empirically determined label weights \ud835\udc74 improves performance considerably. In addition, each training example is weighted by a user-dependent weight \ud835\udc64 \ud835\udc62 , which is determined by user attributes, such as the user state 4 , gender and location. We compute \ud835\udc64 \ud835\udc62 by multiplying user state weight, user gender weight, and user location weight: \ud835\udc64 \ud835\udc62 = \ud835\udc64 state \u00d7 \ud835\udc64 location \u00d7 \ud835\udc64 gender . These weights are adjusted based on specific business needs.", "3.2 Realtime User Action Sequence Features": "User's past action history is naturally a variable length feature - different users have different amounts of past actions on the platform. Although a longer user action sequence usually means more accurate user interest representation, in practice, it is infeasible to include all user actions. Because the time needed to fetch user action features and perform ranking model inference can also grow substantially, which in turn hurts user experience and system efficiency. Considering infrastructure cost and latency requirements, we choose to include each user's most recent 100 actions in the sequence. For users with less than 100 actions, we pad the feature to the length of 100 with 0s. The user action sequence features are sorted by timestamp in descending order, i.e. the first entry being the most recent action. All actions in the user action sequence are pin-level actions. For each action, we use three primary features: the timestamp of the action, action type, and the 32-dimensional PinSage embedding [38] of the pin. PinSage is a compact embedding that encodes a pin's content information.", "3.3 Our Approach: TransAct": "Unlike static features, the realtime user action sequence feature \ud835\udc7a ( \ud835\udc62 ) = [ \ud835\udc4e 1 , \ud835\udc4e 2 , ..., \ud835\udc4e \ud835\udc5b ] is handled using a specialized sub-module called TransAct. TransAct extracts sequential patterns from the user's historical behavior and predicts ( \ud835\udc62, \ud835\udc5d ) relevance scores. 3.3.1 Feature encoding. The relevance of pins that a user has engaged with can be determined by the types of actions taken on them in the user's action history. For example, a pin repinned to a user's board is typically considered more relevant than one that the user only viewed. If a pin is hidden by the user, the relevance should be very low. To incorporate this important information, we use trainable embedding tables to project action types to low-dimensional vectors. The user action type sequence is then projected to a user action embedding matrix \ud835\udc7e \ud835\udc4e\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60 \u2208 R | \ud835\udc46 | \u00d7 \ud835\udc51 \ud835\udc4e\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b , where \ud835\udc51 \ud835\udc4e\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b is the dimension of action type embedding. As mentioned earlier, the content of pins in the user action sequence is represented by PinSage embeddings [38]. Therefore, the content of all pins in the user action sequence is a matrix \ud835\udc7e \ud835\udc5d\ud835\udc56\ud835\udc5b\ud835\udc60 \u2208 R | \ud835\udc46 | \u00d7 \ud835\udc51 \ud835\udc43\ud835\udc56\ud835\udc5b\ud835\udc46\ud835\udc4e\ud835\udc54\ud835\udc52 . The final encoded user action sequence feature is CONCAT ( \ud835\udc7e \ud835\udc4e\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60 , \ud835\udc7e \ud835\udc5d\ud835\udc56\ud835\udc5b\ud835\udc60 ) \u2208 R | \ud835\udc46 | \u00d7 ( \ud835\udc51 \ud835\udc43\ud835\udc56\ud835\udc5b\ud835\udc46\ud835\udc4e\ud835\udc54\ud835\udc52 + \ud835\udc51 \ud835\udc4e\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b ) . 3.3.2 Early fusion. One of the unique advantages of using user action sequence features directly in the ranking model is that we can explicitly model the interactions between the candidate pin and the user's engaged pins. Early fusion in recommendation tasks refers to merging user and item features at an early stage of the recommendation model. Through experiments, we find that early fusion is an important factor to improve ranking performance. Two early fusion methods are evaluated: \u00b7 append : Append candidate pin's PinSage embedding to user action sequence as the last entry of the sequence, similar to BST [4]. Use a zero vector to serve as a dummy action type for candidate pin. \u00b7 concat : For each action in the user action sequence, concatenate the candidate pin's PinSage embedding with user action features. We choose concat as our early fusion method based on the offline experiment results. The resulting sequence feature with early fusion is a 2-d matrix \ud835\udc7c \u2208 R | \ud835\udc46 | \u00d7 \ud835\udc51 , where \ud835\udc51 = ( \ud835\udc51 \ud835\udc4e\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b + 2 \ud835\udc51 \ud835\udc43\ud835\udc56\ud835\udc5b\ud835\udc46\ud835\udc4e\ud835\udc54\ud835\udc52 ) 3.3.3 Sequence Aggregation Model. With the user action sequence feature \ud835\udc7c prepared, the next challenge is to efficiently aggregate all the information in the user action sequence to represent the user's short-term preference. Some popular model architectures for sequential modeling in the industry include CNN[40], RNN [25] and recently transformer [33], etc. We experimented with different sequence aggregation architectures and choose transformer-based architectures. We employed the standard transformer encoder with 2 encoder layers and one head. The hidden dimension of feed forward network is denoted as \ud835\udc51 \u210e\ud835\udc56\ud835\udc51\ud835\udc51\ud835\udc52\ud835\udc5b . Positional encoding is not used here because our offline experiment showed that position information is ineffective 5 . 3.3.4 Random Time Window Mask. Training on all recent actions of a user can lead to a rabbit hole effect, where the model recommends content similar to the user's recent engagements. This hurts the diversity of users' Homefeeds, which is harmful to long-term user retention. To address this issue, we use the timestamps of the user action sequence to build a time window mask for the transformer encoder. This mask filters out certain positions in the input sequence before the self-attention mechanism is applied. In each forward pass, a random time window \ud835\udc47 is sampled uniformly from 0 to 24 hours. All actions taken within ( \ud835\udc61 \ud835\udc5f\ud835\udc52\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc60\ud835\udc61 -\ud835\udc47,\ud835\udc61 \ud835\udc5f\ud835\udc52\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc60\ud835\udc61 ) are masked, where \ud835\udc61 \ud835\udc5f\ud835\udc52\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc60\ud835\udc61 stands for the timestamp of receiving the ranking request. It is important to note that the random time window mask is only applied during training, while at inference time, the mask is not used. 3.3.5 Transformer Output Compression. The output of the transformer encoder is a matrix \ud835\udc76 = ( \ud835\udc90 0 : \ud835\udc90 | \ud835\udc46 | -1 ) \u2208 R | \ud835\udc46 | \u00d7 \ud835\udc51 . We only take the first \ud835\udc3e columns ( \ud835\udc90 0 : \ud835\udc90 \ud835\udc3e -1 ) , concatenated them with the max pooling vector MAXPOOL ( \ud835\udc76 ) \u2208 R \ud835\udc51 , and flattened it to a vector \ud835\udc9b \u2208 R ( \ud835\udc3e + 1 )\u2217 \ud835\udc51 . The first \ud835\udc3e output columns capture users' most recent interests and MAXPOOL ( \ud835\udc76 ) represents users' longer-term preference over \ud835\udc46 ( \ud835\udc62 ) . Since the output is compact enough, it can be easily integrated into the Pinnability framework using the DCN v2 [35] feature crossing layer. action pin embedding action type embedding copy n times transformer encoder layer 1 transformer encoder layer 2 candidate pin embedding max pool ... . . . time window masks concatenate Output cols | S | - k cols k a 1 a 2 a n S ( u ) d", "3.4 Model Productionization": "3.4.1 Model Retraining. Retraining is important for recommender systems because it allows the system to continuously adapt to changing user behavior and preferences over time. Without retraining, a recommender system's performance can degrade as the user's behavior and preferences change, leading to less accurate recommendations [26]. This holds especially true when we use realtime features in ranking. The model is more time sensitive and requires frequent retraining. Otherwise, the model can become stale in a matter of days, leading to less accurate predictions. We retrain Pinnability from scratch twice per week. We find that this retraining frequency is essential to ensure a consistent engagement rate and still maintain a manageable training cost. We will dive into the importance of retraining in Section 4.4.3. < l a t e x i s h 1 _ b 6 4 = \" p r T B + I 5 d M 2 K R S Y V N X y j > A H c D L g E O z G 8 3 Q Z U C n m 7 w u k / o q v W J F 0 9 P f < l a t e x i s h 1 _ b 6 4 = \" w V N P T d n O F 0 H j 3 9 c p z B Y > A D L S g E y r f U o K 2 Q 8 I 5 J m 7 v k G q W Z u R C + X M / < l a t e x i s h 1 _ b 6 4 = \" d / R c S M E v p A n L 8 T o f > B H V D g N O z G r Y 3 Q Z U C y J m X k w I K q 2 j 7 0 W P 5 u + F 9 < l a t e x i s h 1 _ b 6 4 = \" q 9 A I S U u r W M T X P + c 7 z > B n V D L g N E O G H Y 8 d 3 j 0 C y J k Z m R / w o K v 2 p Q f F 5 < l a t e x i s h 1 _ b 6 4 = \" K Q J L k o F X p 3 B u g 8 T w d 7 r 0 > A + n c V C M W Z H I q S 5 U m 2 R 9 G v E f P O j z N y D Y / < l a t e x i s h 1 _ b 6 4 = \" 0 q n S N w p 5 T 8 9 J u 2 B H M I / > A 7 c V E 3 v k y K X F Q L W Z + D z m G j Y O f d g U o R C r P < l a t e x i s h 1 _ b 6 4 = \" + H k Q 2 I m A J D q U j B 5 P N d w C > n c V L S g E O y r f Y 9 o K 8 Z z 7 F v p 3 G T X W / R 0 M u 3.4.2 GPU serving. Pinnability with TransAct is 65 times more computationally complex compared to its predecessors in terms of floating point operations. Without any breakthroughs in model inference, our model serving cost and latency would increase by the same scale. GPU model inference allows us to serve Pinnability with TransAct at neutral latency and cost 6 . The main challenge to serve Pinnability on GPUs is the CUDA kernel launch overhead. The CPU cost of launching operations on the GPU is very high, but it is often overshadowed by the prolonged GPU computation time. However, this is problematic for Pinnability GPU model serving in two ways. First, Pinnability and recommender models in general process hundreds of features, which means that there is a large number of CUDA kernels. Second, the batch size during online serving is small and hence each CUDA kernel requires little computation. With a large number of small CUDA kernels, the launching overhead is much more expensive than the actual computation. We solved the technical challenge through the following optimizations: Fuse CUDAkernels. Aneffective approach is to fuse operations as much as possible. We leverage standard deep learning compilers such as nvFuser 7 but often found human intervention is needed for many of the remaining operations. One example is our embedding table lookup module, which consists of two computation steps: raw id to table index lookup and table index to embedding lookup. This is repeated hundreds of times due to the large number of features. We significantly reduce the number of operations by leveraging cuCollections 8 to support hash tables for the raw ids on GPUs and implementing a custom consolidated embedding lookup module to merge the lookup for multiple features into one lookup. As a result, we reduced hundreds of operations related to sparse features into one. Combine memory copies. For every inference, hundreds of features are copied from the CPU to the GPU memory as individual tensors. The overhead of scheduling hundreds of tensor copies becomes the bottleneck. To decrease the number of tensor copy operations, we combine multiple tensors into one continuous buffer before transferring them from CPU to GPU. This approach reduces the scheduling overhead of transferring hundreds of tensors individually to transferring one tensor. Form larger batches. For CPU-based inference, smaller batches are preferred to increase parallelism and reduce latency. However, for GPU-based inference, larger batches are more efficient [29]. This led us to re-evaluate our distributed system setup. Initially, we used a scatter-gather architecture to split requests into small batches and run them in parallel on multiple leaf nodes for better latency. However, this setup did not work well with GPU-based inference. Instead, we use the larger batches in the original requests directly. To compensate for the loss of cache capacity, we implemented a hybrid cache that uses both DRAM and SSD. Utilize CUDA graphs. We relied on CUDA Graphs 9 to completely eliminate the remaining small operations overhead. CUDA Graphs capture the model inference process as a static graph of operations instead of individually scheduled ones, allowing the computation to be executed as a single unit without any kernel launching overheads. 3.4.3 Realtime Feature Processing. When a user takes an action, a realtime feature processing application based on Flink 10 consumes user action Kafka 11 streams generated from front-end events. It validates each action record, detects and combines duplicates, and manages any time discrepancies from multiple data sources. The application then materializes the features and stores them in Rockstore [3]. At serving time, each Homefeed logging/serving request triggers the processor to convert sequence features into a format that can be utilized by the model.", "4 EXPERIMENT": "In this section, we will present extensive offline and online A/B experiment results of TransAct. We compare TransAct with baseline models using Pinterest's internal training data.", "4.1 Experiment Setup": "4.1.1 Dataset. We construct the offline training dataset from three weeks of Pinterest Homefeed view log (FVL). The model is trained on the first two weeks of FVL and evaluated on the third week. The training data is sampled based on user state and labels. For example, we design the sampling ratio for different label actions based on their statistical distribution and importance. In addition, since users only engage with a small portion of pins shown on their Homefeed page, most of the training samples are negative samples. To balance the highly skewed dataset and improve model accuracy, we employ downsampling on the negative samples and set a fixed ratio between the positive and negative samples. Our training dataset contains 3 billion training instances of 177 million users and 720 million pins. In this paper, we conduct all experiments with the Pinterest dataset. We do not use public datasets as they lack the necessary realtime user action sequence metadata features, such as item embeddings and action types, required by TransAct. Furthermore, they are incompatible with our proposal of realtime-batch hybrid model, which requires both realtime and batch user features. And they cannot be tested in online A/B experiments. 4.1.2 Hyperparameters. Realtime user sequence length is | \ud835\udc46 | = 100 and the dimension of action embedding \ud835\udc51 \ud835\udc4e\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b = 32. The encoded sequence feature is passed through a transformer encoder composed of 2 transformer blocks, with a default dropout rate of 0.1. The feed forward network in the transformer encoder layer has a dimension of \ud835\udc51 \u210e\ud835\udc56\ud835\udc51\ud835\udc51\ud835\udc52\ud835\udc5b = 32, and positional encoding is not used. The implementation is done using PyTorch. We use an Adam [14] optimizer with a learning rate scheduler. The learning rate begins with a warm-up phase of 5000 steps, gradually increasing to 0.0048, and finally reduced through cosine annealing. The batch size is 12000.", "4.2 Offline Experiment": "4.2.1 Metrics. The offline evaluation data, unlike training data, is randomly sampled from FVL to represent the true distribution of the real-world traffic. With this sampling strategy, the offline evaluation data is representative of the entire population, reducing the variance of evaluation results. In addition to sampling bias, we also eliminate position bias in offline evaluation data. Position bias refers to the tendency for items at the top of a recommendation to receive more attention and engagement than the items lower down the list. This can be a problem when evaluating a ranking model, as it can distort the evaluation results and make it difficult to accurately assess the model's performance. To avoid position bias, we randomize the order of pins in a very small portion of Homefeed recommendation sessions. This is done by shuffling the recommendations before presenting them to users. We gather the FVL for those randomized sessions and only use randomized data to perform the offline evaluation. Our model is evaluated on HIT@3. A chunk \ud835\udc84 = [ \ud835\udc5d 1 , \ud835\udc5d 2 , . . . , \ud835\udc5d \ud835\udc5b ] refers to a group of pins that are recommended to a user at the same time. Each input instance to the ranking model is associated with a user id \ud835\udc62 _ \ud835\udc56\ud835\udc51 , a pin id \ud835\udc5d _ \ud835\udc56\ud835\udc51 , and a chunk id \ud835\udc50 _ \ud835\udc56\ud835\udc51 . The evaluation output is grouped by ( \ud835\udc62 _ \ud835\udc56\ud835\udc51, \ud835\udc50 _ \ud835\udc56\ud835\udc51 ) so that it contains the model output from the same ranking request. We sort the pins from the same ranking request by a final ranking score S , which is a linear combination of Pinnability output heads \ud835\udc53 ( \ud835\udc99 ) . Then we take the top \ud835\udc3e ranked pins in each chunk and calculate the hit@K for all heads, denoted by \ud835\udefd \ud835\udc50,\u210e , which is defined as the number of topK-ranked pins whose labels of \u210e are 1. For example, if a chunk \ud835\udc84 = [ \ud835\udc5d 1 , \ud835\udc5d 2 , \ud835\udc5d 3 , . . . , \ud835\udc5d \ud835\udc5b ] is sorted by S , and the user repins \ud835\udc5d 1 and \ud835\udc5d 4, then hit@K of repin \ud835\udefd \ud835\udc50,\ud835\udc5f\ud835\udc52\ud835\udc5d\ud835\udc56\ud835\udc5b = 1 when \ud835\udc3e = 3. We calculate the aggregated HIT@3 for each head \u210e as follows: It is important to note that for actions indicating positive engagement, such as repin or click, a higher HIT@K score means better model performance. Conversely, for actions indicating negative engagement, such as hide, a lower HIT@K/hide score is desirable. At Pinterest, a non-core user is defined as a user who has not actively saved pins to boards within the past 28 days. Non-core users tend to be less active and therefore pose a challenge in terms of improving their recommendation relevance due to their limited historical engagement. This is also referred to as the cold-start user problem in recommendation [19]. Despite the challenges, it is important to retain non-core users as they play a crucial role in maintaining a diverse and thriving community, contributing to long-term platform growth. All reported results are statistically significant (p-value < 0 . 05) unless stated otherwise. 4.2.2 Results. We compare TransAct with existing methods of sequential recommendation. The first baseline is the WDL model [5] that incorporates sequence features as part of its wide features. Due to the large size of the sequence features, the number of parameters in the feature cross layer would grow quadratically, making it unfeasible for both training and online serving. Therefore, we used an averaging pooling for PinSage embeddings of user actions to encode the sequence. The second baseline is Alibaba's behavior sequence transformer (BST) model [4]. We trained 2 BST model variants here: one with only positive actions in user sequence, the other with all actions. We opted not to compare our results with DIN [43] as BST has already demonstrated its superiority over DIN. Additionally, we did not compare with variants like BERT4Rec [28] as the problem formulations are different and a direct comparison is not feasible. The results of the model comparison are presented in Table 1. It is evident that BST and TransAct outperform the WDL model, demonstrating the necessity of using a specialized sequential model to effectively capture short-term user preferences through real-time user action sequence features. BST performs well when only positive actions are encoded, however, it struggles to distinguish negative actions. In contrast, TransAct outperforms BST, particularly in terms of hide prediction, due to its ability to distinguish between different actions by encoding action types. Furthermore, TransAct also exhibits improved performance in HIT@3/repin compared to BST, which can be attributed to its effective early fusion and output compression design. A common trend across all groups is that the performance for non-core users is better than for all users, this is due to realtime user action features being crucial for users with limited engagement history on the platform, as they provide the only source of information for the model to learn their preferences.", "4.3 Ablation Study": "4.3.1 Hybrid ranking model. First, we investigate the effect of the realtime-batch hybrid design by examining the individual impact of TransAct(realtime component) and Pinnerformer(batch component). Table 2 shows the relative decrease in offline performance from the model containing all user features as we remove each component. TransAct captures users' immediate interests, which contribute the most to the user's overall engagement, while PinnerFormer (PF) [20] extracts users' long-term preferences from their historical behavior. We observe that TransAct is the most important user understanding feature in the model, but we still see value from the large-scale training and longer-term interests captured by PinnerFormer, showing that longer-term batch user understanding can complement a realtime engagement sequence for recommendations. In the last row of Table 2, we show that removing all user features other than TransAct and PinnerFormer only leads to a relatively small drop in performance, demonstrating the effectiveness of our combination of a realtime sequence model with a pre-trained batch model. 4.3.2 Base sequence encoder architecture. We perform an offline evaluation on different sequential models that process realtime user sequence features. We use different architectures to encode the PinSage embedding sequence from users' realtime actions. Average Pooling : use the average of PinSage embeddings in user sequence to present the user's short-term interest CNN : use a 1-d CNN with 256 output channels to encode the sequence. Kernel size is 4 and stride is 1. RNN : use 2 RNN layers with a hidden dimension of 256, to encode a sequence of PinSage embeddings. LSTM : use Long Short-Term Memory (LSTM) [10], a more sophisticated version of RNN that better captures longer-term dependencies by using memory cells and gating. We use 2 LSTM layers with the hidden size of 256. Vanilla Transformer : encodes only PinSage embeddings sequence directly using the Transformer encoder module. We use 2 transformer encoder layers with a hidden dimension of 32. The baseline group is the Pinnability model without realtime user sequence feature. From Table 3, we learned that using realtime user sequence features, even with a simple average pooling method, improves engagement. Surprisingly, more complex architectures like RNN, CNN, and LSTM do not always perform better than average pooling. However, the best performance is achieved with the use of a vanilla transformer, as it significantly reduces HIT@3/hide and improves HIT@3/repin. 4.3.3 Early fusion and sequence length selection. As discussed in Section 3.3.2, early fusion plays a crucial role in the ranking model. By incorporating early fusion, the model can not only take into account the dependency between different items in the user's action history but also explicitly learn the relationship between the ranking candidate pin and each pin that the user has engaged with in the past. Longer user action sequences naturally are more expressive than short sequences. To learn the effect of input sequence length on the model performance, we evaluate the model on different lengths of user sequence input. no early fusion concat append 50% 9.00% 8.50% 8,00% 7.50% 712.08 7.00% 6.50% 6.00% 100 100 Sequence length An analysis of Figure 4 reveals that there is a positive correlation between sequence length and performance. The performance improvement increases at a rate that is sub-linear with respect to the sequence length. The use of concatenation as the early fusion method was found to be superior to the use of appending. Therefore, the optimal engagement gain can be achieved by utilizing the maximum available sequence length and employing concatenation as the early fusion method. 4.3.4 Transformer hyperparameters. Weoptimized TransAct's transformer encoder by adjusting its hyperparameters. As shown in Figure 5, increasing the number of transformer layers and feed forward dimension leads to higher latency and also better performance. While the best performance was achieved using 4 transformer layers and 384 as the feed forward dimension, this came at the cost of a 30% increase in latency, which does not meet the latency requirement. To balance performance and user experience, we chose 2 transformer layers and 32 as the hidden dimension. -1.50% -1.00% -0.50% 0.00% 0.50% HIT@3/repin (vs TransAct) -10.0% 0.0% 10.0% 20.0% 30.0% Latency (vs TransAct) Feedforward Dimension 32 64 128 384 # Transformer Layers 1 2 4 4.3.5 Transformer output compression. The transformer encoder produces \ud835\udc76 \u2208 R \ud835\udc51 \u00d7| \ud835\udc46 | , with each column corresponding to an input user action. However, directly using \ud835\udc76 as input to the DCN v2 layers for feature crossing would result in excessive time complexity, which is quadratic to the input size. To address this issue, we explored several approaches to compress the transformer output. Figure 3 shows that the highest HIT@3/repin is achieved by combining the first K columns and applying max pooling to the entire sequence. The first K column represents the most recently engaged pins and the max pooling is an aggregated representation of the entire sequence, Although using all columns improved HIT@3/hide slightly, the combination of the first K columns and max pooling provided a good balance between performance and latency. We use K=10 for TransAct.", "4.4 Online Experiment": "Compared with offline evaluation, one advantage of online experiments in recommendation tasks is that they can be run on live user data, allowing the model to be tested in a more realistic and dynamic environment. For the online experiment, we serve the ranking model trained on the 2-week offline training dataset. We set the control group to be the Pinnability model without any realtime user sequence features. The treatment group is Pinnability model with TransAct. Each experiment group serves 1.5% of the total users who visit Homefeed page. 4.4.1 metrics. On Homefeed, one of the most important metrics is Homefeed repin volume . Repin is the strongest indicator that users find the recommended pins relevant, and is usually positively correlated to the amount of time users spend on Pinterest. Empirically, we found that offline HIT@3/repin usually aligns very well with Homefeed online repin volume. Another important metric is Homefeed hide volume , which measures the proportion of recommended items that users choose to hide or remove from their recommendations. High hide rates indicate that the system is recommending items that users do not find relevant, which can lead to a poor user experience. Conversely, low hide rates indicate that the system is recommending items that users find relevant and engaging, which can lead to a better user experience. 4.4.2 Online engagement. We observe significant online metric improvement with TransAct introduced to ranking. Figure 5 shows that we improved the Homefeed repin volume by 11%. It's worth noting that engagement gains for non-core users are higher because they do not have a well-established user action history. And realtime features can capture their interest in a short time. Using TransAct, the Homefeed page is able to respond quickly and adjust the ranking results timely. We see hide volume dropped and that the overall time spent on Pinterest is increased. 4.4.3 Model retrain. Onechallenge observed in the TransAct group was the decay of engagement metrics over time for a given user. As shown in Figure 6, we compare the Homefeed repin volume gain of TransAct to the baseline, with both groups either fixed or retrained. We observed that if TransAct was not retrained, despite having a significantly higher engagement on the first day of the experiment, it gradually decreased to a lower level over the course of two weeks. However, when TransAct was retrained on fresh data, there was a noticeable increase in engagement compared to not retraining the model. This suggests that TransAct, which utilizes realtime features, is highly sensitive to changes in user behavior and requires frequent retraining. Therefore, it is desired to have a high retrain frequency when using TransAct. In our production, we set the retrain frequency to twice a week and this retrain frequency has been proven to keep the engagement rate stable. no retrain retrain 10 12 Days in experiment 4.4.4 Random time window masking. Another challenge observed was dropping diversity in recommendations. Diversity measures the broadness and variety of the items being recommended to a user. Previous literature[36] finds diversity is associated with increasing user visiting frequency. However, diversity is not always desirable as it can lead to a drop in relevance. Therefore, it is crucial to find the right balance between relevance and diversity in recommendations. At Pinterest, we have a 28k-node hierarchical interest taxonomy [15] that classifies all the pins. The top-level interests are coarse. Some examples of top-level interests are art, beauty, and sport. Here, we measure the impression diversity as the summation of the number of unique top-level interests viewed per user. We observe that with TransAct introduced to Homefeed ranking, the impression diversity dropped by 2% to 3%. The interpretation is that by adding the user action sequence feature, the ranking model learns to optimize for the user's short-term interest. And by focusing on mainly short-term interest, the diversity of the recommendation dropped. We mitigate the diversity drop by using a random time window mask in the transformer as mentioned in Section 3.3.3. This random masking encourages the model to focus on content other than only the most recent items a user engaged with. With this design, the diversity metric drop was brought back to only -1% without influencing relevance metrics like repin volume. We also tried using a higher dropout rate in the transformer encoder layer and randomly masking out a fixed percentage of actions in the user action sequence input. However, neither of these methods yielded better results than using random time window masking. They increased the diversity at the cost of engagement drop.", "5 DISCUSSION": "", "5.1 Feedback Loop": "An interesting finding from our online experiment is that the true potential of TransAct is not fully captured. We observed a greater improvement in performance when the model was deployed as the production Homefeed ranking model for full traffic. This is due to the effect of a positive feedback loop: as users experience a more responsive Homefeed built on TransAct, they tend to engage with more relevant content, leading to changes in their behavior (such as more clicks or repins). These changes in behavior lead to shifts in the realtime user sequence feature, which are then used to generate new training data. Retraining the Homefeed ranking model with this updated data results in a positive compounding effect, leading to a higher engagement rate and a stronger feedback loop. This phenomenon is similar to \"direct feedback loops\" in literature [26] which refers to a model that directly influences the selection of its own future training data, and it is more difficult to detect if they occur gradually over time.", "5.2 TransAct in Other Tasks": "The versatility of TransAct extends beyond just ranking tasks. It has been successfully applied in the contextual recommendation and search ranking scenarios as well. TransAct is used in Related Pins [17] ranking, a contextual recommendation model to provide personalized recommendations of pins based on a given query pin. TransAct is also applied in Pinterest's Search ranking [1] system and notification ranking [41]. Table 6 showcases the effectiveness of TransAct in a variety of use cases and its potential to drive engagement in more real-world applications.", "6 CONCLUSIONS": "In this paper, we present TransAct, a transformer-based realtime user action model that effectively captures users' short-term interests by encoding their realtime actions. Our novel hybrid ranking model merges the strengths of both realtime and batch approaches of encoding user actions, and has been successfully deployed in the Homefeed recommendation system at Pinterest. The results of our offline experiments indicate that TransAct significantly outperforms state-of-the-art recommender system baselines. In addition, we have discussed and provided solutions for the challenges faced during online experimentation, such as high serving complexity, diversity decrease, and engagement decay. The versatility and effectiveness of TransAct make it applicable for other tasks, such as contextual recommendations and search ranking.", "REFERENCES": "[1] 2016. Search serving and ranking at Pinterest . https://medium.com/the-graph/ search-serving-and-ranking-at-pinterest-224707599c92 [2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473 (2014). [3] Jessica Chan. 2022. 3 Innovations While Unifying Pinterest's Key-Value Storage . https://medium.com/@Pinterest_Engineering/3-innovations-whileunifying-pinterests-key-value-storage-8cdcdf8cf6aa [4] Qiwei Chen, Huan Zhao, Wei Li, Pipei Huang, and Wenwu Ou. 2019. Behavior Sequence Transformer for E-Commerce Recommendation in Alibaba. In Proceedings of the 1st International Workshop on Deep Learning Practice for High-Dimensional Sparse Data (Anchorage, Alaska) (DLP-KDD '19) . Association for Computing Machinery, New York, NY, USA, Article 12, 4 pages. https://doi.org/10.1145/3326937.3341261 [5] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems . 7-10. [6] Tim Donkers, Benedikt Loepp, and J\u00fcrgen Ziegler. 2017. Sequential user-based recurrent neural network recommendations. In Proceedings of the eleventh ACM conference on recommender systems . 152-160. [7] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. arXiv preprint arXiv:1703.04247 (2017). [8] Ruining He and Julian McAuley. 2016. Fusing similarity models with markov chains for sparse sequential recommendation. In 2016 IEEE 16th international conference on data mining (ICDM) . IEEE, 191-200. [9] Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2015. Session-based recommendations with recurrent neural networks. arXiv preprint arXiv:1511.06939 (2015). [10] Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long Short-Term Memory. Neural Computation 9 (1997), 1735-1780. [11] Haoji Hu, Xiangnan He, Jinyang Gao, and Zhi-Li Zhang. 2020. Modeling personalized item frequency information for next-basket recommendation. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . 1071-1080. [12] Rong Jin, Joyce Y. Chai, and Luo Si. 2004. An Automatic Weighting Scheme for Collaborative Filtering. In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (Sheffield, United Kingdom) (SIGIR '04) . Association for Computing Machinery, New York, NY, USA, 337-344. https://doi.org/10.1145/1008992.1009051 [13] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recommendation. In 2018 IEEE international conference on data mining (ICDM) . IEEE, 197-206. [14] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , Yoshua Bengio and Yann LeCun (Eds.). http://arxiv.org/abs/1412.6980 [15] Eileen Li. 2019. Pin2Interest: A scalable system for content classification. https://medium.com/pinterest-engineering/pin2interest-a-scalablesystem-for-content-classification-41a586675ee7 [16] Jiacheng Li, Yujie Wang, and Julian McAuley. 2020. Time interval aware selfattention for sequential recommendation. In Proceedings of the 13th international conference on web search and data mining . 322-330. [17] David C. Liu, Stephanie Rogers, Raymond Shiau, Dmitry Kislyuk, Kevin C. Ma, Zhigang Zhong, Jenny Liu, and Yushi Jing. 2017. Related Pins at Pinterest: The Evolution of a Real-World Recommender System. In Proceedings of the 26th International Conference on World Wide Web Companion (Perth, Australia) (WWW '17 Companion) . International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, CHE, 583-592. https://doi.org/10.1145/3041021. 3054202 [18] Hao Ma, Irwin King, and Michael R. Lyu. 2007. Effective Missing Data Prediction for Collaborative Filtering. In Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (Amsterdam, The Netherlands) (SIGIR '07) . Association for Computing Machinery, New York, NY, USA, 39-46. https://doi.org/10.1145/1277741.1277751 [19] Senthilselvan Natarajan, Subramaniyaswamy Vairavasundaram, Sivaramakrishnan Natarajan, and Amir H Gandomi. 2020. Resolving data sparsity and cold start problem in collaborative filtering recommender system using linked open data. Expert Systems with Applications 149 (2020), 113248. [20] Nikil Pancha, Andrew Zhai, Jure Leskovec, and Charles Rosenberg. 2022. PinnerFormer: Sequence Modeling for User Representation at Pinterest. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (Washington DC, USA) (KDD '22) . Association for Computing Machinery, New York, NY, USA, 3702-3712. https://doi.org/10.1145/3534678.3539156 [21] Qi Pi, Guorui Zhou, Yujing Zhang, Zhe Wang, Lejian Ren, Ying Fan, Xiaoqiang Zhu, and Kun Gai. 2020. Search-based user interest modeling with lifelong sequential behavior data for click-through rate prediction. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management . 2685-2692. [22] Steffen Rendle. 2010. Factorization Machines. In 2010 IEEE International Conference on Data Mining . 995-1000. https://doi.org/10.1109/ICDM.2010.127 [23] Steffen Rendle. 2010. Factorization machines. In 2010 IEEE International conference on data mining . IEEE, 995-1000. [24] Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. 2001. Item-based collaborative filtering recommendation algorithms. In Proceedings of the 10th international conference on World Wide Web . 285-295. [25] Mike Schuster and Kuldip K Paliwal. 1997. Bidirectional recurrent neural networks. IEEE transactions on Signal Processing 45, 11 (1997), 2673-2681. [26] D. Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar Ebner, Vinay Chaudhary, Michael Young, Jean-Fran\u00e7ois Crespo, and Dan Dennison. 2015. Hidden Technical Debt in Machine Learning Systems. In Advances in Neural Information Processing Systems , C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett (Eds.), Vol. 28. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2015/file/ 86df7dcfd896fcaf2674f757a2463eba-Paper.pdf [27] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer. In Proceedings of the 28th ACM international conference on information and knowledge management . 1441-1450. [28] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer. CoRR abs/1904.06690 (2019). arXiv:1904.06690 http://arxiv.org/abs/1904.06690 [29] Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, and Joel S Emer. 2017. Efficient processing of deep neural networks: A tutorial and survey. Proc. IEEE 105, 12 (2017), 2295-2329. [30] Yong Kiam Tan, Xinxing Xu, and Yong Liu. 2016. Improved recurrent neural networks for session-based recommendations. In Proceedings of the 1st workshop on deep learning for recommender systems . 17-22. [31] Jiaxi Tang and Ke Wang. 2018. Personalized top-n sequential recommendation via convolutional sequence embedding. In Proceedings of the eleventh ACM international conference on web search and data mining . 565-573. [32] Trinh Xuan Tuan and Tu Minh Phuong. 2017. 3D convolutional networks for session-based recommendation with content features. In Proceedings of the eleventh ACM conference on recommender systems . 138-146. [33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems , I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.), Vol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf [34] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & Cross Network for Ad Click Predictions. In Proceedings of the ADKDD'17 (Halifax, NS, Canada) (ADKDD'17) . Association for Computing Machinery, New York, NY, USA, Article 12, 7 pages. https://doi.org/10.1145/3124749.3124754 [35] Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong, and Ed Chi. 2021. DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-Scale Learning to Rank Systems. In Proceedings of the Web Conference 2021 (Ljubljana, Slovenia) (WWW'21) . Association for Computing Machinery, New York, NY, USA, 1785-1797. https://doi.org/10.1145/3442381.3450078 [36] Yuyan Wang, Mohit Sharma, Can Xu, Sriraj Badam, Qian Sun, Lee Richardson, Lisa Chung, Ed H. Chi, and Minmin Chen. 2022. Surrogate for Long-Term User Experience in Recommender Systems. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (Washington DC, USA) (KDD '22) . Association for Computing Machinery, New York, NY, USA, 4100-4109. https://doi.org/10.1145/3534678.3539073 [37] Jiajing Xu, Andrew Zhai, and Charles Rosenberg. 2022. Rethinking Personalized Ranking at Pinterest: An End-to-End Approach. In Proceedings of the 16th ACM Conference on Recommender Systems (Seattle, WA, USA) (RecSys '22) . Association for Computing Machinery, New York, NY, USA, 502-505. https://doi.org/10. 1145/3523227.3547394 [38] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and Jure Leskovec. 2018. Graph Convolutional Neural Networks for Web-Scale Recommender Systems. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (London, United Kingdom) (KDD '18) . Association for Computing Machinery, New York, NY, USA, 974-983. https://doi.org/10.1145/3219819.3219890 [39] Shuai Zhang, Yi Tay, Lina Yao, Aixin Sun, and Jake An. 2019. Next item recommendation with self-attentive metric learning. In Thirty-Third AAAI Conference on Artificial Intelligence , Vol. 9. [40] Bendong Zhao, Huanzhang Lu, Shangfeng Chen, Junliang Liu, and Dongya Wu. 2017. Convolutional neural networks for time series classification. Journal of Systems Engineering and Electronics 28, 1 (2017), 162-169.", "A HEAD WEIGHTING": "We illustrate how head weighting helps the multi-task prediction task here. Consider the following example, of a model using 3 actions: repins, clicks, and hides. The label weight matrix is set as Table 7. Hides are a strong negative action, while repins and clicks are both positive engagements, although repins are considered a stronger positive signal than clicks. We set the value of \ud835\udc74 manually, to control the weight on cross-entropy loss. Here, we give some examples of how this is achieved. \u00b7 If a user only hides a pin ( \ud835\udc9a \u210e\ud835\udc56\ud835\udc51\ud835\udc52 = 1), and does not repin or click ( \ud835\udc9a \ud835\udc5f\ud835\udc52\ud835\udc5d\ud835\udc56\ud835\udc5b = \ud835\udc9a \ud835\udc50\ud835\udc59\ud835\udc56\ud835\udc50\ud835\udc58 = 0). Then we want to penalize the model more if it predicts repin or click, by setting the \ud835\udc74 \ud835\udc5f\ud835\udc52\ud835\udc5d\ud835\udc56\ud835\udc5b,\u210e\ud835\udc56\ud835\udc51\ud835\udc52 and \ud835\udc74 \ud835\udc50\ud835\udc59\ud835\udc56\ud835\udc50\ud835\udc58,\u210e\ud835\udc56\ud835\udc51\ud835\udc52 to a large value. \ud835\udc64 \ud835\udc5f\ud835\udc52\ud835\udc5d\ud835\udc56\ud835\udc5b = \ud835\udc74 \ud835\udc5f\ud835\udc52\ud835\udc5d\ud835\udc56\ud835\udc5b,\u210e\ud835\udc56\ud835\udc51\ud835\udc52 \u2217 \ud835\udc9a \u210e\ud835\udc56\ud835\udc51\ud835\udc52 = 100. \ud835\udc64 \ud835\udc50\ud835\udc59\ud835\udc56\ud835\udc50\ud835\udc58 = \ud835\udc74 \ud835\udc50\ud835\udc59\ud835\udc56\ud835\udc50\ud835\udc58,\u210e\ud835\udc56\ud835\udc51\ud835\udc52 \u2217 \ud835\udc9a \u210e\ud835\udc56\ud835\udc51\ud835\udc52 = 100. \u00b7 If a user only repins a pin ( \ud835\udc9a \ud835\udc5f\ud835\udc52\ud835\udc5d\ud835\udc56\ud835\udc5b = 1), but does not hide or click ( \ud835\udc9a \u210e\ud835\udc56\ud835\udc51\ud835\udc52 = \ud835\udc9a \ud835\udc50\ud835\udc59\ud835\udc56\ud835\udc50\ud835\udc58 = 0). We want to penalize the model if it predicts hide: \ud835\udc64 \u210e\ud835\udc56\ud835\udc51\ud835\udc52 = \ud835\udc74 \u210e\ud835\udc56\ud835\udc51\ud835\udc52,\ud835\udc5f\ud835\udc52\ud835\udc5d\ud835\udc56\ud835\udc5b \u2217 \ud835\udc9a \ud835\udc5f\ud835\udc52\ud835\udc5d\ud835\udc56\ud835\udc5b = 5. But we do not need to penalize the model if it predicts a click, because a user could repin and click the same pin. \ud835\udc64 \ud835\udc50\ud835\udc59\ud835\udc56\ud835\udc50\ud835\udc58 = \ud835\udc74 \ud835\udc50\ud835\udc59\ud835\udc56\ud835\udc50\ud835\udc58,\ud835\udc5f\ud835\udc52\ud835\udc5d\ud835\udc56\ud835\udc5b \u2217 \ud835\udc9a \ud835\udc5f\ud835\udc52\ud835\udc5d\ud835\udc56\ud835\udc5b = 0", "B POSITIONAL ENCODING": "Wetried several positional encoding approaches: learning positional embedding from scratch, sinusoidal positional encoding [33], and linear projection positional encoding as proposed in [4]. Table8 shows that positional encoding does not add much value.", "C MODEL EFFICIENCY": "Table 9 shows more detailed information on the efficiency of our model, including number of flops, model forward latency per batch (batch size = 256), and serving cost. The serving cost is not linearly correlated with model forward latency because it is also related to server configurations such as time out limit, batch size, etc. GPU serving optimization is important to maintain low latency and serving cost."}
