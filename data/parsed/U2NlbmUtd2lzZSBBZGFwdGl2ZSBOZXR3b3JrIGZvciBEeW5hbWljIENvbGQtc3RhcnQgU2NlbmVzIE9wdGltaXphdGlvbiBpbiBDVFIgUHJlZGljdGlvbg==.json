{"Scene-wise Adaptive Network for Dynamic Cold-start Scenes Optimization in CTR Prediction": "Wenhao Li Huazhong University of Science and Technology Beijing, China rzliwenhao@hust.edu.cn Jie Zhou School of Software, Beihang University Beijing, China zhoujiee@buaa.edu.cn Chuan Luo \u2217 School of Software, Beihang University Beijing, China chuanluo@buaa.edu.cn Chao Tang Meituan Beijing, China tangchao12@meituan.com", "Kun Zhang": "Meituan Beijing, China zhangkun32@meituan.com", "Abstract": "", "Shixiong Zhao \u2217": "The University of Hong Kong Hong Kong, China sxzhao@cs.hku.hk", "Keywords": "In the realm of modern mobile E-commerce, providing users with nearby commercial service recommendations through locationbased online services has become increasingly vital. While machine learning approaches have shown promise in multi-scene recommendation, existing methodologies often struggle to address cold-start problems in unprecedented scenes: the increasing diversity of commercial choices, along with the short online lifespan of scenes, give rise to the complexity of effective recommendations in online and dynamic scenes. In this work, we propose S cenew ise A daptive N etwork (SwAN 1 ), a novel approach that emphasizes high-performance cold-start online recommendations for new scenes. Our approach introduces several crucial capabilities, including scene similarity learning, user-specific scene transition cognition, scene-specific information construction for the new scene, and enhancing the diverged logical information between scenes. We demonstrate SwAN's potential to optimize dynamic multi-scene recommendation problems by effectively online handling cold-start recommendations for any newly arrived scenes. More encouragingly, SwAN has been successfully deployed in Meituan's online catering recommendation service, which serves millions of customers per day, and SwAN has achieved a 5.64% CTR index improvement relative to the baselines and a 5.19% increase in daily order volume proportion.", "CCS Concepts": "", "\u00b7 Computing methodologies \u2192 Probabilistic reasoning .": "1 https://github.com/ChrisLiiiii/SwAN Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. RecSys '24, October 14-18, 2024, Bari, Italy https://doi.org/10.1145/3640457.3688115 Recommendation, Multi-Scene, Cold-Start", "ACMReference Format:": "Wenhao Li, Jie Zhou, Chuan Luo, Chao Tang, Kun Zhang, and Shixiong Zhao. 2024. Scene-wise Adaptive Network for Dynamic Cold-start Scenes Optimization in CTR Prediction. In 18th ACM Conference on Recommender Systems (RecSys '24), October 14-18, 2024, Bari, Italy. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3640457.3688115", "1 Introduction": "Delivering users with nearby commercial service suggestions through location-based online systems [14, 18, 28] has grown increasingly crucial within the era of modern mobile E-commerce. Learning to Rank (LTR) involves applying machine learning algorithms [3, 12] in optimizing the rank strategy, and is the fundamental technique to facilitate better recommendation services. Contemporary recommendation systems not only focus on users' habits derived from historical information but also endeavor to infer the preferences of the same user across diverse scenes, facilitating more accurate and high-quality multi-scene recommendation (MSR) [23]. Despite the considerable advancements in MSR research, a majority of these developments are grounded in the assumption that scenes are predefined and classified prior to offline training, with all subsequent recommendations adhering to established categories during online operations. Therefore, the existing literature (e.g.SAML [2], STAR [19], and HMoE [11]) on MSR primarily concentrates on a static model architecture that distinguishes scenes by directing inputs of each scene to a fixed structural branch within the model. However, empirical evidence reveals that this assumption does not always hold true. As the assortment of items and options expands in today's world, a proliferation of distinct scenes arises. Consequently, users' behaviors tend to diverge more frequently, leading to an increased variety of scenes without previous identical scenes available for reference in historical data [9]. According to Fig. 1, the online recommendation service will launch different scenes during specific periods in spring or winter, taking into account user preferences and merchant demands. Additionally, it will also design exclusive activities for specific holidays, such as New Year's Day. On the other hand, scenes often have a limited online lifespan before vanishing (e.g.Valentine's Day Monday Tuesday Wednesday Thursday Friday Saturday Sunday Thursday Friday Saturday Weekend Spring Outing KFC Crazy Thursday Working Meal Valentine's Day Afternoon Tea Spring Winter Skiing New Year's Day Travel Winter\u00a0Hot Spring \" means Offline \" in Fig. 1), leaving no opportunity for a recommendation system to collect data, go offline for fine-tuning, and return online [16]. The Hybrid of implicit and explicit Mixture-of-Experts (HMoE) [11] demonstrates that the performance of one scene can be enhanced (through training) by the prediction of other scenes. Unfortunately, HMoE still requires learning the historical data of a new scene and sharing information between scenes through re-parameterization. logic of scenes. In particular, a novel component named \ud835\udc37\ud835\udc56\ud835\udc50\ud835\udc60 has been proposed for the AEG to achieve gradient propagation and select appropriate model structures adaptively. In this paper, we demonstrate that the performance of a newlyarrived scene can be directly and significantly improved through online prediction using our S cenew ise A daptive N etwork (SwAN) model. This suggests that cold-starting new scenes is not only feasible but also surpasses the recommendation performance of existing approaches on known scenes. In general, SwAN employs the typical Embedding&MLP (Multilayer Perceptron) paradigm for the recommendation [24] (Sec. 3). In the Embedding part, SwAN utilizes the Scene Relation Graph (SRG) to capture graph-structured similarities between scenes based on inherent attributes and user interaction features, thereby learning the inertial patterns among scenes. The SwAN model also incorporates the Similarity Attention Network (SAN) to capture users' habits during scene transitions by applying user attention on scene similarity knowledge. Furthermore, SwAN assigns each known scene a separate feature embedding (Scene Embedding Layers) to understand how scenes individually influence user behavior and interlace them with the SAN, allowing the impact of new scenes on users to be directly derived. In the MLP part, SwAN generally adopts the Adaptive Ensemble-experts Module (AEM), which is a Mixture-of-Experts (MoE) architecture and includes an Adaptive Expert Group (AEG) of Sparse MoE that uniquely leverages Cosine Loss to enhance diversities between scenes, as well as a Shared Expert Group (SEG) of Multi-gate MoE that captures the shared Extensive evaluation on both the public and industrial datasets shows that the SwAN model outperforms existing MSR approaches by seamlessly adapting to new scenes and providing more accurate and high-quality recommendations. SwAN achieves up to 5.64% online CTR improvement relative to the baselines and up to 5.19% increase in daily order volume proportion, as evaluated in Sec. 4.5. The main contributions of this paper are as follows: \u00b7 We propose SwAN, an innovative high-performance multiscene cold-start optimization network. \u00b7 Innovatively, we propose SRG to acquire prior information from similar scenes for cold-start scenes and employ SAN to get the attention weight of these scenes from user's perspective. Finally, AEM dynamically allocates model structures to enhance the extraction capability of shared and specific information across different scenes. \u00b7 SwAN has been deployed in a real-world online business recommendation system of Meituan and achieved a 5.64% improvement in CTR compared to the baseline model.", "2 Related work": "Multi-scene learning tackles recommendations for users across various scenes [29]. Traditional models for multi-scene learning have been developed to enhance performance in multiple fixed scenes. Drawing inspiration from the Multi-task Mixture-of-Experts model, Li [11] introduced HMoE that implicitly identifies scene disparities and similarities in the feature space and explicitly enhances performance in the label space using a stacked model. Scene-wise Adaptive Network for Dynamic Cold-start Scenes Optimization in CTR Prediction A Similarity Attention Network (SAN) Embedding Layer Other features User features Target scene features Similar scene features Concatenation ... Adaptive Expert Shared Expert SS-3 Emb Layer SS-2 Emb Layer SS-1 Emb Layer Cross-scene Feature Representation (CFR) A Element-wise Add Expert Selector \u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7 Cosine Loss Tower Gate Network G G Shared Expert Group (SEG) Adaptive Expert Group (AEG) SS-1 Emb Layer Similar Scene Embedding Layer AEM Scene Relation Graph (SRG) Unfortunately, the model did not distinguish different scenes expert-wise, resulting in insufficient mining of scene-specific information and affecting the model's ability to represent scenes. To capture the diverse characteristics of various scenes and thus serve them equitably, Sheng et al. [19] brings the STAR model which leverages data from all scenes. PEPNet [1] implements an efficient, low deployment cost, plug-and-play multi-scene modeling paradigm by constructing parameters and embedding personalization. Zhou et al. [27] proposed HiNet, which employs multi-tasks and multi-scenes explicitly and hierarchically using a hierarchical MoE to model commonality and individuality among multi-scenes. However, the aforementioned models are designed for multiple fixed scenes, so they need re-training with additional model structure when applied to dynamically increasing cold-start scenes. The cold-start problem is an open and challenging research problem in the field of recommendation systems [6]. Zhu et al. [31] proposed the Meta Warm Up Framework (MWUF) based on meta-learning and considered that the embeddings for cold-start and warm-up stages are in different spaces. The MWUF designed Meta Scaling Network and Meta Shifting Network to map cold-start embeddings to the warm-up space and eliminate noise. However, the MWUF mainly optimizes item cold-start and is unsuitable for scene cold-start. Besides, Du et al. [5] developed the scene-specific Sequential Meta learner ( \ud835\udc60 2 \ud835\udc40\ud835\udc52\ud835\udc61\ud835\udc4e ) based on metalearning. The \ud835\udc60 2 \ud835\udc40\ud835\udc52\ud835\udc61\ud835\udc4e model mainly learns the gradient and loss changes of the target model while fitting the distribution of old 1 Scene - 1 Fast food Cheap Scene - 2 Dinner Expensive Party meal Single meal Scene - 3 Expensive Fast food Party meal Workday Workday Weekend 1 : Weight between scenes 2 scene data through the Long Short-Term Memory (LSTM) [8] module to guide the model's training direction and early-stop timing in new scenes. Nevertheless, it results in high-cost consumption when applied to multiple cold-start scenes, and it cannot comprehensively consider the distribution rules of multiple scene data.", "3 Approach": "This section presents our design of the proposed SwAN model (Fig. 2). In essence, SwAN follows the key principle of optimizing multi-scene (by extracting scene-specific and shared information) and cold-start (by incorporating data from similar scenes as supplements) problem and consists of multiple modules: the Scene Relation Graph (Sec.3.1), Similarity Attention Network (Sec.3.2), Cross-scene Feature Representation (Sec.3.3), Adaptive Ensembleexperts Module (Sec.3.4). The Decision Layer (Sec.3.5) of SwAN and the loss function (Sec.3.6) are appended.", "3.1 Scene Relation Graph (SRG)": "A scene comprises inherent attribute features and user interaction features [1]. In dynamic multi-scene problems, there is no historical interaction data between users and new scenes, which means that only scene attribute features can be invoked to collect information. Fortunately, users exhibit similar preferences in comparable scenes. Based on our post-fact online business analysis, users often perceive a positive correlation between the similarity of scenes and the similarity of item features within those scenes [7]. This allows a recommendation system to optimize the cold-start process by leveraging prior information from analogous scenes to resemble the target scene closely. Based on these premises, SwAN invokes a Scene Relation Graph (SRG) module that builds a relational graph between the current scene (to be predicted) and the existing scenes based on the scene features. The construction process of SRG is as follows: (1) Firstly, it lists the basic features (unrelated to online interactions, e.g.price and category) of the items to be sorted and uses user key interactions as labels to calculate the Pearson correlation coefficient of various features in the existing scenes, selecting the top\ud835\udc5b (Sec. 4.3 for details) key features. ... Scene_1 Scene_2 Scene_n ... score score score Target Scene Emb User Emb Softmax Similar Scenes (2) Secondly, it aggregates the key features of items in coldstart scenes to obtain scene-level features such as averages, variances, maximums, and minimums (measuring the distribution patterns of each feature). (3) Lastly, it categorizes the above features and counts the number of identical features between scenes as the edge weights (Fig. 3, Scene-2 and Scene-3 share 2 identical attributes). By doing so, the SRG module obtains a similarity rank between the current and existing scenes by calculating raw feature explicit similarity, making SwAN flexible in choosing a threshold to invoke scenes with certain weighted similarities to the target scene.", "3.2 Similarity Attention Network (SAN)": "However, determining similar scenes based solely on attributes is inadequate. In real-world applications, various users perceive the same scene pair differently, and the model must incorporate user cognition to comprehend the latent similarity between scenes on a deeper level [30]. For instance 2 , some individuals consider horror movies and zombie movies part of the same genre, while others do not. Consequently, our model enhances the SRG module by introducing user information for attention. This is achieved by incorporating a Similarity Attention Network (SAN, shown in Fig. 4) to calculate learned latent similarity from the user's perspective. The input of the SAN includes the features of the target scene, the similar scenes defined in the SRG, and the user. The specific attention calculation (referred to the DIN [26]) is as follows: where \ud835\udc38 \ud835\udc62 , \ud835\udc38 \ud835\udc61 , and \ud835\udc38 \ud835\udc56 \ud835\udc60 are the embeddings of users, target scene, and the \ud835\udc56 -th similar scene, respectively; \ud835\udc3c denotes all the scenes; \u02c6 \ud835\udc46 \ud835\udc56 is the intermediate variable (The output of the blue \"score\" module in Fig. 4.); \ud835\udc46 \ud835\udc56 is the latent learned similarity between the \ud835\udc56 -th similar scene and the target scene; operator \u2295 means concatenation, and operator \u2297 means element-wise product. This structure effectively integrates user cognition to learn the genuine similarity between scenes and outputs a weighted representation of prior information from analogous scenes, which enables a more rational ranking of samples in new scenes. Furthermore, the SRG and SAN structures boosted the performance of SwAN during the cold-starting of new scenes.", "3.3 Cross-scene Feature Representation (CFR)": "There are differences in the bottom-level feature representation for each scene as well [2]. For example, the distance between users and dining locations has different importance in the breakfast and regular meal scenes 2 . To reflect these differences and provide information supplementation for the cold-start embedding of the target scene, SwAN added a Cross-scene Feature Representation (CFR) structure to the feature processing module (Fig. 2), which essentially assigns each extent scene a separate embedding to capture a scene's properties solely. Specifically, the input of CFR is the scene-related features \ud835\udc53 \ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc52\ud835\udc61 _ \ud835\udc60\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc52 and the similarity between scenes output by SAN, and the calculation formula is as follows: where \ud835\udc38\ud835\udc40\ud835\udc35 \ud835\udc56 (\u00b7) is the embedding layer corresponding to the \ud835\udc56 -th similar scene. The input of the subsequent model is: where \ud835\udc38 \ud835\udc5c means the embedding of other features, and + means element-wise addition. The model transfers prior information from similar scenes regarding feature representation dimensions through CFR, optimizing the cold-start problem and enhancing the expression of differences between scenes. In addition, since CFR essentially involves multiple dictionary lookups and weighted vector summation, it does not introduce excessive computational overhead.", "3.4 Adaptive Ensemble-experts Module (AEM)": "Traditional static multi-scene models usually set up separate model branches for each scene (e.g.STAR [19]), using structural differences to improve the ability to mine diverged information and optimize negative transfer problem, which are the cores of multi-scene modeling. However, in dynamic multi-scene problems, numerous scenes go online and offline frequently. The traditional model design approach cannot assign model structure for cold-start scenes, while the strategy of retraining the model based on a small number of coldstart scene samples leads to computational redundancy and require frequent offline fine-tuning to update the model architecture. To solve the above problems, we designed Adaptive Ensemble-experts Module (AEM) as the backbone network of the model to enhance the ability to extract differential information and optimize negative transfer in dynamic and multi-scene environments (Fig. 2). Firstly, we draw inspiration from the MMoE model [15] and develop multiple expert networks to enhance the model's ability to mine information. Secondly, we divide the expert networks into groups that improve the model's ability to extract scene-specific and shared information. The Adaptive Experts Group (AEG) is responsible for extracting scene-specific information. To avoid the high cost of model training caused by frequent scene updates, AEG adopts a dynamic combination of experts to calculate differentiated weights for different scene samples, which means learning how to allocate model structures adaptively. This function is mainly implemented by the Expert Selector (ES). As shown in Fig. 2, the input of ES is the weighted similar scene representation obtained by SAN, and the output is the gate weight (0 or 1) of each expert in AEG. In this way, ES transfers the prior information of expert selection from similar scenes. AEG enhances the model's ability to extract different scene-specific information and optimizes the cold-start phase of new scenes. In more detail, the computations in ES consist of two steps: (1) Generate selection probabilities for each expert and the unique threshold (to unify all expert-selected baselines and enhance stability). (2) Output a weight of 1 if the probability exceeds the threshold and 0 otherwise. The specific formulas are as follows: where \ud835\udc43 \ud835\udc58 is the selection probability of the \ud835\udc58 -th expert, and \ud835\udc47 is the probability threshold. In addition, incorporating \ud835\udc38 \ud835\udc62 into the calculations of \ud835\udc43 \ud835\udc58 allows for more accurate computations from the user's perspective, similar to the SAN. However, using a step function or a threshold function to compare probabilities and thresholds can lead to gradient interruption, which means that the MLP model for generating probabilities and thresholds cannot be trained. To solve this problem, we have designed a Differentiable conditional selection unit ( \ud835\udc37\ud835\udc56\ud835\udc50\ud835\udc60 ) based on the sigmoid function: where \ud835\udf0f is a temperature coefficient greater than 0 and \ud835\udc4a \ud835\udc58 is the weight of the \ud835\udc58 -th expert. By performing the aforementioned operations, \ud835\udc37\ud835\udc56\ud835\udc50\ud835\udc60 dynamically selects appropriate model structures for cold-start scenes based on similar scene information, achieving adaptability at the model architecture level. When the value of \ud835\udf0f is close to 0, the output weight is more relative to 0 or 1. However, excessively small \ud835\udf0f leads to unstable model training. To address this issue, we introduce the variance loss of the gate values for each expert to increase the variance between gate values and move them closer to 0 and 1. AEG is built on ensemble learning, so improving the differences between sub-learners' outputs can help enhance model performance [17]. We add a cosine similarity loss function between the outputs of each expert in AEG, with the specific formula as follows: where \ud835\udc38 \ud835\udc5a \ud835\udc4e and \ud835\udc38 \ud835\udc5b \ud835\udc4e are output vectors of two different experts in the AEG, and \ud835\udc34\ud835\udc38\ud835\udc3a is the whole AEG. In addition, SwAN constructs the Shared Experts Group (SEG) to enhance the extraction performance of shared information between scenes, following the approach of classic multi-scene models[3]. The experts of this module remain consistent across all scenes. In summary, AEM adaptively transfers the model-building approach for similar scenes to the current one. The model enhances the generalization ability to new scenes and improves the ability to mine scene-specific and shared information, providing a solution to optimize dynamic multi-scene problems.", "3.5 Decision Layer": "By utilizing AEM, SwAN extracts the shared and specific information of scenes, which is contained in the output vectors of each expert in SEG and AEG, respectively. However, the contribution of each vector to the final prediction target varies. To address this issue, inspired by the solution of MMoE, we add a gating network for each expert: where \ud835\udc38 \ud835\udc53 \ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc59 _ \ud835\udc56\ud835\udc5b is the input of the MLP structure in the output stage of SwAN, \ud835\udc3a \ud835\udc56 is the gate value of each expert, \ud835\udc40\ud835\udc3f\ud835\udc43 \ud835\udc54 (\u00b7) is the MLP structure to calculate the gate value, and \ud835\udc49\ud835\udc52\ud835\udc50 \ud835\udc60 and \ud835\udc49\ud835\udc52\ud835\udc50 \ud835\udc4e are the expert outputs of SEG and AEG, respectively. The output of the decision layer, namely, the final output of SwAN, can be expressed as:", "3.6 Composition of Losses": "SwAN uses the Cross-Entropy loss function between output and label to guide training. The formula is as follows: where \ud835\udc66 and \u02c6 \ud835\udc66 are label and predicted value, respectively. In addition, as described in Sec. 3.4, a variance loss is added: where \ud835\udc41 \ud835\udc4e is the number of experts in AEG. To sum up, the loss function is as follows: where \ud835\udefc , \ud835\udefd , and \ud835\udefe are hyper-parameters set according to the actual dataset (Sec. 4.3).", "4 Experiments": "To verify the effectiveness and generalization of the SwAN model, this study conducts experiments based on two datasets: a closedsource dataset from Meituan's online catering recommendation service with millions of daily users and an open-source dataset constructed from the Taobao public dataset [5].", "4.1 Experimental Settings": "Industrial Dataset. Samples from Dataset-1 are obtained from the online catering recommendation platform of Meituan, specifically from the business of Sales Campaign Session with an average daily customer level of millions. This business designs promotional activity scenes to cater to the consumption preferences of users at different periods, thus involving many scenes and frequent updates. We selected three months and one month of actual user online interaction behaviors as the training and test sets, respectively. The training set contains 70 million samples, while the test set contains 30 million. The ratio of the number of positive samples between the number of negative samples is about 1 to 3. Table 1 shows that this dataset's training and testing sets contain 751 and 309 scenes, respectively. Among them, 207 scenes in the testing set have never appeared in the training set, representing cold-start scenes. Public Dataset. Samples from Dataset-2 are obtained from user click logs of cloud-based theme scenes on Taobao 3 . We followed the official instructions [5] and divided the dataset into training and testing sets, which include 250 and 105 different recommendation scenes, respectively. Actually, after dealing with Dataset-2 through the official instructions, it is guaranteed that none of the testing set scenes appears in the training set. In addition, to adapt the features of the Taobao dataset to the cold-start multi-scene recommendation case studied in our paper, we performed data clustering and further processing of the Taobao dataset. The original Taobao dataset only contains item embeddings, scene theme IDs, and some item categories. The original dataset is not well suited for an effective evaluation of SwAN due to two rationales. First, the coverage of item category features is deficient (only 23.18%), which cannot produce practical scene attributes. Second, when we use t-SNE to reduce the dimensionality of item embeddings (as shown in Fig. 5), it can be observed that the distribution of items on the two-dimensional plane is not optimal (different colors represent different original categories). Specifically, there are cases where different categories of items are clustered together, and items of the same category are dispersed. This graph also indicates a significant dissimilarity between different original categories. Therefore, it is unreasonable to calculate scene features using the item category information from the original data. In summary, the information in the original dataset does not meet the case setting of this paper. To address this problem, we applied the k-means algorithm to cluster the item embeddings provided in the dataset. We used the silhouette coefficient to evaluate the appropriateness of the selected hyper-parameter \ud835\udc58 . The relationship between \ud835\udc58 and the silhouette coefficient is shown in Table 3. The table shows that the optimal value for \ud835\udc58 is 3. Using clustering of item embeddings, we obtained the category information for all items. Further, we derived the inherent attributes of each scene, which meets the data requirements of our model in this paper. Settings. Thecross-entropy loss function and Adam optimizer [10] are used in the experiments. The number of experts in AEG and SEG is set equally to 10 as the default value. The value of \ud835\udf0f , which is used in the \ud835\udc37\ud835\udc56\ud835\udc50\ud835\udc60 (\u00b7) function (Eq. 8), was set to 10 -3 as default. Furthermore, we set \ud835\udefc as 1, \ud835\udefd and \ud835\udefe as 10 -3 in Eq. 15. Hyper-parameter experiments can be found in Sec. 4.3. In order to make our comparison fair, for the baseline model, its hyper-parameter settings were configured through the same method as SwAN does. Metrics. In recommendation systems, items can be classified as relevant or irrelevant for a given user. We use the AUC (Area Under the Curve) score to evaluate how effectively the model can distinguish between these two classes of items. A higher AUC score indicates that the model can differentiate between relevant and nonrelevant items for users in a more effective way, and have stronger capability of recommending items that users find interesting and relevant. We used CTR (Click-Through-Rate) in online experiments, and the CTR measures the ratio of the number of clicks on recommended items to the number of items displayed. A higher CTR indicates that users find the recommended items more relevant and are more likely to click on them. Furthermore, the Gini coefficient is utilized to measure the uniformity of model improvement across different scenes. A lower Gini coefficient indicates less interference by differences between scenes, better generalization performance, and better suitability for application in cold-start scenes.", "4.2 Experimental Results": "The following part mainly introduces the experimental setup and analyzes the comparison among our SwAN model and other singlescene models (SSM) and static multi-scene models (SMSM) in the recommendation datasets of Meituan and Taobao. SSM Experiments. This experiment is first based on classical SSM, including DNN [4], MMoE [15], and PLE [20]. DNN is a singlescene and single-task model. As shown in Table 2, our model has significantly improved AUC compared to DNN in both datasets, especially for the recommendation effect of new scenes in Meituan's dataset. MMoE and PLE are all single-scene and multi-task models. In this experiment, we added two objectives, click prediction and order prediction, for Dataset-1. In contrast, we conducted singleobjective prediction for the Taobao dataset due to only one objective provided. In addition, to be consistent with the industrial application strategy, the SSM model uses all samples from various scenes for training and incorporates scene IDs as features. However, no multi-scene model structure optimization has been performed. The experimental results also prove that our model outperforms the baseline models in new and old scenes. SMSM Experiments. To verify the effectiveness of SwAN compared to existing state-of-the-art SMSMs, we trained the HMoE [11], STAR [19], PEPNet [1] and HiNet [27] and then conducted comparative experiments. According to the definition mentioned earlier, both of these models belong to the static multi-scene model, which is suitable for multiple fixed scenes with stable traffic, and therefore contradicts the definition of dynamic multi-scene. To solve this problem, we adopted a standard solution in industrial applications: clustering scenes based on their attributes and treating the resulting cluster of new and old scenes as a sizeable stable scene. The experimental results showed that SwAN still achieved the best performance. Sub-scenes Experiments. In this context, a sub-scene refers to an individual scene within each source in Table 1. We randomly selected 10 sub-scenes from the test set and tested the effect comparison of different models, as shown in Table 4. It can be seen that SwAN achieves the highest AUC in each sub-scene. In summary, SwAN has shown advantages in solving dynamic multi-scene problems compared to other widely-used single-scene and multi-scene models. This further proves that SwAN is theoretically practical and widely applicable.", "4.3 Hyperparameters Experiments": "To illustrate the impact of hyperparameters on the experimental results, we conducted relevant experiments based on dataset-1. Hyperparameters of SRG. We tested the experimental results of constructing SRG by filtering features according to different correlation coefficients ( \ud835\udc50\ud835\udc50 ) thresholds (Table.5). It can be found from the experimental results that a reasonable threshold can filter out noise features and select as many effective features as possible to improve the model performance. The empirical threshold is 0 . 05, which can also be experimented with and adjusted according to specific business data. Hyperparameters of Loss Functions. Regarding the hyperparameter selection of loss functions, the experimental outcomes for \ud835\udefc , \ud835\udefd , and \ud835\udefe are presented in Table 6, Table 7, and Table 8 (Section.3.6 for details). Given that \ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60 \ud835\udc50\ud835\udc52 plays a pivotal role in model optimization, it is advisable to set \ud835\udefc to a relatively substantial value. Conversely, as both \ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60 \ud835\udc50\ud835\udc5c\ud835\udc60 and \ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60 \ud835\udc63\ud835\udc4e\ud835\udc5f serve as auxiliary components in the training process, it is recommended to keep \ud835\udefd and \ud835\udefe small-valued. Hyperparameters of \ud835\udc37\ud835\udc56\ud835\udc50\ud835\udc60 . Based on the experimental findings for temperature coefficients \ud835\udf0f of \ud835\udc37\ud835\udc56\ud835\udc50\ud835\udc60 as shown in Table 10, it is evident that excessively high temperature coefficients result in a shared information for each scene. The control group for this experiment used uniformly distributed experts instead of the original dynamic allocation in AEG. The results showed a certain degree of degradation in AUC (from 0.7860 to 0.7819). Thirdly, the CFR was masked, and the AUC dropped to 0.7840. Finally, we performed ablation experiments on \ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60 \ud835\udc63\ud835\udc4e\ud835\udc5f and \ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60 \ud835\udc50\ud835\udc5c\ud835\udc60 . After adding \ud835\udc37\ud835\udc56\ud835\udc50\ud835\udc60 (\u00b7) , AEG can dynamically allocate experts, and \ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60 \ud835\udc63\ud835\udc4e\ud835\udc5f further enhances the discrimination of the allocation. The experimental results showed that removing the two types of losses decreased 22 BP (Basic Point) and 19 BP in the model AUC, respectively. decline in performance, leading to reduced diversity among various scenes in AEG. Conversely, referencing Eq. 8, overly small temperature coefficients introduce discontinuities in the curve of the \ud835\udc37\ud835\udc56\ud835\udc50\ud835\udc60 function, thereby destabilizing the training process. Hence, a temperature coefficient value of around 0.001 can be selected and appropriately increase the value of \ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60 \ud835\udc63\ud835\udc4e\ud835\udc5f . Hyperparameters of AEM. The experiments regarding the number of experts in AEG ( \ud835\udc41 \ud835\udc4e ) and SEG ( \ud835\udc41 \ud835\udc60 ) can be found in Table 11 and Table 12. It's worth noting that the impact of adding experts is most pronounced when \ud835\udc41 \ud835\udc4e and \ud835\udc41 \ud835\udc60 are relatively small. However, as these values grow larger, the model's computational efficiency decreases, and the gains in performance become less significant. Therefore, it is advisable to strike a reasonable balance between effectiveness and efficiency.", "4.4 Ablation Study and Analysis": "The results of the ablation experiments are shown in Table 9. Firstly, we tested the impact of SRG on the model performance. SRG is mainly responsible for introducing prior knowledge of similar scenes to the model, which is the theoretical basis of SwAN. After removal, other affected structures must be randomly initialized and uniformly distributed. This structure significantly impacts the recommendation performance (the AUC decreased from 0.7860 to 0.7805 after removal). Secondly, we conducted ablation experiments on the AEM structure. This component is responsible for learning how to design the model structure and extracting exclusive and In the research field of recommendation, improving a model to achieve higher AUC than state-of-the-art recommendation approaches is generally recognized to be highly challenging in practice [19, 25, 26]. The comparative experiments in this paper were repeated 10 independent times for validation, and the results were statistically significant at the 0 . 05 level (Friedman test), indicating confidence in the effectiveness of SwAN.", "4.5 Application in Practice": "SwAN has been deployed in the online recommendation system to validate its practical effectiveness and component efficacy. Online Application Performance. Besides the importance in theory, recommendation plays a pivotal role in commercial situations. To further demonstrate the superior performance of SwAN in practical applications, we deployed it in the online recommendation system of Meituan's catering business with an average daily user level of millions, which has typical dynamic multi-scene characteristics, and conducted an A/B test with 20% of the traffic over a period of two months. More than 200 new online scenes were added during the experiment, and the total number exceeded 400. The experimental results showed that SwAN achieved a 5.64% increase in the CTR index compared to the best baseline model (PLE) and a 5.19% increase in daily order volume proportion after full traffic promotion. In addition, we randomly selected 6 new scenes online and calculated the CTR improvements of both SwAN and baseline models relative to the default ranking method (Table 13): First, the baseline models and SwAN have significant CTR enhancements relative to the default ranking. However, SwAN has a smaller Gini coefficient for the improvement ratio between scenes, demonstrating superior stability. We also calculated the Gini coefficient of the improvement ratio of SwAN relative to the baseline model in each scene, which is 0.2651 (0.2351 in all scenes). This value proves that the improvement of SwAN relative to the baseline model is evenly distributed among different scenes instead of only focusing on large scenes and ignoring small ones. Second, combining each scene's daily exposure samples, we found that SwAN has a more significant improvement ratio than \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 the baseline model in small scenes, demonstrating its better generalization ability and competence to optimize cold-start problems for scenes with sparse user behaviors. Visualization of the Expert Selector. To demonstrate the effectiveness of information transfer in Expert Selector, we randomly selected 6 scenes and one sample from each scene. The \ud835\udc4a \ud835\udc58 values in AEG calculated from these samples are shown in Fig. 6. Firstly, it can be seen that the expert selection in different scenes is significantly different, indicating that Expert Selector can distinguish between different scenes. Secondly, from the figure, it can be observed that scene#1 and scene#2 exhibit a high degree of similarity in the selection of experts. Upon examining the actual scene data, we find that both scenes primarily focus on selling afternoon tea. This further confirms that the selection of experts is related to the similarity of the actual scenes. Finally, it can be seen that the model increases the number of selected experts in scene#6 due to the diverse item types, which strengthens its generalization performance.", "4.6 Model Complexity": "All experiments were conducted on NVIDIA Tesla A100 GPU, 80G RAM, and Intel(R) Xeon(R) Gold 5218 CPU servers. Table 14 shows that SwAN maintains a reasonable number of parameters and prediction time. Generally, we retrieve online data in real-time for training and update the model approximately every half hour.", "5 Conclusion": "In this paper, we propose the SwAN model, a novel approach to addressing the cold-start problem in Multi-scene Recommendation (MSR) systems. The proposed model overcomes the limitations of traditional MSR approaches by directly and significantly enhancing the performance of newly-arrived scenes through online prediction. The unique architecture of the SwAN model, which combines the Scene Relation Graph (SRG), Similarity Attention Network (SAN), and Adaptive Ensemble-experts Module (AEM), enables it to capture graph-structured similarities between scenes, understand user behavior transitions, and identify shared logic among different scenes. Our extensive evaluation of SwAN on both public and Meituan industrial datasets demonstrate the superiority of the SwAN model over existing MSR approaches, providing more accurate and high-quality recommendations in a dynamic and adaptable manner. Furthermore, SwAN has been deployed in the online catering recommendation service of Meituan, which serves millions of daily customers, and has achieved a significant improvement in CTR (Click-Through Rate) index. This work represents a significant step forward in developing efficient and adaptable recommendation systems, particularly in the context of a rapidly evolving E-commerce landscape.", "Acknowledgments": "This work was supported in part by the National Key Research and Development Program of China under Grant 2023YFB3307503, in part by the National Natural Science Foundation of China under Grant 62202025, in part by the Young Elite Scientist Sponsorship Program by CAST under Grant YESS20230566, in part by the CCFHuawei Populus Grove Fund, in part by the Frontier Cross Fund Project of Beihang University, and in part by the Fundamental Research Fund Project of Beihang University.", "References": "[1] Jianxin Chang, Chenbin Zhang, Yiqun Hui, Dewei Leng, Yanan Niu, Yang Song, and Kun Gai. 2023. Pepnet: Parameter and embedding personalized network for infusing with personalized prior information. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 3795-3804. [2] Yuting Chen, Yanshi Wang, Yabo Ni, An-Xiang Zeng, and Lanfen Lin. 2020. Scenario-aware and Mutual-based approach for Multi-scenario Recommendation in E-Commerce. In 2020 International Conference on Data Mining Workshops (ICDMW) . IEEE, 127-135. [3] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems . 7-10. [4] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks for youtube recommendations. In Proceedings of the 10th ACM conference on recommender systems . 191-198. [5] Zhengxiao Du, Xiaowei Wang, Hongxia Yang, Jingren Zhou, and Jie Tang. 2019. Sequential scenario-specific meta learner for online recommendation. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 2895-2904. [6] Jyotirmoy Gope and Sanjay Kumar Jain. 2017. A survey on solving cold start problem in recommender systems. In 2017 International Conference on Computing, Communication and Automation (ICCCA) . IEEE, 133-138. [7] Yulong Gu, Wentian Bao, Dan Ou, Xiang Li, Baoliang Cui, Biyu Ma, Haikuan Huang, Qingwen Liu, and Xiaoyi Zeng. 2021. Self-Supervised Learning on Users' Spontaneous Behaviors for Multi-Scenario Ranking in E-commerce. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management . 3828-3837. [8] Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural computation 9, 8 (1997), 1735-1780. [9] Yang Hu, Adriane Chapman, Guihua Wen, and Dame Wendy Hall. 2022. What can knowledge bring to machine learning?-a survey of low-shot learning for structured data. ACM Transactions on Intelligent Systems and Technology (TIST) 13, 3 (2022), 1-45. [10] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014). [11] Pengcheng Li, Runze Li, Qing Da, An-Xiang Zeng, and Lijun Zhang. 2020. Improving multi-scenario learning to rank in e-commerce by exploiting task relationships in the label space. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management . 2605-2612. [12] Wenhao Li, Haiou Zhang, Guilan Wang, Gang Xiong, Meihua Zhao, Guokuan Li, and Runsheng Li. 2023. Deep learning based online metallic surface defect detection method for wire and arc additive manufacturing. Robotics and ComputerIntegrated Manufacturing 80 (2023), 102470. [13] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong Sun. 2018. xdeepfm: Combining explicit and implicit feature interactions for recommender systems. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1754-1763. [14] Greg Linden, Brent Smith, and Jeremy York. 2003. Amazon. com recommendations: Item-to-item collaborative filtering. IEEE Internet computing 7, 1 (2003), 76-80. [15] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. 2018. Modeling task relationships in multi-task learning with multi-gate mixture-ofexperts. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1930-1939. [16] Kiran Rama, Pradeep Kumar, and Bharat Bhasker. 2019. Deep learning to address candidate generation and cold start challenges in recommender systems: A research survey. arXiv preprint arXiv:1907.08674 (2019). [17] Omer Sagi and Lior Rokach. 2018. Ensemble learning: A survey. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 8, 4 (2018), e1249. [18] J Ben Schafer, Dan Frankowski, Jon Herlocker, and Shilad Sen. 2007. Collaborative filtering recommender systems. The adaptive web: methods and strategies of web personalization (2007), 291-324. [19] Xiang-Rong Sheng, Liqin Zhao, Guorui Zhou, Xinyao Ding, Binding Dai, Qiang Luo, Siran Yang, Jingshan Lv, Chi Zhang, Hongbo Deng, et al. 2021. One model to serve all: Star topology adaptive recommender for multi-domain ctr prediction. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management . 4104-4113. [20] Hongyan Tang, Junning Liu, Ming Zhao, and Xudong Gong. 2020. Progressive layered extraction (ple): A novel multi-task learning (mtl) model for personalized recommendations. In Proceedings of the 14th ACM Conference on Recommender Systems . 269-278. [21] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network for ad click predictions. In Proceedings of the ADKDD'17 . 1-7. [22] Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong, and Ed Chi. 2021. Dcn v2: Improved deep & cross network and practical lessons for web-scale learning to rank systems. In Proceedings of the web conference 2021 . 1785-1797. [23] Zheni Zeng, Chaojun Xiao, Yuan Yao, Ruobing Xie, Zhiyuan Liu, Fen Lin, Leyu Lin, and Maosong Sun. 2021. Knowledge transfer via pre-training for recommendation: A review and prospect. Frontiers in big Data 4 (2021), 602071. [24] Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. 2019. Deep learning based recommender system: A survey and new perspectives. ACM computing surveys (CSUR) 52, 1 (2019), 1-38. [25] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Deep interest evolution network for click-through rate prediction. In Proceedings of the AAAI conference on artificial intelligence , Vol. 33. 5941-5948. [26] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through rate prediction. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1059-1068. [27] Jie Zhou, Xianshuai Cao, Wenhao Li, Lin Bo, Kun Zhang, Chuan Luo, and Qian Yu. 2023. Hinet: Novel multi-scenario & multi-task learning with hierarchical information extraction. In 2023 IEEE 39th International Conference on Data Engineering (ICDE) . IEEE, 2969-2975. [28] Jie Zhou, Qian Yu, Chuan Luo, and Jing Zhang. 2023. Feature decomposition for reducing negative transfer: a novel multi-task learning method for recommender system. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 37. [29] Feng Zhu, Yan Wang, Chaochao Chen, Jun Zhou, Longfei Li, and Guanfeng Liu. 2021. Cross-domain recommendation: challenges, progress, and prospects. arXiv preprint arXiv:2103.01696 (2021). [30] Yongchun Zhu, Zhenwei Tang, Yudan Liu, Fuzhen Zhuang, Ruobing Xie, Xu Zhang, Leyu Lin, and Qing He. 2022. Personalized transfer of user preferences for cross-domain recommendation. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining . 1507-1515. [31] Yongchun Zhu, Ruobing Xie, Fuzhen Zhuang, Kaikai Ge, Ying Sun, Xu Zhang, Leyu Lin, and Juan Cao. 2021. Learning to warm up cold item embeddings for coldstart recommendation with meta scaling and shifting networks. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1167-1176."}
