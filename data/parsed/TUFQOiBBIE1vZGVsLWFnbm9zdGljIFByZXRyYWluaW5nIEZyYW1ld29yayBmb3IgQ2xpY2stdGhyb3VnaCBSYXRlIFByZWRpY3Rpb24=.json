{
  "MAP: A Model-agnostic Pretraining Framework for Click-through Rate Prediction": "Jianghao Lin chiangel@sjtu.edu.cn Shanghai Jiao Tong University Shanghai, China",
  "Xinyi Dai": "daixinyi@sjtu.edu.cn Shanghai Jiao Tong University Shanghai, China",
  "Yanru Qu ∗": "kevinqu16@gmail.com Shanghai Jiao Tong University Shanghai, China Ruiming Tang tangruiming@huawei.com Huawei Noah's Ark Lab Shenzhen, China",
  "Weinan Zhang ∗": "wnzhang@sjtu.edu.cn Shanghai Jiao Tong University Shanghai, China",
  "ABSTRACT": "With the widespread application of personalized online services, click-through rate (CTR) prediction has received more and more attention and research. The most prominent features of CTR prediction are its multi-field categorical data format, and vast and daily-growing data volume. The large capacity of neural models helps digest such massive amounts of data under the supervised learning paradigm, yet they fail to utilize the substantial data to its full potential, since the 1-bit click signal is not sufficient to guide the model to learn capable representations of features and instances. The self-supervised learning paradigm provides a more promising pretrain-finetune solution to better exploit the large amount of user click logs, and learn more generalized and effective representations. However, self-supervised learning for CTR prediction is still an open question, since current works on this line are only preliminary and rudimentary. To this end, we propose a M odela gnostic P retraining (MAP) framework that applies feature corruption and recovery on multi-field categorical data, and more specifically, we derive two practical algorithms: masked feature prediction (MFP) and replaced feature detection (RFD). MFP digs into feature interactions within each instance through masking and predicting a small portion of input features, and introduces noise contrastive estimation (NCE) to handle large feature spaces. RFD further turns MFP into a binary classification mode through replacing and detecting changes in input features, making it even simpler and more effective for CTR pretraining. Our extensive experiments on two ∗ Weinan Zhang and Yanru Qu are co-corresponding authors. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. KDD '23, August 6-10, 2023, Long Beach, CA, USA © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0103-0/23/08...$15.00 https://doi.org/10.1145/3580305.3599422 real-world large-scale datasets ( i.e. , Avazu, Criteo) demonstrate the advantages of these two methods on several strong backbones ( e.g. , DCNv2, DeepFM), and achieve new state-of-the-art performance in terms of both effectiveness and efficiency for CTR prediction.",
  "CCS CONCEPTS": "· Information systems → Data mining ; Recommender systems .",
  "KEYWORDS": "CTR Prediction, Self-supervised Learning, Model Pretraining",
  "ACMReference Format:": "Jianghao Lin, Yanru Qu, Wei Guo, Xinyi Dai, Ruiming Tang, Yong Yu, and Weinan Zhang. 2023. MAP: A Model-agnostic Pretraining Framework for Click-through Rate Prediction. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '23), August 6-10, 2023, Long Beach, CA, USA. ACM, New York, NY, USA, 13 pages. https://doi.org/10.1145/3580305.3599422",
  "1 INTRODUCTION": "Click-through rate (CTR) prediction aims to estimate the probability of a user's click [25, 56, 63] given a specific context, and plays a fundamental role in various personalized online services, including recommender systems [62], display advertising [44], web search [9, 12, 31], etc. Traditional CTR models ( e.g. , logistic regression [49] and FM-based models [22, 48]) can only capture low-order feature interactions, which might lead to relatively inferior performance in real-world applications. With the rise of deep learning techniques and the massive amount of user behavior data collected online, many delicate neural CTR models have been proposed to model higher-order feature interactions with different operators ( e.g. , product [14, 24, 29, 45, 46, 59], convolution [28, 32, 33], and attention [27, 50, 64]). These works generally follow a supervised learning paradigm shown in Figure 1(a), where a model is randomly initialized and trained from scratch based on the supervised signals (click or not). Nevertheless, the 1-bit click signal is not sufficient Wei Guo guowei67@huawei.com Huawei Noah's Ark Lab Shenzhen, China Yong Yu yyu@sjtu.edu.cn Shanghai Jiao Tong University Shanghai, China KDD '23, August 6-10, 2023, Long Beach, CA, USA Jianghao Lin and Yanru Qu, et al. Figure 1: The illustration of (a) supervised learning paradigm, and (b) self-supervised learning paradigm. The supervised learning paradigm directly trains a randomly initialized model from scratch without pretraining. The selfsupervised learning paradigm contains two stages, where we first pretrain the model based on the pretext task and then finetune it for the downstream task. Pretrain Train from Scratch Finetune I nitialized Model Pretrained Model Task-specific Model I nitialized Model Task-specific Model (a) Supervised Learning Paradigm (b) Self-supervised Learning Paradigm Target Task Target Task Pretext Task enough for the model to learn capable representations of features and instances, resulting in suboptimal performance. Self-supervised learning provides a more powerful training paradigm to learn more generalized and effective representations of data samples, and proves to be effectual in Natural Language Processing (NLP) [8, 10] and Computer Vision (CV) [20] domains. As shown in Figure 1(b), they usually adopt a pretrain-finetune scheme, where we first pretrain an encoder based on pretext tasks ( i.e. , pretraining tasks), and then finetune a model initialized by the pretrained encoder for downstream tasks based on specific training data with supervised signals. According to the pretext tasks, selfsupervised learning can be mainly classified into two categories: (1) contrastive methods and (2) generative methods [34]. Contrastive methods [2, 5, 7, 13, 20, 42] aim to learn generalized representations from different views or distortions of the same input. Generative methods [3, 10, 19, 67] reconstruct the original input sample from the corrupted one. Self-supervised learning has flourished in the NLP domain to pretrain transformers for unlabeled sentences, making it a perfect mimicry target for sequential recommendations. Various methods are proposed to treat user behavior sequences as sentences, and adopt language models [51] or sentence augmentations [37, 65] for better user or item embeddings. Moreover, many pretraining methods are designed for different types of data formats ( e.g. , graph data [17, 39, 54], or multi-modal data [36, 38]) to further enrich the recommendation family. However, these sequential/graph/multimodal based pretraining methods are essentially incompatible for the CTR data format, i.e. , multi-field categorical data format:  In this paper, we focus on self-supervised pretraining over multifield categorical data for CTR prediction. There exist preliminary works [2, 57] that explore the pretraining methods for CTR data. MF4UIP [57] leverages the BERT framework [10] to predict the masked features for user intent prediction. However, it is nonscalable when the feature space grows, and thus suffers from severe inefficiency problem for industrial applications with million-level feature spaces. SCARF [2] is a contrastive method that adopts SimCLR framework [5] and InfoNCE loss [42] to learn robust representations for each data sample. Its contrastive property requires to calculate representations from different views of the same instance, which doubles the throughput time and memory usage, leading to low efficiency. Moreover, contrastive based methods only provide coarse-grained instance-level supervisions from sample pairs, and therefore might get trapped in representation spaces' early degeneration problem [34], where the model overfits the pretext task too early and loses the ability to generalize. To this end, we propose a M odela gnostic P retraining (MAP) framework that applies feature corruption and recovery towards multi-field categorical data for CTR pretraining. Specifically, we derive two algorithms based on different strategies of corruption and recovery: masked feature prediction (MFP) and replaced feature detection (RFD). MFP requires the model to recover masked features according to corrupted samples, and adopts noise contrastive estimation (NCE) to reduce the computational overhead caused by the large feature space (million level) in CTR data. Moreover, RFD turns MFP into a binary classification mode and requires the model to detect whether each feature in the corrupted sample is replaced or not. Compared with MFP that only utilizes a subset of fields and predict over the entire feature space, RFD is simpler yet more effective and more efficient, which provides fine-grained and more diverse field-wise self-supervised signals. RFD can achieve better CTR performance with fewer parameters, higher throughput rates, and fewer pretraining epochs compared to other pretraining methods. Derived from the MAP framework, MFP and RFD are compatible with any neural CTR models and can promote performance without altering the model structure or inference cost. Main contributions of this paper are concluded as follows: · Wepropose a Model-agnostic Pretraining (MAP) framework that applies feature corruption and recovery on multi-field categorical data. Different pretraining algorithms could be derived by customizing the strategies of corruption and recovery. · We derive a masked feature prediction (MFP) pretraining algorithm from MAP, where the model predicts the original features that are replaced by <MASK> tokens. We also adopt noise contrastive estimation (NCE) to reduce the computational overhead. · We derive a replaced feature detection (RFD) pretraining algorithm from MAP, where the model is required to detect whether the feature of each field is replaced or not. RFD is simpler yet more effective and more efficient, and it can achieve better CTR performance with fewer computational resources. · Extensive experiments on two real-world large-scale datasets validate the advantages of MFP and RFD on several strong backbones, and achieve new state-of-the-art performance in terms of both effectiveness and efficiency for CTR prediction.",
  "2 PRELIMINARIES": "Without loss of generality, the basic form of CTR prediction casts a binary classification problem over multi-field categorical data. Each instance for CTR prediction contains 𝐹 fields with each field taking one single value from multiple categories, and can be represented by { 𝑥 𝑖 , 𝑦 𝑖 } . 𝑥 𝑖 is a sparse one-hot vector as shown in Eq. 1, MAP: A Model-agnostic Pretraining Framework for Click-through Rate Prediction KDD '23, August 6-10, 2023, Long Beach, CA, USA and 𝑦 𝑖 ∈ { 1 , 0 } is the true label (click or not). For simplicity, we build a global feature map of size 𝑀 , and assign a unique feature index for each category, and thus we can represent each sample as 𝑥 𝑖 = [ 𝑥 𝑖, 1 , . . . , 𝑥 𝑖,𝐹 ] , where 𝑥 𝑖,𝑓 ( 𝑓 = 1 , . . . , 𝐹 ) is the index of corresponding feature. CTR models aim to estimate the click probability 𝑃 ( 𝑦 𝑖 = 1 | 𝑥 𝑖 ) for each sample. According to [56, 71], the structure of most recent CTR models can be abstracted as three layers: (1) embedding layer, (2) feature interaction layer, and (3) prediction layer. Embedding layer transforms the sparse binary input 𝑥 𝑖 into dense low-dimensional embedding vectors E = [ 𝑣 1; 𝑣 2; . . . ; 𝑣 𝐹 ] ∈ R 𝐹 × 𝑑 , where 𝑑 is the embedding size, and each feature is represented as a fixed-length vector 𝑣 𝑓 ∈ R 𝑑 . Feature interaction layer , as the main functional module of CTR models, is designed to capture the second- or higher-order feature interactions with various operations ( e.g. , product, attention). This layer produces a compact representation 𝑞 𝑖 based on the dense embedding vectors E for the sample 𝑥 𝑖 . Prediction layer estimates the click probability ˆ 𝑦 𝑖 = 𝑃 ( 𝑦 𝑖 = 1 | 𝑥 𝑖 ) based on the representation 𝑞 𝑖 generated by the feature interaction layer. It is usually a linear layer or an MLP module followed by a sigmoid function:  After the prediction layer, the CTR model is trained in an endto-end manner with the binary cross-entropy loss:  where 𝑁 is the number of training samples.",
  "3 METHODOLOGY": "In this section, we introduce our proposed Model-agnostic Pretraining (MAP) framework, and derive two pretraining algorithms based on different strategies of feature corruption and recovery. The illustration of the framework and algorithms is shown in Figure 2.",
  "3.1 MAP Framework Overview": "We adopt the common pretrain-finetune scheme in self-supervised learning for NLP [8, 10] and CV [20], where we first pretrain a CTR model for a pretext task, and then finetune the pretrained model with click signals. We propose a Model-agnostic Pretraining (MAP) framework for the pretraining stage. The pretext task for the model is to recover the original information ( e.g. , original features, corrupted field index) from the corrupted samples. It is worth noting that MAP is compatible with any neural CTR models, since we only corrupt the input sample ( i.e. , feature corruption layer) and alter the prediction head ( i.e. , feature recovery layer) for the recovery target. Finally, by customizing the design of feature corruption and recovery layers, we derive two specific pretraining algorithms as follows: · Masked feature prediction (MFP) requires the model to recover the original features from the corrupted sample which contains multiple <MASK> tokens. · Replaced feature detection (RFD) tells the model to perform field-wise detection about whether the feature of each field is replaced or not. Hereinafter, we omit the detailed structure of feature interaction layer for certain CTR models, since MFP and RFD are both modelagnostic pretraining algorithms.",
  "3.2 Masked Feature Prediction": "In the masked feature prediction (MFP) pretraining stage, we first corrupt the original input sample 𝑥 𝑖 with a feature masking layer, where we randomly replace a certain proportion of the features with <MASK> tokens. Then, we feed the corrupted sample 𝑥 𝑐 𝑖 through the embedding layer and feature interaction layer to get the compact representation 𝑞 𝑐 𝑖 . Finally, the vector 𝑞 𝑐 𝑖 is inputted to a field-wise prediction layer to predict the original feature for each <MASK> token. To ensure efficiency and practicability, we introduce noise contrastive estimation (NCE) to allow the model to predict among a large feature space ( e.g. , millions of candidate features). 3.2.1 Feature Masking Layer. For an input sample with 𝐹 features ( i.e. , 𝑥 𝑖 = [ 𝑥 𝑖, 1 , . . . , 𝑥 𝑖,𝐹 ] ), we randomly replace a part of the features with <MASK> tokens, resulting in a corrupted sample 𝑥 𝑐 𝑖 . The proportion of features to be masked is a hyperparameter denoted as corrupt ratio 𝛾 . We represent the set of indices of masked fields as I . The <MASK> token is also regarded as a special feature in the embedding table, and it is shared among all the feature fields. That is, we do not maintain field-specific mask tokens, in order to avoid introducing prior knowledge about the masked fields. A harder pretraining task with less prior knowledge can force the model to learn more generalized feature representations, which benefits the downstream task [8, 35]. 3.2.2 Field-wise Prediction Layer. After the embedding layer and feature interaction layer, we obtain the representation 𝑞 𝑐 𝑖 for the corrupted sample 𝑥 𝑐 𝑖 . For each masked feature 𝑥 𝑖,𝑓 of the 𝑓 -th field, we maintain an independent multi-layer perceptron (MLP) network 𝑔 𝑓 followed by a softmax function to compute the predictive probability 𝑝 𝑖,𝑓 ∈ R 𝑀 over the candidate features:   Weexpand the predictive space ( i.e. , candidate features) for every masked field 𝑓 from the field-specific feature space to the global feature space, in order to increase the difficulty of pretext task and thus benefit the downstream CTR prediction task [8, 35]. That is, the model has to select the original feature 𝑥 𝑖,𝑓 out of the whole feature space, which usually contains millions of features in recommender systems. Finally, we view MFP pretraining as a multi-class classification problem and employ the multi-class cross-entropy loss for optimization:  KDD '23, August 6-10, 2023, Long Beach, CA, USA Jianghao Lin and Yanru Qu, et al. Figure 2: The illustration of (a) MAP framework, (b) masked feature prediction (MFP), (c) replaced feature detection (RFD), and (d) finetune. MFP and RFD are derived from the MAP framework by customizing the design of feature corruption and recovery layers. In the finetuning stage, we maintain the same model structure, and load the parameters from the pretrained model to initialize the embedding layer and feature interaction layer. ... ... Student Male T-shirt Beijing Feature Masking Layer Field-wise Prediction Layer Feature Interaction Layer Original Feature Embedding Layer Student MASK T-shirt MASK 0 0 1 ... ... 1 0 0 ... 0 1 0 ... 1 0 0 ... Original Feature ... ... ... Student Male T-shirt Beijing Feature Replacement Layer Field-wise Prediction Layer Feature Interaction Layer Replaced or Not Embedding Layer Student Female Car Beijing 0 0 1 ... ... 0 1 0 ... 0 0 0 ... 1 0 0 ... Replaced or Not ... Replaced or Not Replaced or Not ... Prediction Layer Feature Interaction Layer Embedding Layer Student Male T-shirt Beijing 0 0 1 ... ... 1 0 0 ... 1 0 0 ... 1 0 0 ... Predicted CTR (b) Masked Feature Prediction (c) Replaced Feature Detection (d) Finetune Load ... ... Student Male T-shirt Beijing Feature Corruption Layer Feature Recovery Layer Feature Interaction Layer Embedding Layer Student Corrupted Feature T-shirt Corrupted Feature 0 0 1 ... ... 1 0 0 ... 0 1 0 ... 0 0 0 ... Recovered Result (a) MAP Framework 3.2.3 Noise Contrastive Estimation. The MFP method introduced above is still impractical and extremely expensive, since in Eq. 5 we have to calculate the softmax function over the large global feature space. Such a million-level multi-class classification problem leads to a tremendous amount of memory usage and unacceptable pretraining time cost for real-world applications. To this end, we adopt noise contrastive estimation (NCE) [16, 40, 41] to reduce the softmax overhead. and feature interaction layers, we employ a field-wise prediction layer to detect whether the feature in each field is replaced or not. NCE converts the multi-class classification problem into a binary classification task, where the model tries to distinguish the positive feature ( i.e. , the masked feature 𝑥 𝑖,𝑓 ) from noise features. Specifically, for the 𝑓 -th masked field, we sample 𝐾 noise features from 𝑀 candidate features according to their frequency distribution in the training set. Then, we employ the binary cross-entropy loss:    where 𝑧 𝑖,𝑓 is the output of 𝑓 -th MLP predictor, 𝑡 is the feature index of the positive feature, and 𝜎 is the sigmoid function. In this way, we reduce the complexity of loss calculation from 𝑂 ( 𝑀 ) to 𝑂 ( 𝐾𝑚 ) , where 𝐾𝑚 ≪ 𝑀 , and 𝑚 is the number of masked fields. In our experiment, 𝐾 = 25 is enough to achieve a good CTR performance for the large global feature space ( e.g. , 𝑀 = 4 millions).",
  "3.3 Replaced Feature Detection": "As shown in Figure 2(c), we further propose the replaced feature detection (RFD) algorithm to provide fine-grained and more diverse pretraining signals from all the feature fields, instead of a subset of fields in MFP ( i.e. , the masked fields). In the RFD pretraining stage, we first corrupt the original input sample 𝑥 𝑖 by a feature replacement layer, where we randomly replace a certain proportion of the features with other features. Then, after obtaining the compact representation 𝑞 𝑐 𝑖 from the embedding 3.3.1 Feature Replacement Layer. For an input sample with 𝐹 features ( i.e. , 𝑥 𝑖 = [ 𝑥 𝑖, 1 , . . . , 𝑥 𝑖,𝐹 ] ), we randomly replace a part of the features, and denote the set of indices of replaced fields as I . The proportion of features to be replaced is a hyperparameter represented as corrupt ratio 𝛾 . Next, we replace each of these selected features by a random sampling from the empirical marginal distribution ( i.e. , sample from the field-specific feature space by the feature frequency distribution) of the corresponding field ˆ F 𝑓 in the training set, resulting in the corrupted sample 𝑥 𝑐 𝑖 . 3.3.2 Field-wise Prediction Layer. Similar to MFP, we obtain the representation 𝑞 𝑐 𝑖 for the corrupted sample 𝑥 𝑐 𝑖 through the embedding layer and feature interaction layers. Then, we feed 𝑞 𝑐 𝑖 to an MLP predictor followed by an element-wise sigmoid function, resulting in an 𝐹 -length predictive vector 𝑝 𝑖 :  where 𝑝 𝑖,𝑓 ( 𝑓 = 1 , . . . , 𝐹 ) denotes the probability that the feature in the 𝑓 -th field is replaced in the feature replacement layer. Finally, we employ the binary cross-entropy loss for RFD pretraining:  where 𝑟 𝑖,𝑓 ∈ { 1 , 0 } ( 𝑓 = 1 , . . . , 𝐹 ) is the label indicating whether the feature in the 𝑓 -th field of sample 𝑥 𝑖 is replaced or not. Apparently, RFD serves as a simpler pretraining algorithm compared with MFP which requires NCE to reduce the computational overhead. While MFP only utilizes the masked fields (the corrupt ratio 𝛾 is usually 10% - 30%) as self-supervised signals, RFD involves all the feature fields to provide more sufficient and more diverse signal guidance. We will evaluate the effectiveness and efficiency of RFD later in Section 4.2 and Section 4.3, respectively. MAP: A Model-agnostic Pretraining Framework for Click-through Rate Prediction KDD '23, August 6-10, 2023, Long Beach, CA, USA",
  "3.4 Complexity Analysis": "We analyze the time complexity of our proposed MFP and RFD, as well as two baseline pretraining algorithms ( i.e. , MF4UIP and SCARF). We only analyze the component above the feature interaction layer ( i.e. , prediction layer and loss calculation) due to their model-agnostic property. Suppose the batch size is 𝐵 , and we adopt a linear layer 𝑓 : R 𝑙 → R 𝑛 to transform the compact representation 𝑞 𝑖 (or 𝑞 𝑐 𝑖 ) to the final predictive place. The time complexity of MF4UIP is 𝑂 ( 𝐵𝑙𝑛 + 𝐵𝑀 ) , where the main overhead of MF4UIP is the softmax computation over the million-level feature space of size 𝑀 . By adopting NCE, the time complexity of MFP reduces to 𝑂 ( 𝐵𝑙𝑛 + 𝐵𝐾𝑚 ) , where 𝐾𝑚 ≪ 𝑀 , and 𝑚 is the number of masked features. By introducing the binary classification mode, the time complexity of RFD further reduces to 𝑂 ( 𝐵𝑙𝑛 + 𝐵𝐹 ) , where 𝐹 is the number of feature fields. Besides, as a contrastive algorithm with InfoNCE loss, SCARF has a quadratic complexity over the batch size: 𝑂 ( 2 𝐵𝑙𝑛 + 4 𝐵 2 𝑛 ) . In summary, MF4UIP and SCARF are non-scalable in terms of feature space 𝑀 and batch size 𝐵 , respectively. MFP and RFD can achieve lower complexity and is scalable for industrial applications with million-level feature space and large batch size.",
  "4 EXPERIMENT": "In this section, we conduct extensive experiments to answer the following research questions: RQ1 Can MFP and RFD improve the predictive performance for various base CTR models? RQ2 How do MFP and RFD perform compared with existing CTR pretraining methods? RQ3 Does RFD improve the pretraining efficiency compared with other pretraining methods? RQ4 What are the influences of different pretraining configurations for MFP and RFD?",
  "4.1 Experiment Setup": "4.1.1 Datasets. We conduct extensive experiments on two largescale CTR prediction benchmarks, i.e. , Avazu and Criteo datasets. Both datasets are divided into training, validation, and test sets with proportion 8:1:1. The basic statistics of these two datasets are summarized in Table 1. Note that the training set for the pretraining stage and finetuning stage are the same, in order to make full use of the large-scale datasets. We describe the preprocessing for the two datasets as follows: · Avazu originally contains 23 fields with categorical features. We remove the id field that has a unique value for each data sample, and transform the timestamp field into four new fields: weekday , day_of_month , hour_of_day , and is_weekend , resulting in 25 fields. We remove the features that appear less than 2 times and replace them with a dummy feature <Unknown>. · Criteo includes 26 anonymous categorical fields and 13 numerical fields. We discretize numerical features and transform them into categorical features by log transformation 1 . We remove the features that appear less than 10 times and replace them with a dummy feature <Unknown>. 1 https://www.csie.ntu.edu.tw/~r01922136/kaggle-2014-criteo.pdf Table 1: The dataset statistics 4.1.2 Evaluation Metrics. To evaluate the performance of CTR prediction methods, we adopt AUC (Area under the ROC curve) and Log Loss (binary cross-entropy loss) as the evaluation metrics. Slightly higher AUC or lower Log Loss ( e.g. , 0.001) can be regarded as significant improvement in CTR prediction [29, 56, 59] 4.1.3 Base Models & Baselines. We evaluate the self-supervised pretraining methods on various base CTR models with three different feature interaction operators: (1) product operator, including DeepFM [14], xDeepFM [29], and DCNv2 [59]; (2) convolutional operator, including FiGNN [28] and FGCNN[32]; (3) attention operator, including AutoInt [50] and Transformer [53]. Additionally, we adopt the classical DNN model, which proves to be a strong base model for self-supervised learning in our experiments. We compare our proposed MFP and RFD with two existing pretraining methods: MF4UIP [57] and SCARF [2], which are chosen as representative generative and contrastive algorithms, respectively. 4.1.4 Implementation Details. Weprovide detailed implementation details in the supplementary material ( i.e. , Appendix A). The code is available 2 .",
  "4.2 Effectiveness Comparison (RQ1 & RQ2)": "We apply five training schemes on each base model, and report the results in Table 2. In the 'Scratch' scheme, we train the randomly initialized base model from scratch ( i.e. , supervised learning). In other schemes, we first pretrain the base model according to the corresponding method, and then finetune the pretrained model for CTR prediction ( i.e. , self-supervised learning). From Table 2, we can obtain the following observations: · All the pretrain-finetune schemes can improve the performance of base model on both metrics by a large margin compared with the 'Scratch' scheme, which demonstrates the effectiveness of self-supervised learning for CTR prediction. · The product based CTR models ( e.g. , DCNv2), together with the DNN model, win the top places among the base models, when equipped with self-supervised learning. · Among the pretrain-finetune schemes, MFP and RFD generally gain significant improvement over the two baseline pretraining methods except for few cases, and RFD can consistently achieve the best performance. In addition to the performance comparison above, we also find some interesting phenomena as follows: · The pretrain-finetune scheme can greatly reduce the demand on model structure design for CTR prediction . To our surprise, although DNN model is inferior under the 'Scratch' scheme, it gains huge improvement under the pretrain-finetune schemes (especially RFD) and wins the second place, outperforming a 2 The PyTorch implementation is available at: https://github.com/CHIANGEL/MAPCODE. The MindSpore implementation is available at: https://gitee.com/mindspore/ models/tree/master/research/recommend/MAP KDD '23, August 6-10, 2023, Long Beach, CA, USA Jianghao Lin and Yanru Qu, et al. Table 2: The AUC and Log Loss (LL) performance of different base CTR models under different training schemes. We give the relative performance improvement of each pretrain-finetune scheme over the 'scratch' scheme. The best result for each base model is given in bold, while the second-best value is underlined. The symbol * indicates statistically significant improvement of our proposed MFP and RFD schemes over the two baseline schemes with 𝑝 < 0 . 001 . range of carefully designed CTR models. Such a phenomenon suggests that a simple MLP structure is capable of capturing useful feature crossing patterns from the multi-field categorical data with the help of self-supervised signals. · The two training paradigms (supervised learning V.S. self-supervised learning) favor different types of model structures . Previous works always seek better designs of model structures under the 'Scratch' scheme. However, we can observe several counterexamples that a good model under the 'Scratch' scheme is relatively bad for pretrain-finetune schemes ( e.g. , FGCNN), or a bad model under the 'Scratch' scheme achieves competitive performance for pretrain-finetune schemes ( e.g. , DNN). The above two phenomena bring about the research question of what model structures are more economic and effective for the MAP: A Model-agnostic Pretraining Framework for Click-through Rate Prediction KDD '23, August 6-10, 2023, Long Beach, CA, USA Figure 3: The model size (Top) and run time per epoch (Bottom) of different pretraining methods. We perform logarithmic scale on the 𝑦 axis, and denote the original value on the top of each bar. The experiment is conducted on the same server with one GeForce RTX 3090 GPU. We only consider the learning parameters above the embedding layer for model size . We consider the whole pretraining loop for run time per epoch , including the corruption operations as well as the backpropagation. DNN AutoInt Trans. FiGNN FGCNN DeepFM xDeepFM DCNv2 10 0 10 2 10 4 #Params 1589 1475 1476 1476 1594 1580 1596 1773 113 2 2 3 67 111 127 292 1589 1475 1476 1477 1594 1580 1596 1773 128 14 15 15 133 119 135 312 #Params ( ×10 5 ) on Avazu Dataset DNN AutoInt Trans. FiGNN FGCNN DeepFM xDeepFM DCNv2 10 -1 10 1 10 3 489 390 375 375 570 620 492 1051 106 1 2 1 40 237 107 653 489 390 375 375 570 620 492 1051 130 332 17 17 212 261 133 693 #Params ( ×10 3 ) on Criteo Dataset MF4UIP SCARF MFP RFD DNN AutoInt Trans. FiGNN FGCNN DeepFM xDeepFM DCNv2 10 0 10 1 10 2 10 3 Run Time per Epoch 494 608 565 630 605 498 471 511 40 43 46 78 69 39 54 50 15 15 17 35 26 15 24 19 9 11 12 34 21 9 18 14 Run Time per Epoch ( minute ) on Avazu Dataset DNN AutoInt Trans. FiGNN FGCNN DeepFM xDeepFM DCNv2 10 0 10 1 10 2 89 88 113 164 126 98 104 104 36 37 63 117 54 39 63 55 8 9 23 55 17 11 25 19 4 5 18 50 11 6 21 15 Run Time per Epoch ( minute ) on Criteo Dataset MF4UIP SCARF MFP RFD pretrain-finetune schemes. We give one conjecture about this topic here, and leave further studies as future works. Our hypothesis is that the pretrain-finetune scheme might prefer the bit-wise feature interaction ( e.g. , DCNv2) to the field-wise feature interaction ( e.g. , Transformer). The bit-wise feature interaction enables larger model capacity to learn better feature crossing patterns during the pretraining stage.",
  "4.3 Efficiency Analysis (RQ3)": "After validating the effectiveness of our proposed MFP and RFD, we conduct experiments to further analyze the efficiency of these pretraining methods from the following two perspectives: RQ3.1 What is the complexity of each pretraining method? RQ3.2 How many pretraining epochs should a method takes to achieve a certain performance ( i.e. , sample efficiency)? For RQ3.1 , we have already provided the complexity analysis in Section 3.4. Following [56], we further empirically compare the model size and run time per epoch of different pretraining methods for different base CTR models in Figure 3. The experiments are conducted on the same server with one GeForce RTX 3090 GPU. For fair comparison, we launch one pretraining at a time as a single process to exclusively possess all the computational resources. We maintain the same structure of each base model for the four pretraining algorithms, and set the corrupt ratio 𝛾 = 0 . 3. Since different pretraining algorithms require different amounts of dynamic GPU memory, we choose a proper batch size from { 256 , 512 , 1024 , 2048 , 4096 } to make full use of GPU memory. From Figure 3, we can obtain the following observations: 0 10 20 30 40 50 60 Pretraining Epoch 0.7960 0.7980 0.8000 0.8020 0.8040 AUC AUC of DCNv2 on Avazu dataset. RFD MFP SCARF 0 10 20 30 40 50 60 Pretraining Epoch 0.8110 0.8120 0.8130 0.8140 0.8150 0.8160 0.8170 AUC AUC of DCNv2 on Criteo dataset. RFD MFP SCARF 0 10 20 30 40 50 60 Pretraining Epoch 0.7920 0.7940 0.7960 0.7980 0.8000 0.8020 AUC AUC of DNN on Avazu dataset. RFD MFP SCARF 0 10 20 30 40 50 60 Pretraining Epoch 0.8100 0.8110 0.8120 0.8130 0.8140 0.8150 AUC AUC of DNN on Criteo dataset. RFD MFP SCARF 0 10 20 30 40 50 60 Pretraining Epoch 0.7920 0.7940 0.7960 0.7980 0.8000 AUC AUC of DeepFM on Avazu dataset. RFD MFP SCARF 0 10 20 30 40 50 60 Pretraining Epoch 0.8100 0.8110 0.8120 0.8130 0.8140 AUC AUC of DeepFM on Criteo dataset. RFD MFP SCARF · SCARF is relatively time-consuming, though it has minimal parameters. The reason is that SCARF, as a contrastive method, requires computing representations of different views from the same instance, which doubles the run time of training loops. · Although MFP maintains similar amount of learning parameters compared with MF4UIP, it approximately exhibits an 18 × speedup over MF4UIP in terms of run time per epoch since we adopt NCE to resolve the softmax overhead for million-level predictive spaces. · RFD is the most efficient pretraining algorithm with the lowest complexity. It has relatively fewer learning parameters and the Figure 4: The AUC performance of three representative models (DCNv2, DNN, DeepFM) with different pretraining epochs on Avazu (left column) and Criteo (right column) datasets. We use black dashed lines to illustrate how many epochs should RFD take to achieve a dominant performance over other methods. lowest run time per epoch, showing its simplicity and practicability for industrial applications. Next, to study RQ3.2 , we investigate the sample efficiency and give the AUC performance of base models under different pretraining epochs in Figure 4. We choose DCNv2, DNN, and DeepFM as the representative base models due to the page limitation. MF4UIP is excluded due to its tremendous cost of time per epoch. Note that zero pretraining epoch indicates that we train the model from scratch KDD '23, August 6-10, 2023, Long Beach, CA, USA Jianghao Lin and Yanru Qu, et al. Figure 5: The hyperparameter study on corrupt ratio 𝛾 . We give the AUC and negative log loss performance of DNN, DCNv2 and DeepFM with MFP and RFD methods on Avazu (left two columns) and Criteo (right two columns) datasets. 0.1 0.2 0.3 0.4 0.5 Corrupt Ratio γ 0.8000 0.8002 0.8004 0.8006 0.8008 0.8010 AUC DNN on Avazu with MFP AUC -0.3680 -0.3678 -0.3676 -0.3674 -0.3672 -0.3670 -0.3668 Negative Log Loss Negative Log Loss 0.1 0.2 0.3 0.4 0.5 Corrupt Ratio γ 0.8006 0.8008 0.8010 0.8012 0.8014 0.8016 0.8018 AUC DNN on Avazu with RFD AUC -0.3674 -0.3672 -0.3670 -0.3668 -0.3666 -0.3664 Negative Log Loss Negative Log Loss 0.1 0.2 0.3 0.4 0.5 Corrupt Ratio γ 0.8130 0.8135 0.8140 0.8145 0.8150 AUC DNN on Criteo with MFP AUC -0.4385 -0.4380 -0.4375 -0.4370 Negative Log Loss Negative Log Loss 0.1 0.2 0.3 0.4 0.5 Corrupt Ratio γ 0.8140 0.8142 0.8145 0.8147 0.8150 0.8152 0.8155 AUC DNN on Criteo with RFD AUC -0.4378 -0.4375 -0.4373 -0.4370 -0.4368 -0.4365 -0.4363 Negative Log Loss Negative Log Loss 0.1 0.2 0.3 0.4 0.5 Corrupt Ratio γ 0.8010 0.8015 0.8020 0.8025 0.8030 0.8035 AUC DCNv2 on Avazu with MFP AUC -0.3670 -0.3668 -0.3665 -0.3662 -0.3660 -0.3658 -0.3655 Negative Log Loss Negative Log Loss 0.1 0.2 0.3 0.4 0.5 Corrupt Ratio γ 0.8020 0.8025 0.8030 0.8035 0.8040 AUC DCNv2 on Avazu with RFD AUC -0.3665 -0.3662 -0.3660 -0.3658 -0.3655 -0.3652 -0.3650 Negative Log Loss Negative Log Loss 0.1 0.2 0.3 0.4 0.5 Corrupt Ratio γ 0.8160 0.8161 0.8162 0.8163 0.8164 0.8165 0.8166 AUC DCNv2 on Criteo with MFP AUC -0.4360 -0.4359 -0.4358 -0.4357 -0.4356 -0.4355 -0.4354 -0.4353 Negative Log Loss Negative Log Loss 0.1 0.2 0.3 0.4 0.5 Corrupt Ratio γ 0.8158 0.8160 0.8162 0.8164 0.8166 AUC DCNv2 on Criteo with RFD AUC -0.4362 -0.4360 -0.4358 -0.4356 -0.4354 Negative Log Loss Negative Log Loss 0.1 0.2 0.3 0.4 0.5 Corrupt Ratio γ 0.7990 0.7992 0.7994 0.7996 0.7998 0.8000 AUC DeepFM on Avazu with MFP AUC -0.3684 -0.3682 -0.3680 -0.3678 -0.3676 Negative Log Loss Negative Log Loss 0.1 0.2 0.3 0.4 0.5 Corrupt Ratio γ 0.8000 0.8002 0.8004 0.8006 0.8008 0.8010 0.8012 AUC DeepFM on Avazu with RFD AUC -0.3680 -0.3678 -0.3675 -0.3672 -0.3670 -0.3668 -0.3665 Negative Log Loss Negative Log Loss 0.1 0.2 0.3 0.4 0.5 Corrupt Ratio γ 0.8120 0.8122 0.8124 0.8126 0.8128 AUC DeepFM on Criteo with MFP AUC -0.4405 -0.4400 -0.4395 -0.4390 -0.4385 Negative Log Loss Negative Log Loss 0.1 0.2 0.3 0.4 0.5 Corrupt Ratio γ 0.8130 0.8132 0.8135 0.8137 0.8140 0.8142 AUC DeepFM on Criteo with RFD AUC -0.4384 -0.4382 -0.4380 -0.4378 -0.4376 Negative Log Loss Negative Log Loss without pretraining. In Figure 4, we can observe that MFP and RFD consistently achieve better performance over SCARF under different pretraining epochs. Moreover, as illustrated by the black dashed lines, RFD can simply achieve the best performance with limited pretraining epochs (10 ∼ 30), showing its superior sample efficiency for CTR prediction. Table 3: The hyperparameter study on the number of noise features 𝐾 in NCE for MFP. Different 𝐾 s only result in 0.0003 performance fluctuation on each model. In summary, we validate the pretraining efficiency of our proposed methods (especially RFD), i.e. , they can achieve better CTR performance with fewer learning parameters, higher throughput rates, and fewer pretraining epochs.",
  "4.4 Ablation & Hyperparameter Study (RQ4)": "In this section, we analyze the impact of hyperparameters or components in MFP and RFD, including the corrupt ratio 𝛾 for both MFP and RFD, the number of noise features 𝐾 in NCE for MFP, and the feature replacement strategy for RFD. Similarly, we select DNN, DCNv2 and DeepFM as the representative base models due to the page limitation. 4.4.1 Corrupt Ratio 𝛾 . We select the value of corrupt ratio 𝛾 from { 0 . 1 , 0 . 2 , 0 . 3 , 0 . 4 , 0 . 5 } , and show the impact in Figure 5. Both MFP and RFD favor a small corrupt ratio ( i.e. , 0.1 ∼ 0.3). The reason is that the over-corruption caused by a large corrupt ratio may change the sample semantics and disturb the model pretraining. 4.4.2 The Number of Noise Samples 𝐾 . We select the number of noise samples 𝐾 in NCE for MFP from { 10 , 25 , 50 , 75 , 100 } , and show the impact in Table 3. Surprisingly, the performance fluctuations of both metrics ( i.e. , AUC and log loss) brought by different 𝐾 s are all within 0.0003, indicating that MFP is not sensitive to the number of noise samples 𝐾 in NCE. A small number of noise features (10 noise features out of the million-level feature space) is sufficient for the model to learn effective feature crossing patterns and benefit the final CTR performance. 4.4.3 Feature Replacement Strategy. We investigate the impact of the feature replacement strategy in RFD, which needs to sample a replacer for the original feature. We compare four different replacement strategy variants shown in Table 4, and give the results in Table 5. We observe that sampling by the feature frequency distribution is relatively better than uniform sampling. Moreover, sampling from the global feature space can greatly hurt the CTR performance, MAP: A Model-agnostic Pretraining Framework for Click-through Rate Prediction KDD '23, August 6-10, 2023, Long Beach, CA, USA Table 4: The feature replacement strategy variants for RFD pretraining method. It is worth noting that our proposed RFD adopts the field-frequency strategy. Table 5: The ablation study on feature replacement strategy for RFD. F-F, F-U, G-F, G-U is short for field-frequency, field-uniform, global-frequency, global-uniform strategies, respectively. The best results are given in bold, while the second-best values are underlined. since an out-of-field replacer feature forms a simplistic pretext task where the model can easily detect the replaced field. The model might overfit the easy pretext task during the pretraining, and thus lose the generalization ability for downstream CTR prediction task.",
  "5 RELATED WORK": "",
  "5.1 Click-through Rate Prediction": "The click-through rate (CTR) prediction serves as a core function module in various personalized online services, including online advertising, recommender systems, and web search, etc [30, 71]. With the rise of deep learning, many deep neural CTR models have been recently proposed. The core idea of them is to capture the feature interactions, which indicates the combination relationships of multiple features. The deep CTR models usually leverage both implicit and explicit feature interactions. While the implicit feature interactions are captured by a deep neural network (DNN), the explicit feature interactions are modeled by a specially designed learning function. According to the explicit feature interaction operators, these deep CTR models can be mainly classified into three categories: (1) product operator, (2) convolutional operator, and (3) attention operator. Product Operator . The product-based CTR models originate from classical shallow models such as FM [47] and POLY2 [4]. FFM [26] and NFM [21] are variants of FM, where the secondorder feature interactions are captured by the inner product of feature embeddings. DeepFM [14] and PNN [45] combine the FM layer with DNNs for higher-order feature interactions. PIN [46] further extends PNN by introducing a network-in-network structure to replace the inner product interaction function. FiBiNET [24] introduces the SENET mechanism [23] to learn the weights of features dynamically before product interactions. Moreover, DCN [58], xDeepFM [29], DCNv2 [59] are proposed for the explicit high-order feature interaction modeling by applying product-based feature interactions at each layer explicitly. Therefore, the order of feature interactions to be modeled increases at each layer and is determined by the layer depth. Convolutional Operator . Apart from the product operator, Convolutional Neural Networks (CNN) and Graph Convolutional Networks (GCN) are also explored for feature interaction modeling in CTR prediction. CCPM [33] is the first work adopts the CNN module for CTR prediction. However, CCPM can only learn part of feature interactions between adjacent features since it is sensitive to the field order. FGCNN [32] improves CCPM by introducing a recombination layer to model non-adjacent features. FiGNN [28] treats the multi-field categorical data as a fully connected graph, where each field serves as a graph node and feature interactions are captured via graph propagation. Attention Operator . AFM [64] improves FM by leveraging an additional attention network to allow feature interactions to contribute differently to the final CTR prediction. AutoInt [50] utilizes a multi-head self-attentive neural network with residual connections to explicitly model the feature interactions with different orders. InterHAt [27] combines a transformer network with multiple attentional aggregation layers for feature interaction learning. These attention-based CTR models can also provide explainable prediction via attention weights.",
  "5.2 Self-supervised Learning": "Self-supervised learning has achieved great success in Nature Language Processing (NLP) [8, 10] and Computer Vision (CV) [20]. They usually adopt a pretrain-finetune scheme, where we first pretrain an encoder based on pretext tasks, and then finetune a model initialized by the pretrained encoder for downstream tasks. According to the pretext task, self-supervised learning can be mainly classified into two categories: (1) contrastive methods and (2) generative methods. Contrastive methods learn a latent space to draw positive samples together ( e.g. , different views of the same image) and push apart negative samples ( e.g. , images from different categories) [11]. Numerous techniques are proposed to promote the performance of contrastive methods such as data augmentation [18, 70], contrastive losses [5, 6], momentum encoders [7, 13, 20], and memory banks [52, 61]. Generative methods [3, 10, 19, 67] reconstruct the original input sample from the corrupted one. For example, BERT [10] requires the model to recover the masked token from the corrupted sentences. Besides, ELECTRA [8] adopts an adversarial structure and pretrains the model as a discriminator to predict corrupted tokens, which is proven to be more sample-efficient. In recommender systems, many works apply self-supervised learning to user behavior sequences [15, 37, 51, 60, 65], manually KDD '23, August 6-10, 2023, Long Beach, CA, USA Jianghao Lin and Yanru Qu, et al. designed graph data [17, 39, 54], or multi-modal data [36, 38]. They explore the pretraining methods for better representations to further enhance the recommendation performance with enriched side information. However, these sequential/graph/multi-modal methods are essentially incompatible with the CTR data, i.e. , multi-field categorical data format. In CTR prediction, there exist works [43, 55, 68] that incorporate the self-supervised signals in a semi-supervised manner, where the cross-entropy loss is jointly optimized with an auxiliary loss in one stage. As for CTR pretraining methods, VIME [69] proposes a semi-supervised learning algorithm to learn a predictive function based on the frozen pretrained encoder. MF4UIP [57] leverages the BERT framework [10] for user intent prediction. SCARF [2] adopts SimCLR framework [5] and InfoNCE loss [42] to pretrain the model in a contrastive manner. Compared with these works, our proposed MFP and RFD methods are more scalable for industrial applications and achieve the state-of-art performance in terms of both effectiveness and efficiency for CTR prediction.",
  "6 CONCLUSION": "In this paper, we propose a Model-agnostic Pretraining (MAP) framework that applies feature corruption and recovery on multifield categorical data for CTR prediction. Based on different strategies of corruption and recovery, we derive two practical algorithms: masked feature prediction (MFP), and replaced feature detection (RFD). Extensive experiments show that MFP and RFD achieve new state-of-the-art performance in terms of both effectiveness and efficiency for CTR prediction. For future work, a promising direction is to explore what model structures are more suitable for self-supervised paradigm, since we find different models receive quite different performance gains combined with self-supervised learning in Section 4.2. Furthermore, we will investigate on the possible saturation of downstream CTR performance as the pretraining volume grows ( e.g. , from million to billion or even more).",
  "ACKNOWLEDGMENTS": "The SJTU team is supported by Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102) and National Natural Science Foundation of China (62177033). The work is also sponsored by Huawei Innovation Research Program. We thank MindSpore [1] for the partial support of this work.",
  "REFERENCES": "[1] 2020. MindSpore. https://www.mindspore.cn/ [2] Dara Bahri, Heinrich Jiang, Yi Tay, and Donald Metzler. 2021. Scarf: Selfsupervised contrastive learning using random feature corruption. arXiv preprint arXiv:2106.15147 (2021). [3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877-1901. [4] Yin-Wen Chang, Cho-Jui Hsieh, Kai-Wei Chang, Michael Ringgaard, and Chih-Jen Lin. 2010. Training and testing low-degree polynomial data mappings via linear SVM. Journal of Machine Learning Research 11, 4 (2010). [5] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A simple framework for contrastive learning of visual representations. In International conference on machine learning . PMLR, 1597-1607. [6] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E Hinton. 2020. Big self-supervised models are strong semi-supervised learners. Advances in neural information processing systems 33 (2020), 22243-22255. [7] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. 2020. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297 (2020). [8] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. 2020. Electra: Pre-training text encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555 (2020). [9] Xinyi Dai, Jianghao Lin, Weinan Zhang, Shuai Li, Weiwen Liu, Ruiming Tang, Xiuqiang He, Jianye Hao, Jun Wang, and Yong Yu. 2021. An adversarial imitation click model for information retrieval. In Proceedings of the Web Conference 2021 . 1809-1820. [10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018). [11] Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman. 2021. With a little help from my friends: Nearest-neighbor contrastive learning of visual representations. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 9588-9597. [12] Lingyue Fu, Jianghao Lin, Weiwen Liu, Ruiming Tang, Weinan Zhang, Rui Zhang, and Yong Yu. 2023. An F-shape Click Model for Information Retrieval on Multiblock Mobile Pages. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining . 1057-1065. [13] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. 2020. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems 33 (2020), 21271-21284. [14] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. Deepfm: a factorization-machine based neural network for ctr prediction. In IJCAI . [15] Wei Guo, Can Zhang, Zhicheng He, Jiarui Qin, Huifeng Guo, Bo Chen, Ruiming Tang, Xiuqiang He, and Rui Zhang. 2022. Miss: Multi-interest self-supervised learning framework for click-through rate prediction. In 2022 IEEE 38th International Conference on Data Engineering (ICDE) . IEEE, 727-740. [16] Michael Gutmann and Aapo Hyvärinen. 2010. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of the thirteenth international conference on artificial intelligence and statistics . JMLR Workshop and Conference Proceedings, 297-304. [17] Bowen Hao, Jing Zhang, Hongzhi Yin, Cuiping Li, and Hong Chen. 2021. Pretraining graph neural networks for cold-start users and items representation. In Proceedings of the 14th ACM International Conference on Web Search and Data Mining . 265-273. [18] Xiaoshuai Hao, Yi Zhu, Srikar Appalaraju, Aston Zhang, Wanqian Zhang, Bo Li, and Mu Li. 2022. MixGen: A New Multi-Modal Data Augmentation. arXiv preprint arXiv:2206.08358 (2022). [19] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. 2022. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 16000-16009. [20] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition . 9729-9738. [21] Xiangnan He and Tat-Seng Chua. 2017. Neural factorization machines for sparse predictive analytics. In SIGIR . 355-364. [22] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural collaborative filtering. In WWW . 173-182. [23] Jie Hu, Li Shen, and Gang Sun. 2018. Squeeze-and-excitation networks. In Proceedings of the IEEE conference on computer vision and pattern recognition . 7132-7141. [24] Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. 2019. FiBiNET: combining feature importance and bilinear feature interaction for click-through rate prediction. In Proceedings of the 13th ACM Conference on Recommender Systems . 169-177. [25] Yanhua Huang, Hangyu Wang, Yiyun Miao, Ruiwen Xu, Lei Zhang, and Weinan Zhang. 2022. Neural Statistics for Click-Through Rate Prediction. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1849-1853. [26] Yuchin Juan, Yong Zhuang, Wei-Sheng Chin, and Chih-Jen Lin. 2016. Field-aware factorization machines for CTR prediction. In RecSys . 43-50. [27] Zeyu Li, Wei Cheng, Yang Chen, Haifeng Chen, and Wei Wang. 2020. Interpretable click-through rate prediction through hierarchical attention. In Proceedings of the 13th International Conference on Web Search and Data Mining . 313-321. [28] Zekun Li, Zeyu Cui, Shu Wu, Xiaoyu Zhang, and Liang Wang. 2019. Fi-gnn: Modeling feature interactions via graph neural networks for ctr prediction. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management . 539-548. [29] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong Sun. 2018. xdeepfm: Combining explicit and implicit feature interactions for recommender systems. In KDD . 1754-1763. [30] Jianghao Lin, Xinyi Dai, Yunjia Xi, Weiwen Liu, Bo Chen, Xiangyang Li, Chenxu Zhu, Huifeng Guo, Yong Yu, Ruiming Tang, et al. 2023. How Can Recommender Systems Benefit from Large Language Models: A Survey. arXiv preprint arXiv:2306.05817 (2023). MAP: A Model-agnostic Pretraining Framework for Click-through Rate Prediction KDD '23, August 6-10, 2023, Long Beach, CA, USA [31] Jianghao Lin, Weiwen Liu, Xinyi Dai, Weinan Zhang, Shuai Li, Ruiming Tang, Xiuqiang He, Jianye Hao, and Yong Yu. 2021. A Graph-Enhanced Click Model for Web Search. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1259-1268. [32] Bin Liu, Ruiming Tang, Yingzhi Chen, Jinkai Yu, Huifeng Guo, and Yuzhou Zhang. 2019. Feature generation by convolutional neural network for click-through rate prediction. In WWW . 1119-1129. [33] Qiang Liu, Feng Yu, Shu Wu, and Liang Wang. 2015. A Convolutional Click Prediction Model. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management . ACM, 1743-1746. [34] Xiao Liu, Fanjin Zhang, Zhenyu Hou, Li Mian, Zhaoyu Wang, Jing Zhang, and Jie Tang. 2021. Self-supervised learning: Generative or contrastive. IEEE Transactions on Knowledge and Data Engineering (2021). [35] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019). [36] Yong Liu, Susen Yang, Chenyi Lei, Guoxin Wang, Haihong Tang, Juyong Zhang, Aixin Sun, and Chunyan Miao. 2021. Pre-training graph transformer with multimodal side information for recommendation. In Proceedings of the 29th ACM International Conference on Multimedia . 2853-2861. [37] Zhiwei Liu, Yongjun Chen, Jia Li, Philip S Yu, Julian McAuley, and Caiming Xiong. 2021. Contrastive self-supervised sequential recommendation with robust augmentation. arXiv preprint arXiv:2108.06479 (2021). [38] Zhuang Liu, Yunpu Ma, Matthias Schubert, Yuanxin Ouyang, and Zhang Xiong. 2022. Multi-Modal Contrastive Pre-training for Recommendation. In Proceedings of the 2022 International Conference on Multimedia Retrieval . 99-108. [39] Zaiqiao Meng, Siwei Liu, Craig Macdonald, and Iadh Ounis. 2021. Graph neural pre-training for enhancing recommendations using side information. arXiv preprint arXiv:2107.03936 (2021). [40] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781 (2013). [41] Andriy Mnih and Yee Whye Teh. 2012. A fast and simple algorithm for training neural probabilistic language models. arXiv preprint arXiv:1206.6426 (2012). [42] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018). [43] Yujie Pan, Jiangchao Yao, Bo Han, Kunyang Jia, Ya Zhang, and Hongxia Yang. 2021. Click-through Rate Prediction with Auto-Quantized Contrastive Learning. arXiv preprint arXiv:2109.13921 (2021). [44] Jiarui Qin, W. Zhang, Xin Wu, Jiarui Jin, Yuchen Fang, and Y. Yu. 2020. User Behavior Retrieval for Click-Through Rate Prediction. In SIGIR . [45] Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, and Jun Wang. 2016. Product-based neural networks for user response prediction. In ICDM . [46] Yanru Qu, Bohui Fang, Weinan Zhang, Ruiming Tang, Minzhe Niu, Huifeng Guo, Yong Yu, and Xiuqiang He. 2018. Product-based neural networks for user response prediction over multi-field categorical data. TOIS 37, 1 (2018), 1-35. [47] Steffen Rendle. 2010. Factorization machines. In ICDM . [48] Steffen Rendle. 2012. Factorization machines with libfm. TIST (2012). [49] Matthew Richardson, Ewa Dominowska, and Robert Ragno. 2007. Predicting clicks: estimating the click-through rate for new ads. In WWW . ACM, 521-530. [50] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, and Jian Tang. 2019. Autoint: Automatic feature interaction learning via selfattentive neural networks. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management . 1161-1170. [51] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer. In Proceedings of the 28th ACM international conference on information and knowledge management . 1441-1450. [52] Yonglong Tian, Dilip Krishnan, and Phillip Isola. 2020. Contrastive multiview coding. In European conference on computer vision . Springer, 776-794. [53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems . 5998-6008. [54] Chen Wang, Yueqing Liang, Zhiwei Liu, Tao Zhang, and S Yu Philip. 2021. Pretraining Graph Neural Network for Cross Domain Recommendation. In 2021 IEEE Third International Conference on Cognitive Machine Intelligence (CogMI) . IEEE, 140-145. [55] Fangye Wang, Yingxu Wang, Dongsheng Li, Hansu Gu, Tun Lu, Peng Zhang, and Ning Gu. 2022. CL4CTR: A Contrastive Learning Framework for CTR Prediction. arXiv preprint arXiv:2212.00522 (2022). [56] Fangye Wang, Yingxu Wang, Dongsheng Li, Hansu Gu, Tun Lu, Peng Zhang, and Ning Gu. 2022. Enhancing CTR Prediction with Context-Aware Feature Representation Learning. arXiv preprint arXiv:2204.08758 (2022). [57] Peng Wang, Jiang Xu, Chunyi Liu, Hao Feng, Zang Li, and Jieping Ye. 2020. Masked-field Pre-training for User Intent Prediction. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management . 27892796. [71] Weinan Zhang, Jiarui Qin, Wei Guo, Ruiming Tang, and Xiuqiang He. 2021. Deep learning for click-through rate estimation. IJCAI (2021).",
  "A IMPLEMENTATION DETAILS": "In this section, we describe the implementation details for our empirical experiments. We conduct both supervised learning ( i.e. , the 'Scratch' scheme) and self-supervised learning ( i.e. , four pretrainfinetune schemes) over different base models ( i.e. , CTR model). We first introduce the model configuration for each base model, and then give the training settings for supervised learning and self-supervised learning, respectively. Finally, we describe how to employ the pretraining methods for the assembled models ( e.g. , DCNv2 and DeepFM).",
  "A.1 Configuration for Base Models": "We choose the embedding size from { 16 , 32 , 64 } . The dropout rate is selected from { 0 . 0 , 0 . 1 , 0 . 2 } . We utilize one linear layer after the feature interaction layer to make the final CTR prediction. Unless stated otherwise, we adopt ReLU as the activation function. The model-specific hyperparameter settings for base models are as follows: · DNN . We select the size of DNN layer from { 1000 , 2000 } , and the number of DNN layers from { 3 , 6 , 9 , 12 } . KDD '23, August 6-10, 2023, Long Beach, CA, USA Jianghao Lin and Yanru Qu, et al. · AutoInt . We select the number of attention layers from { 3 , 6 , 9 , 12 } . The number of attention heads per layer and the attention size are set to 1 and 64, respectively. · Transformer . We select the number of layers from { 3 , 6 , 9 , 12 } . The number of attention heads is set to 1, and the intermediate size of feed-forward network is set to quadruple the embedding size. We also try both the post-norm [53] and pre-norm [66] structure. · FiGNN . We select the number of layers from { 3 , 6 , 9 , 12 } , and apply residual connection for the graph layers. · FGCNN . We maintain 4 tanh-activated convolutional layers with a kernel size of 7 and pooling size of 2 for each layer. The number of channels for each layer is set to 8 , 10 , 12 , 14, respectively. The numbers of channels for recombination layers are all set to 3. · DeepFM . We select the size of DNN layer from { 1000 , 2000 } , and the number of DNN layers from { 3 , 6 , 9 , 12 } . · xDeepFM . We choose the number of CIN layers from { 2 , 3 , 4 , 5 } , and the number of units per CIN layer is set to 25. We select the size of DNN layer from { 1000 , 2000 } , and the number of DNN layers from { 3 , 6 , 9 , 12 } . · DCNv2 . We select the size of DNN layer from { 1000 , 2000 } , and the number of DNN layers from { 3 , 6 , 9 , 12 } . We force the CrossNet module to have the same number of layer as the DNN network.",
  "A.2 Settings for Supervised Learning": "We train each base model from scratch based on click signals without pretraining. We adopt the Adam optimizer with weight decay rate selected from { 0 . 01 , 0 . 05 } . The batch size is 4096, and the learning rate is chosen from { 10 -3 , 7 × 10 -4 , 5 × 10 -4 } without decay. We adopt early stop if the AUC performance on the validation set stops increasing for two consecutive epochs. Finally, we choose the model at the iteration with the highest validation AUC performance for evaluation in the test set.",
  "A.3 Settings for Self-supervised Learning": "For self-supervised learning paradigm, we implement four different pretraining methods for CTR prediction. We give the settings for the pretraining and finetuning stages as follows. A.3.1 Pretraining Stage. We adopt the Adam optimizer with the weight decay rate of 0 . 05. The learning rate is initialized at 10 -3 , and is scheduled by cosine decay. There is no warm-up epochs during the pretraining. The corrupt ratio is selected from { 0 . 1 , 0 . 2 , 0 . 3 } . We adopt a two-layer MLP with 32 hidden units for the (field-wise) output layer. The method-specific settings are as follows: · MF4UIP . We set the batch size to 256 and only pretrain each base model for 5 epoch, due to its tremendous cost of GPU memory and throughput time. · SCARF . We set the batch size to 2048, and pretrain each base model for 60 epochs. The temperature in InfoNCE loss is 1 . 0. · MFP . We set the batch size to 4096, and pretrain each base model for 60 epochs. The number of noise samples 𝐾 for NCE is 25. · RFD . We set the batch size to 4096, and pretrain each base model for 60 epochs. A.3.2 Finetuning Stage. The batch size is set to 4096. The initial learning rate is selected from { 10 -3 , 7 × 10 -4 , 5 × 10 -4 } , and is scheduled by cosine decay. The total finetuning epoch is chosen from { 1 , 2 , 3 , 4 } . We adopt the Adam optimizer and choose the weight decay rate from { 0 . 01 , 0 . 05 } . We choose the model at the iteration with the highest validation AUC performance for evaluation in the test set.",
  "A.4 How to Pretrain the Assembled Models?": "The deep CTR models usually leverage the parallel structure to incorporate the explicit feature interaction model with the DNN module as follows:  where 𝑓 (·) is the explicit feature interaction model. For examples, DCNv2 assembles DNN and CrossNet. DeepFM assembles DNN and FM. We abstract them as assembled models that add up outputs from multiple ( ≥ 2) modules for final CTR prediction. Suppose we have 𝑁 modules to be assembled. Each of them produces one vector 𝑞 𝑖,𝑘 ( 𝑘 = 1 , . . . , 𝑁 ) for the input sample 𝑥 𝑖 before the last prediction layer. If the module is a shallow network that outputs a scalar ( e.g. , LR, FM), we simply denote it as a vector that has only one dimension. Then, we get the compact representation 𝑞 𝑖 from the assembled model by concatenating the 𝑁 output vectors:  where ⊕ is the vector concatenation. After obtaining the compact representation 𝑞 𝑖 , we can follow the methodology described in Section 3 to pretrain and finetune the assembled model.",
  "B ADDITIONAL EXPERIMENTS": "",
  "B.1 Finetuning Strategy": "Since the embedding layer and feature interaction (FI) layer will be further updated during the finetuning stage. We provide an additional ablation study to investigate the influence of different finetuning strategies ( i.e. , freezing different parts of CTR models during finetuning). We choose DCNv2, DNN, and DeepFM as the representative models, and study the effect for both MFP and RFD tasks. The results are reported in Table 6. From Table 6, we observe that either freezing the embedding layer or the feature interaction layer will badly hurt the final performance for all base models and pretraining methods. This indicates that there exists gap between the pretraining objective and CTR prediction task. The pretrained parameters provide a useful warm-up initialization, but still require further updating for the downstream CTR prediction task.",
  "B.2 Joint Pretraining of MFP and RFD": "We conduct ablation experiments to further analyze the effect of joint pretraining of our proposed MFP and RFD, where the loss function for each input 𝑥 𝑖 is:  where 𝛼 is a hyperparameter to balance the loss terms from MFP and RFD. We set other hyperparameters to be the same as the best configuration in the RFD pretrain-finetune scheme for simplicity, MAP: A Model-agnostic Pretraining Framework for Click-through Rate Prediction KDD '23, August 6-10, 2023, Long Beach, CA, USA Table 6: The ablation study on the finetuning strategies ( i.e. . whether freeze the embedding layer and feature interaction layer or not). The best results are given in bold, while the second-best values are underlined. Table 7: The ablation study on the joint pretraining of MFP and RFD. The best results are given in bold, while the secondbest values are underlined. and apply grid search to select 𝛼 from { 0 . 1 , 0 . 3 , 0 . 5 , 0 . 7 , 0 . 9 } . DCNv2, DNN, DeepFM are chosen as the representative base models. The results are reported in Table 7. Our observation and discussion towards the results in Table 7 are in three folds: · The joint method could achieve slightly better (or comparable) performance compared to the best pretraining method RFD. · It is worth noting that the joint pretraining method consistently reaches the best performance with 𝛼 = 0 . 1. This indicates that RFD surpasses MFP and mainly contributes to the performance improvement. · In addition, since the corruption and recovery strategies for MFP and RFD are different, the joint training method requires two forward/backward propagations for each data instance, which greatly increases the training cost ( e.g. , GPU memory usage and run time per epoch). Although the joint training method might achieve better performance with finer-grained hyperparameter search, we think RFD is still a more elegant and practical pretraining method in terms of both effectiveness and efficiency.",
  "keywords_parsed": [
    "CTR Prediction",
    "Self-supervised Learning",
    "Model Pretraining"
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "MindSpore"
    },
    {
      "ref_id": "b2",
      "title": "Scarf: Selfsupervised contrastive learning using random feature corruption"
    },
    {
      "ref_id": "b3",
      "title": "Language models are few-shot learners"
    },
    {
      "ref_id": "b4",
      "title": "Training and testing low-degree polynomial data mappings via linear SVM"
    },
    {
      "ref_id": "b5",
      "title": "A simple framework for contrastive learning of visual representations"
    },
    {
      "ref_id": "b6",
      "title": "Big self-supervised models are strong semi-supervised learners"
    },
    {
      "ref_id": "b7",
      "title": "Improved baselines with momentum contrastive learning"
    },
    {
      "ref_id": "b8",
      "title": "Electra: Pre-training text encoders as discriminators rather than generators"
    },
    {
      "ref_id": "b9",
      "title": "An adversarial imitation click model for information retrieval"
    },
    {
      "ref_id": "b10",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding"
    },
    {
      "ref_id": "b11",
      "title": "With a little help from my friends: Nearest-neighbor contrastive learning of visual representations"
    },
    {
      "ref_id": "b12",
      "title": "An F-shape Click Model for Information Retrieval on Multiblock Mobile Pages"
    },
    {
      "ref_id": "b13",
      "title": "Bootstrap your own latent-a new approach to self-supervised learning"
    },
    {
      "ref_id": "b14",
      "title": "Deepfm: a factorization-machine based neural network for ctr prediction"
    },
    {
      "ref_id": "b15",
      "title": "Miss: Multi-interest self-supervised learning framework for click-through rate prediction"
    },
    {
      "ref_id": "b16",
      "title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models"
    },
    {
      "ref_id": "b17",
      "title": "Pretraining graph neural networks for cold-start users and items representation"
    },
    {
      "ref_id": "b18",
      "title": "MixGen: A New Multi-Modal Data Augmentation"
    },
    {
      "ref_id": "b19",
      "title": "Masked autoencoders are scalable vision learners"
    },
    {
      "ref_id": "b20",
      "title": "Momentum contrast for unsupervised visual representation learning"
    },
    {
      "ref_id": "b21",
      "title": "Neural factorization machines for sparse predictive analytics"
    },
    {
      "ref_id": "b22",
      "title": "Neural collaborative filtering"
    },
    {
      "ref_id": "b23",
      "title": "Squeeze-and-excitation networks"
    },
    {
      "ref_id": "b24",
      "title": "FiBiNET: combining feature importance and bilinear feature interaction for click-through rate prediction"
    },
    {
      "ref_id": "b25",
      "title": "Neural Statistics for Click-Through Rate Prediction"
    },
    {
      "ref_id": "b26",
      "title": "Field-aware factorization machines for CTR prediction"
    },
    {
      "ref_id": "b27",
      "title": "Interpretable click-through rate prediction through hierarchical attention"
    },
    {
      "ref_id": "b28",
      "title": "Fi-gnn: Modeling feature interactions via graph neural networks for ctr prediction"
    },
    {
      "ref_id": "b29",
      "title": "xdeepfm: Combining explicit and implicit feature interactions for recommender systems"
    },
    {
      "ref_id": "b30",
      "title": "How Can Recommender Systems Benefit from Large Language Models: A Survey"
    },
    {
      "ref_id": "b31",
      "title": "A Graph-Enhanced Click Model for Web Search"
    },
    {
      "ref_id": "b32",
      "title": "Feature generation by convolutional neural network for click-through rate prediction"
    },
    {
      "ref_id": "b33",
      "title": "A Convolutional Click Prediction Model"
    },
    {
      "ref_id": "b34",
      "title": "Self-supervised learning: Generative or contrastive"
    },
    {
      "ref_id": "b35",
      "title": "Roberta: A robustly optimized bert pretraining approach"
    },
    {
      "ref_id": "b36",
      "title": "Pre-training graph transformer with multimodal side information for recommendation"
    },
    {
      "ref_id": "b37",
      "title": "Contrastive self-supervised sequential recommendation with robust augmentation"
    },
    {
      "ref_id": "b38",
      "title": "Multi-Modal Contrastive Pre-training for Recommendation"
    },
    {
      "ref_id": "b39",
      "title": "Graph neural pre-training for enhancing recommendations using side information"
    },
    {
      "ref_id": "b40",
      "title": "Efficient estimation of word representations in vector space"
    },
    {
      "ref_id": "b41",
      "title": "A fast and simple algorithm for training neural probabilistic language models"
    },
    {
      "ref_id": "b42",
      "title": "Representation learning with contrastive predictive coding"
    },
    {
      "ref_id": "b43",
      "title": "Click-through Rate Prediction with Auto-Quantized Contrastive Learning"
    },
    {
      "ref_id": "b44",
      "title": "User Behavior Retrieval for Click-Through Rate Prediction"
    },
    {
      "ref_id": "b45",
      "title": "Product-based neural networks for user response prediction"
    },
    {
      "ref_id": "b46",
      "title": "Product-based neural networks for user response prediction over multi-field categorical data"
    },
    {
      "ref_id": "b47",
      "title": "Factorization machines"
    },
    {
      "ref_id": "b48",
      "title": "Factorization machines with libfm"
    },
    {
      "ref_id": "b49",
      "title": "Predicting clicks: estimating the click-through rate for new ads"
    },
    {
      "ref_id": "b50",
      "title": "Autoint: Automatic feature interaction learning via self-attentive neural networks"
    },
    {
      "ref_id": "b51",
      "title": "BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer"
    },
    {
      "ref_id": "b52",
      "title": "Contrastive multiview coding"
    },
    {
      "ref_id": "b53",
      "title": "Attention is all you need"
    },
    {
      "ref_id": "b54",
      "title": "Pretraining Graph Neural Network for Cross Domain Recommendation"
    },
    {
      "ref_id": "b55",
      "title": "CL4CTR: A Contrastive Learning Framework for CTR Prediction"
    },
    {
      "ref_id": "b56",
      "title": "Enhancing CTR Prediction with Context-Aware Feature Representation Learning"
    },
    {
      "ref_id": "b57",
      "title": "Masked-field Pre-training for User Intent Prediction"
    },
    {
      "ref_id": "b71",
      "title": "Deep learning for click-through rate estimation"
    }
  ]
}