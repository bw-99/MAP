{"Bagpipe: Accelerating Deep Recommendation Model Training": "Saurabh Agarwal Chengpo Yan University of Wisconsin-Madison Ziyi Zhang University of Chicago", "Abstract": "Deep learning based recommendation models (DLRM) are widely used in several business critical applications. Training such recommendation models efficiently is challenging because they contain billions of embedding-based parameters, leading to significant overheads from embedding access. By profiling existing systems for DLRM training, we observe that around 75% of the iteration time is spent on embedding access and model synchronization. Our key insight in this paper is that embedding access has a specific structure which can be used to accelerate training. We observe that embedding accesses are heavily skewed, with around 1% of embeddings representing more than 92% of total accesses. Further, we also observe that during offline training we can lookahead at future batches to determine which embeddings will be needed at what iteration in the future. Based on these insights, we develop Bagpipe, a system for training deep recommendation models that uses caching and prefetching to overlap remote embedding accesses with the computation. We design an Oracle Cacher, a new component that uses a lookahead algorithm to generate optimal cache update decisions while providing strong consistency guarantees against staleness. We also design a logically replicated, physically partitioned cache and show that our design can reduce synchronization overheads in a distributed setting. Finally, we propose a disaggregated system architecture and show that our design can enable low-overhead fault tolerance. Our experiments using three datasets and four models show that Bagpipe provides a speed up of up to 5.6x compared to state of the art baselines, while providing the same convergence and reproducibility guarantees as synchronous training. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SOSP '23, October 23-26, 2023, Koblenz, Germany \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0229-7/23/10...$15.00 https://doi.org/10.1145/3600006.3613142 University of Wisconsin-Madison Shivaram Venkataraman University of Wisconsin-Madison CCSConcepts: \u00b7 Computingmethodologies \u2192 Distributed computing methodologies ; Machine learning ; Keywords: Distributed Training, Recommendation Models", "ACMReference Format:": "Saurabh Agarwal, Chengpo Yan, Ziyi Zhang, and Shivaram Venkataraman. 2023. Bagpipe: Accelerating Deep Recommendation Model Training. In ACM SIGOPS 29th Symposium on Operating Systems Principles (SOSP '23), October 23-26, 2023, Koblenz, Germany. ACM, Koblenz, Germany, 16 pages. https://doi.org/10.1145/3600006.3613142", "1 Introduction": "Recommendation models are widely deployed in enterprises to personalize and improve user experience. Applications that use recommendations range from personalized web search results [59] to friend recommendations in social networks [12, 44] and product recommendations in e-commerce [40]. With growing data sizes [2] and the use of the recommendation in increasingly sophisticated tasks [22, 37], recent trends have seen the adoption of new deep learning based recommendation models [9, 55]. Deep learning based recommendation models have become one of the largest ML workloads with companies like Meta reporting that more than 50% of all ML training cycles [2] and more than 70% of inference cycles [17] are being used for deep learning based recommendation (DLRM) models. The structure of recommendation models differs significantly from other popular deep neural networks. Recommendation models contain two types of model parameters: embedding tables that store vector representation of categorical features and neural networks that consist of multi-layer perceptrons (MLP) for numeric features (Figure 1). If we consider a click-through-rate (CTR) prediction model, given a batch of training examples (user clicks), in the forward pass we first look up the relevant embeddings for categorical features of the input. We note that the embedding lookup is sparse ; for instance, if the user location is New York, we only need to fetch the embeddings corresponding to that location from the embedding table which contains embeddings for all the locations. The numerical features are processed by a dense MLP, and the representations are then combined to form a prediction as shown in Figure 1. Similar to existing DNNs, the embeddings and the MLP parameters are updated in the backward pass based on the gradients. Figure 1. Architecture of a recommendation model : Model parameters include top, bottom NNs and embedding tables. Top Neural Network Embedding Table 1 Embedding Table 2 Numerical Feature 1 Numerical Feature M Categorical Feature 1 Categorical Feature N Pairwise Interaction Concatenate Bottom Neural Network Prediction Forward Backward+MLP GetEmb Embedding Sync TorchRec Bagpipe 0 20 40 60 80 100 120 140 Time (ms) (a) DLRM TorchRec Bagpipe 0 200 400 600 800 Time (ms) (b) DeepFM Figure 2. Training Time Breakdown: Average time spent in various stages of training when using 8 p3.2xlarge instances with TorchRec [47] (left) and Bagpipe (right) on DLRM and DeepFM models (Table 2). For large models like DeepFM, we observe that TorchRec spends 75% of each iteration on embedding access, while Bagpipe can bring it down to 10%. The unique structure of recommendation models introduces a new design challenge which is not handled by existing DL training systems [29, 39, 52]. The challenge arises from the extremely large, memory-intensive embedding tables. In production systems it is common for size of embedding tables to be in terabytes [17, 72] and Table 1 shows the embedding table sizes for three open-source datasets. Since embedding tables will not fit in the memory of a single worker, a hybrid parallelization approach [2, 47, 51] is currently used to train these models. With hybrid parallelization, dense MLP layers are replicated across GPUs on each machine while embedding tables are partitioned and stored on the CPU memory of workers, leading to a design where model parallelism is used for embeddings, and data parallelism is used for dense layers. batch and observe the data access of future batches. Thus, we can revisit classic perfect-caching algorithms [8] to prefetch embeddings based on when they will be used and cache embeddings in GPU memory if they will be reused in the near future. Unlike prior systems, we propose jointly using prefetching and caching to overlap embedding fetches for future batches with compute of the current batch, effectively hiding the latency of embedding access. Despite using hybrid parallelism, existing approaches to DLRM training suffer from embedding access bottlenecks as each iteration of training requires remote access of embeddings. In Figure 2, profiling TorchRec [47] on an 8 GPU cluster with the Criteo dataset (details in \u00a75) shows that on each iteration, up to 75% of the time is spent in embedding fetch and write back operations. Reducing the embedding access overhead forms the primary motivation of our work. To reduce embedding access overhead we observe that the embedding accesses have a specific structure which can be leveraged to accelerate training. Our analysis across three datasets indicates significant skew in embedding accesses; e.g. , with the Kaggle (Criteo) dataset, we observe that 90% of accesses are for just 0.1% of the embeddings. While this might indicate that we can just cache popular embeddings in GPU memory [4], we find that to be insufficient as each training batch also requires a number of embeddings that are not present in the cache. When caching 0.1% of the most popular embeddings for the same Criteo datasets with a batch size of 16,384, only 15% of the total unique embedding access are served from the cache(\u00a72.3). Given the above embedding access patterns, to alleviate overheads, our key insight is that when performing offline training of ML models we can lookahead beyond the current However, there are additional challenges in extending this lookahead-based design to a distributed setting. To ensure embeddings are not stale, we need to perform cache synchronization across workers which leads to additional communication overheads. We propose a new logically replicated, physically partitioned (LRPP) cache design (\u00a73.3) to minimize the cache synchronization time; having a logically replicated cache minimizes the overhead of tracking state separately for each trainer, while having a physically partitioned design helps in reducing the number of bytes transferred across trainers. We also enhance this design using Critical Path Analysis (CPA) [70] to only synchronize necessary embeddings on the critical path and delay the rest, thereby overlapping part of cache synchronization with forward/backward compute of future iterations. In combination, we find that our techniques can reduce communication overheads by 65%-70%. We build the above techniques in Bagpipe, a system for large-scale distributed training of recommendation models. Central to our design is an Oracle Cacher , a new service that looks beyond the current batch, to determine which embeddings to pre-fetch and/or cache. The caching decisions made by Oracle Cacher are realized on each training worker, and trainers overlap prefetching and cache synchronization with model training. We show that this design enables independent scaling of various training components based on workload requirements and also minimizes the time required to recover from failures (\u00a73.4). To evaluate Bagpipe we use three datasets, Criteo Kaggle [34], Avazu [27], and Criteo Terabyte [35], and four popular models, DLRM [53], Wide&Deep [9], DeepFM [16] and D&C [65]. We scale up our training to model sizes of 4.4 billion parameters, (similar to models used in MLperf) using up to 32 GPUs. Overall we find that Bagpipe can improve iteration time by 3 . 7 \u00d7 compared to TorchRec [47] and 2 . 3 \u00d7 compared to asynchronous training in HET [48]. We also show that Bagpipe's design enables fault tolerance with low overhead and can recover from a trainer failure 13 \u00d7 faster compared to FB-Research's system [55]. Finally, unlike prior work [4, 48], our optimizations in Bagpipe maintain consistent access to embeddings. Thus, we maintain the same statistical efficiency as synchronous training, and our time-per-iteration improvements directly translate to timeto-accuracy speedups. By using lookahead to fetch parts of the model (embeddings) out-of order and LRPP distributed caches, Bagpipe is the first distributed recommendation model training system that: (i) alleviates embedding access overhead by up to 92% using both prefetching and caching, (ii) transparently accelerates training while maintaining the same guarantees as synchronous training and (iii) reduces network overheads and thus enables an efficient, dis-aggregated deployment where we can independently scale memory intensive (embedding servers) and compute intensive (trainers) workers based on requirements.", "2 Background & Motivation": "We first provide background on recommendation model training and then motivate the need for a new system to optimize data movement.", "2.1 Deep Learning Recommendation Models": "Recommendation models power widely used large-scale internet services. Recently deep learning is being used to improve the accuracy of recommendations [9, 28, 53]. All deep recommendation models consist of two types of components (i) a memory-intensive embedding layer that stores a mapping between the categorical features and their numerical representations (ii) a compute-intensive neural networkbased modeling layer which models interactions between numerical features and vector representations of categorical features. Across models, the structure of embedding tables typically remains the same. The number of rows (elements in the table) usually depends on the dataset, i.e. , the number of categories, while machine learning engineers vary the dimension of embedding i.e. , the size of the vector to represent a categorical feature. Common dimensions of embeddings are 16, 32, 48, and 64 but sometimes can be as large as 384 [51]. However, the rows in embedding tables vary widely and can be as small as 3 elements or as big as a few billion elements depending on the dataset [17, 72]. Neural network layers have more diversity, and the type of neural network usually depends on the modeling task at hand. DLRM [53], a recommendation model popular at Meta uses fully connected layers for both bottom and top Table 1. Dataset and their embedding tables Table 2. Model Descriptions: For DLRM, W&D, D&C the numbers indicate the structure of the different Fully Connected (FC) layers. For DeepFM, Linear Features represent a linear layer that is used to store feature interactions. For all the models, we used the standard architectures as suggested by original authors. neural networks. While, DeepFM [16] uses a factorization module that learns up to two-order feature interactions between sparse and dense features (Table 2 summarizes other models we consider in this paper). The forward pass of training involves looking up embedding vectors corresponding to the data items. All deep learning based recommendation models [10, 64, 66-68] use this step to handle categorical features such as location, product type, gender, etc. Our focus is to reduce data access overheads that arise from performing embedding lookups and thus speed up the training of all recommendation models which use embedding tables.", "2.2 Training Recommendation models": "Next, we discuss the state-of-the-art systems used for training recommendation models. Offline Training vs Online Training. Recommendation models are trained in both online and offline mode. Offline training involves training the model on large amounts of historical data with emphasis on throughput. Alternatively, online training is performed only on the recently acquired data to account for latest user preferences and is latency sensitive. To boost performance and prevent catastrophic forgetting [14, 32], researchers actively perform offline training, even for models in production. According to a study by Meta [2] offline training is responsible for more than 50% cycles of all ML model training cycles. This shows that offline training of recommendation models is an important workload. In this work our primary focus is on offline training of recommendation models. Training Setup. Recommendation models are extremely large and are currently among the largest ML models used in enterprises. Meta recently released a 12 Trillion parameter recommendation model [51]; in comparison GPT-3 has 175 Billion parameters. However, embedding tables with sparse access patterns account for more than 99% of parameters. The combination of extremely large model sizes with the sparse access pattern introduces several new challenges in distributed training. Figure 1 shows a schematic of a deep learning based recommendation model. In a typical DLRMtraining setup, dense neural network (NN) parameters are replicated and stored on the GPUs and trained in dataparallel fashion, where gradients are synchronized using the all-reduce communication collective. However, embedding tables are extremely large to hold in the GPU memory and are usually partitioned. Existing Systems. Several systems have been designed to perform offline recommendation model training due to it's popularity. Training systems like TorchRec [47], FBResearch's DLRM [55] and HugeCTR [26] partition the embedding table across different GPUs and train them in a model-parallel fashion. Embeddings are fetched using all-toall collective [50]. While, TorchRec tries to overlap embeddingrelated operations, like remote embedding reads and writebacks, with the compute-intensive portion of the neural network, the amount of embedding data that needs to be fetched still adds significant overhead during training. Figure 2 shows a breakdown of the time taken for one iteration of training when using TorchRec [47]. We observe that when using 8 training machines (AWS p3.2xlarge instances ), the overheads when compared to an ideal baseline that does not perform any embedding lookups, is around 70% for the DLRM [53] and 75% for DeepFM [16]. Beyond spending a majority of time in embedding lookups, existing systems also couple storage and compute resources, e.g. , if the embedding tables for a model are extremely large but the compute requirements are small, one still has to use a large number of GPU machines to store the embedding tables. This often leads to sub-optimal use of resources. To alleviate the embedding access overhead and improve resource utilization, FAE [4] performed an analysis of embedding accesses and observed a similar skew in embedding access (\u00a72.3) patterns. However, FAE uses a reordering approach by dividing examples into hot and cold batches based on their embedding accesses, this impacts the statistical efficiency as training continuously with hot batches changes the order of the training examples and can affect convergence [4]. Further, it is not always possible to create batches that only access cached embeddings, because some models [37, 42] use features like Unique User ID (UUID) or Session ID [60, 62, 63] that are unlikely to be repeated and thus requiring at least one cache miss per example. Several other prior works [2, 18, 25, 43, 48] have proposed using asynchronous training to reduce embedding access overhead. With asynchronous training, embedding fetches can happen in the background, e.g. , in HET trainers can use embeddings that are stale up to a certain number of iterations. If embeddings are stale beyond the bound HET synchronizes 10 -2 10 -1 10 0 10 1 10 2 0.75 0.80 0.85 0.90 0.95 1.00 CDF of total accesses CDF of access across datasets Criteo Terabyte Criteo Kaggle Avazu Percent of most frequent embeddings Figure 3. CDF of embedding accesses: The embedding access pattern is heavily skewed, with just the top 0.1% of the embeddings responsible for more than 90% of total accesses. those embeddings with the embedding server before a training iteration. However, similar to other ML models, recent works [25, 51] have observed that asynchronous training can lead to degradation in accuracy for recommendation models. Accuracy degradation is unacceptable to large enterprises as it often directly leads to a loss in revenue [51]. Asynchronous training is also avoided due to the lack of reproducibility, which is necessary to reason about and compare different model versions. Therefore, in this work we focus on designing a system for synchronous distributed training .", "2.3 Embedding Access Patterns": "Next, we describe some observations from analyzing embedding access patterns in recommendation model training. Skew in embedding accesses. When profiling embedding accesses across three datasets (details in Table 1), similar to prior work [4], we find that the embedding access pattern shows a large degree of skew. As shown in Figure 3 we observe that almost 90% of embedding accesses come from just 0.1% of the embeddings. This indicates that caching hot embeddings can reduce the number of embedding lookups significantly. Thus, when a training example needs a popular embedding, it can access it from a cache. However, we also observe that the set of popular embeddings can change over time. We measured the change in popularity over time across three datasets (Criteo, Alibaba, and Avazu) by, first choosing a fixed fraction of the most popular embeddings on the first day ( e.g. , top 0.05% most popular embeddings), and then measuring the percentage of accesses consisting of those embeddings in later days. We observed that such a scheme leads to a degrading cache hit ratio, e.g. , in the case of the Avazu, if we chose the top 0.05% of the most popular embeddings to be cached, the cache hit rate changes from 91% on day 1 to 82% on day 9, showing that static caching approaches [4] will require regular updates for good performance. The presence of similar skew in web scale data is common [3]. Prior work [38] also highlights that, at web scale, users create small communities and are more likely to strongly interact with members/items within those communities and very sparsely with items beyond their community. Therefore, we believe Figure 4. Distribution of unique embeddings accessed from the cache : The hit rate of the cache decreases with increase in batch size. The blue line represents the expected hit rate when storing the most frequently accessed 0.1% embeddings. Criteo Kaggle Avazu Criteo Terabyte Alibaba 256 1024 4096 16384 256 1024 4096 16384 0.6 0.5 0.4 0.3 0.2 0.1 0.0 0.6 0.8 0.4 0.2 0.0 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.70 0.65 0.60 0.55 0.50 256 1024 4096 16384 256 1024 4096 16384 Hit Rate Hit Rate Hit Rate Hit Rate Batch Size Batch Size Batch Size Batch Size that most recommendation models trained on click stream data will have a similar skew as observed in Figure 3. Long-tail of accesses limits benefits from caching. The analysis presented in previous paragraphs is for a scenario where embeddings are accessed for one example after another (i.e., batch size of 1). However, during recommendation model training it is common to use a large batch size (e.g., 16,384 [51]). When embeddings are fetched for a batch, fetching only the unique embeddings within a batch is sufficient. Therefore, when calculating cache hit rates we should only account for the number of unique accesses. We next calculate the effectiveness of only caching the popular embeddings when using large batch sizes. Figure 4 shows the distribution of unique embeddings fetched from the cache as we increase the batch size. Most notably, we see that as batch size increases the ratio of embeddings fetched from the cache to the total number of unique embeddings (accounting for duplicates across examples in a batch) needed in a batch keeps decreasing drastically, e.g. , for a batch size of 16,384 (standard batch size used in MLPerf [46]), only around 10% of the total unique embeddings required are fetched from the cache. Quantitatively, for the Criteo dataset, we observed that a batch of 16,384 needs 425,984 embeddings. Within this batch there are only around 65,000 unique embeddings (or unique categorical features); out of these 65,000 embeddings, the cache only has a 10% hit rate.", "2.4 Design Goals": "Based on the above discussion we have three primary design goals: (i) High throughput training of deep learning based recommendation models by reducing the embedding access overhead, (ii) Provide the same guarantees as synchronous training to improve reproducibility and maintain training accuracy, and (iii) Design a disaggregated architecture where memory and compute can be independently scaled. We approach our design based on the workload patterns observed and aim to improve performance by speeding up access to both frequently used and long-tail of embeddings. We design Bagpipe, a framework that caches hot embeddings and performs out-of-order prefetching for the long-tail embeddings. We describe our design next. Figure 5. Bagpipe setup: All the components of Bagpipe can be individually scaled. The dashed arrows signify async RPCs while solid ones signify sync RPCs. Data Processors - 1 Data Processors - N Data Storage service Oracle Cacher -1 Embedding Server - Partition 1 Embedding Server - Partition N Trainer 1 Trainer N Dense NN Parameters Oracle Cacher-N Local Caches", "3 Bagpipe Design": "We begin by providing an overview of our design.", "3.1 Design Overview": "Bagpipe consists of four components that collectively perform training as shown in Figure 5. Each iteration of training begins with sampling a batch of examples, the DataProcessors pre-process the examples and send them to the Oracle Cacher. Based on the examples, Oracle Cacher runs a lookahead algorithm to determine embeddings to prefetch and to cache, and dispatches this information to the Trainers . The trainers typically run on GPU machines and perform gradient computation. Trainers hold the dense parameters of the model and Bagpipe's cache in the GPU memory. Also, trainers fetch necessary embeddings from the EmbeddingServers . Embedding servers hold the embedding tables of the recommendation model. This design introduces the following contributions: \u00b7 Bagpipe utilizes both caching and prefetching to reduce embedding access overhead. Given an offline training regime, we introduce the concept of lookahead , where we can look beyond the current batch and decide which elements to cache and prefetch (\u00a73.2). \u00b7 We extend our scheme to the distributed setting and introduce a logically replicated, physically partitioned cache design (\u00a73.3) to minimize communication overheads. To further reduce synchronization overheads we use CPA, to selectively synchronize parts of the cache that are immediately needed on the critical path while synchronizing the rest in the background. \u00b7 Finally, we discuss how Bagpipe's dis-aggregated design can help improve efficiency, by scaling components depending on the properties of the dataset and the model, and enable low-overhead fault tolerance (\u00a73.4).", "Input: LookAheadValue": "1 \ud835\udc35\ud835\udc4e\ud835\udc61\ud835\udc50\u210e\ud835\udc44\ud835\udc62\ud835\udc52\ud835\udc62\ud835\udc52 = \ud835\udc44\ud835\udc62\ud835\udc52\ud835\udc62\ud835\udc52 () ; 2 \ud835\udc3f\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61\ud835\udc47\ud835\udc5f\ud835\udc4e\ud835\udc50\ud835\udc58\ud835\udc52\ud835\udc5f = \ud835\udc37\ud835\udc56\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc4e\ud835\udc5f\ud835\udc66 () ; 3 \ud835\udc3c\ud835\udc5b\ud835\udc36\ud835\udc4e\ud835\udc50\u210e\ud835\udc52 = \ud835\udc46\ud835\udc52\ud835\udc61 () ; 4 while BatchQueue .\ud835\udc60\ud835\udc56\ud835\udc67\ud835\udc52 () > 0 do 11 12 13 14 23 24 5 while \u210e\ud835\udc4e\ud835\udc60\ud835\udc41\ud835\udc52\ud835\udc65\ud835\udc61\ud835\udc35\ud835\udc4e\ud835\udc61\ud835\udc50\u210e () and \ud835\udc35\ud835\udc4e\ud835\udc61\ud835\udc50\u210e\ud835\udc44\ud835\udc62\ud835\udc52\ud835\udc62\ud835\udc52.\ud835\udc60\ud835\udc56\ud835\udc67\ud835\udc52 () < \ud835\udc3f\ud835\udc5c\ud835\udc5c\ud835\udc58\ud835\udc34\u210e\ud835\udc52\ud835\udc4e\ud835\udc51\ud835\udc49\ud835\udc4e\ud835\udc59\ud835\udc62\ud835\udc52 do 6 \ud835\udc35\ud835\udc4e\ud835\udc61\ud835\udc50\u210e = \ud835\udc54\ud835\udc52\ud835\udc61\ud835\udc41\ud835\udc52\ud835\udc65\ud835\udc61\ud835\udc35\ud835\udc4e\ud835\udc61\ud835\udc50\u210e () ; 7 \ud835\udc3c\ud835\udc61\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc41\ud835\udc62\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc5f = \ud835\udc35\ud835\udc4e\ud835\udc61\ud835\udc50\u210e.\ud835\udc3c\ud835\udc61\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc41\ud835\udc62\ud835\udc5a ; 8 for EmbID \u2208 \ud835\udc35\ud835\udc4e\ud835\udc61\ud835\udc50\u210e.\ud835\udc48\ud835\udc5b\ud835\udc56\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc38\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc51\ud835\udc51\ud835\udc56\ud835\udc5b\ud835\udc54\ud835\udc60 () do 9 \ud835\udc3f\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61\ud835\udc47\ud835\udc5f\ud835\udc4e\ud835\udc50\ud835\udc58\ud835\udc52\ud835\udc5f [ EmbID ] = \ud835\udc3c\ud835\udc61\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc41\ud835\udc62\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc5f ; 10 \ud835\udc35\ud835\udc4e\ud835\udc61\ud835\udc50\u210e\ud835\udc44\ud835\udc62\ud835\udc52\ud835\udc62\ud835\udc52.\ud835\udc5d\ud835\udc62\ud835\udc60\u210e\ud835\udc35\ud835\udc4e\ud835\udc50\ud835\udc58 ( \ud835\udc35\ud835\udc4e\ud835\udc61\ud835\udc50\u210e ) ; TTLUpdateRequests = {} ; \ud835\udc36\ud835\udc4e\ud835\udc50\u210e\ud835\udc52\ud835\udc39\ud835\udc52\ud835\udc61\ud835\udc50\u210e\ud835\udc45\ud835\udc52\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc60\ud835\udc61\ud835\udc60 = {} ; \ud835\udc36\ud835\udc62\ud835\udc5f\ud835\udc5f\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc35\ud835\udc4e\ud835\udc61\ud835\udc50\u210e = \ud835\udc35\ud835\udc4e\ud835\udc61\ud835\udc50\u210e\ud835\udc44\ud835\udc62\ud835\udc52\ud835\udc62\ud835\udc52.\ud835\udc5d\ud835\udc5c\ud835\udc5d\ud835\udc39\ud835\udc5f\ud835\udc5c\ud835\udc5b\ud835\udc61 () ; for EmbID \u2208 \ud835\udc36\ud835\udc62\ud835\udc5f\ud835\udc5f\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc35\ud835\udc4e\ud835\udc61\ud835\udc50\u210e.\ud835\udc48\ud835\udc5b\ud835\udc56\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc38\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc51\ud835\udc51\ud835\udc56\ud835\udc5b\ud835\udc54\ud835\udc60 () do TTL = \ud835\udc3f\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61\ud835\udc47\ud835\udc5f\ud835\udc4e\ud835\udc50\ud835\udc58\ud835\udc52\ud835\udc5f [ EmbID ] ; 15 16 TTLUpdateRequests .\ud835\udc4e\ud835\udc5d\ud835\udc5d\ud835\udc52\ud835\udc5b\ud835\udc51 (( EmbID , TTL )) ; 17 if EmbID is not in \ud835\udc3c\ud835\udc5b\ud835\udc36\ud835\udc4e\ud835\udc50\u210e\ud835\udc52 then 18 \ud835\udc36\ud835\udc4e\ud835\udc50\u210e\ud835\udc52\ud835\udc39\ud835\udc52\ud835\udc61\ud835\udc50\u210e\ud835\udc45\ud835\udc52\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc60\ud835\udc61\ud835\udc60.\ud835\udc4e\ud835\udc5d\ud835\udc5d\ud835\udc52\ud835\udc5b\ud835\udc51 ( EmbID ) ; 19 \ud835\udc3c\ud835\udc5b\ud835\udc36\ud835\udc4e\ud835\udc50\u210e\ud835\udc52.\ud835\udc56\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5f\ud835\udc61 ( EmbID ) ; 20 if TTL = \ud835\udc36\ud835\udc62\ud835\udc5f\ud835\udc5f\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc35\ud835\udc4e\ud835\udc61\ud835\udc50\u210e.\ud835\udc5b\ud835\udc62\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc5f then 21 \ud835\udc3c\ud835\udc5b\ud835\udc36\ud835\udc4e\ud835\udc50\u210e\ud835\udc52.\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc60\ud835\udc52 ( EmbID ) ; 22 \ud835\udc3f\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61\ud835\udc47\ud835\udc5f\ud835\udc4e\ud835\udc50\ud835\udc58\ud835\udc52\ud835\udc5f.\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc60\ud835\udc52 ( EmbID ) ; \ud835\udc46\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc47\ud835\udc5c\ud835\udc47\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc5f\ud835\udc60 ( TTLUpdateRequests ) ; \ud835\udc46\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc47\ud835\udc5c\ud835\udc47\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc5f\ud835\udc60 ( \ud835\udc36\ud835\udc4e\ud835\udc50\u210e\ud835\udc52\ud835\udc39\ud835\udc52\ud835\udc61\ud835\udc50\u210e\ud835\udc45\ud835\udc52\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc60\ud835\udc61\ud835\udc60 ) ; Algorithm 1: Lookahead Algorithm", "3.2 Caching and Prefetching in Bagpipe": "In Bagpipe, we introduce the idea of each trainer having a local cache. When designing a system with caching, we need to design a cache insertion policy (what to cache?) and a cache eviction policy (what to evict?) so as to maximize the hit rate. However, offline batch training of machine learning jobs like recommendation model training has additional structure: in offline batch training future batches and their contents are predictable , i.e. , in context of recommendation models we can look beyond the current batch and infer which embeddings will be accessed in future batches. This insight helps us create a perfect or oracular cache . To utilize this insight, we design a lookahead algorithm. For ease of explanation, the discussion in this section assumes there is only one trainer (only one cache). We extend this to the distributed setting in \u00a73.3. Lookahead Algorithm. To decide what to cache and what to evict we develop a low overhead (benchmark in \u00a75.3) lookahead algorithm (Algorithm 1) which also ensures consistent access to embeddings. We denote the lookahead value ( \u2112 ), as the number of batches beyond the current batch, which will be analyzed to determine what to cache, e.g. , if the current batch is \ud835\udc65 , we consider embedding accesses in batches from \ud835\udc65 to \ud835\udc65 + \u2112 , to determine which elements in batch \ud835\udc65 should be cached. The lookahead algorithm takes three inputs: the current batch, future batches (next \u2112 number of batches), and current state of the cache on the trainer. The lookahead algorithm outputs two pieces of information. First, for the current batch, it generates the list of embeddings that will not be found in the cache on the trainer's GPUs. This allows Bagpipe to prefetch these embeddings out of order before the current batch is used for training. Prefetching allows Bagpipe to hide the data access latency for the long tail of embeddings that are not frequently accessed. Second, the lookahead algorithm determines which embeddings from the current batch will be used in future batches , and the last iteration they will be accessed in the current lookahead window. Any embeddings from the current batch that will be used by future batches in the lookahead window will be marked for caching, so they can be accessed from the GPU memory in the future. The last iteration an embedding is used within the lookahead range and serves as time-to-live (TTL) for the embedding in the cache. Next we describe an example of how lookahead algorithm processes batches (Figure 6), with lookahead value ( \u2112 ) as 2: \u00b7 Batch 1 Embedding 3 and 9 are in the batch. For both embeddings we launch prefetches. However, embedding 3, is accessed again, and the last occurrence in the window is at Batch 2, so we cache it with the TTL set to 2. \u00b7 Batch 2 Embedding 3 is in the cache so we do not send a prefetch request. But the last occurrence for 3 in the window is now in Batch 3. Therefore, a TTL update is sent for 3. We will prefetch 4 since it is not in the cache. \u00b7 Batch 3 Weprefetch embedding 6 and cache it with a TTL of 4 since it will be reused in Batch 4. At this point in our lookahead window Embedding 3 has no future occurrence so it will be evicted after batch 3. However, if embedding 3 was being used by batch 4, we would have kept it in the cache and sent a TTL update with eviction batch as 4. \u00b7 Batch 4 We prefetch 1 since it is not in the cache. We do not send any TTL updates for 6 as it is absent in future batches and will be evicted after this batch. Consistency with the Lookahead algorithm. Our consistency goal is to avoid staleness and ensure that trainers do not prefetch an embedding from the embedding servers while it has updates that have not yet been written back. Despite pre-fetching embeddings out of order, our formulation of what to cache and what to prefetch (Algorithm 1), provides an extremely important guarantee that allows us to maintain consistency and match the execution of synchronous training. When the trainer is processing batch number \ud835\udc65 , an embedding used by the batch will either be available in the cache with it's most recent value or no preceding batch in the lookahead range (any batch number in [ \ud835\udc65 -\u2112 , \ud835\udc65 ) ) would have updated that specific embedding. That is, if an embedding was needed by a batch in batch number in range [ \ud835\udc65 -\u2112 , \ud835\udc65 ) it will be in the cache; if an embedding is not in the cache it means no batch in range of [ \ud835\udc65 -\u2112 , \ud835\udc65 ) has updated it. Therefore, as long as the prefetch request for batch \ud835\udc65 is issued after updates from training batch number Figure 6. Lookahead Algorithm : The above figure shows an illustration at different batch steps of how the lookahead algorithm functions. In the above example, the lookahead value is 2 and the batch size is also 2. 3 9 4 3 Batch 2 Batch 1 EMB ID TTL Oracle Cacher 3, 9 Prefetch (3,2) (Cache EMB ID, TTL Val) 3 2 4 3 Batch 2 Oracle Cacher 4 Prefetch (3,3) 3 3 3 6 Batch 3 3 6 Batch 3 Oracle Cacher 6 Prefetch (6,4) 6 1 Batch 4 Batch 1 Batch 2 Batch 3 6 1 Batch 4 Oracle Cacher 1 Prefetch 6 9 7 Batch 5 4 EMB ID TTL EMB ID TTL 3 3 6 4 EMB ID TTL Batch 4 Dynamic Cache Dynamic Cache Dynamic Cache Dynamic Cache (Cache EMB ID, TTL Val) (Cache EMB ID, TTL Val) \ud835\udc65 -\u2112 have been written back, we can guarantee that we will not see stale embeddings.", "3.3 Distributed Cache Design in Bagpipe": "First we discuss the requirements for the distributed cache design and our goals. Next, we discuss the design space for cache design and finally we compare these designs both quantitatively and qualitatively. Distributed Cache Requirements. When extending the caching scheme described above to a distributed setting, Bagpipe can provide consistency as long as the following two requirements are satisfied (i) Each trainer sends prefetch requests for batch number \ud835\udc65 only when cache eviction and updates have been performed by all the trainers on \ud835\udc65 -\u2112 batch. (ii) Each trainer's cache should contain the latest value of the embedding. The first requirement is a direct extension of our prior discussion and can be satisfied by synchronizing the iteration number that each trainer has processed. The second condition, however, creates additional communication overheads and we next discuss the design space and techniques to reduce these overheads. of training, each trainer fetches the embeddings not available locally from their peer trainers. Post backward pass, each trainer writes back the gradients to the respective peer trainer which has ownership of the embedding. These steps are required to ensure we always use the latest version of the embedding. Unlike the replicated cache, where all the embeddings are synchronized irrespective of whether a trainer needs it, in the case of a partitioned cache, trainers only fetch and write back the embeddings they utilize. Further, partitioned caches are more space efficient as there is only one copy of each embedding in the distributed cache. Goals of distributed cache design. The primary objective of our distributed cache design is to minimize the time spent on cache synchronization on the critical path . Thus our objective includes, accounting for the number of bytes transferred (bandwidth) and connection overheads (latency) [61]. Next, we explore the distributed cache design space and discuss synchronization costs with each design. Replicated Cache. In a replicated cache, each trainer will pre-fetch all the embeddings which are required by the whole batch (not just a worker's partition of the batch). After performing the backward pass we synchronize all the elements which have updated gradients across all the workers, such that embeddings in the caches are synchronized at the end of each iteration using all-reduce . This trivially ensures that each trainer's cache has the latest version of the embedding. A replicated design results in high bandwidth cost due to synchronization of all the elements across all trainers even if the element's updated value would not be required in future by other trainers, i.e. , it will be evicted from the cache. However, there is very small control (latency) overhead because all elements are synchronized. Partitioned Cache. A partitioned cache is on the other end of the design spectrum where each trainer is assigned an exclusive portion of the cache. Before the forward pass The number of bytes communicated when using a partitioned cache depends on how batches are partitioned across trainers. To study the scenario where batch partitioning is communication aware, i.e. , batches are partitioned so as to minimize bytes communicated across trainers, we formulate a mixed integer linear program (MILP). Given the cache state on all trainers, the MILP computes a partitioning of examples which minimizes the amount of inter-node communication. Given a batch of examples ( \ud835\udc4f ) and \ud835\udc5d trainers, we introduce \ud835\udc4f \u00d7 \ud835\udc5d variables in our MILP. Each variable is denoted by \ud835\udc65 \ud835\udc56,\ud835\udc57 where if \ud835\udc65 \ud835\udc56,\ud835\udc57 = 1, then example \ud835\udc56 will be assigned to trainer \ud835\udc57 . We then compute a cost matrix \ud835\udc36 , where given the cache state, \ud835\udc36 \ud835\udc56,\ud835\udc57 represents the cost of inter-node communication that will be required to fetch embeddings for example \ud835\udc56 to location \ud835\udc57 . Our objective is to minimize the amount of internode communication. We formulate it using our variables and cost matrix as:  Where \ud835\udc3c and \ud835\udc3d represent the set of examples and trainers respectively. Further, we include two constraints to ensure that the solution is feasible and avoids load imbalance: (i) Each example must be placed on one trainer and all examples need to be placed on at least one trainer node. \u2200 \ud835\udc56 \u2208 \ud835\udc3c \u02dd \ud835\udc57 \u2208 \ud835\udc3d \ud835\udc65 \ud835\udc56,\ud835\udc57 = 1 . (ii) We add another constraint to make sure the batch is equally distributed across machines to prevent load imbalance. The optimization problem can be solved using existing MILP solvers like Gurobi [20]. However, using communication aware partitioned caches has two disadvantages: first solving the MILP takes around 2.36s on a 16-core machine making it infeasible when iteration times are around 100ms (Figure 2). Secondly, sync time does not solely depend on bytes communicated, as overheads Figure 7. Comparing cache designs: We observe that LRPP provides best performance among all other cache options. Replicated Partitioned Random Partitioned Communication Aware LRPP 0 50 100 150 200 250 300 Time Per Iteration (ms) DLRM DeepFM 0 10 20 30 40 50 60 Cache Sync Time (ms) Immediate Synchronization Delayed Synchronization Figure 8. Effect of Delayed Synchronization :Delayed Sync can reduce time for cache synchronization by up to 44%. from maintaining data-structures and establishing connections also play a role. With partitioned caches we would need to introduce additional data structures to keep track of embedding locations and establish multiple connections. Logically Replicate Physically Partitioned Cache. Ideally, we would like to design a cache that does not perform unnecessary synchronization of embeddings but does not introduce additional overheads due to state tracking. To achieve this goal, we propose using Logically Replicated, Physically Partitioned (LRPP) caches. By logically replicated we mean that from the view of Oracle Cacher all caches have all data and are fully replicated but by being physically partitioned , the trainers decide which elements need synchronization and which elements can be evicted without synchronization. The primary insight behind our idea comes from the observation that for the Criteo dataset, around 25% of the embeddings are used by only one of the examples in batch. Therefore, these embeddings are updated at only one trainer before being evicted. Thus, fetching or synchronizing them across all trainers is a waste of network bandwidth. We design a new protocol that modifies the replicated cache based on this insight. With LRPP caches, the Oracle Cacher marks embeddings which are only used by a single trainer. Given this metadata, these embeddings are only fetched by the trainer which needs them and are ignored by other trainers. After the forward and backward pass completes, the trainers skip synchronization for these embeddings and use all-reduce to synchronize the other embeddings. In the background, the trainer which made the only update to the marked embedding evicts it back to the Embedding Server. This optimization is able to reduce the volume of embeddings prefetched and synchronize with very minimal control logic. LRPP can be further extended with more fine-grained partitioning, i.e. , we can synchronize embeddings updated by two workers using a separate communication group containing just those workers. However, further fine-grained partitioning will create additional control logic, which in turn would add additional latency and thus yield diminishing returns. There are parallels between design of LRPP to [71] a concurrent work which only caches elements which are going to be utilized more than once with a FIFO eviction policy. This is analogous to LRPP only synchronizing elements which are going to be used in future. We plan to study extensions of LRPP in future work. Comparingcachedesignchoices. For Kaggle critieo dataset with batch size 16,384 and 8 trainer machines (p3.2xlarge) we observe that Replicated Cache communicates around 65K embeddings per iteration, while Communication AwarePartitioned Cache communicates 21K embeddings per iteration and LRPP communicates around 48K embeddings. Further, we implement all these in Bagpipe and evaluate them in terms of per-iteration training time using the same setup. To consider the best case scenario for partitioned caches, we ignore the time taken by the Gurboi solver. In Figure 7, we observe that LRPP outperforms replicated by 22.8% and communication aware partitioned by 59.8%. Our analysis shows that despite synchronizing fewer embeddings partitioned caches do not perform well due to hotspots and additional control logic. Since some embeddings are accessed extremely frequently, the trainers that own those embeddings become a bottleneck. Further, in partitioned caches, the overhead of performing multiple collective communication calls, creating memory buffers for each collective communication call and tracking which peer to access embeddings from, leads to an additional overhead of 80-90ms for a batch size of 16K with 8 trainers. Therefore, we configure Bagpipe to use LRPP cache synchronization scheme due to it's superior performance. Delayed Synchronization. To further optimize the LRPP protocol we use Critical Path Analysis [70]. In this scenario, CPAimplies that as long as the embeddings are synchronized before being critically required it can suffice. However, directly using CPA in context of embedding synchronizations for recommendation models will lead to network contention with other competing synchronizations. Therefore, we introduce delayed synchronization, where we only synchronize the embeddings which will be required in the next iteration on the critical path. The embeddings which are not needed immediately are synchronized in the background. To, avoid network contention due to background synchronization we ensure that all background synchronizations are completed before we launch other critical path synchronizations for future iterations. On Kaggle Criteo dataset with 8 trainers and batch size 16K, we see that only 22.7K embeddings out of 48K embeddings (47.3%) need to be synchronized on the critical path, the rest can be overlapped with the forward pass of the next iteration. For two models on Criteo dataset, Figure 8 shows that delayed synchronization can further reduce cache synchronization time by up to 44% (in addition to LRPP) by overlapping synchronization with forward pass. We also observe that LRPP and delayed synchornization can together reduce bytes communicated on critical path by around 70%.", "3.4 Disaggregated Design and Fault Tolerance": "Existing recommendation model training systems [47, 51, 55] couple storage and compute resources, i.e. , it is not possible to scale the number of embedding table partitions without increasing the number of trainers. This affects fault tolerance and resource utilization. For fault tolerance, given the extremely large embedding table sizes, checkpointing a trainer can take several minutes [13] during which the compute resources stay idle. The lack of disaggregation also leads to poor resource utilization [5, 11], e.g. , when embedding tables are extremely large but the dense neural network parameters are small, an optimal configuration would be to use more servers for embedding tables but have fewer trainers. Thus, we design a disaggregated architecture for Bagpipe (Figure 5) with four major components: (i) Data Processors (ii) Oracle Cacher (iii) Distributed Trainer (iv) Embedding Servers. Data Processor. Data processors read and batch training data which is resource intensive. Similar to prior designs [73] we offload data processing to reduce trainer overheads. Data processors are stateless and can be restarted on failure. Oracle Cacher. Oracle Cacher is a centralized service that inspects all the training batches using the lookahead algorithm (Algorithm 1). Oracle Cacher decides which elements to prefetch for the current batch and the TTL for eviction of elements being cached. Oracle Cacher sends the training data as well as the embedding ids that need to be cached/prefetched using async RPC calls to the trainers. Oracle Cacher is designed such that all the necessary internal state is also present on the trainers. Therefore, whenever Oracle Cacher has to be restarted we only need to fetch the last iteration number processed by the trainers and the embedding IDs present on them. Trainer. Trainers hold the dense neural network portion of the recommendation model and the LRPP cache in the GPU memory. The trainers perform forward and backward passes in a synchronous fashion. Trainers also: (i) prefetch the embeddings based on requests sent by Oracle Cacher (ii) perform cache maintenance including addition and eviction of embeddings. When a trainer fails, Oracle Cacher makes an RPC call to ask existing trainers to checkpoint their state (model parameters and cache contents) and then copies this state to the newly started trainer. Each of the trainers then discard their gradients and Oracle Cacher starts from the previous iteration. With delayed synchronization and LRPP enabled we might loose updates of at most one iteration, which is unlikely to affect model convergence [58]. Embedding Server. Embedding servers store all the embedding tables and act as a sharded parameter server, handling the prefetch and update requests from the trainers. We use the techniques presented in prior work [13] to checkpoint embedding servers periodically.", "3.5 Discussion": "Next, we discuss some benefits and limitations of our design. Generalizing across skew patterns. Unlike prior work [4], Bagpipe's optimizations are resistant to embedding access skew changes (evaluated in \u00a75.3). This is because Bagpipe does not just rely on caching of a fixed set of hot embeddings, it speeds up access to cold embeddings using pre-fetching. So if there exists datasets that do not display a high degree of skew, Bagpipe will still outperform prior work. Applicability in online training. Bagpipe's optimizations are applicable to offline setup, as it relies on the ability to look at future batches to build a cache. In case of online training examples arrive sporadically thus restricting lookahead. Scalability of Oracle Cacher. The overhead of Oracle Cacher is extremely small even for extremely large batch sizes and lookahead values. In \u00a75.3 we find that Oracle Cacher, even for large batch size of 131K, can dispatch 3.27 Million samples per second. Further, Oracle Cacher only needs to be faster than the time taken by trainers for the forward and backward pass. However, if required, Oracle Cacher can be partitioned to increase scalability for datasets with a large number of embedding tables. To split the work done by Oracle Cacher, we can partition the embedding tables such that each partition of the Oracle Cacher can work on a different embedding table. For instance, if there are 1000 categorical features and we launch 10 Oracle Cacher; for each example, each Oracle Cacher generates caching decisions for their subset of 100 categorical features.", "4 Implementation": "Bagpipe is implemented in around 5000 lines of Python. Async RPC's are used to communicate across different components. For synchronization of dense parameters and caches weuse collective communication primitives present in NCCL [1]. Bagpipe is completely integrated with PyTorch and existing model training code can use it with 4 to 5 lines of changes. API details will be present in our open source version. Overlapping cache management with training. We perform, all cache management operations in a separate thread thus not affecting the training process. Our caching data structure can operate completely lock free, because in our Oracle Cacher's lookahead formulation, we guarantee that the training thread and cache maintenance thread will operate on completely separate indices of the cache. This ensures that cache management has minimal overhead on training. Automatically Calculating Lookahead. Bagpipe uses two configuration parameters: max cache size and lookahead value ( \u2112 ). Providing the max cache size is mandatory, it can be determined by computing amount of free memory available after allocating space for the dense neural network parameters. \u2112 can be automatically calculated if it is missing. To calculate \u2112 , at startup Bagpipe keeps prefetching until it detects the cache is full. On detecting that the cache is full, Bagpipe selects the number of batches prefetched so far as the \u2112 . Further, Bagpipe can also handle scenarios where the configuration variables are incompatible. Since Oracle Cacher always has a consistent view of the cache, if it observes that the cache is going to be full it can reduce the \u2112 . We perform a sensitivity analysis on \u2112 in \u00a75.3.", "5 Evaluation": "We evaluate Bagpipe by measuring improvements in per iteration time against four baselines, observing a speedup of 2 . 1 \u00d7 to 5 . 6 \u00d7 for the DLRM model. Further, we vary the recommendation model architecture and compare Bagpipe against the best-performing baseline with four different models and observe a speedup of up to 3 . 7 \u00d7 . We also analyze the performance of Bagpipe on different hardware and datasets and evaluate other aspects of Bagpipe like fault tolerance (\u00a75.2) and sensitivity to configuration parameters (\u00a75.3). Baseline Systems. To compare Bagpipe we use four open source baselines discussed in \u00a72.2. We compare Bagpipe with FAE [4], FB-Research's training system [55], TorchRec [47] and HET [48]. We discuss additional details of these systems when comparing them with Bagpipe in \u00a75.1. Models and Datasets. We use four different recommendation models, Facebook's DLRM [53], Google's Wide&Deep [9], Deep&Cross Networks [65], and Huawei's DeepFM [16]. Table 2 describes the models used to evaluate Bagpipe. The models differ markedly in terms of the dense parameters, e.g. , the largest model has 33.8 Million parameters while the smallest one only has 136K parameters. For datasets, we use the Kaggle Criteo [34], Avazu [27] and Criteo Terabyte dataset [35] (largest publicly available dataset). Table 1 describes the embedding table size for each dataset. Cluster Setup. We run all our experiments on Amazon Web Services(AWS). For trainers, we use p3.2xlarge instances while Embedding Server and Oracle Cacher run on a c5.18xlarge instance each. Each p3.2xlarge instance contains a Nvidia V100 GPU, 8 CPU cores and 64 GB of memory with internode bandwidth of up to 10 Gbps. Each c5.18xlarge has 72 CPU cores and 144 GB of memory. For Bagpipe we launched dataloaders on the same c5.18xlarge as Oracle Cacher since the machine had ample compute. To study the performance of Bagpipe in a setting with different amounts of compute and bandwidth we also run some experiments on g5.8xlarge where each machine has an Nvidia A10G GPU,32 CPU cores with inter-node bandwidth of 25 Gbps. Bagpipe Configuration. Unless otherwise stated, for all our experiments we set the cache size to enable lookahead of up to 200 batches. We study the sensitivity of these parameters and their effect on throughput in \u00a75.3. We run all our experiments for 2000 iterations, which roughly translates to 1 epoch of Criteo Kaggle Dataset with batch size of 16,384. Metrics. For all our experiments we plot average per-iteration time with error bars representing standard deviation. This directly translates to the time taken to train a fixed number of epochs. As Bagpipe guarantees consistent access to embeddings, the accuracy after each iteration exactly matches other synchronous training baselines (validated in \u00a75.1).", "5.1 Comparing Bagpipe": "We first evaluate Bagpipe by comparing it against a number of existing systems and study how our benefits change as we vary the models, datasets, and hardware. Comparing Bagpipe with existing systems. In Figure 9, we compare Bagpipe with four existing systems, FAE [4], FBResearch training system [55], TorchRec [47] and HET [48]. We use Criteo Kaggle dataset with batch size 16,384 (a common batch size among MLPerf [46] entries) and two popular recommendation models DLRM [53] and W&D [9]. FAE performs pre-processing on training data to classify embeddings as either hot or cold. To evaluate the best case scenario for FAE, we do not account for the additional time FAE spends in partitioning batches and deciding the placement of embeddings. As shown in Figure 9, Bagpipe achieves 3 . 4 \u00d7 speedups for the DLRM model and 3 . 7 \u00d7 speedups for W&D. As discussed in \u00a72.2, during hot batch training FAE has similar cache synchronization overheads as Bagpipe, but when it switches to cold batches, it suffers additional embedding access overheads due to no prefetching. Next, we compare Bagpipe with open source FB-Research training system [55], built over PyTorch and Caffe-2, is designed for DLRM models [53] but can be easily modified to support other embedding-based deep recommendation models like W&D [9]. Bagpipe provides 5 . 6 \u00d7 and 4 . 2 \u00d7 speedups over FB-Research training system. FB-Research system is slow due to spending almost 60% of the time on data loading, which has also been observed by prior works [56, 73] as well, leading to worse throughput compared to Bagpipe which offloads data-preprocessing to remote machines. When compared against TorchRec, a recent open source system built over PyTorch [41] and FBGEMM [57] to facilitate training of recommendation of models, we observe that Bagpipe is around 2 . 1 \u00d7 faster for DLRM models and around 1 . 3 \u00d7 faster for W&D models. Unlike Bagpipe, TorchRec does not perform any caching or pre-fetching, and therefore fetches and writes back a large number of embeddings on the critical path. Bagpipe reduces and overlaps the amount of embedding-related communication on the critical path. TocomparewithHET[48],asystemthat performs bounded asynchronous training as described in \u00a72.2, we use the authorprovided code implemented in C++ with Python bindings to evaluate HET and set the asynchrony bound to 100, as suggested by the authors for maximum speedup. We find that Bagpipe is around 2 . 3 \u00d7 faster than HET for DLRM and 1 . 6 \u00d7 faster for W&D. We observe that, despite performing asynchronous training, HET needs to fetch embeddings that are not available in the local cache from the parameter server on the critical path. With increase in batch size the number of cache misses increases as well, due to the long tail of accesses (discussed in \u00a72.3). We also verify that our performance closely matches with those reported in the paper [48]. Bagpipe FAE FB-Research TorchRec HET 0 50 100 150 200 250 300 350 400 Per Iteration Time (ms) DLRM W&D Figure 9. Compare Bagpipe with Existing Systems : We compare per iteration time of Bagpipe against existing FAE [4], FB-Research training system [55], TorchRec [47] and HET [48]. Bagpipe provides speedups betwen 1 . 2 \u00d7 and 5 . 6 \u00d7 . Criteo Kaggle Avazu Criteo Terabyte 0 25 50 75 100 125 150 175 Per Iteration Time (ms) Bagpipe TorchRec DLRM W&D D&C DeepFM 0 200 400 600 800 1000 Per Iteration Time (ms) Bagpipe TorchRec Figure 10. Compare Bagpipe with different models : We compare Bagpipe and TorchRec on four different models, DLRM [53], W&D [9], D&C [65], and DeepFM [16]. We observe speedups between 1 . 2 \u00d7 and 3 . 7 \u00d7 . 0.55 0.60 0.65 0.70 0.75 Bagpipe TorchRec 0 250 500 750 1000 1250 1500 1750 2000 Figure 11. Compare Bagpipe on different Hardware : Speedup provided by Bagpipe over TorchRec on p3.2xlarge decreases from 3 . 7 \u00d7 to 2 . 5 \u00d7 on g5.8xlarge (high bandwidth) depicting that TorchRec is more constrained by bandwidth. p3.2xlarge g5.8xlarge 0 200 400 600 800 1000 Per Iteration Time (ms) DLRM-Bagpipe DLRM-TorchRec DeepFM-Bagpipe DeepFM-TorchRec Figure 12. Compare with Different Datasets : Bagpipe consistently provides speedups between 1 . 9 \u00d7 to 2 . 4 \u00d7 across datasets. 0 200 400 600 800 Time(s) 0 20000 40000 60000 80000 100000 Throughput (Examples/sec) New Machine is available Trainer shut down signal Throughput Bagpipe Throughput FB-Research Figure 13. Loss convergence for Bagpipe and TorchRec: Convergence of Bagpipe and TorchRec is very similar, with slight differences due to random initialization. Figure 14. Recovery from trainer failure : Bagpipe requires less than 60 seconds to recover from a trainer failure compared to 13 minutes for FB-Research System. Forward-Bagpipe Forward-Ideal Backward+MLPsync-Bagpipe Backward+MLPsync-Ideal Get-Embedding Bagpipe Get-Embedding Ideal Cache Synchronization Bagpipe Ideal 0 10 20 30 40 50 60 Per Iteration Time (ms) (a) DLRM: p3.2xlarge Bagpipe Ideal 0 50 100 150 200 250 Per Iteration Time (ms) (b) DeepFM: p3.2xlarge Bagpipe Ideal 0 5 10 15 20 25 30 Per Iteration Time (ms) (c) DLRM: g5.8xlarge Bagpipe Ideal 0 20 40 60 80 100 120 Per Iteration Time (ms) (d) DeepFM: g5.8xlarge Figure 15. Comparing with Ideal: Comparing Bagpipe with an ideal system which has no overhead for embedding fetch, we observe that system comes within 10% of time per iteration for large models where there is potential to overlap embedding accesses. 4-trainer BS-8192 8-trainer BS-16384 16-trainer BS-32768 32-trainer BS-65536 0 50 100 150 200 Per Iteration Time (ms) 16384 32768 65536 131072 Batch Size 0 25 50 75 100 125 150 175 200 Per Iteration Time (ms) (a) Increasing Trainers (b) Increasing Batch Size Figure 16. Scalability of Bagpipe: (left) weincrease the number of trainers such that batch size per machine is constant; Bagpipe provides sublinear scalability due to increasing communication bottlenecks. (right) Increasing batch size with 8 trainers results in better throughput as we are able to better overlap communication. As the speedup of Bagpipe varies across models, we perform a detailed investigation to understand this. We observe TorchRec to be the best performing baseline as it efficiently overlaps different parts of the training pipeline and make better use of network bandwidth using the all2all primitive for embedding fetches. Thus, for the next set of experiments we compare Bagpipe with TorchRec. Comparing Bagpipe on other models. In addition to W&D and DLRM, we also train the Deep&Cross Network (D&C) [65] and DeepFM models [16] with Bagpipe and TorchRec. D&C models contain an additional Cross Network component, which performs explicit feature crossing of sparse features to learn predictive features of bounded degree without manual feature engineering. DeepFM introduces a factorization module that learns up to 2-order feature interactions between sparse and dense features. Details of these models are available in Table 2. In Figure 10 we observe that performance gains provided by Bagpipe over TorchRec depends on the size and computation requirements of the dense portion of recommendation models, e.g. , for W&D which only has around 131,000 dense parameters we observe that Bagpipe provides only a 1 . 2 \u00d7 speedup, while for DeepFM which has 33.8 million parameters Bagpipe provides a speedup of over 3 . 7 \u00d7 . We believe that this is due to the pipelining mechanism present in TorchRec where the authors overlap the embedding write-back with the synchronization of the dense model. As the model size increases, the bandwidth requirement for synchronization also increases and the synchronization of dense model and embedding write-backs ends up competing for the same set of network resources. Meanwhile, Bagpipe significantly reduces the amount of embedding synchronization due to caching. Further, delayed synchronization allows Bagpipe overlap forward pass of the next iteration. Thus, our analysis indicates that TorchRec, unlike Bagpipe, is heavily bottlenecked by the network bandwidth available. To verify this, we next run experiments on a different hardware. Comparing Bagpipe on different hardware. To understand the performance of Bagpipe on different hardware setups, especially in terms of network bandwidth, we evaluate Bagpipe on g5.8xlarge (A10G GPU and 25 Gbps bandwidth) instances. In terms of compute, A10G performs similar to V100, but the bandwidth on g5.8xlarge is 25 Gbps compared to 10 Gbps on p3.2xlarge . We use the same hyper-parameters and model configurations as in previous sections. Figure 11 shows a comparison of per-iteration time between DLRM and DeepFM, for both Bagpipe and TorchRec. We observe that Bagpipe with DLRM model on g5.8xlarge trainers is around 1 . 9 \u00d7 faster than \ud835\udc5d 3 . 2 \ud835\udc65\ud835\udc59\ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc52 trainers. On other hand, the DLRMmodelwithTorchRecon g5.8xlarge trainers is around 2 . 4 \u00d7 faster than p3.2xlarge trainers. Similarly for DeepFM, time for TorchRec reduces by 2 . 4 \u00d7 (1015ms to 414ms) when we switch from p3.2xlarge instances to g5.8xlarge . This confirms our hypothesis that for larger models TorchRec is bounded by bandwidth, while Bagpipe, because of caching and efficient pipelining of communication, makes better use of network resources. However, it is unclear yet, what fraction of the iteration time in Bagpipe is spent on networkbound embedding access and to understand this, we next compare Bagpipe to an ideal system which has no overhead of embedding accesses. Comparing Bagpipe with an ideal system. Comparing Bagpipe to an ideal system will show how far Bagpipe is from completely alleviating embedding access overheads. To create such an ideal system, we prefetch all necessary embeddings to the GPU memory before starting training and switch off prefetch, cache sync, and cache eviction modules of Bagpipe. In Figure 15, we perform this comparison for DLRM and DeepFM models on both p3.2xlarge and g5.8xlarge . DLRM model on p3.2xlarge instance on ideal system takes around 30ms while, Bagpipe takes around 56ms. DLRM model on g5.8xlarge on the ideal system takes around 19ms while Bagpipe takes around 30ms. This shows that at lower bandwidths for DLRM, there are periods in the pipeline when model training is blocked on embedding operations. We also study the same effect with DeepFM, a larger model that provides more opportunities for Bagpipe to overlap embedding-related operations. For DeepFM we observe that on p3.2xlarge instances ideal takes around 236ms while Bagpipe takes around 253ms (overhead 17ms). On the high bandwidth g5.8xlarge instance Figure 17. Latency of Oracle Cacher : We observe that overall Oracle Cacher scales very well, it increases sub-linearly with the increase in the number of features and batch size. However, training time will always hide the latency of Oracle Cacher. 30 40 50 60 70 80 90 100 Number of Features 0 50 100 150 200 Per Iteration Time (ms) Oracle Cacher Trainer Batch Size-16384, Lookahead Value-200 (a) Categorical Features 16384 32768 65536 131072 Batch Size 0 25 50 75 100 125 150 175 200 Per Iteration Time (ms) Oracle Cacher Trainer Lookahead Value-200, Num Features-26 (b) Batch Size ideal system takes around 116ms while Bagpipe takes around 128ms (overhead of 12ms). These results indicate that Bagpipe gets within 10% of an ideal system with deeper models and has almost constant overhead for providing embeddings (around 12 to 20 ms) even at lower bandwidths. Comparison on Different Datasets. We also analyse performance of Bagpipe on Avazu [27] and Criteo Terabyte [35] (largest publicly available dataset). Using eight p3.2xlarge instances as trainers, in Figure 12 we see that compared to TorchRec on DLRM model, Bagpipe is 1 . 9 \u00d7 to 2 . 4 \u00d7 faster. This shows that irrespective of the dataset Bagpipe provides a significant speedup over the best-performing baseline. Convergence Comparison. Since Bagpipe ensures that embedding reads are not stale, it should have the same convergence properties as synchronous training using TorchRec. We verify this in Figure 13 where we see that Bagpipe's convergence is very close to TorchRec with minor differences arising from random initialization. For HET we observed that the convergence depends on the model complexity; for DLRM, HET's open source code [23] did not converge to the same loss as TorchRec, while we observed similar convergence as TorchRec for W&D, a smaller model (We have reported this issue to the HET authors). Overall, we see that Bagpipe retains the convergence of synchronous training while providing per-iteration speedups.", "5.2 Scalability and Fault Tolerance": "Scalability. In Figure 16a, we scale batch size and number of machines (up to 32 GPUs and batch size of 65,536). With increase in batch size the number of embeddings to fetch and synchronize increases. Despite increase in communication, Bagpipe scales around 1 . 4 \u00d7 for 2 \u00d7 increase in resources and work (320K samples/sec for 16 trainers vs 446K samples/sec for 32 trainers). In Figure 16b we scale just batch size, we observe that batch size 65,536 takes a very similar time as batch size 131,072. Because with a higher batch size Bagpipe is able to overlap a bigger proportion of cache synchronization with the longer forward pass. Fault Tolerance. In Figure 14 we observe that trainer in Bagpipe recover in less than a minute, compared to FB-research system which is close to 13 minutes. For the FB-Research system we make a best-case assumption that the framework can Table 3. Effect of increasing \u2112 : With increase in \u2112 the cache size required increases but improves throughput till \u2112 of 100. checkpoint the iteration just before failure, to avoid checkpointing at every iteration. Even in this case, FB-Research system takes around 13 minutes to recover since the amount of state on each trainer includes a large shard of the embeddings. Meanwhile, trainers in Bagpipe are able to recover in less than a minute and do not require checkpointing at every iteration (\u00a73.4). Other systems do not discuss fault tolerance.", "5.3 Sensitivity Analysis of Bagpipe": "Next, we study the performance of Bagpipe with different configurations and also micro-benchmark components of Bagpipe. Unless stated otherwise, we use the same setup described \u00a75, with a batch size of 16,384 on Criteo Kaggle dataset. Overhead of Oracle Cacher. In Figure 17a, 17b we observe Oracle Cacher's overhead increases sub-linearly with increase in categorical features and batch size. However, the time per iteration is still significantly higher than the time taken to perform lookahead by Oracle Cacher. Since Oracle Cacher is overlaped with training, it only becomes a bottleneck if it's time exceeds that of trainer. Overall, we find that Oracle Cacher can almost dispatch 3.27 Million examples per second. We find that this is sufficient to power the most optimized systems reported in prior work (e.g., 8 ZionEx nodes (128 A100 GPUs) processing up to 1.6 Million samples per second [51]). We benchmarked Oracle Cacher for other parameters like different \u2112 and observe constant throughput, i.e. , complexity of Oracle Cacher does not depend on \u2112 . Effect of \u2112 . In Table 3 we study how cache size required and throughout changes for different \u2112 . As \u2112 increases, cache size required increases sub-linearly. This sub-linear behavior due to reuse of embeddings found in the previous batches during lookahead process. We also observe that throughput benefits from increasing \u2112 start plateauing beyond 100. This is because as the lookahead value goes over 200, we are keeping all the popular elements in the cache, and increasing \u2112 at this point does not affect communication much. Effect of Access Pattern Skew. Unlike some prior systems [4], Bagpipe is designed to handle skew pattern changes. To study performance of Bagpipe when the skew pattern changes, we create an artificial dataset similar to Criteo Kaggle dataset with the same number of features and samples but with different skew patterns. We choose top 1% of embeddings and then create an exponential function such that cumulative probability of sampling from top 1% embeddings is equivalent to the chosen skew, e.g. , top 1% of embeddings are responsible for 40% accesses. The remaining embeddings Figure 18. Effect of change in skew: Comparing when 1% of embeddings perform 90% of embedding accesses to just 1% of embedding access (no skew). Unlike FAE, Bagpipe's time only increases from 60.9ms to 69.7ms showing resistance to change in skew. 90% 40% 20% 10% 1% % of access by top 1% embeddings 0 250 500 750 1000 1250 1500 1750 2000 Per Iteration Time (ms) 60.9 61.8 63.1 63.7 69.7 Bagpipe FAE Figure 19. Effect of change in skew using Zipf Distribution: Varying the \ud835\udefc parameter in Zipf distribution; a higher \ud835\udefc indicates higher skew. Even with drastic increase in the skew, the time taken by Bagpipe remains almost constant. 1 2 3 4 5 Zipf Parameter 0 100 200 300 400 500 600 700 800 Per Iteration Time (ms) 64.4 64.1 63.4 61.4 61.1 Bagpipe FAE are sampled uniformly such that they lead to the remaining 60% of accesses. In Figure 18 we study how the iteration time changes as the embedding reuse of top 1% embeddings changes between 90% and 1%; e.g. , the 40% bar reflects the runtime when 1% of embeddings are reused 40% of the time. We observe that due to optimizations present in Bagpipe like pre-fetching, LRPP and delayed synchronization even when the degree of skew changes from 90% skew to no skew, Bagpipe's per iteration time changes at most by 13%. On the other hand, FAE [4], which relies only on caching degrades by 7 . 2 \u00d7 . Next we vary the skew of embedding accesses using the popular Zipf [31] distribution. The \ud835\udefc parameter in Zipf distribution determines the skew, with a higher \ud835\udefc denoting higher skew. In Figure 19 we observe that even with a large change in skew (varying Zipf's parameter between 1 and 5), Bagpipe's throughput does not vary significantly. This shows Bagpipe is resistant to changes in skew.", "6 Related Work": "Multiple systems have been developed for training large DNN's. Systems like PipeDream [52] and Gpipe [24] introduce pipeline parallelism to enable efficient model parallel training. These systems are designed for dense training, while recommendation models have sparse access patterns, also requiring a hybrid data and model-parallel setup which is not supported in these systems. Prior domain specific systems like Marius [49] and P3 [15] for Graph Neural Network training, Oort [36] and FedML [21] for federated learning do not support offline recommendation model training. Pipeswitch [6] and SwitchFlow [69] build mechanisms for fast preemption to schedule training and inference jobs to improve resource utilization. [6] relies on the idea that model weights have a deterministic access pattern (a simple example for PipeSwitch is that Layer 1 will be accessed before Layer 2), and uses this to start computation before moving the full model to the GPU. Unlike Bagpipe, Pipeswitch does not handle cases where the model weights (embeddings) are accessed dynamically depending on the training data. For offline training of recommendation models several prior systems like FAE [4], TorchRec [47], HET [48] have been designed. We have compared to these systems in the \u00a75 and discussed how Bagpipe is different. Prior work [2, 45] has also proposed system designs to enable disaggregated training of recommendation models. Unlike Bagpipe, prior systems (e.g., Monolith [45]) suffer from embedding access overheads. There have been other systems like cDLRM [7] and ScratchPipe [33] which have used the idea of lookahead to provide access to embeddings. However, these systems are primarily designed for single node training. Scaling these systems to multi-node setting is non-trivial [33] or will impose a restriction on users to use specific optimizers like [7]. Systems like cDLRM [7] only allow embedding averaging rather than gradient averaging, which restricts users from using optimizers like Adam [30] and SGD with Momentum [54] thereby potentially harming convergence and changing the underlying training algorithm. Other approaches like TTRec [72] and [19] improve performance using approximations like tensor compression and gradient compression, which unlike Bagpipe, change the training algorithm and can lead to accuracy loss.", "7 Conclusion": "We presented Bagpipe, a new system that can accelerate the training of deep learning based recommendation models. Our gains are derived from better resource utilization and by overlapping computation with data movement. Our disaggregated architecture also allows independent scaling of resources and better fault tolerance, while retaining synchronous training semantics. Our experiments show that Bagpipe provides an end-to-end speedup of up to 5 . 6 \u00d7 over state-of-the-art baselines. Acknowledgements Wewould like to thank Carole-Jean Wu for introducing us to deep learning recommendation models and the challenges in scaling their training. We would also like to thank Yibo Zhu for early discussion about our work. Finally, we would like to thank our shepherd, Junfeng Yang, the anonymous SOSP reviewers, Bilge Acun-Uyan and Muhammad Adnan for their invaluable feedback that helped in making this work better. This research was supported in part by NSF-CAREER grant CNS-2237306.", "References": "[1] Massively scale your deep learning training with nccl 2.4. https://bit. ly/341nGfs . Accessed: August 31, 2023. [2] Bilge Acun, Matthew Murphy, Xiaodong Wang, Jade Nie, Carole-Jean Wu, and Kim Hazelwood. Understanding training efficiency of deep learning recommendation models at scale. In 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA) , pages 802-814. IEEE, 2021. [3] Lada A Adamic and Bernardo A Huberman. Power-law distribution of the world wide web. science , 287(5461):2115-2115, 2000. [4] MuhammadAdnan, Yassaman Ebrahimzadeh Maboud, Divya Mahajan, and Prashant J. Nair. Accelerating recommendation system training by leveraging popular choices. Proc. VLDB Endow. , 15(1):127-140, sep 2021. [5] Michael Armbrust, Ali Ghodsi, Reynold Xin, and Matei Zaharia. Lakehouse: a new generation of open platforms that unify data warehousing and advanced analytics. In Proceedings of CIDR , 2021. [6] Zhihao Bai, Zhen Zhang, Yibo Zhu, and Xin Jin. Pipeswitch: Fast pipelined context switching for deep learning applications. In Proceedings of the 14th USENIX Conference on Operating Systems Design and Implementation , pages 499-514, 2020. [7] Keshav Balasubramanian, Abdulla Alshabanah, Joshua D Choe, and Murali Annavaram. cdlrm: Look ahead caching for scalable training of recommendation models. In Proceedings of the 15th ACM Conference on Recommender Systems , pages 263-272, 2021. [8] Laszlo A. Belady. A study of replacement algorithms for a virtualstorage computer. IBM Systems journal , 5(2):78-101, 1966. [9] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems , pages 7-10, 2016. [10] Paul Covington, Jay Adams, and Emre Sargin. Deep neural networks for youtube recommendations. In Proceedings of the 10th ACM conference on recommender systems , pages 191-198, 2016. [11] Benoit Dageville, Thierry Cruanes, Marcin Zukowski, Vadim Antonov, Artin Avanes, Jon Bock, Jonathan Claybaugh, Daniel Engovatov, Martin Hentschel, Jiansheng Huang, et al. The snowflake elastic data warehouse. In Proceedings of the 2016 International Conference on Management of Data , pages 215-226, 2016. [12] Shuiguang Deng, Longtao Huang, Guandong Xu, Xindong Wu, and Zhaohui Wu. On deep learning for trust-aware recommendations in social networks. IEEE transactions on neural networks and learning systems , 28(5):1164-1177, 2016. [13] Assaf Eisenman, Kiran Kumar Matam, Steven Ingram, Dheevatsa Mudigere, Raghuraman Krishnamoorthi, Krishnakumar Nair, Misha Smelyanskiy, and Murali Annavaram. { Check-N-Run } : a checkpointing system for training deep learning recommendation models. In 19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22) , pages 929-943, 2022. [14] Robert M French. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences , 3(4):128-135, 1999. [15] Swapnil Gandhi and Anand Padmanabha Iyer. P3: Distributed deep graph learning at scale. In 15th USENIX Conference on Operating Systems Design and Implementation (OSDI) 21 , pages 551-568, 2021. [16] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. Deepfm: A factorization-machine based neural network for ctr prediction. In IJCAI , 2017. [17] Udit Gupta, Samuel Hsia, Vikram Saraph, Xiaodong Wang, Brandon Reagen, Gu-Yeon Wei, Hsien-Hsin S Lee, David Brooks, and CaroleJean Wu. Deeprecsys: A system for optimizing end-to-end at-scale neural recommendation inference. In 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA) , pages 982995. IEEE, 2020. [69] Xiaofeng Wu, Jia Rao, Wei Chen, Hang Huang, Chris Ding, and Heng Huang. Switchflow: preemptive multitasking for deep learning. In Proceedings of the 22nd International Middleware Conference , pages 146-158, 2021. [70] C-Q Yang and Barton P Miller. Critical path analysis for the execution of parallel and distributed programs. In The 8th International Conference on Distributed , pages 366-367. IEEE Computer Society, 1988. [71] Juncheng Yang, Yazhuo Zhang, Ziyue Qiu, Yao Yue, and Rashmi Vinayak. Fifo queues are all you need for cache eviction. In Proceedings of the 29th Symposium on Operating Systems Principles , pages 130-149, 2023. [72] Chunxing Yin, Bilge Acun, Carole-Jean Wu, and Xing Liu. Tt-rec: Tensor train compression for deep learning recommendation models. In A. Smola, A. Dimakis, and I. Stoica, editors, Proceedings of Machine Learning and Systems , volume 3, pages 448-462, 2021. [73] Mark Zhao, Niket Agarwal, Aarti Basant, Bugra Gedik, Satadru Pan, Mustafa Ozdal, Rakesh Komuravelli, Jerry Pan, Tianshu Bao, Haowei Lu, et al. Understanding and co-designing the data ingestion pipeline for industry-scale recsys training. arXiv preprint arXiv:2108.09373 , 2021."}
