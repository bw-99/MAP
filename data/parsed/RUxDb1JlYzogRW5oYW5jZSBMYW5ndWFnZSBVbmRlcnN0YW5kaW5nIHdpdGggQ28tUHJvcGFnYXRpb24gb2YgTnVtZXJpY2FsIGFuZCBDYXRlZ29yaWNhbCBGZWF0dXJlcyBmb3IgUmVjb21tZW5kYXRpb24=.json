{
  "ELCoRec: Enhance Language Understanding with Co-Propagation of Numerical and Categorical Features for Recommendation": "Jizheng Chen ∗ Shanghai Jiao Tong University Shanghai, China humihuadechengzhi@sjtu.edu.cn Kounianhua Du ∗ Shanghai Jiao Tong University Shanghai, China kounianhuadu@sjtu.edu.cn Jianghao Lin Shanghai Jiao Tong University Shanghai, China chiangel@sjtu.edu.cn Bo Chen Huawei Noah's Ark Lab Shanghai, China chenbo116@huawei.com Ruiming Tang Huawei Noah's Ark Lab Shenzhen, China tangruiming@huawei.com",
  "ABSTRACT": "Large language models have been flourishing in the natural language processing (NLP) domain, and their potential for recommendation has been paid much attention to. Despite the intelligence shown by the recommendation-oriented finetuned models, LLMs struggle to fully understand the user behavior patterns due to their innate weakness in interpreting numerical features and the overhead for long context, where the temporal relations among user behaviors, subtle quantitative signals among different ratings, and various side features of items are not well explored. Existing works only fine-tune a sole LLM on given text data without introducing that important information to it, leaving these problems unsolved. In this paper, we propose ELCoRec to E nhance L anguage understanding with Co -Propagation of numerical and categorical features for Rec ommendation. Concretely, we propose to inject the preference understanding capability into LLM via a GAT expert model where the user preference is better encoded by parallelly propagating the temporal relations, and rating signals as well as various side information of historical items. The parallel propagation mechanism could stabilize heterogeneous features and offer an informative user preference encoding, which is then injected into the language models via soft prompting at the cost of a single token embedding. To further obtain the user's recent interests, we proposed a novel R ecent interaction A ugmented P rompt ( RAP ) template. Experiment results over three datasets against strong baselines validate the effectiveness of ELCoRec. The code is available at https://anonymous.4open.science/r/CIKM_Code_Repo-E6F5/README.md. ∗ Equal Contribution. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CIKM '24, Oct 21-25, 2024, Boise © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-XXXX-X/18/06...$15.00 https://doi.org/XXXXXXX.XXXXXXX Weinan Zhang Shanghai Jiao Tong University Shanghai, China wnzhang@sjtu.edu.cn The user watched the following movies in order in the past, and rated them: ['0. Taxi Driver (1976), timestamp 978229138 (5 stars)', '1. North by Northwest (1959) timestamp 987229140 (5 stars)] Based on the movie he has watched, deduce if he will like the movie ***Ronin***. LLM Target Item Feature Value Genre Director Warner Action Cameron Producer Semantical User Interaction Retrieval Prompt Construction Numerical Insensitivity Encoding Overhead Instruction Tuning Figure 1: Illustration of the motivation. LLM struggles to understand numerical features such as rating and timestamps due to numerical insensitivity. Encoding overhead fails abundant side features such as producer and genre to form the template, and time series information is lost during retrieval.",
  "CCS CONCEPTS": "· Information systems → Recommender systems .",
  "KEYWORDS": "Sequential Recommendation, Graph Attention Network, Prompt Engineering, Large Language Model",
  "ACMReference Format:": "Jizheng Chen, Kounianhua Du, Jianghao Lin, Bo Chen, Ruiming Tang, and Weinan Zhang. 2018. ELCoRec: Enhance Language Understanding with Co-Propagation of Numerical and Categorical Features for Recommendation. In Proceedings of CIKM (CIKM '24). ACM, New York, NY, USA, 11 pages. https://doi.org/XXXXXXX.XXXXXXX",
  "1 INTRODUCTION": "Recommender systems have permeated every corner of people's lives, including e-commerce [45], streaming media [15, 65, 66], and entertainment [4] platforms. They efficiently filter the parts of vast amounts of data that users are interested in, thereby improving user engagement and company benefits. Besides, large language models CIKM '24, Oct 21-25, 2024, Boise Kounianhua Du et al. (LLMs) have garnered widespread attention in the field of natural language processing (NLP) due to their outstanding reasoning and text-generation capabilities. Consequently, attempts to explore LLMs' potential for recommender systems emerge [24, 44, 61]. Nonetheless, LLMs have difficulties in comprehensively modeling user interests [26, 46], hindering further performance improvements in recommendation. In this paper, we identify two main challenges in accurate user behavior modeling, as shown in Figure 1: (1) Numerical insensitivity . LLMs struggle to accurately understand the quantitative meaning of numerical information in prompt templates and treat them as normal tokens. For example, it may ignore the temporal relation among user behavior or overlook the precise quantification of user preferences provided by numerical ratings, instead treating them as plain textual strings. (2) Encoding overhead . LLMs have high inference latency and training costs, and their dialog windows have strict length limitations. This makes it unrealistic to construct prompt templates that encompass all abundant side features like genre and producer for the whole user sequence [26]. Additionally, it necessitates filtering long user interaction histories to ensure the input response length is appropriate. Although user history retrieval has proved useful in extracting related items from whole user behavior sequence [26, 37, 38], valuable continuous time series information is lost due to the intermittent retrieved sub-sequence. These two issues result in the loss of a significant amount of information that is beneficial to recommendation performance during the process of constructing prompt templates from raw recommendation data. Existing works haven't solved the above two problems efficiently. Early works [44, 64] only leveraged the reasoning capabilities of LLMs for zero-shot prediction, neglecting the critical information that LLMs struggle to understand in the input prompt. Some works [1, 7, 9, 26, 30] transform the recommendation task into a generative task via training LLMs on recommendation data.However, this paradigm cannot deal with the numerical insensitivity problem. Recent works [20, 23, 62] introduce traditional click-through rate (CTR) prediction models such as SASRec [16] and DCN [55] to provide collaborative knowledge to LLMs. However, these works either: (1) directly apply existing general CTR prediction models without effectively capturing the relative numerical relation or (2) introduce excessive overhead of encoding side features for each historical item. In this paper, we propose ELCoRec to E nhance L anguage understanding with Co -Propagation of numerical and categorical features to address the mentioned two problems for better Rec ommendation performance. To address the numerical insensitivity problem, numerical and categorical features are parallelly propagated by a GAT expert network to offer an informative user preference encoding that enhances LLM's understanding towards numerical features. To alleviate the the encoding overhead, the preference encoding is injected into the LLM's semantic space via soft prompting at the cost of a single token embedding. ELCoRec introduces a downstream graph attention network (GAT) as a unified expert network for feature encoding. GAT has unique advantages compared to other networks [14, 51, 56], and we have meticulously designed the graph structure and feature construction approach. Compared to general CTR prediction models, our GAT expert network can better utilize heterogeneous nodes to encode features of different kinds. To capture the user's global interests while maximizing the recent information from the historical sequences, we further proposed a R ecent interaction A ugmented P rompt ( RAP ) template. Based on user history retrieval technology that filters out a sub-sequence of interacted items similar to the target item, RAP template additionally supplements the user's recent interaction history sequence to address the issue of continuous time series information loss caused by user history retrieval. Our main contributions can be summarized as follows: · We identify two problems of LLMs, i.e., numerical insensitivity and encoding overhead, for user interest modeling. Further, we proposed to address the two problems simultaneously by co-propagation of numerical and categorical features via the unified GAT expert network. To the best of our knowledge, we are the first to identify and solve the two problems altogether in a unified framework. · We proposed a novel framework ELCoRec, that injects informative user preference encoding generated by the unified GATexpert network via soft prompting at the cost of a single token embedding. A novel RAP template to better obtain user's recent interests is further proposed to form the textual input of ELCoRec. · We conduct extensive experiments on three public datasets. Extensive experimental results demonstrate the outstanding performance and effectiveness of our model.",
  "2 RELATED WORK": "",
  "2.1 Feature Interaction Methods": "Early CTR prediction methods focus on leveraging collaborative signals [25]. Classical FM [42] learns a hidden vector for each feature, capturing second-order feature interactions. Based on it, FFM [35] introduces the concept of feature fields and divides each feature into different fields to capture combinatorial interaction relationships. Deep Crossing [43] learns the interactions among features through deep neural networks. PNN [40] replaces the stacking layer with the proposed product layer to achieve more comprehensive feature interactions. Based on logistic regression, Wide&Deep [2] aims to enhance the model's memory and generalization capabilities with the wide part and the deep part, respectively. DCN [54] designed the cross network to replace the original wide part and introduced MoE structure along with low-rank operations in its improved version [55]. DeepFM [8] replaces wide layer with an FM layer, enhancing the shallow model's feature interaction capabilities. xDeepFM [22] uses a multi-layer CIN network to better capture high-order information.",
  "2.2 User Behavior Methods": "Recently more works leverage user behavior sequences to better model user interest [29]. GRU4Rec [10] proposes to apply GRU [5] units for sequential prediction. DIN [66] uses attention mechanisms to explicitly model the relevance between target items and user historical behaviors. DIEN [65] models the evolution process of user interests using recurrent neural networks and captures the dynamic changes in user interests over time. SASRec [16] applies self-attention mechanisms to user historical behavior sequences and applies position encoding to capture sequential relationships. ELCoRec: Enhance Language Understanding with Co-Propagation of Numerical and Categorical Features for Recommendation CIKM '24, Oct 21-25, 2024, Boise DSIN [6] divides user behavior sequence into sessions to capture dynamic interests. Caser [47] applies CNNs [34] to extract short-term preference sequence. MIMN [36] proposes to capture multiple aspects of user interests via a memory network architecture. SIM [37] leverages retrieval to model long user behaviors.",
  "2.3 Semantic-Enhanced Methods": "Large language models (LLMs) are being researched to assist recommendation tasks [1, 9, 17, 18, 20, 23, 30, 31, 33, 39, 44, 52, 53, 57, 59, 61-64]. CTRL [19] aligns tabular data space with response space using contrastive learning. UniSRec [12] achieves unified cross-domain recommendation by learning item representations through a fixed BERT model. Building on it, VQ-Rec [11] applies vector quantization to learn better item representations. U-BERT [39] enhances user representation quality by encoding review information through the BERT model. LMRecSys [61] directly uses a pre-trained language model as a recommendation system. P5 [7] trains a large language model on the T5 [41] model for various downstream recommendation tasks. PTab [30] models tabular data via modality conversion, language model fine-tuning, and prediction model fine-tuning steps. TALLRec [1] trains a large language recommendation model (LRLM) and designs a complete training paradigm and prompt templates for it. CoLLM [62] proposed to provide collaborative knowledge to the large model using ID and interaction information. ReLLa [26] trains the large language model on retrieved data, addressing the issue of long-sequence interest modeling.",
  "3 PRELIMINARIES": "",
  "3.1 Problem Formulation": "We focus on the click-through rate (CTR) prediction task, which is the core task in recommender systems to estimate a user's click probability towards a target item given a certain context. For the 𝑖 's sample in the dataset, traditional CTR prediction task can be formulated as:  where 𝑦 𝑖 denotes the prediction results (0 or 1). X 𝑈 𝑖 and X 𝐼 𝑖 is the set of user features and item features, respectively. X 𝐶 𝑖 is the corresponding context features (e.g. timestamp or weather), and 𝜃 denotes model parameters. [⟨ X 𝐼 𝑖 𝑘 , 𝑦 𝑖 𝑘 ⟩] 𝐾 𝑖 𝑘 = 1 represents the user behavior sequence, which contains 𝐾 𝑖 items in total and we use 𝑦 𝑖 𝑘 to represent each history ratings. For LLMs, prompt template construction is required to convert data sample 𝑥 𝑖 into regular text 𝑥 𝑡𝑒𝑥𝑡 𝑖 followed by a binary question about the user preference towards the target item. The binary label 𝑦 𝑖 also needs to be transformed into corresponding answers 𝑦 𝑡𝑒𝑠𝑡 𝑖 ∈ { ' Yes ' , ' No ' } . As we mentioned before, only the key features such as item title and a sub-sequence of user behavior items will be adopted to form a prompt template due to window length restrict and time concern. Auxiliary information can also be add to LLM, and finally the task is formulated as:  where 𝐼 𝑥 represents the external auxiliary information, 𝑥 𝑡𝑒𝑠𝑡 𝑖 is the textual prompt template.",
  "3.2 Graph Attention Networks": "Graph attetion networks [50] (GATs) are commonly used to process graph-structured data. It combines the principles of Graph Neural Networks [58] (GNNs) with the attention mechanisms to more accurately and robustly learn user and item feature via dynamically assigning different levels of importance to different neighbors. GAT works by first representing inputs as a topological graph structure. Each node 𝑖 in the graph has an initial feature vector h 𝑖 ∈ R 𝑑 . In the 𝑙 's graph attention layer, a feature aggregation process is first conducted by calculating the attention coefficient 𝑒 𝑖 𝑗 for a given node 𝑖 and each of its neighbor 𝑗 . 𝑒 𝑖 𝑗 reflects the importance of node 𝑗 's features to node 𝑖 , which is formulated as:  where 𝑎 is the activation function, and W is learnable parameter matrix. The normalized attention coefficients across all neighbors of a node can be calculated using a softmax function, denoted as:  where 𝐿𝑒𝑎𝑘𝑦𝑅𝑒𝐿𝑈 stands for the LeakyReLU activation function [60]. Afterwards, the feature vector of node 𝑖 is updated by aggregating the features of its neighbors weighted by the attention coefficients, which is formulated as:  where ∥ denotes concatenation, 𝐿𝑖𝑛𝑒𝑎𝑟 is a linear layer mapping the vector to the embedding size. A multi-head attention mechanism is applied and 𝛼 𝑘 𝑖 𝑗 and W 𝑘 stands for the attention coefficients and weight matrix of the 𝑘 's attention head, respectively. By feature aggregation and updating, GAT can effectively model the bidirectional interaction across graph nodes, thus can be applied to capture collaborative signal between user and interacted items. In this work we view each user and item, as well as each categorical item feature as a node, and initialize the node embedding using feature encoding. Detailed explanation of our GAT expert network is in Section 4.3, and we have the whole propagation process of graph attention layer defined as:",
  "4 METHODOLOGY": "In this section, we describe the methodology of the proposed RAP template and ELCoRec framework in detail, which integrates extra beneficial information into large language models with downstream GAT expert network.",
  "4.1 Overview": "Our model framework is illustrated in Figure 2. To conduct user history retrieval based on semantic similarity in the construction of RAP prompt templates, we first encode the feature descriptions of all items using a large language model encoder. Specifically, the feature information of each item is formed into textual descriptions as field-value pairs, followed by being fed into a large language model to obtain semantic representations. To reuse the encoded information, we build a knowledge index library and pre-store CIKM '24, Oct 21-25, 2024, Boise Kounianhua Du et al. Figure 2: Framework overview. (a) RAP that connects user history retrieved item sequence (marked in green) and recent item sequence (marked in blue) along with the placeholder token for embedding injection (marked in orange), to form textual prompt. (b) GAT expert network. User and item' side features along with numerical infomantion are encoded and interacted. (c) Representations from GAT expert network are injected into LLM's latent space and (d) perform instruction tuning. \" input \":  A watching history that reflects the user's preference is: 1. Taxi Driver (1976) (5 stars), 2. North by Northwest (1959) (5 stars), 3. Face/Off (1997) (4 stars). He watched the following movies in order in the past, and rated them: 1. Hard-Boiled (Lashou shentan)(1992) (5 stars), 2. Thelma & Louise (1991) (3 stars), 3. Street Fighter (1994) (2 stars). We also have information about the user's preferences encoded in the feature <Pref.>. Based on the movies he has watched, deduce if he will like the movie Ronin (1998). You should ONLY tell me yes or no. \" output \": no. ID Emb. TimeStamp Emb. Rating Emb Item Node Emb. User Node Emb. Recent Items User Behavior Sequence RAP Construction Graph Construction Retrieved Items Tokenizer Text Encoder GNN Layers Linear Projection LLM Embedding Layer LLM Hidden Layers & Head Lora (a) RAP Construction (b) GAT Expert Network (c) Embedding Injection (d) ELCoRec Training & Inference <Pref.> Preference Token the encodings of all items, with the item index as the key and corresponding semantic embedding as the value. The workflow of our proposed model contains mainly four stages: · RAP template construction. For a given user interaction history sequence as input data, we first obtain two different item sub-sequences through retrieval enhancement and recent item retention, respectively. RAP template combines the obtained two sequences along with placeholders for subsequent semantic injections. This will form the textual input for the large language model. · GATexpertnetworktraining. The GAT expert network is first pre-trained, and the user's interaction history will be encoded by a downstream GAT expert network to generate user's preference embedding towards the target item. Here is a Movie, title is Terminator 2: Judgment Day, genre is Action, publication year is 1991 Field Value Title Genre Year Terminator 2 Action 1991 Here is a {Item name}, Field_1 is Value_1, Field_2 is Value_2, Field_  is Value_3 Movie Generated Description Movie Feature Prompt Template · Embedding injection. User preference representation is injected into the large language model's semantic space at the cost of one single token after being mapped to the same dimensionality through a linear layer. · LLMfinetuning. Instruction tuning is performed using LoRA [13] on the injected representation. In this manner, continuous time-series information is maximally preserved in the input template, and the large language model can also learn essential information which it can not inherently understand from the high-quality representations encoded by the GAT expert network.",
  "4.2 RAP Template": "Due to the context window limitation and sequence incomprehension problem [26], it is crucial to properly design the prompt Figure 3: Item description construction. Each item's semantic description is obtained via the 'Feature is Value' template. template for LLMs to include the user's historical items that could largely reflect interest trends. To this end, we propose the R ecent Interaction A ugmented P rompt (RAP) template to capture both the global information related to the target item and the recent information emphasizing the latest trends of user preferences. To capture the global information related to the target item, we propose to retrieve the semantically relevant behaviors from the user's historical sequence via vector matching. We first build a knowledge index library to efficiently extract and store the semantic representations of items in the dataset. Specifically, for each item, we construct a semantic description template using a key-value pair structure. As illustrated in the example of the MovieLens-1M ELCoRec: Enhance Language Understanding with Co-Propagation of Numerical and Categorical Features for Recommendation CIKM '24, Oct 21-25, 2024, Boise movie recommendation scenario, the movie in the original dataset is named \"Terminator 2: Judgment Day,\" with the genre as Action and the release year as 1991. According to the key-value pair template construction rules, generated item descriptions can be obtained, as shown in Figure 3. Next, we input the obtained textual descriptions of the items into large language model Φ 𝑆 (·) to obtain their textual embedding representations. These representations are stored in an indexed knowledge base 𝐾𝐵 [·] , which is indexed by item numbers as lookup keys and the textual embeddings of the items as values. The construction of this knowledge index not only allows for the efficient reuse of encoded text vectors but also makes the process of similarity calculation orderly and standardized. Previous works [37, 38] indicate that user history retrieval can purify and denoise historical sequences, allowing the large language model to better capture the user's interest concerning on specific target items. Concretely, given target item 𝑖 𝑡 along with user history item sequence 𝑖 1 , 𝑖 2 , ..., 𝑖 𝑁 , we first acquire their semantic embedding from constructed knowledge base, denoted as 𝑖 𝑠 𝑡 and 𝑖 𝑠 1 , 𝑖 𝑠 2 , ..., 𝑖 𝑠 𝑁 ∈ R 𝑑 , respectively. 𝑑 represents LLM's embedding dim, and since we use Vicuna-13B [3], a text generation model based on Transformer[49] structure, we take the vectors from the last hidden layer as the semantic embedding representations. Subsequently, we measure the semantic relevance of each historical item to the target item using cosine similarity, and retrieve the most similar 𝐾 items, denoted by:  Besides the retrieved sub-sequence, we further extract user's recent interacted items from the whole behavior sequence 𝑖 1 , 𝑖 2 , ..., 𝑖 𝑁 , denoted as 𝑖 𝑁 -𝐾 , 𝑖 𝑁 -𝐾 + 1 , ...𝑖 𝑁 , to form another sub-sequence { 𝑥 𝑖 } 𝑁 𝑖 = 𝑁 -𝐾 of user history that contains original time series information. We design different prompts for user historical items extracted from two perspectives and integrate them into one single template. Additionally, to ensure that the RAP template can adapt to the subsequent GAT expert network's representation injection operation, we added placeholder token and design corresponding link prompts to smoothly form the whole template. An illustration of our RAP template construction process is shown in Figure 4. Figure 4: Construction process of RAP template. 𝑥 𝑟𝑒𝑡 𝑖 denotes the retrieved history items. Recent Interaction Augmented Prompt (RAP) Data samples with Full user interaction behaviors Semantic User Interaction Retrieval Recent User History Reservation <Pref.> \"input\": A watching history that reflects the user's preference is {Semantic User Interaction Retrieval Results}, He watched the following movies in order in past and rated them: {Recent User History Reservation}. We also have the user's preference encoded in <Pref.>, Based on the movies he watched, deduce whther he will like the movie {Target Movie}. You should only tell me yes or no. LLM Instruction Tuning",
  "4.3 GAT Expert Network": "After constructing the RAP template, this section continues to introduce the downstream GAT expert network based on it. Graph Attention Network [50] (GAT) can efficiently perform bidirectional transmission and aggregation of node information, thereby comprehensively modeling the interaction relationships between users and items, as well as the affiliation relationships between items and features. Consequently, compared with tranditional CTR prediction models like DIN [66] and DCN [54], our elaborate GAT expert network can better model the user-relecant item relationship. For 𝑖 -th user's feature 𝑋 𝑈 𝑖 = ⟨ 𝑢 𝑖𝑑 , 𝑢 𝑓 ⟩ , 𝑢 𝑖𝑑 is the user ID and 𝑢 𝑓 represents user features such as age and occupation. The features of the most recent 𝐾 items that this user has interacted with are denoted as { 𝑋 𝐼 𝑖 𝑘 } 𝑁 𝑘 = 𝑁 -𝐾 . Each item can be represented by a quadruple consisting of item ID, item features, user rating, and interaction timestamp, denoted as 𝑋 𝐼 𝑖 𝑘 = ⟨ 𝑖 𝑘 𝑖𝑑 , 𝑖 𝑘 𝑓 , 𝑦 𝑖 𝑘 , 𝑇 𝑖 𝑘 ⟩ . The target item to be predicted is represented as 𝐼 𝑡 = ⟨ 𝑖 𝑡 𝑖𝑑 , 𝑖 𝑡 𝑓 , 𝑇 𝑡 ⟩ . Here we use 𝑇 ∗ to represent the interaction timestamp, and for the target item, this field 𝑇 𝑡 is initialized to 0. The constructed user-item interaction graph has a set of nodes denoted as V = { 𝑈,𝐼 𝑡 , { 𝐼 𝑘 } 𝑁 𝑘 = 𝑁 -𝐾 , 𝐹 𝑡 , { 𝐹 𝑘 } 𝑁 𝑘 = 𝑁 -𝐾 } and a set of edges represented by E = {{ 𝑒 𝑈 ↔ 𝐼 𝑘 } 𝑁 𝑘 = 𝑁 -𝐾 , { 𝑒 𝐼 𝑘 ↔ 𝐹 𝑘 } 𝑁 𝑘 = 𝑁 -𝐾 , 𝑒 𝐼 𝑡 ↔ 𝑈 , 𝑒 𝐼 𝑡 ↔ 𝐹 𝑡 } Namely, for the user, the target item, the 𝐾 most recent items and all item features, we construct a corresponding node for each of them. We also assign bidirectional edges between the user and the items, as well as between the items and their features. This forms a specific user-item-feature interaction graph 𝑔 . The initial feature of the user node can be represented as:  where 𝐸𝑚𝑏𝑒𝑑𝑑𝑖𝑛𝑔 stands for the embedding layer of GAT, 𝐿𝑖𝑛𝑒𝑎𝑟 is linear transform layer that maps the embedding of different kind of nodes to the same dimension 𝑑 , 𝑛 is the the number of user feature fields, and ∥ represents vector concatenation. The features of an item node consist of three parts, namely ID embedding, timestamp embedding and rating embedding. The embeddings of the item ID and the rating can be directly encoded by the embedding layer, represented as:  where 𝑖 𝑘 𝑖𝑑 and 𝑦 𝑖 𝑘 stands for the item ID and numerical rating, respectively, and 𝑙 𝑘 𝑖𝑑 and 𝑙 𝑘 𝑦 is their corresponding embedding vector. To encode interaction timestamp in Unix timestamp form, we refer to the positional encoding approach of Transformer [49] in order to better model the time interval between current interaction time and the interaction times of each historical item. Concretely, For an item 𝑖 𝑘 in the user's interaction history, its interaction timestamp is 𝑇 𝑖 𝑘 . The gap between this timestamp and the target item's click time can be denoted as Δ 𝑡 𝑘 = 𝑇 𝑡 -𝑇 𝑖 𝑘 . For the 𝑝 -th element in the encoded embedding vector, when 𝑝 = 2 𝑖 is even, its timestamp encoding can be formulated as:  . CIKM '24, Oct 21-25, 2024, Boise Kounianhua Du et al. where 𝑑 is the embedding vector's dimension. And when 𝑝 = 2 𝑖 + 1 is odd, its timestamp encoding can be formulated as:  Finally, the item representation can be obtained by concatenating the embedding vectors of the above three parts and then performing linear dimensionality reduction, represented as:  where ℎ 0 𝑖 𝑘 is the initial embedding of item node. Note that for target item, the time gap Δ 𝑡 𝑘 is set to 0. For the feature nodes in the graph, the initial representation is expressed as the result of linear dimensionality reduction of the feature embedding vector, formulated as:  where 𝑙 𝑘 𝑓 denotes the embedding vector, and ℎ 0 𝑓 𝑘 is the initial representation.",
  "4.4 Embedding Injection": "Based on the GAT expert network, we further inject the embedded representations of the graph neural network into the semantic space of LLM and perform instruction tuning together with textual embedding from LLM's embedding layer. A data sample from RAP template can be formulated as 𝑝 = ( 𝑡, 𝑠, 𝑢, 𝑟, 𝑦 ) , in which 𝑡 represents the share conjunction tokens in the template, 𝑠 and 𝑟 are the textual tokens of the retrieved and the recent interacted items, respectively. 𝑦 represents the tokens of labeled output, 𝑢 represents the placeholder token for the position where we are about to inject the embedding from the GAT expert network. The above text is then tokenized and encoded by the LLM's textual embedding layer, generating semantic embedding:  where e 𝑖 ∈ R 𝑑 𝐿𝐿𝑀 × 1 is a one-dimensional vector with the same dimension as the hidden layer of the large language model, e 𝑈 ∈ R 𝑑 𝐿𝐿𝑀 × 1 is the vector of the placeholder token encoded by the large language model. On the other hand, let the target item be 𝑖 𝑡 . The representation of the target item encoded by the trained GAT model is denoted as e 𝑖 𝑡 ∈ R 𝑑 𝐺𝐴𝑇 × 1 , where 𝑑 𝐺𝐴𝑇 is the GAT network's embedding size. We then apply a linear mapping layer to map the target item representation to the same dimension as LLM's hidden layer. The whole process can be expressed as:  where 𝑓 𝐺𝐴𝑇 refers to the GAT expert network, 𝑔 𝑡 represents the linear mapping layer with weight matrix W ∈ R 𝑑 𝐺𝐴𝑇 × 𝑑 𝐿𝐿𝑀 . Finally, we feed the following embedding vector to the latter transformer layers of LLM to perform instruction tuning:",
  "4.5 ELCoRec: Training & Inference": "This section explains the training and inference process of our proposed ELCoRec. The training process can be divided into two stages. Westart by pretraining our GAT expert network. After constructing the feature interaction graph, we then utilize the Graph Attention Network (GAT) to train the GAT expert network. Since the constructed graph is a heterogeneous graph which contains three types of nodes: User, Item, and Feature, as well as four different types of edges: Interacted (from User to Item), ClickBy (from Item to User), BelongTo (from Item to Feature), and HasInstance (from Feature to Item). For each type of edge, the graph attention mechanism is applied to aggregate and update the nodes. Concretely, given a node 𝑖 and its embedding at 𝑙 -th graph attention layer, its representation is updated using graph attention mechanism, denoted by:  where 𝑓 𝐺𝐴𝑇 is the graph attention layer defined at Equation 6, N ∗ 𝑖 denotes node 𝑖 's neighbor under one certain edge type, 𝑊 𝑙 is the weight matrix in 𝑙 -th layer. The above formula first calculates the node similarity, and then computes the attention weight coefficients using the softmax function. The node representations are updated based on the multi-head attention mechanism, and generate next layer's representation ℎ 𝑙 + 1 𝑖 . After multiple layers of graph neural network propagation, we obtain the final representations of the user and the target item, denoted as e 𝑢 and e 𝑖 𝑡 , respectively. The final click-through rate estimation of the model is expressed as:  where 𝜎 (·) is the Sigmoid activation function which maps the cosine similarity to the [ 0 , 1 ] interval. The training loss function is expressed as the cross-entropy loss between the estimated value and the true value, denoted as:  Upon obtaining the pretrained GAT expert network, we train our proposed framework by first constructing the RAP prompt template using the method described in Section 4.2 for each data sample. This data sample simultaneously enters the GAT expert network to obtain user interest representation, which is then injected into the semantic space of the large language model according to the method described in Section 4.4 for subsequent instruction finetuning. During the instruction fine-tuning process, we fix the main parameters of the GAT expert network and the large language model, and only update the parameters of the linear mapping layer in the injection process and the parameters of the LoRA [13] module used for instruction tuning. We applied the causal LLM's objective for instruction tuning, which is formulated as:  where Θ is the trainable parameter after LoRA wrapping, D denotes the dataset generated by RAP construction, E ℎ𝑖𝑑 is the hidden ELCoRec: Enhance Language Understanding with Co-Propagation of Numerical and Categorical Features for Recommendation CIKM '24, Oct 21-25, 2024, Boise representation after feature injection. We use 𝑦 < 𝑗 to represent the token before position 𝑗 . During the inference process, we fix the model parameters. For each sample, we construct the RAP prompt template and obtain the user interest representation. This representation is then injected into the large language model to obtain its output result.",
  "5 EXPERIMENTS": "In this section, we conduct experiments to answer the following research questions (RQs). RQ1 How does ELCoRec perform against the baselines? RQ2 Does the RAP template contributes to the performance? RQ3 Does the GAT expert network contributes to the performance? RQ4 How is the extra cost for ELCoRec to encode extra features? RQ5 How does the training sample number affects performance?",
  "5.1 Setup": "5.1.1 Datasets. We evaluate our model on three representative public datasets. Detailed statistical feature is listed in Table 1. Table 1: Dataset statistics. · ML-1M 1 is a collection of 1 million movie ratings from the MovieLens website, commonly used in recommendation researches. · ML-25M 2 is a larger popular movie recommendation dataset containing 25 million interactions on movies from 1995 to 2019. · BookCrossing 3 is a comprehensive collection of user ratings for books, including demographic information, gathered from the BookCrossing community. For Ml-1M and ML-25M dataset, we split the training, validation and testing dataset with the ratio of 8:1:1, according to the global timestamp, and 81,920 samples are randomly selected from the ML-25M testing dataset to form our testing data. For BookCrossing dataset, we randomly split training and testing datasets based on user with the ratio 9:1, following previous works [1, 26]. 5.1.2 Evaluation Metrics. Three commonly used metrics are adopted to evaluate the performance: AUC (area under the ROC curve) measures model's ability to distinguish between classes, LogLoss (binary cross-entropy loss) quantifies the accuracy of predicted probabilities by penalizing false classifications, ACC (accuracy) directly calculates the proportion of correctly predicted instances. 5.1.3 Baseline Models. CTR prediction baseline models can be classified into three mainly categories: (1) Feature interaction models capture high-order relationships between user and item features by modeling collaborative signals. (2) User behavior models leverage user behavior sequences to capture the development trends of user 1 https://grouplens.org/datasets/movielens/1m/ 2 https://files.grouplens.org/datasets/movielens/ml-25m.zip 3 http://www2.informatik.uni-freiburg.de/ cziegler/BX/ interests. (3) Semantic-enhanced models leverages the encoding or inference capabilities of LMs to enhance recommendation performance. We choose AutoInt [27], DCNv1 [54], DCNv2 [55], DeepFM [8], FGCNN [28], FiGNN [21], PNN [40], Wide&Deep [2] and xDeepFM [22] as the representative feature interaction models, and choose DIN [66], DIEN [65], GRU4Rec [10], Caser [47] and SASRec [16] as representative user behavior models. For semantic enhanced models, we choose P5 [7], PTab [30], CTR-BERT [52], CoLLM [62] and ReLLa [26] as the competitive baselines. 5.1.4 Implementation Details. For feature interaction and user behavior models, we train and test the models on full datasets. We add the average vector of historical item embeddings as an extra feature for the feature interaction models for fair comparison. The learning rates for these models are searched within range { 1 𝑒 -4 , 3 𝑒 -4 , 5 𝑒 -4 , 1 𝑒 -3 } , and the weight decay parameters are searched within { 1 𝑒 -5 , 3 𝑒 -5 , 5 𝑒 -5 , 1 𝑒 -4 , 3 𝑒 -4 } . For each model the embedding dim is set to 32, the batch size is set to 1024, and we set the patience of early stop to 10. We train our GAT expert network with two graph attention layers on the full training dataset of ML-1M and BookCrossing, and sample 1 , 000 , 000 data points to train on ML-25M. Other settings are kept consistent with the traditional CTR models. For the three LLM-based recommendation algorithms we evaluate i.e., CoLLM [62], ReLLa [26], and our proposed ELCoRec, we uniformly select Vicuna-13B [3] as the backbone LLM to ensure a fair comparison. To enhance training efficiency, we apply 8-bit quantization and use LoRA for parameter-efficient fine-tuning. The rank of the low-rank matrices in LoRA is uniformly set to 8, alpha set to 16, and dropout is set to 0 . 05. The LoRA modules are uniformly applied to the 𝑄 and 𝑉 parameter matrices of the attention modules in each layer of the LLM. All the three models are optimized using the AdamW [32] optimizer. Due to the expensive training overhead of LLM, we randomly sample 65 , 536 training samples on ML-1M and ML-25M datasets, and use all 15 , 942 training data on BookCrossing. Each model is trained for one epoch, with the learning rate fixed at 1 𝑒 -3, weight decay set to 0, and batch size set to 256. We only encode the item title feature to form the textual prompt template. Traditional CTR models, however, use all feature information.",
  "5.2 Overall Performance (RQ1)": "In this section, we compare our proposed model ELCoRec with various baseline models. Their performance is listed in Table 2. · Our proposed RAP prompt template and ELCoRec framework can achieve stable performance improvements over all the baseline models on three evaluation metrics. This demonstrates that our proposed model's prompt template design for time-series completion and the method of explicitly injecting timestamp and rating information using an GAT expert network, are indeed effective and can significantly enhance the model's recommendation performance. · Our proposed method further surpasses the LLM-based methos i.e., ReLLa and CoLLM. This also validates that our GAT expert network's advantage over traditional CTR models in feature encoding and information propagation via a unified graph design. CIKM '24, Oct 21-25, 2024, Boise Kounianhua Du et al. Table 2: Major results. The best results for each metric are given in bold, and the second best results are underlined. Rel.Impr. denotes the relative AUC improvement of ELCoRec w/ RAP against other baselines. The symbol ∗ represents significant improvement with 𝑝 -value < 0.001. For feature interaction models we add history item embedding for a fair comparison. This enables the model to capture the comprehensive trend of changes in user interests, thereby better modeling user interests. achieve better performance, and its performance can be further enhanced with the addition of RAP template. · In the comparison of baseline model performance, although P5, PTab, and CTR-BERT utilize LM's textual encoding capability, their performance is still sub-optimal. Meanwhile, ReLLa and CoLLM surpass other traditional recommendation models and semantic recommendation models, indicating that methods of purifying user interaction history and encoding external information can improve the LLM's ability to model user interests.",
  "5.3 Ablation Studies (RQ2 & RQ3)": "5.3.1 Impact of RAP Template (RQ2). In this section, we verify the effectiveness of the RAP template. Concretely, we remove the RAP template within with both retrieved items and recent interacted items are integrated, only retaining retrieved items. This results in the same prompt template as ReLLa. We also apply our RAP template to ReLLa. The results are shown in Table 3. For fairness, the two item sequences using the RAP template each takes 𝐾 = 15 items, while the sequence without the template retains 𝐾 = 30 historical items from history retrieval. All models are trained using 65 , 536 samples. Table 3: Effect of RAP template. Fromthe results, it can be observed that the RAP prompt template can enhance the model's understanding of user sequences solely from the data side. Moreover, our proposed ELCoRec itself can 5.3.2 Impact of the GAT Expert Network (RQ3). In this section, we validate the effectiveness of the GAT expert network that is used to encode external numerical features. Specifically, we sequentially removed the item interaction timestamps and ratings from the encoding of the graph node representations and verify their impact on model performance. The corresponding results are shown in Table 4, where TS refers to timestamp field, and R refers to rating field. We train our ELCoRec model without the RAP template using 32 , 768 samples and 𝐾 = 15 history length on ML-1M and ML-25M datasets. For comparison purposes, we also conduct tests using the AutoInt [27] model without rating information on both datasets. Table 4: Ablation study on numerical fields. Firstly we can observe that the encoding of the rating information has the greatest impact on model performance. This is mainly because the user's historical behavior sequence contains both positive and negative samples. Without rating information, the model cannot perceive the user's preference for history items. But even under the same experimental conditions without rating information, our model still achieves performance improvements compared to traditional recommendation models. One can also observe that timestamp information contributes to model performance. The above experimental results indicate that ELCoRec: Enhance Language Understanding with Co-Propagation of Numerical and Categorical Features for Recommendation CIKM '24, Oct 21-25, 2024, Boise our design of explicitly encoding timestamp and rating information into the GAT expert network indeed injects information that cannot be obtained from the textual domain into LLM, thereby improving the user interest modeling capability of ELCoRec. We also perform t-SNE [48] visualization on the feature vectors encoded by the GAT expert network. Specifically, we randomly select 10,000 test sample points and feed them to the trained GAT model and the text embedding layer of the large language model. We then conduct t-SNE visualization results after dimensionality reduction on the two embeddings. The visualization results on ML1M and ML-25M datasets are shown in Figure 5, from which we can see that the representations encoded by our GAT expert network form different manifolds in the response space, proving that they can indeed provide information that the semantic space of the large language model does not inherently possess. t-SNE Visualization on ML-IM t-SNE Visualization on ML-25M LLH emb Figure 5: t-SNE visualiztion of GAT and LLM's embedding. LLH emb GAT erb",
  "5.4 Feature Encoding Cost Analysis (RQ4)": "In this section, we validate whether our proposed model can alleviate encoding overhead. The GAT expert network has its parameters freezed during the finetune stage. ELCoRec incurs only a tokenlevel cost to inject rich user historical interaction features, along with numerical rating and timestamp information that LLM finds difficult to understand, into the semantic space of it. During training, the only additional trainable parameters are the mapping layers for the GAT's embedding features. During inference, each sample only requires an extra forward pass through the GAT network. Therefore, compared to previous methods that only fine-tune a sole LLM, ELCoRec can more efficiently encode additional features and achieve significant improvements in recommendation performance. Table 5: Comparison of training and inference time. First we display the training and inference time of ELCoRec and ReLLa in Table 5. The experiments are done on a single A6000 with AMD EPYC 7763 64-Core Processor. Note that for ELCoRec, the training and inference time includes time cost on feature encoding with the GAT expert network. From the results, we can see that though more features are encoded, our proposed method does not cause a heavy load compared with ReLLa. Wefurther count the average input prompt's length our proposed ELCoRec in training and testing dataset, and compare with ReLLa's input prompt and the template that directly puts all other features Table 6: Prompt length statistic. that our model use GAT expert network to encode into the prompt. The results are displayed in Table 6. It can be observed from the results that our model reduces the input length by about one-quater compared to encoding all features into the input textual prompt template.",
  "5.5 Data Efficiency (RQ5)": "In this section we validate our proposed method's data efficiency by alternating the number of training samples 𝑁 , and compare the performance with ReLLa. The results are depicted in Figure 6. 8192 16384 32768 65536 # Training Samples 0.70 0.72 0.74 0.76 0.78 0.80 AUC ML-1M ELCoRec AUC ReLLa AUC 0.62 0.60 0.58 0.56 0.54 0.52 Negative Log Loss ELCoRec Neg. Log Loss ReLLa Neg. Log Loss 8192 16384 32768 65536 # Training Samples 0.70 0.72 0.74 0.76 0.78 0.80 0.82 0.84 ML-25M ELCoRec AUC ReLLa AUC 0.54 0.52 0.50 0.48 0.46 0.44 Negative Log Loss ELCoRec Neg. Log Loss ReLLa Neg. Log Loss AUC Figure 6: Data efficiency visualization. We can observe that as the number of samples 𝑁 increases, both ReLLa and ELCoRec enjoy performance improvements. However, even when training with a smaller number of samples, ELCoRec can still maintain higher performance than ReLLa. This indicates that our model can effectively digest and learn the different perspectives of information and knowledge brought by the prompt template and GAT expert network even under small-sample conditions, demonstrating its sample efficiency.",
  "6 CONCLUSION": "Large language models have demonstrated remarkable potential in the recommendation, but they also face challenges in comprehensively modeling user interests. In this paper, we address two critical challenges hindering the performance of LLMs in recommender systems, i.e. numerical insensitivity and encoding overhead. Our proposed framework, ELCoRec, along with the Recent interaction Augmented Prompt (RAP) template, provides a novel solution by incorporating recent user interactions and leveraging a graph attention network to enhance feature encoding. By injecting encoded representations into the LLM with minimal additional costs, ELCoRec effectively mitigates the identified issues. Extensive experiments on three public datasets validate the superior performance and efficacy of our approach. Our work sets a precedent in resolving these challenges within a unified and efficient framework, paving the way for more effective use of LLMs in recommendation tasks. CIKM '24, Oct 21-25, 2024, Boise Kounianhua Du et al.",
  "REFERENCES": "[1] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation. In Proceedings of the 17th ACM Conference on Recommender Systems (RecSys '23). ACM. https://doi.org/10. 1145/3604915.3608857 [2] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems. 7-10. [3] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality. https://lmsys.org/blog/2023-03-30-vicuna/ [4] Ingrid A. Christensen and Silvia Schiaffino. 2011. Entertainment recommender systems for group of users. Expert Systems with Applications 38, 11 (2011), 14127-14135. https://doi.org/10.1016/j.eswa.2011.04.221 [5] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555 (2014). [6] Yufei Feng, Fuyu Lv, Weichen Shen, Menghan Wang, Fei Sun, Yu Zhu, and Keping Yang. 2019. Deep session interest network for click-through rate prediction. arXiv preprint arXiv:1905.06482 (2019). [7] Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022. Recommendation as language processing (rlp): A unified pretrain, personalized prompt & predict paradigm (p5). In Proceedings of the 16th ACM Conference on Recommender Systems. 299-315. [8] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. arXiv preprint arXiv:1703.04247 (2017). [9] Stefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi Jiang, and David Sontag. 2023. Tabllm: Few-shot classification of tabular data with large language models. In International Conference on Artificial Intelligence and Statistics. PMLR, 5549-5581. [10] Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2015. Session-based recommendations with recurrent neural networks. arXiv preprint arXiv:1511.06939 (2015). [11] Yupeng Hou, Zhankui He, Julian McAuley, and Wayne Xin Zhao. 2023. Learning Vector-Quantized Item Representation for Transferable Sequential Recommenders. arXiv:2210.12316 [cs.IR] [12] Yupeng Hou, Shanlei Mu, Wayne Xin Zhao, Yaliang Li, Bolin Ding, and Ji-Rong Wen. 2022. Towards Universal Sequence Representation Learning for Recommender Systems. arXiv:2206.05941 [cs.IR] [13] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. LoRA: Low-Rank Adaptation of Large Language Models. arXiv:2106.09685 [cs.CL] [14] Ziniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou Sun. 2020. Heterogeneous graph transformer. In Proceedings of the web conference 2020. 2704-2710. [15] Yanhua Huang, Hangyu Wang, Yiyun Miao, Ruiwen Xu, Lei Zhang, and Weinan Zhang. 2022. Neural Statistics for Click-Through Rate Prediction. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. 1849-1853. [16] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recommendation. In 2018 IEEE international conference on data mining (ICDM). IEEE, 197-206. [17] Wang-Cheng Kang, Jianmo Ni, Nikhil Mehta, Maheswaran Sathiamoorthy, Lichan Hong, Ed Chi, and Derek Zhiyuan Cheng. 2023. Do llms understand user preferences? evaluating llms on user rating prediction. arXiv preprint arXiv:2305.06474 (2023). [18] Jiacheng Li, Ming Wang, Jin Li, Jinmiao Fu, Xin Shen, Jingbo Shang, and Julian McAuley. 2023. Text is all you need: Learning language representations for sequential recommendation. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 1258-1267. [19] Xiangyang Li, Bo Chen, Lu Hou, and Ruiming Tang. 2023. CTRL: Connect Collaborative and Language Model for CTR Prediction. arXiv:2306.02841 [cs.IR] [20] Xinhang Li, Chong Chen, Xiangyu Zhao, Yong Zhang, and Chunxiao Xing. 2023. E4SRec: An Elegant Effective Efficient Extensible Solution of Large Language Models for Sequential Recommendation. arXiv:2312.02443 [cs.IR] [21] Zekun Li, Zeyu Cui, Shu Wu, Xiaoyu Zhang, and Liang Wang. 2019. Fi-gnn: Modeling feature interactions via graph neural networks for ctr prediction. In Proceedings of the 28th ACM international conference on information and knowledge management. 539-548. [22] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong Sun. 2018. xdeepfm: Combining explicit and implicit feature interactions for recommender systems. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining. 1754-1763. [23] Jianghao Lin, Bo Chen, Hangyu Wang, Yunjia Xi, Yanru Qu, Xinyi Dai, Kangning Zhang, Ruiming Tang, Yong Yu, and Weinan Zhang. 2024. ClickPrompt: CTR Models are Strong Prompt Generators for Adapting Language Models to CTR Prediction. In Proceedings of the ACM on Web Conference 2024 (WWW '24). 3319-3330. [24] Jianghao Lin, Xinyi Dai, Yunjia Xi, Weiwen Liu, Bo Chen, Hao Zhang, Yong Liu, Chuhan Wu, Xiangyang Li, Chenxu Zhu, Huifeng Guo, Yong Yu, Ruiming Tang, and Weinan Zhang. 2023. How Can Recommender Systems Benefit from Large Language Models: A Survey. arXiv preprint arXiv:2306.05817 (2023). [25] Jianghao Lin, Yanru Qu, Wei Guo, Xinyi Dai, Ruiming Tang, Yong Yu, and Weinan Zhang. 2023. MAP: A Model-agnostic Pretraining Framework for Clickthrough Rate Prediction. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 1384-1395. [26] Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang Quan, Ruiming Tang, Yong Yu, and Weinan Zhang. 2024. ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation. In Proceedings of the ACM on Web Conference 2024 (WWW '24). 3497-3508. [27] David B Lindell, Julien NP Martel, and Gordon Wetzstein. 2021. Autoint: Automatic integration for fast neural volume rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1455614565. [28] Bin Liu, Ruiming Tang, Yingzhi Chen, Jinkai Yu, Huifeng Guo, and Yuzhou Zhang. 2019. Feature generation by convolutional neural network for click-through rate prediction. In The World Wide Web Conference. 1119-1129. [29] Chengkai Liu, Jianghao Lin, Jianling Wang, Hanzhou Liu, and James Caverlee. 2024. Mamba4Rec: Towards Efficient Sequential Recommendation with Selective State Space Models. arXiv preprint arXiv:2403.03900 (2024). [30] Guang Liu, Jie Yang, and Ledell Wu. 2022. Ptab: Using the pre-trained language model for modeling tabular data. arXiv preprint arXiv:2209.08060 (2022). [31] Zhenghao Liu, Sen Mei, Chenyan Xiong, Xiaohua Li, Shi Yu, Zhiyuan Liu, Yu Gu, and Ge Yu. 2023. Text Matching Improves Sequential Recommendation by Reducing Popularity Biases. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management. 1534-1544. [32] Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 (2017). [33] Zhiming Mao, Huimin Wang, Yiming Du, and Kam-Fai Wong. 2023. Unitrec: A unified text-to-text transformer and joint contrastive learning framework for text-based recommendation. arXiv preprint arXiv:2305.15756 (2023). [34] Keiron O'shea and Ryan Nash. 2015. An introduction to convolutional neural networks. arXiv preprint arXiv:1511.08458 (2015). [35] Harshit Pande. 2020. Field-embedded factorization machines for click-through rate prediction. arXiv preprint arXiv:2009.09931 (2020). [36] Qi Pi, Weijie Bian, Guorui Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Practice on long sequential user behavior modeling for click-through rate prediction. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2671-2679. [37] Qi Pi, Guorui Zhou, Yujing Zhang, Zhe Wang, Lejian Ren, Ying Fan, Xiaoqiang Zhu, and Kun Gai. 2020. Search-based user interest modeling with lifelong sequential behavior data for click-through rate prediction. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management. 2685-2692. [38] Jiarui Qin, Weinan Zhang, Xin Wu, Jiarui Jin, Yuchen Fang, and Yong Yu. 2020. User behavior retrieval for click-through rate prediction. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. 2347-2356. [39] Zhaopeng Qiu, Xian Wu, Jingyue Gao, and Wei Fan. 2021. U-BERT: Pre-training user representations for improved recommendation. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35. 4320-4327. [40] Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, and Jun Wang. 2016. Product-based neural networks for user response prediction. In 2016 IEEE 16th international conference on data mining (ICDM). IEEE, 1149-1154. [41] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research 21, 140 (2020), 1-67. http://jmlr.org/papers/v21/20-074.html [42] Steffen Rendle. 2010. Factorization machines. In 2010 IEEE International conference on data mining. IEEE, 995-1000. [43] Ying Shan, T. Ryan Hoens, Jian Jiao, Haijing Wang, Dong Yu, and J. C. Mao. 2016. Deep Crossing: Web-Scale Modeling without Manually Crafted Combinatorial Features. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (2016). https://api.semanticscholar.org/ CorpusID:9704646 [44] Damien Sileo, Wout Vossen, and Robbe Raymaekers. 2022. Zero-shot recommendation as language modeling. In European Conference on Information Retrieval. Springer, 223-230. [45] Brent Smith and Greg Linden. 2017. Two Decades of Recommender Systems at Amazon.com. IEEE Internet Computing 21, 3 (2017), 12-18. https://doi.org/10. ELCoRec: Enhance Language Understanding with Co-Propagation of Numerical and Categorical Features for Recommendation CIKM '24, Oct 21-25, 2024, Boise 1109/MIC.2017.72 [46] Zhaoxuan Tan and Meng Jiang. 2023. User Modeling in the Era of Large Language Models: Current Research and Future Directions. arXiv preprint arXiv:2312.11518 (2023). [47] Jiaxi Tang and Ke Wang. 2018. Personalized top-n sequential recommendation via convolutional sequence embedding. In Proceedings of the eleventh ACM international conference on web search and data mining. 565-573. [48] Laurens van der Maaten and Geoffrey E. Hinton. 2008. Visualizing Data using t-SNE. Journal of Machine Learning Research 9 (2008), 2579-2605. https://api. semanticscholar.org/CorpusID:5855042 [49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. Attention Is All You Need. arXiv:1706.03762 [cs.CL] [50] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint arXiv:1710.10903 (2017). [51] Saurabh Verma and Zhi-Li Zhang. 2019. Stability and generalization of graph convolutional neural networks. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 1539-1548. [52] Dong Wang, Kavé Salamatian, Yunqing Xia, Weiwei Deng, and Qi Zhiang. 2023. BERT4CTR: An Efficient Framework to Combine Pre-trained Language Model with Non-textual Features for CTR Prediction. arXiv:2308.11527 [cs.CL] [53] Hangyu Wang, Jianghao Lin, Xiangyang Li, Bo Chen, Chenxu Zhu, Ruiming Tang, Weinan Zhang, and Yong Yu. 2023. FLIP: Towards Fine-grained Alignment between ID-based Models and Pretrained Language Models for CTR Prediction. arXiv e-prints (2023), arXiv-2310. [54] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network for ad click predictions. In Proceedings of the ADKDD'17. 1-7. [55] Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong, and Ed Chi. 2021. DCN V2: Improved Deep &amp; Cross Network and Practical Lessons for Web-scale Learning to Rank Systems. In Proceedings of the Web Conference 2021 (WWW '21). ACM. https://doi.org/10.1145/3442381.3450078 [56] Yangkun Wang, Jiarui Jin, Weinan Zhang, Yongyi Yang, Jiuhai Chen, Quan Gan, Yong Yu, Zheng Zhang, Zengfeng Huang, and David Wipf. 2021. Why propagate alone? parallel use of labels and features on graphs. arXiv preprint arXiv:2110.07190 (2021). [57] Likang Wu, Zhaopeng Qiu, Zhi Zheng, Hengshu Zhu, and Enhong Chen. 2024. Exploring large language model for graph data understanding in online job recommendations. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 9178-9186. [58] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. 2020. A comprehensive survey on graph neural networks. IEEE transactions on neural networks and learning systems 32, 1 (2020), 4-24. [59] Yunjia Xi, Weiwen Liu, Jianghao Lin, Jieming Zhu, Bo Chen, Ruiming Tang, Weinan Zhang, Rui Zhang, and Yong Yu. 2023. Towards open-world recommendation with knowledge augmentation from large language models. arXiv preprint arXiv:2306.10933 (2023). [60] Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. 2015. Empirical evaluation of rectified activations in convolutional network. arXiv preprint arXiv:1505.00853 (2015). [61] Yuhui Zhang, Hao Ding, Zeren Shui, Yifei Ma, James Zou, Anoop Deoras, and Hao Wang. 2021. Language models as recommender systems: Evaluations and limitations. In NeurIPS 2021 Workshop on I (Still) Can't Believe It's Not Better. https://www.amazon.science/publications/language-models-asrecommender-systems-evaluations-and-limitations [62] Yang Zhang, Fuli Feng, Jizhi Zhang, Keqin Bao, Qifan Wang, and Xiangnan He. 2023. Collm: Integrating collaborative embeddings into large language models for recommendation. arXiv preprint arXiv:2310.19488 (2023). [63] Zizhuo Zhang and Bang Wang. 2023. Prompt Learning for News Recommendation. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '23). ACM. https://doi.org/10. 1145/3539618.3591752 [64] Aakas Zhiyuli, Yanfang Chen, Xuan Zhang, and Xun Liang. 2023. Bookgpt: A general framework for book recommendation empowered by large language model. arXiv preprint arXiv:2305.15673 (2023). [65] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Deep Interest Evolution Network for ClickThrough Rate Prediction. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019. 5941-5948. [66] Guorui Zhou, Xiaoqiang Zhu, Chengru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep Interest Network for Click-Through Rate Prediction. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2018, London, UK, August 19-23, 2018. 1059-1068. Received 20 February 2007; revised 12 March 2009; accepted 5 June 2009",
  "keywords_parsed": [
    "Sequential Recommendation",
    " Graph Attention Network",
    " Prompt Engineering",
    " Large Language Model"
  ]
}