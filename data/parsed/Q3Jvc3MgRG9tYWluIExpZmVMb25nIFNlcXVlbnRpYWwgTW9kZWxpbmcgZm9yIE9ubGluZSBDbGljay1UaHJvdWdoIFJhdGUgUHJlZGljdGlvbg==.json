{
  "Cross-Domain LifeLong Sequential Modeling for Online Click-Through Rate Prediction": "Ruijie Hou Wechat Channels, Tencent Beijing, China jerriehou@tencent.com Zhaoyang Yang ∗ Wechat Channels, Tencent Guangzhou, China terrellyang@tencent.com Yu Ming Wechat Channels, Tencent Guangzhou, China julyeyang@tencent.com Hongyu Lu Wechat Channels, Tencent Guangzhou, China dreamlu@tencent.com",
  "Zhuobin Zheng": "Wechat Channels, Tencent Guangzhou, China jackzbzheng@tencent.com Qinsong Zeng Wechat Channels, Tencent Guangzhou, China qinzzeng@tencent.com Yu Chen Wechat Channels, Tencent Guangzhou, China nealcui@tencent.com Ming Chen Wechat Channels, Tencent Guangzhou, China mingchen@tencent.com",
  "ABSTRACT": "Lifelong sequential modeling (LSM) has significantly advanced recommendation systems on social media platforms. Diverging from single-domain LSM, cross-domain LSM involves modeling lifelong behavior sequences from a source domain to a different target domain. In this paper, we propose the Lifelong Cross Network (LCN), a novel approach for cross-domain LSM. LCN features a Cross Representation Production (CRP) module that utilizes contrastive loss to improve the learning of item embeddings, effectively bridging items across domains. This is important for enhancing the retrieval of relevant items in cross-domain lifelong sequences. Furthermore, we propose the Lifelong Attention Pyramid (LAP) module, which contains three cascading attention levels. By adding an intermediate level and integrating the results from all three levels, the LAP module can capture a broad spectrum of user interests and ensure gradient propagation throughout the sequence. The proposed LAP can also achieve remarkable consistency across attention levels, making it possible to further narrow the candidate item pool of the top level. This allows for the use of advanced attention techniques to effectively mitigate the impact of the noise in cross-domain sequences and improve the non-linearity of the representation, all while maintaining computational efficiency. Extensive experiments conducted on both a public dataset and an industrial dataset from the WeChat Channels platform reveal that the LCN outperforms current methods in terms of prediction accuracy and online performance metrics.",
  "CCS CONCEPTS": "· Information systems → Learning to rank ; Recommender systems ; · Computing methodologies → Neural networks .",
  "KEYWORDS": "Click-Through Rate Prediction; Lifelong Sequential Modeling; Recommendation System ∗ corresponding author",
  "1 INTRODUCTION": "Click-through rate (CTR) prediction stands as a fundamental task in many real-world applications. The precision of CTR prediction heavily relies on comprehending the users' intentions towards the potential candidates. In recent years, deep neural networks (DNNs) have made significant strides in improving the accuracy of CTR prediction. They accomplish this by extracting representations of user interests from behavior sequences, specifically in relation to the candidate items [12, 36, 39, 41, 42]. Nowadays, social media platforms such as TikTok, YouTube, and WeChat Channels, presents billions of items to users every day. The complexity of user interactions on these platforms has significantly increased, posing new challenges in modeling user behavior sequences. On one hand, the volume of data has expanded exponentially, with some sequences extending to a length of lifelong. On the other hand, users often engage with a variety of content types, leading to behavior sequences that contains items from different domains. This complexity becomes particularly pronounced when focusing on CTR prediction for smaller-scale domains. In order to overcome the lack of direct user data in the target domain, it becomes imperative for models to extract user interests from behavior sequences in an auxiliary source domain. For instance, as illustrated in 1, within WeChat Channels, the median behavior sequence length for live content is a mere 500, which is a fraction of the length for video content. Therefore, for live content CTR prediction, it is imperative to employ cross-domain lifelong sequential modeling (LSM) of video item interactions. This approach ensures comprehensive prediction that captures the user's integrated interests across the platform. A common approach to managing lifelong sequences involves segmenting the modeling into two units: the General Search Unit (GSU) and the Exact Search Unit (ESU) [28]. The GSU's role is to sift through the sequence to identify items that are most relevant to the candidate items. Subsequently, the ESU is responsible for extracting user interest representations from the items identified KDD '24, 2024, Conference Ruijie and Zhaoyang, et al. Figure 1: (a) A showcase of video content and live content in Wechat Channels platform. (b) A comparison between the statistics of the length of behavior sequence of video content and live content of users in Wechat Channels platform. Live fced 6000 content 4000 2000 539 Median Maximum Live content (a) (b) Vidco Video by the GSU. This framework has been the foundation for numerous studies, leading to notable advancements in the field [4, 5, 8]. Despite their successes, applying these methods to cross-domain LSM faces two primary challenges. Firstly, the effectiveness of GSU relies on expressive item embeddings to measure similarity between users' sequence items and candidate items. However, only optimizing these embeddings based on supervision signals from the target domain severely restricted their generalizability across domains and makes it difficult to capture cross-domain relations accurately. Aligning the representation space between candidate items in target domain and the sequence items in source domain becomes challenging. Secondly, most ESU methods use simpler attention techniques to minimize computational demands and maintain consistency with the GSU. However, cross-domain sequence contains more noise, which highly increases the difficulty and complexity of modeling. Therefore, more advanced attention techniques are necessary to effectively filter out irrelevant data within the sequence and enhance the final user interest representations in cross-domain LSM. To address these challenges, we propose a novel Lifelong Cross Network (LCN) for cross-domain LSM. The LCN features a Cross Representation Production (CRP) module to improve the model's capabilities for item similarities across domains. Our experience suggests that users' short-term behaviors from different domains indicate similar intrinsic interests, which guides us in designing auxiliary supervision tasks. Inspired by contrastive learning, the CRP selects positive and negative item pairs from user behavior sequences within and across domains. This approach provides additional supervision on the item embeddings, which enhances the learning of relationships between items across domains. Additionally, we integrate a Lifelong Attention Pyramid (LAP) module to enhance the extraction of user interest representations from lifelong sequences. The LAP module consists of three levels of cascading attentions, with each level processing top-ranked items from the previous level. The final output of LAP combines the results from all three levels, providing a comprehensive representation of user interests. This structure ensures full gradient propagation throughout the sequence, improving consistency across levels. This consistency further narrows the item pool of the top attention level, enabling the incorporation of advanced attention techniques to filter out noise in cross-domain lifelong sequences and enhance the non-linearity of representations while maintaining computational efficiency. We conducted comprehensive experiments using both a public dataset and an industrial dataset collected from user interactions on the WeChat Channels platform. The results demonstrate that the proposed LCN outperforms existing methods in terms of prediction accuracy. The integration of CRP and LAP modules within the LCN has shown applicability to different LSM backbones and scenarios. Notably, the LCN has also achieved significant improvements in the A/B testing phase for online live content recommendations, with a relative increase of +2.93% in CTR and +3.27% in stay time. These results highlight the effectiveness and robustness of the proposed LCN in enhancing CTR prediction in cross-domain LSM.",
  "2 RELATED WORK": "The modeling of user behavior sequences plays a central role in understanding user intentions, and extensive research has been dedicated to this field. It has been observed that models tend to achieve better performance when longer sequence lengths are incorporated [27]. With the exponential growth of data, the concept of lifelong sequential modeling (LSM) has emerged, aiming to extract user interests with respect to the candidate items over extensive sequence lengths. One notable approach addressing LSM is SIM [28], which divides the process into a General Search Unit (GSU) and an Exact Search Unit (ESU), thereby managing lifelong sequences with reduced computational demands. Subsequent research has built upon this framework, leading to notable advancements [4, 5, 8, 29]. However, these methods have not fully addressed the unique challenges associated with cross-domain LSM, and may experience performance declines due to insufficient transfer from the source to the target domain. There are a diverse array of studies contributing to the field of cross-domain recommendation (CDR). A significant body of research advocates for the training of independent models within a source domain [16, 19, 24]. Additionally, the variational autoencoder (VAE) framework has been employed in several work to learn domain-invariant embeddings [3, 21, 30]. Another approach involves the integration of Meta Networks [43, 44]. Sequential modeling has also seen the incorporation of CDR techniques. Notably, PSJNet [32] and 𝜋 -Net [23] have adopted gating mechanisms to achieve this integration. In parallel, DA-GCN [7] and C2DSR [2] have leveraged Graph Neural Networks (GNNs), and RecGURU [18] has utilized Transformer architectures. However, most of these studies focus on short sequence modeling, which limit their applicability to LSM due to the inherent differences in sequence length. Contrastive learning has seen significant advancements in the fields of computer vision and natural language processing [9-11, 13, 26]. It has also been adapted to enhance the Click-Through Rate (CTR) prediction tasks [2, 20, 22, 31, 35, 37, 38, 40]. In the context of sequential modeling, MISS [15] utilizes contrastive learning to refine interest representation, while AQCL [25] introduces an auxiliary loss to learn item relationships from sparse training samples. CL4CTR [35] aims to enhance item representation quality through contrastive learning, yet it requires the management of a large embedding table and triplets during training, which may be impractical in real-world applications. In this paper, we harness the principles of contrastive learning to impose additional supervision, enabling the model to learn item embeddings that can facilitate the bridging of items across different domains. Cross-Domain LifeLong Sequential Modeling for Online Click-Through Rate Prediction KDD '24, 2024, Conference",
  "3 PRELIMINARIES": "Cross-domain LSM involves modeling sequences of user behaviors from one domain (the source) and applying results gained to a different domain (the target). The objective of cross-domain LSM is to extract a representation of user interests based on their lifelong sequences in the source domain, which can then be used to enhance the accuracy of click-through rate (CTR) predictions in the target domain. A key aspect of this scenario is that the same user base is active in both domains, yet there is no item overlap between them. Specifically, to formalize the modeling, we consider three distinct categories of features for each user: · Basic profile features, denoted as { 𝛽 } . · Short-term behavior sequences in the target domain and the source domain, represented by fi 𝐻𝑇 𝑡 = { ℎ𝑡 1 , ℎ𝑡 2 , · · · , ℎ𝑡 𝑁 } and fi 𝐻𝑆 𝑡 = { ℎ𝑠 1 , ℎ𝑠 2 , · · · , ℎ𝑠 𝑁 } respectively. · The lifelong behavior sequence in the source domain, represented by fi 𝐿𝐻𝑆 𝑡 = { 𝑙ℎ𝑠 1 , 𝑙ℎ𝑠 2 , · · · , 𝑙ℎ𝑠 𝑀 } . For a given user-item pair < 𝑢 𝑖 , 𝑣 𝑖 >, where 𝑣 𝑖 is an item from the target domain, the model aims to predict the click-through rate (CTR) for user 𝑢 𝑖 with respect to item 𝑣 𝑖 as follows:  where 𝜃 represents the parameters of the model. The main network is optimized using a cross-entropy loss function, which is defined as follows:  where 𝑦 𝑖 ∈ { 0 , 1 } represents the actual user feedback, and 𝐵 denotes the total number of user-item sample pairs in a training batch.",
  "4 METHODOLOGY": "In this paper, we propose a novel approach, the Lifelong Cross Network (LCN) for cross-domain LSM. The LCN is comprised of two major components: the Cross Representation Production (CRP) module and the Lifelong Attention Pyramid (LAP) module. The CRP module is a jointly trained sub-network with the objective of learning item embeddings that can bridge items across domains. These embeddings are then utilized within the main network to enhance its ability to identify the most relevant items with respect to a given candidate item from a lifelong sequence in the source domain. The LAP module is structured with three levels of cascading attentions, each processing the top ranked items of the previous level. The progressive nature of the LAP module ensures a more context-aware extraction of interest representations that are directly relevant to the target item. We show an overview of the proposed LCN in Figure 2.",
  "4.1 Cross Representation Production": "A lifelong behavior sequence records a user's interactions over an extended period. When considering a particular item, only a fraction of this sequence may hold predictive value for the user's click-through rate (CTR) on that item. This is particularly true for cross-domain lifelong sequences, where the items originate from a different domain than the target item. It is crucial for the model to identify the most relevant items within the sequence to optimize model capacity and computational efficiency. Typically, Lifelong Sequence Modeling (LSM) is segmented into two units: a General Search Unit (GSU) and an Exact Search Unit (ESU). The role of GSU is to sift through the lifelong sequence and identify the items most relevant to the candidate item. Its effectiveness highly depends on the quality of the item embeddings utilized. Previous approaches re-use item embeddings learned during training of the model, which are proved to perform well within the training data's distribution. However, when the candidate item and the sequence items belong to different domains, the embeddings must be developed to bridge the gap between source and target domains. Achieving this is non-trivial, given that the model is primarily trained on the target domain data. To overcome this challenge, we propose a Cross Representation Production (CRP) module that jointly refines cross-domain item embeddings along with the main network. Drawing inspiration from contrastive learning, the CRP module pairs positive and negative examples from user's short-term behavior sequences fi 𝐻𝑇 𝑡 and fi 𝐻𝑆 𝑡 . It then imposes extra supervision on the item embeddings to enhance learning of relationships between items across domains. 4.1.1 Positive and Negative Sampling. The construction of sample pairs is fundamental to contrastive learning. Within our CRP module, we achieve this by selecting items from user's short-term behavior sequences. This selection process is grounded in the understanding that a user's interests tend to remain stable across different domains, especially within a short-term period of time. Consequently, items within a user's short-term behavior sequence are likely to exhibit similar characteristics, regardless of their domain of origin. This consistency enables us to uniformly sample both positive and negative pairs across domains. As depicted in Figure 3, we have designed three distinct types of positive pairs for each user. To begin with, we select two items, ℎ𝑡 𝑝𝑡 1 and ℎ𝑡 𝑝𝑡 2 , from the target domain's short-term sequence fi 𝐻𝑇 𝑡 , forming a positive pair within the target domain. Similarly, we choose two items, ℎ𝑠 𝑝𝑠 1 and ℎ𝑠 𝑝𝑠 2 , from the source domain's shortterm sequence fi 𝐻𝑆 𝑡 , creating a source domain positive pair. These pairs are intended to encourage the model to bring item embeddings of similar items closer within their respective domains. To learn alignment between similar items across domains, we construct a cross-domain positive pair by selecting one item each from fi 𝐻𝑇 𝑡 and fi 𝐻𝑆 𝑡 , denoted as ℎ𝑡 𝑝𝑐 1 and ℎ𝑠 𝑝𝑐 2 . Regarding negative sample pairs, we employ a parallel approach to the positive sampling, but with items from different users' behavior sequences within the same training batch. This results in three types of negative sample pairs: < ℎ𝑡 𝑛𝑡 1 , ℎ𝑡 𝑛𝑡 2 >, < ℎ𝑠 𝑛𝑠 1 , ℎ𝑠 𝑛𝑠 2 > and < ℎ𝑡 𝑛𝑐 1 , ℎ𝑠 𝑛𝑐 2 >. For each type, we sample 𝑀 pairs per training batch, which are then used as the negative counterparts for all positive pairs within that batch. 4.1.2 Loss Function. The CRP module employs a contrastive loss function designed to enforce the model to minimize the cosine distance between item embeddings for positive pairs within a given batch. Each type of positive pairs is associated with a corresponding loss function as follows: KDD '24, 2024, Conference Ruijie and Zhaoyang, et al. Sigmoid Positon-Wisc FFN PReLu MultiHcad PRcLu Flaren LPs itcm id Itcm id Sidc Info The Focus-Scopc Attention (FSA) Thc Focuscd-Scope Positon-Wisc FFN htn Projcction Thc Complcte MultiHead Targct Attention Targct item_id Side Info Inncr Product Uscr Targct Itcm Targct-domain Uscr Sourcc-domain Uscr Uscr Profilc Contcxt Lifelong Bchavior Sequence Target Samples Short-term Scquencc ilem_id Source Scq itcm_id Attention (CSAL Lifelong Attention Pyramid. Cross Representation Production. LcRP LpT op-A 2 hs\" Figure 2: An overview of the proposed Lifelong Cross Network (LCN). There are two major components in the model: (i) The Cross Representation Production (CRP) module is a jointly trained sub-network to learn item embeddings capable of identifying similar items across domains. (ii) The Lifelong Attention Pyramid (LAP) module is composed of three levels of cascading attentions. These attentions are designed to progressively extract interest representations with respect to candidate items from the cross-domain lifelong sequence. LpT Sumpling Sampling 5 Negative pairs",
  "4.2 Lifelong Attention Pyramid": "Figure 3: An illustration of the Cross Representation Production (CRP) module. It samples three distinct types of positive pairs and negative pairs. We introduce a contrastive loss to reduce the cosine distance between items in positive pairs while expand those of negative pairs.    where 𝐵 represents the size of training batch, and 𝑠 ( 𝑥,𝑦 ) denotes the cosine similarity between 𝑥 and 𝑦 . The final loss of the CRP module, denoted as L 𝐶𝑅𝑃 , is a integration of Equation 3, 4 and 5:  where 𝜆 𝑃𝑇 , 𝜆 𝑃𝑆 and 𝜆 𝑃𝐶 represent the loss weights. In the experimental section, we will demonstrate that integrating the CRP module substantially enhances the representational quality of the item embeddings. This improvement, in turn, boosts the final prediction accuracy across a broad spectrum of LSM frameworks. Classic LSM framework segments interest extraction into two units: the General Searching Unit (GSU) and the Exact Searching Unit (ESU), with the latter generally employing more complex attention techniques than the former. Although the GSU stage significantly narrows down the item pool for the ESU stage, the attention utilized in ESU are often less sophisticated than state-of-the-art (SOTA) attention techniques to maintain computational efficiency. In the context of cross-domain LSM, items from the source domain in the behavior sequence and the candidate items from the target domain do not overlap, and the contextual and behavioral patterns can vary greatly between domains. Consequently, crossdomain lifelong sequences may introduce more noise, calling for better consistency across different searching stages and the use of more advanced attention techniques to interpret the sequences. To address these challenges, we propose the Lifelong Attention Pyramid (LAP) module. This module extends the traditional twostage framework into a three-level attention pyramid, featuring cascading levels of attention that aim to refine and streamline the search process within the lifelong sequence. By achieving better consistency across levels, the LAP reduces the number of items progressing to the top level. This reduction allows for the application of more advanced attention techniques at the top level to filter noise and enhance the non-linearity of the representation. We detail each of the levels, named the Complete-Scope Attention (CSA), the Median-Scope Attention (MSA), and the Focused-Scope Attention (FSA), in the following sections. 4.2.1 The Complete-Scope Attention (CSA). As the first level of attention, the Complete-Scope Attention (CSA) mirrors the function of the GSU from previous frameworks. Within the CSA, a broad yet general search is executed across the entire lifelong sequence. The objective is to ensure that every item within the sequence is accounted for, thereby excluding the least relevant items from subsequent levels. Meanwhile, the CSA can provide a preliminary Cross-Domain LifeLong Sequential Modeling for Online Click-Through Rate Prediction KDD '24, 2024, Conference interest representation by implementing a weighted average pooling based on the attention scores derived during the searching. Given the considerable length of the input at this level and the less exact precision requirements, we employ a simple inner product calculation as the attention for the CSA. Formally, for a given candidate item 𝑣 𝑖 and the lifelong behavior sequence fi 𝐿𝐻𝑆 𝑡 = { 𝑙ℎ𝑠 1 , 𝑙ℎ𝑠 2 , · · · , 𝑙ℎ𝑠 𝑁 } , the attention score 𝑟 𝑐𝑠𝑎 𝑘 for each item 𝑙ℎ𝑠 𝑘 within the sequence is computed as follows:  where 𝑒 ℎ 𝑘 represents the embedding of the item 𝑙ℎ𝑠 𝑘 in the sequence, and 𝑒 𝑣 𝑖 is the embedding of the candidate item. A straightforward weighted summation based on the score 𝑟 𝑐𝑠𝑎 𝑘 is utilized to generate the interest representation output 𝐼𝑅 𝐶𝑆𝐴 of this level:  The top𝐾 1 items, as ranked by 𝑟 𝑐𝑠𝑎 𝑘 , are then selected to construct a reduced sub-sequence fi 𝐿𝐻𝑆 𝑐 1 , which is subsequently fed into the second level of attention. 4.2.2 The Median-Scope Attention (MSA). In contrast to previous two-stage methods, our approach introduces an intermediate Median-Scope Attention (MSA) level to effectively bridge the first and third levels. This addition is inspired by the observation that a significant majority of attention scores, often more than 90%, tend to concentrate on a mere 20% of the items in a sequence. By incorporating the MSA, we distribute some of the CSA's function of omitting less relevant items to this intermediate level, enhancing the overall consistency of the LAP module and further narrowing the item pool for the final attention level. In the MSA, we incorporate additional contextual information about the items to perform a secondary attention search. It is important to note that the contextual details used here, such as interaction specifics like viewing duration, are consistent with the information that will be utilized in the final level. This consistency is crucial, as these contextual elements have been shown to significantly benefit sequential modeling [5]. Formally, let fi 𝐿𝐶𝑆 𝑐 1 = { 𝑙𝑐𝑠 1 , 𝑙𝑐𝑠 2 , ..., 𝑙𝑐𝑠 𝐾 1 } represents the contextual information of the items provided by the CSA. The attention score 𝑟 𝑚𝑠𝑎 𝑘 at this level is calculated using the following formulas:  where 𝑊 𝑄 and 𝑊 𝐾 denote the attention weights, 𝑑 represents the inner dimension, 𝑒 𝑐 𝑘 is the embedding of the contextual information 𝑙𝑐𝑠 𝑘 for item 𝑙ℎ𝑠 𝑘 , and | | signifies the concatenation operation. The interest representation within the MSA, denoted as 𝐼𝑅 𝑀𝑆𝐴 , is then derived through a weighted average pooling, expressed as:  where 𝑊 𝑉 is the projection matrix. Following the first level, we form a sub-sequence fi 𝐿𝐻𝑆 𝑐 2 by selecting the top𝐾 2 items according to their 𝑟 𝑚𝑠𝑎 𝑘 rankings. These selected items will then serve as the input for the final level. 4.2.3 The Focused-Scope Attention (FSA). The objective of the final attention level mirrors that of the ESU, aiming to deliver a detailed and targeted interest representation with respect to the candidate item, based on the most relevant items filtered through the previous attention levels. The Focused-Scope Attention (FSA) benefits from a smaller set of items, enabling the use of a more advanced attention technique to increase the non-linearity of the representation. We employ an attention similar to the decoder of the multi-head transformer [34] to extract interests from various perspectives. Formally, for a candidate item 𝑣 𝑖 and the sub-sequence fi 𝐿𝐻𝑆 𝑐 2 , the output of the ℎ -th head in the multi-head attention is computed as:    This attention process differs from the one used in MSA only in the multi-head aspect to maintain consistency. Building upon this, we integrate results of different head and use an additional feedforward layer to further enhance the non-linearity of the model:  where 𝑙 is the number of attention heads, 𝑤 1, 𝑤 2, 𝑏 1 and 𝑏 2 are the weights and biases of the feed-forward layer, respectively. The final interest representation, denoted as 𝐼𝑅 𝐹𝑆𝐴 , is then obtained by:  At the end of the LAP module, we integrate the three interest representations, 𝐼𝑅 𝐶𝑆𝐴 , 𝐼𝑅 𝑀𝑆𝐴 and 𝐼𝑅 𝐹𝑆𝐴 , from each attention level to generate a representation that encapsulates a broad spectrum of user interests as reflected in the lifelong sequence. This integration also ensures full gradient propagation throughout the sequence, potentially enhancing the consistency between the different attention levels. The implications and benefits of this approach will be further explored and discussed in the experimental section. The entire LCN, including the CRP module, is designed for endto-end training. The final loss function for LCN is an combination of the CTR loss in Equation 2 and the CRP loss in Equation 6:  where 𝜆 𝐶𝑅𝑃 denotes the factor to control the importance of 𝐿 𝐶𝑅𝑃 .",
  "5 EXPERIMENTS": "To assess the performance of the proposed LCN, we carried out extensive experiments and compared it against previous methods using both a public dataset and an industrial dataset. Additionally, we implemented online A/B testing to further validate the effectiveness of LCN. The details of our experimental setup, along with the results and discussions, are introduced in this section.",
  "5.1 Experimental Settings": "5.1.1 Datasets . Ourexperiments was conducted using two datasets: one public dataset and one an industrial dataset. Public Dataset . We utilizes the Taobao Dataset 1 , which consists of traffic logs from Taobao's recommendation system. This dataset 1 https://tianchi.aliyun.com/dataset/56 KDD '24, 2024, Conference Ruijie and Zhaoyang, et al. encompasses more than 2 million records from over 1 million users, collected over a period of 7 days. We have split the items in the the dataset into two distinct domains based on the 'Category ID' associated with each item. Items with a 'Category ID' below 2000 are allocated to the target domain, while those with a 'Category ID' of 2000 or higher are allocated to the source domain. We consider the latest 24 user actions in both the source and target domains to construct short-term sequences. For lifelong sequences, we use the most recent 500 user actions from the source domain. The dataset is partitioned temporally, with the initial 6 days' data reserved for training and the data from the 7th day used as the test set. Industrial Dataset . This dataset was collected from traffic logs of the Wechat Channels platform, containing user behavior sequences related to both video and live content. For each user, we collected his short-term behavior sequences for video and live items, as well as lifelong behavior sequence up to a maximum length of 2,000 for video items. The video item sequences were designated as the sequences in the source domain. The label of each sample is the user's click action with live items. The dataset comprises a collection of 2 billion records from 0.4 billion users, gathered over a period of 7 days. We partitioned the dataset temporally, with the data from the initial 6 days used for training and the data from the 7th day served as the test set. 5.1.2 Competitors . To evaluate the proposed LCN, we selected an array of state-of-the-art (SOTA) methods in the field of LSM for comparison. Our initial baseline was established using SIM Soft [28]. Below is an overview of the competing methods: · SIM Soft [28]: An early work that proposes to segment LSM into GSU and ESU stages. · ETA [8]: An approach for end-to-end lifelong sequential modeling that incorporates SimHash [6] for efficient GSU. · SDIM [4]: Introduces a novel Hash Attention technique to integrate GSU with ESU. · TWIN [5]: An approach that employs dimension compression of cross features to enhance the consistency between GSU and ESU. For training these models, we adhered to the parameter configurations recommended in their respective original publications or their open-source implementations. 5.1.3 Metrics . For offline evaluation, we employed the following three key metrics: Area Under the Curve (AUC), Grouped Area Under the Curve (GAUC) and the value of the Logarithmic Loss (logloss) for the CTR prediction tasks. For online A/B testing, the evaluation was standardized by using the CTR and stay time of users on the items presented. We also monitored the inference latency as a secondary metric. 5.1.4 Parameter Settings . The core architecture of our network consists of a straightforward two-layer feed-forward neural network. For the public dataset, the dimensions of the layers are configured to 256 and 128, while for the industrial dataset, we scale up to 512 and 256 to accommodate its larger size. The user profile features { 𝐵 } comprises profile information such as age, gender, and income level. We standardized the maximum length of the short-term behavior sequence to 50 for both datasets. All features, including item and user ids, are assigned distinct embedding spaces. Table 1: Results of the ablation study of the CRP module. A uniform embedding size of 64 is applied across all feature types. Model parameters are initialized using the Xavier Initialization method [14], and the model is optimized with the Adam optimizer [17] at a learning rate of 0.001. We set the batch size to 2048. Our models are developed using TensorFlow [1]. We trained the model using single A100 GPU for the public dataset, while 8 A100 GPUs are used a distributed manner for the industrial dataset.",
  "5.2 Module Analyses": "We detail experiments designed to assess the efficiency of the two major components of LCN. These experiments are carried out on the industrial dataset, which is more appropriate for our topic given its substantial data volume and extended sequence length. 5.2.1 Analyses of CRP . For the CRP module, we performed an ablation study alongside an embedding analysis to quantitatively and qualitatively assess its impact. Furthermore, we demonstrate the effectiveness of the CRP module when being incorporated with previous LSM frameworks. Ablation Study. The outcomes of the experiments are presented in Table 1. In the experiment where the contrastive loss for the cross-domain sample pairs L 𝑃𝐶 was omitted, the process of sampling cross-domain positive and negative pairs was also excluded. Additionally, in the case where no loss components were applied, the CRP module was entirely removed. The results reveal that even without L 𝑃𝐶 , the CRP module contributes to a considerable enhancement over the initial baseline. This improvement suggests that the supervision provided by the contrastive loss aids the model in more effectively capturing the relationships between items within the embeddings, thereby refining the search quality in the GSU. The best performance was achieved when all three contrastive losses were employed. This underscores the importance of incorporating an additional contrastive loss to learn the similarities between items across the source and target domains, which is crucial for cross-domain LSM. It appears that training the model solely with the target domain's CTR loss is insufficient for aligning embeddings across different domains. This misalignment is a primary factor in the degraded performance of previous LSM methods in crossdomain LSM, as the GSU struggles to deliver stable and precise search results without embeddings that encapsulate cross-domain information. The CRP module addresses this, significantly improve the efficiency of cross-domain LSM. Representation Quality. To provide a clearer insight into the quality improvement of the learned item embeddings, we visualized the clustering of item embeddings, as depicted in Figure 4. Specifically, we employed the T-SNE technique [33] to distill the first two principal components from the learned embeddings, treating these values as coordinates in a two-dimensional space. By plotting these coordinates and color them according to item categories, Cross-Domain LifeLong Sequential Modeling for Online Click-Through Rate Prediction KDD '24, 2024, Conference Source-domain Item Target-domain Item Source-domain Item Target-domain Item Item Item Embedding from LCN w/o CRP . Embedding from LCN. Figure 4: Visualization of the item embeddings from 𝐿𝐶𝑁 𝑤 / 𝑜 𝐶𝑅𝑃 and 𝐿𝐶𝑁 . Table 2: Performance comparisons for different LSM methods when incorporating the CRP module. we created a visual map that illustrates the embeddings' ability to differentiate between item categories, which is a direct measure of the embeddings' capacity to pair similar items. We can observe that while embeddings learned without the CRP module can cluster items in the target domain with a quality comparable to those learned with the CRP module, there is a marked difference in the source domain, where embeddings refined with the CRP module exhibit superior clustering. This suggests that the additional contrastive loss introduced by the CRP module significantly enhances the quality of the embeddings in the source domain. This enhancement is a key factor in the performance improvements achieved by implementing the CRP module. Backbone Substitution. We extended our evaluation of the CRP module by integrating it into multiple LSM frameworks. In these experiments, the CRP module was utilized to refine item embeddings, which were then re-used in the GSU and ESU of the respective frameworks. The results are summarized in Table 2. The results indicate that the CRP module consistently enhances the performance of all tested frameworks in managing cross-domain LSM. This improvement is largely attributed to the superior quality of the item embeddings generated for search purposes. These embeddings are proven to be beneficial regardless of variations in the underlying framework architecture. It is important to note that in all experiments, the CRP module was trained jointly along with the main network. This means that while CRP introduces additional losses during the training phase, it does not impose any extra computational cost during inference. Table 3: Comparison of model performance under different MSA and FSA settings. 5.2.2 Analyses of LAP . Wecarried out experiments to investigate various factors influencing the performance of the LAP, focusing on the consistency across different levels and the impact of level sizes. Additionally, we evaluated LAP in a single-domain LSM to demonstrate the applicability of the proposed LAP. Ablation Study. A series of experiments was conducted to evaluate the influence of the LAP module, particularly examining its performance in relation to the size of the item pools for MSA ( 𝐾 1) and FSA ( 𝐾 2). The results are summarized in Table 3. In the absence of both 𝐾 1 and 𝐾 2, we established our initial baseline with the SIM Soft model. When either 𝐾 1 or 𝐾 2 was not included, the corresponding attention level was omitted from the LAP module. The results indicate that LAP outperforms the initial baseline, even with the removal of either MSA or FSA. This improvement is largely attributed to the enhanced gradient flow across its attention levels, which improves the learning efficiency of the model. The performance further improves with the complete implementation of both MSA and FSA. Furthermore, it can be observed that the performance exhibits minimal sensitivity to the specific values assigned to 𝐾 1 and 𝐾 2, particularly once they exceed a quarter of the size of the previous level. This confirms that the introduction of an intermediate MSA level promotes greater consistency across the levels. Consequently, the performance of the model remains stable, even with a reduced number of items progressing to the advanced attention in the FSA, which can contribute to decreased inference latency. Consistency Analyses. We conducted a comparative analysis to examine the consistency across different stages of LSM among various LSM methods. For each method, we began by processing the entire lifelong sequence using the parameter and attention settings of the final LSM stage, ESU for previous methods and FSA for our proposed LAP. From this, we extracted the top-50 items based on their attention scores and used it as the real set. Similarly, we identified the top-50 items using the parameter and attention settings of the previous stages, GSU for previous methods and CSA, MSAfor LAP, to obtain the test set. The consistency of each method was then evaluated by calculating the proportion of items in the real set that were identified in the test set. As depicted in Figure 5-(a), the LAP method, with 𝐾 1 and 𝐾 2 set to 200 and 50 respectively, demonstrated superior consistency compared to other methods. This can be attributed to the presence KDD '24, 2024, Conference Ruijie and Zhaoyang, et al. Figure 5: Comparisons of the consistencies between GSU & ESU (a) of different LSM methods and (b) under different settings of 𝐾 1 . SIM Soft TWIN LAP 1 SIM Soft ETA TWIN LAP LAP 200 500 1000 2000 (a) Table 4: Comparison of performance on single-domain LSM. of the MSA level, which effectively bridges the CSA and FSA. It is also worth noting that a significant enhancement was achieved whenintegrating (concatenating) the results from all three attention levels as the final output of LAP. This is mainly because that the integration facilitates gradient flow throughout the sequence, which is crucial for improving consistency across different levels. Wefurther conducted a set of experiments to assess the influence of 𝐾 1 on consistency. The results, depicted in Figure 5-(b), reveal that consistency is augmented with larger values of 𝐾 1 and tends to plateau once 𝐾 1 exceed a quarter of the maximum sequence length, corroborating the model performance trends observed in the previous section. This indicates that the interplay between MSA and FSA is highly consistent, owing to the contextual information included in MSA. This also highlights the value of MSA, which is key to maintain consistency across levels while considerably narrowing the item pool needed in the FSA. Adaptability Test. We conducted experiments to assess the adaptability of the proposed LAP in the context of single-domain LSM. For these experiments, we omitted CRP module and relied on user interactions within the original source domain (video content) as the training labels. The results are summarized in Table 4. The results indicate that LAP continues to outperform previous methods in the context of single-domain LSM, though with a smaller margin of improvement compared to cross-domain LSM. This reduced gain is likely due to the relative ease of modeling lifelong sequences within a single domain, where the data distribution and behavioral patterns among items in the sequence and the candidate items tend to be more consistent.",
  "5.3 Overall Performance": "The overall performance results of the proposed LCN alongside previous methods on both public and industrial datasets are presented in Table 5. We denote different settings of 𝐾 1 and 𝐾 2 as LCN𝐾 1𝐾 2. Table 5: Final performance comparisons on both the public and the industrial datasets. Across all metrics and datasets, the proposed LCN consistently outperforms the other methods. It is important to note, however, that the margin of improvement on the public dataset is narrower than that on the industrial dataset. This is largely due to the public dataset's smaller sequence lengths and data volume. The performance difference on the industrial dataset is likely a more accurate reflection of the model's true generalization capabilities and its effectiveness in real-world recommendation scenarios.",
  "5.4 Online Testing": "To further validate the efficiency of the proposed LCN, we conducted an online A/B test to assess the quality of its recommendations. We utilized LCN-200-50 to maintain a balance between computational efficiency and performance. Over a seven-day period, we collected user feedback to calculate online metrics. The results were significant, with group B achieving a relative increase of +2.93% in CTR and +3.27% in stay time. Moreover, the inference latency for group B was only 3 ms longer than that of group A, which is a negligible trade-off considering the substantial gains in user experience delivered.",
  "6 CONCLUSIONS": "In this paper, we propose the Lifelong Cross Network (LCN) for cross-domain lifelong sequential modeling (LSM). LCN is composed of two components: the Cross Representation Production (CRP) module and the Lifelong Attention Pyramid (LAP) module. The CRP module is a sub-network that is supervised by contrastive loss to learn item embeddings that can bridge similar items across domains. The LAP module is structured with three levels of cascading attentions to extract interest representations from the lifelong sequence with respect to the candidate item. By integrating these two modules, the proposed LCN can effectively identify relevant items across domains, achieving a highly consistent and computational efficient interest extraction process within the lifelong sequence in the source and improving the CTR prediction in the target domain. Cross-Domain LifeLong Sequential Modeling for Online Click-Through Rate Prediction KDD '24, 2024, Conference The results of experiments on public and industrial datasets reveal that the proposed LCN significantly improves the model's capacity to handle cross-domain LSM. The results also indicate the adaptability of the CRP and LAP modules for their effectiveness when integrated with different LSM backbones or applied to LSM within a single domain. As for future work, we aim to refine the integration of the CRP module with the main network and investigate enhancements to the LAP module's inter-level connections.",
  "REFERENCES": "[1] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. 2016. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467 (2016). [2] Jiangxia Cao, Xin Cong, Jiawei Sheng, Tingwen Liu, and Bin Wang. 2022. Contrastive Cross-Domain Sequential Recommendation. In Proceedings of the 31st ACMInternational Conference on Information & Knowledge Management . 138-147. [3] Jiangxia Cao, Jiawei Sheng, Xin Cong, Tingwen Liu, and Bin Wang. 2022. CrossDomain Recommendation to Cold-Start Users via Variational Information Bottleneck. arXiv:2203.16863 [cs.IR] [4] Yue Cao, XiaoJiang Zhou, Jiaqi Feng, Peihao Huang, Yao Xiao, Dayao Chen, and Sheng Chen. 2022. Sampling Is All You Need on Modeling Long-Term User Behaviors for CTR Prediction. arXiv:2205.10249 [cs.IR] [5] Jianxin Chang, Chenbin Zhang, Zhiyi Fu, Xiaoxue Zang, Lin Guan, Jing Lu, Yiqun Hui, Dewei Leng, Yanan Niu, Yang Song, and Kun Gai. 2023. TWIN: TWo-stage Interest Network for Lifelong User Behavior Modeling in CTR Prediction at Kuaishou. arXiv:2302.02352 [cs.IR] [6] Moses S Charikar. 2002. Similarity estimation techniques from rounding algorithms. In Proceedings of the thiry-fourth annual ACM symposium on Theory of computing . 380-388. [7] Fengwen Chen, Shirui Pan, Jing Jiang, Huan Huo, and Guodong Long. 2019. DAGCN: Dual Attention Graph Convolutional Networks. arXiv:1904.02278 [cs.LG] [8] Qiwei Chen, Changhua Pei, Shanshan Lv, Chao Li, Junfeng Ge, and Wenwu Ou. 2021. End-to-End User Behavior Retrieval in Click-Through RatePrediction Model. arXiv:2108.04468 [cs.IR] [9] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A Simple Framework for Contrastive Learning of Visual Representations. arXiv:2002.05709 [cs.LG] [10] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A Simple Framework for Contrastive Learning of Visual Representations. arXiv:2002.05709 [cs.LG] [11] Xinlei Chen and Kaiming He. 2020. Exploring Simple Siamese Representation Learning. arXiv:2011.10566 [cs.CV] [12] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and Hemal Shah. 2016. Wide & Deep Learning for Recommender Systems. arXiv:1606.07792 [cs.LG] [13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs.CL] [14] Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics . JMLR Workshop and Conference Proceedings, 249-256. [15] Wei Guo, Can Zhang, Zhicheng He, Jiarui Qin, Huifeng Guo, Bo Chen, Ruiming Tang, Xiuqiang He, and Rui Zhang. 2022. MISS: Multi-Interest Self-Supervised Learning Framework for Click-Through Rate Prediction. arXiv:2111.15068 [cs.IR] [16] Guangneng Hu, Yu Zhang, and Qiang Yang. 2018. CoNet: Collaborative Cross Networks for Cross-Domain Recommendation. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management (CIKM '18) . ACM. https://doi.org/10.1145/3269206.3271684 [17] Diederik P. Kingma and Jimmy Ba. 2017. Adam: A Method for Stochastic Optimization. arXiv:1412.6980 [cs.LG] [18] Chenglin Li, Mingjun Zhao, Huanming Zhang, Chenyun Yu, Lei Cheng, Guoqiang Shu, BeiBei Kong, and Di Niu. 2022. RecGURU: Adversarial Learning of Generalized User Representations for Cross-Domain Recommendation. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining (WSDM '22) . ACM. https://doi.org/10.1145/3488560.3498388 [19] Meng Liu, Jianjun Li, Guohui Li, and Peng Pan. 2020. Cross domain recommendation via bi-directional transfer graph collaborative filtering networks. In Proceedings of the 29th ACM international conference on information & knowledge management . 885-894. [20] Qi Liu, Xuyang Hou, Defu Lian, Zhe Wang, Haoran Jin, Jia Cheng, and Jun Lei. 2023. AT4CTR: Auxiliary Match Tasks for Enhancing Click-Through Rate Prediction. arXiv preprint arXiv:2312.06683 (2023). [21] Weiming Liu, Xiaolin Zheng, Mengling Hu, and Chaochao Chen. 2022. Exploiting Variational Domain-Invariant User Embedding for Partially Overlapped Cross Domain Recommendation. arXiv:2205.06440 [cs.IR] [22] Jianxin Ma, Chang Zhou, Hongxia Yang, Peng Cui, Xin Wang, and Wenwu Zhu. 2020. Disentangled self-supervision in sequential recommenders. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 483-491. [23] Muyang Ma, Pengjie Ren, Yujie Lin, Zhumin Chen, Jun Ma, and Maarten de Rijke. 2019. 𝜋 -Net: A Parallel Information-Sharing Network for Shared-Account CrossDomain Sequential Recommendations. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (Paris, France) (SIGIR'19) . Association for Computing Machinery, New York, NY, USA, 685-694. https://doi.org/10.1145/3331184.3331200 [24] Wentao Ouyang, Xiuwu Zhang, Lei Zhao, Jinmei Luo, Yu Zhang, Heng Zou, Zhaojie Liu, and Yanlong Du. 2021. Minet: Mixed interest network for crossdomain click-through rate prediction. In CIKM . 2669-2676. [25] Yujie Pan, Jiangchao Yao, Bo Han, Kunyang Jia, Ya Zhang, and Hongxia Yang. 2021. Click-through Rate Prediction with Auto-Quantized Contrastive Learning. arXiv:2109.13921 [cs.IR] [26] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A. Efros. 2016. Context Encoders: Feature Learning by Inpainting. arXiv:1604.07379 [cs.CV] [27] Qi Pi, Weijie Bian, Guorui Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Practice on Long Sequential User Behavior Modeling for Click-Through Rate Prediction. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD '19) . ACM. https://doi.org/10.1145/3292500. 3330666 [28] Pi Qi, Xiaoqiang Zhu, Guorui Zhou, Yujing Zhang, Zhe Wang, Lejian Ren, Ying Fan, and Kun Gai. 2020. Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate Prediction. arXiv:2006.05639 [cs.IR] [29] Jiarui Qin, Weinan Zhang, Xin Wu, Jiarui Jin, Yuchen Fang, and Yong Yu. 2020. User Behavior Retrieval for Click-Through Rate Prediction. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '20) . ACM. https://doi.org/10.1145/3397271.3401440 [30] Aghiles Salah, Thanh Binh Tran, and Hady Lauw. 2021. Towards Source-Aligned Variational Models for Cross-Domain Recommendation. In Proceedings of the 15th ACM Conference on Recommender Systems (Amsterdam, Netherlands) (RecSys '21) . Association for Computing Machinery, New York, NY, USA, 176-186. https: //doi.org/10.1145/3460231.3474265 [31] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer. arXiv:1904.06690 [cs.IR] [32] Wenchao Sun, Muyang Ma, Pengjie Ren, Yujie Lin, Zhumin Chen, Zhaochun Ren, Jun Ma, and Maarten de Rijke. 2021. Parallel Split-Join Networks for Sharedaccount Cross-domain Sequential Recommendations. arXiv:1910.02448 [cs.IR] [33] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. Journal of machine learning research 9, 11 (2008). [34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. Attention Is All You Need. arXiv:1706.03762 [cs.CL] [35] Fangye Wang, Yingxu Wang, Dongsheng Li, Hansu Gu, Tun Lu, Peng Zhang, and Ning Gu. 2023. CL4CTR: A Contrastive Learning Framework for CTR Prediction. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining . 805-813. [36] Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong, and Ed Chi. 2021. DCN V2: Improved Deep &amp; Cross Network and Practical Lessons for Web-scale Learning to Rank Systems. In Proceedings of the Web Conference 2021 (WWW '21) . ACM. https://doi.org/10.1145/3442381.3450078 [37] Xin Xin, Alexandros Karatzoglou, Ioannis Arapakis, and Joemon M. Jose. 2020. Self-Supervised Reinforcement Learning for Recommender Systems. arXiv:2006.05779 [cs.LG] [38] Tiansheng Yao, Xinyang Yi, Derek Zhiyuan Cheng, Felix Yu, Ting Chen, Aditya Menon, Lichan Hong, Ed H. Chi, Steve Tjoa, Jieqi Kang, and Evan Ettinger. 2021. Self-supervised Learning for Large-scale Item Recommendations. arXiv:2007.12865 [cs.LG] [39] Li Zhang, Weichen Shen, Shijian Li, and Gang Pan. 2019. Field-aware Neural Factorization Machine for Click-Through Rate Prediction. arXiv:1902.09096 [cs.LG] [40] Chang Zhou, Jianxin Ma, Jianwei Zhang, Jingren Zhou, and Hongxia Yang. 2021. Contrastive Learning for Debiased Candidate Generation in Large-Scale Recommender Systems. arXiv:2005.12964 [cs.IR] [41] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Deep Interest Evolution Network for Click-Through Rate Prediction. arXiv:1809.03672 [stat.ML] KDD '24, 2024, Conference Ruijie and Zhaoyang, et al. [42] Guorui Zhou, Chengru Song, Xiaoqiang Zhu, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep Interest Network for Click-Through Rate Prediction. arXiv:1706.06978 [stat.ML] [43] Yongchun Zhu, Kaikai Ge, Fuzhen Zhuang, Ruobing Xie, Dongbo Xi, Xu Zhang, Leyu Lin, and Qing He. 2021. Transfer-Meta Framework for Cross-domain Recommendation to Cold-Start Users. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '21) . ACM. https://doi.org/10.1145/3404835.3463010 [44] Yongchun Zhu, Zhenwei Tang, Yudan Liu, Fuzhen Zhuang, Ruobing Xie, Xu Zhang, Leyu Lin, and Qing He. 2021. Personalized Transfer of User Preferences for Cross-domain Recommendation. arXiv:2110.11154 [cs.IR]",
  "keywords_parsed": [
    "Click-Through Rate Prediction",
    " Lifelong Sequential Modeling",
    " Recommendation System ∗ corresponding author"
  ]
}