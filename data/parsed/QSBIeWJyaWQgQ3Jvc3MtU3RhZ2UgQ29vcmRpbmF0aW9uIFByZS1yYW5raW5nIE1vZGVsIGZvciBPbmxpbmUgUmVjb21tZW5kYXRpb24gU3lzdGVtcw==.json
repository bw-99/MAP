{"A Hybrid Cross-Stage Coordination Pre-ranking Model for Online Recommendation Systems": "Binglei Zhao zhaobinglei1@jd.com JD.com, Inc Beijing, China Houying Qi qihouying@jd.com JD.com, Inc Beijing, China Guang Xu xuguang66@jd.com JD.com, Inc Beijing, China Mian Ma mamian@jd.com JD.com, Inc Beijing, China Xiwei Zhao zhaoxiwei@jd.com JD.com, Inc Beijing, China Feng Mei meifeng6@jd.com JD.com, Inc Beijing, China Jinghe Hu hujinghe@jd.com JD.com, Inc Beijing, China and 1.3% UCTR in the JD E-commerce recommendation system. Concerning code privacy, we provide a pseudocode for reference.", "CCS Concepts": "\u00b7 Information systems \u2192 Information retrieval .", "Keywords": "Pre-ranking, Cross-Stage Coordination, Long-tail Precision, Consistency, Sample Selecting Bias", "ACMReference Format:": "Binglei Zhao, Houying Qi, Guang Xu, Mian Ma, Xiwei Zhao, Feng Mei, Sulong Xu, and Jinghe Hu. 2025. A Hybrid Cross-Stage Coordination Preranking Model for Online Recommendation Systems. In Companion of the 16th ACM/SPEC International Conference on Performance Engineering (WWW Companion '25), April 28-May 2, 2025, Sydney, NSW, Australia. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3701716.3715208", "1 Introduction": "Large-scale industrial recommendation systems typically employ cascade structures, consisting mainly of retrieval, pre-ranking, ranking, and re-ranking stages. The retrieval stage samples massive items ( \ud835\udc46 1 ~ 10 5 ) and then pre-ranking selects a reduced set ( \ud835\udc46 2 ~ 10 3 ). Afterward, they go through ranking and re-ranking stages, and ultimately, about ten items ( \ud835\udc46 4 ~ 10) are chosen to be exposed, as shown in Figure1. Pre-ranking plays a pivotal role in recommendation systems, performing a preliminary selection from large-scale retrieval candidates and distilling an optimal subset for ranking. To strike an optimal balance between effectiveness and efficiency, pre-ranking typically adopts vector-product-based models[9, 22]. Sulong Xu xusulong@jd.com JD.com, Inc Beijing, China", "Abstract": "Large-scale recommendation systems often adopt cascading architecture consisting of retrieval, pre-ranking, ranking, and re-ranking stages. With strict latency requirements, pre-ranking utilizes lightweight models to perform a preliminary selection from massive retrieved candidates. However, recent works focus solely on improving consistency with ranking, relying exclusively on downstream stages. Since downstream input is derived from the pre-ranking output, they will exacerbate the sample selection bias (SSB) issue and Matthew effect, leading to sub-optimal results. To address the limitation, we propose a novel H ybrid C ross-Stage C oordination P re-ranking model ( HCCP ) to integrate information from upstream (retrieval) and downstream (ranking, re-ranking) stages. Specifically, cross-stage coordination refers to the pre-ranking's adaptability to the entire stream and the role of serving as a more effective bridge between upstream and downstream. HCCP consists of Hybrid Sample Construction and Hybrid Objective Optimization . Hybrid sample construction captures multi-level unexposed data from the entire stream and rearranges them to become the optimal guiding \"ground truth\" for pre-ranking learning. Hybrid objective optimization contains the joint optimization of consistency and long-tail precision through our proposed Margin InfoNCE loss. It is specifically designed to learn from such hybrid unexposed samples, improving the overall performance and mitigating the SSB issue. The appendix describes a proof of the efficacy of the proposed loss in selecting potential positives. Extensive offline and online experiments indicate that HCCP outperforms SOTA methods by improving cross-stage coordination. It contributes up to 14.9% UCVR https://doi.org/10.1145/3701716.3715208 Some works design new models[13, 19, 31] to achieve improvements, which optimize pre-ranking as an independent entity. Despite advancements, lightweight models lead to an inevitable gap and inconsistency with ranking, which lowers system performance particularly when ranking favors items overlooked by pre-ranking. Recent studies[8, 24, 27, 37] focus on strict score alignment to approximate ranking ability. Considering alignment errors, a \u0394 NDCGbased method[38] is proposed to distinguish relative orders of chunks. They overlook the accuracy distinction of ranking models for top and tail items when imitating ranking orders, either treating them indistinguishably[38] or outright discarding tail items[37], potentially affecting pre-ranking's accuracy on tail items. Furthermore, given that downstream input is derived from the pre-ranking output, blindly imitating ranking orders and relying exclusively on downstream will limit the ability of pre-ranking to serve as a bridge connecting retrieval and ranking, which may exacerbate sample selection bias (SSB)[35] and Matthew effect. For pre-ranking, selecting high-quality mid-tail items and enhancing long-tail precision is essential. A fundamental idea is increasing massive unexposed data during training. Nevertheless, since user feedback on these data is inherently unknown, directly considering them as negatives will introduce a significant bias. The pivotal question addressed in this paper is: How can unexposed data with entire stream information be leveraged to make pre-ranking a more suitable bridge? We analyze the information gain of unexposed samples in entire stream : 1. downstream (ranking, re-ranking) gives an accurate prediction on click-through rate (CTR) and conversion rate (CVR) of top items. 2. upstream (retrieval) offers relevant items with greater efficiency, albeit they may not be exposed. To this end, we propose an innovative concept of cross-stage coordination of pre-ranking, which refers to integrating information from upstream and downstream stages. We also propose a H ybrid C ross-Stage C oordination P re-ranking method ( HCCP ) for online recommendation systems. The crux lies in hybrid sample construction and hybrid objective joint optimization on sampled multi-level data. Hybrid sample construction aims at capturing multi-level unexposed data from the entire stream, such as ranking sequence, pre-ranking sequence, in-batch negatives, and pool sampling negatives. The ranking sequence is non-uniformly sampled from downstream and rearranged to integrate with the diversity information of re-ranking and user preferences, which guarantees the \"ground truth\" is more suitable for pre-ranking learning. Compared to in-batch and pool sampling negatives, unexposed ranking and pre-ranking sequences are usually regarded as hard negatives. Various samples serve different learning objectives. Hybrid objective optimization is specifically designed to learn such instances that do not consistently correlate with user feedback. Hybrid objectives encompass consistency with downstream and long-tail item precision. For consistency learning, we propose list-wise global and local consistency losses to achieve soft order alignment on ranking sequence, which enables pre-ranking to approximate ranking ability of complex ranking models. For long-tail precision optimizing, we propose a novel Margin InfoNCE loss and incorporate priors of negative difficulties into contrastive learning, further enhancing discriminative capacity and mitigating SSB issues. In the appendix, we provide theoretical proof for the Margin InfoNCE loss in distinguishing potential positives from hard negatives. HCCP explicitly integrates entire stream data, rather than treating pre-ranking as an isolated stage or focusing solely on consistency. The contributions are summarized as follows: \u00b7 We propose the H ybrid C rossS tage C oordination P re-ranking model, which explicitly integrates entire stream information and makes pre-ranking a more suitable bridge. It can migrate to any model with almost no latency increase. \u00b7 We propose a hybrid sample construction module to capture multi-level unexposed data from upstream and downstream and a hybrid objective optimization module to enhance consistency and long-tail precision through a novel Margin InfoNCE loss. \u00b7 Extensive experiments verify HCCP's effectiveness in offline dataset and online system. It outperforms state-of-the-art baselines, achieving a 14.9% increase in UCVR and a 1.3% increase in UCTR in JD E-commerce recommendation system.", "2 Related Work": "We briefly describe three aspects of research: pre-ranking, learningto-rank algorithms, and sample selection bias issue. Pre-ranking . In industrial recommendation systems, pre-ranking plays a vital role in selecting high-quality items from large-scale retrieval items and relieving ranking pressure. To balance effectiveness and efficiency, it widely adopts a vector-product-based deep neural network model[9, 22]. However, lightweight model architectures degrade the model expression ability. For critical calibration ability, recent works design model architectures to improve effectiveness under a computing power constraint, such as COLD[31], FSCD[19], and IntTower[13]. Some works[23, 33] focus on knowledge distillation. Despite some advancements, they optimize the pre-ranking model as an independent entity, neglecting the consistency with ranking, leading to sub-optimal outcomes. For ranking ability, some methods[7, 26] propose the importance of improving the consistency with ranking. Some works propose strict score alignment via point-wise distillation loss[27, 37], MSE loss[24], or ListNet distillation loss[8] to align with raw scores predicted by ranking. COPR[38] proposes a rank alignment module and a chunkbased sampling module to optimize the consistency with ranking. However, they treat all items equally, neglecting accuracy discrepancies between head and tail items predicted by ranking. Moreover, pre-ranking has larger-scale candidates than ranking and the only utilization of ranking logs can not adapt to changes in retrieval distribution, leading to a sample selection bias problem. Learning-To-Rank (LTR) Algorithm . LTR algorithms are extensively used to enhance ranking abilities by approximating relevance degree, which can be categorized into point-wise, pairwise, and listwise. Point-wise[4] approaches usually treat ranking as regression or classification problems, which assign score for each item independently. Considering mutual information, pairwise[1, 6] methods achieve local optimality by predicting relative orders of item pairs but ignoring the overall order. Listwise methods[2, 3, 32] consider the entire sequence quality from a global perspective, aligning with ranking objectives. To preserve calibration and ranking quality, some studies combine the above losses[25, 34]. Sample Selection Bias (SSB) Problem . As the magnitude of retrieval items increases, SSB issues[35] in pre-ranking receive growing attention. Some works sample negatives through a pre-defined distribution[20] or in-batch sampling mechanisms[5, 10]. Since the diversity of negatives sampled by in-batch methods is constrained A Hybrid Cross-Stage Coordination Pre-ranking Model for Online Recommendation Systems WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia. List Item Sequence Global Consistency Task Cxr Estimation Tasks (Ctr, Cvr, List Item Sequence Pool Sampling Item Pool Ranking Seq In-batch Clobal ListMLE Retrieval User Emb User Embs Margin Pre- ranking Impressions MLP MLP InfoNCE Pre-ranking Consistency Estimation MLP Layer MLP Layers Shared MLP Ranking Hybrid Objective Learning MMOE Layer Ranking Global consistency Logits GateA Gates Cxr Estimation Logits Experts Impressions \" 1 Re- List Item List User-item Calibration Sum of dot products of user User Features Features Cross Features BCE Ranking and item embeddings User Tower Cross-feathre Tower Local ListMLE Loss User Weighted sum of scores Negs Negs Loss Seq Loss Seq Loss Item Tower by batch size, some works propose cross-batch sampling based on memory banks[29, 30]. In addition to negative sampling techniques, some approaches enhance the learning of long-tail items by crossdecoupling network[36] or modified losses, such as the modified softmax loss integrated with multi-positive samples[12]. We propose an approach that leverages entire stream data to alleviate SSB issues and enhance adaptability when retrieval distribution shifts. instead of an isolated stage. It can be seamlessly incorporated into any pre-ranking model with almost no latency increase.", "3 METHODOLOGY": "In this section, we present H ybrid C ross-Stage C oordination P reranking Model ( HCCP ). As outlined in Section 3.2, we construct hybrid multi-level samples from the entire stream. To leverage the strength of sampled unexposed information, we propose a hybrid object optimization approach in Section 3.3, designed for flexible adaptation across various pre-ranking models.", "3.1 Overall Architecture": "3.1.1 Base Model . To balance efficiency and effectiveness, we design pre-ranking based on the Three-Tower[31] paradigm, which includes user, item, and cross-feature towers. An MMOE[18] architecture is implemented on the user-side tower to facilitate multi-task learning. Traditional multi-objective estimation tasks usually serve as calibration tasks, adopting binary cross-entropy (BCE) loss on impressions (exposed items). The loss \ud835\udc3f \ud835\udc61 \ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc56 is defined as: , where \ud835\udc66 \ud835\udc61 \ud835\udc57 and \ud835\udc59 \ud835\udc61 \ud835\udc57 are predicted score and corresponding label of item \ud835\udc57 in task \ud835\udc61 . \ud835\udc45 \ud835\udc61 is the impression set and \ud835\udc48 is the user request set. Item embeddings are pre-computed and updated in near real-time. Upon online request, user embedding and cross-feature scores are calculated in real-time. Scores are computed by dot-product across user and item embeddings, weighted with cross-feature scores. 3.1.2 Innovation . To alleviate the SSB issue and improve precision, we employ a hybrid sampling technique that captures multilevel data from downstream and upstream stages of pre-ranking. Since unexposed samples are not associated with user feedback, we introduce an optimal hybrid objective optimization approach specifically tailored for such instances. By explicitly integrating entire stream information, pre-ranking achieves cross-stage coordination and becomes a more suitable bridge between retrieval and ranking, Hybrid Multi-level Sample Construction . To improve consistency, we sample ranking sequence ( \ud835\udc46 2 in Figure1) from downstream (ranking, re-ranking) stages in a non-uniform manner, where items ranked top have a higher sampling rate compared to lower. Then the ranking sequence is rearranged based on user feedback of impressions (exposed items), which not only integrates ranking information but also the context and diversity information of reranking, and user interest. From upstream, we collect three types of data: pre-ranking sequence with latter order in retrieved items ( \ud835\udc46 1 in Figure1), in-batch and pool sampling negatives from the item pool space ( \ud835\udc46 0 in Figure1). Typically, since ranking and preranking sequences are retrieved from upstream, they inherently exhibit a degree of alignment with user preferences. Compared to inbatch and pool sampling negatives, they are usually considered hard negative samples. The above samples improve the pre-ranking's discriminative capacity, especially when upstream provides a large number of long-tail items during online service. Hybrid Objective Optimization . Various multi-level samples serve different learning objectives. (1) Learning consistency with downstream : The ranking sequence offers unexposed information along with downstream feedback, which is crucial to improve the consistency with downstream. We design list-wise global (on ranking candidate space) and local consistency (on exposure space) modules based on ranking sequence, as illustrated in Figure2. Despite the constraints inherent in lightweight pre-ranking models, soft order alignment enables the pre-ranking to approximate the ranking ability of complex ranking models. (2) Optimizing long-tail precision throughout entire-stream data : Since hybrid unexposed data generally lack user feedback, we identify unexposed negatives and potential positives through contrastive learning. To incorporate prior knowledge of negative difficulties into learning, we introduce the Margin InfoNCE loss. Its efficacy in selecting potential positives is elaborated upon in the appendix. While enhancing long-tail item precision, our method concurrently mitigates the SSB issue.", "3.2 Hybrid Multi-level Sample Construction": "Except for clicked or purchased items as positives and un-clicked or un-purchased as negatives in impressions (N1), we collect various samples from downstream and upstream stages by online serving and offline sampling to enhance precision and alleviate SSB issues: Ranking Sequence Arrangement: positive feedback items @t the forefront as r For Global Consistency Rc Itemn with /t IRII+1 IRzI Item with For Local Consitency in Cxr Estimation Rt Item with [Rm| Rm-1 Random Sampling NI+1 Nm-1+1 Nn NI Chunk Chunk Chunk m 1 Chunk m Non-uniform Sampling Sm \u00b7 N2, unexposed ranking sequence \ud835\udc35 \ud835\udc5f = { \ud835\udc57 | \ud835\udc57 \u2208 \ud835\udc45 \ud835\udc50 , \ud835\udc52 \ud835\udc57 = 0 } . \ud835\udc45 \ud835\udc50 is non-uniformly sampled from downstream and then arranged based on exposure information and user feedback. \u00b7 N3, pre-ranking sequence \ud835\udc35 \ud835\udc5d with latter orders, uniformly sampled from retrieved candidates( \ud835\udc46 1 ), but not entering downstream. \u00b7 N4, in-batch negatives \ud835\udc35 \ud835\udc56 , randomly sampled from impressions paired with other users in the same batch[10]. \u00b7 N5, pool sampling negatives \ud835\udc35 \ud835\udc50 , randomly sampled from the predefined item pool space ( \ud835\udc46 0 ) but not been retrieved. We delineate the multi-level sample construction methods as follows. The final dataset is organized in a list-wise format to ensure that samples in one request are learned in the same batch. The listwise manner can enhance model throughput and training efficiency (Table4) and improve the diversity of unexposed in-batch negatives. 3.2.1 Ranking Sequence . The ranking sequence provides feedback information predicted by downstream stages, which can help to enhance consistency and precision. To construct the optimal guiding \"ground truth\" ranking candidates conducive to pre-ranking training, we implement a non-uniform sampling paradigm and arrange them according to exposure information and user feedback. Non-uniform Sampling Strategy . Ranking models typically optimize the precision of impressions and neglect tail items[11, 14], leading to long-tail items being lower, yet they may align with user interests. Directly learning scores or orders predicted by ranking cannot achieve optimum efficiency due to inaccurate predictions of tail items[37]. Thus, it is necessary to prevent long-tail bias and ensure pre-ranking's accuracy for long-tail items is not affected by orders predicted by ranking. Considering accuracy discrepancies between top and tail items of ranking, we design the non-uniform sampling strategy assigning higher sampling rates to high-ranked (top) items and lower rates to low-ranked (tail) items. In each request of user \ud835\udc62 , pre-ranking delivers top \ud835\udc41 \ud835\udc5f items to ranking. We denote \ud835\udc45 \ud835\udc5f as sorted ranking sequence with ranking orders. , where \ud835\udc5f is the ranking order, \ud835\udc52 and l are one-hot exposure and a vector of task labels, such as click or purchase \ud835\udc59 \ud835\udc50\ud835\udc59\ud835\udc58 , \ud835\udc59 \ud835\udc5c\ud835\udc5f\ud835\udc51 . \ud835\udc59 \ud835\udc50\ud835\udc59\ud835\udc58 = 1 or \ud835\udc52 = 1 mean item is clicked or exposed, while \ud835\udc59 \ud835\udc50\ud835\udc59\ud835\udc58 = 0 and \ud835\udc52 = 0 indicate the opposite. Items with smaller \ud835\udc5f are sorted to the front. Then we map the original ranking sequence to \ud835\udc5a chunks with different sample rates \ud835\udc60 \ud835\udc57 , which is illustrated in Figure3 and Eq 2. pre-rankingibase) ASH@K ranking ASH@K \u0161 500 750 1000 1250 1500 1750 2000 , where \ud835\udc5f \ud835\udc57 is the predicted ranking order of item \ud835\udc57 . It is adaptable to various mapping functions. The sampled ranking candidates are recorded as e \ud835\udc45 \ud835\udc5f , which keep \ud835\udc40 = \u02dd \ud835\udc5a \ud835\udc56 \ud835\udc60 \ud835\udc56 \u00b7 \ud835\udc41 \ud835\udc5f \ud835\udc56 samples. Noticeably, impressions are retained during sampling. Sample rates control the consistency granularity and are flexibly adjusted according to online logging pressure. As \ud835\udc60 \ud835\udc57 increases, consistency shifts from coarse-grained to fine-grained. It ensures increased sampling for top items and reduced sampling for tails while preserving randomness. For instance, in JD recommendation scenario, the pre-ranking's ASH@K (All-Scenario Click Hitrate@K)[37] exceeding ranking as \ud835\udc3e exceeds 600, as shown in Figure 4. Thus, a threshold of 600 can be established, whereby items ranked above it are assigned a higher sampling rate, while those below it receive a lower sampling rate. Sequence Arrangement . To mitigate the bias of raw ranking sequences, we rearrange sampled sequence e \ud835\udc45 \ud835\udc5f to align pre-ranking learning requirements. Exposure and click-task labels reflect the item selection tendency of downstream and user interests. Impressions incorporate a broader range of diversity and contextual information. As shown in Figure3, we define the arranged ranking sequence \ud835\udc45 \ud835\udc50 for the global consistency task as follows. , where \ud835\udc45 \ud835\udc50 is sorted in ascending order according to adjusted order \ud835\udc5f \ud835\udc50 = \ud835\udc5f - ( \ud835\udc52 + \ud835\udc59 \ud835\udc50\ud835\udc59\ud835\udc58 ) \u2217 \ud835\udc41 \ud835\udc5f . It preserves global order prioritizing clicked items first, followed by unclicked impressions, and finally unexposed items, while also maintaining the local order within the three parts as per the original sampled sequence e \ud835\udc45 \ud835\udc5f . Additionally, to increase local ranking ability, we integrate ranking sequence orders with different task labels in exposure space. The order of exposure sequence \ud835\udc45 \ud835\udc61 = [ \ud835\udc57 | \ud835\udc57 \u2208 \ud835\udc45 \ud835\udc50 , \ud835\udc52 \ud835\udc57 = 1 ] in Cxr estimation tasks is defined as \ud835\udc5f \ud835\udc61 = \ud835\udc5f - ( \ud835\udc52 + \ud835\udc59 \ud835\udc61 ) \u2217 \ud835\udc41 \ud835\udc5f , ( \ud835\udc61 \u2208 { \ud835\udc50\ud835\udc59\ud835\udc58, \ud835\udc5c\ud835\udc5f\ud835\udc51, ... }) . 3.2.2 Pre-ranking Sequence with Latter Order . Retrieval stages provide \ud835\udc41 \ud835\udc5d items to pre-ranking, in which top \ud835\udc41 \ud835\udc5f items are delivered downstream. We denote \ud835\udc45 \ud835\udc5d as pre-ranking sequence with latter pre-ranking order. It captures items not entering the ranking and exposed. Although they belong to hard negatives, they may still contain a wealth of information and match user interests, including potential positives to some extent. The sampled pre-ranking sequence \ud835\udc35 \ud835\udc5d is randomly sampled from the original sequence \ud835\udc45 \ud835\udc5d as a fixed sample rate. They are usually discarded in previous pre-ranking training. However, we can incorporate them into training through contrastive learning. It improves the ability to distinguish hard negatives especially when upstream stages provide long-tail items during online service. 3.2.3 In-batch Negatives . We sample unexposed negatives by employing a list-wise in-batch sampling method within impressions of other users. They usually act as easy negatives to achieve popular item suppression, which is described as the \"gold negative\" in [10]. The initial set to be scored is the intersection of impressions \ud835\udc45 \ud835\udc61 , ranking \ud835\udc35 \ud835\udc5f and pre-ranking negatives \ud835\udc35 \ud835\udc5d . As illustrated in figure5, for estimation task \ud835\udc61 , the list-wise in-batch negative sampling is achieved as following steps: (1) Reshape predicted item embeddings \u02c6 \ud835\udc3c into a shape of (| \ud835\udc48 | \u2217 | \ud835\udc45 \ud835\udc61 \u222a \ud835\udc35 \ud835\udc5f \u222a \ud835\udc35 \ud835\udc5d | , \ud835\udc51 ) , where | \ud835\udc48 | is request numbers in a batch and \ud835\udc51 is the embedding dimension. (2) Apply a transpose operation on \u02c6 \ud835\udc3c to the shape of ( \ud835\udc51, | \ud835\udc48 | \u2217 | \ud835\udc45 \ud835\udc61 \u222a \ud835\udc35 \ud835\udc5f \u222a \ud835\udc35 \ud835\udc5d |) . (3) Perform a cross product on user embedding \u02c6 \ud835\udc48 | \ud835\udc48 | \u00d7 \ud835\udc51 and item embedding \u02c6 \ud835\udc3c \ud835\udc51 \u00d7| \ud835\udc48 | \u2217| \ud835\udc45 \ud835\udc61 \u222a \ud835\udc35 \ud835\udc5f \u222a \ud835\udc35 \ud835\udc5d | to obtain negative score tensor \u02c6 \ud835\udc41 | \ud835\udc48 | \u00d7 | \ud835\udc48 | \u2217| \ud835\udc45 \ud835\udc61 \u222a \ud835\udc35 \ud835\udc5f \u222a \ud835\udc35 \ud835\udc5d | . (4) For user \ud835\udc62 , mask negative scores that belong to corresponding item collection \ud835\udc45 \ud835\udc61 , \ud835\udc35 \ud835\udc5f , \ud835\udc35 \ud835\udc5d and sample final in-batch negatives \ud835\udc35 \ud835\udc56 from \u02c6 \ud835\udc41 . To reduce the computational complexity, the cross product is calculated on item embedding \u02c6 \ud835\udc3c | \ud835\udc48 | \u00d7 | \ud835\udc45 \ud835\udc61 | \u00d7 \ud835\udc51 of exposure sequence \ud835\udc45 \ud835\udc61 . Compared to traditional in-batch sampling, whose diversity is determined by batch size | \ud835\udc48 | , while the magnitude of negatives in list-wise in-batch sampling is | \ud835\udc48 | \u00d7 | \ud835\udc45 \ud835\udc61 | , which helps to improve the diversity of unexposed negatives. 3.2.4 Pool Sampling Negatives . Such negatives \ud835\udc35 \ud835\udc50 are randomly sampled from the item pool space, usually containing items that have not been retrieved, which can be regarded as easy negatives. When upstream expands the retrieval magnitude and provides massive unseen items when online serving, introducing this kind of negatives can improve the pre-ranking's discriminative ability.", "3.3 Hybrid Objective Optimization": "3.3.1 Consistency with Downstream Learning . To improve pre-ranking's consistency with downstream, we design a global consistency task on ranking sequence space and a local consistency module on exposure space. It achieves a soft alignment with downstream, which helps to obtain the ranking ability of complex ranking models, further improving the precision. (1) Global Consistency : It enables pre-ranking to optimize the ranking ability by approximating downstream from a global perspective . In Figure2B, the global consistency loss is designed based on ListMLE loss[32] to explicitly learn ranking sequence order \ud835\udc45 \ud835\udc50 , which provides consideration of global orders while pairwise loss considers only relative order. The loss \ud835\udc3f \ud835\udc50 is defined as: , where \ud835\udc66 \ud835\udc50 \ud835\udc57 is the predicted score of item \ud835\udc57 in global consistency task. Minimizing \ud835\udc3f \ud835\udc50 makes each item in the sequence \ud835\udc45 \ud835\udc50 have the highest top-one probability in the subsequence with latter order. (2) Local Consistency in Cxr Estimation : For Cxr estimation tasks (Ctr, Cvr, ...), we integrate traditional calibration task with sub-task local consistency, as shown in Figure2C. Expect the calibration loss \ud835\udc3f \ud835\udc61 \ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc56 in Eq 1 for ensuring calibration capability, we design the local consistency loss to emphasize ranking ability from a local perspective in exposure space. The corresponding loss \ud835\udc3f \ud835\udc61 \ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc60 ensures the local order alignment on impressions to enhance the ranking ability based on the exposure sequence \ud835\udc45 \ud835\udc61 . Similar to global consistency loss defined in Eq 3, minimizing \ud835\udc3f \ud835\udc61 \ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc60 makes each item in exposure sequence \ud835\udc45 \ud835\udc61 have the highest top-one probability in the subsequence with latter order. 3.3.2 Long-tail Precision Optimization . In cascade recommendation systems, given that downstream input is derived from the pre-ranking output, relying only on the downstream will limit the pre-ranking model, which may further aggravate the Matthew effect. Since pre-ranking serves as a bridge between retrieval and ranking stages, it is necessary to strengthen the ranking ability and precision of mid and long-tail items and provide high-quality items to downstream. Expect N1, samples N2 ~N5 extracted from the entire stream can alleviate the SSB issue but typically lack user feedback. To leverage these unexposed samples, we introduce an effective approach for negative learning from the perspective of contrastive learning. We propose the Margin InfoNCE loss to incorporate priors about hard or easy negatives into learning, which helps identify unexposed potential positives and negatives. As illustrated in Figure2C, cxr estimation tasks integrate traditional calibration and local consistency with negative contrastive learning. It enhances pre-ranking's adaptability to upstream and guarantees robust discriminative capacity when upstream provides massive long-tail items. In the Appendix, we provide an analysis of how the proposed loss distinguishes potential positives from hard negatives. Margin InfoNCE Loss . Different categories of negatives above vary in their levels of difficulty. Group N1 is usually top-ranked downstream and has a definite selection tendency from users, we directly utilize the BCE loss on them, as described in Section 3.1.1. Unexposed samples in N2 ~N3 are retrieved due to their relevance with the user, but they are ranked lower by downstream or preranking models. Although they have not been exposed, they contain potential positive samples. Introducing these hard negatives helps to improve the ability to distinguish difficult samples. Negatives in N4 ~N5 typically lack direct relevance or interest alignment with users, classified as easy negatives . Although they are both easy negatives, there are subtle differences in effects: introducing N4 can suppress popular items and relieve the Matthew Effect, while N5 introduces massive unseen items, which helps improve discrimination ability, especially when retrieval magnitude increases. Compared to hard negatives that may contain potential positives, easy negatives are less aligned with user preferences, hence, their distance from positives needs to be sufficiently large. Some traditional losses do not explicitly differentiate between types of negatives. Inspired by InfoNCE[21] and Margin Softmax[16, 17, 28] losses that explicitly encourage discriminative feature learning to reduce intra-class variation and increase inter-class difference, we propose the Margin InfoNCE loss to explicitly differentiate between positives, hard (N2 ~N3), and easy negatives (N4 ~N5), as defined: WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia. , where \ud835\udf0f is the temperature value. \ud835\udc35 \ud835\udc61 = { \ud835\udc57 | \ud835\udc57 \u2208 \ud835\udc45 \ud835\udc61 , \ud835\udc59 \ud835\udc61 \ud835\udc57 = 1 } contain positive-label items in impressions of task \ud835\udc61 . \ud835\udc35 \u210e = \ud835\udc35 \ud835\udc5f \u222a \ud835\udc35 \ud835\udc5d is the union of ranking and pre-ranking negatives. \ud835\udc35 \ud835\udc52 = \ud835\udc35 \ud835\udc56 \u222a \ud835\udc35 \ud835\udc50 is the union of in-batch and pool sampling negatives. \ud835\udc63 \ud835\udc61 \ud835\udc57 = \u2225 u t \u2225\u2225 i j \u2225 is the product of user and item embeddings. The angle between user and item embedding is defined as \ud835\udc50\ud835\udc5c\ud835\udc60\ud835\udf03 = u \u00b7 i \u2225 u t \u2225 \u2225 i j \u2225 . Referring to AM-Softmax[28], we introduce the additive margin \ud835\udf19 ( \ud835\udf03 ) = \ud835\udc50\ud835\udc5c\ud835\udc60\ud835\udf03 -\ud835\udc5a, ( \ud835\udc5a > 0 ) based on InfoNCE loss. In addition, we analyze why the Margin InfoNCE loss can distinguish potential positive items from hard negatives in the Appendix. The overall loss is defined as: , where \ud835\udf06 \ud835\udc50 , \ud835\udf06 \ud835\udc61 , \ud835\udefc, \ud835\udefd are loss weight of global consistency, multiobjective estimation, local consistency, and negative tasks.", "4 EXPERIMENT": "We conduct a series of offline experiments on public Taobao 1 and JD production datasets and online A/B testing to evaluate the effectiveness of our method. We aim to answer the following questions: \u00b7 Q1 : Does HCCP enhance the overall performance and serve as a suitable bridge between retrieval and ranking stages? \u00b7 Q2 : How does each learning objective perform in performance? \u00b7 Q3 : Does HCCP have an advantage over previous methods in mitigating the mid and long-tail bias issue? \u00b7 Q4 : Does the complexity of HCCP adapt to industrial systems?", "4.1 Experiment Setup": "4.1.1 Dataset. Public Taobao Dataset contains 26 million expression logs in 8 days. We utilize DIN[39] as the ranking model to guide the learning of pre-ranking methods. Note that the public dataset lacks multi-level unexposed data, causing a significant gap between simulation and industrial systems for methods Rethink[37] and ours. JD Production Dataset contains sampled data from the entire stream of clicked requests in one week on the JD homepage. The request amount for training is the magnitude of billions. The validation is conducted on pre-ranking candidates from one day that occurred one week later. 4.1.2 Metrics. The model performance is evaluated on the metrics. Offline metrics : 1) Traditional AUC metrics reflect ranking abilities on impressions (top items). Considering that the public dataset contains only impressions, we use AUC to evaluate the performance. Due to inherent bias in historical impressions, AUC is difficult to align with online performance. 2) Since pre-ranking aims at distilling an unordered optimal subset, we follow in-scenario click/purchase hitrate ( ISH , ISPH ) and all-scenario click/purchase ( ASH , ASPH ) described in [37] to measure the quality of topK items selected by pre-ranking. , where 1 is the indicator function to determine whether item \ud835\udc57 in topK set is clicked and \ud835\udc3c \ud835\udc62 denotes the click or purchase sets of the current scenario and all scenarios when calculating ISH, ASH, ISPH and ASPH, respectively. When calculating ASH/ASPH in recommendation systems, we directly use clicked and purchased items in all scenarios without query relevance filtering, which is different from search systems[37]. ISH/ISPH better reflects users' interest in the current scenario, while ASH/ASPH reflects users' long-term and non-instant gratification interests. Without focusing solely on top items, we set \ud835\udc3e = 10 , 100 , 1000 , 2000 to evaluate the pre-ranking performance on top, middle, and tail items. 3) Mean average precision ( \ud835\udc40\ud835\udc34\ud835\udc43 @ \ud835\udc3e ) and normalized discounted cumulative gain ( \ud835\udc41\ud835\udc37\ud835\udc36\ud835\udc3a @ \ud835\udc3e )[15] metrics can measure the consistency with ranking. However, consistency with ranking does not fully measure the performance of pre-ranking, which is analyzed in the experimental section. In the JD production dataset, we use topK (K=10,100,1000) candidates selected by ranking as relative ground-truth values. Due to the insufficient accuracy of ranking for tail items, we do not measure the tail set consistency (K=2000). Online metrics : In the JD E-commerce recommendation system, UCTR (click PV/exposure UV) and UCVR (purchase PV/exposure UV) are used for online performance evaluation. 4.1.3 Baselines. Baseline methods are divided into 3 groups: G1 , using only impressions; G2 , using impressions with ranking orders; G3 : using impressions and other unexposed samples. And we design multi-versions of HCCP for ablation studies. (1) Base (G1) adopts an MMoE architecture [18] and is trained on exposures by point-wise binary cross-entropy (BCE) loss. (2) Base+ListNet (G1) implements a ranking and calibration joint optimization method through BCE classification loss and ListNet losses[3] based on the base MMoE model. (3) Base+ListMLE (G2) employs listMLE [32] and BCE losses to improve ranking ability by training on impressions with ranking orders. It can be seen as HCCP with only local consistency tasks. (4) COPR [38] (G3) implements a relaxation net, and CTR, pairwise consistency, regularization losses using ranking candidates. We modify the underlying model to MMoE for a fair comparison. (5) Rethink [37] (G3) achieves a list-wise loss on impressions, ranking, and pre-ranking candidates and a distillation loss on impressions with predicted CTR, which provides a hard consistency alignment. This paper describes ASPH and online GMV drop when distillation extends to all ranking candidates. (6) HCCP (G3) implements some different versions of HCCP. \u00b7 HCCP(w/o Up) improves consistency through a global consistency task, without N2~N5 samples in estimation tasks. \u00b7 HCCP(w/o PRC) achieves an InfoNCE loss only on in-batch negatives and pool sampling negatives without ranking and preranking candidates N2~N3 based on HCCP(w/o Up). \u00b7 HCCP(w/o Neg) uses the InfoNCE loss on candidates N2~N3 without sampled negatives N4~N5 based on HCCP(w/o Up). \u00b7 HCCP(w/o Margin) uses InfoNCE loss instead of our margin InfoNCE loss on N2~N5 samples. There is no explicit discrimination between N2~N3 (hard negatives) and N4~N5 (easy negatives). \u00b7 HCCP(Ours) is the final version described in this paper. 4.1.4 Implementation Detail. When constructing samples, we split ranking candidates into 7 non-uniform chunks and set sample rates Due to the data sensitivity, the table only displays the absolute increase value. of different chunks as 0 . 5, 0 . 125, 0 . 1, 0 . 05, 0 . 025, 0 . 01, 0 . 005 from top to tail. In each request, the sampled ranking sequence \ud835\udc45 \ud835\udc50 and preranking sequence \ud835\udc35 \ud835\udc5d contain 35 and 15 items with a 0 . 15% sample rate. In-batch sampling is implemented during model training, and its sampling rate is set to be 0 . 05. For pool sampling negatives, we sample 10 items as easy negatives in each request. The number of experts in base MMoE model is set as 8. Since it is difficult to obtain cross-features of N4~N5 negatives, such as the proportion of the item brand in the user click sequence, we directly use the InfoNCE loss to optimize the final weighted score of the Three Tower on N2~N3 samples. User and item embedding dotproducts are optimized on N2~N5 samples by the Margin InfoNCE loss. The final loss is the average of the two losses. Cxr estimation tasks contain click, purchase, and diversity tasks. The diversity task regulates the item category diversity delivered to downstream stages. The loss weight for overall global consistency, click, purchase, and diversity tasks are 0 . 05 , 0 . 98 , 0 . 2 , 0 . 1, respectively. Within cvr estimation tasks, loss weights of local consistency \ud835\udefc and margin InfoNCE losses \ud835\udefd are 0 . 05 and 0 . 5. Margin and temperature values are \ud835\udc5a = 0 . 9 and \ud835\udf0f = 0 . 1 in Margin InfoNCE loss. Performance is somewhat sensitive to hyper-parameters, especially the global consistency's loss weight and the margin value in Margin InfoNCE loss, which are determined through ablation experiments.", "4.2 Offline Performance Comparison (Q1)": "4.2.1 Performance on Taobao Dataset. Although introducing only impressions with ranking orders, HCCP achieves a slight improvement, as illustrated in Table2. Since the public dataset lacks unexposed data, Table2 can not fully reflect differences between Rethink and HCCP, we conduct experiments on JD production dataset. 4.2.2 Performance on JD Production Dataset. As demonstrated in Table1, HCCP markedly surpasses other baseline methods on JD dataset. The increase of the hitrate metrics when K=10 and 100 demonstrates HCCP's superior capability in identifying top items that closely align with user preferences. When K=1000 and 2000, the highest hitrate metric of HCCP shows its higher prediction accuracy on mid and tail items. The performance difference between Base and Base+ListNet is minimal, which suggests that directly introducing Listnet loss of learning-to-rank methods on original impressions cannot lead to significant improvements. Additionally, we observe that the consistency of HCCP is highest on top items. As shown in Table3, as K increases to 1000, both MAP and NDCG exhibit signs of decline, which is attributed to the fact that the optimization objective of HCCP is not consistency. On the one hand, since ranking models predict top items more accurately, blindly improving the consistency with ranking on tail items may affect the pre-ranking accuracy on tail items. On the other hand, since the role of pre-ranking is performing a preliminary selection from large-scale items, and distilling an optimal unordered subset in the cascading system, consistency affects pre-ranking performance to a certain extent, but it does not fully reflect the changes in preranking performance. Above analyses indicate that the optimization objective for pre-ranking should be to find a balance between consistency with ranking and precision on mid and tail items . 140.00% 120.00% 95,5096 95,409 100,.00% 68,089 1 63,5286 60.00% 38.3286 39,759 37.9136 40.00% 28.2186 20.00% 0.00% Relative ISH(\") 350.0096 300.0096 200.00%6 150 OO% 40.00% 100.0096 HCCP HCCP (w/o UP) HCCP", "4.3 Online A/B Test": "We conduct the A/B test on JD homepage recommendation system for 7 days. HCCP achieves a 14.9% increase in UCVR and 1.3% in UCTR, which is deployed online to serve main user traffic. Among them, HCCP(w/o Up) with consistency modeling contributes up to 7.1% UCVR and 0.6% UCTR, and the inclusion of long-tail precision optimizing on multi-level samples provides an additional 7.8% to UCVR and 0.7% UCTR. Moreover, we analyze the CTR and CVR improvement of items with different frequencies in Figure6. When HCCPisdeployed in JD system, there is a notable trend that mid and tail items have a significant improvement in CVR. The phenomenon elucidates the more pronounced enhancement observed in UCVR, underscoring the efficacy of optimization long-tail precision.", "4.4 Futher Analysis": "4.4.1 Ablation studies (Q2). Ablation studies conduct to evaluate the effectiveness of each objective in joint optimization. Results are displayed beneath the dashed lines in Table1 and Table3. (1) The Importance of Local Consistency in Cxr Estimation Tasks . Slight improvement on Base+ListNet compared with Base indicates that incorporating ranking orders of impressions (local consistency) can enhance pre-ranking accuracy to some extent. (2) The Importance of Global Consistency . HCCP(w/o UP) trains global consistency on the ranking sequence of both exposed and unexposed data based on Base+ListNet, which achieves an enhancement across all metrics. It significantly outperforms COPR on consistency metrics, demonstrating the efficiency of ListMLE loss for order learning from a global perspective in learning-to-rank. (3) The Importance of Hybrid Samples in Long-tail Precision Optimization . We validate the impact of easy (N4~N5) and hard (N2~N3) negatives through two experiments, designated as HCCP(w/o PRC) and HCCP(w/o Neg). N2~N5 samples are important, however, directly incorporating them cannot maximize utility; for instance, HCCP(w/o Margin) leads to a performance or consistency decline on ISP@1000, ISPH@1000, and ASPH@2000, which is due to the introduction of hard negatives. Thus we propose the Margin InfoNCE loss to distinguish these negatives explicitly. However, B means \"billion\" in #Data. TP99 is the 99th percentile of latency. more hard negatives increase the difficulty of learning long-tail items, which provides guidance for subsequent optimization. (4) TheImportance of the Margin InfoNCE Loss . We propose the Margin InfoNCE Loss to enhance the discriminative ability of negatives, where hard negatives in N2~N3 may contain potential positives and easy negatives in N4~N5 are less aligned with user interest. Compared to HCCP(w/o Margin), HCCP better leverages unexposed data and outperforms on top, middle, and tail items. 4.4.2 Performance on long-tail items (Q3). HCCP attains superior outcomes in hitrate metrics when K=2000 in Table1, indicating that it outperforms other methods in performance on tail items. Moreover, we divide our validation dataset into octiles based on the frequency of item occurrences, with the first being popular items. Figure7 demonstrates that HCCP with multi-level samples significantly improves the mid and long-tail item precision while enhancing top item precision, effectively alleviating the seesaw phenomenon compared to HCCP(w/o Up) and COPR. HCCP(w/o Up) and COPR exclusively emphasize consistency, which is demonstrably less effective in mitigating long-tail bias compared to HCCP. 4.4.3 Complexity and Time Analysis(Q4). Base method utilizes the JD production dataset with only impression data, which contains 39.5 million instances. After adding multi-level unexposed data without user feedback, the data volume reachs 4.75 billion. Table4 shows that the parameter count increase of HCCP compared to Base is mainly due to adding a global consistency task based on MMOE. Similarly, the increased parameter of Rethink[37] is also attributed to introducing an exposure task. \ud835\udc39 \ud835\udc62 and \ud835\udc39 \ud835\udc56 represent the model complexity of the user tower and the item/cross-feature tower, respectively. Offline training complexity is affected by several common factors, i.e., \ud835\udc5f (#request of training data), \ud835\udc51 (#base training data), \ud835\udc51 1 (#augmented data in COPR), \ud835\udc51 2 (#augmented data in Rethink), \ud835\udc51 3 (#augmented data in our method). Compared HCCP(Ours) with HCCP(Point), although the training data is augmented, the list-wise organization manner reduces the offline training complexity. During online serving, the latency (TP99) cost increase of 0.73% caused by a slight parameter increase is acceptable in our system.", "5 CONCLUSION": "We propose HCCP, a hybrid pre-ranking model to improve crossstage coordination within the entire stream, jointly optimizing consistency and long-tail precision through multi-level sample construction and hybrid objective optimization modules. It outperforms SOTA methods, contributing up to 14.9% UCVR and 1.3% UCTR within 7 days of deployment in the JD E-commerce recommendation system. In future work, we will extend the training data to all scenarios with all-scenario positive feedback information.", "A Appendix": "We analyze the ability of the proposed Margin InfoNCE loss to discriminate potential positives from hard negatives. In formal binary cross-entropy (BCE) calibration tasks, unclicked impressions that reflect a clear user's tendency not to choose are seen as negatives. We calculate the derivative of BCE loss to the predicted score \ud835\udc66 . When \ud835\udc57 is a negative sample, \ud835\udc59 \ud835\udc5b = 0 , \ud835\udf15\ud835\udc3f \ud835\udc35\ud835\udc36\ud835\udc38 \ud835\udf15\ud835\udc66 \ud835\udc5b = 1 1 + \ud835\udc52 -\ud835\udc66\ud835\udc5b . We calculate the derivative of the Margin InfoNCE loss to the predicted score \ud835\udc66 . 2.00 g(yn) 1.75 1.50 1.25 1.00 0.75 0.50 0.25 0.00 =2.5 0.0 2.5 5.0 7.5 10.0 Yn , where \ud835\udc57 and \ud835\udc5b ( \ud835\udc5b \u2208 \ud835\udc35 \u210e ) are the positive and negative items to be calculated the partial derivative. \ud835\udc35 \u210e = \ud835\udc35 \ud835\udc5f \u222a \ud835\udc35 \ud835\udc52 is the hard negative set and \ud835\udc35 \ud835\udc52 = \ud835\udc35 \ud835\udc56 \u222a \ud835\udc35 \ud835\udc50 is the easy negative set. Since potential positives usually exist in hard negatives \ud835\udc35 \u210e , we only consider the gradient of positives \ud835\udc66 \ud835\udc57 and hard negatives \ud835\udc66 \ud835\udc5b ( \ud835\udc5b \u2208 \ud835\udc35 \u210e ) here. When the same negative sample \ud835\udc66 \ud835\udc5b is optimized using different loss functions \ud835\udc3f \ud835\udc35\ud835\udc36\ud835\udc38 and \ud835\udc3f \ud835\udc40 , the corresponding gradients can be approximated as \ud835\udc54 ( \ud835\udc66 \ud835\udc5b ) and \ud835\udc53 ( \ud835\udc66 \ud835\udc5b ) as Eq11, respectively. We show them with different parameters in Figure8, where \ud835\udc5e and \ud835\udc50 control the left and right movement of the entire \ud835\udc53 ( \ud835\udc66 \ud835\udc5b ) function along the x-axis, and \ud835\udf0f and \ud835\udc50 control the function's slope. As the value of the negative sample \ud835\udc66 \ud835\udc5b approaches 0 (and includes subsequent steps), gradient \ud835\udc53 ( \ud835\udc66 \ud835\udc5b ) will be smaller than \ud835\udc54 ( \ud835\udc66 \ud835\udc5b ) , meaning that the rate of descent will slow down. Different \ud835\udc5e , \ud835\udc50 , and \ud835\udf0f will only affect the position of this dividing point (like A and B in Figure8), but will not affect overall trend. When optimizing, using our proposed Margin InfoNCE loss \ud835\udc3f \ud835\udc40 instead of the BCE loss \ud835\udc3f \ud835\udc35\ud835\udc36\ud835\udc38 will result in higher scores for potential positives in hard negative samples, thereby effectively selecting the potential positives.", "References": "[1] Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hullender. 2005. Learning to rank using gradient descent. In Proceedings of the 22nd International Conference on Machine Learning (Bonn, Germany) (ICML '05) . Association for Computing Machinery, New York, NY, USA, 89-96. https: //doi.org/10.1145/1102351.1102363 [2] Chris J.C. Burges. 2010. From RankNet to LambdaRank to LambdaMART: An Overview . Technical Report MSR-TR-2010-82. https://www.microsoft.com/enus/research/publication/from-ranknet-to-lambdarank-to-lambdamart-anoverview/ [3] Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. 2007. Learning to rank: from pairwise approach to listwise approach. In Proceedings of the 24th International Conference on Machine Learning (Corvalis, Oregon, USA) (ICML '07) . Association for Computing Machinery, New York, NY, USA, 129-136. https: //doi.org/10.1145/1273496.1273513 [4] Fredric C. Gey. 1994. Inferring probability of relevance using the method of logistic regression. In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (Dublin, Ireland) (SIGIR '94) . Springer-Verlag, Berlin, Heidelberg, 222-231. [5] Daniel Gillick, Sayali Kulkarni, Larry Lansing, Alessandro Presta, Jason Baldridge, Eugene Ie, and Diego Garcia-Olano. 2019. Learning Dense Representations for Entity Retrieval. arXiv:1909.10506 [cs.CL] [6] Bin Gu, Victor S. Sheng, Keng Yeow Tay, Walter Romano, and Shuo Li. 2015. Incremental Support Vector Learning for Ordinal Regression. IEEE Transactions on Neural Networks and Learning Systems 26, 7 (2015), 1403-1416. https://doi. org/10.1109/TNNLS.2014.2342533 [7] Siyu Gu and Xiangrong Sheng. 2022. On Ranking Consistency of Pre-ranking Stage. arXiv:2205.01289 [cs.IR] [8] Xiaoqiang Gui, Yueyao Cheng, Xiang-Rong Sheng, Yunfeng Zhao, Guoxian Yu, Shuguang Han, Yuning Jiang, Jian Xu, and Bo Zheng. 2023. Calibrationcompatible Listwise Distillation of Privileged Features for CTR Prediction. arXiv:2312.08727 [cs.IR] [9] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. 2013. Learning Deep Structured Semantic Models for Web Search Using Clickthrough Data. In Proceedings of the 22nd ACM International Conference on Information & Knowledge Management (San Francisco, California, USA) (CIKM '13) . Association for Computing Machinery, New York, NY, USA, 2333-2338. https://doi.org/10.1145/2505515.2505665 [10] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for OpenDomain Question Answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, Online, 6769-6781. https://doi.org/10.18653/v1/2020.emnlp-main.550 [11] Jingjing Li, Ke Lu, Zi Huang, and Heng Tao Shen. 2017. Two Birds One Stone: On both Cold-Start and Long-Tail Recommendation. In Proceedings of the 25th ACM International Conference on Multimedia (Mountain View, California, USA) (MM '17) . Association for Computing Machinery, New York, NY, USA, 898-906. https://doi.org/10.1145/3123266.3123316 [12] Longbin Li, Chao Zhang, Sen Li, Yun Zhong, Qingwen Liu, and Xiaoyi Zeng. 2023. Graph Contrastive Learning with Multi-Objective for Personalized Product Retrieval in Taobao Search. arXiv preprint arXiv:2307.04322 (2023). [13] Xiangyang Li, Bo Chen, Huifeng Guo, Jingjie Li, Chenxu Zhu, Xiang Long, Sujian Li, Yichao Wang, Wei Guo, Longxia Mao, Jinxing Liu, Zhenhua Dong, and Ruiming Tang. 2022. IntTower: The Next Generation of Two-Tower Model for Pre-Ranking System. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management (Atlanta, GA, USA) (CIKM '22) . Association for Computing Machinery, New York, NY, USA, 3292-3301. https://doi.org/10.1145/3511808.3557072 [14] Siyi Liu and Yujia Zheng. 2020. Long-tail Session-based Recommendation. In Proceedings of the 14th ACM Conference on Recommender Systems (Virtual Event, Brazil) (RecSys '20) . Association for Computing Machinery, New York, NY, USA, 509-514. https://doi.org/10.1145/3383313.3412222 [15] Tie-Yan Liu. 2009. Learning to Rank for Information Retrieval. Found. Trends Inf. Retr. 3, 3 (mar 2009), 225-331. https://doi.org/10.1561/1500000016", "Algorithm 1 A Tensorflow-style Pseudocode of our proposed Margin InfoNCE Loss.": "1: function Cal_Margin_InfoNCE_Loss( ) 2: cos_theta = logits / (u_norm \u2217 i_norm + 1 \ud835\udc52 -10 ) 3: logits_m = (cos_theta - margin) \u2217 (u_norm \u00b7 i_norm + 1 \ud835\udc52 -10 ) 4: l_update = (logits_m + beta \u2217 logits) / (1 + beta) # logits, u_norm, i_norm: [B*K], to calculate \ud835\udc50\ud835\udc5c\ud835\udc60 \ud835\udf03 in Eq5 # margin: float, to get the logits with the margin \ud835\udc63 \u00b7 \ud835\udf19 ( \ud835\udf03 \ud835\udc58 ) in Eq5 # beta: float, using a weighted combination to maintain stability 5: beta \u2217 = scale # scale: float, beta=9999 at the beginning of training. As iteration steps increase, beta becomes smaller. 6: l_p = tf.reshape(tf.where(p_mask), l_update, tf.fill(tf.shape(p_mask), neg_min) ), [-1,1]) # p_mask: [B*K] positive mask, neg_min: float 7: l_n_hard = tf.where(n_hard_mask, l_update, tf.fill(tf.shape(n_hard_mask), neg_min) ) # n_hard_mask: [B*K] hard negative mask 8: l_n_easy = tf.where(n_easy_mask, logits, tf.fill(tf.shape(n_easy_mask), float(neg_min)) ) # n_easy_mask: [B*K] easy negative mask 9: l_n = tf.concat([tf.reshape(l_n_hard, [B,K]), tf.reshape(l_n_easy], [B,K]), axis=1) # B: batch_size, K: sequence length. Get l_n: [B,2*K] l_n = tf.reshape(tf.repeat(tf.reshape(l_n, [B,1,K]), repeats=K, axis=1), [-1,2*K]) # repeat l_n ([B*K,2*K]) to match l_p([B*K,1]) 10: 11: l_all = tf.gather_nd(tf.concat([l_p, l_n], axis=1), tf.where(p_mask)) # get positive and corresponding negatives. l_all: [len(pos), 2*K+1] 12: loss = -tf.math.log_softmax( l_all/t )[:,0] # t: float, temperature. The first column of l_all is positive logit, while remaining is negative 13: return tf.reduce_sum(loss) 14: end function [16] Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song. 2017. Sphereface: Deep hypersphere embedding for face recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition . 212-220. [17] Weiyang Liu, Yandong Wen, Zhiding Yu, and Meng Yang. 2016. Large-margin softmax loss for convolutional neural networks. arXiv preprint arXiv:1612.02295 (2016). [18] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H. Chi. 2018. Modeling Task Relationships in Multi-task Learning with Multi-gate Mixtureof-Experts. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (London, United Kingdom) (KDD '18) . Association for Computing Machinery, New York, NY, USA, 1930-1939. https: //doi.org/10.1145/3219819.3220007 [19] Xu Ma, Pengjie Wang, Hui Zhao, Shaoguo Liu, Chuhan Zhao, Wei Lin, KuangChih Lee, Jian Xu, and Bo Zheng. 2021. Towards a Better Tradeoff between Effectiveness and Efficiency in Pre-Ranking: A Learnable Feature Selection based Approach. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (Canada) (SIGIR '21) . Association for Computing Machinery, New York, NY, USA, 2036-2040. https://doi.org/10.1145/3404835.3462979 [20] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. In Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2 (Lake Tahoe, Nevada) (NIPS'13) . Curran Associates Inc., Red Hook, NY, USA, 3111-3119. [21] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018). [22] H. Palangi, L. Deng, Y. Shen, J. Gao, X. He, J. Chen, X. Song, and R. Ward. 2015. Semantic Modelling with Long-Short-Term Memory for Information Retrieval. arXiv:1412.6629 [cs.IR] [23] Haojie Pan, Chengyu Wang, Minghui Qiu, Yichang Zhang, Yaliang Li, and Jun Huang. 2020. Meta-KD: A meta knowledge distillation framework for language model compression across domains. arXiv preprint arXiv:2012.01266 (2020). [24] Jiarui Qin, Jiachen Zhu, Bo Chen, Zhirong Liu, Weiwen Liu, Ruiming Tang, Rui Zhang, Yong Yu, and Weinan Zhang. 2022. RankFlow: Joint Optimization of MultiStage Cascade Ranking Systems as Flows. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (Madrid, Spain) (SIGIR '22) . Association for Computing Machinery, New York, NY, USA, 814-824. https://doi.org/10.1145/3477495.3532050 [25] Xiang-Rong Sheng, Jingyue Gao, Yueyao Cheng, Siran Yang, Shuguang Han, Hongbo Deng, Yuning Jiang, Jian Xu, and Bo Zheng. 2023. Joint Optimization of Ranking and Calibration with Contextualized Hybrid Model. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (Long Beach, CA, USA) (KDD '23) . Association for Computing Machinery, New York, NY, USA, 4813-4822. https://doi.org/10.1145/3580305.3599851 [26] Jinbo Song, Ruoran Huang, Xinyang Wang, Wei Huang, Qian Yu, Mingming Chen, Yafei Yao, Chaosheng Fan, Changping Peng, Zhangang Lin, Jinghe Hu, and Jingping Shao. 2022. Rethinking Large-Scale Pre-Ranking System: EntireChain Cross-Domain Models. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management (Atlanta, GA, USA) (CIKM '22) . Association for Computing Machinery, New York, NY, USA, 4495-4499. https://doi.org/10.1145/3511808.3557683 [27] Jiaxi Tang and Ke Wang. 2018. Ranking Distillation: Learning Compact Ranking Models With High Performance for Recommender System. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (London, United Kingdom) (KDD '18) . Association for Computing Machinery, New York, NY, USA, 2289-2298. https://doi.org/10.1145/3219819.3220021 [28] Feng Wang, Jian Cheng, Weiyang Liu, and Haijun Liu. 2018. Additive Margin Softmax for Face Verification. IEEE Signal Processing Letters 25, 7 (2018), 926-930. https://doi.org/10.1109/LSP.2018.2822810 [29] Jinpeng Wang, Jieming Zhu, and Xiuqiang He. 2021. Cross-Batch Negative Sampling for Training Two-Tower Recommenders. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (Canada) (SIGIR '21) . Association for Computing Machinery, New York, NY, USA, 1632-1636. https://doi.org/10.1145/3404835.3463032 [30] X. Wang, H. Zhang, W. Huang, and M. R. Scott. 2020. Cross-Batch Memory for Embedding Learning. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . IEEE Computer Society, Los Alamitos, CA, USA, 6387-6396. https://doi.org/10.1109/CVPR42600.2020.00642 [31] Zhe Wang, Liqin Zhao, Biye Jiang, Guorui Zhou, Xiaoqiang Zhu, and Kun Gai. 2020. COLD: Towards the Next Generation of Pre-Ranking System. CoRR abs/2007.16122 (2020). arXiv:2007.16122 https://arxiv.org/abs/2007.16122 [32] Fen Xia, Tie-Yan Liu, Jue Wang, Wensheng Zhang, and Hang Li. 2008. Listwise approach to learning to rank: theory and algorithm. In Proceedings of the 25th International Conference on Machine Learning (Helsinki, Finland) (ICML '08) . Association for Computing Machinery, New York, NY, USA, 1192-1199. https: //doi.org/10.1145/1390156.1390306 [33] Chen Xu, Quan Li, Junfeng Ge, Jinyang Gao, Xiaoyong Yang, Changhua Pei, Fei Sun, Jian Wu, Hanxiao Sun, and Wenwu Ou. 2020. Privileged Features Distillation at Taobao Recommendations. arXiv:1907.05171 [cs.IR] [34] Le Yan, Zhen Qin, Xuanhui Wang, Michael Bendersky, and Marc Najork. 2022. Scale Calibration of Deep Ranking Models. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (Washington DC, USA) (KDD '22) . Association for Computing Machinery, New York, NY, USA, 4300-4309. https://doi.org/10.1145/3534678.3539072 [35] Bianca Zadrozny. 2004. Learning and Evaluating Classifiers under Sample Selection Bias. In Proceedings of the Twenty-First International Conference on Machine Learning (Banff, Alberta, Canada) (ICML '04) . Association for Computing Machinery, New York, NY, USA, 114. https://doi.org/10.1145/1015330.1015425 [36] Yin Zhang, Ruoxi Wang, Derek Zhiyuan Cheng, Tiansheng Yao, Xinyang Yi, Lichan Hong, James Caverlee, and Ed H. Chi. 2023. Empowering Long-tail Item Recommendation through Cross Decoupling Network (CDN). In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (Long Beach, CA, USA) (KDD '23) . Association for Computing Machinery, New York, NY, USA, 5608-5617. https://doi.org/10.1145/3580305.3599814 [37] Zhixuan Zhang, Yuheng Huang, Dan Ou, Sen Li, Longbin Li, Qingwen Liu, and Xiaoyi Zeng. 2023. Rethinking the Role of Pre-ranking in Large-scale E-Commerce Searching System. arXiv:2305.13647 [cs.IR] [38] Zhishan Zhao, Jingyue Gao, Yu Zhang, Shuguang Han, Siyuan Lou, Xiang-Rong Sheng, Zhe Wang, Han Zhu, Yuning Jiang, Jian Xu, and Bo Zheng. 2023. COPR: Consistency-Oriented Pre-Ranking for Online Advertising. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management (Birmingham, United Kingdom) (CIKM '23) . Association for Computing Machinery, New York, NY, USA, 4974-4980. https://doi.org/10.1145/3583780.3615465 [39] Guorui Zhou, Chengru Song, Xiaoqiang Zhu, Xiao Ma, Yanghui Yan, Xingya Dai, Han Zhu, Junqi Jin, Han Li, and Kun Gai. 2017. Deep Interest Network for Click-Through Rate Prediction. (2017)."}
