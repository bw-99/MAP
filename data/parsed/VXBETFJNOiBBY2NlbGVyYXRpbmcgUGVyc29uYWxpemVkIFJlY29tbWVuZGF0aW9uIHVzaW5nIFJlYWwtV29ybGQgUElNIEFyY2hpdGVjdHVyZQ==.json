{
  "UpDLRM: Accelerating Personalized Recommendation using Real-World PIM Architecture": "Sitian Chen 1 , Haobin Tan 2 , Amelie Chi Zhou 1 ∗ , Yusen Li 3 , Pavan Balaji 4 1 Hong Kong Baptist University 2 Shenzhen University 3 Nankai University 4 Meta",
  "ABSTRACT": "Deep Learning Recommendation Models (DLRMs) have gained popularity in recommendation systems due to their effectiveness in handling large-scale recommendation tasks. The embedding layers of DLRMs have become the performance bottleneck due to their intensive needs on memory capacity and memory bandwidth. In this paper, we propose UpDLRM , which utilizes real-world processingin-memory (PIM) hardware, UPMEM DPU, to boost the memory bandwidth and reduce recommendation latency. The parallel nature of the DPU memory can provide high aggregated bandwidth for the large number of irregular memory accesses in embedding lookups, thus offering great potential to reduce the inference latency. To fully utilize the DPU memory bandwidth, we further studied the embedding table partitioning problem to achieve good workload-balance and efficient data caching. Evaluations using real-world datasets show that, UpDLRM achieves much lower inference time for DLRM compared to both CPU-only and CPU-GPU hybrid counterparts. Some recent work proposed to load popular data into limited GPU memory to further reduced data access latency [4]. However, the hybrid architecture introduces unavoidable communication between CPUs and GPUs, and may cause GPU stalls due to the memory bottleneck of CPUs. Since DLRM inference is less compute-intensive compared to training, recent studies [8] found it is possible to make real-time recommendations at scale based on only CPUs. However, memory is still a major performance bottleneck and workloadaware pre-processing (e.g., prefetching, caching) are needed to reduce inference latency [8]. In this paper, we proposed to boost the memory bandwidth of CPUs using real-world processing-inmemory (PIM) hardware, UPMEM DPU [2].",
  "1 INTRODUCTION": "Deep Learning Recommendation Models (DLRMs) have become widely utilized for click-through rate (CTR) prediction and news rankings [5, 13]. According to Meta, DLRM models account for more than 60% of their AI inference cycles in production, making them a primary target for optimization [7]. However, performance optimization for DLRMs is challenging, mainly due to their intensive needs on memory capacity and bandwidth [19]: 1) The number of parameters in DLRMs is much larger than traditional deep learning models due to the embedding layers which map high-dimensional categorical features (e.g., user IDs) to lower-dimensional dense vectors. The categorical features are encoded using embedding tables which can be very large in real-world scenarios (e.g., 10s of GBs). 2) During inference, DLRMs perform multiple embedding lookups and reductions , requiring frequent and irregular memory accesses. These contribute significantly to the memory footprint and introduce memory bottlenecks, leading to increased latency. UPMEM DPU is equipped with dedicated processing units connected closely to memory cells (details in Section 2.2). We propose to store the large EMTs using DPU memory and perform costly memory lookups and reductions using DPUs. This design offers two compelling benefits. First , by offloading the memory-bound embedding operations to DPUs, we can reduce the resource contention on CPU memory bandwidth and hence accelerate the inference time. Second , the parallel nature of DPU memory banks allows for efficient processing of multiple embedding lookups and reductions simultaneously, further accelerating the inference time. Due to the limited memory capacity of GPUs, existing studies mainly adopt CPU-GPU hybrid architecture for DLRM inference, where the embedding tables (EMTs) are stored in CPU memories and GPUs are employed for highly parallel computations [13, 20]. *Corresponding author: Amelie Chi Zhou. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. DAC '24, June 23-27, 2024, San Francisco, CA, USA https://doi.org/10.1145/3649329.3658266 © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0601-1/24/06. In this paper, we propose UpDLRM , which utilizes UPMEM PIM architecture to accelerate the memory-intensive embedding layers of DLRM, in order to achieve low inference time. Considering the above mentioned design issues, we study the EMT partitioning problem at three different levels. First, we consider the EMT partitioning only consider the hardware features, in order to fully utilize the memory bandwidth and maximize the throughput for embedding lookup operations (Section 3.1). Second, we take the imbalanced data access frequencies into account, and proposed non-uniform EMT partitioning to achieve workload balance among DPUs. This ensures efficient utilization of DPU resources and minimizes embedding layer processing time (Section 3.2). Third, we further consider There exist several design issues for efficiently utilizing DPUs. Since the capacity of each DPU memory bank (MRAM) is limited to 64MB, it is usually necessary to partition an EMT onto multiple DPUs. When partitioning EMTs, we have to jointly consider the architectural features of DPUs and the memory access patterns of embedding operations to efficiently utilize the DPU memory bandwidth. First , UPMEM DIMM does not support inter-DPU communication directly. Data movement across DPUs has to go through CPU memory. Thus, when partitioning EMTs, we should avoid frequent inter-DPU data exchange. Second , each DPU core supports multi-threads and pipeline execution. We should fully utilize this hardware feature to improve the efficiency of DPU-supported embedding operations. Third , the popularities of items differ in real-world datasets [20]. Thus, when items in EMTs are partitioned onto different DPUs, we have to consider the aggregated access frequency of each partition to achieve workload balance. DAC '24, June 23-27, 2024, San Francisco, CA, USA Sitian Chen 1 , Haobin Tan 2 , Amelie Chi Zhou 1 ∗ , Yusen Li 3 , Pavan Balaji 4 … FC FC Dense inputs … Bottom-FC Layers FC FC … CTR output … … + Concatenate EMT 1 Reduction Sparse inputs Top-FC Layers … … EMT N Look up … Look up Sparse inputs Embedding Layers Figure 1: Architecture of DLRMs the co-occurrence feature of multiple items in real recommendation systems, and adopt existing caching technique [20] to further reduce memory traffic. To deal with the workload imbalance problem introduced by data caching, we proposed a cache-aware partitioning method that jointly balance the memory accesses on cache storage and regular EMT storage (Section 3.3). Evaluations on six real-world datasets and four DLRM implementations using different hardware architectures have demonstrated the effectiveness of UpDLRM on optimizing the inference time. Overall, it achieved up to 4.6x speedup on inference time compared to the other comparisons.",
  "2 BACKGROUND AND MOTIVATION": "",
  "2.1 DLRM Model": "Figure 1 shows the structure of DLRMs. The bottom-FC layers process 'dense' inputs that are usually continuous attributes (e.g., age, price, rating) to generate dense features. The embedding layers handle 'sparse' inputs that are usually categorical features (e.g., user ID, gender, location) encoded using one-hot or multi-hot to generate embeddings. Each embedding layer contains a large embedding table (EMT) responsible for storing feature vectors of a specific category. After obtaining the dense and sparse features, they are concatenated and fed into the top FC layer to predict the CTR. The embedding layer is the main component that causes high memory capacity and bandwidth requirements of DLRMs. Memory Bandwidth Requirement: During DLRM inference, the embedding layer takes the input categorical values (indices) and performs a lookup operation in the EMT to retrieve corresponding embedding vectors. This operation requires fetching embedding vectors from different memory locations based on the input indices, which can result in frequent memory accesses and require high bandwidth to sustain the data transfer rates. In real-world scenarios, the popularity of different categorical values can vary significantly [20]. This can lead to imbalanced accesses to different memory locations in the EMTs. Memory Capacity Requirement: The EMTs are essentially lookup tables, where each row in the table represents the embedding vector associated with a specific value of the category. Hence the size of a EMTis determined by the number of distinct values in a categorical feature. For example, consider the category being user ID. The number of users in a commercial recommendation system can easily reach billions (e.g., over 3 billions in Meta [14]). Assume the number of users is one billion and the embedding dimension is 20. The size of the table is 10 9 ∗ 20 ∗ 4bytes, namely 80GB. Since there are usually many categories considered in real-world recommendation systems, the memory consumption of EMTs is substantial. PIM Chip PIM Chip DPU IRAM WRAM DMA Engine MRAM 64 bits DRAM Chip Host CPU DRAM Chip … … × 8 8 16 32 64 128 256 512 1024 2048 0 300 600 900 1200 Data tranfer size(byte) Latency(cycle) Figure 2: UPMEM's PIM architecture Figure 3: MRAM read latency",
  "2.2 UPMEM PIM Architecture": "The UPMEM PIM architecture is the first real-world PIM system commercialized in 2019 [2, 6]. Figure 2 plots the architecture of UPMEM's PIM system. A UPMEM module consists of a standard DDR4 DIMM module that incorporates 8-16 PIM chips. Each chip comprises 8 processing cores known as DRAM Processing Units (DPUs). Each DPU has exclusive access to specific memory components, including a 64-MB DRAM bank MRAM, a 24-KB instruction memory IRAM, and a 64-KB scratchpad memory WRAM. The MRAM can be accessed by the host CPU for transferring input data (main memory to MRAM) and retrieving results (MRAM to main memory). These data transfers can occur concurrently if the buffers transferred to and from all MRAM banks are of the same size. Otherwise, the transfers happen sequentially. It's important to note that inter-DPU communication relies on the host CPU. Different from RRAM-based PIM architectures [11, 19], which perform arithmetic operations using NOR gates within the RRAM arrays, UPMEM's PIM is the first 'real' process in memory hardware that contains dedicated processing units, namely DPUs. Each DPU is a multi-threaded 32-bit RISC core with its own specific ISA [3] to facilitate data transfers between the MRAM and the WRAM. The maximum MRAM-WRAM bandwidth per DPU can achieve approximately 800 MB/s [2]. All DPUs in the UPMEM module function together as a parallel co-processor to the host CPU.",
  "2.3 Motivation": "Embedding layers are the primary performance bottlenecks of DLRM inference due to their high requirements on memory capacity and memory bandwidth. The PIM architecture can be utilized to alleviate the memory bandwidth bottleneck and process embedding data near memory. Additionally, the UPMEM technical paper [2] indicates that the total cost of ownership (TCO) gain for the PIM platform is estimated to be around 10x, with a potential reduction of 60% in energy consumption. Thus, studying PIM-accelerated DLRM inference could potentially lead to cost- and energy-efficient solution to industry. In this paper, we propose to store the large embedding tables in the MRAM of DPUs and perform lookup operations using near memory DPU cores. Since each DPU MRAM has a limited capacity of 64MB, an EMT that is larger than 64MB has to be partitioned first to fit in the DPU memory. The partitioning has to consider several design issues as discussed next.",
  "3 DESIGN DETAILS": "",
  "3.1 Embedding Table Partitioning": "We partition each embedding table into tiles smaller than 64MB to fit in the DPU memories. To simplify the problem, let's first consider UpDLRM: Accelerating Personalized Recommendation using Real-World PIM Architecture DAC '24, June 23-27, 2024, San Francisco, CA, USA Input Samples EMT 1 Forward Propagation Pre-process 1 2 3 4 EMT 0 IDX EMT 0 OFFSET EMT 1 IDX EMT 1 OFFSET Dense Feature Feature Interaction Top MLP CTR Tile 0 DPU Group 0 DPU Group 0 DPU Group 1 Multi-hot lookup Aggregate EMT 0 Tile 3 Multi-hot lookup Aggregate Partial sum Final results Dense Feature Received result CTR Computation Tile 4 Tile 5 Tile 0 DPU Group 1 Tile 3 Tile 4 Tile 5 Tile 0 Tile 5 … … … Tile 0 Tile 5 … … … Tile 1 Tile 2 Tile 1 Tile 2 Node CPU-DPU DPU-CPU DPU CPU Node CPU-DPU DPU Figure 4: Pre-processing and forward propagation stages of our DPU-based DLRM implementation 1 2 3 4 5 6 7 8 0.0 0.2 0.4 0.6 0.8 Normalized number of accesses id Goodreads Movie Twitch 1 2 3 4 5 6 7 8 0.00 0.05 0.10 0.15 Normalized number of accesses id w/o cache cache miss cache hit 0.20 Figure 5: Proportion of partitions being accessed Figure 6: Access pattern w/ and w/o cache using Movie uniform partitioning, namely all tiles have the same number of rows (denoted as 𝑁 𝑟 ) and columns (denoted as 𝑁 𝑐 ). Assume each feature value is represented using 32-bit data, we have 𝑁 𝑟 ∗ 𝑁 𝑐 ∗ 4B ≤ 64MB. Each embedding lookup operation reads 𝑁 𝑐 *4 bytes of data from MRAMatthesametime. We study the performance of each memory access on MRAM as shown in Figure 3. Note that each MRAM read has to be 8 bytes aligned and can be 2,048 bytes maximum. Interestingly, we found that when the size of data increases from 8B to 32B, the latency increases rather slowly. After 32B, the latency increases more dramatically. The partitioning approach should carefully consider this feature when deciding the tile size. That is, we prefer to have 𝑁 𝑐 *4B ≤ 32B, namely 𝑁 𝑐 ≤ 8. As shown in Figure 4, during the forward propagation, given a batch of embedding table indices and offsets (multi-hot vectors), the corresponding feature data are fetched from DPUs and transferred back to CPU. The feature data fetching in stage 2 contains two steps. First, the multi-hot lookup operation searches in DPU MRAMs for corresponding feature data specified by IDX and OFFSET. Second, multiple vectors are returned and aggregated within DPU to utilize its computation power. Due to EMT partitioning, the aggregated embedding vectors are only partial results and need to be transferred back to the CPU to generate final feature vectors. We study the time spent on embedding layers as the sum of CPU-DPU communication time (stage 1), embedding lookup time (stage 2) and DPU-CPU communication time (stage 3). Multiple DPUs perform lookup operations concurrently. Assuming balanced distribution of memory accesses in the dataset, uniform EMT partitioning can result in the same embedding lookup time on each DPU. Approximately, we can estimate the lookup time on each DPU as 𝑇 𝑙𝑘𝑝 = 𝑁 𝑟 𝑅 ∗ batch_size ∗ Avg_Red ∗ 𝑡 𝑎 , where 𝑡 𝑎 represents the memory access time indicated in Figure 3 and Avg_Red represents the average reduction in the number of active features (ones) in the multi-hot encoding. Similarly, the communication time for transferring indices from CPU to DPU memory can be estimated as 𝑇 𝑐 -𝑐𝑜𝑚𝑚 = 𝑁 𝑟 𝑅 ∗ batch_size ∗ Avg_Red ∗ 𝑡 𝑐 , where 𝑡 𝑐 represents the time to transfer one value from CPU to DPU. The time for transferring partial results from DPU to CPU memory can be estimated as 𝑇 𝑑 -𝑐𝑜𝑚𝑚 = 𝑁 𝑐 ∗ batch_size ∗ 𝑡 𝑑 , where 𝑡 𝑑 represents the time to transfer one value from DPU to CPU. Thus, to minimize the time on embedding layers, we aim to find a good tradeoff between 𝑁 𝑟 and 𝑁 𝑐 to achieve the following goal:  s.t.,   where 𝑁 𝑑𝑝𝑢 represents the number of DPUs used to stored EMTs. 𝑅 and 𝐶 represent the number of rows and columns of the EMT, respectively. Both 𝑁 𝑑𝑝𝑢 and 𝑅 ∗ 𝐶 are fixed inputs to the problem. The constraints greatly reduce the solution space of the problem. Thus we can simply search for the best 𝑁 𝑟 and 𝑁 𝑐 exhaustively.",
  "3.2 Non-Uniform EMT Partitioning": "In the above discussion, we assume balanced distribution of memory accesses in a dataset, which is usually not true in real-world applications. We study the data access patterns of EMTs using three real-world datasets Goodreads [17, 18], Movie [15] and Twitch [16]. Figure 5 shows the total number of accesses per row block when dividing the rows into 8 blocks. Clearly, the three datasets all show imbalanced access distributions. The most popular block has 340 times higher number of accesses compared to the least popular block. This will lead to workload imbalance problem to DPUs and make the uniform partitioning no longer optimal. To accommodate this, we propose a non-uniform EMT partitioning method that considers the imbalanced data access pattern in real datasets. Specifically, consider using the same number of DPUs as the uniform partitioning, our non-uniform partitioning cuts an EMT into tiles of varying sizes and stores each tile in a separate DPU. DPUs used to store the same EMT collectively form a group as shown in Figure 4. The goal is to achieve optimal workload balance among DPUs. We adopt a greedy method to achieve this goal. Specifically, by profiling the historical user-item access trace, we can obtain the access frequencies of all items. Consider each DPU as a bin, the problem of assigning each row to DPUs is a classical Bin-Packing problem with a fixed number of bins. We first order items according to their frequencies and then iteratively DAC '24, June 23-27, 2024, San Francisco, CA, USA Sitian Chen 1 , Haobin Tan 2 , Amelie Chi Zhou 1 ∗ , Yusen Li 3 , Pavan Balaji 4",
  "Algorithm 1 Cache-Aware Non-Uniform Partitioning": "Figure 7: Example of cache-aware partitioning 1, 4, 5 EMT Lookup Request EMT 1 Cache 1 3 1 index EMT Vector … 6 + 7 DPU 0 EMT 0 Cache 0 2 0 index EMT Vector 8 + 9 4 + 5 1 0 … 6, 7 4, 5 Cache index dest 0 0 … 2 0 EMB index dest 4 + 5 1 + Reduction DPU 1 1 + 4 + 5 1 1 Frequency Miss Hit Pre-processing assign each row to the bin with the lowest total frequency unless the bin is full (64MB capacity). Note that we adopt the same 𝑁 𝑐 as optimized in uniform partitioning. The complexity of the algorithm is hence 𝑂 ( 𝑅 ) . One could batch items when doing the assignment to reduce algorithm complexity. Consider the Movie dataset which has a skewed item access frequency as shown in Figure 5. When partitioning the EMT into 8 row blocks with our non-uniform algorithm, it can be observed from Figure 6 that the number of accesses per partition is much more balanced (i.e., w/o cache).",
  "3.3 Cache-Aware Non-Uniform Partitioning": "Existing studies [9, 20] leverage the power-law distribution of item access frequencies to cache the partial sum of popular items to reduce memory traffic and accelerate DLRM inference. For example, the recent GRACE [20] algorithm takes the co-occurrences of popular items into account, and adopts a graph-based algorithm to identify frequently accessed item combinations. These combinations' partial sums are cached to reduce memory traffic. The caching techniques have already demonstrated effectiveness on reducing DLRM inference latency. However, when applied to our DPU-based DLRM, they further exacerbate the workload imbalance problem among DPUs. As shown in Figure 6, when partitioning the Movie dataset using our non-uniform algorithm with GRACEbased caching, the total number of memory accesses is reduced by 40% compared to without caching, while the access pattern is much imbalanced across DPUs. To address this issue, we propose cache-aware non-uniform partitioning as shown in Algorithm 1. The function takes three inputs, including 𝑁 𝑑𝑝𝑢 , 𝑜𝑏𝑗 _ 𝑓 𝑟𝑒𝑞 which records the access frequency history of different items, and 𝑐𝑎𝑐ℎ𝑒 _ 𝑟𝑒𝑠 which is a cache list containing partial sums generated by existing caching technique [20]. Table 1: Workload Configurations A cache list of { 𝑎, 𝑏, 𝑐 } means that items 𝑎 , 𝑏 and 𝑐 frequently coexist in the same sample, so that partial sums 𝑎 , 𝑏 , 𝑐 , 𝑎 + 𝑏 , 𝑎 + 𝑐 , 𝑏 + 𝑐 and 𝑎 + 𝑏 + 𝑐 are cached to reduce the memory traffic. We divide each MRAM capacity into two parts: one storing embedding vectors and the other storing the cached partial sums. Initially, we sort 𝑜𝑏𝑗 _ 𝑓 𝑟𝑒𝑞 in descending order. That is, items with higher frequencies are ranked in front. To decide the partition of an item, we first look for it in the cache list (Line 4). If it exists in the list, we store the item in a partition with the minimum total frequency and has enough capacity for cache storage (Line 5-10). If the item does not exist in cache list, we store it in the EMT part of MRAM storage (Line 12-15). With this greedy algorithm, we aim to balance the combined number of memory accesses (EMT+cache) per partition. In the above design, one parameter is the cache capacity. The cache space and EMT space share the 64MB MRAM size. The larger share for cache space, the higher reduction can be obtained on embedding lookup time. For example, using GoodReads dataset and varying the cache capacity to 40%, 70% and 100% of the required storage size of cache list, our cache-aware non-uniform partitioning is able to reduce the embedding lookup time by 17%, 22% and 26%, respectively, compared to that of without caching. The downside of storing large cache list is that it requires larger memory capacity. By default, we set the cache capacity to 100% of the required size. Figure 7 illustrates an example of our cache-aware partitioning algorithm. Consider a sample request to look up indices 1, 4 and 5. The system checks the cache index to determine if the indices are stored in the cache space. For example, in our scenario, indices 4 and 5 are stored in DPU0's cache space, allowing us to read the MRAM once to obtain the partial sum 4 + 5. This helps to reduce the number of memory accesses from two to one. Index 1 is not stored in the cache space, so we retrieve its embedding vector from the EMT space. Ultimately, each DPU returns a partial sum to the CPU (e.g., 4+5 and 1). We combine and aggregate these partial sums using CPU to obtain final results.",
  "4 EVALUATION": "",
  "4.1 Experimental Setup": "Workloads. WeadoptMeta's deep learning recommendation model (DLRM) [13] with six real-world datasets 1 , as shown in Table 1. The datasets can be categorized into three groups, namely low hot, medium hot and high hot, according to the average reduction frequency of the dataset. In our experiments, we duplicate each dataset to form eight EMTs and each embedding vector has 32 dimensions. To measure inference performance, we conduct a sampling of 12,800 inferences in each set of experiments. The batch size is set to 64. Comparisons. We compare UpDLRM with three other opensource DLRM implementations, as detailed in Table 2. DLRM-CPU 1 https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2 https://mengtingwan.github.io/data/goodreads.html UpDLRM: Accelerating Personalized Recommendation using Real-World PIM Architecture DAC '24, June 23-27, 2024, San Francisco, CA, USA Table 2: Specifics of evaluated hardware architectures Figure 8: Inference performance speedup of compared approaches over that of DLRM-CPU clo home meta1 meta2 read read2 0 1 2 3 Forward Speedup DLRM-Hybrid DLRM-CPU FAE UpDLRM Normalizd Figure 9: Performance speedup of embedding layers obtained by three partitioning methods over DLRM-CPU clo home meta1 meta2 read read2 U NU CA U NU CA U NU CA U NU CA U NU CA U NU CA Low Hot Medium Hot High Hot 0 2 4 6 Speedup N c = 2 N c = 4 N c = 8 is the CPU-only implementation that adopts CPU for both EMT storage and parallel computations. DLRM-Hybrid and FAE [4] are two implementations based on the CPU-GPU hybrid architecture. In the hybrid architecture, CPU is used to store EMTs and perform embedding lookups, while the GPU manages CTR computation. Embedding lookup results are communicated to the GPU via PCI-e. The FAE approach differs from DLRM-Hybrid in that it involves placing a subset of highly accessed item embedding vectors in a cache space, such as the GPU memory, to accelerate DLRM inference time. Our UpDLRM implementation is illustrated in Figure 4, where cache-aware EMT partitioning is used in the pre-process stage. We adopt two UPMEM modules, totalling 256 DPUs. Each DPU employs 14 tasklets.",
  "4.2 Effectiveness of UpDLRM": "Figure 8 illustrates the performance of the compared implementations on the six datasets, in terms of inference time speedup compared to that of DLRM-CPU. Overall, UpDLRM obtains the best speedup performance among all comparisons for all datasets. Specifically, UpDLRM accelerate the inference time by 1.9x-3.2x, 2.2x-4.6x and 1.1x-2.3x compared to DLRM-CPU, DLRM-Hybrid and FAE, respectively. Especially, it obtains higher speedup when the average reduction of the dataset is high. A larger average reduction represents a higher number of embedding lookup operations in a dataset. Thus, the results demonstrate that our DPU-based architecture is effective on improving the performance of embedding layers. Unsurprisingly, FAE outperforms DLRM-CPU and DLRMHybrid, demonstrating the effectiveness of its caching technique. DLRM-Hybrid performs the worst among all, mainly due to the fact that significant time is required by the CPU to execute embedding lookups, leading to GPUs waiting for the embedding results before proceeding with CTR computation. To learn the improvements obtained by UpDLRM in details, we further perform comparative evaluation of the three proposed EMT Ratio 2 4 8 2 4 8 2 4 8 U NU CA 0.0 0.2 0.4 0.6 0.8 1.0 CPU-DPU DPU Lookup DPU-CPU 50 100 150 200 250 300 400 800 1200 1600 Avg.Reduction Frequency 8 B 16 B 32 B 64 B 128 B DPU lookup time (us) Figure 10: Latency breakdown of embedding layers with three partitioning methods using Goodreads. 𝑁 𝑐 is fixed at 2, 4 or 8. Figure 11: The DPU lookup time under varying average reduction frequencies and lookup data sizes (8B to 128B). partitioning methods, namely uniform (U), non-uniform (NU) and cache-aware (CA), using the six datasets. Figure 9 illustrates the performance speedup of the embedding layer obtained by the three methods compared to DLRM-CPU. The number of columns per partition (i.e., 𝑁 𝑐 ) is fixed to 2, 4 or 8 for different partitioning methods. We have the following observations: 1) Under the same 𝑁 𝑐 value, CA partitioning obtains larger speedup compared to U and NU for High Hot datasets. This explains the good performance of UpDLRM under High Hot in Figure 8. 2) The three partitioning methods perform almost the same for the 'clo' dataset. This is because the item access pattern in this dataset is quite balanced, and the cache rate is low. 3) There's no universally good choice of 𝑁 𝑐 for different datasets. For example, 𝑁 𝑐 = 4 works the best for the first three datasets while 𝑁 𝑐 = 8 is optimal for the latter three. Recall that a large 𝑁 𝑐 value leads to higher DPU-CPU communication time but lower embedding lookup time and CPU-DPU communication time, as discussed in Section 3.1. UpDLRM can automatically achieve good balance between the two for different datasets.",
  "4.3 Latency breakdown": "To study the performance of each component of Figure 4, we further breakdown the latency of UpDLRM into three parts, namely CPUDPU communication time (stage 1), DPU lookup time (stage 2) and DPU-CPU communication time (stage 3). Figure 10 shows the breakdown results when using uniform (U), non-uniform (NU) and cache-aware (CA) partitioning in pre-process stage of UpDLRM. 𝑁 𝑐 is fixed at 2, 4 and 8. We have the following observations. Second, comparing the same partitioning method under different 𝑁 𝑐 values, the CPU-DPU communication time increases and DPU-CPU communication time decreases with the increase of 𝑁 𝑐 . For example, for CA, the CPU-DPU time ration reduces from 31%21% and DPU-CPU time ratio increases from 17%-35% when 𝑁 𝑐 increases from 2 to 8. This is consistent with our analysis of UpDLRM performance in Section 3.1. First, comparing different partitioning methods, CA reduces the ratio of DPU lookup time in the embedding time from 71%77% to 43%-52%, compared to the other partitioning methods. This means that the partial sum caching together with the cache-aware partitioning are effective on reducing the embedding lookup time in UpDLRM. With the ratio of embedding lookup time decreases to below 50%, it means the bottleneck effect of the embedding operations has been greatly mitigated. DAC '24, June 23-27, 2024, San Francisco, CA, USA Sitian Chen 1 , Haobin Tan 2 , Amelie Chi Zhou 1 ∗ , Yusen Li 3 , Pavan Balaji 4",
  "4.4 Sensitivity Study": "In Figure 8, we have observed better performance of UpDLRM for datasets with higher average reduction. To better study the potential of UpDLRM, we create a set of synthetic datasets with balanced item access patterns and varying average reduction frequencies ranging from 50 to 300. We vary 𝑁 𝑐 from 2, 4, ..., to 32, thus resulting in varying data sizes of 8B, 16B, ..., to 128B. These sizes correspond to the amount of data loaded from MRAM per lookup. We set batch size to 64 samples. Figure 11 shows the DPU lookup time results under different configurations. We have the following observations. Second, given a fixed average reduction frequency, the DPU lookup time greatly reduces when the lookup data size increases from 8B to 32B. Recall our previous observation in Figure 3, the MRAM access latency remains almost unchanged when the access data size varies between 8B and 32B. With larger per lookup data size, less number of lookup operations is needed and thus lower lookup time can be obtained. When the access data size increases to larger than 32B, the per lookup latency also increases, thus hinders the reduction of DPU lookup time. Thus, in previous experiments, we only set 𝑁 𝑐 to 2, 4 and 8 to study the performance of UpDLRM. First, under the same lookup data size, the DPU lookup time increases almost linearly with the increase of average reduction frequency. For example, when the lookup data size is 8B, the DPU lookup time increases from 406us to 1786us. When the data size gets larger, the increase of DPU lookup time with the increase of average reduction becomes rather stable. For example, when the lookup data size is 64B, the DPU lookup time increases from 456us to 787us when the average reduction increases from 50 to 200, and remains almost unchanged when the average reduction continues to increase to 300. This is because we employ 14 tasklets in one DPU, and the pipeline in high hot scenarios effectively masks the MRAM read latency, resulting in a comparable latency. The results demonstrate that, the embedding lookup performance of UpDLRM is quite scalable with the increase of average reduction frequency.",
  "5 RELATED WORK": "PIMPR [19] utilizes ReRAM crossbar, using NOR gates within the ReRAM arrays to maintain the Computing-In-Memory capability, to enhance DLRM acceleration. However, its hardware structure differs from UPMEM PIM, the first commercial processing-in-memory chip. RecNMP [10] implements near-memory processing of the embedding vector around specialized DIMMs to expand the accelerator's memory space to tens of GBs. These studies are orthogonal to ours due to the different hardware architectures. EVStore [12] implements a three-degree cache method to store popular items. It utilizes mixed precision datatype to cache a larger number of embedding vectors and leverages the similarity between embedding vectors to further enhance the cache hit rate. In addition, GRACE [20] and SPACE [9] leverage hot items to reduce memory access, caching the hot embedding vector and partial sum in a cache space. However, they do not consider scenarios involving allocating the hot embedding vector in multiple cache spaces, such as caching in multiple DPUs, potentially leading to workload imbalance. UpDLRM introduces a cache allocation method across multiple cache spaces and addresses the workload imbalance issue arising after caching. Note that, although we adopt GRACE to generate cache list in this paper, UpDLRM does not rely on GRACE and can work with any other caching technique.",
  "6 CONCLUSION": "This paper proposes UpDLRM , which utilizes real-world processingin-memory (PIM) hardware, UPMEM DPU, to boost the memory bandwidth and reduce recommendation latency. The parallel nature of the DPU memory can provide high aggregated bandwidth for the large number of irregular memory accesses in embedding lookups, thus offering great potential to reduce the inference latency of DLRMs. To fully utilize the DPU memory bandwidth, we further studied the embedding table partitioning problem to achieve good workload-balance and efficient data caching. Evaluations using realworld datasets show that, UpDLRM achieves up to 4.6x speedup in terms of inference performance compared to both CPU-only and CPU-GPU hybrid counterparts. In the future, we plan to work on designing a DPU-GPU heterogeneous system to further optimize the inference time of DLRM systems.",
  "ACKNOWLEDGEMENT": "This work is supported by National Natural Science Foundation of China (62172282, 62293510, 62293513, 62272252, 62272253), Guangdong Natural Science Foundation 2022A1515010122, Shenzhen Science and Technology Foundation RCYX20221008092908029, and a startup grant of HKBU.",
  "REFERENCES": "[1] 2022. Meta Research. Embedding Lookup Synthetic Dataset. https://github.com/ facebookresearch/dlrm_datasets. [3] 2023. UPMEM User Manual. https://sdk.upmem.com/2023.1.0/index.html. [2] 2023. Introducing the most advanced Processing In Memory product. https: //www.upmem.com/. [4] Muhammad Adnan et al. 2021. Accelerating recommendation system training by leveraging popular choices. arXiv preprint arXiv:2103.00686 (2021). [6] Fabrice Devaux. 2019. The true processing in memory accelerator. In HCS'19 . IEEE Computer Society, 1-24. [5] Paul Covington et al. 2016. Deep neural networks for youtube recommendations. In RecSys'16 . 191-198. [7] Udit Gupta et al. 2020. The architectural implications of facebook's dnn-based personalized recommendation. In HPCA'20 . IEEE, 488-501. [9] Hongju Kal et al. 2021. Space: locality-aware processing in heterogeneous memory for personalized recommendations. In ISCA'21 . IEEE, 679-691. [8] Rishabh Jain et al. 2023. Optimizing CPU Performance for Recommendation Systems At-Scale. In ISCA'23 . Article 77, 15 pages. [10] Liu Ke et al. 2020. Recnmp: Accelerating personalized recommendation with near-memory processing. In ISCA'20 . IEEE, 790-803. [12] Daniar H Kurniawan et al. 2023. EVStore: Storage and Caching Capabilities for Scaling Embedding Tables in Deep Recommendation Systems. In Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 . 281-294. [11] Heewoo Kim et al. 2023. RecPIM: A PIM-Enabled DRAM-RRAM Hybrid Memory System For Recommendation Models. In ISLPED'23 . 1-6. [13] Maxim Naumov et al. 2019. Deep learning recommendation model for personalization and recommendation systems. arXiv preprint arXiv:1906.00091 (2019). [15] Jianmo Ni et al. 2019. Justifying recommendations using distantly-labeled reviews and fine-grained aspects. In EMNLP-IJCNLP'19 . 188-197. [14] Maxim Naumov et al. 2020. Deep learning training in facebook data centers: Design of scale-up and scale-out systems. arXiv preprint arXiv:2003.09518 (2020). [16] Jérémie Rappaz et al. 2021. Recommendation on live-streaming platforms: Dynamic availability and repeat consumption. In RecSys'21 . 390-399. [18] Mengting Wan et al. 2019. Fine-grained spoiler detection from large-scale review corpora. arXiv preprint arXiv:1905.13416 (2019). [17] Mengting Wan et al. 2018. Item recommendation on monotonic behavior chains. In RecSys'18 . 86-94. [19] Tao Yang et al. 2023. PIMPR: PIM-based Personalized Recommendation with Heterogeneous Memory Hierarchy. In DATE'23 . 1-6. [20] Haojie Ye et al. 2023. GRACE: A Scalable Graph-Based Approach to Accelerating Recommendation Model Inference. In ASPLOS'23 . 282-301.",
  "keywords_parsed": [],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "Embedding Lookup Synthetic Dataset"
    },
    {
      "ref_id": "b3",
      "title": "UPMEM User Manual"
    },
    {
      "ref_id": "b2",
      "title": "Introducing the most advanced Processing In Memory product"
    },
    {
      "ref_id": "b4",
      "title": "Accelerating recommendation system training by leveraging popular choices"
    },
    {
      "ref_id": "b6",
      "title": "The true processing in memory accelerator"
    },
    {
      "ref_id": "b5",
      "title": "Deep neural networks for youtube recommendations"
    },
    {
      "ref_id": "b7",
      "title": "The architectural implications of facebook's dnn-based personalized recommendation"
    },
    {
      "ref_id": "b9",
      "title": "Space: locality-aware processing in heterogeneous memory for personalized recommendations"
    },
    {
      "ref_id": "b8",
      "title": "Optimizing CPU Performance for Recommendation Systems At-Scale"
    },
    {
      "ref_id": "b10",
      "title": "Recnmp: Accelerating personalized recommendation with near-memory processing"
    },
    {
      "ref_id": "b12",
      "title": "EVStore: Storage and Caching Capabilities for Scaling Embedding Tables in Deep Recommendation Systems"
    },
    {
      "ref_id": "b11",
      "title": "RecPIM: A PIM-Enabled DRAM-RRAM Hybrid Memory System For Recommendation Models"
    },
    {
      "ref_id": "b13",
      "title": "Deep learning recommendation model for personalization and recommendation systems"
    },
    {
      "ref_id": "b15",
      "title": "Justifying recommendations using distantly-labeled reviews and fine-grained aspects"
    },
    {
      "ref_id": "b14",
      "title": "Deep learning training in facebook data centers: Design of scale-up and scale-out systems"
    },
    {
      "ref_id": "b16",
      "title": "Recommendation on live-streaming platforms: Dynamic availability and repeat consumption"
    },
    {
      "ref_id": "b18",
      "title": "Fine-grained spoiler detection from large-scale review corpora"
    },
    {
      "ref_id": "b17",
      "title": "Item recommendation on monotonic behavior chains"
    },
    {
      "ref_id": "b19",
      "title": "PIMPR: PIM-based Personalized Recommendation with Heterogeneous Memory Hierarchy"
    },
    {
      "ref_id": "b20",
      "title": "GRACE: A Scalable Graph-Based Approach to Accelerating Recommendation Model Inference"
    }
  ]
}