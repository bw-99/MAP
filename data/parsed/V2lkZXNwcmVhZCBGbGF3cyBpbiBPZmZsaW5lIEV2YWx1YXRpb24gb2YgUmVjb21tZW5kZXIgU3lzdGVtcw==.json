{
  "Widespread Flaws in Offline Evaluation of Recommender Systems": "",
  "BAL√ÅZS HIDASI and √ÅD√ÅM TIBOR CZAPP, Gravity R&D, a Taboola company, Hungary": "Even though offline evaluation is just an imperfect proxy of online performance - due to the interactive nature of recommenders - it will probably remain the primary way of evaluation in recommender systems research for the foreseeable future, since the proprietary nature of production recommenders prevents independent validation of A/B test setups and verification of online results. Therefore, it is imperative that offline evaluation setups are as realistic and as flawless as they can be. Unfortunately, evaluation flaws are quite common in recommender systems research nowadays, due to later works copying flawed evaluation setups from their predecessors without questioning their validity. In the hope of improving the quality of offline evaluation of recommender systems, we discuss four of these widespread flaws and why researchers should avoid them.",
  "CCS Concepts: ¬∑ Information systems ‚Üí Recommender systems .": "Additional Key Words and Phrases: recommender systems, offline evaluation, evaluation setups, evaluation flaws",
  "ACMReference Format:": "Bal√°zs Hidasi and √Åd√°m Tibor Czapp. 2023. Widespread Flaws in Offline Evaluation of Recommender Systems. In Seventeenth ACM Conference on Recommender Systems (RecSys '23), September 18-22, 2023, Singapore, Singapore. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3604915.3608839",
  "1 INTRODUCTION": "Evaluation is one of the biggest challenges of recommender systems research. Online A/B testing is already an approximation of the recommender's true goal, limited by KPIs that can be measured. Splitting traffic fairly, preventing information leak between groups and eliminating biases [18] are also non-trivial. From the business perspective, A/B tests are expensive and slow because a portion of the traffic is served by suboptimal models, and getting statistically significant differences for informative but noisy KPIs can take weeks/months. From the scientific perspective, A/B tests are not reproducible due to the proprietary nature of production recommenders preventing independent validation of A/B test setups and verification of online results. Therefore, the inherently imperfect approximation of online performance through offline evaluation is likely to stay the main way of assessing performance in research, at least until simulators [23] become more mature. Thus, it is imperative that offline evaluation is correct and follows how production recommenders work as closely as possible. Unfortunately, flaws are quite common, due to later works copying flawed evaluation setups from their predecessors without questioning their validity [7]. In this paper we discuss four widespread evaluation flaws we observed en masse in research papers of the last decade. Our main contribution is presented in Section 2, where we go through the most important steps of designing offline evaluation setups and discuss common pitfalls. We closely examine four severe flaws commonly found in the evaluation setup of various recommenders: dataset-task mismatch, general claims on heavily preprocessed data, information leaking through time, and negative item sampling. Their effect is demonstrated on sequential recommenders. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ¬© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. Manuscript submitted to ACM 1 RecSys '23, September 18-22, 2023, Singapore, Singapore Hidasi and Czapp",
  "2 DESIGN FLAWS IN EVALUATION SETUPS": "In this section we go through the main steps of designing offline evaluation and point out potential pitfalls and flaws. Recommender systems are utilized in various scenarios covering a wide spectrum of domains, use-cases, data and goals, which might require completely different approaches. Therefore, step zero is task definition, since the offline evaluation is designed around the task, not vice versa. The actual first design step is deciding on the evaluation methodology and metrics. Selecting options that are inappropriate for the task undermines the whole evaluation. The two main methodologies differ in how they model user preference. Under behavior prediction , user preferences are described by both interactions with the recommender and organic events. Model performance is measured by how well it can predict user behavior. The research community has been utilizing this setup since the early days of implicit feedback based recommenders (e.g. [15]). This methodology suffers from the lack of negative feedback. It is also imperfect since flawless behavior prediction would recommend the same items that the user would have interacted with on their own. However, it is still a satisfactory setup because the behavior modeling capability of an algorithm loosely correlates with online performance. Interaction prediction only considers the interactions with the recommender. Positive/negative feedback is generated when a user does/doesn't interact with a recommended item. The traditional use-case of this methodology has been CTR prediction [9]. The main challenge is that reliance on bandit feedback results in a closed feedback loop, making it hard to estimate the uplift of a new algorithm through off-policy evaluation. Counterfactual evaluation [30] aims at alleviating the problem, but still does not scale up to the large state-action space of recommenders. The two main options for metrics are ranking/IR (e.g. recall@N, MRR@N, NDCG@N, etc.) and classification metrics (e.g. AUC, accuracy, etc.). Behavior prediction often uses the former and interaction prediction the latter, but this is not a necessity. Auxiliary metrics (e.g. diversity, novelty, serendipity) can be used along with accuracy metrics, and recent research shows that serendipity can have a significant impact on online performance [4]. Common flaws of subsequent design steps are discussed in detail. These flaws are generic, but for demonstration purposes, we chose the next item prediction of session-based recommenders [13] as the task, behavior prediction as the appropriate methodology, and recall@N and MRR@N as offline metrics. Most of our experiments utilize the official implementation 1 of GRU4Rec[12, 13], because the speed-optimized code allows for quick experimentation on both small and large datasets. The code of our experiments, hyperparameters and additional results are publicly available 2 .",
  "2.1 Dataset-task mismatch": "The next step is choosing datasets. For the sake of reproducibility, public datasets should be used along with or instead of proprietary ones. This is where a common flaw can happen. Not every dataset is appropriate for every task or evaluation methodology. Data might be transformed to somewhat accommodate a task, e.g. rating data was often used as implicit feedback (e.g. [14, 25]), when public implicit feedback datasets were not available. However, the validity of this transformation can be questionable. If all ratings are treated as implicit feedback, negative feedback is knowingly treated as positive; on the other hand, if only high ratings are treated as implicit feedback, it becomes artificially less noisy than feedback in real-life datasets. Nowadays more and more datasets [19, 28, 36] are made available for a wide array of recommendation tasks. However, many researchers still use the same datasets regardless of the task. Sequential recommendation - i.e. when the collections of events (e.g. user histories, sessions) are treated as sequences - only makes sense if the data has sequential patterns. E.g. sessions of similar topics happening within a few days often 1 https://github.com/hidasib/GRU4Rec 2 https://github.com/hidasib/recsys_eval_flaws 2 Widespread Flaws in Offline Evaluation of Recommender Systems RecSys '23, September 18-22, 2023, Singapore, Singapore Table 1. Basic statistics of train/test splits and event collision rate of the datasets have similar patterns because the service with which the users interact sets them on similar paths. [24] summarized that the most commonly (e.g. [16, 19, 31, 39]) used datasets for evaluating sequential recommenders are rating datasets 3 (MovieLens, Steam, Yelp, Amazon (Beauty)) wherein user rating histories are treated as sequences. However, the presence of sequential patterns is questionable, because the time of rating is disjoint from the time of interacting with the item. E.g. the user might skip rating items they have no strong opinion on. The aforementioned datasets are compared to three session datasets 4 - Rees46, Coveo [28] and RetailRocket - to investigate the presence of sequential patterns. Data preprocessing is as follows (see Table 1 for basic statistics): (1) Session datasets might contain multiple event types. If so, only the one corresponding to item views is kept. (2) Sequences are corresponding to user histories in rating datasets, precomputed sessions in Coveo, and sessions with one hour session gap in Rees46 and RetailRocket. (3) Subsequent repeating items are removed from the sequences, e.g. ( ùëñ, ùëñ, ùëó ) ‚Üí ( ùëñ, ùëó ) , but ( ùëñ, ùëó, ùëñ ) is unchanged. These are not informative for recommenders because recommending the same item to the user is not useful. (4) The dataset is iteratively filtered for sequences shorter than 2 and items occurring less than 5 times until there is no change to the dataset. Sequences consisting of one item are useless for this task. The weak item support filter is applied because of the collaborative filtering nature of the algorithms. (5) Train/test splits are time based. Split time is set so that the size of the test data is satisfactory, but it is at least one day before the last event of the dataset. The test set consists of sequences that started after the split time. The train set consists of events that happened before the split time, i.e. sequences extending over are cut off. Analyzing the data reveals that the resolution of timestamps is one day for the Amazon and Steam datasets, which might result in event collision, i.e. two or more events of the same user having the same timestamp. The order of the events of an event collision can not be determined, and thus their sequence is unreliable and might be changed unintentionally during training or testing. The two rightmost columns in Table 1 show the proportion of collisions to all user-timestamp pairs and the proportion of participating events to all events. Beside Amazon and Steam, MovieLens10M also has many event collisions, which is extremely problematic, because this dataset comes presorted by user and item ID by default. This means that if user ùê¥ originally had a rating sequence on a set of items (e.g. ùëò ‚Üí ùëñ ‚Üí ùëó ), while user ùêµ rated the same items in a different order (e.g. ùëó ‚Üí ùëò ‚Üí ùëñ ), then timestamp collision and the default sorting by item ID together produces the same sequence ( ùëñ ‚Üí ùëó ‚Üí ùëò ) for both. This is not the original sequence of user ùê¥ or ùêµ but introduces two 3 ML10M: https://grouplens.org/datasets/movielens/10m/; Steam: https://cseweb.ucsd.edu/~jmcauley/datasets.html#steam_data; Yelp: https://www.yelp. com/dataset; Amazon: http://jmcauley.ucsd.edu/data/amazon/links.html 4 Rees46: https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store; Coveo: https://github.com/coveooss/ shopper-intent-prediction-nature-2020; Retailrocket: https://www.kaggle.com/datasets/retailrocket/ecommerce-dataset 3 RecSys '23, September 18-22, 2023, Singapore, Singapore Hidasi and Czapp Table 2. Recommendation accuracy using the same model with and without sequence modelling instances of an artificial sequence that did not even occur. On a larger scale, this phenomenon introduces artificial sequential patterns even if the data was not sequential originally. Table 2 shows recommendation accuracy of a sequential recommender (GRU4Rec) and the same algorithm with the sequence modeling part (GRU) replaced with a feedforward layer. Hyperparameters are optimized separately for the two algorithms and seven datasets, since replacing the GRU layer might alter the optimal parameters. A validation set - created from the full training set using the same process as the train/test split - is used for optimization. Then the algorithm is retrained on the full training set using the optimal parameters, and performance is measured on the test set. Results show that sequence modeling is not important for the Amazon and Yelp datasets 5 , indicating the lack of sequential patterns and that they are unfit for evaluating sequential algorithms. Sequence modeling has a small positive impact on Steam, despite the daily resolution of the timestamp due to the specific user behavior of its domain. The rest of the datasets greatly benefit from session modeling, suggesting the presence of sequential patterns. However, in the case of MovieLens10M, these are artificial patters resulting from event collisions and presorting.",
  "2.2 Overzealous preprocessing": "Real-life data is often noisy due to data collection errors, unusual user behavior, bot traffic, etc. Data preprocessing can partially eliminate this noise, enabling better modeling. E.g. step (3) of the preprocessing described in section 2.1 and step (2) for Rees46. Preprocessing can also be used to adapt the dataset to the selected task and evaluation setup. E.g. step (1) and (4) of our preprocessing. However, it is important to consider if preprocessing affects (a) how the outcome of the experiment can be interpreted; (b) whether the results are directly comparable with earlier work; (c) and if the experiment is still suitable to support the articulated claims. Modifying only the training set usually does not hurt the generality of claims about model performance, but direct comparison with earlier work and interpretation of the results might become non-trivial, since changes might affect algorithms differently. E.g. shrinking the training window and using more recent data might benefit recommenders [32] until the uplift from reduced concept drift is balanced out by the degradation from training on less data. Offline evaluation is already biased towards memorization type algorithms, e.g. neighbor methods. This bias is stronger on smaller training windows due to more static user behavior and because generalization requires more data. Thus, reduced training window is likely to have more severe effect on more complex algorithms with better generalization capabilities. Figure 1a demonstrates this by measuring the performance of the model-based GRU4Rec and the neighbor-based V-SkNN [22] algorithms using training windows of varying sizes 6 . While the performance of both models decrease as the training window shortens, the model-based method is affected more severely: GRU4Rec outperforms V-SkNN by 5 Small differences in offline metrics don't translate to any change in online performance due to the proxy nature of the offline setup. 6 Hyperparameters are optimized separately for each training window size and algorithm. 4 Widespread Flaws in Offline Evaluation of Recommender Systems RecSys '23, September 18-22, 2023, Singapore, Singapore 5 . 7% (8 . 9%) in recall@5 (recall@20) when both are trained on the full training set, but performs 29 . 6% (25 . 1%) worse when trained on the last 14 days. Unfortunately, connections between properties of training sets and their effect on the correlation between offline and online performance is not known, thus there is no 'right way' to set the window size. However, biases like this should be considered when designing an evaluation setup. Modifying the test set creates an entirely new evaluation setup, and heavy changes might result in less general performance claims. E.g. evaluating on test users with more than 200 events is informative on the performance on established users only. Real-life recommenders should work for every user, but they can contain multiple algorithms. Therefore, evaluating on certain subsets is not a flaw, if the validity of claims is clearly communicated. Making only the most necessary preprocessing steps is advisable, because the stronger the filtering, the less general the claims can be. Unfortunately, this advice is often ignored [10, 11, 16, 33, 37].",
  "2.3 Information leaking through time": "131 91 56 28 14 7 Size of training set (days) 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 Recall GRU4Rec: Recall@20 V-SkNN:    Recall@20 GRU4Rec: Recall@5 V-SkNN:    Recall@5 0 20 40 60 80 100 120 140 Days 40 50 60 70 80 90 100 Rees46 Coveo Retailrocket New rules (%) (a) The effect of using only recent data on the recommendation accuracy of model and neighbor based methods (b) Proportion of ùëñ ‚Üí ùëó item transitions observed first on day ùëÅ to the number of unique sequences of the same day Fig. 1 The next step in the design process is the train/test split. Proper evaluation requires eliminating access to any information that would not be available during inference. E.g. training examples must not be used for evaluation. Recommenders should also not have access to future information during evaluation. However, some commonly used splitting strategies (e.g. random split, leave-one-out) inherently enable information leaking through time. Random splits are appropriate if the preference of users is considered to be long-term and mostly static. This is not the case due to changes in the item catalog, shifting user interest and external factors (e.g. promotions). Figure 1b shows the proportion of ùëñ ‚Üí ùëó item transitions observed first on day ùëÅ and the number of unique sequences on the same day. While it declines, it stabilizes on fairly high values, indicating constantly changing user behavior. This concept-drift has been studied by the research community [8, 34]. Overlapping train/test splits lessen the need of modeling the concept drift, and provide an easier, unrealistic problem for which less generalization is needed. Therefore, its effect is not uniform over different types of algorithms. Leave-one-out splitting is often used in the evaluation of sequential recommenders [16, 19, 21, 31, 39]. The last event of the training sequences are moved from the train set to the test set. Sequences end at different times, thus the two sets overlap in time. Figure 2 shows the concept drift between train and test sets as the proportion of the ùëñ ‚Üí ùëó test item transitions that are shared with the train set 7 for leave-one-out and time based splits (2a); and two variants of 7 Note that the exact value also depends on the proportion of the training and test sets, therefore the size of the test sets was matched for both comparisons. 5 RecSys '23, September 18-22, 2023, Singapore, Singapore Hidasi and Czapp Fig. 2. Proportion of the ùëñ ‚Üí ùëó test item transitions that are shared with the training set Rees46 Coveo Retailrocket 0.0 0.1 0.2 0.3 0.4 0.5 Ratio 0.5254 0.3701 0.3644 0.4222 0.3812 0.2428 Leave-one-out Time based (a) Leave-one-out and time based split Rees46 Coveo Retailrocket 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Ratio 0.7290 0.4442 0.4868 0.6756 0.4500 0.3969 Leave-one-out: random Leave-one-out: time split (b) Leave-one-out on random vs. most recent sessions the leave-one-out strategy (2b) using the most recent and random sequences. Concept drift is significantly smaller for non-overlapping splits of Rees46 and RetailRocket. Coveo has similar proportions for both splits due to consisting of only 18 days of data, which is too short for concept drift to be too prevalent. This clearly indicates that by not requiring strict separation by time, the evaluation setup becomes compromised by information leaking through time, and thus the algorithms are evaluated on a somewhat easier problem than what they would face in a production system.",
  "2.4 Negative sampling during testing": "Negative item sampling is a common and severe flaw of the final step, testing. Recently, [20] discussed how sampling introduces bias to measurements and its potential impact on the comparison of algorithms, while [3, 5] demonstrated that it can change the performance based ordering of models. Thus, results achieved with sampling are not reliable. Besides confirming the result of earlier work, we demonstrate how the bias introduced by negative sampling can change the ordering of models, demonstrate its impact, as well as discuss alternatives and whether sampling is needed at all. This flaw is rooted in the transition from error metrics to IR metrics from a decade ago that brought along a significant increase in the amount of compute needed for evaluation. Opposed to rating prediction that needs only a few score computations per test case, ranking requires scoring all items for every test case, since test items are treated as relevant ones that need to be ranked against other irrelevant items. Different alternatives [1] were proposed including ranking over all items and ranking the target(s) against a number of sampled negative items. Ranking over all items is how real-life recommenders work, but nevertheless, negative sampling also became a widespread practice, utilized in well cited papers of prestigious conferences (e.g. [2, 10, 16, 19, 26, 31, 39]). Metrics focus on the top few items, thus negative items should be highly ranked for sampled and full ranking results to be similar. Uniform sampling is not able to provide strong samples if the size of the item catalog is significantly larger than the number of samples. The probability of a target item ranked ùëÖ in the full ranking of ùëÅ items making it into the recommendation list of ùê∂ items ( ùê∂ < ùëÖ ) when ranked against ùëÜ negative samples is Àù ùê∂ -1 ùëñ = 0 GLYPH<0> ùëÖ -1 ùëñ GLYPH<1> GLYPH<0> ùëÅ -ùëÖ ùëÜ -ùëñ GLYPH<1> / GLYPH<0> ùëÅ -1 ùëÜ GLYPH<1> . If the target item is ranked 1 , 490 th among 10 , 000 items or 14 , 878 th among 100 , 000, it still has more than 90% chance to make it into the top 20 with 100 uniform samples. Weak negative samples are easily distinguishable from the target and thus their use results in the severe overestimation of offline accuracy metrics. Using stronger samples provides a more accurate estimation of the accuracy of the non sampled setup. The strength of negative samples - i.e. how hard it is to distinguish them from the target - is depicted on Figure 3 for 8 sampling strategies: 6 Widespread Flaws in Offline Evaluation of Recommender Systems RecSys '23, September 18-22, 2023, Singapore, Singapore Fig. 3. Comparison of the strength of various negative samples of 100 items and no sampling. Recall@1 Recall@5 Recall@10 Recall@20 Rees46 0.0 0.2 0.4 0.6 0.8 1.0 Recall@1 Recall@5 Recall@10 Recall@20 Coveo 0.0 0.2 0.4 0.6 0.8 All items Uniform sampling Most similar Inverse popularity sampling Closest Farthest Most popular Least similar Popularity sampling Recall@1 Recall@5 Recall@10 Recall@20 Retailrocket 0.0 0.2 0.4 0.6 0.8 ¬∑ Sampling strategies: Sampling probability is either uniform or proportional to the items' support. Popular items are considered to be stronger samples, since recommenders tend to rank them higher. ¬∑ Most popular items: The target items are ranked against the 100 most popular items. ¬∑ Similar/close items: Soft upper bound. Requires at least as much compute as full ranking. Items are selected based on the (cosine) similarity or closeness of their embeddings to that of the target item. ¬∑ Weak baselines: Soft lower bound. Sampling with probability proportional to reciprocal item support, and items with least similar or farthest embeddings to the target item's embedding. Figure 3 indicates that even the computationally expensive most similar and closest approaches can not produce strong item sets consistently. Popularity sampling and most popular items are better than uniform sampling, but these approaches are still not able to estimate the true rank of the target. This can falsify the results, because (1) the difference between the performance of two algorithms can vanish due to the lack of challenging negative items. (2) The relative performance at recommendation list length ùëÄ shifts to length ùëÅ (‚â™ ùëÄ ) because sampling makes it easier to push the target item up on the list. Since the relative performance of two models might depend on the length of the recommendation list, this can also change their ordering for relevant list lengths. Figure 4 demonstrates this point by showing recall@N as the function of ùëÅ for pairs of models for full ranking (100%), and for randomly sampling 10%, 1%, 0 . 1% of the item catalog or 100 items. The ordering of models is not static over ùëÅ , and it might change multiple times (marked by red dots). Decreasing sample size moves these intersections to the left, pushing the change in model ordering to lower ùëÅ values. Changes to the relative performance are drastic: model A on Coveo (4a) outperforms model B by 8 . 1% in recall@20, but A underperforms B by 4 . 7% when 100 negative samples are used. While the ordering of model A and B does not change for Retailrocket (4b), 14 . 3% uplift in recall@20 vanishes and drops to 1 . 5%. The biggest change can be observed between model A and B on Rees46 (4c) where the uplift in recall@1 (and MRR@1) drops from 49 . 5% to -1 . 6%. Since MRR is even more sensitive to the top of the list, this affects relative MRR based performance for any ùëÅ : the true order of models A, B and C on Rees46 (4f) is ùê¥ > ùê∂ ‚â´ ùêµ , but with 100 samples it reads as ùêµ > ùê¥ ‚â´ ùê∂ . Computational complexity of evaluation via full ranking depends on the number of test recommendations and the size of the item catalog. Most public dataset are quite small, and even the bigger ones have item catalogs of easily manageable sizes. One test on Rees46 requires 887 , 369 recommendation lists over 172 , 756 items each that is executed 7 RecSys '23, September 18-22, 2023, Singapore, Singapore Hidasi and Czapp Fig. 4. Accuracy as the function of recommendation list length, with and without sampling 10 0 10 1 10 2 10 3 10 4 N 0.2 0.4 0.6 0.8 1.0 Recall model A (100%) model B (100%) model A (10%) model B (10%) model A (#100) model B (#100) model A (0.1%) model B (0.1%) (a) Coveo - Recall@N 10 0 10 1 10 2 10 3 10 4 N 0.2 0.4 0.6 0.8 1.0 Recall model A (100%) model B (100%) model A (10%) model B (10%) model A (1%) model B (1%) model A (#100) model B (#100) model A (0.1%) model B (0.1%) (b) Retailrocket - Recall@N 10 0 10 1 10 2 10 3 10 4 10 5 N 0.2 0.4 0.6 0.8 1.0 Recall model A (100%) model B (100%) model A (10%) model B (10%) model A (1%) model B (1%) model A (#100) model B (#100) (c) Rees46 - Recall@N - AB 10 0 10 1 10 2 10 3 10 4 10 5 N 0.2 0.4 0.6 0.8 1.0 Recall model C (100%) model B (100%) model C (10%) model B (10%) model C (1%) model B (1%) model C (#100) model B (#100) (d) Rees46 - Recall@N - BC 10 0 10 1 10 2 10 3 10 4 10 5 N 0.2 0.4 0.6 0.8 1.0 Recall model A (100%) model B (100%) model C (100%) model A (10%) model B (10%) model C (10%) model A (1%) model B (1%) model C (1%) model A (#100) model B (#100) model C (#100) (e) Rees46 - Recall@N - ALL 10 0 10 1 10 2 10 3 10 4 10 5 N 0.2 0.4 0.6 0.8 MRR model A (100%) model B (100%) model C (100%) model A (10%) model B (10%) model C (10%) model A (1%) model B (1%) model C (1%) model A (#100) model B (#100) model C (#100) (f) Rees46 - MRR@N - ALL in only 205 . 1 seconds on an A30 GPU. Even before GPUs, evaluation on public datasets of the time could be executed in a few minutes using optimized code. If a dataset is really too large for quick experimentation, the sampling of test recommendations (e.g. users) gives more representative results. Certain scoring models are not designed for full ranking. If one such model must be evaluated on ranking, generating candidate sets via another algorithm can solve the problem. However, this limits the generality of claims about its performance, because currently there are no universally accepted candidate set generators.",
  "3 RELATED WORK": "Offline evaluation of top-N recommenders has been discussed since top-N recommendation became dominant. Early works mainly focused on metrics [17, 35] and finding connections between offline and online results [29]. The pace of the work has increased in recent years [38], simultaneously with the sharp increase of papers lacking satisfactory evaluation. Evaluation has many aspects from metrics [17, 35] to hyperparameter optimization [27]. [6, 7] suggests that one of the reasons behind the low reproducibility rate of recent papers is that evaluation setups are lifted from earlier work without their validity being checked. 8 Widespread Flaws in Offline Evaluation of Recommender Systems RecSys '23, September 18-22, 2023, Singapore, Singapore Our work focuses on this problem, i.e. the design of evaluation setups and the flaws within. Of the four flaws discussed in this paper, only negative item sampling has been scrutinized, aside from [33] briefly mentioning that the Amazon dataset has weak sequential signals. [20] discussed the background of how sampling introduces bias to measurements, and [3, 5] demonstrated that the relative performance of models can change when sampling is applied. Our experiments confirm their results and give additional explanation on why this change happens.",
  "4 DISCUSSION & CONCLUSION": "Offline evaluation is fundamentally imperfect, but it will likely remain the main approach for assessing performance in recommendation systems research. Online A/B tests are not just expensive and slow, but inherently not reproducible; and simulators are still in their early stages. Unfortunately, flaws are quite widespread in offline evaluation setups. In this paper, we went through the main steps of designing evaluation setups and pointed out four widespread flaws and demonstrated their effect through the example of sequential recommendations. We chose sequential recommendation for the demonstration, because it is one of the areas severely plagued by low quality evaluation. Evaluation flaws of some of the earlier sequential recommender papers - e.g. [19] has all four we discussed - were missed by both the authors and reviewers. Later works copied the setups - with their flaws included - without questioning whether they are correct. There is not a single best way for offline evaluation, but by pointing out these four flaws, we hope to see the quality of experimentation sections of research papers improving in the future.",
  "ACKNOWLEDGMENTS": "The work leading to these results received funding from National Research, Development and Innovation Office, Hungary under grant agreement number 2020-1.1.2-PIACI-KFI-2021-00289. The authors would also like to thank Domonkos Tikk for his valuable support as the project leader of the abovementioned R&D grant.",
  "REFERENCES": "[1] Alejandro Bellogin, Pablo Castells, and Ivan Cantador. 2011. Precision-oriented evaluation of recommender systems: an algorithmic comparison. In Proceedings of the fifth ACM conference on Recommender systems . 333-336. [2] Wei Cai, Weike Pan, Jingwen Mao, Zhechao Yu, and Congfu Xu. 2022. Aspect Re-distribution for Learning Better Item Embeddings in Sequential Recommendation. In Proceedings of the 16th ACM Conference on Recommender Systems . 49-58. [3] Roc√≠o Ca√±amares and Pablo Castells. 2020. On target item sampling in offline recommender system evaluation. In Proceedings of the 14th ACM Conference on Recommender Systems . 259-268. [4] Minmin Chen, Yuyan Wang, Can Xu, Ya Le, Mohit Sharma, Lee Richardson, Su-Lin Wu, and Ed Chi. 2021. Values of User Exploration in Recommender Systems. In Proceedings of the 15th ACM Conference on Recommender Systems (Amsterdam, Netherlands) (RecSys '21) . Association for Computing Machinery, New York, NY, USA, 85-95. https://doi.org/10.1145/3460231.3474236 [5] Alexander Dallmann, Daniel Zoller, and Andreas Hotho. 2021. A case study on sampling strategies for evaluating neural sequential item recommendation models. In Proceedings of the 15th ACM Conference on Recommender Systems . 505-514. [6] Maurizio Ferrari Dacrema, Paolo Cremonesi, and Dietmar Jannach. 2019. Are we really making much progress? A worrying analysis of recent neural recommendation approaches. In Proceedings of the 13th ACM conference on recommender systems . 101-109. [7] Maurizio Ferrari Dacrema, Paolo Cremonesi, Dietmar Jannach, et al. 2020. Methodological issues in recommender systems research. In IJCAI , Vol. 2021. International Joint Conferences on Artificial Intelligence, 4706-4710. [8] Jo√£o Gama, IndrÀô e ≈ΩliobaitÀô e, Albert Bifet, Mykola Pechenizkiy, and Abdelhamid Bouchachia. 2014. A survey on concept drift adaptation. ACM computing surveys (CSUR) 46, 4 (2014), 1-37. [9] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. arXiv preprint arXiv:1703.04247 (2017). [10] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural collaborative filtering. In Proceedings of the 26th international conference on world wide web . 173-182. 9 RecSys '23, September 18-22, 2023, Singapore, Singapore Hidasi and Czapp [11] Xiangnan He, Hanwang Zhang, Min-Yen Kan, and Tat-Seng Chua. 2016. Fast matrix factorization for online recommendation with implicit feedback. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval . 549-558. [12] Bal√°zs Hidasi and Alexandros Karatzoglou. 2018. Recurrent neural networks with top-k gains for session-based recommendations. In Proceedings of the 27th ACM international conference on information and knowledge management . 843-852. Bal√°zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2015. Session-based recommendations with recurrent neural networks. [13] arXiv preprint arXiv:1511.06939 (2015). [14] Bal√°zs Hidasi and Domonkos Tikk. 2012. Fast ALS-based tensor factorization for context-aware recommendation from implicit feedback. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2012, Bristol, UK, September 24-28, 2012. Proceedings, Part II 23 . Springer, 67-82. [15] Yifan Hu, Yehuda Koren, and Chris Volinsky. 2008. Collaborative filtering for implicit feedback datasets. In 2008 Eighth IEEE international conference on data mining . Ieee, 263-272. [16] Jin Huang, Wayne Xin Zhao, Hongjian Dou, Ji-Rong Wen, and Edward Y Chang. 2018. Improving sequential recommendation with knowledgeenhanced memory networks. In The 41st international ACM SIGIR conference on research & development in information retrieval . 505-514. [17] Neil Hurley and Mi Zhang. 2011. Novelty and diversity in top-n recommendation-analysis and evaluation. ACM Transactions on Internet Technology (TOIT) 10, 4 (2011), 1-30. [18] Olivier Jeunen. 2023. A Common Misassumption in Online Experiments with Machine Learning Models. arXiv preprint arXiv:2304.10900 (2023). [19] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recommendation. In 2018 IEEE international conference on data mining (ICDM) . IEEE, 197-206. [20] Walid Krichene and Steffen Rendle. 2020. On sampled metrics for item recommendation. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining . 1748-1757. [21] Jiacheng Li, Yujie Wang, and Julian McAuley. 2020. Time interval aware self-attention for sequential recommendation. In Proceedings of the 13th international conference on web search and data mining . 322-330. [22] Malte Ludewig and Dietmar Jannach. 2018. Evaluation of session-based recommendation algorithms. User Modeling and User-Adapted Interaction 28 (2018), 331-390. [23] James McInerney, Ehtsham Elahi, Justin Basilico, Yves Raimond, and Tony Jebara. 2021. Accordion: a trainable simulator for long-term interactive systems. In Proceedings of the 15th ACM Conference on Recommender Systems . 102-113. [24] Aleksandr Petrov and Craig Macdonald. 2022. A Systematic Review and Replicability Study of BERT4Rec for Sequential Recommendation. In Proceedings of the 16th ACM Conference on Recommender Systems . 436-447. [25] Istv√°n Pil√°szy, D√°vid Zibriczky, and Domonkos Tikk. 2010. Fast als-based matrix factorization for explicit and implicit feedback datasets. In Proceedings of the fourth ACM conference on Recommender systems . 71-78. [26] Ahmed Rashed, Shereen Elsayed, and Lars Schmidt-Thieme. 2022. Context and Attribute-Aware Sequential Recommendation via Cross-Attention. In Proceedings of the 16th ACM Conference on Recommender Systems . 71-80. [27] Steffen Rendle, Walid Krichene, Li Zhang, and Yehuda Koren. 2022. Revisiting the performance of ials on item recommendation benchmarks. In Proceedings of the 16th ACM Conference on Recommender Systems . 427-435. [28] Borja Requena, Giovanni Cassani, Jacopo Tagliabue, Ciro Greco, and Lucas Lacasa. 2020. Shopper intent prediction from clickstream e-commerce data with minimal browsing information. Scientific reports 10, 1 (2020), 1-23. [29] Marco Rossetti, Fabio Stella, and Markus Zanker. 2016. Contrasting Offline and Online Results When Evaluating Recommendation Algorithms. In Proceedings of the 10th ACM Conference on Recommender Systems (Boston, Massachusetts, USA) (RecSys '16) . Association for Computing Machinery, New York, NY, USA, 31-34. https://doi.org/10.1145/2959100.2959176 [30] Yuta Saito and Thorsten Joachims. 2021. Counterfactual learning and evaluation for recommender systems: Foundations, implementations, and recent advances. In Proceedings of the 15th ACM Conference on Recommender Systems . 828-830. [31] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer. In Proceedings of the 28th ACM international conference on information and knowledge management . 1441-1450. [32] Yong Kiam Tan, Xinxing Xu, and Yong Liu. 2016. Improved recurrent neural networks for session-based recommendations. In Proceedings of the 1st workshop on deep learning for recommender systems . 17-22. [33] Jiaxi Tang and Ke Wang. 2018. Personalized top-n sequential recommendation via convolutional sequence embedding. In Proceedings of the eleventh ACM international conference on web search and data mining . 565-573. [34] Alexey Tsymbal. 2004. The problem of concept drift: definitions and related work. Computer Science Department, Trinity College Dublin 106, 2 (2004), 58. [35] Sa√∫l Vargas and Pablo Castells. 2011. Rank and relevance in novelty and diversity metrics for recommender systems. In Proceedings of the fifth ACM conference on Recommender systems . 109-116. [36] Mengting Wan and Julian McAuley. 2018. Item recommendation on monotonic behavior chains. In Proceedings of the 12th ACM conference on recommender systems . 86-94. [37] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019. Neural graph collaborative filtering. In Proceedings of the 42nd international ACM SIGIR conference on Research and development in Information Retrieval . 165-174. 10 Widespread Flaws in Offline Evaluation of Recommender Systems RecSys '23, September 18-22, 2023, Singapore, Singapore [38] Eva Zangerle, Christine Bauer, and Alan Said. 2022. Second Workshop: Perspectives on the Evaluation of Recommender Systems (PERSPECTIVES 2022). In Proceedings of the 16th ACM Conference on Recommender Systems (Seattle, WA, USA) (RecSys '22) . Association for Computing Machinery, New York, NY, USA, 652-653. https://doi.org/10.1145/3523227.3547408 [39] Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, and Ji-Rong Wen. 2020. S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization. In Proceedings of the 29th ACM international conference on information & knowledge management . 1893-1902. 11",
  "keywords_parsed": [
    "recommender systems",
    "offline evaluation",
    "evaluation setups",
    "evaluation flaws"
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "Precision-oriented evaluation of recommender systems: an algorithmic comparison"
    },
    {
      "ref_id": "b2",
      "title": "Aspect Re-distribution for Learning Better Item Embeddings in Sequential Recommendation"
    },
    {
      "ref_id": "b3",
      "title": "On target item sampling in offline recommender system evaluation"
    },
    {
      "ref_id": "b4",
      "title": "Values of User Exploration in Recommender Systems"
    },
    {
      "ref_id": "b5",
      "title": "A case study on sampling strategies for evaluating neural sequential item recommendation models"
    },
    {
      "ref_id": "b6",
      "title": "Are we really making much progress? A worrying analysis of recent neural recommendation approaches"
    },
    {
      "ref_id": "b7",
      "title": "Methodological issues in recommender systems research"
    },
    {
      "ref_id": "b8",
      "title": "A survey on concept drift adaptation"
    },
    {
      "ref_id": "b9",
      "title": "DeepFM: a factorization-machine based neural network for CTR prediction"
    },
    {
      "ref_id": "b10",
      "title": "Neural collaborative filtering"
    },
    {
      "ref_id": "b11",
      "title": "Fast matrix factorization for online recommendation with implicit feedback"
    },
    {
      "ref_id": "b12",
      "title": "Recurrent neural networks with top-k gains for session-based recommendations"
    },
    {
      "ref_id": "b13",
      "title": "Session-based recommendations with recurrent neural networks"
    },
    {
      "ref_id": "b14",
      "title": "Fast ALS-based tensor factorization for context-aware recommendation from implicit feedback"
    },
    {
      "ref_id": "b15",
      "title": "Collaborative filtering for implicit feedback datasets"
    },
    {
      "ref_id": "b16",
      "title": "Improving sequential recommendation with knowledge-enhanced memory networks"
    },
    {
      "ref_id": "b17",
      "title": "Novelty and diversity in top-n recommendation-analysis and evaluation"
    },
    {
      "ref_id": "b18",
      "title": "A Common Misassumption in Online Experiments with Machine Learning Models"
    },
    {
      "ref_id": "b19",
      "title": "Self-attentive sequential recommendation"
    },
    {
      "ref_id": "b20",
      "title": "On sampled metrics for item recommendation"
    },
    {
      "ref_id": "b21",
      "title": "Time interval aware self-attention for sequential recommendation"
    },
    {
      "ref_id": "b22",
      "title": "Evaluation of session-based recommendation algorithms"
    },
    {
      "ref_id": "b23",
      "title": "Accordion: a trainable simulator for long-term interactive systems"
    },
    {
      "ref_id": "b24",
      "title": "A Systematic Review and Replicability Study of BERT4Rec for Sequential Recommendation"
    },
    {
      "ref_id": "b25",
      "title": "Fast als-based matrix factorization for explicit and implicit feedback datasets"
    },
    {
      "ref_id": "b26",
      "title": "Context and Attribute-Aware Sequential Recommendation via Cross-Attention"
    },
    {
      "ref_id": "b27",
      "title": "Revisiting the performance of ials on item recommendation benchmarks"
    },
    {
      "ref_id": "b28",
      "title": "Shopper intent prediction from clickstream e-commerce data with minimal browsing information"
    },
    {
      "ref_id": "b29",
      "title": "Contrasting Offline and Online Results When Evaluating Recommendation Algorithms"
    },
    {
      "ref_id": "b30",
      "title": "Counterfactual learning and evaluation for recommender systems: Foundations, implementations, and recent advances"
    },
    {
      "ref_id": "b31",
      "title": "BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer"
    },
    {
      "ref_id": "b32",
      "title": "Improved recurrent neural networks for session-based recommendations"
    },
    {
      "ref_id": "b33",
      "title": "Personalized top-n sequential recommendation via convolutional sequence embedding"
    },
    {
      "ref_id": "b34",
      "title": "The problem of concept drift: definitions and related work"
    },
    {
      "ref_id": "b35",
      "title": "Rank and relevance in novelty and diversity metrics for recommender systems"
    },
    {
      "ref_id": "b36",
      "title": "Item recommendation on monotonic behavior chains"
    },
    {
      "ref_id": "b37",
      "title": "Neural graph collaborative filtering"
    },
    {
      "ref_id": "b38",
      "title": "Second Workshop: Perspectives on the Evaluation of Recommender Systems (PERSPECTIVES 2022)"
    },
    {
      "ref_id": "b39",
      "title": "S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization"
    }
  ]
}