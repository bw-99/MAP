{"Widespread Flaws in Offline Evaluation of Recommender Systems": "", "BAL\u00c1ZS HIDASI and \u00c1D\u00c1M TIBOR CZAPP, Gravity R&D, a Taboola company, Hungary": "Even though offline evaluation is just an imperfect proxy of online performance - due to the interactive nature of recommenders - it will probably remain the primary way of evaluation in recommender systems research for the foreseeable future, since the proprietary nature of production recommenders prevents independent validation of A/B test setups and verification of online results. Therefore, it is imperative that offline evaluation setups are as realistic and as flawless as they can be. Unfortunately, evaluation flaws are quite common in recommender systems research nowadays, due to later works copying flawed evaluation setups from their predecessors without questioning their validity. In the hope of improving the quality of offline evaluation of recommender systems, we discuss four of these widespread flaws and why researchers should avoid them.", "CCS Concepts: \u00b7 Information systems \u2192 Recommender systems .": "Additional Key Words and Phrases: recommender systems, offline evaluation, evaluation setups, evaluation flaws", "ACMReference Format:": "Bal\u00e1zs Hidasi and \u00c1d\u00e1m Tibor Czapp. 2023. Widespread Flaws in Offline Evaluation of Recommender Systems. In Seventeenth ACM Conference on Recommender Systems (RecSys '23), September 18-22, 2023, Singapore, Singapore. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3604915.3608839", "1 INTRODUCTION": "Evaluation is one of the biggest challenges of recommender systems research. Online A/B testing is already an approximation of the recommender's true goal, limited by KPIs that can be measured. Splitting traffic fairly, preventing information leak between groups and eliminating biases [18] are also non-trivial. From the business perspective, A/B tests are expensive and slow because a portion of the traffic is served by suboptimal models, and getting statistically significant differences for informative but noisy KPIs can take weeks/months. From the scientific perspective, A/B tests are not reproducible due to the proprietary nature of production recommenders preventing independent validation of A/B test setups and verification of online results. Therefore, the inherently imperfect approximation of online performance through offline evaluation is likely to stay the main way of assessing performance in research, at least until simulators [23] become more mature. Thus, it is imperative that offline evaluation is correct and follows how production recommenders work as closely as possible. Unfortunately, flaws are quite common, due to later works copying flawed evaluation setups from their predecessors without questioning their validity [7]. In this paper we discuss four widespread evaluation flaws we observed en masse in research papers of the last decade. Our main contribution is presented in Section 2, where we go through the most important steps of designing offline evaluation setups and discuss common pitfalls. We closely examine four severe flaws commonly found in the evaluation setup of various recommenders: dataset-task mismatch, general claims on heavily preprocessed data, information leaking through time, and negative item sampling. Their effect is demonstrated on sequential recommenders. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Manuscript submitted to ACM", "2 DESIGN FLAWS IN EVALUATION SETUPS": "In this section we go through the main steps of designing offline evaluation and point out potential pitfalls and flaws. Recommender systems are utilized in various scenarios covering a wide spectrum of domains, use-cases, data and goals, which might require completely different approaches. Therefore, step zero is task definition, since the offline evaluation is designed around the task, not vice versa. The actual first design step is deciding on the evaluation methodology and metrics. Selecting options that are inappropriate for the task undermines the whole evaluation. The two main methodologies differ in how they model user preference. Under behavior prediction , user preferences are described by both interactions with the recommender and organic events. Model performance is measured by how well it can predict user behavior. The research community has been utilizing this setup since the early days of implicit feedback based recommenders (e.g. [15]). This methodology suffers from the lack of negative feedback. It is also imperfect since flawless behavior prediction would recommend the same items that the user would have interacted with on their own. However, it is still a satisfactory setup because the behavior modeling capability of an algorithm loosely correlates with online performance. Interaction prediction only considers the interactions with the recommender. Positive/negative feedback is generated when a user does/doesn't interact with a recommended item. The traditional use-case of this methodology has been CTR prediction [9]. The main challenge is that reliance on bandit feedback results in a closed feedback loop, making it hard to estimate the uplift of a new algorithm through off-policy evaluation. Counterfactual evaluation [30] aims at alleviating the problem, but still does not scale up to the large state-action space of recommenders. The two main options for metrics are ranking/IR (e.g. recall@N, MRR@N, NDCG@N, etc.) and classification metrics (e.g. AUC, accuracy, etc.). Behavior prediction often uses the former and interaction prediction the latter, but this is not a necessity. Auxiliary metrics (e.g. diversity, novelty, serendipity) can be used along with accuracy metrics, and recent research shows that serendipity can have a significant impact on online performance [4]. Common flaws of subsequent design steps are discussed in detail. These flaws are generic, but for demonstration purposes, we chose the next item prediction of session-based recommenders [13] as the task, behavior prediction as the appropriate methodology, and recall@N and MRR@N as offline metrics. Most of our experiments utilize the official implementation 1 of GRU4Rec[12, 13], because the speed-optimized code allows for quick experimentation on both small and large datasets. The code of our experiments, hyperparameters and additional results are publicly available 2 .", "2.1 Dataset-task mismatch": "The next step is choosing datasets. For the sake of reproducibility, public datasets should be used along with or instead of proprietary ones. This is where a common flaw can happen. Not every dataset is appropriate for every task or evaluation methodology. Data might be transformed to somewhat accommodate a task, e.g. rating data was often used as implicit feedback (e.g. [14, 25]), when public implicit feedback datasets were not available. However, the validity of this transformation can be questionable. If all ratings are treated as implicit feedback, negative feedback is knowingly treated as positive; on the other hand, if only high ratings are treated as implicit feedback, it becomes artificially less noisy than feedback in real-life datasets. Nowadays more and more datasets [19, 28, 36] are made available for a wide array of recommendation tasks. However, many researchers still use the same datasets regardless of the task. Sequential recommendation - i.e. when the collections of events (e.g. user histories, sessions) are treated as sequences - only makes sense if the data has sequential patterns. E.g. sessions of similar topics happening within a few days often have similar patterns because the service with which the users interact sets them on similar paths. [24] summarized that the most commonly (e.g. [16, 19, 31, 39]) used datasets for evaluating sequential recommenders are rating datasets 3 (MovieLens, Steam, Yelp, Amazon (Beauty)) wherein user rating histories are treated as sequences. However, the presence of sequential patterns is questionable, because the time of rating is disjoint from the time of interacting with the item. E.g. the user might skip rating items they have no strong opinion on. The aforementioned datasets are compared to three session datasets 4 - Rees46, Coveo [28] and RetailRocket - to investigate the presence of sequential patterns. Data preprocessing is as follows (see Table 1 for basic statistics): (1) Session datasets might contain multiple event types. If so, only the one corresponding to item views is kept. Analyzing the data reveals that the resolution of timestamps is one day for the Amazon and Steam datasets, which might result in event collision, i.e. two or more events of the same user having the same timestamp. The order of the events of an event collision can not be determined, and thus their sequence is unreliable and might be changed unintentionally during training or testing. The two rightmost columns in Table 1 show the proportion of collisions to all user-timestamp pairs and the proportion of participating events to all events. Beside Amazon and Steam, MovieLens10M also has many event collisions, which is extremely problematic, because this dataset comes presorted by user and item ID by default. This means that if user \ud835\udc34 originally had a rating sequence on a set of items (e.g. \ud835\udc58 \u2192 \ud835\udc56 \u2192 \ud835\udc57 ), while user \ud835\udc35 rated the same items in a different order (e.g. \ud835\udc57 \u2192 \ud835\udc58 \u2192 \ud835\udc56 ), then timestamp collision and the default sorting by item ID together produces the same sequence ( \ud835\udc56 \u2192 \ud835\udc57 \u2192 \ud835\udc58 ) for both. This is not the original sequence of user \ud835\udc34 or \ud835\udc35 but introduces two instances of an artificial sequence that did not even occur. On a larger scale, this phenomenon introduces artificial sequential patterns even if the data was not sequential originally. Table 2 shows recommendation accuracy of a sequential recommender (GRU4Rec) and the same algorithm with the sequence modeling part (GRU) replaced with a feedforward layer. Hyperparameters are optimized separately for the two algorithms and seven datasets, since replacing the GRU layer might alter the optimal parameters. A validation set - created from the full training set using the same process as the train/test split - is used for optimization. Then the algorithm is retrained on the full training set using the optimal parameters, and performance is measured on the test set. Results show that sequence modeling is not important for the Amazon and Yelp datasets 5 , indicating the lack of sequential patterns and that they are unfit for evaluating sequential algorithms. Sequence modeling has a small positive impact on Steam, despite the daily resolution of the timestamp due to the specific user behavior of its domain. The rest of the datasets greatly benefit from session modeling, suggesting the presence of sequential patterns. However, in the case of MovieLens10M, these are artificial patters resulting from event collisions and presorting.", "2.2 Overzealous preprocessing": "Real-life data is often noisy due to data collection errors, unusual user behavior, bot traffic, etc. Data preprocessing can partially eliminate this noise, enabling better modeling. E.g. step (3) of the preprocessing described in section 2.1 and step (2) for Rees46. Preprocessing can also be used to adapt the dataset to the selected task and evaluation setup. E.g. step (1) and (4) of our preprocessing. However, it is important to consider if preprocessing affects (a) how the outcome of the experiment can be interpreted; (b) whether the results are directly comparable with earlier work; (c) and if the experiment is still suitable to support the articulated claims. Modifying only the training set usually does not hurt the generality of claims about model performance, but direct comparison with earlier work and interpretation of the results might become non-trivial, since changes might affect algorithms differently. E.g. shrinking the training window and using more recent data might benefit recommenders [32] until the uplift from reduced concept drift is balanced out by the degradation from training on less data. Offline evaluation is already biased towards memorization type algorithms, e.g. neighbor methods. This bias is stronger on smaller training windows due to more static user behavior and because generalization requires more data. Thus, reduced training window is likely to have more severe effect on more complex algorithms with better generalization capabilities. Figure 1a demonstrates this by measuring the performance of the model-based GRU4Rec and the neighbor-based V-SkNN [22] algorithms using training windows of varying sizes 6 . While the performance of both models decrease as the training window shortens, the model-based method is affected more severely: GRU4Rec outperforms V-SkNN by 5 . 7% (8 . 9%) in recall@5 (recall@20) when both are trained on the full training set, but performs 29 . 6% (25 . 1%) worse when trained on the last 14 days. Unfortunately, connections between properties of training sets and their effect on the correlation between offline and online performance is not known, thus there is no 'right way' to set the window size. However, biases like this should be considered when designing an evaluation setup. Modifying the test set creates an entirely new evaluation setup, and heavy changes might result in less general performance claims. E.g. evaluating on test users with more than 200 events is informative on the performance on established users only. Real-life recommenders should work for every user, but they can contain multiple algorithms. Therefore, evaluating on certain subsets is not a flaw, if the validity of claims is clearly communicated. Making only the most necessary preprocessing steps is advisable, because the stronger the filtering, the less general the claims can be. Unfortunately, this advice is often ignored [10, 11, 16, 33, 37].", "2.3 Information leaking through time": "131 91 56 28 14 7 Size of training set (days) 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 Recall GRU4Rec: Recall@20 V-SkNN:    Recall@20 GRU4Rec: Recall@5 V-SkNN:    Recall@5 0 20 40 60 80 100 120 140 Days 40 50 60 70 80 90 100 Rees46 Coveo Retailrocket New rules (%) The next step in the design process is the train/test split. Proper evaluation requires eliminating access to any information that would not be available during inference. E.g. training examples must not be used for evaluation. Recommenders should also not have access to future information during evaluation. However, some commonly used splitting strategies (e.g. random split, leave-one-out) inherently enable information leaking through time. Random splits are appropriate if the preference of users is considered to be long-term and mostly static. This is not the case due to changes in the item catalog, shifting user interest and external factors (e.g. promotions). Figure 1b shows the proportion of \ud835\udc56 \u2192 \ud835\udc57 item transitions observed first on day \ud835\udc41 and the number of unique sequences on the same day. While it declines, it stabilizes on fairly high values, indicating constantly changing user behavior. This concept-drift has been studied by the research community [8, 34]. Overlapping train/test splits lessen the need of modeling the concept drift, and provide an easier, unrealistic problem for which less generalization is needed. Therefore, its effect is not uniform over different types of algorithms. Leave-one-out splitting is often used in the evaluation of sequential recommenders [16, 19, 21, 31, 39]. The last event of the training sequences are moved from the train set to the test set. Sequences end at different times, thus the two sets overlap in time. Figure 2 shows the concept drift between train and test sets as the proportion of the \ud835\udc56 \u2192 \ud835\udc57 test item transitions that are shared with the train set 7 for leave-one-out and time based splits (2a); and two variants of Rees46 Coveo Retailrocket 0.0 0.1 0.2 0.3 0.4 0.5 Ratio 0.5254 0.3701 0.3644 0.4222 0.3812 0.2428 Leave-one-out Time based (a) Leave-one-out and time based split Rees46 Coveo Retailrocket 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Ratio 0.7290 0.4442 0.4868 0.6756 0.4500 0.3969 Leave-one-out: random Leave-one-out: time split (b) Leave-one-out on random vs. most recent sessions the leave-one-out strategy (2b) using the most recent and random sequences. Concept drift is significantly smaller for non-overlapping splits of Rees46 and RetailRocket. Coveo has similar proportions for both splits due to consisting of only 18 days of data, which is too short for concept drift to be too prevalent. This clearly indicates that by not requiring strict separation by time, the evaluation setup becomes compromised by information leaking through time, and thus the algorithms are evaluated on a somewhat easier problem than what they would face in a production system.", "2.4 Negative sampling during testing": "Negative item sampling is a common and severe flaw of the final step, testing. Recently, [20] discussed how sampling introduces bias to measurements and its potential impact on the comparison of algorithms, while [3, 5] demonstrated that it can change the performance based ordering of models. Thus, results achieved with sampling are not reliable. Besides confirming the result of earlier work, we demonstrate how the bias introduced by negative sampling can change the ordering of models, demonstrate its impact, as well as discuss alternatives and whether sampling is needed at all. This flaw is rooted in the transition from error metrics to IR metrics from a decade ago that brought along a significant increase in the amount of compute needed for evaluation. Opposed to rating prediction that needs only a few score computations per test case, ranking requires scoring all items for every test case, since test items are treated as relevant ones that need to be ranked against other irrelevant items. Different alternatives [1] were proposed including ranking over all items and ranking the target(s) against a number of sampled negative items. Ranking over all items is how real-life recommenders work, but nevertheless, negative sampling also became a widespread practice, utilized in well cited papers of prestigious conferences (e.g. [2, 10, 16, 19, 26, 31, 39]). Metrics focus on the top few items, thus negative items should be highly ranked for sampled and full ranking results to be similar. Uniform sampling is not able to provide strong samples if the size of the item catalog is significantly larger than the number of samples. The probability of a target item ranked \ud835\udc45 in the full ranking of \ud835\udc41 items making it into the recommendation list of \ud835\udc36 items ( \ud835\udc36 < \ud835\udc45 ) when ranked against \ud835\udc46 negative samples is \u02dd \ud835\udc36 -1 \ud835\udc56 = 0 GLYPH<0> \ud835\udc45 -1 \ud835\udc56 GLYPH<1> GLYPH<0> \ud835\udc41 -\ud835\udc45 \ud835\udc46 -\ud835\udc56 GLYPH<1> / GLYPH<0> \ud835\udc41 -1 \ud835\udc46 GLYPH<1> . If the target item is ranked 1 , 490 th among 10 , 000 items or 14 , 878 th among 100 , 000, it still has more than 90% chance to make it into the top 20 with 100 uniform samples. Weak negative samples are easily distinguishable from the target and thus their use results in the severe overestimation of offline accuracy metrics. Using stronger samples provides a more accurate estimation of the accuracy of the non sampled setup. The strength of negative samples - i.e. how hard it is to distinguish them from the target - is depicted on Figure 3 for 8 sampling strategies: Recall@1 Recall@5 Recall@10 Recall@20 Rees46 0.0 0.2 0.4 0.6 0.8 1.0 Recall@1 Recall@5 Recall@10 Recall@20 Coveo 0.0 0.2 0.4 0.6 0.8 All items Uniform sampling Most similar Inverse popularity sampling Closest Farthest Most popular Least similar Popularity sampling Recall@1 Recall@5 Recall@10 Recall@20 Retailrocket 0.0 0.2 0.4 0.6 0.8 \u00b7 Sampling strategies: Sampling probability is either uniform or proportional to the items' support. Popular items are considered to be stronger samples, since recommenders tend to rank them higher. \u00b7 Most popular items: The target items are ranked against the 100 most popular items. \u00b7 Similar/close items: Soft upper bound. Requires at least as much compute as full ranking. Items are selected based on the (cosine) similarity or closeness of their embeddings to that of the target item. \u00b7 Weak baselines: Soft lower bound. Sampling with probability proportional to reciprocal item support, and items with least similar or farthest embeddings to the target item's embedding. Figure 3 indicates that even the computationally expensive most similar and closest approaches can not produce strong item sets consistently. Popularity sampling and most popular items are better than uniform sampling, but these approaches are still not able to estimate the true rank of the target. This can falsify the results, because (1) the difference between the performance of two algorithms can vanish due to the lack of challenging negative items. (2) The relative performance at recommendation list length \ud835\udc40 shifts to length \ud835\udc41 (\u226a \ud835\udc40 ) because sampling makes it easier to push the target item up on the list. Since the relative performance of two models might depend on the length of the recommendation list, this can also change their ordering for relevant list lengths. Figure 4 demonstrates this point by showing recall@N as the function of \ud835\udc41 for pairs of models for full ranking (100%), and for randomly sampling 10%, 1%, 0 . 1% of the item catalog or 100 items. The ordering of models is not static over \ud835\udc41 , and it might change multiple times (marked by red dots). Decreasing sample size moves these intersections to the left, pushing the change in model ordering to lower \ud835\udc41 values. Changes to the relative performance are drastic: model A on Coveo (4a) outperforms model B by 8 . 1% in recall@20, but A underperforms B by 4 . 7% when 100 negative samples are used. While the ordering of model A and B does not change for Retailrocket (4b), 14 . 3% uplift in recall@20 vanishes and drops to 1 . 5%. The biggest change can be observed between model A and B on Rees46 (4c) where the uplift in recall@1 (and MRR@1) drops from 49 . 5% to -1 . 6%. Since MRR is even more sensitive to the top of the list, this affects relative MRR based performance for any \ud835\udc41 : the true order of models A, B and C on Rees46 (4f) is \ud835\udc34 > \ud835\udc36 \u226b \ud835\udc35 , but with 100 samples it reads as \ud835\udc35 > \ud835\udc34 \u226b \ud835\udc36 . Computational complexity of evaluation via full ranking depends on the number of test recommendations and the size of the item catalog. Most public dataset are quite small, and even the bigger ones have item catalogs of easily manageable sizes. One test on Rees46 requires 887 , 369 recommendation lists over 172 , 756 items each that is executed 10 0 10 1 10 2 10 3 10 4 N 0.2 0.4 0.6 0.8 1.0 Recall model A (100%) model B (100%) model A (10%) model B (10%) model A (#100) model B (#100) model A (0.1%) model B (0.1%) (a) Coveo - Recall@N 10 0 10 1 10 2 10 3 10 4 N 0.2 0.4 0.6 0.8 1.0 Recall model A (100%) model B (100%) model A (10%) model B (10%) model A (1%) model B (1%) model A (#100) model B (#100) model A (0.1%) model B (0.1%) (b) Retailrocket - Recall@N 10 0 10 1 10 2 10 3 10 4 10 5 N 0.2 0.4 0.6 0.8 1.0 Recall model A (100%) model B (100%) model A (10%) model B (10%) model A (1%) model B (1%) model A (#100) model B (#100) (c) Rees46 - Recall@N - AB 10 0 10 1 10 2 10 3 10 4 10 5 N 0.2 0.4 0.6 0.8 1.0 Recall model C (100%) model B (100%) model C (10%) model B (10%) model C (1%) model B (1%) model C (#100) model B (#100) (d) Rees46 - Recall@N - BC 10 0 10 1 10 2 10 3 10 4 10 5 N 0.2 0.4 0.6 0.8 1.0 Recall model A (100%) model B (100%) model C (100%) model A (10%) model B (10%) model C (10%) model A (1%) model B (1%) model C (1%) model A (#100) model B (#100) model C (#100) (e) Rees46 - Recall@N - ALL 10 0 10 1 10 2 10 3 10 4 10 5 N 0.2 0.4 0.6 0.8 MRR model A (100%) model B (100%) model C (100%) model A (10%) model B (10%) model C (10%) model A (1%) model B (1%) model C (1%) model A (#100) model B (#100) model C (#100) (f) Rees46 - MRR@N - ALL in only 205 . 1 seconds on an A30 GPU. Even before GPUs, evaluation on public datasets of the time could be executed in a few minutes using optimized code. If a dataset is really too large for quick experimentation, the sampling of test recommendations (e.g. users) gives more representative results. Certain scoring models are not designed for full ranking. If one such model must be evaluated on ranking, generating candidate sets via another algorithm can solve the problem. However, this limits the generality of claims about its performance, because currently there are no universally accepted candidate set generators.", "3 RELATED WORK": "Offline evaluation of top-N recommenders has been discussed since top-N recommendation became dominant. Early works mainly focused on metrics [17, 35] and finding connections between offline and online results [29]. The pace of the work has increased in recent years [38], simultaneously with the sharp increase of papers lacking satisfactory evaluation. Evaluation has many aspects from metrics [17, 35] to hyperparameter optimization [27]. [6, 7] suggests that one of the reasons behind the low reproducibility rate of recent papers is that evaluation setups are lifted from earlier work without their validity being checked. Our work focuses on this problem, i.e. the design of evaluation setups and the flaws within. Of the four flaws discussed in this paper, only negative item sampling has been scrutinized, aside from [33] briefly mentioning that the Amazon dataset has weak sequential signals. [20] discussed the background of how sampling introduces bias to measurements, and [3, 5] demonstrated that the relative performance of models can change when sampling is applied. Our experiments confirm their results and give additional explanation on why this change happens.", "4 DISCUSSION & CONCLUSION": "Offline evaluation is fundamentally imperfect, but it will likely remain the main approach for assessing performance in recommendation systems research. Online A/B tests are not just expensive and slow, but inherently not reproducible; and simulators are still in their early stages. Unfortunately, flaws are quite widespread in offline evaluation setups. In this paper, we went through the main steps of designing evaluation setups and pointed out four widespread flaws and demonstrated their effect through the example of sequential recommendations. We chose sequential recommendation for the demonstration, because it is one of the areas severely plagued by low quality evaluation. Evaluation flaws of some of the earlier sequential recommender papers - e.g. [19] has all four we discussed - were missed by both the authors and reviewers. Later works copied the setups - with their flaws included - without questioning whether they are correct. There is not a single best way for offline evaluation, but by pointing out these four flaws, we hope to see the quality of experimentation sections of research papers improving in the future.", "ACKNOWLEDGMENTS": "The work leading to these results received funding from National Research, Development and Innovation Office, Hungary under grant agreement number 2020-1.1.2-PIACI-KFI-2021-00289. The authors would also like to thank Domonkos Tikk for his valuable support as the project leader of the abovementioned R&D grant.", "REFERENCES": "[1] Alejandro Bellogin, Pablo Castells, and Ivan Cantador. 2011. Precision-oriented evaluation of recommender systems: an algorithmic comparison. In Proceedings of the fifth ACM conference on Recommender systems . 333-336. [2] Wei Cai, Weike Pan, Jingwen Mao, Zhechao Yu, and Congfu Xu. 2022. Aspect Re-distribution for Learning Better Item Embeddings in Sequential Recommendation. In Proceedings of the 16th ACM Conference on Recommender Systems . 49-58. [20] Walid Krichene and Steffen Rendle. 2020. On sampled metrics for item recommendation. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining . 1748-1757. [24] Aleksandr Petrov and Craig Macdonald. 2022. A Systematic Review and Replicability Study of BERT4Rec for Sequential Recommendation. In Proceedings of the 16th ACM Conference on Recommender Systems . 436-447. [28] Borja Requena, Giovanni Cassani, Jacopo Tagliabue, Ciro Greco, and Lucas Lacasa. 2020. Shopper intent prediction from clickstream e-commerce data with minimal browsing information. Scientific reports 10, 1 (2020), 1-23. [38] Eva Zangerle, Christine Bauer, and Alan Said. 2022. Second Workshop: Perspectives on the Evaluation of Recommender Systems (PERSPECTIVES 2022). In Proceedings of the 16th ACM Conference on Recommender Systems (Seattle, WA, USA) (RecSys '22) . Association for Computing Machinery, New York, NY, USA, 652-653. https://doi.org/10.1145/3523227.3547408 [39] Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, and Ji-Rong Wen. 2020. S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization. In Proceedings of the 29th ACM international conference on information & knowledge management . 1893-1902."}
