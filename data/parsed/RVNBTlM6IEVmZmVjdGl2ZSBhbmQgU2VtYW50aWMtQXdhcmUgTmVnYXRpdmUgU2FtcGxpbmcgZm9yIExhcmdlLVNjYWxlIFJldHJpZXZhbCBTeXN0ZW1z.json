{"ESANS: Effective and Semantic-Aware Negative Sampling for Large-Scale Retrieval Systems": "Haibo Xing Alibaba International Digital Commerce Group Hangzhou, Zhejiang, China Kanefumi Matsuyama Alibaba International Digital Commerce Group Hangzhou, Zhejiang, China Hao Deng Alibaba International Digital Commerce Group Beijing, Beijing, China Jinxin Hu \u2217 Alibaba International Digital Commerce Group Beijing, Beijing, China Yu Zhang Alibaba International Digital Commerce Group Beijing, Beijing, China", "Abstract": "Industrial recommendation systems typically involve a two-stage process: retrieval and ranking, which aims to match users with millions of items. In the retrieval stage, classic embedding-based retrieval (EBR) methods depend on effective negative sampling techniques to enhance both performance and efficiency. However, existing techniques often suffer from false negatives, high cost for ensuring sampling quality and semantic information deficiency. To address these limitations, we propose Effective and SemanticAware Negative Sampling (ESANS), which integrates two key components: Effective Dense Interpolation Strategy (EDIS) and Multimodal Semantic-Aware Clustering (MSAC). EDIS generates virtual samples within the low-dimensional embedding space to improve the diversity and density of the sampling distribution while minimizing computational costs. MSAC refines the negative sampling distribution by hierarchically clustering item representations based on multimodal information (visual, textual, behavioral), ensuring semantic consistency and reducing false negatives. Extensive offline and online experiments demonstrate the superior efficiency and performance of ESANS.", "CCS Concepts": "", "\u00b7 Information systems \u2192 Retrieval models and ranking .": "", "Keywords": "Recommendation systems, Embedding-based retrieval, Negative sampling", "ACMReference Format:": "Haibo Xing, Kanefumi Matsuyama, Hao Deng, Jinxin Hu, Yu Zhang, and Xiaoyi Zeng. 2025. ESANS: Effective and Semantic-Aware Negative Sampling Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. https://doi.org/10.1145/3696410.3714600 Xiaoyi Zeng Alibaba International Digital Commerce Group Hangzhou, Zhejiang, China for Large-Scale Retrieval Systems. In Proceedings of the ACM Web Conference 2025 (WWW '25), April 28-May 2, 2025, Sydney, NSW, Australia. ACM, Sydney, SYD, Australia, 10 pages. https://doi.org/10.1145/3696410.3714600", "1 Introduction": "Recommendation systems have been widely adopted across diverse domains, including online e-commerce, advertising, short video platforms and delivery services [16, 17, 59], owing to their effectiveness in mitigating information overload by providing tailored recommendations from large-scale item collections [18, 19]. Industrial recommendation systems typically involve two stages: retrieval and ranking. The retrieval stage is responsible for retrieving thousands of candidate items, whereas the ranking stage predicts the likelihood of user interaction with these candidates. Considering that retrieval tasks can be formulated as identifying the nearest neighbors in a vector space, substantial research has been devoted to developing high-quality representations for both users and items. Collaborative Filtering (CF) methods [8, 24, 42, 45] address this issue by encoding user preference and item representation into low-dimensional embedding space, based on historical interacted information. With the rapid development of deep learning, neural networks have been widely adopted in personalized recommendation systems [5, 20, 55]. Recently, Embedding-Based Retrieval (EBR) methods [3, 12, 30] have demonstrated significantly better performance compared to traditional CF methods, establishing themselves as the dominant approach in recommendation systems. EBR methods encode user and item information into separate embeddings using parallel neural networks, and these embeddings are trained through the strategy of contrastive learning [15, 36, 44]. EBR methods rely heavily on the contrast between positive and negative samples to produce distinguishable representations. The careful selection of negatives is crucial to enhancing the model's ability to differentiate between relevant and irrelevant items, significantly impacting overall retrieval performance. The classic Uniform Negative Sampling (UNS) method [23, 44] randomly selects negatives from the item candidate set, providing efficiency but yielding low-quality samples . Following this, additive margin [50] and temperature coefficient [33, 51] adjust the contrastive loss function to mine high-quality negatives from naive negatives sampled by UNS. FairNeg [9] reweights negatives in accordence with item group fairness to provide high-quality samples. Adap\ud835\udf0f [4] dynamically adjusts the temperature coefficient for reweighting uniform Positive Sample Semantic Distance between Clusters Primary Semantic Cluster(neg) Secondary Semantic Cluster Primary Semantic Cluster(pos) Dense Interpolation Plane UNS method randomly selects negatives, failing to capture minority cluster semantics. FairNeg method samples uniformly across groups but fails to generate sufficient hard negatives. In-batch method selects popular items(denoted as    ) as hard negatives, causing false negatives. ESANS method adjusts sampling difficulty by the semantic distance, avoids false negatives with secondary clustering, and increases the sampling scale with interpolation strategy. negatives in accordence with their relevance to user interests. However, these methods fail to introduce more challenging negatives and further expand the scale of sampling which limit the performance of EBR methods. To address this issue, In-batch sampling [7] introduces relatively harder negatives by the in-batch sharing strategy. Airbnb [21] heuristically introduces orders from the same city as harder negatives. MixGCF [27] employs a hop-mixing interpolation technique in Graph Neural Networks (GNNs) to generate virtual hard negatives. However, these methods fail to effectively adjust the difficulty of negatives and distinguish users' potential interests from hard negatives, which may exacerbate the issue of false negatives (i.e. items relevant to users' potential interests but incorrectly regarded as negatives). Moreover, existing methods require substantial computational resources to further improve the sampling quality (i.e. sufficient hard negatives) [6]. From the contrastive learning perspective, these methods are unable to regulate sampling strategies based on semantic information in the real world, rendering the sampling process a black box. To address these issues, we redesign the sampling space from a multimodal perspective and propose a controllable negative sampling algorithm based on well-defined semantic distance. Specifically, Inspired by recent works in multi-modal learning [35, 40] and vector quantization techniques [47], we propose the E ffective and S emanticalA ware N egative S ampling (ESANS) to address these challenges in the sampling process. Our method consists of two main components: the first part is Effective Dense Interpolation Strategy (EDIS), and the second part is Multimodal Semantic-Aware Clustering (MSAC). EDIS is devised to generate a sufficient number of virtual samples within the low-dimensional embedding space. More specifically, generating virtual samples among existing negatives creates a more uniform, dense, and diverse sampling distribution. Virtual samples positioned between the positive sample and surrounding negative samples contribute to gradually enhance the discriminatory ability of the neural network. By adjusting the interpolation parameters and strategies, we can control the difficulty of generated negatives and generate sufficient hard negatives. Meanwhile, in contrast to memory banks [22], interpolation within the low-dimensional embedding space leads to minimal computational cost and eliminates the need for extra memory storage. Nevertheless, EDIS strongly relies on the judicious selection of negative sample anchors. In practice, virtual negative samples generated via interpolation may lack clear semantic information, occasionally producing meaningless samples. For example, interpolating between \"iPhone\" and \"Cola\" produces meaningless results, potentially introducing noise. Moreover, interpolating among randomly sampled negative anchors may introduce false negatives, further complicating the training process. To address these deficiencies, we propose the MSAC method to optimize the sampling space by integrating the real-world semantic information. Firstly, we propose a multimodal-aligned technique to fuse multi-perspective item information from visual, textual and behavioral perspectives. Subsequently, a two-level vector quantized clustering approach is employed to assign semantic representations into multiple secondary clusters. Consequently, we can mitigate the issue of false negatives by selecting hard negatives from the same primary cluster as the positive sample, while ensuring they belong to a different secondary cluster. Additionally, we dynamically calibrate the sampling probabilities for each negative cluster to control the difficulty of negatives and refine the sampling quality. It is worth noting that this calibration is precisely guided by the semantic distance between the cluster centers of positives and negatives. This allows us to adjust the difficulty of the sampling process by increasing the sampling probabilities of clusters that are semantically similar to the positive cluster. Once the MSAC is introduced, EDIS based on semantics can be performed within the well-established semantic clusters. More specifically, we can ensure that the interpolated outcomes remain confined within the convex hull of that cluster. This intrinsic constraint preserves a measurable degree of semantic consistency and \"real-world applicability\" in the interpolated samples. Furthermore, interpolation between positives and hard negatives is also employed to generate additional high-quality hard negatives. Figure 1 shows the comparison between our ESANS and other methods. Our contributions can be summarized as follows: \u00b7 Weproposeanovelandeffective sampling approach called ESANS, which provides explicit semantics guidance for interpolation negative sampling. Moreover, ESANS effectively enhances the diversity and richness of negative samples and allows for controllable negative sample difficulty, thereby boosting performance. \u00b7 We propose a general multimodal-aligned clustering approach that captures the multi-perspective similarities among candidate items on e-commerce platforms, thereby enabling a more refined semantic description in the interpolation space and eliminating false negative instances in the hard negative sampling process. \u00b7 We provide both extensive offline and online experiments to demonstrate the effectiveness and the efficiency of ESANS.", "2 Related Work": "This section presents a brief review of the relevant literature, specifically addressing techniques for negatives re-weighting, heuristic negative sampling, and model-based negative sampling. Negatives Re-weighting. UNS[23, 44] represents the foundational negative sampling method, where negative samples are uniformly drawn from the entire dataset. The simplicity of UNS's algorithmic design provides substantial efficiency gains. Nevertheless, it exhibits notable deficiencies in the quality of negative samples. UMA2 [33] computes the sampling probabilities of random negative samples according to the current model and subsequently employs the Inverse Probability Weighting (IPW) technique to assign loss weights to these negative samples. The method proposed by [43] implements position-weighted approach for negative samples, where the weight is determined by the sample's ranking position. These approaches mine high-quality negatives from naive negatives sampled by UNS, which, however, fails to introduce more challenging negatives. Heuristic Negative Sampling. Heuristic negative sampling algorithms primarily define the sampling distribution by predefined heuristic rules. Popularity-biased Negative Sampling (PNS) [7] utilizes item popularity as the sampling probability. Airbnb [21] applies personalized negative sampling within the same city, assuming bookings in the same location exhibit similar patterns. While this approach enhances the sampling process, it solely focuses on similarity-based sampling, neglecting sampling bias. CBNS [54] employs in-btch negative sampling and expands the negative sample set by incorporating previously trained items. The method [57] incorporates estimated item frequency into the batch softmax crossentropy loss to reduce sampling bias within the batch. MNS [56] integrates UNS with in-batch negative sampling, adopting a hybrid strategy. While these methods enhance sampling quality, they introduce popularity bias, aggravating the Sample Selection Bias (SSB) issue. In contrast, our method enhances sampling quality via a multimodal-aligned clustering algorithm and dense interpolation negative sampling, while effectively mitigating sampling bias. Model-based Negative Sampling. Model-based negative sampling algorithms are highly effective at selecting high-quality negative samples. Model-based scoring methods are demonstrated by Dynamically Negative Sampling (DNS) [58] and ESAM [10], where the current model scores samples and selects the highestscoring ones as negative samples. Adversarial learning methods also contribute to sampling improvements. MixGCF [27] employs a hop-mixing technique to synthesize hard negative samples by leveraging the user-item graph structure and the aggregation mechanism of Graph Neural Networks (GNNs). IRGAN [53] utilizes two recommendation models, a discriminator and a generator, trained adversarially. AdvIR [38] and RNS [13] further optimize IRGAN's structure, improving both efficiency and performance. The Adap\ud835\udf0f [4] adaptively adjusts the temperature coefficient of the loss function by calculating the loss for each user and the corresponding random negative samples. This method leverages personalized user preferences to effectively identify hard negative samples. FairNeg [9] enhances the sampling distribution by fairly sampling from groups and then reweighting based on their relevance to the user. Our method precisely controls the difficulty of negatives, improving sampling quality and eliminating false negatives without increasing the complexity of the retrieval model.", "3 Methodology": "In this section, we formulate the problem and describe our proposed framework specifically, as well as introducing the detailed process of our negative sampling method.", "3.1 Problem Formulation": "The primary objective of the retrieval stage in industrial recommendation systems is to efficiently retrieve a potentially relevant subset of items from a large item pool I for each user \ud835\udc62 \u2208 U . In pursuit of this objective, each instance can be represented by a tuple (B \ud835\udc62 , P \ud835\udc62 , I \ud835\udc56 ) where B \ud835\udc62 denotes the sequence of user historical behaviors, P \ud835\udc62 denotes the basic profile of user \ud835\udc62 , I \ud835\udc56 denotes the information of target item such as item id and category id. In the classical two-tower architecture [50] of the EBR models, users and items are separated into two individual encoders to reduce online computational complexity. We can define the user encoder as \ud835\udc53 \ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc5f and the item encoder as \ud835\udc54 \ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc5a , so we have: where u \ud835\udc62 \u2208 R \ud835\udc51 \ud835\udc58 \u00d7 1 is the output vector of the user encoder called user embedding, and v \ud835\udc56 \u2208 R \ud835\udc51 \ud835\udc58 \u00d7 1 is the output vector of the item encoder called item embedding. \ud835\udc3e denotes the dimension of output embeddings. Finally, the relevance of a user-item pair can be estimated by a scoring function:", "3.2 Overall Framework": "As previously discussed, existing methods fail to balance sampling quality, bias, and efficiency simultaneously. To address these limitations, we designed ESANS, as illustrated in Figure 2. ESANS consists of two main components: \u00b7 Multimodal Semantic-Aware Clustering (MSAC) , which performs hierarchical clustering based on visual, textual, and behavior based representations to optimize the sampling process by integrating semantic information. Our proposed method addresses the limitations of unclear anchor semantics, improves sampling quality, and reduces the risk of introducing false negatives. \u00b7 Effective Dense Interpolation Strategy (EDIS) , which employs linear interpolation among existing samples within the same semantic cluster to make sure the semantic consistency. Our proposed method works with minimal computational cost, enhances the diversity and richness of negative samples, and facilitates the controllable difficulty of hard negative samples.", "3.3 Multimodal Semantic-Aware Clustering": "Most existing negative sampling methods ignore semantic correlations among samples. Against this deficiency, our MSAC is proposed to capture the multi-perspective similarities among items and incorporate explicit semantics into the negative sampling process. a) b) Multimodal-aligned Technique Vector Quantized Clustering primary codebook secondary codebook - = (      ,      ) Item Cluster ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... Image Encoder Text Encoder Image Encoder Behavior Encoder ... ... 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 Mean 7 7 1 ... ... ... ... ... ... ... ... ... Text Encoder Behavior Encoder Primary Clustering Center Negatives The Positive Primary Cluster of the Positive Virtual Simple Negatives Interpolation Process Secondary Cluster of the Positive Semantic-Aware Negative Sampling c)", "Semantic-Aware Negative Sampling & EDIS": "Virtual Hard Negatives Primary Cluster of the Negatives Distance contour lines to the primary cluster center of positives 3.3.1 Multimodal-aligned Technique. When users browse items on the e-commerce platform, they primarily perceive items through three views: visual images, descriptive text, and collaborative filtering recommendations. To generate a comprehensive description of items, it is necessary to consider these views concurrently. The visual representations RI and textual representations RT can be pretrained by individual specific encoders [28, 31]. The behaviorbased representations RG can be pre-trained using graph representation learned based on a substantial number of user behaviors. It is worth noting that modal embeddings can be generated in advance using existing pre-trained models and then kept frozen during the multimodal alignment process . we call positive samples. We sample from the product of marginals, y I-T \u223d P (MI) P (MT) or y I-T = {M \ud835\udc56 I , M \ud835\udc57 T } , which we call negative samples. Multimodal-aligned encoders are optimized to correctly select a single positive sample x I-T out of the set S = { x I-T , y 1 I-T , ..., y \ud835\udc41 -1 I-T } which contains \ud835\udc41 -1 negative samples: Given a mini-batch of \ud835\udc41 items, we design multimodal-aligned linear transformations for each view. where \ud835\udc3b \u2217 denotes the linear transformation of each view, M\u2217 denotes the output embedding of each view, \ud835\udc51 \ud835\udc5a denotes the output dimension of each view. Inspired by the Contrastive Language-Image Pre-training (CLIP) [40], We propose a multimodal alignment method to fuse item representations from three perspectives. Given a dataset of M\u2217 that consists of a collection of output embeddings {M \ud835\udc56 I , M \ud835\udc56 T , M \ud835\udc56 G } \ud835\udc41 \ud835\udc56 = 1 , we contrast congruent and incongruent pairs across any two modalities. For instance, we sample from the joint distribution of imagetext modals x I-T \u223d P (MI , MT) or x I-T = {M \ud835\udc56 I , M \ud835\udc56 T } , which where \u210e (\u00b7) is the cosine similarity operation after exponentiation, L I-T \ud835\udc4e\ud835\udc59\ud835\udc56\ud835\udc54\ud835\udc5b is the alignment loss between visual and textual modals, L I-G \ud835\udc4e\ud835\udc59\ud835\udc56\ud835\udc54\ud835\udc5b is the alignment loss between visual and behavior-based modals, L G-T \ud835\udc4e\ud835\udc59\ud835\udc56\ud835\udc54\ud835\udc5b is the alignment loss between behavior-based and textual modals. 3.3.2 Vector Quantized Clustering with Cascaded Codebooks. While aligning MI , MT , MG into the same embedding space, we simultaneously quantize these representations into several clusters with cascaded codebooks, as illustrated in Figure 2. Specifically, the primary codebook is designed to effectively differentiate coarse-level item representations, while the secondary codebook enhances this distinction by refining the differentiation of fine-grained item representations, especially when significant disparities persist among aligned representations across partial modalities. Mean emb Visual emb Textual emb Behavior-based emb Item_1 Item_2 Item_3 Item_4 Secondary Boundary Item_5 The primary codebook \ud835\udc36 \ud835\udc5d = { \ud835\udc67 \ud835\udc58 \ud835\udc5d } \ud835\udc3e \ud835\udc5d \ud835\udc58 = 1 consists of \ud835\udc3e \ud835\udc5d codewords [29] and the dimension of each codeword is \ud835\udc51 \ud835\udc5a . The clustering stage is conducted by calculating the mean of the aligned embeddings: Subsequently R \ud835\udc5d = {R \ud835\udc56 \ud835\udc5d } \ud835\udc41 \ud835\udc56 = 1 is quantized by assigning it to the nearest codeword within the primary codebook. We denote that the nearest codeword to R \ud835\udc56 \ud835\udc5d is \ud835\udc36 \ud835\udc56 \ud835\udc5d = arg min \ud835\udc58 \u2225R \ud835\udc56 \ud835\udc5d -\ud835\udc67 \ud835\udc58 \ud835\udc5d \u2225 . In the secondary codebook , we compute the residual between {MI , MT , MG} and the primary corresponding codeword \ud835\udc67 \ud835\udc36 \ud835\udc56 \ud835\udc5d \ud835\udc5d . These residuals are concatenated to a vector R \ud835\udc56 \ud835\udc60 , which is used to describe the modal-specific information between different items. The advantages of using information from three modalities for secondary clustering are illustrated in Figure 3. Similar to the primary clustering, we select the codeword closest to R \ud835\udc60 = {R \ud835\udc56 \ud835\udc60 } \ud835\udc41 \ud835\udc56 = 1 from another codebook \ud835\udc36 \ud835\udc60 = { \ud835\udc67 \ud835\udc58 \ud835\udc60 } \ud835\udc3e \ud835\udc60 \ud835\udc58 = 1 , where \ud835\udc3e \ud835\udc60 denotes the number of codewords in the secondary codebook. The nearest secondary codeword to R \ud835\udc56 \ud835\udc60 is recorded as \ud835\udc36 \ud835\udc56 \ud835\udc60 = arg min \ud835\udc58 \u2225R \ud835\udc56 \ud835\udc60 -\ud835\udc67 \ud835\udc58 \ud835\udc60 \u2225 . Once we have all cluster indice for an item, the clustering loss can be defined as: Weproperly initialize \ud835\udc36 \ud835\udc5d and \ud835\udc36 \ud835\udc60 with the \ud835\udc58 -\ud835\udc5a\ud835\udc52\ud835\udc4e\ud835\udc5b\ud835\udc60 algorithm to avoid the codebook collapse. Finally, the loss function for multimodalaligned clustering is given by Equation 8: 3.3.3 Semantic-Aware Negative Sampling. Based on the above framework, we divide the whole set of candidate items into multiple semantic clusters. Then we introduce the semantic-aware negative sampling which includes simple negative sampling and hard negative sampling. In simple negative sampling, we select primary clusters for each positive sample based on the following probability formula, ensuring that none of these selected clusters are the same as the primary cluster of the positive sample. where \ud835\udc51 (\u00b7 , \u00b7) measures the distance between primary codewords using an inner-product operation, which is subsequently normalized to a range from 0 to 1. \ud835\udc67 + \ud835\udc5d is the primary cluster of the positive sample, \ud835\udc44 ( \ud835\udc36 \ud835\udc5d = \ud835\udc56 ) is the unnormalized sampling probability of similar primary clusters with \ud835\udefe , \ud835\udc43 ( \ud835\udc50 \ud835\udc5d = \ud835\udc56 ) is the normalized sampling probability of primary cluster \ud835\udc67 \ud835\udc56 \ud835\udc5d . Then, we randomly select samples from each cluster which enhances the diversity of negative samples. After being encoded by the item tower [25], the embedding set of simple negative samples can be represented as \ud835\udc49 \ud835\udc60 : where \ud835\udc49 \ud835\udc58 \ud835\udc60 is the embedding set of the simple negative samples in \ud835\udc58 -th cluster, \ud835\udc5a \ud835\udc50 is the number of selected clusters and \ud835\udc5a \ud835\udc5c is the number of selected samples in each cluster. In this way, we dynamically adjust the difficulty of the simple negatives as well as mitigate group-level sampling biases. In hard negative sampling strategy, we randomly select partially similar samples within the positive primary cluster. Then, we consider samples in the same secondary cluster as false negatives and remove these samples from the hard negative samples set. The output embedding set of hard negative samples can be represented as \ud835\udc49 \u210e = { v 1 \u210e , v 2 \u210e , ..., v \ud835\udc5a \u210e \u210e } , where \ud835\udc5a \u210e is the number of selected samples in the positive primary cluster.", "3.4 Effective Dense Interpolation Strategy": "By employing our negative sampling process, we obtain semantic clusters and randomly selected negatives from each cluster. It's a well-established principle [6] that increasing the negative sampling size can enhance the performance of the EBR models. However, the process mentioned above does not guarantee a sufficient sampling size for each cluster. To solve this problem, we propose a parameteradaptive negative sampling augmentation technique based on the linear interpolation to increase the number of negative samples. The detailed interpolation process is applied to both simple negatives and hard negatives, which is illustrated in Figure 2. 3.4.1 Interpolation on Simple Negative Samples. Suppose we select \ud835\udc5b \ud835\udc5c negative anchors (2 \u2264 \ud835\udc5b \ud835\udc5c \u2264 \ud835\udc5a \ud835\udc5c ) from the \ud835\udc58 -th cluster. The output item embeddings are reordered as \ud835\udc49 \ud835\udc60 \ud835\udc58 = { v 1 \ud835\udc60 \ud835\udc58 , ..., v \ud835\udc5b \ud835\udc5c \ud835\udc60 \ud835\udc58 } . Each vector in the embedding set is selected once as the anchor vector v \ud835\udc4e \ud835\udc60 \ud835\udc58 , and generate the virtual negative samples similar to the embedding set by linear interpolation: where \u02dc v \ud835\udc4e \ud835\udc60 \ud835\udc58 denotes the virtual negative sample obtained by linear interpolation, \ud835\udc51 (\u00b7 , \u00b7) is the inner-product operation to measure the embedding distance between item vectors, which is subsequently normalized to a range from 0 to 1. \ud835\udf02 is designed to adjust the magnitude of impact resulting from surrounding vectors, \ud835\udefc \ud835\udc57 is the adaptive parameter to fuse negative samples. Our method ensures that each virtual sample is proximate to the anchor and can be disturbed by other negative samples in terms of similarity. When the number of selected negatives \ud835\udc5b \ud835\udc5c ranges from 2 to \ud835\udc5a \ud835\udc5c , the quantity of virtual samples \ud835\udc5a \ud835\udc63 \ud835\udc50 in the \ud835\udc58 -th primary cluster is proportional to \ud835\udc42 ( \ud835\udc5a 2 \ud835\udc5c ) , which can be solved as follow: In this way, we efficiently enhance the diversity and richness of negative samples. 3.4.2 Interpolation on Hard Negative Samples. As mentioned in the previous explanation, the hard negative samples are selected from the same primary cluster but different secondary cluster. The interpolation on hard negative samples is proposed to further augment the quantity of samples and facilitate the controllable difficulty of hard negative samples. We denote the output embedding of positive sample as v + . We conduct the linear interpolation between v + and each existing hard negative sample v \ud835\udc4e \u210e : where \ud835\udf06 is a hyperparameter used to adjust the difficulty of hard negatives during the training process. When 0 < \ud835\udf06 < 1, virtual hard negatives are generated between positive samples and existing hard nagetives which provides more challenging samples. When \ud835\udf06 < 0, virtual hard negatives are easier than existing hard nagetives which provides relatively simple samples. By this strategy, we enhance the challenge of discriminating the classification boundary and incorporate the stochastic uncertainty into the model which also improves its generalization performance.", "3.5 Model Learning": "Following the widely used EBR method, Deep Structured Semantic Model (DSSM) [26, 50], we can optimize the similarity between user embeddings u and item embeddings v by contrastive learning method. The objective function applied is the InfoNCE loss, which is defined as follows: where \ud835\udc41 \ud835\udc60 is the number of negatives effectively sampled by our ESANS method, v \ud835\udc57 -\ud835\udc56 denotes the \ud835\udc57 -th negative sample of the \ud835\udc56 -th positive sample v + \ud835\udc56 , u \ud835\udc47 v is also called as the target logit [39] of the \ud835\udc56 -th positive sample, \ud835\udf0f is the temperature hyperparameter used to adjust the distribution of logits. The algorithm process is presented in Appendix A .", "4 Offline Experiments": "In this section, we conduct offline experiments on three real-world datasets to demonstrate the effectiveness and efficiency of our proposed method. The first two are public datasets, while the third is an in-house industrial dataset. The descriptions and statistics of the two public datasets and the industrial dataset are detailed in Table 1, respectively. Additionally, we perform an ablation study of our modules and address the following research questions: \u00b7 RQ1 : How does our ESANS perform compared to other state-ofthe-art models? \u00b7 RQ2 : What is the impact of each component on the overall model's performance? \u00b7 RQ3 : What is the effect of the hyper-parameters on the performance of our model?", "4.1 Experimental Setup": "", "Dataset.": "\u00b7 Amazon Review. It was first introduced by Van Gysel et al. [48, 49] and has become a benchmark dataset for evaluating product recommendation methods [14, 34, 46]. We select the Electronics subset which products a sufficient number of user reviews and includes comprehensive metadata, such as product titles and categories. The textual features are extracted by sentence-transformers [41] from [60] and the visual features are extracted and published in [37]. \u00b7 Pixel-Rec. This dataset [11] is derived from a global online video platform which captures approximately 200 million user consumption from September 2021 to October 2022. It focuses on content-driven recommendations spanning diverse categories such as food, games, fashion, and makeup. The textual and visual features of these contents have already been extracted using PixelNet, a network proposed concurrently with Pixel-Rec. \u00b7 Industrial Dataset. We establish the in-house offline dataset by collecting the users' sequential behaviors and feedback logs from Alibaba's international e-commerce platform, Lazada. The dataset comprises four categories, each representing a distinct Southeast Asian country, labeled from #A1 to #A4. Graph Construction. Due to space limitation, the introduction of behavior-based graph construction is provided in Appendix B. Baselines. We compared our ESANS with five representative negative sampling methods based on the classical two-tower architecture. The methods are as follows: \u00b7 UNS [23, 44]: A widely used negative sampling approach involves randomly selecting instances from a uniform distribution. \u00b7 PNS [7]: A negative sampling method that adjusts the sampling distribution based on item popularity. \u00b7 Debiased MNS [56, 57]: A method that integrates UNS with inbatch negative sampling, and introduces a technique to address the oversampling issue of popular items. \u00b7 MixGCF [27]: A method synthesizes hard negatives between negatives and positives in a graph-based model. We adapt this to a two-tower structure to generate virtual hard negatives in the item representation space. \u00b7 FairNeg [9]: A method that improves item group fairness by adaptively adjusting the distribution of negative samples at the group level. \u00b7 Adap\ud835\udf0f [4]: A method that adjusts the temperature coefficient of the loss function by the embedding similarity between users and corresponding negatives. Evaluation Metrics. For the evaluation metrics in recommendation tasks, we follow [2, 52] and use Recall@K for each group based on the Top-K recommendation results. Finally, the Recall@K is averaged over all users. In summary, our method effectively addresses the inherent limitations of these methods and achieves SOTA performance across all datasets in terms of retrieval efficiency. It is worth noting that the MSAC module is actually detached from the EBR model's training process and the EDIS module is only applied to the output embeddings of the EBR model. Therefore, our method does not introduce additional computational complexity for offline training. The comparison of time costs among these methods in the training process is shown in Table 3, which strongly supports our statement. Furthermore, ESANS can be deployed online similarly to other classical EBR models, with the user and item towers deployed separately. This ensures that the online service costs remain comparable to those of other baseline models. Parameter settings. Due to space limitation, the implementation details are provided in Appendix C. Table 2 summarizes the overall performance of our ESANS as well as the baselines on both industrial and public datasets, with the best results emphasized in bold and the second-best results underlined. It is noteworthy that ESANS consistently outperforms all baseline methods across the aforementioned datasets, achieving an average improvement of up to 15.32% in Recall@50 and 10.73% in Recall@200 compared to its base method UNS. PNS generally outperforms UNS across most datasets, indicating that boosting the sampling possibility for popular items improves sampling quality. However, it is worth noting that PNS does not exceed UNS performance in the #A3 dataset, which might be attributed to the introduced popularity bias. Once the challenge of popularity bias is addressed, the debiased MNS Sampling method outperforms UNS and PNS across all datasets and outperforms other baselines on #A4. MixGCF introduces virtual hard negatives by hop-mixing interpolation which achieves similar performance with the debiased MNS and proves the feasibility of hard negatives augmentation. However, the interpolation process fails to consider semantics and yields noisy negatives, so it is outperformed by our method. FairNeg is another work conducted to reduce the sampling bias via adjusting the group-level negative sampling distribution which provides the best recommendation utility on Pixel-Rec and #A2 in all baselines. However, this work determines the groups by the only item attribute view which is not comprehensive and thus is surpassed by our method. Ada\ud835\udf0f is proposed to design a learnable \ud835\udf0f , which enables the adaptive adjustment of the difficulty level for negatives. This work outperforms other baseline models on Amazon Elecs. However, Ada\ud835\udf0f fails to provide incremental information by deriving more challenging negatives so that it is beaten by our method.", "4.2 Ablation Study (RQ2)": "To investigate the effectiveness of each component in the proposed model, in this subsection, we conduct a series of ablation studies on the #A2 industrial dataset, as it represents the most complex and representative scenario with the largest user scale and the richest behavior on our platform. The specific experiment settings are introduced as follows: \u00b7 w/o MSAC , removes the Multimodal Semantic-Aware Clustering before the Interpolation-based negative sampling. \u00b7 w/o EDIS , removes the Effective Dense Interpolation Strategy employed in both simple negative sampling and hard negative sampling strategies. Furthermore, we conduct additional ablation studies on both simple and complex interpolation strategies. \u00b7 w/o Multimodal Aligning , removes the textual and visual modalities and reserves the behavior-based modality for further clustering. Considering the relatively high cost of using three modals, we also remove each of the three modals to evaluate their individual contributions. \u00b7 w/o Secondary Codebook , removes the secondary codebook in the Vector Quantized Clustering, thereby invalidating the interpolation-based hard negative sampling. Table 4 presents the performance of these ablation experiments. Firstly, we can observe that adopting Multimodal-aligned Clustering Algorithm improves Recall@50 by 6.41% and Recall@200 by 4.02% , which proves that the semantic clustering algorithm employed for interpolation significantly improves sampling quality. The dense interpolation respectively brings a 2.61% and a 1.34% improvement for Recall@50 and Recall@200, which demonstrates the efficient of our sample augment strategy. We also find that hard interpolation achieves more improvement compared with simple interpolation, which implies the importance of hard negatives. Besides, the Multimodal clustering performs better than the Unimodal clustering ( 2.24% on Recall@50 and 0.86% on Recall@200), which superiority the multi-view representations. Among the three modals, the behavior-based modality exhibits the best performance, while the visual and textual modalities also contribute positively to the MSAC. The interpolation-based hard negative sampling conducted by the secondary codebook also shows the improvement ( 4.38% on Recall@50 and 2.20% on Recall@200), which proves the feasibility of selecting hard negative samplings with heuristics semantic constraint.", "4.3 Hyperparameters Sensitivity Analysis (RQ3)": "In this section, we investigate the sensitivity of our model's hyperparameters, specifically the number of primary clusters \ud835\udc3e \ud835\udc5d , the number of secondary clusters \ud835\udc3e \ud835\udc60 and the interpolation coefficient \ud835\udf06 . These experiments are carried out on the #A1-#A4 industrial datasets, employing five distinct values for \ud835\udc3e \ud835\udc5d (100, 200, 300, 400, 500), \ud835\udc3e \ud835\udc60 (5, 10, 15, 20, 25) and \ud835\udf06 (-0.3, -0.1, 0.1, 0.3, 0.5). Figure 4 illustrates the performance of these hyperparameter tuning experiments. We observe that the model's performance stays consistently high when \ud835\udc3e \ud835\udc5d is increased from 200 to 500. However, reducing \ud835\udc3e \ud835\udc5d to 100 leads to a slight decrease in performance. This observation encourages us to consider a higher value for \ud835\udc3e \ud835\udc5d to further enhance the intra-cluster semantic consistency. \ud835\udc3e \ud835\udc60 shows optimal prediction performance between 5 to 15. We recommend to set a relatively small value for \ud835\udc3e \ud835\udc60 in order to minimize the occurrence of false negatives. According to our experiments on \ud835\udf06 , \ud835\udf06 = 0 . 1 achives the best performance on #A2-#A4 while \ud835\udf06 = -0 . 1 achives the best performance on #A1. We find that \ud835\udf06 is relatively sensitive. As we increase \ud835\udf06 to 0.5, the performance across all datasets declined significantly. This is due to the fact that the generated virtual false negatives are very close to the positives, which may confuse the model. It is also worth noting that sometimes \ud835\udf06 can be set to less than 0 to achieve better performance, which suggests that introducing easier hard negatives may also be helpful. 0 0 2 @ e t a r t i H 0 0 2 @ e t a r t i H 0 0 2 @ e t a r t i H 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 . . . . . . . . . . . . . . . . . . . . . 7 7 6 6 6 6 5 7 7 6 6 6 6 5 7 7 6 6 6 6 5 2 0 7 5 2 0 7 2 0 7 5 2 0 7 2 0 7 5 2 0 7 5 0 5 0 5 0 5 5 0 5 0 5 0 5 5 0 5 0 5 0 5 S s t i f o P a r a m e t e K s r e n s i i v i t y A n a l y # A 1 # A 2 # A 3 # A 4 I n d u s t r i a l d a t a s e t s p K p = 1 0 0 K p = 2 0 0 K p = 3 0 0 K p = 4 0 0 K p = 5 0 0 # A 1 # A 2 # A 3 # A 4 I n d u s t r i a l d a t a s e t s S e n s i t i v i t y A n a l y s i s o f P a r a m e t e r K s K s = 5 K s = 1 0 K s = 1 5 K s = 2 0 K s = 2 5 S e n s i t i v i t y A n a l y s i s o f P a r a m e t e r \u03bb \u03bb = 0 . 5 \u03bb = 0 . 3 \u03bb = 0 . 1 \u03bb = - 0 . 1 \u03bb = - 0 . 3 # # 1 A 4 2 A 3 A # A # I i a s n u s s l a e t d r t a d t", "5 Online Experiments": "To further validate the effectiveness of our approach, we conducted an online A/B test on an e-commerce recommendation platform from September 13 to 19, 2024. The control group used a two-tower model with debiased mixed negative sampling (MNS) [56], while the experiment group applied our proposed method. Both groups consisted of 30% randomly selected users. Specifically, we observed 2.83% increase in the Advertising Revenue , 1.19% increase in the Click-Through-Rate(CTR) and 1.94% increase in the Gross Merchandise Volume(GMV) . The results of the online experiment once again confirm the efficiency and effectiveness of our method ESANS in negative sampling for recommendation systems.", "6 Conclusion": "In this study, we proposed a novel negative sampling method, Effective and Semantic-Aware Negative Sampling (ESANS), which integrates an Effective Dense Interpolation Strategy (EDIS) and Multimodal Semantic-Aware Clustering (MSAC). Extensive experiments demonstrated that ESANS significantly improves sampling quality and efficiency compared to baselines. Specifically, EDIS improves the diversity and density of the sampling distribution. MSAC enhances semantic consistency and reduces false negatives. These modules advance the effectiveness of negative sampling in recommendation systems. For future work, we will pursue two directions. The first is to further optimize the multimodal representations based on MSAC. The second direction is to design a more complex interpolation strategy among the outputs of hidden layers.", "References": "[1] Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. 2016. TensorFlow: a system for Large-Scale machine learning. In 12th USENIX symposium on operating systems design and implementation (OSDI 16) . 265-283. [2] Trapit Bansal, David Belanger, and Andrew McCallum. 2016. Ask the gru: Multitask learning for deep text recommendations. In proceedings of the 10th ACM Conference on Recommender Systems . [3] Yukuo Cen, Jianwei Zhang, Xu Zou, Chang Zhou, Hongxia Yang, and Jie Tang. 2020. Controllable multi-interest framework for recommendation. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . [53] Jun Wang, Lantao Yu, Weinan Zhang, Yu Gong, Yinghui Xu, Benyou Wang, Peng Zhang, and Dell Zhang. 2017. Irgan: A minimax game for unifying generative and discriminative information retrieval models. In Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval . [54] Jinpeng Wang, Jieming Zhu, and Xiuqiang He. 2021. Cross-batch negative sampling for training two-tower recommenders. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval . [55] Yinwei Wei, Xiang Wang, Liqiang Nie, Xiangnan He, Richang Hong, and Tat-Seng Chua. 2019. MMGCN: Multi-modal graph convolution network for personalized recommendation of micro-video. In Proceedings of the 27th ACM international conference on multimedia . [56] Ji Yang, Xinyang Yi, Derek Zhiyuan Cheng, Lichan Hong, Yang Li, Simon Xiaoming Wang, Taibai Xu, and Ed H Chi. 2020. Mixed negative sampling for learning two-tower neural networks in recommendations. In Companion proceedings of the web conference 2020 . [57] Xinyang Yi, Ji Yang, Lichan Hong, Derek Zhiyuan Cheng, Lukasz Heldt, Aditee Kumthekar, Zhe Zhao, Li Wei, and Ed Chi. 2019. Sampling-bias-corrected neural modeling for large corpus item recommendations. In Proceedings of the 13th ACM conference on recommender systems . [58] Weinan Zhang, Tianqi Chen, Jun Wang, and Yong Yu. 2013. Optimizing top-n collaborative filtering via dynamic negative item sampling. In Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval . [59] Xiangyu Zhao, Haochen Liu, Wenqi Fan, Hui Liu, Jiliang Tang, and Chong Wang. 2021. Autoloss: Automated loss function search in recommendations. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining . [60] Xin Zhou, Hongyu Zhou, Yong Liu, Zhiwei Zeng, Chunyan Miao, Pengwei Wang, Yuan You, and Feijun Jiang. 2023. Bootstrap latent representations for multimodal recommendation. In Proceedings of the ACM Web Conference 2023 .", "A Algorithm": "CORD: Generalizable Cooperation via Role Diversity 715 717 716 718 720 719 721 723 722 724 726 725 727 729 728 730 732 731 733 735 734 736 738 737 739 741 740 742 744 743 745", "Algorithm 1 : ESANS": "Input: A large item pool I with multimodal representation ( R I , R T , R G ) . Training set R = { ( u, i ) \u2208 ( U , I ) } , embedding dimension K ; Output: Multimodal-aligned Model ( H I T G 2 Vector Quantization Codebook ( C p , C s ) . , H , H ) . Final user embeddings { u u | u \u2208 U} and item embeddings { v i | i \u2208 I} ; 1: Initialize ( H I , H T , H G ) , ( C p , C s ) , { u u | u \u2208 U} and { v i | i \u2208 I} ; 2: for t = 1 To T do 3: Sample a mini-batch I batch \u2208 I of size B ; 4: Get the output embedding after aligning ( M batch , M batch , M batch ) from Equation (3); I T G 5: Get the embedding ( R batch p , R batch s ) from Equation (5) and Equation (6); 6: Update Models ( H I , H T , H G ) and codebooks ( C p , C s ) based on Loss function (8).", "7: end for": "8: for t = 1 To T do 9: Sample a mini-batch R batch \u2208 R of size B ; 10: for each ( u, i ) \u2208 R batch do 11: Get the prime C i p and secondary cluster C i s of item i ; 12: Sample m c prime clusters base on the Equation (9); 13: Uniformly sample m o items from each prime cluster k and m h hard negatives; 14: for k = 1 To m c do 15: for j = 1 To n o do 16: Get the virtual hard negatives \u02dc v a s k based on the Equation (11); 17: end for 18: end for 19: for j = 1 To m h do 20: Get the virtual hard negatives \u02dc v a h based on the Equation (13); 21: end for 22: Update embeddings ( u u , v i ) based on gradient w.r.t. (14); 23: end for 24: end for 746", "748 749 B Graph Construction": "747 750 751 752 753 754 755 756 757 758 759 and bring them home while intercepting invaders to defend the home. Agents have 5 actions and observe entities within 0.2 units. Agents can only hold one resource at a time, so they must bring the holding resource home before collecting another one. Invaders periodically appear and move to the home. Episodes last 145 timesteps. In resource collection, we implement the default settings of COPA and AQMIX in Liu et al. (2021) and we use the configuration of REFIL in Shao et al. (2022) with the MIT license. For the implementation of CORD, the details are the same in MPE (C.1) except we train all methods for 10M timesteps. The environment and model are implemented in Python. All models are built on PyTorch and are trained For each dataset, we pretrain a heterogeneous graph network [32] based on user behaviors. The types of graph nodes include user, item, and its side information (brand / category / price features for Amazon Review dataset, tag / statistical features for Pixel-Rec dataset and brand / shop / category for industrial datasets). The graph edges include: 1) user-item edge. If user \ud835\udc62 clicks item \ud835\udc56 , there on a machine with 1 Nvidia GPU (GTX 1080 TI) and 12 Intel CPU Cores. is an edge between \ud835\udc62 and \ud835\udc56 . 2) user-side information edge. If user \ud835\udc62 clicks an item with side information \ud835\udc63 (e.g., shop), there is an edge between \ud835\udc62 and \ud835\udc63 . 3) item-item edge. If item \ud835\udc56 and item \ud835\udc57 are adjacent in user behavior sequence and the time interval between item \ud835\udc56 and item \ud835\udc57 is within 60 seconds, there is an edge between \ud835\udc56 and \ud835\udc57 . 4) item-side information edge. If item \ud835\udc56 has a side info \ud835\udc63 , there is an edge between \ud835\udc56 and \ud835\udc63 .", "C Parameter Settings": "In this section, we elaborate on the parameter settings for the implementation of our algorithm. To ensure computational manageability, we limit the length of user behavior sequences to 10 for the Amazon Review dataset, 32 for the Pixel-Rec dataset and 64 for the Industrial dataset. The training process is implemented using a distributed TensorFlow[1] platform, consisting of 10 parameter servers and 40 workers with 12 CPUs per worker. It is worth noting that the performance of our method can be further enhanced as the sampling scale increases, as shown in Table 5. To ensure fairness between our ESANS and the baseline models, in the negative sampling process, for each in-batch positive sample, we randomly select \ud835\udc5a \ud835\udc50 = 2 clusters and then draw \ud835\udc5a \ud835\udc5c = 5 negative samples from each of these clusters. In contrast, the baseline models select 10 negative samples randomly for each positive sample. These negatives are sampled based on an online sampling framework in the training process and shared across the batch. Additionally, the interpolation coefficient \ud835\udf06 of hard negatives is set to 0.1 for harder interpolation and -0.1 for easier interpolation. The rest of the hyperparameters settings are demonstrated in Table 6. 761 760 762 C.3. SMAC 763 765 764 766 768 767 769 In StarCraft II, for CORD, we use a learning rate of 5 10 4 causal inference in role \u03bb role heterogeneity \u00d7 - . The hyperparameters of c and \u03bb the implementation of CORD is the same as it in MPE. For AQMIX and REFIL, we implement the default configurations d are fixed as 0.0025 throughout the 6M training timesteps for all maps. Except the above three parameters, for each scenario. Our implementation of CORD and COPA derives from REFIL (Iqbal et al., 2021) with the MIT license. with 4 Nvidia GPUs (A100) and 224 Intel CPU cores. The environment and model are configured in Python. All models are constructed utilizing PyTorch and trained on a system 14"}
