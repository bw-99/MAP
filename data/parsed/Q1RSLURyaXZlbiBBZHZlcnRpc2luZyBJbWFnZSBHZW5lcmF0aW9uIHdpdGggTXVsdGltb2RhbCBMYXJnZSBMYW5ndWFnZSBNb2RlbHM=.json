{
  "CTR-Driven Advertising Image Generation with Multimodal Large Language Models": "Xingye Chen ∗† Huazhong University of Science and Technology Wuhan, China chenxingye@hust.edu.cn Wei Feng † JD.COM Beijing, China fengwei25@jd.com Weizhen Wang JD.COM Beijing, China wangweizhen5@jd.com Yanyin Chen JD.COM Beijing, China chenyanyin6@jd.com Jinyuan Zhao JD.COM Beijing, China zhaojinyuan1@jd.com Haohan Wang JD.COM Beijing, China wanghaohan1@jd.com Yu Li JD.COM Beijing, China liyu1078@jd.com Junjie Shen JD.COM Beijing, China shenjunjie@jd.com Zhangang Lin JD.COM Beijing, China linzhangang@jd.com Xinge You Huazhong University of Science and Technology Wuhan, China youxg@hust.edu.cn Zhenbang Du ∗ Huazhong University of Science and Technology Wuhan, China dzb99@hust.edu.cn",
  "Linkai Liu ∗": "Sun Yat-sen University Shenzhen, China liulk6@mail2.sysu.edu.cn Zheng Zhang JD.COM Beijing, China zhangzheng11@jd.com Jingping Shao JD.COM Beijing, China shaojingping@jd.com Yaoyu Li JD.COM Beijing, China liyaoyu1@jd.com Jingjing Lv JD.COM Beijing, China lvjingjing1@jd.com",
  "Yuanjie Shao ‡": "Huazhong University of Science and Technology Wuhan, China shaoyuanjie@hust.edu.cn Changxin Gao Huazhong University of Science and Technology Wuhan, China cgao@hust.edu.cn",
  "Abstract": "In web data, advertising images are crucial for capturing user attention and improving advertising effectiveness. Most existing methods generate background for products primarily focus on the aesthetic quality, which may fail to achieve satisfactory online performance. To address this limitation, we explore the use of Multimodal Large Language Models (MLLMs) for generating advertising images by optimizing for Click-Through Rate (CTR) as the primary objective. Firstly, we build targeted pre-training tasks, and leverage a large-scale e-commerce multimodal dataset to equip MLLMs with ∗ Work done while interning at JD.COM. † Both authors contributed equally to this research. ‡ Corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WWW'25, Sydney, NSW, Australia © 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-1274-6/25/04 https://doi.org/10.1145/3696410.3714836 Nong Sang Huazhong University of Science and Technology Wuhan, China nsang@hust.edu.cn initial capabilities for advertising image generation tasks. To further improve the CTR of generated images, we propose a novel reward model to fine-tune pre-trained MLLMs through Reinforcement Learning (RL), which can jointly utilize multimodal features and accurately reflect user click preferences. Meanwhile, a product-centric preference optimization strategy is developed to ensure that the generated background content aligns with the product characteristics after fine-tuning, enhancing the overall relevance and effectiveness of the advertising images. Extensive experiments have demonstrated that our method achieves state-of-the-art performance in both online and offline metrics. Our code and pre-trained models are publicly available at: https://github.com/Chenguoz/CAIG.",
  "CCS Concepts": "· Computing methodologies → Computer vision .",
  "Keywords": "CTR-Driven, Advertising Image Generation, Online Advertising, Multimodal Large Language Models",
  "ACMReference Format:": "Xingye Chen, Wei Feng, Zhenbang Du, Weizhen Wang, Yanyin Chen, Haohan Wang, Linkai Liu, Yaoyu Li, Jinyuan Zhao, Yu Li, Zheng Zhang, Jingjing WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia Xingye Chen, et al. Lv, Junjie Shen, Zhangang Lin, Jingping Shao, Yuanjie Shao, Xinge You, Changxin Gao, and Nong Sang. 2025. CTR-Driven Advertising Image Generation with Multimodal Large Language Models. In Proceedings of the ACM Web Conference 2025 (WWW '25), April 28-May 2, 2025, Sydney, NSW, Australia. ACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/3696410. 3714836",
  "1 Introduction": "Advertising images play a pivotal role in attracting user attention and boosting advertising efficacy [18, 32]. Recent advancements in image generation techniques, particularly the integration of Stable Diffusion [38] and ControlNet [53], have enabled the creation of harmonious and realistic backgrounds for product images. However, most existing advertising image generation approaches [23, 44, 46, 54] primarily focus on offline metrics, such as image quality or semantic consistency, without fully considering the critical connection between visual content and online performance metrics like Click-Through Rate (CTR). This results in a notable discrepancy between the generated advertising images and the ideal images that align with actual user preferences. Inspired by recent approaches [9, 21, 48, 51] that incorporate Reinforcement Learning from Human Feedback (RLHF) [8, 31, 42] to align with human preferences, we can adopt a two-stage method to better capture online user preferences. The first stage involves collecting and analyzing online user feedback to train a Reward Model (RM) that accurately simulates user preferences in the ecommerce domain. In the second stage, we employ Reinforcement Learning (RL) algorithms to fine-tune the generation model, with the RM providing rewards to guide the optimization process. A critical aspect of this pipeline is the RM's ability to accurately reflect users' click preferences for images. However, previous methods that incorporate visual content for CTR prediction face two major limitations: First, applying different backgrounds to the same product can lead to significantly different CTR outcomes, as illustrated in Figure 1 (a). Existing methods [10, 25, 45, 51] often rely on models with limited image understanding capabilities, such as CNNs, vision transformers, or embedding-based methods. To compensate for this deficiency, these methods typically require incorporating numerous auxiliary tasks, such as object detection and OCR, which leads to additional annotation costs and labor-intensive data preparation processes. Second, integrating diverse yet crucial features from multiple modalities (such as product titles and attributes) is of paramount importance, as these significantly influence product CTR. Nevertheless, current methods primarily focus on dense visual features and require additional complex modules to fuse different types of features, potentially limiting the model's adaptability to the rapidly changing online advertising environments. For instance, products from distinct categories, such as water bottles and office chairs illustrated in Figure 1 (a), exhibit remarkably different baseline CTR due to their disparate nature and associated consumer behavior patterns. To address these issues, leveraging the advanced multimodal understanding and representation capabilities of MLLMs [27, 28, 50] offers a promising solution. On the one hand, these models excel in zero-shot visual analysis, encompassing image representation [15, 29], object detection [22, 52], and various visual tasks without requiring task-specific training. On the other hand, by transforming sparse features (such as categories, tags, or other attributes) into natural language descriptions, MLLMs can process and reason about this textual information alongside visual data, offering a simpler paradigm for integrating multimodal information. While the introduction of MLLMs can effectively guide generation models to produce backgrounds with higher CTR, it is crucial to consider the relationship between the background and the product in advertising image generation. Existing RL algorithms [20, 21, 49] focus solely on optimizing rewards, neglecting the crucial balance between visual appeal and contextual appropriateness. This oversight can result in disharmonious backgrounds that mislead users and lead to poor shopping experiences. As illustrated in Figure 1 (b), while dynamic, sports-oriented backgrounds might boost CTR for athletic shoes, the model might erroneously apply similar backgrounds to unrelated products like cosmetics, compromising visual harmony and product relevance. In this work, we propose a novel method called C TR-driven A dvertising I mage G eneration (CAIG), which leverages the MLLMs as core components to generate advertising images that are both CTR-optimized and coherent with product characteristics. As illustrated in Figure 2, we first design targeted pre-training tasks that utilize a large-scale e-commerce multimodal dataset to equip MLLMs with comprehensive e-commerce domain knowledge for generating advertising images. To further optimize the CTR of generated images, we propose a novel RM that transforms the traditional CTR prediction task into a binary classification problem, enabling the selection of positive and negative samples in subsequent RL process. By focusing on the relative performance between image pairs, our method can effectively mitigate the impact of absolute CTR variations across different product categories. Lastly, to avoid generating background-irrelevant advertisement images, we develop a Product-Centric Preference Optimization (PCPO) strategy. This strategy uses multimodal information of the product as the sole variable and constructs additional preference pairs, forcing the MLLM to generate background content that aligns with the product's characteristics during the RL process. To the best of our knowledge, this is the first work that utilizes MLLMs for CTR-driven advertising image generation. We summarize our contributions as three-folds: · We design targeted pre-training tasks using a large-scale e-commerce multimodal dataset to equip MLLMs with comprehensive domain knowledge, providing them with foundational capabilities for downstream tasks. · We propose a two-branch RM that combines the powerful image understanding capabilities of MLLMs with multimodal product information fusion to effectively simulate human click preferences in e-commerce scenarios. · We develop a product-centric preference optimization strategy, compelling the model to focus on the product's intrinsic information to generate both visually appealing and contextually consistent advertising images. Extensive experiments on both public and commercial datasets demonstrate that our method achieves state-of-the-art performance across multiple key metrics, significantly improving online CTRs in real-world e-commerce scenarios. CTR-Driven Advertising Image Generation with Multimodal Large Language Models WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia Before CTR-driven After CTR-driven Before CTR-driven After CTR-driven CTR:0.023 CTR:0.019 CTR:0.013 CTR:0.017 CTR:0.012 CTR:0.009 showcase showcase Attributes Caption … Tags Category Views Multimodal product information Attributes Caption … Tags Category Views Multimodal product information 1 1 6 (a) (b) Figure 1: (a) Example of the impact of different backgrounds on product CTR. While visual features play a crucial role, other modalities such as textual caption and product attributes also have a significant influence on CTR. (b) Examples of productbackground mismatches using existing reinforcement learning algorithms.",
  "2 Related Works": "",
  "2.1 Advertising Image Generation": "The primary goal of advertising image generation is to create natural and contextually relevant images while preserving the integrity and identity of the original product. Initially, template-based methods [6, 32, 47, 47] were employed for assembling advertising images, offering high efficiency but lacking personalization and flexibility. With the advent of generative adversarial networks (GANs) [12], researchers began exploring more flexible and automated approaches to advertising image creation. Ku et al. [18] introduced a novel approach of using GAN models as retrieval-assisted techniques for enhancing product images in advertising contexts. More recently, diffusion models have shown promise in producing high-quality, realistic ad images. InsertDiffusion [33] introduced a training-free diffusion architecture that effectively embeds objects into images while preserving their structural and identity features. Recognizing that ad quality involves multiple aspects such as aesthetics and text-image consistency, researchers have begun exploring multistage optimization methods [5, 21, 24]. A notable example is VirtualModel [5], which employs a multi-branch structure to enhance the credibility of human-object interactions and ensure consistency in generation quality. Unlike previous methods primarily focusing on visual quality or text-image consistency, our method uniquely leverages MLLMs to generate CTR-optimized contextual descriptions, guiding diffusion models to produce visually appealing and product-specific advertising images.",
  "2.2 Click-Through Rate Prediction": "Click-Through Rate (CTR) prediction plays a crucial role in online advertising and recommendation systems, directly impacting user experience and revenue generation. In the context of CTR-driven advertising image generation, precise CTR estimation enables more effective selection and positioning of visual content, thereby enhancing the overall performance of online advertising campaigns. The advent of deep learning has revolutionized traditional CTR prediction [16, 17, 19], enabling models to automatically learn hierarchical feature representations from raw input data. This paradigm shift not only improved the performance of textual or numericalbased CTR prediction methods [16, 19] but also paved the way for incorporating visual elements into the prediction process. For instance, Wang et al. [45] proposed a hybrid bandit approach that integrates visual priors with a dynamic ranking mechanism, demonstrating the potential of incorporating visual information in CTR prediction models. Recognizing that real-world advertisements are inherently multimodal, comprising text, visuals, and other data types, researchers have begun to explore methods that can effectively integrate these diverse modalities. CG4CTR [51] leveraged a multi-head self-attention module to jointly process textual and visual information from multimodal advertisements, extracting rich features for more accurate CTR estimation. However, these approaches often struggle with complex image understanding tasks and fail to effectively integrate multimodal information. Therefore, it is imperative to explore a more robust CTR estimation method that can seamlessly interpret visual content and harmoniously fuse information from multiple modalities.",
  "2.3 Learning from Human Feedback": "Reinforcement Learning from Human Feedback (RLHF) [4, 34, 56] involves collecting human feedback on model outputs. This feedback is then used to optimize the generation model using reinforcement learning algorithms such as PPO[39] or DPO [37]. For example, Lee et al. [20] proposed a three-stage fine-tuning method to improve text-image alignment in text-to-image (T2I) models using human feedback and reward-weighted likelihood maximization. Wu et al.[49] introduced a human preference score derived from a classifier trained on human-curated image choices, which is then utilized to adapt T2I models. Parrot [21] proposed a multi-reward RL approach that jointly optimizes the T2I model and prompt expansion network to improve image quality. However, current preference optimization methods for image generation, while showing WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia Xingye Chen, et al. promise in text-to-image (T2I) tasks, face significant challenges when applied to scenarios with strict visual requirements, such as advertising background generation. These methods often focus solely on optimizing specific metrics, neglecting the contextual relevance and visual harmony of the generated content. Therefore, our method emphasizes exploring optimization techniques that enable the model to effectively integrate multimodal information to generate diverse and coherent background descriptions that better align with user preferences.",
  "3 Method": "",
  "3.1 Overview": "In this work, we introduce a novel method called C TR-Driven A dvertising I mage G eneration (CAIG), designed to generate compelling advertising images that capture user interest, as shown in Figure 2. We first pre-train the MLLM on a large-scale multimodal e-commerce dataset, injecting domain-specific knowledge into the model. This serves as the foundation for our Prompt Model (PM) and Reward Model (RM). Then, we initialize the RM from the pretrained MLLM and further train it on extensive multimodal online user click data, enabling the RM to simulate human feedback. Finally, we introduce a CTR-driven preference optimization stage, which adopts Product-Centric Preference Optimization (PCPO) as its core strategy, detailed in Algorithm 1. This stage uses the RM's feedback to fine-tune the PM, ultimately generating advertising images that balance attractiveness and relevance.",
  "3.2 E-commerce Knowledge Pre-training": "To address the challenge of efficient and scalable advertising creative generation, we leverage the power of MLLMs by injecting domain-specific e-commerce knowledge through pre-training on a large-scale multimodal e-commerce dataset comprising 1.2M samples from major e-commerce platforms, as shown in Figure 2 (a). Specifically, the pre-training tasks involve three main tasks: (1) Image Understanding: Describing the products or backgrounds based on product images. (2) Multimodal Content Comprehension: Describing product background or generating product titles based on multimodal product information (e.g., titles, categories, tags). (3) Prompt Generation: Generating or rewriting description prompts based on multimodal product information. To facilitate the model's understanding of product information, we design an instruction function that elegantly integrates diverse product attributes into a unified, semantically rich description. Formally, this can be expressed as:  where 𝐶 is the instruct prompt constructed by the instruct function 𝑓 𝑖𝑛𝑠𝑡𝑟𝑢𝑐𝑡 from 𝑛 individual product attributes i = [ 𝑖 1 , 𝑖 2 , ..., 𝑖 𝑛 ] (such as title, category, price, etc.), 𝑄 is the task-specific question. For example, an instruct statement for a specific product might be formulated as: \"Generate a suitable product background description based on the following attributes: Product Title: 'Wireless Bluetooth Earbuds', Product Category: 'Electronics', Price: '$49.99', Customer Rating: '4.5 stars', Color Options: 'Black, White, Blue'\" . Algorithm 1 CTR-Driven Preference Optimization Input: 𝑁 - Number of training epochs 𝑀 - Number of products PM Θ - Pre-trained Prompt Model RM Θ - Pre-trained Reward Model 1: for epoch 𝑖 = 1 to 𝑁 do 2: 𝑃 ←∅ Initialize set for positive and negative sample pairs 3: for product 𝑗 = 1 to 𝑀 do 4: ( 𝐼 𝑜 , 𝐶 ) ← Get product image and instruct prompt 5: ( 𝑦 1 , 𝑦 2 ) ← PM Θ ( 𝐼 𝑜 , 𝐶 ) Generate two background descrip- tions 6: ( 𝐼 1 , 𝐼 2 ) ← Generate advertising images using Stable Diffu- sion and ControlNet with 𝐼 𝑜 and ( 𝑦 1 , 𝑦 2 ) 7: ( 𝑝 1 , 𝑝 2 ) ← RM Θ ([ 𝐼 1; 𝐼 2 ] , 𝐶 ) Predict relative CTR by RM 8: ( 𝑦 + , 𝑦 - ) ← ( 𝑦 1 , 𝑦 2 ) if 𝑝 1 > 𝑝 2 else ( 𝑦 2 , 𝑦 1 ) 9: 𝑃 ← 𝑃 ∪ {( 𝐼 𝑜 , 𝐶, 𝑦 + , 𝑦 - )} Insert preference pair 10: end for 11: Update PM Θ using 𝑃 with L DPO + L PCPO (Equation 12) 12: end for Output: PM Θ - Fine-tuned Prompt Model By leveraging the power of MLLMs and our specialized pretraining tasks, our MLLM gains a deep understanding of e-commerce products and their attributes. This understanding lays a solid foundation for vision-based CTR prediction and advertising image generation in subsequent tasks, enabling the creation of more relevant and engaging visual content for e-commerce advertising.",
  "3.3 Reward Model based on MLLM": "To optimize the alignment between generated advertising images and online user preferences, we leverage user feedback data to train a RM for fine-tuning the advertising image generation pipeline. First, our method utilizes the strong visual representation capabilities and flexible multimodal input of the MLLM which is pre-trained with e-commerce knowledge to extract robust product features. Furthermore, to mitigate the impact of absolute CTR variations across different product categories, we reformulate the CTR regression task into a relative comparison task between pairs of images, as illustrated in Figure 2 (b). Specifically, we construct pair-wise training samples from user click data, where each pair consists of two advertising images for the same product with corresponding CTRs. For each product pair ( 𝐼 1 , 𝐼 2 ) sharing common attributes i = [ 𝑖 1 , 𝑖 2 , ..., 𝑖 𝑛 ] , we first create an instruct prompt 𝐶 RM by integrating the product attributes with the RM-specific question template 𝑄 RM through the prompt engineering function 𝑓 instruct . The multimodal input is then formed by concatenating the visual representations of both images with the textual prompt embedding, which can be formally expressed as:   where 𝑓 vision and 𝑓 text denote the vision and text encoders respectively. The LLM processes this combined representation to produce hidden states 𝐻 ∈ R 𝑙 × 𝑑 , with 𝑑 representing the hidden dimension and 𝑙 the sequence length. Following standard practice in sequence CTR-Driven Advertising Image Generation with Multimodal Large Language Models WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia Figure 2: (a) E-commerce knowledge pre-training. The MLLM is pre-trained on a large-scale multimodal e-commerce dataset to incorporate domain-specific knowledge. (b) The Structure of RM. The RM integrates multimodal product features using visual and textual encoders, with dual branches to estimate CTR and identify appealing ad images. (c) CTR-driven preference optimization stage. The PM generates background descriptions for background generation model to create product images with various backgrounds. The RM then estimates the CTR for these images, simulating human feedback to optimize the PM. Describe the product image Optimize the product description prompt Generate a suitable product background description Generate a suitable product background description (c) MLLM (b) (a) a monitor sits on a wooden tabletop … modern minimalist shelf, wooden desk, assorted potted plants … soft pink cherry blossoms, ethereal white silk ribbons … … … minimalist modern office interior, dark green walls … Instructions Answers wooden desk surface, warm lighting, bokeh effect, shallow depth of field luxury texture, high-end product display, subtle lighting, dark gradient background Prompt Model Generate a suitable product background description based on the < Product Caption > , < Product Image > and < Product Attributes > Description 1 Description 2 Reward Model Generated Image 1 Generated Image 2 Policy Gradient Update Background Generation Model Description Product Image Stable Diffusion ControlNet Output Background Generation Model Inpainting Concatenate Multimodal product information Attributes Caption … Tags Category Views Instruct Prompt Judge whether the left picture or the right picture is more attractive based on the < Product Caption > , < Concatenated Image > and < Product Attributes > Concatenated Image Text Encoder Vision Encoder LLM Projection Classification Head CTR Regression Head hidden state Reward Model Input Judge whether the left picture or the right picture is more attractive based on the < Product Caption > , < Concatenated Image > and < Product Attributes > Product-Centric Preference Optimization PMo(ytlIoC) PMo(ytlIo log o Blog y+|o,C) Blog PMref classification with LLMs [43], we leverage the hidden state of the final token ℎ ∈ R 𝑑 as the discriminative representation, capturing the cumulative contextual information of the complete input sequence. Additionally, to enable the model to predict the CTR of the left and right images in a composite image with fine-grained accuracy, we introduce a point-wise loss using a separate CTR regression branch: Subsequently, we transform the CTR regression task into a binary classification problem that directly compares the relative CTR performance between the left and right images in each pair. A classification head 𝐹𝐶 cls is employed to map the final token's hidden state ℎ to a two-dimensional probability distribution 𝑝 ∈ R 2 :  To train the RM, we initialize it with pre-trained weights infused with e-commerce domain knowledge and utilize the binary crossentropy loss function for training. The loss function is defined as:  where 𝑁 is the number of training samples, 𝑡 𝑖 ∈ {[ 1 , 0 ] , [ 0 , 1 ]} indicates whether the left or right side of the concatenated image has a higher CTR, and 𝑝 𝑖 is the predicted probability distribution.  where 𝐹𝐶 ctr represents the fully connected layer for CTR regression, 𝐹𝐶 ctr ( ℎ 𝑖 ) represents the predicted CTR values for the 𝑖 -th image pair, and ˆ 𝑡 𝑖 ∈ R 2 corresponds to the true CTRs for the left and right images in the pair. The final loss function for training the RM is a combination of the binary cross-entropy loss and the PointLoss:  where 𝜆 1 and 𝜆 2 are hyperparameters that balance the contribution of each loss component. This combined design of two components enables the model to learn the relative CTR of comparative advertising images during the training phase while incorporating absolute CTR as an auxiliary input. During the inference stage, we utilize WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia Xingye Chen, et al. the comparison results from the classification head as the basis for comparing CTR.",
  "3.4 Product-Centric Preference Optimization": "We formulate the task of generating higher CTR advertising images as a preference selection problem, encouraging the advertising generation model to choose higher attractive positive images 𝐼 + and reject less attractive negative images 𝐼 -. This process involves two key steps: (1) generating image pairs and comparing their CTR with RM, (2) fine-tuning the generation model based on the feedback from the RM, as illustrated in Algorithm 1. For advertising image generation, we utilize the background description 𝑦 generated by our PM as input to Stable Diffusion [38], along with the original product image 𝐼 𝑜 . We employ ControlNet [53] and inpainting techniques [30] to seamlessly integrate the product into the generated background. The process uses DDIM [41] as the denoising schedule, where the latent representation 𝑥 𝑡 at step 𝑡 is calculated as :  where 𝜖 𝜃 ( 𝑥 𝑡 + 1 , 𝑦 ) represents the noise predicted by the model [38, 53], and ¯ 𝛼 is a set of coefficients controlling the forward noiseadding process. The latent representation 𝑥 𝑡 is processed by:  where 𝑥 𝑜 is the latent of 𝐼 𝑜 , 𝑰 represents an identity matrix, 𝑴 is product mask and ⊗ denotes the element-wise multiplication. The final latent 𝑥 0 is then converted to the generated image 𝐼 𝑔 . Considering that collecting real CTR feedback is time-consuming and resource-intensive, we leverage the RM to distinguish in realtime between more attractive 𝐼 + and less attractive 𝐼 -generated images to fine-tune the generation pipeline. Similar to Parrot [21], we empirically find that fine-tuning the background generation model has a much smaller impact on the image content compared to changing the background description. Therefore, to enhance training efficiency, we focus solely on fine-tuning the PM to choose higher attractive background descriptions 𝑦 + and reject less attractive ones 𝑦 -. The Direct Preference Optimization (DPO) [37] is then adopted as our fundamental strategy due to its simplicity and efficiency. Specifically, given an optimization policy model 𝑃𝑀 𝜃 and a reference model 𝑃𝑀 ref , the DPO objective is:  where ( 𝐼 𝑜 , 𝐶 ) represent the original product image and corresponding instruct prompt. 𝜎 is the sigmoid activation function, 𝛽 is a regularization parameter. During the DPO process, the reference model 𝑃𝑀 ref is frozen to optimize the policy model 𝑃𝑀 𝜃 . It is worth noting that excessive focus on CTR optimization during DPO training may ignore the product information in preference data, causing a mismatch between the foreground and background in the generated image. Therefore, we introduce the ProductCentric Preference Optimization (PCPO). The core mechanism of PCPO is to control product information as the sole variable during the training process and construct additional preference data pairs, thereby encouraging the model to generate background descriptions that match the product characteristics. Specifically, given a matched ( 𝑦 + , 𝐼 𝑜 , 𝐶 ) and a mismatched ( 𝑦 + , d 𝐼 𝑜 , 𝐶 ) , the PCPO objective is formulated as:  We consider two strategies to construct a product information ( d 𝐼 𝑜 , 𝐶 ) that mismatches 𝑦 + : (1) visual-aware optimization: randomly masking 75% of input product images. (2) textual-aware optimization: randomly selecting and replacing textual information from other products. These strategies are designed to create hard negative samples that are not fully compatible with 𝑦 + while retaining some common features with the original input. The total objective is a combination of the standard DPO and PCPO:  Finally, we utilize the fine-tuned PM to generate background descriptions for products. These descriptions are then fed into the background generation model to create product advertising images suitable for online environments.",
  "4 Experiments": "",
  "4.1 Experimental Setup": "Datasets: For training and validating our RM, we conduct experiments on both public and commercial datasets. The public dataset [45] covers 500K product samples with 1.2M unique advertising images. The CTR data for public dataset is collected over an average period of 10 days for each creative placement. Our commercial dataset, collected from a well-known e-commerce platform, contains 1M product samples with 3.4M unique advertising images. For commercial dataset, the CTR data is collected over a one-month period. It is worth noting that our commercial dataset contains more detailed product information, including titles, categories, tags, and other relevant attributes. To ensure the quality and reliability of our training and test data, we apply specific criteria to both datasets. To improve the confidence of CTR estimates, we require each image to have a minimum exposure threshold (E). Additionally, to ensure distinguishable CTR differences within pairs, we require the relative CTR difference between paired images to exceed a certain threshold (D). For the training set, we set E = 50 and D = 1%, while for the test set, we apply more stringent criteria with E = 1,000 and D = 5%. After applying these preprocessing steps, the public dataset yields 890K training pairs and 1,034 test pairs, while our commercial dataset contains 1.15M training pairs and 1,528 test pairs. In the CTR-driven preference optimization stage, we fine-tune our generation pipeline using a dataset of 32K samples, which includes original product images along with multimodal information. These product samples are also collected from the same e-commerce platform, ensuring consistency in data source and characteristics. Models: We employ the LLaVA-v1.6-7B [28] as our foundation MLLMs, which utilizes Vicuna-7B [7] as the text encoder and CLIPViT-L/14-336 [36] as the vision encoder. For our background generation model, we use MajicmixRealistic-v7 1 as the base model, enhanced with ControlNet-v1.1 [53] 2 to provide finer control over the generated images. 1 https://civitai.com/models/43331/majicmix-realistic 2 https://github.com/lllyasviel/ControlNet CTR-Driven Advertising Image Generation with Multimodal Large Language Models WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia GLM4V Claude3.5 Sonnet GPT4o GPT4V LLaVA VAM CG4CTR Ours 40.0 42.5 45.0 47.5 50.0 52.5 55.0 57.5 60.0 Pair Accuracy 49.6% 48.7% 50.6% 49.7% 53.3% 53.2% 54.0% 58.6% Closed-source Open-source Ours (a) Commercial data Figure 3: Comparison of Pair Accuracy across different methods on commercial and public datasets. GLM4V Claude3.5 Sonnet GPT4o GPT4V LLaVA VAM CG4CTR Ours 40.0 42.5 45.0 47.5 50.0 52.5 55.0 57.5 60.0 Pair Accuracy 49.6% 50.1% 50.2% 49.8% 52.8% 52.2% 53.1% 56.2% Closed-source Open-source Ours (b) Public data [45] Implementation Details: For the pre-training task of MLLMs, we perform full model fine-tuning. We optimize the learning process over 10 epochs using a cosine learning rate scheduler with an initial learning rate of 2e-6. The pre-training task takes approximately 5 days to complete. We then initialize the RM with the pre-trained weights and employ the same learning strategy to train on massive user click data, simulating user feedback. The hyperparameters 𝜆 1 and 𝜆 2 are set to 1 and 0.5, respectively. Finally, in the CTR-driven preference optimization stage, we utilize the frozen RM to drive the proposed advertising image generation pipeline. We employ LoRA [14] fine-tuning with a learning rate of 2e-5. This phase consists of 5 epochs and takes about 20 hours to complete. All experiments are conducted on a machine equipped with 8 NVIDIA A100 GPUs. capabilities, are not specifically tuned for CTR regression tasks in advertising contexts. Open-source models like VAM [45] and CG4CTR [51], while showing slight improvements, still demonstrate limited performance due to their weak visual representation capabilities and inability to effectively integrate multimodal information. In contrast, our proposed method, which leverages MLLM, achieves state-of-the-art performance on both commercial and public datasets. By effectively combining visual and textual modalities of product information, our method demonstrates superior ability in predicting relative CTR performance between image pairs. Specifically, our method achieves a Pair Accuracy of 58.6% on commercial data and 56.2% on public data, significantly outperforming all baseline models.",
  "4.2 Analysis on Reward Model": "4.2.1 Evaluation Metric. To evaluate the performance of our RM, we introduce the Pair Accuracy metric, defined as:  where 𝑁 is the total number of image pairs, 𝑝 𝑖 is the predicted probability distribution for the i-th pair, 𝑦 𝑖 is the ground truth label, and 1 is the indicator function. It is worth noting that the task of CTR comparison is highly challenging, where even small improvements can lead to significant economic benefits. 4.2.2 Comparison with State-of-the-Art Methods. We conduct extensive experiments on both commercial and public datasets, comparing our method with various state-of-the-art open-source and closed-source models based on MLLMs, as shown in Figure 3. The open-source models are fine-tuned on the corresponding datasets to ensure a fair comparison. For closed-source models, we provide them with the same instructions and image pairs as our RM, then convert their textual responses into predicted labels. From the results, we can observe that existing closed-source models (GLM4V [11], Claude3.5 Sonnet [2], GPT4o [3], and GPT4V [1]) lack the ability to effectively compare the CTR of advertising images, as evidenced by their near-random performance (around 50% Pair Accuracy). This suggests that these models, despite their general 4.2.3 Ablation Study. To further analyze the contribution of each component in our proposed RM, we conduct a detailed ablation study on the commercial dataset, with results shown in Table 1. We start with the base LLaVA-v1.6-Vicuna-7B [28] and progressively add key components to observe their impact on model performance. First, we observe that incorporating a pre-training step with ecommerce domain knowledge increases the Pair Accuracy from 53.3% to 54.4%. This suggests that domain-specific pre-training provides a good starting point for the model, enhancing its baseline understanding of e-commerce concepts and product characteristics. A more substantial improvement is seen when replacing the original output layer with a dedicated classification head, which boosts the Pair Accuracy to 56.4%. This notable increase can be attributed to the classification head's ability to enable the model to learn explicit classification boundaries, thereby reducing the ambiguity often associated with natural language outputs in the original model architecture. Incorporating product captions and additional product information further enhances the model's accuracy to 58.2%. This demonstrates the importance of supplementary product data in improving the model's ability to compare advertising image attractiveness. Finally, we add an extra CTR regression branch and introduce the point loss, which improve the final performance to 58.6%. This enhancement demonstrates that by directly incorporating CTR values into the training objective, the model can more accurately capture subtle differences in CTR, thereby further improving prediction accuracy. Xingye Chen, et al. WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia Table 1: Ablation study for the reward model.",
  "4.3 Analysis of Product-Background Matching": "4.3.1 Evaluation Metric. Existing preference optimization methods focus solely on optimizing rewards, which may neglect the crucial balance between visual appeal and contextual appropriateness. To quantify the impact of different optimization methods on the compatibility between foreground products and generated backgrounds, we introduce the match rate metric. We calculate the match rate by randomly selecting 1,000 products and generating backgrounds for each using the generation models under evaluation. Experienced advertising professionals then assess whether the foreground and background are compatible based on comprehensive product information, considering factors such as style consistency, color harmony, and contextual appropriateness. Detailed annotation guidelines and criteria are provided in Appendix A.2. 4.3.2 Comparison with Standard DPO. To ensure a fair comparison, we evaluate PCPO against standard DPO during preference optimization, using identical RM for CTR feedback and equal training epochs. Figure 4 illustrates the performance of both methods over training epochs. Notably, the standard DPO experiences a significant drop in match rate, declining from 0.842 to 0.597 after 5 epochs of training. In contrast, our PCPO demonstrates a more gradual decline in match rate, maintaining a higher value of 0.798 at the 5th epoch, which represents a 33.7% relative improvement over DPO at the same stage of training. Additionally, we showcase several examples in Figure 5 where the standard DPO produces images with mismatched foreground and background elements, highlighting the effectiveness of our PCPO in preserving product-context coherence throughout the optimization process. 4.3.3 Effectiveness of E-commerce Knowledge Pre-training. Asshown in Figure 4, we compare the performance of our pre-trained model with the original LLaVA model without pre-training (indicated by asterisks). The results demonstrate a significant improvement in match rate after injecting e-commerce knowledge through pretraining, with our model achieving a score of 0.842 compared to LLaVA's 0.753. This performance gap highlights that our pretraining strategy provides a strong initialization point for the subsequent preference optimization process, underscoring the importance of domain-specific knowledge in MLLMs. 4.3.4 Ablation Studies. To further validate the effectiveness of our method, we conduct ablation studies on two key components of PCPO: PCPO without textual-aware optimization (w/o textual) and PCPO without visual-aware optimization (w/o visual), as illustrated in Figure 4. Both ablation variants show improvements over standard DPO but fall short of the full PCPO method. The \"w/o textual\" and \"w/o visual\" variants highlight the importance of both textual and visual components in our method. These results emphasize 0 1 2 3 4 5 Epoch 60.0% 65.0% 70.0% 75.0% 80.0% 85.0% Match Rate (%) LLaVA DPO w/o textual w/o visual Ours Figure 4: Comparison of Match Rate across different preference optimization strategies over training epochs. Table 2: Online CTR improvement compared with the baseline of using the pre-trained MLLM with percentage. that controlling the textual or visual modality input as the sole variable and constructing less relevant product information as negative samples during training can effectively prevent the model from generating contextually mismatched advertisement images. This strategy enhances the model's focus on the multimodal information of the products themselves, leading to more accurate and relevant product descriptions. The full PCPO strategy, which combines both textual and visual perturbations, is the most effective in optimizing the model's performance for product-centric tasks.",
  "4.4 Online Results": "To validate the effectiveness of our proposed CAIG in enhancing the CTRofgenerated advertising images, we conduct a one-week online experiment in a well-known e-commerce platform. We use different methods to generate two images for each product in 44 categories, which almost cover all common products, greatly exceeding the previous method [51] scope of only five categories. It is worth noting that to enhance user experience, we engage professional advertising practitioners to ensure that the images displayed online are front and background-matched. This experiment accumulates over 10 million impressions to validate the reliability and statistical significance of the CTR results. We use a multi-armed bandit based model as online display strategy. We report the results of different methods in all categories and five common categories in Table 2, where the improvement of CTR is compared to directly using pre-trained MLLM. To demonstrate the superiority of our RM, we use different RMs during the CTRdrive preference optimization phase. Our RM outperforms previous methods [45, 51] in all categories and five common categories, demonstrating that more accurate CTR prediction can drive the CTR-Driven Advertising Image Generation with Multimodal Large Language Models WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia Mooncake Gift Box Oolong Tea serene lake, lush green trees, water reflection, clear blue sky, dynamic water contrast, tranquil natural setting DPO Ours Skin T oner bustling city scene, vibrant yellow red brick building, urban architecture,reflective windows, low angle perspective Air Conditioner tranquil river, rocky outcropping background, long lighter trunk, lush green trees, vibrant green river Orange Juice vibrant city street scene, black roof, right side road, bustling cityscape, diverse buildings, reflective windows, traffic lights bright kitchen interior, wooden countertop, window with natural light, modern cozy Body Wash majestic horse, horse carriage background, city street, tall buildings,  nighttime city lights, bustling atmospher Monitor luxury car interior, guitar-shaped case, minimalist design, beige and black colors, elegant texture, high-end product presentation traditional Chinese wooden chest, ancient Chinese pattern, intricate designs, highresolution, isolated on a plain red background minimalist white product display table, monstera leaf patterned wallpaper, clean modern skincare product background amber liquid, golden liquid, molasses texture, dark wooden table surface, warm lighting,  product photography backdrop luxury beige spa background, wooden tray, textured towel, soft lighting, elegant home decor, minimalist style fresh orange juice splash, vibrant orange slices, white background, high-resolution, product photography modern office room, sleek black desk, computer monitors, ambient lighting, clean setup, high-tech workspace modern air conditioner in a warm wall texture, minimalist design, soft lighting, minimalist style Figure 5: Comparison between DPO and the proposed PCPO. The first line shows the name of the product, followed by the generated results for each method, including the generated image and corresponding background prompt. generative model to produce images with higher CTR. We also compare using only DPO [37] as the optimization algorithm, and the results show that using our PCPO can enable the generated model to focus on product characteristics, resulting in an increase in CTR. We further conduct an online A/B test to verify the attractiveness of our generated images, and the results show that adding these images improves 2% in CTR with over 60 million impressions.",
  "5 Conclusion": "In this paper, we present an innovative C TR-Driven A dvertising I mage G eneration (CAIG) method, leveraging the powerful capabilities of Multimodal Large Language Models (MLLMs) to successfully address the limitations in optimizing online performance metrics. Our comprehensive framework, comprising targeted pre-training tasks, an MLLM-based two-branch reward model, and a productcentric preference optimization strategy, enables the generation of visually appealing and product-relevant advertising images. Extensive experiments demonstrate that CAIG achieves state-of-the-art performance in both online and offline metrics, significantly improving CTR in real-world e-commerce scenarios. This work not only advances the field of advertising image generation but also opens up new possibilities for applying MLLMs to complex multimodal tasks in e-commerce and digital advertising, laying a solid foundation for future research in this domain.",
  "Acknowledgments": "This work was partially supported by the National Key R&D Program of China 2022YFC3301000 and Knowledge Innovation Program of Wuhan-Shuguang Project under Grant 2023010201020226.",
  "References": "[1] 2023. GPT-4V(ision) System Card. https://openai.com/index/gpt-4v-systemcard/ [2] 2024. Claude3.5-sonnet. https://www.anthropic.com/news/claude-3-5-sonnet [3] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023). [4] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 (2022). [5] Binghui Chen, Chongyang Zhong, Wangmeng Xiang, Yifeng Geng, and Xuansong Xie. 2024. VirtualModel: Generating Object-ID-retentive Human-object Interaction Image by Diffusion Model for E-commerce Marketing. arXiv preprint arXiv:2405.09985 (2024). [6] J Chen, J Xu, G Jiang, T Ge, Z Zhang, D Lian, and K Zheng. [n. d.]. Automated Creative Optimization for E-Commerce Advertising. arXiv 2021. arXiv preprint arXiv:2103.00436 ([n. d.]). [7] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality. https://lmsys.org/blog/2023-03-30-vicuna/ [8] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences. Advances in neural information processing systems 30 (2017). [9] Zhenbang Du, Wei Feng, Haohan Wang, Yaoyu Li, Jingsen Wang, Jian Li, Zheng Zhang, Jingjing Lv, Xin Zhu, Junsheng Jin, et al. 2025. Towards Reliable Advertising Image Generation Using Human Feedback. In European Conference on Computer Vision . Springer, 399-415. [10] Tiezheng Ge, Liqin Zhao, Guorui Zhou, Keyu Chen, Shuying Liu, Huimin Yi, Zelin Hu, Bochao Liu, Peng Sun, Haoyu Liu, et al. 2018. Image matters: Visually modeling user behaviors using advanced model server. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management . 2087-2095. [11] Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, et al. 2024. ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools. arXiv preprint arXiv:2406.12793 (2024). [12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020. Generative adversarial WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia Xingye Chen, et al. in neural information processing systems 35 (2022), 27730-27744. [35] Sriyash Poddar, Yanming Wan, Hamish Ivison, Abhishek Gupta, and Natasha Jaques. 2024. Personalizing Reinforcement Learning from Human Feedback with Variational Preference Learning. arXiv preprint arXiv:2408.10075 (2024). [36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning . PMLR, 8748-8763. [37] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems 36 (2024). [38] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition . 10684-10695. [39] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 (2017). [40] Anand Siththaranjan, Cassidy Laidlaw, and Dylan Hadfield-Menell. 2023. Distributional preference learning: Understanding and accounting for hidden context in RLHF. arXiv preprint arXiv:2312.08358 (2023). [41] Jiaming Song, Chenlin Meng, and Stefano Ermon. 2020. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502 (2020). [42] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. 2020. Learning to summarize with human feedback. Advances in Neural Information Processing Systems 33 (2020), 3008-3021. [43] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023). [44] Haohan Wang, Wei Feng, Yang Lu, Yaoyu Li, Zheng Zhang, Jingjing Lv, Xin Zhu, Junjie Shen, Zhangang Lin, Lixing Bo, et al. 2025. Generate E-commerce Product Background by Integrating Category Commonality and Personalized Style. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) . IEEE. [45] Shiyao Wang, Qi Liu, Tiezheng Ge, Defu Lian, and Zhiqiang Zhang. 2021. A hybrid bandit model with visual priors for creative ranking in display advertising. In Proceedings of the web conference 2021 . 2324-2334. [46] Shiyao Wang, Qi Liu, Yicheng Zhong, Zhilong Zhou, Tiezheng Ge, Defu Lian, and Yuning Jiang. 2022. CreaGAN: An Automatic Creative Generation Framework for Display Advertising. In Proceedings of the 30th ACM International Conference on Multimedia . 7261-7269. [47] Penghui Wei, Shaoguo Liu, Xuanhua Yang, Liang Wang, and Bo Zheng. 2022. Towards personalized bundle creative generation with contrastive nonautoregressive decoding. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval . 2634-2638. [48] Penghui Wei, Xuanhua Yang, Shaoguo Liu, Liang Wang, and Bo Zheng. 2022. CREATER: CTR-driven advertising text generation with controlled pre-training and contrastive fine-tuning. arXiv preprint arXiv:2205.08943 (2022). [49] Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. 2023. Better aligning text-to-image models with human preference. arXiv preprint arXiv:2303.14420 1, 3 (2023). [50] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2 technical report. arXiv preprint arXiv:2407.10671 (2024). [51] Hao Yang, Jianxin Yuan, Shuai Yang, Linhe Xu, Shuo Yuan, and Yifan Zeng. 2024. ANewCreative Generation Pipeline for Click-Through Rate with Stable Diffusion Model. In Companion Proceedings of the ACM on Web Conference 2024 . 180-189. [52] Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, and Chen Change Loy. 2024. Contextual object detection with multimodal large language models. International Journal of Computer Vision (2024), 1-19. [53] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 3836-3847. [54] Kang Zhao, Xinyu Zhao, Zhipeng Jin, Yi Yang, Wen Tao, Cong Han, Shuanglong Li, and Lin Liu. 2024. Enhancing Baidu Multimodal Advertisement with Chinese Text-to-Image Generation via Bilingual Alignment and Caption Synthesis. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval . 2855-2859. [55] Zhanhui Zhou, Jie Liu, Chao Yang, Jing Shao, Yu Liu, Xiangyu Yue, Wanli Ouyang, and Yu Qiao. 2023. Beyond one-preference-for-all: Multi-objective direct preference optimization. arXiv preprint arXiv:2310.03708 (2023). [56] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593 (2019). CTR-Driven Advertising Image Generation with Multimodal Large Language Models WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia",
  "A Appendices": "This supplementary material provides: (1) Section A.1. Visualization and analysis of the multi-task pre-training effectiveness. (2) Section A.2. Detailed annotation guidelines and criteria used for evaluating generated images. (3) Section A.3. Extensive visual examples demonstrating the capabilities of CAIG across various product categories. (4) Section A.4. The composition of pre-training tasks and design of instruction sets. (5) Section A.5. Discussion on current limitations of the method and proposed directions for future research. (6) Section A.6. Analysis of potential social impacts, ethical considerations, and safeguards implemented.",
  "A.1 Visualization of Pre-training Model": "To validate the effectiveness of our proposed e-commerce knowledge injection pre-training method, we directly utilize the model pre-trained at this stage as a prompt model to generate a set of product images, as illustrated in Figure 6. The generated images demonstrate the model's ability to capture key visual attributes and styles commonly found in e-commerce product photography, such as visual focus with the product as the main subject. This visual evidence suggests that our pre-training method successfully incorporated domain-specific knowledge, resulting in a model capable of generating contextually relevant and visually coherent product images. Furthermore, the quality and diversity of the generated images indicate that the pre-trained MLLM provides a well-initialized distribution space for the subsequent preference optimization phase based on the CTR objective.",
  "A.2 Annotation Guidance": "During the match rate evaluation stage of the generation process, annotators are provided with the original product image, product title, and the generated image, along with the following strict guidelines regarding mismatches: (1) Scale Mismatch. Images where the relative size of the product and background elements are disproportionate, such as a washing machine next to an oversized laundry detergent bottle. (2) Scene Mismatch. Images where the product is placed in a setting that contradicts its intended use or cultural context, such as winter coats displayed in a tropical beach scene. (3) Color Mismatch. Images exhibiting stark color conflicts between the product and background, creating visual discomfort or detracting from the product's appeal. (4) Available. Images deemed suitable for advertising purposes, not falling into any of the aforementioned categories. Additionally, Figure 7 illustrates some examples identified by the annotators.",
  "A.3 More Visual Examples": "As illustrated in Figure 8, we present an extensive array of additional examples showcasing our proposed CAIG method. These diverse visual results demonstrate the remarkable versatility and effectiveness of our method across a wide spectrum of product categories. From electronics to fashion items, and from household goods to specialty products, our method consistently generates varied and contextually appropriate backgrounds. This comprehensive set of examples not only highlights the robustness of CAIG in handling diverse product types but also underscores its ability to create visually appealing and relevant contextual environments.",
  "A.4 Pre-training Tasks and Instruction Set": "Our pre-training method for high-quality MLLMs in advertising background generation encompasses diverse tasks and instruction sets, as illustrated in Table 3. We utilize a mix of public and proprietary datasets: <Product Images> and <Product Caption> from our e-commerce knowledge pre-training dataset, <Prompt> from both Promptist [13] and our dataset, and COCO Caption [26] for unconstrained background description generation. All target outputs are generated by GPT4V [1] and subsequently reviewed by experienced annotators to ensure quality and relevance. Additionally, we design diverse instruction sets for the PM and RM, as shown in Table 4. These guide the models in generating and evaluating advertising backgrounds from various perspectives, leveraging multimodal product information. The PM set contains 8 distinct prompts for background creation, while the RM set includes 13 distinct prompts for the CTR comparison task.",
  "A.5 Limitations and Future Work": "A key limitation of this work is that our CTR optimization is based on aggregated data from all users, which may overlook the preferences of minority user groups or niche market segments. This lack of personalization could result in suboptimal experiences for diverse user segments. In future work, we plan to explore personalized RLHF [35, 40, 55] to better capture and integrate individual user preferences. By doing so, we aim to develop more inclusive and tailored advertising strategies that cater to a wider range of user needs and behaviors.",
  "A.6 Social Impact": "Regarding image processing and automatic advertisement image generation, there are risks of producing unethical or illegal content, such as infringing on personal portrait rights or creating discriminatory content. Therefore, these technologies require stricter regulation. During the generation process, we use Stable Diffusion's official safety checker to filter out inappropriate content. We also ensure that the generated images do not contain portraits or other elements that may infringe on privacy. Finally, professionals review and screen the generated images to ensure they are free from bias or offensive content and comply with relevant laws. To ensure the ethical use of AI in advertising, we maintain transparency by clearly labeling AI-generated images and adhering to established commercial and ethical guidelines. The automation of creative tasks may alter the job market in the creative industry. We should view AI as an auxiliary tool for creative professionals rather than a replacement to maintain the important role of human creativity in advertising. WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia Lavas Cosmetics Electric Fan icy blue landscape, snow-covered mountains, endless sky, soft-focus abstract swirling blue and gold patterns, luxurious texture, elegant light blue waves Toy Car Floral Water",
  "Xingye Chen, et al.": "toy racing car controller in indoor setting, soft lighting, blurred background, no people visible moonlit park scene, dark green grass, night sky with moon, tall trees silhouette in background rustic wooden cabin interior, warm ambient lighting, string lights, elegant table setting, tranquil, dark wood table backdrop cozy interior tea making, wooden table, wooden wall, bright natural light, minimalist style wooden surface, warm sunlight, cozy interior setting, soft shadows, no people visible bright green radial background, subtle light effects, clean and fresh appearance, product-focused composition green mountain landscape, traditional Chinese ink wash painting, tranquil nature, elegant packaging background warm cozy interior, soft natural light, breakfast table with tea time accessories, elegant teapot, pastel blue vase toy car controller on light green background, bright lighting, playful atmosphere transparent glass bottles, soft green gradient background, subtle floating particles, reflective surface Children's Chair light blue walls, decorative objects, wooden stool with cushion, window with blinds, bright airy atmosphere soft green background, plant leaves visible, natural outdoor setting wooden background, warm lighting, cozy indoor setting for a plush toy bear outdoor setting with trees, surrounded by pink green foliage trees bushes, water reflection sunny day, park background, green trees and grass, focus on plush toy in foreground Strawberry Bear minimalist white and gray gradient background with soft shadow for baby product display Figure 6: Advertising images generated by directly using the e-commerce knowledge-injected MLLM as PM. For each product, we display the original transparent background product image in the first column, along with three different background images generated through random repetition.",
  "Product Caption": "Pure Handmade Kung Fu Teapot, Original Mine Purple Eggplant Clay, Small Capacity for Home Use, T ea Brewing Teapot with Bright Stove. Frosch Wool/Silk Clean Laundry Detergent Protects Fabric Fibers Fluffy and Soft 7 5 0 Milliliters Imported from Germany. Vivienne Westwood West Queen Women's Vintage Leather Strap European and American Quartz Watch Birthday Gift. Ferrero Hazelnut Milk Chocolate, 30 Pieces, Wedding Celebration Favor, Birthday Gift for Him and Her, 37 5 grams. Frosch Xinge Baby Toy Hand Drum Early Education Wobbling T oy with Blinking Eyes, Biteable for Newborns 0-1 Year Old, Suitable for 3-6-12 Months. OlarHike Big White Goose Sleeping Pillow for Adults, Companion Sleeping Leg Pillow, Long Plush T oy Doll, Birthday Gift for Her. Figure 7: Some match and mismatch examples identified by annotators. CTR-Driven Advertising Image Generation with Multimodal Large Language Models WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia Figure 8: Extensive visual examples of our CAIG method applied to diverse product categories. @mfort PROYA Mobil 4 B 00 WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia Xingye Chen, et al. Table 3: Overview of pre-training tasks, input-target pairs, and data volume for our multimodal model. The tasks include image understanding, multimodal content understanding, and prompt generation, utilizing both public datasets COCO Caption 1 [26], Promptist 2 [13] and our e-commerce knowledge pre-training dataset 3 . Table 4: Instruct directives for Prompt and Reward Models.",
  "Generated Image": "",
  "Product": "",
  "keywords_parsed": [
    "CTR-Driven",
    " Advertising Image Generation",
    " Online Advertising",
    " Multimodal Large Language Models"
  ]
}