{"A Thorough Performance Benchmarking on Lightweight Embedding-based Recommender Systems": "HUNG VINH TRAN, The University of Queensland, Australia TONG CHEN, The University of Queensland, Australia NGUYEN QUOC VIET HUNG, Griffith University, Australia ZI HUANG, The University of Queensland, Australia LIZHEN CUI \u2217 , Shandong University, China HONGZHI YIN \u2217 , The University of Queensland, Australia Since the creation of the Web, recommender systems (RSs) have been an indispensable personalization mechanism in information filtering. Most state-of-the-art RSs primarily depend on categorical features such as user and item IDs, and use embedding vectors to encode their information for accurate recommendations, resulting in an excessively large embedding table owing to the immense feature corpus. To prevent the heavily parameterized embedding table from harming RSs' scalability, both academia and industry have seen increasing efforts compressing RS embeddings, and this trend is further amplified by the recent uptake in edge computing for online services. However, despite the prosperity of existing lightweight embedding-based RSs (LERSs), a strong diversity is seen in the evaluation protocols adopted across publications, resulting in obstacles when relating the reported performance of those LERSs to their real-world usability. On the other hand, among the two fundamental recommendation tasks, namely traditional collaborative filtering and content-based recommendation, despite their common goal of achieving lightweight embeddings, the outgoing LERSs are designed and evaluated with a straightforward 'either-or' choice between the two tasks. Consequently, the lack of discussions on a method's cross-task transferability will likely hinder the development of unified, more scalable solutions for production environments. Motivated by these unresolved issues, this study aims to systematically investigate existing LERSs' performance, efficiency, and crosstask transferability via a thorough benchmarking process. To create a generic, task-independent baseline, we propose an efficient embedding compression approach based on magnitude pruning, which is proven to be an easy-to-deploy yet highly competitive baseline that outperforms various complex LERSs. Our study reveals the distinct performance of different LERSs across the two recommendation tasks, shedding light on their effectiveness and generalizability under different settings. Furthermore, to account for edge-based recommendation - an increasingly popular use case of LERSs, we have also deployed and tested all LERSs on a Raspberry Pi 4, where their efficiency bottleneck is exposed compared with GPU-based deployment. Finally, we conclude this paper with critical summaries on the performance comparison, suggestions on model selection based on task objectives, and underexplored challenges around the applicability of existing LERSs for future research. To encourage and support future LERS research, we publish all source codes and data, checkpoints, and documentation at https://github.com/chenxing1999/recsys-benchmark. \u2217 Corresponding authors. Authors' addresses: Hung Vinh Tran, h.v.tran@uq.edu.au, The University of Queensland, Brisbane, Queensland, Australia, 4072; Tong Chen, tong.chen@uq.edu.au, The University of Queensland, Brisbane, Queensland, Australia, 4072; Nguyen Quoc Viet Hung, henry.nguyen@griffith. edu.au, Griffith University, Gold Coast, Queensland, Australia, 4222; Zi Huang, helen.huang@uq.edu.au, The University of Queensland, Brisbane, Queensland, Australia, 4072; Lizhen Cui, clz@sdu.edu.cn, Shandong University, Jinan, Shandong, China, 250000; Hongzhi Yin, h.yin1@uq.edu.au, The University of Queensland, Brisbane, Queensland, Australia, 4072. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \u00a9 2018 Association for Computing Machinery. XXXX-XXXX/2025/1-ART $15.00 https://doi.org/10.1145/3712589 , Vol. 1, No. 1, Article . Publication date: January 2025.", "2 \u00b7 Tran, et al.": "", "CCS Concepts: \u00b7 Information systems \u2192 Recommender systems .": "Additional Key Words and Phrases: Benchmarking, On-device Recommendation, Lightweight Recommender Systems.", "ACMReference Format:": "Hung Vinh Tran, Tong Chen, Nguyen Quoc Viet Hung, Zi Huang, Lizhen Cui, and Hongzhi Yin. 2025. A Thorough Performance Benchmarking on Lightweight Embedding-based Recommender Systems. 1, 1 (January 2025), 31 pages. https://doi.org/10. 1145/3712589", "1 INTRODUCTION": "Nowadays, recommender systems (RSs) play a crucial role in assisting users to identify relevant information in their daily lives. A study [44] shows that RSs contributed substantially across various platforms, including 35% of Amazon's revenue, 23.7% of BestBuy's growth, as well as up to 75% and 60% views on Netflix and Youtube, respectively. Since most modern RSs predominantly rely on categorical features, such as user occupations and movie tags in content-based recommendation [15], a crucial way to ensure model expressiveness is to represent each categorical feature as a unique embedding. The embeddings are generally defined as fixed-length vectors for the ease of downstream computations (e.g., dot product for modeling feature interactions or user-item affinity), and are hosted by the RS model in a dense matrix form, commonly referred to as the embedding table. Given the huge amount and diversity of real-world categorical features, embedding tables consequently dominate the parameter consumption in many modern RSs [69]. For example, LightGCN [21], a graph-based collaborative filtering model, spends all parameters on its embedding table, while DeepFM [15], a representative content-based recommender, relies on an embedding table that consumes more than 80% of its parameters for the Criteo benchmark dataset [1]. Another concrete example is the RS deployed by Meta, which consumes 12T parameters and can demand up to 96TB of memory and multiple GPUs to train [41]. The bulkiness of an RS's embedding table has consequently triggered the recent proliferation of lightweight embedding-based recommender systems (LERSs), which have seen popularity in both research [17, 38] and industry deployments [30, 44]. On the one hand, the reduced parameterization in the embedding table provides a direct cure to the scalability bottleneck of large-scale RSs, where some work has reported a 10 \u00d7 parameter reduction with negligible performance compromise [39, 65]. On the other hand, this line of research also co-evolves with the ongoing trend of decentralization in the RS service architecture, which provides various benefits such as low latency, better privacy, and reduced server hosting costs. Federated [3, 6], on-device [8, 46, 67], and IoT-enhanced [42] recommendation paradigms all align with this decentralization trend, while they unanimously put a higher demand for nimble RS solutions that can operate on less resourceful edge devices (e.g., a smartphone) - hence the rise of LERSs. Despite the promising blueprint, providing accurate recommendations is challenging under limited memory and computing budgets [12]. For LERSs, researchers proposed diverse approaches to compress the embedding table of recommender systems, as illustrated in Figure 1.(a). We hereby summarize the three main categories of methods for achieving lightweight yet expressive embeddings for RSs, which are also depicted in Figure 1.(b)-(d): (1) Compositional embedding (Figure 1.(b)) leverages one or multiple smaller embedding tables (also called meta-embedding tables [35]), where each feature is represented with a unique combination of metaembeddings. Various operators (e.g., sum, element-wise multiply, concatenate) can be used to transform a collection of meta-embeddings into a single, unique embedding for each feature [56, 65]. (2) Embedding pruning (Figure 1.(c)), a well-established method to compress learnable weights [23], has also been applied extensively to RSs' embedding tables [37]. These pruning techniques zero out unimportant embedding parameters and utilize sparse data structures to reduce the storage cost of a pruned embedding table. , Vol. 1, No. 1, Article . Publication date: January 2025. A Thorough Performance Benchmarking on Lightweight Embedding-based Recommender Systems \u00b7 3 (3) Neural architecture search (NAS, Figure 1.(d)) is the third mainstream type of LERS solutions. A standard NAS searches for the best model structure and hyperparameters from a predefined search space [8, 27]. In the context of LERSs, the search space is generally the embedding dimension of each feature, and the search is normally performed towards a combination of accuracy and efficiency objectives. Wedefer our discussions on the detailed methodological differences among these solutions to Section 2. In addition to those three main types, there are LERS solutions that ensemble different methods to achieve embedding compression [35, 39], which we term hybrid solutions. Amid the dedication to building more advanced LERSs, it also comes to our attention that, a systematic discussion on a consistent, universal evaluation protocol for these LERSs is not yet in place in the existing literature. As the area of LERSs is still gradually taking its shape, these methods are often evaluated with heuristically designed protocols and a variety of benchmark datasets, which are not synchronized among publications. For example, as an LERS for collaborative filtering (CF) tasks, DHE [28] is deployed with Generalized Matrix Factorization (GMF) and Multi-layer Perceptron (MLP) [22] as recommendation backbones, and evaluated on ML-20M [19] and Amazon-book [20] datasets with Area under the ROC Curve (AUC) as the metric. In contrast, another CF solution CERP [35] is evaluated on Gowalla [33] and Yelp2020 [2] benchmarks with MLP and LightGCN [21] as backbones, where and Normalized Discounted Cumulative Gain and Recall as metrics. In other words, although different methods are developed for the same task, they are commonly evaluated using different datasets and metrics to benchmark performance. Even when the same datasets and metrics are employed, the evaluation protocol can also differ in various details. An example is PEP [37], a pruning method and OptEmbed [39], a hybrid of NAS and pruning. However, the full recommendation models compressed in [37] and [39] differ in their original embedding dimensions (24 in [37] and 64 in [39]), and the compressed models also vary in parameter sizes. These seemingly minor details can result in a significant impact on the results, as both the preand post-compression recommender models can reach hugely different performance when configured to different parameter sizes [69, 74]. As a consequence, such inconsistency inevitably impairs the confidence when pinpointing the performance gain of the LERS proposed, and less intuitive to understand where each LERS's advantages (e.g., fast training/inference) are. As a result, this can lead to the adoption of a less effective or inappropriate solution in practical applications. Thus, a natural research question (RQ) arises: (RQ1) Can we fairly benchmark a representative collection of LERSs from all categories under the same, practical evaluation setting? Moreover, although both collaborative filtering and content-based recommendation are arguably the most representative recommendation tasks, in LERSs, the majority of methods are designed for content-based recommendation tasks [37, 39, 56] given the huge number of categorical features used to describe users and items. On the contrary, LERSs dedicated to collaborative filtering (CF) [28, 49] are not intensively investigated until recently, which potentially attributes to the explosive growth of user base and item catalog in major e-commerce sites in the past few years [16, 60]. It is worth noting that, despite the differences in downstream outputs and recommendation models, LERSs designed for both collaborative filtering and content-based recommendation in fact bear the same goal of reducing the parameter usage of the embedding table - one for representing content features and the other for representing user/item IDs. However, the embedding compression paradigms in LERSs are commonly developed in a task-specific fashion, so do the evaluations in existing papers. Given the shared goal of embedding compression between tasks, there is another important question related to cross-task generalizability to answer: (RQ2) Do methods that demonstrate strong performance in one task exhibit similarly strong performance in a different recommendation task? At the same time, considering that LERSs are centered around model scalability, many related metrics other than the parameter size and recommendation accuracy, especially the inference speed and runtime memory consumption, remain largely unexplored in the existing research. The inference speed is crucial to user experience , Vol. 1, No. 1, Article . Publication date: January 2025. 4 \u00b7 Tran, et al. and energy efficiency, while the runtime memory consumption is closely connected to scalability as it determines whether or not an LERS is executable on specific memory-constrained devices (e.g., TV boxes), and lower runtime memory also supports a larger batch size to speed up training. Unfortunately, in the pursuit of lower parameter sizes, most LERSs introduce an overhead on these two metrics. For example, the compositional embedding table in TTRec [65] needs to be computed via a series of tensor multiplications, compromising the inference speed. Pruning-based methods, such as PEP [37], introduce substantial memory overhead due to additional masks over the full embedding table. The absence of those scalability metrics in evaluation renders it unclear if a particular LERS is a feasible solution for each given deployment configuration. Consequently, real-world adoptions for LERSs will likely be deterred without comprehensively benchmarking their resource requirements. Thus, we wonder: (RQ3) How is the real-world usability of these LERSs in terms of efficiency and memory consumption during training and inference? Motivated to answer these questions, in this paper, we take a formal approach to benchmark various recently proposed embedding compression techniques for LERSs. Specifically, to address RQ1 , we select a diversity of methods designed for collaborative filtering or content-based recommendation tasks, and performed thorough benchmarking on both tasks. For each task, we further use two real-world datasets, where all methods are universally tested with three different compression goals (i.e., the target parameter sizes after compression). For each model in each dataset, we scientifically fine-tune the hyperparameters to ensure a fair comparison. We also point out a practically important issue - unlike performance-oriented RS research, the field of LERSs still lacks effective baselines, that is, an easy-to-use model that provide competitive performance across different settings. Hence, we additionally put forward magnitude-based pruning , a simple, effective, and performant baseline in serveral settings. Moreover, we empirically justify and analyze the suitable use cases for magnitude pruning through our experiments. To address RQ2 , we extend our benchmarking across both collaborative filtering or content-based recommendation tasks for all LERSs selected, regardless of their original downstream tasks. To measure the cross-task generalizability, we first define performance retain rate, a metric to quantify how well each method preserves the full model's performance after compression. Then, we systematically analyze the obtained results to compare performance across tasks, highlighting the similarities and differences between each method's performance when being applied to the two representative recommendation tasks. To address RQ3 , we deploy all tested LERSs into two typical environments for LERSs: a GPU workstation and an edge device. We benchmark the time consumption and memory usage of both training and inference steps, covering both recommendation tasks. By doing so, we shed light on the overhead introduced by each method in real-world deployment. In summary, our main contributions are: \u00b7 We extensively evaluate various LERSs' performance in two main recommendation tasks: content-based recommendation and collaborative filtering. Concretely, we cross-test different methods to verify their generalizability under different sparsity rates and tasks. \u00b7 We show that magnitude-based pruning, a simple baseline for embedding compression, can also achieve competitive results compared to recently proposed methods. \u00b7 We perform an efficiency benchmark and outline the key differences between the on-device and GPU-based settings, thus providing insights into the real-world performance of those LERSs in varying deployment environments. \u00b7 We release all the source codes at https://github.com/chenxing1999/recsys-benchmark, which include the implementation of various embedding compression methods in PyTorch, such that the community can reuse and apply them to subsequent research problems. , Vol. 1, No. 1, Article . Publication date: January 2025. A Thorough Performance Benchmarking on Lightweight Embedding-based Recommender Systems \u00b7 5 Fig. 1. Illustration for main archetypes of LERSs embedding. (a) The original embedding table uses a dedicated vector e \ud835\udc56 from table E to represent feature \ud835\udc56 . (b) The compositional embedding approach employs a smaller embedding table E \ud835\udc5a\ud835\udc52\ud835\udc61\ud835\udc4e and a hash function that hashes \ud835\udc56 into ( \ud835\udc56 1 , \ud835\udc56 2 ) . The final embedding e \ud835\udc56 is constructed by combining e \ud835\udc5a\ud835\udc52\ud835\udc61\ud835\udc4e \ud835\udc56 1 and e \ud835\udc5a\ud835\udc52\ud835\udc61\ud835\udc4e \ud835\udc56 2 . (c) Pruning reduces the size of the original embedding table by zeroing out parts of it (shown in gray), resulting in a sparse embedding table \u02c6 E with a smaller storage footprint. (d) Methods based on neural architecture search (NAS) commonly search for an optimal embedding dimension configuration \ud835\udc46 for the embedding table from a given search space, optimizing the trade-off between memory cost and model performance. (a) Original (b) Compositional (c) Pruning search space (d) NAS-based", "2 RELATED WORK": "In this section, we provide relevant background for our research by reviewing the classic recommendation tasks, representative LERSs, and the benchmarking efforts in the recommendation literature.", "2.1 Lightweight Embeddings in Recommendation": "2.1.1 Compositional Embedding-based LERSs. This type of LERSs involves representing the original \ud835\udc5b embedding vectors with substantially fewer parameters, where a common approach is to employ a smaller set of \ud835\udc5a \u226a \ud835\udc5b meta-embedding vectors. To compose a single embedding for each discrete feature, a unique subset of \ud835\udc61 metaembeddings is selected and combined. Mathematically, this is achieved by:  where \ud835\udc56 \u2208 N is the original index of the feature, hash (\u00b7) maps \ud835\udc56 into \ud835\udc61 distinct indices { \ud835\udc56 1 , \ud835\udc56 2 , ..., \ud835\udc56 \ud835\udc61 } \u2208 N < \ud835\udc5a . To simplify notation, let's assume that there is only one set of meta-embedding vectors, or one meta-embedding table E \ud835\udc5a\ud835\udc52\ud835\udc61\ud835\udc4e \u2208 R \ud835\udc5a \u00d7 \ud835\udc51 , then the compositional embedding e \ud835\udc56 of the \ud835\udc56 -th feature is:  where each e \ud835\udc5a\ud835\udc52\ud835\udc61\ud835\udc4e \ud835\udc56 \u2032 corresponds to the \ud835\udc56 \u2032 -th row of E \ud835\udc5a\ud835\udc52\ud835\udc61\ud835\udc4e , and combine (\u00b7) is any operation that merges multiple vectors into one, e.g., multiplication, sum, or concatenation. As an extension to this basic form, Shi et al. [56] use the quotient-remainder trick (QR) to hash the original embedding index \ud835\udc56 into two new indices, which are used to extract embedding vectors from two meta-embedding tables. These vectors can be combined through mathematical operations, such as multiplying, adding, or concatenating, to create an embedding vector representing the original. Following this, MEmCom [44] utilizes two meta embedding tables ( E1 \u2208 R \ud835\udc5a \u00d7 \ud835\udc51 , E2 \u2208 R \ud835\udc5b \u00d7 1 ) and a pair of indices ( \ud835\udc56 1 = \ud835\udc56 mod \ud835\udc5a,\ud835\udc56 2 = \ud835\udc56 ), then multiplying two meta embedding vectors to get the final embedding associated with \ud835\udc56 . The above methods are efficient as they only apply simple aggregation functions; however, this typically limits the performance due to collided meta-embeddings, especially at higher compression rates. Another approach is , Vol. 1, No. 1, Article . Publication date: January 2025. 6 \u00b7 Tran, et al. TT-Rec [65], which employs tensor-train decomposition (TTD) and a customized weight initialization to compress the embedding table. Because TTD can transform the exponential storage requirement into a linear function, it can achieve a higher compression rate than previous methods. ODRec [64] proposes to further compress the model with semi-tensor product-based tensor-train decomposition (STTD), and compensates for the higher compression rate with knowledge distillation. DHE [28] proposes to use a deterministic hash function to create a pseudo-embedding table, which the authors fed to an MLP model to produce the embedding. DHE model achieves good results and can be further compressed with other techniques; however, requiring much more time to train and infer than other methods. Based on the user-item interaction graph, LEGCF [34] proposes an adaptive assignment scheme through a learnable matrix, representing the weight for each meta-embedding. 2.1.2 Pruning. This is one of the most classic approaches to compressing deep learning models in general and recommendation models in particular. These methods involve setting a portion of neuron values to zero, effectively removing them from the model.  where E denotes the full learnable embedding table, M is the embedding mask, \u2299 is element-wise multiplication. The main challenge of pruning-based methods is how to effectively find M . The most traditional approach in this category is magnitude pruning [18, 26], where after the initial training phase, the weights are sorted by the magnitude of their values and the lowest-ranked ones are zeroed out. Leveraging this, DeepLight[11] gradually prunes the embedding table in the training phase based on the magnitude values until reaching the target memory budget. Later, PEP [37] applies 'soft-thresholding' to iteratively prune the parameters in the first training step. Subsequently, the model is retrained based on the found mask with the same initialized parameters (Lottery ticket hypothesis [13]). The 'soft-thresholding' technique allows PEP to provide a flexible embedding size for each feature. However, PEP has high training overhead and is hard to tune for a specific compression rate due to two training steps and various hyperparameters affecting performance. In contrast, SSEDS [47] first trains the original model. Then, they determine the embedding mask with the proposed saliency score computed through the gradient and retrain the model with the newfound mask. While SSEDS only introduces minor overhead in one forward and backward pass to calculate the embedding mask, it assumes similar embedding sizes for features in the same field, leading to constrained performance. Dynamic Sparse Learning (DSL) [62] dynamically adjusts the sparsity distribution of model weights by pruning and growth strategies to eliminate redundant parameters and activate important ones. Specifically, During training, they initially prune a significant portion of the model weights, fine-tune the model, and then prune and regrow again with a smaller amount after a few iterations. Shaver [59] employs Shapley values to efficiently and effectively prune the embedding table to any required budget for content-based recommendation models. 2.1.3 NAS-based. Methods from this category search for the most optimal model structure in a predefined search space, typically by reinforcement learning [49, 50] or an evolutionary algorithm [8]. They are commonly formulated as a two-level optimization problem w.r.t. both training and validation data [70, 72]:  where \ud835\udc46 denotes structure parameters, \u0398 denotes the model parameters. One of the first works in this category is NIS (Neural Input Search) [27], which splits the original embedding table into multiple smaller embedding blocks to create the search space. Then, NIS applies a policy network to determine the best set of embedding blocks given a memory budget. AutoEmb [71] uses controllers that take features' popularity to suggest the embedding size of various users and items for the recommendation network. They employ differentiable architecture search (DARTS [36]) to solve the bi-level optimization problem, where the first and second stages are to optimize the recommendation network's weight on the training set and the controllers' weight on the validation set , Vol. 1, No. 1, Article . Publication date: January 2025. A Thorough Performance Benchmarking on Lightweight Embedding-based Recommender Systems 7 \u2022 respectively. While also applying DARTS, AutoDim [70] uses a set of weights, which directly represent the probability of dimension sizes, and leverages the Gumbell-Softmax technique [25] to optimize these parameters. However, DARTS-based methods suffer from high training costs [5]. RULE [8] suggests training a supernet containing various embedding blocks and an evolutionary search to search for the best embedding block set given a memory budget. To reduce the computation cost for evolutionary search, they train a performance estimator to predict an estimated performance of a given embedding block set. CIESS [49] applies reinforcement learning with a random walk-based exploration strategy to efficiently identify the optimal embedding size for each user and item. BET [48] leverages a non-parametric sampler to eliminate the implicit necessity of fine-tuning a coefficient trade-off between performance and storage. This approach, however, requires multiple fine-tuning iterations of the model. To address this overhead, BET introduces a parametric performance estimator. 2.1.4 Hybrid. Methods within this category combine approaches from various categories. OptEmbed [39] learns the pruning mask for embedding rows based on magnitude while training the supernet with uniform sampled masks for dimension sizes. Then, they apply an evolutionary algorithm to find the most optimal configuration and retrain the model with the found configuration. CERP [35] integrates soft-thresholding pruning into the compositional embedding with two balanced-size embedding tables. Thus, CERP could achieve a higher compression rate than the original compositional embedding but suffers from the complexity introduced by the pruning step. It is worth mentioning that, recent years have also seen methods approaching lightweight RSs via hardware-software co-design [24], but we focus our comparative study on algorithmic (software) solutions given their stronger applicability and flexibility. 2.1.5 Summary. The compositional methods are more straightforward to fine-tune as they define the number of parameters at the start of the training, and the training efficiency generally is better compared to other approaches. However, they suffer from limited performance because every feature has the same memory allocation, and they also introduce more inference time overhead. On the other hand, pruning trade-off training efficiency for a better performance [69]. The training pipelines are more complicated and involve multiple steps. Moreover, pruning demanded specific hardware to process sparse matrices efficiently. NAS-based generally demands the most training resources, while having the best inference efficiency and performance. Last but not least, hybrid methods combine the advantages of other categories to create better recommendations.", "2.2 Benchmarking Efforts in Recommender Systems": "With the increasing awareness of reproducibility in recommendation research, there has been a series of efforts in benchmarking different recommendation models. Rendle et al. [52] show that well-tuned baselines could outperform newly proposed methods, which initiated a heated debate in the RSs studies. Aligning with the previous research, Maurizio et al. [10] also indicate that simple baselines could defeat the more sophisticated deep learning models. Responding to [10], DaisyRec [57, 58] performs an extensive study on how various hyperparameters affect recommendation models' performance. Shehzad et al. [55] conduct experiments to show that the worst well-fine-tuned model will outperform the best non-fine-tuned model. With the rising concerns of reproducibility, BarsCTR [74] focuses on the reproducibility of CTR models by unifying the data pre-processing logic and providing an open-source implementation for various methods. After that, Zhu et al. [73] extended previous works by working on both collaborative filtering and content-based recommendation. However, these studies exclusively examine different backbones used in recommendation models. Li et al. [32] provide an overview of various LERSs methods. Yin et al. [67] extensively explore the on-device settings for RSs, including inference, training, and security concerns. Zhang et al. [69] share similarities with our work, studying various embedding compression methods, albeit with the limited scope of recommendation task (CTR prediction only) and minimal , Vol. 1, No. 1, Article . Publication date: January 2025.", "8 \u00b7 Tran, et al.": "hyperparameter fine-tuning. On the other hand, our work provides a more extensive hyperparameter tuning by adapting the methodology from [58] and studies LERSs' transferability onto collaborative filtering tasks.", "3 BASE RECOMMENDERS": "The LERSs we have selected for benchmarking are compatible with the majority of base recommenders, as long as they are latent factor models with an embedding layer. To ensure fairness, all LERSs are plugged into the same base recommender in every test. We evaluate two representative recommendation tasks, specifically the content-based recommendation and collaborative filtering. This section describes the backbones selected for the two tasks, namely NeuMF [22] and LightGCN [21, 68] for collaborative filtering, and DeepFM [15] and DCN_Mix [61] for content-based recommendation.", "3.1 Collaborative Filtering": "The most classic approach for RSs is collaborative filtering, which solely takes user-item past interactions to produce user-item affinity. In our experiments, users' interests are represented by a binary value to indicate whether an interaction occurred (implicit feedback). For testing LERSs on the collaborative filtering task, we adopt two commonly used backbones: a latent-factor-based model NeuMF [22] and a graph-based model LightGCN [21]. We will further elaborate on these methods in the section below. 3.1.1 NeuMF. The latent-factor-based collaborative filtering models' target is to find the shared latent representation for users and items from the interaction matrix [40]. One of the most well-established in this category is matrix factorization, which associates each user and item with an embedding vector. Let e \ud835\udc62 and e \ud835\udc63 denote the embedding vector for user \ud835\udc62 and item \ud835\udc63 , respectively. Matrix factorization computes the relevant score \ud835\udc5f \ud835\udc62\ud835\udc63 as the dot product between e \ud835\udc62 and e \ud835\udc63 . Later, NeuMF [22] proposes generalized matrix factorization (GMF), which modifies the original dot product formula by adding learnable parameters h , and combines it with the DNN branch to further enhance matrix factorization. The outputs of these two branches are defined as:   where e GMF \u00b7 and e DNN \u00b7 is the embedding corresponding for GMF and DNN branch, respectively. \u2299 denotes elementwise product of vectors. It is noteworthy that these two embedding vectors are different. The final score is computed by summing two branches' results:  where \ud835\udf0e (\u00b7) is the sigmoid function. Finally, based on the original paper, we optimize the model with log loss function and an \ud835\udc3f 2 penalty, as defined in the following:  where each training batch B consists of training samples ( \ud835\udc62, \ud835\udc63 + , V - ) that are constructed by pairing a user \ud835\udc62 with one of her interacted item \ud835\udc63 + and a set V of uninteracted items. Following the common practice [35, 58], we also apply an \ud835\udc3f 2 regularization on all learnable parameters \u0398 with weight \ud835\udf06 to prevent overfitting. , Vol. 1, No. 1, Article . Publication date: January 2025. A Thorough Performance Benchmarking on Lightweight Embedding-based Recommender Systems \u00b7 9 3.1.2 LightGCN. For graph-based collaborative filtering, the user-item interactions are formulated as a bipartite graph, where an edge between user \ud835\udc62 and item \ud835\udc63 nodes exists if \ud835\udc62 has an observed interaction with item \ud835\udc63 . Taking a user node \ud835\udc62 as an example, we denote N( \ud835\udc62 ) as the set of \ud835\udc62 's one-hop neighbors. At each layer \ud835\udc59 , the node embedding is updated by aggregating embeddings from all the neighbors within N( \ud835\udc62 ) :  where e ( \ud835\udc59 ) \ud835\udc62 is the embedding of \ud835\udc62 in the \ud835\udc59 -th layer, (|N( \ud835\udc62 )| \u00b7 |N ( \ud835\udc67 )|) -1 2 is the normalization term. Notably, when \ud835\udc59 = 0, e ( 0 ) \ud835\udc62 corresponds to the \ud835\udc62 -th row of the embedding table E , which is randomly initialized and learned via back-propagation. After \ud835\udc3f layers' propagation, the final representation of user \ud835\udc62 is obtained by averaging the embeddings from all layers:  where the embedding e \ud835\udc63 for an arbitrary item \ud835\udc63 is obtained analogously. To facilitate ranking, the relevance score \ud835\udc5f \ud835\udc62\ud835\udc63 for the user-item pair is the dot product between their final embeddings, i.e., \ud835\udc5f \ud835\udc62\ud835\udc63 = \u27e8 e \ud835\udc62 , e \ud835\udc63 \u27e9 . The loss function employed in the original paper [21] combines the Bayesian Personalized Ranking (BPR) loss and \ud835\udc3f 2 penalty, as defined in the following:  where each training batch B consists of training samples ( \ud835\udc62, \ud835\udc63 + , \ud835\udc63 - ) that are constructed by pairing a user \ud835\udc62 with one of her interacted item \ud835\udc63 + and an unvisited one \ud835\udc63 -. As the trainable parameters in LightGCN only contain the embedding table, the \ud835\udc3f 2 regularization is only enforced on E with weight \ud835\udf06 . However, on larger datasets, Yu et al. [68] point out that LightGCN is prone to prolonged convergence time. To address this issue, it is recommended to incorporate InfoNCE [43] into the loss function of LightGCN:  where \ud835\udefe is the weight for the InfoNCE loss, \ud835\udf0f is the temperature, and \ud835\udc67 and \ud835\udc67 \u2032 are users/items in the sampled batch B . Note that the above InfoNCE requires no data augmentation for the sake of efficiency [68], hence the softmax directly uses 1 / \ud835\udf0f as the numerator. As proven in [68], the addition of the InfoNCE loss effectively improves the quality of embeddings learned in every epoch and substantially reduces the convergence time 1 as a result.", "3.2 Content-based Recommendation": "In this section, we first introduce the content-based recommendation task, then two backbones we used in our benchmark. In content-based recommendation, we have \ud835\udc5b features encoded in vector x \u2208 R \ud835\udc5b , where \ud835\udc65 \ud835\udc56 \u2208 x represents the value of feature \ud835\udc56 . Taking CTR prediction as an example, x is commonly the concatenation of the features of a user and an advertisement, where the recommender is expected to estimate the probability of a click behavior. For the ground truth \ud835\udc66 to be predicted, \ud835\udc66 = 1 if a click is observed, and \ud835\udc66 = 0 otherwise. Considering the features in x are commonly sparse [51], it is essential to learn effective feature interactions. To this end, the factorization machine (FM) [51] has been a well-established approach for content-based recommendation:  1 There is an approximately 50 \u00d7 speed-up in our observation, hence we resort to this configuration throughout the benchmarking. , Vol. 1, No. 1, Article . Publication date: January 2025. 10 \u00b7 Tran, et al. where \u27e8\u00b7 , \u00b7\u27e9 is the dot product, and \ud835\udc64 0 is the bias term to be learned. For each feature \ud835\udc56 , \ud835\udc64 \ud835\udc56 is the learnable scalar weight, and e \ud835\udc56 \u2208 R \ud835\udc51 is its corresponding embedding drawn from the embedding table E . For model optimization, we follow the common practice [11, 31, 39] and adopt the following log loss:  which quantifies the prediction error between \u02c6 \ud835\udc66 and the ground truth \ud835\udc66 . In addition, the second term denotes the \ud835\udc3f 2 regularization over all trainable parameters \u0398 to prevent overfitting, and \ud835\udf06 is a tunable coefficient that controls its weight in the loss function. Despite the versatility of FM, it only accounts for second-order feature interactions, hence being insufficient for the more complex applications. Therefore, various methods were proposed to model higher-order feature interactions. 3.2.1 DeepFM. [15] proposes to uplift the expressiveness of FM by incorporating a deep neural network (DNN) branch into the original FM model to model the higher-order interactions between features:  with the pooling operator pool (\u00b7) . In DeepFM, the pooling operation is done by firstly performing sum pooling over the weighted embeddings \ud835\udc65 \ud835\udc56 e \ud835\udc56 in each feature field (e.g., user region and movie genre), and then concatenating the results from all fields [15]. The final prediction is calculated as an ensemble:  where \ud835\udf0e (\u00b7) is the sigmoid function. Notably, the feature embeddings used in both FM and DNN branches are drawn from the same embedding table E . 3.2.2 DCN-Mix. DCNv2 [61] explicitly models ( \ud835\udc59 + 1 ) -level interaction with \ud835\udc59 cross layers:  where e ( 0 ) \u2208 R \ud835\udc53 is the result of pooling operator pool ({ \ud835\udc65 \ud835\udc56 e \ud835\udc56 } \ud835\udc5b \ud835\udc56 = 1 ) . e ( \ud835\udc59 ) , e ( \ud835\udc59 + 1 ) represents the input and output of the (l+1)-th cross layer respectively. W ( \ud835\udc59 ) \u2208 R \ud835\udc53 \u00d7 \ud835\udc53 and b ( \ud835\udc59 ) \u2208 R \ud835\udc53 is the learnable weight matrix and bias vector for the corresponding \ud835\udc59 -th layer. To reduce the training cost, we employ the DCN-Mix backbone. First, Wang et al. [61] suggest to utilize low-rank approximation to reduce the compute cost of Eq. 1:  where U ( \ud835\udc59 ) , V ( \ud835\udc59 ) \u2208 R \ud835\udc53 \u00d7 \ud835\udc5f and rank \ud835\udc5f \u226a \ud835\udc53 . Based on this intuition, DCN-Mix applied the idea of Mixture-of-Experts (MoE), which consists of two modules: experts (typically small models, denoted as \ud835\udc43 \ud835\udc56 : R \ud835\udc53 \u2192 R \ud835\udc53 ) and gating (a function \ud835\udc3a \ud835\udc56 : R \ud835\udc53 \u2192 R ). Each expert specializes in handling a specific data distribution, while the gating network assigns a weight to each expert, determining how much each expert should contribute to the final output. The specific computation is provided below:  , Vol. 1, No. 1, Article . Publication date: January 2025. A Thorough Performance Benchmarking on Lightweight Embedding-based Recommender Systems \u00b7 11 where \ud835\udc3e is the number of experts, U ( \ud835\udc59 ) \ud835\udc56 , V ( \ud835\udc59 ) \ud835\udc56 \u2208 R \ud835\udc53 \u00d7 \ud835\udc5f and C ( \ud835\udc59 ) \ud835\udc56 \u2208 R \ud835\udc5f \u00d7 \ud835\udc5f is the learnable parameters of \ud835\udc56 -th expert's \ud835\udc59 -th layer, W \ud835\udc56 \u2208 R \ud835\udc53 \u00d7 1 is the learnable parameters of \ud835\udc56 -th gate, tanh is the non-linear activation function. Finally, as \ud835\udc3f cross layers can only model upto ( \ud835\udc3f + 1 ) interaction orders, the authors propose incorporating a deep neural network to further enhance modeling capacity. Specifically, we adopt the proposed stacked structure, which works empirically better for the Criteo dataset [61]. The specific procedure is defined below:  where e ( \ud835\udc3f ) is the output of the last cross layer.", "3.3 Tree-structured Parzen Estimator (TPE) for Hyperparameter Tuning": "To ensure the fairness of comparisons, hyperparameter tuning is an indispensable step. Grid search is the most basic hyperparameter optimization algorithm, which tests each set of hyperparameters based on a predefined grid. However, grid search has an exponential cost with the number of hyperparameters and cannot be applied on a continuous range, thus limiting its performance. However, the vast combinatorial space of tunable hyperparameters in most embedding compression methods introduces a major challenge in terms of efficiency. Therefore, in this work, we employ Tree-structured Parzen Estimator (TPE) [4] as our hyperparameter optimization algorithm, which is a common choice in the recommender system literature [57, 58, 61]. In what follows, we introduce how TPE is leveraged for hyperparameter tuning. In a nutshell, TPE attempts to estimate \ud835\udc43 (X| \ud835\udc66 ) , where X is a set of hyperparameters, and \ud835\udc66 is the associated performance measured by AUC (for CTR prediction tasks) or NDCG (for collaborative filtering tasks). Specifically, TPE maintains two distributions \ud835\udc43 (X| \ud835\udc66 > \ud835\udc66 \u2217 ) and \ud835\udc43 (X| \ud835\udc66 \u2264 \ud835\udc66 \u2217 ) with \ud835\udc66 \u2217 being a predefined performance threshold. These two distributions are modeled by two Gaussian Mixture models. Initially, we randomly run each method with \ud835\udf14 ( \ud835\udf14 = 10 in our case) sets of hyperparameters, and \ud835\udc66 \u2217 is selected based on the most performant setting. Then, based on results from the initial \ud835\udf14 trials, we select the next hyperparameter setting X such that it maximizes the expected ratio \ud835\udc43 ( X| \ud835\udc66 > \ud835\udc66 \u2217 ) \ud835\udc43 ( X| \ud835\udc66 \u2264 \ud835\udc66 \u2217) . After finishing a trial, both \ud835\udc43 (X| \ud835\udc66 > \ud835\udc66 \u2217 ) and \ud835\udc43 (X| \ud835\udc66 \u2264 \ud835\udc66 \u2217 ) are updated according to the sampled X and its evaluation result. When performing hyperparameter tuning for each method, we repeat the described process until a maximum number of 30 trials (including the initial \ud835\udf14 trials) has been reached, and use the setting X that yields the best validation performance for testing.", "4 LERS BASELINES COMPARED": "In this section, we first introduce the LERSs methods used in our benchmark. Then, we propose a straightforward baseline for comparative analysis.", "4.1 Chosen Baselines": "We select these techniques based on two primary aspects: influence (should be from a renowned venue or highly cited) and diversity (should cover all main LERS types). Table 1 provides a summary of our chosen methods and their categories. 4.1.1 QR. Shi et al. [56] divided the original embedding table E \u2208 R \ud835\udc5b \u00d7 \ud835\udc51 into two new tables, E 1 \u2208 R \ud835\udc5d \u00d7 \ud835\udc51 , E 2 \u2208 R \ud835\udc5e \u00d7 \ud835\udc51 , where \ud835\udc5d is a hyperparameter and \ud835\udc5e = \u2308 \ud835\udc5b / \ud835\udc5d \u2309 . To retrieve the embedding for the original index \ud835\udc56 , we use two new indices \ud835\udc56 1 = \ud835\udc56 mod \ud835\udc5d and \ud835\udc56 2 = \ud835\udc56 div \ud835\udc5d to extract embedding vectors from E 1 and E 2 . Finally, these vectors can be combined through various mathematical operations. In our experiments, we employ element-wise product between two vectors as it is the most competitive method in the original paper. We chose QR as it is one of the most simple compositional embedding methods while being widely accepted as a baseline and inspired various other methods [9, 35]. , Vol. 1, No. 1, Article . Publication date: January 2025. 12 \u00b7 Tran, et al. Table 1. Summary of chosen LERSs with a high-level description. P , C , and N denote Pruning-, Composition-, and NAS-based methods, respectively. Section 4.1 provides further discussions on each method. 4.1.2 TTRec. Yin et al. [65] proposed a novel algorithm to compress RSs embedding based on tensor-train decomposition (TT). TTRec factorizes the number of items \ud835\udc5b \u2264 \u02db \ud835\udc61 \ud835\udc56 = 1 \ud835\udc5b \ud835\udc56 and the hidden size \ud835\udc51 \u2264 \u02db \ud835\udc61 \ud835\udc56 = 1 \ud835\udc51 \ud835\udc56 into integers, and decomposes the embedding table by E \u2248 G 1 G 2 . . . G \ud835\udc61 , where TT-core G \ud835\udc56 \u2208 R \ud835\udc5f \ud835\udc56 -1 \u00d7 \ud835\udc5b \ud835\udc56 \u00d7 \ud835\udc51 \ud835\udc56 \u00d7 \ud835\udc5f \ud835\udc56 . To further improve the efficiency, they introduced a cache mechanism and leveraged the cuBLAS (CUDA Basic Linear Algebra Subroutine) library. We selected TTRec as the representative for a more complex compositional embedding method. TTRec also inspired various other works [64, 66]. Notably, compared with QR which only reduces the feature dimension of embedding table ( \ud835\udc5b ), TTRec reduces both the feature dimension ( \ud835\udc5b ) and the hidden dimension ( \ud835\udc51 ) via tensor decomposition. 4.1.3 DHE. The core idea of DHE [28] is utilizing a hash function to transform the original index \ud835\udc56 into a new dense vector v \u2208 R \ud835\udc58 , with \ud835\udc58 being a large number (for example, 1024). This hash function is deterministic, thus requiring almost no storage cost. They then feed this vector v into an MLP model, whose number of parameters is much smaller than the original embedding table, to produce the actual embedding. We chose this method due to its initial design for CF and its proven effectiveness in compressing item and user embeddings. 4.1.4 PEP. PEP [37] draws inspiration from Soft Thresholding Reparameterization (STR) [29], which gradually prunes the model by learning threshold s through backpropagation. This threshold determines which parameters to prune dynamically. STR pushes small weights towards zero while preserving significant ones, allowing the model to retain important features and maintain performance. Finally, PEP retrained the model from scratch , Vol. 1, No. 1, Article . Publication date: January 2025. A Thorough Performance Benchmarking on Lightweight Embedding-based Recommender Systems \u00b7 13", "Algorithm 1 Magnitude-based Pruning (MagPrune)": "procedure MagPrune( \ud835\udc61, \ud835\udc5b \ud835\udc5a\ud835\udc56\ud835\udc5b , E 10: 1: ) 2: \ud835\udc5b\ud835\udc62\ud835\udc5a _ \ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc5a\ud835\udc52\ud835\udc5b\ud835\udc61 \u2190 E .\ud835\udc60\u210e\ud835\udc4e\ud835\udc5d\ud835\udc52 [ 0 ] \u2217 E .\ud835\udc60\u210e\ud835\udc4e\ud835\udc5d\ud835\udc52 [ 1 ] 3: \ud835\udc5b\ud835\udc62\ud835\udc5a _ \ud835\udc5d\ud835\udc5f\ud835\udc62\ud835\udc5b\ud835\udc52 \u2190 \ud835\udc5b\ud835\udc62\ud835\udc5a _ \ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc5a\ud835\udc52\ud835\udc5b\ud835\udc61 \u2217 \ud835\udc61 4: \u02c6 E \u2190\u2225 E \u2225 \u22b2 Calculate absolute value by element-wise 5: \ud835\udc61\ud835\udc5c\ud835\udc5d\ud835\udc58 _ \ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc56\ud835\udc50\ud835\udc52\ud835\udc60 = \ud835\udc54\ud835\udc52\ud835\udc61 _ \ud835\udc61\ud835\udc5c\ud835\udc5d\ud835\udc58 ( \u02c6 E , \ud835\udc58 = \ud835\udc5b \ud835\udc5a\ud835\udc56\ud835\udc5b , \ud835\udc51\ud835\udc56\ud835\udc5a = 1 ) \u22b2 Get \ud835\udc5b \ud835\udc5a\ud835\udc56\ud835\udc5b largest elements for each row 6: \u02c6 E [ \ud835\udc61\ud835\udc5c\ud835\udc5d\ud835\udc58 _ \ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc56\ud835\udc50\ud835\udc52\ud835\udc60 ]\u2190\u221e 7: \ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc56\ud835\udc50\ud835\udc52\ud835\udc60 \u2190 \ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc60\ud835\udc5c\ud835\udc5f\ud835\udc61 ( \u02c6 E .\ud835\udc53 \ud835\udc59\ud835\udc4e\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b ()) 8: E [ \ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc56\ud835\udc50\ud835\udc52\ud835\udc60 [ : \ud835\udc5b\ud835\udc62\ud835\udc5a _ \ud835\udc5d\ud835\udc5f\ud835\udc62\ud835\udc5b\ud835\udc52 ]] \u2190 0 9: return E end procedure with the same parameter initialization with the found pruning mask. PEP also influences various other pruning methods [35, 39, 47, 62], generally regarding finding the winning lottery ticket. 4.1.5 OptEmbed. OptEmbed [39] considers pruning models as two tasks separately: Which feature to keep represented by a binary mask m \ud835\udc52 , and what is the dimension size should be allocated to each field - represented by an integer array m \ud835\udc51 . The model is trained with three separate steps. The first step is to train a supernet model and find m \ud835\udc52 , while possible values for m \ud835\udc51 are sampled from uniform distribution and incorporated into the training procedure. In the second step, OptEmbed performs an evolutionary search to find the best settings for m \ud835\udc51 . Finally, they retrained the model from scratch with the same parameter initialization with the found pruning mask. We chose OptEmbed due to its better training efficiency compared to other NAS-based approaches. 4.1.6 CERP. CERP [35] integrates STR into two balanced embedding tables E 1 and E 2 . To compensate for two sparse meta-embedding tables, the authors incorporate a regularization loss and opt for vector summation as the combination operation, thus creating a dense embedding vector from two sparse meta-embedding vectors. CERP is a prime example of a hybrid approach between the two most common LERSs: compositional embedding and pruning.", "4.2 Magnitude-based Pruning as An Intuitive Baseline": "In this section, we describe magnitude-based pruning (MagPrune), a pruning method we have proposed for lightweight embeddings. MagPrune is a simple, intuitive, and easy-to-deploy baseline in a wide range of recommendation tasks. We also provide the rationale for its working mechanism in embedding pruning. To be specific, for either the content-based recommendation or collaborative filtering task, we first train the base recommender with \ud835\udc3f 2 regularization. Given the formulation of \ud835\udc3f 2 , it restrains the magnitude for model parameters, and further encourages a lower magnitude for less vital parameters. As such, for the learned embedding table E , we sort all entries' absolute values and set the lower value to zeros until the memory budget is satisfied. This process is defined in Algorithm 1. In the above steps, \ud835\udc61 \u2208 ( 0% , 100% ] is a specified target compression ratio, and \ud835\udc5b \ud835\udc5a\ud835\udc56\ud835\udc5b is the minimum number of parameters allocated to each embedding row. For the CTR task, \ud835\udc5b \ud835\udc5a\ud835\udc56\ud835\udc5b will be set to 0, while for the CF task, \ud835\udc5b \ud835\udc5a\ud835\udc56\ud835\udc5b will be searched by choosing the value that optimizes the validation NDCG. This hyperparameter is introduced with the intuition that each user and item should have at least one parameter representing their information. In what follows, we justify the reason why MagPrune is able to be competent as a baseline for embedding pruning. In Figure 2, we plot the relationship between the frequency of discrete features and the mean absolute magnitude of their embeddings learned with \ud835\udc3f 2 regularizer. In the content-based recommendation dataset Criteo , Vol. 1, No. 1, Article . Publication date: January 2025. 14 \u00b7 Tran, et al. 5 10 15 log frequency 0.00 0.01 0.02 0.03 0.04 mean magnitude (a) Field 1 features in Criteo 5 10 15 log frequency 0.00 0.02 0.04 0.06 mean magnitude (b) Field 2 features in Criteo Fig. 2. Log frequency of features and their respective mean magnitude in the learned embeddings. Dataset details are provided in Section 5. We visualize all features in the first and second feature fields in Criteo. For Gowalla, we limit the user/item number to 200 for better visibility. 2 4 6 log frequency 0.2 0.3 0.4 0.5 mean magnitude (c) Items in Gowalla 2 3 4 5 6 log frequency 0.2 0.4 0.6 0.8 1.0 mean magnitude (d) Users in Gowalla (Figure 2a and 2b), embeddings with higher frequency correspond to the common features, whose embedding magnitude appears to be higher than that of the low-frequency features. This phenomenon depicts how the \ud835\udc3f 2 and recommender systems' embedding table work together. In short, low-frequency features have lower contributions to the main recommendation loss, leading to zero or near-zero gradients for their embeddings. As a result, their embeddings are predominantly influenced by gradients from the \ud835\udc3f 2 term, which keeps most of their values within a small range. This aligns with a general observation [37, 71] that the high-frequency features appear in more training samples, and their embeddings are usually more informative and dominant when generating predictions. Thus, MagPrune can effectively take out the less informative dimensions of low-frequency feature embeddings. Interestingly, in the collaborative filtering dataset Gowalla (Figure 2c and 2d), due to the nature of the graphbased model LightGCN, a different trend is observed as the high-frequency features (i.e., popular users and items) tend to have a lower average magnitude in their learned embeddings. In this occasion, pruning the embedding table by maintaining high-magnitude embedding dimensions means that, more emphasis is laid on users/items that are in the mid or tail range of the long-tail distribution. Furthermore, as the user/item representations in Gowalla are learned by LightGCN, popular users/items are in fact high-degree nodes in the interaction graph with abundant neighbors, hence being able to mostly depend on their neighbors' embeddings to form their own representations. Generally, for learned model weights, an absolute value close to zero commonly indicates less contribution to the model outputs. Following this intuition, we can remove most parameters with low magnitudes. In Section 6, we will further demonstrate the competitive recommendation accuracy achieved by MagPrune. At the same time, MagPrune provides strong efficiency advantages due to its capability of fitting multiple target compression ratios after a one-off training cycle.", "5 EXPERIMENT SETTINGS": "To answer the three questions raised in Section 1, we have designed a series of experiments to benchmark the performance and efficiency of the selected LERSs. In this section, we present our experimental settings in detail. Table 2. Statistics of the preprocessed datasets for content-based recommendation (CTR prediction). , Vol. 1, No. 1, Article . Publication date: January 2025. A Thorough Performance Benchmarking on Lightweight Embedding-based Recommender Systems \u00b7 15 Table 3. Statistics of the preprocessed datasets for collaborative filtering (top\ud835\udc58 recommendation).", "5.1 Datasets": "For the content-based recommendation (CTR prediction) task, we utilize two widely used benchmark datasets: Criteo 2 and Avazu 3 [37, 39]. The data pre-processing logic is based on [74]. Table 2 shows the core statistics of processed datasets. For Criteo, we apply the optimal solution from the Criteo contest, where we discretize each value \ud835\udc65 in numeric feature fields to \u2308 log 2 ( \ud835\udc65 )\u2309 if \ud835\udc65 > 2. For Avazu, we remove the 'ID' feature, which is unique for every sample and not useful for the CTR task. Then, for both datasets, we replace infrequent features (appearing less than 10 times in Criteo and 2 times in Avazu) as out-of-vocabulary (OOV) tokens. For both two datasets, we randomly split them into 8:1:1 as the training, validation, and test sets respectively. For the collaborative filtering (top\ud835\udc58 recommendation) task, we utilize two datasets, Gowalla and Yelp2018 , that are widely adopted in the literature [21, 63]. The main statistics are presented in Table 3. To enhance reproducibility, we directly employ the data split provided by LightGCN [21]. As [21] did not supply a validation set, we generate our own training and validation sets from the original training data. Specifically, we divide each user's interactions in the training set with a 9-to-1 ratio, where the former part is used for training and the latter is for validation.", "5.2 Evaluation Protocols": "Each method is tested under three sparsity rates \ud835\udc61 50% (low sparsity), 80% (moderate sparsity), and 95% (high sparsity). To calculate the sparsity rate, we only consider the embedding table parameters instead of all trainable parameters. The sparsity rate \ud835\udc61 is defined as follows:  where the memory function \ud835\udc40 (\u00b7) outputs the number parameters to construct embedding table. We implemented different functions to calculate parameters to suit each compression method. 5.2.1 Evaluating Recommendation Accuracy. We adopt different evaluation metrics for CTR prediction and top\ud835\udc58 recommendation tasks, which are introduced below. \u00b7 For CTR prediction, we evaluate all models with LogLoss and AUC (area under the ROC curve). AUC measures the probability that a random positive example has a higher probability than a random negative example. In RSs, an improvement of 0.001 in AUC is generally considered significant [40, 74]. LogLoss measures the difference between the predicted likelihood of a click and the actual outcome. It is worth noting that AUC only considers ranking, while LogLoss takes into the exact output value. The higher AUC indicates better RSs, while lower LogLoss indicates better performance. Each compression method is applied once to the single general embedding table instead of separately for each field. Based on the settings from DaisyRec [58], we train each base configuration for 30 trials, with 15 epochs for each trial with AUC as the target metric. We choose the best configuration based on the validation set from those 30 \u00d7 15 checkpoints. 2 https://www.kaggle.com/c/criteo-display-ad-challenge 3 https://www.kaggle.com/c/avazu-ctr-prediction/data , Vol. 1, No. 1, Article . Publication date: January 2025. 16 \u00b7 Tran, et al. \u00b7 For top\ud835\udc58 recommendation, we adopt the commonly used ranking metric NDCG@ \ud835\udc58 and Recall@ \ud835\udc58 with \ud835\udc58 = 20. Normalized Discounted Cumulative Gain (NDCG) evaluates the quality of a top\ud835\udc58 recommendation by comparing the ranked relevance of recommended items to the ideal ranking. Recall quantifies the proportion of relevant items successfully retrieved out of the total relevant ones available in the test set. In the evaluation step of top\ud835\udc58 recommendation, we first calculated scores for each user and pair, then removed all existing pairs in the training set. Finally, for each user, we take the top \ud835\udc58 scored items. Each compression method, except for the pruning-based ones, is applied individually to user and item embedding tables separately. The exception is due to the difficulty in attaining the target sparsity for both embedding tables simultaneously in pruning-based scenarios. Similarly to CTR prediction, we train each base configuration for 30 trials, with 40 epochs for each. Similarly, the best configuration is chosen based on the validation performance from all checkpoints. 5.2.2 OptEmbed [39] Modification. In all CF settings, we remove feature mask m \ud835\udc52 , which chooses rows to prune, thus keeping every user and item embedding vectors. Because the authors didn't design the method to reach a flexible memory, besides providing the original method's performance, we modify the distribution of sampling m \ud835\udc51 in evolutionary step to increase sparsity rate when required. Specially, Lyu et al. [39] use a uniform distribution to sample the embedding dimension mask m \ud835\udc51 , which results in the expected sparsity rate of roughly 50%. To increase sparsity rates, we modified the distribution of OptEmbed based on [45]. The modified distribution is defined as follows:  where \ud835\udc5d \ud835\udc56 is probability of sampled embedding size \ud835\udc56 , \u210e is maximum hidden size, \ud835\udefc is a hyperparameter to control the distribution. If \ud835\udefc = 1, the above distribution is equal to the original uniform distribution used by OptEmbed [39]. We have following equations:   The expected hidden size is:  With the above formula, we could approximate \ud835\udefc with gradient descent and mean squared error. Note that for the original uniform distribution, E ( \ud835\udc51 ) = ( \u210e + 1 )/ 2, which approximated the 50% sparsity rate ( \u210e / 2). Then, we sample from the proposed distribution and only take results with a sparsity higher than the target sparsity. , Vol. 1, No. 1, Article . Publication date: January 2025. A Thorough Performance Benchmarking on Lightweight Embedding-based Recommender Systems \u00b7 17 In 80% of Criteo - DeepFM and Avazu - DCN pairs, we keep the original uniform distribution but only take candidates with sparsity higher than the target because the original model already has a close sparsity rate with the target. We only use our method to increase sparsity and don't train the model with lower sparsity than the original models. 5.2.3 Implementation Details. We hereby provide an additional note on the hyperparameter tuning process: \u00b7 For compositional-based methods (TTRec, QR and DHE), we first find a hyperparameter configuration that satisfies the parameter budget constraint. Then, we keep modifying the hyperparameters within the parameter budget (e.g., adding more MLP layers with reduced width in DHE). Finally, for each parameter budget, we choose the best configuration for every dataset-backbone combination. \u00b7 For other methods, it is trickier to control the trade-off between accuracy and model size. Generally, for these methods (PEP, OptEmbed and CERP), we use the existing configuration reported by the authors if possible. If not, we focus on tuning the hyperparameters specific to each method, e.g., the soft threshold initialization in PEP. After the optimal method-specific hyperparameters are tuned, we finetune the general hyperparameters, such as learning rate and weight decay. This step is done through TPE as per Section 3.3, which is a widely used hyperparameter optimization approach [58, 61]. 5.2.4 Evaluating Cross-task Transferability. To evaluate each method's cross-task transferability, we calculate the performance retain ratio between the compressed model and the original model:  In our experiments, we employ NDCG as the evaluation metric for CF and use LightGCN as the model backbone due to its superior performance. For the CTR task, we use AUC as the evaluation metric and DCN as the backbone, selected for its improved performance compared to DeepFM. Then, we compute the overall performance retain ratio by averaging the results between two datasets. 5.2.5 Evaluating Real-world Efficiency. For efficiency, we aim to benchmark two main metrics - runtime and peak memory of training and inference phase on two devices, namely a GPU workstation and a Raspberry Pi which respectively mimic the deployment environments of a GPU server and a smaller edge device. \u00b7 The GPU workstation uses an i7-13700K CPU, NVIDIA RTX A5000 GPU, and 32GB RAM. \u00b7 The Raspberry Pi is the 4B generation with quad-core Cortex-A72 (ARM v8) and 4GB SDRAM. We implemented most methods using Python and PyTorch High-level API. For deployment, we will utilize the corresponding compiled version of Pytorch (the CUDA version for workstations and the ARM version for Raspberry Pi). For methods that cannot be with PyTorch high-level API, namely TTRec and accessing elements in sparse matrices, we provided the Numba and CUDA implementation. We deploy all LERSs in both environments, and record the runtime during both training and inference, as well as the peak VRAM (for GPU) and RAM (for Raspberry Pi) usage. Based on device capacity, the batch sizes in use are 2,048 for the workstation, and 64 for the Raspberry Pi. For edge device training benchmarks, we only ran 20% and 5% of an epoch for CF and CTR, respectively, then linearly scaled the runtime accordingly. We use DeepFM as backbone for CTR prediction and LightGCN as backbone for top\ud835\udc58 recommendation. In the CTR task, the runtime is measured as the time needed to get the predicted score \u02c6 \ud835\udc66 . In top\ud835\udc58 recommendation, the runtime is measured as the necessary time to compute all user and item embeddings. This is because the runtime for recovering a full user/item embedding from the reduced embedding parameters (1.87ms) is substantially more significant than calculating the user-item similarity via dot product (0.041ms) and ranking (0.1ms). , Vol. 1, No. 1, Article . Publication date: January 2025. 18 \u00b7 Tran, et al. Table 4. Hyperparameter settings. LightGCN and NeuMF correspond to the base models for collaborative filtering, while DeepFM and DCN correspond to content-based recommendation tasks. The hyperparameters are searched with the TPE [4] algorithm. We have benchmarked all methods under the moderate 80% sparsity rate configuration. In this work, we adopt the conventional SparseCSR format to store sparse matrices, acknowledging that the overhead of sparse matrices depends on the level of sparsity [23].", "6 ANALYSIS ON EXPERIMENTAL RESULTS": "In this section, we discuss the results of the experiments conducted using the aforementioned settings.", "6.1 Overall CTR Performance (RQ1)": "Table 5 shows the CTR prediction task experiment results. In line with [9, 69], we observe that for the Criteo dataset, the pruning methods usually outperform others, while in the Avazu dataset, the compositional encodingbased methods outperform others. Coleman et al. [9] hypothesize that the Criteo dataset has a heavier tail feature distribution than the Avazu dataset, consequently, having more colliding tokens and worsening the errors. However, we could see that the log loss of pruning methods in the Avazu dataset is lower than compositional encoding-based methods. Interestingly, PEP and MagPrune perform similarly at a low sparsity rate across three out of four pairs of dataset and backbone. This similarity indicates the strong performance of MagPrune, especially in low sparsity rates. Additionally, the difference is minimal between the original model and the worst performance in 95% compression rate (less than 1% for Criteo and 3% for Avazu for both backbones). , Vol. 1, No. 1, Article . Publication date: January 2025. A Thorough Performance Benchmarking on Lightweight Embedding-based Recommender Systems 19 \u2022 Table 5. Results on CTR prediction with DeepFM and DCN as the base models. The sparsity \ud835\udc61 indicates the number of pruned parameters (the higher \ud835\udc61 is, the more parameters are reduced). #Para indicates the parameter size of the embedding table (M: million, K: thousand), and Loss refers to the LogLoss. In each column, the best result is marked in bold and the second best one is underlined. For the Criteo dataset, PEP demonstrated the most competitive performance relative to other methods across two backbones and three sparsity rates, only failing for DCN at the medium sparsity rate. With only 5% of the parameters, PEP could achieve the performance of the original DCN model, further highlighting PEP's competitiveness on the Criteo dataset. As mentioned above, the pruning methods generally outperform the compositional-based methods. For example, in the 50% sparsity rate, both PEP and MagPrune achieved the highest performance for both backbones. OptEmbed relative performance for DCN is higher than DeepFM, as shown at 80% and 95% sparsity rates. It appears that OptEmbed is more suitable with the DCN backbone compared to DeepFM. For the Avazu dataset, we would argue that TTRec is generally a good choice for the high sparsity rate, while MagPrune and QR are for low sparsity. First, TTRec consistently demonstrated impressive performance, being , Vol. 1, No. 1, Article . Publication date: January 2025. 20 \u00b7 Tran, et al. Table 6. Results on top\ud835\udc58 recommendation with LightGCN and NeuMF as the base models. In each column, the best result is marked in bold and the the second best one is underlined. the second-best for both backbones. Similarly, in the low sparsity rate, both QR and MagPrune achieve the top three in AUC for both backbones. Regarding the fluctuation in the relative performance between sparsity rates, one potential explanation is that at lower sparsity rates, removing parameters has less catastrophic impact than at higher sparsity rates. Thus, simple compression methods under proper tuning already achieved competitive performance at lower sparsity rates. While higher sparsity rates, where the parameters are much less, require complex methods to maintain the performance. By taking a deep look at each method, we could see that TTRec and DHE could have performance gains when allocated fewer parameters. Furthermore, CERP performs competitively despite being initially developed for CF tasks. An interesting note is that we observe no performance gain for OptEmbed on the validation set through the retraining step of the CTR prediction in most experiment settings (except for DCN - Avazu). In this task, magnitude pruning shows competitive results even at a high compression rate. , Vol. 1, No. 1, Article . Publication date: January 2025. A Thorough Performance Benchmarking on Lightweight Embedding-based Recommender Systems \u00b7 21", "6.2 Overall Collaborative Filtering Performance (RQ1)": "Table 6 shows the CF task experiment results. PEP delivers competitive results in every setting. In general, similar to the CTR task, simple methods (QR, MagPrune) perform better at low sparsity rates, while more complex methods (TTRec, CERP) perform better at higher sparsity rates. As mentioned above, we hypothesize that complex model compression methods preserve essential features more effectively, which becomes vital as sparsity increases, thus maintaining better performance in higher sparsity rates. On the other hand, simple methods perform well at lower sparsity rates, where the impact of removed parameters is less pronounced, making their straightforward approach sufficient. Compared to the CF task, where the relative performance depends more on the dataset side, the relative performance in the CF task depends more on backbones, as NeuMF differs from LightGCN more greatly compared to DCN and DeepFM. Additionally, LightGCN consistently outperforms NeuMF across all settings, highlighting the robust performance of graph-based models in CF in general and LERS in particular. For NeuMF, PEP outperforms all the other methods significantly in medium and high sparsity rates while still having competitive performance in low sparsity rates, consistently in the top 2. In the medium sparsity rate, the second best is OptEmbed; however, it cannot maintain its performance in the high sparsity rate. The performance of MagPrune already significantly dropped as it cannot leverage the graph structure. In the high sparsity rate, the second best is CERP - the method specifically catered to high sparsity scenarios. Interestingly, for NeuMF, the gap between the best method and the others is much more notable. For the LightGCN backbone, simple MagPrune achieves the best results at a moderate compression rate, followed by PEP, which shows that the fine-grain pruning-based method achieves high performance at various compression rates. Additionally, DHE also demonstrated competitive performance with LightGCN. In contrast, OptEmbed assigns zero to the latter dimensions of most embedding, leading to zero embedding in most latter dimensions with LightGCN's propagation method. Moreover, during our hyperparameter tuning process for OptEmbed 95% for both datasets, the best hyperparameter's InfoNCE loss coefficient is 0, as they don't converge with InfoNCE loss, further limiting their performance. This has led to poor performance of OptEmbed for LightGCN backbone.", "6.3 Cross-task Transferability (RQ2)": "Table 7 shows the performance ratio between the compressed model and the original model. First, we could observe that both PEP and MagPrune consistently perform well in two recommendation tasks when the sparsity rate is low. At higher sparsity rates of the CF task, PEP maintains its strong performance, followed by DHE, a method designed for collaborative filtering. DHE performs less competitively in the CTR task, especially in lower sparsity rate. This might be because a neural network with the same parameter size has a much stronger representation capability than the traditional embedding table, thus making DHE more prone to overfit in a higher parameter budget. In contrast, OptEmbed demonstrates strong results on CTR but performs poorly on CF. As mentioned above, OptEmbed is not suitable for the LightGCN model. For the CTR task, MagPrune still performs well at the medium sparsity rate but fails slightly short compared to other methods at the high sparsity rate. Surprisingly, at the high sparsity rate, PEP performs best for LightGCN but is the worst for DCN. TTRec appears to be a more consistent method for this task, especially as the sparsity rate increases. This result might be because the Avazu dataset presents more challenges for the methods. Consequently, when averaging the metric results, the overall performance is more heavily influenced by the results from the Avazu dataset. And PEP, despite being the best for the Criteo dataset, is the worst for Avazu. While TTRec has a more consistent performance across these two datasets. The poor performance of PEP on Avazu compared with that on Criteo can be attributed to differences in dataset sparsity and feature distribution. Though the two datasets have similar numbers of interactions, Avazu has many more features, and the long-tail distribution , Vol. 1, No. 1, Article . Publication date: January 2025. 22 \u00b7 Tran, et al. Table 7. Performance retain ratio between the original and the compressed models. In each column, the best result is marked in bold and the second best one is underlined. among features is more severe than Criteo. As PEP uses weight decay to determine the embedding sizes, a larger number of low-frequency features in Avazu are assigned fewer parameters, hurting the final model performance. Nonetheless, the gap between the original model and the higher compression rate model in CF is much higher than in CTR prediction (more than 20% NDCG@20 drop for the most optimal method). There are various possible explanations for this gap. First, the CTR prediction task has multiple fields for a single output, while the CF task only has two fields, which makes it much harder for CF-based models to predict with limited information. Second, most methods are initially tested on the CTR prediction task, making methods provide more competitive results. Third, the CTR model embedding is much bigger than the CF model, thus having more room to compress the model.", "6.4 Real-world Efficiency (RQ3)": "Figure 3 shows the resource usage in training and inference in the Criteo dataset, and figure 4 depicts the resource usage for the Yelp2018 dataset. The packages are Python library data required to be loaded on RAM (mainly PyTorch), whose memory consumption is independent from the models used. Package memory consumption can be reduced by using more compact coding languages such as C++. Metadata is mainly the hash dictionary that maps the raw string of features to its corresponding ID, which is especially memory-intensive as it increases with the number of features. There are techniques to reduce the memory cost of this dictionary [14], but they are outside the scope of this work as we lay emphasis on the memory consumption from the model itself. It is worth noting that the Raspberry Pi environment is different from the workstation setting. This is because packages and , Vol. 1, No. 1, Article . Publication date: January 2025. A Thorough Performance Benchmarking on Lightweight Embedding-based Recommender Systems \u00b7 23 0 320 Original QR DHE TTRec (no cache) TTRec (build cache) TTRec (cache) PEP Find Mask PEP Retrain OptEmbed OptEmbed-D OptEmbed Retrain CERP Find Mask CERP Retrain 117.64 84.43 187.33 243.34 122.85 122.00 119.47 128.49 122.41 87.78 1200 3200 2123.37 1368.57 1391.73 seconds (a) Workstation Train Time 0 1300 526.34 158.73 209.16 236.72 250.59 990.68 542.92 660.87 525.93 659.01 281.41 168.28 7400 8200 7761.69 MiB (b) Workstation Train VRAM 0 5 10 15 20 hours 7.00 4.22 OOM Not supported Not supported Not supported 16.50 8.60 7.31 7.19 8.42 6.65 4.64 (c) Raspberry Train Time 0 500 1000 1500 MiB 1090.72 768.29 OOM Not supported Not supported Not supported 1459.55 1128.64 1112.09 1114.34 1328.79 1054.75 823.67 (d) Raspberry Train Mem 0 3 Original QR DHE DHE TTRec PEP* OptEmbed OptEmbed* CERP 0.58 0.61 0.92 0.58 0.86 0.60 5 15 8.93 10.90 8.31 miliseconds (e) Workstation Infer Time 0 400 94.56 45.54 317.07 52.10 76.03 94.56 72.66 48.63 4000 5000 4420.07 Load model Inference memory MiB (f) Workstation Infer VRAM 0 10 7.46 7.80 OOM 7.91 7.49 8.04 7.88 600 2200 1464.89 757.71 miliseconds (g) Raspberry Infer Time 0 200 400 600 800 1000 1200 MiB 654.07 574.44 OOM 582.84 782.14 603.28 654.15 599.91 573.27 Packages Metadata Model Inference (h) Raspberry Infer Mem Fig. 3. Training and Inference resource usage on the Criteo dataset. The asterisk '*' in inference means we store the weight matrix under SparseCSR format. \u2021 means creating hash codes on-the-fly for DHE. TTRec implements a custom CUDA kernel for training, which naturally cannot be implemented on the CPU. TTRec's cache is assumed to be 10% of the original model. 'Mem' is a shorthand for memory, 'Packages' refers to Python packages and overhead in general, and 'Metadata' refers to the CPU memory required to store the mapping from features (as string data type) to the corresponding feature IDs (as integer data type). metadata do not exist in VRAM used by workstation's GPU, and only exist in the CPU RAM of Raspberry Pi. As some methods are comprised of multiple steps, we benchmark those intermediate steps separately for a clearer view of each method's performance: \u00b7 TTRec [65]: We test two main scenarios for TTRec training, i.e., with and without cache. The original paper suggests storing the most accessed embedding vectors in an uncompressed format, requiring a few warm-up iterations (e.g., the first epoch) to build this cache. This step is denoted as 'build cache'. Once the cache is built, it is treated as normal trainable parameters and directly updated with backward propagation in the following iterations besides the TT-cores. To study the overhead introduced for the cache-building process, we include the without-cache configuration in our experiment. \u00b7 'Find Mask' and 'Retrain': Most pruning methods first find the embedding mask, followed by a retraining step. If this is the case, we provide results for each step separately. \u00b7 OptEmbed [39]: We benchmark two 'Find Mask' versions for OptEmbed: OptEmbed (original model) and OptEmbed-D (removes feature mask \ud835\udc5a \ud835\udc52 ). We could train all methods on Raspberry Pi except for TTRec, which is designed specifically for GPU. Among the methods, QR, the simplest, uses the least resource, while DHE is the most resource-consuming as it requires an MLP model inside. Overall, an increased memory footprint is mostly observed during training, and the majority of the methods can improve their memory usage during inference. As for the additional training memory overhead, pruning-based methods (PEP, OptEmbed) yield increased memory usage due to the embedding mask used during fine-tuning, and the full embedding table has to be in place for learning the embedding mask as well. , Vol. 1, No. 1, Article . Publication date: January 2025. 24 \u00b7 Tran, et al. 0 25 50 75 100 125 seconds Original QR DHE TTRec (no cache) TTRec (build cache) TTRec (cache) PEP Find Mask PEP Retrain OptEmbed OptEmbed-D OptEmbed Retrain CERP Find Mask CERP Retrain 57.47 57.99 106.69 78.92 78.99 73.44 65.50 55.94 62.82 57.53 56.64 62.13 58.19 (a) Workstation Train Time Original 1.87 QR DHE TTRec PEP* OptEmbed OptEmbed* CERP 2.13 2.18 3.22 1.88 2.53 2.13 0 miliseconds 727.29 673.07 675.33 678.36 683.46 794.98 732.07 727.38 726.32 760.95 708.43 680.49 0 MiB 500 2000 1500 1000 (b) Workstation Train VRAM 143.03 126.48 130.89 133.52 143.03 133.74 136.76 542.85 Adj Matrix Model Inference 0 MiB 10 8 6 4 2 600 200 400 12 4.41 4.39 Not supported Not supported Not supported 5.07 4.53 4.70 4.59 4.49 4.67 4.50 0 hours 5 10 15 (c) Raspberry Train Time 1030.38 1070.99 1222.70 1164.61 1051.85 1164.24 1080.34 0 2000 4000 6000 miliseconds 1179.12 1187.43 Not supported Not supported Not supported 1474.14 1215.49 1148.36 1171.77 1257.06 1318.94 1184.26 0 MiB 3000 2000 1000 (d) Raspberry Train Mem 607.25 663.52 994.62 600.15 641.37 617.31 647.02 679.84 Packages Adj Matrix Model Inference 0 500 1000 1500 MiB (e) Workstation Infer Time (f) Workstation Infer VRAM (g) Raspberry Infer Time (h) Raspberry Infer Mem Fig. 4. Training and Inference resource usage on Yelp2018 dataset. The asterisk '*' in inference means we store the weight matrix under SparseCSR format. TTRec implements a custom CUDA kernel for training, which naturally cannot be implemented on the CPU. TTRec's cache is assumed to be 10% of the original model. 'Mem' is a shorthand for memory, 'Packages' refers to Python packages and overhead in general, and 'Metadata' refers to the CPU memory required to store the mapping from features (as string data type) to the corresponding feature IDs (as integer data type). For DHE, the overhead stems from computing high-dimensional hash codes and training an MLP to generate dense embeddings on-the-fly with those codes. For instance, the hash code dimension (1024) is much larger than the embedding dimension (16) in DHE, leading to a significant memory overhead. In contrast, methods using compositional embeddings like QR, TTRec and CERP can effectively cut the memory usage in most cases because of the inherently smaller parameter space they use to represent all features. It is worth noting that DHE performance could be further improved with other methods from lightweight machine learning models such as pruning and quantization; however, this is outside of our research scope. Although training on edge devices is theoretically possible, it remains impractical in most scenarios, as training a single epoch requires over four hours due to the limited computational resources available. In the context of the CTR task, TTRec demands significant resources during its first epoch to create a cache but demonstrates substantially increased efficiency in the subsequent phase. Compositional-based methods (except for DHE) and CERP have better training efficiency than the original model in both VRAM and time consumption. In contrast, most pruning-based methods use more resources during the training phase as they require extra memory to store the found mask. Another key contributor to the training efficiency is the \ud835\udc3f 2 regularization optimization. While the gradient for log loss is sparse, the gradient for \ud835\udc3f 2 regularization is dense. Given the large amount of RSs' parameters, this results in slow computation when updating the model's parameters. Compositional-based methods benefit from this as they have less parameter count in the training phase. In contrast, pruning methods still retain their dense format in the training phase. Regarding top\ud835\udc58 recommendation efficiency, the memory inference cost is much higher than the model size. There are two main reasons for this. First, LightGCN requires extracting the full embedding tables for feedforward. Second, the intermediate layer results dominate memory consumption. So, despite saving the storage 9.94 1531.20 11.30 5176.44 2560.91 , Vol. 1, No. 1, Article . Publication date: January 2025. A Thorough Performance Benchmarking on Lightweight Embedding-based Recommender Systems \u00b7 25 Table 8. Absolute (Abs.) and relative differences in efficiency metrics of various methods during inference compared to original models in the workstation settings. cost, smaller LightGCN models don't benefit from saved memory costs. Consequently, training and inference memory usage differ much less significantly between each method, and no method could reduce both time and memory consumption in training efficiency. Similar trends with the CTR task, such as pruning consuming more training resources, are also observed. 6.4.1 Overall Summary of The Real-world Efficiency Test. It is worth mentioning that, none of our chosen methods outperforms the original model regarding inference runtime. This is because the compressed model parameters generally incur some computational overhead when the model is being decompressed during inference, e.g., TTRec requires a sequence of tensor multiplication operations to recover the full-size embedding table. However, methods that use less memory could benefit from larger batch sizes, and LERSs could also reduce inference time when required to transfer embedding from bulk storage (e.g., disk) to faster storage (e.g., RAM, VRAM) [24], but this is outside our research scope. Compositional embedding methods creates embeddings from meta vectors, while pruning pays the extra overhead in accessing embeddings stored in a sparse matrix format. 6.4.2 Additional Results on The Computational Overhead from Embedding Compression. To better illustrate the overhead differences, we calculate the runtime and memory differences between the original model and their compressed versions, and the results are shown in Table 8. Firstly, QR provides the smallest sacrifice in efficiency, evidenced by the lowest runtime overhead and largest memory reduction. DHE has the highest runtime overhead and does not reduce peak memory consumption during inference. Secondly, DHE is able to reduce the parameter consumption from embedding table, in our case, DHE's heavy MLP component has offset the embedding parameter reduction. Lastly, despite both PEP and OptEmbed are pruning-based methods and use the same underlying implementation with SparseCSR [23], OptEmbed is faster than PEP due to a more hardware-friendly implementation.", "6.5 Ablation Study on MagPrune": "In this section, we explore the effect of \ud835\udc5b _ \ud835\udc5a\ud835\udc56\ud835\udc5b in MagPrune performance. Figure 5 shows the performance of MagPrune across different \ud835\udc5b _ \ud835\udc5a\ud835\udc56\ud835\udc5b . In general, optimizing \ud835\udc5b _ \ud835\udc5a\ud835\udc56\ud835\udc5b can further improve model performance, especially under moderate sparsity. These results suggest that ensuring each user and item has at least some non-zero value can positively impact the performance of LightGCN. However, it is crucial to determine \ud835\udc5b _ \ud835\udc5a\ud835\udc56\ud835\udc5b through systematic optimization rather than arbitrary selection, as too high \ud835\udc5b _ \ud835\udc5a\ud835\udc56\ud835\udc5b can decrease performance. , Vol. 1, No. 1, Article . Publication date: January 2025. 26 \u00b7 Tran, et al. Fig. 5. Performance of MagPrune with different \ud835\udc5b _ \ud835\udc5a\ud835\udc56\ud835\udc5b . For Yelp2018 and Avazu, the backbones are respectively LightGCN and DCN. 0 1 2 3 4 5 6 0.0195 0.0200 0.0205 0.0210 NDCG@20 nmf-80 0.0 0.2 0.4 0.6 0.8 1.0 0.0106 0.0108 0.0110 0.0112 0.0114 nmf-95 0 2 4 6 8 10 12 n_min 0.0495 0.0496 0.0497 0.0498 NDCG@20 lightgcn-80 0.0 0.5 1.0 1.5 2.0 2.5 3.0 n_min 0.0175 0.0180 0.0185 0.0190 0.0195 lightgcn-95 (a) Yelp2018 0 1 2 3 4 5 6 0.0540 0.0545 0.0550 0.0555 NDCG@20 nmf-80 0.0 0.2 0.4 0.6 0.8 1.0 0.0230 0.0235 0.0240 0.0245 0.0250 0.0255 0.0260 nmf-95 0 2 4 6 8 10 12 n_min 0.120 0.121 0.122 0.123 0.124 0.125 0.126 NDCG@20 lightgcn-80 0.0 0.5 1.0 1.5 2.0 2.5 3.0 n_min 0.022 0.024 0.026 0.028 0.030 0.032 0.034 0.036 lightgcn-95 (b) Gowalla", "6.6 Study on Auxiliary Loss Functions": "50% 80% 95% 0 0 . 5 1 \u00b7 10 - 2 Sparsity Rate Weight Decay Yelp2018 50% 80% 95% 0 1 2 3 \u00b7 10 - 3 Sparsity Rate Avazu QR; DHE; PEP; Original Fig. 6. The relationship between the weight decay and the sparsity rate. For Yelp2018 and Avazu, the backbones are respectively LightGCN and DCN. As a common practice for recommendation model training, additional optimization objectives like \ud835\udc3f 2 regularization and self-supervision loss (e.g., InfoNCE) can potentially improve the generalizability of the trained , Vol. 1, No. 1, Article . Publication date: January 2025. A Thorough Performance Benchmarking on Lightweight Embedding-based Recommender Systems \u00b7 27 Fig. 7. The relationship between the InfoNCE coefficient and sparsity rates. 50% 80% 95% 0 0 . 2 0 . 4 0 . 6 0 . 8 Sparsity Rate InfoNCE Coefficient Yelp 50% 80% 95% 0 0 . 5 1 Sparsity Rate Gowalla QR; DHE; PEP; Original model. By default, we have also applied both \ud835\udc3f 2 and InfoNCE losses on top of the recommendation objective. In this section, we aim to further study the potential impact from those additional loss functions. Specifically, we showcase the best loss coefficient settings for three methods, namely QR, DHE, and PEP for demonstration. Figure 6 shows the relationship between weight decay ( \ud835\udc3f 2 coefficient) and sparsity rate. We observe that higher sparsity model prefers lower weight decay coefficient. Since these models are smaller, they are less prone to overfitting, lowering the demand for strong weight decays. Figure 7 depicts the relationship between InfoNCE and sparsity rates. Interestingly, we observe a similar trend with weight decay. One possible hypothesis is that InfoNCE improves LightGCN's performance by reducing the dimensional collapse ('over-squashing') phenomenon [7, 68], where the embeddings only span a low-dimension subspace instead of the entire feature space. When the model becomes smaller/sparse, there would be less dimensional collapse, reducing the necessity of InfoNCE loss.", "7 CONCLUSIONS AND RECOMMENDATIONS": "In this study, we conducted a comprehensive benchmark of various embedding compression methods on two main tasks of recommendation models. Our evaluation highlighted the trade-offs between performance and efficiency, offering valuable insights for both researchers and practitioners. However, exciting questions remain for future researchers to explore: \u00b7 Suggestions for future research on LERSs: First, since most methods currently hinder training efficiency, especially for graph-based collaborative filtering, we suggest that further studies aim to improve this aspect. Second, we recommend incorporating magnitude pruning as a simple yet effective baseline for future research on LERSs. Third, we encourage future researchers to include benchmarks for training and inference efficiency to better demonstrate their methods' effectiveness, particularly for inference, which is often overlooked or unimplemented. \u00b7 Relationship between dataset, compression rate, and method performance: Our experiments reveal varying performances across recommendation datasets; however, the reasons for these disparities remain , Vol. 1, No. 1, Article . Publication date: January 2025. 28 \u00b7 Tran, et al. unclear. Zhang et al. [69] observed similar patterns in their CTR task experiments. This opens a critical question for future research on the relationship between dataset characteristics and method performance. \u00b7 Further evaluations: While our study provides valuable insights, various metrics, such as energy consumption, diversity, and novelty, remain unexplored despite their importance for edge devices [12] and RSs [53]. Common libraries like CodeCarbon [54] lack support for edge devices. Additionally, our study is limited to Raspberry Pi and Python, which introduces considerable overhead and instability due to garbage collection. Future investigations could expand to include other programming languages and edge devices, enabling a more thorough benchmark of system efficiency. Additionally, we provide suggestions for practitioners on the effective application of LERS methods: \u00b7 First , in the CTR task, all compression techniques have drawbacks. The performance difference depends on dataset characteristics and sparsity. In our evaluation, pruning methods generally perform better on the Criteo dataset, while compositional embedding achieves better results on the Avazu dataset. Moreover, a method that performs well at one compression rate for a specific dataset is not guaranteed to perform well at another. However, in general, the difference between each method is minor. We therefore recommend starting with QR and PEP for the CTR task, as these methods have good training efficiency. Furthermore, in our tests, methods within the same category tend to demonstrate similar performance on a given dataset. After testing simpler methods, more complex ones within the best-performing category can be trained if necessary. \u00b7 Second , in the CF task, graph-based models outperform latent-factor-based models; therefore, we suggest focusing on LERS graph-based models. Moreover, the performance drop is more significant in the CF task, likely due to the greater difficulty in compressing CF-based models. Nonetheless, every method still provides competitive results compared to the baseline and methods developed specifically for the CF task. This suggests that future research on LERSs should investigate performance in CF tasks in general and in graph-based models in particular. \u00b7 Third , PEP typically outperforms other tested methods. Interestingly, simple approaches such as QR and MagPrune perform comparably to more complex ones, especially at lower sparsity rates. This suggests that practitioners should use simple methods when dealing with low sparsity rates, as these methods offer a good balance between simplicity and effectiveness. \u00b7 Fourth , most methods are deployable on edge devices with reduced memory consumption. Both pruning and compositional approaches create runtime overhead, which varies based on the deployment environment and specific methods. In general, compositional methods appear to be a better choice for training efficiency, while pruning is more suitable for edge device settings, as CPUs can efficiently access unstructured weight matrices. Furthermore, a reduction in storage does not necessarily translate to lower memory costs in training.", "ACKNOWLEDGMENTS": "This work is supported by the Australian Research Council under the streams of Future Fellowship (No. FT210100624), Discovery Early Career Researcher Award (No. DE230101033), Discovery Project (No. DP240101108 and No. DP240101814), and Linkage Project (No. LP230200892).", "REFERENCES": "[1] 2014. The Criteo Dataset. https://www.kaggle.com/c/criteo-display-ad-challenge [2] 2018. The Yelp Dataset. https://www.kaggle.com/datasets/yelp-dataset/yelp-dataset [3] Muhammad Ammad-Ud-Din et al. 2019. Federated collaborative filtering for privacy-preserving personalized recommendation system. arXiv preprint arXiv:1901.09888 (2019). , Vol. 1, No. 1, Article . Publication date: January 2025. A Thorough Performance Benchmarking on Lightweight Embedding-based Recommender Systems \u00b7 29 [4] James Bergstra, R\u00e9mi Bardenet, Yoshua Bengio, and Bal\u00e1zs K\u00e9gl. 2011. Algorithms for Hyper-Parameter Optimization. In NeurIPS . 2546-2554. [5] Han Cai, Ligeng Zhu, and Song Han. 2019. ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware. In ICLR . [6] Di Chai, Leye Wang, Kai Chen, and Qiang Yang. 2020. Secure federated matrix factorization. IEEE Intelligent Systems 36, 5 (2020), 11-20. [7] Huiyuan Chen, Vivian Lai, Hongye Jin, Zhimeng Jiang, Mahashweta Das, and Xia Hu. 2024. Towards mitigating dimensional collapse of representations in collaborative filtering. In WSDM . 106-115. [8] Tong Chen, Hongzhi Yin, Yujia Zheng, Zi Huang, Yang Wang, and Meng Wang. 2021. Learning Elastic Embeddings for Customizing On-Device Recommenders. In SIGKDD . 138-147. [9] Benjamin Coleman, Wang-Cheng Kang, Matthew Fahrbach, Ruoxi Wang, Lichan Hong, Ed H. Chi, and Derek Zhiyuan Cheng. 2023. Unified Embedding: Battle-tested feature representations for web-scale ML systems. In NeuRIPS . [10] Maurizio Ferrari Dacrema, Paolo Cremonesi, and Dietmar Jannach. 2019. Are we really making much progress? A worrying analysis of recent neural recommendation approaches. In RecSys . 101-109. [11] Wei Deng, Junwei Pan, Tian Zhou, Deguang Kong, Aaron Flores, and Guang Lin. 2021. DeepLight: Deep Lightweight Feature Interactions for Accelerating CTR Predictions in Ad Serving. In Proceedings of the 14th ACM International Conference on Web Search and Data Mining . 922-930. [12] Sauptik Dhar, Junyao Guo, Jiayi (Jason) Liu, Samarth Tripathi, Unmesh Kurup, and Mohak Shah. 2021. A Survey of On-Device Machine Learning: An Algorithms and Learning Theory Perspective. ACM Trans. Internet Things 2 (jul 2021). [13] Jonathan Frankle and Michael Carbin. 2018. The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks. In ICLR . [14] Tim Gubner, Viktor Leis, and Peter Boncz. 2021. Optimistically Compressed Hash Tables & Strings in theUSSR. SIGMOD Rec. (2021). [15] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: A Factorization-Machine based Neural Network for CTR Prediction. In IJCAI , Carles Sierra (Ed.). 1725-1731. [16] Saket Gurukar, Nikil Pancha, Andrew Zhai, Eric Kim, Samson Hu, Srinivasan Parthasarathy, Charles Rosenberg, and Jure Leskovec. 2022. MultiBiSage: A Web-Scale Recommendation System Using Multiple Bipartite Graphs at Pinterest. Proceedings of the VLDB Endowment 16, 4 (2022), 781-789. [17] Jialiang Han, Yun Ma, Qiaozhu Mei, and Xuanzhe Liu. 2021. Deeprec: On-device deep learning for privacy-preserving sequential recommendation in mobile commerce. In WebConf . 900-911. [18] Song Han, Jeff Pool, John Tran, and William J. Dally. 2015. Learning Both Weights and Connections for Efficient Neural Networks. In NeurIPS . 1135-1143. [19] F. Maxwell Harper and Joseph A. Konstan. 2016. The MovieLens Datasets: History and Context. ACM Trans. Interact. Intell. Syst. (2016), 19:1-19:19. [20] Ruining He and Julian McAuley. 2016. Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering. In WebConf . 507-517. [21] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, YongDong Zhang, and Meng Wang. 2020. LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation. In SIGIR . 639-648. [22] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural collaborative filtering. In WebConf . 173-182. [23] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. 2021. Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks. JMLR 22, 1 (2021), 10882-11005. [24] Samuel Hsia, Udit Gupta, Bilge Acun, Newsha Ardalani, Pan Zhong, Gu-Yeon Wei, David Brooks, and Carole-Jean Wu. 2023. Mprec: Hardware-software co-design to enable multi-path recommendation. In Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3 . 449-465. [25] Eric Jang, Shixiang Gu, and Ben Poole. 2017. Categorical Reparameterization with Gumbel-Softmax. In ICLR . [26] Steven A. Janowsky. [n. d.]. Pruning versus clipping in neural networks. Phys. Rev. A 39 ([n. d.]), 6600-6603. Issue 12. [27] Manas R. Joglekar, Cong Li, Jay K. Adams, Pranav Khaitan, and Quoc V. Le. 2019. Neural Input Search for Large Scale Recommendation Models. SIGKDD (2019). https://api.semanticscholar.org/CorpusID:195874115 [28] Wang-Cheng Kang et al. 2021. Learning to Embed Categorical Features without Embedding Tables for Recommendation. In SIGKDD . 840-850. [29] Aditya Kusupati et al. 2020. Soft Threshold Weight Reparameterization for Learnable Sparsity. In ICLR . [30] Maximilian Lam et al. 2023. GPU-based Private Information Retrieval for On-Device Machine Learning Inference. arXiv preprint arXiv:2301.10904 (2023). [31] Shiwei Li et al. 2023. Adaptive Low-Precision Training for Embeddings in Click-through Rate Prediction. In AAAI . [32] Shiwei Li et al. 2024. Embedding Compression in Recommender Systems: A Survey. ACM Comput. Surv. (jan 2024). [33] Dawen Liang, Laurent Charlin, James McInerney, and David M Blei. 2016. Modeling user exposure in recommendation. In WebConf . 951-961. , Vol. 1, No. 1, Article . Publication date: January 2025.", "30 \u00b7 Tran, et al.": "[34] Xurong Liang, Tong Chen, Lizhen Cui, Yang Wang, Meng Wang, and Hongzhi Yin. 2024. Lightweight Embeddings for Graph Collaborative Filtering. In SIGIR . 1296-1306. [35] Xurong Liang, Tong Chen, Quoc Viet Hung Nguyen, Jianxin Li, and Hongzhi Yin. 2023. Learning Compact Compositional Embeddings via Regularized Pruning for Recommendation. arXiv:2309.03518 [cs.IR] [36] Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2018. DARTS: Differentiable Architecture Search. arXiv preprint arXiv:1806.09055 (2018). [37] Siyi Liu, Chen Gao, Yihong Chen, Depeng Jin, and Yong Li. 2021. Learnable Embedding Sizes for Recommender Systems. In ICLR . https://openreview.net/forum?id=vQzcqQWIS0q [38] Jing Long, Tong Chen, Quoc Viet Hung Nguyen, Guandong Xu, Kai Zheng, and Hongzhi Yin. 2023. Model-Agnostic Decentralized Collaborative Learning for On-Device POI Recommendation. In SIGIR . 423-432. [39] Fuyuan Lyu et al. 2022. OptEmbed: Learning Optimal Embedding Table for Click-through Rate Prediction. In CIKM . 1399-1409. [40] Matteo Marcuzzo, Alessandro Zangari, Andrea Albarelli, and Andrea Gasparetto. 2022. Recommendation systems: An insight into current development and future research challenges. IEEE Access 10 (2022), 86578-86623. [41] Dheevatsa Mudigere et al. 2022. Software-Hardware Co-Design for Fast and Scalable Training of Deep Learning Recommendation Models. In Proceedings of the 49th Annual International Symposium on Computer Architecture . 993-1011. [42] Dina Nawara and Rasha Kashef. 2020. IoT-based Recommendation Systems - An Overview. In 2020 IEEE International IOT, Electronics and Mechatronics Conference (IEMTRONICS) . 1-7. [43] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018). [44] Niketan Pansare et al. 2022. Learning compressed embeddings for on-device inference. Proceedings of Machine Learning and Systems 4 (2022), 382-397. [45] Aleksandr Petrov and Craig Macdonald. 2022. Effective and Efficient Training for Sequential Recommendation using Recency Sampling. In RecSys . 81-91. [46] Jiarui Qin et al. 2023. Learning to Distinguish Multi-User Coupling Behaviors for TV Recommendation. In Proceedings of the 16th ACM International Conference on Web Search and Data Mining . 204-212. [47] Liang Qu, Yonghong Ye, Ningzhi Tang, Lixin Zhang, Yuhui Shi, and Hongzhi Yin. 2022. Single-shot Embedding Dimension Search in Recommender System. In SIGIR . 513-522. [48] Yunke Qu, Tong Chen, Quoc Viet Hung Nguyen, and Hongzhi Yin. 2024. Budgeted Embedding Table For Recommender Systems. In WSDM (WSDM '24) . 557-566. [49] Yunke Qu, Tong Chen, Xiangyu Zhao, Lizhen Cui, Kai Zheng, and Hongzhi Yin. 2023. Continuous Input Embedding Size Search For Recommender Systems. In SIGIR . 708-717. [50] Yunke Qu, Liang Qu, Tong Chen, Xiangyu Zhao, Quoc Viet Hung Nguyen, and Hongzhi Yin. 2024. Scalable Dynamic Embedding Size Search for Streaming Recommendation. In CIKM . 1941-1950. [51] Steffen Rendle. 2010. Factorization Machines. In ICDM . 995-1000. https://doi.org/10.1109/ICDM.2010.127 [52] Steffen Rendle, Li Zhang, and Yehuda Koren. 2019. On the difficulty of evaluating baselines: A study on recommender systems. arXiv preprint arXiv:1905.01395 (2019). [53] Harrisen Scells, Shengyao Zhuang, and Guido Zuccon. 2022. Reduce, reuse, recycle: Green information retrieval research. In SIGI . 2825-2837. [54] Victor Schmidt et al. 2021. CodeCarbon: estimate and track carbon emissions from machine learning computing. (2021). [55] Faisal Shehzad and Dietmar Jannach. 2023. Everyone's a Winner! On Hyperparameter Tuning of Recommendation Models. In RecSys . 652-657. [56] Hao-Jun Michael Shi, Dheevatsa Mudigere, Maxim Naumov, and Jiyan Yang. 2020. Compositional Embeddings Using Complementary Partitions for Memory-Efficient Recommendation Systems. In KDD . 165-175. [57] Zhu Sun et al. 2020. Are We Evaluating Rigorously? Benchmarking Recommendation for Reproducible Evaluation and Fair Comparison. In RecSys . 23-32. [58] Zhu Sun et al. 2022. DaisyRec 2.0: Benchmarking Recommendation for Rigorous Evaluation. TPAMI (2022). [59] Hung Vinh Tran, Tong Chen, Guanhua Ye, Quoc Viet Hung Nguyen, Kai Zheng, and Hongzhi Yin. 2024. On-device Content-based Recommendation with Single-shot Embedding Pruning: A Cooperative Game Perspective. arXiv preprint arXiv:2411.13052 (2024). [60] Jizhe Wang, Pipei Huang, Huan Zhao, Zhibo Zhang, Binqiang Zhao, and Dik Lun Lee. 2018. Billion-scale commodity embedding for e-commerce recommendation in alibaba. In SIGKDD . 839-848. [61] Ruoxi Wang et al. 2021. Dcn v2: Improved deep & cross network and practical lessons for web-scale learning to rank systems. In WebConf . 1785-1797. [62] Shuyao Wang, Yongduo Sui, Jiancan Wu, Zhi Zheng, and Hui Xiong. 2024. Dynamic Sparse Learning: A Novel Paradigm for Efficient Recommendation. In WSDM . 740-749. [63] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019. Neural Graph Collaborative Filtering. In SIGIR . 165-174. , Vol. 1, No. 1, Article . Publication date: January 2025. A Thorough Performance Benchmarking on Lightweight Embedding-based Recommender Systems \u00b7 31 [64] Xin Xia, Hongzhi Yin, Junliang Yu, Qinyong Wang, Guandong Xu, and Quoc Viet Hung Nguyen. 2022. On-Device Next-Item Recommendation with Self-Supervised Knowledge Distillation. In SIGIR . 546-555. [65] Chunxing Yin, Bilge Acun, Xing Liu, and Carole-Jean Wu. 2021. TT-Rec: Tensor Train Compression for Deep Learning Recommendation Models. ArXiv abs/2101.11714 (2021). https://api.semanticscholar.org/CorpusID:231719841 [66] Chunxing Yin, Da Zheng, Israt Nisa, Christos Faloutsos, George Karypis, and Richard Vuduc. 2022. Nimble gnn embedding with tensor-train decomposition. In KDD . 2327-2335. [67] Hongzhi Yin, Liang Qu, Tong Chen, Wei Yuan, Ruiqi Zheng, Jing Long, Xin Xia, Yuhui Shi, and Chengqi Zhang. 2024. On-Device Recommender Systems: A Comprehensive Survey. arXiv preprint arXiv:2401.11441 (2024). [68] Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Lizhen Cui, and Quoc Viet Hung Nguyen. 2022. Are Graph Augmentations Necessary? Simple Graph Contrastive Learning for Recommendation. In SIGIR . 1294-1303. [69] Hailin Zhang et al. 2023. Experimental Analysis of Large-scale Learnable Vector Storage Compression. arXiv:2311.15578 [cs.LG] [70] Xiangyu Zhao et al. 2021. AutoDim: Field-aware Embedding Dimension Searchin Recommender Systems. In WebConf . 3015-3022. [71] Xiangyu Zhaok et al. 2021. AutoEmb: Automated Embedding Dimensionality Search in Streaming Recommendations. In ICDM . 896-905. [72] Ruiqi Zheng, Liang Qu, Tong Chen, Kai Zheng, Yuhui Shi, and Hongzhi Yin. 2024. Personalized Elastic Embedding Learning for On-Device Recommendation. TKDE (2024). [73] Jieming Zhu et al. 2022. BARS: Towards Open Benchmarking for Recommender Systems. In SIGIR . 2912-2923. [74] Jieming Zhu, Jinyang Liu, Shuai Yang, Qi Zhang, and Xiuqiang He. 2021. Open benchmarking for click-through rate prediction. In CIKM . 2759-2769. , Vol. 1, No. 1, Article . Publication date: January 2025."}
