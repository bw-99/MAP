{"Discrete Semantic Tokenization for Deep CTR Prediction": "Qijiong Liu \u2217 PolyU, Hong Kong liu@qijiong.work Hengchang Hu National University of Singapore hengchang.hu@u.nus.edu Jiahao Wu PolyU, Hong Kong jiahao.wu@connect.polyu.hk Jieming Zhu Huawei Noah's Ark Lab, China jiemingzhu@ieee.org Min-Yen Kan \u2020 National University of Singapore kanmy@comp.nus.edu.sg", "ABSTRACT": "Incorporating item content information into click-through rate (CTR) prediction models remains a challenge, especially with the time and space constraints of industrial scenarios. The contentencoding paradigm, which integrates user and item encoders directly into CTR models, prioritizes space over time. In contrast, the embedding-based paradigm transforms item and user semantics into latent embeddings, subsequently caching them to optimize processing time at the expense of space. In this paper, we introduce a new semantic-token paradigm and propose a discrete semantic tokenization approach, namely UIST, for user and item representation. UIST facilitates swift training and inference while maintaining a conservative memory footprint. Specifically, UIST quantizes dense embedding vectors into discrete tokens with shorter lengths and employs a hierarchical mixture inference module to weigh the contribution of each user-item token pair. Our experimental results on news recommendation showcase the effectiveness and efficiency (about 200-fold space compression) of UIST for CTR prediction.", "CCS CONCEPTS": "\u00b7 Information systems \u2192 Recommender systems ; Data mining .", "ACMReference Format:": "Qijiong Liu, Hengchang Hu, Jiahao Wu, Jieming Zhu, Min-Yen Kan, and XiaoMing Wu. 2024. Discrete Semantic Tokenization for Deep CTR Prediction. In Companion Proceedings of the ACM Web Conference 2024 (WWW '24 Companion), May 13-17, 2024, Singapore, Singapore. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3589335.3651558", "1 INTRODUCTION": "Click-through rate (CTR) prediction models [21] aim to predict the probability of users interacting with items. The real-time demands of online services [8, 9] pose a significant challenge in seamlessly \u2217 The work was done when the author was a visiting student at NUS. \u2020 Min-Yen Kan and Xiao-Ming Wu are the corresponding authors. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WWW'24 Companion, May 13-17, 2024, Singapore, Singapore \u00a9 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0172-6/24/05 https://doi.org/10.1145/3589335.3651558 Xiao-Ming Wu \u2020 PolyU, Hong Kong xiao-ming.wu@polyu.edu.hk merging such deep CTR models with valuable semantic knowledge, encompassing item content and user history. Traditional deep CTR models commonly depend on ID-based approaches, incorporating features like item and user IDs, along with other categorical and statistical data [4, 13]. Recognizing the effectiveness of user history [2, 14] in various recommendation scenarios, certain methods [20] explore the incorporation of a shallow user encoder into CTR models, a strategy widely adopted in practice. It offers two key advantages over user ID representation in the context of large-scale industrial recommendation settings: 1) improved accuracy facilitated by the sequential features encoded by the user encoder, and 2) reduced memory usage by eliminating the need for a large user ID embedding table. Meanwhile, the value of item contents, such as texts and images, has been recognized for providing more detailed and nuanced item representations compared to basic item IDs [8, 18]. However, incorporating such item content information into CTR prediction models remains a challenge, especially within the time and space constraints of industrial scenarios. The prevalent use of pretrained models across various domains prompts the integrating of such models as end-to-end item encoders into CTR models, which we term as the content-encoding paradigm . Unfortunately, the user encoder requires the behavior sequence as input, necessitating the encoding of each item before fusing the behavior sequence to derive a user representation. This sequential dependency results in unacceptable training and inference inefficiency, impeding its adoption in industrial settings. To address the efficiency challenge, the embedding-based paradigm opts to trade space for time. Some methods [8, 16] utilize pretrained content encoders to convert item semantics into embedding vectors and cache them for subsequent CTR models. Furthermore, some methods [8, 15] explore the pretraining of user encoders, transforming user sequences into cached user embeddings. This approach effectively decouples item and user encoders via offline computing, leading to a substantial acceleration in both training and inference time. However, the use of pretrained models introduces a significant memory bottleneck; simply loading these extracted embeddings for training requires considerable memory. Recent work involves extracting item content features and condensing them into semantic IDs [6, 11], which efficiently capture the semantic representation of an item while maintaining its hierarchical structure. Building on this concept, we introduce a semantictokenparadigm and present UIST , a U serI tem S emantic T okenization approach that converts user sequences and item content into discrete user and item tokens, respectively. In contrast to the embedding-based paradigm, our approach greatly reduces space WWW'24 Companion, May 13-17, 2024, Singapore, Singapore Qijiong Liu, Hengchang Hu, Jiahao Wu, Jieming Zhu, Min-Yen Kan, & Xiao-Ming Wu Figure 1: Our UIST framework for CTR prediction: featuring semantics-based item (d), and user (h) tokenizers, and a hierarchical mixture inference module. Embedding Table Embedding Table Embedding Table Embedding Table Item ID User ID User Embedding Item Embedding Token Embeddings Token Embeddings (a) (e) (b) (f ) (c) (g) (d) (h) User Embedding Item Embedding User Embedding Item Embedding Item Tokens User Tokens Item Content Item Representer User Representer User Sequence Item Encoder User Encoder Item Tokenizer User Tokenizer Item Encoder User Encoder detach detach detach indexing indexing detach Item Representer Item Representer Click-through Rate Prediction Model Linear Layer CTR CTR CTR CTR CTR CTR CTR CTR CTR Click Probability Click Probability User Representer (a, b, c) (d) (h) (e, f, g) Hierarchical Mixture Inference Table 1: Encoding paradigms for CTR prediction comparison. User and item representations correspond to the approaches in Fig. 1. \ud835\udc49 = 30 , 522 , \ud835\udc41 = 100 \ud835\udc40 , and \ud835\udc40 = 100 \ud835\udc40 represent the vocabulary size of natural language, items, and users, respectively. \ud835\udc3e = 4 denotes the number of tokens used to represent an item or user; \ud835\udc51 = 256 and \ud835\udc37 = 768 denote the embedding dimensionality of the model and pretrained encoders. We use IST to represent single-layered semantic tokenization. consumption while maintaining time efficiency, offering a substantial advantage for large-scale industrial recommender systems. In Table 1, we present a detailed efficiency and memory consumption comparison of such paradigms used for CTR prediction. Specifically, in the 'Instantiation' column, we report the memory usage with vocabulary, user and item numbers set to reasonable settings for industrial applications. Compared to the embedding-based paradigm, our UIST achieves a remarkable \u223c 200-fold (572.20G/2.98G) space compression, utilizing only four tokens. Moreover, we devise a hierarchical mixture inference module to enhance the integration of hierarchical item and user tokens. This module dynamically adjusts the significance of various levels of granularity for user-item interactions.", "2 USER-ITEM SEMANTIC TOKENIZATION": "To balance time (model training and inference) and space (memory usage) efficiency, we introduce a user-item semantic tokenization framework, UIST, following the semantic-token paradigm. UIST comprises of three modules, illustrated in Figure 1: two semantic tokenizers and one hierarchical mixture inference module. The semantic tokenizers initially transform dense and high-dimensional item and user embeddings into discrete tokens. Subsequently, the hyper controller, a hierarchical mixture inference (HMI) module, evaluates each user-item token pair using the base CTR modules and autonomously learns weights for each pair with supervision of the click labels.", "2.1 Discrete Semantic Tokenization": "Initially proposed for generative retrieval [6, 11], semantic tokenization has primarily been applied to summarize features on the item side. In this context, we introduce a unified pipeline that extends tokenization to both item and user sides, and we uniformly refer to both item content and user behavior as 'sequence'. It is important to note that tokenization of items precedes that of users, and during user tokenization, each item in user sequence is initialized with its corresponding representation extracted from the item tokenizer. Stage 1: Semantic Representation. Traditional approaches often leverage pretrained language models like SentenceBERT to extract text-based content features on the item side by concatenating item attributes into a single sequence. However, this method is not directly applicable to the user side. Instead, we employ an autoencoder network for sequence representation learning. Specifically, given a sequence s with length \ud835\udc3f , we initially transform it into an embedding sequence E 0 through an embedding layer. Subsequently, a transformer [12] encoder with \ud835\udc3b layers learns the contextual knowledge of the sequence, expressed as: Discrete Semantic Tokenization for Deep CTR Prediction WWW'24 Companion, May 13-17, 2024, Singapore, Singapore E \u210e = ENC \u210e ( E \u210e -1 ) , where \u210e = 1 , 2 , . . . , \ud835\udc3b . Next, an additive attention module [1] merges the sequences into a unified representation, defined by: z = ATTN ( E \ud835\udc3b ) . Following this, a transformer decoder reconstructs the original sequence. Comprising multiple decoder layers with causal attention, it integrates information from the sequence representation for autoregressive generation: D \u210e = DEC \u210e ( D \u210e -1 , z ) , where \u210e = 1 , 2 , . . . , \ud835\udc3b, and the reconstruction loss is given by:  in which G represents the multi-layer perceptron classifier, and s [ \ud835\udc56 ] is the ground truth label for the output embedding D \ud835\udc3b \ud835\udc56 . Stage 2: Discrete Tokenization. The above enables random access to sequence embedding z for arbitrary user histories and item contents. Subsequently, we employ the residual quantization technique, RQ-VAE [19], to discretize the dense sequence representation into concise tokens. RQ-VAE is designed under an encoderquantizer-decoder framework. From a macro perspective, the encoder maps the sequence embedding z to a latent vector by v . The residual quantizer learns codes for the latent vector and sums up the code vectors (\u00af v ) to approximate the latent vector v . Finally, the decoder reconstruct the embedding \u00af z from \u00af v . Specifically, the residual quantizer operates iteratively as follows: i. For each layer \ud835\udc58 (where \ud835\udc58 ranges from 1 to \ud835\udc3e ), it maps the current vector v \ud835\udc58 to an index \ud835\udc56 \ud835\udc58 by finding the nearest vector in the \ud835\udc58 -th layer's codebook, minimizing a distance function \ud835\udc54 (like Euclidean or Manhattan distance) between v \ud835\udc58 and each code vector C \ud835\udc58 \ud835\udc57 in the codebook. ii. The next vector v \ud835\udc58 + 1 is computed by subtracting the chosen code vector C \ud835\udc58 \ud835\udc56 \ud835\udc58 from v \ud835\udc58 , thereby focusing on the residual (the part of the vector not yet quantized) for the next layer. iii. The process begins with v 1 = v , and iteratively refines the representation through the layers. Formally, the above operations can be denoted as:    where \ud835\udc36 is the codebook size for each layer. Therefore, ( \ud835\udc56 1 , . . . , \ud835\udc56 \ud835\udc3e ) is the tokenization result and the original vector v can be approximated by:  The overall loss function \ud835\udc3f et is calculated as:  where \ud835\udc60\ud835\udc54 represents the stop gradient mechanism, and where the first term is the embedding reconstruction loss and the second term is the quantization loss. \ud835\udefd is the commitment cost that controls the influence of the vector movement.", "2.2 Hierarchical Mixture Inference (HMI)": "Following the semantic tokenization process, we obtain item and user tokens for each item t and user u , represented as ( t 1 , . . . , t \ud835\udc3e ) and ( u 1 , . . . , u \ud835\udc3e ) , respectively. For simplicity, we use the same number of layers \ud835\udc3e during tokenization. Due to the nature of the residual quantization, these tokens are organized hierarchically, with the lower indices carrying primary component information. To effectively utilize user-item pairs at different levels, our hierarchical mixture inference module analyzes the contribution of each pair in the click-through rate prediction task. Specifically, we transform one-hot tokens into dense embeddings through the user and item token embedding layers, denoted as Eu \u2208 R \ud835\udc3e \u00d7 \ud835\udc51 and Et \u2208 R \ud835\udc3e \u00d7 \ud835\udc51 . We then construct coarse-to-fine item and user embeddings based on the hierarchical tokens, defined as:  where e \ud835\udc56 x is the \ud835\udc56 -th vector in E \ud835\udc56 x . For each user-item pair ( \u00af e \ud835\udc56 u , \u00af e \ud835\udc57 t ) , we use a deep CTR model M, such as DCN [13], to predict click scores. Finally, a linear layer is employed to automatically weigh these scores. This yields the final click probability, formulated as:  As is standard, we use binary cross-entropy loss to train the recommendation task, calculated by:  where \ud835\udc59 \u2208 { 0 , 1 } is the ground truth click label. Furthermore, we also develop an item-only semantic tokenization ( IST ), tailored for the item side. By virtue of the shorter item tokens compared to the original item content, IST enhances both time and space efficiency in comparison to the content-encoding paradigm. Nonetheless, when contrasted with UIST, IST is a slower alternative due to its retention of the user encoder.", "3 EXPERIMENTS": "We conduct offline experiments on a real-world news recommendation dataset, MIND [17], containing over 65K items and 94K users. We evaluate the effectiveness of our proposed UIST against three modern deep CTR models: DCN [13], DeepFM [4], and FinalMLP [10]. We follow common practice [8] to evaluate the recommendation effectiveness with AUC [3] and nDCG [5]. We also measure the inference time (latency) of each baseline for a single sample. During training, we employ the Adam optimizer [7] with a learning rate of 1e-3 for all paradigms. We set the number of transformer layers to 6 for 1) the item encoder (Fig. 1b), 2) user encoder (Fig. 1f), and 3) encoder-decoder used in semantic tokenization. For fair comparison, z in Section 2.1 serves as the pretrained embeddings for the embedding-based paradigms. During semantic tokenization, we set the residual depth to 4 and the codebook size to 64. We will release the code and data for reproducible research 1 . Table 2 compares the various paradigms across three deep CTR models, averaged over five independent runs. We make three key 1 https://github.com/Jyonn/SemanticTokenizer WWW'24 Companion, May 13-17, 2024, Singapore, Singapore Qijiong Liu, Hengchang Hu, Jiahao Wu, Jieming Zhu, Min-Yen Kan, & Xiao-Ming Wu Table 2: Performance comparison among different paradigms. We use red and green background to represent inefficient and efficient memory usage or inference latency, respectively. Table 3: Various aggregation mechanisms for user-item tokens. 'Add' indicates the addition of item and user token embeddings to create a unified item and user representation. 'Layer' signifies that only tokens from the same layer are input into the base CTR models. HMI represents our hierarchical mixture inference module. observations. i. The content-based paradigm yields latencies exceeding 60ms, intolerable for industrial scenarios. ii. The single-layered ID-based and embedding-based approaches - i.e., (a)/(f) and (c)/(f) pairings - exhibit similar latency, as both include a user encoder to model user behavior; however, the performance of the embeddingbased approaches are superior due to the use of the content-based item representation. On the other hand, the single-layered semanticbased IST approach is slightly slower because the item tokenization leads to a longer user sequence. iii. Our proposed IST and UIST achieve substantial memory compression (approximately 200 times) compared to other paradigms, while maintaining up to 99% accuracy (for IST) and 98% accuracy (for UIST) when compared to the state-of-the-art embedding-based paradigm. These findings, observed from diverse base models, validate the effectiveness and efficiency of our semantic-based approach. Table 3 examines various aggregation mechanisms for dual tokens, revealing that the simple addition and layer-wise approaches are inferior to our proposed HMI module.", "4 CONCLUSION": "We introduce a user-item semantic tokenization method, providing a streamlined approach to integrating item content into deep CTR models. Through our experimentation, we demonstrate the significant potential of semantic tokenization, initially proposed for generative retrieval, in boosting recommendation efficiency, particularly in industrial scenarios. Upon reflection, this method also offers new perspectives for applications such as dataset compression. We encourage researchers to further explore its potential.", "REFERENCES": "[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv (2014). [2] Qiwei Chen, Huan Zhao, Wei Li, Pipei Huang, and Wenwu Ou. 2019. Behavior sequence transformer for e-commerce recommendation in alibaba. In 1st DLP4Rec . 1-4. [3] Tom Fawcett. 2006. An introduction to ROC analysis. PRL (2006). [4] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. arXiv (2017). [5] Kalervo J\u00e4rvelin and Jaana Kek\u00e4l\u00e4inen. 2002. Cumulated gain-based evaluation of IR techniques. TOIS 20, 4 (2002). [6] Bowen Jin, Hansi Zeng, Guoyin Wang, Xiusi Chen, Tianxin Wei, Ruirui Li, Zhengyang Wang, Zheng Li, Yang Li, Hanqing Lu, et al. 2023. Language Models As Semantic Indexers. arXiv (2023). [7] Diederik P Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. ICLR (2015). [8] Qijiong Liu, Jieming Zhu, Quanyu Dai, and Xiaoming Wu. 2022. Boosting Deep CTR Prediction with a Plug-and-Play Pre-trainer for News Recommendation. In COLING . International Committee on Computational Linguistics. [9] Qijiong Liu, Jieming Zhu, Quanyu Dai, and Xiao-Ming Wu. 2023. Only Encode Once: Making Content-based News Recommender Greener. [10] Kelong Mao, Jieming Zhu, Liangcai Su, Guohao Cai, Yuru Li, and Zhenhua Dong. 2023. FinalMLP: An Enhanced Two-Stream MLP Model for CTR Prediction. In AAAI . [11] Shashank Rajput, Nikhil Mehta, Anima Singh, Raghunandan H Keshavan, Trung Vu, Lukasz Heldt, Lichan Hong, Yi Tay, Vinh Q Tran, Jonah Samost, et al. 2023. Recommender Systems with Generative Retrieval. arXiv (2023). [12] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. NIPS 30 (2017). [13] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & Cross Network for Ad Click Predictions. In ADKDD (Halifax, NS, Canada) (ADKDD'17) . ACM, New York, NY, USA, Article 12, 7 pages. [14] Chuhan Wu, Fangzhao Wu, Suyu Ge, Tao Qi, Yongfeng Huang, and Xing Xie. 2019. Neural News Recommendation with Multi-head Self-attention. In EMNLPIJCNLP . [15] Chuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng Huang. 2022. UserBERT: Pre-training User Model with Contrastive Self-supervision. In SIGIR . 2087-2092. [16] Chuhan Wu, Fangzhao Wu, Yang Yu, Tao Qi, Yongfeng Huang, and Qi Liu. 2021. NewsBERT: Distilling pre-trained language model for intelligent news application. arXiv (2021). [17] Fangzhao Wu, Ying Qiao, Jiun-Hung Chen, Chuhan Wu, Tao Qi, Jianxun Lian, Danyang Liu, Xing Xie, Jianfeng Gao, Winnie Wu, et al. 2020. Mind: A large-scale dataset for news recommendation. In ACL . 3597-3606. [18] Zheng Yuan, Fajie Yuan, Yu Song, Youhua Li, Junchen Fu, Fei Yang, Yunzhu Pan, and Yongxin Ni. 2023. Where to go next for recommender systems? id-vs. modality-based recommender models revisited. arXiv (2023). [19] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. 2021. Soundstream: An end-to-end neural audio codec. TASLP 30 (2021). [20] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through rate prediction. In SIGKDD . 1059-1068. [21] Jieming Zhu, Jinyang Liu, Shuai Yang, Qi Zhang, and Xiuqiang He. 2021. Open Benchmarking for Click-Through Rate Prediction. In CIKM . 2759-2769."}
