{"FairRec: Fairness Testing for Deep Recommender Systems": "Huizhong Guo Zhejiang University,Alibaba Group Hangzhou, China huiz_g@zju.edu.cn Jinfeng Li Alibaba Group Hangzhou, China jinfengli.ljf@alibaba-inc.com Jingyi Wang Zhejiang University Hangzhou, China wangjyee@zju.edu.cn Xiangyu Liu Alibaba Group Hangzhou, China eason.lxy@alibaba-inc.com Dongxia Wang \u2217 Zhejiang University Hangzhou, China dxwang@zju.edu.cn Rong Zhang Alibaba Group Hangzhou, China stone.zhangr@alibaba-inc.com Zehong Hu Alibaba Group Hangzhou, China zehong.hzh@alibaba-inc.com Hui Xue Alibaba Group Hangzhou, China hui.xueh@alibaba-inc.com", "ABSTRACT": "Deep learning-based recommender systems (DRSs) are increasingly and widely deployed in the industry, which brings significant convenience to people's daily life in different ways. However, recommender systems are also shown to suffer from multiple issues, e.g., the echo chamber and the Matthew effect 1 , of which the notation of 'fairness' plays a core role. For instance, the system may be regarded as unfair to 1) a certain user, if the user gets worse recommendations than other users, or 2) an item (to recommend), if the item is much less likely to be exposed to the user than other items. While many fairness notations and corresponding fairness testing approaches have been developed for traditional deep classification models, they are essentially hardly applicable to DRSs. One major difficulty is that there still lacks a systematic understanding and mapping between the existing fairness notations and the diverse testing requirements for deep recommender systems, not to mention further testing or debugging activities. To address the gap, we propose FairRec, a unified framework that supports fairness testing of DRSs from multiple customized perspectives, e.g., model utility, item diversity, item popularity, etc. We also propose a novel, efficient search-based testing approach to tackle the new challenge, i.e., double-ended discrete particle swarm optimization (DPSO) algorithm, to effectively search for hidden fairness issues in the form of certain disadvantaged groups from a vast number of candidate groups. Given the testing report, by adopting a simple re-ranking mitigation strategy on these identified disadvantaged groups, we show that the fairness of DRSs can be significantly improved. We \u2217 Corresponding author. 1 The echo chamber refers to the effect that the user interest is reinforced by being repeatedly recommended similar items, resulting in a person being limited to a narrow range of content. The Matthew effect describes a phenomenon in which popular items have a cumulative advantage and therefore take up more exposure, limiting users' opportunities to see other items. ISSTA 2023, 17-21 July, 2023, Seattle, USA \u00a9 2023 Association for Computing Machinery. This is the author's version of the work. It is posted here for your personal use. Not for redistribution. The definitive Version of Record was published in Proceedings of ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA 2023) , https://doi.org/10.1145/nnnnnnn.nnnnnnn. conducted extensive experiments on multiple industry-level DRSs adopted by leading companies. The results confirm that FairRec is effective and efficient in identifying the deeply hidden fairness issues, e.g., achieving \u223c 95% testing accuracy with \u223c half to 1/8 time.", "ACMReference Format:": "Huizhong Guo, Jinfeng Li, Jingyi Wang, Xiangyu Liu, Dongxia Wang, Zehong Hu, Rong Zhang, and Hui Xue. 2023. FairRec: Fairness Testing for Deep Recommender Systems . In Proceedings of ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA 2023). ACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn", "1 INTRODUCTION": "Recommender systems (RSs) effectively bridge the gap between quality content and people who might be interested, benefiting and facilitating potential consumers and content providers in many fields, e.g., e-commerce, social media, etc. A recommender system (RS) collects the feedback data of users in the real-time recommendation and updates itself on a periodic (e.g., daily) basis. In the dynamic process of data collection, model training or updating, item display, user clicks, etc., various biases may be introduced, which will subtly accumulate in the feedbackupdate loop of RSs. [49]. If these biases are not disclosed in time, they could eventually lead to serious fairness issues, some of which are known as the echo chamber and the Matthew effect. For example, it is shownthat in the MOOC platforms, courses taught by teachers from the United States are over-exposed by RSs, reducing the chances of teachers from the rest of the world being exposured [14]. In recent years, with the rise of deep learning, deep learningbased recommender systems are increasingly and widely deployed in the industry, influencing millions of users in the world on a daily basis. For example, Microsoft released Deep Crossing [33] for advertisement recommendations for its products. Google released the Wide&Deep [10], which was applied to application recommendations in Google play. More recently, different types of deep recommendation models have been developed and introduced in both academia and industry [16, 41]. In traditional statistical/collaborative filtering-based RSs, the correlation between the recommendation output and input features can be directly and ISSTA 2023, 17-21 July, 2023, Seattle, USA Huizhong Guo, Jinfeng Li, Jingyi Wang, Xiangyu Liu, Dongxia Wang, Zehong Hu, Rong Zhang, and Hui Xue explicitly obtained, which makes fairness issues easily detected by expert check.However, In DRSs, complicated DNN-based feature extraction and representation are unexplainable to humans. Therefore, it is challenging to analyze the deeply hidden fairness issues, and calls for a fine-grained fairness testing system for DRSs. Fairness testing of deep learning models is an emerging research area in software engineering that aims to expose multiple kinds of fairness issues of a deep learning model, e.g., individual discrimination [23], group disparity [17], etc., using various kinds of search strategies, e.g., random sampling [13], probabilistic sampling [38], gradient-based approach [48] and symbolic execution [3]. Despite the significant progress, existing fairness testing approaches are mostly for traditional deep classification problems and are essentially not applicable to DRSs, given the following critical challenges. First, there still lacks a systematic understanding and mapping between the fairness metrics from existing deep learning research [11, 43] and the customized testing requirements of DRSs from a system point of view. For instance, a DRS user may not only require accurate recommendation recommendation results but also diverse recommendation results. Second, the industry urgently calls for an efficient fairness testing approach which can be used to periodically evaluate their recommender systems and identify the discriminated user groups in time. The reason is that DRSs use multiple sensitive attributes of sparsely distributed users, forming a vast search space with a much more extensive collection of candidate groups (e.g., millions) than traditional fairness testing fields. Even worse, the large amount of user and behavior data makes it even more challenging to develop an efficient fairness testing algorithm for DRSs. In this work, we propose FairRec, a novel unified fairness testing framework specifically designed for DRSs to address the above challenges. As shown in Figure 1, FairRec supports the testing of multi-dimensional fairness metrics such as model performance, diversity, and popularity, etc., meaning it considers not only how the model performs for different users but also how badly they might be influenced by user-tailored fairness issues such as the echo chamber and the Matthew effect , so as to meet the practical needs of different users on fair recommendation. Moreover, FairRec embraces a novel fairness testing algorithm based on the carefully designed doubleended discrete particle swarm optimization (DPSO) algorithm that can improve the testing efficiency by magnitude (compared to exhaustive search) while ensuring high testing accuracy. Lastly, given the testing report of FairRec, by adopting some simple re-ranking mitigation strategy on those identified disadvantaged groups, the overall fairness of the DRSs can be significantly improved. In summary, we make the following contributions: \u00b7 We establish a systematic understanding and mapping between the diverse testing requirements and the existing fairness evaluation metrics to meet the various practical testing needs of DRSs. \u00b7 Weproposeanovelfairness testing approach based on doubleended discrete particle swarm optimization (DPSO) algorithm which significantly improves the testing efficiency by magnitude while ensuring high testing accuracy. \u00b7 We evaluate FairRec with multiple industry-level deep recommendation models on popular large-scale datasets in this field. Our experiments confirm that FairRec is significantly more effective and efficient than the previous available methods, e.g., achieving \u223c 95% testing accuracy with \u223c half to 1/8 time. \u00b7 We uncover the relation between the fairness performance w.r.t the different evaluation metrics in the experiments. \u00b7 Weimplementandrelease FairRec as an open source toolkit 2 together with all the data and models which could benchmark and facilitate future studies on testing and debugging of DRSs.", "2 PRELIMINARIES": "In this section, we briefly introduce the relevant background of DRSs and multi-attribute fairness, then formalize our problem.", "2.1 Deep Recommender System": "A recommender system takes the historical behavior of users as input and outputs the items 3 each user might be interested in. Let U be a user set and V be an item set, where |U| = \ud835\udc5b , and |V| = \ud835\udc5a . In this paper, we focus on deep learning-based recommender systems, which are usually more complicated and widely used in practice. A DRS usually contain a recall layer and a ranking layer. For any user \ud835\udc62 \ud835\udc56 \u2208 U , the recall layer will initially select \ud835\udc5a \u2032 candidates from V based on the user's historical behavior. The ranking layer usually consists of a Deep Click-Through-Rate (CTR) Prediction model which will estimate the user's preferences for all candidate items, and finally selects \ud835\udc58 items to form the user's recommendation list \ud835\udc93\ud835\udc8d \ud835\udc56 . Definition 1. Deep recommender model Let \ud835\udc96 \ud835\udc56 be the feature vector of user \ud835\udc62 \ud835\udc56 , \ud835\udc96 \ud835\udc56 = ( \ud835\udc94 \ud835\udc56 , ^ \ud835\udc94 \ud835\udc56 ) , where \ud835\udc94 \ud835\udc56 denotes its sensitive attributes such as gender, age, and ^ \ud835\udc94 \ud835\udc56 denotes its non-sensitive attributes. Let \ud835\udc97 \ud835\udc57 be the feature vector of item \ud835\udc63 \ud835\udc57 , the deep learning model is trained to predict the probability \ud835\udc5d \ud835\udc56 \ud835\udc57 that the user \ud835\udc62 \ud835\udc56 clicks the item \ud835\udc63 \ud835\udc57 . A deep recommender model will then output \ud835\udc93\ud835\udc8d \ud835\udc56 , which is a vector of \ud835\udc58 items that have the top -\ud835\udc58 highest probability values, as the recommendation result for each user \ud835\udc62 \ud835\udc56 .", "2.2 Fairness of Recommender System": "There is a lack of consensus on the definition of fairness in tasks using deep learning. From a high level, the most widely recognized concepts of fairness are individual fairness [15, 23] and group fairness [17]. In personalized recommendation scenarios, individual fairness, which requires that two similar individuals should be treated similarly enough, is rather tricky to use. In this work, we focus on group fairness which requires groups differing only in sensitive attributes to be treated with little distinction. Especially we focus on the fairness of groups formed by multiple sensitive attributes [13]. Definition 2. Multi-attribute group Let \ud835\udc59 \u2208 N be the size of vector variable of the sensitive features \ud835\udc94 . Let \ud835\udc4b 1 , \ud835\udc4b 2 . . . . . . \ud835\udc4b \ud835\udc59 be the possible sets of values of its entries \ud835\udc60 1 , \ud835\udc60 2 \u00b7 \u00b7 \u00b7 , \ud835\udc60 \ud835\udc59 respectively. The set of multi-attribute groups defined by sensitive attributes can then be expressed as the Cartesian product S = \ud835\udc4b 1 \u00d7 \ud835\udc4b 2 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 \ud835\udc4b \ud835\udc59 , an 2 https://github.com/Grey-z/FairRec 3 Weuse'item' as an abstract concept to represent any content for the recommendation, e.g., news, jobs, goods, etc. FairRec: Fairness Testing for Deep Recommender Systems ISSTA 2023, 17-21 July, 2023, Seattle, USA Figure 1: Our proposed FairRec framework. Recommender System Sparse Feature \u2026 \u2026 \u2026 \u2026 \u2026 User Profiles Item 1 Item N Dense Embedding FM Layer Hidden Layer Addition Inner Product Sigmoid Activation Weight Connection Embedding Normal Connection Output Unit Profiles Items History Configs Sensitive features Results MRR NDCG@K Popularity Diversity Report Mitigation Age Gender Insight Disadvantaged Group Testing Module Individual best Global best candidate Current Distribution Modeling Init Particles Optimization Fairness Evaluation Fitness Update Candidates element of which describes a specific user group e.g., group \ud835\udc6e \ud835\udc94 \ud835\udc8b with \ud835\udc94 \ud835\udc8b = ( \ud835\udc65 \ud835\udc57 1 , \ud835\udc65 \ud835\udc57 2 , \u00b7 \u00b7 \u00b7 , \ud835\udc65 \ud835\udc57\ud835\udc59 ) , \ud835\udc94 \ud835\udc8b \u2208 S .", "3 THE FAIRREC FRAMEWORK": "Let M( \ud835\udc62 \ud835\udc56 , \ud835\udc93\ud835\udc8d \ud835\udc56 ) represents the performance value regarding an evaluation metric of a recommendation result for a user. Given two multi-attribute groups and a metric in concern, we use the following absolute value to measure the distance between the performance of the recommendations they receive, i.e.,  which enables us to define multi-attribute group fairness. Definition 3. Multi-attribute group fairness For the entire user population, we use the maximum gap w.r.t. an evaluation metric M between two groups to measure the group fairness of a DRS, which has been widely adopted in previous work [13, 30]. Given any metrics M , the unfairness of a DRS can be defined as follows:  \ud835\udc48\ud835\udc39 M measures the degree of group unfairness of a DRS w.r.t an evaluation metric M . The larger the value, the more unfair a DRS is w.r.t the metric. The value itself and also the user groups that correspond to the value form important parts of our testing results.", "2.3 Problem Formalization": "Fairness testing refers to any activity designed to reveal fairness bugs [9]. For a given DRS R that recommends \ud835\udc58 items from the item set for each user \ud835\udc56 , the primary goal of FairRec is to 1) measure the multi-attribute group fairness score defined above. Note that the reported score \ud835\udc48\ud835\udc39 M is parameterized by a performance evaluation metric M , which allows FairRec to measure the system fairness from different perspectives. Along with the measurable fairness score, 2) FairRec also reports a certain user-defined number of advantaged user groups and disadvantaged groups , which could facilitate the model developer to further debug the model and improve the model's group fairness. To achieve this goal, we need to find the testing candidates (user groups) for which the performance values regarding a metric differ as much as possible , such that we can accurately uncover the severity of unfairness problem of a DRS. In this section, we first provide an overview of our testing framework, and then present the detailed metrics and testing approach.", "3.1 System Overview": "Wepresent the framework of FairRec in Figure 1, which consists of three main components, i.e., the input module, the testing module and the results display module. Concretely, in the input module , the user and item data, as well as the recommendation models, should be included. Notice that FairRec is a model-agnostic testing system, it only requires black-box queries access to the target recommender models. In addition, the testing requirement of interest such as the sensitive user attributes \ud835\udc46 (e.g., gender and age) and fairness metrics M of interest should also be configured. In the testing module , FairRec first loads the data and models and then identifies the advantaged and disadvantaged groups concerning the specified fairness metric via our specifically designed double-ended discrete particle swarm optimization algorithm according to the configured requirements. Specifically, more details of our designed effective and efficient search-based testing algorithm will be included in Section 3.3. Finally in the display module , FairRec displays a multi-dimensional testing report that includes the overall fairness evaluation results as well as the details of the found disadvantaged groups, aiming to provide insight on the bias mitigation followed.", "3.2 RS-Tailored Evaluation Metrics": "Group fairness needs to be measured in terms of an evaluation metric (EM) for traditional deep classification tasks or deep recommendation tasks. However, different from the classification tasks where the commonly agreed evaluation metric is the prediction accuracy, it is more than challenging to define 'accuracy' for a personalized recommendation task given the flexibility, diversity, and subjectivity of user's practical needs. In fact, how to define the metrics which can properly reflect these aspects is still popular ongoing research in the RS community [40, 46]. In this work, we focus on fairness from a user's perspective and systematically adopt five RS-tailored evaluation metrics in FairRec. ISSTA 2023, 17-21 July, 2023, Seattle, USA Huizhong Guo, Jinfeng Li, Jingyi Wang, Xiangyu Liu, Dongxia Wang, Zehong Hu, Rong Zhang, and Hui Xue Note that FairRec is flexible to incorporate more evaluation metrics later. These metrics are selected considering multiple practical needs, i.e., 1) model performance metrics (AUC, MRR and NDCG), 2) diversity metric (URD to measure the echo chamber effect), and 3) popularity metric (URP to measure the impact of Matthew effect on users). In the following, we elaborate the details of these metrics in the top-k recommendation. EM1.1 Area under Curve (AUC) [26]. It is defined as the area under the receiver operating characteristic curve, which is plotted with the true positive rate as the vertical coordinate and the false positive rate as the horizontal coordinate. AUC is the most commonly used metric to measure the performance of DRSs. AUC measures how likely an item of interest is ranked higher than another out of interest in a recommended list. A higher AUC value means a DRS recommends more accurately. EM1.2 Mean Reciprocal Rank (MRR) [39]. It measures on average, how top user-target items are ranked in their recommended lists. MRR is calculated with reciprocal ranks as below:  where \ud835\udc5f \ud835\udc56 \ud835\udc57 = 1 denotes that the item \ud835\udc63 \ud835\udc57 is the target item of \ud835\udc62 \ud835\udc56 and \ud835\udc5d\ud835\udc5c\ud835\udc60 ( \ud835\udc63 \ud835\udc57 ) denotes the ranking position of \ud835\udc63 \ud835\udc57 in the recommendation list of \ud835\udc62 \ud835\udc56 . If for a user, there is no target item in his list, then the fraction in the sum is assigned 0 . A higher MRR value means the target items are ranked higher (which can lead to more clicks), indicating more accurate recommendation. EM1.3 Normalized Cumulative Gain for k Shown Recommendations (NDCG@k) [21]. It is a commonly used metric for evaluating the performance of a recommender system based on the graded relevance of the recommended items, which varies from 0.0 to 1.0, with 1.0 representing the ideal recommendation result, i.e., users get the recommendations they are interested in and their favorite items get the most exposure.   where \ud835\udc4e denotes the sorting position of the item, and \ud835\udc5d ( \ud835\udc63 \ud835\udc4e ) denotes the model prediction output of \ud835\udc63 \ud835\udc4e . \ud835\udc37\ud835\udc36\ud835\udc3a \ud835\udc62 and \ud835\udc3c \ud835\udc37\ud835\udc36\ud835\udc3a \ud835\udc62 are calculated based on the recommended list and the ideal list of \ud835\udc62 \ud835\udc56 , respectively. A higher value of (NDCG@k) indicates better recommendation. It can be noticed that in general, the metrics AUC, MRR and NDCG@k all describe how well the recommendations match the interests of users. However, sometimes a user may not only want to be shown the items that he already knows he would be interested in, but also want to see other types of items. We need recommendation diversity metric to capture this. EM2 Diversity (URD) [31]. It measures the level of diversity in recommendations for users. A higher value indicates that users receive a more varied set of recommendations, thereby reducing the impact of the echo chamber effect. Specifically, we use the intra-list similarity [50] to calculate the diversity of a recommendation list,  where \ud835\udc4e , \ud835\udc4f denotes the sorting position of the item in \ud835\udc93\ud835\udc8d \ud835\udc56 , \ud835\udc63 \ud835\udc4e denotes the \ud835\udc4e -th item and \ud835\udc46\ud835\udc56\ud835\udc5a ( \ud835\udc63 \ud835\udc4e , \ud835\udc63 \ud835\udc4f ) denotes the similarity between \ud835\udc63 \ud835\udc4e and \ud835\udc63 \ud835\udc4f . In this work, we use the Jaccard similarity to calculate the diversity of the recommended items. EM3 Popularity (URP) [2]. It measures how well the popularity of the recommended items matches that of the user's preference. A higher value indicates the user is more affected by the Matthew effect, e.g., a user prefers niche movies but only be recommended with very popular ones. How popular an item \ud835\udc63 \ud835\udc57 is in general is calculated by  where T denotes the training set and \ud835\udf19 ( \ud835\udc57 ) denotes the number of times item \ud835\udc57 has been interacted in the training set. Based on Equation. 6, we use the following absolute value to define the URP metric. The smaller the value is, the better the popularity of the recommended items matches the popularity preference of the user.  where \ud835\udc89\ud835\udc8d \ud835\udc56 and \ud835\udc93\ud835\udc8d \ud835\udc56 denote the historical interaction list and the recommendation list of \ud835\udc62 \ud835\udc56 , respectively.", "3.3 Search-based Testing Algorithm": "Given a specific evaluation metric, the core challenge of multiattribute group fairness testing is to measure the maximum gaps between the most advantaged and disadvantaged user groups. This is highly non-trivial for realistic DRSs due to the following challenges: (i) First, compared to traditional deep learning fairness testing literature which only target a single or few sensitive attributes, there might be dozens of sensitive attributes (e.g., gender, age, nationality, etc.) with multi-dimensional discrete values in a DRS . The combinations of these attributes maps the testing candidates into an extremely high-dimensional and sparse search space (e.g., the number of user groups to search for could be in millions); (ii) Even worse, in an industrial DRS, there can be a huge amount of users (e.g., 360K in some of our experiment) who are sparsely distributed in the search space . The complexity of testing such a real DRS inherently requires extremely high testing budget while it is often required to be completed within an acceptable limited time, making high testing efficiency critical. In FairRec, we propose a novel double-ended discrete particle swarm optimization (DPSO) algorithm to address the challenging search problem for a DRS and make the testing much more efficient while ensuring high accuracy. The Particle swarm optimization (PSO) algorithm is an evolutionary computation technique [22] to search for the optimal solution of an optimization problem, derived from the simulation of the social behavior of birds within a flock. Previously, PSO has been shown to be effective and efficient in solving multiple kinds of testing problems [28, 44]. Algorithm 1 shows the overall testing workflow of FairRec. The inputs include the test dataset \ud835\udc37 , the set of sensitive attribute \ud835\udc46 , and the testing budget, i.e., the maximum number of iteration \ud835\udc5b . At line FairRec: Fairness Testing for Deep Recommender Systems ISSTA 2023, 17-21 July, 2023, Seattle, USA 1, DPSO first sets up the search space, the optimization objective of testing and initialize the particle swarms. From line 2 to line 6, DPSO will iteratively evaluate the testing objective and update the particle swarms until the testing budget is exhausted. Several important customization and optimizations are particularly made for the fairness testing problem of DRS. First, to effectively evaluate the maximum gap between two user groups, we propose a bi-end search solution with two particle swarms running simultaneously, where one particle swarm is designed to find the most advantaged group \ud835\udc6e \ud835\udc94 \ud835\udc82 and the other swarm is designed to find the most disadvantaged group \ud835\udc6e \ud835\udc94 \ud835\udc85 in an opposite direction according to Equation 1. Specifically, each particle represents a user group segmented by \ud835\udc59 sensitive attributes, which is a point in the \ud835\udc59 -dimensional search space. For instance, the group consisting of 30-year-old male doctors (i.e., gender =male/0, age =30 and occupation =doctor/9) can be denoted as the point ( 0 , 30 , 9 ) in the 3-dimension search space. Second, during the testing process, each particle flies through the search space and memorizes its individual best position \ud835\udc43\ud835\udc4f\ud835\udc52\ud835\udc60\ud835\udc61 (i.e., the grouping condition that results in a current maximum fairness gap between the divided groups), which is shared with the other particles. The global optimal grouping condition \ud835\udc3a\ud835\udc4f\ud835\udc52\ud835\udc60\ud835\udc61 can then be obtained from the \ud835\udc43\ud835\udc4f\ud835\udc52\ud835\udc60\ud835\udc61 of the two swarms (with different goals in mind) from the whole search history. In each search iteration, particles in the two swarms share better positions with each other and dynamically adjust their own position and velocity in each dimension according to the calculated fitness (i.e., the difference between the metrics of the two divided groups) of its own and other members. The whole search process is iterated until it finally converges to the global optimal position \ud835\udc3a\ud835\udc4f\ud835\udc52\ud835\udc60\ud835\udc61 , and the difference in the group-fairness-metric values \ud835\udc40\ud835\udc3a\ud835\udc37 M( \ud835\udc6e \ud835\udc94 \ud835\udc82 , \ud835\udc6e \ud835\udc94 \ud835\udc85 ) (Refer to Equation 1) is reported as the fairness measurement of the DRS. Note that the algorithm can be easily configured to memorize a certain number of disadvantaged groups for the final report through maintaining a list of \ud835\udc43\ud835\udc4f\ud835\udc52\ud835\udc60\ud835\udc61 for them during the search process. We then introduce the carefully designed optimizations of the two key steps in DPSO to further improve the testing efficiency. Figure 2: Comparison of different initialization methods. (a) Uniform-based (b) Distribution-based Initialize. As mentioned above, users are sparsely distributed in the multi-dimensional partitioned feature space. We therefore propose to initialize the two particle swarms according to the actual distribution of the testing candidate users to quickly locate the target area in the search space. The concrete initialization scheme is illustrated in Algorithm 2. Firstly, we set the search boundary \ud835\udc35 \ud835\udc60 \ud835\udc56 (line 1) and the probability distribution \ud835\udc37 \ud835\udc60 \ud835\udc56 (line 2) of each sensitive attribute \ud835\udc60 \ud835\udc56 \u2208 \ud835\udc94 . For example, given \ud835\udc60 \ud835\udc56 = 'gender\", assume a group contains 4 females ( \ud835\udc60 \ud835\udc56 = 0 ) and 6 males ( \ud835\udc60 \ud835\udc56 = 1 ) , we have \ud835\udc35 \ud835\udc60 \ud835\udc56 = [ 0 , 1 ] and \ud835\udc37 \ud835\udc60 \ud835\udc56 = [ 0 . 4 , 0 . 6 ] . Then, we initialize the particle swarms based on the distribution of users, where \ud835\udc36\ud835\udc62\ud835\udc5f\ud835\udc43\ud835\udc46 \ud835\udc4e and \ud835\udc36\ud835\udc62\ud835\udc5f\ud835\udc43\ud835\udc46 \ud835\udc51 denote the particle swarms with search goals of \ud835\udc6e \ud835\udc94 \ud835\udc82 and \ud835\udc6e \ud835\udc94 \ud835\udc85 , respectively (line 3-4). Finally the velocity \ud835\udc49 and individual best \ud835\udc43\ud835\udc4f\ud835\udc52\ud835\udc60\ud835\udc61 of every particle \ud835\udc43 are initialized (line 5-8). We also visualize the distribution-based initialization method we proposed and the traditional initialization method in Figure 2 for a more intuitive comparison, in which the green circles represent the actual users distribution in testing candidates. The red stars in Figure 2(a) denote the particles initialized via random initialization, while the red stars in Figure 2(b) are the particles initialized in FairRec based on the modeled distribution. It can be seen from Figure 2 that the distribution-based initialization we proposed significantly improves the particle coverage, which allows the generated particles to be more concentrated on the target region, thus improving the effectiveness of searching for the two target groups. Update. Algorithm 3 illustrates the details of fairness evaluation and update. After initialization, all particles will move toward the target groups at each iteration, guided by \ud835\udc43\ud835\udc4f\ud835\udc52\ud835\udc60\ud835\udc61 and \ud835\udc3a\ud835\udc4f\ud835\udc52\ud835\udc60\ud835\udc61 (line 3). The velocity and position of all particles is updated according to the formula defined as follows,   where \ud835\udc43 \ud835\udc58 \ud835\udc56 and \ud835\udc49 \ud835\udc58 \ud835\udc56 are the position and velocity of i-th particle at the k-th iteration, \ud835\udc5f 1 and \ud835\udc5f 2 are random numbers between 0 and 1, and \ud835\udf11 \ud835\udc56 is a random number that obeys the standard normal distribution. ISSTA 2023, 17-21 July, 2023, Seattle, USA Huizhong Guo, Jinfeng Li, Jingyi Wang, Xiangyu Liu, Dongxia Wang, Zehong Hu, Rong Zhang, and Hui Xue", "Algorithm 2: Initialize": "Input: Test Dataset \ud835\udc37 , Sensitive Attributes \ud835\udc94 , Size \ud835\udc5b \ud835\udc5d\ud835\udc61 Output: \ud835\udc3c\ud835\udc5b\ud835\udc56\ud835\udc43\ud835\udc46 \ud835\udc4e , \ud835\udc3c\ud835\udc5b\ud835\udc56\ud835\udc43\ud835\udc46 \ud835\udc51 1 \ud835\udc35 \ud835\udc60 \ud835\udc56 = \ud835\udc46\ud835\udc52\ud835\udc61\ud835\udc35\ud835\udc5c\ud835\udc62\ud835\udc5b\ud835\udc51\ud835\udc4e\ud835\udc5f\ud835\udc66 ( \ud835\udc37, \ud835\udc94 ) ; 2 \ud835\udc37 \ud835\udc60 \ud835\udc56 = \ud835\udc3a\ud835\udc52\ud835\udc61\ud835\udc37\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc4f\ud835\udc62\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b ( \ud835\udc37, \ud835\udc94 ) ; 3 \ud835\udc3c\ud835\udc5b\ud835\udc56\ud835\udc43\ud835\udc46 \ud835\udc4e = \ud835\udc46\ud835\udc4e\ud835\udc5a\ud835\udc5d\ud835\udc59\ud835\udc52 ( \ud835\udc35 \ud835\udc60 \ud835\udc56 , \ud835\udc37 \ud835\udc60 \ud835\udc56 , \ud835\udc5b \ud835\udc5d\ud835\udc61 ) ; 4 \ud835\udc3c\ud835\udc5b\ud835\udc56\ud835\udc43\ud835\udc46 \ud835\udc51 = \ud835\udc46\ud835\udc4e\ud835\udc5a\ud835\udc5d\ud835\udc59\ud835\udc52 ( \ud835\udc35 \ud835\udc60 \ud835\udc56 , \ud835\udc37 \ud835\udc60 \ud835\udc56 , \ud835\udc5b \ud835\udc5d\ud835\udc61 ) ; 5 \ud835\udc3c\ud835\udc5b\ud835\udc56\ud835\udc43\ud835\udc46 = \ud835\udc3c\ud835\udc5b\ud835\udc56\ud835\udc43\ud835\udc46 \ud835\udc4e \u222a \ud835\udc3c\ud835\udc5b\ud835\udc56\ud835\udc43\ud835\udc46 \ud835\udc51 ; 6 foreach particle \ud835\udc43 \ud835\udc56 in \ud835\udc3c\ud835\udc5b\ud835\udc56\ud835\udc43\ud835\udc46 do 7 \ud835\udc49\ud835\udc56 = \ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc51\ud835\udc5c\ud835\udc5a.\ud835\udc62\ud835\udc5b\ud835\udc56\ud835\udc53 \ud835\udc5c\ud835\udc5f\ud835\udc5a (- \ud835\udc63 \u2217 , \ud835\udc63 \u2217 ) ; 8 \ud835\udc43\ud835\udc4f\ud835\udc52\ud835\udc60\ud835\udc61 \ud835\udc56 \u2190 \ud835\udc43 \ud835\udc56 ; 9 end 10 return \ud835\udc3c\ud835\udc5b\ud835\udc56\ud835\udc43\ud835\udc46 \ud835\udc4e , \ud835\udc3c\ud835\udc5b\ud835\udc56\ud835\udc43\ud835\udc46 \ud835\udc51", "Algorithm 3: Fairness evaluate and update": "Input: \ud835\udc36\ud835\udc62\ud835\udc5f\ud835\udc43\ud835\udc46 \ud835\udc4e , \ud835\udc36\ud835\udc62\ud835\udc5f\ud835\udc43\ud835\udc46 \ud835\udc51 Output: \ud835\udc36\ud835\udc62\ud835\udc5f\ud835\udc43\ud835\udc46 \ud835\udc4e , \ud835\udc36\ud835\udc62\ud835\udc5f\ud835\udc43\ud835\udc46 \ud835\udc51 1 InfoBase \u2190\u2205 ; 2 foreach particle \ud835\udc43 \ud835\udc56 in \ud835\udc36\ud835\udc62\ud835\udc5f\ud835\udc43\ud835\udc46 \ud835\udc4e \u222a \ud835\udc36\ud835\udc62\ud835\udc5f\ud835\udc43\ud835\udc46 \ud835\udc51 do 3 \ud835\udc43 \ud835\udc58 + 1 \ud835\udc56 , \ud835\udc49 \ud835\udc58 + 1 \ud835\udc56 = \ud835\udc48\ud835\udc5d\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc52 ( \ud835\udc43 \ud835\udc58 \ud835\udc56 , \ud835\udc49 \ud835\udc58 \ud835\udc56 , \ud835\udc43\ud835\udc4f\ud835\udc52\ud835\udc60\ud835\udc61 \ud835\udc56 , \ud835\udc3a\ud835\udc4f\ud835\udc52\ud835\udc60\ud835\udc61 \ud835\udc56 ) ; 4 if \ud835\udc43 \ud835\udc56 in InfoBase then 5 \ud835\udc53 \ud835\udc56\ud835\udc61\ud835\udc5b\ud835\udc52\ud835\udc60\ud835\udc60 ( \ud835\udc43 \ud835\udc56 ) \u2190 \ud835\udc45\ud835\udc52\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc63\ud835\udc52 ( InfoBase ) ; 6 end 7 else 8 \ud835\udc53 \ud835\udc56\ud835\udc61\ud835\udc5b\ud835\udc52\ud835\udc60\ud835\udc60 ( \ud835\udc43 \ud835\udc56 ) = \ud835\udc39\ud835\udc4e\ud835\udc56\ud835\udc5f\ud835\udc5b\ud835\udc52\ud835\udc60\ud835\udc60\ud835\udc38\ud835\udc63\ud835\udc4e\ud835\udc59\ud835\udc62\ud835\udc4e\ud835\udc61\ud835\udc52 ( \ud835\udc43 \ud835\udc56 )) ; 9 InfoBase \u2190 InfoBase \u222a \ud835\udc53 \ud835\udc56\ud835\udc61\ud835\udc5b\ud835\udc52\ud835\udc60\ud835\udc60 ( \ud835\udc43 \ud835\udc56 ) ; 10 end 11 if \ud835\udc43 \ud835\udc56 \u2208 \ud835\udc36\ud835\udc62\ud835\udc5f\ud835\udc43\ud835\udc46 \ud835\udc4e then 12 \ud835\udc43\ud835\udc4f\ud835\udc52\ud835\udc60\ud835\udc61 \ud835\udc56 \u2190 \ud835\udc5a\ud835\udc4e\ud835\udc65 { \ud835\udc53 \ud835\udc56\ud835\udc61\ud835\udc5b\ud835\udc52\ud835\udc60\ud835\udc60 ( \ud835\udc43\ud835\udc4f\ud835\udc52\ud835\udc60\ud835\udc61 \ud835\udc56 ) , \ud835\udc53 \ud835\udc56\ud835\udc61\ud835\udc5b\ud835\udc52\ud835\udc60\ud835\udc60 ( \ud835\udc43 \ud835\udc56 )} ; 13 end 14 if \ud835\udc43 \ud835\udc56 \u2208 \ud835\udc36\ud835\udc62\ud835\udc5f\ud835\udc43\ud835\udc46 \ud835\udc51 then 15 \ud835\udc43\ud835\udc4f\ud835\udc52\ud835\udc60\ud835\udc61 \ud835\udc56 \u2190 \ud835\udc5a\ud835\udc56\ud835\udc5b { \ud835\udc53 \ud835\udc56\ud835\udc61\ud835\udc5b\ud835\udc52\ud835\udc60\ud835\udc60 ( \ud835\udc43\ud835\udc4f\ud835\udc52\ud835\udc60\ud835\udc61 \ud835\udc56 ) , \ud835\udc53 \ud835\udc56\ud835\udc61\ud835\udc5b\ud835\udc52\ud835\udc60\ud835\udc60 ( \ud835\udc43 \ud835\udc56 )} ; 16 end 17 end 18 return \ud835\udc36\ud835\udc62\ud835\udc5f\ud835\udc43\ud835\udc46 \ud835\udc4e , \ud835\udc36\ud835\udc62\ud835\udc5f\ud835\udc43\ud835\udc46 \ud835\udc51 , InfoBase Each particle \ud835\udc43 \ud835\udc56 will update the next search direction under the dual guidance of the individual best position \ud835\udc43\ud835\udc4f\ud835\udc52\ud835\udc60\ud835\udc61 \ud835\udc56 and the global best position \ud835\udc3a\ud835\udc4f\ud835\udc52\ud835\udc60\ud835\udc61 . Constants \ud835\udc50 1 and \ud835\udc50 2 are the weighting factors of the stochastic acceleration terms, which pull particle \ud835\udc43 \ud835\udc56 towards \ud835\udc43\ud835\udc4f\ud835\udc52\ud835\udc60\ud835\udc61 \ud835\udc56 and \ud835\udc3a\ud835\udc4f\ud835\udc52\ud835\udc60\ud835\udc61 positions. To improve the ability of the particles to escape from the local optimal position, we introduce the thermal motion [37], as the first term of the Equation. 8, where \ud835\udefc denotes the inertia factor which controls the magnitude of the thermal motion. After each particle updates its position, the fairness fitness will be calculated and stored in the InfoBase (line 8-9). The InfoBase provides a platform for all particles to share information, thus avoiding duplicate computation (line 4-6). After each iteration, \ud835\udc43\ud835\udc4f\ud835\udc52\ud835\udc60\ud835\udc61 \ud835\udc56 of each particle \ud835\udc43 \ud835\udc56 will be updated (line 11-16) and the two swarms update the candidate global best position \ud835\udc3a\ud835\udc4f\ud835\udc52\ud835\udc60\ud835\udc61 using the shared information (line 17-18). Notice that the update directions of the two swarms are opposite since the search targets are different. Table 1: Recommendation performance of the models.", "4 EXPERIMENT": "In this section, we first describe our experimental setup, and then evaluate the effectiveness and efficiency of FairRec by answering several key research questions.", "4.1 Experimental Setup": "Datasets. We adopt four widely used large-scale public recommendation benchmark datasets in our experiments, the details of these datasets are as follow: MovieLens [18] is composed of 1 million movie rating data from 6,040 users for 3,883 movies. The sensitive attributes in this dataset are gender, age, and occupation, which can divide users into 294 groups. LFM360K [7] contains music listening history data from 360,000 users with sensitive attributes of gender, age and nationality. In our experiment, a interaction data of 260,000 users and 140,000 music tracks is reserved for training after filtering out those data with missing values, which can be divided into 53,058 groups based on sensitive attributes. BlackFriday 4 is a sales dataset obtained from the Kaggle website which consists of 550,068 purchases from 5,889 users for a total of 3,566 items. The sensitive attributes in this dataset are gender, age, occupation, city category, years of stay in the current city, and marital status, based on which users can be divided into 8,820 groups. Amazon [42] electronics dataset includes purchase behavior data from 175,878 users with 944,347 reviews for 29,391 items on Amazon. It does not contain any personal user information. Referring to the setup of [25], we categorize users based on their historical behavior regarding the number of interactions, total consumption, and the highest price of items purchased, and set them as the sensitive attributes. Each attribute contains ten values from 0 to 9, thus users can be divided into 1,000 groups. Following the setting of [19], we divide the last positive interaction data of users into the test set and the remaining into the training set. For each positive interaction, we randomly sample one item without user interaction as a negative interaction in the training set. We further randomly sample 49 items that users have not interacted with and combine them with the target item to form the candidate items for recommendation during testing. Recommendation Models. We adopt four classic deep CTR prediction models for the recommendation, i.e., Wide&Deep [10], DeepFM [16], DCN [41] and FGCNN [27], owing to the exhaustive academic efforts on it and tremendous amount of industry applications. These models are trained on the above four datasets following the setup of [34]. In the testing stage, the top-5 recommendation list is generated from the 50 candidate items according to the predicted CTR in the descending order. The evaluated recommendation performance of the reproduced models are shown in Table 1. 4 https://www.kaggle.com/datasets/sdolezel/black-friday FairRec: Fairness Testing for Deep Recommender Systems ISSTA 2023, 17-21 July, 2023, Seattle, USA Table 2: The main results of fairness testing from the perspective of effectiveness and efficiency. Baselines. We are not aware of a fairness testing method specialized for DRSs from the perspective of multi-sensitive attributes. Since Themis [13] and TestSGD [47] have explored the issues of multi-attributes group fairness in the classification tasks, we take them as the baselines by extending them to our tasks. For fair comparison, all three methods group users based on the same sensitive attributes and quantify unfairness using the metrics introduced in Section 3.2. Specifically, in Themis, the target groups are found via brute-force enumeration based on sparse values of different attributes, which we consider as the globally optimal target groups, and the fairness scores it obtains represent the accurate values. In TestSGD, a parameter \ud835\udf03 is used to improve testing efficiency by filtering out groups with users less than \ud835\udf03 percent of the total users. Then it calculates the fairness scores of the remaining groups and finds the target groups by traversal. In our experiment, we set \ud835\udf03 to 0.01, 0.005, and 0.002 for datasets of different scales to investigate the impact of \ud835\udf03 on the testing results of TestSGD. Moreover, to mitigate the impact of long-tail distribution of super subgroups, we also filter out those groups with users less than 0.001 percent of the population for all the testing methods. Implementation. All experiments are conducted on a server running Ubuntu 1804 operating system with 1 Intel(R) Xeon(R) E5-2682 v4 CPU running at 2.50GHz, 64GB memory and 2 NVIDIA GeForce GTX 1080 Ti GPU. To mitigate the effect of randomness, all experiment results are the average of 5 runs. The hyper-parameters used in our experiments are: a) inertial factor \ud835\udefc = 0 . 09 , b) the acceleration parameters \ud835\udc50 1 = \ud835\udc50 2 = 2 , and c) the velocity limits \ud835\udc63 \u2217 = 2 , which are obtained by experimental analysis and more details of the analysis will be discussed in the following section.", "4.2 Research Questions": "", "RQ1: Is FairRec effective enough to reveal and measure the unfairness of a DRS?": "Toanswerthis question, we evaluate FairRec on the four datasets and four DRSs using the five metrics described in Section 3.2. The main results are summarized in Table 2, in which each entry denotes the multi-attribute group fairness scores (Definition 3), which is the difference in the metric-performance values of the most advantaged and the disadvantaged groups as found by the corresponding testing method. The larger the score, the more unfair is the DRS. The rows underlined with a dotted line denote the results of Themis, which are optimal values (corresponding to the optimal target groups) for each fairness metric. In order to evaluate the effectiveness of TestSGD and FairRec, we define the testing accuracy as the ratio of the entry of a testing method to that of Themis. Either TestSGD or FairRec may end up with the locally optimal target groups. The bold value in each column denotes the most accurate result, or the one closest to that of Themis. Table 2 presents us a variety of observations and insights as follows. First, FairRec almost always obtains the most accurate testing results (occupying the bold values on \u223c 90% cases), i.e., achieving \u223c 95% testing accuracy compared to 94% and 43% for TestSGD. A closer look reveals that for all the metrics, FairRec achieves an average of 95.27%,97.37%, 92.38% and 92.83% of the optimal results ISSTA 2023, 17-21 July, 2023, Seattle, USA Huizhong Guo, Jinfeng Li, Jingyi Wang, Xiangyu Liu, Dongxia Wang, Zehong Hu, Rong Zhang, and Hui Xue Figure 3: The correlation between different metrics. UFauc UFmrr UFndcg UFurd UFurp UFauc UFmrr UFndcg UFurd UFurp MovieLens UFauc UFmrr UFndcg UFurd UFurp LFM360K UFauc UFmrr UFndcg UFurd UFurp BlackFriday UFauc UFmrr UFndcg UFurd UFurp Amazon 0.0 0.2 0.4 0.6 0.8 1.0 on MovieLens, LFM360K, BlackFriday and Amazon respectively. Besides, the performance of FairRec is rather stable despite the random nature of DPSO. For instance, even for LFM360k which has the most user groups, the test accuracy of FairRec only fluctuates in a small range of 88.68%-100% due to the specific optimization we designed for DRSs, as mentioned in Section 3.3, which greatly improves the stability of FairRec. The testing results can also be reliably used as evidence to assess the fairness deficiencies of different DRSs. For example, considering MovieLens and \ud835\udc48\ud835\udc39 \ud835\udc62\ud835\udc5f\ud835\udc51 (whether user gets diverse recommendations), with Themis we can conclude that Wide&Deep is the most unfair with a difference of 0 . 1786 (for FGCNN it is 0 . 1346 ), indicating that Wide&Deep presents users with the least diverse recommendations and echo chamber is easier to occur. With FairRec, we can get a similar conclusion that Wide&Deep is more unfair than DCN and FGCNN. However, TestSGD \ud835\udf03 = 0 . 01 would conclude incorrectly that Wide&Deep is the most fair, deriving the smallest \ud835\udc48\ud835\udc39 \ud835\udc62\ud835\udc5f\ud835\udc51 value. It is worth noticed that FGCNN exhibits the most serious fairness issues among the four DRSs in most cases. A reasonable explanation is that the feature intersection process of FGCNN is too complex, which cause it to be more sensitive to the individual information and tends to amplify individual differences. In addition, we also designed a set of experiments to compare the testing accuracy of the three methods given a fixed testing budget, as shown in Table 3. The testing time are dynamically set as follows,  where \ud835\udc41 \ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc5f\ud835\udc60 and \ud835\udc41 \ud835\udc54\ud835\udc5f\ud835\udc5c\ud835\udc62\ud835\udc5d\ud835\udc60 denote the number of users and groups after being divided based on sensitive attributes, respectively. In Table 3, the largest value in each column is marked bold, representing the biggest difference in the target groups found in the given time. Note that different from Table 2, here the group difference found by Themis is not always the largest anymore, while that found by FairRec is for most cases (e.g., 79% compared to 62% of Themis and 55% of TestSGD). The reason is that for the fixed-time testing, Themis degraded to random search which may not be able to find the target groups with the globally maximal difference, while FairRec is more effective than Themis and TestSGD in general. The results on BlackFriday and Amazon are attached in the Appendix. Second, how much a DRS is fair is not purely determined by the model itself, but also influenced by the choice of the fairness metrics. For instance, for BlackFriday, DeepFM achieves the smallest fairness score (being the most fair) regarding \ud835\udc48\ud835\udc39 \ud835\udc62\ud835\udc5f\ud835\udc5d ( 0 . 0591 by Themis), while it also the most unfair regarding \ud835\udc48\ud835\udc39 \ud835\udc4e\ud835\udc62\ud835\udc50 , among the four DRSs. What's more, we observe that fairness performance regarding different metrics may not be correlated in an expected way. For example, while all the three metrics \ud835\udc48\ud835\udc39 \ud835\udc4e\ud835\udc62\ud835\udc50 , \ud835\udc48 \ud835\udc39 \ud835\udc5a\ud835\udc5f\ud835\udc5f and \ud835\udc48\ud835\udc39 \ud835\udc5b\ud835\udc51\ud835\udc50\ud835\udc54 describe how well the recommendations capture user interest, a DRS may not achieve the same levels of fairness for them. Given Figure 4: The effectiveness improvements of FairRec compared to the vanilla DPSO. MovieLens Lastfm360K BlackFriday Amazon 0% 20% 40% 60% 80% 100% 120% 140% Improvements AUC MRR NDCG Diversity Popularity Time MovieLens, DeepFM is the most fair regarding \ud835\udc48\ud835\udc39 \ud835\udc4e\ud835\udc62\ud835\udc50 , but is also the most unfair regarding \ud835\udc48\ud835\udc39 \ud835\udc5a\ud835\udc5f\ud835\udc5f . To capture the observations above more in detail, we depict heat maps of correlation between different metrics with regard to DRSs, for the four datasets in Figure 3. Given a map, each block represents the correlation value between the two corresponding metrics (computed with their values taken for the four DRSs). We can see that for all the four datasets, \ud835\udc48\ud835\udc39 \ud835\udc5a\ud835\udc5f\ud835\udc5f is negatively correlated with \ud835\udc48\ud835\udc39 \ud835\udc4e\ud835\udc62\ud835\udc50 , while positively correlated with \ud835\udc48\ud835\udc39 \ud835\udc5b\ud835\udc51\ud835\udc50\ud835\udc54 . This implies that if the replacement of the DRS improves \ud835\udc48\ud835\udc39 \ud835\udc5a\ud835\udc5f\ud835\udc5f , then it may increase (decrease) \ud835\udc48\ud835\udc39 \ud835\udc5b\ud835\udc51\ud835\udc50\ud835\udc54 ( \ud835\udc48\ud835\udc39 \ud835\udc4e\ud835\udc62\ud835\udc50 ). Also, in general the popularity metric \ud835\udc48\ud835\udc39 \ud835\udc62\ud835\udc51\ud835\udc5d seems to be negatively correlated with \ud835\udc48\ud835\udc39 \ud835\udc4e\ud835\udc62\ud835\udc50 , meaning if we want the system to know better whether users prefer popular items, then we may end up with the sacrificed performance. More importantly, while it may usually be perceived that more diverse recommendations might reduce performance, we observe the opposite, namely \ud835\udc48\ud835\udc39 \ud835\udc62\ud835\udc5f\ud835\udc51 and \ud835\udc48\ud835\udc39 \ud835\udc4e\ud835\udc62\ud835\udc50 (or \ud835\udc48\ud835\udc39 \ud835\udc5a\ud835\udc5f\ud835\udc5f , \ud835\udc48\ud835\udc39 \ud835\udc5b\ud835\udc51\ud835\udc50\ud835\udc54 ) are positively correlated sometimes. Hence it is barely possible to achieve simultaneous improve on fairness of all the metrics, by choosing DRSs (none of the four DRSs is the most fair for all the metrics). And for a DRS developer, s/he has to consider the specific requirements of the application and carefully make trathe appropriate metrics. Third, we compare FairRec with the vanilla PSO to validate the delicate designs of our DPSO algorithm in FairRec. As shown in Figure 4, where the horizontal axis represents the different data sets, and the vertical axis represents the improvement obtained by the optimized method. On the four datasets, the test results of the optimized DPSO algorithm have improved by 4.90%, 103.01%, 22.3% and 9.35% respectively, while the average time consumption only increased by 11.01%. In particular, in the case of sparse user distribution, the test results of the optimized algorithm improve by 103.01% on average for LFM360K (the largest dataset), while the time consumption increases by only 8.75%. The reasons behind is that our method makes the particles more concentrated in the densely distributed region of the user and then move quickly towards the optimal solution, thus automatically filtering many invalid groups and saving a lot of time for computation. With all the observations above, we get the following answer: Answer to RQ1: FairRec is effective enough, even in limited time, to reveal and measure the unfairness of DRSs, achieving \u223c 95% testing accuracy with \u223c half to 1/8 time. Given fixed testing budget, FairRec is the most effective. FairRec: Fairness Testing for Deep Recommender Systems ISSTA 2023, 17-21 July, 2023, Seattle, USA Table 3: The fairness testing results on MovieLens and LFM360K within limited time.", "RQ2: How efficient is FairRec compared with existing work?": "To answer this question, we evaluate the efficiency of FairRec and make comparisons with Themis and TestSGD. The main quantitative comparison results are shown in Table 2. It is clearly observed that FairRec can achieve comparable testing performance to Themis ( \u223c 95% testing accuracy) in around \u223c half to 1/8 of the time Themis requires. Moreover, the advantage of FairRec in efficiency becomes more significant when there are more users and sensitive attributes in the testing candidates. For instance, on the LFM360K dataset, FairRec has promoted the testing efficiency by more than 5 times and 1 time compared to Themis and TestSGD \ud835\udf03 = 0 . 002 , respectively. Specifically, Themis requires more than 4 . 5 hours for testing just 260 , 000 users, which is almost infeasible in the industry scenarios with critical efficiency needs. Considering that numbers of users and groups are two critical factors affecting the testing efficiency, in this experiment, we further explore their impact on the testing efficiency of the three testing approaches. The results are shown in Figure 5, in which the horizontal axis denotes the number of groups after division and vertical axis represents the total number of users. From Figure 5, we can see that the time consumption of Themis increases exponentially as the number of groups and users increases. Concretely, in the test against DCN, as the number of users increased from 20,000 to 100,000 and the number of groups increased from 10,000 to 40,000, the time consumption increased from 756 seconds to 9768 seconds. Such level of efficiency is unacceptable to the realistic industry recommendation scenarios, where there are usually tens of millions or even hundreds of millions of users. It also shows a similar trend on the efficiency of TestSGD as the number of users grows. When the total number of users remains the same and the number of groups increases, i.e., the user distribution becomes more sparse, TestSGD thus filters out more groups that do not meet the number requirement, so its testing efficiency becomes higher. However, it comes at the expense of testing effectiveness and the improvement is not significant enough either, which is also corroborated in Table 2. In comparison, the growth trend on time consumption of FairRec is relatively smoother when the number of users and groups increases. Concretely, the time overhead of FairRec increases from Figure 5: Comparison of the efficiency of the three methods. The horizontal axis indicates the number of groups after division and vertical axis represents the total number of users, with unit of ten thousand. The contour lines in the graph indicate the time required for the test. Intuitively, denser contours indicate a higher slope of time growth. The different colored areas in the graph indicate the distribution of the values of the test time, with blue areas indicating smaller values and red indicating more significant time consumption. Themis TestSGD DPSO Wide&Deep DeepFM DCN FGCNN 114 seconds to 1242 seconds as the test scenario changes from the simple to the most complex, which are only 15% and 12% of the time required by Themis under the same conditions, respectively. We believe such high efficiency mainly benefits from the three delicate designs in our FairRec. First, the distribution-based particle initialization modeled from testing candidates can effectively improve the coverage of particles, thus helping to quickly locate the target area in the search space. Second, the specifically designed ISSTA 2023, 17-21 July, 2023, Seattle, USA Huizhong Guo, Jinfeng Li, Jingyi Wang, Xiangyu Liu, Dongxia Wang, Zehong Hu, Rong Zhang, and Hui Xue (a) Impact of particle size 1 2 3 4 5 6 7 8 9 10 1e 3 4.50 4.75 5.00 5.25 5.50 5.75 6.00 6.25 Testing result of UFauc 1e 2 Evaluation Performance Time consumption 0.4 0.6 0.8 1.0 1.2 1.4 1.6 Time(s) 1e3 0 10 20 30 40 50 iter 1.95 2.00 2.05 2.10 2.15 2.20 2.25 2.30 Testing result of UFauc 1e 1 Evaluation Performance Time consumption 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 Time(s) 1e3 (b) Impact of iterations Figure 6: The impact of \ud835\udf16 and \ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc5f on efficacy of FairRec. Table 4: Fairness mitigation based on our testing results. double-ended search scheme can boost the search process and guarantee the search convergence. Besides, the designed InfoBase can help different particle swarms share information with each other, which would avoid many unnecessary computational overheads. AnswertoRQ2: FairRec's testing efficiency outperforms Themis and TestSGD in a significant margin and is more efficient in more complex test scenarios. Hyperparametric Experiment. Usually, the size of the initialized populations and the maximum number of iterations influence the accuracy and efficiency of the testing. We design the hyperparametric experiment to analyze the effect of these two hyperparameters. Define \ud835\udf16 = \ud835\udc5b \ud835\udc5d\ud835\udc61 /| \ud835\udc41 \ud835\udc54\ud835\udc5f\ud835\udc5c\ud835\udc62\ud835\udc5d\ud835\udc60 | , which indicates the ratio of the initialized particles to the set of user groups. The result is shown in Figure 6(a). The smaller \ud835\udf16 is, the more difficult it is for the particles to collect enough information in a high-dimensional search space, resulting in low accuracy. As \ud835\udf16 increases, the accuracy of FairRec gets more stable, and the testing result is getting closer to the optimal result while the testing time becomes longer. We suggest choosing \ud835\udf16 in the range of [0.002, 0.005] in the complex case, which can ensure both high testing accuracy and testing efficiency. Then we set \ud835\udf16 = 0 . 005 to discuss the impact of the number of iterations as shown in Figure 6(b). As \ud835\udc5b \ud835\udc56\ud835\udc61 increases, the overall testing accuracy improves and eventually approaches the optimum. In this case, we suggest choosing \ud835\udc5b \ud835\udc56\ud835\udc61 in the range of [15, 20]. Note that in the relatively simple cases such as MovieLens and Amazon where \ud835\udc41 \ud835\udc54\ud835\udc5f\ud835\udc5c\ud835\udc62\ud835\udc5d\ud835\udc60 \u2264 1 , 000 , we set the \ud835\udf16 in the range of [0.1, 0.2] and \ud835\udc5b \ud835\udc56\ud835\udc61 in the range of [5,10] to get better results. Our results on the two hyperparameters provide testers hindsight of how to make a trade-off between testing efficiency and accuracy. RQ3: Can we use the the testing results of FairRec to improve the fairness of DRSs? Besides uncovering and evaluating the severity of fairness problem, our testing also aims to provide insight and guidance on improving the group fairness of a DRS. Note that our testing report not only includes the multi-attribute group fairness scores as shown in Table 2, but also provides a user-defined number of advantaged groups and disadvantaged groups (remember that the InfoBase records the metric values of each group). In this way, a model developer can target any specific disadvantaged groups (not necessarily the worst one) regarding a specific metric for bias mitigation. We adopt a simple re-ranking based method [14] in the experiment to improve the recommendation performance over disadvantaged groups to show the usefulness of fixing these groups in improving the overall fairness. We selected the 10% groups with the worst performance in \ud835\udc40\ud835\udc45\ud835\udc45 , \ud835\udc41\ud835\udc37\ud835\udc36\ud835\udc3a , \ud835\udc48\ud835\udc45\ud835\udc37 , and \ud835\udc48\ud835\udc45\ud835\udc43 for optimization, respectively. To better demonstrate the effect of bias mitigation, we first directly show the changes in the relevant metrics for these groups before and after optimization, as shown in Table 4. For example, considering MovieLens and Wide&Deep, \ud835\udc40\ud835\udc45\ud835\udc45 increased by 87.72%, meaning that disadvantaged groups have more access to their target items. In addition, considering Amazon and DeepFM, \ud835\udc48\ud835\udc45\ud835\udc43 decreases by 29.79%, which indicates that the recommendation results are more in line with popularity preferences of disadvantaged groups. The change of overall fairness score is shown in Table 6 in the Appendix due to space limit. Answer to RQ3: The test results of FairRec can provide valuable insight and guidance for bias mitigation.", "5 RELATED WORK": "Fairness testing. This work is closely related to fairness testing of deep learning models. There are many existing works focusing on individual fairness testing [12, 48]. They try to generate test cases that are discriminated on sensitive attributes, e.g., by changing the value of a sensitive attribute. In terms of group fairness, a number of definitions have been proposed [6, 29]. In regards to DRSs, our main objective is to identify and address any discrimination that may be present towards real users within the recommendation system, rather than attempting to expose all instances of discrimination across the population. Therefore, while individual fairness testing methods based on generated inputs is not applicable, we focus on group fairness testing in this work. However, the current level of attention towards group fairness testing is deemed insufficient in contemporary scholarship. Si et al. [35] proposed a statistical testing framework to detect whether a model satisfy multiple notions of group fairness, such as equal opportunity, equalized odds, etc. Galhotra et al. proposed Themis [4, 13] to measure software group discrimination with two metrics, i.e., group discrimination score and causal discrimination score. Themis groups users by various sensitive attribute values and then gets the group fairness score for each group by brute-force enumeration. Zhang et al. proposed TestSGD [47] to measure group discrimination. It automatically generates an interpretable rule set (i.e., reference for grouping) and each rule can be used to dichotomize the population. TestSGD filters the groups with a size smaller than a certain threshold like 0 . 01 and then takes brute-force enumeration through the remaining groups FairRec: Fairness Testing for Deep Recommender Systems ISSTA 2023, 17-21 July, 2023, Seattle, USA to obtain their fairness scores. However, the testing methods based on group fairness mentioned above are mostly focused on software and classification systems, which are intrinsically different to DRSs. Fairness of recommender systems. The fairness of recommender systems is nowadays receiving more and more attention [5, 32]. Recommender systems involve multiple stakeholders, including users, content/product providers, and side stakeholders [1]. There has been some work discussing fairness from user perspective [20, 24]. Yao et al. in [46] proposed four group fairness metrics to evaluate collaborative filtering models. They dichotomize the population by gender and study sexism in recommender systems on the MovieLens dataset as well as on synthetic datasets. Li et al. [25] divided users into active and inactive groups according to their behavioral history in the e-commerce recommender system. They find that users less active were treated significantly worse than those who are more active. Wu et al. [45] introduced a sensitive attribute predictor to measure the association between news recommendations and gender, and used it to evaluate the fairness of the news recommender system. However, the above mentioned works all use a single attribute to evaluate the fairness of recommender systems, which cannot capture the deeply hidden fairness issues in a multi-attribute perspective (Refer to Table 5 in Appendix). In addition, they do not propose efficient methods to test the group fairness for DRSs. Unfairness mitigation. Methods for unfairness mitigation can be divided into three main categories: pre-processing, in-processing and post-processing [8]. Pre-processing methods ensure fairness by eliminating the bias present in the training data. Ryosuke et al. [36] proposed a pre-processing method based on pairwise ordering for weighting pairs of training data, which can improve the fairness of the ranking model. In-processing methods consider fairness constraints in the model training process. For example, Yao et al. in [46] proposed four unfairness metrics, which can be optimized by adding fairness terms to the learning objective. Wu et al. [45] introduced an unfairness mitigation approach for news recommendation using decomposed adversarial learning, which eliminated the biases brought by sensitive features to ensure users get unbiased recommendations. Post-processing method adjusts the output of the base recommendation model to mitigate unfairness. Li et al. [25] employed a re-ranking algorithm to produce new recommendation lists for users by leveraging the initial recommendation results, thereby meeting the fairness constraint. Compared with the previous two methods for mitigating unfairness, post-processing methods do not require retraining the existing recommendation models, saving a significant amount of resources, and are also more practically feasible in real-world systems. In this work, we employ post-processing methods to demonstrate how the results of FairRec can help disadvantaged groups and alleviate fairness concerns across the entire DRS.", "6 CONCLUSION": "In this work, we study the problem of multi-attribute group fairness testing on deep learning based recommender systems by presenting FairRec, a systematical fairness testing framework built upon a specifically designed search-based testing algorithm. We answer the three key research questions through quantitative and qualitative experiments on four extensively used recommendation models over four public benchmarks. The experiment results demonstrate that FairRec is effective and efficient in revealing the fairness problems of DRSs from different perspectives, and outperforms the compared baselines by a significant margin. Furthermore, it also shows that our testing results can provide insight and guidance for mitigating the bias of the tested DRSs with little negative impact on the recommendation performance. Our work may shed new light on the research of building more fair deep recommender systems from a testing point of view and benchmark future testing research in this area. Furthermore, FairRec has the potential to be used as a generic testing framework in other systems for multi-attribute group fairness testing.", "ACKNOWLEDGEMENT": "This research was supported by the Key R&D Program of Zhejiang (2022C01018), the NSFC Programs (62102359 and 62106223), Alibaba Group through Alibaba Innovative Research Program, and the Research Center of Alibaba Artificial Intelligence Governance. In addition, We are very grateful to the anonymous reviewers for their valuable comments to improve our paper.", "REFERENCES": "[1] HimanAbdollahpouri and Robin Burke. 2019. Multi-stakeholder recommendation and its connection to multi-sided fairness. arXiv preprint arXiv:1907.13158 (2019). [2] Himan Abdollahpouri, Masoud Mansoury, Robin Burke, Bamshad Mobasher, and Edward Malthouse. 2021. User-centered evaluation of popularity bias in recommender systems. In Proceedings of the 29th ACM Conference on User Modeling, Adaptation and Personalization . 119-129. [3] Aniya Aggarwal, Pranay Lohia, Seema Nagar, Kuntal Dey, and Diptikalyan Saha. 2019. Black box fairness testing of machine learning models. In Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering . 625-635. [4] Rico Angell, Brittany Johnson, Yuriy Brun, and Alexandra Meliou. 2018. Themis: Automatically testing software for discrimination. In Proceedings of the 2018 26th ACM Joint meeting on european software engineering conference and symposium on the foundations of software engineering . 871-875. [5] Alex Beutel, Jilin Chen, Tulsee Doshi, Hai Qian, Li Wei, Yi Wu, Lukasz Heldt, Zhe Zhao, Lichan Hong, Ed H Chi, et al. 2019. Fairness in recommendation ranking through pairwise comparisons. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 2212-2220. [6] Alessandro Castelnovo, Riccardo Crupi, Greta Greco, Daniele Regoli, Ilaria Giuseppina Penco, and Andrea Claudio Cosentini. [n. d.]. A clarification of the nuances in the fairness metrics landscape. Scientific Reports 12, 1 ([n. d.]), 1-21. [7] \u00d2scar Celma Herrada et al. 2009. Music recommendation and discovery in the long tail . Universitat Pompeu Fabra. [8] Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan He. 2023. Bias and debias in recommender system: A survey and future directions. ACM Transactions on Information Systems 41, 3 (2023), 1-39. [9] Zhenpeng Chen, Jie M Zhang, Max Hort, Federica Sarro, and Mark Harman. 2022. Fairness Testing: A Comprehensive Survey and Analysis of Trends. arXiv preprint arXiv:2207.10223 (2022). [10] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems . 7-10. [11] Mengnan Du, Fan Yang, Na Zou, and Xia Hu. 2020. Fairness in deep learning: A computational perspective. IEEE Intelligent Systems 36, 4 (2020), 25-34. [12] Ming Fan, Wenying Wei, Wuxia Jin, Zijiang Yang, and Ting Liu. 2022. Explanation-Guided Fairness Testing through Genetic Algorithm. arXiv preprint arXiv:2205.08335 (2022). [13] Sainyam Galhotra, Yuriy Brun, and Alexandra Meliou. 2017. Fairness testing: testing software for discrimination. In Proceedings of the 2017 11th Joint meeting on foundations of software engineering . 498-510. [14] Elizabeth G\u00f3mez, Carlos Shui Zhang, Ludovico Boratto, Maria Salam\u00f3, and Mirko Marras. 2021. The winner takes it all: geographic imbalance and provider (un) fairness in educational recommender systems. In Proceedings of the 44th International ISSTA 2023, 17-21 July, 2023, Seattle, USA Huizhong Guo, Jinfeng Li, Jingyi Wang, Xiangyu Liu, Dongxia Wang, Zehong Hu, Rong Zhang, and Hui Xue ACM SIGIR Conference on Research and Development in Information Retrieval . 1808-1812. [15] Nina Grgic-Hlaca, Muhammad Bilal Zafar, Krishna P Gummadi, and Adrian Weller. 2016. The case for process fairness in learning: Feature selection for fair decision making. In NIPS symposium on machine learning and the law , Vol. 1. Barcelona, Spain, 2. [16] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. arXiv preprint arXiv:1703.04247 (2017). [17] Moritz Hardt, Eric Price, and Nati Srebro. 2016. Equality of opportunity in supervised learning. Advances in neural information processing systems 29 (2016). [18] F Maxwell Harper and Joseph A Konstan. 2015. The movielens datasets: History and context. Acm transactions on interactive intelligent systems (tiis) 5, 4 (2015), 1-19. [19] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural collaborative filtering. In Proceedings of the 26th international conference on world wide web . 173-182. [20] Rashidul Islam, Kamrun Naher Keya, Ziqian Zeng, Shimei Pan, and James Foulds. 2021. Debiasing career recommendations with neural fair collaborative filtering. In Proceedings of the Web Conference 2021 . 3779-3790. [21] Kalervo J\u00e4rvelin and Jaana Kek\u00e4l\u00e4inen. 2002. Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems (TOIS) 20, 4 (2002), 422-446. [22] James Kennedy and Russell Eberhart. 1995. Particle swarm optimization. In Proceedings of ICNN'95-international conference on neural networks , Vol. 4. IEEE, 1942-1948. [23] Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. 2017. Counterfactual fairness. Advances in neural information processing systems 30 (2017). [24] Jurek Leonhardt, Avishek Anand, and Megha Khosla. 2018. User fairness in recommender systems. In Companion Proceedings of the The Web Conference 2018 . 101-102. [25] Yunqi Li, Hanxiong Chen, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2021. User-oriented fairness in recommendation. In Proceedings of the Web Conference 2021 . 624-632. [26] Charles X Ling, Jin Huang, Harry Zhang, et al. 2003. AUC: a statistically consistent and more discriminating measure than accuracy. In Ijcai , Vol. 3. 519-524. [27] Bin Liu, Ruiming Tang, Yingzhi Chen, Jinkai Yu, Huifeng Guo, and Yuzhou Zhang. 2019. Feature generation by convolutional neural network for click-through rate prediction. In The World Wide Web Conference . 1119-1129. [28] Chenyang Lyu, Shouling Ji, Chao Zhang, Yuwei Li, Wei-Han Lee, Yu Song, and Raheem Beyah. 2019. { MOPT } : Optimized mutation scheduling for fuzzers. In 28th USENIX Security Symposium (USENIX Security 19) . 1949-1966. [29] Shira Mitchell, Eric Potash, Solon Barocas, Alexander D'Amour, and Kristian Lum. 2021. Algorithmic fairness: Choices, assumptions, and definitions. Annual Review of Statistics and Its Application 8 (2021), 141-163. [30] Anjana Perera, Aldeida Aleti, Chakkrit Tantithamthavorn, Jirayus Jiarpakdee, Burak Turhan, Lisa Kuhn, and Katie Walker. 2022. Search-based fairness testing for regression-based machine learning systems. Empirical Software Engineering 27, 3 (2022), 1-36. [31] Lijing Qin and Xiaoyan Zhu. 2013. Promoting diversity in recommendation by entropy regularizer. In Twenty-Third International Joint Conference on Artificial Intelligence . Citeseer. [32] Hossein A Rahmani, Mohammadmehdi Naghiaei, Mahdi Dehghan, and MohammadAliannejadi. 2022. Experiments on Generalizability of User-Oriented Fairness in Recommender Systems. arXiv preprint arXiv:2205.08289 (2022). [33] Ying Shan, T Ryan Hoens, Jian Jiao, Haijing Wang, Dong Yu, and JC Mao. 2016. Deep crossing: Web-scale modeling without manually crafted combinatorial features. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining . 255-262. [34] Weichen Shen. 2017. DeepCTR: Easy-to-use,Modular and Extendible package of deep-learning based CTR models. https://github.com/shenweichen/deepctr. [35] Nian Si, Karthyek Murthy, Jose Blanchet, and Viet Anh Nguyen. 2021. Testing group fairness via optimal transport projections. In International Conference on Machine Learning . PMLR, 9649-9659. [36] Ryosuke Sonoda. 2021. A Pre-processing Method for Fairness in Ranking. arXiv preprint arXiv:2110.15503 (2021). [37] Jun Sun, Wei Fang, Xiaojun Wu, Choi-Hong Lai, and Wenbo Xu. 2011. Solving the multi-stage portfolio optimization problem with a novel particle swarm optimization. Expert Systems with Applications 38, 6 (2011), 6727-6735. [38] Sakshi Udeshi, Pryanshu Arora, and Sudipta Chattopadhyay. 2018. Automated directed fairness testing. In Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering . 98-108. [39] Ellen M Voorhees. 2001. The TREC question answering track. Natural Language Engineering 7, 4 (2001), 361-378. [40] Hao Wang. 2022. Fairness Metrics for Recommender Systems. In 2022 9th International Conference on Wireless Communication and Sensor Networks (ICWCSN) (Dalian, China) (icWCSN 2022) . Association for Computing Machinery, New York, NY, USA, 89-92. https://doi.org/10.1145/3514105.3514120 [41] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network for ad click predictions. In Proceedings of the ADKDD'17 . 1-7. [42] Wenjie Wang, Fuli Feng, Xiangnan He, Xiang Wang, and Tat-Seng Chua. 2021. Deconfounded recommendation for alleviating bias amplification. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining . 1717-1725. [43] Xiuling Wang and Wendy Hui Wang. 2022. Providing Item-side Individual Fairness for Deep Recommender Systems. In 2022 ACM Conference on Fairness, Accountability, and Transparency . 117-127. [44] Andreas Windisch, Stefan Wappler, and Joachim Wegener. 2007. Applying particle swarmoptimization to software testing. In Proceedings of the 9th annual conference on Genetic and evolutionary computation . 1121-1128. [45] Chuhan Wu, Fangzhao Wu, Xiting Wang, Yongfeng Huang, and Xing Xie. 2021. Fairness-aware news recommendation with decomposed adversarial learning. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 35. 4462-4469. [46] Sirui Yao and Bert Huang. 2017. Beyond parity: Fairness objectives for collaborative filtering. Advances in neural information processing systems 30 (2017). [47] Mengdi Zhang, Jun Sun, Jingyi Wang, and Bing Sun. 2022. TESTSGD: Interpretable Testing of Neural Networks Against Subtle Group Discrimination. arXiv preprint arXiv:2208.11321 (2022). [48] Peixin Zhang, Jingyi Wang, Jun Sun, Guoliang Dong, Xinyu Wang, Xingen Wang, Jin Song Dong, and Ting Dai. 2020. White-box fairness testing through adversarial sampling. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering . 949-960. [49] Ziwei Zhu, Yun He, Xing Zhao, and James Caverlee. 2021. Popularity Bias in Dynamic Recommendation. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining . 2439-2449. [50] Cai-Nicolas Ziegler, Sean M McNee, Joseph A Konstan, and Georg Lausen. 2005. Improving recommendation lists through topic diversification. In Proceedings of the 14th international conference on World Wide Web . 22-32. FairRec: Fairness Testing for Deep Recommender Systems ISSTA 2023, 17-21 July, 2023, Seattle, USA", "APPENDIX": "Table 5 shows the results of the fairness testing after dividing the population based on combinations of different sensitive attributes, which allows us to investigate the fairness issues under a multiattribute view. We select the dataset MovieLens (with sensitive attributes gender, age and occupation ) and BlackFriday (with sensitive attributes gender,age, occupation, city category, years of stay in the current city, and marital status ). We can observe that with more sensitive attributes considered, more serious fairness issues can be disclosed. For example, for MovieLens, when only Gender is considered (i.e., there are only two groups), the fairness-performance gap between the most advantaged group and disadvantaged group is 0 . 0021 . However, when Gender, Age and Occupation are all considered (i.e., users get divided w.r.t the combinatorial values of the three dimensions), that gap turns to be 0 . 2537 , implying the recommendations are more unfair. Table 5: Multi-attribute fairness based on different cases.", "Note:": "\u00b7 \ud835\udc34 1 : Gender. \u00b7 \ud835\udc34 2 : Gender, Age. \u00b7 \ud835\udc34 3 : Gender, Age, City category. \u00b7 \ud835\udc34 4 : Gender, Age, City category, Years of stay in the current city. \u00b7 \ud835\udc34 5 : Gender, Age, City category, Years of stay in the current city, Marital status. \u00b7 \ud835\udc34 6 : Gender, Age, City category, Years of stay in the current city, Marital status, Occupation. Table 6 shows the change of overall fairness scores before and after mitigation mentioned in the RQ3 of section 4.2. Each entry represents the gap between the most advantaged and disadvantaged groups is reduced w.r.t the corresponding metric after mitigation is applied. We can notice that after mitigation, the fairnessperformance w.r.t each metric gets improved (i.e., reduced gap between the target groups). Table 6: Unfairness mitigation based on our testing results. Table 7 in the next page shows the fixed-time testing results on BlackFriday and Amazon as mentioned in the RQ1 of section 4.2. We can observe that in most cases, our FairRec finds the largest gap between the advantaged and disadvantaged groups, revealing the severity of the fairness problem more accurately. ISSTA 2023, 17-21 July, 2023, Seattle, USA Huizhong Guo, Jinfeng Li, Jingyi Wang, Xiangyu Liu, Dongxia Wang, Zehong Hu, Rong Zhang, and Hui Xue Table 7: The fairness testing results on BlackFriday and Amazon within limited time."}
