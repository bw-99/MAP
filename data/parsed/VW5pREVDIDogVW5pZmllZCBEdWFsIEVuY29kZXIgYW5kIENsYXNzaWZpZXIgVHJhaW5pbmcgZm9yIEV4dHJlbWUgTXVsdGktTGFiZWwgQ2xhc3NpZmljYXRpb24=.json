{"UniDEC : Unified Dual Encoder and Classifier Training for Extreme Multi-Label Classification": "Siddhant Kharbanda \u2020 University of California Los Angeles, United States skharbanda17@g.ucla.edu Devaansh Gupta \u2020 University of California Los Angeles, United States devaansh@cs.ucla.edu Gururaj K Microsoft Bengaluru, India gururajk@microsoft.com Pankaj Malhotra Microsoft Bengaluru, India pamalhotra@microsoft.com Amit Singh Microsoft Bengaluru, India siamit@microsoft.com Cho-Jui Hsieh University of California Los Angeles, United States chohsieh@cs.ucla.edu", "Abstract": "Rohit Babbar University of Bath Bath, United Kingdom Aalto University Espoo, Finland rb2608@bath.ac.uk", "ACMReference Format:": "Extreme Multi-label Classification (XMC) involves predicting a subset of relevant labels from an extremely large label space, given an input query and labels with textual features. Models developed for this problem have conventionally made use of dual encoder (DE) to embed the queries and label texts and one-vs-all (OvA) classifiers to rerank the shortlisted labels by the DE. While such methods have shown empirical success, a major drawback is their computational cost, often requiring upto 16 GPUs to train on the largest public dataset. Such a high cost is a consequence of calculating the loss over the entire label space. While shortlisting strategies have been proposed for classifiers, we aim to study such methods for the DE framework. In this work, we develop UniDEC, a loss-independent, end-to-end trainable framework which trains the DE and classifier together in a unified manner with a multi-class loss, while reducing the computational cost by 4 -16 \u00d7 . This is done via the proposed pick-some-label (PSL) reduction, which aims to compute the loss on only a subset of positive and negative labels. These labels are carefully chosen in-batch so as to maximise their supervisory signals. Not only does the proposed framework achieve state-of-the-art results on datasets with labels in the order of millions, it is also computationally and resource efficient in achieving this performance on a single GPU. Code is made available at https://github.com/the-catalyst/UniDEC.", "CCS Concepts": "\u00b7 Information systems \u2192 Learning to rank ; Top-k retrieval in databases ; Retrieval models and ranking ; \u00b7 Computing methodologies \u2192 Ranking .", "Keywords": "multi-label classification, extreme classification, dual-encoders, extreme classifiers, contrastive learning, hard-negative mining This work is licensed under a Creative Commons Attribution International 4.0 License. WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia \u00a9 2025 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-1274-6/25/04 https://doi.org/10.1145/3696410.3714704 Siddhant Kharbanda \u2020 , Devaansh Gupta \u2020 , Gururaj K, Pankaj Malhotra, Amit Singh, Cho-Jui Hsieh, and Rohit Babbar. 2025. UniDEC : Unified Dual Encoder and Classifier Training for Extreme Multi-Label Classification. In Proceedings of the ACM Web Conference 2025 (WWW '25), April 28May 2, 2025, Sydney, NSW, Australia. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3696410.3714704", "1 Introduction": "Extreme Multi-label Classification (XMC) is described as the task of identifying i.e. retrieving a subset, comprising of one or more labels, that are most relevant to the given data point from an extremely large label space, potentially consisting of millions of possible choices. Over time, XMC has increasingly found its relevance for solving multiple real world use cases. Typically, long-text XMC approaches are leveraged for the tasks of document tagging and product recommendation and short-text XMC approaches target tasks such as query-ad keyword matching and related query recommendation. Notably, in the real world manifestations of these use cases, the distribution of instances among labels exhibits a fit to Zipf's law [1]. This implies, the vast label space ( \ud835\udc3f \u2248 10 6 ) is skewed and is characterized by the existence of head , torso and tail labels [34]. For example, in query-ad keyword matching for search engines like Bing, Google etc. head keywords are often exact match or related phrase extensions of popularly searched queries while tail keywords often target specific niche queries. Typically, we can characterize head, torso and tail keywords as having > 100, 10 100, and 1 - 10 annotations, respectively. Typically, per-label classifiers are employed for solving the XMC task. A naive strategy to train classifiers for XMC involves calculating the loss over the entire label set. This method has seen empirical success, particularly in earlier works [2, 3, 32], which learn one-vsall classifiers by parallelisation across multiple CPU cores. With the adoption of deep encoders, various works moved to training on GPUs, which due to VRAM constraints, led to the use of treebased shortlisting strategies [5, 8, 19, 20] to train classifiers on the hardest negatives. This reduced the computational complexity from \u2020 Equal Contribution WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia Kharbanda and Gupta et al. Figure 1: The architecture for the UniDEC framework, denoting the the classifiers and DE trained in parallel, along with the loss functions used. The inference pipeline is shown in the rectangular box. y(LB) Label Pool LB Lc $(QB) Dual Encoder $ Queries QB Inference 42(LB) ANNS 42(9) Renee UniDEC DEXML VRAM: 256 GB VRAM: 48 GB VRAM: 640 GB OvA Dual Classifiers Encoders O( \ud835\udc3f ) to O( log \ud835\udc3f ) . While leveraging a label shortlist made XMC training more feasible via modular training [7, 9] or joint training using a meta-classifier [16, 19, 20] , it still left out scope for empirical improvements. Consequently, Renee [14] demonstrated the extreme case for methods which employ classifiers with deep encoders by writing custom CUDA kernels to scale classifier training over the entire label space. This, however, leads to a GPU VRAM usage of 256GB (Figure 1) for training a DistilBERT model on LF-AmazonTitles-1.3M, the largest XMC dataset. Notably, what remains common across XMC classifier-training algorithms is the advocacy of OvA reduction for the multi-label problem [6]. Theoretically, the alternative, pick-all-labels (PAL), should lead to better optimization over OvA, since it promotes 'competition' amongst labels [23]. However, PAL has neither been well-studied nor successfully leveraged to train classifiers in XMC since such losses require considering all labels, which is prohibitively expensive. Aparallel line of research involves leveraging dual encoders (DE) for XMC. While DE models are a popular choice for dense retrieval (DR) and open-domain question answering (ODQA) tasks, these are predominantly few and zero-shot scenarios. In contrast, XMC covers a broader range of scenarios (see Table 7). Consequently, modelling the XMC task as a retrieval problem is tantamount to training a DE simultaneously on many , few and one-shot scenarios. While DE trained with triplet loss was thought to be insufficient for XMC, and thus augmented with per-label classifiers to enhance performance [7, 12], a recent work Dexml [12] proved the sufficiency of the DE framework for XMC by proposing a new multi-class loss function Decoupled Softmax , which computed the loss over the entire label space. This, however is very computationally expensive, as Dexml requires 640GB VRAM to train on LF-AmazonTitles-1.3M. At face value, PAL reduction of multi-label problems for DE training should be made tractable by optimizing over in-batch labels, however in practice, it does not scale to larger datasets due to the higher number of positives per label. For instance, for LFAmazonTitles-1.3M a batch consisting of 1,000 queries will need an inordinately large label pool of size \u223c 22.2K (considering in-batch negatives) to effectively train a DE with the PAL loss. Alternatively, the stochastic implementation of PAL in the form of pick-one-label (POL) reduction used by Dexml, either convergences slowly [12] or fails to reach SOTA performance. In order to enable efficient training, in this work, we propose 'pick-some-labels' (PSL) relaxation of the PAL reduction for the multi-label classification problem which enables scaling to large datasets ( \u223c 10 6 labels). Here, instead of trying to include all the positive labels for instances in a batch, we propose to randomly sample at max \ud835\udefd positive labels per instance. To the best of our knowledge, ours is the first work to study the effect of multi-class losses for training classifiers at an extreme scale. Further, we aim to develop an end-to-end trainable loss-independent framework, UniDEC Uni fied D ual E ncoder and C lassifier, for XMC that leverages the multi-positive nature of the XMC task to create highly informative in-batch labels to train the DE, and be used as a shortlist for the classifier. As shown in Figure 1, UniDec, in a single pass, performs an update step over the combined loss computed over two heads: (i) between DE head's query and sampled label-text embeddings, (ii) between classifier (CLF) head's query embeddings and classifier weights corresponding to sampled labels. By unifying the two compute-heavy ends of the XMC spectrum in such a way, UniDEC is able to significantly reduce the training computational cost down to a single 48GB GPU , even for the largest dataset with 1.3M labels. End-to-end training offers multiple benefits as it (i) helps us do away with a meta-classifier and modular training, (ii) dynamically provides progressively harder negatives with lower GPU VRAM consumption, which has been shown to outperform static negative mining [16, 19, 20] (iii) additionally, with an Approximate Nearest Neighbour Search (ANNS), it can explicitly mine hard negative labels added to the in-batch negatives. While UniDEC is a loss independent framework (see Table 3), the focus of this work also includes studying the use of multi-class losses for training multi-label classifiers at an extreme scale via the proposed PSL reduction. To this end, we benchmark UniDEC on 6 public datasets, forwarding the state-of-the-art in each, and a proprietary dataset containing 450M labels. Finally, we also experimentally show how OvA losses like BCE can be applied in tandem with multi-class losses for classifier training.", "2 Related Works & Preliminaries": "For training, we have a multi-label dataset D = {{ x \ud835\udc56 , P \ud835\udc56 } \ud835\udc41 \ud835\udc56 = 1 , { z \ud835\udc59 } \ud835\udc3f \ud835\udc59 = 1 } comprising of \ud835\udc41 data points and \ud835\udc3f labels. Each x \ud835\udc56 is associated with a small ground truth label set P \ud835\udc56 \u2282 [ \ud835\udc3f ] out of \ud835\udc3f \u223c 10 6 possible labels. Further, x \ud835\udc56 , z \ud835\udc59 \u2208 X denote the textual descriptions of the data point \ud835\udc56 and the label \ud835\udc59 respectively, which, in this setting, derive from the same vocabulary universe V [6]. The goal is to learn a parameterized function \ud835\udc53 which maps each instance x \ud835\udc56 to the vector of its true labels y \ud835\udc56 \u2208 [ 0 , 1 ] \ud835\udc3f where y \ud835\udc56,\ud835\udc59 = 1 \u21d4 \ud835\udc59 \u2208 P \ud835\udc56 . UniDEC : Unified Dual Encoder and Classifier Training for Extreme Multi-Label Classification WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia Figure 2: (a) Visualizing UniDEC's batching strategy. Such a framework naturally leads to higher number of positives per query, enabling us to scale without increasing the batch size significantly. (b) Scatter plot showing the average number of positive labels per query, when we sample \ud835\udefd positives and \ud835\udf02 hard negatives in the batch. Note that, even with \ud835\udefd = 3 and \ud835\udf02 = 0 , avg( | \ud835\udc43 B \ud835\udc56 |) = 13 . 6 . (a) (b) QB Average # tives per label 13.6 Randomly sampled batch, where Negative-mining aware sampled The same batch; when trained with UniDEC The 14.0 one positive is sampled per query. batch; where positives of multiple additional positives are also incorporated in the 14.4 Due this batching strategy; queries overlap_ These additional function The explicitly added hardest negatives positives sampled from other positives are typically ignored The mined hard negatives when 14.8 queries usually do not overlap_ while calculating the loss_ collated, overlap and be positives for other queries. PB # sampled hard negatives (n) posi - may Dual Encoder . A DE consists of the query encoder \u03a6 \ud835\udc5e , and a label encoder \u03a6 \ud835\udc59 . Conventionally, the parameters for \u03a6 \ud835\udc5e and \u03a6 \ud835\udc59 are shared, and thus we will simply represent it as \u03a6 [7, 12, 17, 29]. The mapping \u03a6 ( . ) projects the instance x \ud835\udc56 and label-text z \ud835\udc59 into a shared \ud835\udc51 -dimensional unit hypersphere S \ud835\udc51 -1 . For each instance x \ud835\udc56 , its similarity with label z \ud835\udc59 is computed via an inner product i.e., \ud835\udc60 \ud835\udc56,\ud835\udc59 = \u27e8 \u03a6 ( x \ud835\udc56 ) , \u03a6 ( z \ud835\udc59 )\u27e9 to produce a ranked list of top-K labels. \u03a8 \ud835\udc3f \ud835\udc59 = 1 \u2208 R \ud835\udc3f \u00d7 \ud835\udc51 [5, 38, 39]. The relevance of a label \ud835\udc59 to an instance is scored using an inner product, \ud835\udc60 \ud835\udc56,\ud835\udc59 = \u27e8 \u03a6 ( x \ud835\udc56 ) , \u03a8 \ud835\udc59 \u27e9 to select the \ud835\udc58 highest-scoring labels. Under the conventional OvA paradigm, each label is independently treated with a binary loss function \u2113 \ud835\udc35\ud835\udc36 applied to each entry in the score vector. It can be expressed as, Training two-tower algorithms for XMC at scale is made possible by recursively splitting (say, via a hierarchical clustering strategy) instance encoder embeddings { \u03a6 ( x \ud835\udc56 )} \ud835\udc41 \ud835\udc56 = 1 into disjoint clusters \ud835\udd05 [7], where each cluster represents a training batch B . The queries in the cluster/batch will be denoted as \ud835\udc44 B . In other words, the conventionally randomised batches now consist of semantically similar queries. For each query, a set of positive labels are sampled (typically, a single label) and collated into the batch. Post collation, positive labels for a particular instance become negative labels for other instances in the batch [7, 12, 17, 29], i.e. N \ud835\udc56 = \ud835\udc3f B - P \ud835\udc56 , where \ud835\udc3f B is the collated label pool in the batch. As compared to random batching, [7] posit that the batches created from instanceclustering are negative-mining aware i.e. for every instance, the sampled positives of the other instances in the batch serve as the set of appropriate 'hard' negatives. An additional effect of this is the accumulation of multiple inbatch positives for most queries (see Figure 2a). This makes the direct application of commonly used multi-class loss - InfoNCE loss - infeasible for training DE. Hence XMC methods find it suitable to replace InfoNCE loss with a triplet loss [7, 9] or probabilistic contrastive loss [6], as it can be potentially applied over multiple positives and hard negatives (equation 1 in [7]). While this would seem favourable, these approaches still fail to leverage the additional positive signals owing to multiple positives in the batch as they calculate loss over only a single sampled positive i.e. employing POL reduction instead of PAL reduction. Classifiers in XMC . The traditional XMC set-up considers labels as featureless integer identifiers which replace the encoder representation of labels \u03a6 ( z \ud835\udc59 ) with learnable classifier embeddings", "3 Method: UniDEC": "In this work, we propose a novel multi-task learning framework which, in an end-to-end manner, trains both - a dual encoder and extreme classifiers - in parallel. The framework eliminates the need of a meta classifier for a dynamic in-batch shortlist. Further, it provides the encoder with the capability to explicitly mine hardnegatives, obtained by querying an ANNS, created over { \u03a6 ( z \ud835\udc59 )} \ud835\udc3f \ud835\udc59 = 1 , which is refreshed every \ud835\udf00 epochs. The DE head is denoted by \u03a6 \ud835\udd07 (\u00b7) = \ud835\udd11 ( \ud835\udc54 1 ( \u03a6 (\u00b7))) and the classifier head by \u03a6 \u212d (\u00b7) = \ud835\udc54 2 ( \u03a6 (\u00b7)) , where \ud835\udd11 represents the L2 normalization operator and \ud835\udc54 1 (\u00b7) and \ud835\udc54 2 (\u00b7) represent separate nonlinear projections. Unlike DE, and as is standard practice for OvA classifiers, we train them without additional normalization [6, 8].", "3.1 Pick-some-Labels Reduction": "L PAL-N [23] is a multiclass loss that computes the loss over the entire label space; this also makes it computationally expensive for XMC scenarios. It is formulated as :  where \u03a6 1 and \u03a6 2 are encoding networks. To reduce the computational costs associated with this reduction, we propose a relaxation by computing loss over some labels \ud835\udc3f B in the batch B , which we call pick-some-labels (PSL) . WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia Kharbanda and Gupta et al.  Any multi-class loss 1 can be used in place of \u2113 \ud835\udc40\ud835\udc36 . By varying \u03a6 1 and \u03a6 2 , we get a generic loss function for training classifier as well as DE. This approximation enables employing PALN over a minibatch \ud835\udc44 B by sampling a subset of positive labels \u02c6 P \ud835\udc56 \u2286 P \ud835\udc56 \ud835\udc60.\ud835\udc61 . | \u02c6 P \ud835\udc56 | \u2264 \ud835\udefd . Typical value for \ud835\udefd can be found in Figure 2b. The collated label pool, considering in-batch negative mining, is defined as \ud835\udc3f B = { -\ud835\udc56 \u2208 \ud835\udc44 B \u02c6 P \ud835\udc56 } . Here, P B \ud835\udc56 = {P \ud835\udc56 \u2229 \ud835\udc3f B } denotes all the in-batch positives for an instance x \ud835\udc56 , i.e., the green and pale green in Figure 2.", "3.2 Dual Encoder Training with Pick-some-Labels": "The PSL loss to train a DE is formulated as,  Specifically, we perform k-means clustering on the queries such that similar queries are clustered into the same batch B , leading to both positive and negative-aware batching [7]. An optimal batch, would consist of all the positives for a particular instance with the smallest batch size. Naively adding all positive labels to a batch leads to an intractable batch size, thereby motivating a subsampling of positive labels. Interestingly, for a query, the sampled positives of semantically similar queries are often the same as its unsampled positives. Although we sample | \u02c6 P \ud835\udc56 | \u2264 \ud835\udefd for all queries in the batch, for many queries, the actual in-batch positives |P B \ud835\udc56 | \u2265 \ud835\udefd due to accumulation, thus tending to an optimal batch. As per our observations, P B \ud835\udc56 = P \ud835\udc56 for most tail and torso queries. For e.g., even if \ud835\udefd = 1 for LF-AmazonTitles-1.3M, for | \ud835\udc44 B | = 10 3 , Avg (| \u02c6 P \ud835\udc56 |) = [ 12 , 14 ] . Thus, it makes PSL reduction same as PAL for torso and tail queries and only taking form of PSL for head queries. DynamicANNSHard-NegativeMining . While the above strategy leads to collation of hard negatives in a batch, it might not mine hardest-to-classify negatives [7]. We explicitly add them by querying an ANNS created over { \u03a6 \ud835\udd07 ( z \ud835\udc59 )} \ud835\udc3f \ud835\udc59 = 1 for all { \u03a6 \ud835\udd07 ( x \ud835\udc56 )} \ud835\udc41 \ud835\udc56 = 1 . For each training instance, we create a list of hard negative labels H \ud835\udc56 , mined by querying instances on an ANNS built on label embeddings. Every iteration, we uniformly sample a \ud835\udf02 -sized hard-negative label subset per instance (denoted by red in Figure 2) \u02c6 H \ud835\udc56 \u2282 H \ud835\udc56 , independent of the positive labels. Thus, the new batch label pool can be denoted as \ud835\udc3f B = { -\ud835\udc56 \u2208 \ud835\udc44 B \u02c6 P \ud835\udc56 \u222a \u02c6 H \ud835\udc56 } . Due to the multi-positive nature of XMC, sampled hard-negatives for x \ud835\udc56 might turn out to be an unsampled positive label for x \ud835\udc57 (represented by the dark green square in Figure 2a). This effect is also quantified in Figure 2b. Query clustering for batching and dynamic ANNS hard-negative mining strategies complement each other, since the presence of similar queries leads to a higher overlap in their positives and hard negatives , enabling us to scale the effective size of the label pool. Further, to provide \u03a6 \ud835\udd07 and \u03a6 \u212d with progressively harder negatives, 1 While binary class loss functions can also be used, in this work, our focus is to study multi-class losses the ANNS is refreshed every \ud835\udf0f epochs and to uniformly sample hard negatives, we keep |H| = \ud835\udf02 \u00d7 \ud835\udf0f . Note that \ud835\udc3f \ud835\udd07 ,\ud835\udc5e 2 \ud835\udc59 denotes the multi-class loss between x \ud835\udc56 and z \ud835\udc59 \u2200 \ud835\udc59 \u2208 \ud835\udc3f B . As the data points and labels in XMC tasks belong to the same vocabulary universe (such as product recommendation), we find it beneficial to optimize \ud835\udc3f \ud835\udd07 ,\ud835\udc59 2 \ud835\udc5e alongside \ud835\udc3f \ud835\udd07 ,\ud835\udc5e 2 \ud835\udc59 , making \ud835\udc3f \ud835\udd07 a symmetric loss. Since [30], a plethora of works have leveraged symmetric optimizations in the vision-language retrieval pretraining domain. For XMC, the interchangability of \ud835\udc44 B and \ud835\udc3f B in the symmetric objective can be viewed equivalent to (i) feeding more data relations in a batch, and (ii) bridging missing relations in the dataset [21]. Further, we formulate XMC as a symmetric problem from \ud835\udc3f B to \ud835\udc44 B , thus calculating the multi-class loss between z \ud835\udc59 and x \ud835\udc56 \u2200 \ud835\udc56 \u2208 \ud835\udc44 B given by:  Note that, P \ud835\udc3f = { \ud835\udc56 | \ud835\udc56 \u2208 \ud835\udc44 B , \ud835\udc59 \u2208 \ud835\udc3f B , P \ud835\udc56,\ud835\udc59 = 1 } . The total DE contrastive loss can thus be written as :  For simplicity we use \ud835\udf06 \ud835\udd07 = 0 . 5 for all datasets, which works well in practice", "3.3 Unified Classifier Training with Pick-some-Labels": "XMC classifiers are typically trained on a shortlist consisting of all positive and O( \ud835\udc3f\ud835\udc5c\ud835\udc54 ( \ud835\udc3f )) hard negative labels [8]. As the reader can observe from Figure 1 and algorithm 1, the document and label embedding computation and batch pool is shared between \u03a6 \ud835\udd07 and \u03a6 \u212d . We simply unify the classifier training with that of DE by leveraging the same PSL reduction used for contrastive learning, with only minor changes: \u03a6 \u212d ( x \ud835\udc56 )| \ud835\udc56 \u2208 \ud835\udc44 B replaces \u03a6 \ud835\udd07 ( x \ud835\udc56 )| \ud835\udc56 \u2208 \ud835\udc44 B and, the label embeddings \u03a6 \ud835\udd07 ( z \ud835\udc59 )| \ud835\udc59 \u2208 \ud835\udc3f B are replaced by \u03a8 \ud835\udc59 | \ud835\udc59 \u2208 \ud835\udc3f B . The multi-class PSL loss for classifier \ud835\udc3f \u212d ,\ud835\udc5e 2 \ud835\udc59 can, therefore, be defined as:  Similar to DE training, we find it beneficial to employ a symmetric loss for classifier training, defined (with \ud835\udf06 \u212d = 0 . 5) as:  Finally, we combine the two losses and train together in an endto-end fashion, thereby achieving Unification of DE and classifier training for XMC.  As before, we fix \ud835\udf06 = 0 . 5 for all experiments.", "3.4 Inference": "For ANNS inference, the label graph can either be created over the encoded label embeddings { \u03a6 \ud835\udd07 ( \ud835\udc67 \ud835\udc59 )} \ud835\udc3f \ud835\udc59 = 1 or the label classifier embeddings { \ud835\udd11 ( \u03a8 ( \ud835\udc59 ))} \ud835\udc3f \ud835\udc59 = 1 , which are queried by { \u03a6 \ud835\udd07 ( x \ud835\udc56 )} \ud835\udc41 \ud835\udc56 = 1 or { \ud835\udd11 ( \u03a6 \u212d ( x \ud835\udc56 ))} \ud835\udc41 \ud835\udc56 = 1 respectively. Even though we train the classifiers over an un-normalized embedding space, we find it empirically beneficial to perform ANNS search over the unit normalized embedding UniDEC : Unified Dual Encoder and Classifier Training for Extreme Multi-Label Classification WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia Table 1: Experimental results showing the effectiveness of Depsl and UniDEC against both state-of-the-art dual encoder approaches and extreme classifiers. DE and classifier results are compared separately, with the best performing DE model underlined, and classifier model in bold. space [11, 22]. Interestingly, the concatenation of these two embeddings leads to a much more efficient retrieval. More specifically, we create the ANNS retrieval graph over the concatenated label representation { \u03a6 \ud835\udd07 ( \ud835\udc67 \ud835\udc59 ) \u2295 \ud835\udd11 ( \u03a8 ( \ud835\udc59 ))}| \ud835\udc3f \ud835\udc59 = 0 , which is queried by the concatenated document representations { \u03a6 \ud835\udd07 ( x \ud835\udc56 ) \u2295 \ud835\udd11 ( \u03a6 \u212d ( x \ud835\udc56 ))}| \ud835\udc41 \ud835\udc56 = 0 . Intuitively, this is a straight-forward way to ensemble the similarity scores from both the embedding spaces.", "4 Experiments": "Baselines & Evaluation Metrics: We compare against two classes of baselines, (i) DE Approaches ( \u03a6 ) consisting of only an encoder [7, 12, 17, 37] and, (ii) Classifier Based Approaches ( \u03a8 ) which use linear classifiers, with or without the encoder [7, 14]. A more comprehensive comparison with baselines has been provided in Appendix A. We use popular metrics such as Precision@K and Propensity-scored Precision@K (K \u2208 { 1 , 3 , 5 } ), defined in [4]. Datasets: Webenchmarkourexperiments on 6 standard datasets, comprising of both long-text inputs (LF-Amazon-131K, LF-WikiSeeAlso320K) and short-text inputs (LF-AmazonTitles-131K, LF-AmazonTitles1.3M, LF-WikiTitles-500K, LF-WikiSeeAlsoTitles-320K). We also evaluate baselines on a proprietary Query2Bid dataset, comprising of 450M labels, which is orders of magnitude larger than any public dataset. Details of these datasets can be found at [4] and in Table 7. Implementation Details and Ablation Study . We initialize both DePSL, our purely dual encoder method and UniDEC with a pre-trained 6l-Distilbert and train the \u03a6 , \ud835\udc54 (\u00b7) and \u03a8 with a learning rate of 1 \ud835\udc52 -4 , 2 \ud835\udc52 -4 and 1 \ud835\udc52 -3 respectively using cosine annealing with warm-up as the scheduler, hard-negative shortlist refreshed every \ud835\udf0f = 5 epochs. We make an effort to minimize the role of hyperparameters by keeping them almost same across all datasets.", "4.1 Evaluation on XMC Datasets": "In these settings, we evaluate DePSL and UniDEC against both DE and XMC baselines. UniDEC differs from these baselines in the following ways, (i) on training objective, UniDEC uses the proposed PSL relaxation of PAL for both DE and CLF training, instead of POL reduction used by existing methods like Ngame and Dexml, (ii) UniDEC does away with the need of modular training by unifying DE and CLF, (iii) finally, UniDEC framework adds explicitly mined hard negatives to the negative mining-aware batches which helps increase P@K metrics (see Table 4). UniDEC/DePSL vs Ngame( \u03a6 ): Table 1 depicts that UniDEC ( \u03a6 \u2295 \u03a8 ) consistently outperforms Ngame( \u03a8 ) (it's direct comparison baseline), where we see gains of 2 -8% in P@K and upto 10% on PSP@K. DePSL, on the other hand, outperforms Ngame on P@k with improvements ranging from 2 -9%. For PSP@k, DePSL ( \u03a6 ) always outperforms Ngame ( \u03a6 ) on long-text datasets, while the results are mixed on short-text datasets. DePSL vs DPR/ANCE: . Empirical performance of DPR demonstrates the limits of a DE model trained with InfoNCE loss and random in-batch negatives (popular in DR methods). Evidently, WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia Kharbanda and Gupta et al. Ance improves over DPR in the P@K metrics, which can be observed as the impact of explicitly mining hard-negative labels per instance instead of solely relying on the random in-batch negatives. Even though, these approaches use 12l-Bert-base instead of 6l-Distilbert common in XMC methods, Ance only shows marginal gains over Ngame on both datasets. Our proposed DE method, DePSL, despite using half the # Layers and half the search embedding dimension, is able to surpass these DR approaches by 15 -20% for P@K metrics over LF-AmazonTitles-1.3M dataset. Search Dimensionality . As mentioned before, DePSL outperforms Ngame on P@K metrics across benchmarks. Notably, DePSL does so by projecting (using \ud835\udc54 1 (\u00b7) ) and training the encoder embeddings in a low-dimension space of \ud835\udc51 = 384. Similarly, for UniDEC, inference is carried out by concatenating \ud835\udd11 ( \u03a6 \u212d ) and \u03a6 \ud835\udd07 embeddings. Here, both \ud835\udc54 1 (\u00b7) and \ud835\udc54 2 (\u00b7) consist of linear layers projecting \u03a6 (\u00b7) into a low-dimensional space of \ud835\udc51 = 256 or \ud835\udc51 = 384. On the other hand, all aforementioned baselines use a higher dimension of 768 for both DE and CLF evaluations. For the proprietary Query2Bid-450M dataset, we use final dimension of 64 for all the methods necessitated by constraints of online serving.", "4.2 Efficiency Comparison with Spectrum of XMCmethods": "In this section, we provide a comprehensive comparison (refer Table 2) of our proposed DePSL and UniDEC with two extreme ends of XMC spectrum (refer Figure 1): (i) Renee, which is initialized with pre-trained Ngame encoder, trains OvA classifiers with the BCE loss and, (ii) DEXML which achieves SOTA performance by training a DE using their proposed loss function decoupled softmax . Note that, these approaches do not pose a fair comparison with our proposed approaches as both Renee and Dexml do not use a label shortlist and backpropagate over the entire label space, requiring an order of magnitude higher GPU VRAM to run an iteration on LFAmazonTitles-1.3M. Therefore, for the same encoder, they can be considered as the upper bound of empirical performance of CLF (OvA) and DE methods respectively. Table 2 shows that similar, and perhaps better, performance is possible by using our proposed UniDEC and leveraging the proposed PSL reduction of multi-class losses over a label shortlist. Comparison with Renee : We observe that UniDEC delivers matching performance over P@K and PSP@K metrics on long-text datasets and significantly outperforms Renee on LF-AmazonTitles1.3M. In fact, our proposed DE method outperforms Renee on LF-Wikipedia-500K without even employing classifiers. We posit that UniDEC is therefore more effective for skewed datasets, with higher avg. points per label and more tail labels. Furthermore, these observations imply while Renee helps BCE loss reach it's empirical limits by scaling over the entire label space, with the UniDEC framework, we can match this limit with a shortlist that is 86 -212 \u00d7 smaller than the label space, thereby consuming significantly lower compute (1 \u00d7 A6000 vs 8 \u00d7 V100). DePSL vs Dexml (with shortlist): While DePSL leverages the proposed PSL reduction in the UniDEC framework, the latter uses the POL reduction with the same loss function. As evident in the LF-AmazonTitles-1.3M, Table 2, (i) For a comparable label pool Table 2: Experimental results showing the effectiveness of DePSL and UniDEC against the two ends of XMC spectrum. | \ud835\udc44 B | denotes batch size, | \ud835\udc3f B | denotes label pool size and TT denotes Training Time(in hrs). Note, these comparisons are not fair owing to the significant gap in used resources. size (4000 vs 8192), DePSL significantly outperforms DEXML by \u223c 20% in P@K metrics. (ii) To achieve similar performance as DePSL, DEXML need to use an effective label pool size of 90K. However in the same setting, DePSL needs only 1 / 4 \ud835\udc61\u210e batch size and 1 / 22 \ud835\udc61\u210e label pool size. A similar trend is seen in LF-Wikipedia-500K. These observations empirically demonstrate the informativeness of the batches in UniDEC - the same information can be captured by it with significantly smaller batch sizes. UniDEC vs DEXML-Full: UniDEC, scales to LF-AmazonTitles1.3M on a single A6000 GPU using a label shortlist of only 3000 labels, as opposed to DEXML-Full which requires 16 A100s and uses the entire label space of 1.3M. Despite this, Table 2 indicates that UniDEC matches DEXML-Full on P@5 and PSP@5 metrics.", "4.3 Ablation Study": "Evaluation with Multiple Loss Functions : As mentioned previously, any loss function can be chosen in the UniDEC framework, however, we experiment with two multi-class losses in particular, namely SupCon loss (SC) [22] and Decoupled Softmax (DS) [12]. Replacing \u2113 \ud835\udc40\ud835\udc36 with these gives   Notably, from Table 3 and Table 5, we observe that Decoupled Softmax turns out to be a better loss for XMC tasks as it helps the logits scale better [12] as compared to SupCon which caps the gradient due to a hard requirement of producing a probability UniDEC : Unified Dual Encoder and Classifier Training for Extreme Multi-Label Classification WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia distribution. We further observe that classifier performance can further improve by adding BCE loss as an auxiliary OvA loss to the classifier loss. While this helps enhance P@K metrics, the PSP@K metrics take a significant dip on the inclusion of auxiliary BCE loss. These observations are in line with the performance of Renee which leverages BCE loss and suffers on PSP@K metrics. Simply using BCE loss for classifier works in our pipeline, however, ends up performing worse than using multi-class loss to train the classifiers. UniDEC Framework : We show the effect of the two individual components \u03a6 \ud835\udd07 and \u03a6 \u212d of UniDEC in Table 4. The scores are representative of the evaluation of the respective component of the UniDEC framework, (i) UniDEC-de ( \u03a6 \ud835\udd07 ) performs inference with an ANNS built over \u03a6 \ud835\udd07 ( z \ud835\udc59 )| \ud835\udc3f \ud835\udc59 = 0 , (ii) UniDEC-clf ( \u03a6 \u212d ) performs inference with an ANNS built over \ud835\udd11 ( \u03a8 ( \ud835\udc59 ))| \ud835\udc3f \ud835\udc59 = 0 and (iii) UniDEC uses the ANNS built over the concatenation of both { \u03a6 \ud835\udd07 ( z \ud835\udc59 ) + \ud835\udd11 ( \u03a8 ( \ud835\udc59 ))}| \ud835\udc3f \ud835\udc59 = 0 . Notably, concatenation of embeddings leads to a more effective retrieval. We attribute its performance to two aspects, (i) as seen in previous XMC models, independent classifier weights significantly improve the discriminative capabilities of these models and (ii) we hypothesise that normalized and unnormalized spaces learn complementary information which leads to enhanced performance when an ANNS is created on their aggregation. Note that, the individual search dimensions of UniDEC-de : and UniDEC-clf are \ud835\udc51 / 2 and searching with a concatenated embedding leads to a fair comparison with other baselines which use a dimensionality of \ud835\udc51 . Note that for all experiments, \ud835\udc54 1 (\u00b7) , \ud835\udc54 2 (\u00b7) is defined as follows,  )) Effect of ANNS-mind Hard Negatives : The effect of explicitly adding ANNS-mined hard negatives is shown via a vis-a-vis comparison with UniDEC (w/o Hard Negatives) in Table 4. Here, when we do not add hard negatives, we compensate by adding other positives of the batched queries. More broadly, we observe a P vs PSP trade-off in this ablation. We find that not including hard negatives in the shortlist performs better on PSP@K metrics, due to inclusion of more positive labels. Consequently, adding (typically \ud835\udf02 = 6) hard negatives generally increases performance on P@K metrics, while compromising on PSP@K metrics. While the smaller datasets show only marginal improvements with added hard negatives, these effects are more pronounced in the larger datasets, proving its necessity in the pipeline. Effect of Symmetric Loss . As shown in Table 5, making the loss function symmetric has a favorable effect on all metrics for long-text datasets. However, this makes the short-text datasets favor PSP@K, at an expense of P@K. We believe this happens because of mixing data distributions. While adding a short-text loss over longtext document helps the model understand the label distribution better, this has a reverse effect on short-text datasets. Table 3: Experimental results showing the effect of different loss functions while training UniDEC. Further, the table also shows the scores of inference done using only the DE head \u03a6 \ud835\udd07 ( x ) or the normalized CLF head \ud835\udd11 ( \u03a6 \u212d ( x )) , instead of the concatenated vector. Table 4: Experimental results showing the effect of adding ANNS-mined hard negatives while training UniDEC. The table also shows the scores of inference done using either the DE head embedding \u03a6 \ud835\udd07 ( x ) or the normalized CLF head embedding \ud835\udd11 ( \u03a6 \u212d ( x )) , instead of the concatenated vector { \u03a6 \ud835\udd07 ( x ) \u2295 \ud835\udd11 ( \u03a6 \u212d ( x ))} . The P vs PSP trade-off associated with adding ANNS-mined hard-negatives can be observed by comparing the performance with and without hard negatives.", "5 Offline Evaluation and Live A/B testing on Sponsored Search": "To demonstrate the effectiveness of our method on proprietary datasets and real-world scenarios, we do experiments in sponsored WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia Kharbanda and Gupta et al. Table 5: Experimental results showing the effect of adding dual loss while training our DePSL. search setting. The proposed model was evaluated on proprietary dataset for matching queries to advertiser bid phrases (Query2Bid) consisting of 450M labels. Query2Bid-450M dataset was created by mining the logs from search engine and enhancing it through Data Augmentation techniques using a ensemble of leading (proprietary) algorithms such as Information Retrieval models (IR), Dense retrieval models (DR), Generative Non-Autoregressive models (NAR), Extreme-Multi-label Classification models (XMC) and even GPT Inference techniques. Experimental Setup : The BERT Encoder is initialized with 6Layer DistilBERT base architecture. Since the search queries and bid phrases are of short-text in nature, a max-sequence-length of 12 is used. We evaluate DePSL against XMC and DR models deployed in production which could scale to the magnitude of chosen dataset. Training batch-size is set to 2048 and other Hyperparameters are chosen to be same as for public benchmark datasets. Training is carried out on 8 V100 GPUs and could easily complete within 48 hours . Performance is measured using popular metrics such as Precision@K (P@K) with \ud835\udc3e \u2208 1 , 3 , 5 , 10.", "Table 6: Results on sponsored search dataset Query2Bid-450M": "Offline Results : Table 6 shows that on DePSL can be 1.15-1.83% more accurate than the leading DR & XMC methods in Sponsored Search setting. This indicates that leveraging DePSL can yield superior gains in real-world search applications. Live A/B Testing in a Search Engine: DePSL was deployed on Live Search Engine and A/B tests were performed on real-world traffic. The effect of adding DePSL to the ensemble of existing models in the system was measured through popular metrics such as Impression Yield (IY), Click Yield (CY), Click-Through Rate (CTR) and Query Coverage (QC). Refer [7] for definitions and details about these metrics. DePSL was observed to improve IY, CY, CTR and QC by 0.87%, 0.66%, 0.21% and 1.11% respectively. Gains in IY, CY and CTR establish that DePSL is able to predict previously unmatched relations and the predictions are more relevant to the end user. QC boost indicates that DePSL is able to serve matches for queries to which there were no matches before in the system. This ascertains the zero-shot capabilities of the model.", "6 Other Related Works": "To reduce computational costs of training classifiers, previous XMC methods tend to make use of various shortlisting strategies, which serves as a good approximation to the loss over the entire label space [5, 8, 38]. This shortlist can be created in one of the two ways : (i) by training a meta classifier on coarser levels of a hierarchically-split probabilistic label tree. The leaf nodes of the top-k nodes constitute the shortlist [16, 19, 20] (ii) by retrieving the top-k labels for a query from an ANNS built on the label representations from a contrastively trained DE [6]. Both these methods have different trade-offs. The meta-classifier based approach has a higher memory footprint due to the presence of additional meta classifier ( \u223c R \ud835\udc3f / 10 \u00d7 \ud835\udc51 in size) along with the extreme classifier, but it gives enhanced performance since this provides progressively harder negatives in a dynamic shortlist, varying every epoch [16, 19, 20]. The shortlisting based on ANNS requires training the model in multiple stages, which has low memory usage, but needs longer training schedules and uses a static shortlist for training extreme classifiers [7, 8, 24, 25]. Previous research has also explored various other methods : (i) label trees [15, 18, 26, 35], (ii) classifiers based on hierarchical label trees [5, 27, 39]. Tangentially, various initialisation methods [10, 32] and data augmentation approaches [21] have also been studied. Alongside previous negative-mining works, the statistical consequences of this sampling [31] and missing labels [13, 28, 33, 34, 36] have led to novel insights in designing unbiased loss functions - which can also be applied in UniDEC.", "7 Conclusion": "In this paper, we present a new loss-independent end-to-end XMC framework, UniDEC, that aims to leverage the best of both, a dual encoder and a classifier in a compute-efficient manner. The dualencoder is used to mine hard negatives, which are in turn used as the shortlist for the classifier, eliminating the need for meta classifiers. Highly informative in-batch labels are created which maximise the supervisory signals while keeping the GPU memory footprint as low as possible - to the extent that we outperform previous SOTAs with just a single GPU. The dual encoders and classifiers are unified and trained with the same multi-class loss function, which follows the proposed pick-some-labels paradigm. To the best of our knowledge, we are the first work to study the effect of PAL-like losses for training XMC classifiers. We hope this inspires future works to study the proposed PSL reduction for multilabel problems as a compute-efficient means to further eliminate the need of high-capacity classifiers in XMC, bringing the scope of this problem closer to the more general dense retrieval regime.", "8 Acknowledgements": "RB acknowledges the support of Academy of Finland (Research Council of Finland) via grants 347707 and 348215. DG acknowledges the support of computational resources provided by the Aalto Science-IT project, and CSC IT Center for Science, Finland. UniDEC : Unified Dual Encoder and Classifier Training for Extreme Multi-Label Classification WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia", "References": "[1] L. A. Adamic and B. A. Huberman. Zipf's law and the internet. Glottometrics , 3(1):143-150, 2002. [2] R. Babbar and B. Sch\u00f6lkopf. DiSMEC: Distributed Sparse Machines for Extreme Multi-label Classification. In WSDM , 2017. [3] R. Babbar and B. Sch\u00f6lkopf. Data scarcity, robustness and extreme multi-label classification. Machine Learning , 108(8):1329-1351, 2019. [4] K. Bhatia, K. Dahiya, H. Jain, A. Mittal, Y. Prabhu, and M. Varma. The extreme classification repository: Multi-label datasets and code, 2016. [5] W.-C. Chang, H.-F. Yu, K. Zhong, Y. Yang, and I. Dhillon. Taming Pretrained Transformers for Extreme Multi-label Text Classification. In KDD , 2020. [6] K. Dahiya, A. Agarwal, D. Saini, K. Gururaj, J. Jiao, A. Singh, S. Agarwal, P. Kar, and M. Varma. Siamesexml: Siamese networks meet extreme classifiers with 100m labels. In International Conference on Machine Learning , pages 2330-2340. PMLR, 2021. [7] K. Dahiya, N. Gupta, D. Saini, A. Soni, Y. Wang, K. Dave, J. Jiao, G. K, P. Dey, A. Singh, et al. Ngame: Negative mining-aware mini-batching for extreme classification. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining , pages 258-266, 2023. [8] K. Dahiya, D. Saini, A. Mittal, A. Shaw, K. Dave, A. Soni, H. Jain, S. Agarwal, and M. Varma. Deepxml: A deep extreme multi-label learning framework applied to short text documents. In Conference on Web Search and Data Mining (WSDM'21) , 2021. [9] K. Dahiya, S. Yadav, S. Sondhi, D. Saini, S. Mehta, J. Jiao, S. Agarwal, P. Kar, and M. Varma. Deep encoders with auxiliary parameters for extreme classification. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining , pages 358-367, 2023. [10] H. Fang, M. Cheng, C.-J. Hsieh, and M. Friedlander. Fast training for large-scale one-versus-all linear classifiers using tree-structured initialization. In Proceedings of the 2019 SIAM International Conference on Data Mining , pages 280-288. SIAM, 2019. [11] B. Gunel, J. Du, A. Conneau, and V. Stoyanov. Supervised contrastive learning for pre-trained language model fine-tuning, 2021. [12] N. Gupta, F. Devvrit, A. S. Rawat, S. Bhojanapalli, P. Jain, and I. S. Dhillon. Dualencoders for extreme multi-label classification. In The Twelfth International Conference on Learning Representations , 2024. [13] H. Jain, Y. Prabhu, and M. Varma. Extreme multi-label loss functions for recommendation, tagging, ranking & other missing label applications. In KDD , pages 935-944, 2016. [14] V. Jain, J. Prakash, D. Saini, J. Jiao, R. Ramjee, and M. Varma. Renee: End-to-end training of extreme classification models. Proceedings of Machine Learning and Systems , 2023. [15] K. Jasinska, K. Dembczynski, R. Busa-Fekete, K. Pfannschmidt, T. Klerx, and E. Hullermeier. Extreme F-measure Maximization using Sparse Probability Estimates. In ICML , June 2016. [16] T. Jiang, D. Wang, L. Sun, H. Yang, Z. Zhao, and F. Zhuang. Lightxml: Transformer with dynamic negative sampling for high-performance extreme multi-label text classification. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 35, pages 7987-7994, 2021. [17] V. Karpukhin, B. Oguz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen, and W.-t. Yih. Dense passage retrieval for open-domain question answering. In B. Webber, T. Cohn, Y. He, and Y. Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 6769-6781, Online, Nov. 2020. Association for Computational Linguistics. [18] S. Khandagale, H. Xiao, and R. Babbar. Bonsai: diverse and shallow trees for extreme multi-label classification. Machine Learning , 109(11):2099-2119, 2020. [19] S. Kharbanda, A. Banerjee, D. Gupta, A. Palrecha, and R. Babbar. Inceptionxml: A lightweight framework with synchronized negative sampling for short text extreme classification. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 760-769, 2023. [20] S. Kharbanda, A. Banerjee, E. Schultheis, and R. Babbar. Cascadexml: Rethinking transformers for end-to-end multi-resolution training in extreme multi-label classification. Advances in Neural Information Processing Systems , 35:2074-2087, 2022. [21] S. Kharbanda, D. Gupta, E. Schultheis, A. Banerjee, V. Verma, and R. Babbar. Gandalf : Data augmentation is all you need for extreme classification, 2023. [22] P. Khosla, P. Teterwak, C. Wang, A. Sarna, Y. Tian, P. Isola, A. Maschinot, C. Liu, and D. Krishnan. Supervised contrastive learning. Advances in neural information processing systems , 33:18661-18673, 2020. [23] A. K. Menon, A. S. Rawat, S. Reddi, and S. Kumar. Multilabel reductions: what is my loss optimising? Advances in Neural Information Processing Systems , 32, 2019. [24] A. Mittal, K. Dahiya, S. Agrawal, D. Saini, S. Agarwal, P. Kar, and M. Varma. Decaf: Deep extreme classification with label features. In Proceedings of the 14th ACM International Conference on Web Search and Data Mining , pages 49-57, 2021. [25] A. Mittal, N. Sachdeva, S. Agrawal, S. Agarwal, P. Kar, and M. Varma. Eclare: Extreme classification with label graph correlations. In Proceedings of the Web Conference 2021 , pages 3721-3732, 2021. [26] Y. Prabhu, A. Kag, S. Gopinath, K. Dahiya, S. Harsola, R. Agrawal, and M. Varma. Extreme multi-label learning with label features for warm-start tagging, ranking and recommendation. In WSDM , 2018. [27] Y. Prabhu, A. Kag, S. Harsola, R. Agrawal, and M. Varma. Parabel: Partitioned label trees for extreme classification with application to dynamic search advertising. In Proceedings of the 2018 World Wide Web Conference , pages 993-1002, 2018. [28] M. Qaraei, E. Schultheis, P. Gupta, and R. Babbar. Convex surrogates for unbiased loss functions in extreme classification with missing labels. In Proceedings of the Web Conference 2021 , pages 3711-3720, 2021. [29] Y. Qu, Y. Ding, J. Liu, K. Liu, R. Ren, W. X. Zhao, D. Dong, H. Wu, and H. Wang. Rocketqa: An optimized training approach to dense passage retrieval for opendomain question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 5835-5847, 2021. [30] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning , 2021. [31] S. J. Reddi, S. Kale, F. Yu, D. N. H. Rice, J. Chen, and S. Kumar. Stochastic Negative Mining for Learning with Large Output Spaces. CoRR , 2018. [32] E. Schultheis and R. Babbar. Speeding-up one-vs-all training for extreme classification via smart initialization. arXiv preprint arXiv:2109.13122 , 2021. [33] E. Schultheis and R. Babbar. Unbiased loss functions for multilabel classification with missing labels. arXiv preprint arXiv:2109.11282 , 2021. [34] E. Schultheis, M. Wydmuch, R. Babbar, and K. Dembczynski. On missing labels, long-tails and propensities in extreme multi-label classification. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining , pages 1547-1557, 2022. [35] M. Wydmuch, K. Jasinska, M. Kuznetsov, R. Busa-Fekete, and K. Dembczynski. A no-regret generalization of hierarchical softmax to extreme multi-label classification. In NIPS , 2018. [36] M. Wydmuch, K. Jasinska-Kobus, R. Babbar, and K. Dembczynski. Propensityscored probabilistic label trees. In SIGIR , pages 2252-2256, 2021. [37] L. Xiong, C. Xiong, Y. Li, K.-F. Tang, J. Liu, P. Bennett, J. Ahmed, and A. Overwijk. Approximate nearest neighbor negative contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808 , 2020. [38] R. You, Z. Zhang, Z. Wang, S. Dai, H. Mamitsuka, and S. Zhu. Attentionxml: Label tree-based attention-aware deep model for high-performance extreme multi-label text classification. In NeurIPS , 2019. [39] J. Zhang, W.-C. Chang, H.-F. Yu, and I. Dhillon. Fast multi-resolution transformer fine-tuning for extreme multi-label text classification. Advances in Neural Information Processing Systems , 34:7267-7280, 2021. WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia Kharbanda and Gupta et al.", "A Appendix": "", "A.1 Dataset details": "Table 7: Details of the benchmarks with label features. APpL stands for avg. points per label, ALpP stands for avg. labels per point and AWpP is the length i.e. avg. words per point. Comparison with MS-MARCO. MS-MARCO, a dataset for document dense retrieval, averages only 1.1 relevant documents per query across its 3.2M documents [29]. In contrast, LF-AmazonTitles1.3M, a product recommendation dataset, has 1.3M products where each product title is related to about 22 other titles, and each title appears as a relation for about 38 other products. This shows how XMC tasks involve many more connections compared to the single-answer nature of open-domain question answering.", "A.2 Algorithm": "", "Algorithm 1 Training step in UniDEC": "Input: instance x , label features z , positive labels P , encoder \u03a6 , classifier lookup-table \u03a8 , non-linear transformations \ud835\udc54 1 (\u00b7) and \ud835\udc54 2 (\u00b7) \u03a6 \ud835\udd07 (\u00b7) , \u03a6 \u212d (\u00b7) \u2254\ud835\udd11 ( \ud835\udc54 1 ( \u03a6 (\u00b7) ) ) , \ud835\udc54 2 ( \u03a6 (\u00b7) ) for 1 ..\ud835\udf16 do e in if \ud835\udc52 % \ud835\udf0f is 0 then \ud835\udd05 \u2190 Cluster ( \u03a6 \ud835\udd07 ( x \ud835\udc56 ) | \ud835\udc41 \ud835\udc56 = 0 ) H \u2190 topk ( ANNS ( \u03a6 \ud835\udd07 ( x \ud835\udc56 ) | \ud835\udc41 \ud835\udc56 = 0 , \u03a6 \ud835\udd07 ( z \ud835\udc59 ) | \ud835\udc3f \ud835\udc59 = 0 ) ) for \ud835\udc44 B \ud835\udc56\ud835\udc5b \ud835\udd05 do for \ud835\udc56 \ud835\udc56\ud835\udc5b \ud835\udc44 B do \u02c6 P \ud835\udc56 \u2190 sample ( P \ud835\udc56 , \ud835\udefd ) \u02c6 H \ud835\udc56 \u2190 sample ( H \ud835\udc56 - P \ud835\udc56 , \ud835\udf02 ) \ud835\udc3f B \u2190 { - \ud835\udc56 \u2208 \ud835\udc44 B \u02c6 P \ud835\udc56 \u222a \u02c6 H \ud835\udc56 } P B \u2190 {{P \ud835\udc56 \u2229 \ud835\udc3f B } | \ud835\udc56 \u2208 \ud835\udc44 B } P \ud835\udc3f \u2190 {{ \ud835\udc56 | \ud835\udc56 \u2208 \ud835\udc44 B , P \ud835\udc56,\ud835\udc59 = 1 }| \ud835\udc59 \u2208 \ud835\udc3f B } L \ud835\udd07 ,\ud835\udc5e 2 \ud835\udc59 \u2190 L \ud835\udc43\ud835\udc46\ud835\udc3f ( \u03a6 \ud835\udd07 ( x \ud835\udc56 ) , \u03a6 \ud835\udd07 ( z \ud835\udc59 ) | B , P B ) L \ud835\udd07 ,\ud835\udc59 2 \ud835\udc5e \u2190 L \ud835\udc43\ud835\udc46\ud835\udc3f ( \u03a6 \ud835\udd07 ( z \ud835\udc59 ) , \u03a6 \ud835\udd07 ( x \ud835\udc56 ) | B , P \ud835\udc3f ) L \ud835\udd07 \u2190 \ud835\udf06 \ud835\udd07 \u00b7 L \ud835\udd07 ,\ud835\udc5e 2 \ud835\udc59 + ( 1 - \ud835\udf06 \ud835\udd07 ) \u00b7 L \ud835\udd07 ,\ud835\udc59 2 \ud835\udc5e L \u212d ,\ud835\udc5e 2 \ud835\udc59 \u2190 L \ud835\udc43\ud835\udc46\ud835\udc3f ( \u03a6 \u212d ( x \ud835\udc56 ) , \u03a8 ( \ud835\udc59 ) | B , P B ) L \u212d ,\ud835\udc59 2 \ud835\udc5e \u2190 L \ud835\udc43\ud835\udc46\ud835\udc3f ( \u03a8 ( \ud835\udc59 ) , \u03a6 \u212d ( x \ud835\udc56 ) | B , P \ud835\udc3f ) L \u212d \u2190 \ud835\udf06 \u212d \u00b7 L \u212d ,\ud835\udc5e 2 \ud835\udc59 + ( 1 - \ud835\udf06 \u212d ) \u00b7 L \u212d ,\ud835\udc59 2 \ud835\udc5e L \u2190 \ud835\udf06 \u00b7 L \ud835\udd07 + ( 1 - \ud835\udf06 ) \u00b7 L \u212d adjust \u03a6 , \ud835\udc54 1 (\u00b7) , \ud835\udc54 2 (\u00b7) and \u03a8 to reduce loss L .", "A.3 Additional Results": "Table 8: Results on Short and Long Text Datasets"}
