{"Pacer and Runner: Cooperative Learning Framework between Single- and Cross-Domain Sequential Recommendation": "Chung Park cpark88kr@gmail.com SK Telecom, KAIST Seoul, Republic of Korea Taesan Kim ktmountain@sk.com SK Telecom Seoul, Republic of Korea Hyungjun Yoon hjyoon@sk.com SK Telecom Seoul, Republic of Korea Junui Hong skt.juhong@sk.com SK Telecom, KAIST Seoul, Republic of Korea Yelim Yu yelim.yu@sk.com SK Telecom Seoul, Republic of Korea", "Mincheol Cho": "skt.mccho@sk.com SK Telecom Seoul, Republic of Korea", "ABSTRACT": "Cross-Domain Sequential Recommendation (CDSR) improves recommendation performance by utilizing information from multiple domains, which contrasts with Single-Domain Sequential Recommendation (SDSR) that relies on a historical interaction within a specific domain. However, CDSR may underperform compared to the SDSR approach in certain domains due to negative transfer, which occurs when there is a lack of relation between domains or different levels of data sparsity. To address the issue of negative transfer, our proposed CDSR model estimates the degree of negative transfer of each domain and adaptively assigns it as a weight factor to the prediction loss, to control gradient flows through domains with significant negative transfer. To this end, our model compares the performance of a model trained on multiple domains (CDSR) with a model trained solely on the specific domain (SDSR) to evaluate the negative transfer of each domain using our asymmetric cooperative network. In addition, to facilitate the transfer of valuable cues between the SDSR and CDSR tasks, we developed an auxiliary loss that maximizes the mutual information between the representation pairs from both tasks on a per-domain basis. This cooperative learning between SDSR and CDSR tasks is similar to the collaborative dynamics between pacers and runners in a marathon. Our model outperformed numerous previous works in extensive experiments on two real-world industrial datasets across ten service domains. We also have deployed our model in the recommendation system of our personal assistant app service, resulting in 21.4% increase in click-through rate compared to existing models, which is valuable to real-world business 1 . \u2217 Corresponding Author (jchoo@kaist.ac.kr) 1 The implementation code of our model is available on https://github.com/cpark88/ SyNCRec. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '24, July 14-18, 2024, Washington, DC, USA \u00a9 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0431-4/24/07 https://doi.org/10.1145/3626772.3657710 Minsung Choi ms.choi@sk.com SK Telecom Seoul, Republic of Korea", "Jaegul Choo \u2217": "jchoo@kaist.ac.kr Korea Advanced Institute of Science and Technology Daejeon, Republic of Korea", "CCS CONCEPTS": "\u00b7 Information systems \u2192 Recommender systems .", "KEYWORDS": "Cross-Domain Sequential Recommendation; Asymmetric Cooperative Network; Negative Transfer; Multi-Gate Mixture-of-Experts", "ACMReference Format:": "Chung Park, Taesan Kim, Hyungjun Yoon, Junui Hong, Yelim Yu, Mincheol Cho, Minsung Choi, and Jaegul Choo. 2024. Pacer and Runner: Cooperative Learning Framework between Single- and Cross-Domain Sequential Recommendation. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '24), July 14-18, 2024, Washington, DC, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3626772.3657710", "1 INTRODUCTION": "In most real-world recommender systems, various business domains require understanding of the different interests and needs of users. A practical approach to address this involves Single Domain Sequential Recommendation (SDSR), which focuses on recommending the next item within a specific domain using only the single-domain sequence and builds many domain-specific models [9, 25, 32, 36]. Another solution is Cross-Domain Sequential Recommendation (CDSR): it predicts the next item a user will interact with by leveraging their historical interaction sequences across multiple domains [3, 11, 20]. In particular, CDSR has proven to be a promising approach to improve the performance of sequential recommendations across multiple domains simultaneously with one model [11, 13, 16, 20]. Note that the difference between CDSR and SDSR tasks for each domain lies in whether or not they use information from other domains in addition to the specific domain. However, in certain domains, CDSR significantly underperforms relative to the SDSR approach, while in other domains it may outperform the SDSR approach. This is because transferring knowledge from other weakly related domains or domains with different levels of data sparsity can have a negative impact on a particular domain: this is known as negative transfer [20]. To verify this experimentally, we measured the difference in the model performance between the CDSR and the SDSR approaches on a per-domain basis. As illustrated in Fig. 1, the recommendation performance of the \ud835\udc35\ud835\udc5c\ud835\udc5c\ud835\udc58 and \ud835\udc36\ud835\udc59\ud835\udc5c\ud835\udc61\u210e\ud835\udc56\ud835\udc5b\ud835\udc54 domains in the Amazon dataset, as well as the \ud835\udc36\ud835\udc4e\ud835\udc59\ud835\udc59 domain SIGIR '24, July 14-18, 2024, Washington, DC, USA Park Chung, et al. Figure 1: In the Book , Clothing (Amazon dataset), and Call (Telco dataset) domains, the SDSR (trained with a single domain) outperformed the CDSR approach (trained with multiple domains), which indicates negative transfer from other domains. We used CGRec [20] for the experiments. -5.84 -0.56 +2.56 +1.71 +14.46 +4.02 -2.49 +24.93 +11.07 +9.56 in the Telco dataset, was worse when trained with other domains (i.e., CDSR) than when trained separately (i.e., SDSR). This result is particularly reasonable for the \ud835\udc35\ud835\udc5c\ud835\udc5c\ud835\udc58 domain, considering the weak relationship between the \ud835\udc35\ud835\udc5c\ud835\udc5c\ud835\udc58 domain and other domains such as \ud835\udc46\ud835\udc5d\ud835\udc5c\ud835\udc5f\ud835\udc61\ud835\udc60 or \ud835\udc47\ud835\udc5c\ud835\udc66\ud835\udc60 . These experiments demonstrate that CDSR does not always outperform the SDSR approach across all domains. In light of this, our primary goal is to improve the performance of the domains with significant negative transfer to ensure consistent high performance across multiple domains in the CDSR approach. For this purpose, we propose the CDSR model SyNCRec , which stands for a Asymmetric Cooperative Network for Cross-Domain Sequential Recommendation. In SyNCRec, we assess the degree of negative transfer of each domain and adaptively assign this value as weight to the prediction loss corresponding to a specific domain. This adaptive control of the gradient reduces its flow in domains with significant negative transfer, consequently enhancing the performance of domains adversely affected by other unrelated domains [20]. To this end, our model estimates the negative transfer of a particular domain by comparing the performance of a model trained on domain-hybrid sequences (CDSR task) to the model trained solely on specific domain sequences (SDSR task) in our asymmetric cooperative network. In the asymmetric cooperative network, the learning process of both tasks is partially decoupled to allow for proper calculation of their pure loss values without mutual interference, contributing to accurate computation of negative transfer. In addition, to enhance the transfer of valuable clues across SDSR and CDSR tasks, we developed an auxiliary loss that maximizes the mutual information between the representation pairs from both tasks on a per-domain basis. From this objective, we exploit the effective correlation signals inherent in the representation pairs of SDSR and CDSR tasks within a specific domain. Our experimental results confirm that this auxiliary loss improves the positive transfer of information between the representations derived from the SDSR and CDSR tasks. SDSR serves as a pacer to reduce negative transfer during the learning process of CDSR, similar to how pacers in marathons assist runners in avoiding a pace that is too fast or too slow. As a result, SyNCRec improves the performance of the CDSR task compared to 25 state-of-the-art models, with two real-world datasets across ten domains. Importantly, our model significantly improves the performance of domains that underperformed compared to the SDSR approach due to negative transfer. In addition, extensive online experiments were conducted on large-scale recommender using industrial datasets to confirm its proficiency in real-world applications. Note that our model can handle both SDSR and CDSR tasks simultaneously, allowing for the learning of multiple domains at once. This eliminates the need to build many domain-specific models, making it highly relevant for real-world scenarios.", "2 RELATED WORK": "", "2.1 Single-Domain Sequential Recommendation": "The SDSR framework is designed to model the temporal dynamics in user-item interactions, effectively capturing how user preferences evolve over time within a specific domain. For example, GRU4Rec [8], STAMP [15], and NARM [12] were introduced to predict the user's next item using GRU-based models. Moreover, in SASRec [9], BERT4Rec [25], SINE [27], and LightSANs [4], attention mechanisms are utilized to encapsulate sequential patterns, thereby addressing both short- and long-term dependencies. In addition, other machine learning architectures such as a convolutional network (e.g., NextItNet [31]) or Markov Chain (e.g., TransRec [6]) are adopted in the SDSR task. Meanwhile, FDSA [32], S 3 Rec [36], and CARCA [22] utilize item features (e.g., text description) in addition to item sequences to demonstrate their efficiency. Nonetheless, studies on the SDSR framework continue to encounter issues of negative transfer in CDSR task.", "2.2 Cross-Domain Sequential Recommendation": "The goal of Cross-Domain Recommendation (CDR) is to improve recommendations in a given domain by leveraging information from various other domains. For example, CMF [24] and CLFM [5] modeled the matrix factorization of user-item interactions across multiple domains, capturing their shared patterns. Other approaches, DTCDR [37], DeepAPF [30], and BiTGCF [14], modeled user-item relationships in paired domains by a multi-task learning method. However, these methods modeled pairwise relationships between domains for the CDR task, a strategy that may be less effective in scenarios involving a large number of domains. To fill this research gap, CAT-ART [11] simultaneously modeled user and item representations in five domains using a matrix factorization approach. However, these studies have not taken into account the sequential nature of user interaction sequence. Meanwhile, the CDSR aims to enhance the task of sequential recommendation, focusing on items that span multiple domains. \ud835\udf0b -Net [17] introduced gating mechanisms designed to transfer information from a single domain to another paired domain. Meanwhile, C 2 DSR [3] employed a self-attention based encoder and graph neural network to model both single- and cross-domain representations. MIFN [16] introduced the concept of mixed information flow, which reflects the knowledge flows between multiple domains. MAN [13] designed group-prototype attention mechanisms to capture domainspecific and cross-domain relationships. However, these studies only modeled relationships between pairs of domains. In scenarios involving more than three domains, this approach would require handling an impractically large number of domain pairs, potentially limiting its feasibility in real-world applications [11]. To address this limitation, CGRec [20] proposed a CDSR model that deals with more than three domains at once. They approximated the negative transfer for each domain using Shapley values, and applied them to penalize items in domains with high negative transfer. However, even in CGRec, we encountered problems with negative transfer, SyNCRec SIGIR '24, July 14-18, 2024, Washington, DC, USA Figure 2: We illustrate SyNCRec using three domains ( \ud835\udc34 , \ud835\udc35 , \ud835\udc36 ), each represented by a distinct color (blue, yellow, and pink). The notations \u2295 , \u2296 and \u2297 indicate element-wise summation, subtraction, and multiplication, respectively. For the SDSR task for each domain, we input \ud835\udc4b \ud835\udc34 = [( SOS ) , \ud835\udc65 \ud835\udc34 1 , \ud835\udc65 \ud835\udc34 3 , \ud835\udc65 \ud835\udc34 5 ] , \ud835\udc4b \ud835\udc35 = [( SOS ) , \ud835\udc65 \ud835\udc35 2 , \ud835\udc65 \ud835\udc35 4 ] , and \ud835\udc4b \ud835\udc36 = [( SOS ) , \ud835\udc65 \ud835\udc36 6 , \ud835\udc65 \ud835\udc36 7 ] , and predict their shifted sequences [ \ud835\udc65 \ud835\udc34 1 , \ud835\udc65 \ud835\udc34 3 , \ud835\udc65 \ud835\udc34 5 , ( PAD )] , [ \ud835\udc65 \ud835\udc35 2 , \ud835\udc65 \ud835\udc35 4 , \ud835\udc65 \ud835\udc35 8 ] , and [ \ud835\udc65 \ud835\udc36 6 , \ud835\udc65 \ud835\udc36 7 , ( PAD )] . For the CDSR task, we input \ud835\udc4b = [( SOS ) , \ud835\udc65 \ud835\udc34 1 , \ud835\udc65 \ud835\udc35 2 , \ud835\udc65 \ud835\udc34 3 , \ud835\udc65 \ud835\udc35 4 , \ud835\udc65 \ud835\udc34 5 , \ud835\udc65 \ud835\udc36 6 , \ud835\udc65 \ud835\udc36 7 ] into the model, and predict the shifted sequence [ \ud835\udc65 \ud835\udc34 1 , \ud835\udc65 \ud835\udc35 2 , \ud835\udc65 \ud835\udc34 3 , \ud835\udc65 \ud835\udc35 4 , \ud835\udc65 \ud835\udc34 5 , \ud835\udc65 \ud835\udc36 6 , \ud835\udc65 \ud835\udc36 7 , \ud835\udc65 \ud835\udc35 8 ] . We do not consider losses for ( PAD ) tokens. Single-Domain Sequences Cross-Domain Sequence Expert 1 \u2026 \ud835\udc88 ! \ud835\udc88 \" \ud835\udc88 # \ud835\udc88 Tower \u210e ! Tower \u210e \" Tower \u210e # Tower \u210e $%&'' + \u00d7 \u00d7 \u00d7 Loss Correction with Negative Transfer Gap (LC-NTG) Single-Domain Gates Cross-Domain Gate \ud835\udc65 ! \" \ud835\udc65 # $ \ud835\udc65 % \" \ud835\udc65 & $ \ud835\udc65 ' ( \ud835\udc65 ) ( \ud835\udc65 * \" \ud835\udc65 + $ Single-Domain Next Item Prediction Loss ( \ud835\udcdb \ud835\udc94\ud835\udc8a\ud835\udc8f\ud835\udc88\ud835\udc8d\ud835\udc86 ) token ID embedding vector \ud835\udc59 loss + + + \u00d7 \u00d7 \ud835\udc59 + $ \ud835\udc59 # $ \ud835\udc59 & $ \ud835\udc59 ! \" \ud835\udc59 % \" \ud835\udc59 * \" \ud835\udc59 ' ( \ud835\udc59 ) ( \ud835\udc65 ! \" \ud835\udc65 # $ \ud835\udc65 % \" \ud835\udc65 & $ \ud835\udc65 ' ( \ud835\udc65 ) ( \ud835\udc60\ud835\udc5c\ud835\udc60 \ud835\udc65 + $ \ud835\udc65 ! \" \ud835\udc65 # $ \ud835\udc65 % \" \ud835\udc65 & $ \ud835\udc65 ' ( \ud835\udc65 ) ( \ud835\udc65 + $ \ud835\udc60\ud835\udc5c\ud835\udc60 \ud835\udc60\ud835\udc5c\ud835\udc60 \ud835\udc60\ud835\udc5c\ud835\udc60 \ud835\udcdb \ud835\udc84\ud835\udc93\ud835\udc90\ud835\udc94\ud835\udc94 \ud835\udc8d\ud835\udc93\ud835\udc83 cross \ud835\udc59 ! \ud835\udc59 ! \ud835\udc59 # \ud835\udc59 % \ud835\udc59 & \ud835\udc59 ' \ud835\udc59 ) \ud835\udc59 * \ud835\udc7f ! \ud835\udc04 \ud835\udc04 ! \ud835\udc04 \" \ud835\udc04 # \ud835\udc18 $%&'() \ud835\udc7f \" \ud835\udc7f # \ud835\udc7f split \u00d7 \ud835\udc04 $%&'() Shifted Single-Domain sequences Expert j Expert j+1 \u2026 Expert K Stop Gradient (only CDSR) Stop Gradient (only SDSR) Shared Embedding Layer - - - - - - - - \ud835\udefe ! \ud835\udefe \" \ud835\udefe # Relative Negative Transfer Gap + + + Single-Cross Mutual Information Maximization (SC-MIM) \ud835\udc65 ! \" \ud835\udc65 # $ \ud835\udc65 % \" \ud835\udc65 & $ \ud835\udc65 ' ( \ud835\udc65 ) ( \ud835\udc65 * \" \ud835\udc65 + $ Shifted Cross-Domain Sequence \ud835\udc59 ! \ud835\udc59 ! \ud835\udc59 # \ud835\udc59 % \ud835\udc59 & \ud835\udc59 ' \ud835\udc59 ) \ud835\udc59 * \ud835\udc18 *+,$$ Cross-Domain Next Item Prediction Loss ( \u2112 67899 ) (\ud835\udc18 \" ) #$%&'( (\ud835\udc18 ) ) #$%&'( (\ud835\udc18 * ) #$%&'( (\ud835\udc18 \" ) +,#-# (\ud835\udc18 ) ) +,-## (\ud835\udc18 * ) +,-## split Loss Correction (a) (b) (c-3) (c-4) (c-6) (c-7) (c-8) (c-9) (e-1) (e-2) (e-3) (d-3) (d-2) SDSR CDSR \ud835\udc59 ! \ud835\udc59 # \ud835\udc59 & \ud835\udc59 ! \ud835\udc59 % \ud835\udc59 * \ud835\udc59 ' \ud835\udc59 ) \ud835\udc59 + $ \ud835\udc59 # $ \ud835\udc59 & $ \ud835\udc59 ! \" \ud835\udc59 % \" \ud835\udc59 * \" \ud835\udc59 ' ( \ud835\udc59 ) ( \u00d7 Asymmetric Cooperative Network with Mixture of Sequential Experts (ACMoE) (c-1) (c-2) (d-1) (c-5) where some domains performed poorly over the SDSR approach (Fig. 1). Additionally, CGRec showed the exponential computational complexity resulting from the calculation of Shapley values, which increases with the number of domains involved. Therefore, our focus is on reducing negative transfer of all domains in the CDSR task while considering computational efficiency for rapid industrial deployment (Section 5.4 and 5.5).", "3 PRELIMINARY": "We consider an expanding CDSR task characterized by interaction sequences spanning more than three domains. These domains may encompass diverse service platforms such as e-commerce or video platforms [11, 38]. Formally, the set of domains is denoted as D = { \ud835\udc34, \ud835\udc35,\ud835\udc36, ... } , where |D| \u2265 3. We also consider the notation \ud835\udc51 \u2208 D as a specific domain \ud835\udc51 . The notation \ud835\udc49 \ud835\udc51 is introduced to represent a set of items specific to the domain \ud835\udc51 . Then, \ud835\udc49 indicates the total item set across all domains. of domain \ud835\udc51 (i.e., \ud835\udc4b \ud835\udc51 ) while the CDSR is to recommend the next item of domain \ud835\udc51 based on the cross-domain sequence (i.e., \ud835\udc4b ). Definition 2. Negative Transfer Gap : Based on previous studies [29, 33], we quantitatively measured the degree of negative transfer of each domain in the context of the CDSR task, referred to as the Negative Transfer Gap (NTG). Suppose that L \ud835\udc51 \ud835\udf0b is the empirical loss of the sequential recommendation task using a model \ud835\udf0b for a specific domain \ud835\udc51 , which takes the user's single-domain sequence \ud835\udc4b \ud835\udc51 (i.e., SDSR task) or the cross-domain sequence \ud835\udc4b (i.e., CDSR task). We can then define the negative transfer gap \ud835\udf19 \ud835\udf0b ( \ud835\udc51 ) of the domain \ud835\udc51 as a quantifiable measure of negative transfer: \ud835\udf19 \ud835\udf0b ( \ud835\udc51 ) = L \ud835\udc51 \ud835\udf0b ( \ud835\udc4b \ud835\udc51 ) - L \ud835\udc51 \ud835\udf0b ( \ud835\udc4b ) , and negative transfer is identified when this value is negative. In Section 4.3, we introduce the module that explicitly calculates the negative transfer gap \ud835\udf19 \ud835\udf0b ( \ud835\udc51 ) and we use it as the weighting factor for the domain-specific loss function. Definition 1. Single- and Cross-Domain Sequential Recommendation : The single-domain sequences of domain \ud835\udc51 are represented by \ud835\udc4b \ud835\udc51 = [( SOS ) , \ud835\udc65 \ud835\udc51 1 , \ud835\udc65 \ud835\udc51 2 , ..., \ud835\udc65 \ud835\udc51 | \ud835\udc4b \ud835\udc51 | -1 ] , where each element \ud835\udc65 \ud835\udc51 \ud835\udc61 signifies an interaction occurring at time \ud835\udc61 and ( SOS ) is a start token of the sequence. We then define \ud835\udc4b = ( \ud835\udc4b \ud835\udc34 , \ud835\udc4b \ud835\udc35 , \ud835\udc4b \ud835\udc36 , ... ) as the cross-domain sequence where the |D| single-domain sequences are merged and rearranged in chronological order. Conversely, the cross-domain sequence \ud835\udc4b can be split into |D| of \ud835\udc4b \ud835\udc51 (Fig. 2a). For example, consider a scenario where a user engages with three domains, labeled { \ud835\udc34, \ud835\udc35,\ud835\udc36 } . The cross-domain sequence for these domains is \ud835\udc4b = [( SOS ) , \ud835\udc65 \ud835\udc34 1 , \ud835\udc65 \ud835\udc35 2 , \ud835\udc65 \ud835\udc34 3 , \ud835\udc65 \ud835\udc35 4 , \ud835\udc65 \ud835\udc34 5 , \ud835\udc65 \ud835\udc36 6 , \ud835\udc65 \ud835\udc36 7 ] , and is then split into \ud835\udc4b \ud835\udc34 = [( SOS ) , \ud835\udc65 \ud835\udc34 1 , \ud835\udc65 \ud835\udc34 3 , \ud835\udc65 \ud835\udc34 5 ] , \ud835\udc4b \ud835\udc35 = [( SOS ) , \ud835\udc65 \ud835\udc35 2 , \ud835\udc65 \ud835\udc35 4 ] , and \ud835\udc4b \ud835\udc36 = [( SOS ) , \ud835\udc65 \ud835\udc36 6 , \ud835\udc65 \ud835\udc36 7 ] from the split process. Note that the SDSR solely focuses on recommending the next item within a specific domain \ud835\udc51 using the single-domain sequence Problem Statement : Given the historical cross-domain sequences \ud835\udc4b 1: \ud835\udc61 observed up to the time step \ud835\udc61 , our goal is to predict the next item \ud835\udc65 \ud835\udc51 \ud835\udc61 + 1 :  where \ud835\udc43 GLYPH<16> \ud835\udc65 \ud835\udc51 \ud835\udc61 + 1 GLYPH<12> GLYPH<12> GLYPH<12> \ud835\udc4b 1: \ud835\udc61 GLYPH<17> is the probability that the target item \ud835\udc65 \ud835\udc51 \ud835\udc61 + 1 in domain \ud835\udc51 will be interacted with by the given item sequences. This goal is achieved through the SDSR and the CDSR tasks simultaneously. In other words, there are |D| single-domain sequences for the SDSR and one cross-domain sequence for the CDSR task, and therefore |D| +1 next item prediction tasks are performed in one model in a multi-task learning manner.", "4 MODEL": "We illustrate the overall architecture of our model in Fig. 2. SIGIR '24, July 14-18, 2024, Washington, DC, USA Park Chung, et al.", "4.1 Shared Embedding Layer": "During the embedding look-up phase, we acquire initialized representations of items for |D| single-domain sequences (i.e., \ud835\udc4b \ud835\udc51 ) and one cross-domain sequence (i.e., \ud835\udc4b ). For each domain \ud835\udc51 , we define an item embedding matrix \ud835\udc40 \ud835\udc51 represented by R | \ud835\udc49 \ud835\udc51 | \u00d7 \ud835\udc5f , where | \ud835\udc49 \ud835\udc51 | indicates the total number of items in domain \ud835\udc51 , and \ud835\udc5f is the embedding dimension. We then concatenate the embedding matrices of all |D| domains to form an aggregated embedding matrix \ud835\udc40 \u2208 R | \ud835\udc49 | \u00d7 \ud835\udc5f , where | \ud835\udc49 | is the total number of item sets across all domains. The single- and cross-domain sequences (i.e., \ud835\udc4b \ud835\udc51 and \ud835\udc4b ) are subsequently converted into a fixed length, \ud835\udc47 . If the sequence length is shorter than \ud835\udc47 , special tokens ( PAD ) are added to the left side, serving as placeholders for past interactions on \ud835\udc4b \ud835\udc51 and \ud835\udc4b . Conversely, if the sequence length exceeds \ud835\udc47 , only the \ud835\udc47 most recent actions are preserved. As shown in Fig. 2b, the final embedding matrix \ud835\udc40 is shared by the single- and cross-domain sequence by a look-up operation to extract the initialized sequences representation E \ud835\udc51 \u2208 R \ud835\udc47 \u00d7 \ud835\udc5f and E \u2208 R \ud835\udc47 \u00d7 \ud835\udc5f , respectively (Fig. 2(c-1) and 2(c-2)). The aggregation |D| of E \ud835\udc51 in chronological order can be described as E \ud835\udc60\ud835\udc56\ud835\udc5b\ud835\udc54\ud835\udc59\ud835\udc52 (Fig. 2(c-1)). Additionally, a learnable position embedding matrix P \u2208 R \ud835\udc47 \u00d7 \ud835\udc5f is integrated into E \ud835\udc51 and E to encapsulate the sequential nature of user interaction ( E \ud835\udc51 \u2190 E \ud835\udc51 + P , E \u2190 E + P ) [28]. The embedding representations of E \ud835\udc51 and E at the \ud835\udc61 -th step are denoted as \ud835\udc52 \ud835\udc51 \ud835\udc61 and \ud835\udc52 \ud835\udc61 , respectively.", "4.2 Asymmetric Cooperative Network with Mixture-of-Sequential Experts (ACMoE)": "Referring to previous studies [29, 33], the degree of the negative transfer of a specific domain can be defined as the difference between the 1 loss of the SDSR and 2 the loss of CDSR tasks for the domain (i.e., 1 -2 ). This value is called the negative transfer gap (NTG) , and helps determine whether samples from other domains are beneficial or detrimental to the specific domain. The degree of negative transfer in a specific domain increases as the NTG decreases, and therefore, we assign this value as the weight for the prediction loss in the domain, resulting in a smaller gradient flow. To efficiently calculate this, we designed an asymmetric cooperative network that concurrently executes SDSR and CDSR tasks for each domain in a multi-task learning manner. For our multi-task learning, we utilized the multi-gate Mixture of Sequential Experts (MoE) architecture [21]. This architecture explicitly models the relationships between different tasks and learns task-specific functionalities, enabling it to effectively leverage shared representations. During this phase, we employed a decoupled mechanism allowing expert networks to be specialized in either SDSR or CDSR tasks, ensuring both tasks are implemented without mutual interference. For this, in the decoupled mechanism, a stop-gradient operation is adopted on some experts for the SDSR tasks and on other experts for the CDSR task. This approach enables the losses for both tasks to be computed independently, leading to a more precise evaluation of the NTG. We used Transformer [28] as the expert to handle sequential data. 4.2.1 Architecture. Mathematically, given the initialized representations of single- and cross-domain sequences E \ud835\udc51 and E from the shared embedding layer, we perform many-to-many sequence learning in each sequential expert. In the case of single-domain sequences, for a given \ud835\udc47 -length inputs E \ud835\udc51 , we can formulate the output of domain \ud835\udc51 for this module as given below:   where \u210e \ud835\udc51 is the tower network for domain \ud835\udc51 (Fig. 2(c-7)), \ud835\udc53 \ud835\udc51 is the multi-gated mixture of the sequential experts layer, SG (\u00b7) is the stopgradient operation (Fig. 2(c-4)), and \ud835\udc53 \ud835\udc58 TRM is the \ud835\udc58 -th transformerbased [28] sequential expert (Fig. 2(c-3); there are \ud835\udc3e experts in total). The tower network \u210e \ud835\udc51 consists of a feed-forward network with layer normalization [1]. The stop-gradient operation SG (\u00b7) serves as an identity function during the forward pass, but it drops the gradient for variables enclosed within it during the backward pass. This operation is only used in the output from the 1 to the \ud835\udc57 -th expert network ( \ud835\udc57 < \ud835\udc3e ). Therefore, these experts cannot be updated by the single-domain sequences, but experts from \ud835\udc57 +1 to \ud835\udc3e can learn the unique sequential pattern of single-domain sequences. In other words, we can say that \ud835\udc3e -\ud835\udc57 represents the number of experts trained by the SDSR task with the gradient descent algorithm. We set \ud835\udc57 to be 0.2 times the total number of experts \ud835\udc3e , and report a sensitivity analysis on the hyper-parameter \ud835\udc57 in Section 5.6. In addition, \ud835\udc54 \ud835\udc51 is the gating network for domain \ud835\udc51 (Fig. 2(c-6)), which converts the input into a distribution across the \ud835\udc3e experts:  where \ud835\udc4a \ud835\udc51 \ud835\udc54 \u2208 R \ud835\udc3e \u00d7 \ud835\udc51\ud835\udc47 is the trainable fully-connected layer. The aggregation |D| of ( Y \ud835\udc51 ) \ud835\udc60\ud835\udc56\ud835\udc5b\ud835\udc54\ud835\udc59\ud835\udc52 in chronological order is described as Y \ud835\udc60\ud835\udc56\ud835\udc5b\ud835\udc54\ud835\udc59\ud835\udc52 (Fig. 2(d-1)). The \ud835\udc61 -th element of Y \ud835\udc60\ud835\udc56\ud835\udc5b\ud835\udc54\ud835\udc59\ud835\udc52 is then ( \ud835\udc66 \ud835\udc51 \ud835\udc61 ) \ud835\udc60\ud835\udc56\ud835\udc5b\ud835\udc54\ud835\udc59\ud835\udc52 . We also perform the CDSR task using a similar scheme to that above for the single-domain case. For a given \ud835\udc47 -length input E , we represent the output of ACMoE module as follows:  where \u210e \ud835\udc50\ud835\udc5f\ud835\udc5c\ud835\udc60\ud835\udc60 is the tower network (Fig. 2(c-9)) and \ud835\udc53 \ud835\udc50\ud835\udc5f\ud835\udc5c\ud835\udc60\ud835\udc60 is the multi-gated mixture of sequential experts layer for a cross-domain sequence. The stop-gradient operation SG (\u00b7) (Fig. 2(c-5)) is only used in the output from \ud835\udc57 +1 to \ud835\udc3e -th \ud835\udc53 \ud835\udc58 TRM . From this operation, certain experts (from 1 to \ud835\udc57 ) are able to learn the distinct sequential patterns present in cross-domain sequences. In other words, we can say that \ud835\udc57 is the number of experts trained by the CDSR task. The gating network \ud835\udc54 \ud835\udc50\ud835\udc5f\ud835\udc5c\ud835\udc60\ud835\udc60 ( E ) = \ud835\udc60\ud835\udc5c\ud835\udc53 \ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65 ( \ud835\udc4a \ud835\udc50\ud835\udc5f\ud835\udc5c\ud835\udc60\ud835\udc60 \ud835\udc54 E ) for the crossdomain sequence maps the input to a distribution over the \ud835\udc3e experts (Fig. 2(c-8)). We can then formulate the output for this module as Y \ud835\udc50\ud835\udc5f\ud835\udc5c\ud835\udc60\ud835\udc60 = ( \ud835\udc66 1 , \ud835\udc66 2 , ..., \ud835\udc66 \ud835\udc47 ) \ud835\udc50\ud835\udc5f\ud835\udc5c\ud835\udc60\ud835\udc60 . Note that ( \ud835\udc66 \ud835\udc51 \ud835\udc61 ) \ud835\udc60\ud835\udc56\ud835\udc5b\ud835\udc54\ud835\udc59\ud835\udc52 and ( \ud835\udc66 \ud835\udc61 ) \ud835\udc50\ud835\udc5f\ud835\udc5c\ud835\udc60\ud835\udc60 are two representations of different views for the same item (i.e., also same domain \ud835\udc51 ) at time step \ud835\udc61 , depending on whether it was encoded based on the single- or cross-domain sequence. SyNCRec SIGIR '24, July 14-18, 2024, Washington, DC, USA 4.2.2 Transformer Experts. We adopted the self-attention module introduced in Transformer [28] as the sequential expert layer \ud835\udc53 \ud835\udc58 TRM (Fig. 2(c-3)), which consists of two distinct sub-layers as follows: Multi-head Self-Attention (MSA) At each MSA head, the inputs \ud835\udc4d \u2208 R \ud835\udc47 \u00d7 \ud835\udc5f undergo a linear transformation resulting in three hidden representations, i.e., queries \ud835\udc44 \ud835\udc56 \u2208 R \ud835\udc47 \u00d7 \ud835\udc5f / \ud835\udc5d , keys \ud835\udc3e \ud835\udc56 \u2208 R \ud835\udc47 \u00d7 \ud835\udc5f / \ud835\udc5d , and values \ud835\udc49 \ud835\udc56 \u2208 R \ud835\udc47 \u00d7 \ud835\udc5f / \ud835\udc5d , where \ud835\udc56 indicates a specific head, \ud835\udc5d is the number of heads, and \ud835\udc5f is the dimension of the input. Using these three hidden representations, the scaled dot-product attention ( Attn ) is then computed as follows:  where W \ud835\udc44 \ud835\udc56 ,W \ud835\udc3e \ud835\udc56 ,W \ud835\udc49 \ud835\udc56 \u2208 R \ud835\udc5f \u00d7 \ud835\udc5f / \ud835\udc5d are the trainable parameters. In the MSA, the above Attn operation is implemented \ud835\udc5d times in parallel, and then involves concatenating the outputs of each head and linearly projecting them to extract the final output H \u2208 R \ud835\udc47 \u00d7 \ud835\udc5f :  where | | is the concatenate operation and \ud835\udc4a \ud835\udc39 \u2208 R \ud835\udc5f \u00d7 \ud835\udc5f is the trainable parameters. For the sequential recommendation, only information from prior time steps can be utilized. Therefore, a masking operation is applied to the output of the MSA layer, effectively eliminating all connections between \ud835\udc44 \ud835\udc56 and \ud835\udc3e \ud835\udc57 whenever \ud835\udc57 > \ud835\udc56 . Point-wise Feed-Forward Network (FFN) TheFFNisthenadopted into H to introduce nonlinearity and enable interactions among various latent subspaces using a fully-connected layer (FC), as follows:  where H \ud835\udc61 is the \ud835\udc61 -th representation of H , W1 \u2208 R \ud835\udc5f \u00d7 \ud835\udc5f , \ud835\udc4f 1 \u2208 R \ud835\udc5f \u00d7 1 ,W2 \u2208 R \ud835\udc5f \u00d7 \ud835\udc5f , and \ud835\udc4f 2 \u2208 R \ud835\udc5f \u00d7 1 are learnable parameters, and \ud835\udc3a\ud835\udc38\ud835\udc3f\ud835\udc48 is the gelu activation [7]. Therefore, the transformer-based sequential expert \ud835\udc53 \ud835\udc58 TRM in Eq 2 and 4 is described as follows: \ud835\udc53 \ud835\udc58 TRM (\u00b7) = FFN \ud835\udc58 ( MSA \ud835\udc58 (\u00b7)) .", "4.3 Loss Correction with Negative Transfer Gap (LC-NTG)": "From our ACMoE layer, we can explicitly measure the negative transfer gap (NTG) of each domain as described in Section 3, and use the NTG to adaptively assign lower weights to the loss of items in domains that exhibit significant negative transfer during the training stage (Fig. 2(e-3)). 4.3.1 Single-Domain Item Prediction. As presented in Fig. 2(e-1), given the single-domain sequence \ud835\udc4b \ud835\udc51 1: \ud835\udc61 and its expected next item \ud835\udc65 \ud835\udc51 \ud835\udc61 + 1 , we use the pairwise ranking loss [23] to optimize our model as follows:  where \ud835\udc65 \ud835\udc51 + is the ground-truth item paired with a negative item \ud835\udc65 \ud835\udc51 -sampled from a uniform distribution, and \ud835\udf0e indicates the sigmoid activation function. Note that \ud835\udc43 ( \ud835\udc65 \ud835\udc51 \ud835\udc61 + 1 = \ud835\udc65 \ud835\udc51 | \ud835\udc4b \ud835\udc51 1: \ud835\udc61 ) is calculated as \ud835\udf0e GLYPH<16> ( \ud835\udc66 \ud835\udc51 \ud835\udc61 ) \ud835\udc60\ud835\udc56\ud835\udc5b\ud835\udc54\ud835\udc59\ud835\udc52 \u00b7 \ud835\udc40 ( \ud835\udc65 \ud835\udc51 ) GLYPH<17> , where \u00b7 is the dot-product operation. 4.3.2 Cross-Domain Item Prediction. Similar to the single-domain item prediction, the CDSR task using the cross-domain sequence \ud835\udc4b 1: \ud835\udc61 is performed with the following objective (Fig. 2(e-2)):   4.3.3 Calculating the Negative Transfer Gap. We can describe the negative transfer gap (NTG) using the losses of Eq. 8 and Eq. 9 as follows:  where \ud835\udc59 \ud835\udc51 \ud835\udc61 and \ud835\udc59 \ud835\udc61 are losses of the SDSR and CDSR tasks in time step \ud835\udc61 for the domain \ud835\udc51 , respectively, calculated with our model \ud835\udf0b . Note that in the cross-domain item prediction loss \ud835\udc59 \ud835\udc61 , only the loss values corresponding to domain \ud835\udc51 are used in Eq. 10. We then calculated the trainable relative NTG in each domain. Let \ud835\udf06 \ud835\udc51 represent the relative NTG of domain \ud835\udc51 , and \ud835\udf06 = ( \ud835\udf06 1 , \ud835\udf06 2 , ..., \ud835\udf06 | D| ) be the vector of the relative NTG. The vector of the relative NTG is initialized to be 0 .From Eq. 10, we obtain \ud835\udf19 \ud835\udf0b ( \ud835\udc51 ) for all domains at each training batch. \ud835\udf06 is then updated in every batch as follows:  where softmax ( ; \ud835\udeff ) is the softmax function with the temperature \ud835\udeff [26], and \ud835\udefc and \ud835\udefd are learnable parameters. 4.3.4 Loss Correction. The relative NTG serves as a weight for the cross-domain item prediction loss computed in Eq. 9. This loss is re-aggregated by multiplying the relative NTG for each domain separately as follows (Fig. 2(e-3)): L \ud835\udc59\ud835\udc50 \ud835\udc50\ud835\udc5f\ud835\udc5c\ud835\udc60\ud835\udc60  This adaptive regulation of the gradient diminishes its flow in domains where there is significant negative transfer, thereby alleviating negative transfer effects across all domains.", "4.4 Single-Cross Mutual Information Maximization (SC-MIM)": "To improve the transfer of valuable information between SDSR and CDSR tasks, we developed the Single-Cross Mutual Information Maximization (SC-MIM), an auxiliary loss that maximizes the mutual information between the representation pairs from the SDSR and CDSR tasks on a per-domain basis. The SC-MIM extracts the correlation signals between the intrinsic characteristics of the both tasks for a specific domain. This module is an application of the SIGIR '24, July 14-18, 2024, Washington, DC, USA Park Chung, et al. concept of mutual information, which reflects how knowledge of one random variable reduces uncertainty in the other random variable. Mathematically, the mutual information \ud835\udc3c between random variables \ud835\udc4b and \ud835\udc4c is given as follows:  where \ud835\udc37 \ud835\udc3e\ud835\udc3f (\u00b7||\u00b7) is the Kullback-Leibler divergence. However, maximizing mutual information in high-dimensional spaces can be a challenging task. Therefore, in practice, it is common to maximize a tractable lower bound on \ud835\udc3c ( \ud835\udc4b,\ud835\udc4c ) . A specific lower bound that has demonstrated effective practical results is InfoNCE [10, 36], which is defined as follows:  where \ud835\udc65 and \ud835\udc66 are two distinct viewpoints of the same input, and \ud835\udf0c \ud835\udf03 represents a parameterized similarity function with \ud835\udf03 . And \u02c6 \ud835\udc4c is a set of samples chosen from a proposal distribution \ud835\udc5e ( \u02c6 \ud835\udc4c ) , which includes a positive sample \ud835\udc66 and | \u02c6 \ud835\udc4c | -1 negative samples. Refer to the detailed derivations of InfoNCE (Eq. 14) as a lower bound for mutual information in Oord et al. [19]. If \u02c6 \ud835\udc4c contains all possible outcomes of the uniformly distributed random variable \ud835\udc4c , then maximizing InfoNCE is equal to maximizing the standard cross-entropy loss:  In our problem, we focus on maximizing the mutual information of single- and cross-domain representations (i.e., Y \ud835\udc60\ud835\udc56\ud835\udc5b\ud835\udc54\ud835\udc59\ud835\udc52 and Y \ud835\udc50\ud835\udc5f\ud835\udc5c\ud835\udc60\ud835\udc60 ), which are different views of the user sequence. For this, we split the cross-domain representation Y \ud835\udc50\ud835\udc5f\ud835\udc5c\ud835\udc60\ud835\udc60 into a subsequence per domain ( Y \ud835\udc51 ) \ud835\udc50\ud835\udc5f\ud835\udc5c\ud835\udc60\ud835\udc60 (Fig. 2(d-2)). Then, as shown in Fig. 2(d-3), we can describe the mutual information between single- and crossdomain sequences for domain \ud835\udc51 as follows:  where \ud835\udc62 -is the other users in a training batch, and ( Y \ud835\udc51 ) \ud835\udc60\ud835\udc56\ud835\udc5b\ud835\udc54\ud835\udc59\ud835\udc52 -is the subsequence of domain \ud835\udc51 of user \ud835\udc62 -. In addition, \ud835\udf0c (\u00b7 , \u00b7) is implemented according to the following equation:  where \ud835\udf0e is the sigmoid function and \ud835\udc4a \ud835\udc3b \u2208 R \ud835\udc5f \u00d7 \ud835\udc5f is a trainable parameter matrix.", "4.5 Model Training and Evaluation": "The total training loss function of our model is as follows:  where \ud835\udf02 is the harmonic factor. In the evaluation stage, we only used cross-domain representations to make predictions. For example, given the latest representations ( \ud835\udc66 \ud835\udc47 ) \ud835\udc50\ud835\udc5f\ud835\udc5c\ud835\udc60\ud835\udc60 , the next recommended item is selected based on the highest prediction score in domain \ud835\udc51 : Table 1: Statistics of datasets  where \ud835\udc49 \ud835\udc51 is the item set of domain \ud835\udc51 and \ud835\udc40 is the embedding layer.", "5 EXPERIMENTS": "The experiments are designed to answer the following research questions: (RQ1) : Does the performance of our model surpass the current stateof-the-art baselines in practical applications that involve more than three domains? (RQ2) : Can our model effectively address the challenge of negative transfer across all domains in the CDSR task? (RQ3) : What is the impact of various components of our model on its performance in CDSR tasks? (RQ4) : How do variations in hyper-parameter settings influence the performance of our model? (RQ5) : How does the model perform when deployed online?", "5.1 Datasets": "5.1.1 Amazon Dataset . Amazon review datasets [18] was used in our experiments, encompassing five domains: Books , Clothing Shoes and Jewelry , Video and Games , Toys and Games , and Sports and Outdoors (Table 1). These domains are abbreviated as Books , Clothing , Video , Toys , and Sports , respectively. 5.1.2 Industrial Dataset (Telco) . We collected user logs from diverse real-world applications operated by a leading global telecommunications company. The dataset comprised customers who consented to the collection and analysis of their data. The data contain five domains: Application Usage ( APP-Use ), Call Record ( Call-Use ), Navigation Service ( Navi ), e-commerce service ( e-comm ), and Discount Coupon Usage ( Coupon-Use ). We identified 99,936 users, and there are 48,887 non-overlapping users across five domains.", "5.2 Experimental Settings": "5.2.1 Baseline Models. The performance of our model was evaluated with four categories of baseline models: (1) General Recommendation ( GR ), (2) Single-Domain Sequential Recommendation ( SR ), (3) Cross-Domain Recommendation ( CDR ), and (4) CrossDomain Sequential Recommendation ( CDSR ), as shown in Tables 2 and 3. The baselines in the GR include BPRMF [23] and GCMC [2]. BPRMFis a classical approach that develops a pairwise loss function for modeling the relative preferences of users. GCMC proposes a graph autoencoder framework to address the matrix completion. Refer to Section 2 for a description of SR, CDR, and CDSR baselines. The RecBole framework [34, 35] was used to implement all CDR baselines and some SR models. CARCA [22], S 3 Rec [36], C 2 DSR [3], SyNCRec SIGIR '24, July 14-18, 2024, Washington, DC, USA Table 2: Overall model performance comparison in Amazon dataset. The top two methods are highlighted in bold and underlined. Table 3: Overall model performance comparison in Telco dataset. The top two methods are highlighted in bold and underlined. MIFN [16], CAT-ART [11], \ud835\udf0b -net [17], and MAN [13] were executed with their official code. For the implementation of CGRec [20], we made use of the source code that the authors provided. 5.2.2 Evaluation Settings and Metrics. In line with prior studies [9, 22, 36], we employed the leave-one-out approach for evaluating recommendation performance. Specifically, each user interaction sequence was divided into three parts: the last item designated as test data, the penultimate item reserved for validation, and all preceding items used as training data. Performance for each domain was assessed independently, depending on the domain of the last item in the test data, resulting in domain-specific performance metrics derived from different user groups. Given the vast number of items, using all items as test candidates was impractical due to time constraints. Consequently, a widely adopted method was implemented, where the ground truth item (positive sample) is paired with 99 randomly chosen items (negative samples) that the user had not engaged with before. We then reported the performance of Top\ud835\udc58 recommendations, which is based on a ranked list of 100 items. The assessment focused on several key metrics: Hit Ratio (HR), Normalized Discounted Cumulative Gain (NDCG), and Mean Reciprocal Rank (MRR). Note that the performance of GR, SR, CDR, and CDSR baselines including ours was measured using cross-domain sequences on the CDSR scenario. 5.2.3 Implementation Details. The size of the embedding ( \ud835\udc5f ), training batch, and maximum sequence length ( \ud835\udc47 ) were all set to 128. The number of experts in ACMoE is set to four for the Amazon SIGIR '24, July 14-18, 2024, Washington, DC, USA Park Chung, et al. dataset and five for the Telco dataset. The transformer layer in the expert network utilizes two single-head attention blocks, each with a head size of four. For updating all parameters, the Adam optimizer was employed. To guarantee a fair comparison, we configured the baseline model's sequence length, embedding size, batch size, and optimizer to match those of our model.", "5.3 Performance Evaluation (RQ1)": "We evaluated the performance of predicting the next item among all baselines and our model. The results for the two datasets are reported in Table 2 ( Amazon ) and Table 3 ( Telco ). Based on these results, we can draw the following conclusions. (1) The effectiveness of our model can be observed . Our model outperforms most baseline models on two real-world datasets. 2 Our model improved upon the best baseline model (second best for each domain) by +3.13% ( Book ), +1.18% ( Clothing ), +7.29% ( Video ), +9.65% ( Toys ), and +5.28% ( Sports ) for HR@5 on the Amazon dataset, and +11.96% ( Call -Use ), +4.65% ( Navi ), +6.47% ( Coupon -Use ), and +38.81% ( e -comm ) on the Telco dataset. In the App -Use domain, our model did not achieve the top performance, but the difference from the best-performing baseline (i.e., CMF) was minimal, at only -0.63%. Our model thus performs consistently well across all domains, unlike some other models such as CMF and DeepAFP which show high performance in certain domains but significantly lower performance in others. (2) Integrating information from all domains simultaneously in a model can improve performance in each domain compared to modeling a pairwise domain-domain relationship. CDSR baselines that modeled five domains simultaneously, such as CAT-ART, CGRec, and our model, outperformed other baselines that only trained on domain pairs (e.g., C 2 DSR, MAN, MIFN, and \ud835\udf0b -net). Furthermore, extending these domain pair-wise CDSR models to a multi-domain CDSR scenario with |D| domains would require managing at least GLYPH<0> | D| 2 GLYPH<1> pairs of relations, which becomes impractical when the number of domains is substantial (See footnote 2). As a result, we confirmed the effectiveness of combining information from various domains at once for the CDSR task.", "5.4 Discussion of the negative transfer (RQ2)": "The phenomenon of negative transfer becomes apparent when comparing the performance of the domain-specific model, trained exclusively on single-domain sequence, with the performance of baseline models trained on cross-domain sequences. For the domainspecific model, we used CGRec [20], a state-of-the-art CDSR model that can also be trained on a single-domain sequence. As shown in Table 4, all models, except for ours, performed worse than the SDSR approach for two to nine of the ten domains in both datasets. Our model outperformed the SDSR approach for all domains, particularly in the Clothing , Toys , and Call -Use domains, where other models performed poorly. In conclusion, our model significantly mitigates the negative transfer problem compared to other CDSR baselines for all domains. 2 All CDR and CDSR baselines, except CAT-ART and CGRec, were designed to model only pairwise relationships between domains. Therefore, for these baselines, two out of five domains were chosen for training. In other words, each domain was paired with the other four domains, and the average performance is reported in Tables 2 and 3. Due to space constraints, we cannot provide a detailed pairwise performance analysis of these CDR and CDSR models across domains. Table 4: The performance improvement(%) of the representative CDSR baselines trained on the cross-domain sequences compared to the CGRec [20] trained on the single-domain sequence only. If this value is positive, then it is highlighted in blue; otherwise red. *In the Amazon column, B, C, V, T, and S indicate \ud835\udc35\ud835\udc5c\ud835\udc5c\ud835\udc58\ud835\udc60 , \ud835\udc36\ud835\udc59\ud835\udc5c\ud835\udc61\u210e\ud835\udc56\ud835\udc5b\ud835\udc54 , \ud835\udc49\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc5c , \ud835\udc47\ud835\udc5c\ud835\udc66\ud835\udc60 , and \ud835\udc46\ud835\udc5d\ud835\udc5c\ud835\udc5f\ud835\udc61\ud835\udc60 domain, respectively. In addition, A, C, N, M, and C in the Telco column are the abbreviations for \ud835\udc34\ud835\udc5d\ud835\udc5d -\ud835\udc48\ud835\udc60\ud835\udc52 , \ud835\udc36\ud835\udc4e\ud835\udc59\ud835\udc59 -\ud835\udc48\ud835\udc60\ud835\udc52 , \ud835\udc41\ud835\udc4e\ud835\udc63\ud835\udc56 , \ud835\udc36\ud835\udc5c\ud835\udc62\ud835\udc5d\ud835\udc5c\ud835\udc5b -\ud835\udc48\ud835\udc60\ud835\udc4e\ud835\udc54\ud835\udc52 , and \ud835\udc52 -\ud835\udc50\ud835\udc5c\ud835\udc5a\ud835\udc5a domain, respectively. Table 5: Comparison of MRR for various combinations of components. If the difference between Full SyNCRec and its variants is positive, then it is colored in blue; otherwise red.", "5.5 Discussion of Model Variants (RQ3)": "To examine the validity of each component, we introduce various variants, each aligned with their specific design purpose (Table 5): (A) w/o LC-NTG : The LC-NTG module is removed in this model variant. Therefore, there is no gradient explicit controller in the training for high NTG domains in the model. (B) w/o SC-MIM : This model variant removes the SC-MIM objective, which means it does not take into account the correlation between single- and cross-domain representations for each domain. (C) w/o ACMoE : The ACMoE layer is replaced with the vanilla MoE layer without the stop gradient operation. This variant does not explicitly require experts to be specialized in SDSR and CDSR tasks. Therefore, it is not possible to compute the pure losses of SDSR and CDSR tasks, and as a result, the exact negative transfer in LC-NTG cannot be obtained. (1) Each of the three main components of our model contributes to improved performance. The prediction performance of our model and its three variants was compared, and it was discovered that removing or replacing any key component resulted in significant performance degradation. In particular, the performance gap between full SyNCRec and the (A) \ud835\udc64 / \ud835\udc5c LC-NTG variant indicates the advantage of a penalty for the high NTG domain in the training process. The comparison of full SyNCRec with (B) \ud835\udc64 / \ud835\udc5c SC-MIM shows that the SC-MIM module led to performance improvements in most domains, demonstrating the advantage of SyNCRec SIGIR '24, July 14-18, 2024, Washington, DC, USA modeling correlations between single- and cross-domain sequences. Furthermore, ACMoE effectively decoupled the expert networks to specialize them for SDSR and CDSR tasks. This reflects the accurate negative transfer effect in the model, as the (C) w/o ACMoE variant exhibits performance degradation compared to full SyNCRec in most domains in both datasets. In short, LC-NTG is seen to be more effective when SDSR and CDSR are decoupled via the ACMoE. (2) Our model, along with its variations, demonstrates enhanced performance when compared to baseline models that share similar design purposes. CGRec measures domain-specific NTGusing the concept of the Shapley value in terms of the training loss for each domain, and tries loss re-balancing with it. However, CGRec underperformed relative to the (B) \ud835\udc64 / \ud835\udc5c SC-MIM variant (LC-NTG+ACMoE variant) for all domains in both datasets. In addition, to derive domain-specific NTG, CGRec needs to compute the losses of all combinations of domains, requiring O(|D| ! ) time complexity, whereas our model only computes the losses for two tasks, SDSR and CDSR, requiring only O(|D|) time complexity. This indicates that the proposed loss correction approach is more efficient than the Shapley value based computation in CGRec.", "5.6 Hyper-parameter Analysis (RQ4)": "As shown in Fig. 3, to assess how different hyper-parameter configurations affect our model, we performed experiments on two datasets using various configurations of crucial hyper-parameters, the model dimensionality ( \ud835\udc5f ) and the number of experts ( \ud835\udc3e ) in ACMoE. The other hyperparameters are set to the same values as those mentioned in Section 5.2.3. (1) Model dimensionality \ud835\udc5f : We observed that when \ud835\udc5f rises from 8 to 16, there is an improvement in the average performance of all domains on the Amazon dataset. For the Telco dataset, there is an improvement in the average performance of all domains when \ud835\udc5f rises from 8 to 64. From the results, we confirmed that the threshold for \ud835\udc5f depends on the characteristics of the datasets. The number of interactions of Telco is larger than that of the Amazon dataset, and thus a higher dimensionality is required for the Telco (i.e., 64) than for the Amazon dataset (i.e., 16) to achieve the best performance. (2) Number of Sequential Experts \ud835\udc3e : We experimented with a range of experts in ACMoE, from 2 to 6, and the results are presented in Fig. 3. Under all tested settings, our approach consistently surpassed the best baseline model (second best for each domain), in most domains of the two datasets. This result underscores the robustness of our approach with respect to the configuration of the mixture of experts. We also verified the effects of the number of decoupled experts ( \ud835\udc57 ) for CDSR in ACMoE (Section 4.2.1). In both datasets, when three ( \ud835\udc57 ) out of five experts ( \ud835\udc3e ) are trained by the CDSR task (i.e., the stop gradient is applied to two experts for the CDSR task), the highest performance is achieved for all domains. This indicates that assigning more experts to the challenging CDSR task improves model performance. Due to limited space, a detailed analysis of \ud835\udc57 cannot be provided in this section.", "6 ONLINE A/B TEST (RQ 5)": "We deployed our recommendation model in the personal assistant service of our company. Online experiments were conducted within an A/B testing framework from October 2023 to January 2024. The Figure 3: Hyper-parameter study of our model. 8 16 32 64 128 0.0 0.2 0.4 0.6 0.8 1.0 NDCG@5 Books Clothing Toys Sports Video 8 16 32 64 128 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 NDCG@5 App-Use Call-Use Navi. Coupon-Use e-comm. 2 3 4 5 6 0.0 0.2 0.4 0.6 0.8 NDCG@5 Books Clothing Toys Sports Video 2 3 4 5 6 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 NDCG@5 App-Use Call-Use Navi. Coupon-Use e-comm. Dimensionality r Dimensionality r # Experts K # Experts K Amazon Telco Amazon Telco serving model used during this period was a variant of our model, adapted to suit business characteristics. Our model was trained on sequential logs of approximately 12 million users from nine domains, including the personal assistant service domain. It recommends the Top @1 next item within the personal assistant service domain to the customer. Our primary metric for online evaluation is the Click-Through Rate (CTR), defined as the ratio of clicks to impressions. Results from the online A/B test revealed that our model achieved 17.3% and 25.6% increases in online CTR compared to the SASRec-based [9] model and rule-based model, respectively, affirming the practical effectiveness of our model.", "7 CONCLUSION": "We introduce a CDSR framework that tackles negative transfer by dynamically applying a weight to the prediction loss, which is determined based on the estimated evaluation of negative transfer. Additionally, we developed an auxiliary loss to enhance the exchange of valuable information between SDSR and CDSR tasks on a per-domain basis. Our model outperformed existing models in extensive tests on two real-world datasets with ten domains. Its implementation in our personal assistant app's recommendation system yielded a significant increase in click-through rate, demonstrating substantial business value. SIGIR '24, July 14-18, 2024, Washington, DC, USA Park Chung, et al.", "REFERENCES": "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450 (2016). [2] Rianne van den Berg, Thomas N Kipf, and Max Welling. 2017. Graph convolutional matrix completion. arXiv preprint arXiv:1706.02263 (2017). [3] Jiangxia Cao, Xin Cong, Jiawei Sheng, Tingwen Liu, and Bin Wang. 2022. Contrastive Cross-Domain Sequential Recommendation. In Proceedings of the 31st ACMInternational Conference on Information & Knowledge Management . 138-147. [4] Xinyan Fan, Zheng Liu, Jianxun Lian, Wayne Xin Zhao, Xing Xie, and Ji-Rong Wen. 2021. Lighter and better: low-rank decomposed self-attention networks for next-item recommendation. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval . 1733-1737. [5] Sheng Gao, Hao Luo, Da Chen, Shantao Li, Patrick Gallinari, and Jun Guo. 2013. Cross-domain recommendation via cluster-level latent factor model. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2013, Prague, Czech Republic, September 23-27, 2013, Proceedings, Part II 13 . Springer, 161-176. [6] Ruining He, Wang-Cheng Kang, and Julian McAuley. 2017. Translation-based recommendation. In Proceedings of the eleventh ACM conference on recommender systems . 161-169. [7] Dan Hendrycks and Kevin Gimpel. 2016. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415 (2016). [8] Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2015. Session-based recommendations with recurrent neural networks. arXiv preprint arXiv:1511.06939 (2015). [9] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recommendation. In 2018 IEEE international conference on data mining (ICDM) . IEEE, 197-206. [10] Lingpeng Kong, Cyprien de Masson d'Autume, Wang Ling, Lei Yu, Zihang Dai, and Dani Yogatama. 2019. A mutual information maximization perspective of language representation learning. arXiv preprint arXiv:1910.08350 (2019). [11] Chenglin Li, Yuanzhen Xie, Chenyun Yu, Bo Hu, Zang Li, Guoqiang Shu, Xiaohu Qie, and Di Niu. 2023. One for All, All for One: Learning and Transferring User Embeddings for Cross-Domain Recommendation. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining . 366-374. [12] Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Tao Lian, and Jun Ma. 2017. Neural attentive session-based recommendation. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management . 1419-1428. [13] Guanyu Lin, Chen Gao, Yu Zheng, Jianxin Chang, Yanan Niu, Yang Song, Kun Gai, Zhiheng Li, Depeng Jin, Yong Li, et al. 2023. Mixed Attention Network for Cross-domain Sequential Recommendation. arXiv preprint arXiv:2311.08272 (2023). [14] Meng Liu, Jianjun Li, Guohui Li, and Peng Pan. 2020. Cross domain recommendation via bi-directional transfer graph collaborative filtering networks. In Proceedings of the 29th ACM international conference on information & knowledge management . 885-894. [15] Qiao Liu, Yifu Zeng, Refuoe Mokhosi, and Haibin Zhang. 2018. STAMP: shortterm attention/memory priority model for session-based recommendation. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1831-1839. [16] Muyang Ma, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Lifan Zhao, Peiyu Liu, Jun Ma, and Maarten de Rijke. 2022. Mixed information flow for cross-domain sequential recommendations. ACM Transactions on Knowledge Discovery from Data (TKDD) 16, 4 (2022), 1-32. [17] Muyang Ma, Pengjie Ren, Yujie Lin, Zhumin Chen, Jun Ma, and Maarten de Rijke. 2019. \ud835\udf0b -net: A parallel information-sharing network for shared-account crossdomain sequential recommendations. In Proceedings of the 42nd international ACM SIGIR conference on research and development in information retrieval . 685694. [18] Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. 2015. Image-based recommendations on styles and substitutes. In Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval . 43-52. [19] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018). [20] Chung Park, Taesan Kim, Taekyoon Choi, Junui Hong, Yelim Yu, Mincheol Cho, Kyunam Lee, Sungil Ryu, Hyungjun Yoon, Minsung Choi, et al. 2023. Cracking the Code of Negative Transfer: A Cooperative Game Theoretic Approach for Cross-Domain Sequential Recommendation. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management . 2024-2033. [21] Zhen Qin, Yicheng Cheng, Zhe Zhao, Zhe Chen, Donald Metzler, and Jingzheng Qin. 2020. Multitask mixture of sequential experts for user activity streams. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 3083-3091. [22] Ahmed Rashed, Shereen Elsayed, and Lars Schmidt-Thieme. 2022. Context and attribute-aware sequential recommendation via cross-attention. In Proceedings of the 16th ACM Conference on Recommender Systems . 71-80. [23] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2012. BPR: Bayesian personalized ranking from implicit feedback. arXiv preprint arXiv:1205.2618 (2012). [24] Ajit P Singh and Geoffrey J Gordon. 2008. Relational learning via collective matrix factorization. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining . 650-658. [25] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer. In Proceedings of the 28th ACM international conference on information and knowledge management . 1441-1450. [26] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. 2016. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition . 2818-2826. [27] Qiaoyu Tan, Jianwei Zhang, Jiangchao Yao, Ninghao Liu, Jingren Zhou, Hongxia Yang, and Xia Hu. 2021. Sparse-interest network for sequential recommendation. In Proceedings of the 14th ACM international conference on web search and data mining . 598-606. [28] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems . 5998-6008. [29] Zirui Wang, Zihang Dai, Barnab\u00e1s P\u00f3czos, and Jaime Carbonell. 2019. Characterizing and avoiding negative transfer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition . 11293-11302. [30] Huan Yan, Xiangning Chen, Chen Gao, Yong Li, and Depeng Jin. 2019. Deepapf: Deep attentive probabilistic factorization for multi-site video recommendation. TC 2, 130 (2019), 17-883. [31] Fajie Yuan, Alexandros Karatzoglou, Ioannis Arapakis, Joemon M Jose, and Xiangnan He. 2019. A simple convolutional generative network for next item recommendation. In Proceedings of the twelfth ACM international conference on web search and data mining . 582-590. [32] Tingting Zhang, Pengpeng Zhao, Yanchi Liu, Victor S Sheng, Jiajie Xu, Deqing Wang, Guanfeng Liu, Xiaofang Zhou, et al. 2019. Feature-level Deeper SelfAttention Network for Sequential Recommendation.. In IJCAI . 4320-4326. [33] Wei Zhang, Pengye Zhang, Bo Zhang, Xingxing Wang, and Dong Wang. 2023. A Collaborative Transfer Learning Framework for Cross-domain Recommendation. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 5576-5585. [34] Wayne Xin Zhao, Yupeng Hou, Xingyu Pan, Chen Yang, Zeyu Zhang, Zihan Lin, Jingsen Zhang, Shuqing Bian, Jiakai Tang, Wenqi Sun, et al. 2022. RecBole 2.0: Towards a More Up-to-Date Recommendation Library. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management . 4722-4726. [35] Wayne Xin Zhao, Shanlei Mu, Yupeng Hou, Zihan Lin, Kaiyuan Li, Yushuo Chen, Yujie Lu, Hui Wang, Changxin Tian, Xingyu Pan, Yingqian Min, Zhichao Feng, Xinyan Fan, Xu Chen, Pengfei Wang, Wendi Ji, Yaliang Li, Xiaoling Wang, and Ji-Rong Wen. 2021. Recbole: Towards a unified, comprehensive and efficient framework for recommendation algorithms. In CIKM . [36] Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, and Ji-Rong Wen. 2020. S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization. In Proceedings of the 29th ACM international conference on information & knowledge management . 1893-1902. [37] Feng Zhu, Chaochao Chen, Yan Wang, Guanfeng Liu, and Xiaolin Zheng. 2019. Dtcdr: A framework for dual-target cross-domain recommendation. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management . 1533-1542. [38] Feng Zhu, Yan Wang, Chaochao Chen, Jun Zhou, Longfei Li, and Guanfeng Liu. 2021. Cross-domain recommendation: challenges, progress, and prospects. arXiv preprint arXiv:2103.01696 (2021)."}
