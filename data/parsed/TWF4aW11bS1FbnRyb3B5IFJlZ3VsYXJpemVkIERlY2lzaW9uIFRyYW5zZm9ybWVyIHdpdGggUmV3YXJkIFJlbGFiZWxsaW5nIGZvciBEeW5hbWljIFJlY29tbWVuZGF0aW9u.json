{"Maximum-Entropy Regularized Decision Transformer with Reward Relabelling for Dynamic Recommendation": "Xiaocong Chen Data 61, CSIRO Eveleigh, Australia xiaocong.chen@data61.csiro.au Siyu Wang The University of New South Wales Sydney, Australia siyu.wang5@unsw.edu.au Lina Yao Data 61, CSIRO Eveleigh, Australia The University of New South Wales Sydney, Australia lina.yao@data61.csiro.au", "ABSTRACT": "Reinforcement learning-based recommender systems have recently gained popularity. However, due to the typical limitations of simulation environments (e.g., data inefficiency), most of the work cannot be broadly applied in all domains. To counter these challenges, recent advancements have leveraged offline reinforcement learning methods, notable for their data-driven approach utilizing offline datasets. A prominent example of this is the Decision Transformer. Despite its popularity, the Decision Transformer approach has inherent drawbacks, particularly evident in recommendation methods based on it. This paper identifies two key shortcomings in existing Decision Transformer-based methods: a lack of stitching capability and limited effectiveness in online adoption. In response, we introduce a novel methodology named Max-Entropy enhanced Decision Transformer with Reward Relabeling for Offline RLRS (EDT4Rec). Our approach begins with a max entropy perspective, leading to the development of a max-entropy enhanced exploration strategy. This strategy is designed to facilitate more effective exploration in online environments. Additionally, to augment the model's capability to stitch sub-optimal trajectories, we incorporate a unique reward relabeling technique. To validate the effectiveness and superiority of EDT4Rec, we have conducted comprehensive experiments across six real-world offline datasets and in an online simulator.", "KEYWORDS": "Offline Reinforcement Learning, Recommender Systems, Deep Learning", "1 INTRODUCTION": "Reinforcement Learning (RL)-based Recommender Systems (RS) have emerged as powerful tools in a variety of domains, ranging from e-commerce [3, 15] and advertising [2] to streaming services. Their effectiveness is particularly pronounced in dynamic environments, where users' interests are dynamic [9]. In RLRS, an agent interacts with users by recommending items and receiving feedback. This feedback, typically in the form of user responses to recommendations, serves as rewards that inform the agent's decisions. Utilizing this feedback loop, the agent continually refines its policy to better align with user preferences, ultimately aiming to maximize long-term rewards. This goal often translates into enhancing user click with the system. Despite their proven utility, RLRS frequently suffers data inefficiency, a challenge inherent to the interaction-centric nature of RL algorithms. This inefficiency arises as the systems must learn from limited user interactions, making the process of policy improvement slower and less robust. Recent studies have proposed using offline RL to address the challenges faced by RL-based Recommender Systems (RS) [7, 24, 26]. Known as data-driven RL, offline RL [17] leverages extensive offline datasets for the preliminary training of agents, allowing for the utilization of pre-existing data to train RLRS agents. A notable implementation is CDT4Rec [24], which integrates the Decision Transformer(DT) [4] as its foundational structure, coupled with a causal mechanism for reward estimation. Similarly, DT4Rec, proposed by Zhao et al. [26], utilizes DT to focus on capturing user retention, implementing an efficient reward prompting method for RLRS. However, directly applying the DT to RS still has several obstacles which require further investigation. The DT requires an offline dataset that contains sufficient expert trajectories (i.e., those trajectories should be optimal and dense) that can cover almost all of the possibilities that the agent may face when interacting with the real environment. It will lead to two problems when applying the DT algorithm in RS. Firstly, in RS, the offline dataset is highly sparse [8] which may not be able to generate enough expert trajectories. Under such a scenario, the DT needs to have the capability to learn from the sub-optimal trajectories. However, recent literature [7, 18, 21, 25, 29] indicate that the vanilla DT lacks the capability of learning from sub-optimal trajectories (i.e., stitching). To better understand the stitching capability in DT, we have provided an illustration example under recommendation scenario in Figure 1 to explain. Consider a user has three previous trajectories consisting of item click sequences: ( \ud835\udc56 1 , \ud835\udc56 2 , \ud835\udc56 3 , \ud835\udc56 4 , \ud835\udc56 8 ) , ( \ud835\udc56 1 , \ud835\udc56 2 , \ud835\udc56 6 , \ud835\udc56 7 ) , and ( \ud835\udc56 1 , \ud835\udc56 2 , \ud835\udc56 3 , \ud835\udc56 5 ) . In this illustration, each node represents an item. Notably, the trajectory ( \ud835\udc56 1 , \ud835\udc56 2 , \ud835\udc56 3 , \ud835\udc56 4 , \ud835\udc56 8 ) receives a reward of 0 (i.e., the user does not click it), whereas a similar trajectory ( \ud835\udc56 1 , \ud835\udc56 2 , \ud835\udc56 6 , \ud835\udc56 7 ) yields a positive reward. We categorize the 0 reward scenario as sub-optimal. Upon closer examination, we observe a linkage between items \ud835\udc56 4 and \ud835\udc56 7 , suggesting the potential to create a more rewarding trajectory by combining elements from ( \ud835\udc56 1 , \ud835\udc56 2 , \ud835\udc56 3 , \ud835\udc56 4 , \ud835\udc56 8 ) and ( \ud835\udc56 1 , \ud835\udc56 2 , \ud835\udc56 6 , \ud835\udc56 7 ) . For instance, a newly synthesized trajectory ( \ud835\udc56 1 , \ud835\udc56 2 , \ud835\udc56 3 , \ud835\udc56 4 , \ud835\udc56 7 ) is predicted to achieve a positive reward. Our reward relabeling strategy addresses this by assigning weights to each node, ensuring the agent avoids less rewarding paths like reaching node \ud835\udc56 8 . Moreover, since the DT believes that all of the possibilities are covered in the offline dataset, the exploration will be abandoned when fine-tuning the online environments [18]. It will be crucial when applying to the RS, since the users' interests are dynamic and rapidly changing, and the recorded offline dataset can not reflect users' intentions completely. The agent still needs to conduct certain steps of exploration to collect trajectories as suggested by recent works [5, 9]. These limitations hinder the agent's performance, particularly when working with sub-optimal offline datasets that do not fully encapsulate the range of possible user actions, a situation more akin to real-world scenarios. To address those two challenges, in this study, we introduce a novel model, named Max-Entropy enhanced Decision Transformer with Reward Relabeling for Offline RLRS (EDT4Rec), to overcome the identified challenges in RL-based Recommender Systems. Drawing inspiration from the Soft Actor-Critic (SAC) approach [14], EDT4Rec incorporates the concept of max-entropy exploration. It also introduces an innovative reward relabeling strategy, whereby each node in a trajectory is assigned a reward to foster the generation of more optimal trajectories and facilitate the learning of a more effective recommendation policy 1 . Our contributions in this work are threefold: \u00b7 We propose EDT4Rec, a novel model that integrates maxentropy exploration and a unique reward relabeling strategy, enhancing the offline RLRS framework. \u00b7 To address the stitching problem in DT, we design a novel reward relabeling strategy. Moreover, we design a new maxentropy exploration mechanism to enable the agent can conduct the exploration when fine-tuning on the online environments. \u00b7 We validate the efficacy of EDT4Rec through extensive experiments on six public datasets and in an online simulation environment, demonstrating its superiority over existing methods.", "2 PROBLEM FORMULATION": "The recommendation problem can be conceptualized as an agent striving to achieve a specific goal through learning from user interactions, such as item recommendations and subsequent feedback. This scenario is aptly formulated as a RL problem, where the agent is trained to interact within an environment, typically described as a Markov Decision Process (MDP) [9]. The components of an MDP are represented as a tuple (S , A , P , R , \ud835\udefe ) where, \u00b7 State S : The state space, with \ud835\udc60 \ud835\udc61 \u2208 S representing the state at timestep \ud835\udc61 , which normally contain users' previous interest, users' demographic information etc. \u00b7 Action A : The action space, where \ud835\udc4e \ud835\udc61 \u2208 A( \ud835\udc60 \ud835\udc61 ) . \ud835\udc4e \ud835\udc61 is the action taken when given a state \ud835\udc60 \ud835\udc61 . It normally refers to recommended items. \u00b7 Transition Probability P : denoted as \ud835\udc5d ( \ud835\udc60 \ud835\udc61 + 1 | \ud835\udc60 \ud835\udc61 , \ud835\udc4e \ud835\udc61 ) \u2208 P , is the probability of transitioning from \ud835\udc60 \ud835\udc61 to \ud835\udc60 \ud835\udc61 + 1 when action \ud835\udc4e \ud835\udc61 is taken. \u00b7 Reward R : S \u00d7 A \u2192 R is the reward distribution, where \ud835\udc5f ( \ud835\udc60, \ud835\udc4e ) indicates the reward received for taking action \ud835\udc4e when observing the state \ud835\udc60 . \u00b7 Discount-rate Parameter \ud835\udefe : \ud835\udefe \u2208 [ 0 , 1 ] is the discount factor which use to balance previous reward and immediate reward. In this context, an RL agent follows its policy \ud835\udf0b , a mapping from states to actions to make recommendations. The objective of RL is expressed as: Offline reinforcement learning diverges from traditional RL by exclusively utilizing pre-collected data for training, without the necessity for further online interaction [17]. In offline RL, the agent is trained to maximize total rewards relies on a static dataset of transitions D for learning. This dataset could comprise previously collected data or human demonstrations. Consequently, the agent in offline RL lacks the capability to explore and interact with the environment for additional data collection. The dataset D in an offline RL-based RS can be formally described as {( \ud835\udc60 \ud835\udc62 \ud835\udc61 , \ud835\udc4e \ud835\udc62 \ud835\udc61 , \ud835\udc60 \ud835\udc62 \ud835\udc61 + 1 , \ud835\udc5f \ud835\udc62 \ud835\udc61 )} , adhering to the MDP framework (S , A , P , R , \ud835\udefe ) . For each user \ud835\udc62 at timestep \ud835\udc61 , the dataset includes the current state \ud835\udc60 \ud835\udc62 \ud835\udc61 \u2208 S , the items recommended by the agent via action \ud835\udc4e \ud835\udc62 \ud835\udc61 , and the user feedback \ud835\udc5f \ud835\udc62 \ud835\udc61 .", "3 RELATED WORK": "RL-basedRecommenderSystems. Reinforcement learning-based recommendation methods approach the interactive process of making recommendations as a MDP [9]. This approach can be categorized into two primary branches: model-based and model-free methods. In the realm of model-based techniques, Bai et al. [1] introduced a method employing generative adversarial training to simultaneously learn the user behavior model and update the recommendation policy. Recently, there has been a noticeable shift in the literature towards embracing model-free techniques for reinforcement learning-based recommendations. Notably, Zheng et al. [28] pioneered the introduction of RL to enhance news recommender systems, and Zhao et al. [27] further extended its application to the page-wise recommendation scenario. Both of these approaches employ Deep Q-Networks (DQN) [20] to encode user and item information, effectively improving the quality of news recommendations. Moreover, Chen et al. [6] incorporated knowledge graphs into the reinforcement learning framework, resulting in enhanced decision-making efficiency. Additionally, Chen et al. [10] introduced a novel generative inverse reinforcement learning approach for online recommendations. This method autonomously extracts a reward function from user behavior, further advancing state-of-the-art in online recommendation systems. Offline RL in RS. Recent studies have started exploring the prospect of integrating offline RLRS [7]. Notably, Wang et al. [24] introduced a novel model called the Causal Decision Transformer for RS (CDT4Rec). This model incorporates a causal mechanism designed to estimate the reward function, offering promising insights into the offline RL-RS synergy. Similarly, Zhao et al. [26] introduced the Decision Transformer for RS (DT4Rec), utilizing the vanilla Decision Transformer as its core architecture to provide recommendations, demonstrating its potential in the field. Furthermore, Gao et al. [12] delved into the examination of the Matthew effect within offline RL in Recommender Systems. This work contributes to our understanding of how offline RLRS can mitigate or address this phenomenon.", "4 METHODOLOGY": "", "4.1 Decision Transformer and Overall Model Architecture": "In this work, we proposed a max-Entropy enhanced Decision Transformer with reward relabeling for offline RLRS(EDT4Rec). Our proposed EDT4Rec leverages the DT framework [4], a pivotal element in recent advancements in offline RLRS, as seen in works like [7, 24, 26] The Decision Transformer processes a trajectory, denoted as \ud835\udf0f , by treating it as a sequence comprising three distinct types of input: return-to-go (RTG), states, and actions, represented as ( \ud835\udc54 1 , \ud835\udc60 1 , \ud835\udc4e 1 , \u00b7 \u00b7 \u00b7 , \ud835\udc54 | \ud835\udf0f | , \ud835\udc60 | \ud835\udf0f | , \ud835\udc4e | \ud835\udf0f | ) . Specifically, the initial RTG \ud835\udc45 1 , corresponds to the trajectory's total return, calculated as \u02dd | \ud835\udf0f | \ud835\udc56 = 0 \ud835\udc5f \ud835\udc56 . At each timestep \ud835\udc61 , DT utilizes tokens from the most recent \ud835\udc3e timesteps to generate an action \ud835\udc4e \ud835\udc61 . This \ud835\udc3e is a hyperparameter, defines the context length the transformer uses for its computations. The deterministic policy learned by DT, denoted as \ud835\udf0b \ud835\udc37\ud835\udc47 ( \ud835\udc4e \ud835\udc61 | s \ud835\udc3e,\ud835\udc61 , g \ud835\udc3e,\ud835\udc61 ) , where s \ud835\udc3e,\ud835\udc61 is shorthand for the sequence of \ud835\udc3e past states s max ( 1 ,\ud835\udc61 -\ud835\udc3e + 1 ) : \ud835\udc61 and similarly for g \ud835\udc3e,\ud835\udc61 . A crucial aspect of DT in this context is the application of a causal mask to predict the sequence of actions. To simplify the following analysis, we postulate that the data distribution (i.e., for the dataset), denoted as T , produces subsequences of actions, states, and RTG values, each of length \ud835\udc3e , originating from the same trajectory. To facilitate our explanation, we slightly diverge from standard notation and represent a sample from T as ( a , s , g ) where all the vectors contain subelements. This notation choice simplifies the presentation of our training objective. The core of DT training strategy involves instructing the policy to accurately predict action tokens. This prediction adheres to the standard \ud835\udc59 2 loss, which is mathematically represented as: This equation encapsulates the expectation of the mean squared difference between the actual action tokens and those predicted by the DT policy, averaged over all \ud835\udc3e elements in the subsequence. In practice, to implement this approach within the framework of an offline dataset D , we uniformly sample these length\ud835\udc3e subsequences. In EDT4Rec, we have introduced the following two key modifications: max-entropy enhanced Exploration and RTG relabeling. The existing approach of training policies in decision-transformer-based offline RLRS [7, 24] on purely offline datasets often encounters a critical limitation: the trajectories in these datasets usually do not yield high returns and cover only a restricted portion of the state space. This limitation stems from two parts: i). the potential distribution difference between the offline dataset and the online environment and, ii). the sub-optimality of the training data available offline. A straightforward strategy to mitigate the distribution difference is the fine-tuning of pre-trained RL agents through online interactions. However, the learning formulation of a standard decision transformer does not naturally lend itself to effective online learning. Moreover, in scenarios with sub-optimal trajectories, the lack of informative rewards (often zero) hinders effective policy updates.", "4.2 Max-Entropy Enhanced Exploration": "In the initial phase of our methodology, we propose a probabilistic learning objective, intended to augment exploration within the standard DT framework. This setup is geared towards developing a stochastic policy, with the primary goal of maximizing the likelihood of the observed dataset. For contexts involving continuous action spaces, we opt for the conventional approach of using a multivariate Gaussian distribution with a diagonal covariance matrix. This distribution models the action probabilities, conditional upon states and RTG values. The policy parameters are represented by \ud835\udf03 , where \u02dd \ud835\udf03 denotes the diagonal covariance matrix. Consider a stochastic policy, we aim to maximize the log-likelihood of the trajectories present in our training dataset. This objective is equivalently achieved by minimizing the negative log-likelihood (NLL) loss. The loss function, \ud835\udc3d ( \ud835\udf03 ) , is defined as follows: It's important to note that the policies considered here encompass the deterministic policies utilized by the standard DT. Optimizing Equation (1) is effectively equivalent to optimizing Equation (3), under the assumption that the covariance matrix \u02dd \ud835\udf03 is diagonal and the variances remain constant across all dimensions. This scenario represents a special case encompassed by our assumption. A crucial aspect of an online RL algorithm is its capacity to balance exploration and exploitation. However, the traditional formulation of a DT, as seen in Equation (3), does not inherently facilitate exploration. To remedy this, we can use the policy entropy to encourage exploration, defined as: where \ud835\udc3b [ \ud835\udf0b \ud835\udf03 ( \ud835\udc4e \ud835\udc58 )] denotes the entropy of the distribution \ud835\udf0b \ud835\udf03 ( \ud835\udc4e \ud835\udc58 ) . This policy entropy is contingent on the data distribution T , which remains static during the offline pretraining phase but becomes RTG Relabling Causal Decision Transformer RTG Relabling RTG Relabling Causal Decision Transformer OFFLINE DATASET OFFLINE ONLINE ONLINE ENVIRONMENT interaction k dynamic in the finetuning phase as it incorporates online data acquired through exploration. In line with max-entropy RL algorithms like SAC [14], we explicitly impose a lower bound on the policy entropy to encourage exploration. We aim to solve the following constrained problem: the current policy \ud835\udf0b \ud835\udf03 and the transition probability P . In the offline training phase, T is a fixed offline data distribution, T \ud835\udc5c\ud835\udc53 \ud835\udc53 \ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52 , while during the online phase, T is represented through a replay buffer that stores online data and is dependent on the current policy \ud835\udf0b \ud835\udc61 \u210e\ud835\udc52\ud835\udc61\ud835\udc4e and previously gathered data. where \ud835\udefd is a predetermined hyperparameter. Practically, we address the dual problem of Equation (5) to avoid the direct handling of the inequality constraint. Thus, we consider the Lagrangian, The solution involves alternating optimization of \ud835\udf03 and \ud835\udf06 . Specifically, optimizing \ud835\udf03 with fixed \ud835\udf06 is is achieved by, and optimizing \ud835\udf06 with a fixed \ud835\udf03 , Next, we will conduct a theoretical analysis to delineate how our EDT4Rec differs from SAC. Equation (7) represents the regularized form of our primal problem as outlined in Equation (6), where the dual variable \ud835\udf06 assumes the role of a temperature variable, a common element in many regularized RL formulations. A fundamental distinction of our approach from SAC and other traditional RL methods lies in our loss function \ud835\udc3d ( \ud835\udf03 ) , which is defined as the negative log-likelihood (NLL) rather than the discounted return. This shift signifies a focus on supervised learning of action sequences rather than explicitly aiming to maximize returns. Therefore, the objective in Equation (7) can be interpreted as minimizing the expected discrepancy between the log-probability of observed actions and the \ud835\udf06 -scaled log-probability of actions sampled from \ud835\udf0b \ud835\udf03 (\u00b7| s , g ) . In essence, we aim to train \ud835\udf0b \ud835\udf03 to approximate the observed action distribution, allowing for a controlled degree of deviation, with \ud835\udf06 explicitly regulating this mismatch. It's important to note that \ud835\udc3b T \ud835\udf03 [ a | s , g ] is technically a cross conditional entropy. This arises because the training data distribution T is not identical to the action-state-RTG marginals induced by It is worth mentioning that our policy entropy, as defined in Equation (4), operates at the sequence level rather than at the transition level. This distinction leads to a significant difference in the constraints applied in our primal problem (Equation (6)) compared to those used in the SAC framework. For the sake of simplicity, if we momentarily set aside the RTG variable g in our policy entropy calculation, we can see a clear contrast. While SAC imposes a lower bound \ud835\udefd on the policy entropy at each timestep, our entropy \ud835\udc3b T \ud835\udf03 [ a | s , g ] is computed as an average over \ud835\udc3e consecutive timesteps.", "4.3 RTG Relabeling": "The reward conditioning approach prevalent in prior works typically involves conditioning on an entire trajectory sequence using the sum of rewards for that sequence (i.e., \u02dd | \ud835\udf0f | \ud835\udc56 = 0 \ud835\udc5f \ud835\udc56 ). However, this method encounters difficulties in tasks that require stitching - the process of learning an optimal policy from sub-optimal trajectories by effectively combining them. By contrast, the Q-learning approach addresses this by independently propagating the value function backwards at each timestep using the Bellman backup. It integrates information for each state across different trajectories, thereby circumventing the stitching issue inherent in the reward conditioning approach. Our proposed solution to this challenge involves a novel approach: relabeling the RTG values using learned Q-functions. By applying this relabeling to the dataset, the reward conditioning approach can then effectively utilize optimal segments from within sub-optimal trajectories. This enhancement allows for the extraction and integration of valuable sub-trajectory segments, thus addressing the fundamental limitation of the original reward conditioning method in tasks requiring effective stitching of policy learning. In addressing the relabeling of RTG values with learned Qfunctions, it's important to recognize that simply substituting all RTG values with Q-function estimates is not always effective. This is particularly true in scenarios characterized by long time horizons and sparse rewards, where the accuracy of learned Q-functions can vary significantly. Our goal is to selectively replace RTG values in instances where the learned Q-functions provide reliable estimations. To achieve this, we incorporate the CQL framework [16], renowned for learning the lower bounds of value functions in offline Q-learning algorithms. The relabeling of RTG values is conducted selectively: it occurs when the RTG within a trajectory is lower than the lower bound estimated by CQL. This approach ensures that RTG values are replaced only when the learned value function is reasonably accurate or closer to the true value. Furthermore, our method extends the impact of this relabeling process through the trajectory. We apply reward recursion, defined as \ud835\udc45 \ud835\udc61 -1 = \ud835\udc5f \ud835\udc61 -1 + \ud835\udc45 \ud835\udc61 , to propagate the revised RTG values to all preceding timesteps in the trajectory. This ensures that the influence of the adjusted RTG is not isolated but is instead integrated throughout the trajectory, enhancing the overall consistency and accuracy of our approach. To implement our approach, we begin by initializing the RTG of the final state in a trajectory to zero, denoted as \ud835\udc45 \ud835\udc47 = 0. Then, we proceed in reverse chronological order, starting from the end of the trajectory and moving backwards towards the initial state. The process involves a series of steps at each state in the trajectory. Firstly, for the current state \ud835\udc60 \ud835\udc61 , we compute its state value using the learned value function, expressed as \u02c6 \ud835\udc49 ( \ud835\udc60 \ud835\udc61 ) = E \ud835\udc4e \u223c \ud835\udf0b ( \ud835\udc4e | \ud835\udc60 \ud835\udc61 ) [ \u02c6 \ud835\udc44 ( \ud835\udc60 \ud835\udc61 , \ud835\udc4e )] , where \ud835\udf0b represents the learned policy. This value function estimation reflects the expected value of actions taken in the current state according to the policy. Next, we compare this computed value function, \u02c6 \ud835\udc49 ( \ud835\udc60 \ud835\udc61 ) , with the existing RTG value at the same state (i.e., \ud835\udc45 ( \ud835\udc61 ) ). If \u02c6 \ud835\udc49 ( \ud835\udc60 \ud835\udc61 ) is greater than \ud835\udc45 \ud835\udc61 , we update the RTG for the previous timestep \ud835\udc45 \ud835\udc61 -1 to the sum of the reward at timestep \ud835\udc61 -1, \ud835\udc5f \ud835\udc61 -1 and the estimated state value \u02c6 \ud835\udc49 ( \ud835\udc60 \ud835\udc61 ) . If not, \ud835\udc45 \ud835\udc61 -1 is set to \ud835\udc5f \ud835\udc61 -1 + \ud835\udc45 \ud835\udc61 . This relabeling process is repeated for each state in the trajectory until we reach the initial state. Through this backward iteration, we effectively update the RTG values based on the learned value function, ensuring that the RTG reflects a more accurate estimate of the expected returns from each state. The relabeling process described earlier, while effective, can potentially disrupt the inherent consistency between rewards and RTG values in the input sequence of a Decision Transformer (DT). Ideally, the RTG value should always align with the sum of future rewards, adhering to the relationship \ud835\udc45 \ud835\udc61 = \ud835\udc5f \ud835\udc61 + \ud835\udc45 \ud835\udc61 + 1 . However, the relabeling process may inadvertently violate this rule. To preserve this critical consistency within the DT input sequence, we adopt a method of regenerating the RTG values. For an input sequence { \u02c6 \ud835\udc45 \ud835\udc61 -\ud835\udc3e + 1 , \u00b7 \u00b7 \u00b7 , \u02c6 \ud835\udc45 \ud835\udc61 -1 , \u02c6 \ud835\udc45 \ud835\udc61 } , we start by setting the last RTG \u02c6 \ud835\udc45 \ud835\udc61 = \ud835\udc45 \ud835\udc61 . Then, we apply the formula \u02c6 \ud835\udc45 \ud835\udc61 \u2032 = \ud835\udc5f \u2032 \ud835\udc61 + \u02c6 \ud835\udc45 \ud835\udc61 \u2032 + 1 in a backward sequence until we reach \ud835\udc61 \u2032 = \ud835\udc61 -\ud835\udc3e + 1. By doing so, we maintain the consistency between rewards and RTGs throughout the sequences. The comprehensive algorithm of RTG relabeling process can be found in Algorithm 1. Apotential concern with our relabeling strategy is validating that the learned value function indeed represents the lower bound of the true value function, meaning it should be as close as possible to the optimal value function. To address this, we utilize the CQL [16]", "Algorithm 1: RTG Relabeling without inconsistency": "input : reward \ud835\udc5f \u02c6 1: \ud835\udc47 , learned value function \ud835\udc49 ( \ud835\udc60 ) , Trajectory length \ud835\udc47 , context length \ud835\udc3e RTG \u02c6 \ud835\udc45 1: \ud835\udc47 1. relabel RTG \ud835\udc45 \ud835\udc47 output: // Step 1 \ud835\udc45 \ud835\udc47 \u2190 0; 2 \ud835\udc56 \u2190 \ud835\udc47 ; 3 while i > 0 do 4 \ud835\udc45 \ud835\udc61 -1 \u2190 \ud835\udc5f \ud835\udc56 + max ( \ud835\udc45 \ud835\udc61 , \u02c6 \ud835\udc49 ( \ud835\udc60 \ud835\udc56 ) ; 5 \ud835\udc56 \u2190 \ud835\udc56 - 1;", "6 end": "// Step 2. relabel RTG \u02c6 \ud835\udc45 \ud835\udc47 by considering the inconsistency to learn the Q-value which is defined as, Supporting this approach, we reference a theorem from the CQL framework (the proof of which can be found in the original paper): Theorem 1 (Lower Bound of CQL [16]). The value of the policy under the Q-function from Equation (9) , \u02c6 \ud835\udc49 \ud835\udf0b ( s ) = E \ud835\udf0b ( a | s ) [ \u02c6 \ud835\udc44 \ud835\udf0b ( s , a )] , lower-bounds the true value of the policy obtained via exact policy evaluation. In practice, the optimal value function \ud835\udc44 \u2217 ( \ud835\udc60, \ud835\udc4e ) is unknown, compelling DT to rely on RTG as an alternative. RTG values, typically gathered through the behavior policy or policies, are often not optimal and tend to be significantly lower than their optimal value function counterparts, \ud835\udc44 \u2217 ( \ud835\udc60, \ud835\udc4e ) This discrepancy presents a challenge in accurately modeling the optimal policy. However, by employing CQL for learning the Q-function, we can effectively address this challenge. The aforementioned theorem from the CQL framework assures that the value function estimated under CQL acts as a reliable lower bound of the true policy value. Consequently, this theorem supports the premise that our relabeling process, guided by the learned Q-function, shifts the RTG values in the training dataset closer to the optimal value function.To further substantiate this claim, we also provide a straightforward proof. This proof demonstrates how the application of CQL in learning the Q-function ensures that the relabeled RTG values in our training dataset are more aligned with what would be expected from the optimal value function, thus enhancing the overall accuracy and effectiveness of the DT in policy modeling.", "4.4 Training Procedure": "We implement the aforementioned formulation through a transformer architecture, adapting and extending the existing DT framework. In EDT4RRec, we introduce modifications that align with the changes we've previously outlined. To predict the policy's mean and log variance, we utilize two separate fully connected layers at the output stage of the model. This architectural choice allows for a more nuanced and effective representation of the policy's characteristics. Algorithm 2 summarizes the overall training pipeline in the proposed method.", "Algorithm 2: Overall training algorithm": "1 input : offline dataset D , exploration RTG g \ud835\udc5c\ud835\udc5b\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52 , reply buffer D \ud835\udc5f\ud835\udc52 , number of round \ud835\udc56 , number of iteration \ud835\udc3c , batch size \ud835\udc35 , target policy parameter \ud835\udf03 D \ud835\udc5f\ud835\udc52 \u2190 top \ud835\udc41 trajectories from D ; 2 for round 1 \u00b7 \u00b7 \u00b7 \ud835\udc56 do 3 Trajectory \ud835\udf0f \u2190 \ud835\udf0b \ud835\udf03 (\u00b7| s , g \ud835\udc5c\ud835\udc5b\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52 ) ; // if the D \ud835\udc5f\ud835\udc52 is full, the oldest trajectories will be removed. 4 D \ud835\udc5f\ud835\udc52 \u2190D \ud835\udc5f\ud835\udc52 \u222a \ud835\udf0f ; 5 Compute the trajectory sampling probability \ud835\udc5d ( \ud835\udf0f ) = | \ud835\udf0f |/ \u02dd \ud835\udf0f \u2208D | \ud835\udf0f | ; 6 for \ud835\udc61 = 1 \u00b7 \u00b7 \u00b7 \ud835\udc3c do 7 Sample \ud835\udc35 trajectories from D \ud835\udc5f\ud835\udc52 ; 8 for each trajectory \ud835\udf0f do 9 g \u2190 relabeling using Algorithm 1; 10 ( a , s , g ) \u2190 a length of \ud835\udc3e sub-trajectory uniformly sampled from \ud835\udf0f ; 11 end 12 \ud835\udf03 \u2190 one gradient update using the sampled ( a , s , g ) ; 13 end 14 end In our implementation, both the online goal g \ud835\udc5c\ud835\udc5b\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52 and the context length \ud835\udc3e are set to 2, a decision whose rationale and implications are further explored in our hyperparameter study.", "5 EXPERIMENTS": "In this section, we report the outcomes of experiments that focus on the following three main research questions: \u00b7 RQ1 : How does EDT4Rec compare with other DT-based methods and traditional deep RL algorithms in online recommendation environments and offline dataset environments? \u00b7 RQ2 : How do the hyper-parameters affect the performance in the online simulation environment? \u00b7 RQ3 : How does each component in EDT4Rec contribute to the final performance in the online simulation environment? We concentrate our hyper-parameters and ablation study (i.e., RQ2 and RQ3) on online simulation settings since they are more closely suited to real-world environments, whereas offline datasets are fixed and do not reflect users' dynamic interests.", "5.1 Datasets and Environments": "In this section, we evaluate the performance of our proposed algorithm, EDT4Rec, against other state-of-the-art algorithms, employing both real-world datasets and an online simulation environment. We first introduce six diverse, public real-world datasets from various recommendation domains for our offline experiments: \u00b7 Kuairand-1k-15policies [13]: An unbiased sequential recommendation dataset sourced from recommendation logs of a video-sharing mobile app. \u00b7 LibraryThing : This online service facilitates book cataloging for users 2 . It is notable for its inclusion of social relationships between users, making it ideal for studying social recommendation algorithms. \u00b7 MovieLens : We utilize the MovieLens-20M dataset 3 , a widely recognized benchmark in recommender system research. \u00b7 GoodReads : Sourced from the book review website GoodReads , this dataset, compiled by [23], includes various user interactions like ratings and reviews. \u00b7 Netflix : Originating from the Netflix Prize Challenge 4 , this well-known dataset primarily consists of user ratings. \u00b7 Book-Crossing : Similar to MovieLens, this book-related dataset by [30] focuses on rating information. To evaluate EDT4Rec's performance, we transformed the offline datasets into simulation environments that allow for interactive reinforcement learning experiences. This conversion aligns with methodologies outlined in existing research [6, 24, 26]. Specifically, user interactions are converted into binary click behaviors to construct trajectories. For datasets containing ratings, any rating exceeding 75% of the maximum rating is considered positive feedback, while the rest are treated as negative. The evaluation process adheres to the standards established in previous works [24]. In addition, we also experiment on a real online simulation platform to validate the proposed method. We use VirtualTB [22] as the major online platform in this work. In terms of evaluation metrics, we use the click-through rate (CTR) for the online simulation platform as the CTR is one of the built-in evaluation metrics of the VirtualTB simulation environment. For offline dataset evaluation, we employ a variety of evaluation metrics, including recall, precision, and normalized discounted cumulative gain (nDCG).", "5.2 Baselines": "In our study, we focus on evaluating EDT4Rec within the context of DT-based methods. Most existing works in RLRS have been assessed using customized settings, which makes fair comparison challenging [9]. To address this, we have selected a range of baselines, encompassing both prominent DT-based methods and well-established RL algorithms, for a comprehensive evaluation: \u00b7 Deep Deterministic Policy Gradient (DDPG) [19]: An offpolicy method designed for environments with continuous action spaces. \u00b7 SAC [14]: This approach is an off-policy, maximum entropy Deep RL method that focuses on optimizing a stochastic policy. While we are using deterministic policy here. \u00b7 Twin Delayed DDPG (TD3) [11]: An enhancement of the baseline DDPG, TD3 improves performance by learning two Q-functions, updating the policy less frequently. \u00b7 DT [4]: An offline RL algorithm that leverages the transformer architecture to infer actions. particularly in the later timesteps. Moreover, we observe that DTbased methods outperform traditional RL algorithms. The reason would be those methods accessing more recorded offline trajectories and benefiting from the transformer's high expressiveness. \u00b7 DT4Rec [26]: Building on the standard DT framework, DT4Rec integrates a conservative learning method to better understand users' intentions in offline RLRS. \u00b7 CDT4Rec [24]: This model introduces a causal layer to the DT framework, aiming to more effectively capture user intentions and preferences in offline RLRS.", "5.3 Overall Comparison (RQ1)": "The summarized results of our experiments with the offline dataset are presented in Table 1. Notably, EDT4Rec demonstrates superior performance over all baseline methods, including those DT based offline RLRS and traditional RL approaches. A key observation is the general superiority of DT-based methods over RL-based methods, likely attributable to the transformer framework's expressiveness. In our online simulator experiments, EDT4Rec was compared against the baselines mentioned earlier. The comparative results, as detailed in Figure 3a, reveal a significant improvement by EDT4Rec, However, it's observed that DT, DT4Rec, and CDT4Rec perform worse than EDT4Rec, given that the pre-collected data are suboptimal. This observation reinforces our assertion that DT lacks exploration capability and that its pre-training stage closely resembles behavior cloning.", "5.4 Hyperparameter Study (RQ2)": "In this section, we delve into the hyperparameter analysis for our proposed EDT4Rec model, particularly examining the impacts of g \ud835\udc5c\ud835\udc5b\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52 and \ud835\udc3e . The hyperparameter g \ud835\udc5c\ud835\udc5b\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52 , set at 2 in our experiments, plays a crucial role in regulating exploration. Considering that the maximum reward per interaction is capped (e.g., at 10 in the VirtualTB environment), a smaller g \ud835\udc5c\ud835\udc5b\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52 value is preferable to encourage exploration over exploitation during the fine-tuning phase. The results, illustrating the performance of EDT4Rec with varying g \ud835\udc5c\ud835\udc5b\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52 , are presented in Figure 3b. We observe that the optimal performance is achieved when g \ud835\udc5c\ud835\udc5b\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52 is set to 2. Turning our attention to the context length \ud835\udc3e , this parameter determines (a) EDTARec DTARec DDPG TD3 CDTARec SAC 20000 60000 80000 100000 Timestpes (b) online (c) Context Length K EDTARec EDTARec-E EDTARec-R 20000 40000 60000 80000 100000 Timestpes the number of previous actions accessible to the RTG. Following insights from previous work [24] that suggest a smaller context length is beneficial, we explore a range of \ud835\udc3e values: { 2 , 4 , 6 , 8 , 10 } , while keeping g \ud835\udc5c\ud835\udc5b\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52 fixed at 2. Similar to g \ud835\udc5c\ud835\udc5b\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52 , the average CTR over 100,000 timesteps for different \ud835\udc3e values is reported in Figure 3c. It becomes evident that the proposed EDT4Rec model reaches its peak performance when both \ud835\udc3e and g \ud835\udc5c\ud835\udc5b\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52 are set to 2.", "5.5 Ablation Study (RQ3)": "In order to comprehensively understand the impact of exploration and reward relabeling on the proposed EDT4Rec model, we conducted an ablation study. This study isolates the effects of these components by evaluating two variations of EDT4Rec: EDT4RecE, which excludes the online exploration goal, and EDT4Rec-R, which omits the reward relabeling feature. The results of this study, shows in Figure 4, provides some insightful trends. Notably, in the absence of the exploration component (EDT4Rec-E), the model initially outperforms the complete EDT4Rec. This could be attributed to EDT4Rec-E solely leveraging exploitation based on previously learned policies without engaging in exploration. However, as the agent encounters unfamiliar states over time, the lack of exploration leads to a decline in performance due to the inadequacy of the existing policy in making accurate recommendations in these new scenarios. In addition, the removal of the reward relabeling component in EDT4Rec-R results in a significant drop in performance. This outcome underscores the critical role of online exploration in the effectiveness of EDT4Rec. The inability to adaptively relabel rewards based on online experiences evidently hampers the model's ability to effectively navigate and learn from the environment, thereby highlighting the essential contribution of each component to the overall success of the EDT4Rec model. It also support out claim about that DT lacks the capability of stitching.", "6 CONCLUSION": "In this study, we introduced Max-Entropy enhanced Decision Transformer with Reward Relabeling for Offline RLRS (EDT4Rec), a novel model crafted to overcome two significant challenges in Decision Transformer (DT)-based systems: i) the lack of stitching capability, and ii) insufficient online exploration, which can lead to sub-optimal performance in scenarios with sub-optimal datasets. Our comprehensive experiments demonstrate that EDT4Rec surpasses existing DT-based methods in performance. Looking ahead, our future work will delve deeper into the stitching problem. Although the current reward relabeling strategy in EDT4Rec relies on learned value functions from optimal trajectories, such trajectories may not always be available. Therefore, we aim to develop a more straightforward method of reward relabeling, one that does not necessitate reliance on optimal trajectories. This advancement will further enhance the applicability and robustness of EDT4Rec in diverse real-world scenarios.", "REFERENCES": "[1] Xueying Bai, Jian Guan, and Hongning Wang. 2019. A model-based reinforcement learning with adversarial training for online recommendation. Advances in Neural Information Processing Systems 32 (2019). [2] Han Cai, Kan Ren, Weinan Zhang, Kleanthis Malialis, Jun Wang, Yong Yu, and Defeng Guo. 2017. Real-time bidding by reinforcement learning in display advertising. In Proceedings of the tenth ACM international conference on web search and data mining . 661-670. [3] Qingpeng Cai, Aris Filos-Ratsikas, Pingzhong Tang, and Yiwei Zhang. 2018. Reinforcement Mechanism Design for e-commerce. In Proceedings of the 2018 World Wide Web Conference . 1339-1348. [4] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. 2021. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems 34 (2021), 15084-15097. [5] Minmin Chen. 2021. Exploration in recommender systems. In Proceedings of the 15th ACM Conference on Recommender Systems . 551-553. [6] Xiaocong Chen, Chaoran Huang, Lina Yao, Xianzhi Wang, Wenjie Zhang, et al. 2020. Knowledge-guided deep reinforcement learning for interactive recommendation. In 2020 International Joint Conference on Neural Networks (IJCNN) . IEEE, 1-8. [7] Xiaocong Chen, Siyu Wang, Julian McAuley, Dietmar Jannach, and Lina Yao. 2023. On the Opportunities and Challenges of Offline Reinforcement Learning for Recommender Systems. arXiv preprint arXiv:2308.11336 (2023). [8] Xiaocong Chen, Lina Yao, Julian McAuley, Weili Guan, Xiaojun Chang, and Xianzhi Wang. 2022. Locality-sensitive state-guided experience replay optimization for sparse rewards in online recommendation. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1316-1325. [9] Xiaocong Chen, Lina Yao, Julian McAuley, Guanglin Zhou, and Xianzhi Wang. 2023. Deep reinforcement learning in recommender systems: A survey and new perspectives. Knowledge-Based Systems 264 (2023), 110335. [10] Xiaocong Chen, Lina Yao, Aixin Sun, Xianzhi Wang, Xiwei Xu, and Liming Zhu. 2021. Generative inverse deep reinforcement learning for online recommendation. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management . 201-210. [11] Scott Fujimoto, Herke van Hoof, and David Meger. 2018. Addressing Function Approximation Error in Actor-Critic Methods. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018 (Proceedings of Machine Learning Research, Vol. 80) . PMLR, 1582-1591. http://proceedings.mlr.press/v80/fujimoto18a.html [12] Chongming Gao, Kexin Huang, Jiawei Chen, Yuan Zhang, Biao Li, Peng Jiang, Shiqi Wang, Zhong Zhang, and Xiangnan He. 2023. Alleviating matthew effect of offline reinforcement learning in interactive recommendation. arXiv preprint arXiv:2307.04571 (2023). [13] Chongming Gao, Shijun Li, Yuan Zhang, Jiawei Chen, Biao Li, Wenqiang Lei, Peng Jiang, and Xiangnan He. 2022. KuaiRand: An Unbiased Sequential Recommendation Dataset with Randomly Exposed Videos. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management (Atlanta, GA, USA) (CIKM '22) . 3953-3957. https://doi.org/10.1145/3511808.3557624 [14] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning . PMLR, 18611870. [15] Yujing Hu, Qing Da, Anxiang Zeng, Yang Yu, and Yinghui Xu. 2018. Reinforcement learning to rank in e-commerce search engine: Formalization, analysis, and application. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 368-377. [16] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. 2020. Conservative q-learning for offline reinforcement learning. Advances in Neural Information Processing Systems 33 (2020), 1179-1191. [17] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. 2020. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643 (2020). [18] Wenzhe Li, Hao Luo, Zichuan Lin, Chongjie Zhang, Zongqing Lu, and Deheng Ye. 2023. A survey on transformers in reinforcement learning. arXiv preprint arXiv:2301.03044 (2023). [19] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2016. Continuous control with deep reinforcement learning. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings . http://arxiv.org/abs/1509.02971 [20] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. 2013. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602 (2013). [21] Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. 2020. Awac: Accelerating online reinforcement learning with offline datasets. arXiv preprint arXiv:2006.09359 (2020). [22] Jing-Cheng Shi, Yang Yu, Qing Da, Shi-Yong Chen, and An-Xiang Zeng. 2019. Virtual-Taobao: Virtualizing Real-World Online Retail Environment for Reinforcement Learning. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 33. 4902-4909. [23] Mengting Wan and Julian McAuley. 2018. Item Recommendation on Monotonic Behavior Chains. In Proceedings of the 12th ACM Conference on Recommender Systems (Vancouver, British Columbia, Canada) (RecSys '18) . Association for Computing Machinery, New York, NY, USA, 86-94. https://doi.org/10.1145/ 3240323.3240369 [24] Siyu Wang, Xiaocong Chen, Dietmar Jannach, and Lina Yao. 2023. Causal Decision Transformer for Recommender Systems via Offline Reinforcement Learning. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (Taipei, Taiwan) (SIGIR '23) . Association for Computing Machinery, New York, NY, USA, 1599-1608. https://doi.org/10. 1145/3539618.3591648 [25] Taku Yamagata, Ahmed Khalil, and Raul Santos-Rodriguez. 2023. Q-learning decision transformer: Leveraging dynamic programming for conditional sequence modelling in offline rl. In International Conference on Machine Learning . PMLR, 38989-39007. [26] Kesen Zhao, Lixin Zou, Xiangyu Zhao, Maolin Wang, and Dawei Yin. 2023. User Retention-oriented Recommendation with Decision Transformer. In Proceedings of the ACM Web Conference 2023 . 1141-1149. [27] Xiangyu Zhao, Long Xia, Liang Zhang, Zhuoye Ding, Dawei Yin, and Jiliang Tang. 2018. Deep reinforcement learning for page-wise recommendations. In Proceedings of the 12th ACM Conference on Recommender Systems . 95-103. [28] Guanjie Zheng, Fuzheng Zhang, Zihan Zheng, Yang Xiang, Nicholas Jing Yuan, Xing Xie, and Zhenhui Li. 2018. DRN: A deep reinforcement learning framework for news recommendation. In Proceedings of the 2018 World Wide Web Conference . 167-176. [29] Qinqing Zheng, Amy Zhang, and Aditya Grover. 2022. Online decision transformer. In international conference on machine learning . PMLR, 27042-27059. [30] Cai-Nicolas Ziegler, Sean M. McNee, Joseph A. Konstan, and Georg Lausen. 2005. Improving Recommendation Lists through Topic Diversification. In Proceedings of the 14th International Conference on World Wide Web (Chiba, Japan) (WWW '05) . Association for Computing Machinery, New York, NY, USA, 22-32. https: //doi.org/10.1145/1060745.1060754"}
