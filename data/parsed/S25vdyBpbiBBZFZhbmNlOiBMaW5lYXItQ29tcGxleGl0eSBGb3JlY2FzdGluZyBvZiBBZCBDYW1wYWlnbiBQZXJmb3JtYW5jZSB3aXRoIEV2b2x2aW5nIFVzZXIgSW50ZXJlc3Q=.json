{
  "Know in AdVance : Linear-Complexity Forecasting of Ad Campaign Performance with Evolving User Interest": "XiaoYu Wang âˆ— , YongHui Guo Â§ , Hui Sheng Â§ , Peili Lv Â§ , Chi Zhou Â§ , Wei Huang Â§ , ShiQin Ta Â§ , Dongbo Huang Â§ , XiuJin Yang Â§ , Lan Xu Â§ , Hao Zhou â€  , and Yusheng Ji âˆ— âˆ— National Institute of Informatics, Tokyo, Japan Â§ Tencent â€  University of Science and Technology of China China wangxiaoyu1001@gmail.com {brookguo,hughsheng,paleylv,fredchizhou,johnwhuang,secondta,andrewhuang,xiujinyang,lanxu}@tencent.com kitewind@ustc.edu.cn kei@nii.ac.jp",
  "ABSTRACT": "Real-time Bidding (RTB) advertisers wish to know in advance the expected cost and yield of ad campaigns to avoid trial-and-error expenses. However, Campaign Performance Forecasting (CPF), a sequence modeling task involving tens of thousands of ad auctions, poses challenges of evolving user interest, auction representation, and long context, making coarse-grained and static-modeling methods sub-optimal. We propose AdVance , a time-aware framework that integrates local auction-level and global campaign-level modeling. User preference and fatigue are disentangled using a time-positioned sequence of clicked items and a concise vector of all displayed items. Cross-attention, conditioned on the fatigue vector, captures the dynamics of user interest toward each candidate ad. Bidders compete with each other, presenting a complete graph similar to the self-attention mechanism. Hence, we employ a Transformer Encoder to compress each auction into embedding by solving auxiliary tasks. These sequential embeddings are then summarized by a conditional state space model (SSM) to comprehend long-range dependencies while maintaining global linear complexity. Considering the irregular time intervals between auctions, we make SSM's parameters dependent on the current auction embedding and the time interval. We further condition SSM's global predictions on the accumulation of local results. Extensive evaluations and ablation studies demonstrate its superiority over state-of-the-art methods. AdVance has been deployed on the Tencent Advertising platform, and A/B tests show a remarkable 4.5% uplift in Average Revenue per User (ARPU).",
  "1 INTRODUCTION": "Online display advertising, especially the dominant Real-time Bidding (RTB) paradigm, has evolved into a $300 billion market [10] and becomes the primary revenue source for tech giants such as Google, Meta, Alibaba, Tencent, etc. Its success lies in a win-win situation: platforms monetize user visits into the opportunities of displaying ads ( a.k.a. user impression ), while advertisers purchase such impressions to reach potential customers and promote marketing. RTB allows advertisers to pre-define certain criteria for launching ad campaigns. Criteria specify bid prices, target audience ( e.g. , females aged 20-35 living in Shanghai), and optimization objectives ( e.g. , pursuing more exposure, clicks, or conversions). Then, a long series of auctions competing for the user impressions 57 that satisfy such criteria constitutes the ad campaign. 58 RTBfeatures non-guaranteed delivery (NGD) modes, i.e. , both the cost and yield of a campaign remain uncertain before its fulfillment. Consequently, it is critical for advertisers to know in advance the expected performance, rendering the Campaign Performance Forecasting (CPF) problem. This foresight brings two-fold benefits: 1) Advertisers use a few tentative predictions to balance a wider audience and higher conversion rates, thus improving Return on Investment (ROI). 2) Platforms can stimulate advertisers to invest additional budget for more yield, thus promoting revenue. CPF problem induces a sequence-to-sequence task, with the input of an auction series satisfying the campaign criteria, and the output of the corresponding cost and yield so far. Significant academic and industry attention has been attracted: Kalish et al. from Bidtellect [21] constructed a multi-variate time series of similar campaigns to predict new campaigns. Wu et al. from Tencent [49] estimated a scaling factor of the total future impression volume. These coarse-grained methods fail to harness the information of each auction. In contrast, Wang et al. from Yahoo [45] aggregated qualified auctions from bid logs, and Jiang et al. from Meta [20] further considered reaching distinct users, albeit lacking a global viewpoint from campaign-level modeling. Nath et al. from Microsoft [29] combined dynamic linear models with Bayes net for winning price estimation. Cui et al. [8] used probabilistic methods of a mixed log-normal distribution, while Ren et al. [33] replaced it with recurrent neural networks (RNN) to approximate a discrete winning distribution. However, neglecting evolving user interest results in a substantial gap between predictions and online results. To fill this gap, we propose AdVance , a time-aware framework that integrates local and global modeling. Designing such a framework faces the following challenges: (1) Evolving user interest: During the exposure to a series of ads, a user clicks the preferred ads and accumulates fatigue toward the similar ones, causing future click and conversion rates to decline. This accounts for the diminishing marginal utility issues where the yield is not proportional to the cost increment. Neglecting this phenomenon renders overestimated campaign performance. (2) Auction representation: Each auction involves user features, contextual information, and a dynamic number of candidate ads competing with each other. The platform's filtering rules further complicate the auction process. Effectively compressing and extracting useful information 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 1 from such a multi-source , variable-length input remains a significant challenge. (3) Long context: Accurate predictions require summarizing a sequence of tens of thousands of auctions with irregular time intervals. Traditional linear architectures like RNNs and CNNs struggle to model long-range dependency, while the self-attention mechanism suffers quadratic complexity, making it impractical for CPF tasks. AdVance converts each auction and corresponding user interest into a single embedding. It summarizes the embedding sequence with a conditional State Space Model (SSM) to achieve linear complexity. Specifically, we use a time-positioned click sequence and a fatigue vector compressing all displayed ads to reflect interest dynamics. A Transformer encoder conducts self- and cross-attention on candidate ads and user-related features and predicts the auctionlevel cost and yield. This fully utilizes the supervision signals from historical records and forges an informative representation. SSMs feature linear structures like RNNs and CNNs while achieving comparative long-range modeling ability as self-attention. We propose its conditional variant. We condition its parameters on the current auction and time interval to handle the irregular input sequence, and we condition the campaign-level prediction on the accumulated auction-level outputs. In summary, our contributions are as follows: Â· We focus on the challenging task of forecasting ad campaign performance with evolving user interest, which benefits both advertisers and platforms by providing valuable insights and stimulating ad budgets. Â· We propose AdVance , a time-aware framework that combines auction- and campaign-level modeling. AdVance leverages the attention mechanism to vectorize each auction locally and summarizes the whole sequence with a conditional SSM, achieving global linear complexity. Â· We conduct evaluations and ablation studies using largescale industrial datasets, demonstrating the superiority of AdVance over state-of-the-art methods. AdVance has been deployed on the Tencent advertising platform, and we uploaded the PyTorch implementation. 1",
  "2 RELATED WORK": "",
  "2.1 Campaign Performance Forecasting": "Accurate modeling of campaigns grants advertisers insights into their investment and return, thus attracting significant research interests. Based on the granularity, existing works can be categorized into campaign-level and auction-level methods. Kalish et al. [21] estimated campaign performance by aggregating statistics from similar historical campaigns. Wu et al. [49] focused on calculating scaling coefficients to adjust predicted volumes of future impressions to earned ones. Despite having low complexity, they discard fine-grained information within each auction, leading to a non-negligible accuracy gap. Auction-level methods, in contrast, lift the complexity for higher forecasting accuracy, as the benefits for publishers and advertisers are significant. Wang et al. [45] estimated a quality score for each 1 https://github.com/anonymousauthor113/CPF (ad, user)-pair using regression modeling and used it as a threshold to select qualified impressions. Cui et al. [8] enhanced this work by incorporating probabilistic methods and assuming a mixture of log-normal distribution. Jiang et al. [20] calculated corresponding threshold bid prices for winning historical auctions and counted the number of exposed users. Chen et al. [4] further augmented it with multi-task learning and campaign information. Following the spirit of estimating threshold prices to win, another line of research known as bid landscape or market price modeling has gained traction and can be applied to campaign modeling tasks. As a representative, Ren et al. [33] removed assumptions on the distribution forms and utilized a recurrent neural network to flexibly model the conditional winning probability for each bid price. Yang et al. [51] further incorporated multi-task learning to jointly model click-through rate and market price, thereby providing multiple results in a single return to enhance the robustness and online inference efficiency. The main drawback of these methods is the neglect of user interest evolvement in the future campaign environment and directly using historical click/conversion probability. When a particular ad wins more auctions, the user preference evolves, and fatigue accumulates toward repetitive similar ads. Neglecting such evolvement and assuming static user interest causes an overestimate of campaign performance and budget waste.",
  "2.2 User Interest Modeling": "User interest modeling mainly focuses on the probability of certain explicit behaviors, such as clicking or conversion, by modeling the feature interaction between users and ads. Early machine-learning and deep-learning methods, including logistic regression [35], gradient boosting decision trees (GBDT) [17], collaborative filtering [37], Wide&Deep [6], DeepFM [15], DCN [44], and PNN [32], adopt a static viewpoint and overlook the dynamics of user preference. To address the limitation, DIN [57] first incorporated the sequence of the user's historic clicked items and utilized an attention mechanism to build an enriched user feature. Subsequently, a series of works such as DIEN [56], DSIN [11], SIM [30], UBR4CTR [31], SMR[30] emerged to model user interest evolution using recurrent neural network (RNN) [18] or Transformers [41]. However, the aforementioned methods discard the abundant displayed but non-clicked ads, which account for user fatigue towards repeated similar ads. In contrast, AdVance considers all displayed ads to comprehensively understand interest evolution.",
  "3 PRELIMINARY": "",
  "3.1 Attention Mechanism": "Attention mechanism [41] excels at modeling long-range dependencies. It allows different parts ( a.k.a. tokens) of the input sequence to interact, regardless of their distance and position. This is achieved by representing the input Queries as the weighted sum of Values . The weights depend on the similarity between queries and Keys , measured by the dot-product:  where ğ‘‘ ğ¾ is the dimension of each key vector. 2 For the self-attention, Q = XW ğ‘„ , K = XW ğ¾ , and V = XW ğ‘‰ are the projections of the same sequence X , thereby focusing on information exchange and aggregation within single sequence. In contrast, the cross-attention involves two different sequences X and Y , where Q = XW ğ‘„ , K = YW ğ¾ , and V = YW ğ‘‰ . This allows X to 'borrow\" information from Y , thus useful in multi-modality learning such as vision-language models [1, 36]. The attention mechanism's main drawback is its quadratic complexity, as each new token has to attend to all previous tokens. This incurs heavy burdens for handling numerous ad auctions.",
  "3.2 State Space Model": "As a promising competitor to Transformers, the State Space Model (SSM) [14] shares the same virtue of linear recurrence of RNN while achieving comparative long-range modeling capacity in sequence analysis [39], time series prediction [53], and large language models [12, 13]. It defines a continuous differential system and recurrently updates a hidden state â„ ( ğ‘¡ ) :  where ğ‘¥ ( ğ‘¡ ) âˆˆ R is the 1-D input signal, ğ‘¦ ( ğ‘¡ ) âˆˆ R is the output signal, A âˆˆ R ğ‘ Ã— ğ‘ is the state transition matrix, B âˆˆ R ğ‘ Ã— 1 is the input matrix, and C âˆˆ R 1 Ã— ğ‘ is the output matrix. To adapt Eq. (2) to sequence modeling tasks, we employ zeroorder hold (ZOH), a technique for discretizing continuous equations, and we have the linear recurrence form:  where A = exp ( Î” A ) , B = ( Î” A ) -1 ( exp ( Î” A )-I )Â· Î” B , and Î” denotes the step size. Note that â„ ğ‘¡ , ğ‘¥ ğ‘¡ , and ğ‘¦ ğ‘¡ are now discrete time series. When the input is a sequence of ğ· -dimension vectors, we stack ğ· SSMs to model each vector dimension separately, resulting in a total ( ğ‘ğ· ) -dimension hidden space. Like the Transformers, a multilayer perceptron (MLP) is often added to process the concatenation of all ğ· SSMs' output ( a.k.a. channel mixing [52]).",
  "3.3 Problem Formulation": "Advertisers pre-define the criteria of ad campaigns to expose target users to their ads within a specific period. An auction is launched whenever a qualified user impression comes, and âˆ¼ 200 selected candidates compete for it. Eventually, an irregular time series of auctions constitutes the campaign. We define campaign performance as the expected cost and yield of an ad campaign. Advertisers may pursue more ad exposure, clicks, or conversions, giving rise to CPM (Cost-per-Mille), CPC (Cost-per-Click), and CPA (Cost-per-Action) ad types. The expected yield of an auction is defined as:  ï£³ Here, win-rate is the target ad's probability of winning the auction, pCTR (predicted click-through rate) is the probability of the user clicking the ad, and pCVR (predicted conversion rate) denotes the probability of the user's conversion like adding to cart or purchase. Then we define the expected cost of such an auction as ( bid price Ã— expected yield ) . This is also known as the effective cost-per-mille (eCPM). Given advertiser-defined criteria and a long sequence of qualified auctions, our target problem is to predict the corresponding cost and yield of the campaign with evolving user interest and maintain acceptable algorithm complexity for practical needs.",
  "4 METHOD": "AdVance operates on a sequence-to-sequence paradigm by mapping a series of auctions to a series of estimated campaign performances at the moment, as illustrated in Fig. 1. AdVance consists of three modules, i.e. , user interest, local auction, and global campaign modeling. Click records with time-stamp embedding reflect user preference. The local SSM recurrently updates the fatigue vector based on the whole display history. An encoder conducts self- and cross-attention on candidate ads and user features to predict auction performance, thereby fully utilizing the log data and building an informative representation. The generated sequence of auction embedding has long lengths and irregular time intervals. The linearcomplexity, global SSM with parameters dependent on inputs and intervals summarizes the sequence. The final prediction relies on the global SSM's output and the accumulated auction performance, thereby tightly integrating the fine-grained auction and holistic campaign knowledge.",
  "4.1 User Interest Modeling": "The sequence of a user's previous clicks and conversions offers a basis for estimating the preference towards similar ads. Many works [11, 56, 57] incorporate it as part of user features, albeit with two drawbacks. Â· Irregular interval: RNN- and Transformer-based methods treat user behaviors as an evenly distributed time series, while the interval lengths affect the timeliness of the relevance between historical and current behaviors ( e.g. , a purchase made a week ago is often more informative than one made two months ago). Â· User fatigue: Displayed yet non-clicked records account for fatigue accumulation and yield declines. Ignoring them causes overestimated preferences and displaying similar ads repeatedly. Recent behaviors deserve more emphasis for modeling user interest, as discovered by [16, 23]. At the Tencent advertising platform, each display record has a time-stamp. Inspired by the absolute and relative positional embedding [38, 41], we propose relative time-stamp positional embedding as:  where ğ‘¡ is the difference between the current time-stamp and a manually set origin ( e.g. , 2023.1.1 0:00 AM), 2 ğ‘‘ equals the dimension of input embedding, and ğœ” 1 , . . . , ğœ” ğ‘‘ are ğ‘‘ trainable parameters. We calculate Pos ( ğ‘¡ ) for each click record and add it to the click record's embedding. We use trigonometric functions due to their good properties for dot-product:  3 Figure 1: AdVance disentangles user interests as time-stamped click sequences representing user preference and fatigue vectors compressing all displayed items (Sec. 4.1). The attention mechanism compresses auctions into dense embeddings, and AdVance accumulates auction-level performance (Sec. 4.2). A global SSM recurrently summarizes all embeddings, and AdVance returns final results based on the summary and accumulated performance (Sec. 4.3). During training, a causal mask blocks out 'future\" records after the current time-stamp (Sec. 4.4.1). âŠ— ğœ½ ğ‘ªğ‘»ğ‘¹ ğœ½ ğ‘ªğ‘½ğ‘¹ ğ’• ğŸ ğ’• ğŸ ğ’• ğŸ‘ ğ’• ğŸ’ Causal Mask Time-stamp Embedding Local SSM ğ’• ğŸ ğ’• ğŸ ğ’• ğŸ‘ ğ’• ğŸ’ Fatigue Vector Competitors User Context Self- & Cross-Attention concat MLP ğœ½ ğ’˜ğ’Šğ’ {ğ’‘ ğŸ , ğ’‘ ğŸ , ğ’‘ ğŸ‘ , ğ’‘ ğŸ’ } ğ’‰ ğ’• = ğ‘¨ ) ğ’‰ ğ’•&ğŸ +ğ‘© ) ğ’™ ğ’• ğ’š ğ’• = ğ‘ªğ’‰ ğ’• Clicked Ad Non-clicked User Interest Modeling Clicks Local Auction Modeling Accumulate Auction Performance concat ğœ½ ğ‘ªğ‘· ğ’™ ğŸ ğ’™ ğŸ ğ’™ ğŸ‘ ğ’™ ğŸ’ ğœ¹ ğŸ ğœ¹ ğŸ ğœ¹ ğŸ‘ ğ’š ğŸ“ Global SSM Exposure Click Conversion Cost Global Campaign Modeling Auction Representation ğ’™ ğŸ“ ğœ¹ ğŸ’ where ğ›¿ represents a time interval. Therefore, the time distance information is preserved for AdVance to pay attention to more relevant user behaviors. A seemingly feasible solution to handle non-clicked records is to include them as user features, just like what we do to the clicked ones. However, the number of non-clicked is usually 20 or more times larger than that of clicked records, making it impractical due to the quadratic complexity of the self-attention mechanism. We employ a local state space model (SSM) to compress the whole sequence of displayed ads into a fatigue vector in linear complexity, which serves as conditional information in the auction representation (Sec. 4.2). It processes the display records one by one and recursively updates the fatigue vector. We make the SSM's parameters data-dependent and interval-dependant, granting the model the ability to selectively memorize salient knowledge from the irregular input series. Sec. 4.3 provides more details about the conditional SSM.",
  "4.2 Local Auction Modeling": "This module takes an input of click sequence, fatigue vector, a varying number of candidate ads, user profile, and other contextual information to learn an informative representation for each auction. This demands the model architecture capable of 1) handling variable-length input, 2) modeling competitive relations between any two of the candidates, and 3) aggregating knowledge from multiple input sources into one vector. We employ an attention-based encoder that satisfies the aforementioned demands. The encoder conducts self-attention on the candidate ads, where the competitive relationship forms a complete graph. The encoder applies cross-attention between candidate ads and the rest of the input to extract knowledge from user profiles, interests, and context. This knowledge indicates the user's value to advertisers. Formally, we have: X = X + Pos ( ğ‘¡ ) , X = LN ( X + Attn ( XW ğ‘„ , [ X ; Y ] W ğ¾ , [ X ; Y ] W ğ‘‰ )) ,  where X denotes the candidate ad embeddings, and Y denotes the embeddings of user click sequence, user profile, and contextual information. We calculate Pos ( ğ‘¡ ) using the time-stamp of the current auction. We use [ X ; Y ] to calculate the keys and values, thus integrating the self- and cross-attention in one pass. LN denotes the layer norm function [2], and MLP represents a multi-layer perceptron, often stacked fully-connected layers with ReLU activation as in [41]. We use the ğ‘ğ‘œğ‘›ğ‘ğ‘ğ‘¡ (Â·) operator to concatenate the fatigue vector fi ğ‘“ to each ad embedding, as this factor greatly affects user clicks and conversions. Empirically, supervised learning is a more straightforward and sample-efficient paradigm for representation learning [28]. The common practice is to first train a model on a labeled dataset, then remove the classifier (usually the last few layers of the model). Then, the rest of the model serves as a discriminative representation extractor. This inspires us to devise a multi-task architecture of predicting each auction's win-rate and expected yield, with the shared representation of ad embedding X . Win-rate prediction: Besides bid prices and user-ad matching, the Tencent platform manually sets filtering rules that can not be described as analytic functions. Inspired by PointerNet [43] and AlphaStar [42], we treat the auction process as a black box and approximate it with a discrete distribution over all ads. We train a win-rate model ğ‘“ (Â· ; ğœƒ ğ‘¤ğ‘–ğ‘› ) to compress each ad embedding ğ‘‹ ğ‘– into a scalar ğ‘¤ ğ‘– that describes its relative advantage over other ads. A Softmax layer then turns all scalars into a discrete distribution of the winning probability ğ‘ ğ‘– = exp ( ğ‘¤ ğ‘– )/ Ë ğ‘— = 1 , 2 ,... exp ( ğ‘¤ ğ‘— ) for each 4 ad. The ground truth is recorded as a one-hot vector [ 0 . . . 1 . . . 0 ] , where 1 indicates the winner. Hence, we use the categorical crossentropy as our loss function. Yield prediction: We focus on estimating pCTR and pCVR (Sec. 3.3). We model the task as a binary classification problem and use a Sigmoid function ğ‘“ ( ğ‘¥ ) = 1 /( 1 + ğ‘’ -ğ‘¥ ) to output a probability. However, predicting pCVR faces a great challenge due to the sparser positive samples than the pCTR problem. Inspired by ESMM [26], we introduce two sub-models ğ‘“ (Â· ; ğœƒ ğ¶ğ‘‡ğ‘… ) and ğ‘“ (Â· ; ğœƒ ğ¶ğ‘‰ğ‘… ) : the former predicts pCTR, and the latter predicts the conversion probability conditioned onthat the ad has been clicked. Apparently, the ad conversion must come after the ad click. Thus, ğ‘“ (Â· ; ğœƒ ğ¶ğ‘‡ğ‘… ) Ã— ğ‘“ (Â· ; ğœƒ ğ¶ğ‘‰ğ‘… ) equals pCVR, according to the chain rule. This design lowers the difficulty of learning pCVR by treating pCTR as an intermediate task and solving a conditional probability problem in a smaller space. Furthermore, it allows AdVance to output multiple yield metrics of exposure, click, and conversion volumes with Eq. (4), regardless of the campaign objectives. Notably, win-rate and yield prediction are correlated tasks, as ads with high pCTR/pCVR also have a higher rate of winning the auction. This connection benefits their training mutually and helps to learn a more effective representation, as discovered by [51] and our experiments. We select the target ad's embedding as the auction representation , as it has aggregated information from all other tokens after the cross- and self-attention.",
  "4.3 Global Campaign Modeling": "This module takes input from a time series of auction embedding and summarizes it into a summary vector . Then, AdVance forecasts the campaign performance based on such a vector. Self-attention models preserve all previous tokens as Key and Value matrices, and each new token has to traverse the sequence before it, resulting in a quadratic complexity. In contrast, State Space Models (SSMs) maintain a hidden state to compress historical input. This allows SSMs to process new tokens recurrently and update the hidden state correspondingly, thus achieving a linear complexity. However, the vanilla SSM's parameters ( Î” , A , B , C ) remain the same for all tokens [14]. A constant stepsize Î” is unsuitable for irregular auction intervals, and a static input matrix B and output matrix C can not selectively preserve or discard information based on the current token, causing a redundant hidden state. Inspired by the gating mechanism [7, 18, 19], recent research suggests a data-dependent design that makes SSM's parameters as functions of input tokens [13, 39]. Therefore, we define the conditional SSM as  Here, X = [ ğ‘¥ 1 , ğ‘¥ 2 , . . . ] âˆˆ R ğ¿ Ã— ğ· denotes an ğ¿ -length sequence of ğ· -dimension auction embedding. ğ‘Š B , ğ‘Š C âˆˆ R ğ· Ã— ğ‘ map input tokens into the input matrix and output matrix, respectively. ğ›¿ X denotes time intervals between the successive auctions. Its first entry is set to 0. We concatenate X and ğ›¿ X along the dimension axis, thereby making AdVance aware of the time irregularity . ğ‘Š Î” âˆˆ R ( ğ· + 1 ) Ã— ğ· maps input tokens and time intervals into the SSM's stepsizes, and ğœ Î” ( ğ‘¥ ) = log ( 1 + exp ( ğ‘¥ )) denotes the Softplus function, a smooth approximation to the ReLU function, making sure the stepsizes always positive. ğ‘ B , ğ‘ C , and ğ‘ Î” are all biases. Following the same setting as [13, 27, 53], we set the transition matrix A âˆˆ R ğ‘ Ã— ğ‘ as diagonal to save computation. Note that the hidden state's dimension ğ‘ is often much smaller than ğ· . To slim ğ‘Š Î” âˆˆ R ( ğ· + 1 ) Ã— ğ· , we replace it by the product of ğ‘Š ( 1 ) Î” âˆˆ R ( ğ· + 1 ) Ã— ğ‘ and ğ‘Š ( 2 ) Î” âˆˆ R ğ‘ Ã— ğ· , reducing the O( ğ· 2 ) to O( ğ‘ğ· ) . Once we get ( Î” , A , B , C ) , we calculate the discretized version ( A , B , C ) using Eq. (3) and train the conditional SSM. Multiple techniques can accelerate AdVance's training speed, such as FlashAttention [9] and parallel scan [3]. We use the SSM's last output ğ‘¦ ğ¿ as the whole campaign's summary vector. We also accumulate each auction's expected cost, exposure, click, and conversion and concatenate them into a vector ğ‘ƒ ğ‘ğ‘ğ‘ğ‘¢ âˆˆ R 4 . Finally, AdVance predicts all metrics of campaign-level performance in one pass as a 4D vector:  where ğœƒ ğ¶ğ‘ƒ is the model parameter.",
  "4.4 Training and Inference": "4.4.1 Offline Training. Each training sample corresponds to one logged ad campaign. It contains a time-stamped sequence of auctions in which this ad has participated. Each auction sample records all competitor ads, user-related features, contextual information, the auction winner, and the users' click/conversion behavior. To lower variance and stabilize model convergence, we split the input sequence into chunks of 100 auctions. The training label is a time series of the corresponding total cost and yields up to that moment and is also aggregated by chunks. The training follows a Seq2Seq paradigm [40]: AdVance sequentially processes input auctions and predicts the current campaign performance whenever a chunk has been finished. The loss is calculated w.r.t. the labels, and AdVance updates its parameters using back-propagation. Note that the global SSM only takes gradients w.r.t. campaign performances, while the auction-level and userinterest models take gradients from both campaign performance and auxiliary tasks. This creates a mini-batch training for the winrate and yield prediction models. To prevent label leakage from the user's display history, we devise a causal mask that 'covers\" the records after the current time-stamp. Hence, the user preference and fatigue vector only involve the behaviors so far. 4.4.2 Online Inference. At this stage, advertisers launch service requests with (multiple sets of) campaign criteria, and AdVance returns the expected performances. We begin with building a simulated future campaign environment. Following the same methods as [4, 8, 20], we predict the number of impressions that satisfy the specified user targeting. We then sample auction records from the previous day's log according to the predicted volume. This offers more realistic competitor features and timely user interest. To better approximate the future environment, the Tencent platform delicately categorizes user targeting into 188,785 classes and utilizes CLOCK [46], a multi-variable neural forecaster, to accurately predict the impression volume of each class. 5 Once the future environment is built, we insert the target ad and its bid price into each auction. We feed the modified auction sequence to AdVance to re-calculate each auction's win-rate, expected yield, and the final campaign performance. To simulate interest evolution, we randomly append new ads to the user's display history according to win-rates and update the click sequence according to pCTR and pCVR. The fatigue vector is recurrently updated accordingly. After traversing the auction sequence with linear complexity, AdVance outputs the expected cost, exposure, click, and conversion volumes.",
  "5 EXPERIMENTS": "We conduct experiments and ablation studies on industrial datasets from Tencent Advertising to validate our AdVance framework and investigate four research questions, i.e. , RQ1: Prove AdVance's efficacy and superiority over state-of-the-art methods for campaign performance forecasting. RQ2: Demonstrate the necessity of modeling user preference and fatigue evolution. RQ3: Highlight the importance of introducing auxiliary tasks for auction representation and campaign-level prediction. RQ4: Evaluate the impact of different sequence-modeling techniques for campaign-level summarization. Lastly, we introduce the online A/B testing of AdVance to present its practical value in real-world scenarios.",
  "5.1 Experimental Settings": "5.1.1 Dataset. We aim to train models that can integrate auctionand campaign-level information. Hence, the dataset should contain user history and each auction's competitor ads. No public dataset satisfies the requirements, so we collected our dataset from the Tencent Advertising platform. This dataset comprises 1.5 billion records from June 1, 2023, to June 30, 2023. Each record contains the user feature, user display history with corresponding behaviors, contextual information, and all competitor ads with their ad content, category ID, targeting criteria, bid price, etc. We focus on campaigns with over 20,000 records for better data quality and lower variance. Two business concerns also support this: First, advertisers with higher investments are more sensitive to budget efficiency. They are also more likely to increase investment when receiving positive feedback from AdVance. Second, ads with more frequent exposure in a longer period are often more prone to interest evolution and fatigue. We select 6,000 campaigns, with 1,000 for CPM, 2,000 for CPC, and 3,000 for CPA ads. 5.1.2 Compared Methods. We compare with auction-level methods, which beat coarse-grained ones by a large margin. The baseline methods include those from the industry like Yahoo [8], Microsoft [29], and Alibaba [4], and academic works as follows: 1) CPF [8] assumes a mixed log-normal distribution for bid prices and estimates its mean and standard deviation by regression. The win-rate is calculated by the cumulative density function (CDF). CPF trains decision trees to predict click/conversion rates and multiply them with the win-rates, thereby obtaining the expected yield. The final result is the accumulation of auction-level performance. 2) GMIF [29] uses a first-order Dynamic Linear Model to forecast the number of future impressions. It then trains a Bayes net to estimate the threshold bid price to win a specific auction. Its paper omits the model design of pCTR/pCVR, so we use DeepFM [15] instead. 3) MTLN [4] assumes static user traffic and uses DeepFM to estimate yields. Like us, MTLN also introduces a campaign-level model conditioned on the accumulated performance. An MMoE [25] model generates the final result. 4) DLF [33] surpasses previous works [22, 47, 48, 55] in win-rate estimation. It discards the prior assumptions of win-rate distribution and uses a dedicated RNN to learn a discrete probability over the bid price. 5) MTAE [51] further enhances DLF with multi-task learning, leveraging the correlation between win-rates and click-through rates prediction. 5.1.3 Evaluation Metric. The Tencent platform keeps storing new auction records and uses them to update numerous online models. The records form an ever-increasing time series, and we adopt a sliding-window paradigm: trained on an input window of records, the model predicts the campaign performance for a future period ( a.k.a. , forecasting horizon ) of records. The window then goes on with a stepsize of 1 hour, and we fine-tune the model using new samples. This process is executed recurrently. At each time step, we calculate the weighted mean absolute percentage error (WMAPE):  where weight ğ‘– is the ratio between the ğ‘– -th campaign's cost and the total cost of all campaigns, ğ‘¦ ğ‘– and Ë† ğ‘¦ ğ‘– represent the ground truth and estimation, respectively. As a warm-up, we pre-train all models on the records from June 1, 2023, to June 7, 2023. Then, we accumulate the WMAPE per step and calculate the average as the evaluation metric. We retrain the model on the whole dataset every 24 hours. We vary the forecasting horizon to evaluate the capacity of modeling long sequences as 1, 6, 12, and 24 hours. 5.1.4 Implementation Details. We set the display history length to 300. Displayed items, fatigue vectors, user features, contextual information, and candidate ads are all 256-dimensional embeddings. We stack three encoder layers with four heads and 1024 hidden dimensions. ğœƒ ğ‘¤ğ‘–ğ‘› , ğœƒ ğ¶ğ‘‡ğ‘… , and ğœƒ ğ¶ğ‘‰ğ‘… are all three-layer MLPs with hidden neurons [128, 64, 1] and ReLU activation. We stack three SSM layers with the hidden state dimension ğ‘ = 16 for local and global modeling. The final campaign performance model ğœƒ ğ¶ğ‘ƒ is an MLP of [128, 64, 4] with ReLU activation. The model is trained with an AdamW optimizer with a learning rate of 0.001, ğ›½ 1 of 0.9, ğ›½ 2 of 0.995, and ğœ– of 1e-07. Batch-size = 32. Due to their equal value, the win-rate and yield prediction loss weights are set as [0.5, 0.5].",
  "5.2 System Performance": "As shown in Table 1, all models exhibit performance declines when the forecasting horizons are prolonged. This is mainly caused by the distribution shift of the campaign environment, such as newly introduced campaigns and old ones adjusting their criteria. Despite these declines, AdVance consistently outperforms the other methods by integrating auction- and campaign-level information and capturing interest evolution, addressing RQ1 . Among the compared methods, CPF's log-normal assumption of bid prices severely limits its capacity to model the complex competition among bidders. In contrast, Bayes net captures the probabilistic connections among factors contributing to the auction victory, leading to improved GMIF performance. MTLN's multi-task structure 6 Table 1: Timestep-averaged WMAPE of exposure, click, conversion, and cost for five baselines and AdVance w.r.t. different forecasting horizons from 1H to 24H. The best results are highlighted in bold. also considers the auction- and campaign-level information. However, MTLN discards the auction representation. Its global model can not handle the long sequence of auction records; it only takes the accumulated performance and coarse-grained campaign statistics. DLF surpasses the aforementioned methods by a large margin. It replaces the pre-defined win-rate distribution with a more flexible RNN, allowing DLF to adapt to the varying competition environment. Finally, MTAE outperforms DLF regarding reduced model complexity and maintenance overheads. MTAE improves accuracy by leveraging correlations between multiple tasks. In summary, a more flexible form of win-rate modeling and multitask learning can promote forecasting accuracy significantly. However, the lack of modeling user interest evolution and campaignlevel sequence leads to inferior performance.",
  "5.3 Ablation Study": "We conduct ablation studies involving five AdVance variants to assess each component's individual contributions and effectiveness: 1) Static: We discard the display history and assume a static user interest. 2) Pref: We preserve the clicked items for user preference and disregard the user fatigue. 3) Aux: We remove the auxiliary tasks of win-rates and pCTR and directly learn the auction representation. 4) Accu: We do not accumulate the auction-level results, making the campaign-level forecasting independent. 5) Reg: We assume auction and display sequences have regular time intervals. As depicted in Table 2, Static 's performance drops severely and further declines with a longer horizon. Pref alleviates such degradation by considering preference evolution, but its incomplete view of user interest makes it inferior to AdVance, thus addressing RQ2 . The absence of supervision signals during model training imposes significant difficulty in learning a meaningful representation. This accounts for Aux 's performance gap. Using the Divide & Conquer policy, we decompose the campaign performance into Table 2: Timestep-averaged WMAPE of exposure, click, conversion, and cost for five variants of AdVance w.r.t. different forecasting horizons from 1H to 24H. The best results are highlighted in bold. Table 3: Timestep-averaged WMAPE results. Run on a V10016GB GPU. OOM indicates out-of-memory . numerous auction performances and accumulate them. The accumulated results serve as an informative reference and reduce the overall difficulty of forecasting. This improves Accu 's accuracy and addresses RQ3 . The changes in traffic affect the supply of user impressions and stimulate the intensity of auctions. Reg omits the non-stationary traffic, causing an accuracy drop. To answer RQ4 , we modify AdVance's global summarizer and obtain four variants: 1) Ind: No global model, using accumulated auction performance. 2) RNN: Using LSTM to summarize auction sequence. 3) Transformer: Using a quadratic-complexity encoder 7 with time-stamped position embedding. 4) S4: Using an SSM with parameters independent of inputs and time intervals. As depicted in Table 3, the lack of a holistic view of the campaign environment leads to Ind 's performance drop, proving the necessity of a global model. RNN features linear complexity but has difficulty memorizing too long context. In contrast, Transformer explicitly preserves all previous tokens, achieving the best performance for a moderate context length. However, the inference memory grows linearly w.r.t. input length and reports OOM for the 24-hour horizon. It also presents more than five times the latency of AdVance due to its quadratic complexity, making it unsuitable for online service. S4 [14] can compress long sequences with linear-complexity calculation, but it cannot selectively store salient information from an unevenly distributed time series, leading to its accuracy declines. In summary, a comprehensive solution for CPF tasks should consider the user's preference & fatigue evolution, long-context modeling with low complexity, and a tight connection between auction- and campaign-level information.",
  "5.4 Further Investigation": "AdVance's user interest and local auction modules constitute a click-through rate model. We compare it with representative pCTR models to demonstrate the impact of interest evolution, especially for long-period campaigns: 1) Wide&Deep [6]: A combination of a deep neural network and a linear model that captures low- and high-order feature interactions. 2) DIEN [56]: A sequential model that considers interactions between user clicks and candidate ads. A dedicated RNN captures the evolution of user preference over time. 3) FAN [24]: An improved interest model that incorporates the frequency-domain feature for user fatigue. Weselect five campaigns from various industries, i.e. , food, smartphones, clothes, cosmetics, and games. For each campaign, we use 80% records as the training set and 20% as the testing set. The results are shown in Fig. 2. Wide&Deep performed the worst as it only considers static user features and can not model the user's sequential behaviors. In contrast, DIEN performed better by capturing user preferences hidden in clicked items. FAN calculates the fast Fourier transformation of the displayed yet non-clicked items to model user fatigue. However, FAN assumes regular time intervals, and FFT features are inadequate to represent interest evolution compared to deep neural networks. In conclusion, both clicked and non-clicked items are necessary to capture the evolution of preference and fatigue, significantly affecting yield prediction accuracy.",
  "5.5 Online A/B Testing": "Our AdVance has been implemented on the Tencent Advertising platform, allowing advertisers to try various campaign criteria and receive corresponding performances in real-time. Advertisers can then decide the appropriate campaign settings based on predictions and business demands. Given specific criteria, we use the inverted index to retrieve qualified records from the system log of the past 24 hours with a maximum number of 20,000 records, ensuring a response delay within 5 seconds. We scale the prediction accordingly to maintain consistency. Figure 2: The AUC of three baselines and AdVance on five campaigns from various industries. 0.82 0.84 0.86 0.88 0.9 0.92 0.94 0.96 0.98 Campaign 1 Campaign 2 Campaign 3 Campaign 4 Campaign 5 Wide&Deep DIEN FAN AdVance AUC Food Smartphone Clothes Games Cosmetics 0.82 0.84 0.86 0.88 0.90 0.92 0.94 0.96 0.98 To evaluate AdVance's effectiveness, we conducted online A/B tests on advertisers of the same industry. We split them into two groups with similar average revenue per user (ARPU). Only advertisers in group B were granted access to the AdVance services. Over two weeks, the comparison revealed a 4.5% uplift in ARPU for group B advertisers due to optimized campaign configuration. AdVance is processing thousands of queries daily, greatly enhancing the platform's income and attractiveness to advertisers.",
  "6 DISCUSSION AND FUTURE WORK": "Like many other industrial practices, AdVance mainly considers user traffic fluctuation when modeling environment dynamics and handles it with a fine-grained time series model. However, new campaigns may participate, and other advertisers may adjust their bid prices or user targeting as a counterbalance. This can cause a drop in accuracy over long periods, shown in Table 1. One possible mitigation is introducing advertiser modeling techniques and game-theory-based competition modeling [50, 54]. The former can help predict when and how new campaigns will be launched, and the latter can predict the possible response from competitors. These future directions hold promise for advancing the field of ad campaign performance forecasting and facilitating more effective decisions in online advertising.",
  "7 CONCLUSION": "We propose AdVance, a time-aware framework integrating auctionand campaign-level modeling. We introduce user preference as a time-positioned click sequence and emphasize fatigue modeling by compressing all displayed history into a concise vector. We trained an encoder in a supervised manner to predict cost and yield per auction. The encoder applies self-attention/cross-attention on candidate ads and user features, thereby converting each auction into informative embedding. To comprehend the generated long, irregular sequence, we make the linear-complexity SSM's parameters dependent on current embedding and time interval. The conditional SSM then outputs expected campaign performance, with its prediction conditioned on the accumulation of auction-level results. AdVance outperforms state-of-the-art methods on large-scale industrial datasets, and has been deployed on the Tencent advertising system, showing a 4.5% uplift in Average Revenue per User in the A/B test. 8",
  "REFERENCES": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. 2022. Flamingo: a Visual Language Model for Few-shot Learning. Advances in Neural Information Processing Systems (NIPS) 35 (2022), 23716-23736. [2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450 (2016). [3] Guy E Blelloch. 1990. Prefix sums and their applications. (1990). [4] Jun Chen, Cheng Chen, Huayue Zhang, and Qing Tan. 2022. A Unified Framework for Campaign Performance Forecasting in Online Display Advertising. arXiv preprint arXiv:2202.11877 (2022). [5] Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . 785-794. [6] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems . 7-10. [7] Kyunghyun Cho, Bart Van MerriÃ«nboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078 (2014). [8] Ying Grace Cui and Ruofei Zhang. 2013. Campaign Performance Forecasting for Non-guaranteed Delivery Advertising. US Patent App. 13/495,614. [9] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher RÃ©. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems 35 (2022), 16344-16359. [10] Dentsu. 2022. Global Ad Spend Forecast . https://www.dentsu.com [11] Yufei Feng, Fuyu Lv, Weichen Shen, Menghan Wang, Fei Sun, Yu Zhu, and Keping Yang. 2019. Deep session interest network for click-through rate prediction. arXiv preprint arXiv:1905.06482 (2019). [12] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. 2023. Hungry Hungry Hippos: Towards Language Modeling with State Space Models. In The Eleventh International Conference on Learning Representations . [13] Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752 (2023). [14] Albert Gu, Karan Goel, and Christopher Re. 2022. Efficiently Modeling Long Sequences with Structured State Spaces. In International Conference on Learning Representations . [15] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. arXiv preprint arXiv:1703.04247 (2017). [16] Tong Guo, Xuanping Li, Haitao Yang, Xiao Liang, Yong Yuan, Jingyou Hou, Bingqing Ke, Chao Zhang, Junlin He, Shunyu Zhang, et al. 2023. Query-dominant User Interest Network for Large-Scale Search Ranking. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management . 629638. [17] Xinran He, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Yanxin Shi, Antoine Atallah, Ralf Herbrich, Stuart Bowers, et al. 2014. Practical lessons from predicting clicks on ads at facebook. In Proceedings of the eighth international workshop on data mining for online advertising . 1-9. [18] Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long short-term memory. Neural computation 9, 8 (1997), 1735-1780. [19] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. 2022. Transformer quality in linear time. In International Conference on Machine Learning . PMLR, 9099-9117. [20] Xiaohu Jiang, Dan Zhang, Wenjie Fu, Linji Yang, and Spencer Powell. 2015. Predicting the Performance of an Advertising Campaign. US Patent App. 14/292,277. [21] Kristopher Kalish, Yuan-Chyuan Sheu, Jeremy Kayne, Michael Weaver, John Ferber, and Lon Otremba. 2016. Method and system for forecasting a campaign performance using predictive modeling. US Patent App. 14/747,706. [22] Changhee Lee, William Zame, Jinsung Yoon, and Mihaela Van Der Schaar. 2018. Deephit: A deep learning approach to survival analysis with competing risks. In Proceedings of the AAAI conference on artificial intelligence , Vol. 32. [23] Jiacheng Li, Yujie Wang, and Julian McAuley. 2020. Time interval aware selfattention for sequential recommendation. In Proceedings of the 13th International Conference on Web Search and Data Mining . 322-330. [24] Ming Li, Naiyin Liu, Xiaofeng Pan, Yang Huang, Ningning Li, Yingmin Su, Chengjun Mao, and Bo Cao. 2023. FAN: Fatigue-Aware Network for ClickThrough Rate Prediction in E-commerce Recommendation. In International Conference on Database Systems for Advanced Applications . Springer, 502-514. [25] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. 2018. Modeling task relationships in multi-task learning with multi-gate mixture-ofexperts. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1930-1939. [26] Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu, and Kun Gai. 2018. Entire space multi-task model: An effective approach for estimating [48] Wush Chi-Hsuan Wu, Mi-Yen Yeh, and Ming-Syan Chen. 2015. Predicting winning price in real time bidding with censored data. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . 1305-1314. [49] Zhengtao Wu, Lan Zhang, and Hui Sheng. 2021. Efficient Ad-level Impression Forecasting based on Monotonicity and Sampling. In 2021 7th International 9 Conference on Big Data Computing and Communications (BigCom) . IEEE, 180187. [50] Haifeng Xu, Bin Gao, Diyi Yang, and Tie-Yan Liu. 2013. Predicting advertiser bidding behaviors in sponsored search by rationality modeling. In Proceedings of the 22nd International Conference on World Wide Web (Rio de Janeiro, Brazil) (WWW '13) . Association for Computing Machinery, New York, NY, USA, 1433-1444. https://doi.org/10.1145/2488388.2488513 [51] Haizhi Yang, Tengyun Wang, Xiaoli Tang, Qianyu Li, Yueyue Shi, Siyu Jiang, Han Yu, and Hengjie Song. 2021. Multi-task learning for bias-free joint ctr prediction and market price modeling in online advertising. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management . 2291-2300. [52] Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, and Shuicheng Yan. 2022. Metaformer is Actually What You Need for Vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . 10819-10829. [53] Michael Zhang, Khaled Kamal Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher Re. 2023. Effectively Modeling Time Series with Simple Discrete State Spaces. In The Eleventh International Conference on Learning Representations . [54] Qianqian Zhang, Xinru Liao, Quan Liu, Jian Xu, and Bo Zheng. 2022. Leaving No OneBehind: A Multi-Scenario Multi-Task Meta Learning Approach for Advertiser Modeling. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining (Virtual Event, AZ, USA) (WSDM '22) . Association for Computing Machinery, New York, NY, USA, 1368-1376. https://doi.org/10.1145/ 3488560.3498479 [55] Weinan Zhang, Tianxiong Zhou, Jun Wang, and Jian Xu. 2016. Bid-aware gradient descent for unbiased learning with censored data in display advertising. In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining . 665-674. [56] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Deep interest evolution network for click-through rate prediction. In Proceedings of the AAAI conference on artificial intelligence , Vol. 33. 5941-5948. [57] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through rate prediction. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1059-1068. 10",
  "A TENCENT ADVERTISING PLATFORM": "This section offers more background knowledge of the Tencent advertising platform on which AdVance has been implemented, including auction workflow, data log, and filtering rules.",
  "A.1 Real-time Bidding Workflow": "Whenever a user visits Tencent's platforms ( e.g. , Tencent Video or Tencent News), an opportunity of showing an advertisement emerges. We name such opportunities as impressions and sell them to advertisers for revenue. For each impression, the ad platform retrieves relevant ads with matched campaign criteria and initiates an auction. As shown in Fig. A-1, the platform adopts a funnel-shaped structure to handle millions of ads in the corpus, including matching, pre-ranking, ranking, and re-ranking phases. This structure strikes a balance between precise ad retrieval and timely response. Each phase progressively reduces the number of candidate ads and employs more complex and accurate algorithms. Finally, about 200 candidates can participate in the re-ranking competition, and the decision-making process considers user-ad matching, bid price, and the platform's own strategy. Figure A-1: Real-time bidding workflow and the funnelshaped structure. visit ï½ 1 M ï½ 10 ! ï½ 10 \" ï½ 10 # Matching Pre-ranking Re-ranking Corpus Ranking ï½ 10 \" Log Server Real-time Bidding Ad Advertiser 1 Advertiser 2 Advertiser 3 Campaign Orders",
  "A.2 Data Log and Embedding": "The Tencent platform stores auction records in a log server to support various data-driven algorithms such as pCTR/pCVR estimation and campaign modeling. Each record contains multi-source information: Â· User features : age, gender, location, device type, etc. Â· Contextual information : ad slot placement (web, app), content topic category, timestamp, etc. Â· History : ad content, ad category ID, corresponding user behaviors (click or purchase), etc. Â· Candidate ads : ad creative ID, campaign criteria (user attributes, demographics, keywords), ad type (CPM/CPC/CPA), bid price, auction winner, etc. Note that the platform adopts a down-sampling strategy to handle the overly large user queries (often billion-level per second), i.e. , only the auctions of a particular group of user IDs are recorded. User IDs are obtained by uniform sampling from the total ID dictionary. The ratio depends on the I/O and computation capacity of the log servers. The recorded features can be categorized into continuous ( e.g. , age, timestamp) and categorical ( e.g. , gender, location) features. We convert the auction records into a set of fixed-length embedding. Specifically, each categorical feature is represented as a vector of one-hot encoding, and each continuous feature is represented as the value itself. One-hot vectors are extremely sparse, so we employ a domain-specific embedding layer to compress them to a lowdimensional, dense vector before feeding into the model. Such an embedding layer is dedicated to each feature domain to lower the total parameter volume. Finally, we concatenate these vectors to obtain the corresponding user feature, context, and candidate ads embeddings as model input shown in Fig. 1.",
  "A.3 Manual Filtering Rules": "In Sec. 4.1, we adopt a data-driven method to capture user interest evolution, which can be visualized in Fig. A-2. However, the platform must consider various factors in the real-world business scenario to satisfy advertiser demands and enhance long-term user experience. These factors make the auction process more than a simple bid ranking problem. Hence, Tencent manually defines multiple filtering rules in the re-ranking phase to discard certain ads as a post-process, including but not limited to Â· Budget Pacing: Ensures budget is spent evenly throughout the campaign period, avoiding front-loading or overspending. Â· Frequency Capping: Limits the number of times a user sees the same ad or ads from the same industry, preventing ad fatigue and maximizing reach. Â· Brand Safety: Protects advertisers from their ads appearing alongside inappropriate or harmful content. These filtering rules are designed based on human experience and can not be described by analytic functions to insert into models. Therefore, we employ a supervised training paradigm with the data log to approximate the effect of such rules on the auction process. Figure A-2: The CTR trends vary with the number of exposures to ads of the same category. We present three categories, all showing a decline when over-exposed. We normalize the data for business privacy. Normalized CTR Exposure times w.r.t. the same category Category-1 Category-2 Category-3 11",
  "B BASELINE SETTING": "This section offers more details of the implementation of the baseline methods.",
  "B.1 Setting of Compared Methods": "CPF [8]: We adopt a mixture of two log-normal distribution. The density function is ğ‘” ( ğ‘¥ ; ğœ‡ 1 , ğœ 1 , ğœ‡ 2 , ğœ 2 , ğ‘ ) = ( 1 -ğ‘ ) ğ‘“ ( ğ‘¥ ; ğœ‡ 1 , ğœ 1 ) + ğ‘ğ‘“ ( ğ‘¥ ; ğœ‡ 2 , ğœ 2 ) and we set ğ‘ as 0.1. We train a Factorization-Machine [34] on the feature embeddings introduced in App. A.2 to estimate ğœ‡ 1 , ğœ 1 , ğœ‡ 2 , ğœ 2. We adopt the classical XGBoost model [5] to predict the pCTR/pCVR. We use the accumulated yield and cost as the final campaign performance. GMIF [29]: For the impression forecasting part, we train a DLM for each user attribute at the hour level. We follow the original recursive function in the paper but change its parameters to ğ‘Š = 5 , ğ‘‰ = 15 , ğ¶ 0 = 20. To estimate the threshold price of winning the auction, we train a Bayes net to estimate the conditional probability between input variables. Here, each variable corresponds to one feature domain, such as age, gender, location, etc. Its paper omits the model design of pCTR/pCVR, so we use DeepFM [15] trained on the feature embeddings introduced in App. A.2. We use the accumulated yield and cost as the final campaign performance. MTLN [4]: We train a DeepFM to estimate the pCTR/pCVR for each auction and multiply the result with the bid price to obtain the eCPM. We compare the calculated eCPM with the threshold price of each auction record and accumulate the yield. We feed the accumulated yield and campaign-level statistics into an MMoE model consisting of four Expert-MLPs of [128, 64], four Tower-MLPs of [64, 32, 1] with ReLU activation, and one gate model of MLP [64, 4]. The four Tower-MLPs correspond to the cost, impression, click, and conversion volumes. DLF [33]: We discretize the scale of bid price into 100 subintervals. As the number of candidate ads varies, we feed our auction representation as the DLF's input. We stack two layers of LSTM with a hidden dimension of 512 to predict the conditional win-rate for each auction. The pCTR and pCVR are estimated using the same DeepFM model as MTLN. We use the accumulated yield and cost as the final campaign performance. MTAE [51]: We feed our auction representation as the MTAE's input. MTAE adopts a multi-task paradigm and two top-models share the input: one consists of MLPs as our ğ‘“ (Â· ; ğœƒ ğ¶ğ‘‡ğ‘… ) and ğ‘“ (Â· ; ğœƒ ğ¶ğ‘‰ğ‘… ) , and the other is an MLP of [256, 100] for estimating a discrete distribution over threshold price. Here, we again evenly divide the scale of the bid price into 100 sub-intervals. MTAE further superimposes the DLF model over the bid price model as an auxiliary task. We use the accumulated yield and cost as the final campaign performance.",
  "B.2 Setting of Ablation Study": "RNN -variant stacks three layers of LSTM with a hidden dimension of 512. Transformer -variant stacks three encoder layers with four heads and a hidden dimension of 1024 (expansion = 4). S4 -variant stacks three layers of SSM with ğ‘ = 16 using the authors' opensource code.",
  "B.3 Setting of Further Investigation": "We follow the original structure of Wide&Deep [6], where the deep model is an MLP of [128, 64, 32] on our auction representation, and the wide model is a generalized linear model on the one-hot vector in App. A.2. For the DIEN [56] model, we employ a two-layer Gated Recurrent Network (GRN) with a hidden dimension of 512 to capture the interest evolution. The obtained interest vector is concatenated with user features, context, and target ad embedding. Then we feed it into an MLP of [256, 128, 1] to predict the pCTR. For the FAN [24] model, we set the length of N -point FFT as 300 and keep the other settings unchanged as the original paper. 12",
  "keywords_parsed": [],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "Flamingo: a Visual Language Model for Few-shot Learning"
    },
    {
      "ref_id": "b2",
      "title": "Layer normalization"
    },
    {
      "ref_id": "b3",
      "title": "Prefix sums and their applications"
    },
    {
      "ref_id": "b4",
      "title": "A Unified Framework for Campaign Performance Forecasting in Online Display Advertising"
    },
    {
      "ref_id": "b5",
      "title": "Xgboost: A scalable tree boosting system"
    },
    {
      "ref_id": "b6",
      "title": "Wide & deep learning for recommender systems"
    },
    {
      "ref_id": "b7",
      "title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation"
    },
    {
      "ref_id": "b8",
      "title": "Campaign Performance Forecasting for Non-guaranteed Delivery Advertising"
    },
    {
      "ref_id": "b9",
      "title": "Flashattention: Fast and memory-efficient exact attention with io-awareness"
    },
    {
      "ref_id": "b10",
      "title": "Global Ad Spend Forecast"
    },
    {
      "ref_id": "b11",
      "title": "Deep session interest network for click-through rate prediction"
    },
    {
      "ref_id": "b12",
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models"
    },
    {
      "ref_id": "b13",
      "title": "Mamba: Linear-time sequence modeling with selective state spaces"
    },
    {
      "ref_id": "b14",
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces"
    },
    {
      "ref_id": "b15",
      "title": "DeepFM: a factorization-machine based neural network for CTR prediction"
    },
    {
      "ref_id": "b16",
      "title": "Query-dominant User Interest Network for Large-Scale Search Ranking"
    },
    {
      "ref_id": "b17",
      "title": "Practical lessons from predicting clicks on ads at facebook"
    },
    {
      "ref_id": "b18",
      "title": "Long short-term memory"
    },
    {
      "ref_id": "b19",
      "title": "Transformer quality in linear time"
    },
    {
      "ref_id": "b20",
      "title": "Predicting the Performance of an Advertising Campaign"
    },
    {
      "ref_id": "b21",
      "title": "Method and system for forecasting a campaign performance using predictive modeling"
    },
    {
      "ref_id": "b22",
      "title": "Deephit: A deep learning approach to survival analysis with competing risks"
    },
    {
      "ref_id": "b23",
      "title": "Time interval aware selfattention for sequential recommendation"
    },
    {
      "ref_id": "b24",
      "title": "FAN: Fatigue-Aware Network for ClickThrough Rate Prediction in E-commerce Recommendation"
    },
    {
      "ref_id": "b25",
      "title": "Modeling task relationships in multi-task learning with multi-gate mixture-ofexperts"
    },
    {
      "ref_id": "b26",
      "title": "Entire space multi-task model: An effective approach for estimating"
    },
    {
      "ref_id": "b48",
      "title": "Predicting winning price in real time bidding with censored data"
    },
    {
      "ref_id": "b49",
      "title": "Efficient Ad-level Impression Forecasting based on Monotonicity and Sampling"
    },
    {
      "ref_id": "b50",
      "title": "Predicting advertiser bidding behaviors in sponsored search by rationality modeling"
    },
    {
      "ref_id": "b51",
      "title": "Multi-task learning for bias-free joint ctr prediction and market price modeling in online advertising"
    },
    {
      "ref_id": "b52",
      "title": "Metaformer is Actually What You Need for Vision"
    },
    {
      "ref_id": "b53",
      "title": "Effectively Modeling Time Series with Simple Discrete State Spaces"
    },
    {
      "ref_id": "b54",
      "title": "Leaving No One Behind: A Multi-Scenario Multi-Task Meta Learning Approach for Advertiser Modeling"
    },
    {
      "ref_id": "b55",
      "title": "Bid-aware gradient descent for unbiased learning with censored data in display advertising"
    },
    {
      "ref_id": "b56",
      "title": "Deep interest evolution network for click-through rate prediction"
    },
    {
      "ref_id": "b57",
      "title": "Deep interest network for click-through rate prediction"
    }
  ]
}