{
  "Integrity and Junkiness Failure Handling for Embedding-based Retrieval: A Case Study in Social Network Search": "Wenping Wang Meta Platforms, Inc Menlo Park, California, USA wenping@meta.com Yunxi Guo Meta Platforms, Inc Menlo Park, California, USA yguo0418@meta.com Chiyao Shen Meta Platforms, Inc Menlo Park, California, USA chiyao@meta.com Shuai Ding Meta Platforms, Inc Menlo Park, California, USA sding@meta.com Guangdeng Liao Meta Platforms, Inc Menlo Park, California, USA guangdeng@meta.com",
  "Pramodh Karanth Prabhakar": "Meta Platforms, Inc Menlo Park, California, USA pramodh@meta.com",
  "ABSTRACT": "",
  "KEYWORDS": "Embedding based retrieval has seen its usage in a variety of search applications like e-commerce, social networking search etc. While the approach has demonstrated its efficacy in tasks like semantic matching and contextual search, it is plagued by the problem of uncontrollable relevance. In this paper, we conduct an analysis of embedding-based retrieval launched in early 2021 on our social network search engine, and define two main categories of failures introduced by it, integrity and junkiness. The former refers to issues such as hate speech and offensive content that can severely harm user experience, while the latter includes irrelevant results like fuzzy text matching or language mismatches. Efficient methods during model inference are further proposed to resolve the issue, including indexing treatments and targeted user cohort treatments, etc. Though being simple, we show the methods have good offline NDCG and online A/B tests metrics gain in practice. We analyze the reasons for the improvements, pointing out that our methods are only preliminary attempts to this important but challenging problem. We put forward potential future directions to explore.",
  "CCS CONCEPTS": "· Information systems → Web and social media search ; Multimedia and multimodal retrieval ; Novelty in information retrieval. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '23, July 23-27, 2023, Taipei, Taiwan © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-9408-6/23/07...$15.00 https://doi.org/10.1145/3539618.3591831 Embedding, information retrieval, social network search, search relevance, integrity and junkiness failure",
  "ACMReference Format:": "Wenping Wang, Yunxi Guo, Chiyao Shen, Shuai Ding, Guangdeng Liao, Hao Fu, and Pramodh Karanth Prabhakar. 2023. Integrity and Junkiness Failure Handling for Embedding-based Retrieval: A Case Study in Social Network Search. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '23), July 23-27, 2023, Taipei, Taiwan. ACM,Taipei, Taiwan, 5 pages. https://doi.org/10.1145/3539618.3",
  "1 INTRODUCTION": "Traditional search engines rely a lot on lexical matching for retrieving relevant results from a given query (mostly reverted index for documents). However this approach has the well known shortcoming in terms of semantic match and personalized matching, which are common and vital in the success of commercial search products like amazon, google, etc. Starting from facebook's approach [3], embedding based retrieval is developed to deal with these scenarios, and have proved their success in real world business like amazon[8], taobao [4], facebook [3], jingdong [9], baidu [6] etc. Though having different assumptions, improvements in the deep model training and use cases, these embedding systems all share a similar two tower structure. During training, the two tower model is trained for user and candidate together, with various use case specific embeddings like user context, query n-gram, etc as inputs. A similarity score is computed as distance between two embeddings. In online inference, candidate embedding is pre-calculated and saved into index, while user side embedding is computed on the fly. Such approaches, though having proved their success in industry, suffer from low controllability of relevance. From TaoBao's learning, the 'inability to exactly matching query terms' [4] has caused user complaints and bad cases in production. Based on the survey result on our social network search product, more than half Hao Fu Meta Platforms, Inc Menlo Park, California, USA haofu@meta.com SIGIR '23, July 23-27, 2023, Taipei, Taiwan Wenping Wang, Yunxi Guo, Chiyao Shen, Shuai Ding, Guangdeng Liao, Hao Fu, and Pramodh Karanth Prabhakar of the negative user feedback are caused by embedding-based retrieval. The issue greatly hurts the user experience. Meanwhile, it is not an easy task to address these failures. Few previous works, with their success stories, have explicitly called out the issue. Taobao [4], put forward a relevance control module which applies another 'and' with the retrieved results. The method drops the unmatched results with query, requiring results to have both embedding similarity and text match, greatly limiting the embedding retrieval's power in finding semantic matching or personalized results for users. In this paper, we start from the status of failures on the platform generated by embedding-based retrieval (EBR), classifying the failures into two main categories, integrity and junkiness. The former refers to hard failures of embedding-based retrieval like misinformation, hate speech, porny results while the latter refers to general irrelevance between query and doc. We then propose separate treatments that are applied during model inference. For integrity violation cases, we apply strict clean up in index building. For junkiness feedback, we purpose customized discarding threshold method that discarding EBR retrieved results in a customized way, and triggering condition control of EBR. Being simple and generic to the two tower structure, we show with both offline metric and online A/B test results that it gives us surprisingly good metric wins. In the end, we point out there is still a long way to go in the domain and several potential future directions. The purposed methods have been applied in our search engine production since 2022. The work is based on our social network search engine product - group search, where we have a baseline launch of the embedding based retrieval in early 2021 based on [3]. The group refers to a product on our social network platform, providing a community of people with similar interests, goals or a shared target a channel to engage and explore. The search engine has hundreds of millions of queries everyday, and one of the main use cases are finding directly connected group and exploring unconnected groups. For downstream actions, Connected Navigation , denoted as CN , refers to user being navigate to targeted connected group after clicking, while Unconnected Navigation , denoted as UN , represents for users join a group. Our goal of the paper is mainly to draw attention to the problem of EBR generated failures in real world search applications, in the setting of social network search especially, and provide our preliminary attempts to the problem. The rest of the paper is organized as follows. In the next section, we provide an overview of the initial status of the problem on our search engine. Section 3 talks about our methodology in detail. Section 4 discusses the metrics we used for offline evaluation, and the online A/B test setting with the results. We discuss the remaining problem in the end of Section 4. We conclude our work in the last section and point out the potential future directions for improvement.",
  "2 EBR STATUS ANALYSIS": "We use search log to generate queries and ask human raters to rate our search engine returned sessions of results. (Section 4 gives detailed offline metrics for analysis). We selected 15k queries, representing major query intents and languages. Table 1: Distribution of EBR failures",
  "2.1 Baseline Model Architecture": "Our baseline model implemented and launched in 2021 is based on the system in [3], and detailed model architecture in [7]. We adopted several changes in input compared with [7] that resulted in good online performance. For query tower, we added country, region, query text and trigram. The first three embedding dim is 16 and the latter two are 128. For doc tower, doc title, description, title trigram, language, region, topic, country are used. The title, description and trigram has dim 128 while the rest has dim 32. Query text, doc title, doc description have the top importance in our model. We used batch negative sampling and scaled loss (a multiplier on model output in loss function) during training. We gather training data from search log. The rest details remains the same as [7]. Our model AUC is 0.79. The baseline launch impact is shown in the bottom of table 3.",
  "2.2 Results": "2.2.1 Total failure rate and EBR trigger rate. The total prod failure rate are 7.3%, 32.6%, and 43.1% at top 1, 3, and 5. By checking the source of the generated failure results, roughly 70% are generated by the EBR node during the retrieval stage, for our group search product. 2.2.2 Break down failure reason. Table 1 shows the reasons of failure and corresponding percentage.",
  "2.3 Failure Categories": "From the practice of our selected social network search system, we define the failures into two levels, · junkiness failures : irrelevant results given clear query intent, like random people returned given exact match connected friend name query · integrity failures : serious failures that would hurt users severely, like porny result, hate speech, fake news, etc The two main categories of Embedding-based retrieval failures differs mostly in the severity. Ideally we would want search engine to provide as few irrelevant results as possible, and no integrity violations at all. In practice, Embedding-based retrieval serves those semantic matching case while generating a lot of irrelevant results at the same time. Integrity and Junkiness Failure Handling for Embedding-based Retrieval: A Case Study in Social Network Search SIGIR '23, July 23-27, 2023, Taipei, Taiwan",
  "3 METHODOLOGY": "In the below sections, we purpose several treatments after the two tower model (in Section 2.1) is trained. The baseline model uses cosine similarity to compute the similarity score and apply a threshold to either keep or reject the retrieved results.",
  "3.1 Junkiness Failure Reduction": "3.1.1 Sigmoid. One way to control the relevance of output is to view the similarity score as a probability measure of document relevance for a given user + query (user and query embeddings are in the query tower), i.e.  This naturally remind us of the sigmoid function, as its basic probabilistic intuition are similar. The idea behind is though we discard results if they fall below a threshold of similarity, different (user+query, doc) pair have different best trade-off point, for selected query intent, user cohort and doc topic. Applying sigmoid help centering the output distribution across different input segments, and transform those high confidence predictions, either positive or negative, towards 1 or 0. We choose the logistic function for its simplicity in implementation  the s_score refers to the output from similarity function, and depending on the specific model and application case, we apply different linear transform before applying the sigmoid function. 3.1.2 Discarding threshold customization. We can go further from the idea in 3.1.1. Instead of centering the output distribution, we seek to discard retrieved results in a customized way for different segments of inputs, i.e. a customized threshold for different user+query segment. This is actually the same topic in the domain of machine learning fairness [2] [1] [5]. Theintuition behind is for application like social network search, the potential candidate size number far exceeds the allowed number of results to return on mobile or web. Therefore the goal of the EBR design is to pick top semantic matching items in a very big candidate pool, i.e. precision driven. In our practice, we fit a simple linear regression model to do the customization.  /u1D465 /u1D456 /u1D457 above represents the input feature j for result i we believe our two tower model behaves differently on. We use user country, language, query intent and doc source type (CN or UN) to fit as input. /u1D466.alt /u1D456 is a threshold that discards results confidently. We use the following method to choose it. The trained model is first launched into production for a short period without any discarding (i.e. threshold being -inf) to gather logging data. With these data, we select from those results that have click or downstream action (group join or post comment after join, etc), and apply a threshold /u1D466.alt /u1D456 /u1D45D such that top p% of these results are left. For example, all the clicked unconnected English groups in US with people name intent query have similarity score within [0.2, 1.0], while the /u1D466.alt /u1D456 30 threshold is 0.7. The hyper parameter p is chosen globally via A/B test. In our practice, p is 0.9. We use MSE (mean squared error) to compute the /u1D6FD . We will show in section 4 that this method contributes most to the online evaluation metric win. 3.1.3 Triggering control. Another observation is that EBR does not fit for some clear query intents, for instance, directly connected celebrity finding intents. Mismatch result types also negates the usage, like friend photo intent with group EBR triggered. Such hypothesis can be easily verified by the /u1D466.alt /u1D456 /u1D45D distribution on the given scenarios. We disable the EBR sources completely for such cases found in analysis, and delegate the text based retrieval to handle the search request in replace. Examples are in section 4.",
  "3.2 Integrity Failure Reduction": "Unlike junky results, integrity violation results are more critical to a user-facing search engine. In general we should avoid such cases from happening at all, even at some cost. Once a result is tagged as integrity sensitive, a ground truth label is saved in database. With this label, we apply · ranking filtering : for edge cases lies between recommendable and non-recommendable. Examples like hate speech on opinion seeking topics, etc. We apply rule based demotion or model based demotion in ranking layers for this cases. · indexing removal : results that should not be returned at any time. Examples like child pornography, etc. We directly remove these pre-computed embedding from index to avoid retrieving at all.",
  "4 EXPERIMENTS AND ANALYSIS": "In this section, we report the statistics and analysis of offline and online metrics for our methods. We show that our treatment have surprisingly good results in failure reduction and online metric win, and the scale of online metric win is comparable to that of the provided baseline.",
  "4.1 Evaluation Metric": "4.1.1 Offline metrics for failure control. Relevance metric are widely used in search engines as offline metrics for quick development purpose. For junkiness failures, we adopted Normalized Discounted Cumulative Gain (NDCG) as offline evaluation measure to give the relevance labels for top 5 ranked (query, result) pairs. Results will be calculated on a user session level, i.e. given user and query, whether a session of returned result is better in relevance, denoted as NDCG . For integrity failures, the top 10 results will be checked in a user session and mark it to be integrity violated if one of the results fails, denoted as NONREC for being not recommendable. 4.1.2 Online Metrics as Golden Targets. The most important metrics for social network product are the online metrics, which are directly related to app's popularity and revenue. When experiment goes to the online A/B test, we use online metrics to judge whether a social network app is really improved by the experiment. We selected below representative online metrics in our experiments: (1) Click and Click though rate : Click measures total result click count while Click though rate measures the ratio of SIGIR '23, July 23-27, 2023, Taipei, Taiwan Wenping Wang, Yunxi Guo, Chiyao Shen, Shuai Ding, Guangdeng Liao, Hao Fu, and Pramodh Karanth Prabhakar Table 2: The offline ndcg metrics comparison between Control and Test for each treatment. The number in boldface means that it is statistically significant ( /u1D45D < 0 . 05 ) compared to the Control. Table 3: The online engagement metrics comparison between Control and Test for each treatment. The number in boldface means that it is statistically significant ( /u1D45D < 0 . 05 ) compared to the Control. users clicks vs search results shown counts. Improving the click through rate can make the search result page to be more effective. Click is denoted as CLK . Click though rate is denoted as CTR . (2) Search referred connections : this metric measures how many connections are built via search engine. Users can add friends/join groups/following pages via people/group/page search, respectively. An improved search results can bring more connections for social network. This metric can be statistically significantly moved via a single experiment in several days. Denoted as GJ for the group join case.",
  "4.2 Results and Findings": "We conduct the experiments during second half of 2022 from July to Dec. Each experiment will allocate 4% of total user traffic and split to control and test groups randomly. For single experiment, we monitor the offline and online metrics for at least one week and make decision whether to launch it based on overall metric evaluations. We read and compare the metrics between Test and Control user groups to represent the performance of each launch. 4.2.1 Offline experiment results. In Table 2, we show the comparison results with the above methods. The downward trend of junkiness reduction is clear. 4.2.2 Online A/B test. In Table 3, we show the results of different methods' online metric impact. We add a row in the bottom in the table 3 to provide the online metric movement during the baseline launch of our group search EBR in early 2021. Two key insights can be drawn from the above results. (1) Failure handling in EBR is an important issue in terms of both offline and online metric. (2) Though majority of them target on discarding results, our purposed methods are effective in practice. Thefirst insight is from comparing online metrics in the last row and the rest rows in Table 3, we can see the accumulated online impact of failure handling methods are comparable to the baseline. Meanwhile, according to Table 2 row 4 and Table 3 row 4, with the NDCG improvement on Unconnected Navigation, there is a statistically significant improvement on online metrics.",
  "4.3 Jobs To Be Done": "Given the positive results of our purposed solutions, the journey is not yet finished. We compare the user survey conducted between the end of 2021 and the end of 2022 (during which we launched the solutions in the paper), the junkiness and integrity failures introduced by EBR remain to be a big concern. For instance, the location mismatch dropped by 2% during the period to 18%, the language mismatch remains to be at the same ratio. Clearly our posttraining methods are not designed to handle these specific failure categories. In contrast, the EBR usage in the group search product is seeing an upward trending from 60% to 70%. The reason for the change is diversified, like inventory shift and text based retrieval being removed to save infra cost, etc. We still need to put focus on the problem and if possible, more targeted solution on each of the failure categories in the long run. Integrity and Junkiness Failure Handling for Embedding-based Retrieval: A Case Study in Social Network Search SIGIR '23, July 23-27, 2023, Taipei, Taiwan",
  "5 CONCLUSIONS AND FUTURE WORK": "In this paper we explicitly call out two failure categories introduced by applying embedding-based retrieval, and several techniques to address the issue with offline and online evaluation. There are several potential directions to explore. The user cohort modeling in our linear regression model is preliminary. Dimensions like user demo-graphical feature, social context feature, preference and search history are not well studied. Feature selection and regularization are not included. A more direct approach is to understand how deep retrieval model behave in different user cohorts and improve directly in the model training stage, for these different failure categories.",
  "ACKNOWLEDGMENTS": "The author would like to thank Haocheng Wu and Qing Liu for their generous help during the development, discussion and experimenting of this paper.",
  "COMPANY PORTRAIT": "Meta Platforms, Inc. is an American technology company based in Menlo Park, California. The company owns Facebook, one of the largest social media and social networking service, among other products and services. It strives to give people the power to build community, bring the world closer together.",
  "PRESENTER'S BIO": "Wenping Wang is a software engineer at Facebook. His team is responsible for Facebook search's quality, the retrieval, ranking, etc. Before that, he received Master's degree from Carnegie Mellon University. He is interested in computer systems, machine learning in general and their real-world applications.",
  "REFERENCES": "[1] Mengnan Du, Fan Yang, Na Zou, and Xia Hu. 2020. Fairness in deep learning: A computational perspective. IEEE Intelligent Systems 36, 4 (2020), 25-34. [2] Kenneth Holstein, Jennifer Wortman Vaughan, Hal Daumé, Miro Dudik, and Hanna Wallach. 2019. Improving Fairness in Machine Learning Systems: What Do Industry Practitioners Need?. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (CHI '19) . Association for Computing Machinery, New York, NY, USA, 1-16. https://doi.org/10.1145/3290605.3300830 [3] Jui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip Pronin, Janani Padmanabhan, Giuseppe Ottaviano, and Linjun Yang. 2020. Embeddingbased retrieval in facebook search. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 2553-2561. [4] Sen Li, Fuyu Lv, Taiwei Jin, Guli Lin, Keping Yang, Xiaoyi Zeng, Xiao-Ming Wu, and Qianli Ma. 2021. Embedding-based product retrieval in taobao search. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining . 3181-3189. [5] Yunqi Li, Hanxiong Chen, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2021. User-oriented fairness in recommendation. In Proceedings of the Web Conference 2021 . 624-632. [6] Yiding Liu, Weixue Lu, Suqi Cheng, Daiting Shi, Shuaiqiang Wang, Zhicong Cheng, and Dawei Yin. 2021. Pre-trained language model for web-scale retrieval in baidu search. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining . 3365-3375. [7] Yiqun Liu, Kaushik Rangadurai, Yunzhong He, Siddarth Malreddy, Xunlong Gui, Xiaoyi Liu, and Fedor Borisyuk. 2021. Que2Search: fast and accurate query and document understanding for search at Facebook. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining . 3376-3384. [8] Priyanka Nigam, Yiwei Song, Vijai Mohan, Vihan Lakshman, Weitian Ding, Ankit Shingavi, Choon Hui Teo, Hao Gu, and Bing Yin. 2019. Semantic product search. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 2876-2885. [9] Han Zhang, Songlin Wang, Kang Zhang, Zhiling Tang, Yunjiang Jiang, Yun Xiao, Weipeng Yan, and Wen-Yun Yang. 2020. Towards personalized and semantic retrieval: An end-to-end solution for e-commerce search via embedding learning. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . 2407-2416.",
  "keywords_parsed": [
    "Embedding",
    "information retrieval",
    "social network search",
    "search relevance",
    "integrity and junkiness failure"
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "Fairness in deep learning: A computational perspective"
    },
    {
      "ref_id": "b2",
      "title": "Improving Fairness in Machine Learning Systems: What Do Industry Practitioners Need?"
    },
    {
      "ref_id": "b3",
      "title": "Embedding-based retrieval in facebook search"
    },
    {
      "ref_id": "b4",
      "title": "Embedding-based product retrieval in taobao search"
    },
    {
      "ref_id": "b5",
      "title": "User-oriented fairness in recommendation"
    },
    {
      "ref_id": "b6",
      "title": "Pre-trained language model for web-scale retrieval in baidu search"
    },
    {
      "ref_id": "b7",
      "title": "Que2Search: fast and accurate query and document understanding for search at Facebook"
    },
    {
      "ref_id": "b8",
      "title": "Semantic product search"
    },
    {
      "ref_id": "b9",
      "title": "Towards personalized and semantic retrieval: An end-to-end solution for e-commerce search via embedding learning"
    }
  ]
}