{"title": "Cloud-Client Cooperative Deep Learning for Temporal Recommendation in the Post-GDPR Era", "authors": "Jialiang Han; Yun Ma", "pub_date": "2020-01", "abstract": "Mobile devices enable users to retrieve information at any time and any place. Considering the occasional requirements and fragmentation usage pattern of mobile users, temporal recommendation techniques are proposed to improve the efficiency of information retrieval on mobile devices by means of accurately recommending items via learning temporal interests with short-term user interaction behaviors. However, the enforcement of privacy-preserving laws and regulations, such as the General Data Protection Regulation (GDPR), may overshadow the successful practice of temporal recommendation. The reason is that state-of-the-art recommendation systems require to gather and process the user data in centralized servers but the interaction behaviors data used for temporal recommendation are usually non-transactional data that are not allowed to gather without the explicit permission of users according to GDPR. As a result, if users do not permit services to gather their interaction behaviors data, the temporal recommendation fails to work. To realize the temporal recommendation in the post-GDPR era, this paper proposes \ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50, a cloud-client cooperative deep learning framework of mining interaction behaviors for recommendation while preserving user privacy. \ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50 constructs a global recommendation model on centralized servers using data collected before GDPR and fine-tunes the model directly on individual local devices using data collected after GDPR. We design two modes to accomplish the recommendation, i.e., pull mode where candidate items are pulled down onto the devices and fed into the local model to get recommended items, and push mode where the output of the local model is pushed onto the server and combined with candidate items to get recommended ones. Evaluation results show that \ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50 achieves comparable recommendation accuracy to the centralized approaches, with minimal privacy concern.", "sections": [{"heading": "INTRODUCTION", "text": "Due to the information explosion, recommending interesting information to users becomes more and more important [35]. With the rapid growth and widespread of smartphones and tablet computers, users are able to access the Internet to retrieve various kinds of information and dive into oceans of applications without the restriction of time and locations [6,25,46]. The special characteristics of user interactions on mobile devices call for new requirements of recommendation services. On one hand, users are likely to raise occasional requirements according to the situation they stay. As a result, recommendation services should efficiently capture users' the Least Absolute Shrinkage and Selection Operator (Lasso) regression, or \ud835\udc3f 1 regularization, to sparsify the item embeddings of item candidate set in pull mode or user embeddings in push mode. To reduce computational overhead on the devices, we perform an Automated Gradual Pruner (AGP) [51] to compress the recommendation model.\nTo evaluate the performance of the proposed framework, we conduct experiments in terms of accuracy, communication overhead and computational overhead, based on a public user behavior dataset from Taobao [50], a large-scale e-commercial application in China, which contains nearly a million users and nearly 100 million interactions of users and items. The experimental results show that we achieve up to 10x reduction in communication overhead, reduce the computational overhead of on-device training towards the range of computational resources of middle-class mobile devices, with minimal loss in accuracy. We summary our contributions of \ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50 as follows.\n\u2022 We propose a cloud-client cooperative deep learning framework for temporal recommendation, which alleviates the risks of user privacy leakage and violation of privacy-related laws, in benefit of recommendation service providers. \u2022 We reduce both communication and computational overhead of individual devices by AGP weight pruning and Lasso regression (\ud835\udc3f 1 regularization), to make sure that it is more feasible and less resource-consuming to perform on-device training of personalized models on real-world mobile devices, in benefit of application developers. \u2022 We evaluate the performance of \ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50 on a large-scale public dataset, and the experimental results show that we achieve up to 10x reduction in communication overhead and reduce the computational overhead towards the range of middle-class mobile devices, with minimal loss in accuracy.\nThe rest of the paper is organized as follows. In Section 2, we formally define the problem of temporal recommendation in the post-GDPR era. In Section 3, we present the general framework of \ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50 and optimizations to reduce communication and computational overhead. In Section 4, we evaluate \ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50 on a large-scale ecommerce dataset to demonstrate its effectiveness and efficiency. In Section 5, we discuss several limitations of our framework and possible solutions. In Section 6, we compare \ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50 with related work. Finally, Section 7 concludes the paper.", "publication_ref": ["b34", "b5", "b24", "b45", "b50", "b49"], "figure_ref": [], "table_ref": []}, {"heading": "PROBLEM DEFINITION", "text": "In this section, we present the problem definition of temporal recommendation in the post-GDPR era. The problem statement of recommendation is that, given a recommendation context \ud835\udc36, recommendation is to learn a function \ud835\udc53 which maps \ud835\udc36 to the recommendation target \ud835\udc61 : \ud835\udc61 \u21d0 \ud835\udc53 (\ud835\udc36). The recommendation context \ud835\udc36 can be historical user interaction with the data source, or the data from other users, etc. The recommendation target \ud835\udc61 is to predict the likelihood \ud835\udc5d \ud835\udc56,\ud835\udc57 that a user \ud835\udc56 would prefer an item \ud835\udc57. Now let us define some important concepts in our setting:\nDefinition 1 (Transactional Data): Transactional data are user interaction data that require logging-in to produce and collect in the recommendation system, including purchasing, adding to chart, and adding to favorites, etc. We use \ud835\udc47 \ud835\udc56,\ud835\udc61 to represent the item ID that user \ud835\udc56 purchases, adds to chart, or adds to favorite at timestamp \ud835\udc61.\nDefinition 2 (Non-transactional Data): Non-transactional data are user interaction data that do not require logging-in to produce and collect in the recommendation system, such as clicking. We use \ud835\udc41\ud835\udc47 \ud835\udc56,\ud835\udc61 to represent the item ID that user \ud835\udc56 clicks at timestamp \ud835\udc61.\nDefinition 3 (Privacy-preserving Recommendation): A traditional recommendation system requires uploading all interaction data to complete the recommendation, which faces the problem of privacy leakage. However, the goal of a privacy-preserving recommendation is to eliminate the uploading of any unnecessary user privacy data, i.e. non-transactional data, while still providing a customized recommendation, i.e. the next click. In order to build a privacy-preserving recommendation system, non-transactional data should be kept on-device during the whole process of recommendation, which means it is not allowed to upload any raw data or intermediate results of non-transactional data. However, the cloud can still leverage the transactional data to provide a rough recommendation candidate set.\nDefinition 4 (Recommendation Item Candidate Set): As mentioned above, a candidate set \ud835\udc36\ud835\udc4e\ud835\udc5b\ud835\udc51\ud835\udc56\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc52 \ud835\udc56,\ud835\udc61 of user \ud835\udc56 at timestamp \ud835\udc61, can be built or learned from all users' transactional data before timestamp \ud835\udc61.\nDefinition 5 (Cloud-Client Cooperative Recommendation): Because non-transactional data are not allowed to upload during privacy-preserving recommendation, while transactional data can only help build a rough recommendation candidate set, a cloud-client cooperative recommendation system is required to learn from the local non-transactional data on the device of each user, and complete the process of personalized recommendation. Given all local non-transational data \ud835\udc46 \ud835\udc41\ud835\udc47 ,\ud835\udc56,\ud835\udc61 = {\ud835\udc41\ud835\udc47 \ud835\udc56,\ud835\udc61 \ud835\udc58 |\ud835\udc61 \ud835\udc58 < \ud835\udc61 } of user \ud835\udc56 before timestamp \ud835\udc61, and the candidate set \ud835\udc36\ud835\udc4e\ud835\udc5b\ud835\udc51\ud835\udc56\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc52 \ud835\udc56,\ud835\udc61 of user \ud835\udc56 at timestamp \ud835\udc61, the likelihood \ud835\udc5d \ud835\udc56,\ud835\udc57,\ud835\udc61 that user \ud835\udc56 would click item \ud835\udc57 at timestamp \ud835\udc61 is predicted by the on-device recommendation model \ud835\udc40 \ud835\udc56,\ud835\udc61 of user \ud835\udc56 at timestamp \ud835\udc61: \ud835\udc5d \ud835\udc56,\ud835\udc57,\ud835\udc61 = \ud835\udc40 \ud835\udc56,\ud835\udc61 (\ud835\udc46 \ud835\udc41\ud835\udc47 ,\ud835\udc56,\ud835\udc61 , \ud835\udc36\ud835\udc4e\ud835\udc5b\ud835\udc51\ud835\udc56\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc52 \ud835\udc56,\ud835\udc61 ; \ud835\udc57).\n(\n)1\nNote that the on-device recommendation model \ud835\udc40 \ud835\udc56,\ud835\udc61 is updated (fine tuned) from time to time for each user \ud835\udc56, as the computation resources of user devices are not sufficient as those of cloud, the computational overhead of it should be relatively small to match the limited resources of devices.\nIf we assume that there exists a loss function L : [0, 1] \u00d7 {0, 1} \u00d7 \u0398 \u2192 R, where \u0398 is the parameter set domain of the on-device recommendation model, the objective of cloud-client cooperative recommendation is to solve the following optimization problem:\n\ud835\udf03 * = \ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc5a\ud835\udc56\ud835\udc5b \ud835\udf03 \u2211\ufe01 \ud835\udc61 \u2211\ufe01 \ud835\udc56 \u2211\ufe01 \ud835\udc57 L (\ud835\udc5d \ud835\udc56,\ud835\udc57,\ud835\udc61 , \ud835\udc59\ud835\udc4e\ud835\udc4f\ud835\udc52\ud835\udc59 \ud835\udc56,\ud835\udc57,\ud835\udc61 ; \ud835\udf03 ), \ud835\udc60\ud835\udc62\ud835\udc4f \ud835\udc57\ud835\udc52\ud835\udc50\ud835\udc61 \ud835\udc61\ud835\udc5c \ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc60 \ud835\udc5c\ud835\udc5b \ud835\udc50\ud835\udc5c\ud835\udc5a\ud835\udc5d\ud835\udc62\ud835\udc61\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc4e\ud835\udc59 \ud835\udc5c\ud835\udc63\ud835\udc52\ud835\udc5f\u210e\ud835\udc52\ud835\udc4e\ud835\udc51,(2)\nwhere L can be a classification loss like the cross-entropy loss, or a learning-to-rank loss [33,36,39] ", "publication_ref": ["b32", "b35", "b38"], "figure_ref": [], "table_ref": []}, {"heading": "APPROACH", "text": "In this section, we first introduce the working flow of both pull mode and push mode of \ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50, after that, we explain two fundamental structures in this framework in detail. In addition, we explain the optimization methods to tackle with the communication and computational overhead challenges. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Working Flow Overview", "text": "Figure 1 and Figure 2 show the working flows of pull mode and push mode, respectively. First, we train a Gate Recurrent Unit (GRU) [9] based recommendation model, i.e. global recommendation model, with existing user behavior data collected before GDPR for all users, and extract a recommendation item candidate set through a collaborative filtering based model on the cloud, and push the global recommendation model to individual devices. Second, we train a GRU based recommendation model, i.e. personalized recommendation model, with real-time user behavior data collected after GDPR for each user on his device, and extract a unique user embedding from his personalized recommendation model. Third, individual devices pull a recommendation item candiate set from the cloud (in pull mode), or push user embeddings toward the cloud (in push mode), and in both modes, we combine the latent information of user embeddings and item embeddings to calculate a score representing the probability a user would visit an item in future.", "publication_ref": ["b8"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Filtering Model", "text": "In order to reduce the computational overhead of ranking the combination of a given user embedding and recommendation item embeddings, we use a k-nearest neighbor item-based collaborative filtering (item-CF) model [10,34] to extract a recommendation item candidate set from the whole item set, whose main idea is that a user is likely to purchase an item that shares similar features, i.e. user-item interactions, with items he purchases before. The filtering model predicts a rough probability \ud835\udc5d \ud835\udc58,\ud835\udc5a , representing the probability user \ud835\udc58 clicks item \ud835\udc5a, from a user-item interactive matrix (\ud835\udc65 \ud835\udc58\ud835\udc5a ) \ud835\udc3e * \ud835\udc40 where \ud835\udc65 \ud835\udc58\ud835\udc5a represents whether user \ud835\udc58 purchases item \ud835\udc5a.\nIn particular, \ud835\udc5d \ud835\udc58,\ud835\udc5a = \ud835\udc56 \ud835\udc4f \ud835\udc60\ud835\udc56\ud835\udc5a (\ud835\udc56 \ud835\udc5a ,\ud835\udc56 \ud835\udc4f )\ud835\udc65 \ud835\udc58,\ud835\udc4f \ud835\udc56 \ud835\udc4f |\ud835\udc60\ud835\udc56\ud835\udc5a (\ud835\udc56 \ud835\udc5a ,\ud835\udc56 \ud835\udc4f ) | , where \ud835\udc56 \ud835\udc5a represents the \ud835\udc5a \ud835\udc61\u210e column of the interactive matrix (\ud835\udc65 \ud835\udc58\ud835\udc5a ) \ud835\udc3e * \ud835\udc40 , which is a vector with a length of \ud835\udc3e, \ud835\udc60\ud835\udc56\ud835\udc5a(\ud835\udc56 \ud835\udc5a , \ud835\udc56 \ud835\udc4f ) = \ud835\udc56 \ud835\udc5a \u2022\ud835\udc56 \ud835\udc4f \u2225\ud835\udc56 \ud835\udc5a \u2225 \u2225\ud835\udc56 \ud835\udc4f \u2225 . Note that the interactive matrix is composed of only purchase records, instead of click records, which is necessary for the basic business of any online organization providing an e-commercial or likewise service. Therefore, to use at least the basic service provided like online shopping or rating movies, users have no choice but to allow uploading these purchase records, which should not be considered as privacy from a GDPR perspective. However, it is still not common practice to use an interactive matrix of purchase to predict the action of click. Therefore, we only use this filtering model for guidelines and obtain a candidate set from the original recommendation item set. We manually set a threshold probability \ud835\udc5d \ud835\udc50\ud835\udc4e\ud835\udc5b\ud835\udc51\ud835\udc56\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc52 and obtain a personalized candidate set \ud835\udc36\ud835\udc4e\ud835\udc5b\ud835\udc51\ud835\udc56\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc52 \ud835\udc58 = \ud835\udc5a|\ud835\udc5d \ud835\udc58,\ud835\udc5a > \ud835\udc5d \ud835\udc50\ud835\udc4e\ud835\udc5b\ud835\udc51\ud835\udc56\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc52 .", "publication_ref": ["b9", "b33"], "figure_ref": [], "table_ref": []}, {"heading": "Recommendation Model", "text": "Inspired by GRU4REC [16], we use a Gated Recurrent Unit (GRU) based deep neural network to model the temporal information behind the behavior histories of users and then extract user embeddings. The reason why we use GRU instead of classic RNN units or LSTM [18] is that GRU based models perform better than other units on this temporal recommendation task [16,31]. The detailed network framework is shown in detail in Figure 3. The input of the recommendation model is a sequence of actual items in a one-hot encoding style, which is projected into a low-dimensional dense embedding through the embedding layer. In pull mode, we need to pass the dense embedding into a sparsity layer to ensure that the output item embedding is as sparse as possible, in order to reduce the communication overhead between the cloud and the client. Meanwhile, in push mode, we simply output the raw dense item embedding, for it need not to be downloaded by the client. After that, the sequence of item embeddings is passed into one or several GRU layers to mine the temporal information behind the user behavior history. If multiple GRU layers are used, the input of the next layer is the hidden states of the last layer. The output of GRU layer(s) represents the probability distribution of the next item the user will click, which is named as the user embedding. In push mode, we need to pass the user embedding into a sparsity layer to ensure that the output user embedding is as sparse as possible, in order to reduce the communication overhead between the cloud and the client. Meanwhile, in pull mode, we simply obtain the raw dense user embedding, for it need not to be uploaded to the cloud. Finally, the user embedding is passed into one or multiple feed-forward layer(s) to output the scores of actual items that the user is about to click next.", "publication_ref": ["b15", "b17", "b15", "b30"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Sparsity of Embeddings and the Recommendation Model", "text": "There are two challenges we are supposed to tackle. To reduce network communication overhead between the cloud and devices, we perform the Lasso regression, or \ud835\udc3f 1 regularization, to sparsify the item embeddings of item candidate set in pull mode or user embeddings in push mode. To reduce computational overhead on the devices, we perform an Automated Gradual Pruner (AGP) [51] to compress the recommendation model.\nWe perform Lasso regression to extract the main information in item embeddings or user embeddings by sparse encoding. We formulate Lasso regression as follows.\n\u00ca\ud835\udc56 = \ud835\udc38 \ud835\udc56 , \ud835\udc56 \ud835\udc53 |\ud835\udc38 \ud835\udc56 | > \ud835\udefe 0, \ud835\udc5c\ud835\udc61\u210e\ud835\udc52\ud835\udc5f\ud835\udc64\ud835\udc56\ud835\udc60\ud835\udc52 , L \ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60\ud835\udc5c = \u2211\ufe01 \ud835\udc56 \u00ca\ud835\udc56 , L = L + \ud835\udf06 \ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60\ud835\udc5c L \ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60\ud835\udc5c ,(3)\nwhere \ud835\udc38 \ud835\udc56 is the raw item embedding or user embedding, \u00ca\ud835\udc56 is the sparse embedding, L is the original loss function, which is a cross entropy loss in our work, \ud835\udf06 \ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60\ud835\udc5c is a weight to measure the importance of lasso penalty, and L is the final loss function with lasso penalty. In the sparse layer of both pull mode and push mode of \ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50, the input embedding is truncated with a threshold \ud835\udefe, and only the dimensions above \ud835\udefe are kept in the output embedding. Then, the lasso penalty term \ud835\udf06 \ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60\ud835\udc5c L \ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60\ud835\udc5c is added to the final loss function, inducing the output embedding to be sparse.\nWe perform Automated Gradual Pruner (AGP) [51] to reduce the computational overhead of training personalized models on the devices by pruning the recommendation model. Pruning is a commonly used methodology to induce sparsity (i.e. a measure of how many elements in a tensor are exact zeros, relative to the tensor size.) in weights in deep neural networks, which assigns weights satisfying a certain criteria to zero. Note that those trimmed weights will be \"shutdown\" permanently and not participate in back-propagation. A level pruner is a stable pruner where a neural network is pruned into a specific sparsity level, through a threshold magnitude criteria. AGP is an algorithm to schedule a level pruner that can be formulated as follows.\n\ud835\udc60 \ud835\udc61 = \ud835\udc60 \ud835\udc53 + \ud835\udc60 \ud835\udc56 -\ud835\udc60 \ud835\udc53 1 - \ud835\udc61 -\ud835\udc61 0 \ud835\udc5b\u0394\ud835\udc61 3 \ud835\udc53 \ud835\udc5c\ud835\udc5f \ud835\udc61 \u2208 {\ud835\udc61 0 , \ud835\udc61 0 + \u0394\ud835\udc61, ..., \ud835\udc61 0 + \ud835\udc5b\u0394\ud835\udc61 } ,(4)\nwhere \ud835\udc60 \ud835\udc61 is the sparsity level in \ud835\udc61 time step (i.e. epoch), \ud835\udc60 \ud835\udc56 is the initial sparsity level and \ud835\udc60 \ud835\udc53 is the final target sparsity level. The main idea of AGP is to initially prune the weights rapidly when redundant connections are abundant and gradually reduce the number of pruned weights when redundant connections are becoming fewer. The reason why we choose AGP is that it does not make strong assumptions on the structure of the network or layers, and it is straight-forward and not difficult to tune hyper-parameters.", "publication_ref": ["b50", "b50"], "figure_ref": [], "table_ref": []}, {"heading": "EVALUATION", "text": "In this section, we evaluate \ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50 from three folds: the accuracy improvement gained by personalization, the network communication overhead between the cloud and the client, and the computational overhead of on-device training.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Dataset", "text": "To evaluate the performance of \ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50 and baselines, we use a public user behavior dataset from Taobao [50], named UserBehaviorfoot_1 , which contains nearly a million users and nearly 100 million behaviors including click, purchase, adding items to shopping cart and item favoring during November 25 to December 03, 2017. In specific, a user-item interaction contains domains including user ID, item ID, item's category ID, behavior type, and timestamp. We manually partition the interaction data into sessions by using a 706-second idle threshold. Like GRU4Rec [16], we filter out sessions of length 1, filter out sessions with less than 12 clicks, and filter out clicks from the test set where the item clicked is not in the train set.\nTo simulate the realistic scene of GDPR, we manually set a GDPR deadline \ud835\udc47 \ud835\udc51\ud835\udc52\ud835\udc63\ud835\udc56\ud835\udc50\ud835\udc52 = 1512057600 (i.e. 00:00 on December 01, 2017), after which the click data can only be used to train a personalized on one's device, rather than training the global model on the cloud. To evaluate the performance of \ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50, we manually set a training deadline \ud835\udc47 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 = 1512230400 (i.e. 00:00 on December 03, 2017), after which the click data can only be used to test, rather than training whether the global model or a personalized model.\nTo evaluate the generalization of \ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50 on new users, we manually partition users into old users and new users by the average of timestamps of a user's clicks, i.e. users with a quantile of his/her timestamps mean above 0.9 is assigned to new users while others are assigned to old users. New users appear themselves after the GDPR deadline \ud835\udc47 \ud835\udc51\ud835\udc52\ud835\udc63\ud835\udc56\ud835\udc50\ud835\udc52 , which means that a new user' click can only be used to train his personalized model rather than the global model. Note that old users' click are used to train both the global model and their own personalized models. After the above filtering and partitions, the scale and statistics of UserBehavior are shown in Table 1.", "publication_ref": ["b49", "b15"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Metrics and Implementation", "text": "Like GRU4Rec [16], we choose Recall@20 and MRR@20 (Mean Reciprocal Rank) for evaluation. Recall@20 is the proportion of instances with the ground truth next-click item among the top-20 predicted items in all instances and does not take the ranks of the ground truth next-click items into consideration. MRR@20 is the average of reciprocal ranks of ground truth next-click items and takes the rank of their ranks into consideration.\nWe implement the \ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50 prototype based on the popular TensorFlow [1] to train a global model on the cloud and TensorFlow.js [37] to train a personalized model on the device and deploy it with the WebView of an experimental application [30]. For the global training on the cloud, we take advantage of TensorFlow, and We choose the optimal hyper-parameter combination by selecting the highest Recall@20 and MRR@20 on the validation set (which is manually divided from the global training set). The number of GRU layers is one, with 100 hidden units in it. The batch size is 50 and the loss function is cross-entropy. The embedding size of the embedding layer is 10000. The optimization method is Adagrad [11]. The learning rate is 0.01, both momentum and weight decay are 0.\nFor the personalized training on the devices, we take advantage of TensorFlow.js and WebView. TensorFlow.js is a deep learning framework to build and execute deep learning based models in JavaScript to run in a web browser or the Node.js environment. WebView is a control for displaying web content inside Android applications based on the Chrome engine, which provides a built-in JavaScript parser. We leverage WebView to implement the adaptation of TensorFlow.js interpreter inside Android applications, including bidirectional calling of both the Web environment and the Android application, and dynamically loading Android files in the Web environment. Besides, instead of packaging the model file (.pth) together with the application installation package (.apk), we dynamically loads the individual compressed model file as a configuration, so as to keep the installation package light-weight and flexible. In specific, we store the model file and other configuration files in the external storage directory of Android. When the application starts, our framework reads the configuration file from the storage and registers the corresponding model metadata. When the on-device training starts, our framework decompresses and loads the global model according to the file path in model metadata. We use the Google Pixel 2 smartphone model (Android 8.0, Octa-core (4x2.45 GHz + 4x1.9 GHz) Kryo, Adreno 540) to conduct our experiments on the cost of on-device training.", "publication_ref": ["b15", "b0", "b36", "b29", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Accuracy of Personalized Model", "text": "We evaluate the effectiveness of both the global models and personalized models. Figure 4 illustrates the training and testing schedule of \ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50 and three other baselines. In this figure, \ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50 indicates to train a global model with all users' clicks in the first 7 days and fine-tune a personalized model for each user with his/her click on the 8th day, which is suitable for recommendation in the post-GDPR era where GDPR takes effect in the 8 \ud835\udc61 \u210e day. Global + personal models indicate to both train a global model and fine-tune personalized models for all users with all users' clicks in the first 8 days, which is not suitable for GDPR because GDPR prohibits uploading click data on the 8 \ud835\udc61 \u210e day without users' consent, however, this is the upper bound that frameworks with both global models and personal models can achieve. Only personal models indicate to only train a personalized model for each user with his/her click in the first 8 days. The only global model indicates to only train a global model for all users with all users' clicks in the first 8 days, which is the training strategy for almost all state-of-the-art approaches for temporal recommendation, as far as we are concerned. In Figure 5, we show the learning curves of Recall@20 and MRR@20 on testing set. We notice that only personal models and only global models converges earlier than mixture models. However, only personal models suffer from a very poor accuracy, because a single user's training data is so few that personal models are easy to over-fit and converge to a local optimal point. Our approach performs much better than the only global model, i.e. GRU4Rec [16], because our approach both takes advantage of common click patterns through the global model and mines a user's unique click pattern through his/her personal model. Although global + personal models actually predicts a little better than our approach, they are not suitable for the post-GDPR era, as mentioned above. The detailed recommendation accuracy are shown in table 2. To answer the question that if only business necessary transactional data, i.e. purchase behaviors, can predict the item a user click next, we train our model with only transactional data and only click data, to evaluate the effectiveness of click data. As shown in Figure 6, the accuracy of the model trained with only click data is close to that of \ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50, however, the accuracy of the model trained with only transactional data is much lower than that of \ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50. The explanation is rather straight-forward, the task of temporal recommendation is to provide the next click item based on the history of user behaviors, therefore, it makes no sense to predict the probability of click based on purchase data. Therefore, we cannot use only business necessary transactional data, i.e. purchase behaviors, to achieve an accurate temporal recommendation.\nTo measure how GDPR deadline \ud835\udc47 \ud835\udc51\ud835\udc52\ud835\udc63\ud835\udc56\ud835\udc50\ud835\udc52 , i. Note that personalized training is not supposed to be performed once new interactions appear on the device, because users' devices are not always idle and available for training which consumes many computational resources and battery. However, if the personalized model is not updated for a long time, the recommendation accuracy is tend to drop. To measure how the update intervals influences the accuracy, we scale the size of mini-batch from 25 to 200(the personal model is not updated until a mini-batch comes to its end). As shown in Figure 9, within a relatively wide range, the intervals of fine-tuning personalized models influence little on accuracy, which indicates that even if a user keep his/her personalized model on device unchanged for a relatively long period of time, it would not affect his/her recommendation user experience much.\n(a) Recall@20 (b) MRR@20 Fig. 9. The influence of update intervals of fine-tuning personalized models", "publication_ref": ["b15"], "figure_ref": [], "table_ref": []}, {"heading": "Communication Overhead", "text": "There are mainly two parts of network communication overhead between the cloud and the client. In pull mode, the device needs to download the recommendation item candidate set from the cloud, while in push mode, the device needs to upload the user embedding to the cloud. In both pull mode and push mode, the device needs to download the global recommendation model from the cloud. As mentioned before, we use Lasso regression (\ud835\udc3f 1 regularization) to sparsify item embeddings or user embeddings, and AGP to sparsify the global recommendation model.\nTo measure how the sparsity of item embeddings in pull mode and user embeddings in push mode affects the recommendation accuracy, we scale the target sparsity of embeddings and compare recall@20 and MRR@20. As shown in Figure 10, as the sparsity of item embeddings or user embeddings increases, the recommendation accuracy does not suffer from a significant drop, which encourages us to choose an aggressive embedding sparsity of 90%, to reduce the communication overhead between the cloud and the device.  To measure how the sparsity of the global recommendation model affects the recommendation accuracy, we scale the target sparsity of the global recommendation model and compare recall@20 and MRR@20. As shown in Figure 11, as the sparsity of the model increases, the recommendation accuracy does not suffer from a significant drop, which encourages us to choose an aggressive model sparsity of 90%, to reduce the communication overhead between the cloud and the device. Note that we also use an item-CF based filtering model to reduce the computational overhead of ranking the combination of a given user embedding and recommendation item embeddings. To measure how the size of the candidate set affects the recommendation accuracy, we scale the proportion of the candidate set over all items and compare recall@20 and MRR@20. As shown in Figure 12, it turns out that the recommendation accuracy would drop dramatically as the proportion of candidate set becomes smaller. Here, we carefully choose a conservative proportion of 10%, in order to make a trade-off between the computational overhead and the recommendation accuracy. ", "publication_ref": [], "figure_ref": ["fig_6", "fig_7", "fig_0"], "table_ref": []}, {"heading": "Computational Overhead", "text": "We use several techniques to make on-device training more feasible and less resource-consuming, including AGP, item embedding, and embedding sparsity. To evaluate how each technique affects the feasibility and computational overhead of on-device training, we conduct ablation experiments both in pull mode and push mode. Note that \ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50 w/o item embedding means removing the embedding layer and use one-hot encoding to represents items. As shown in Table 3, \ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50 without item embedding cannot be deployed on devices because the memory it needs to deploy and train is dramatically beyond that of mainstream mobile devices. In addition, AGP helps to both save training time and reduce the size of the downloaded global model. As shown in Table 4, item sparsity helps reduce the size of the uploaded user embedding and AGP has similar benefits to pull mode. However, the effect of embedding sparsity in push mode is not as significant as that of item embedding in pull mode, because the device does not need to download the item candidate set from the cloud, which dramatically reduces the communication overhead. However, the push mode has its own problems that it still suffers from potential risks of partial leakage of user privacy, because it requires to upload the intermediate results of the recommendation model (i.e. the user embedding). Still, the privacy leakage risk of push mode is much lower than that of current approaches that upload all user behavior histories to the cloud, therefore, users are more likely to agree with privacy consents in push mode, compared with current frameworks. We conclude the advantages and disadvantages of pull mode and push mode in Table 5.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_3", "tab_4"]}, {"heading": "DISCUSSION", "text": "The accuracy improvement of \ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50 reveals an interesting finding that after training a global model with all users' data, fine-tuning a personalized model for each user can significantly improve the recommendation accuracy. However, due to the time limit, we could not explore the underlying reasons for this accuracy improvement. We assume that different kinds of users might have different behavior patterns and personalized models are able to model those behavior patterns case by case, therefore, this framework predicts the next click item with a higher accuracy. We plan to mine the underlying reasons for this accuracy improvement With the emerge of deep learning techniques, researchers focus on applying deep learning techniques to recommendation systems to improve its performance. Nonetheless, most deep learning based approaches are still variants and/or hybrids of collaborative filtering. In 2016, Cheng et al. from Google proposed the Wide and Deep Learning algorithm for recommending systems [8]. The wide model takes one-hot encoded binary features as input, and mines the correlation between items or features from the historical information, whose recommendation result is often the item directly related to the items in the historical record. The deep model takes numerical features and dense embeddings of continuous features as input, which learns new feature combinations and is able to recommend an item that has never existed in the historical record. Jointly training the Wide model and Deep model allows the framework both to memorize and to generalize. However, in order to mine cross-features, the wide model requires an explicit feature engineering. In order to learn the low-order and high-order cross-features in an end-to-end manner, in 2017, Guo et al. proposed the DeepFM model [15]. DeepFM uses a Factorization Machine (FM) model to take the place of the wide model, which shares the high-dimensional sparse features and low-dimensional dense features as input with the deep model. The FM extracts second-order cross-features and the deep model extracts high-order cross-features automatically.\nThe user data collected by mainstream business recommendation platforms usually contains timestamps, however, both CF-based traditional approaches and CF-based deep learning models cannot model the temporal information behind the sequential dataset, because CF-based frameworks treat each item independently of the other items appeared in the same period, which cannot model a user's continuous preference trends through time.\nTo solve this problem, Hidasi et al. proposed the GRU4REC [16] to leverage recurrent neural networks (RNN) to extract the temporal information behind a user's preferences over items. GRU4Rec takes the click sequence in a period, i.e. a session, makes several adaptions to allow RNN based model fit for the recommendation setting, trains the model with point-wise or pair-wise loss [33], and finally outputs the possibility of each item to be clicked next. In recent years, many researchers focus on improving the performance of GRU4Rec. For example, Hidasi et al. takes unstructured data such as images and texts as features [17]. Bogina et al. takes the user's dwell time on items into consideration [3]. Quadrana et al. takes the user's identity into consideration and construct a hierarchical RNN with the user-level GRU and the session-level GRU [32]. Jannach et al. ensembles the recommendation results of both an RNN based model and a K-nearest neighbor (KNN) based model [21].\nHowever, the above approaches all require to train a global model by collecting all users' to the cloud, which would be impractical without the user's consent in the post-GDPR era. DeepType [44,45,47] made an effort to solve this dilemma between accuracy and privacy concerns by training a global model on the cloud using massive public corpora, and then incrementally customizing the global model with data on individual devices. However, this framework can not be directly deployed to recommendation settings, due to the heterogeneity of item metadata. There is hardly any public dataset of cross-platform recommendation, to extract common prior knowledge for most recommendation systems. Besides, CCMF [13] preserves user privacy operated by different companies with a differential privacy based protection mechanism and utilizes a CF-based model to perform the cross-domain location recommendation. However, this model cannot be directly applied to deep learning based models because they could suffer more from computational overhead and time consumption with a differential privacy mechanism, and CF-based models could not fully model the temporal information in our setting. An intermittent learning framework [24] is proposed to execute certain types of machine learning tasks effectively and efficiently with a carefully selected and optimized training and inference process. However, because it requires to train machine learning based models from scratch and perform few specialized optimizations on structures of deep neural networks (like tensors, filters, RNN cells), intermittent learning is limited to classic machine learning models and small-scaled neural networks.", "publication_ref": ["b7", "b14", "b15", "b32", "b16", "b2", "b31", "b20", "b43", "b44", "b46", "b12", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "CONCLUSION", "text": "We propose \ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50, a cloud-client cooperative deep learning framework of mining interaction behaviors for recommendation while preserving the user privacy. \ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50 supports pull mode, where the item candidate set is pulled from the cloud, and push mode, where the user embedding is uploaded to the cloud. To reduce both communication and computational overhead of devices, we introduce an Automated Gradual Pruner and a Lasso regression in our \ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50 framework to induce sparsity. We evaluate the performance of \ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50 on user behaviors from a large-scale public dataset, and the experimental results show that we achieve up to 10x reduction in communication overhead and reduce the computational overhead towards the range of middle-class mobile devices, with minimal loss in accuracy. In our future work, to mine and take advantage of the underlying reasons for the accuracy increase of fine-tuning personalized models, we will explore incremental learning based recommendations. In addition, to maintain the recommendation sustainability of \ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50, we will explore federated learning based recommendations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "and detect the concept drift [12] when users' behavior patterns have a sudden change. When a concept drift is detected, we incrementally shift from the current recommendation model to another model with a higher accuracy on a certain window of training data, and this is an online learning [2] or incremental learning [49] manner. Essentially, incremental learning is to train a personalized model for each stable stages between two concept drifts, instead of for each user (as in \ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50). Therefore, compared with \ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50, incremental learning based recommendations could significantly reduce the number of personalized models and therefore reduce the overall computational overhead, while keeping the recommendation accuracy from dropping much (but still higher than the only global model). However, incremental learning based recommendation is not suitable for the post-GDPR era, therefore, it is not the scope of this paper, and we will explore that in our future work. Another problem lies in the sustainability of \ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50. As shown in Figure 7, before the 7 \ud835\udc61 \u210e day, the recommendation accuracy increases as the size of the global training set increases, which indicates that \ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50 still requires a considerable amount of behavior data to train the global model. In an extreme situation when personal models are trained for a much longer period than the global model, our \ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50 will degenerate to only personal model, which suffers from an unstable accuracy. This problem can be temporarily relieved by encouraging some users agree to upload their behavior data in the consent by certain incentives. We can update the global model with the new behavior data of those users and download it to other users' devices for personal training. To solve this problem, we have to explore the emerging federated learning (FL) [4,23,48] and deploy FL in the recommendation settings in our future work.", "publication_ref": ["b11", "b1", "b48", "b3", "b22", "b47"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Tensorflow: A system for large-scale machine learning", "journal": "", "year": "2016", "authors": "Mart\u00edn Abadi; Paul Barham; Jianmin Chen; Zhifeng Chen; Andy Davis; Jeffrey Dean; Matthieu Devin; Sanjay Ghemawat; Geoffrey Irving; Michael Isard"}, {"ref_id": "b1", "title": "Dynamic Local Models for Online Recommendation", "journal": "", "year": "2018", "authors": "Marie Al-Ghossein; Talel Abdessalem; Anthony Barr\u00e9"}, {"ref_id": "b2", "title": "Incorporating Dwell Time in Session-Based Recommendations with Recurrent Neural Networks", "journal": "", "year": "2017", "authors": "Veronika Bogina; Tsvi Kuflik"}, {"ref_id": "b3", "title": "Towards federated learning at scale: System design", "journal": "", "year": "2019", "authors": "Keith Bonawitz; Hubert Eichner; Wolfgang Grieskamp; Dzmitry Huba; Alex Ingerman; Vladimir Ivanov; Chloe Kiddon; Jakub Konecny; Stefano Mazzocchi; H Brendan Mcmahan"}, {"ref_id": "b4", "title": "A comprehensive study on challenges in deploying deep learning based software", "journal": "", "year": "2020", "authors": "Zhenpeng Chen; Yanbin Cao; Yuanqiang Liu; Haoyu Wang; Tao Xie; Xuanzhe Liu"}, {"ref_id": "b5", "title": "Through a gender lens: learning usage patterns of emojis from large-scale android users", "journal": "", "year": "2018", "authors": "Zhenpeng Chen; Xuan Lu; Wei Ai; Huoran Li; Qiaozhu Mei; Xuanzhe Liu"}, {"ref_id": "b6", "title": "An empirical study on deployment faults of deep learning based mobile applications", "journal": "", "year": "2021", "authors": "Zhenpeng Chen; Huihan Yao; Yiling Lou; Yanbin Cao; Yuanqiang Liu; Haoyu Wang; Xuanzhe Liu"}, {"ref_id": "b7", "title": "Wide & deep learning for recommender systems", "journal": "", "year": "2016", "authors": "Heng-Tze Cheng; Levent Koc; Jeremiah Harmsen; Tal Shaked; Tushar Chandra; Hrishi Aradhye; Glen Anderson; Greg Corrado; Wei Chai; Mustafa Ispir"}, {"ref_id": "b8", "title": "On the properties of neural machine translation: Encoder-decoder approaches", "journal": "", "year": "2014", "authors": "Kyunghyun Cho; Bart Van Merri\u00ebnboer; Dzmitry Bahdanau; Yoshua Bengio"}, {"ref_id": "b9", "title": "The YouTube video recommendation system", "journal": "", "year": "2010", "authors": "James Davidson; Benjamin Liebald; Junning Liu; Palash Nandy; Taylor Van Vleet; Ullas Gargi; Sujoy Gupta; Yu He; Mike Lambert; Blake Livingston"}, {"ref_id": "b10", "title": "Adaptive subgradient methods for online learning and stochastic optimization", "journal": "Journal of machine learning research", "year": "2011-07", "authors": "John Duchi; Elad Hazan; Yoram Singer"}, {"ref_id": "b11", "title": "A survey on concept drift adaptation", "journal": "ACM computing surveys (CSUR)", "year": "2014", "authors": "Jo\u00e3o Gama; Indr\u0117 \u017dliobait\u0117; Albert Bifet; Mykola Pechenizkiy; Abdelhamid Bouchachia"}, {"ref_id": "b12", "title": "Privacy-preserving Cross-domain Location Recommendation", "journal": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies", "year": "2019", "authors": "Chen Gao; Chao Huang; Yue Yu; Huandong Wang; Yong Li; Depeng Jin"}, {"ref_id": "b13", "title": "Using collaborative filtering to weave an information tapestry", "journal": "Commun. ACM", "year": "1992", "authors": "David Goldberg; David Nichols; Brian M Oki; Douglas Terry"}, {"ref_id": "b14", "title": "DeepFM: a factorization-machine based neural network for CTR prediction", "journal": "", "year": "2017", "authors": "Huifeng Guo; Ruiming Tang; Yunming Ye; Zhenguo Li; Xiuqiang He"}, {"ref_id": "b15", "title": "Session-based recommendations with recurrent neural networks", "journal": "", "year": "2015-01", "authors": "Bal\u00e1zs Hidasi; Alexandros Karatzoglou; Linas Baltrunas; Domonkos Tikk"}, {"ref_id": "b16", "title": "Parallel recurrent neural network architectures for feature-rich session-based recommendations", "journal": "", "year": "2016", "authors": "Bal\u00e1zs Hidasi; Massimo Quadrana; Alexandros Karatzoglou; Domonkos Tikk"}, {"ref_id": "b17", "title": "Long short-term memory", "journal": "Neural computation", "year": "1997", "authors": "Sepp Hochreiter; J\u00fcrgen Schmidhuber"}, {"ref_id": "b18", "title": "Programming situational mobile web applications with cloud-mobile convergence: An internetware-oriented approach", "journal": "IEEE Transactions on Services Computing", "year": "2016", "authors": "Gang Huang; Xuanzhe Liu; Yun Ma; Xuan Lu; Ying Zhang; Yingfei Xiong"}, {"ref_id": "b19", "title": "Early destination prediction with spatio-temporal user behavior patterns", "journal": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies", "year": "2018", "authors": "Ryo Imai; Kota Tsubouchi; Tatsuya Konishi; Masamichi Shimosaka"}, {"ref_id": "b20", "title": "When recurrent neural networks meet the neighborhood for session-based recommendation", "journal": "", "year": "2017", "authors": "Dietmar Jannach; Malte Ludewig"}, {"ref_id": "b21", "title": "Mobile taskflow in context: a screenshot study of smartphone usage", "journal": "", "year": "2009", "authors": " Amy K Karlson; T Shamsi; Brian Iqbal; Gonzalo Meyers; Kathy Ramos; John C Lee;  Tang"}, {"ref_id": "b22", "title": "Federated learning: Strategies for improving communication efficiency", "journal": "", "year": "2016", "authors": "Jakub Kone\u010dn\u1ef3; Brendan Mcmahan; Felix X Yu; Peter Richt\u00e1rik; Ananda Theertha Suresh; Dave Bacon"}, {"ref_id": "b23", "title": "Intermittent Learning: On-Device Machine Learning on Intermittently Powered System", "journal": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies", "year": "2019", "authors": "Seulki Lee; Bashima Islam; Yubo Luo; Shahriar Nirjon"}, {"ref_id": "b24", "title": "Systematic Analysis of Fine-Grained Mobility Prediction with On-Device Contextual Data", "journal": "IEEE Transactions on Mobile Computing", "year": "2020", "authors": "Huoran Li; Fuqi Lin; Xuan Lu; Chenren Xu; Gang Huang; Jun Zhang; Qiaozhu Mei; Xuanzhe Liu"}, {"ref_id": "b25", "title": "Characterizing smartphone usage patterns from millions of android users", "journal": "", "year": "2015", "authors": "Huoran Li; Xuan Lu; Xuanzhe Liu; Tao Xie; Kaigui Bian; Felix Xiaozhu Lin; Qiaozhu Mei; Feng Feng"}, {"ref_id": "b26", "title": "Enlister: baidu's recommender system for the biggest chinese Q&A website", "journal": "", "year": "2012", "authors": "Qiwen Liu; Tianjian Chen; Jing Cai; Dianhai Yu"}, {"ref_id": "b27", "title": "Understanding diverse usage patterns from large-scale appstore-service profiles", "journal": "IEEE Transactions on Software Engineering", "year": "2017", "authors": "Xuanzhe Liu; Huoran Li; Xuan Lu; Tao Xie; Qiaozhu Mei; Feng Feng; Hong Mei"}, {"ref_id": "b28", "title": "Inferring correlation between user mobility and app usage in massive coarse-grained data traces", "journal": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies", "year": "2018", "authors": "Zheng Lu; Yunhe Feng; Wenjun Zhou; Xiaolin Li; Qing Cao"}, {"ref_id": "b29", "title": "Moving deep learning into web browser: How far can we go?", "journal": "", "year": "2019", "authors": "Yun Ma; Dongwei Xiang; Shuyu Zheng; Deyu Tian; Xuanzhe Liu"}, {"ref_id": "b30", "title": "Embedding-based news recommendation for millions of users", "journal": "", "year": "1933", "authors": "Shumpei Okura; Yukihiro Tagami; Shingo Ono; Akira Tajima"}, {"ref_id": "b31", "title": "Personalizing session-based recommendations with hierarchical recurrent neural networks", "journal": "", "year": "2017", "authors": "Massimo Quadrana; Alexandros Karatzoglou; Bal\u00e1zs Hidasi; Paolo Cremonesi"}, {"ref_id": "b32", "title": "BPR: Bayesian personalized ranking from implicit feedback", "journal": "", "year": "2012", "authors": "Steffen Rendle; Christoph Freudenthaler; Zeno Gantner; Lars Schmidt-Thieme"}, {"ref_id": "b33", "title": "Item-based collaborative filtering recommendation algorithms", "journal": "", "year": "2001", "authors": "Badrul Sarwar; George Karypis; Joseph Konstan; John Riedl"}, {"ref_id": "b34", "title": "Recommender systems handbook", "journal": "Springer-verlag New York Incorporated", "year": "2015", "authors": "Bracha Shapira"}, {"ref_id": "b35", "title": "CLiMF: learning to maximize reciprocal rank with collaborative less-is-more filtering", "journal": "", "year": "2012", "authors": "Yue Shi; Alexandros Karatzoglou; Linas Baltrunas; Martha Larson; Nuria Oliver; Alan Hanjalic"}, {"ref_id": "b36", "title": "Tensorflow. js: Machine learning for the web and beyond", "journal": "", "year": "2019", "authors": "Daniel Smilkov; Nikhil Thorat; Yannick Assogba; Ann Yuan; Nick Kreeger; Ping Yu; Kangyi Zhang; Shanqing Cai; Eric Nielsen; David Soergel"}, {"ref_id": "b37", "title": "Multi-rate deep learning for temporal recommendation", "journal": "", "year": "2016", "authors": "Yang Song; Ali Mamdouh Elkahky; Xiaodong He"}, {"ref_id": "b38", "title": "Gaussian ranking by matrix factorization", "journal": "", "year": "2015", "authors": "Harald Steck"}, {"ref_id": "b39", "title": "From fingerprint to footprint: cold-start location recommendation by learning user interest from app data", "journal": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies", "year": "2019", "authors": "Zhen Tu; Yali Fan; Yong Li; Xiang Chen; Li Su; Depeng Jin"}, {"ref_id": "b40", "title": "Modeling Spatio-Temporal App Usage for a Large User Population", "journal": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies", "year": "2019", "authors": "Huandong Wang; Yong Li; Sihan Zeng; Gang Wang; Pengyu Zhang; Pan Hui; Depeng Jin"}, {"ref_id": "b41", "title": "Recurrent recommender networks", "journal": "", "year": "2017", "authors": " Chao-Yuan; Amr Wu; Alex Ahmed; Alexander J Beutel; How Smola;  Jing"}, {"ref_id": "b42", "title": "A first look at deep learning apps on smartphones", "journal": "", "year": "2019", "authors": "Mengwei Xu; Jiawei Liu; Yuanqiang Liu; Felix Xiaozhu Lin; Yunxin Liu; Xuanzhe Liu"}, {"ref_id": "b43", "title": "Deeptype: On-device deep learning for input personalization service with minimal privacy concern", "journal": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies", "year": "2018", "authors": "Mengwei Xu; Feng Qian; Qiaozhu Mei; Kang Huang; Xuanzhe Liu"}, {"ref_id": "b44", "title": "Deepwear: Adaptive local offloading for on-wearable deep learning", "journal": "IEEE Transactions on Mobile Computing", "year": "2019", "authors": "Mengwei Xu; Feng Qian; Mengze Zhu; Feifan Huang; Saumay Pushp; Xuanzhe Liu"}, {"ref_id": "b45", "title": "Approximate query service on autonomous iot cameras", "journal": "", "year": "2020", "authors": "Mengwei Xu; Xiwen Zhang; Yunxin Liu; Gang Huang; Xuanzhe Liu; Felix Xiaozhu; Lin "}, {"ref_id": "b46", "title": "DeepCache: Principled cache for mobile deep vision", "journal": "", "year": "2018", "authors": "Mengwei Xu; Mengze Zhu; Yunxin Liu; Felix Xiaozhu Lin; Xuanzhe Liu"}, {"ref_id": "b47", "title": "Federated machine learning: Concept and applications", "journal": "ACM Transactions on Intelligent Systems and Technology (TIST)", "year": "2019", "authors": "Qiang Yang; Yang Liu; Tianjian Chen; Yongxin Tong"}, {"ref_id": "b48", "title": "Adaptive Deep Models for Incremental Learning: Considering Capacity Scalability and Sustainability", "journal": "", "year": "2019", "authors": "Yang Yang; Da-Wei Zhou; De-Chuan Zhan; Hui Xiong; Yuan Jiang"}, {"ref_id": "b49", "title": "Learning tree-based deep model for recommender systems", "journal": "", "year": "2018", "authors": "Han Zhu; Xiang Li; Pengye Zhang; Guozheng Li; Jie He; Han Li; Kun Gai"}, {"ref_id": "b50", "title": "To prune, or not to prune: exploring the efficacy of pruning for model compression", "journal": "", "year": "2017", "authors": "Michael Zhu; Suyog Gupta"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Fig. 1 .1Fig. 1. Pull Mode of \ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Fig. 3 .3Fig. 3. Recommendation Model", "figure_data": ""}, {"figure_label": "4205", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Fig. 4 . 20 Fig. 5 .4205Fig. 4. An illustration of the training and testing timeline", "figure_data": ""}, {"figure_label": "206207", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "20 Fig. 6 . 20 Fig. 7 .206207Fig. 6. The necessity of click history data global training is actually only personalized models, and 8-day global training is actually the only global model, as mentioned above. The results indicates that GDPR deadline \ud835\udc47 \ud835\udc51\ud835\udc52\ud835\udc63\ud835\udc56\ud835\udc50\ud835\udc52 would affect the recommendation accuracy to some extent, the optimal \ud835\udc47 \ud835\udc51\ud835\udc52\ud835\udc63\ud835\udc56\ud835\udc50\ud835\udc52 comes at some particular point between only global model and only personalized models.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Fig. 8 .8Fig. 8. The generalization of \ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50 on new users", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "(a) Pull Mode (b) Push Mode", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Fig. 10 .10Fig. 10. The influence of lasso regularization", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Fig. 11 .11Fig. 11. Sparsity of the model Fig. 12. Size of candidate set", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": ", \ud835\udc5d \ud835\udc56,\ud835\udc57,\ud835\udc61 is the predicted likelihood of user \ud835\udc56 click item \ud835\udc57 at timestamp \ud835\udc61 by the on-device recommendation model, \ud835\udc59\ud835\udc4e\ud835\udc4f\ud835\udc52\ud835\udc59 \ud835\udc56,\ud835\udc57,\ud835\udc61 \u2208 {0, 1} is the ground truth whether item \ud835\udc57 is clicked by user \ud835\udc56 at timestamp \ud835\udc61, \ud835\udf03 is the parameter set of the on-device recommendation model.We partition the whole procedure into 3 stages: global training, personalized training, testing, by \ud835\udc47 \ud835\udc51\ud835\udc52\ud835\udc63\ud835\udc56\ud835\udc50\ud835\udc52 and \ud835\udc47 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 . \ud835\udc47 \ud835\udc51\ud835\udc52\ud835\udc63\ud835\udc56\ud835\udc50\ud835\udc52 is the timestamp after which non-transactional data are used for training a personalized model for each user, instead of training a global model for all users, and \ud835\udc47 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 is the timestamp after which labeled non-transactional data are used for testing, instead of training. Transactional data can always be collected, whether during training or testing, therefore, the recommendation candidate set is kept up-to-date for all time, whether during training or during testing. Before \ud835\udc47 \ud835\udc51\ud835\udc52\ud835\udc63\ud835\udc56\ud835\udc50\ud835\udc52 , we use the non-transactional data of all users to train a global recommendation model. After \ud835\udc47 \ud835\udc51\ud835\udc52\ud835\udc63\ud835\udc56\ud835\udc50\ud835\udc52 , we use each user's private non-transactional data to train his/her personalized recommendation model. Before \ud835\udc47 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 , we use the non-transactional data for global training and personalized training. After \ud835\udc47 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 , we only use non-transactional data for testing.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Description of the dataset", "figure_data": "ScaleUsersItemsClicksGlobal training (old users)876,914 922,390 51,908,146Personalized training and testing(old users) 788,472 886,658 17,200,197Personalized training and testing(new users) 97,199 567,213 2,810,257"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Accuracy of our approach and baselines", "figure_data": "ApproachRecall@20 MRR@20Global + Personal0.1680.069\ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50 (Our Approach)0.1550.064Only global (GRU4Rec)0.1280.052Only personal0.1010.043hyper-parameters are chosen carefully by running experiments at several combinations of random points of thehyper-parameter space."}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Cost of on-device training in pull mode", "figure_data": "Metric\ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50\ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50 w/o AGP \ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50 w/o item embeddingTraining Time1163 seconds3725 seconds-CPU%66.3%65.8%-Memory842.9 MB844.1 MB\u221eDownloaded Item Candidate Set391.1 MB391.1 MB0.4 MBDownloaded Model1.71 MB16.2 MB156.8 MB"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Cost of on-device training in push mode", "figure_data": "Metric\ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50\ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50 w/o AGP \ud835\udc36 3 \ud835\udc37\ud835\udc45\ud835\udc52\ud835\udc50 w/o embedding sparsityTraining Time1117 seconds3706 seconds1123 secondsCPU%66.9%65.5%66.2%Memory479.8 MB480.9 MB475.6 MBUploaded User Embedding4.2 KB4.2 KB40.0 KBDownloaded model1.71 MB16.2 MB1.69 MB"}], "formulas": [{"formula_id": "formula_0", "formula_text": ")1", "formula_coordinates": [4.0, 524.26, 263.14, 7.05, 9.96]}, {"formula_id": "formula_1", "formula_text": "\ud835\udf03 * = \ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc5a\ud835\udc56\ud835\udc5b \ud835\udf03 \u2211\ufe01 \ud835\udc61 \u2211\ufe01 \ud835\udc56 \u2211\ufe01 \ud835\udc57 L (\ud835\udc5d \ud835\udc56,\ud835\udc57,\ud835\udc61 , \ud835\udc59\ud835\udc4e\ud835\udc4f\ud835\udc52\ud835\udc59 \ud835\udc56,\ud835\udc57,\ud835\udc61 ; \ud835\udf03 ), \ud835\udc60\ud835\udc62\ud835\udc4f \ud835\udc57\ud835\udc52\ud835\udc50\ud835\udc61 \ud835\udc61\ud835\udc5c \ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc60 \ud835\udc5c\ud835\udc5b \ud835\udc50\ud835\udc5c\ud835\udc5a\ud835\udc5d\ud835\udc62\ud835\udc61\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc4e\ud835\udc59 \ud835\udc5c\ud835\udc63\ud835\udc52\ud835\udc5f\u210e\ud835\udc52\ud835\udc4e\ud835\udc51,(2)", "formula_coordinates": [4.0, 202.38, 370.29, 328.92, 35.49]}, {"formula_id": "formula_2", "formula_text": "\u00ca\ud835\udc56 = \ud835\udc38 \ud835\udc56 , \ud835\udc56 \ud835\udc53 |\ud835\udc38 \ud835\udc56 | > \ud835\udefe 0, \ud835\udc5c\ud835\udc61\u210e\ud835\udc52\ud835\udc5f\ud835\udc64\ud835\udc56\ud835\udc60\ud835\udc52 , L \ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60\ud835\udc5c = \u2211\ufe01 \ud835\udc56 \u00ca\ud835\udc56 , L = L + \ud835\udf06 \ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60\ud835\udc5c L \ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60\ud835\udc5c ,(3)", "formula_coordinates": [7.0, 261.3, 347.1, 270.0, 64.1]}, {"formula_id": "formula_3", "formula_text": "\ud835\udc60 \ud835\udc61 = \ud835\udc60 \ud835\udc53 + \ud835\udc60 \ud835\udc56 -\ud835\udc60 \ud835\udc53 1 - \ud835\udc61 -\ud835\udc61 0 \ud835\udc5b\u0394\ud835\udc61 3 \ud835\udc53 \ud835\udc5c\ud835\udc5f \ud835\udc61 \u2208 {\ud835\udc61 0 , \ud835\udc61 0 + \u0394\ud835\udc61, ..., \ud835\udc61 0 + \ud835\udc5b\u0394\ud835\udc61 } ,(4)", "formula_coordinates": [7.0, 176.53, 588.41, 354.77, 21.87]}], "doi": "10.1145/nnnnnnn.nnnnnnn"}
