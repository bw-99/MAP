{
  "Dataset Regeneration for Sequential Recommendation": "",
  "Mingjia Yin": "",
  "Hao Wang ∗": "mingjia-yin@mail.ustc.edu.cn University of Science and Technology of China & State Key Laboratory of Cognitive Intelligence Hefei, China Wei Guo Yong Liu guowei67@huawei.com liu.yong6@huawei.com Huawei Singapore Research Center Singapore",
  "Suojuan Zhang Sirui Zhao": "sjzsj@ustc.edu.cn sirui@mail.ustc.edu.cn University of Science and Technology of China & State Key Laboratory of Cognitive Intelligence Hefei, China wanghao3@ustc.edu.cn University of Science and Technology of China & State Key Laboratory of Cognitive Intelligence Hefei, China",
  "Defu Lian": "liandefu@ustc.edu.cn University of Science and Technology of China & State Key Laboratory of Cognitive Intelligence Hefei, China",
  "ABSTRACT": "",
  "Enhong Chen": "cheneh@ustc.edu.cn University of Science and Technology of China & State Key Laboratory of Cognitive Intelligence Hefei, China",
  "KEYWORDS": "The sequential recommender (SR) system is a crucial component of modern recommender systems, as it aims to capture the evolving preferences of users. Significant efforts have been made to enhance the capabilities of SR systems. These methods typically follow the model-centric paradigm, which involves developing effective models based on fixed datasets. However, this approach often overlooks potential quality issues and flaws inherent in the data. Driven by the potential of data-centric AI, we propose a novel datacentric paradigm for developing an ideal training dataset using a model-agnostic dataset regeneration framework called DR4SR. This framework enables the regeneration of a dataset with exceptional cross-architecture generalizability. Additionally, we introduce the DR4SR+ framework, which incorporates a model-aware dataset personalizer to tailor the regenerated dataset specifically for a target model. To demonstrate the effectiveness of the data-centric paradigm, we integrate our framework with various model-centric methods and observe significant performance improvements across four widely adopted datasets. Furthermore, we conduct in-depth analyses to explore the potential of the data-centric paradigm and provide valuable insights. The anonymous code can be found at https://github.com/USTC-StarTeam/DR4SR.",
  "CCS CONCEPTS": "",
  "· Information systems → Recommender systems .": "∗ Corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. KDD '24, August 25-29, 2024, Barcelona, Spain © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0490-1/24/08...$15.00 https://doi.org/10.1145/3637528.3671841 Recommendation System, Sequential Recommendation, Data-Centric AI, Data Generation",
  "ACMReference Format:": "Mingjia Yin, Hao Wang, Wei Guo, Yong Liu, Suojuan Zhang, Sirui Zhao, Defu Lian, and Enhong Chen. 2024. Dataset Regeneration for Sequential Recommendation. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '24), August 25-29, 2024, Barcelona, Spain. ACM,NewYork,NY,USA,13pages. https://doi.org/10.1145/3637528.3671841",
  "1 INTRODUCTION": "The sequential recommender (SR) system is a critical component of modern recommender systems, aiming to capture the evolving preferences of users through their sequential interaction records [50, 56]. In recent years, significant efforts have been made to enhance SR. This includes the development of sophisticated deep models [11, 15, 20, 27, 43, 57, 66], effective training strategies [30, 37], and the refinement of representation space through self-supervised learning [3, 39, 59, 67, 68, 74, 81], among others. These methods follow the model-centric paradigm, which specifically aims to develop more effective models given fixed datasets. Despite the remarkable achievements, these methods often overlook potential quality issues inherent within the data [71], which may lead to overfitting or amplification of data errors [25, 72]. To address these challenges, the data-centric paradigm [25, 71, 79] has been proposed, focusing on developing high-quality data with fixed models. For example, graph structure learning [19, 33, 63, 83] has been introduced to uncover valuable graph structures from data [79]. Additionally, generative models such as GAN [5], VAE [23], and Diffusion model [12] have been employed to synthesize new training samples [13, 16]. Inspired by the potential of data-centric AI, we aim to acquire an informative and generalizable training dataset for sequential recommender systems. We define this as the training data development problem. To the best of our knowledge, we are the first to investigate this problem in the context of sequential recommendation systems. KDD '24, August 25-29, 2024, Barcelona, Spain Mingjia Yin, Hao Wang, Wei Guo, Yong Liu, Suojuan Zhang, Sirui Zhao, Defu Lian, and Enhong Chen Figure 1: Model-centric paradigm v.s. Data-centric pradigm. Data-centric paradigm: personalized datasets for each model Model-centric paradigm: one dataset for all models Regenerate Original Sequence 𝐃𝐚𝐭𝐚 𝑹𝑵𝑵 𝐃𝐚𝐭𝐚 𝑨𝒕𝒕𝒏 𝐃𝐚𝐭𝐚 𝑮𝒓𝒂𝒑𝒉 RNN Attention Graph All Models Personalize 1 2 3 5 2 4 1 3 4 5 6 1 2 4 6 2 3 4 5 …… 1 2 3 1 2 1 2 3 2 4 3 4 5 … Feed Indiscriminate Data Feeding RNN Attention Graph To address the issue, dataset generation is the most relevant topic. However, to the best of our knowledge, it has been rarely studied in the field of recommender systems. UPC-SDG[31] proposed to generate a synthetic dataset, but its primary goal is to preserve privacy rather than recommendation performance. Another potential solution is dataset distillation (DD), aiming to derive a smaller synthetic dataset and enable trained models to attain comparable performance[40, 46, 54, 55, 69]. However, they prioritize training efficiency over effectiveness, which differs from our motivation. Notably, recent advancements in DD [6, 80] have reported that even increasing synthetic dataset size, the performance will ultimately plateau around the optimal performance achieved with the original dataset. Yet another viable approach is denoising sequential recommendation, which involves the removal of noisy information from the original data [38, 44, 70, 73, 82]. However, denoising is just one aspect of the larger issue concerning training data development. Our objective is to excavate innovative data content and formats within the developed data, further enhancing model training. To acquire the optimal training data, our key idea is to learn a new dataset that explicitly contains item transition patterns. In detail, we decompose the modeling process of a recommender into two stages: extracting transition patterns X ′ from the original dataset X and learning user preferences Y based on X ′ . The learning of the mapping X → Y is challenging since it involves two implicit mappings: X → X ′ and X ′ →Y . Therefore, we explore the possibility of developing a dataset that explicitly represents item transition patterns of X ′ . This allows us to explicitly decompose the learning process into two phases, with X ′ →Y being intuitively easier to learn. Hence, our main focus is on learning an effective mapping function for X → X ′ , which is a one-to-many mapping. We define the learning process as a dataset regeneration paradigm as depicted in Figure 1, where \"re-\" indicates that we do not incorporate any additional information but solely rely on the original dataset. To realize dataset regeneration, we propose a novel data-centric paradigm D ataset R egeneration for S equential R ecommendation ( DR4SR ), to regenerate the original dataset into a highly informative and generalizable dataset. In detail, we first construct a pre-training task, which makes it possible to perform dataset regeneration. Subsequently, a diversity-promoted regenerator is proposed to model the one-to-many relationship between sequences and patterns in the regeneration process. Lastly, we propose a hybrid inference strategy to regenerate a new dataset with balanced exploration and exploitation. To confirm the superiority of DR4SR, we integrated our framework with various model-centric methods and conducted experiments on four widely adopted datasets. The experimental results can demonstrate the ideal cross-architecture generalizability of DR4SR and the high complementarity of datacentric and model-centric paradigms. Upon regenerating the dataset, we encounter a novel challenge: the dataset regeneration process operates independently of the target model. While it demonstrates promising cross-architecture generalizability, the regenerated dataset may be sub-optimal for a particular target model. Consequently, our objective is to further customize the regenerated dataset to a particular target model. Nevertheless, the non-differentiable nature of the hybrid inference process poses a difficulty, as optimizing the dataset regenerator based on the downstream recommendation performance via gradient backpropagation becomes unfeasible. To mitigate the aforementioned challenges, we augment DR4SR to a model-aware dataset regeneration process, denoted as DR4SR+. DR4SR+ takes into account the unique attributes of each target model and modifies the regenerated dataset accordingly, as depicted in Figure 1. In particular, we have implemented a dataset personalizer that assigns a score to every pattern within the regenerated dataset. To prevent model collapse, we formulate the optimization of the dataset personalizer as a bi-level optimization problem, which can be efficiently addressed using implicit differentiation. Empirical results substantiated that DR4SR+ can enhance the regenerated dataset further. Additionally, our investigations suggest that the data forms amenable to regeneration are not confined to sequences. Our contributions are summarized as follows: · To the best of our knowledge, this study is the first to address the issue of training data development for sequential recommendations from a data-centric perspective, intending to create an informative and generalizable training dataset. · We propose a model-agnostic dataset regeneration framework DR4SR to tackle the problem. The framework comprises a pretraining task, a diversity-promoted regenerator, and a hybrid inference strategy, all of which contribute to the regeneration of an informative and generalizable dataset. · We further develop DR4SR into a model-aware version, denoted as DR4SR+, to tailor the regenerated dataset to various target models. This is accomplished through the use of a dataset personalizer, which assigns scores to each regenerated pattern and can be optimized efficiently using implicit differentiation. · We integrated DR4SR and DR4SR+ with various model-centric methods, resulting in substantial improvements on four widely adopted datasets. Additionally, we conducted comprehensive analyses to explore the potential of the data-centric paradigm.",
  "2 RELATED WORKS": "",
  "2.1 Sequential Recommendation": "To reveal the dynamic preferences embedded within users' interaction sequences, sequential recommendation (SR) has become a prominent branch within the realm of recommendation systems[32, 47, 48, 50, 75]. Significant efforts have been directed towards enhancing the capabilities of sequential recommender systems. Initial endeavors concentrated on developing intricate sequential deep models to capture complex sequential preferences, encompassing CNN[43, 61], RNN[8, 11, 26], GNN[53, 57, 60], and Transformer[45]based approaches[20, 42]. Beyond model design, ongoing efforts aim DR4SR KDD '24, August 25-29, 2024, Barcelona, Spain to augment the efficacy of SR by devising superior training strategies, such as RSS[37] and CT4Rec[30]. Additionally, self-supervised learning (SSL) has been incorporated into SR to refine the representation space with diverse SSL tasks, including sequence-level tasks[3, 59], model-level tasks[39], and hybrid tasks[81]. Recognizing the presence of noise in sequential data, denoising sequential recommendation has emerged as an effective method to filter out noisy information from the original data (e.g., [7, 29, 38, 44, 70, 73, 82]). One approach achieves this implicitly in the representation space through elaborate model architectures, such as the sparse attention network in DSAN[70], and the filter-enhanced MLP in FMLP[82]. Another approach explicitly filters out irrelevant items for a target item, including CLEA[38] and RAP[44]. Denoising-based methods partially align with our objective, but denoising alone falls short of producing an optimal dataset as it only focuses on removing information from the data. Dataset generation should produce diverse and novel samples to alleviate the constraints imposed by the original dataset [1].",
  "2.2 Data-Centric AI in Recommender System": "Recently, the importance of data in the field of AI has been significantly magnified, culminating in the emergence of the Data-Centric AI (DCAI) paradigm. DCAI represents an all-encompassing concept that comprises three principal components: training data development, inference data development, and data maintenance [25, 71]. Within these significant themes, our primary emphasis is on the development of training data, specifically, the creation of an optimal training dataset for sequential recommendations. In pursuit of the objective of training data development, dataset generation is the most relevant topic. In the field of recommender systems, UPC-SDG[31] proposed to develop a privacy-preserving synthetic dataset. GTN [4] generated adaptive graph data for graph collaborative filtering. For sequential recommendation, ASReP [34] and METL [21] focused on generating fabricated data for longtailed sequences. Another promising technique in DCAI is Dataset Distillation (DD). DD aims to distill a condensed dataset from the original dataset, enabling trained models to attain comparable performance[69]. DD is predominantly popular in the field of computer vision (CV) and can be categorized into three main classes: performance matching[51], parameter matching[77], and distribution matching[76]. Specifically, DD has been introduced to the field of recommender systems by some pioneer works[40, 41, 46, 54, 55]. ∞ -AE[40] adopted neural tangent kernel (NTK) to approximate an infinitely wide autoencoder and synthesized fake users with sampling-based reconstruction. DConRec[54] proposed to distill a synthesized dataset by sampling user-item pairs from a learnable probabilistic matrix. Farzi [41] achieved efficient sequential data distillation within the latent space. Different from the previous two performance matching methods, CGM[46] followed a parameter matching[78] paradigm to condense categorical recommendation data in the CTR scenario. While these methods have achieved remarkable success, they diverge from our specific objectives. Our goal is to utilize the generated dataset to enhance performance, whereas these methods primarily prioritize privacy or efficiency concerns.",
  "3 PROBLEM DEFINITION": "In the domain of sequential recommendation, the primary objective is to model user preferences based on their interaction records and provide the next recommended item for them. We can formally define the problem as follows: Definition1 . ( Sequential Recommendation ). In a sequential recommendation system, we denote U as the user set and V as the item set, and |U| and |V| is the respective number of users and items. The interaction sequences of users are ordered chronologically. We define them as 𝑠 𝑢 = [ 𝑣 1 , 𝑣 2 , . . . , 𝑣 𝑡 , . . . , 𝑣 | 𝑠 𝑢 | ] , where 𝑢 ∈ U is some user, 𝑣 𝑡 ∈ V is one item interacted by the user at the time step 𝑡 , 𝑠 𝑢 represents the sequence, and | 𝑠 𝑢 | denotes its length, which has a maximum value of 𝑁 . Then the next item prediction task aims to predict the item at the next time step 𝑣 | 𝑠 𝑢 | + 1 for each user 𝑢 ∈ U . In deviating from conventional model-centric approaches, which concentrate on enhancing recommendation performance with fixed datasets, we endeavor to construct an optimal training dataset from a data-centric perspective, which is formalized as a training data development problem as follows: Definition2 . ( Training Data Development ). Given an original dataset X , traditional methods aim to learn a target model 𝑓 ′ satisfying Y = 𝑓 ′ (X) , where Y is the user preferences. Training data development aims to learn a new informative and generalizable dataset X ′ satisfying Y = 𝑓 (X ′ ) , where the target model 𝑓 can be easier to learn than 𝑓 ′ based on X ′ . Then the next crucial issue is how to guarantee an optimal X ′ . We aim to accomplish this through a model-agnostic dataset regeneration paradigm, which is explained in detail below: Definition3 . ( Dataset Regeneration ). Given a sequence 𝑠 𝑢 in X , dataset regeneration aims to learn a one-to-many dataset regenerator 𝑓 ′ satisfying { 𝑝 𝑢 1 , 𝑝 𝑢 2 , . . . , 𝑝 𝑢𝐾 } = 𝑓 ′ ( 𝑠 𝑢 ) , where { 𝑝 𝑢 1 , 𝑝 𝑢 2 , . . . , 𝑝 𝑢𝐾 } are K informative patterns and 𝑓 ′ is learned independently from the target model 𝑓 . By collecting all regenerated patterns, we can obtain the new dataset X ′ . Upon regenerating a new dataset, there is an imperative need to tailor the regenerated dataset specifically for a target recommendation model, as the regeneration process remains independent of the target model. We hence propose a model-aware dataset regeneration process that can evaluate the score of each data sample for the target model. The formal definition is as follows: Definition4 . ( Model-aware Dataset Regeneration ). Given the regenerated dataset X ′ , a dataset personalizer 𝑔 with parameter 𝜙 is used to generate a scoring matrix W ∈ R | X ′ | to evaluate the score of each data sample. The learning of 𝑔 will be guided by a target model 𝑓 , thereby achieving dataset personalization. The dataset-score pair (X ′ , W ) will serve as the personalized dataset. In the next section, we will illustrate the details of dataset regeneration and model-aware dataset regeneration.",
  "4 METHODOLOGY": "",
  "4.1 Overall Framework": "In this paper, we propose a data-centric framework named D ata R egeneration for S equential R ecommendation ( DR4SR ), aiming to regenerate the original dataset into a highly informative and generalizable dataset, as shown in Figure 2. Since the data regeneration process is independent of target models, the regenerated dataset KDD '24, August 25-29, 2024, Barcelona, Spain Mingjia Yin, Hao Wang, Wei Guo, Yong Liu, Suojuan Zhang, Sirui Zhao, Defu Lian, and Enhong Chen 1 2 3 4 5 1 2 Encoder Diversity Promoter Decoder 1 2 Decoder 𝒎 𝟏 \" 𝒎 𝟐 \" … 𝒎 𝑲 \" 𝝅 𝟏 𝝅 𝟐 … 𝝅 𝑲 𝒎 𝟏 \" 𝒎 𝑲 \" Personalizer 1 ? 2 6 𝛾 1- 𝛾 Restrictive Generative 3 4 ? 5 7 𝛾 1- 𝛾 Restrictive Generative …… 1 2 1 2 7 …… …… 1 2 1 2 3 2 4 1 2 7 … Target Model (A) (B) (C) 0.7 0.2 0.8 ⋯ 0.5 × × × × ∑ 𝑠 ! in 𝒳 𝑝 \" in 𝒳′ Items not in 𝑠 ! Forward Implicit  gradient 7 1 2 1 2 1 2 1 2 3 2 4 1 2 7 … 1 2 1 2 3 2 4 1 2 7 … 1 2 <BOS> <EOS> Figure 2: The framework of the proposed data-centric paradigm: (A) The Pre-training stage of DR4SR involves training a diversity-promoted data regenerator utilizing the curated pre-training dataset. (B) The inference stage of DR4SR regenerates each source sequence into multiple target patterns with a hybrid inference strategy. (C) Model-aware dataset regeneration with a personalizer further tailors the regenerated dataset to each target model. 1 2 1 3 2 3 1 2 3 1 2 1 3 1 2 3 1 2 3 4 5 𝒳 !\"# 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 1 3 1 2 3 4 5 ⋯⋯ Specifically, we utilize a sliding window strategy to extract patterns. Within a specified sliding window of size 𝛼 , we count the occurrence of item transition patterns. Patterns with an occurrence count exceeding a predetermined threshold 𝛽 will be selected to form the pre-training task. A toy example is depicted in Figure 3, where the window size is set to 3 and the threshold is set to 2. Figure 3: Pre-training task construction. Assuming two given sequences (1,2,3,4,5), (1,2,3) with a window size 3 and a threshold 2, the following patterns can be extracted: (1,2), (1,3), (2,3), (1,2,3). This is because these patterns appear twice within the sliding window. Then the regenerator is supposed to regenerate each sequence into multiple corresponding patterns. may not necessarily align with their requirements. Therefore, we extend DR4SR to a model-aware version DR4SR+ in Section 4.3, which tailors the regenerated dataset to a particular target model.",
  "4.2 Model-agnostic Dataset Regeneration": "To develop an informative and generalizable dataset, we aim to construct a dataset regenerator to facilitate the automated dataset regeneration. However, the original dataset lacks supervised information for learning a dataset regenerator. Therefore, we must implement it in a self-supervised learning manner. To achieve this, we introduce a pre-training task in Section 4.2.1 that can guide the learning of the proposed diversity-promoted regenerator detailed in Section 4.2.2. After completing the pre-training, we proceed to regenerate a new dataset using a hybrid inference strategy in Section 4.2.3. The pipeline is detailed in Algorithm 1. 4.2.1 Construction of Pre-training Task. To construct a pre-training task, we first obtain item transition patterns through rule-based methods. Then, the pre-training task is formalized as training the regenerator to learn how to regenerate these patterns from the sequences in the original dataset. After obtaining the pattern set, one pre-training instance pair can be constructed as ( 𝑠 𝑢 , 𝑝 𝑖 ) , where 𝑠 𝑢 is the interaction sequence of user 𝑢 , 𝑝 𝑖 is 𝑖 -th pattern extracted from 𝑠 𝑢 . Then the regenerator 𝑓 ′ is supposed to regenerate 𝑠 𝑢 into the corresponding pattern 𝑝 𝑖 for each pre-training instance pair ( 𝑠 𝑢 , 𝑝 𝑖 ) . We denote the entire pre-training dataset as X pre . Note : This strategy is related to sequential pattern mining, which aims to uncover patterns for guiding sequential recommendations [49, 65]. However, these methods are known to generate redundant information and neglect infrequent patterns, thus failing to meet the requirements of a dataset regenerator. 4.2.2 Diversity-promoted Regenerator. With the aid of the pretraining task, we can now pre-train a dataset regenerator. In this paper, we employ the Transformer[45] model as the main architecture of our regenerator, whose generation capabilities have been extensively validated. The dataset regenerator consists of three modules: an encoder to obtain representations of sequences in the original dataset, a decoder to regenerate the patterns, and a diversity promoter to capture the one-to-many mapping relationship. Next, we will proceed to introduce each of these modules individually. The encoder consists of several stacked layers of multi-head self-attention (MHSA) and feed-forward (FFN) layers. Considering a sequence in X , we can get the sequence representation by:  where H ( 𝑙 -1 ) ∈ R 𝑁 × 𝑑 is the output at the ( 𝑙 -1 ) -th layer, and H ( 0 ) is item embeddings with added learnable positional encoding. As DR4SR KDD '24, August 25-29, 2024, Barcelona, Spain for the decoder, it takes patterns in the regenerated dataset X ′ as input. The objective of the decoder is to reconstruct the pattern given the sequence representations from the encoder:  where ( 𝑠 𝑢 , 𝑝 𝑖 ) is a sequence-pattern pair in X pre , 𝑡 is the position in the target pattern, h ( 𝑙 ) 𝑢 is the representation of the original sequence 𝑠 𝑢 , ˆ 𝑝 < 𝑡 is the pattern generated before 𝑡 , and 𝑃 is the prediction probability for the target item 𝑝 𝑖𝑡 . However, as mentioned in Section 4.2.1, multiple patterns can be extracted from a sequence, which presents challenges during the training process. For instance, a sequence like (1, 2, 3, 4, 5) may yield two distinct patterns, namely (1, 2) and (4, 5). Consequently, this can introduce conflicts during training, thereby impeding the convergence of the dataset regenerator. To address this one-to-many mapping issue, we further propose a diversity promoter. Specifically, we employ a more aggressive approach to adaptively regulate the impact of the original sequence during the decoding stage by incorporating information about the target pattern. First, we project the memory m ∈ R 𝐷 (representations of the original sequence) generated by the encoder into K distinct vector spaces, i.e., { m ′ 1 , m ′ 2 , . . . , m ′ 𝐾 } and m ′ 𝑘 ∈ R 𝐷 . This projection enables the acquisition of memories with diverse semantic information. Ideally, different target patterns should match distinct memories. To achieve this, we additionally introduce a Transformer encoder to encode the target patterns and obtain h ( 𝑙 ) pattern ∈ R 𝐷 . Notably, it is imperative to exercise caution when incorporating h ( 𝑙 ) pattern into the decoder, otherwise the model may inadvertently collapse into an identity mapping function by simply duplicating the input. Therefore, we compress h ( 𝑙 ) pattern into a probability vector by:  where 𝜋 = { 𝜋 1 , 𝜋 2 , . . . , 𝜋 𝑘 , . . . , 𝜋 𝐾 } and 𝜋 𝑘 is the probability of choosing the 𝑘 -th memory. To ensure that each memory space receives sufficient training, we do not perform hard selection. Instead, we obtain the final memory through a weighted sum:  Ultimately, we can leverage the acquired memory to facilitate the decoding process and effectively capture the intricate one-to-many relationship between sequences and patterns. 4.2.3 Dataset Regeneration with Hybrid Inference Strategy. After pre-training, we utilize the regenerator for inference to obtain a regenerated dataset X ′ . Specifically, we re-feed the sequences in X into the regenerator to obtain new patterns. Due to the unavailability of target patterns, we cannot generate a category vector 𝜋 for selecting memories. As a result, we respectively input each memory directly into the decoder, leading to the generation of K patterns. Besides, we consider the following two decoding modes: Restrictive mode . Decoding is limited to selecting items from the input sequence, which focuses on the exploitation of existing information from the current sequence. Generative mode . There are no restrictions, which allows for the exploration of patterns that may not exist in X . For instance, suppose the original dataset contains item transition patterns like (1, 2) and (2, 6). When decoding a sequence like (1, 2, 3, 4, 5), the model may identify a high-order pattern like (1, 2, 6), which was not explicitly present in X . To achieve a harmonious balance between exploitation and exploration, we propose a hybrid strategy: When decoding an item, there is a probability of 𝛾 to adopt the generative mode and a probability of 1 -𝛾 to adopt the restrictive mode.",
  "4.3 Model-aware Dataset Regeneration": "Since the previous regeneration process is independent of target models, the regenerated dataset may be sub-optimal for a particular target model. Hence, we extend the model-agnostic dataset regeneration process to a model-aware one. To achieve this, built upon the dataset regenerator, we further introduce a dataset personalizer to evaluate the scores of each data sample in the regenerated dataset in Section 4.3.1. Then the dataset personalizer can be efficiently optimized by implicit differentiation introduced in Section 4.3.2. The pipeline is detailed in Algorithm 2. 4.3.1 Dataset personalizer. As outlined in Section 3, our objective is to train a dataset personalizer 𝑔 with parameter 𝜙 that can evaluate the score of each data sample W for the target model. Specifically, given a data sample of pattern 𝑖 in the position 𝑡 , the score 𝑤 𝑖,𝑡 for this data sample can be computed as follows:  where 𝑔 𝜙 is the dataset personalizer implemented as an MLP, h 𝑖 𝑡 is the representation of pattern 𝑖 at position 𝑡 generated by the target model 𝑓 ′ , z ∈ R 2 is the computed logits vector, Gumbel noise G is sampled from a Gumbel distribution[18], 𝜏 is a temperature parameter to control the smoothness of Gumbel distribution, and the subscript 1 means selecting the first element of ˆ z as the score. Since the personalizer takes pattern representations as input, the personalizer possesses a receptive field over the entire pattern, enabling a comprehensive evaluation of the pattern. Additionally, the Gumbel noise is introduced to encourage the scores to approximate a discrete selection operation, which is a desirable property[18]. To ensure the generality of the framework, we utilize the calculated score to adjust the weighting of training losses, which does not necessitate any additional modifications to target models. We start by defining the original next-item prediction loss:   where 𝑝 𝑖 represents one regenerated pattern, h 𝑖 𝑡 -1 is the representation of pattern 𝑖 at position 𝑡 -1, v 𝑖 𝑡 is the embedding of the target item in 𝑝 𝑖 , 𝜎 is the sigmoid function, and we randomly sample one negative item 𝑣 𝑗 at each time step 𝑡 for each pattern. The purpose KDD '24, August 25-29, 2024, Barcelona, Spain Mingjia Yin, Hao Wang, Wei Guo, Yong Liu, Suojuan Zhang, Sirui Zhao, Defu Lian, and Enhong Chen of Equation 7 is to maximize the probability of recommending the expected next item 𝑣 𝑖 𝑡 given the pattern representation h 𝑖 𝑡 -1 . Subsequently, the training loss function on the personalized dataset can be defined as follows:  4.3.2 Efficient optimization of dataset personalizer. The optimization of the dataset personalizer presents a nontrivial challenge. Should we attempt to optimize it concurrently with the target model using Equation 8, we are likely to confront a model collapse issue, wherein the personalizer assigns zero to each data sample. To confront this challenge, we formalize the optimization process as a bi-level optimization problem. Specifically, our goal is to train an optimal dataset personalizer to obtain an optimal personalized dataset, on which the target model can achieve the best recommendation performance:  where 𝐿 train ( 𝜃, 𝜙 ) is defined in Equation 8. The lower optimization aims to get an optimal target model with parameter 𝜃 ∗ while fixing the dataset personalizer. Besides, 𝜃 ∗ is an implicit function of 𝜙 , so it can be denoted as 𝜃 ∗ ( 𝜙 ) . 𝐿 dev ( 𝜃 ∗ ( 𝜙 )) is defined in Equation 6, which is the unweighted recommendation loss of the target model on a validation dataset. In the context of batch training, we can shuffle the original dataset to create a validation set[2]. Then we can elaborate on how to optimize the bi-level optimization problem. Theoretically, achieving the optimal point of the target model through training is a prerequisite before conducting a one-step upper-level optimization. For the sake of efficiency, we conduct a one-step upper optimization after only 𝑇 lower rounds of lower-level optimization in our implementation. The lower optimization can be directly optimized with gradient descent. As for the upper optimization, we need to compute ∇ 𝜙 𝐿 dev ( 𝜃 ∗ ( 𝜙 )) . Given that 𝜃 ∗ ( 𝜙 ) is an implicit function of 𝜙 , we can derive:  The detailed derivations can be found in Appendix A.2. Equation 10 can be efficiently calculated with vector-Jacobi product[36]. Through this, we can personalize the regenerated dataset without incurring a substantial time overhead.",
  "5 EXPERIMENTAL EVALUATION": "",
  "5.1 Experimental Settings": "5.1.1 Datasets. To validate the effectiveness of our proposed approach, we follow previous works[3, 59] to conduct experiments on four commonly used and publicly available datasets: · Beauty, Sports, Toys 1 : The Amazon-review dataset has emerged as a prominent resource for evaluating recommendation systems. For our study, we specifically choose three categories, namely \"Beauty,\" \"Sports and Outdoors,\" and \"Toys and Games.\" 1 http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ Table 1: Statistics of the datasets. · Yelp 2 : The Yelp dataset is a widely used business review dataset that contains information about businesses, users, and reviews. We follow the preprocessing in [3, 59] to guarantee a minimum of 5 interactions associated with each user and item. Statistics of the preprocessed datasets are summarized in Table 1. 5.1.2 Compared Baselines and Target Models. To verify the superiority of DR4SR, we select two representative data-centric baselines: · ∞ -AE [40]: an AutoEncoder-based framework, synthesizing user interaction data with sampling-based reconstruction. · MELT [21]: it focused on the long-tail problem through data generation for long-tail users and items, respectively. Tofully demonstrate the cross-architecture generalizability of DR4SR, we select target models from various categories: · RNN-based: GRU4Rec[11] is a sequential recommender system that applies GRU layers to capture dynamic user preferences. · Transformer-based: SASRec[20] is a sequential recommendation model that leverages self-attention mechanisms[45] to make item recommendations for the next user action. · Denoising-based: FMLP[82] is one of the most representative denoising-based SR methods, which adopts a filter-enhanced MLP to remove noises in interaction sequences. · GNN-based: GCE-GNN[53] constructs an item graph to capture global context information facilitating recommendation. · Contrastive-learning-based: CL4SRec[59] is one of the most powerful contrastive sequential recommenders, which introduces three sequence-level augmentation strategies. The proposed framework is a data-centric framework, which is complementary to those model-centric methods. Therefore, we integrate DR4SR with all of them to validate the cross-architecture generalizability of DR4SR: · Backbone: We train one target model on the original dataset. · DR4SR: We integrate DR4SR with the target model. · DR4SR+: We integrate DR4SR+ with the target model. 5.1.3 Evaluation Protocols. To assess the performance of the nextitem recommendations, we adopt the widely-used leave-one-out strategy[3, 20, 59]. Considering evaluation metrics, we employ two ranking metrics Recall@{10, 20} and NDCG@{10, 20}. Furthermore, we follow the suggestion of Krichene and Rendle[24] to adopt the whole item set as the candidate item set during evaluation. 5.1.4 Implementation Details. To ensure a fair comparison environment, we implement DR4SR and all target models based on an open-source recommender system library RecStudio[28]. The training process is configured with a maximum of 1000 epochs, and early stopping is employed if the NDCG@20 on the validation data 2 https://www.yelp.com/dataset DR4SR KDD '24, August 25-29, 2024, Barcelona, Spain Table 2: Statistics of the regenerated datasets. does not improve for 20 consecutive epochs. The training batch size 𝐵 is fixed to 256, Adam[22] with a learning rate of 1e-3 is used as the optimizer, the embedding size 𝑑 is set to 64, and the maximum sequence length 𝑁 is set to 50 for all datasets. BCE loss is used for all baselines. Detailed hyper-parameter settings of each model can be found in Appendix A.4. Notably, we fixed the hyper-parameters of each target model when integrating DR4SR with them.",
  "5.2 Overall Performance": "We compared the performance of each target model with \"DR4SR\" and \"DR4SR+\" variants to verify the efficacy of the proposed framework. The statistics of regenerated datasets can be found in Table 2 in the Appendix. From the overall performance presented in Table 3, we can draw the following conclusions: (1) DR4SR can regenerate an informative and generalizable dataset: Across all datasets and target models, we can observe a considerable performance enhancement when comparing the \"DR4SR\" variant with target models. (2) Different target models prefer different datasets: An additional improvement can be observed when comparing the \"DR4SR\" variant with the \"DR4SR+\" variant. This is because DR4SR is independent of target models, which makes the regenerated dataset sub-optimal. (3) Denoising is just one aspect of the broader training data development problem: We can find that FMLP can yield significant performance improvement when integrated with DR4SR and DR4SR+. This is because dataset regeneration can additionally capture the one-to-many relationship of sequences and patterns. Moreover, dataset regeneration has the potential to discover latent transition patterns with the hybrid inference mode. (4) Data-centric and model-centric paradigms are complementary: The proposed data-centric paradigm has consistently demonstrated benefits across various datasets and target models, emphasizing the complementarity of the data-centric and model-centric paradigms. This underscores the potential for advancing recommender systems from a data-centric perspective.",
  "5.3 Ablation Study": "In this section, we aim to evaluate the efficacy of each module within DR4SR. An ablation study is conducted using SASRec as the target model. We have designed the following model variants: (A) \"-diversity\": This aims to confirm the significance of capturing the one-to-many relationship by replacing the diversity-promoted regenerator with a vanilla transformer. (B) \"pattern\": This seeks to substantiate the need for a trainable regenerator by employing patterns extracted based on predefined rules as the regenerated dataset. (C) \"end-to-end\": This endeavors to illustrate the importance of bi-level optimization by optimizing the dataset personalizer in an end-to-end fashion. Results are presented in Table 4. From the table, we can obtain the following conclusions: (1) Comparing (A) with \"DR4SR+\", we can observe significant performance loss, which demonstrates the necessity of modeling the one-tomany mapping relationship between the original sequences and patterns. (2) Comparing (B) and \"SASRec\", we can observe that using only the pattern dataset leads to a decrease in performance since many valuable infrequent patterns are ignored. Thus highlighting the superiority of the proposed powerful regenerator. (3) Comparing (C) and \"DR4SR+\", we find that the end-to-end learning approach yields poor performance, which is unsurprising as the end-to-end training may readily converge to a trivial solution where the dataset personalizer assigns zero scores to all data samples.",
  "5.4 Advanced Study": "To demonstrate the superiority of the proposed paradigm, we have conducted in-depth analysis experiments from various aspects. The obtained experimental results and analysis are as follows:",
  "5.4.1 Analysis of efficiency of dataset personalization . Bi-": "level optimization is widely recognized for its inefficiency and high memory consumption[36], which hinders its practical applicability. Consequently, we conducted an efficiency analysis to validate the efficiency of the adopted bi-level optimization process. The experimental results are presented in Table 5. It can be observed from the results that the training of the dataset personalizer using the implicit differentiation method does not incur significant additional time and space overhead. This demonstrates that the regenerated dataset can be efficiently tailored to different target models.",
  "5.4.2 Analysis of data sample scores of different target mod-": "els . We randomly selected 25 sequences from the regenerated dataset and visualized the sample scores learned by different target models, as depicted in Figure 4. The figure reveals noticeable variations in dataset preferences among different models. Notably, GRU4Rec demonstrates a higher degree of score variance compared to other methods, which exhibit more uniform weight assignments. This is because the powerful self-attention mechanism or filterenhanced MLP can adaptively determine the utilization of items in the sequence to some extent. However, even among these methods, discernible preference differences can still be observed. These observations further validate the crucial role of the dataset personalizer.",
  "5.4.3 Analysis of data forms that should be regenerated.": "After regenerating a dataset, we can construct a new graph for GNN and perform data augmentation based on the new dataset. This naturally raises the question of which dataset should be adopted for graph construction or data augmentation: the original or the regenerated one. To shed light on this matter, we further conducted two experiments. Firstly, we compared the effects of using different graphs, and the results are shown in Figure 5a. The results are subtle, with the original graph performing better on half of the dataset, while the regenerated graph yields superior results on the other half. Subsequently, we compared the effects of using different augmentation data, and the results are shown in Figure 5b. From the table, we can observe that DR4SR yields superior data augmentation samples for CL4SRec on the first two datasets. This is attributed to the regenerated sequences containing richer semantic KDD '24, August 25-29, 2024, Barcelona, Spain Mingjia Yin, Hao Wang, Wei Guo, Yong Liu, Suojuan Zhang, Sirui Zhao, Defu Lian, and Enhong Chen Table 3: The overall performance. Considering a target model, the best result is bolded while the second-best result is underlined. Superscript * means improvements are statistically significant with p<0.05 while ** meaning p<0.01. 0 10 20 0 5 10 15 20 GRU4Rec 0 10 20 SASRec 0 10 20 GNN 0 10 20 CL4SRec 0 10 20 FMLP 0.0 0.2 0.4 0.6 0.8 Sequence ID Position of items in the sequence Figure 4: Scores assigned by the dataset personalizer for different target models on Toys. From left to right, the variances of the scores for each target model are 0.0280, 0.0244, 0.0241, 0.0248, and 0.0247. Table 4: Abalation study of DR4SR on NDCG@20. Table 5: Time and Space Efficiency Analysis. information. However, for the last two datasets, we notice that there is no significant improvement. Considering these results, we can observe that while DR4SR can provide curated sequence training datasets for diverse target models, certain advanced data formats such as graphs and data used Figure 5: Relative NDCG@20 improvement of graphs and data augmentations on different datasets. Beauty Sport Toys Yelp 0.80 0.85 0.90 0.95 1.00 1.05 1.10 Relative NDCG@20 improvement Original Regenerated (a) Different graphs Beauty Sport Toys Yelp 0.8 0.9 1.0 1.1 1.2 1.3 Relative NDCG@20 improvement Original Regenerated (b) Different augmentations for augmentation still need to be constructed based on the original dataset in certain scenarios. Therefore, we conclude that for the sequential recommendation, the scope of data regeneration can be extended beyond the interaction sequences. For instance, graph structure learning[83] can be employed to construct a more suitable correlation graph, and leverage learnable data augmentation[14, DR4SR KDD '24, August 25-29, 2024, Barcelona, Spain Figure 6: Recommendation Performance w.r.t different K. 1 3 5 7 9 0.0820 0.0845 0.0870 0.0895 0.0920 Recall@20 Recall@20 NDCG@20 0.032 0.034 0.036 0.038 0.040 NDCG@20 (a) Beauty 1 3 5 7 9 0.0940 0.0975 0.1010 0.1045 0.1080 Recall@20 Recall@20 NDCG@20 0.045 0.047 0.049 0.051 0.053 NDCG@20 (b) Toys Figure 7: Recommendation Performance w.r.t different 𝛾 . 0 0.1 0.3 0.5 1 0.08100 0.08425 0.08750 0.09075 0.09400 Recall@20 Recall@20 NDCG@20 0.03400 0.03575 0.03750 0.03925 0.04100 NDCG@20 (a) Beauty 0 0.1 0.3 0.5 1 0.0980 0.1005 0.1030 0.1055 0.1080 Recall@20 Recall@20 NDCG@20 0.0450 0.0475 0.0500 0.0525 0.0550 NDCG@20 (b) Toys 62, 64, 84] can be utilized for contrastive learning. In this study, we primarily focus on regenerating sequences, and the exploration of regenerating other data forms will be addressed in future work.",
  "5.5 Hyper-parameter Sensitivity": "In this subsection, we investigate the hyper-parameter sensitivity of the proposed paradigm. Specifically, we focus on the impact of two parameters on model performance, the diversity factor 𝐾 and the generative decoding probability 𝛾 . For the sake of simplicity, we conducted experiments on Toys and Beauty based on SASRec. 5.5.1 The diversity factor K. We set K among [ 1 , 3 , 5 , 7 , 9 ] and the results are depicted in Figure 6, where the performance exhibits an increasing trend as K increases. This is because by increasing K, we can effectively reduce confusion during training and simultaneously enhance the diversity of the regenerated data. Furthermore, once K exceeded a certain threshold, further increasing did not lead to significant changes. This indicates that the regenerator had already captured all available patterns, reaching a plateau phase. 5.5.2 The generative decoding probability 𝛾 . We set the parameter 𝛾 within the range of [ 0 , 0 . 1 , 0 . 3 , 0 . 5 , 1 ] and visualized the results in Figure 7. The performance initially shows an increasing trend followed by a decrease. This observation highlights the importance of striking a balance between exploration and exploitation during the regeneration process. When 𝛾 is too low, the regenerator is unable to sufficiently explore higher-order information. Conversely, when 𝛾 is set too high, the model underutilizes the existing information, leading to a decrease in the reliability of the generated results.",
  "6 CONCLUSIONS": "In this paper, we have investigated a novel problem concerning the development of training data for sequential recommendations. To tackle this problem, we have introduced a data-centric dataset regeneration framework called DR4SR. Our framework has displayed exceptional cross-architecture generalizability and has underscored the complementary relationship between data-centric and model-centric paradigms. Moreover, we have extended DR4SR to a model-aware version, dubbed DR4SR+, which allows for personalized adaptation of the regenerated datasets to various target models. Additionally, we have conducted in-depth analysis to investigate the potential of the data-centric approach, providing valuable insights. For our future work, we intend to propose a more comprehensive framework that can regenerate various forms of data, such as sequences, graphs, and augmented data. We also plan to explore the integration of large language Models [10, 17] to guide the dataset regeneration process, enabling the generation of data that maintains both collaborative and semantic information.",
  "REFERENCES": "[1] André Bauer, Simon Trapp, Michael Stenger, Robert Leppich, Samuel Kounev, Mark Leznik, Kyle Chard, and Ian T. Foster. 2024. Comprehensive Exploration of Synthetic Data Generation: A Survey. CoRR abs/2401.02524 (2024). [2] Hong Chen, Xin Wang, Ruobing Xie, Yuwei Zhou, and Wenwu Zhu. 2023. Crossdomain Recommendation with Behavioral Importance Perception. In Proceedings of the ACM Web Conference 2023, WWW 2023, Austin, TX, USA, 30 April 2023 - 4 May 2023 . 1294-1304. [3] Yongjun Chen, Zhiwei Liu, Jia Li, Julian J. McAuley, and Caiming Xiong. 2022. Intent Contrastive Learning for Sequential Recommendation. In WWW'22: The ACM Web Conference 2022, Virtual Event, Lyon, France, April 25 - 29, 2022 . ACM, 2172-2182. [4] Wenqi Fan, Xiaorui Liu, Wei Jin, Xiangyu Zhao, Jiliang Tang, and Qing Li. 2022. Graph Trend Filtering Networks for Recommendation. In SIGIR '22: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . 112-121. [5] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David WardeFarley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. 2014. Generative Adversarial Networks. CoRR abs/1406.2661 (2014). [6] Ziyao Guo, Kai Wang, George Cazenavette, Hui Li, Kaipeng Zhang, and Yang You. 2023. Towards Lossless Dataset Distillation via Difficulty-Aligned Trajectory Matching. CoRR abs/2310.05773 (2023). [7] Yongqiang Han, Hao Wang, Kefan Wang, Likang Wu, Zhi Li, Wei Guo, Yong Liu, Defu Lian, and Enhong Chen. 2024. END4Rec: Efficient Noise-Decoupling for Multi-Behavior Sequential Recommendation. CoRR abs/2403.17603 (2024). [8] Yongqiang Han, Likang Wu, Hao Wang, Guifeng Wang, Mengdi Zhang, Zhi Li, Defu Lian, and Enhong Chen. 2023. Guesr: A global unsupervised dataenhancement with bucket-cluster sampling for sequential recommendation. In International Conference on Database Systems for Advanced Applications . Springer, 286-296. [9] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yong-Dong Zhang, and Meng Wang. 2020. LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 . 639-648. [10] Zhicheng He, Weiwen Liu, Wei Guo, Jiarui Qin, Yingxue Zhang, Yaochen Hu, and Ruiming Tang. 2023. A survey on user behavior modeling in recommender systems. arXiv preprint arXiv:2302.11087 (2023). [11] Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2016. Session-based Recommendations with Recurrent Neural Networks. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings . [12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising Diffusion Probabilistic Models. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual . [13] Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet, Mohammad Norouzi, and Tim Salimans. 2022. Cascaded Diffusion Models for High Fidelity Image Generation. J. Mach. Learn. Res. 23 (2022), 47:1-47:33. [14] Chengkai Hou, Jieyu Zhang, and Tianyi Zhou. 2023. When to Learn What: ModelAdaptive Data Augmentation Curriculum. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023 . 1717-1728. [15] Yupeng Hou, Shanlei Mu, Wayne Xin Zhao, Yaliang Li, Bolin Ding, and Ji-Rong Wen. 2022. Towards Universal Sequence Representation Learning for Recommender Systems. In KDD '22: The 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Washington, DC, USA, August 14 - 18, 2022 . ACM, 585-593. KDD '24, August 25-29, 2024, Barcelona, Spain Mingjia Yin, Hao Wang, Wei Guo, Yong Liu, Suojuan Zhang, Sirui Zhao, Defu Lian, and Enhong Chen [37] Aleksandr V. Petrov and Craig Macdonald. 2022. Effective and Efficient Training for Sequential Recommendation using Recency Sampling. In RecSys . ACM, 81-91. [56] Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, Hui Xiong, and Enhong Chen. 2023. A Survey on Large Language Models for Recommendation. CoRR abs/2305.19860 (2023). DR4SR KDD '24, August 25-29, 2024, Barcelona, Spain [57] Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, and Tieniu Tan. 2018. Session-based Recommendation with Graph Neural Networks. CoRR abs/1811.00855 (2018). [58] Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and Zhifang Sui. 2024. Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding. CoRR abs/2401.07851 (2024). [59] Xu Xie, Fei Sun, Zhaoyang Liu, Shiwen Wu, Jinyang Gao, Jiandong Zhang, Bolin Ding, and Bin Cui. 2022. Contrastive Learning for Sequential Recommendation. In 38th IEEE International Conference on Data Engineering, ICDE 2022, Kuala Lumpur, Malaysia, May 9-12, 2022 . IEEE, 1259-1273. [60] Chengfeng Xu, Pengpeng Zhao, Yanchi Liu, Victor S. Sheng, Jiajie Xu, Fuzhen Zhuang, Junhua Fang, and Xiaofang Zhou. 2019. Graph Contextualized SelfAttention Network for Session-based Recommendation. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019 . 3940-3946. [61] An Yan, Shuo Cheng, Wang-Cheng Kang, Mengting Wan, and Julian J. McAuley. 2019. CosRec: 2D Convolutional Neural Networks for Sequential Recommendation. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management, CIKM 2019, Beijing, China, November 3-7, 2019 . 2173-2176. [62] Xihong Yang, Jin Jiaqi, Siwei Wang, Ke Liang, Yue Liu, Yi Wen, Suyuan Liu, Sihang Zhou, Xinwang Liu, and En Zhu. 2023. Dealmvc: Dual contrastive calibration for multi-view clustering. In Proceedings of the 31st ACM International Conference on Multimedia . 337-346. [63] Xihong Yang, Yue Liu, Sihang Zhou, Siwei Wang, Wenxuan Tu, Qun Zheng, Xinwang Liu, Liming Fang, and En Zhu. 2023. Cluster-guided Contrastive Graph Clustering Network. In Proceedings of the AAAI conference on artificial intelligence , Vol. 37. 10834-10842. [64] Xihong Yang, Cheng Tan, Yue Liu, Ke Liang, Siwei Wang, Sihang Zhou, Jun Xia, Stan Z Li, Xinwang Liu, and En Zhu. 2023. Convert: Contrastive graph clustering with reliable augmentation. In Proceedings of the 31st ACM International Conference on Multimedia . 319-327. [65] Ghim-Eng Yap, Xiaoli Li, and Philip S. Yu. 2012. Effective Next-Items Recommendation via Personalized Sequential Pattern Mining. In Database Systems for Advanced Applications - 17th International Conference, DASFAA 2012, Busan, South Korea, April 15-19, 2012, Proceedings, Part II . 48-64. [66] Mingjia Yin, Hao Wang, Wei Guo, Yong Liu, Zhi Li, Sirui Zhao, Defu Lian, and Enhong Chen. 2024. Learning Partially Aligned Item Representation for CrossDomain Sequential Recommendation. arXiv preprint arXiv:2405.12473 (2024). [67] Mingjia Yin, Hao Wang, Xiang Xu, Likang Wu, Sirui Zhao, Wei Guo, Yong Liu, Ruiming Tang, Defu Lian, and Enhong Chen. 2023. APGL4SR: A Generic Framework with Adaptive and Personalized Global Collaborative Information in Sequential Recommendation. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, CIKM 2023, Birmingham, United Kingdom, October 21-25, 2023 . ACM, 3009-3019. [68] Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Jundong Li, and Zi Huang. 2024. Self-Supervised Learning for Recommender Systems: A Survey. IEEE Trans. Knowl. Data Eng. 36, 1 (2024), 335-355. [69] Ruonan Yu, Songhua Liu, and Xinchao Wang. 2024. Dataset Distillation: A Comprehensive Review. IEEE Trans. Pattern Anal. Mach. Intell. 46, 1 (2024), 150-170. [70] Jiahao Yuan, Zihan Song, Mingyou Sun, Xiaoling Wang, and Wayne Xin Zhao. 2021. Dual Sparse Attention Network For Session-based Recommendation. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021 . 4635-4643. [71] Daochen Zha, Zaid Pervaiz Bhat, Kwei-Herng Lai, Fan Yang, Zhimeng Jiang, Shaochen Zhong, and Xia Hu. 2023. Data-centric Artificial Intelligence: A Survey. CoRR abs/2303.10158 (2023). [72] An Zhang, Wenchang Ma, Jingnan Zheng, Xiang Wang, and Tat-Seng Chua. 2023. Robust Collaborative Filtering to Popularity Distribution Shift. CoRR abs/2310.10696 (2023). [73] Chi Zhang, Rui Chen, Xiangyu Zhao, Qilong Han, and Li Li. 2023. Denoising and Prompt-Tuning for Multi-Behavior Recommendation. In Proceedings of the ACM Web Conference 2023, WWW 2023, Austin, TX, USA, 30 April 2023 - 4 May 2023 . 1355-1363. [74] Luankang Zhang, Hao Wang, Suojuan Zhang, Mingjia Yin, Yongqiang Han, Jiaqing Zhang, Defu Lian, and Enhong Chen. 2024. A Unified Framework for Adaptive Representation Enhancement and Inversed Learning in Cross-Domain Recommendation. arXiv preprint arXiv:2404.00268 (2024). [75] Yuren Zhang, Enhong Chen, Binbin Jin, Hao Wang, Min Hou, Wei Huang, and Runlong Yu. 2022. Clustering based behavior sampling with long sequential data for CTR prediction. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval . 2195-2200. [76] Bo Zhao and Hakan Bilen. 2023. Dataset Condensation with Distribution Matching. In IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2023, Waikoloa, HI, USA, January 2-7, 2023 . 6503-6512. [77] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. 2021. Dataset Condensation with Gradient Matching. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 . [78] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. 2021. Dataset Condensation with Gradient Matching. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 . [79] Xin Zheng, Yixin Liu, Zhifeng Bao, Meng Fang, Xia Hu, Alan Wee-Chung Liew, and Shirui Pan. 2023. Towards Data-centric Graph Machine Learning: Review and Outlook. CoRR abs/2309.10979 (2023). [80] Daquan Zhou, Kai Wang, Jianyang Gu, Xiangyu Peng, Dongze Lian, Yifan Zhang, Yang You, and Jiashi Feng. 2023. Dataset Quantization. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023 . 1715917170. [81] Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, and Ji-Rong Wen. 2020. S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization. In Proceedings of the 29th ACM international conference on information & knowledge management . 1893-1902. [82] Kun Zhou, Hui Yu, Wayne Xin Zhao, and Ji-Rong Wen. 2022. Filter-enhanced MLP is All You Need for Sequential Recommendation. In WWW'22: The ACM Web Conference 2022, Virtual Event, Lyon, France, April 25 - 29, 2022 . 2388-2399. [83] Yanqiao Zhu, Weizhi Xu, Jinghao Zhang, Qiang Liu, Shu Wu, and Liang Wang. 2021. Deep Graph Structure Learning for Robust Representations: A Survey. CoRR abs/2103.03036 (2021). [84] Xinyu Zuo, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao, Weihua Peng, and Yuguang Chen. 2021. LearnDA: Learnable knowledge-guided data augmentation for event causality identification. arXiv preprint arXiv:2106.01649 (2021).",
  "A APPENDIX": "",
  "A.1 Pseudo Code of DR4SR and DR4SR+": "The pipeline of DR4SR is detailed in Algorithm 1, and the pipeline of DR4SR+ is detailed in Algorithm 2.",
  "A.2 Detailed Derivations of Equation 10": "By applying the chain rule, we can compute calculate the implicit gradient of 𝐿 dev ( 𝜃 ∗ ( 𝜙 )) as follows:  The first term ∇ 𝜃 𝐿 dev ( 𝜃 ∗ ( 𝜙 )) can be obtained with automatic differentiation (autograd) tools. As for the second term, we notice that 𝜃 ∗ ( 𝜙 ) is the optimal point of 𝐿 train ( 𝜃 ; 𝜙 ) , so we have:  Then, we calculate the gradient with respect to 𝜙 in Equation 12: ∇ 2 𝐿 𝜃 ∗ 𝜙 , 𝜙 𝜃 ∗ 𝜙 𝐿 𝜃 ∗ 𝜙 , 𝜙 , 𝜃 train ( ( ) )∇ 𝜙 ( ) + ∇ 𝜙 ∇ 𝜃 train ( ( ) ) = 0 (13) which leads to:  where the Hessian inverse (∇ 2 𝜃 𝐿 train ( 𝜃 ∗ ( 𝜙 ) , 𝜙 )) -1 can be approximated to ˝ 𝐾 𝑛 = 0 ( 𝐼 - ∇ 2 𝜃 𝐿 train ( 𝜃 ∗ ( 𝜙 )) 𝑛 with K-truncated Neumann series[35, 36]. Therefore, we can obtain the following results:  Finally, the implicit gradient of 𝐿 dev ( 𝜃 ∗ ( 𝜙 )) is:  where the parameters are omitted for the sake of brevity. KDD '24, August 25-29, 2024, Barcelona, Spain Mingjia Yin, Hao Wang, Wei Guo, Yong Liu, Suojuan Zhang, Sirui Zhao, Defu Lian, and Enhong Chen Table 6: The overall performance on two additional datasets.",
  "A.3 Computational Complexity": "The scale of datasets can be measured with the number of sequences N, maximum sequence length L, maximum pattern length M, and the number of extracted patterns q (empirically, q is of the same order of magnitude as N). We denote the diversity factor as K. The pre-training dataset is constructed with sequential pattern mining, which is implemented with Seq2Pat [52] and its complexity is 𝑂 ( 𝑁𝑀𝑞 ) . Empirically, this process can be accelerated with offline pre-computation or batching technique [52]. During pretraining, the regenerator needs to process each sequence. Considering the quadratic complexity of the self-attention mechanism, the overall complexity is 𝑂 ( 𝑞𝐿 2 𝑑 ) . During inference, the complexity is 𝑂 ( 𝐾𝑁𝐿 2 𝑑 ) , as we need to regenerate K patterns from each sequence. Meanwhile, autoregressive decoding can be further accelerated with efficient decoding [58]. As for dataset personalization, denote the complexity of the target model as 𝑎 . The additional overhead",
  "Algorithm 1 Pseudo code of DR4SR": "Input: The original sequence dataset X . Output: Aregenerated dataset X ′ and scores for each data sample W ∈ R | X ′ | . 1: Construct a pre-training dataset X pre as in Section 4.2.1 2: while not converged do ⊲ Pre-training 3: Extract a batch of sequence-pattern pairs ( 𝑠 𝑢 , 𝑝 𝑖 ) from X pre 5: Get a category vector 𝜋 = DiversityPromoter ( 𝑝 𝑖 ) 4: Get K memories { m ′ 1 , m ′ 2 , . . . , m ′ 𝐾 } = Encoder ( 𝑠 𝑢 ) 6: Get a mixed memory by m final = ˝ 𝐾 𝑘 = 1 𝜋 𝑘 m ′ 𝑘 7: Get a decoded sequence 𝑝 ′ 𝑖 = Decoder ( 𝑝 𝑖 , m final ) 8: Optimize the dataset generator with Equation 2",
  "9: end while": "10: while not converged do ⊲ Inference 11: Extract a sequence 𝑠 𝑢 from the original dataset X 12: Get K memories { m ′ 1 , m ′ 2 , . . . , m ′ 𝐾 } = Encoder ( 𝑠 𝑢 ) 13: Init the regenerated dataset X ′ as an empty set 14: for m ′ 𝑘 in { m ′ 1 , m ′ 2 , . . . , m ′ 𝐾 } do 15: Init the regenerated pattern 𝑝 ′ 𝑖 as the <BOS> token 16: while 𝑣 ≠ <EOS> token do 17: Get a probability vector q = Decoder ( 𝑝 ′ 𝑖 , m ′ 𝑘 ) 18: if Generative decoding then 19: Get item 𝑣 ∈ V with the highest probability 20: else 21: Get item 𝑣 ∈ 𝑠 𝑢 with the highest probability 22: end if 23: Update the regenerated pattern 𝑝 ′ 𝑖 ← 𝐶𝑜𝑛𝑐𝑎𝑡 ( 𝑝 ′ 𝑖 , 𝑣 ) 24: end while 25: X ′ ←X ′ ∪ 𝑝 ′ 𝑖 26: end for 27: end while incurred by the bi-level optimization stems from the upper optimization conducted every 𝑇 lower rounds. In the upper optimization, complexity mainly results from the implicit gradient computation, which necessitates (K+2) backward for vector-Jacobi product[36]. Herein, K represents the truncated number of Neumann series, which is fixed at 3. Consequently, the complexity merely amounts to 𝑂 GLYPH<16> GLYPH<16> 1 + 5 𝑇 lower GLYPH<17> 𝑎 GLYPH<17> . Therefore, with some acceleration strategies, DR4SR and DR4SR+ is scalable when applied to large datasets.",
  "A.4 Detailed Hyper-parameters": "We first introduce the hyper-parameter settings of target models: For GRU4Rec, we search the layer number among [2, 3], and dropout probability among [0.2, 0.5]. For SASRec, the layer number is 2, the dimension of hidden layers is 128, and the dropout probability is 0.5. For FMLP, the layer number is 2, the dimension of hidden layers is 256, and the dropout probability is 0.5. For GCEGNN, we first replace its graph encoder with LightGCN[9] and its sequential encoder with SASRec[20], which can significantly improve its performance. Then, we search the number of graph convolution layers among [2, 3]. For CL4SRec, we set the weight of contrastive learning loss to 0.1, the crop/mask/reorder rate in data augmentation is set to 0.2/0.7/0.2, and other hyper-parameters are the same as SASRec. Then we consider particular hyper-parameters of DR4SR. For dataset regeneration, we set sliding window size 𝛼 to 5 for Beauty and 10 for others, and we set the threshold 𝛽 to 2 for all datasets. We fix the diversity number 𝐾 to 5, and the generative decoding probability 𝛾 to 0.1. For DR4SR+, we set 𝑇 lower to 30, and adopt",
  "Algorithm 2 Pseudo code of DR4SR+": "Input: The regenerated sequence dataset X ′ , a target model 𝑓 with parameters 𝜃 . Output: Apersonalizer 𝑔 with parameters 𝜙 that can assign scores W ∈ R | X ′ | for each data sample in X ′ . 1: Generate a dev dataset X ′ dev by shuffling X ′ 2: while not converged do 3: // Lower optimization 4: for round = 1 → 𝑇 lower do 5: 6: 7: Extract a batch 𝑏 from X ′ Get data sample scores W = 𝑔 ( 𝑏 ) Optimize 𝑓 based on Equation 8 8: end for 9: // Upper optimization 10: Extract a batch 𝑏 X ′ 11: Get ∇ 𝜃 𝐿 dev ( 𝜃 ∗ ( 𝜙 )) with 𝑏 dev and Equation 6 dev from dev 12: Get ∇ 𝜙 𝜃 ∗ ( 𝜙 ) by implicit differentiation 13: Get ∇ 𝜙 𝐿 dev ( 𝜃 ∗ ( 𝜙 )) as in Equation 11 14: update 𝜙 with ∇ 𝜙 𝐿 dev ( 𝜃 ∗ ( 𝜙 )) 15: end while DR4SR KDD '24, August 25-29, 2024, Barcelona, Spain SGD as the upper optimizer with learning rate and weight decay searched among [1e-3, 1e-2, 1e-1].",
  "A.5 Generalization to More Datasets": "Weprovide the results on two additional datasets Amazon-book and Gowalla in Table 6, where we can find that the proposed method can be generalized to datasets with large-scale or different distribution.",
  "keywords_parsed": [
    "Recommendation System",
    "Sequential Recommendation",
    "Data-Centric AI",
    "Data Generation"
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "Comprehensive Exploration of Synthetic Data Generation: A Survey"
    },
    {
      "ref_id": "b2",
      "title": "Crossdomain Recommendation with Behavioral Importance Perception"
    },
    {
      "ref_id": "b3",
      "title": "Intent Contrastive Learning for Sequential Recommendation"
    },
    {
      "ref_id": "b4",
      "title": "Graph Trend Filtering Networks for Recommendation"
    },
    {
      "ref_id": "b5",
      "title": "Generative Adversarial Networks"
    },
    {
      "ref_id": "b6",
      "title": "Towards Lossless Dataset Distillation via Difficulty-Aligned Trajectory Matching"
    },
    {
      "ref_id": "b7",
      "title": "END4Rec: Efficient Noise-Decoupling for Multi-Behavior Sequential Recommendation"
    },
    {
      "ref_id": "b8",
      "title": "Guesr: A global unsupervised data-enhancement with bucket-cluster sampling for sequential recommendation"
    },
    {
      "ref_id": "b9",
      "title": "LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation"
    },
    {
      "ref_id": "b10",
      "title": "A survey on user behavior modeling in recommender systems"
    },
    {
      "ref_id": "b11",
      "title": "Session-based Recommendations with Recurrent Neural Networks"
    },
    {
      "ref_id": "b12",
      "title": "Denoising Diffusion Probabilistic Models"
    },
    {
      "ref_id": "b13",
      "title": "Cascaded Diffusion Models for High Fidelity Image Generation"
    },
    {
      "ref_id": "b14",
      "title": "When to Learn What: Model-Adaptive Data Augmentation Curriculum"
    },
    {
      "ref_id": "b15",
      "title": "Towards Universal Sequence Representation Learning for Recommender Systems"
    },
    {
      "ref_id": "b37",
      "title": "Effective and Efficient Training for Sequential Recommendation using Recency Sampling"
    },
    {
      "ref_id": "b56",
      "title": "A Survey on Large Language Models for Recommendation"
    },
    {
      "ref_id": "b57",
      "title": "Session-based Recommendation with Graph Neural Networks"
    },
    {
      "ref_id": "b58",
      "title": "Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding"
    },
    {
      "ref_id": "b59",
      "title": "Contrastive Learning for Sequential Recommendation"
    },
    {
      "ref_id": "b60",
      "title": "Graph Contextualized Self-Attention Network for Session-based Recommendation"
    },
    {
      "ref_id": "b61",
      "title": "CosRec: 2D Convolutional Neural Networks for Sequential Recommendation"
    },
    {
      "ref_id": "b62",
      "title": "Dealmvc: Dual contrastive calibration for multi-view clustering"
    },
    {
      "ref_id": "b63",
      "title": "Cluster-guided Contrastive Graph Clustering Network"
    },
    {
      "ref_id": "b64",
      "title": "Convert: Contrastive graph clustering with reliable augmentation"
    },
    {
      "ref_id": "b65",
      "title": "Effective Next-Items Recommendation via Personalized Sequential Pattern Mining"
    },
    {
      "ref_id": "b66",
      "title": "Learning Partially Aligned Item Representation for Cross-Domain Sequential Recommendation"
    },
    {
      "ref_id": "b67",
      "title": "APGL4SR: A Generic Framework with Adaptive and Personalized Global Collaborative Information in Sequential Recommendation"
    },
    {
      "ref_id": "b68",
      "title": "Self-Supervised Learning for Recommender Systems: A Survey"
    },
    {
      "ref_id": "b69",
      "title": "Dataset Distillation: A Comprehensive Review"
    },
    {
      "ref_id": "b70",
      "title": "Dual Sparse Attention Network For Session-based Recommendation"
    },
    {
      "ref_id": "b71",
      "title": "Data-centric Artificial Intelligence: A Survey"
    },
    {
      "ref_id": "b72",
      "title": "Robust Collaborative Filtering to Popularity Distribution Shift"
    },
    {
      "ref_id": "b73",
      "title": "Denoising and Prompt-Tuning for Multi-Behavior Recommendation"
    },
    {
      "ref_id": "b74",
      "title": "A Unified Framework for Adaptive Representation Enhancement and Inversed Learning in Cross-Domain Recommendation"
    },
    {
      "ref_id": "b75",
      "title": "Clustering based behavior sampling with long sequential data for CTR prediction"
    },
    {
      "ref_id": "b76",
      "title": "Dataset Condensation with Distribution Matching"
    },
    {
      "ref_id": "b77",
      "title": "Dataset Condensation with Gradient Matching"
    },
    {
      "ref_id": "b78",
      "title": "Dataset Condensation with Gradient Matching"
    },
    {
      "ref_id": "b79",
      "title": "Towards Data-centric Graph Machine Learning: Review and Outlook"
    },
    {
      "ref_id": "b80",
      "title": "Dataset Quantization"
    },
    {
      "ref_id": "b81",
      "title": "S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization"
    },
    {
      "ref_id": "b82",
      "title": "Filter-enhanced MLP is All You Need for Sequential Recommendation"
    },
    {
      "ref_id": "b83",
      "title": "Deep Graph Structure Learning for Robust Representations: A Survey"
    },
    {
      "ref_id": "b84",
      "title": "LearnDA: Learnable knowledge-guided data augmentation for event causality identification"
    }
  ]
}