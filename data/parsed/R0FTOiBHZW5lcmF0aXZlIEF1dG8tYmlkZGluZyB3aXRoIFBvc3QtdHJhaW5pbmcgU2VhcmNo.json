{"GAS: Generative Auto-bidding with Post-training Search": "", "Yewen Li \u2217": "", "Shuai Mao \u2217": "Nanyang Technological University Singapore, Singapore yewen001@e.ntu.edu.sg Jingtong Gao City University of Hong Kong Hong Kong, China jt.g@my.cityu.edu.hk Nan Jiang Kuaishou Technology Beijing, China jiangnan07@kuaishou.com", "Fei Pan": "Kuaishou Technology Beijing, China panfei05@kuaishou.com The Chinese University of Hong Kong Hong Kong, China smao@mae.cuhk.edu.hk", "Yunjian Xu": "The Chinese University of Hong Kong Hong Kong, China yjxu@mae.cuhk.edu.hk", "Peng Jiang": "Qingpeng Cai \u2020 Kuaishou Technology Beijing, China caiqingpeng@kuaishou.com", "Bo An": "Kuaishou Technology Beijing, China jiangpeng@kuaishou.com", "Abstract": "Auto-bidding is essential in facilitating online advertising by automatically placing bids on behalf of advertisers. Generative autobidding, which generates bids based on an adjustable condition using models like transformers and diffusers, has recently emerged as a new trend due to its potential to learn optimal strategies directly from data and adjust flexibly to preferences. However, generative models suffer from low-quality data leading to a mismatch between the condition, like return to go , and true action value, especially in long sequential decision-making. Besides, the majority preference in the dataset may hinder models' generalization ability on minority advertisers' preferences. While it is possible to collect high-quality data and retrain multiple models for different preferences, the high cost makes it unaffordable, hindering the advancement of autobidding into the era of large foundation models. To address this, we propose a flexible and practical G enerative A uto-bidding scheme using post-training S earch, termed GAS , to refine a base policy model's output and adapt to various preferences. We use weakto-strong search alignment by training small critics for different preferences and an MCTS-inspired search to refine the model's output. Specifically, a novel voting mechanism with transformer-based critics trained with policy indications could enhance search alignment performance. Additionally, utilizing the search, we provide a fine-tuning method for high-frequency preference scenarios considering computational efficiency. Extensive experiments conducted https://doi.org/10.1145/3701716.3715226 Nanyang Technological University Skywork AI Singapore, Singapore boan@ntu.edu.sg on the real-world dataset and online A/B test on the Kuaishou advertising platform demonstrate the effectiveness of GAS, achieving significant improvements, e.g. , 4.60% increment of target cost. Code is available here.", "CCS Concepts": "\u00b7 Information systems \u2192 Computational advertising .", "Keywords": "Auto-bidding, Generative Model, Search, Preference Alignment", "ACMReference Format:": "Yewen Li, Shuai Mao, Jingtong Gao, Nan Jiang, Yunjian Xu, Qingpeng Cai, Fei Pan, Peng Jiang, and Bo An. 2025. GAS: Generative Auto-bidding with Post-training Search. In Companion Proceedings of the ACM Web Conference 2025 (WWW Companion '25), April 28-May 2, 2025, Sydney, NSW, Australia. ACM,NewYork,NY,USA,10pages.https://doi.org/10.1145/3701716.3715226", "1 Introduction": "The rapid digitalization of commerce has expanded online advertising platforms' reach, making them essential for engaging audiences and boosting sales [12, 45]. Due to numerous impression opportunities, manually adjusting bids to optimize costs under budget and KPI constraints is impractical. To address this, ad platforms now offer auto-bidding services that automate the bidding process using advanced strategies [4, 9, 34]. These strategies consider factors from immediate or historical bidding information, such as the distribution of impression opportunities and remaining budgets [26]. Besides, according to different types of advertisers, the strategy should take their different preferences into consideration. For example, brand advertisers, who aim for long-term growth and awareness, typically aim to show their ads to as many people as possible under constraints like average cost per impression. However, performance advertisers, who focus on maximizing the value of winning impressions, aim to maximize conversions with cost-perconversion constraints [19]. To meet these diverse needs, advertising platforms like Google, Facebook, and Alibaba provide tailored multiple bidding strategies for their customers [2, 13, 17]. Additionally, facing the dynamic advertising environment, strategies must be regularly optimized to stay closely aligned with customers' preferences, thereby achieving long-term commercial benefits [18]. Reinforcement learning [37, 44] (RL) has long been a leading method for enhancing auto-bidding strategies by training agents with an advertising simulator or offline advertising logs. However, RL approaches are predominantly based on the Markovian decision process (MDP), which assumes that future states are determined solely by the current single step's state and action. Recent statistical analyses [18] question this assumption in auto-bidding, revealing a strong correlation between the lengths of historical state sequences and subsequent states. This finding suggests that relying solely on the most recent state can lead to instability in the unpredictable online advertising environment. Moreover, the preference for the RL strategy is not easy to control. USCB [19] proposes calculating an optimal solution under multiple constraints using historical data and then training RL to refine the strategy. However, once deployed, the strategy's preference is fixed, limiting interactivity and controllability. Therefore, this has led to a new trend in developing generative auto-bidding methods based on conditional generative models like transformers and diffusers. With a vector condition representing the preferences, these methods directly output an action or even a trajectory without MDP assumption. For example, decision transformers [7] can utilize extensive historical information for decision-making. Diffusers [1] can directly generate a trajectory for planning given a condition. More importantly, generative models provide flexibility in controlling preferences by simply modifying their condition's value during deployment. As large foundational models based on generative models have shown remarkable progress in applications like natural language processing and computer vision, exemplified by ChatGPT [33] and Stable Diffusion [41], it is foreseeable that the auto-bidding field will also advance towards the foundation model era. This will involve developing a decision foundation model to learn optimal decision strategies directly from large-scale datasets. However, there are also two major intrinsic challenges in applying generative auto-bidding methods. First, the performance of generative bidding methods is significantly influenced by the quality of the dataset, where the collected conditions, i.e. , return to go, cannot reflect the true value of the action. For instance, a good action \ud835\udc4e \ud835\udc61 followed by a bad future action \ud835\udc4e \ud835\udc56 , \ud835\udc56 > \ud835\udc61 , can result in a low return to go for \ud835\udc4e \ud835\udc61 , and vice versa. Thus, the learned strategy is hard to achieve the optimum due to a mismatch between the condition and the true action value during training. Second, realistic bidding tasks often involve varying preferences over time, but generative methods are always trained or even biased to imitate the majority preference [32], necessitating retraining to adapt to new minority preferences. However, since the scaling law could also happen in this auto-bidding field, the bidding model based on a foundation model like a transformer would get larger and larger, making it impractical and costly to retrain a set of large decision transformers for varying preferences, hindering the advancement of auto-bidding into the era of large foundation models. Therefore, a question is raised: 'Could we only use one policy model to efficiently achieve optimum strategy under various preferences?' To address these issues, we propose a G enerative A uto-bidding scheme using post-training S earch, termed GAS , to refine a single base policy model's output and adapt to various preferences efficiently. Instead of revising the condition setting and retraining the policy model to fit various preferences, we adopt the weakto-strong search alignment idea, i.e. , refining the large foundation model's output with small models [6]. Specifically, we train a set of small critics to assess the value of different preferences, then use a search method to refine the model's output inspired by Monte Carlo Tree Search (MCTS) [22]. In this scheme, the mismatch between the condition value and the true action value, approximated by the critics using Q-learning, would be alleviated. This is due to the Q-learning based on a Bellman backup only requires the current reward that is not affected by the future trajectory. A large dataset collected by various policies is also beneficial to training a critic as it could assess actions of different quality, alleviating the out-of-domain overestimation issue [23]. This scheme could also achieve better alignment with different preferences by refining the action guided by these critics without retraining or fine-tuning the model. To summarize, our contribution could be summarized as follows: \u00b7 This paper proposes a flexible and practical framework using a post-training search method for refining and aligning generative auto-bidding models to various preferences, which unlocks the door of auto-bidding foundation models. \u00b7 To enhance the accuracy of the search process, we utilize transformer-based critics that are aware of the underlying policy by leveraging historical bidding sequences and introduce a novel voting mechanism to enhance value approximation accuracy during the value backpropagation stage of the search process. \u00b7 In addition to performing a search during testing time, we provide a fine-tuning method for high-frequency preference scenarios or computational-efficiency-sensitive scenarios. \u00b7 Extensive experiments conducted on a large real-world dataset and an online A/B test on the Kuaishou advertising platform demonstrate the effectiveness of the proposed generative auto-bidding scheme.", "2 Preliminary": "", "2.1 Problem Statement": "During a time period, suppose there are \ud835\udc3b impression opportunities arriving sequentially and indexed by \ud835\udc56 . In a bidding platform, advertisers submit bids to compete for each impression opportunity. An advertiser wins the impression if its bid \ud835\udc4f \ud835\udc56 is higher than the bids of others. Upon winning, the advertiser incurs a cost of \ud835\udc50 \ud835\udc56 , which is typically the highest bid of the other advertisers in a second-price auction setting. During the period, the goal of an advertiser is to maximize the sum value of winning impressions \u02dd \ud835\udc56 \ud835\udc5c \ud835\udc56 \ud835\udc63 \ud835\udc56 , where \ud835\udc63 \ud835\udc56 is the value of impression \ud835\udc56 and \ud835\udc5c \ud835\udc56 is the binary indicator of whether the advertiser wins impression \ud835\udc56 . Besides, the budget and multiple KPI constraints [19] are critical for an ad advertiser to control the ad delivery performance. The budget constraint is considered as \u02dd \ud835\udc56 \ud835\udc5c \ud835\udc56 \ud835\udc50 \ud835\udc56 \u2264 \ud835\udc35 , where \ud835\udc50 \ud835\udc56 is the cost of impression \ud835\udc56 and \ud835\udc35 is the budget. Other KPI constraints are more complicated and can be classified into two categories. The first category is the cost-related (CR) constraints, which restrict the unit cost of certain advertising events, such as CPC and CPA. The second category is the non-cost-related (NCR) constraints, which restrict the average advertising effect, such as CTR and CPI. For simplicity, we consider the auto-bidding with cost-related constraints, which have a unified formulation: \u02dd \ud835\udc56 \ud835\udc50 \ud835\udc56 \ud835\udc57 \ud835\udc5c \ud835\udc56 \u02dd \ud835\udc56 \ud835\udc5d \ud835\udc56 \ud835\udc57 \ud835\udc5c \ud835\udc56 \u2264 \ud835\udc36 \ud835\udc57 , where \ud835\udc36 \ud835\udc57 is the upper bound of \ud835\udc57 -th constraint provided by the advertiser. \ud835\udc5d \ud835\udc56 \ud835\udc57 can be any performance indicator, e.g. , return or constant. \ud835\udc50 \ud835\udc56 \ud835\udc57 is the cost of constraint \ud835\udc57 . Given \ud835\udc3d constraints, we have the Multi-Constrained Bidding (MCB): A previous study [19] has already shown the optimal solution: where \ud835\udc4f \u2217 \ud835\udc56 is the optimal bid prediction for the impression \ud835\udc56 . \ud835\udf06 \ud835\udc57 , \ud835\udc57 \u2208 { 0 , . . . , \ud835\udc3d } are the optimal bidding parameters. However, due to the uncertainty and dynamics of the advertising environment, these optimal bidding parameters are hard or even impossible to calculate directly. Different types of advertisers may have different preferences expressed by various constraint compositions. For example, max-return bidding advertisers only consider the budget constraint and target-CPA bidding advertisers consider both the budget constraint and the CPA constraint.", "2.2 Decision-making Process for Auto-bidding": "As the advertising environment is highly dynamic, the optimal bidding parameters should be adjusted regularly to maximize the total received value over the entire period. This leads to the formulation of the auto-bidding task as a sequential decision-making task. We consider a standard decision-making setup consisting of an auto-bidding agent interacting with advertising environment \ud835\udc38 in discrete timesteps. At each timestep \ud835\udc61 of a bidding period, the agent receives a state \ud835\udc60 \ud835\udc61 \u2208 S describing the real-time advertising status and then outputs an action \ud835\udc4e \ud835\udc61 \u2208 A for the final bid. The advertising environment has an underlying unknown state transition dynamic T . Under an MDP assumption, the transition dynamic T could be expressed by T : \ud835\udc60 \ud835\udc61 \u00d7 \ud835\udc4e \ud835\udc61 \u2192 \ud835\udc60 \ud835\udc61 + 1, i.e. , the next state \ud835\udc60 \ud835\udc61 + 1 \u2208 S is determined by the current state \ud835\udc60 \ud835\udc61 and action \ud835\udc4e \ud835\udc61 . In this case, the agent's policy is expressed as \ud835\udf0b ( \ud835\udc4e \ud835\udc61 | \ud835\udc60 \ud835\udc61 ) . Without an MDP assumption, the next state could be determined by more factors like the historical trajectory \ud835\udf0f . After transitioning to the next state, the advertising environment would emit a reward \ud835\udc5f \ud835\udc61 representing the value contributed to the objective obtained within the time period \ud835\udc61 . Repeating this process until the end of a bidding period, one day for instance, the goal of the auto-bidding agent is to maximize the total received value over the entire period as stated in Equation 1. A detailed description of our modeling are listed below: \u00b7 \ud835\udc60 \ud835\udc61 : the state is a collection of information that describes the advertising status from the perspective of a campaign. The information should in principle reflect time, budget consumption and KPI constraints satisfying status, such as left time, left budget, budget consumption speed, current KPI ratio for constraint \ud835\udc57 (i.e. ( \u03a3 \ud835\udc56 \ud835\udc50 \ud835\udc56 \ud835\udc57 \ud835\udc5c \ud835\udc56 / \u03a3 \ud835\udc56 \ud835\udc5d \ud835\udc56 \ud835\udc57 \ud835\udc5c \ud835\udc56 )/ \ud835\udc36 \ud835\udc57 ), etc. \u00b7 \ud835\udc4e \ud835\udc61 : adjustment to the bidding parameters \ud835\udf06 \ud835\udc57 , \ud835\udc57 = 0 , . . . , \ud835\udc3d , at the time period \ud835\udc61 , and modeled as ( \ud835\udc4e \ud835\udf06 0 \ud835\udc61 , . . . , \ud835\udc4e \ud835\udf06 \ud835\udc3d \ud835\udc61 ) . \u00b7 \ud835\udc5f \ud835\udc61 : at time-step \ud835\udc61 , let C be the candidate impression set between step \ud835\udc61 and \ud835\udc61 + 1, then a typical setting of the reward could be the value contributed to the objective obtained on this C within the time period. For simplicity, we will omit C in the following sections.", "3 Generative Auto-bidding with Search": "In this section, we first introduce how an MCTS-inspired posttraining search process is developed to refine a base policy model's output action and then, we introduce two practical paradigms of applying this post-training search in auto-bidding.", "3.1 MCTS-inspired Post-training Search": "As the decision transformer is widely used as a backbone in generative decision-making, we also adopt the decision transformer as our backbone for auto-bidding and the policy for generating an action could be formulated as where the condition \ud835\udc45 \ud835\udc61 is the return to go at time step \ud835\udc61 , i.e. , where \ud835\udefe is the discounting factor and \ud835\udc5f ( \ud835\udc60 \ud835\udc56 , \ud835\udc4e \ud835\udc56 ) is a rewarding function representing the preference, e.g. , it could be set as the \ud835\udc5c \ud835\udc56 \ud835\udc63 \ud835\udc56 representing a preference considering only the value. Our purpose of the search scheme is to find a better action that could align better with the preference like a higher value. Applying the typical MCTS method to the decision-making process should involve four parts in every time step \ud835\udc61 : \u00b7 Selection: start from a root state node \ud835\udc60 \ud835\udc61 and randomly select successive valid child action nodes \ud835\udc4e \ud835\udc56 \ud835\udc61 within exploration budget \ud835\udc41 , i.e. , \ud835\udc56 = 1 \u223c \ud835\udc41 . \u00b7 Expansion: unless the child action nodes end the bidding process, create one further child state node \ud835\udc60 \ud835\udc56 \ud835\udc61 + 1 with respect to the transition dynamic \ud835\udc60 \ud835\udc56 \ud835\udc61 + 1 \u223c T . \u00b7 Simulation: complete one rollout from node \ud835\udc60 \ud835\udc56 \ud835\udc61 + 1 given the policy \ud835\udf0b ( \ud835\udc4e | \ud835\udc60 ) until the end. \u00b7 Backpropagation: use the result of the rollout to update value information in the nodes \ud835\udc4e \ud835\udc56 \ud835\udc61 given root \ud835\udc60 \ud835\udc61 . After these four parts, we could choose a final action \ud835\udc4e \ud835\udc57 \ud835\udc61 based on some principles such as balance between exploration and exploitation. In this work, we choose the action of maximum preference value to execute given a state \ud835\udc60 \ud835\udc61 with uncertainty consideration modeled as a random action selection operation. However, unlike the GO game where we can simulate all possible moves, this could be impractical in the bidding scenarios as bidding is a typical partially observable MDP (POMDP) task where other advertisers' behaviors are not predictable. Therefore, we make modifications to the typical MCTS process by approximating the expansion and simulation process together via learning an enhanced transformerbased Q-value function, without actual simulation in a simulator (which is also invalid). Now, we will introduce how we implement GAS in the following three parts, with an illustration in Figure 1(a). 3.1.1 Selection . Given a decision transformer policy DT \ud835\udf03 , we generate a base action \ud835\udc4e \ud835\udc56 \ud835\udc61 first, and then we perpetuate it by multiplying a random factor uniformly between 90% and 110% to get \ud835\udc41 -1 random actions { \ud835\udc4e \ud835\udc56 \ud835\udc61 } \ud835\udc56 = 1: \ud835\udc41 -1 , expressed as We also preserve the initial base action \ud835\udc4e \ud835\udc61 to get the final \ud835\udc41 action proposals { \ud835\udc4e \ud835\udc56 \ud835\udc61 } \ud835\udc56 = 1: \ud835\udc41 = { \ud835\udc4e \ud835\udc56 \ud835\udc61 } \ud835\udc56 = 1: \ud835\udc41 -1 \u2295 \ud835\udc4e \ud835\udc61 . Then, the selection process is choosing an action from these action proposals. 3.1.2 Expansion and Simulation . As we cannot rollout in a simulator or real environment, we need to approximate this rollout process after taking an action proposal \ud835\udc4e \ud835\udc56 \ud835\udc61 given \ud835\udc60 \ud835\udc61 . As we actually need only the result of the rollout, we could directly estimate the value of \ud835\udc4e \ud835\udc56 \ud835\udc61 given \ud835\udc60 \ud835\udc61 using a Q-value function, expressed as We could employ the IQL [23] method to the learning of \ud835\udc44 \ud835\udf19 , which introduces an extra value net \ud835\udc49 \ud835\udf13 ( \ud835\udc60 ) to avoid the overestimation issues due to the distributional shift, expressed as where \ud835\udc3f \ud835\udf0f 2 ( \ud835\udc62 ) = | \ud835\udf0f -\u2736 ( \ud835\udc62 < 0 )| \ud835\udc62 2 is an expectile regression loss. The value net \ud835\udc49 \ud835\udf13 ( \ud835\udc60 ) is used to the Q-value learning: L \ud835\udc44 ( \ud835\udf19 ) = E ( \ud835\udc60\ud835\udc61 ,\ud835\udc4e \ud835\udc61 ,\ud835\udc60 \ud835\udc61 + 1 )\u223cD [ \ud835\udc5f ( \ud835\udc60 \ud835\udc61 , \ud835\udc4e \ud835\udc61 ) + \ud835\udefe\ud835\udc49 \ud835\udf13 ( \ud835\udc60 \ud835\udc61 + 1 ) -\ud835\udc44 \ud835\udf19 ( \ud835\udc60 \ud835\udc61 , \ud835\udc4e \ud835\udc61 ) ) 2 ] . (8) As indicated in Equation 6, the Q is coupled with the underlying policy \ud835\udf0b in the latter expectation term. However, \ud835\udc44 \ud835\udf19 ( \ud835\udc60 \ud835\udc61 , \ud835\udc4e \ud835\udc61 ) only receives a single state-action pair without policy indication, leading to a value prediction based on the underlying policy \ud835\udf0b \ud835\udefd that collected the dataset actually. This causes a policy gap in the generative auto-bidding, as its policy \ud835\udf0b \ud835\udf16 indicated by various conditions is different from \ud835\udf0b \ud835\udefd , leading to a biased value approximation. Rollout via QT. To address the distributional gap by representing the actual policy \ud835\udf0b \ud835\udf16 , we employ the historical trajectory for Q-value learning with a transformer utilizing its sequential modeling ability, termed QT , i.e. , This could be beneficial with a large-scale pertaining set, which contains trajectories collected by various policies because it could empirically use the historical trajectory to predict the future trajectory, i.e. , the rollout { \ud835\udc60 \u2264 \ud835\udc61 , \ud835\udc4e \u2264 \ud835\udc61 } \u2192 { \ud835\udc60 \ud835\udc61 + 1: \ud835\udc47 , \ud835\udc4e \ud835\udc61 + 1: \ud835\udc47 } . After training, the QT \ud835\udf19 ( \ud835\udc60 \ud835\udc61 , \ud835\udc4e \ud835\udc56 \ud835\udc61 ; \ud835\udc60 < \ud835\udc61 , \ud835\udc4e < \ud835\udc61 ) could return an approximation of the value of the rollout until the end of bidding. 3.1.3 Backpropagation . As the overestimation issue of the Qvalue function is notorious, an inaccurate value estimation for the rollout could bias the backpropagation to provide an inaccurate value of action node \ud835\udc4e \ud835\udc56 \ud835\udc61 given root node \ud835\udc60 \ud835\udc61 , leading to the execution of a bad action. To alleviate the overestimate issue, we provide a Q-voting mechanism for value assessment, which is based on a heuristic insight of the consensus. Value Backpropagation via Q-voting. Given the success of offline RL, where Q-value is employed to learn a policy improving over the behavior policy that collected the dataset, we could intuitively believe different randomly trained Q-value nets could achieve an agreement in giving the true best action higher value with high probability. Besides, due to the overestimation being an occasional error, there would be no agreement in giving a specific action a higher value. Formally, if we independently train \ud835\udc40 Q-value nets { \ud835\udc44 \ud835\udf0b \ud835\udf16 \ud835\udf19 \ud835\udc58 } \ud835\udc58 = 1: \ud835\udc40 with different random seeds and we have \ud835\udc41 action proposals { \ud835\udc4e \ud835\udc56 \ud835\udc61 } \ud835\udc56 = 1: \ud835\udc41 with a ground-truth best action \ud835\udc4e \ud835\udc57 \ud835\udc61 , we model the insight of the consensus by a probability density function \ud835\udc5d ( \ud835\udc4e | \ud835\udc44 ) of an action \ud835\udc4e being the best action given a Q-value net, i.e. , Thus, if we only use a single Q, the final win rate of choosing \ud835\udc4e \ud835\udc57 \ud835\udc61 over not choosing it is defined as With this consensus, we could apply the majority voting method to increase the win rate R \ud835\udc58 . For brevity, we assume \ud835\udc5d ( \ud835\udc4e \ud835\udc56 \ud835\udc61 | \ud835\udc44 \ud835\udf0b \ud835\udf16 \ud835\udf19 \ud835\udc58 ) is the same for all \ud835\udc58 \u2208 { 1 , ..., \ud835\udc40 } . The probability of choosing an action \ud835\udc4e \ud835\udc56 \ud835\udc61 as the final action by majority voting is expressed as Employing the Condorcet's jury theorem [5], we could get To ease reading, let us assume \ud835\udc5d ( \ud835\udc4e 1 \ud835\udc61 | \ud835\udc44 \ud835\udf0b \ud835\udf16 \ud835\udf19 \ud835\udc58 ) = 0 . 4, \ud835\udc5d ( \ud835\udc4e 2 \ud835\udc61 | \ud835\udc44 \ud835\udf0b \ud835\udf16 \ud835\udf19 \ud835\udc58 ) = 0 . 3, and \ud835\udc5d ( \ud835\udc4e 3 \ud835\udc61 | \ud835\udc44 \ud835\udf0b \ud835\udf16 \ud835\udf19 \ud835\udc58 ) = 0 . 3, \u2200 \ud835\udc58 . Then, we can get R 1:3 = 0 . 81 > R \ud835\udc58 = 0 . 67. To avoid the useless majority voting outcomes, i.e. , there are no actions that gain more votes than all others, we provide a soft majority voting mechanism using Q-value, termed Q-voting , implemented by two steps: \u00b7 step 1: for a \ud835\udc44 \ud835\udf0b \ud835\udf16 \ud835\udf19 \ud835\udc58 , its vote for each action based on min-max normalization over all \ud835\udc4e \ud835\udc5b \ud835\udc61 is \u00b7 step 2: the final total votes gained of an action \ud835\udc4e \ud835\udc56 \ud835\udc61 is After the above MCTS-inspired search process, we can now get a refined \ud835\udc4e \ud835\udc5b \ud835\udc61 that should have higher preference values than \ud835\udc4e \ud835\udc61 . Sum votes (vertically) sampling voting Bidding Foundation Model \ud835\udc82 \ud835\udc95\"\ud835\udfcf \ud835\udc79 \ud835\udc95 \ud835\udc7a \ud835\udc95 \u22ef \u22ef [emb., pos.enc.] Linear decoder \ud835\udc82 \ud835\udc95 (1) Selection \ud835\udc82 \ud835\udc95 \ud835\udc75 \ud835\udc82 \ud835\udc95 \ud835\udc75\"\ud835\udfcf \ud835\udc82 \ud835\udc95 \ud835\udc8f \ud835\udc82 \ud835\udc95 \ud835\udfcf \u22ef \u22ef \ud835\udc7a \ud835\udc95#\ud835\udfcf \ud835\udc8f \ud835\udc7a \ud835\udc7b \ud835\udc8f \u22ef \u22ef \u22ef \ud835\udc82 \ud835\udc95 \ud835\udc8b \ud835\udc5d(\ud835\udf16) \ud835\udf16 \u22ef (2) Expansion & Simulation \ud835\udc44 $ %,' \ud835\udc44 $ %,( \ud835\udc44 $ %,) \u22ef \u22ef \ud835\udc44 $ *,' \ud835\udc44 $ *,( \ud835\udc44 $ *,) \u22ef \u22ef \ud835\udc44 $ ',' \ud835\udc44 $ ',( \ud835\udc44 $ ',) Critic 1 Critic 2 Critic 3 \ud835\udc63 $ ( \ud835\udc63 $ ' \ud835\udc63 $ ) \u22ef \u22ef (3) Backprop. Refined Action \ud835\udc57 = \ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc5a\ud835\udc4e\ud835\udc65 & QT \u22ef GAS : G enerative A uto-bidding with post-training S earch Transformer \ud835\udc94 \ud835\udc95 \ud835\udc82 \ud835\udc95 \ud835\udc7a \ud835\udc95&\ud835\udfcf \u22ef [emb., pos.enc.] Linear decoder \u22ef \ud835\udc82 \ud835\udc95#\ud835\udfcf V net V net Q net Q net \u22ef \ud835\udc44 $ \ud835\udc49 $+' \ud835\udc44 $+' \u2112 \" \u2112 / \ud835\udc5f $ Preference function Value Budget KPI constraints QT : Transformer-based Q-value net \u22ef \u22ef \u22ef \u22ef \u22ef \u22ef \ud835\udc44 $,,, ',' \ud835\udc44 $,,, ',( \ud835\udc44 $,,, ',) \ud835\udc44 $,,, *,' \ud835\udc44 $,,, *,( \ud835\udc44 $,,, *,) \ud835\udc44 $,,, %,' \ud835\udc44 $,,, %,( \ud835\udc44 $,,, %,) Min-max (mm) normalization (horizontally) \ud835\udc63 $ ' \ud835\udc63 $ * \u22ef \ud835\udc63 $ ) \ud835\udc63 ' ( argmax Q-voting \ud835\udc82 \ud835\udc95 \ud835\udc79 \ud835\udc95 \ud835\udc7a \ud835\udc95 State Action Return Ads Dataset QT 1 QT M GAS Model Conversions? QT 1 QT M GAS Bidders Backbone Other Bidders GSP Auction System Deploy Ad Infor. User Requests GAS Model Training Online System Bids Figure 1: a) The pipeline of GAS for refining a base action \ud835\udc4e \ud835\udc61 to an action \ud835\udc4e \ud835\udc57 \ud835\udc61 better aligned to the preferences represented by the critics, involving three stages: (1) selection by a sampled noisy perturbation, (2) expansion and simulation approximated by the transformer-based Q-value net (QT), and (3) backpropagation through a Q-voting mechanism for better value assessment. b) The online auto-bidding system of Kuaishou including the interaction between the bidding environment and our GAS method.", "3.2 Bidding with GAS": "", "Algorithm 1 Inference with Search ( GASinfer )": "There could be two ways to apply the GAS: i ) searching in the testing time, and ii ) using the search to fine-tune the base policy model. Since both of these approaches need to train a set of critics, we introduce how we train critics representing different preferences. Preference Representation. The preference is expressed by the setting of the reward function. We introduce three examples: \u00b7 Preference that maximizes the winning value under only budget constraint without considering the KPI constraints: \ud835\udc5f \ud835\udc61 = \ud835\udc5c \ud835\udc61 \ud835\udc63 \ud835\udc61 , which is actually the Max Return Bidding; \u00b7 Preference that maximizes a comprehensive performance considering both the winning value and the KPI constraints: \ud835\udc5f \ud835\udc61 = \ud835\udc5c \ud835\udc61 \ud835\udc63 \ud835\udc61 \u00b7 1 \ud835\udc3d \u02dd \ud835\udc57 \ud835\udc5a\ud835\udc56\ud835\udc5b {( \ud835\udc36 \ud835\udc57 \ud835\udc50 \ud835\udc61 \ud835\udc57 \ud835\udc5c \ud835\udc61 / \ud835\udc5d \ud835\udc61 \ud835\udc57 \ud835\udc5c \ud835\udc61 ) \ud835\udefd , 1 } , \ud835\udefd > 1; \u00b7 Preference that prefers more on the KPI constraints by introducing a larger and controllable reweight factor \ud835\udc64 : \ud835\udc5f \ud835\udc61 = \ud835\udc5c \ud835\udc61 \ud835\udc63 \ud835\udc61 + \ud835\udc64 \ud835\udc3d \u02dd \ud835\udc57 \ud835\udc5a\ud835\udc56\ud835\udc5b {( \ud835\udc36 \ud835\udc57 \ud835\udc50 \ud835\udc61 \ud835\udc57 \ud835\udc5c \ud835\udc61 / \ud835\udc5d \ud835\udc61 \ud835\udc57 \ud835\udc5c \ud835\udc61 ) \ud835\udefd , 1 } , \ud835\udefd > 1. Then, we could use these reward functions to train the critics based on Equation 8. 3.2.1 Inference with Search . By repeating the search process introduced in Section 3.1 at every time step during inference, we can get a testing-time version of our method, termed GAS-infer , which could be directly deployed with a base policy model and multiple critics. For a more clear understanding, we provide a detailed procedure in Algorithm 1. Input: Base policy model \ud835\udf0b \ud835\udf03 , critics { \ud835\udc44 \ud835\udf0b \ud835\udf16 \ud835\udf19 \ud835\udc58 } \ud835\udc40 \ud835\udc58 = 1 , initial state \ud835\udc60 0 Output: Action \ud835\udc4e \u2217 \ud835\udc61 at every time step \ud835\udc61 of the bidding period 1: Initialize \ud835\udc60 \ud835\udc61 \u2190 \ud835\udc60 0 2: for each time step \ud835\udc61 do 3: Generate base action \ud835\udc4e \ud835\udc61 \u223c \ud835\udf0b \ud835\udf03 ( \ud835\udc4e | \ud835\udc60 \u2264 \ud835\udc61 , \ud835\udc4e < \ud835\udc61 , \ud835\udc45 \u2264 \ud835\udc61 ) 4: Generate \ud835\udc41 - 1 random actions { \ud835\udc4e \ud835\udc56 \ud835\udc61 } \ud835\udc41 - 1 \ud835\udc56 = 1 by perturbing \ud835\udc4e \ud835\udc61 with a random factor \ud835\udf16 \u223c Uniform ( 90% , 110% ) 5: Form action proposals { \ud835\udc4e \ud835\udc56 \ud835\udc61 } \ud835\udc41 \ud835\udc56 = 1 = { \ud835\udc4e \ud835\udc56 \ud835\udc61 } \ud835\udc41 - 1 \ud835\udc56 = 1 \u222a { \ud835\udc4e \ud835\udc61 } 6: for each action proposal \ud835\udc4e \ud835\udc56 \ud835\udc61 do 7: Estimate value \ud835\udc63 ( \ud835\udc4e \ud835\udc56 \ud835\udc61 |{ \ud835\udc44 \ud835\udf0b \ud835\udf16 \ud835\udf19 \ud835\udc58 } \ud835\udc40 \ud835\udc58 = 1 ) by Q-voting in Eq. 14 8: end for 9: Select action \ud835\udc4e \u2217 \ud835\udc61 = arg max \ud835\udc4e \ud835\udc56 \ud835\udc61 \ud835\udc63 ( \ud835\udc4e \ud835\udc56 \ud835\udc61 |{ \ud835\udc44 \ud835\udf0b \ud835\udf16 \ud835\udf19 \ud835\udc58 } \ud835\udc40 \ud835\udc58 = 1 ) 10: Execute action \ud835\udc4e \u2217 \ud835\udc61 and observe new state \ud835\udc60 \ud835\udc61 + 1 11: Update \ud835\udc60 \ud835\udc61 \u2190 \ud835\udc60 \ud835\udc61 + 1 12: end for 13: return 3.2.2 Finetune with Search . As the search method has the potential to find better action surrounding a base action, especially when the base action is poor quality or misaligned with the preference, we could use search to enhance the training data and fine-tune the base policy model. Firstly, given a data point in the dataset { \ud835\udc60 \ud835\udc61 , \ud835\udc4e \ud835\udc54\ud835\udc61 \ud835\udc61 } , we could do a search for \ud835\udc4e \ud835\udc54\ud835\udc61 \ud835\udc61 and we could get a better action \ud835\udc4e \ud835\udc5d \ud835\udc61 aligning better to the preference. Then, a simple supervised fine-tune (sft) could implemented based on a loss for training DT \ud835\udf03 : Although there could be more choices of preference alignment methods, e.g. , DPO [40] and RLHF [35], they are typically trajectorylevel based on a query dataset and are reported to be unstable, so we leave them as future works.", "4 Experiment": "", "4.1 Setup": "Datasets. Different from the previous dataset preparation procedure that collects private bidding logs from a non-open-source advertising auto-bidding system, we utilize a new public largescale real-world bidding dataset, termed AuctionNet, released by Alibaba [43]. This dataset, to our knowledge the largest available, includes over 500 million records and offers a sparse version for more challenging cases. More details are in Appendix A. Evaluation Metrics. For simplicity, we adopt three metrics to evaluate the performance: \u00b7 Value : the total received value during the bidding period, calculated as \u02dd \ud835\udc56 \ud835\udc5c \ud835\udc56 \ud835\udc63 \ud835\udc56 ; \u00b7 Exceeding rate of the KPI constraints ( ER ): by introducing a binary indicator I ( \ud835\udc65 \u210e \ud835\udc57 , \ud835\udc36 \ud835\udc57 ) whether the final KPI performance \ud835\udc65 \ud835\udc5b \ud835\udc57 = \u03a3 \ud835\udc56 \ud835\udc50 \ud835\udc56 \ud835\udc57 \ud835\udc5c \ud835\udc56 / \u03a3 \ud835\udc56 \ud835\udc5d \ud835\udc56 \ud835\udc57 \ud835\udc5c \ud835\udc56 during a period \u210e exceeds the given constraint \ud835\udc36 \ud835\udc57 , and assuming we have \ud835\udc3b periods in total, we have a exceeding rate of the KPI constraints during \ud835\udc3b periods, defined by \u00b7 Score : by introducing a penalty term the score is a balance between the value and the KPI constraint, i.e. , Baselines. We compare our method to a wide range of baselines involving both RL and generative methods. For RL, we compare with online RL approach USCB [19], offline approaches BCQ [14], CQL [24], and IQL [23]. For generative methods, we compare with DiffBid [18], a generative method based on diffusers, and Decision Transformers DT [7] and its variants CDT [27], a DT-based method that considers a vector of multiple constraints, DT -score: a DTbased method that utilizes the reward function considering both the winning value and the KPI constraints. Implementation Details. The hyperparameters of baselines are set based on the default values referenced in original papers, with further tuning performed to optimize performance. For GAS, it has two components: a base policy model and multiple QT networks. The base policy model could be any model and we choose the DT -score with hyperparameters referenced from the official code provided from [48], which a smaller learning rate at 1 \ud835\udc52 -5 for finetune specifically. For QT network, we employ 6 attention layers with 8 heads for each and set the hidden size as 512 for all layers with 14M parameters in total, which is lightweight. The total training steps are 400k. We adopt the AdamW optimizer [29] with a learning rate of 1 \ud835\udc52 -4 , and set the batch size as 128. During training, we utilize the PyTorch framework on two NVIDIA H100 GPUs. For more detailed hyperparameters of QT networks, please refer to Table 6 from Appendix A.2 .", "4.2 Performance Comparison with Baselines": "In this experiment, we conducted a performance comparison of various baseline methods under different settings, including varying datasets and budget constraints in the MCB bidding. The dataset used for this evaluation is AuctionNet and its sparse version, and the budgets are set at 50%, 75%, 100%, 125%, and 150% of the maximum allowable budget. The results are indicated by the score as a comprehensive assessment of the performance. Our method is based on the baseline policy model DTscore . The results are presented in Table 1, which indicates that both GASinfer and GASsft consistently outperform other methods across all budget settings, achieving the highest scores for all the respective budgets. Other methods such as DT, CDT, and DTscore also perform well demonstrating the power of generative models compared to the traditional RL methods like IQL. We found DiffBid does not perform well in this large-scale task, where a possible reason could be the introduced extra challenges in predicting a whole long trajectory and learning an inverse dynamic model, though it could theoretically alleviate the mismatch between return to go and true action value. Regarding the stability, as shown in Fig. 2(d), GAS also outperforms the base policy model to be more stable. GAS-sft does not perform as well as GASinfer , which could be attributed to encoding additional preference information parameterized in the extra introduced critics and performing clearly during testing. In contrast, the GASsft backbone fuses the actor and critic within the original model, which may cause ambiguity and weak constraints on its critic ability, leading to worse performance. Note that the Qvoting procedure for value assessment can be executed in parallel, with each Q-voting step taking around 0.1 seconds. This duration is considerably small compared to the interval from \ud835\udc61 to \ud835\udc61 + 1 at 30 minutes. This suggests that the GASinfer method is particularly effective in large-scale auto-bidding tasks.", "4.3 Preference Alignment Performance": "To testify whether search with critics of different preferences could improve the performance of a base policy model to specific preference, we conduct preference alignment experiments in Table 2. In this experiment, we evaluated the alignment results of different preference paradigms using two search methods, GASinfer and GASsft , compared to the baseline DTscore method. T The preferences considered include Score-first, Value-first, and ER-first. Table 2 presents the results, showing that GASinfer and GASsft consistently provide better alignment performance across different preferences compared to the base policy model, supporting the effectiveness of preference alignment by the search methods. base 1 3 5 7 10 Number of searched actions 300 310 320 330 340 350 360 370 380 Score Base Policy Score base 1 3 5 7 10 Number of QT 310 320 330 340 350 360 370 380 Score Base Policy Score base 10% 30% 50% min-max Range of search 280 300 320 340 360 Score Base Policy Score (c) Range of Search DT-score GAS-sft GAS-infer Methods 310 320 330 340 350 360 370 Score (d) Stability accurate evaluations. An extreme case could be to search the whole action space, while it could be low-efficiency. Considering the computational efficiency, we randomly search 5 actions within a search range and the ablation study results for different search ranges' results are shown in Fig. 2(c). The experimental results suggest that for the AuctionNet dataset, a 10% search range is better, providing the highest performance. Increasing the search range beyond 10% leads to diminishing returns, and the min-max approach is the least effective due to the limited action budget. More importantly, as the optimal performance is achieved with a small search range of around 10%, it indicates that a small refinement based on the base policy model could be good enough, leaving more space for a more advanced value assessment method.", "4.4 Ablation Study": "Range of Search. The search range directly impacts the balance between exploration (trying out new actions to discover their potential) and exploitation (focusing on actions known to yield good results). A smaller search range means the algorithm will explore fewer actions, potentially missing out on some good actions but focusing more on refining the evaluation of the explored actions. A larger search range means the algorithm will explore more actions, increasing the chances of finding the best possible actions but at the cost of spending less time on each action, which might lead to less Number of Critics. As stated in the Q-voting mechanism, more critics would increase the accuracy of the value assessment empirically, so it would be practical to see how many critics are enough. Therefore, we conduct an ablation study on the number of critics. The search range is fixed as \u00b1 10% and five random actions are selected for value assessment. As shown in Fig. 2(b), the experimental results suggest that increasing the number of QT from one to more improves performance significantly, up to an optimal point of seven critics. Beyond this point, further increases from three critics do not yield significant improvements, suggesting that three critics are enough for the current auto-bidding task. Searching Budget. Sampling more actions from a search range could increase the possibility of finding the optimal action, we conduct an ablation study to investigate it. Note that, in this investigation, we do not use the original base action. As shown in Fig. 2(a), we find that the performance can be efficiently enhanced by increasing the number of actions from one to five. However, beyond five actions, the performance remains unchanged. This suggests that with a higher search budget, we are more likely to find actions that are optimal or near-optimal. Effectiveness of QT. As the rollout process is approximated by the Q-value function, the accuracy of the Q-value function in predicting the expected return with the underlying policy becomes essential. Without historical trajectory information, predicting the rollout process given a state \ud835\udc60 \ud835\udc61 and action \ud835\udc4e \ud835\udc61 is unclear and highly random. This insight is supported by our experimental results in Table 3, where the performance of QT with historical trajectory as policy representation achieves much higher performance than the Q-value function based on a vanilla state-action pair. We also compare QT to various methods without a Q-value net to testify to the effectiveness of MCTS search, including greedy search, and random/mean selection. Greedy search selects the top action with the highest predicted immediate reward, random selection randomly selects an action, and mean selection takes the average value of 5 random actions. As shown in Table 3, our method significantly outperforms other methods.", "5 Live Experiment": "To verify the effectiveness of our proposed GAS, we have deployed it on Kuaishou's advertising system, see Figure 1(b). The deployment scenario is the Multiple Constraint Bidding(MCB). Under the MCB setting, the advertisers set the budget with or without the CPA/ROI constraint, and the bidding strategy aims to get as many conversions as possible under these constraints. Due to the limited online testing resources and the potential impact on the advertisers' value, we only compared GASinfer with the baseline model, DT, which is currently in production. The experimental setup is as follows: \u00b7 State : Budget, cost, time-based budget allocation, time-based cost speed, predicted conversion rates, real CPA or ROI states, etc. \u00b7 Action : Adjustment to the last time bidding coefficient, \ud835\udf06 \ud835\udc61 = \ud835\udf06 \ud835\udc61 -1 + \ud835\udc4e \ud835\udc61 , \ud835\udf06 \ud835\udc61 is the bidding coefficients in Eq. 2. \u00b7 Post-training Search : The critic is trained with the sum value of the winning impressions, and the search is conducted under the value-first setting. The action range for the search is still sampled within \u00b110% of the base action, with 5 points being searched. Our online A/B testing is conducted for six full days. For each MCBcampaigns, 25% budget and traffic are allocated to the baseline bidding model and GAS. The results are shown in Table 4. The experiment results show that GAS improved impressions by +1.65%, cost by +0.94%, target cost by 4.60%, and overall ROI by +3.62%. All metrics showed significant improvements.", "6 Related Work": "Auto-bidding and Offline Reinforcement Learning. Initially, classic control methods like PID [8] were used to optimize budget spending using predefined curves, while OnlineLP [49] adjusted bids based on traffic predictions. With increasing complexity of the online bidding environments, RL algorithms such as USCB [19], SORL [31], and MAAB [46] became crucial for decision-making. Especially offline RL methods, which learn policies from existing datasets without online interaction, have shown significant success. Notable methods include BCQ [14], which narrows the action space to guide the agent towards on-policy behavior; CQL [24], which regularizes Q-values for conservative estimates; and IQL [23], which enables multi-step dynamic programming updates without querying Q values of out-of-sample actions during training. However, these methods are constrained by the MDP assumption, whereas generative models show greater potential for auto-bidding. Generative Models. Generative models learn the data distribution of a given dataset or establish conditional probability distributions between variables. Deep generative models like VAEs [10], Flows [38], and GANs [16] have ushered in a new era of mastering complex data distributions by representing high-dimensional data in a controllable Gaussian form. Recently, the decision transformer (DT) [7, 15, 28] has been employed to model complex decision-making distribution as an auto-regressive model. Diffusion models [20] generate samples based on a condition by iterative denoising via a reverse process. For auto-bidding, DiffBid [18] employs a conditional diffusion model to generate a long trajectory based on a condition like return and then uses an inverse dynamic model to learn how to map the current state to the predicted next state. Preference Alignment. There are two major ways to finetune a base model to specific preferences, including supervised fine-tuning (SFT) and preference optimization from human feedback. For SFT [36], it constructs high-quality trajectories on specific domains to align the base model to these preferences. For the latter, it needs to learn a preference model. Reinforcement learning from human feedback (RLHF) [35], as a pioneering approach, uses a preference model and proximal policy optimization (PPO) [42] to further refine the model. Direct preference optimization (DPO) [40] and its variants [3, 11, 30, 39, 47] then proposes to directly fine-tune a model using preference query data, bypassing the reward modeling phase and the RL optimization process. Recently, some post-training methods have been proposed to align the preference even without finetuning by using reward models only [21, 25].", "7 Conclusion": "This paper presents a flexible and practical framework for generative auto-bidding, termed GAS, which leverages post-training search methods to refine generative auto-bidding policy models with various advertiser preferences. The proposed method utilizes transformer-based critics and a voting mechanism to enhance value approximation accuracy. This approach could open new avenues for the application of foundation models in auto-bidding by addressing the challenge of adapting a single model to diverse preferences without the need for multiple costly training processes. However, there are several limitations in this study. Firstly, the simplifications made in the MCTS process, such as approximating the expansion and simulation steps, may not fully capture the full complexities of real-world bidding scenarios. Secondly, the finetune version of GAS is more computationally efficient than the inference version, but its performance is still limited, which needs more advanced and effective fine-tune methods.", "Acknowledgments": "This research is supported by the National Research Foundation, Singapore, under its Competitive Research Programme (Grant No. NRF-CRP23-2019-0006). Additionally, Shuai Mao and Yunjian Xu are supported in part by the General Research Fund (GRF) project 14200720 of the Hong Kong University Grants Committee, and the National Natural Science Foundation of China (NSFC) Project 62073273.", "References": "[1] Anurag Ajay, Yilun Du, Abhi Gupta, Joshua B. Tenenbaum, Tommi S. Jaakkola, and Pulkit Agrawal. 2023. Is Conditional Generative Modeling all you need for Decision Making?. In ICLR . [2] Alibaba. 2021. Alimama Super Diamond. https://zuanshi.taobao.com/. [3] Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, R\u00e9mi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. 2024. A General Theoretical Paradigm to Understand Learning from Human Preferences. In International Conference on Artificial Intelligence and Statistics, . [4] Santiago R. Balseiro, Yuan Deng, Jieming Mao, Vahab S. Mirrokni, and Song Zuo. 2021. Robust Auction Design in the Auto-bidding World. In NeurIPS . [5] Philip J Boland. 1989. Majority systems and the Condorcet jury theorem. Journal of the Royal Statistical Society Series D: The Statistician 38, 3 (1989), 181-189. [6] Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, Ilya Sutskever, and Jeffrey Wu. 2024. Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision. In ICML . [7] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. 2021. Decision transformer: reinforcement learning via sequence modeling. In NeurIPS . [8] Ye Chen, Pavel Berkhin, Bo Anderson, and Nikhil R Devanur. 2011. Real-time bidding algorithms for performance-based display ad allocation. In KDD . [9] Yuan Deng, Jieming Mao, Vahab S. Mirrokni, and Song Zuo. 2021. Towards Efficient Auctions in an Auto-bidding World. In WWW . [10] Carl Doersch. 2016. Tutorial on variational autoencoders. arXiv preprint arXiv:1606.05908 (2016). [11] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. 2024. Model Alignment as Prospect Theoretic Optimization. In ICML . [12] David S Evans. 2009. The online advertising industry: Economics, evolution, and privacy. Journal of economic perspectives 23, 3 (2009), 37-60. [13] Facebook. 2021. Advertising on Facebook. https://www.facebook.com/business/ ads. [14] Scott Fujimoto, David Meger, and Doina Precup. 2019. Off-policy deep reinforcement learning without exploration. In ICML . [15] Chongming Gao, Kexin Huang, Ziang Fei, Jiaju Chen, Jiawei Chen, Jianshan Sun, Shuchang Liu, Qingpeng Cai, and Peng Jiang. 2025. Future-Conditioned Recommendations with Multi-Objective Controllable Decision Transformer. arXiv preprint arXiv:2501.07212 (2025). [16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020. Generative adversarial networks. Commun. ACM 63, 11 (2020), 139-144. [17] Google. 2021. Google Ads. https://ads.google.com/. [18] Jiayan Guo, Yusen Huo, Zhilin Zhang, Tianyu Wang, Chuan Yu, Jian Xu, Yan Zhang, and Bo Zheng. 2024. AIGB: Generative Auto-bidding via Diffusion Modeling. In KDD . [19] Yue He, Xiujun Chen, Di Wu, Junwei Pan, Qing Tan, Chuan Yu, Jian Xu, and Xiaoqiang Zhu. 2021. A Unified Solution to Constrained Bidding in Online Display Advertising. In KDD . [20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. In NeurIPS 2020 . [21] Maxim Khanov, Jirayu Burapacheep, and Yixuan Li. 2024. ARGS: Alignment as Reward-Guided Search. In ICLR . [22] Levente Kocsis and Csaba Szepesv\u00e1ri. 2006. Bandit Based Monte-Carlo Planning. In ECML . [23] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. 2022. Offline Reinforcement Learning with Implicit Q-Learning. In ICLR . [24] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. 2020. Conservative Q-learning for offline reinforcement learning. In NeurIPS . [25] Bolian Li, Yifan Wang, Ananth Grama, and Ruqi Zhang. 2024. Cascade reward sampling for efficient decoding-time alignment. arXiv preprint arXiv:2406.16306 (2024). [26] Juncheng Li and Pingzhong Tang. 2022. Auto-bidding Equilibrium in ROIConstrained Online Advertising Markets. CoRR abs/2210.06107 (2022). [27] Zuxin Liu, Zijian Guo, Yihang Yao, Zhepeng Cen, Wenhao Yu, Tingnan Zhang, and Ding Zhao. 2023. Constrained decision transformer for offline safe reinforcement learning. In ICML . [28] Ziru Liu, Shuchang Liu, Zijian Zhang, Qingpeng Cai, Xiangyu Zhao, Kesen Zhao, Lantao Hu, Peng Jiang, and Kun Gai. 2024. Sequential recommendation for optimizing both immediate feedback and long-term retention. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1872-1882. [29] Ilya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regularization. In ICLR . [30] Yu Meng, Mengzhou Xia, and Danqi Chen. 2024. SimPO: Simple Preference Optimization with a Reference-Free Reward. CoRR abs/2405.14734 (2024). [31] Zhiyu Mou, Yusen Huo, Rongquan Bai, Mingzhou Xie, Chuan Yu, Jian Xu, and Bo Zheng. 2022. Sustainable online reinforcement learning for auto-bidding. In NeurIPS . [32] Roberto Navigli, Simone Conia, and Bj\u00f6rn Ross. 2023. Biases in Large Language Models: Origins, Inventory, and Discussion. ACM J. Data Inf. Qual. 15, 2 (2023), 10:1-10:21. [33] OpenAI. 2024. ChatGPT. https://chatgpt.com/. [34] Weitong Ou, Bo Chen, Yingxuan Yang, Xinyi Dai, Weiwen Liu, Weinan Zhang, Ruiming Tang, and Yong Yu. 2023. Deep Landscape Forecasting in Multi-Slot Real-Time Bidding. In KDD . [35] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In NeurIPS . [36] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In NeurIPS . [37] Ling Pan, Qingpeng Cai, and Longbo Huang. 2020. Softmax deep double deterministic policy gradients. Advances in neural information processing systems 33 (2020), 11767-11777. [38] Ling Pan, Nikolay Malkin, Dinghuai Zhang, and Yoshua Bengio. 2023. Better Training of GFlowNets with Local Credit and Incomplete Trajectories. In ICML . [39] Ryan Park, Rafael Rafailov, Stefano Ermon, and Chelsea Finn. 2024. Disentangling Length from Quality in Direct Preference Optimization. In ACL . [40] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct Preference Optimization: Your Language Model is Secretly a Reward Model. In NeurIPS . [41] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. 2022. High-Resolution Image Synthesis with Latent Diffusion Models. In CVPR . [42] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal Policy Optimization Algorithms. CoRR abs/1707.06347 (2017). [43] Kefan Su, Yusen Huo, Zhilin Zhang, Shuai Dou, Chuan Yu, Jian Xu, Zongqing Lu, and Bo Zheng. 2024. AuctionNet: A Novel Benchmark for Decision-Making in Large-Scale Games. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track . [44] Richard S Sutton. 2018. Reinforcement learning: An introduction. A Bradford Book (2018). [45] Jun Wang and Shuai Yuan. 2015. Real-Time Bidding: A New Frontier of Computational Advertising Research. In WSDM . [46] Chao Wen, Miao Xu, Zhilin Zhang, Zhenzhe Zheng, Yuhui Wang, Xiangyu Liu, Yu Rong, Dong Xie, Xiaoyang Tan, Chuan Yu, et al. 2022. A cooperative-competitive multi-agent framework for auto-bidding in online advertising. In WSDM . [47] Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, and Young Jin Kim. 2024. Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation. In ICML . [48] Jian Xu, Zhilin Zhang, Zongqing Lu, Xiaotie Deng, Michael P Wellman, Chuan Yu, Shuai Dou, Yusen Huo, Zhiwei Xu, Zhijian Duan, et al. [n. d.]. Auto-Bidding in Large-Scale Auctions: Learning Decision-Making in Uncertain and Competitive Games. In NeurIPS 2024 Competition Track . [49] Hao Yu, Michael J Neely, and Xiaohan Wei. 2017. Online convex optimization with stochastic constraints. In NeurIPS .", "A Appendix": "", "A.1 Datset Details": "Thedatasets consisting of AuctionNet and AuctionNet-sparse, where AuctionNet-sparse is a sparse version of AuctionNet with less conversions . Each dataset consists of 21 advertising delivery periods, with each period containing approximately 500000 impression opportunities, divided into 48 intervals. Detailed parameters are shown in Table 5. Each advertiser will bid for all impressions. Each dataset contains over 500 million records, with each recording information for multiple advertisers across various time steps and multiple periods. In detail, the state includes the following information: \u00b7 time_left: The remaining time steps left in the current advertising period. \u00b7 budget_left: The remaining budget that the advertiser has available to spend in the current advertising period. \u00b7 historical_bid_mean: The average values of bids made by the advertiser over past time steps. \u00b7 last_three_bid_mean: The average values of bids over the last three time steps. \u00b7 historical_LeastWinningCost_mean: The average of the least cost required to win an impression over previous time steps. \u00b7 historical_pValues_mean: The average of historical p-values over past time steps. \u00b7 historical_conversion_mean: The average number of conversions (e.g., sales, clicks, etc.) the advertiser achieved in previous time steps. \u00b7 historical_xi_mean: The average winning status of advertisers in impression opportunities, where 1 represents winning and 0 represents not winning. \u00b7 last_three_LeastWinningCost_mean: The average of the least winning costs over the last three time steps. \u00b7 last_three_pValues_mean: The average of conversion probability of advertising exposure to users over the last three time steps. \u00b7 last_three_conversion_mean: The average number of conversions over the last three time steps. \u00b7 last_three_xi_mean: The average winning status of advertisers over the last three time steps. \u00b7 current_pValues_mean: The mean of p-values at the current time step. \u00b7 current_pv_num: The number of impressions served at the current time step \u00b7 last_three_pv_num_total: The total number of impressions served over the last three time steps. \u00b7 historical_pv_num_total: The total number of impressions served over past time steps. We conduct experiments in a simulated experimental environment resembling real advertising system, provided by Alibaba [48]. During evaluation, an episode, also referred to as an advertising delivery period, is a day divided into 48 intervals, with each interval lasting 30 minutes. Each episode contains approximately 500000 impression opportunities that arrive sequentially. An advertising delivery period involves 48 advertisers from diverse categories, with varying budgets and CPAs. Each advertiser bids for all impression opportunities during each period. In each evaluation, our well-trained model represents a designated advertiser in bidding given specific budget and CPA. In order to comprehensively evaluate the performance of the model under different advertisers, we will use different advertiser configurations and advertising periods to evaluate this model multiple times in the simulated environment, and average the results as the evaluation score.", "A.2 HYPERPARAMETERS SETTING": "We list the hyperparameter details in Table 6 for reproduction."}
