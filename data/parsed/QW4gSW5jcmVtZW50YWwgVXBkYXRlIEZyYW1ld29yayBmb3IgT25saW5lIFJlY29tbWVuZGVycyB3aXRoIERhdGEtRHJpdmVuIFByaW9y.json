{
  "An Incremental Update Framework for Online Recommenders with Data-Driven Prior": "",
  "ABSTRACT": "",
  "ACMReference Format:": "Online recommenders have attained growing interest and created great revenue for businesses. Given numerous users and items, incremental update becomes a mainstream paradigm for learning large-scale models in industrial scenarios, where only newly arrived data within a sliding window is fed into the model, meeting the strict requirements of quick response. However, this strategy would be prone to overfitting to newly arrived data. When there exists a significant drift of data distribution, the long-term information would be discarded, which harms the recommendation performance. Conventional methods address this issue through native model-based continual learning methods, without analyzing the data characteristics for online recommenders. To address the aforementioned issue, we propose an incremental update framework for online recommenders with Data-Driven Prior (DDP), which is composed of Feature Prior (FP) and Model Prior (MP). The FP performs the click estimation for each specific value to enhance the stability of the training process. The MP incorporates previous model output into the current update while strictly following the Bayes rules, resulting in a theoretically provable prior for the robust update. In this way, both the FP and MP are well integrated into the unified framework, which is model-agnostic and can accommodate various advanced interaction models. Extensive experiments on two publicly available datasets as well as an industrial dataset demonstrate the superior performance of the proposed framework.",
  "CCS CONCEPTS": "· Information systems → Recommender systems .",
  "KEYWORDS": "CTR Prediction, Incremental Update, Data-Driven Prior Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CIKM '23, October 21-25, 2023, Birmingham, United Kingdom © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0124-5/23/10...$15.00 https://doi.org/10.1145/3583780.3615456 ChenYang, Jin Chen B , Qian Yu, Xiangdong Wu, Kui Ma, Zihao Zhao, Zhiwei Fang, Wenlong Chen, Chaosheng Fan, Jie He, Changping Peng, Zhangang Lin, and Jingping Shao. 2023. An Incremental Update Framework for Online Recommenders with Data-Driven Prior. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management (CIKM '23), October 21-25, 2023, Birmingham, United Kingdom. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/3583780.3615456",
  "1 INTRODUCTION": "With the rapid development of Web applications, recommender systems have become ubiquitous to solve the serious information overload problem, with the aim of locating potentially preferred items among numerous items for users [5-7, 13]. Many e-commerce companies have generated significant revenue from recommender systems, where a higher click-through rate corresponds to a higher probability of greater benefit. Thus, CTR prediction plays a vital role in nowadays online recommendations. In recent years, in order to better fit in the large-scale target data, expressive models are proposed to capture the complex interactions between multiple features [15, 20], where the models tend to go more complex to fully capture the multiple high-order combinatorial features. However, these models require massive computational resources under the tremendous number of users and items, preventing quick online deployment and updates for industrial online recommendations. A mainstream framework is to feed models with the incremental data, i.e., newly arrived data within the sliding window, to continuously train the latest model rather than training from scratch [2, 23]. This strategy significantly reduces time overhead and adapts to dynamics in online data distribution. However, when the distribution of online data changes significantly, this framework is prone to overfitting recent data. For example, considering the scenarios of large promotions, such as Double 11 shopping carnival and Black Friday, the specific commodities will receive a lot of exposure in a short period, resulting in a different distribution from previously collected feedback. The training framework would suffer from this and be inclined to these recent exposures. As a result, the model pays too much attention to the newly arrived data and gradually ignores the long-term information about user interests, limiting the recommendation performance. Moreover, due to the relatively decreased exposure ratio of long-tail CIKM '23, October 21-25, 2023, Birmingham, United Kingdom Yang Chen et al. items, the model pays less attention to these items, which aggravates the long-tail effect. Existing studies apply continual learning to alleviate this issue for online recommenders. IncCTR [23] adopts distillation logit to guide the latest model while ASMG [19] incorporates the metalearners to fit in recent models. To summarize, these studies tackle the issue by utilizing model-based prior to directly predicting CTR for each instance. However, these studies adopt strategies inspired by conventional continual learning, without analyzing the data characteristic in online recommendations. The extreme data sparsity and feature diversity are two prominent characteristics. Under the numerous numbers of users and items, user clicks are extremely sparse, which makes it difficult to accurately estimate user preference. Recent research [26] suggests that each item requires around 10,000 impressions for convergence. Limited impressions hinder robust estimation during incremental updates. The recent success of CTR prediction can be attributed to the use of complex features, where tail items with similar features to popular items obtain more accurate estimates. Additionally, considering that features are also the most important factor affecting the model effect [17, 24], we are motivated to explicitly incorporate the feature prior to enhancing the performance. To this end, we propose a robust and unified incremental learning framework with data-driven prior(DDP) to improve the performance under the nowadays mainstream training framework. It incorporates the feature prior in an end-to-end manner and provides a more theoretically provable model prior. More concretely, the Feature Prior is intended to explicitly estimate the average CTR of the specific feature value. The distribution of CTR value at the feature granularity presents more stable than the distribution at the instance level, since the data are more aggregated for the features. FP finnaly acts as auxiliary feature information and provides a more stable learning for model updates, thus benefitting the optimization for long-tail items. Furthermore, depending on the Bayes rules, we develop the Model Prior, which approximates the posterior on the complete data by maximizing the likelihood function on the incremental data and lowering the distance to the prior model. Accordingly, the output from previous models can be easily integrated into the framework to achieve the model prior, where the output of the previous model is used to supervise the current model. Our main contribution can be summarized as follows: · We propose a robust and unified incremental update framework with data-driven prior to alleviate the overfitting problem on recent data, which is model-agnostic and can be optimized in an end-to-end manner. · FP is designed to estimate the average CTR value for features, benefitting from a more stable distribution at the feature-level. This module can help CTR model more stable training and more accurate CTR estimation on long-tail items. · MP approximates the posterior on the complete data by optimizing the likelihood function on the incremental data and reducing the distance to the previous model. · Exhaustive experiments are conducted on two public dataset to demonstrate the superiority of the proposed framework. Both offline and online experiments are conducted on an industrial dataset schow the effectiveness in real scenarios.",
  "2 RELATED WORK": "",
  "2.1 CTR Prediction Models": "Click-Through Rate (CTR) prediction plays an important role for online recommenders, such as online advertising, and numerous studies have investigated it in recent decade [8, 10, 18, 20, 21]. These methods mainly vary in the way of capturing the feature interactions. Wide&Deep [8] firstly combines the shallow linear layers and deep neural networks to simultaneously capture feature expression at different levels of features. Motived by Wide&Deep, DeepFM [10] replaces the wide part with the factorization machine models to better capture the second-order interactions between features. DCN [21] explicitly constructs high-level combinatorial relations of features through Cross Network. FwFM [18] effectively captures the heterogeneity of field on second-order interaction features. AutoInt [20] automatically learn the high-order feature interactions of input features with a multi-head self-attention neural network to explicitly model feature interactions in a low-dimensional space. Another mainstream direction is to model the historical behavior of users to better figure out user interests. DIN [29] obtains the user's interest in the target item by modeling the interest distribution of a user's browsing sequence under a given target item. Based on DIN, DIEN [28] investigates how user interests fluctuate over time and improves user interest capture by simulating the evolution of interests over time in the user sequence.",
  "2.2 Incremental Update for Online Recommenders": "Limited by the computational time and resources, the models are trained with streaming data, resulting in the incremental update for online recommendation. This incremental training framework has been widely used for industrial scenarios and attracted growing interest in recent years [2, 9, 16, 19, 23, 27]. Here we briefly introduce a few papers that are most relevant to our work. IncCTR [23] uses the previous model as a teacher and supervises the learning of the current model through the commonlyused distillation loss. Motivated by [23], ReLoop [2] uses the hinge loss to guide the current model to perform better than the previous model by treating the performance of the previous model as the margin value. ASMG [19] draws the idea of meta-learning and generates a better serving model from a sequence of historical models via a meta generator, so as to overcome the problem of overfitting and forgetting.",
  "3 METHODOLOGY": "",
  "3.1 Preliminaries": "Assume each instance (X , 𝑦 ) records a piece of impression, where X denotes features with 𝑀 fields, and 𝑦 ∈ { 0 , 1 } indicates whether the user clicks the item. After one-hot or multi-hot encoding for the features, each instance can be converted into x = [ x 1 , ..., x 𝑀 ] where x 𝑗 denotes the encoding vector for the 𝑗 -th field. The CTRprediction model then learns a mapping ˆ 𝑦 = 𝑓 𝜃 ( x ) to estimate the probability of user click. Previous models can be summarized with three parts: embedding module, interaction module and loss function. The embedding module convert encoding into embedding vector e = [ e 1 , ..., e 𝑀 ] , where e 𝑗 ∈ R 𝑑 is obtained through the An Incremental Update Framework for Online Recommenders with Data-Driven Prior CIKM '23, October 21-25, 2023, Birmingham, United Kingdom Figure 1: The Architecture of DDP. 0 1 0 É É Embedding Layer Feature Prior Layer Discretization & Embedding Layer É Interaction Module Likelihood Loss: ℒ\" Prior Loss: ℒ# 0 1 0 1 0 Feature Prior Model Prior Label Previous Output É É É Concat Feature Prior Loss: ℒ(%, '; Φ) + , -./ SGD Optimizer Adam Optimizer embedding matrix 𝑬 ∈ R 𝑁 × 𝑑 . 𝑁 denotes the number of sparse feature values. The interaction module attempts to capture the complex relationships between features and to predict the probability of the user click through the sigmoid function, i.e., ˆ 𝑦 = 𝑓 𝜃 ( x ) = 𝜎 ( Interaction_Module ( e )) . 𝜃 denotes the set of parameters and 𝑓 (·) summarizes all calculations. Finally, the model is optimized through the binary cross entropy loss L(D ; 𝜃 ) . Considering the industrial scenarios under large scales, the incremental update becomes a popular framework, where the model is updated only with newly arrived data D 𝑡 . D 𝑡 denotes the data collected from the period 𝑡 , whereby a period can be a certain length of time (e.g., an hour, a day). H 𝑡 = D 1 ∪ D 2 , ..., ∪D 𝑡 denotes the historical training data up to the period 𝑡 . Correspondingly, 𝜃 𝑡 -1 denotes model parameters updated in time 𝑡 -1. The incremental update receives the newly arrived data D 𝑡 and previous model 𝜃 𝑡 -1, updating the model parameters, i.e., min L(D 𝑡 , 𝜃 𝑡 -1; 𝜃 ) . It is obvious that the models are prone to overfitting to recent data when there is a drift of online data, and long-term user interest would be discarded under the loop of the incremental update.",
  "3.2 Framework Overview": "We propose an unified framework, namely incremental update framework for online recommenders with Data-Driven Prior (DDP), which consist of two important components, as shown in Figure 1: (1) Feature Prior (FP) estimates the average CTR of the specific value of each feature field, motivated by a more stable distribution at the feature level rather than the frequent dynamics at the instance level. The objective of FP is to help model learn more robust and have more accurate CTR estimates with long-tail data. (2) Model Prior (MP) provides a more robust update depending on the Bayes rules, which acts as a regularizer to minimize the distance between the incremental update and the training with whole data. The purpose of MP is to assist the model in learning stably and avoiding over-fitting on incremental data. Figure 2: KL-Distance w.r.t. statistical CTR of seven consecutive days. The KL-Distance measures the distance between the CTR value within the day and average value. 0 2 4 6 8 10 1 2 3 4 5 6 7 KL Div Day Feature A Feature B Feature C Instance These two data-driven parts can be easily integrated into existing advanced models, resulting in a model-agnostic and general framework. Furthermore, the framework can be updated in an end-to-end manner, showing its feasibility for online recommenders.",
  "3.3 Feature Prior (FP)": "Previous studies adopt the native continual learning methods, by implementing the model-based information to predict the CTR for each instance. A major concern is the extreme data sparsity aggravated by incremental update. Model parameters are considerably sensitive to these data, where sparse data raises the instability of models, resulting in the overfitting to recent data. Intuitively, the feature data among the instances are more frequent, which acts more stable than instances. To further demonstrate it, we summarize the change of CTR values within seven consecutive days at the feature-level and instance-level in Figure 2. Specifically, we select three features and corresponding instances. Feature C is sparser than Feature A and Feature B. The KL-distance is calculated with 𝐾𝐿 = ˝ 1 𝑖 = 0 𝑞 ( 𝑖 ) log 𝑞 ( 𝑖 ) 𝑝 ( 𝑖 ) , where 𝑞 ( 𝑖 ) and 𝑝 ( 𝑖 ) denote the statistical CTR of features (instances) within the day and average value respectively. As shown in Figure 2, the change of CTR with respect to the instances behaves more frequently than that of features, which brings difficulties for the update. Thus, we are motivated to design a module to estimate the CTR values for the features, which is fed into CTR model as stable and useful information, with the aim of improving the performance of recommenders. This feature-level value can generalize to tailed items, allowing CTR models to estimate long-tail characteristics more accurately. To this end, we propose the Feature Prior (FP), which can maintain long-term prior information for each feature, enabling a more stable expression of long-tail characteristics. As aforementioned, each instance is represented with the onehot encoding or multi-hot encoding: x = [ x 1 , x 2 , ..., x 𝑀 ] where 𝑀 denotes feature fields and x 𝑖 denotes the feature encoding of the field 𝑖 . Notice all features are transformed into one-hot vectors here for simplicity. Apart from the dense high-dimensional embedding matrix, we design a feature prior layer to estimate the average CTR of each feature value as follows:  where C ∈ R 𝑁 × 1 denotes the feature prior embedding vector of all sparse features, 𝑁 denotes the number of sparse feature values and 𝜎 is the sigmoid function. A binary cross entropy loss is applied to CIKM '23, October 21-25, 2023, Birmingham, United Kingdom Yang Chen et al. update the embedding vectors by maximum likelihood estimation:  where 𝑦 denotes the label of the instance, D 𝑡 denotes incremental data and Φ denotes the parameters of the feature prior layer. Based on Eq (1), Feature Prior Loss, we can estimate the CTR values for each feature, which further aids the robust training. Thus, we obtain a continuous estimated CTR value for each specific value. In order to integrate with the general embedding layers, i.e, the original feature values, the discretization strategy is adopted here, i.e., s 𝑖 = One_Hot (⌊ 𝐵 · √ ˆ 𝑠 𝑖 ⌋) , where 𝐵 denotes the number of bins with equal width for discretization. Since the statistics CTR value is closer to 0 in real scenarios, the square root function is used to lead the model to focus more on smaller values. After discretizing the estimated values, another embedding matrix 𝑼 𝑖 ∈ R 𝐵 × 𝑑 is used to get the encoding vector for the 𝑖 -th feature field. Specifically, 𝒐 𝑖 = 𝑼 ⊤ 𝑖 s 𝑖 . Thus, the low-dimensional dense embedding for the feature prior follows as:  By concatenating vectors e and o , we obtain the final embedding representation of features and their prior:  where Concat (·) denotes concat function. In this way, the feature prior is well integrated with the original embedding module, followed by any interaction module to capture the interactions between different features.",
  "3.4 Model Prior (MP)": "Empirically, the main reason for the model parameters overfitting to the incremental data distribution is that the model parameter optimization only considers the likelihood of the model parameter distribution on the incremental data, while neglecting the influence of previous data. Therefore, we conduct an in-depth analysis of the model optimization process, and finally simplify it to make our parameters fully learn the complete data distribution H 𝑡 = {H 𝑡 -1 , D 𝑡 } by fitting the likelihood on D 𝑡 and constraining the distance between the output of current model and the output of previous model 𝜃 𝑡 -1, which is optimized by posterior with H 𝑡 -1. From a probabilistic perspective, our optimization goal is to optimize posterior parameter distribution 𝜃 𝑡 given the complete data H 𝑡 :  According to Eq (2), the objective is to maximize the likelihood log 𝑝 (D 𝑡 | 𝜃 ) and the posterior estimation log 𝑝 ( 𝜃 |H 𝑡 -1 ) given the",
  "Algorithm 1: DDP": "Input: Incremental Data D 𝑡 = { x , 𝑦 } , The latest model parameters 𝜃 𝑡 - 1 Output: 𝜃 = { Φ , Θ } // Φ is the parameter of feature prior layer and Θ is the other parameters 1 𝜃 = 𝜃 𝑡 -1; 2 for { x , 𝑦 } in D 𝑡 do 3 Get s = [ s 1 , s 2 , ..., s 𝑀 ] according to the Feature Prior Layer; 4 Obtain L( x , 𝑦 ; Φ ) through Eq (1); 5 Obtain L( x , s , 𝑦, 𝜃 𝑡 -1; Θ ) through Eq (5); 6 Optimize Φ using L( x , 𝑦 ; Φ ) with SGD; 7 Optimize Θ using L( x , s , 𝑦, 𝜃 𝑡 -1; Θ ) with Adam; historical training data. Next we introduce the solutions respectively. Regarding the likelihood 𝑝 (D 𝑡 | 𝜃 ) , by assuming that the user click follows a binomial distribution with the probability of 𝑓 𝜃 ( x ) , the objective function deduces to the binary cross-entropy loss:  where 𝑦 ∈ { 0 , 1 } denotes the click label and 𝑓 summarizes the model output. 𝑓 𝜃 denotes the output parameterized by 𝜃 . This objective function guides the model to fully fit the incremental data D 𝑡 . Regarding the posterior 𝑝 ( 𝜃 |H 𝑡 -1 ) , it is intractable to directly obtain the solution. EWC [12] adopts the Laplace approximation to regularize the distance between parameter space 𝜃 and the optimal parameter space 𝜃 𝑡 -1 obtained given the historical training data H 𝑡 -1, which acts as a surrogate loss to optimize log 𝑝 ( 𝜃 |H 𝑡 -1 ) . MRNFS [1] further simplifies the parameter space into the output of the models, with the aim of more stable learning to alleviate the strong constraints at the parameter level, achieving better performances. Thus, we follow these studies and get the following objective function:  where 𝑓 𝜃 stands for current learning model and 𝑓 𝜃 𝑡 -1 denotes the previous well-learned model given H 𝑡 -1. Finally, the objective function follows:  where 𝜆 is the coefficient to regulate the relationship between the two losses, s = [ s 1 , s 2 , ..., s 𝑀 ] denotes the output of feature prior layer and Θ denotes the other parameters except feature prior layer. By integrating with Eq (1) of the Feature Prior (FP), we can update the whole framework in an end-to-end manner, without changing the structure of the important interaction modules. The whole process is detailed in Alg 1. Note that if we optimize Eq (1) with the Adam optimizer [11], the feature prior layer will overfit within a short period due to the gradient vanish [25], which can not capture the change of data distribution continuously. Thus, we use the SGD optimizer to update the feature prior layer while using Adam optimizer for other parts. An Incremental Update Framework for Online Recommenders with Data-Driven Prior CIKM '23, October 21-25, 2023, Birmingham, United Kingdom Table 1: Statistics of datasets. M represents a million. Table 2: Overall performance with Baselines on the Criteo and CIKM2019 datasets. FP represents the feature prior. Notice DeepFM is chosen as the interaction module in DDP. Boldface denotes the highest score and underline indicates the best result of the baselines. ★ represents significance level 𝑝 -value < 0.05 of comparing with the best baselines.",
  "4 EXPERIMENTS ON PUBLIC DATASETS": "",
  "4.1 Experimental Settings": "4.1.1 Datasets. Two datasets, Criteo and CIKM2019, are adopted here for comparisons, whose statistics are summarized in Tabel 1. · Criteo 1 , which is widely used for CTR prediction, is a collection of user traffic logs from Criteo for seven consecutive days. · CIKM2019 2 is a public dataset from an e-commerce company. We follow the previous work [30] to transform the original label into binary values, where 1/0 indicates whether a user has bought an item or not. It consists of behavior logs for 16 consecutive days. To simulate the online settings of online industrial scenarios, the public data are fed into models in a streaming way. More concretely, within the logs of 𝑇 days, the proceeding data of 𝑤 days is used to warm up the model where the model is incremental updated for the next consecutive days with the time period as a day. Finally, the performance is tested on the last day after 𝑇 -𝑤 -1 trails. For Criteo, 𝑇 = 7 and 𝑤 = 3. For CIKM2019 dataset, 𝑇 = 16 and 𝑤 = 8. 4.1.2 Baselines. We choose the following competing methods as baselines for comparison: IncCTR [23], RLP [2], ASMG [19] and MECP [14]. Moreover, since the proposed DDP is a model-agnostic framework, we apply DDP to six different deep CTR models: DNN , DCN [21], W&D [8], DeepFM [10], FwFM [18] and AutoInt [20] to verify the compatibility of DDP with various interaction module. 4.1.3 Metric and Implementation Details. We use the commonly used metrics in CTR prediction, AUC and LogLoss , for evaluation. 1 https://www.kaggle.com/c/criteo-display-ad-challenge 2 https://tianchi.aliyun.com/competition/entrance/231721/introduction?lang=en-us Figure 3: Performances of Consecutive Days under Incremental Update on CIKM2019 Dataset 0.735 0.743 0.751 0.759 10 11 12 13 14 15 16 AUC Day DDP IncCTR RLP DeepFM (a) AUC 0.122 0.126 0.13 0.134 10 11 12 13 14 15 16 LogLoss Day DDP IncCTR RLP DeepFM (b) LogLoss All the experiments are repeated 5 times by changing the random seeds. The models are trained within mini batch, where the learning rate is tuned over { 1 𝑒 -6 , 5 𝑒 -5 , ..., 1 𝑒 -2 } . The weight of 𝐿 2 regularization is tuned over { 1 𝑒 -6 , 5 𝑒 -5 , ..., 1 𝑒 -3 } . The embedding size is set to 16 and the hidden layers for the deep networks is fixed to [ 200 , 200 , 200 ] . All of these models select the parameters with best performances in terms of AUC.",
  "4.2 Overall Performance": "We compare the performance of different deep CTR models for incremental update and various embedding-based deep CTR models. FP represents that only Feature Prior is included under the DeepFM interaction module. The results of our proposed approaches and baselines on two public datasets are shown in Table 2, from which we have the following observations: · Our methods: FP and DDP consistently outperform other different deep CTR methods for incremental update and various embedding-based deep CTR models. Compared with the best basline ASMG, our method DDP achieves a relative improvement of 0.22% and 0.12% with respect to AUC on Criteo and CIKM2019 respectively. Notice, even a slight 0.1% improvement on the CTR prediction task is significant [10, 21]. · Introducing the feature prior raises more improvements on the long-tail items than the short-hot items. We split the items into short-hot and long-tail according to the occurrence frequency of items. The 20% items with higher frequency are defined as short hot, and the remaining 80% are defined as long tail. The final ratio of short-hot and long-tail items of instances is about 8:2. In general, compared with DeepFM, our DDP improves AUC 0.95% on long-tail items and 0.11% on short hot items respectively. The reason lies in that the FP performs more stable and obtains more general prior information, which is more helpful to provide a more accurate estimation on tailed items with limited training instances. This finding is consistent with the previous conclusion that the use of prior information is a very effective method to solve the long tail problem [3, 4, 22].",
  "4.3 Performances on Consecutive Increments": "Considering that the real scenarios, where models are updated with the newly arrived data in loops, we mimic the online setting with consecutive days to explore the performance during the training loops. Experiments are carried out on the CIKM2019 dataset, where we train a warmed-up model using data from the preceding 8 days. CIKM '23, October 21-25, 2023, Birmingham, United Kingdom Yang Chen et al. Figure 4: Varying Interaction Modules 0.795 0.798 0.801 0.804 DNN DCN W&D DeepFM FwFM AutoInt AUC Base FP DDP (a) AUC on Criteo 0.446 0.449 0.452 0.455 DNN DCN W&D DeepFM FwFM AutoInt LogLoss Base FP DDP (b) LogLoss on Criteo The model is incrementally updated till the data before the test day, then tested on test day. For example, in the tenth day, the data before the tenth day is fed into the model incrementally and the test data is the tenth day. As shown in Figure 3, DDP consistently outperforms the competing baseline IncCTR and RLP on the all seven tested days in terms of both AUC and LogLoss. This indicates the superiority of adopting the data-driven prior, which achieves a more robust update for recommenders. In the early stages, there is a large gap between our method and other baselines, which is primarily due to the fact that our FP can assist to better express the feature when the quantity of each feature is limited.",
  "4.4 Varying Interaction Modules": "Since the DDP is a model-agnostic framework and is easily plugged into various deep CTR models, we conduct experiments with six different interaction modules to verify the compatibility of the proposed method. The experimental results are shown in Figure 4. FP consistently improves the performance on all interaction modules, achieving an average 0.44% improvement on AUC and 0.66% on LogLoss. DDP further improves the performance with the additional MP. The improvements demonstrate the capability of the proposed method being suitable to any modern interaction modules and the superiority of alleviating the overfitting problem in incremental update for online recommenders.",
  "4.5 Deep Analysis of Feature Prior (FP)": "As we claim aforementioned, the FP is optimized with SGD optimizer while MP is updated with Adam optimizer. We conduct experiments to compare the performance of updating FP with SGD and Adam optimizers, as shown in Figure 5. The performance under SGD is tuned over different learning rate. The orange dashed line represents the best performance with Adam optimizer. The grey line represents the performance of the competing method MECP, which adopts the previously trained CTR of features. The results demonstrate the superiority of integrating the FP in the end-to-end framework and we have the following observation: 1) DDP, capturing the dynamics of features along with instances, shows better performances than MECP, indicating the importance of constantly capturing data distribution at the feature level. 2) SGD outperforms Adam over a range of learning rates. This suggests that the decreasing learning rate of Adam throughout training hinders the continuous learning of the FP module. 3) Lower SGD Figure 5: FP Optimized with SGD 0.8 0.801 0.802 0.803 1e-1 1e-2 1e-3 1e-4 1e-5 AUC SGD Learning Rate DDP(SGD) DDP(Adam) MECP Table 3: Experiments on the Industrial Dataset learning rates lead to better performance by preserving long-term knowledge and ensuring parameter stability for feature CTR values.",
  "5 EXPERIMENTS ON INDUSTRIAL DATASET": "The industrial dataset is a collection of logs from the homepage streaming data, which is from an online advertising platform of a famous e-commerce company. It consists of 7 billion instances over 121 consecutive days. We use the 120-day data for incremental training directly. The performance is evaluated on the data of the last day, as reported in Table 3. DDP outperforms the base models, achieving a relative 0.58% improvement of AUC, even though the model has been already optimized with 7 billion logs. This supports our point that under the environment of rapid changes in data distribution, how maintaining the relatively stable prior information of the model and feature is of great significance to improve the overall performance of the model. Furthermore, we conduct online A/B test to validate the effectiveness of the proposed framework DDP, which is deployed on a mainstream advertising position on the homepage. There are around ten million impressions each day and we allocate 5% of all traffic to DDP for 7 days. The results is reported in Table 3. DDP increases CTR at an average 1.99% improvement and increases eCPM (effective cost per mille) at 2.97%, indicating significant improvements for online recommenders.",
  "6 CONCLUSION": "In this paper, we propose an incremental update framework for online recommenders with Data-Driven Prior (DDP) to ease the overfitting problem on incremental data, which is model-agnostic and can be learned end-to-end. The proposed FP is intended to estimate the average CTR value for each feature, allowing for more stable training and more accurate CTR estimates for long-tail items. Further, the proposed MP approximates the posterior on the complete data by optimizing the likelihood function on the incremental data and reducing the distance with the previous model. Extensive experiments on two public datasets demonstrate the advantages of the proposed framework, while offline and online studies on an industrial dataset show its effectiveness. An Incremental Update Framework for Online Recommenders with Data-Driven Prior CIKM '23, October 21-25, 2023, Birmingham, United Kingdom",
  "REFERENCES": "[1] Ari Benjamin, David Rolnick, and Konrad Kording. 2018. Measuring and regularizing networks in function space. In International Conference on Learning Representations . [2] Guohao Cai, Jieming Zhu, Quanyu Dai, Zhenhua Dong, Xiuqiang He, Ruiming Tang, and Rui Zhang. 2022. ReLoop: A Self-Correction Continual Learning Loop for Recommender Systems. Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (2022). [3] Ermei Cao, Difeng Wang, Jiacheng Huang, and Wei Hu. 2020. Open knowledge enrichment for long-tail entities. In Proceedings of The Web Conference 2020 . 384-394. [4] Yixin Cao, Jun Kuang, Ming Gao, Aoying Zhou, Yonggang Wen, and Tat-Seng Chua. 2021. Learning relation prototype from unlabeled texts for long-tail relation extraction. IEEE Transactions on Knowledge and Data Engineering (2021). [5] Jin Chen, Defu Lian, Binbin Jin, Xu Huang, Kai Zheng, and Enhong Chen. 2022. Fast variational autoencoder with inverted multi-index for collaborative filtering. In Proceedings of the ACM Web Conference 2022 . 1944-1954. [6] Jin Chen, Defu Lian, Binbin Jin, Kai Zheng, and Enhong Chen. 2022. Learning Recommenders for Implicit Feedback with Importance Resampling. In Proceedings of the ACM Web Conference 2022 . 1997-2005. [7] Jin Chen, Defu Lian, Yucheng Li, Baoyun Wang, Kai Zheng, and Enhong Chen. 2022. Cache-Augmented Inbatch Importance Resampling for Training Recommender Retriever. Advances in Neural Information Processing Systems 35 (2022), 34817-34830. [8] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems . 7-10. [9] Renchu Guan, Haoyu Pang, Fausto Giunchiglia, Ximing Li, Xuefeng Yang, and Xiaoyue Feng. 2022. Deployable and Continuable Meta-learning-Based Recommender System with Fast User-Incremental Updates. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1423-1433. [10] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: A Factorization-Machine based Neural Network for CTR Prediction. In IJCAI . [11] Diederik P Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In ICLR (Poster) . [12] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. 2017. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences 114, 13 (2017), 35213526. [13] Defu Lian, Qi Liu, and Enhong Chen. 2020. Personalized ranking with importance sampling. In Proceedings of The Web Conference 2020 . 1093-1103. [14] Xiaoliang Ling, Weiwei Deng, Chen Gu, Hucheng Zhou, Cui Li, and Feng Sun. 2017. Model ensemble for click prediction in bing search ads. In Proceedings of the 26th international conference on world wide web companion . 689-698. [15] Bin Liu, Chenxu Zhu, Guilin Li, Weinan Zhang, Jincai Lai, Ruiming Tang, Xiuqiang He, Zhenguo Li, and Yong Yu. 2020. Autofis: Automatic feature interaction selection in factorization models for click-through rate prediction. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 2636-2645. [16] Fei Mi, Xiaoyu Lin, and Boi Faltings. 2020. Ader: Adaptively distilled exemplar replay towards continual learning for session-based recommendation. In Fourteenth ACM Conference on Recommender Systems . 408-413. [17] Boaz Nadler and Ronald R Coifman. 2005. The prediction error in CLS and PLS: the importance of feature selection prior to multivariate calibration. Journal of Chemometrics: A Journal of the Chemometrics Society 19, 2 (2005), 107-118. [18] Junwei Pan, Jian Xu, Alfonso Lobos Ruiz, Wenliang Zhao, Shengjun Pan, Yu Sun, and Quan Lu. 2018. Field-weighted factorization machines for click-through rate prediction in display advertising. In Proceedings of the 2018 World Wide Web Conference . 1349-1357. [19] Danni Peng, Sinno Jialin Pan, Jie Zhang, and Anxiang Zeng. 2021. Learning an Adaptive Meta Model-Generator for Incrementally Updating Recommender Systems. In Fifteenth ACM Conference on Recommender Systems . 411-421. [20] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, and Jian Tang. 2019. Autoint: Automatic feature interaction learning via selfattentive neural networks. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management . 1161-1170. [21] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network for ad click predictions. In Proceedings of the ADKDD'17 . 1-7. [22] Weitao Wang, Meng Wang, Sen Wang, Guodong Long, Lina Yao, Guilin Qi, and Yang Chen. 2020. One-shot learning for long-tail visual relation detection. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 34. 12225-12232. [23] Yichao Wang, Huifeng Guo, Ruiming Tang, Zhirong Liu, and Xiuqiang He. 2020. A practical incremental method to train deep ctr models. arXiv preprint arXiv:2009.02147 (2020). [24] Yejing Wang, Xiangyu Zhao, Tong Xu, and Xian Wu. 2022. Autofield: Automating feature selection in deep recommender systems. In Proceedings of the ACM Web Conference 2022 . 1977-1986. [25] Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. 2017. The marginal value of adaptive gradient methods in machine learning. Advances in neural information processing systems 30 (2017). [26] Kailun Wu, Weijie Bian, Zhangming Chan, Lejian Ren, Shiming Xiang, Shuguang Han, Hongbo Deng, and Bo Zheng. 2021. Adversarial Gradient Driven Exploration for Deep Click-Through Rate Prediction. arXiv preprint arXiv:2112.11136 (2021). [27] Jiafeng Xia, Dongsheng Li, Hansu Gu, Jiahao Liu, Tun Lu, and Ning Gu. 2022. FIRE: Fast Incremental Recommendation with Graph Signal Processing. In Proceedings of the ACM Web Conference 2022 . 2360-2369. [28] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Deep interest evolution network for click-through rate prediction. In Proceedings of the AAAI conference on artificial intelligence , Vol. 33. 5941-5948. [29] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through rate prediction. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1059-1068. [30] Yongchun Zhu, Ruobing Xie, Fuzhen Zhuang, Kaikai Ge, Ying Sun, Xu Zhang, Leyu Lin, and Juan Cao. 2021. Learning to warm up cold item embeddings for coldstart recommendation with meta scaling and shifting networks. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1167-1176.",
  "keywords_parsed": [
    "CTR Prediction",
    "Incremental Update",
    "Data-Driven Prior"
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "Measuring and regularizing networks in function space"
    },
    {
      "ref_id": "b2",
      "title": "ReLoop: A Self-Correction Continual Learning Loop for Recommender Systems"
    },
    {
      "ref_id": "b3",
      "title": "Open knowledge enrichment for long-tail entities"
    },
    {
      "ref_id": "b4",
      "title": "Learning relation prototype from unlabeled texts for long-tail relation extraction"
    },
    {
      "ref_id": "b5",
      "title": "Fast variational autoencoder with inverted multi-index for collaborative filtering"
    },
    {
      "ref_id": "b6",
      "title": "Learning Recommenders for Implicit Feedback with Importance Resampling"
    },
    {
      "ref_id": "b7",
      "title": "Cache-Augmented Inbatch Importance Resampling for Training Recommender Retriever"
    },
    {
      "ref_id": "b8",
      "title": "Wide & deep learning for recommender systems"
    },
    {
      "ref_id": "b9",
      "title": "Deployable and Continuable Meta-learning-Based Recommender System with Fast User-Incremental Updates"
    },
    {
      "ref_id": "b10",
      "title": "DeepFM: A Factorization-Machine based Neural Network for CTR Prediction"
    },
    {
      "ref_id": "b11",
      "title": "Adam: A Method for Stochastic Optimization"
    },
    {
      "ref_id": "b12",
      "title": "Overcoming catastrophic forgetting in neural networks"
    },
    {
      "ref_id": "b13",
      "title": "Personalized ranking with importance sampling"
    },
    {
      "ref_id": "b14",
      "title": "Model ensemble for click prediction in bing search ads"
    },
    {
      "ref_id": "b15",
      "title": "Autofis: Automatic feature interaction selection in factorization models for click-through rate prediction"
    },
    {
      "ref_id": "b16",
      "title": "Ader: Adaptively distilled exemplar replay towards continual learning for session-based recommendation"
    },
    {
      "ref_id": "b17",
      "title": "The prediction error in CLS and PLS: the importance of feature selection prior to multivariate calibration"
    },
    {
      "ref_id": "b18",
      "title": "Field-weighted factorization machines for click-through rate prediction in display advertising"
    },
    {
      "ref_id": "b19",
      "title": "Learning an Adaptive Meta Model-Generator for Incrementally Updating Recommender Systems"
    },
    {
      "ref_id": "b20",
      "title": "Autoint: Automatic feature interaction learning via selfattentive neural networks"
    },
    {
      "ref_id": "b21",
      "title": "Deep & cross network for ad click predictions"
    },
    {
      "ref_id": "b22",
      "title": "One-shot learning for long-tail visual relation detection"
    },
    {
      "ref_id": "b23",
      "title": "A practical incremental method to train deep ctr models"
    },
    {
      "ref_id": "b24",
      "title": "Autofield: Automating feature selection in deep recommender systems"
    },
    {
      "ref_id": "b25",
      "title": "The marginal value of adaptive gradient methods in machine learning"
    },
    {
      "ref_id": "b26",
      "title": "Adversarial Gradient Driven Exploration for Deep Click-Through Rate Prediction"
    },
    {
      "ref_id": "b27",
      "title": "FIRE: Fast Incremental Recommendation with Graph Signal Processing"
    },
    {
      "ref_id": "b28",
      "title": "Deep interest evolution network for click-through rate prediction"
    },
    {
      "ref_id": "b29",
      "title": "Deep interest network for click-through rate prediction"
    },
    {
      "ref_id": "b30",
      "title": "Learning to warm up cold item embeddings for coldstart recommendation with meta scaling and shifting networks"
    }
  ]
}