{"RESUS: Warm-Up Cold Users via Meta-Learning Residual User Preferences in CTR Prediction": "YANYAN SHEN \u2217 , Department of Computer Science and Engineering, Shanghai Jiao Tong University, China LIFAN ZHAO, Department of Computer Science and Engineering, Shanghai Jiao Tong University, China WEIYU CHENG, Department of Computer Science and Engineering, Shanghai Jiao Tong University, China ZIBIN ZHANG, WeChat, Tencent, China WENWEN ZHOU, WeChat, Tencent, China KANGYI LIN, WeChat, Tencent, China Click-Through Rate (CTR) prediction on cold users is a challenging task in recommender systems. Recent researches have resorted to meta-learning to tackle the cold-user challenge, which either perform few-shot user representation learning or adopt optimization-based meta-learning. However, existing methods suffer from information loss or inefficient optimization process, and they fail to explicitly model global user preference knowledge which is crucial to complement the sparse and insufficient preference information of cold users. In this paper, we propose a novel and efficient approach named RESUS, which decouples the learning of global preference knowledge contributed by collective users from the learning of residual preferences for individual users. Specifically, we employ a shared predictor to infer basis user preferences, which acquires global preference knowledge from the interactions of different users. Meanwhile, we develop two efficient algorithms based on the nearest neighbor and ridge regression predictors, which infer residual user preferences via learning quickly from a few user-specific interactions. Extensive experiments on three public datasets demonstrate that our RESUS approach is efficient and effective in improving CTR prediction accuracy on cold users, compared with various state-of-the-art methods.", "CCS Concepts: \u00b7 Information systems \u2192 Recommender systems .": "Additional Key Words and Phrases: Cold-start recommendation, CTR prediction, Few-shot Learning, Metricbased Meta Learning", "ACMReference Format:": "Yanyan Shen, Lifan Zhao, Weiyu Cheng, Zibin Zhang, Wenwen Zhou, and Kangyi Lin. 2022. RESUS: WarmUp Cold Users via Meta-Learning Residual User Preferences in CTR Prediction. J. ACM 37, 4, Article 1 (September 2022), 27 pages. https://doi.org/10.1145/3564283", "1 INTRODUCTION": "Click-Through Rate (CTR) prediction is an essential task in recommender systems, aiming to predict the probability of a user clicking on a recommended item (e.g., ad, article, product) accurately. Developing deep learning models is becoming the norm to achieve the state-of-the-art CTR prediction \u2217 Yanyan Shen is the corresponding author. Authors' addresses: Yanyan Shen, Department of Computer Science and Engineering, Shanghai Jiao Tong University, China, shenyy@sjtu.edu.cn; Lifan Zhao, Department of Computer Science and Engineering, Shanghai Jiao Tong University, China, mogician233@sjtu.edu.cn; Weiyu Cheng, Department of Computer Science and Engineering, Shanghai Jiao Tong University, China, weiyu_cheng@sjtu.edu.cn; Zibin Zhang, WeChat, Tencent, China, bingozhang@tencent.com; Wenwen Zhou, WeChat, Tencent, China, wendizhou@tencent.com; Kangyi Lin, WeChat, Tencent, China, plancklin@tencent.com. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \u00a9 2022 Association for Computing Machinery. 0004-5411/2022/9-ART1 $15.00 https://doi.org/10.1145/3564283 J. ACM, Vol. 37, No. 4, Article 1. Publication date: September 2022. 1:2 Shen, et al. Fig. 1. The distribution of the number of interactions per user in the Movielens-1M dataset. Minimum = 20 Maximum = 2314 Median = 96 Skewness = 2.745 Percentage 0% 0.5% 1.0% 1.5% Number of Interactions per User 0 500 1000 1500 2000 performance. Among the existing deep models [12, 13, 19, 34] for CTR prediction, one important consideration is to learn user preferences from historical interactions, which is effective for users with sufficient interaction data. However, most real-world recommender systems involve large numbers of cold users who have committed very few interactions, e.g., newly registered users and inactive users. As shown in Figure 1 for example, 20% of the users with most interactions in the Movielens-1M dataset contribute nearly 60% of the total interactions. The distribution in practice can be more skewed since Movielens-1M has already filtered out users with fewer than 20 interactions. With very limited interaction records, the existing deep models for CTR prediction suffer from unsatisfactory prediction performance on cold users [39]. Great efforts have been devoted to effectively learning preferences of cold users in a broader scope which is known as user cold-start recommendation . Some researches [5, 20, 32, 47] focus on completely cold users with no interaction data. In this paper, we consider the more general scenario that cold users have a small number of historical interactions. Recent researches have resorted to developing meta-learning algorithms to address the cold-user challenge. The goal of meta-learning is to train a model on a diverse set of tasks, such that the model can learn and adapt quickly to a new task with very few labeled data. In the meta-learning framework, cold users are organized into tasks (or episodes ). Each task contains a support set involving a few historical interactions of a specific cold user and a query set of test items whose interaction labels need to be predicted correctly. A principled meta-learning algorithm involves two nested learning levels: (i) the baselearner works at the level of individual tasks, which acquires user-specific preference information from the support set and perform predictions over the query set; and (ii) the meta-learner learns transferable meta-knowledge from different tasks, in order to improve the performance of the base learner across tasks. Generally, there are two groups of meta-learning algorithms proposed for the user cold-start recommendation. The first group performs few-shot user representation learning . They use the meta-learning framework to compute the representation of a particular user by condensing the user's historical interactions within the support set into one fixed-length latent vector via average pooling [44, 46], attention mechanism [33], or capsule clustering [35]. The user representation will be fused with the representation of a query item to perform prediction. However, the condensation can easily cause information loss [18, 40, 50] and yield suboptimal performance. The second group adopts optimization-based meta-learning , which is inspired by the J. ACM, Vol. 37, No. 4, Article 1. Publication date: September 2022. RESUS: Warm-Up Cold Users via Meta-Learning Residual User Preferences in CTR Prediction 1:3 MAML algorithm [15]. They focus on the learning of meta-knowledge in the form of good initial values of user ID embeddings [39] or base learner's parameters [7, 14, 30, 37]. For every task, the base-learner needs to perform fine-tuning using the support set through gradient descent. However, the optimization-based meta-learning methods have two drawbacks. First, the optimization of the meta-learner is expensive in both time and memory cost during training due to the computation of high-order derivatives [22, 25]. Second, the base learner entails the complexity of fine-tuning when adapting to a new task and thus is inefficient during test time. To tackle the aforementioned problems, one way is to perform metric-based meta-learning (or, metric learning for short) which is a simple yet powerful approach developed for few-shot image classification [29, 42, 43, 45]. The core idea of metric learning is to measure the similarity between a query sample with each of the support samples and infer its label according to the labels of its most similar support samples, which mimics the \ud835\udc58 -nearest neighbor classifier. Following typical metric learning methods, the meta-learner involves a feature encoder shared by different tasks that projects raw inputs into a latent feature space, and the base-learner is a classifier that relies on a pre-defined similarity metric such as cosine similarity [42] or a parameterized distance function [43]. While metric learning is easy to optimize and avoids test-time adaptation, applying it to few-shot CTR predictions for cold users is still challenging due to the following reason. That is, in each task, the support set of a particular cold user involves very few interactions that encode limited or even biased information of the user's preference. As a consequence, the base-learner may easily suffer when a query instance is distant from all the instances in the support set in the encoded feature space. To overcome this limitation, an important observation is that different from few-shot image classification where each task contains novel labels to be recognized, every few-shot CTR prediction task of a cold user involves two labels, i.e., one for clicking and zero for non-clicking, and notably the two labels are shared by all the tasks. This label sharing among different tasks reveals informative global preference knowledge . For example, items with similar features like high ratings and popular categories are very likely to be clicked by most users (i.e., sharing the clicking label), and likewise, debased items often receive few clicks (i.e., sharing the non-clicking label). Arguably, such global knowledge due to the label sharing fact is useful to complement the sparse and insufficient preference information of cold users (provided by the support sets), and hence has the potential to benefit the CTR prediction performance over the query sets, especially when the query samples are dissimilar or irrelevant to all the support samples. It is noteworthy that the global knowledge is different from the meta-knowledge acquired by meta-learner as the latter is typically in the form of transferable embeddings or the initial values of base-learner's parameters, for the purpose of fast adaptation to new tasks. To this end, the existing meta-learning approaches mentioned above are incapable of utilizing the global preference knowledge contributed by all the users. In this paper, we propose RESUS (short for meta-learning RESidual USer preferences), a generic and efficient approach to address the cold-user challenge in CTR prediction. The main idea of RESUS is to decouple user preferences into two parts, namely basis user preferences and residual user preferences , which are learned by different modules and collectively used to predict the probabilities of new clicks. To be specific, RESUS employs a shared predictor to infer basis user preferences on query items based on the input features of query samples. The shared predictor can be implemented with any CTR prediction architecture [12, 13, 19, 34], and it is trained with historical interaction data from different users to absorb global preference knowledge. RESUS then customizes a residual preference predictor as the base-learner to infer residual user preference for each query sample according to its matching results with the user-specific historical interactions in the support set. The rationale of the decoupled preference learning framework is to use the shared predictor to make a rough preference estimation for each query sample based on its input features and then J. ACM, Vol. 37, No. 4, Article 1. Publication date: September 2022. 1:4 Shen, et al. derive residual preference by matching query sample with each of the support samples. To this end, the two components complement each other and RESUS can perform well when the input features in a query sample are informative to infer its CTR label or the support set is useful to transfer label information to the query sample through matching. According to our experiments, the shared predictor is useful to alleviate the limitation of metric learning when support samples are irrelevant to query samples. To realize the residual preference predictor (i.e., the base-learner) in RESUS, we provide two efficient designs. The first design is a nearest-neighbor predictor relying on a similarity function, which is optimized during the training stage without fine-tuning at test time. The second design is a ridge-regression predictor with differentiable closed-form solvers [6]. It allows task-dependent adaptation during test time, but avoids expensive fine-tuning. Certainly, our framework leaves room for other advanced base-learners in the future to be incorporated. The final CTR prediction result takes the two parts of user preferences into account, which is obtained by fusing the outputs of the shared predictor and the base-learner. We conduct extensive experiments on three public datasets and demonstrate that RESUS outperforms the state-of-the-art methods in terms of higher CTR prediction accuracy on cold users and lower computational cost. To summarize, this paper makes the following contributions. \u00b7 We propose to decouple user preferences into two parts, namely basis user preferences and residual user preferences, and further develop a novel RESUS approach that employs a shared predictor to capture global preference knowledge to infer basis user preferences and then predicts residual user preferences based on very few user-specific historical interactions. \u00b7 Our proposed RESUS is a generic decoupled preference learning framework. The shared predictor can be implemented by any model architecture proposed for CTR prediction, and the learning of residual user preferences can be achieved by applying different meta-learning algorithms flexibly. To the best of our knowledge, we are the first to utilize metric-learning for cold-start CTR prediction. \u00b7 We provide two efficient designs for the base-learner to infer residual user preferences in RESUS: (i) the nearest-neighbor predictor is fast and easy to optimize; and (ii) the ridgeregression predictor performs task-dependent adaptation without entailing the complexity of fine-tuning. \u00b7 We conduct extensive experiments on three public datasets demonstrate the superior performance of RESUS in terms of CTR prediction accuracy on cold users, compared with the state-of-the-art approaches. Further analysis shows that the advantage of RESUS is more significant for colder users and confirms the efficiency of RESUS during inference. The remainder of this paper is organized as follows. We present the problem and its metalearning setting in Section 2. We describe a basic metric-based meta-learning approach to few-shot CTR prediction in Section 3 and elaborate the details of our RESUS approach in Section 4. The experimental results are provided in Section 5. We review the related works in Section 6 and conclude this paper in Section 7.", "2 PRELIMINARIES": "In this section, we provide the basic definitions in CTR prediction and then explain how to cast the cold-user CTR prediction problem within the meta-learning framework. Table 1 summarizes all the notations used throughout this paper.", "2.1 Definitions": "Click-Through Rate (CTR) prediction aims to infer the probability that a user would click on a specific item. Each instance in CTR prediction can be denoted as ( x , \ud835\udc66 ) , where x is a vector describing J. ACM, Vol. 37, No. 4, Article 1. Publication date: September 2022. RESUS: Warm-Up Cold Users via Meta-Learning Residual User Preferences in CTR Prediction 1:5 Table 1. Notation table. \ud835\udc39 feature fields and \ud835\udc66 is a binary label indicating a click or non-click behavior. In general, the feature fields include user fields (e.g., gender, occupation), item fields (e.g., category, tag), and contextual fields (e.g., time and location when the behavior occurs). Typically, x is very sparse due to the one-hot encodings of categorical feature fields. We denote by e \ud835\udc56 the dense embedding of the \ud835\udc56 -th feature field's value in x . Let \ud835\udc37 = {( x , \ud835\udc66 )} be the set of all the instances. Definition 2.1 (CTR prediction). The CTR prediction problem is to train a binary classifier \ud835\udc53 ( x ; \u0398 ) , where x is an input feature vector in the raw input feature space X , and \u0398 denotes the parameter set. The ultimate goal of this paper is to address the cold-user challenge in CTR prediction. Let \ud835\udc37 \ud835\udc62 \u2286 \ud835\udc37 denote the set of the observed instances associating with user \ud835\udc62 . User \ud835\udc62 is referred to as a cold user if \ud835\udc37 \ud835\udc62 contains a small number of instances. Specifically, we use a threshold \ud835\udf0f as the upper limit of instances to identify cold users. Letting \ud835\udc48 \ud835\udc50 be the set of cold users with respect to \ud835\udf0f , we have | \ud835\udc37 \ud835\udc62 | \u2264 \ud835\udf0f for any user \ud835\udc62 \u2208 \ud835\udc48 \ud835\udc50 . We assume | \ud835\udc37 \ud835\udc62 | > 0 for any \ud835\udc62 \u2208 \ud835\udc48 \ud835\udc50 and leave the zero-shot CTR prediction for completely cold users as future work. J. ACM, Vol. 37, No. 4, Article 1. Publication date: September 2022. 1:6 Shen, et al. Fig. 2. An illustration of the meta-learning framework. Support Set \ud835\udc46\ud835\udc46 \ud835\udc62\ud835\udc62 Query Set \ud835\udc44\ud835\udc44 \ud835\udc62\ud835\udc62 Task 1 (User 1) Meta- Train \ud835\udcaf\ud835\udcaf \ud835\udc61\ud835\udc61rain Meta- Test \ud835\udcaf\ud835\udcaf \ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61 Task 2 (User 2) Task i (User i) click non-click Definition 2.2 (CTR prediction for cold users). Consider a cold user \ud835\udc62 \u2208 \ud835\udc48 \ud835\udc50 with historical CTR instances \ud835\udc37 \ud835\udc62 ( 0 \u2264 | \ud835\udc37 \ud835\udc62 | \u2264 \ud835\udf0f ) . For any feature vector x \ud835\udc62 \u2208 X involving \ud835\udc62 , we aim to predict the clicking probability \ud835\udc43\ud835\udc5f ( \ud835\udc66 \ud835\udc62 = 1 | x \ud835\udc62 ) , i.e., \ud835\udc62 's preference on an item as described in x \ud835\udc62 .", "2.2 Meta-learning Framework": "Learning the preferences of cold users is challenging due to the severely limited information supplied by the user-specific instances \ud835\udc37 \ud835\udc62 . Fortunately, performing CTR prediction for cold users bears resemblance to few-shot classification that tries to recognize novel concepts from labeled examples where each label appears only a small number of times. In our context, for a cold user \ud835\udc62 \u2208 \ud835\udc48 \ud835\udc50 , we can split the historical CTR instances \ud835\udc37 \ud835\udc62 into two parts \ud835\udc37 + \ud835\udc62 and \ud835\udc37 -\ud835\udc62 according to the label values, i.e., \ud835\udc37 + \ud835\udc62 = {( x , \ud835\udc66 ) \u2208 \ud835\udc37 \ud835\udc62 | \ud835\udc66 = 1 } and \ud835\udc37 -\ud835\udc62 = {( x , \ud835\udc66 ) \u2208 \ud835\udc37 \ud835\udc62 | \ud835\udc66 = 0 } . Since the preferences on items can vary for different users, here we simply consider the labels to be user-specific. In this way, we can obtain 2 labels per cold user and a total of 2 | \ud835\udc48 \ud835\udc50 | labels where each label has up to \ud835\udf0f training instances. Inspired by the similarity of two problems, we naturally cast CTR prediction for cold users into the standard meta-learning framework which has shown promising performance on few-shot learning problems [29, 42, 43, 45]. Figure 2 illustrates the meta-learning framework. We have a meta-train set T \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b and a metatest set T \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 where each set contains a collection of few-shot CTR prediction tasks (or tasks for short). A task \ud835\udc47 \ud835\udc62 aims at promoting CTR prediction performance for cold user \ud835\udc62 . Formally, \ud835\udc47 \ud835\udc62 consists of a support set \ud835\udc46 \ud835\udc62 \u2286 \ud835\udc37 \ud835\udc62 and a query set \ud835\udc44 \ud835\udc62 \u2286 \ud835\udc37 \ud835\udc62 that represent the training and test sets for the task respectively, satisfying (i) | \ud835\udc46 \ud835\udc62 | is small and (ii) \ud835\udc46 \ud835\udc62 \u2229 \ud835\udc44 \ud835\udc62 = \u2205 . In general, \ud835\udc46 \ud835\udc62 is formed by randomly selecting labeled instances from \ud835\udc37 \ud835\udc62 , and the remaining instances in \ud835\udc37 \ud835\udc62 are randomly sampled to form \ud835\udc44 \ud835\udc62 . When the time information is associated with CTR instances, we shall sort \ud835\udc37 \ud835\udc62 in time order and use the first | \ud835\udc46 \ud835\udc62 | instances to ensure that query instances occur after support instances. The tasks in T \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b and T \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 are from two disjoint groups of users, and we defer the details of forming the meta sets in Section 4.2 and 5.1.2. As proposed in [45], in each training iteration, a task \ud835\udc47 \ud835\udc62 = ( \ud835\udc46 \ud835\udc62 , \ud835\udc44 \ud835\udc62 ) is sampled from T \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b and a base-learner model is trained to improve the prediction performance over the query set \ud835\udc44 \ud835\udc62 conditioned on the support set \ud835\udc46 \ud835\udc62 . J. ACM, Vol. 37, No. 4, Article 1. Publication date: September 2022. RESUS: Warm-Up Cold Users via Meta-Learning Residual User Preferences in CTR Prediction 1:7 This training procedure matches inference at test time, i.e., predicting clicking probabilities on unlabeled instances for any cold user with a few labeled instances. While sharing the same meta-learning framework as many few-shot classification problems [29, 42, 43, 45], our problem of cold-user CTR prediction has the following distinction. The goal of few-shot classification is to acquire the ability of fast adapting to novel concepts where each concept corresponds to a new label. In our problem, we actually have two labels, i.e., clicking and non-clicking, shared by all the tasks. This label sharing among different tasks (or users) provides insights on global user preference knowledge as described before. To clarify the distinction, in what follows, we first provide a basic metric-based meta-learning approach and discuss its limitations. We then present our RESUS approach.", "3 MUS: A BASIC METRIC-BASED META-LEARNING APPROACH": "Following the meta-learning framework, for a task \ud835\udc47 \ud835\udc62 = ( \ud835\udc46 \ud835\udc62 , \ud835\udc44 \ud835\udc62 ) , we aim to learn user \ud835\udc62 's preference from \ud835\udc46 \ud835\udc62 and utilize it to perform predictions in \ud835\udc44 \ud835\udc62 . Asimplest approach to exploit the small support set is to mimic the metric-based meta-learning [45] that relies on a similarity function and transfers label information from support set to query set via matching. Without loss of generality, metricbased approaches consist of two modules: (1) an encoder module as the meta-learner that learns transferrable feature representations for the feature vectors; and (2) a predictor module as the base-learner that measures the distances between query and support instances according to their feature representations, and further predicts the label of a query instance using that of its nearest neighbor in the support set. Despite its simplicity, the matching strategy is effective for few-shot learning problems and efficient during test time [43]. This motivates us to develop a basic metricbased approach named MUS (Meta-learning USer preferences) for few-shot CTR prediction. In the following subsections, we first elaborate on the architecture and training objective of MUS. We then discuss its limitations and provide our insight on developing a decoupled learning framework.", "3.1 MUS Architecture": "MUS consists of the two modules: the feature encoder projects input feature vectors into a latent feature space, and the user preference predictor is trained to predict the clicking probabilities of query instances. 3.1.1 Feature Encoder \u03a6 . This module encodes the raw feature vectors of all the instances in \ud835\udc46 \ud835\udc62 and \ud835\udc44 \ud835\udc62 into dense embedding vectors. This can be realized by any existing CTR prediction model architecture that learns feature embeddings and captures complex feature interactions. By default, we employ the structure of DeepFM [19] to implement the feature encoder \u03a6 , which combines the power of factorization machine and deep learning. Recall that we assume \ud835\udc39 feature fields in each instance. We denote the embedding of the \ud835\udc56 -th feature field in x by e \ud835\udc56 , 1 \u2264 \ud835\udc56 \u2264 \ud835\udc39 . According to DeepFM [19], \u03a6 is formally defined as follows.   where x is the feature vector of any support or query instance in \ud835\udc46 \ud835\udc62 \u222a \ud835\udc44 \ud835\udc62 . \u2295 denotes the concatenation, MLP (\u00b7) is a multi-layer perceptron. 3.1.2 User Preference Predictor. This module is to predict the labels of the query instances based on the support set. Note that in our case, the labels essentially denote the clicking probabilities rather than categorical classes. Hence, we predict the label of a query instance by performing a weighted sum over the labels of the support instances where the weights are computed based on J. ACM, Vol. 37, No. 4, Article 1. Publication date: September 2022. 1:8 Shen, et al. a similarity function \ud835\udc54 \ud835\udf03 (\u00b7 , \u00b7) . Formally, for a query instance ( x \ud835\udc44 \ud835\udc62 , \ud835\udc66 \ud835\udc44 \ud835\udc62 ) \u2208 \ud835\udc44 \ud835\udc62 , the predictor module computes the clicking probability \u02c6 \ud835\udc66 \ud835\udc44 \ud835\udc62 as follows:   where \ud835\udc54 \ud835\udf03 (\u00b7 , \u00b7) can be a neural-network-based function parameterized by \ud835\udf03 , or a non-parametric similarity function such as cosine similarity (i.e., \ud835\udf03 = \u2205 ).", "3.2 Objective Function": "We use the average cross-entropy as the loss function for the task \ud835\udc47 \ud835\udc62 , which is defined as follows:  During training, the loss \ud835\udc3f \ud835\udc47 \ud835\udc62 computed on a sampled training task \ud835\udc47 \ud835\udc62 \u2208 T \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b is then backpropagated to update the parameters in the feature encoder \u03a6 and \ud835\udf03 in the predictor. To be more specific, the parameters in the feature encoder \u03a6 and \ud835\udf03 are shared across tasks and hence are optimized by minimizing the loss over a batch of training tasks (within the outer loop of meta-learning).", "3.3 Limitation of MUS and Our Insight": "MUS follows the matching idea as most existing metric-based meta-learning approaches. However, it also inherits the drawback of metric learning. Specifically, due to the small support set, a query instance can be easily distant from all the support instances in the latent feature space, making it difficult to predict the query label accurately. In these cases, the performance of MUS is degenerated to be equivalent to a random guess. To address the limitation, an important observation is that the target labels of different tasks are shared, i.e., clicking and non-clicking. This label sharing across tasks reveals global preference knowledge contributed by collective users. As mentioned, some items with high (resp. low) rating scores would be clicked by most (resp. few) users. Apparently, exploiting such global preference knowledge has potential benefits to the prediction performance over query sets, especially when the query instances are distant from all the support instances. To be more specific, by acquiring global preference knowledge from collective users, we could provide a rough estimation for a query instance of any cold user based on the input features of the query instance. The rough estimation would later be calibrated by referring to user-specific interactions in the support set. However, MUS treats each user as an independent matching task and is thus ignorant of the global preference knowledge among users. This inspires us to decouple the learning of user preferences separately and develop a novel RESUS framewwork.", "4 THE RESUS APPROACH": "In this section, we elaborate the details of our proposed RESUS approach and the training procedure. The key idea of RESUS is to decouple the predictions of user preferences into two parts. For each query instance in the task \ud835\udc47 \ud835\udc62 , we predict basis user preference by explicitly exploiting global preference knowledge from the historical interactions of different users. After that, we predict residual user preference based on the user-specific information in the support set \ud835\udc46 \ud835\udc62 . Finally, we fuse the two preferences to produce the final prediction result. J. ACM, Vol. 37, No. 4, Article 1. Publication date: September 2022. RESUS: Warm-Up Cold Users via Meta-Learning Residual User Preferences in CTR Prediction 1:9 Fig. 3. The overview of RESUS. Note that the shared predictor and the encoder are marked by a dotted background, meaning that these two modules are shared across all tasks. In contrast, the NN/RR predictor is a task-specific module. Task 1 (User 1) Support Set \ud835\udc46\ud835\udc46 \ud835\udc62\ud835\udc62 Query Set \ud835\udc44\ud835\udc44 \ud835\udc62\ud835\udc62 Encoder \u0424 NN/RR predictor \u039b \ud835\udc65\ud835\udc65 \ud835\udc62\ud835\udc62 \ud835\udc46\ud835\udc46 \ud835\udc66\ud835\udc66 \ud835\udc62\ud835\udc62 \ud835\udc46\ud835\udc46 \ud835\udc65\ud835\udc65 \ud835\udc62\ud835\udc62 \ud835\udc44\ud835\udc44 \u0394\ud835\udc66\ud835\udc66 \ud835\udc62\ud835\udc62 \ud835\udc46\ud835\udc46 \u0394\ufffd \ud835\udc66\ud835\udc66 \ud835\udc62\ud835\udc62 \ud835\udc44\ud835\udc44 \ud835\udc66\ud835\udc66 \ud835\udc62\ud835\udc62 \ud835\udc44\ud835\udc44 Loss \ufffd \ud835\udc66\ud835\udc66 \ud835\udc62\ud835\udc62 \ud835\udc44\ud835\udc44 \u2026 Shared predictor \u03a8 \u2026 Task 2 (User 2) \u2026 \u2026 \u03a6 ( \ud835\udc65\ud835\udc65 \ud835\udc62\ud835\udc62 \ud835\udc46\ud835\udc46 ) \u03a6 ( \ud835\udc65\ud835\udc65 \ud835\udc62\ud835\udc62 \ud835\udc44\ud835\udc44 ) \u03a8 ( \ud835\udc65\ud835\udc65 \ud835\udc62\ud835\udc62 \ud835\udc46\ud835\udc46 ) \u03a8 ( \ud835\udc65\ud835\udc65 \ud835\udc62\ud835\udc62 \ud835\udc44\ud835\udc44 ) Task 3 (User 3) \u2026 Task N (User N) \u2026", "4.1 RESUS Architecture": "Figure 3 depicts the overview of RESUS architecture, which consists of three modules: shared predictor \u03a8 , feature encoder \u03a6 , and residual user preference predictor \u039b . The feature encoder \u03a6 follows the same design as the one in MUS using Eq. (1)-(2). Therefore, we next present the structures of \u03a8 and \u039b . Consider the task \ud835\udc47 \ud835\udc62 = ( \ud835\udc46 \ud835\udc62 , \ud835\udc44 \ud835\udc62 ) . 4.1.1 Shared Predictor \u03a8 . This module is to capture global preference knowledge shared by users and predict the basis user preferences for the query instances without referring to the support set. The idea is to mimic the existing CTR prediction models that absorb an input feature vector x and predict the probability of clicking behavior \u02c6 \ud835\udc66 \u03a8 directly. In particular, the implementation of \u03a8 can be realized by various CTR prediction model structures such as Wide&Deep [12], DeepFM [19], etc. We will evaluate the effects of different structures for \u03a8 in the experiments (see Table 5). For any instance ( x , \ud835\udc66 ) in \ud835\udc46 \ud835\udc62 \u222a \ud835\udc44 \ud835\udc62 , we compute the basis user preference by:  where \ud835\udf0e (\u00b7) is the sigmoid function to control the predicted results within the range of ( 0 , 1 ) . In RESUS, \u03a8 is trained across tasks using the historical interactions of different users (see Section 4.2). In this way, it is able to acquire global preference knowledge from collective users and based on which, it can preliminarily assess the user preference according to the input feature vector. 4.1.2 Residual User Preference Predictor \u039b . In this module, we first compute the residual user preference for each support instance ( x \ud835\udc46 \ud835\udc62 , \ud835\udc66 \ud835\udc46 \ud835\udc62 ) \u2208 \ud835\udc46 \ud835\udc62 as follows:  where \u02c6 \ud835\udc66 \ud835\udc46 \u03a8 = \ud835\udf0e ( \u03a8 ( x \ud835\udc46 \ud835\udc62 )) . Henceforth, we obtain the transformed support set \ud835\udc46 \u2032 \ud835\udc62 = {( x \ud835\udc46 \ud835\udc62 , \u0394 \ud835\udc66 \ud835\udc46 \ud835\udc62 )} . J. ACM, Vol. 37, No. 4, Article 1. Publication date: September 2022. 1:10 Shen, et al. We now focus on predicting the residual user preferences for query instances based on the transformed user-specific training data \ud835\udc46 \u2032 \ud835\udc62 = {( x \ud835\udc46 \ud835\udc62 , \u0394 \ud835\udc66 \ud835\udc46 \ud835\udc62 )} . We propose two efficient ways to implement the residual user preference predictor \u039b : (i) the nearest-neighbor predictor is a similarity-based regression model, which prevents fine-tuning on new tasks; and (ii) the ridge-regression predictor enables fast adaptation to new tasks using the closed form solver. It is worth mentioning that both predictors are fully differentiable, enabling end-to-end optimization of RESUS. (i) Nearest-neighbor (NN) predictor. This is similar to the user preference predictor in MUS. It makes the prediction of a query instance's residual preference as a weighted sum over the residual preferences of the support samples. Formally, for a query instance ( x \ud835\udc44 \ud835\udc62 , \ud835\udc66 \ud835\udc44 \ud835\udc62 ) \u2208 \ud835\udc44 \ud835\udc62 , the NN predictor computes the residual user preference \u0394 \u02c6 \ud835\udc66 \ud835\udc44 \ud835\udc62 as follows:   Similar to MUS, \ud835\udc54 \ud835\udf03 (\u00b7 , \u00b7) is a similarity function, which can be non-parametric such as cosine similarity or a neural network parameterized by \ud835\udf03 . In RESUS, we implement \ud835\udc54 as follows:  where w \u2208 R \ud835\udc3e \u00d7 1 , \ud835\udc3e is the output dimension of feature encoder \u03a6 (\u00b7) , and \ud835\udc4f is a bias term. Similar to MUS, \ud835\udf03 = { w , \ud835\udc4f } is shared among tasks and hence can be viewed as the hyperparameters of the predictor \u03a8 . In particular, \ud835\udf03 is optimized over training tasks and is fixed during test time. (ii) Ridge-regression (RR) predictor. This is a task-specific predictor that trains its parameters based on the support set in each task. To avoid expensive training from scratch or fine-tuning over the support set per task, we employ ridge regression that admits a closed form solution [6] that can be computed directly in the inner loop of meta-learning. For ease of description, we use \u03a6 ( X \ud835\udc46 \ud835\udc62 ) \u2208 R | \ud835\udc46 \ud835\udc62 |\u00d7 \ud835\udc3e and \u0394 y \ud835\udc46 \ud835\udc62 \u2208 R | \ud835\udc46 \ud835\udc62 |\u00d7 1 to respectively denote the encoded feature vectors and the residual preferences of all the support instances in \ud835\udc46 \ud835\udc62 , stacked as rows. The ridge regression is parameterized by w \ud835\udc62 \u2208 R \ud835\udc3e \u00d7 1 and fit by solving the following optimization problem over \ud835\udc46 \ud835\udc62 :  where \ud835\udf06 \u2265 0 is optimized within the outer loop of meta-learning. The closed form solution for Eq. (11) is the following:  Note that the computation of Eq. (12) involves an inversion operation over the \ud835\udc3e \u00d7 \ud835\udc3e matrix. To alleviate the expensive computation cost, we adopt the Woodbury formula [41] to obtain w \u2217 \ud835\udc62 as follows:  The inversion is now performed over the | \ud835\udc46 \ud835\udc62 | \u00d7 | \ud835\udc46 \ud835\udc62 | matrix. Since | \ud835\udc46 \ud835\udc62 | \u226a \ud835\udc3e and | \ud835\udc46 \ud835\udc62 | is typically very small (due to cold users), we can reduce the cost of computing w \u2217 \ud835\udc62 significantly. Given w \u2217 \ud835\udc62 , the RR predictor computes the residual user preference for a query instance ( x \ud835\udc44 \ud835\udc62 , \ud835\udc66 \ud835\udc44 \ud835\udc62 ) \u2208 \ud835\udc44 \ud835\udc62 as follows:  J. ACM, Vol. 37, No. 4, Article 1. Publication date: September 2022. RESUS: Warm-Up Cold Users via Meta-Learning Residual User Preferences in CTR Prediction 1:11 Remark. The predicted residual preference \u0394 \u02c6 \ud835\udc66 \ud835\udc44 \ud835\udc62 using Eq. (8) or Eq. (14) is dependent on \u0394 y \ud835\udc46 \ud835\udc62 and may yield negative values. 4.1.3 Putting Two Kinds of Preferences Together. By far, for each query instance ( x \ud835\udc44 \ud835\udc62 , \ud835\udc66 \ud835\udc44 \ud835\udc62 ) in \ud835\udc44 \ud835\udc62 , we obtain the predicted basis user preference \u02c6 \ud835\udc66 \ud835\udc44 \u03a8 = \ud835\udf0e ( \u03a8 ( x \ud835\udc44 \ud835\udc62 )) using Eq. (6), and infer the residual user preference \u0394 \u02c6 \ud835\udc66 \ud835\udc44 \ud835\udc62 using Eq. (8) or Eq. (14). We then fuse them together to produce the final CTR prediction result \u02c6 \ud835\udc66 \ud835\udc44 \ud835\udc62 in the following way.  Note that we fuse \u0394 \u02c6 \ud835\udc66 \ud835\udc44 \ud835\udc62 with \u03a8 ( x \ud835\udc44 \ud835\udc62 ) , followed by the sigmoid function to normalize the final prediction within the range of ( 0 , 1 ) . \ud835\udefd is a hyperparameter of the base learner and used as the rescaling coefficient for the calibration purpose. Intuitively, when the input feature vector involves discriminative information to determine the preference score, we expect the predicted basis user preference to be close to the ground-truth \ud835\udc66 \ud835\udc44 \ud835\udc62 even if the base-learner suffers from insufficient userspecific preference information in the support set. Likewise, when the shared predictor can only infer a rough preference score according to the general preferences among users, we encourage the residual user preference to refine the rough score based on the user-specific historical interactions. In our work, \ud835\udefd is treated as a parameter of the meta-learner and optimized over training tasks in the outer-loop of meta-learning. As an alternative, one may set the value of \ud835\udefd in a more fine-grained way, e.g., \ud835\udefd is shared among tasks with the same support set size | \ud835\udc46 \ud835\udc62 | . The intuition is that users with more historical interactions can be assigned with a large value of \ud835\udefd .", "4.2 Training Procedure": "Before presenting the training procedure for RESUS, we first describe the construction of the meta-train set T \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b . We construct T \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b by sampling historical behaviors of a set of training users \ud835\udc48 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b . Specifically, for each training user \ud835\udc62 \u2208 \ud835\udc48 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b with historical behaviors \ud835\udc37 \ud835\udc62 , we randomly select | \ud835\udc46 \ud835\udc62 | instances from \ud835\udc37 \ud835\udc62 to form the support set and use the remaining instances as the query set, i.e., \ud835\udc44 \ud835\udc62 = \ud835\udc37 \ud835\udc62 \\ \ud835\udc46 \ud835\udc62 . In our experiments, when the time of interactions is known, we use the | \ud835\udc46 \ud835\udc62 | instances with the smallest timestamps rather than random sampling to preserve the time order between support set and query set. The size of the support set | \ud835\udc46 \ud835\udc62 | is sampled from the distribution P| \ud835\udc46 \ud835\udc62 | of the observed interaction numbers of actual cold users in real-world recommender systems, which is defined as follows:  where \ud835\udc56 \u2208 { 1 , \u00b7 \u00b7 \u00b7 , \ud835\udf0f } , \ud835\udc48 \ud835\udc50\ud835\udc5c\ud835\udc59\ud835\udc51 is the set of cold users and I is the indicator function. Recall that \ud835\udf0f is the upper limit on historical instances to identify cold users. It is desirable to sample support set size from the actual distribution to mitigate the mismatch between training and inference tasks. In the case where the distribution of the number of interactions on cold users is unknown, we sample the size of support set | \ud835\udc46 \ud835\udc62 | from a uniform distribution U{ 1 , \ud835\udf0f } , i.e., \ud835\udc43 (| \ud835\udc46 \ud835\udc62 | = \ud835\udc56 ) = 1 / \ud835\udf0f for \ud835\udc56 \u2208 { 1 , \u00b7 \u00b7 \u00b7 , \ud835\udf0f } . In the experiments, as public datasets save filtered out users with very few numbers of historical interactions, we adopt the uniform distribution to sample the size of support set. The above sampling process to form T \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b is repeated at the beginning of each epoch. In RESUS, only w \ud835\udc62 (in ridge-regression predictor) is learned within each task (with the closedform solution), and all the other parameters are optimized over training tasks within the outer loop of meta-learning, including: (i) the parameters in the shared predictor \u03a8 ; (ii) the parameters in the feature encoder module \u03a6 ; (iii) the hyperparameters in the residual user preference predictor \u039b , i.e., \ud835\udf03 or \ud835\udf06 ; (iv) the rescaling coefficient \ud835\udefd for fusing two preferences in Eq. (15). For ease of description, J. ACM, Vol. 37, No. 4, Article 1. Publication date: September 2022. 1:12 Shen, et al.", "Algorithm 1: RESUS - meta-learning RESidual USer preferences": "1: Input: The set of training users \ud835\udc48 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b with \ud835\udc37 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b . 2: Output: The meta-learner parameters \u0398 \ud835\udc5a\ud835\udc52\ud835\udc61\ud835\udc4e . 3: Randomly initialize \u0398 \ud835\udc5a\ud835\udc52\ud835\udc61\ud835\udc4e ; /* Train \u03a8 4: while not converged do 5: Sample a batch of instances B from \ud835\udc37 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b ; 6: Evaluate \ud835\udc3f \u03a8 over B based on Eq. (19); 7: Update the parameters in \u03a8 by minimizing \ud835\udc3f \u03a8 ; 8: end 9: Freeze the parameters in \u03a8 ; /* Train \u0398 \ud835\udc5a\ud835\udc52\ud835\udc61\ud835\udc4e \\ \u03a8 10: while not converged do 11: Sample a batch of training users 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: for \ud835\udc62 \ud835\udc62 do \u2208 B if \ud835\udc62 \ud835\udc46 | then is available P| Sample support set size else Sample support set size \ud835\udc62 \ud835\udc46 1 | \u223c U{ | end Get \ud835\udc62 \ud835\udc46 by sampling Get query set \ud835\udc62 \ud835\udc46 instances from | | \ud835\udc44 \ud835\udc62 \ud835\udc62 \ud835\udc62 \ud835\udc46 \ud835\udc37 ; = \\ Evaluate \ud835\udc47 end Update \ud835\udc5a\ud835\udc52\ud835\udc61\ud835\udc4e \u0398 \ud835\udc3f \ud835\udc62 \\ \u03a8 based on Eq. (17); by minimizing \u03a3 \ud835\udc47\ud835\udc62 \u2208B \u02dd \ud835\udc47\ud835\udc62 \ud835\udc44 \ud835\udc47 | \u2208B \ud835\udc47 \ud835\udc62 | \ud835\udc3f \ud835\udc44 | \ud835\udc47\ud835\udc62 \ud835\udc62 | 23: end we use \u0398 \ud835\udc5a\ud835\udc52\ud835\udc61\ud835\udc4e to denote the set of parameters in (i)-(iv). For a training task \ud835\udc47 \ud835\udc62 = ( \ud835\udc46 \ud835\udc62 , \ud835\udc44 \ud835\udc62 ) \u2208 T \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b , we compute the binary cross entropy loss as follows:  where \u02c6 \ud835\udc66 \ud835\udc44 \ud835\udc62 is the predicted clicking probability computed by Eq. (15). Hence, \u0398 \ud835\udc5a\ud835\udc52\ud835\udc61\ud835\udc4e is optimized by minimizing the following loss function over T \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b :  The above loss function is a weighted sum of \ud835\udc3f \ud835\udc47 \ud835\udc62 , \ud835\udc47 \ud835\udc62 \u2208 T \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b . In practice, we use batch gradient descent and accumulate the gradients of a batch of training tasks to make an update to the parameters in \u0398 \ud835\udc5a\ud835\udc52\ud835\udc61\ud835\udc4e . While the parameters in the shared predictor \u03a8 can be optimized through meta-learning, we found it is more beneficial to train \u03a8 in advance and fix it during the whole meta-learning process. Specifically, we use all the historical interactions from training users \ud835\udc48 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b to pretrain \u03a8 . Let \ud835\udc37 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b = {( x , \ud835\udc66 ) \u2208 \ud835\udc37 \ud835\udc62 | \ud835\udc62 \u2208 \ud835\udc48 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b } denote the interactions of the training users, where \ud835\udc37 \ud835\udc62 = \ud835\udc46 \ud835\udc62 \u222a \ud835\udc44 \ud835\udc62 . | B \ud835\udc62 \ud835\udc46 \ud835\udc62 \ud835\udc48 from | \u223c P| \ud835\udc46 \ud835\udc62 | \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b ; , \ud835\udf0f } \ud835\udc37 \ud835\udc62 ; ; ; ; */ */ J. ACM, Vol. 37, No. 4, Article 1. Publication date: September 2022. RESUS: Warm-Up Cold Users via Meta-Learning Residual User Preferences in CTR Prediction 1:13 We learn \u03a8 by minimizing the following loss function:  where \u02c6 \ud835\udc66 \u03a8 is the predicted basis user preference using Eq. (6). In practice, we update the paremeters in \u03a8 using batch gradient descent. After pretraining, we freeze the parameters of \u03a8 during the optimization of the other parameters in RESUS. Intuitively, this training setting prevents interference between the learning of basis user preferences and residual user preferences, empirically leading to faster convergence and better performance (see the results in Table 4). The overall training procedure of RESUS is summarized in Algorithm 1.", "4.3 Discussions": "4.3.1 Comparison with MUS. It is easy to verify that our basic metric-based meta-learning approach MUS described in Section 3 is equivalent to RESUS (using the NN predictor) without the shared predictor \u03a8 (i.e., always predicting zero). Recall that MUS suffers from the sparse information in the support sets of cold users. In RESUS, the shared predictor \u03a8 can be instantiated with any existing CTR models. It provides rough inference results for query instances based on the input features and the predictions are not affected by the utility of support sets. The predicted basis user preferences, though may not be accurate, are of great assistance in the final prediction performance. Nevertheless, RESUS is not a simple combination of the shared predictor and MUS. Specifically, the metric-learning counterpart in RESUS is not independent of the shared predictor but utilizes it in two ways. First, metric-learning relies on the shared predictor to encode the feature vectors of support and query instances. As we pretrain the shared predictor \u03a8 in a fully supervised manner, it can gain insights from interactions of collective users to derive good representations of feature vectors that are beneficial to the matching performance. Second, thanks to the shared predictor, the tasks performed by metric-learning in RESUS focus on fitting residual user preferences. Similar to the idea of boosting, inferring residuals is relatively simpler than inferring the overall preferences. Our experimental results also confirm the advantages of the decoupled preference learning framework, compared with MUS (see Table 4) and different shared predictors (see Table 5). It is also important to notice that MUS and RESUS are trained using the same training set \ud835\udc37 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b . Particularly, the shared predictor in RESUS is pretrained over \ud835\udc37 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b in a supervised manner (in order to acquire global preference knowledge from different users) and the metric-learning counterpart (i.e., \u0398 \ud835\udc5a\ud835\udc52\ud835\udc61\ud835\udc4e ) is optimized with the support and query sets sampled from \ud835\udc37 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b on an individual user basis (with the purpose of fast acquiring individual preference knowledge from support set). 4.3.2 Time Complexity Analysis. To predict the clicking probability of a query instance for user \ud835\udc62 , the time complexity of RESUS is determined by three modules: the shared predictor \u03a8 , the feature encoder \u03a6 , and the residual user preference predictor \u039b . First, since the shared predictor is applied to each sample in the support set \ud835\udc46 \ud835\udc62 , the time complexity of \u03a8 is O (| \ud835\udc46 \ud835\udc62 | | \ud835\udc4a \u03a8 |) , where | \ud835\udc4a \u03a8 | is the number of parameters in \u03a8 . Now that we can use any typical deep CTR prediction model architecture (e.g., DeepFM [19]) to implement \u03a8 , \ud835\udc4a \u03a8 refers to the set of weights in the corresponding neural network. Second, the time complexity of \u03a6 is O (| \ud835\udc46 \ud835\udc62 | | \ud835\udc4a \u03a6 |) . Third, we have two choices for the residual user preference predictor \u039b . The time complexity of the NN predictor is O (| \ud835\udc46 \ud835\udc62 | \ud835\udc3e ) , and that of the RR predictor is O (| \ud835\udc46 \ud835\udc62 | 2 \ud835\udc3e + | \ud835\udc46 \ud835\udc62 | 3 ) , where \ud835\udc3e is the dimension of the encoded feature vectors. Finally, the total time complexity of RESUS \ud835\udc41\ud835\udc41 is O (| \ud835\udc46 \ud835\udc62 | ( | \ud835\udc4a \u03a8 | + | \ud835\udc4a \u03a6 | + \ud835\udc3e )) , and that of RESUS \ud835\udc45\ud835\udc45 is O (| \ud835\udc46 \ud835\udc62 | ( | \ud835\udc4a \u03a8 | + | \ud835\udc4a \u03a6 | + | \ud835\udc46 \ud835\udc62 | \ud835\udc3e + | \ud835\udc46 \ud835\udc62 | 2 )) . Note that | \ud835\udc46 \ud835\udc62 | and \ud835\udc3e are much smaller than | \ud835\udc4a \u03a8 | + | \ud835\udc4a \u03a6 | , since \ud835\udc62 is a cold user and \ud835\udc46 \ud835\udc62 is usually from tens to hundreds. Hence the time complexity of RESUS is O (| \ud835\udc46 \ud835\udc62 | ( | \ud835\udc4a \u03a8 | + | \ud835\udc4a \u03a6 |) . J. ACM, Vol. 37, No. 4, Article 1. Publication date: September 2022. 1:14 Shen, et al. For a batch of query samples, the time complexity of RESUS becomes O ( \ud835\udc35 | \ud835\udc46 \ud835\udc62 | ( | \ud835\udc4a \u03a8 | + | \ud835\udc4a \u03a6 |)) , where \ud835\udc35 is the batch size. However, RESUS can be implemented in a user-based batch manner where the query samples of the same user correspond to one batch and the results of \u03a8 and \u03a6 are computed once within the batch. In this way, the batch time complexity of RESUS can be reduced to O (( \ud835\udc35 + | \ud835\udc46 \ud835\udc62 |) (| \ud835\udc4a \u03a8 | + | \ud835\udc4a \u03a6 |)) . In comparison, the existing meta-learning approaches for few-shot CTR prediction on cold users can be generally divided into two groups: few-shot user representation learning approaches (e.g., NLBA [44]) and optimization-based meta-learning approaches (e.g., MeLU [30]). The batch time complexities of the two groups of approaches are O (( \ud835\udc35 + | \ud835\udc46 \ud835\udc62 |)| \ud835\udc4a \u03a6 |) and O (( \ud835\udc35 + | \ud835\udc46 \ud835\udc62 |)| \ud835\udc4a \u03a8 |) , respectively. Since | \ud835\udc4a \u03a6 | and | \ud835\udc4a \u03a8 | are typically close, the time complexity of RESUS is the same as the existing meta-learning methods. However, since optimization-based meta-learning approaches need to perform error backpropagation to update the base-learner for every new task, they are empirically more time-consuming during test time (see the results in Table 6). 4.3.3 Comparison with Gradient Boosting. Gradient boosting [16] is a powerful ensemble technique that tries to convert weak learners to a strong one [52]. The idea of gradient boosting is to train multiple learners in an iterative fashion. Each learner attempts to fit the errors of its predecessor, and all the learners are then combined to give the final prediction results. Here we highlight the key differences between RESUS and gradient boosting. First and foremost, in RESUS, the residual user preference predictor used for one user-specific task can be viewed as an individual learner. Each learner tries to fit the residual preferences for a particular user on the query instances. This means different learners do not collaborate with each other. In gradient boosting, however, the predictions from all the learners are accumulated to deliver one output for an input instance. Second, while both the shared predictor \u03a8 and the residual user preference predictor \u039b contribute to the final prediction of an unlabeled instance, they are utilized for different purposes: \u03a8 learns the association from input feature vectors to output binary labels by exploiting global preference knowledge from collective users; \u039b refers to the support set to infer the user-specific residual preferences. In contrast, gradient boosting ensembles multiple weak learners with the same purpose, i.e., fitting the residual errors to form a strong learner. Third, gradient boosting performs optimization on the additive term and hence the learners are iteratively optimized. In contrast, RESUS is cast in the typical meta-learning framework and the parameters \u0398 \ud835\udc5a\ud835\udc52\ud835\udc61\ud835\udc4e is optimized via the end-to-end task-based training procedure. We also conduct the comparison experiment with gradient boosting in Section 5.4.", "5 EXPERIMENTS": "In this section, we conduct experiments to answer the following research questions: RQ1: How does our proposed RESUS approach perform on cold users compared with the state-ofthe-art methods? RQ2: How do different deigns of the key components in RESUS affect its performance? RQ3: How does RESUS perform compared with gradient boosting method? RQ4: How does RESUS perform when support instances are irrelevant to query instances? RQ5: What is the empirical time cost of RESUS?", "5.1 Experimental Settings": "5.1.1 Datasets. We experiment with three publicly available datasets. The statistics of the three datasets are summarized in Table 2. Note that some widely-used CTR prediction datasets like Criteo [2] and Avazu [1] are not applicable in our experiments since the records are anonymous J. ACM, Vol. 37, No. 4, Article 1. Publication date: September 2022. RESUS: Warm-Up Cold Users via Meta-Learning Residual User Preferences in CTR Prediction 1:15 Table 2. The statistics of the datasets. and cannot be organized into user-specific tasks. The details of the experimental datasets are as follows. \u00b7 Movielens [21]: This is the one million version of the Movielens dataset, which consists of users' ratings on movies. We convert the ratings to binary labels by setting a threshold 3, i.e., ratings \u2265 3 and ratings < 3 are marked as 1 and 0, respectively. It contains feature fields about users (i.e., user ID, age, gender and occupation) and about items (i.e., movie ID, genre and release year). \u00b7 Frappe [4]: This dataset contains app usage logs from users under different contexts (e.g., weekday, location) without timestamp information. We converted each log to a feature vector as input. The target value indicates whether the user has used the app in the context. \u00b7 Taobao [51]: This dataset contains ad display/click logs from an e-commercial website. It involves both user profiles (e.g., gender, age, and occupation) and item features (e.g., category, brand, price). The target value indicates whether the user has clicked the advertisement. For all the above datasets, we filter out items with fewer than 100 interactions during preprocessing to alleviate the effects from cold items, since we focus on the cold-user issue. 5.1.2 Evaluation Protocols. To evaluate CTR prediction performance on cold users, for all the datasets, we first randomly split them into training, validation and test sets by user ID with the ratio of 7 : 2 : 1. This ensures that the users in different data splits are non-overlapped. For Movielens and Taobao datasets, we sort each user's interactions in time order. For each user in the validation or test set, we use the first | \ud835\udc46 \ud835\udc62 | instances from the user's historical interactions \ud835\udc37 \ud835\udc62 to form the support set and leave the remaining ones as the query set for testing. As Frappe does not include timestamp information, we randomly sample | \ud835\udc46 \ud835\udc62 | instances from \ud835\udc37 \ud835\udc62 to form the support set for a test user \ud835\udc62 and use the remaining samples as the query set. To evaluate model performance on users with different degrees of coldness, we control the value of | \ud835\udc46 \ud835\udc62 | in meta-test tasks. At each time, we set a uniform value of | \ud835\udc46 \ud835\udc62 | for all the test users which is no greater than the upper limit \ud835\udf0f (we set \ud835\udf0f to be 30, 30 and 150 for Movielens, Frappe and Taobao, respectively). For Movielens and Frappe datasets, | \ud835\udc46 \ud835\udc62 | is chosen from { 1 , 2 , \u00b7 \u00b7 \u00b7 , 30 } . For Taobao dataset, since the dataset is unbalanced (i.e., most instances are negative), we run experiments for | \ud835\udc46 \ud835\udc62 | in { 10 , 20 , \u00b7 \u00b7 \u00b7 , 150 } . To better present the results, we divide each of the value sets equally into three cold-start stages: Cold Start-I/II/III and report the averaged test performance of all the approaches in each stage. Specifically, the three stages for Movielens and Frappe correspond to | \ud835\udc46 \ud835\udc62 | in { 1 , 2 , \u00b7 \u00b7 \u00b7 , 10 } , { 11 , 12 , \u00b7 \u00b7 \u00b7 , 20 } , { 21 , 22 , \u00b7 \u00b7 \u00b7 , 30 } , respectively. The three stages for Taobao correspond to | \ud835\udc46 \ud835\udc62 | in { 10 , 20 , \u00b7 \u00b7 \u00b7 , 50 } , { 60 , 70 , \u00b7 \u00b7 \u00b7 , 100 } , { 110 , 120 , \u00b7 \u00b7 \u00b7 , 150 } , respectively. We use Logloss and AUC as the metrics, which are widely-used for evaluating CTR prediction methods. Following previous work [48, 51], we further use \ud835\udc45\ud835\udc52\ud835\udc59\ud835\udc4e\ud835\udc3c\ud835\udc5a\ud835\udc5d\ud835\udc5f to measure the relative J. ACM, Vol. 37, No. 4, Article 1. Publication date: September 2022. 1:16 Shen, et al. improvement of AUC, as defined:  We ran each experiment for 10 times and reported the averaged results. 5.1.3 Comparison Methods. Weconsider two kinds of meta-learning approaches as the comparison methods:", "(1) Few-shot user representation learning methods:": "\u00b7 LWA [44]. This method encodes a user's interaction records (in the support set) into latent vectors and computes two embeddings by applying average pooling over the user's positive and negative latent vectors, respectively. It then computes a weighted sum over two embeddings to obtain the user representation and encodes all the features in a query instance into a latent vector as the item representation. The user and item representations are concatenated to predict the preference for the query instance via a linear classifier. \u00b7 NLBA [44]. This method is similar to LWA, but uses a non-linear classifier to model the interactions between user and item representations. \u00b7 JTCN [35]. This method first encodes representations of historical items with the dynamic routing-by-agreement mechanism of capsule networks, and then applies attentive aggregation to generate a fixed-length high-level user preference representation, which is later fused with embeddings of user features to output user representation. The predictions are made based on the user representation and the embeddings of query items. All the above three methods optimize model parameters via gradient descent, especially optimizing the encoders to generate user and item representations. They take both historical user behaviors in support set and the input features in query instance as input features to each CTR prediction. However, they encode historical user behaviors into one fixed-length latent vector, which may suffer from information loss. Note that in our RESUS approach, each historical user behavior in a support set contributes to the prediction of a query instance separately.", "(2) Optimization-based meta-learning methods:": "\u00b7 Meta-Embedding [39]. This is a state-of-the-art MAML-like method. It trains an item ID embedding generator based on item content features [15], so that the ID embeddings of cold items fine-tuned on a small number of support samples can perform well on the query set. In our experiments, we employ Meta-Embedding for cold users for comparison. \u00b7 MeLU [30]. This is another MAML-like method, which learns an initialization scheme for the base learner's parameters for fast adaption on a small number of historical interactions of cold users. \u00b7 MAMO [14]. This method is another state-of-the-art method which improves MeLU with memory-augmented networks. It designs task-specific memory and feature-specific memory to guide the model with personalized network parameter initialization. Note that the optimization-based meta-learning methods typically consume high training time and memory cost due to the bi-level optimization. Their goal is to generate a good initialization for base-learner's parameters. The parameters will be fine-tuned through gradient updates based on the support set. Therefore, while fine-tuning is beneficial to the predictions of query instances, it hurts inference time. In contrast, the base-learners (i.e., NN and RR predictors) in RESUS do not require gradient updates during test. Our proposed methods: J. ACM, Vol. 37, No. 4, Article 1. Publication date: September 2022. RESUS: Warm-Up Cold Users via Meta-Learning Residual User Preferences in CTR Prediction 1:17 \u00b7 RESUS \ud835\udc41\ud835\udc41 . This is our RESUS approach using the nearest-neighbor predictor as the baselearner. \u00b7 RESUS \ud835\udc45\ud835\udc45 . This is our RESUS approach using the ridge-regression predictor as the baselearner. Note that the basic metric-based meta-learning approach MUS described in Section 3 is equivalent to RESUS \ud835\udc41\ud835\udc41 without the shared predictor \u03a8 (i.e., always predicting zero). Hence, we report the results of MUS when evaluating the effects of \u03a8 in Section 5.3. 5.1.4 Implementation Details. We implemented our proposed methods based on Pytorch 1 . By default, we used DeepFM [19] as the architecture of the shared predictor \u03a8 and that of the feature encoder \u03a6 (excluding the final prediction layer). For LWA, NLBA, JTCN, Meta-Embedding, MeLU and MAMO, we employed the same DeepFM architecture as the interaction function for a fair comparison. For each dataset, Meta-Embedding includes all the available feature fields in its CTR instances. RESUS and the other baselines use all the feature fields except user ID as they use user ID to form meta-tasks. Following the original paper of Meta-Embedding, we pretrain its base model using \ud835\udc37 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b including user ID. We applied Adam optimizer [28] with an initial learning rate of 0 . 001 and a mini-batch size of 1024 by default. We set the dimension of input feature embeddings to 10 for all the comparison methods. To avoid overfitting, we performed early-stopping according to model's AUC on the validation set. We also tuned the hyperparameters of the baselines using the validation set. All the experiments were conducted on a Linux server equipped with Intel Xeon 2.10GHz CPUs and NVIDIA GeForce RTX 2080Ti GPUs.", "5.2 Performance Comparison (RQ1)": "Table 3 compares different methods on the Logloss and AUC performance over three public datasets. First, we can see that our proposed RESUS approaches achieve the best performance on almost all the cases. Exceptionally, in the Cold Start-I stage on Frappe, RESUS lags a little behind the best performing baseline Meta-Embedding on AUC. In fact, Meta-Embedding achieves better AUC performance than MeLU and MAMO in Cold Start-I on all the datasets. We conjecture the generated user embeddings based on user features acquire useful meta-knowledge that benefits CTR predictions on extremely cold users. We can also observe that the performance of our RESUS approaches is more stable over different datasets than the competitive optimization-based meta-learning methods. The reasons are two-fold. (1) RESUS performs metric-learning and does not rely on meta-learner to initialize base-learner's parameters or input embeddings. In contrast, optimization-based meta-learning methods easily suffer from poor initializations produced by the meta-learner. (2) The matching mechanism in metric-learning distinguishes the utility of each support instance w.r.t. a query instance. Specifically, RESUS can assign lower importance weights to less relevant or noisy support instances, while optimization-based meta-learning approaches treat all the support instances equally. Second, on average, RESUS \ud835\udc41\ud835\udc41 achieves the best performance on Frappe and Taobao, and it outperforms the most competitive baselines by achieving an average 1.8% and 5.0% improvements on \ud835\udc45\ud835\udc52\ud835\udc59\ud835\udc4e\ud835\udc3c\ud835\udc5a\ud835\udc5d\ud835\udc5f of AUC, respectively. RESUS \ud835\udc45\ud835\udc45 performs best on Movielens and achieves an average 2.9% improvement on \ud835\udc45\ud835\udc52\ud835\udc59\ud835\udc4e\ud835\udc3c\ud835\udc5a\ud835\udc5d\ud835\udc5f of AUC, compared with the most competitive baseline MAMO. The results demonstrate that: (1) the decoupling of basis user preferences and residual user preferences contribute to the final prediction performance on cold users; (2) both of our proposed base-learners (i.e., NN and RR predictors) are feasible, which obtain similar performance improvements over the existing meta-learning approaches. Third, we observe that optimization-based meta-learning methods generally perform better than few-shot 1 Our code is available at https://github.com/WeiyuCheng/RESUS J. ACM, Vol. 37, No. 4, Article 1. Publication date: September 2022. 1:18 Shen, et al. Table 3. Performance comparison on three datasets, where the bold values are the best results and the underlined values are the two most competitive results (RQ1). * indicates a statistically significant level \ud835\udc5d -value<0.05 comparing RESUS with the best baseline. user representation learning methods, showing the advantages of performing fine-tuning with user-specific data on new tasks. Note that the Logloss results may not be always consistent with the AUC results because AUC is less sensitive to anomalies and more closely related to model's online ranking performance than Logloss. Fourth, among the three optimization-based meta-learning methods, Meta-Embedding generally performs worse than MeLU and MAMO in the Cold StartII/III stages. Meta-Embedding only refines user embeddings (via optimizing the parameters of the embedding generator) based on support sets. On the contrary, MeLU and MAMO finetune all the model parameters based on support sets and hence benefit more from larger support sets. Finally, regarding the three stages, almost all the methods achieve higher AUC and lower Logloss on larger support sets. This is reasonable because more support samples shall contribute to more accurate user preference estimations. However, few-shot representation learning methods gain little performance improvement in latter stages. The reason is that they apply average pooling, attention mechanism or capsule clustering to encode support instances into a condensed user representation vector, thus risking information loss and yielding suboptimal performance.", "5.3 Effects of Key Components (RQ2)": "In this section, we conduct experiments to evaluate the key components in RESUS, including (i) the effects of the shared predictor, (ii) the effects of pretraining the shared predictor, and (iii) the effects of the metric-learning counterpart. J. ACM, Vol. 37, No. 4, Article 1. Publication date: September 2022. RESUS: Warm-Up Cold Users via Meta-Learning Residual User Preferences in CTR Prediction 1:19 Table 4. Effects of the shared predictor \u03a8 and pretraining, where the bold values are the best results and the underlined values are the most competitive results(RQ2). * indicates a statistically significant level \ud835\udc5d -value<0.05 comparing RESUS with RESUS (w/o pretrain). First, we remove the shared preference predictor \u03a8 by always treating its predictions as zero, and perform meta-learning to infer user preferences. The resultant method is exactly our basic metricbased meta-learning approach MUS (in Section 3). Table 4 shows the results of MUS and RESUS with two different base-learners (i.e., \ud835\udc41\ud835\udc41 , \ud835\udc45\ud835\udc45 ) on three datasets. We can see that RESUS outperforms MUS by a large margin. On average, RESUS \ud835\udc41\ud835\udc41 (RESUS \ud835\udc45\ud835\udc45 ) achieves 84.6% (85.8%), 47.4% (43.3%) and 32.5% (32.0%) \ud835\udc45\ud835\udc52\ud835\udc59\ud835\udc4e\ud835\udc3c\ud835\udc5a\ud835\udc5d\ud835\udc5f of AUC on Movielens, Frappe and Taobao datasets, respectively. This is because MUS fails to capture the global preference knowledge among different users and the performance suffers from the sparse and insufficient user historical interactions in the support set. Second, we train the shared predictor \u03a8 together with the other components in RESUS rather than pretrain it. Specifically, the parameters in \u03a8 can be viewed as meta-parameters which are optimized with the paremeters in \u03a6 and \u039b over meta-train tasks. The resultant method is denoted as RESUS (w/o pretrain). Table 4 shows the effects of two training algorithms applied on \u03a8 . In general, RESUS achieves better or comparable performance than RESUS (w/o pretrain). The reason is that the randomly initialized shared predictor \u03a8 may introduce noises to the target residual user preferences (in Eq. (7)). This can sometimes lead to serious problems to the optimization of the residual user preference predictor \u039b . For example, in the stages of Cold Start-I/II on Taobao, performance degrades significantly without pretraining the shared predictor. It is also interesting to see that in most cases, RESUS (w/o pretrain) achieves better or comparable performance to the most competitive baselines (referring to Table 3), and it even outperforms the best performing method, Meta-Embedding, in the Cold Start-I stage on Frappe. These results further confirm the significance of explicitly learning global preference knowledge in addition to residual user preferences. Third, we remove the residual prediction counterpart and debase RESUS to the shared predictor \u03a8 . We use 6 different architectures to implement \u03a8 , i.e., logistic regression (LR), factorization machine (FM), Wide&Deep [12], DeepFM [19], xDeepFM [34] and LightGBM [27]. Since the base predictor is trained with training samples from different users, it is able to learn global preference J. ACM, Vol. 37, No. 4, Article 1. Publication date: September 2022. 1:20 Shen, et al. Table 5. Effects of different architectures of \u03a8 and \u03a6 and gradient boosting on Movielens, where the bold values are the best results and the underlined values are the most competitive results (RQ2&RQ3). All the improvements of RESUS are statistically significant with \ud835\udc5d -value < 0.01. knowledge including global item characteristics that are discriminative to the prediction results. We report the performance of RESUS using the same architecture for the shared predictor \u03a8 and the encoder \u03a6 , except for RESUS (LightGBM) that employs DeepFM as the encoder. Table 5 provides the results on Movielens, while the same conclusions can be drawn on the other datasets. We have three important observations. (1) RESUS improves the prediction performance significantly by augmenting global preferences with user-specific residual preferences. The AUC performance of RESUS increases with larger support sets. The shared predictor performs the worst on all the cases, which confirms the limitation of supervised learning on cold users. (2) Different architectures of \u03a8 and \u03a6 effect the performance of RESUS. In general, a better architecture of \u03a8 benefits the final performance of RESUS. For example, on average, xDeepFM outperforms LR by 0.5% \ud835\udc45\ud835\udc52\ud835\udc59\ud835\udc4e\ud835\udc3c\ud835\udc5a\ud835\udc5d\ud835\udc5f of AUC, and RESUS \ud835\udc45\ud835\udc45 (xDeepFM) outperforms RESUS \ud835\udc45\ud835\udc45 (LR) by 0.2% \ud835\udc45\ud835\udc52\ud835\udc59\ud835\udc4e\ud835\udc3c\ud835\udc5a\ud835\udc5d\ud835\udc5f of AUC. Among all the compared architectures, LightGBM achieves the best performance. As a result, RESUS \ud835\udc41\ud835\udc41 and RESUS \ud835\udc45\ud835\udc45 based on LightGBM also achieve the best Logloss and AUC results. These results verify the generality of RESUS, encouraging more advanced CTR prediction models to be incorporated into RESUS. (3) The relative performance improvements achieved by RESUS are quite stable over different shared predictors. This is because the metric-learning counterpart in RESUS utilizes the shared predictor to derive representations of feature vectors in support and query instances and it also benefits from stronger shared predictors.", "5.4 Comparison with Gradient Boosting (RQ3)": "We compare RESUS with the gradient boosting technique over different architectures of \u03a8 . For gradient boosting, we first train a shared predictor \u03a8 with the training set (i.e., \ud835\udc37 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b ), and then J. ACM, Vol. 37, No. 4, Article 1. Publication date: September 2022. RESUS: Warm-Up Cold Users via Meta-Learning Residual User Preferences in CTR Prediction 1:21 train a subsequent predictor with the same structure as \u03a8 to fit the residual errors on the support sets of all the test users. After training, the outputs of the two predictors on the query instances for the test users are added to make the final predictions. Table 5 provides the results on Movielens and the same conclusions can be drawn on the other datasets. From the results, we can see that gradient boosting can improve the AUC of the shared predictor on all the cases. On average, gradient boosting achieves 0.3%, 0.2%, 1.1%, 0.6% and 0.4% \ud835\udc45\ud835\udc52\ud835\udc59\ud835\udc4e\ud835\udc3c\ud835\udc5a\ud835\udc5d\ud835\udc5f of AUC for LR, FM, Wide&Deep, DeepFM and xDeepFM, respectively. We can also see that RESUS outperforms gradient boosting significantly over all the architectures of \u03a8 . This indicates that the performance gain of RESUS comes from capturing global preference knowledge and user-specific residual preferences separately instead of model ensemble. Besides, we notice that gradient boosting incurs higher Logloss than the shared predictor while improving AUC performance. This is because boosting methods aim at improving classification accuracy and may not necessarily lead to lower Logloss results.", "5.5 Robustness Study (RQ4)": "0.0695 0.0856 0.0914 0.1004 0.2661 Average Euclidean distance to support instances 0.300 0.325 0.350 0.375 0.400 0.425 0.450 0.475 LogLoss MUS Meta-Embedding RESUS NN RESUS RR (a) Logloss, Movielens 0.0099 0.0127 0.0148 0.0175 0.0250 Average Euclidean distance to support instances 0.142 0.144 0.146 0.148 0.150 0.152 LogLoss MUS Meta-Embedding RESUS NN RESUS RR (c) Logloss, Taobao (b) AUC, Movielens 0.0695 0.0856 0.0914 0.1004 0.2661 Average Euclidean distance to support instances 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 AUC MUS Meta-Embedding RESUS NN RESUS RR Fig. 4. Robustness study results on different support sets (RQ4). 0.0099 0.0127 0.0148 0.0175 0.0250 Average Euclidean distance to support instances 0.54 0.56 0.58 0.60 0.62 0.64 AUC MUS Meta-Embedding RESUS NN RESUS RR (d) AUC, Taobao As support sets may involve noises or unrelated historical interactions to the predictions of query instances, we now evaluate the robustness of RESUS on support sets with different noise levels. In this experiment, we prepare 5 meta-test sets with different support sets but the same query sets. Specifically, for each test user \ud835\udc62 , we sort the historical interactions in time order and consider the first \ud835\udf0f interactions as the temporary support set . The remaining interactions are formed into the query set \ud835\udc44 \ud835\udc62 . For each query instance \ud835\udc65 \ud835\udc44 \ud835\udc62 \u2208 \ud835\udc44 \ud835\udc62 , we divide \ud835\udc46 \ud835\udc62 into 5 equal-sized groups with different noise levels. Since we cannot recognize noisy support instances with respect to the query instance \ud835\udc65 \ud835\udc44 \ud835\udc62 , we compute Euclidean distance between \ud835\udc65 \ud835\udc44 \ud835\udc62 with every support instance in \ud835\udc46 \ud835\udc62 based on their encoded vector representations. We use the pretrained shared predictor (i.e., DeepFM J. ACM, Vol. 37, No. 4, Article 1. Publication date: September 2022. 1:22 Shen, et al. excluding the last prediction layer) to encode instances which is independent of the meta-learning approaches. We then sort the support instances in \ud835\udc46 \ud835\udc62 in ascending order based on the distances and divide them into 5 equal-sized groups, i.e., \ud835\udc46 \ud835\udc62 = \ud835\udc46 \ud835\udc62, 1 \u222a \u00b7 \u00b7 \u00b7 \u222a \ud835\udc46 \ud835\udc62, 5. Intuitively, larger distances imply the support instances are less relevant to the query instance, which would compromise the effectiveness of the matching mechanism in metric-learning. Hence, we regard the groups with larger distances (e.g., \ud835\udc46 \ud835\udc62, 5) as support sets with higher noise levels to the query instance. By aggregating the groups from all the query instances by the noise level, we obtain 5 meta-test sets that share the same query sets but the corresponding support sets are disjoint and in different noise levels. Note that all the derived support sets have the same size, i.e., \ud835\udf0f / 5 support instances, corresponding to the CTR prediction tasks on extremely cold users. Figure 4 shows the Logloss and AUC performance of MUS, RESUS and Meta-Embedding on different meta-test sets from Movielens and Taobao. We exclude the results on Frappe dataset which does not ensure the time order between support sets and query sets. The x-axis reports the average Euclidean distance from query instances to support instances for each of the metatest sets. According to Table 3, Meta-Embedding is the best performing baseline in Cold Start-I. We can observe that RESUS consistently outperforms Meta-Embedding and its performance is stable over the meta-test sets with different noise levels on two datasets. The AUC performance of RESUS slightly decreases with larger distances because the matching mechanism in metric-learning can be affected by the similarity between support and query instances. Nevertheless, compared with MUS, RESUS is much more robust to support sets with little utility (i.e., meta-test sets with larger distances).", "5.6 Time Cost Study (RQ5)": "We evaluate the empirical time cost of different methods. Table 6 reports the training time t \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b and testing time t \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 using Movielens dataset. The relative performance on training and test time is the same on Frappe and Taobao, and we eliminate the results to avoid redundancy. In Table 6, it is easy to see that our RESUS approaches achieve the highest training efficiency in terms of t \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b . The main reason is that the NN/RR predictor in RESUS does not require gradient-based fine-tuning on every task. The training time of optimization-based meta-learning methods is generally higher than that of few-shot user preference learning methods. Besides, RESUS approaches report the lowest testing time, which can perform inference as fast as few-shot user representation learning methods. This is a desirable property in real large-scale recommender systems. We also observe Table 6. Empirical time cost (in second) comparison on Movielens, where the pretraining time cost (in second) is provided in brackets (RQ5). J. ACM, Vol. 37, No. 4, Article 1. Publication date: September 2022. RESUS: Warm-Up Cold Users via Meta-Learning Residual User Preferences in CTR Prediction 1:23 Fig. 5. Converge speed of different models on Movielens (RQ5). \u0000 \u0000 \u0000 \u0000 \u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 NN \u0000\u0000\u0000\u0000\u0000 RR that the training and testing efficiency of the same method may not be consistent. For example, Meta-Embedding reports higher t \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b than MeLU due to more training epochs, but takes lower t \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 than MeLU. In practice, the performance on t \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 is much more crucial since it directly affects online serving latency and system throughput. Figure 5 shows the AUC results on the validation set over training epochs. We perform early stopping when the number of epochs reaches 10 or the AUC validation performance is getting worse for two consecutive epochs. We can see that LWA, NLBA, JTCN, MeLU, RESUS \ud835\udc41\ud835\udc41 and RESUS \ud835\udc45\ud835\udc45 converge quickly, but MAMO and Meta-Embedding take more training epochs till convergence. The reason is that MAMO starts with randomly initialized memories which needs more updates. Meta-Embedding simultaneously minimizes the prediction error and the mimic loss for fast adaptation, which requires more adjustments for optimal balance between two objectives.", "6 RELATED WORK": "Our paper is related with works on cold-start recommendation. While there can be user cold-start and item cold-start problems, most of the proposed methods can be applied to either problem. We thus describe the works from the perspective of user cold-start recommendation. Existing literature focused on two types of cold users: completely-cold users that have no interaction data available, and cold users that have very limited historical behavior data. For the first type, many researches [5, 20, 32, 47] relied on the side information to learn the representation of a target user. For the second type, some studies considered the collaborative filtering (CF) setting where only the user-item interaction or rating matrix is available, and proposed data imputation techniques to fill in empty entries in the extremely sparse rows/columns. For example, zero-injection [26, 31] was proposed to find uninteresting items and then inject zero ratings to them as negative samples. Recent works [10, 11] employed generative adversarial networks to generate plausible ratings to impute the sparse matrix. Different from the above works, our work aims to improve CTR prediction performance on cold users with very few historical behaviors. To address the problem, some works exploited external user information such as social networks [24, 36] and cross-domain user behaviors [3, 8, 9, 17, 23, 38, 49]. For example, Hu et al. [24] proposed to build graph neural networks based on users' social relations and enhance cold user representations by propagating information from neighboring users. Nazari et al. [8] proposed to recommend podcasts for cold users by using their music consumption behaviors. These methods, however, rely on specific types of external user knowledge, which may not always be available in the context of CTR prediction. J. ACM, Vol. 37, No. 4, Article 1. Publication date: September 2022. 1:24 Shen, et al. When no external user information is accessible, recent works formulated the user cold-start problem as few-shot learning tasks and developed meta-learning approaches, which can be generally categorized into two groups. The first group is referred to as few-shot user representation learning methods [33, 35, 44, 46]. The key idea is to absorb the limited historical behaviors of the target cold user and generate a fixed-length vector as the user representation, which can be achieved by a parameterized function. The parameters are optimized via episode-based training in order to learn transferrable knowledge across different tasks (i.e,. cold users). Due to the varying number of historical interactions among different users, these methods generate user representations by applying average [44, 46], attention-based [33], or capsule clustering-based [35] pooling methods on the encoded historical interactions of each user, thus risking information loss and yielding suboptimal performance [18, 40, 50] . The second group includes optimization-based meta-learning approaches [7, 14, 30, 37, 39, 53]. They typically followed the idea of MAML [15], and aimed to learn an initialization scheme for the parameters of the prediction model. For example, Pan et al. [39] proposed to learn an ID embedding generator for cold users, which outputs initial user embeddings based on user-specific features. The initialized user embedding is expected to converge to a good point using very few user's interaction data. Lee et al. [30] proposed to learn the initialization scheme of the parameters of neural networks, where each network performs predictions for a particular cold user. The main drawback of optimization-based meta-learning approaches is the high computation and memory cost caused by the calculation of high-order derivatives in bilevel optimization process and the iterative fine-tuning for every new task [22, 25]. In contrast, our RESUS approach employs two efficient base-learners: the nearest-neighbor predictor and the ridge-regression predictor with closed form solutions, which avoids expensive fine-tuning during both training and test time. Besides, RESUS employs conventional CTR prediction methods such as DeepFM [19] to explicitly capture global preference knowledge, and our research is thus orthogonal to such supervised-learning methods. Furthermore, we have empirically demonstrated the effectiveness and efficiency of RESUS compared with various existing meta-learning approaches.", "7 CONCLUSIONS AND FUTURE WORK": "This paper has introduced a generic decoupled preference learning framework named RESUS for CTR prediction on cold users. There are two key insights in this work. First, we propose to decouple user preferences into two parts, namely basis user preferences and residual user preferences. The former is inferred by a shared predictor based on input feature vectors of CTR instances. We train the shared predictor in a fully supervised manner such that it acquires global preference knowledge from collective users. The latter is inferred based on a few user-specific historical interactions through the matching mechanism in metric-learning. The two parts actually complement each other and RESUS can work well when input features are informative to the prediction or user's historical interactions are useful to transfer label information to the prediction through matching. Second, we customize meta-learning to our few-shot CTR prediction problem and devise two kinds of base-learners which are efficient and easy to optimize. Extensive experiments on three public datasets have demonstrated that our proposed RESUS approach is generic and efficient, achieving the state-of-the-art CTR prediction performance on cold users. To these ends, we believe metric-learning is a promising research direction for cold-start recommendation. Our work can be extended in multiple directions. In this paper, we fuse the basis and residual user preferences via a simple weighted sum. It is interesting to see how different fusing mechanisms (e.g., the gating mechanism) would affect the final prediction accuracy. Typically, in commercial recommender systems, new interaction data is collected continuously, and a cold user would become warmer gradually. How to adapt RESUS to the change in user coldness is another interesting direction. J. ACM, Vol. 37, No. 4, Article 1. Publication date: September 2022. RESUS: Warm-Up Cold Users via Meta-Learning Residual User Preferences in CTR Prediction 1:25", "ACKNOWLEDGEMENTS": "The authors would like to thank the anonymous reviewers for their insightful reviews. This work is supported by the National Key Research and Development Program of China (2022YFE0200500), Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102), the Tencent Wechat Rhino-Bird Focused Research Program, and SJTU Global Strategic Partnership Fund (2021 SJTUHKUST).", "REFERENCES": "[1] 2014. Avazu Dataset . https://www.kaggle.com/c/avazu-ctr-prediction [2] 2014. Criteo Dataset . https://www.kaggle.com/c/criteo-display-ad-challenge [3] Karan Aggarwal, Pranjul Yadav, and S. Sathiya Keerthi. 2019. Domain adaptation in display advertising: an application for partner cold-start. In RecSys . 178-186. [4] Linas Baltrunas, Karen Church, Alexandros Karatzoglou, and Nuria Oliver. 2015. Frappe: Understanding the usage and perception of mobile app recommendations in-the-wild. arXiv preprint arXiv:1505.03014 (2015). [5] Oren Barkan, Noam Koenigstein, Eylon Yogev, and Ori Katz. 2019. CB2CF: a neural multiview content-to-collaborative filtering model for completely cold item recommendations. In RecSys . 228-236. [6] Luca Bertinetto, Jo\u00e3o F. Henriques, Philip H. S. Torr, and Andrea Vedaldi. 2019. Meta-learning with differentiable closed-form solvers. In ICLR . [7] Homanga Bharadhwaj. 2019. Meta-Learning for User Cold-Start Recommendation. In IJCNN . 1-8. [8] Ye Bi, Liqiang Song, Mengqiu Yao, Zhenyu Wu, Jianming Wang, and Jing Xiao. 2020. A Heterogeneous Information Network based Cross Domain Insurance Recommendation System for Cold Start Users. In SIGIR . 2211-2220. [9] Da Cao, Xiangnan He, Liqiang Nie, Xiaochi Wei, Xia Hu, Shunxiang Wu, and Tat-Seng Chua. 2017. Cross-Platform App Recommendation by Jointly Modeling Ratings and Texts. ACM Trans. Inf. Syst. 35, 4, Article 37 (July 2017), 27 pages. https://doi.org/10.1145/3017429 [10] Dong-Kyu Chae, Jin-Soo Kang, Sang-Wook Kim, and Jaeho Choi. 2019. Rating Augmentation with Generative Adversarial Networks towards Accurate Collaborative Filtering. In TheWebConf . 2616-2622. [11] Dong-Kyu Chae, Jihoo Kim, Duen Horng Chau, and Sang-Wook Kim. 2020. AR-CF: Augmenting Virtual Users and Items in Collaborative Filtering for Addressing Cold-Start Problems. In SIGIR . 1251-1260. [12] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and Hemal Shah. 2016. Wide & Deep Learning for Recommender Systems. In Proceedings of the 1st Workshop on Deep Learning for Recommender Systems, DLRS@RecSys . 7-10. [13] Weiyu Cheng, Yanyan Shen, and Linpeng Huang. 2020. Adaptive Factorization Network: Learning Adaptive-Order Feature Interactions. In AAAI . 3609-3616. [14] Manqing Dong, Feng Yuan, Lina Yao, Xiwei Xu, and Liming Zhu. 2020. MAMO: Memory-Augmented Meta-Optimization for Cold-start Recommendation. In SIGKDD . 688-697. [15] Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. In ICML . 1126-1135. [16] Jerome H Friedman. 2001. Greedy function approximation: a gradient boosting machine. Annals of statistics (2001), 1189-1232. [17] Wenjing Fu, Zhaohui Peng, Senzhang Wang, Yang Xu, and Jin Li. 2019. Deeply Fusing Reviews and Contents for Cold Start Users in Cross-Domain Recommendation Systems. In AAAI . 94-101. [18] Hossein Gholamalinezhad and Hossein Khosravi. 2020. Pooling Methods in Deep Neural Networks, a Review. arXiv preprint arXiv:2009.07485 (2020). [19] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: A Factorization-Machine based Neural Network for CTR Prediction. In IJCAI . 1725-1731. [20] Casper Hansen, Christian Hansen, Jakob Grue Simonsen, Stephen Alstrup, and Christina Lioma. 2020. Content-aware Neural Hashing for Cold-start Recommendation. In SIGIR . 971-980. [21] F. Maxwell Harper and Joseph A. Konstan. 2016. The MovieLens Datasets: History and Context. ACM Trans. Interact. Intell. Syst. 5, 4 (2016), 19:1-19:19. [22] Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. 2020. Meta-learning in neural networks: A survey. arXiv preprint arXiv:2004.05439 (2020). [23] Liang Hu, Longbing Cao, Jian Cao, Zhiping Gu, Guandong Xu, and Dingyu Yang. 2016. Learning Informative Priors from Heterogeneous Domains to Improve Recommendation in Cold-Start User Domains. ACM Trans. Inf. Syst. 35, 2, Article 13 (Dec. 2016), 37 pages. https://doi.org/10.1145/2976737 J. ACM, Vol. 37, No. 4, Article 1. Publication date: September 2022. 1:26 Shen, et al. J. ACM, Vol. 37, No. 4, Article 1. Publication date: September 2022. RESUS: Warm-Up Cold Users via Meta-Learning Residual User Preferences in CTR Prediction 1:27 [51] Guorui Zhou, Xiaoqiang Zhu, Chengru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep Interest Network for Click-Through Rate Prediction. In SIGKDD . 1059-1068. [52] Zhi-Hua Zhou. 2019. Ensemble methods: foundations and algorithms . Chapman and Hall/CRC. [53] Yongchun Zhu, Ruobing Xie, Fuzhen Zhuang, Kaikai Ge, Ying Sun, Xu Zhang, Leyu Lin, and Juan Cao. 2021. Learning to Warm Up Cold Item Embeddings for Cold-start Recommendation with Meta Scaling and Shifting Networks. arXiv preprint arXiv:2105.04790 (2021). J. ACM, Vol. 37, No. 4, Article 1. Publication date: September 2022."}
