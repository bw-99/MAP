{"ADSNet: Cross-Domain LTV Prediction with an Adaptive Siamese Network in Advertising": "Ruize Wang Tencent Inc. Shanghai, China rayrzwang@tencent.com Hui Xu Tencent Inc. Shanghai, China peterhxu@tencent.com Ying Cheng \u2217 School of Computer Science Fudan University Shanghai, China chengy18@fudan.edu.cn Qi He Tencent Inc. Shanghai, China nickyhe@tencent.com Wei Xu Tencent Inc. Shanghai, China davidxu@tencent.com Xing Zhou Tencent Inc. Shanghai, China leostarzhou@tencent.com Lei Huang \u2217 Tencent Inc. Shenzhen, China leihuang@tencent.com", "ABSTRACT": "Advertising platforms have evolved in estimating Lifetime Value (LTV) to better align with advertisers' true performance metric which considers cumulative sum of purchases a customer contributes over a period. Accurate LTV estimation is crucial for the precision of the advertising system and the effectiveness of advertisements. However, the sparsity of real-world LTV data presents a significant challenge to LTV predictive model(i.e., pLTV), severely limiting the their capabilities. Therefore, we propose to utilize external data, in addition to the internal data of advertising platform, to expand the size of purchase samples and enhance the LTV prediction model of the advertising platform. To tackle the issue of data distribution shift between internal and external platforms, we introduce an Adaptive Difference Siamese Network (ADSNet), which employs cross-domain transfer learning to prevent negative transfer. Specifically, ADSNet is designed to learn information that is beneficial to the target domain. We introduce a gain evaluation strategy to calculate information gain, aiding the model in learning helpful information for the target domain and providing the ability to reject noisy samples, thus avoiding negative transfer. Additionally, we also design a Domain Adaptation Module as a bridge to connect different domains, reduce the distribution distance between them, and enhance the consistency of representation space distribution. We conduct extensive offline experiments and online A/B KDD '24, August 25-29, 2024, Barcelona, Spain https://doi.org/10.1145/3637528.3671612 Rui Feng School of Computer Science Fudan University Shanghai, China fengrui@fudan.edu.cn Jie Jiang Tencent Inc. Shenzhen, China zeus@tencent.com tests on a real advertising platform. Our proposed ADSNet method outperforms other methods, improving GINI by 2%. The ablation study highlights the importance of the gain evaluation strategy in negative gain sample rejection and improving model performance. Additionally, ADSNet significantly improves long-tail prediction. The online A/B tests confirm ADSNet's efficacy, increasing online LTV by 3.47% and GMV by 3.89%.", "CCS CONCEPTS": "\u00b7 Information systems \u2192 Computational advertising ; Online advertising .", "KEYWORDS": "Lifetime Value Prediction, Adaptive Cross-Domain Transfer Learning, Computational Advertising", "ACMReference Format:": "Ruize Wang, Hui Xu, Ying Cheng, Qi He, Xing Zhou, Rui Feng, Wei Xu, Lei Huang, and Jie Jiang. 2024. ADSNet: Cross-Domain LTV Prediction with an Adaptive Siamese Network in Advertising. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '24), August 25-29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3637528.3671612", "1 INTRODUCTION": "The Lifetime Value (LTV) in an advertising system is defined as the cumulative sum of purchases (i.e., revenue for advertisers) a customer contributes over a given period. Given its direct correlation with return on investment (ROI), this metric attracts significant attention from advertisers. Consequently, advertising platforms have progressively evolved to support LTV estimation [2, 21, 51], thereby aligning more closely with the assessment requirements of advertisers. Accurate estimation of LTV plays a crucial role in improving the precision of advertising systems and ensuring the effectiveness of advertisements. Advertising Conversion Funnel Exposure Click Activation Purchase (a) Sparsity of purchase samples Internal Data Source External Data Distribution LTV LTV Probability Probability (b)\u00a0Expanding Sample Size with External Data Transfer (c) Domain Shift Model Parameter Domain Negative Gain external internal Recently, some efforts have been made to improve the performance of LTV estimation. For example, Wang et al. [44] propose to model LTV as zero-inflated lognormal (ZILN) distribution to address heavy-tailed problem. Some methods [5, 36, 47] propose to model user behaviors and use a feature missing-aware network to reduce the effect of the missing features while training[5, 36, 47]. However, as purchase is close to the end of the advertising conversion funnel, data sparsity becomes an issue as conversion behavior deepens. This sparsity of real-world LTV data presents a formidable challenge to LTV predictive models, severely limiting their capabilities and having received far less attention. To address this problem, it is essential to leverage abundant external data to enhance the LTV prediction model of advertising platforms. Cross-domain transfer learning [18, 29, 55] has emerged as a promising approach to bridge the gap between different domains, particularly when there is a sparsity of labeled data in the target domain. This approach has been successfully applied in various fields, such as computer vision, natural language processing, and recommender/adverisiting systems. Some methods focus on transferring knowledge from the data perspective through adjustment and transformation of samples and features, including instance weighting[19, 49], feature transformation[7, 22, 41, 46]. Multi-domain learning approaches [32, 35] propose to mix multiple sources of data for training a unified model in a multi-task manner. However, existing cross-domain transfer learning methods often suffer from negative transfer, which occurs when knowledge of the source domain is not beneficial or even harmful to the learning process of the target domain. Therefore, it is critical to develop a more robust and adaptive cross-domain transfer learning method to mitigate the impact of negative transfer. In this paper, we propose an A daptive D ifference S iamese Network ( ADSNet ) to address the challenges associated with LTV estimation and cross-domain transfer learning. ADSNet employs a gain evaluation strategy based on a pseudo-siamese structure, effectively learning beneficial information for the target domain while rejecting noisy samples and avoiding negative transfer. Furthermore, our Domain Adaptation Module bridges different domains, reducing distribution distance and fostering consistent representation space distribution. We conduct extensive experiments and perform online A/B tests in a real online advertising scenario. Experimental results demonstrate that our approach significantly improves performance on the LTV prediction dataset. Further analysis highlights the effectiveness of our model in rejecting negative gain samples and improving long-tail prediction capabilities. The contributions of this paper can be summarized as follows: \u00b7 We introduce a novel approach to address data sparsity by integrating external data with the internal data from the advertising system for LTV estimation, utilizing a cross-domain transfer framework. \u00b7 We propose an Adaptive Difference Siamese Network (ADSNet) to tackle negative transfer. Utilizing a pseudo-siamese structure in conjunction with a gain evaluation strategy, we facilitate the assimilation of beneficial external information into the target domain while effectively filtering out noise. Additionally, we incorporate a domain adaptation module to promote consistency across different domains. \u00b7 Extensive experiments reveal that ADSNet surpasses other models, substantially enhancing performance and mitigating negative transfer. Additionally, ADSNet effectively improves long-tail prediction. Online A/B tests further showcase the practical advantages of ADSNet in real-world advertising systems.", "2 RELATED WORK": "", "2.1 Customer Lifetime Value Prediction in Advertising": "Customer Lifetime Value (LTV) prediction has become an integral component of modern advertising platforms, owing to its direct impact on the effectiveness of advertisement placements and overall advertising system precision. Previous studies have focused on various approaches to improve the accuracy of LTV estimation. Some previous works [5, 13, 36] explore deep learning methods for LTV prediction, emphasizing the potential of neural networks in understanding complex user behaviors. Reddy et al. [31] present a comparative analysis of traditional statistical methods versus machine learning techniques for LTV estimation. Addressing the issue of missing features in real-world advertising scenarios, MarfNet [47] proposes a feature missing-aware network designed to mitigate the impact of these missing features during training, rather than fill them with default values. Due to LTV of customers follows a heavy long-tailed distribution with significant fraction of zero value, Wang et al. [44] proposes to model the LTV as a zero-inflated lognormal (ZILN) distribution which is described as a mixture of zero-point mass and lognormal distribution, to address the heavytailed problem. In this modeling manner, the model fits the mean and deviation of the distribution. But the distribution assumption is simple and does not meet the multimodal distribution in real scenarios, which leads to limitations. ODMN [21] introduces an approach that partitions the complex LTV distribution into several sub-distributions, each trained by distribution experts. In the conversion funnel ' exposure->click->activation->purchase ', compared to the CTR ' exposure->click ' and CVR ' click->activation ' prediction, LTV prediction is a more challenging problem due to purchase being the deepest behavior, the data on LTV is very sparse. Although the aforementioned methods obtain better performance LTV LTV Output Shared Structure Reject Accept Gain Evaluation Strategy LTV Vanilla Network Gain Network Input Target Domain Source Domain Domain Shift (b)\u00a0Cross-domain Transfer Learning with Adaptive Difference Siamese Network (ADS-Net) Internal Data External Data Cross Domain Domain Adaption LTV LTV Output LTV Mixer Network Target Domain Source Domain Domain Shift Internal Data External Data Cross Domain Accept (a) Multi-domain Joint Learning Input on LTV prediction, they struggle with the sparsity of real-world LTV data. The amount of data determines the upper limit of the model's performance [1, 37, 39, 45]. Motivated by such constraints, we utilize external data from sources other than the advertising platform's internal data, which aims to increase the number of purchase samples and improve the LTV prediction model of the advertising platform.", "2.2 Cross-domain Transfer Learning": "Cross-domain transfer learning has emerged as a powerful approach to leverage knowledge from one domain to improve learning in another, especially in scenarios where the target domain suffers from data sparsity. However, a significant challenge in this field is the occurrence of negative transfer[18, 23, 29], where irrelevant or noisy information from the source domain hinders the learning process in the target domain, a key challenge in this area. The concept of domain transfer has also been explored in the context of multi-task learning in Click-Through Rate (CTR) and Conversion Rate (CVR) prediction scenarios[25, 40]. PLE [35] basically follows the gate structure and attention network for information transfer, similar to MoEs [17, 24, 53]. It separates task-sharing and task-specific parameters to learn shared and private representations for each task explicitly, and introduces a progressive routing manner. STAR [32] leverages partitioned normalization (PN) to privatize normalization for examples from different domains, and consists of shared centered parameters and domain-specific parameters, adaptively modulating its parameters conditioned on the domain for more refined prediction. CCTL [52] introduces a framework to mitigate the effects of negative transfer for different business domains in CTR prediction scenario. It evaluates the information gain of the source domain on the target domain using a symmetric companion network. Compared to CCTL, ADSNet focuses specifically on LTV prediction and employs a gain evaluation strategy to calculate the information gain and reject noisy samples. Additionally, ADSNet introduces a domain adaptation module to reduce the distribution distance between different domains and enhance the consistency of representation space distribution.", "3 TASK DEFINITION": "Definition 1 (Lifetime Value). Lifetime Value (LTV) in an advertising system is defined as the cumulative sum of purchases (i.e., revenue for advertisers) a customer contributes over a certain period. This metric is crucial for advertisers in assessing their return on investment (ROI). Definition 2 (Customer LTV Prediction). Within the context of an advertising system, the goal of the LTV prediction task is to estimate the LTV \ud835\udc66 \ud835\udc56 \u2208 \ud835\udc4c for a given (user, ad) pair \ud835\udc65 \ud835\udc56 \u2208 \ud835\udc4b . In this scenario, \ud835\udc65 \ud835\udc56 represents the specific (user, ad) combination, and \ud835\udc66 \ud835\udc56 corresponds to the LTV generated by the user for the advertisement. Specifically, given a set of samples D = { \ud835\udc65 \ud835\udc56 , \ud835\udc66 \ud835\udc56 } \ud835\udc41 \ud835\udc56 = 1 consisting of N data-label pairs, we aim to compute the LTV between the user \ud835\udc62 \ud835\udc56 \u2208 U and an advertisement \ud835\udc4e \ud835\udc56 \u2208 A . This progress can be formulated as following: where \u0398 denotes the parameters of the LTV estimation model. The model tasks sample \ud835\udc65 \ud835\udc56 = ( \ud835\udc62 \ud835\udc56 , \ud835\udc4e \ud835\udc56 ) as input, where \ud835\udc62 \ud835\udc56 is user features including user historical behavior (e.g., click and conversion sequences), user profile (e.g., age, gender and purchase frequencey), and \ud835\udc4e \ud835\udc56 is ad features such as the tile and category. \ud835\udc66 \ud835\udc56 \u2208 [ 0 , \u221e) is denoted as LTV label.", "4 METHOD": "As illustrated in Figure 2 (a), most of the previous works improve models by integrating knowledge from the source domain via multidomain joint learning. Regardless of these different variations of methods with multi-domain learning, the common issues not fully studied are negative transfer induced by the domain shift. To address this, we present our approach ADSNet as shown in Figure 2(b) and Figure 3. We utilize the pseudo-siamese network to evaluate the information gain, to support learning information that is beneficial to the target domain and reject noisy samples with the gain evaluation strategy. Additionally, we introduce a domain adaptation module as a bridge to connect different domains, reduce the distribution distance between them, and enhance the uniformity Pseudo-Siamese Network Shared Structure Gain Metrics Gain Expression Reject Accept MLP MLP Purchase Rate MLP MLP Purchase Amount pLTV x MLP Sigmoid MLP WS Gain Network Domain Adaptation Adapter ... Embedding Vanilla Network Encoding Layer Expert Layer Inputs (User, Ad, Context) Tower Layer Output External Data MLP MLP Purchase Amount MLP MLP Purchase Rate pLTV x Adapter Encoding Layer Expert Layer Inputs (User, Ad, Context) Tower Layer Output Embedding ... Internal Data Gain Evaluation 1 3 2 of the representation space distribution. In this section, we first describe the base model(Sec. 4.1) for LTV prediction, the structure of our ADSNet including the pseudo-siamese network(Sec. 4.2), the gain evaluation strategy(Sec. 4.3) and domain adaptation module(Sec. 4.4), and then present the adaptive training process with iterative alignment strategy (Sec. 4.5).", "4.1 Backbone for LTV Prediction": "To better meet the complex distribution of LTV in real advertising scenarios, we develop a deep neural network (DNN) with Ordinal Classification [3, 8, 9, 27, 33] as our backbone for LTV prediction, which is a classic framework consisting of the encoding layer, expert layer, and tower layer. 4.1.1 Encoding Layer . We categorize the features into distinct fields according to their characteristics. For instance, the user's basic profile attributes, such as age, gender, and region, constitute one field, while different user behavior sequences form another field. Given the input features \ud835\udc65 \ud835\udc56 , we encode features in different fields into embedding vectors and employ Field-weighted Factorization Machines (FwFM) [28] to model the different interaction of features between different fields, and the embedding vectors are concatenated as the final embedding representation E = \ud835\udc38\ud835\udc5b\ud835\udc50\ud835\udc5c\ud835\udc51\ud835\udc56\ud835\udc5b\ud835\udc54 ( X ) , where \ud835\udc65 \ud835\udc56 \u2208 X is the input feature. It is worth noting that alternative encoding methods could also be employed, such as Deep Cross Network (DCN)[42], Learning Hidden Unit Contributions (LHUC)[34], Transformer[38], etc., to either replace FwFFM or be used in conjunction with it. 4.1.2 Expert Layer . The expert layer is designed to learn and represent various aspects of the input by incorporating multiple experts, each of which is responsible for capturing specific patterns or characteristics within the data. Inspired by the success of Mixture of Experts(MoE) architecture [17, 24], we employ PLE [35] as expert layer here, which consists of a set of expert networks and a gating network. Each expert network is implemented as a Multi-Layer Perceptron (MLP), and the gating network is responsible for determining the contribution of each expert to the final output. Given the encoded representation \ud835\udc52 \ud835\udc56 \u2208 E from the encoding layer, we feed it into each expert network, obtaining a set of expert outputs \ud835\udc49 = { \ud835\udc63 \ud835\udc57 \ud835\udc56 } \ud835\udc3e \ud835\udc57 = 1 , where \ud835\udc3e is the number of experts. The gating network, which is also an MLP with sofrmax function, takes the same input and produces a set of gating weights { \ud835\udc54 \ud835\udc57 } \ud835\udc3e \ud835\udc57 = 1 , with each corresponding to the weight of expert. The final output \u210e \ud835\udc56 \u2208 \ud835\udc3b of the expert layer is a weighted sum of the expert outputs, with the gating weights determining the contribution of each expert: 4.1.3 Tower Layer . The tower layer takes the expert layer's output and generates the final LTV prediction. The LTV of customers in mobile gaming typically exhibits two distinct traits: 1) a long-tailed distribution with a substantial proportion of zero values, and 2) a multimodal distribution of purchases due to standardized purchase tiers (e.g., $6, $30, $98, and $198). To this end, we extend a multigranularity prediction module, which comprises two components: the probability of purchase prediction at a coarse-grained level and the amount of purchase prediction at a fine-grained level. Probability of Purchase Prediction. In particular, we use a classifier to estimate the purchase likelihood of each sample as \ud835\udc5d \ud835\udc56 . This classifier is an MLP using a sigmoid activation function. For the prediction of purchase probability, we utilize the cross-entropy loss function. The loss L \ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc4f is formally defined as: where 1 {\u00b7} is the indicator function, which represents the presence of a positive sample, i.e., whether a purchase has been made. Amount of Purchase Prediction. Different from ZILN-based methods [44, 48] which typically employ a ZILN loss to approximate the complex purchase distribution's mean and variance, we develop a multi-class classification module with ordinal classification [3, 8, 9]. It divides the LTV distribution into several subdistributions and performs prediction over each sub-distribution with multiple binary classifiers. This helps model to learn the ordered nature of purchase categories, and allows for the direct modeling of the cumulative distribution function of purchases, which is morealigned with the inherently sequential progression of purchase amounts. We transform continuous purchase labels into a set of binary classification labels to reflect rank information. Specifically, the original LTV label \ud835\udc66 \ud835\udc56 is assigned to a segment \ud835\udc60 \ud835\udc56 , which represents the segment label for the LTV ranking level. The segments are determined by frequency equalization to maintain a relatively balanced sample size across them. Then, each segment(rank) label \ud835\udc60 \ud835\udc56 is expanded to \ud835\udc3e -1 binary class labels { \ud835\udc60 1 \ud835\udc56 , . . . , \ud835\udc60 \ud835\udc3e -1 \ud835\udc56 } such that \ud835\udc60 \ud835\udc58 \ud835\udc56 \u2208 { 0 , 1 } is indicates whether \ud835\udc60 \ud835\udc56 exceeds rank \ud835\udc5f \ud835\udc58 . For example, \ud835\udc60 \ud835\udc58 \ud835\udc56 = 1 { \ud835\udc66 \ud835\udc56 > \ud835\udc5f \ud835\udc58 } . The indicator function 1 {\u00b7} is 1 if the inner condition is true and 0 otherwise. Each binary classifier employs a sigmoid activation function, and \ud835\udc5d \ud835\udc58 \ud835\udc56 represents the probability prediction of the \ud835\udc58 -th binary classifier. In the process of inference, the predicted LTV (pLTV) is calculated as: where \ud835\udc5d \ud835\udc56 denotes the probability of purchase. \ud835\udc59\ud835\udc61\ud835\udc63 \ud835\udc58 is the average LTV of the \ud835\udc58 -th segment. \ud835\udc5d \ud835\udc58 \ud835\udc56 is the probability of purchase for the \ud835\udc58 -th segment. At this stage, we adopt the binary cross-entropy loss for amount of purchase prediction with ordinal classification, and define the purchase amount loss L \ud835\udc4e\ud835\udc5a\ud835\udc5c\ud835\udc62\ud835\udc5b\ud835\udc61 as follows: where \ud835\udc66 \ud835\udc56 denotes original LTV. To train the above two tasks jointly, we introduce the loss as:", "4.2 Difference Pseudo-Siamese Network": "The Siamese network [6, 12, 30] is a typical architecture in deep learning, which comprises two branches with identical structures and uses similar and dissimilar pairs to learn similarity. In contrast, the Pseudo-Siamese network [10, 50] offers more flexibility than the Siamese network, as it allows different structures to receive inputs from various modalities. Drawing inspiration from these frameworks, we integrate the Pseudo-Siamese Network to assess the information gain, to support learning information from source domain, e.g., external data outside of advertising platform, that is beneficial to the target domain and reject noisy samples. This selective transfer capability is crucial in practical scenarios. Specifically, our pseudo-siamese network is composed of a vanilla network and a gain Network. Those two networks are based on the backbone for LTV prediction as Sec. 4.1. The gain Network receives inputs from both external and internal samples, while the vanilla Network is exclusively fed with samples from the internal channel. During training, both networks will update their parameters. This concurrent parameter updating allows each network to learn from its respective data stream, with the gain network adjusting to the nuances of both the external and internal channel samples, and the vanilla network refining its understanding based on the internal channel data alone. This process is key to enabling the pseudosiamese network to effectively differentiate and integrate relevant information from diverse data sources. In the training process, we define the losses calculated by L \ud835\udc5d\ud835\udc59\ud835\udc61\ud835\udc63 as L \ud835\udc54\ud835\udc4e\ud835\udc56\ud835\udc5b,\ud835\udc60 of external data and \ud835\udc3f \ud835\udc54\ud835\udc4e\ud835\udc56\ud835\udc5b,\ud835\udc61 of internal data from the gain network. And vanilla network Loss of internal data loss is L \ud835\udc63\ud835\udc4e\ud835\udc5b = L \ud835\udc63\ud835\udc4e\ud835\udc5b,\ud835\udc61 from vanilla network.", "4.3 Gain Evaluation Strategy": "Leveraging a pseudo-siamese architecture, we can establish a metric to contrast the differences between two networks, thereby calculating the contribution of input data to the network's performance as: where \ud835\udc46\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 (\u00b7) is the gain metric function. In this paper, we utilize this approach to quantify the gain provided by external data to the gain network by examining the variance in losses computed by the two networks on internal samples. Formally, this is expressed as: where \ud835\udc4a \ud835\udc3a represents the gain. If \ud835\udc4a \ud835\udc3a > 0, it means there is a positive gain to the internal domain. It is notable that, in addition to the aforementioned methodology, we can extend to employ reinforcement learning by defining metrics that are pertinent to the business objectives. The differential in these metrics can be utilized as a reward signal to train the network by Adversarial Reward Learning [14, 43].", "4.4 Domain Adaptation Module": "The domain adaptation module serves as a bridge between the gain Network and the vanilla Network, mitigating the disparity between domain distributions. To this end, an adapter layer is integrated at the bottom of the tower module, implemented as an MLP. First, the adapter layer within the gain network is employed to estimate the significance of external data as \ud835\udc4a \ud835\udc60 , which is calculated as:", "Algorithm 1 Iterative Alignment Strategy": "Require: internal samples, external samples, T (total training steps) Ensure: \ud835\udf03 \ud835\udc63 , \ud835\udf03 \ud835\udc54 1: Warmup Stage: 2: \ud835\udf03 \ud835\udc63 \u2190 Train ( internal samples ) 3: Joint Training Stage: 4: \ud835\udf03 \ud835\udc54 \u2190 \ud835\udf03 \ud835\udc63 5: Set sync_frequency to 500 6: for \ud835\udc61 = 1 to \ud835\udc47 do 7: \ud835\udf03 \ud835\udc54 \u2190 Train ( internal samples \u222a external samples ) 8: \ud835\udf03 \ud835\udc63 \u2190 Train ( internal samples ) 9: if \ud835\udc61 mod sync_frequency == 0 then 10: \ud835\udf03 \ud835\udc63 \u2190 \ud835\udf03 \ud835\udc54 11: end if 12: end for 13: return \ud835\udf03 \ud835\udc63 , \ud835\udf03 \ud835\udc54 Then we consider two levels of distributional divergence, encompassing both the low-level embedding layer and the high-level tower layer. Following [20], we employ knowledge distillation to constrain the distributions through Mean Squared Error (MSE): where \ud835\udc6c \ud835\udc49 and \ud835\udc6c \ud835\udc3a denote the embeddings from the vanilla network and the gain network, respectively, while \ud835\udc6f \ud835\udc49 \ud835\udc4e\ud835\udc51\ud835\udc4e\ud835\udc5d\ud835\udc61\ud835\udc52\ud835\udc5f and \ud835\udc6f \ud835\udc3a \ud835\udc4e\ud835\udc51\ud835\udc4e\ud835\udc5d\ud835\udc61\ud835\udc52\ud835\udc5f represent the outputs of the adapter layers within the vanilla network and the gain network, respectively. The loss of domain adaption is the sum losses as:", "4.5 Training Objective": "Loss Function. To train the whole model jointly, we introduce a composite loss function L total as: where hyperparameter \ud835\udefd is employed to control the trade-off among these losses. The loss for the gain Nntwork is computed as: where L gain, s is external data loss, L gain, t is internal data loss from gain network respectively. \ud835\udc4a \ud835\udc60 is the weight of external data and \ud835\udc4a \ud835\udc3a is the information gain. The indicator function 1 ( \ud835\udc4a \ud835\udc3a > 0 ) denotes the selective transfer capability of ADSNet, which permits only the advantageous external data to be utilized. Iterative Alignment Strategy. Considering that the performance of the gain network is expected to improve over time, creating a widening gap between it and the vanilla network, such a scenario could eventually render the gain evaluation strategy ineffective. To address this issue, we propose to use iterative alignment strategy during the training process to prevent divergence between the two networks. Specifically, the model training is divided into two stages as in Algorithm 1: (1) Warmup Stage : Only internal samples are used to train the vanilla network. This allows the base network to establish a stable starting point. (2) Joint Training Stage : Initialize the gain network with the vanilla network parameters, and train both networks with parameters synchronized iteratively.", "5 EXPERIMENTS": "", "5.1 Experimental Setup": "5.1.1 Datasets . Due to the lack of public dataset on LTV prediction, we construct an industry dataset to conduct offline evaluation. The dataset is collected from from a sampling of conversion logs from Tencent's online advertising system over a span of 90 days, covering four internal traffics (i.e., business domains) and authorized external data from other platforms. The dataset consists of billions of examples, and is split according to the time axis, with 70, 10, and 10 days' worth of samples allocated for training, validation, and testing, respectively. The main statistics are shown in Table 1, where the sample size is calculated as the average per day. Due to company privacy policy, we only disclose the purchase sample size and LTV. Table 1 displays the sample size, and average LTV for each domain. As indicated in this table, different domains exhibit distinct domain-specific data distribution, as reflected in the varying LTVs. It can be observed that the domain with the highest Avg.LTV (Domain #3) is 4.82, while the domain with the lowest Avg.LTV (Domain #2) is only 0.07. The external data have a substantially larger average LTV, indicating a different data distribution from the internal data. These variations in the dataset provide a comprehensive ground for evaluating the performance of LTV prediction models across different domains and data distributions. 5.1.2 Metrics . wefocus on evaluating the performance of a model that predicts customer lifetime value (LTV) by differentiating highvalue customers from low-value ones. For this purpose, we employ two evaluation metrics: the Area Under the Curve (AUC) and the Normalized Gini Coefficient (Norm GINI). The AUC is a widely used metric for assessing the model's ability to identify purchase users, as it measures the model's classification performance. A higher AUC value indicates that the model can better discriminate between positive (purchase) and negative (non-purchase) classes. However, the AUC does not provide information about the accuracy of the users' ranking based on the predicted LTV. To address this issue, we further adopt the Normalized Gini Coefficient (Norm GINI) [44]. The Norm GINI is a robust measure that captures the model's ability to rank users according to their predicted LTV accurately. It is preferred over the Mean Squared Error (MSE) due to its robustness to outliers and better business interpretation. The Norm GINI ranges between 0 and 1, with a value of 1 indicating perfect consistency between the ranking based on predicted LTV and the ranking based on the real LTV. The lower bound of 0 corresponds to the random ordering of customers. 5.1.3 Hyper-Parameter Settings . We implemented all methods using Tensorflow. In the training stage, we utilize the Follow-theRegularized-Leader (FTRL) optimizer [26] for sparse parameters and the Follow the Moving Leader (FTML) optimizer [54] for dense parameters, in which the learning rate is set within the range of {5e3, 1e-2}, respectively. The batch size is fixed as 512. The embedding dimension is set to 32, which means that each input feature is represented by a 32-dimensional vector. The architecture of each expert consists of an MLP with two hidden layers with a hidden size of [ 128,64 ] . The tower layer of all methods is designed as an MLP with a hidden size of 32. The weight of the domain adaptation loss, denoted as \ud835\udefd , is set to 0.1 according to the grid search performed on the validation set.", "5.2 Models for Comparison": "We compare our proposed methods with several baselines, including both single-domain and cross-domain settings. Each of these methods is described as follows: Single-Domain. These approaches leverage solely internal data for model training. \u00b7 DeepFM [11]: This model integrates the Factorization Machine (FM) model and a deep neural network to extract both low-order and high-order feature interactions. \u00b7 ZILN [44]: This approach designs a novel zero-inflated lognormal loss to address the imbalanced regression problem. \u00b7 FiBiNet [16]: This model employs a Squeeze-and-Excitation network to discern the significance of feature interactions. \u00b7 GateNet [15]: This model utilizes a gating mechanism to distill and select pertinent latent information from feature embeddings. \u00b7 ADSNet-Backbone (Ours): This is the backbone of ADSNet in this paper, characterized by a DNN optimized for ordinal classification. Cross-Domain. The cross-domain methods integrate both internal data and external data during the training stage, while only validating and testing on internal data. \u00b7 Share-Bottom [4]: This model employs a common architecture which shares the parameters of the bottom layers to facilitate multi-task/multi-domain learning, potentially mitigating overfitting while being sensitive to task discrepancies and data distribution variances. \u00b7 MMOE [24]: This method builds upon the Share-Bottom approach by adding multiple experts and a gating mechanism to learn the differences between various domains, alleviating the issue of domain variances. \u00b7 STAR [32]: This model consists of shared centered parameters and domain-specific parameters, adaptively modulating its parameters conditioned on the domain. \u00b7 ADSNet (Ours): This denotes our full model, i.e., our backbone with difference pseudo-siamese network and domain adaptation module.", "5.3 Comparisons with State-of-the-Arts": "Table 2 illustrates the performance of various models on the proposed LTV prediction dataset. It is evident that our backbone model (i.e., ADSNet-Backbone) serves as a strong baseline, and our full model (i.e., ADSNet) further enhances performance. In particular, our base model outperforms other single-domain methods such as GateNet and ZILN, suggesting that the ordinal classification is more suitable for modeling the complex multi-modal 100K 200K 300K 400K 500K Training Steps 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Negative Gain Rejection Rate 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 GINI Negative Gain Rejection ADSNet w/ GES ADSNet w/o GES LTV distribution. Our full model, ADSNet, significantly outperforms cross-domain methods that leverage external data across all domain datasets, achieving absolute overall GINI and AUC improvements of 1% to 5%. This substantiates the effectiveness of integrating external data into the neural network. It is noteworthy that some models experience a decrease in performance when introducing external data. For instance, Share-Bottom w/ZILN, compared to ZILN, shows a decline in overall GINI by 0.4%. This can be attributed to the substantial differences in data distribution across various domains, as illustrated in Table 1. It demonstrates that conventional multi-domain joint learning models fail to avoid the negative transfer phenomenon. In contrast, our ADSNet model exhibits superiority by supporting the rejection of negative gain samples, thus effectively mitigating this issue.", "5.4 Ablation Study": "To investigate the effectiveness and illustrate the impact of different components of our proposed model, we conduct ablation studies on four ablations built upon our full model as shown in Table 3. In particular, (1) ADSNet w/o the Gain Evaluation Strategy (row 3) exhibits a noticeable decrease in GINI, dropping from 0.856 to 0.824. This suggests that the gain evaluation strategy plays a significant role in rejecting noise samples and stressing negative transfer. (2) The removal of the Domain Adaptation Module (row 2) results in GINI dropping by an absolute 2.3%, indicating that constraining the distribution between the gain network and the vanilla network is beneficial to enhance the process of knowledge transfer. Furthermore, (3) the removal of the Iterative Alignment Strategy (row 4) also leads to lower GINI, which indicates that this iterative process is essential for refining the model's predictions in an incremental fashion, aligning the model more closely with the target domain. Iterative alignment likely facilitates a more nuanced adaptation that incrementally bridges the domain gap.", "5.5 Effectiveness of Negative Gain Rejection": "To evaluate the impact of our gain evaluation strategy within ADSNet, we conduct an in-depth analysis to understand the influence of negative transfer during the learning process and to assess the [0, 15] [15, 100] [100, 250] [250, +inf] Overall Sample Intervals 0.4 0.5 0.6 0.7 0.8 0.9 1.0 GINI 0.4761 0.6670 0.7553 0.8462 0.7940 0.6317 0.8154 0.8461 0.9400 0.9467 0.6997 0.8671 0.8891 0.9544 0.9448 0.7832 0.8963 0.9377 0.9631 0.9570 ZILN ADSNet-Backbone STAR ADSNet (full model) model's ability to identify and reject negative gains. We define the negative gain rejection rate, which represents the probability that external data is rejected, i.e., the gain \ud835\udc4a \ud835\udc3a < 0. Figure 4 illustrates the change in the negative gain sample rejection rate and GINI with training steps. The X-axis represents the training steps, the left Y-axis represents the rejection rate of negative gain samples, and the right Y-axis represents GINI. Several observations can be made from this: (1) The GINI exhibits nonlinear growth. It improves rapidly in the early stages and then the rate of improvement slows down over time. (2) The negative gain rejection rate is low in the early stage and then increases with the progress of training. This indicates that the model gradually learns to identify and distinguish negative gain samples during the training process. Ultimately, the negative gain rejection rate stabilizes at 0.64, suggesting that a significant portion of the external samples may not be beneficial if used directly. These observations highlight the effectiveness of the gain evaluation strategy which ensures that only external data that contribute positively to the target domain are utilized.", "5.6 Effectiveness of Improving Long-tail Prediction": "In a practical advertising system, we are struggling with the challenge of data sparsity in the LTV estimation scenario. To further understand the improvement achieved by our models, we conduct a more detailed analysis. We quantify the sample sizes corresponding to each advertisement and categorize them into intervals to examine the relationship between the model's predictive capabilities and the size of samples. The results are presented in Figure 5, which demonstrates two insights. First, it intuitively shows that the performance of the model is positively correlated with the sample size. As the sample size increases, the performance of the model improves accordingly. Moreover, compared to the ADSNet-Backbone that does not utilize external data, our ADSNet gains significant improvements by \u223c 15.2% GINI in the long tail interval [ 0 , 15 ] , which indicates that by introducing external data, the model's ability to predict long-tail advertisements is significantly enhanced.", "5.7 Online A/B Test": "In advertising systems, GMV is a critical outcome metric that cannot be directly optimized. GMV can be represented as GMV = LTV / ROI_bid. The ROI_bid is set by the advertiser and is generally fixed based on the required profit margin. Therefore, from a formulaic perspective, GMV and LTV are highly correlated. We conduct online A/B tests on the real advertising platform of Tencent Ads. Specifically, we employ UV sampling to split the traffic, allocating 5% of the traffic to the control group and another 5% to the experimental group. The base serving model is a variant of STAR, adapted to our business characteristics. For the experimental group, we deploy our ADSNet model. Our online evaluation metrics are the LTV and Gross Merchandise Value (GMV). Due to company privacy policies, we only report the relative improvement. The online A/B test results indicate that ADSNet leads to an increase in online LTV by 3.47% and GMV by 3.89% compared with the base model. These results underscore the practical applicability and effectiveness of ADSNet in the LTV prediction task, demonstrating its potential to significantly enhance the performance of real-world advertising systems.", "6 CONCLUSION AND FUTURE WORK": "We present the Adaptive Difference Siamese Network (ADSNet) to address the challenges of data sparsity of LTV estimation and crossdomain transfer learning in advertising systems. We incorporate external data to expand the sample size. Our ADSNet utilizes crossdomain transfer learning, a gain evaluation strategy, and a domain adaptation module to learn beneficial information, reject noisy samples, and bridge different domains. Extensive experiments and online A/B tests have demonstrated the effectiveness of ADSNet in improving performance and mitigating negative transfer, as well as its ability to enhance long-tail prediction capabilities. In future work, we will extend our approach to other related advertising tasks, such as click-through rate prediction to evaluate the generalizability of ADSNet in broader advertising scenarios.", "ACKNOWLEDGMENTS": "We gratefully acknowledge the contributions of the following: Piao Yang, Ting Wang, Yucheng Hu, Chaoyue Zhao, Liwei Lin, Cong Quan and Kun Bai. This work was partially supported by the Tencent Rhinoceros Project.", "REFERENCES": "[1] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 (2021). [2] Sharad Borle, Siddharth S Singh, and Dipak C Jain. 2008. Customer lifetime value measurement. Management science 54, 1 (2008), 100-112. [3] Wenzhi Cao, Vahid Mirjalili, and Sebastian Raschka. 2020. Rank consistent ordinal regression for neural networks with application to age estimation. Pattern Recognition Letters 140 (2020), 325-331. [4] Rich Caruana. 1997. Multitask learning. Machine learning 28 (1997), 41-75. [5] Benjamin Paul Chamberlain, Angelo Cardoso, CH Bryan Liu, Roberto Pagliari, and Marc Peter Deisenroth. 2017. Customer lifetime value prediction using embeddings. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining . 1753-1762. [6] Xingping Dong and Jianbing Shen. 2018. Triplet loss in siamese network for object tracking. In Proceedings of the European conference on computer vision (ECCV) . 459-474. [7] Lixin Duan, Dong Xu, and Ivor Tsang. 2012. Learning with augmented features for heterogeneous domain adaptation. arXiv preprint arXiv:1206.4660 (2012). [8] Eibe Frank and Mark Hall. 2001. A simple approach to ordinal classification. In Machine Learning: ECML 2001: 12th European Conference on Machine Learning Freiburg, Germany, September 5-7, 2001 Proceedings 12 . Springer, 145-156. [9] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao. 2018. Deep ordinal regression network for monocular depth estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition . 2002-2011. [10] Junyi Gao, Cao Xiao, Lucas M Glass, and Jimeng Sun. 2020. COMPOSE: Crossmodal pseudo-siamese network for patient trial matching. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining . 803-812. [11] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. arXiv preprint arXiv:1703.04247 (2017). [12] Qing Guo, Wei Feng, Ce Zhou, Rui Huang, Liang Wan, and Song Wang. 2017. Learning dynamic siamese network for visual object tracking. In Proceedings of the IEEE international conference on computer vision . 1763-1771. [13] Sunil Gupta, Dominique Hanssens, Bruce Hardie, Wiliam Kahn, V Kumar, Nathaniel Lin, Nalini Ravishanker, and S Sriram. 2006. Modeling customer lifetime value. Journal of service research 9, 2 (2006), 139-155. [14] Jonathan Ho and Stefano Ermon. 2016. Generative adversarial imitation learning. Advances in neural information processing systems 29 (2016). [15] Tongwen Huang, Qingyun She, Zhiqiang Wang, and Junlin Zhang. 2020. GateNet: gating-enhanced deep network for click-through rate prediction. arXiv preprint arXiv:2007.03519 (2020). [16] Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. 2019. FiBiNET: combining feature importance and bilinear feature interaction for click-through rate prediction. In Proceedings of the 13th ACM Conference on Recommender Systems . 169-177. [17] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. 1991. Adaptive mixtures of local experts. Neural computation 3, 1 (1991), 79-87. [18] Junguang Jiang, Baixu Chen, Junwei Pan, Ximei Wang, Dapeng Liu, Mingsheng Long, et al. 2023. ForkMerge: Mitigating Negative Transfer in Auxiliary-Task Learning. In Thirty-seventh Conference on Neural Information Processing Systems . [19] Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in NLP. ACL. [20] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. 2019. Tinybert: Distilling bert for natural language understanding. arXiv preprint arXiv:1909.10351 (2019). [21] Kunpeng Li, Guangcui Shao, Naijun Yang, Xiao Fang, and Yang Song. 2022. Billionuser Customer Lifetime Value Prediction: An Industrial-scale Solution from Kuaishou. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management . 3243-3251. [22] Wen Li, Lixin Duan, Dong Xu, and Ivor W Tsang. 2013. Learning with augmented features for supervised and semi-supervised heterogeneous domain adaptation. IEEE Transactions on Pattern analysis and machine intelligence 36, 6 (2013), 11341148. [23] Zhongqi Lu, Erheng Zhong, Lili Zhao, Evan Wei Xiang, Weike Pan, and Qiang Yang. 2013. Selective transfer learning for cross domain recommendation. In Proceedings of the 2013 SIAM International Conference on Data Mining . SIAM, 641-649. [24] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. 2018. Modeling task relationships in multi-task learning with multi-gate mixture-ofexperts. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1930-1939. [25] Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu, and Kun Gai. 2018. Entire space multi-task model: An effective approach for estimating post-click conversion rate. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval . 1137-1140. [26] Brendan McMahan. 2011. Follow-the-regularized-leader and mirror descent: Equivalence theorems and l1 regularization. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics . JMLR Workshop and Conference Proceedings, 525-533. [27] Zhenxing Niu, Mo Zhou, Le Wang, Xinbo Gao, and Gang Hua. 2016. Ordinal regression with multiple output cnn for age estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition . 4920-4928. [28] Junwei Pan, Jian Xu, Alfonso Lobos Ruiz, Wenliang Zhao, Shengjun Pan, Yu Sun, and Quan Lu. 2018. Field-weighted factorization machines for click-through rate prediction in display advertising. In Proceedings of the 2018 World Wide Web Conference . 1349-1357. [29] Sinno Jialin Pan and Qiang Yang. 2009. A survey on transfer learning. IEEE Transactions on knowledge and data engineering 22, 10 (2009), 1345-1359. [30] Michael Pulis and Josef Bajada. 2021. Siamese Neural Networks for Content-based Cold-Start Music Recommendation.. In Proceedings of the 15th ACM Conference on Recommender Systems . 719-723. [31] Kandula Balagangadhar Reddy, Debabrata Swain, Samiksha Shukla, and Lija Jacob. 2022. Prediction of Customer Lifetime Value Using Machine Learning. In Proceedings of Second Doctoral Symposium on Computational Intelligence: DoSCI 2021 . Springer, 271-278. [32] Xiang-Rong Sheng, Liqin Zhao, Guorui Zhou, Xinyao Ding, Binding Dai, Qiang Luo, Siran Yang, Jingshan Lv, Chi Zhang, Hongbo Deng, et al. 2021. One model to serve all: Star topology adaptive recommender for multi-domain ctr prediction. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management . 4104-4113. [33] Xintong Shi, Wenzhi Cao, and Sebastian Raschka. 2023. Deep neural networks for rank-consistent ordinal regression based on conditional probabilities. Pattern Analysis and Applications 26, 3 (2023), 941-955. [34] Pawel Swietojanski and Steve Renals. 2014. Learning hidden unit contributions for unsupervised speaker adaptation of neural network acoustic models. In 2014 IEEE Spoken Language Technology Workshop (SLT) . IEEE, 171-176. [35] Hongyan Tang, Junning Liu, Ming Zhao, and Xudong Gong. 2020. Progressive layered extraction (ple): A novel multi-task learning (mtl) model for personalized recommendations. In Proceedings of the 14th ACM Conference on Recommender Systems . 269-278. [36] Ali Vanderveld, Addhyan Pandey, Angela Han, and Rajesh Parekh. 2016. An engagement-based customer lifetime value system for e-commerce. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining . 293-302. [37] VNVapnik. 2000. The Nature of Statistical Learning Theory (Information Science and Statistics) Springer-Verlag. New York (2000). [38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017). [39] Pablo Villalobos, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius Hobbhahn, and Anson Ho. 2022. Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning. arXiv preprint arXiv:2211.04325 (2022). [40] Hao Wang, Tai-Wei Chang, Tianqiao Liu, Jianmin Huang, Zhichao Chen, Chao Yu, Ruopeng Li, and Wei Chu. 2022. Escm2: Entire space counterfactual multitask model for post-click conversion rate estimation. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval . 363-372. [41] Jindong Wang, Yiqiang Chen, Shuji Hao, Wenjie Feng, and Zhiqi Shen. 2017. Balanced distribution adaptation for transfer learning. In 2017 IEEE international conference on data mining (ICDM) . IEEE, 1129-1134. [42] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network for ad click predictions. In Proceedings of the ADKDD'17 . 1-7. [43] Xin Wang, Wenhu Chen, Yuan-Fang Wang, and William Yang Wang. 2018. No metrics are perfect: Adversarial reward learning for visual storytelling. arXiv preprint arXiv:1804.09160 (2018). [44] Xiaojing Wang, Tianqi Liu, and Jingang Miao. 2019. A deep probabilistic model for customer lifetime value prediction. arXiv preprint arXiv:1912.07753 (2019). [45] Tianyu Wu, Shizhu He, Jingping Liu, Siqi Sun, Kang Liu, Qing-Long Han, and Yang Tang. 2023. A brief overview of ChatGPT: The history, status quo and potential future development. IEEE/CAA Journal of Automatica Sinica 10, 5 (2023), 1122-1136. [46] Min Xiao and Yuhong Guo. 2014. Feature space independent semi-supervised domain adaptation via kernel matching. IEEE transactions on pattern analysis and machine intelligence 37, 1 (2014), 54-66. [47] Mingzhe Xing, Shuqing Bian, Wayne Xin Zhao, Zhen Xiao, Xinji Luo, Cunxiang Yin, Jing Cai, and Yancheng He. 2021. Learning Reliable User Representations from Volatile and Sparse Data to Accurately Predict Customer Lifetime Value. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining . 3806-3816. [48] Xuejiao Yang, Binfeng Jia, Shuangyang Wang, and Shijie Zhang. 2023. Feature Missing-aware Routing-and-Fusion Network for Customer Lifetime Value Prediction in Advertising. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining . 1030-1038. [49] Yi Yao and Gianfranco Doretto. 2010. Boosting for transfer learning with multiple sources. In 2010 IEEE computer society conference on computer vision and pattern recognition . IEEE, 1855-1862. [50] Xianhua Zeng, Xinyu Wang, and Yicai Xie. 2023. Multiple Pseudo-Siamese Network with Supervised Contrast Learning for Medical Multi-modal Retrieval. ACM Transactions on Multimedia Computing, Communications and Applications (2023). [51] Shijie Zhang, Xin Yan, Xuejiao Yang, Binfeng Jia, and Shuangyang Wang. 2023. Out of the Box Thinking: Improving Customer Lifetime Value Modelling via Expert Routing and Game Whale Detection. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management . 3206-3215. [52] Wei Zhang, Pengye Zhang, Bo Zhang, Xingxing Wang, and Dong Wang. 2023. A Collaborative Transfer Learning Framework for Cross-domain Recommendation. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 5576-5585. [53] Jiejie Zhao, Bowen Du, Leilei Sun, Fuzhen Zhuang, Weifeng Lv, and Hui Xiong. 2019. Multiple relational attention network for multi-task learning. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & Data Mining . 1123-1131. [54] Shuai Zheng and James T Kwok. 2017. Follow the moving leader in deep learning. In International Conference on Machine Learning . PMLR, 4110-4119. [55] Ruixi Zhu, Li Yan, Nan Mo, and Yi Liu. 2019. Semi-supervised center-based discriminative adversarial learning for cross-domain scene-level land-cover classification of aerial images. ISPRS journal of photogrammetry and remote sensing 155 (2019), 72-89."}
