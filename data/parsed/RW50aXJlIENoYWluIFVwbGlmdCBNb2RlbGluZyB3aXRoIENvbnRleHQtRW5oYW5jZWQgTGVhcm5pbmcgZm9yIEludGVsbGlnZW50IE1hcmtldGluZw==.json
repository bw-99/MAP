{"Entire Chain Uplift Modeling with Context-Enhanced Learning for Intelligent Marketing": "", "Yinqiu Huang \u2217": "Chongqing University Chongqing, China yinqiu@cqu.edu.cn Shuli Wang Meituan Chengdu, China wangshuli03@meituan.com Min Gao \u2020 Chongqing University Chongqing, China gaomin@cqu.edu.cn Xue Wei Meituan Chengdu, China weixue06@meituan.com Yinhua Zhu Meituan Chengdu, China zhuyinhua@meituan.com Changhao Li Meituan Chengdu, China lichanghao@meituan.com Xiong Xiao Meituan Chengdu, China xiaoxiong02@meituan.com", "ABSTRACT": "Uplift modeling, vital in online marketing, seeks to accurately measure the impact of various strategies, such as coupons or discounts, on different users by predicting the Individual Treatment Effect (ITE). In an e-commerce setting, user behavior follows a defined sequential chain, including impression, click, and conversion. Marketing strategies exert varied uplift effects at each stage within this chain, impacting metrics like click-through and conversion rate. Despite its utility, existing research has neglected to consider the inter-task across all stages impacts within a specific treatment and has insufficiently utilized the treatment information, potentially introducing substantial bias into subsequent marketing decisions. We identify these two issues as the chain-bias problem and the treatment-unadaptive problem. This paper introduces the Entire Chain UPlift method with context-enhanced learning (ECUP), devised to tackle these issues. ECUP consists of two primary components: 1) the Entire Chain-Enhanced Network, which utilizes user behavior patterns to estimate ITE throughout the entire chain space, models the various impacts of treatments on each task, and integrates task prior information to enhance context awareness across all stages, capturing the impact of treatment on different tasks, and 2) the Treatment-Enhanced Network, which facilitates fine-grained treatment modeling through bit-level feature interactions, thereby enabling adaptive feature adjustment. Extensive experiments on public and industrial datasets validate ECUP's effectiveness. Moreover, ECUP has been deployed on the Meituan food delivery platform, serving millions of daily active users, with the related dataset released for future research.", "CCS CONCEPTS": "\u00b7 Applied computing \u2192 Economics ; \u00b7 Information systems \u2192 Personalization . Chuan Luo Meituan Chengdu, China luochuan03@meituan.com Yi Luo Meituan Chengdu, China luoyi15@meituan.com Coupons Pool Operation Uplift Model Research AD AD AD Advertisements Pool", "KEYWORDS": "Uplift modeling, Context-enhanced learning, Entire chain modeling", "1 INTRODUCTION": "Major e-commerce platforms routinely devise marketing strategies to enhance user engagement and amplify Gross Merchandise Volume, commonly utilizing incentives such as advertising, discounts, and coupons [16, 32]. The implementation of such strategies incurs not only substantial costs but also demands numerous iterations and adjustments, necessitating an experienced operations team and incurring considerable trial and error expenses. Uplift modeling, an innovative predictive analysis technology rooted in causal inference [30], has garnered notable results in the realm of intelligent marketing [17]. The uplift model adeptly forecasts the Individual Treatment Effect (ITE) under various treatments, such as issuing a specific coupon, thereby assisting e-commerce platforms in crafting precise, effective marketing strategies. The process is shown in Fig. 1, and most platforms adopt a two-stage mode. After obtaining the uplift score, using the operations research optimization technology for treatment allocation under cost constraints. The uplift model is the cornerstone of the entire process. Uplift modeling aims to precisely discern the differential in a user's response with and without treatment, subsequently outputting an uplift score, such as the increase in conversion rate resulting from a specific coupon allocation. Unlike traditional supervised learning, uplift modeling presents a counterfactual problem: Observations are limited to the outcomes when an individual is either treated (treatment group) or not treated (control group) and actual ground-truth labels (uplift score) are absent. Existing uplift methodologies mainly establish response models on multiple treatment functions concurrently to deduce counterfactual outcomes. The S-learner [15] estimates the conditional average outcome under various treatments, calculating the ITE (i.e., uplift score) through subtraction. Several variants have been developed on this foundation, such as the T-learner and X-learner [15], among others. Within this domain, various deep learning methods, commended for their exceptional feature-extraction capabilities, serve as fundamental models of uplift frameworks. Nonetheless, prevailing uplift methods manifest two salient limitations. Firstly, existing methods display insufficient modeling of the complete process chain. In e-commerce scenarios, users typically follow a sequential pattern, designated as \ud835\udc56\ud835\udc5a\ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b \u2192 \ud835\udc50\ud835\udc59\ud835\udc56\ud835\udc50\ud835\udc58 \u2192 \ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc63\ud835\udc52\ud835\udc5f\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b , and crafting marketing strategies usually requires integrating the uplift of multiple user behaviors in the chain. However, current research predominantly predicts the uplift score for a single task within the chain, neglecting the inter-task impact despite the model making inferences throughout the entire chain space. This will result in biased results because treatment has varying degrees of impact on each task. For instance, a user group may exhibit a positive click uplift score and a negative conversion uplift score under a specific treatment, yet the culminating clickand-conversion uplift score may still be positive. Relying solely on the prediction of the negative uplift score of the terminal task from a model trained on the click set can yield entirely opposing results, potentially wreaking havoc on subsequent marketing decisions. This phenomenon, referred to as \"chain-bias\" problem, is illustrated in Fig. 2. Concurrently, data sparsity in subsequent tasks compromises the model's generalization capabilities. Secondly, they demonstrate insufficient utilization of the treatment. Treatment notably influences user responses, with individuals displaying varied behaviors under different treatments. For instance, we observed that students are more inclined to place orders when offered high-value coupons on the Meituan food delivery platform, a trend not as prevalent among white-collar workers. However, prior methods typically learn a static representation for each feature, neglecting the adaptation of the same feature under disparate treatments. This lack of adaptability makes capturing individual behavioral distinctions across various treatments challenging. While some researchers leverage the attention mechanism to model the interplay between treatment and non-treatment features [17, 29], they predominantly utilize normalized attention weights to adjust both feature types. Unfortunately, this vector-level interaction approach lacks the flexibility to seize each feature's fluctuating significance under various treatments. This phenomenon is referred to as a \"treatment-unadaptive\" problem. In this paper, we introduce the Entire Chain UPlift method with context-enhanced learning (ECUP), addressing the aforementioned challenges through a thorough consideration of both chain and Increase Decrease Positive number Negative number High-value Unattractive CTR uplift score AD creativity CTR score Coupon conflict conflict CVR score AD CVR uplift score CTCVR uplift score uncertain ; CTCVR score uncertain strict use high-quality conditions CTR CVR CTR CVR uplift uplift, uplift goods treatment information. ECUP is primarily composed of two parts. The first component, the Entire Chain-Enhanced Network, applies the user's sequential pattern to proficiently learn the response functions of each task across the entirety of the chain space. In tandem, a Task-Enhanced Network merges treatment-enhanced features with task information, thereby enhancing context-aware uplift modeling to solve the chain-bias. This component not only comprehensively considers the conversion funnel in uplift modeling, reducing the adverse effects of different uplifts in previous and subsequent tasks, but also solves the data sparsity problem in subsequent tasks. The second component, the Treatment-Enhanced Network, discerns correlation data between the treatment and personalized features by a Treatment Awareness Unit, subsequently directing the refinement of treatment-aware features. It then strikes a balance between the initial and treatment-aware embedding by a Treatment-Enhanced Gate through bit-level weights that facilitate a flexibly adaptive adjustment of features under various treatments. In summary, our contributions are as follows: \u00b7 Problem. We identify and propose a solution to the chain-bias problem in uplift modeling. To the best of our knowledge, it is the first endeavor in large-scale online systems. Furthermore, our method exhibits versatility and promptly applies to various chain uplift modeling tasks. \u00b7 Model. We introduce a novel entire chain context-enhanced uplift modeling method, ECUP, adept at comprehensively considering the conversion funnel in uplift modeling and adapting features under varying treatments, thereby addressing the chainbias and treatment-unadaptive problem. \u00b7 Data. We release a large-scale, comprehensive, unbiased coupon usage dataset from the Meituan food delivery platform 1 . To our knowledge, this represents the first public, unbiased industrial dataset featuring multiple treatments and comprehensive chain label information intended to facilitate research in causal inference. \u00b7 Evaluation. Through extensive experiments on both offline and online A/B tests, we show that our ECUP significantly outperforms the state-of-the-art methods. ECUP has been deployed in Meituan and serves millions of daily active users.", "2 PRELIMINARIES": "", "2.1 Problem Definition": "We define users' sequential behavior patterns in marketing scenarios as \ud835\udc56\ud835\udc5a\ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b \u2192 \ud835\udc50\ud835\udc59\ud835\udc56\ud835\udc50\ud835\udc58 \u2192 \ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc63\ud835\udc52\ud835\udc5f\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b . We assume the observed dataset to be S = {( \ud835\udc65 \ud835\udc56 , \ud835\udc61 \ud835\udc56 , \ud835\udc66 \ud835\udc56 \u2192 \ud835\udc67 \ud835\udc56 )}| \ud835\udc41 \ud835\udc56 = 1 , with a sample drawn from a distribution with domain \ud835\udc4b \u00d7 \ud835\udc47 \u00d7 \ud835\udc4c \u00d7 \ud835\udc4d , where \ud835\udc4b is feature space, \ud835\udc47 is treatment space, \ud835\udc4c and \ud835\udc4d are label spaces, and \ud835\udc41 is the total number of impressions; \ud835\udc65 \ud835\udc56 is the feature vector of the i-th instance, which includes user features, item features, and contextual features; \ud835\udc61 \ud835\udc56 is the treatment ID received by the i-th instance, and the total number of treatments is K, i.e., \ud835\udc61 \ud835\udc56 \u2208 { 1 , 2 , . . . , \ud835\udc3e } ; \ud835\udc66 \ud835\udc56 and \ud835\udc67 \ud835\udc56 are binary labels, with \ud835\udc66 \ud835\udc56 = 1 or \ud835\udc67 \ud835\udc56 = 1 representing click or conversion, respectively; \ud835\udc66 \ud835\udc56 \u2192 \ud835\udc67 \ud835\udc56 represents the order dependency in the chain, and when a subsequent task in the chain occurs, the label of the previous task is 1. There are three associated probabilities: postview click-through rate (CTR), post-click conversion rate (CVR), and post-view click&conversion rate (CTCVR): Wefollow the Neyman-Rubin potential outcome framework [22] to define the ITE estimation problem. We set \ud835\udc66 \ud835\udc56 ( 0 ) and \ud835\udc66 \ud835\udc56 ( \ud835\udc58 ) to represent the potential outcome of \ud835\udc5d\ud835\udc36\ud835\udc47\ud835\udc45 for the i-th instance in the control and k-th treatment group, respectively. Similarly, \ud835\udc67 \ud835\udc56 ( 0 ) and \ud835\udc67 \ud835\udc56 ( \ud835\udc58 ) represent the potential outcome of \ud835\udc5d\ud835\udc36\ud835\udc47\ud835\udc36\ud835\udc49\ud835\udc45 for the ith instance in the control and k-th treatment group, respectively. The challenge of estimating ITE is that we can only observe one potential outcome, either \ud835\udc66 \ud835\udc56 ( 0 ) or \ud835\udc66 \ud835\udc56 ( \ud835\udc58 ) , \ud835\udc67 \ud835\udc56 ( 0 ) or \ud835\udc67 \ud835\udc56 ( \ud835\udc58 ) , so there is no ground-truth label used for supervised training. We estimate the ITE under the k-th treatment for each instance, denoted as where \ud835\udf0f \ud835\udc4c \ud835\udc58 ( \ud835\udc65 ) and \ud835\udf0f \ud835\udc4d \ud835\udc58 ( \ud835\udc65 ) are the uplift scores of the CTR task and CTCVR task respectively. In the inference stage, we estimate the individual treatment effects of all samples for treatment assignment. For the terminal task (conversion) of the current chain, we consider the impact of the previous tasks by modeling the potential outcome of \ud835\udc5d\ud835\udc36\ud835\udc47\ud835\udc36\ud835\udc49\ud835\udc45 instead of \ud835\udc5d\ud835\udc36\ud835\udc49\ud835\udc45 to obtain unbiased ITE estimation \ud835\udf0f \ud835\udc4d \ud835\udc58 ( \ud835\udc65 ) .", "2.2 Initial Embedding Representation": "Given an instance ( \ud835\udc65 \ud835\udc56 , \ud835\udc61 \ud835\udc56 , \ud835\udc66 \ud835\udc56 , \ud835\udc67 \ud835\udc56 ) , we encode not only its non-treatment features but also its treatment feature. For the sparse non-treatment features of sample \ud835\udc65 \ud835\udc56 , we map the features through a parameter matrix \ud835\udc38 , and for dense features, we transform them to the same dimension through an affine transformation: (3) where W and b are learnable parameters. Then, we concatenate them to get the initial embedding representation of \ud835\udc65 \ud835\udc56 : where \ud835\udc51 is the dimension size of each field, and \ud835\udc53 is the number of feature fields. For the treatment feature, we treat it as a sparse feature and obtain the treatment embedding \ud835\udc38 \ud835\udc61\ud835\udc5f \ud835\udc56 \u2208 R 1 \u00d7 \ud835\udc51 by feature mapping through the parameter matrix \ud835\udc38 . In addition, to achieve task-enhanced learning, we obtain each task's features \ud835\udc38 \ud835\udc61\ud835\udc4e \ud835\udc56 \u2208 R 1 \u00d7 \ud835\udc51 in the chain through parameter matrix mapping, where \ud835\udc61\ud835\udc4e \u2208 { 0 , 1 } represents the click and conversion task.", "3 HOWIS CHAIN-BIAS MANIFESTED": "To study the uplift relationship at each stage in the chain, we analyze the public dataset CRITEO-UPLIFT and the dataset MT-LIFT collected from Meituan, detailed in Section 5.1.1. We randomly segment the data and draw the uplift graph with several selected segments, as shown in Fig. 3. We define \ud835\udc5f \ud835\udc61 \ud835\udc58 and \ud835\udc5f \ud835\udc50 \ud835\udc58 as the number of samples with \ud835\udc4d = 1 (conversion) in the k segment for the treatment and control groups, and \ud835\udc5b \ud835\udc61 \ud835\udc58 represents the number of samples for the treatment group, while \ud835\udc5b \ud835\udc50 represents the number of samples \ud835\udc58 for the control group. Next, we calculate the actual uplift \ud835\udc5f \ud835\udc61 \ud835\udc58 \ud835\udc5b \ud835\udc61 \ud835\udc58 -\ud835\udc5f \ud835\udc50 \ud835\udc58 \ud835\udc5b \ud835\udc50 \ud835\udc58 of each segment for drawing, where we calculate the actual uplift score of CTCVR (impression set) and CVR (click set). From the result, the changing trends of CTCVR and CVR between different segments are not entirely consistent, and we cannot get the final actual uplift for the chain just by CVR uplift on the click set. The main reason for this phenomenon is that users have different attention points at each stage of sequential behavior, resulting in varying degrees of impact of treatment on behavior at different stages of the chain. If we only use the uplift model trained on the click set, it will have a devastating bias on later marketing decisions. Therefore, we propose the ECUP to model the uplift score in the entire chain space and inject task prior information to capture the impact of treatment on different tasks for solving the chain-bias. MT-LIFT CRITEO-UPLIFT CVR uplift CVR uplift CTCVR uplift 0.02 CTCVR uplift 0.01 Segment Segment", "4 METHOD": "In this section, we will elaborate on our proposed ECUP. As shown in Fig. 4, the overall architecture of ECUP consists of two main components: FF wlo BP Entire Chain-Enhanced Network (ECENet) pCTCVR FF wl BP pCVR pCTR Element-wise product CVR Layer CTR Layer Task-Enhanced Network TAEGate Layer L-1 Layer L-1 TAEGate Shared Layer Layer CVR Task CTR Task Multi-head Key & Value Attention Qucry Treatment-Enhanced Gate Treatment-Enhanced Network (TENet) Treatment-Aware Unit Treatment-Aware Unit Eta Task feature Treatment feature User Item Contextual features \u00b7 Entire Chain-Enhanced Network (ECENet), which uses the user's sequential pattern to estimate the results of each task in the entire chain space and uses a Task-Enhanced Network (TAENet) to inject task prior information to achieve context-enhanced representation, capturing the different uplifts on each task to avoid the negative impact of chain-bias. \u00b7 Treatment-Enhanced Network (TENet), which aims to guide the treatment-aware refinement for initial features and implements bit-level adaptive adjustment of embedding representations under different treatments to solve the treatment-unadaptive problem.", "4.1 Entire Chain-Enhanced Network (ECENet)": "ECENet models the uplift score of each task in the entire chain space, which injects task information based on treatment-enhanced embedding (detailed in Section 4.2) to enhance both the treatment and task information, solve the chain-bias and alleviate the data sparsity of the subsequent task, whose structure is shown in Fig. 4. Individuals respond differently to different tasks in a specific treatment. We aim to capture task-aware feature representation, while traditional methods learn multi-task targets through multiple towers. Each tower is a stacked deep neural network (DNN), and its expressive ability is so limited that it cannot capture the different effects of treatment on different tasks, and giving specific prior knowledge to each layer of DNN can effectively inject specific information [5]. Therefore, we propose the Task-Enhanced network (TAENet) to deeply fuse task prior with treatment-enhanced representations (the output of TENET) to achieve context-enhanced parameter learning. It performs personalized scaling of DNN's parameters to balance the sparsity of features in different contexts. 4.1.1 Task-Enhanced Network (TAENet). To capture the responses of each instance in different contexts, we adaptively scale DNN's parameters with priori information via a gating mechanism. We use task information as priori input and then model them with treatment-enhanced embedding through a multi-head attention network. To ensure the stability of the initial and treatment embedding, we only update the task embedding \ud835\udc38 \ud835\udc61\ud835\udc4e . Specifically, we treat task information \ud835\udc38 \ud835\udc61\ud835\udc4e as Query and treatment-enhanced embedding \ud835\udc38 \ud835\udc61\ud835\udc5f as Key and Value to capture context-enhanced prior information: where \u2298 represents no gradient propagation, \u210e is the number of attention heads. Next, we inject prior information into the model through a Task-Enhanced Gate (TAEGate), which is a multi-layer perception (MLP): the middle layer of MLP uses the Relu activation function, and the last layer uses the Sigmoid activation function. \ud835\udefe is a regulatory factor that regulates the scaling size. Next, we use the output of TAEGate to scale with each layer of each DNN tower except the last layer. For one layer in DNN, we perform: where \ud835\udc3b \u2208 R \u210e \u00d7 \ud835\udc47 means that the layer has \u210e hidden units, and there are \ud835\udc47 tasks in the entire chain, and \ud835\udc47 = 2 in this paper. Note that \ud835\udeff \ud835\udc61\ud835\udc4e \u2208 R \u210e \u00d7 \ud835\udc47 will be divided into \ud835\udc47 vectors of dimension \u210e and execute the Equation 7 with the layer of each task, and then use the scaled one as the input of the next layer of DNN to achieve task-enhanced representation learning. The output of the last layer of each task uses the sigmoid activation function and is not affected by TAEGate. TAEGate captures the impact of treatment on different tasks by injecting task prior information, combined with treatmentenhanced embedding to achieve context-enhanced learning for addressing chain-bias. 4.1.2 Training & Inference. Unlike ESMM [20], which only shares the underlying parameters, we share all parameters except the last layer of each task. We use the \ud835\udc5d\ud835\udc36\ud835\udc47\ud835\udc45 and \ud835\udc5d\ud835\udc36\ud835\udc47\ud835\udc36\ud835\udc49\ud835\udc45 in model training to calculate the loss across the entire sample space: where \ud835\udc59 is the cross-entropy loss function, \ud835\udf06 and | | \ud835\udf03 | | are the tradeoff parameter and the regularization terms, and \ud835\udc66 \ud835\udc56 & \ud835\udc67 \ud835\udc56 indicates the sequential pattern from click to conversion, using the sequential dependencies to capture relationships between tasks in the chain. The parameter-sharing mechanism dramatically alleviates data sparsity in subsequent tasks, and TAEGate ensures that the model Treatment information Element-wise product cxtractor Element-wise sum Self-attention TAU Sigmoid function Trcatment feature TAUG Eeau (Wb TAUw Wb Uscr Item Contextual features TEGate can still capture the differential impact of treatment on each task during parameter sharing. At the end of model training, ECUP is regarded as an S-learner, and we obtain ITE through Equation 2.", "4.2 Treatment-Enhanced Network (TENet)": "The structure of TENet is shown in Fig. 5, which consists of two parts: the Treatment-Aware Unit (TAU) and the Treatment Enhancement Gate (TEGate). Inspired by [28], we propose bit-level weights to achieve fine-grained treatment-enhanced feature refinement. We first generate treatment-aware features and bit-level weights through two independent TAUs. Next, we use the obtained weights to balance the initial and treatment-aware features through the TEGate adaptively. 4.2.1 Treatment-Aware Unit (TAU). TAU consists of two parts: 1) Self-attention network, which is used to capture the cross relationship between user, item, and contextual features; 2) treatment information extractor, which is used to encode the treatment. TAU interacts with the cross-features extracted from self-attention with treatment information to obtain treatment-aware features and bitlevel weight. Self-attention network. We capture the cross-feature relationships between all feature pairs to obtain a refined representation of individuals in their natural state. Specifically, we map the initial features \ud835\udc38 \ud835\udc65 \u2208 R \ud835\udc53 \u00d7 \ud835\udc51 into three different matrices: where \ud835\udc4a \ud835\udc44 , \ud835\udc4a \ud835\udc3e , \ud835\udc4a \ud835\udc49 \u2208 R \ud835\udc51 \u00d7 \ud835\udc51 \ud835\udc58 is the learnable parameter matrix, and \ud835\udc51 \ud835\udc58 is the attention size. Next, we get the cross-feature through the attention matrices: Finally, we restore its dimensions through a linear transformation layer: Treatmentinformation extractor (TIE). Individuals' behavior often changes significantly in different treatments. To realize the adaptation of each feature to different treatments, we use a TIE to model treatment information. The input of the TIE is the treatment embedding \ud835\udc38 \ud835\udc61\ud835\udc5f , and we perform feature extraction through an MLP: Each layer of MLP uses the Relu activation function, and the output dimension of the last layer of MLP remains the same as the initial dimension. After obtaining the treatment information, we use \ud835\udc38 \ud835\udc4f\ud835\udc56\ud835\udc61 to measure the cross-representation \ud835\udc38 \ud835\udc4e\ud835\udc61\ud835\udc61 to get the treatment-aware representation: where \u2297 is the element-wise product. We achieve treatment awareness by fusing cross-features and treatment information, i.e., features have different representations in different treatments. We use two independent TAUs to get treatment-aware representation and bit-level weights: Next, we will introduce how to obtain treatment-enhanced representations through \ud835\udc38 \ud835\udc47\ud835\udc34\ud835\udc48 and \ud835\udc4a \ud835\udc4f . 4.2.2 Treatment-Enhanced Gate (TEGate). The initial features indicate the natural state of the instance, and some valuable information may be lost if the initial features are not entirely considered [28]. Therefore, after getting the treatment-aware embedding and bitlevel weights, we use a gating mechanism to fuse the two parts at the bit-level to get the final treatment-enhanced embedding: where \ud835\udf0e (\u00b7) is the sigmoid function. To explicit the expression of treatment, we concatenate treatment features on this basis: Compared with the traditional feature interaction by attention weight, our proposed TENet mainly has the following two advantages: 1) it balances the initial and treatment-aware embedding to adapt features in different treatments effectively; 2) it uses bitlevel feature interaction to achieve treatment-enhanced in a more fine-grained way.", "5 EXPERIMENTS": "We conducted extensive experiments to verify and analyze the performance of ECUP in uplift modeling. In this section, first, we introduce the content of the datasets, including the public dataset and the dataset collected from Meituan. We analyze it to elaborate on the chain-bias problem faced by the real industrial scene and further introduce the implementation details, baselines, and experimental settings. Then, we compare the performance of ECUP with baselines and analyze the role of each component of ECUP. Finally, we verify the performance of ECUP on a real large-scale online system.", "5.1 Experimental Setup": "5.1.1 Datasets. We evaluate our method on both public and realworld industrial datasets, and the detailed statistics are shown in Table 1. CRITEO-UPLIFT [9]: Criteo AI Labs constructed it through a collection of online controlled experiments on the network for large-scale advertising uplift modeling. It has nearly 14 million instances, 12 features, binary treatments, and two binary labels: visit and conversion. We estimate the visit rate and conversion rate as two tasks in the chain and randomly split the dataset for training and testing with a ratio of 7/3. Our collected dataset MT-LIFT: To comprehensively evaluate the proposed method, we collected a dataset from two months of intelligent coupon marketing scenarios for food delivery in the Meituan app, which is China's leading local living platform. It has multiple treatments and label information for the entire chain. To eliminate the impact of confounding factors on uplift modeling, we collect it from randomized controlled trials, where treatments were randomly assigned to ensure consistent potential distribution between the treatment and control groups, and the causal graph of the impression space is shown in Fig. 6. The dataset contains nearly 5.5 million instances, 99 features, treatment information, and two labels: click and conversion. Among them, treatment is not binary and has five options. We estimate CTR and CVR as two tasks in the chain, using last week's data for testing and the remaining data for training. Figure 6: Causal graph of the intelligent coupon marketing scenario during the collection period of our dataset MT-LIFT. The treatment (T) is independent of the individual features (X), and the two jointly affect subsequent click (Y) and conversion (Z) behaviors. 5.1.2 Implementation Details. We implement all the deep learning baselines and ELEU with PyTorch 1.9.0 using NVIDIA A100-SXM480GB. We use the area under the uplift curve (AUUC) and the area under the qini curve (QINI) as evaluation metrics, which are standard metrics to evaluate uplift models and computed using CausalML packages [6]. We use Adam as the optimizer, and the batch_size is 2 11 , the total number of layers of each tower is \ud835\udc3f is 3, the number of layers of treatment information extractor is 1, the number of layers of Task-Enhanced Gate is 2, and the number of heads of the multi-head attention mechanism is 2. For the remaining hyper-parameters, we search them using AUUC as a primary evaluation metric, and the range of the values of the hyper-parameters is shown in Table 2. 5.1.3 Baselines. To demonstrate the effectiveness of the ECUP, we selected the following competitive baselines: \u00b7 Causal Forest [2]: A non-parametric ITE estimation model based on generalized random forest. \u00b7 CEVAE [18]: A neural network latent variable model based on variational autoencoders for estimating individual and population causal effects. \u00b7 CFRNet [23]: A deep learning architecture using integral probability metrics to adjust treatment and control group distance for ITE estimation. \u00b7 TarNet [23]: A variant of CFRNet without balancing regularization, which learns the common information representation of treatment and control groups and then estimates ITE through their respective towers. \u00b7 DragonNet [24]: A neural network architecture for causal estimation based on the sufficiency of the propensity score with targeted regularization. \u00b7 DESCN [33]: A method for jointly learning the distribution of treatment and response functions in the entire sample space, which captures the integrated information of the treatment propensity, the response, and the hidden treatment effect through a cross network in a multi-task learning manner. \u00b7 ANU [29]: An uplift model with an attention mechanism, which generates after-treatment representation with relevant features of the given treatment, utilizes attention mechanisms to map the original covariate space into a latent space, sharing information about treatment and control groups. \u00b7 EFIN [17]: An explicit feature interaction aware uplift network, which models treatment features, uses attention weights to describe the sensitivity of non-treatment features to a particular treatment and uses an intervention constraint module to increase the model's robustness. 5.1.4 Experimental Settings. We evaluate the performance at two tasks: 1) Uplift estimation of the CTCVR task, which we evaluate on the dataset of all impressions. The existing uplift baselines are all single-task estimation methods. For a fair comparison, we construct two models for them, the CTR and the CVR models, and obtain outputs by calculating \ud835\udc5d\ud835\udc36\ud835\udc47\ud835\udc36\ud835\udc49\ud835\udc45 = \ud835\udc5d\ud835\udc36\ud835\udc47\ud835\udc45 \u00d7 \ud835\udc5d\ud835\udc36\ud835\udc49\ud835\udc45 . The CTR and CVR models are trained on the impression set and click set, respectively, and the model architecture and hyperparameters are consistent, which eliminates the negative impact of chain-bias to a certain extent. 2) Uplift estimation of CVR tasks on the click dataset.", "5.2 Overall Performance": "In the experiment, since most baselines can only handle binary treatment scenarios, we extended it reasonably to adapt to industrial data with multiple treatments, such as changing the network structure from 2-head to multi-head or constructing K independent models to model different treatments separately. For industrial dataset, we consider them as multiple binary treatment tasks and report their average performance. We report the performance of each model on two datasets in Table 3. Among the baselines, DESCN proposes to model the uplift score in the entire space, but it refers to the entire treatment and control group data space, which is difficult to expand to multiple treatments and cannot solve the chain-bias. For the results of the CTCVR task, we can draw the following conclusions: 1) Some methods show significant performance degradation on MT-LIFT data, which indicates that capturing user behavior changes under various treatments is more difficult in feature-rich marketing scenarios. 2) The methods EFIN and ANU, which consider the interaction between treatment information and other features, bring relatively significant improvements. For MTLIFT data, separate modeling of treatment information brings more significant improvements. 3) Although we separately model CTR and CVR to alleviate the chain-bias in baselines, ECUP achieves excellent performance on the vast majority of metrics, thanks to fine-grained treatment interactions and entire chain space modeling, effectively solving the chain-bias and treatment-unadaptive problems. We can observe similar results on the CVR task, and ECUP achieved optimal performance on most metrics. ECUP models uplift in the entire chain space, and the rich data of previous tasks of the chain can effectively alleviate the data sparsity problem of subsequent tasks, achieving better uplift estimation.", "5.3 Ablation Study": "To assess the effectiveness of each component in ECUP, we conducted a series of ablation studies on the CTCVR task using the MT-LIFT dataset. Specifically, we build several variants of the ECUP: \u00b7 w/o TENet: A variant of ECUP without the TENet, which concatenates the treatment feature with other features as input to the model. \u00b7 w/ Attention: A variant of ECUP with the attention module, which uses a multi-head attention mechanism to replace TENet, models the interaction between treatment and other features. \u00b7 w/o TAEGate: A variant of ECUP without the TAEGate, which does not inject the task's prior information and only captures the impact of treatment on different tasks through the DNN layer. \u00b7 w/o ECENet: A variant of ECUP without the ECENet, which does not model ITE in the entire chain space, does not inject task prior information and trains directly on the click set. This variant evaluates the performance of CVR uplift in real application scenarios (impression space). Table 4 shows the results of the ablation study, and we can draw the following conclusions. Without the TENet, the model's performance decreases; after using the attention mechanism to interact the treatment with other features, the performance has improved but is still not as good as TENet, which illustrates the importance of fine-grained treatment-enhanced feature adaptation. The same results can be observed without the TAEGate; injecting task prior information can help the model capture individual response changes for different tasks. Without the ECENet, the model performance degrades most severely, indicating that the model trained on the click set makes achieving excellent performance in real application scenarios difficult. Through the component analysis of ECUP, we conclude that 1) all components in ECUP contribute to the ITE estimation performance improvement; 2) the ECENet structure is crucial as it accurately captures the degree of individual response changes to different tasks under a specific treatment, eliminating the negative impact of chain-bias on subsequent decisions.", "5.4 Performance on Online System (Meituan)": "To evaluate the online performance of ECUP, we deployed it on the Meituan and conducted a rigorous A/B test for two weeks. Our online scenario adopts a two-stage structure, first obtaining each user's rating on each coupon through the uplift model and then assigning treatment. The baseline model on the online platform is a T-learner, and its basic is a neural network (NN) model, a CVR uplift model trained on the click set. We use three important online metrics for evaluation: the marketing Return On Investment (ROI), the Gross Merchandise Volume (GMV), and the Order Quantity (OQ). Table 5 shows the improvement of ECUP relative to the baseline. The results indicate that ECUP has significantly improved in all online metrics compared to the baseline, achieving significant business benefits. This means that our ECUP can more effectively capture user behavior changes under different treatments and obtain more accurate uplift scores.", "6 RELATED WORK": "", "6.1 Uplift Modeling": "Existing uplift research mainly has two directions: 1) Accurate uplift estimation. The meta-learning method has become a popular framework in uplift estimation by easily integrating existing models [30]. The T-learner [15] applies two models to learn conditional treated/control outcomes and estimates ITE through the subtraction between the two models in the inference stage. In contrast, the S-learner [15] treats treatment as a feature and estimates the combined outcome by a single model, estimating ITE through the subtraction between outcomes from different treatments. The performance of the basic model has a cascading effect on the metalearner, and the deep learning model has become the basic model of each framework due to its excellent feature extraction capabilities [3, 29-31]. 2) Bias calibration. Since the actual treatment is allocated by particular treatment strategies (not by random), there are differences in the potential distribution and data volume between the treatment and control groups (called treatment bias), and existing research performs bias calibration at the data and model levels [1, 12, 14, 21, 25]. Tree-based models [7, 27] and some meta-learning [15] can alleviate this problem, and various deep learning methods have achieved better performance in distribution debiasing [13, 33]. However, existing methods have yet to study chain-bias, which will have a severe negative impact on later marketing decisions. In this paper, we achieve accurate and unbiased uplift estimation by context-enhanced representation and modeling the uplift influence relationship between entire chain tasks, which can quickly generalize to multi-treatment and other chain uplift estimation problems.", "6.2 Multi-Task Learning": "This paper uses the multi-task method to model the uplift of the entire chain. Multi-task learning aims to model multiple related tasks simultaneously and learn shared representations from multitask supervision signals. Compared with single-task learning, the memory footprint is reduced due to their inherent layer sharing, and the effective use of related tasks to share complementary information or act as a regularizer for each other can effectively improve performance [26]. Before the era of deep learning, researchers tried to model common information between tasks to get better generalization performance [8, 34]. As deep learning gradually matures, researchers use multi-task supervision signals to learn shared representations. In the hard parameter sharing days, the parameter set was divided into shared and task-specific parameters, usually branched to the task-specific head by a shared encoder [4, 11]. To avoid negative transfer and achieve better performance, researchers propose soft parameter sharing, where each task is assigned its own parameters, and using a feature sharing mechanism to handle crosstask dialogue [10]. Besides, other methods use gating mechanisms and expert networks to improve fine-grained learning [5, 19]. Although multi-task learning has been successful on many domains, research in uplift modeling still needs to be done. We aim to bridge the gap in this research direction by designing a multitask learning method to solve the chain-bias and alleviate the data sparsity problem in subsequent tasks.", "7 CONCLUSIONS AND FUTURE WORK": "In this paper, we studied the uplift modeling in a chain. We first elaborated on the chain-bias and treatment-unadaptive problems in uplift modeling and proposed the Entire Chain UPlift method with context-enhanced learning (ECUP) to address these challenges. ECUP models the uplift score in the entire chain space through the Entire Chain Enhanced Network to solve the chain-bias and data sparsity in subsequent tasks, capturing the impact of treatment on different tasks by injecting prior task information. ECUP further achieves fine-grained and treatment-aware feature adaptation through the Treatment Enhanced Network to accurately grasp the impact of features on results under different treatments. We demonstrated that ECUP outperforms state-of-the-art models through extensive offline and online experiments and released a large-scale unbiased dataset collected from Meituan for future research. Unbiased data construction requires a particular cost and a period of online performance loss, which cannot be easily constructed in many scenarios. In the future, we will explore multiple chain uplift modeling methods to reduce the model's requirements for data quality.", "REFERENCES": "[1] Alberto Abadie, David Drukker, Jane Leber Herr, and Guido W Imbens. 2004. Implementing matching estimators for average treatment effects in Stata. The stata journal 4, 3 (2004), 290-311. [2] Susan Athey, Julie Tibshirani, and Stefan Wager. 2019. GENERALIZED RANDOM FORESTS. The Annals of Statistics 47, 2 (2019), 1148-1178. [3] Ioana Bica, James Jordon, and Mihaela van der Schaar. 2020. Estimating the effects of continuous-valued interventions using generative adversarial networks. Advances in Neural Information Processing Systems 33 (2020), 16434-16445. [4] David Br\u00fcggemann, Menelaos Kanakis, Stamatios Georgoulis, and Luc Van Gool. 2020. Automated Search for Resource-Efficient Branched Multi-Task Networks. In 31st British Machine Vision Conference 2020, BMVC 2020 . BMVA Press, 359. [5] Jianxin Chang, Chenbin Zhang, Yiqun Hui, Dewei Leng, Yanan Niu, Yang Song, and Kun Gai. 2023. Pepnet: Parameter and embedding personalized network for infusing with personalized prior information. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 3795-3804. [6] Huigang Chen, Totte Harinen, Jeong-Yoon Lee, Mike Yung, and Zhenyu Zhao. 2020. Causalml: Python package for causal machine learning. arXiv preprint arXiv:2002.11631 (2020). [7] Hugh A Chipman, Edward I George, and Robert E McCulloch. 2010. BART: Bayesian additive regression trees. (2010). [8] Abhishek Kumar Hal Daum\u00e9 III and A Kumar. 2013. Learning task grouping and overlap in multi-task learning. In International Conference on Machine Learning . 1723-1730. [9] Eustache Diemert, Artem Betlei, Christophe Renaudin, Massih-Reza Amini, Th\u00e9ophane Gregoir, and Thibaud Rahier. 2021. A large scale benchmark for individual treatment effect prediction and uplift modeling. arXiv preprint arXiv:2111.10106 (2021). [10] Yuan Gao, Jiayi Ma, Mingbo Zhao, Wei Liu, and Alan L Yuille. 2019. Nddr-cnn: Layerwise feature fusing in multi-task cnns by neural discriminative dimensionality reduction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition . 3205-3214. [11] Pengsheng Guo, Chen-Yu Lee, and Daniel Ulbricht. 2020. Learning to branch for multi-task learning. In International conference on machine learning . PMLR, 3854-3863. [12] Guido W Imbens and Donald B Rubin. 2015. Causal inference in statistics, social, and biomedical sciences . Cambridge University Press. [13] Wenwei Ke, Chuanren Liu, Xiangfu Shi, Yiqiao Dai, S Yu Philip, and Xiaoqiang Zhu. 2021. Addressing exposure bias in uplift modeling for large-scale online advertising. In 2021 IEEE International Conference on Data Mining (ICDM) . IEEE, 1156-1161. [14] Kun Kuang, Peng Cui, Bo Li, Meng Jiang, and Shiqiang Yang. 2017. Estimating treatment effect in the wild via differentiated confounder balancing. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining . 265-274. [15] S\u00f6ren R K\u00fcnzel, Jasjeet S Sekhon, Peter J Bickel, and Bin Yu. 2019. Metalearners for estimating heterogeneous treatment effects using machine learning. Proceedings of the national academy of sciences 116, 10 (2019), 4156-4165. [16] Ying-Chun Lin, Chi-Hsuan Huang, Chu-Cheng Hsieh, Yu-Chen Shu, and Kun-Ta Chuang. 2017. Monetary discount strategies for real-time promotion campaign. In Proceedings of the 26th International Conference on World Wide Web . 1123-1132. [17] Dugang Liu, Xing Tang, Han Gao, Fuyuan Lyu, and Xiuqiang He. 2023. Explicit Feature Interaction-aware Uplift Network for Online Marketing. arXiv preprint arXiv:2306.00315 (2023). [18] Christos Louizos, Uri Shalit, Joris M Mooij, David Sontag, Richard Zemel, and Max Welling. 2017. Causal effect inference with deep latent-variable models. Advances in neural information processing systems 30 (2017). [19] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. 2018. Modeling task relationships in multi-task learning with multi-gate mixture-ofexperts. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1930-1939. [20] Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu, and Kun Gai. 2018. Entire space multi-task model: An effective approach for estimating post-click conversion rate. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval . 1137-1140. [21] Paul R Rosenbaum. 1987. Model-based direct adjustment. Journal of the American statistical Association 82, 398 (1987), 387-394. [22] Donald B Rubin. 2005. Causal inference using potential outcomes: Design, modeling, decisions. J. Amer. Statist. Assoc. 100, 469 (2005), 322-331. [23] Uri Shalit, Fredrik D Johansson, and David Sontag. 2017. Estimating individual treatment effect: generalization bounds and algorithms. In International conference on machine learning . PMLR, 3076-3085. [24] Claudia Shi, David Blei, and Victor Veitch. 2019. Adapting neural networks for the estimation of treatment effects. Advances in neural information processing systems 32 (2019). [25] Elizabeth A Stuart. 2010. Matching methods for causal inference: A review and a look forward. Statistical science: a review journal of the Institute of Mathematical Statistics 25, 1 (2010), 1. [26] Simon Vandenhende, Stamatios Georgoulis, Wouter Van Gansbeke, Marc Proesmans, Dengxin Dai, and Luc Van Gool. 2021. Multi-task learning for dense prediction tasks: A survey. IEEE transactions on pattern analysis and machine intelligence 44, 7 (2021), 3614-3633. [27] Stefan Wager and Susan Athey. 2018. Estimation and inference of heterogeneous treatment effects using random forests. J. Amer. Statist. Assoc. 113, 523 (2018), 1228-1242. [28] Fangye Wang, Yingxu Wang, Dongsheng Li, Hansu Gu, Tun Lu, Peng Zhang, and Ning Gu. 2022. Enhancing CTR prediction with context-aware feature representation learning. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval . 343-352. [29] Guoqiang Xu, Cunxiang Yin, Yuchen Zhang, Yuncong Li, Yancheng He, Jing Cai, and Zhongyu Wei. 2022. Learning discriminative representation base on attention for uplift. In Pacific-Asia Conference on Knowledge Discovery and Data Mining . Springer, 200-211. [30] Liuyi Yao, Zhixuan Chu, Sheng Li, Yaliang Li, Jing Gao, and Aidong Zhang. 2021. A survey on causal inference. ACM Transactions on Knowledge Discovery from Data (TKDD) 15, 5 (2021), 1-46. [31] Jinsung Yoon, James Jordon, and Mihaela Van Der Schaar. 2018. GANITE: Estimation of individualized treatment effects using generative adversarial nets. In International conference on learning representations . [32] Kui Zhao, Junhao Hua, Ling Yan, Qi Zhang, Huan Xu, and Cheng Yang. 2019. A unified framework for marketing budget allocation. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 1820-1830. [33] Kailiang Zhong, Fengtong Xiao, Yan Ren, Yaorong Liang, Wenqing Yao, Xiaofeng Yang, and Ling Cen. 2022. DESCN: Deep Entire Space Cross Networks for Individual Treatment Effect Estimation. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 4612-4620. [34] Jiayu Zhou, Jianhui Chen, and Jieping Ye. 2011. Clustered multi-task learning via alternating structure optimization. Advances in neural information processing systems 24 (2011)."}
