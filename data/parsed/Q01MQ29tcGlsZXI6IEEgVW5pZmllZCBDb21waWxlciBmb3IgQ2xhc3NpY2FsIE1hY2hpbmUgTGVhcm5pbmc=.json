{"CMLCompiler: A Unified Compiler for Classical Machine Learning": "", "Xu Wen": "Institute of Computing Technology, Chinese Academy of Sciences University of Chinese Academy of Sciences Beijing, China wenxu@ict.ac.cn", "Wanling Gao": "Institute of Computing Technology, Chinese Academy of Sciences University of Chinese Academy of Sciences Beijing, China gaowanling@ict.ac.cn", "Anzheng Li": "Institute of Computing Technology, Chinese Academy of Sciences University of Chinese Academy of Sciences Beijing, China lianzheng20g@ict.ac.cn", "Lei Wang": "Institute of Computing Technology, Chinese Academy of Sciences University of Chinese Academy of Sciences Beijing, China wanglei_2011@ict.ac.cn", "Zihan Jiang": "Huawei Technologies Co., Ltd. Beijing, China jiangzihan0512@gmail.com", "ABSTRACT": "Classical machine learning (CML) occupies nearly half of machine learning pipelines in production applications. Unfortunately, it fails to utilize the state-of-the-practice devices fully and performs poorly. Without a unified framework, the hybrid deployments of deep learning (DL) and CML also suffer from severe performance and portability issues. This paper presents the design of a unified compiler, called CMLCompiler, for CML inference. We propose two unified abstractions: operator representations and extended computational graphs. The CMLCompiler framework performs the conversion and graph optimization based on two unified abstractions, then outputs an optimized computational graph to DL compilers or frameworks. We implement CMLCompiler on TVM. The evaluation shows CMLCompiler's portability and superior performance. It achieves up to 4.38 \u00d7 speedup on CPU, 3.31 \u00d7 speedup on GPU, and 5.09 \u00d7 speedup on IoT devices, compared to the stateof-the-art solutions - scikit-learn, intel sklearn, and hummingbird. Our performance of CML and DL mixed pipelines achieves up to 3.04x speedup compared with cross-framework implementations. The project documents and source code are available at https://www.computercouncil.org/cmlcompiler.", "CCS CONCEPTS": "\u00b7 Computingmethodologies \u2192 Machinelearning ; \u00b7 Computer systems organization \u2192 Real-time systems .", "KEYWORDS": "Classical Machine Learning, Deep Learning, Compiler", "1 INTRODUCTION": "Deep learning (DL) and classical machine learning (CML), collectively called machine learning (ML), have played an increasingly \u2217 Corresponding author.", "Jianfeng Zhan \u2217": "Institute of Computing Technology, Chinese Academy of Sciences University of Chinese Academy of Sciences Beijing, China zhanjianfeng@ict.ac.cn Figure 1: The CMLCompiler design. Our contributions are highlighted in green color. Hardware CPU GPU IoT ... Models Linear Models Trees SVMs ... Compiler Framework Unified Abstractions CMLCompiler DL Frameworks (PyTorch) PyTorch Runtime DL compilers (TVM) TVM Runtime critical role in recent years. DL refers to those neural network models, such as convolutional neural networks (CNNs) [24], recurrent neural networks (RNNs) [28], and generative adversarial networks (GANs) [16]. Different from DL, CML represents a set of non-neural network models in ML, e.g., linear models [37], decision trees [26], random forests [4], and support vector machines [42]. DL stands out because of its accuracy, while CML is still widely used for lower time and energy costs. Doris Xin et al. [47] analyze 3000 production ML pipelines at Google and find that 40% of them use CML models. Besides, many real-world applications adopt hybrid deployments of CML and DL [2] to guarantee high accuracy and low latency [25, 27, 36, 38], e.g., DL models for feature embedding and CML models for classification or regression. DL compilers, like TVM [7, 10, 23], provide a structural approach to tackle the portability issue and facilitates wide deployment of DL models on a broad spectrum of devices like GPUs, FPGAs, and IoT devices and guarantees an appreciable performance. DL compilers 1 Xu Wen, Wanling Gao, Anzheng Li, Lei Wang, Zihan Jiang, and Jianfeng Zhan use computational graphs as high-level abstractions, supporting a large variety of DL models. Meanwhile, DL compilers propose low-level abstractions such as tensor representation to generate executable code. For newborn hardware, the vendor just needs to provide hardware primitives, instead of a sophisticated highperformance library that is prohibitively costly. Based on the tensor representation and computational graphs abstractions, many optimizations [8, 22, 49] are proposed to boost performance, e.g., they provide sophisticated support for CPU processor architectures as the latter has different architectures, diverse core numbers, extended instructions, and cache sizes. Our intuition is to enable CML to leverage DL's well-defined unified abstractions and highly mature compilers, optimization technologies, and frameworks. Unfortunately, it is not a trivial task. There are significant distinctions in operators and models between CML and DL. DL operators focus on tensors, while CML handles arrays, matrices, scalars, and tables. DL models are all neural network models, while CML models, such as decision trees, can hardly be represented as neural networks. Most DL models are expressible as flat sequences of operations without if-statements [35], but if-statements frequently occur in CML models. Existing DL abstractions, such as tensor representation and computational graphs, can not directly represent CML operators and models. Those distinctions determine CML can hardly leverage the DL ecosystems directly. Several efforts attempt to support CML models on DL frameworks, e.g., TensorFlow [1] provides a CPU-based decision forest library TF-DF [43]. However, these attempts do not solve the generality and portability issue. They only support a narrower range of models, lacking support for GPUs and IoT devices. However, despite its popularity and importance, CML suffers from severe portability and performance issues. State-of-the-practice and state-of-the-art CML frameworks [17, 29, 32] provide ad-hoc solutions, implementing each CML model on every hardware device case by case due to the lack of unified abstractions. These ad-hoc solutions raise considerable difficulties in developing a generalpurpose framework and optimization techniques to achieve optimal performance for every model. They either lack the support or only partially support various hardware devices, such as GPUs, FPGAs, and IoT devices. In addition, adding support for a model on a new hardware device needs great effort, more than several thousands of lines of codes [13], let alone hundreds or thousands of models and devices. Moreover, they also face performance issues. Even on the CPUs - the most popular CML platform, the performance is unsatisfactory due to the lack of specific optimizations for advanced characteristics like multi-cores and SIMD. The hybrid deployment of CML and DL models faces more severe problems. This paper focuses on CML inference for the first step, considering its great significance that occupies nearly half of the total cost [2] and its wide applications in online serving, Internet of things (IoT), etc [18, 46]. We will extend our work to CML training in the near future. As illustrated in Fig. 1, we propose a unified compiler, CMLCompiler, for CML inference, which enables CML to leverage the mature DL ecosystems. At the core of CMLCompiler are two unified abstractions: operator representations and extended computational graphs (ECGs) and a compiler framework. Operator representations convert CML operators into tensor formats, while an ECG organizes these converted operators in an optimizationfriendly way. The two unified abstractions define how to convert and translate CML models into DL computational graphs, which can be recognized and executed by DL frameworks and compilers. The CMLCompiler framework consists of four modules - operator converter, model parser, graph optimizer, and graph translator. The CMLCompiler framework performs the conversion and graph optimization based on two unified abstractions, then outputs an optimized DL computational graph to DL compilers or frameworks. CMLCompiler can also optimize the mixed pipelines of CML and DL. As TVM provides portability and sophisticated optimizations, we choose to implement CMLCompiler on TVM. Currently, it supports up to 35 CML models. This paper makes the following contributions: \u00b7 We propose two unified abstractions - operator representations and extended computational graphs- to represent CML operators and models. \u00b7 CMLCompiler enables the hybrid deployment of CML and DL with a unified framework. \u00b7 We present the design of CMLCompiler, a unified compiler for CML inference, based on these abstractions. The CMLCompiler framework performs the conversion and graph optimization based on two unified abstractions, then outputs an optimized DL computational graph to DL compilers or frameworks. \u00b7 We implement CMLCompiler on top of TVM, achieving up to 4.38x speedup on CPU, 3.31x speedup on GPU, and 5.09x speedup on IoT devices, compared to the state-of-the-art solutions - scikit-learn, intel sklearn, and hummingbird. Our support for CML and DL mixed pipelines achieves up to 3.04x speedup compared with cross-framework implementations. The remainder of the paper is organized as follows. Section 2 introduces the motivation. Section 3 introduces unified abstractions. Section 4 shows design and implementation. Section 5 presents our evaluation. Section 6 illustrates the related work. Finally, we draw a conclusion in Section 7.", "2 MOTIVATION": "CML faces severe portability and performance issues. Fig. 2 compares the performance of sklearn, the most widely used CML framework on GitHub [33]- against CMLCompiler leveraging DL compilers. We find that sklearn can not support GPUs and only supports IoT devices partially. Adding support for a new hardware device needs great effort due to the ad-hoc implementations. For example, adding support for random forest on GPU needs 2.7k lines of code [13]. Many models and hardware devices need to be supported, requiring hundreds or thousands of more effort. Moreover, due to the lack of compilation support for CPU features, sklearn has poor performance. As shown in Fig. 2, CMLCompiler achieves 2.3x speedup by utilizing AVX2 through compilation compared with sklearn. Other CML frameworks such as Spark MLlib [29] and H2O [17] face the same problems. Our solution is to propose unified abstractions to utilize DL compilers and frameworks, achieving portability and high performance. 2 CMLCompiler: A Unified Compiler for Classical Machine Learning Figure 2: This figure compares the performance of sklearn, the most widely used CML framework on GitHub [33]against CMLCompiler. Our evaluation shows that sklearn suffers from both performance and portability issues for a lack of unified abstractions. \" \u00d7 \" means unsupported. CPU IoT GPU RobustScaler 10 1 10 2 time(ms) sklearn our work IoT CPU GPU DecisionTreeClassifier 10 1 10 2 sklearn our work CML and DL models are often deployed hybrid in NLP [36], intelligent healthcare [38], recommendation systems [25], etc., especially in scenarios with limited computational power and small datasets. Many of them are deployed on heterogeneous hardware devices for online serving. As there is no unified system, different frameworks are deployed with three disadvantages. First, this limits the portability. If one framework fails on the target device, the whole pipeline corrupts. Second, there are extra costs due to data conversions across frameworks. Third, it is hard to make optimizations across different frameworks. Using a unified framework can overcome these disadvantages, so we add the support for hybrid deployment of CML and DL in CMLCompiler.", "3 THE UNIFIED ABSTRACTIONS": "CMLCompiler takes CML models as input and returns DL computational graphs as output, utilizing DL frameworks or compilers to compile and deploy them. At the core of CMLCompiler are two unified abstractions. Operator representations are used to represent CML operators in tensor format, as shown in Section 3.1. Extend computational graph (ECG) organizes operator representations in an optimization-friendly way and can be used to represent CML models, as shown in Section 3.2. Section 3.3 shows the supported algorithms and extensions for other algorithms.", "3.1 Operator Representation": "An operator representation uses a combination of one or more DL operators with tensors as input and output to represent a CML operator. We convert CML operators into DL operators and wrap them in the format of operator representations. Data in CML has mainly four formats: arrays, matrices, scalars, and tables [44]. Matrices and arrays are regarded as two types of tensors whose operators can naturally be converted into DL operators. When CML models deal with tables, they take numeric data from tables and operate it, which can also be regarded as scalars. Hereby, we focus on the operators on scalars. 3.1.1 Operator categories and corresponding representations. As shown in Table 1, we classify CML operators into six categories and provide operator representations, respectively. (1) Assignment operators assign values to variables. If we assign n values \ud835\udc63 1 , \ud835\udc63 2 , ..., \ud835\udc63 \ud835\udc5b to n variables \ud835\udc65 1 , \ud835\udc65 2 , ..., \ud835\udc65 \ud835\udc5b , we organize these variables and values in two tensors \ud835\udc4b = [ \ud835\udc65 1 , \ud835\udc65 2 , ..., \ud835\udc65 \ud835\udc5b ] and \ud835\udc49 = [ \ud835\udc63 1 , \ud835\udc63 2 , ..., \ud835\udc63 \ud835\udc5b ] . Then we assign tensor V to tensor X to replace n scalar assignments. Tensor assignments benefit memory copy which stores data in block. (3) Basic arithmetic operators refer to those arithmetic calculations based on scalars, such as \ud835\udc4e\ud835\udc51\ud835\udc51 , \ud835\udc60\ud835\udc62\ud835\udc4f , \ud835\udc5a\ud835\udc62\ud835\udc59 , and \ud835\udc51\ud835\udc56\ud835\udc63 . We use element-wise arithmetic operators based on tensors to replace them, which can utilize SIMD instructions better. (2) Swap operators swap two or more variables. These variables can be represented in a tensor format and use reorganization operators such as \ud835\udc5f\ud835\udc52\ud835\udc60\u210e\ud835\udc4e\ud835\udc5d\ud835\udc52 to swap the elements. (4) Aggregation operators refer to operators that calculate aggregates among many scalars, such as \ud835\udc5a\ud835\udc56\ud835\udc5b , \ud835\udc5a\ud835\udc4e\ud835\udc65 , \ud835\udc60\ud835\udc62\ud835\udc5a , and \ud835\udc4e\ud835\udc63\ud835\udc54 . Reduction operators can be used to accomplish that. (6) Conditional operators are used to represent if-else statements, in the form of \ud835\udc56 \ud835\udc53 ( \ud835\udc52\ud835\udc65\ud835\udc5d\ud835\udc5f 1 ) \ud835\udc52\ud835\udc65\ud835\udc5d\ud835\udc5f 2 \ud835\udc52\ud835\udc59\ud835\udc60\ud835\udc52 \ud835\udc52\ud835\udc65\ud835\udc5d\ud835\udc5f 3 , where \ud835\udc52\ud835\udc65\ud835\udc5d\ud835\udc5f 1 is a comparison operator. If \ud835\udc52\ud835\udc65\ud835\udc5d\ud835\udc5f 2 and \ud835\udc52\ud835\udc65\ud835\udc5d\ud835\udc5f 3 are all assignment or arithmetic operators, we convert all three expressions into tensors. However, the situation gets tricky if one of \ud835\udc52\ud835\udc65\ud835\udc5d\ud835\udc5f 2 or \ud835\udc52\ud835\udc65\ud835\udc5d\ud835\udc5f 3 is still a conditional operator. We call those operators sequential conditional operators. Sequential conditional operators may contain many conditions, where each element in a tensor may have quite different decision paths. The complexity of decision paths makes it difficult to convert those operators into tensor operators. Those frequent if-else statements perform poorly on hardware devices such as GPUs and ASICs. Sequential conditional operators are the most delicate, and we defer their discussion later. (5) Comparison operators make a comparison between scalars and return True or False, such as \ud835\udc59\ud835\udc52\ud835\udc60\ud835\udc60 , \ud835\udc52\ud835\udc5e\ud835\udc62\ud835\udc4e\ud835\udc59 , and \ud835\udc54\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc5f . Comparisons with the same operator can be represented in a tensor format and use an element-wise comparison to replace. 3.1.2 Conditional operators representation. Weanalyze those widely used CML models and find that sequential conditional operators mainly occur in tree-based models. So we use decision tree as an example to introduce the representation of conditional operators in detail, as shown in Fig. 3. We use the combination of DL operators to represent those sequential conditional operators. The top-right shows the original decision tree. The input data is a list of samples; each has many features. \ud835\udc3c refers to internal nodes, numbered in the order of Level Order Traversal. \ud835\udc3f refers to leaf nodes, numbered in the order of In-Order Traversal. Each leaf node is an assignment operator, reaching which node determines the final result. The left shows the corresponding CML operator graph. Scalar-based assignments assign features \ud835\udc39 \ud835\udc57 to temporary variables \ud835\udc65 \ud835\udc56 , scalar-based comparisons compare \ud835\udc65 \ud835\udc56 with thresholds \ud835\udc47 \ud835\udc56 , and conditional operators determine the next nodes to be reached. These scalar-based CML operators are converted to operator representations, as shown in the bottom-right in Fig. 3, and the definitions and properties of weights are shown in Table 2. Multi Scalar-based assignments are converted to one Tensor-based assignment. ((1) in Fig. 3) The data processing order differs from its layout in the tensor, we use the \ud835\udc5f\ud835\udc52\ud835\udc5c\ud835\udc5f\ud835\udc54\ud835\udc4e\ud835\udc5b\ud835\udc56\ud835\udc67\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b operator to change the tensor layout, which is replaced by \ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5a\ud835\udc62\ud835\udc59 with 0-1 matrix \ud835\udc4a 1 . ((1) in Fig. 3) Multi Scalar-based comparisons are converted to one Tensor-based element-wise comparison. ((2) in Fig. 3) Sequential conditional operators are represented by the combination of \ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5a\ud835\udc62\ud835\udc59 and \ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc5a\ud835\udc4e\ud835\udc65 . ((3) in Fig. 3) Output returns the leaf nodes every sample reaches. 3 Xu Wen, Wanling Gao, Anzheng Li, Lei Wang, Zihan Jiang, and Jianfeng Zhan Table 1: The summary of operator representation. Each operator representation represents a CML operator. Scalars are marked as lower-case letters, while tensors are marked as upper-case letters. EW is short for element-wise. Figure 3: Converting Decision Tree to operator representations. Sequential conditional operators are represented by the combination of \ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5a\ud835\udc62\ud835\udc59 and \ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc5a\ud835\udc4e\ud835\udc65 , \ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5a\ud835\udc62\ud835\udc59 is short for matrix multiplication. \ud835\udc39 , \ud835\udc47 , \ud835\udc3c , and \ud835\udc3f refer to features, thresholds, internal nodes, and leaf nodes. \ud835\udc4a 1 , \ud835\udc4a 2 , and \ud835\udc4a 3 are the weights of DL operators, whose definitions and properties are shown in Table 2. F5 < T1 F1 < T 2 F4 < T3 L2 F2 <  T 4 L1 L3 L4 L5 True False I 1 I 2 I 3 I 4 CML operator graph If(True) I2 else I3 x1<T1 x1<-F5 If(True) I4 else L3 x2<T2 x2<-F1 If(True) L4 else L5 x3<T3 x3<-F4 If(True) L1 else L2 x4<T4 x4<-F2 L3 L4 L5 L1 L2 I 1 I 3 I 2 I 4 Scalar-based assignment Scalar-based comparsion Conditional operators Decision Tree matmul greater matmul argmax W1 W2 W3 Input Output (1) Tensor-based assignment & reorganization (2) Tensor-based element-wise comparison (3) Use matmul and argmax to represent conditional operators (1) (2) (3) Operator Representation Table 2: The properties of weights in Fig. 3. \ud835\udc41 \ud835\udc46 , \ud835\udc41 \ud835\udc39 , \ud835\udc41 \ud835\udc3c , and \ud835\udc41 \ud835\udc3f refer to the number of samples, features, internal nodes, and leaf nodes, respectively. \ud835\udc3c\ud835\udc5b\ud835\udc5d\ud835\udc62\ud835\udc61 \u2208 R \ud835\udc41 \ud835\udc46 \u00d7 \ud835\udc41 \ud835\udc39 means \ud835\udc41 \ud835\udc46 samples, each has \ud835\udc41 \ud835\udc39 features. \ud835\udc4a 1 \u2208 { 0 , 1 } \ud835\udc41 \ud835\udc39 \u00d7 \ud835\udc41 \ud835\udc3c captures the relationship between features and internal nodes. \ud835\udc4a 2 \u2208 R \ud835\udc41 \ud835\udc3c is the thresholds used in internal nodes. \ud835\udc4a 3 \u2208 { 0 , 1 } \ud835\udc41 \ud835\udc3c \u00d7 \ud835\udc41 \ud835\udc3f represents the structure between internal nodes and leaf nodes. \ud835\udc42\ud835\udc62\ud835\udc61\ud835\udc5d\ud835\udc62\ud835\udc61 \u2208 N \ud835\udc41 \ud835\udc46 returns the leaf node index each sample reaches. Dtype is the data type of weights. Sparsity is the ratio of non-zero data to all data in weights. Finally, a decision tree is converted to the combination of three operator representations. 3.1.3 The features of CML operator representations. As described above, we represent CML operators in the format of operator representations. These operator representations have unique features different from operators in DL models. Second, the frequent operators in DL and CML are not the same. Almost all operators in DL take float32 as input and return float32 as output. CML uses many comparison operators, such as \ud835\udc59\ud835\udc52\ud835\udc60\ud835\udc60 , \ud835\udc52\ud835\udc5e\ud835\udc62\ud835\udc4e\ud835\udc59 , First, the weights of DL operators and CML operator representations have different meanings. The weights in DL models are all learnable parameters. Without approximate optimizations such as pruning and quantization, those weights are dense, and the data type (dtype) should be float32 to ensure accuracy. Many weights of CML operator representations have other meanings, such as representing the structure of conditional operators. Those weights are sparse and can naturally be expressed as low-precision dtypes such as bool. The natural sparse features bring optimizations described in Section 4.3.2. 4 CMLCompiler: A Unified Compiler for Classical Machine Learning and \ud835\udc54\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc5f , which rarely occur in DL models. Those comparison operators take float or integer as input and return bool, bringing remarkable changes in the dtype of input and output, which can be used to make optimizations as described in Section 4.3.1. Both DL and CML models use indices operators, which compare input and returns indices, such as \ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc60\ud835\udc5c\ud835\udc5f\ud835\udc61 and \ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc5a\ud835\udc4e\ud835\udc65 . Those indices operators have mathematical properties that can be used to make graph-level optimizations, as described in Section 4.3.3. These optimizations can be ignored in DL models with dozens or hundreds of layers but are helpful for those CML models with fewer layers.", "3.2 Extended Computational Graph": "This section introduces extended computational graph (ECG), which organizes operator representations in an optimization-friendly way and can be used to represent CML models. ECG is an extension based on DL computational graph. In general, a DL computational graph is represented as a directed graph where nodes represent operations on tensors or program inputs and edges represent data dependencies between operations [7]. From the perspective of the DL frameworks and compilers, computational graphs are dense and float32 by default, such as neural network models. Using approximate optimizations like pruning and quantization brings sparse and low-precision data to all operators and weights. These optimizations cause a decrease in accuracy and bring extra computation, such as calibration. When we convert CML operators to operator representations, part of those converted operators and weights are sparse and low-precision naturally. Using DL computational graphs to represent CML models directly is not precise enough and ignores many optimization opportunities due to the data type and sparse features. So we extend the computational graph in the DL systems into extended computational graph (ECG) as the unified abstraction for CML models. Before introducing ECG, first, we present more details about data type (dtype) and sparsity. We define the partial order relation for dtypes used in our work:", "\ud835\udc53 \ud835\udc59\ud835\udc5c\ud835\udc4e\ud835\udc61 32 > \ud835\udc56\ud835\udc5b\ud835\udc61 32 / \ud835\udc53 \ud835\udc59\ud835\udc5c\ud835\udc4e\ud835\udc61 16 > \ud835\udc56\ud835\udc5b\ud835\udc61 16 > \ud835\udc56\ud835\udc5b\ud835\udc61 8 > \ud835\udc56\ud835\udc5b\ud835\udc61 4 > \ud835\udc4f\ud835\udc5c\ud835\udc5c\ud835\udc59": "The lower dtype can be converted into a higher dtype without accuracy loss, while a backward conversion with accuracy loss is forbidden. Using lower dtype computation, such as int8 matmul, can speed up and reduce memory usage. However, there are many limitations to dtype optimization. For example, the inputs of the same operator should have the same dtype; thus, the dtype of operators depends on the largest dtype of inputs. Besides, many hardware devices have extended instructions based on specific dtypes. For example, an Intel processor speeds up int8 computation using AVX instruction, while bool cannot benefit from that. Considering the complexity of dtype optimization, we add dtype as a property for ECG. Weclassify the inputs of an operator into two categories: intermediate results and weights. Intermediate results are other operators' Sparsity is defined as the ratio of non-zero data to all data. If data sparsity is relatively small, we take it as sparse data and store it in a compressed sparse row (CSR) format. Using sparse operators to handle those sparse data can perform better than dense operators. Taking advantage of sparsity influences optimization greatly, so we add sparsity as another property for ECG. Table 3: Operators used in ECGs outputs and can only be handled during runtime. Input data is the first intermediate result in ECG, while output data is the last. Intermediate results are represented as { \ud835\udc60\ud835\udc5d\ud835\udc4e\ud835\udc5f\ud835\udc60\ud835\udc56\ud835\udc61\ud835\udc66, \ud835\udc51\ud835\udc61\ud835\udc66\ud835\udc5d\ud835\udc52, \ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc60\ud835\udc5c\ud835\udc5f } . If we want to change the dtype of intermediate results, we should add dtype converting operator in the ECG. Operators are represented in the form of { \ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61\ud835\udc60, \ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f\ud835\udc5a\ud835\udc52\ud835\udc51\ud835\udc56\ud835\udc4e\ud835\udc61\ud835\udc52 _ \ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc62\ud835\udc59\ud835\udc61\ud835\udc60, \ud835\udc62\ud835\udc60\ud835\udc52 _ \ud835\udc60\ud835\udc5d\ud835\udc4e\ud835\udc5f\ud835\udc60\ud835\udc52, \ud835\udc61\ud835\udc66\ud835\udc5d\ud835\udc52, \ud835\udc51\ud835\udc61\ud835\udc66\ud835\udc5d\ud835\udc52, \ud835\udc37\ud835\udc3f _ \ud835\udc5c\ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc5c\ud835\udc5f } . Weights and in-termediate_results are inputs of operators. Use_sparse is a flag of whether using the sparse operator or not, which is closely related to sparse operator replacing optimization described in Section 4.3.2. Operator type is the type of operator. As shown in Table 3, we divide operators used in ECG into five categories. Comparison operators refer to those operators that compare two tensors and return bool tensors. Indices operators refer to those operators that return tensors' indices based on specific conditions. Those two kinds of operators are dtype-lowering operators, the output dtype of which is smaller than the input. Models without those operators, such as most DL models, use the same dtype through the whole graphs, where dtype optimizations cannot be used without approximate optimization. CML models make much use of those operators, which have wide usage of dtype rewriting optimization described in Section 4.3.1. Monotonic operators refer to those operators who meet the following conditions: Weights are model parameters that can be loaded from trained models. Weights can be handled both during compilation and runtime, while a proper transformation during compilation can reduce runtime costs. Weights are represented as { \ud835\udc60\ud835\udc5d\ud835\udc4e\ud835\udc5f\ud835\udc60\ud835\udc56\ud835\udc61\ud835\udc66, \ud835\udc60\ud835\udc5a\ud835\udc4e\ud835\udc59\ud835\udc59\ud835\udc52\ud835\udc60\ud835\udc61 _ \ud835\udc51\ud835\udc61\ud835\udc66 -\ud835\udc5d\ud835\udc52, \ud835\udc4e\ud835\udc50\ud835\udc61\ud835\udc62\ud835\udc4e\ud835\udc59 _ \ud835\udc51\ud835\udc61\ud835\udc66\ud835\udc5d\ud835\udc52, \ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc60\ud835\udc5c\ud835\udc5f } . Smallest_dtype is the smallest dtype for weights without accuracy loss, actual_dtype is the dtype actually used. Smallest_dtype depends on the property of weights, while actual_dtype is fixed based on smallest_dtype and operators. As shown in Fig. 3, \ud835\udc4a 1 represents the relationship between input features and internal nodes for decision trees, which is a 0-1 matrix. The smallest_dtype of \ud835\udc4a 1 is bool. However, W1 is multiplied by input data with a dtype of float32. If we choose bool as the ac-tual_dtype, \ud835\udc4a 1 will be converted to float32 during runtime. To reduce the execution time in runtime, we should convert \ud835\udc4a 1 to float32 during compilation, so we set actual_dtype as float32 rather than bool.  A series of monotonic operators followed by an indices operator is mathematically equivalent to the indices operators alone. Those properties provide more optimizations, as described in Section 4.3.3. Reduction operators calculate aggregates over input. Arithmetic operators refer to other arithmetic calculations. Operator dtype is the operators' data type, such as int8 matmul or float32 matmul. 5 Xu Wen, Wanling Gao, Anzheng Li, Lei Wang, Zihan Jiang, and Jianfeng Zhan Table 4: Supported Algorithms", "Preprocessing Algorithms": "Binarizer, LabelBinarizer, Normalizer, MaxAbsScaler, MinMaxScaler, StandardScaler, RobustScaler, PolynomialFeatures, LabelEncoder", "Feature Selectors": "SelectKBest, VarianceThreshold", "Linear Models": "LogisticRegression, LogisticRegressionCV, Perception, RidgeClassifier, RidgeClassifierCV, SGDClassifier, LinearRegression, Ridge, RidgeCV, SGDRegressor", "Tree-based Models": "DecisionTreeClassifier, DecisionTreeRegressor, ExtraTreeClassifier, ExtraTreeRegressor, RandomForestClassifier, RandomForestRegressor, ExtraTreesClassifier, ExtraTreesRegressor, GradientBoostingClassifier, GradientBoostingRegressor Support Vector Machines LinearSVC, LinearSVR, NuSVR, SVR Operator dtype depends on the dtype of weights and intermedi-ate_results. DL_operator is the native definition of operators in DL computational graphs, which we use to translate ECG to DL computational graphs.", "3.3 Supported Algorithms and Extension for Other Algorithms": "CMLCompiler supports 35 CML algorithms nowadays, as shown in Table 4, covering most of the popular CML algorithms [34]. Our work can also be extended to other algorithms, such as clustering and matrix decomposition. Most CML algorithms use operators categorized in Section 3.1.1, each of which can be converted to corresponding Operator Representations-our low-level abstractions, guaranteeing our extensibility. We take Kmeans as an example. Kmeans use basic arithmetic operators to calculate the distance between nodes, which can be converted to element-wise arithmetic operators and use aggregation operators to make clustering, which can be converted to reduction operators. When all operators of a CML algorithm are converted to Operator Representations, it can utilize our work to compile and make optimizations.", "4 DESIGN AND IMPLEMENTATION": "This section illustrates the design and implementation of CMLCompiler, as shown in Fig. 4. We build our framework based on the two unified abstractions, including four parts. Operator Converter converts CML operators into operator representations, as shown in Section 4.1. Model Parser organizes those operator representations in an optimization-friendly way and uses ECGs to represent CML models, as shown in Section 4.2. Graph Optimizer makes graph level optimizations, as described in Section 4.3. An optimized ECG is converted into a DL computational graph by Graph Translator in Section 4.4. DL frameworks or compilers take DL computational graphs as input and make more optimizations, compiling them into Figure 4: The CMLCompiler architecture.", "CMLCompiler": "Operator Converter Operator Representation Extended Computational Graph Optimized ECG Model Parser Graph Optimizer Graph Translator Unified Abstractions CML Models DL computational graph sparse_dense greater dense_int8 argmax_int8 annotated information matmul greater matmul argmax annotated information matmul greater matmul argmax matmul greater matmul argmax executable modules to deploy. Section 4.5 shows the mixture usage of CML and DL. Section 4.6 shows the implementation details.", "4.1 Operator Converter": "Operator Converter traverses the operators in CML models and converts them into operator representations, respectively. If CML operators handle matrices or arrays, they can be converted to DL operators directly. If CML operators handle scalars or a single element from data tables, they are converted to DL operators based on their categories. Several assignment operators without data dependencies are converted to one tensor assignment. If the data processing order differs from its layout in the tensor, we use the \ud835\udc5f\ud835\udc52\ud835\udc5c\ud835\udc5f\ud835\udc54\ud835\udc4e\ud835\udc5b\ud835\udc56\ud835\udc67\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b operator to change the tensor layout. Basic arithmetic operators are converted to element-wise arithmetic operators. Aggregation operators are converted to reduction operators. Comparison operators are converted to element-wise comparison. Sequential conditional operators are represented as the combination of \ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5a\ud835\udc62\ud835\udc59 and \ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc5a\ud835\udc4e\ud835\udc65 . These converted DL operators are wrapped into operator representations with data dependencies.", "4.2 Model Parser": "Model Parser converts operator representations into an ECG. Operators in an operator representation are initialized as nodes in an ECG, the data structure of which is defined in Section 3.2. Operator.weights and operator.intermediate_results are set according to data dependencies, and edges are built between nodes. Operator.use_sparse and operator.dtype are set as False and Unknown, respectively. Operator.type is set according to operator type, which 6 CMLCompiler: A Unified Compiler for Classical Machine Learning is defined in Table 3. Weights and intermediate_result are initialized after that. Weight.sparsity is set as the ratio of non-zero data and all data, known during compilation. Weight.smallest_dtype is set as the smallest dtype without accuracy loss, and weight.actual_dtype is initialized the same. Intermediate_result.sparsity and intermedi-ate_result.dtype are set according to operator. When all operators are visited, the ECG is established.", "4.3 Graph Optimizer": "Graph Optimizer performs graph-level optimizations, using a functionally equivalent transformation for ECGs. These optimizations are based on the features of CML models and do not influence accuracy. There are three specific graph rewriting optimizations: dtype rewriting, sparse operator replacing, and redundant elimination. 4.3.1 Dtype rewriting. Dtype rewriting uses low-precision computation with faster speed and less memory to replace high-precision computation. As analyzed in Section 3.1.3, many weights used in CMLcan be represented as low-precision dtype such as bool or int8. Besides, comparison operators and indices operators widely used in CML are dtype-lowering operators. The intermediate results after those operators are also low-precision. When intermediate data and weights can both be expressed as low-precision dtype, the corresponding operators can be converted into low-precision computation as well. Now we introduce the dtype rewriting principle in detail. Algorithm 1 shows the procedure of dtype rewriting: As shown in Fig. 5a, the top is the ECG of decision trees before optimization; many details are hidden. Weight \ud835\udc4a 3 represents the relationship between leaf nodes and internal nodes for decision trees, which is a matrix only containing 0 and 1. The smallest_dtype of \ud835\udc4a 3 is bool. The output of \ud835\udc54\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc5f operator has a dtype of bool as well. So the following matrix multiplication (matmul) operator can use a dtype of bool rather than float32. Intel processors speed up int8 computation using AVX instruction, while bool cannot benefit from that feature. So we convert the dtype of matmul to int8 according to hardware specification. In Fig. 5a, below is the ECG after graph rewriting. Those white weights and operators use float32, while gray weights and operators use int8. (1) Visit all operators in ECG. For each operator, dtype is set as the largest dtype of all inputs. After that, operator dtype is converted to the dtype which can utilize hardware's SIMD instructions best. We keep a list of hardware specifications to modulate operator dtype. In order to guarantee accuracy, dtype cannot get smaller. Then we modulate operator implementation based on operator dtype. We explain the differences between dtype rewriting for CML models and model quantization for DL models. Quantization is an approximate algorithm for DL models that causes a decrease in accuracy and brings extra computation, such as calibration. Dtype rewriting for CML models is based on the properties of CML, converting dtype of operators and weights with no accuracy decrease and extra computation. (2) When operator dtype is fixed, we set the input dtype. The dtype of weights is set the same as the operator, reducing dtype conversion in runtime. The dtype of intermediate results cannot be converted during compilation. So we add dtype converting operator, .i.e, cast, before the operator. 4.3.2 Sparse operator replacing. Replacing dense operators with sparse operations can speed up as well. The sparsity of input data can be known until runtime, while the sparsity of weights can be known during compilation. So we convert the data format of weights rather than input data. Different hardware devices have different support for sparse operators. For example, CPUs can benefit from sparse computation while GPUs have little effect. So we set a threshold based on hardware specification. If weight.sparsity is smaller than the threshold, we store it in a compressed sparse row (CSR) format. Then we convert the corresponding operator into a sparse implementation. An example is shown in Fig. 5b, we convert \ud835\udc4a 1 and the corresponding matmul to sparse. 4.3.3 Redundant elimination. Redundant elimination eliminates those operators who do not influence final results due to their mathematical properties. For example, a series of monotonic operators followed by an indices operator is mathematically equivalent to the indices operators alone. For each operator in ECGs, we check its operator type. If another monotonic operator follows a monotonic operator, we fuse them. We eliminate the monotonic operator if an indices operator follows it. An example is shown in Fig. 5c, the softmax before argmax is eliminated.", "4.4 Graph Translator": "Graph Translator converts the optimized ECG into DL computational graph based on ECG topology and chooses the proper operator implementation. DL frameworks or compilers provide different implementations for the same operator. Graph Translator utilizes four properties: \ud835\udc62\ud835\udc60\ud835\udc52 _ \ud835\udc60\ud835\udc5d\ud835\udc4e\ud835\udc5f\ud835\udc60\ud835\udc52 , \ud835\udc61\ud835\udc66\ud835\udc5d\ud835\udc52 , \ud835\udc51\ud835\udc61\ud835\udc66\ud835\udc5d\ud835\udc52 , and \ud835\udc37\ud835\udc3f _ \ud835\udc5c\ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc5c\ud835\udc5f in ECG to choose the most proper implementation. Operator implementation can also be modulated based on hardware information. We can utilize hardware features to optimize operators. If the hardware supports extended instructions like AVX, we use them to speed up operators. If the backend is TVM, we just pass the precise \ud835\udc5a\ud835\udc50\ud835\udc5d\ud835\udc62 and \ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc61\ud835\udc5f information and utilize TVM to make operator-level optimizations. DL frameworks or compilers take DL computational graphs as input and make more optimizations, finally compiling them into executable modules. 7 Xu Wen, Wanling Gao, Anzheng Li, Lei Wang, Zihan Jiang, and Jianfeng Zhan matmul greater matmul argmax W1 W2 W3 input out matmul greater matmul argmax W1 W2 W3 input out float32 int8 (a) Dtype Rewriting matmul greater matmul argmax W1 W2 W3 input out matmul greater matmul argmax W1 W2 W3 input out dense sparse (b) Sparse Operator Replacing matmul add softmax argmax W1 W2 input out matmul add argmax W1 W2 input out redundant operator (c) Redundant Elimination Figure 5: Graph rewriting optimizations. Dtype rewriting converts float32 operators and weights into low-precision. Sparse operator replacing converts dense operators and weights into sparse. Redundant elimination reduces redundant operators. DL models CML models Single ECG for hybrid models Cross-framework implementation", "5.1 Experimental Setup": "Figure 6: CMLCompiler uses a single ECG to represent CML and DL mixed pipeline.", "4.5 Hybrid Deployment of CML and DL with a Unified Framework": "We convert those CML and DL hybrid applications under a unified framework to reduce the cost of switching frameworks and provide an opportunity for end-to-end optimizations, as shown in Fig. 6. We load models from PyTorch and sklearn and convert them into ECG subgraphs. We build edges according to data dependency and merge those subgraphs in a single ECG. Then we can use optimizations both in our work and DL compilers. Finally, we compile and deploy it on various hardware devices.", "4.6 Implementation": "Due to the benefits in portability and performance, we implement CMLCompiler on the basis of TVM. The intermediate representations and transforms are all written in python. We read trained models from CML frameworks such as sklearn and convert them into operator representations, implementing them in the format of TVM relay functions and storing their weights in TVM arrays. We wrap those relay functions in the format of ECGs. After optimizations in Section 4.3, we convert ECGs into TVM's IRModules. Then we utilize TVM to make more optimizations and compile to executable modules based on specific hardware targets. We use cross-compilation to support a broad spectrum of hardware devices. We deploy them on lightweight runtime based on TVM runtime and make inferences on various hardware devices.", "5 EVALUATION": "This section summarizes the evaluation. Section 5.1 shows the experimental setup. Section 5.2 evaluates the performance of graph rewriting optimizations based on ECGs. Section 5.3 compares our work with the state-of-the-art frameworks. Section 5.4 evaluates the hybrid deployment of CML and DL. We deploy a server node equipped with two Xeon E5-2620 V3 (Haswell) CPUs, an Nvidia Titan RTX GPU, and 64 GB memory to conduct the experiments on CPU and GPU. Each CPU contains six physical cores. The GPU contains 4608 Cuda cores and 24 GB memory. The operating system is Ubuntu 16.04, and the other software includes TVM 0.8, PyTorch 1.8.1, hummingbird 0.3.1, scikit-learn 1.0.1, and CUDA 10.2. For the IoT experiments, we use Raspberrypi4b with Raspbian 10 operating system and deploy the above software with the same version. We use YearPrediction [12] as the dataset, with 515345 samples and 90 features. We use 80% data to train models and 20% data to make inferences. We run all the experiments five times and use the average as the final results. We test hummingbird [30] using both two backends (PyTorch and TVM) and select their best results.", "5.2 Optimizations": "This section evaluates graph rewriting optimizations based on ECGs, as described in Section 4.3. These optimizations: dtype rewriting, sparse operator replacing, and redundant elimination, can work together and produce cumulative optimization effects. They can also coexist with the optimizations in TVM. We choose four typical tree models: DecisionTreeClassifier, RandomForestClassifier, ExtraTreeClassifier, and ExtraTreesClassifier, as well as two typical linear models: LogisticRegression and SGDClassifier. We evaluate the dtype rewriting and sparse operator replacing for tree models, and redundant elimination for linear models according to their unique patterns. Fig. 7b shows the result of IoT devices. Note that sklearn lacks enough support for IoT devices. For example, 64-bit tree models trained on servers cannot be executed on Raspberrypi4b with a 32-bit operating system. Retraining those models in 32-bit format on Raspberrypi4b from scratch takes more time, so we regard those Fig. 7a shows the result on CPU. For tree models, using our work without optimizations has a 1.31x-2.54x speedup compared with sklearn; this is due to our abstractions which utilize optimizations of TVM, including better utilization of SIMD instructions and multi cores. Using dtype rewriting and sparse operator replacing bring 1x-1.21x and 1.26x-1.75x speedup, respectively, achieving 1.27x2.11x speedup together, 1.84x-4.44x faster than sklearn. For linear models, our work without optimizations runs slower than sklearn. However, using redundant elimination brings 1.22x-1.51x speedup; the result after our optimizations is 1.06x-1.14x faster than sklearn. 8 CMLCompiler: A Unified Compiler for Classical Machine Learning Figure 7: Graph Rewriting Optimizations. Take sklearn as the baseline. \"base\" means our work without optimizations. \"DR\" means using dtype rewriting. \"SOR\" means using sparse operator replacing. \"RE\" means using redundant elimination. \" \u00d7 \" means unsupported. For those models not supported by sklearn on IoT devices, take our result without optimzations as the baseline. DecisionTreeClf RandomForestClf ExtraTreeClf ExtraTreesClf LogisticRegression SGDClassifier 0 1 2 3 4 speedup sklearn base DR SOR RE (a) CPU DecisionTreeClf RandomForestClf ExtraTreeClf ExtraTreesClf LogisticRegression SGDClassifier 0.0 0.5 1.0 1.5 2.0 2.5 speedup sklearn base DR SOR RE (b) Raspberrypi4b models as unsupported, marked as cross. So we take our work without optimizations as the baseline. Using dtype rewriting and sparse operator replacing bring 1.01x-1.33x and 1.23x-2.3x speedup, respectively, achieving 1.49x-2.53x speedup together. For linear models, our work without optimizations achieves a 1.71x-1.84x speedup. Using redundant elimination brings 1.08x-1.14x more speedup, 1.95x-1.98x faster than sklearn. The computation part of GPU is less than 20%, so those optimizations play a limited role on GPU. In conclusion, CML models can benefit from both TVM's optimizations and our optimizations and achieve obvious speedup. has better performance on GPU on 10 algorithms out of 14 compared with hummingbird, with a 1.41x-4.64x speedup. Our latency on Raspberrypi4b does not differ much compared with sklearn. However, we perform better in model support.", "5.3 Overall Results": "This section evaluates 14 typical CML algorithms covering preprocessing algorithms, linear models, tree-based models, and SVMs, on CPU, GPU, and IoT devices, compared with state-of-the-art frameworks including sklearn, intel extension for sklearn [20], and hummingbird. It contains two parts: batch experiments for all data and query experiments for a single record. Table 6 shows the performance of query experiments for a single record. On CPU, our work achieves the best performance on 11 algorithms out of 14, with a 1.36x-170.68x speedup compared with sklearn, a 1.56x-4.47x speedup compared with hummingbird, and a 1.31x-169.43x speedup compared with intel sklearn. Our work The differences between the results of CMLCompiler and sklearn are all less than 1 \u00d7 10 -5 , which means that our work does not affect the accuracy. The outputs on different hardware are all the same, so we focus on performance hereinafter. Table 5 shows the performance of batch experiments. On CPU, our work reflects the best performance on 12 algorithms out of 14, achieving 1.02x-10.57x speedup compared with sklearn, 1.14x-4.38x speedup compared with hummingbird, and 1.44x-8.47x speedup compared with intel sklearn. On GPU, our work achieves competitive performance compared with hummingbird. Our work performs better on 11 algorithms out of 14, with a 1.11x-3.31x speedup. On an IoT device Raspberrypi4b, our work performs better on 13 algorithms out of 14, with a 1.28x-5.09x speedup. In conclusion, we have advantages in both batch and query experiments for all three hardware devices. Many models in sklearn only support a single core and cannot fully utilize the SIMD instructions. We perform better than sklearn and intel sklearn due to better utilization of multi-cores and SIMD instructions through compilation. Hummingbird uses both PyTorch and TVM as backends, where TVM performs better in most cases of our evaluations. It implements models in PyTorch and converts them into TVM using \ud835\udc53 \ud835\udc5f\ud835\udc5c\ud835\udc5a _ \ud835\udc5d\ud835\udc66\ud835\udc61\ud835\udc5c\ud835\udc5f\ud835\udc50\u210e API. This conversion is not direct and efficient enough, causing a performance decrease. Besides, hardware information is missed during conversion, which limits the optimizations of TVM for hummingbird. We map ECGs into relay operators directly and select the most efficient implementation based on ECGs and hardware specification information. Additionally, our abstractions bring more optimizations, as described in Section 4.3, bringing up to 2.53x speedup, working together to achieve better performance.", "5.4 Hybrid Deployment of CML and DL": "This section shows three hybrid deployment cases of CML and DL. As the baselines, without a unified framework, a DL framework is used to implement DL algorithms, while a CML framework is used to implement CML algorithms. Our work converts CML and DL models into a single ECG, making optimizations and compiling them to various hardware devices. We test the latency of a single query, which is essential in real-world applications. 5.4.1 Sentence Sentiment Classification. The first is a sentence sentiment classification case, which uses Bert to embed English sentences and logistic regression to make a classification [36]. We use BERT-tiny [3] as the pre-trained Bert model and SST2 [40] as the dataset. The baseline implements BERT-tiny in pytorchtransformers [45] and logistic regression in sklearn. The result is shown in Fig .8a. Our work achieves a 1.67x speedup on server 9 Xu Wen, Wanling Gao, Anzheng Li, Lei Wang, Zihan Jiang, and Jianfeng Zhan Table 5: Execution time for batch experiments over all data on CPU (12 cores), GPU, and IoT devices (take Raspberrypi4b as an example) in milliseconds. SK, HB, and Intel is short for scikit-learn, hummingbird, and Intel extension for sklearn, respectively. \"-\" means unsupported. Table 6: Latency for query experiments over one single record on CPU (12 cores), GPU, and IoT devices (take Raspberrypi4b as an example) in milliseconds. The symbols are the same as Table 5. (a) Bert + LogisticRegression for sentence sentiment classification CPU Raspberrypi4b 0 10 5 15 latency (ms) baseline our work CPU Raspberrypi4b 0 40 20 60 latency (ms) baseline our work CPU Raspberrypi4b 0 5 10 15 latency (ms) baseline our work (b) SimpleDNN + RandomForest for radiographic image analysis (c) GBDT + Wide&Deep for click through prediction Figure 8: The latency of a single query for CML and DL mixed pipelines. All three baselines cannot run on IoT devices. 10 CMLCompiler: A Unified Compiler for Classical Machine Learning CPUs. Pytorch-transformers cannot be installed on IoT devices, so the baseline cannot run on Raspberrypi4b. The latency of our work on Raspberrypi4b is 18 milliseconds, which is acceptable in most use cases. 5.4.2 Radiographic Image Analysis. The second case uses Deep Hybrid Learning [38] to analyze radiographic images, which uses simple DNN to make feature engineering and CML models such as random forests to make a classification. We use CheXpert [21] as the dataset. The baseline implements DNN in PyTorch and random forest in sklearn. The result is shown in Fig .8b. Our work achieves a 2.3x speedup on server CPUs. The pre-trained random forest cannot run on IoT devices by sklearn, while our work can support those devices through cross-compilation. 5.4.3 Click Through Rate Prediction. The third case is click-through rate prediction used in recommendation systems of our anonymous industry partners, using GBDT [15] to extract features and the Wide and Deep [9] models to make a prediction. We use avazu 1 as the dataset. The baseline implements GBDT in sklearn and Wide and Deep in PyTorch. The result is shown in Fig .8c. We achieve 3.04x speedup on the server CPUs. The GBDT model in the baseline cannot be executed on IoT devices, while our latency on IoT devices is only 5.06 ms.", "6 RELATED WORK": "CML frameworks and libraries can be divided into three categories. (1) General-purpose solution uses one framework to support various models. Scikit-learn [32] is the most widely used CML framework on GitHub [33]. Spark MLlib [29] is an extension to Spark [48]. H2O [17] uses MapReduce [11] to support both CML and DL. There are many other works, such as Shogun [41] and RapidMiner [19]. These frameworks only support CPU, suffering from severe performance and portability issues. (2) Specific-purpose solution focuses on one type of model. LibLinear [14] supports logistic regression and linear SVM. LibSVM [5] focuses on SVMs. These works are limited to CPUs. Some other works attempt to support various hardware devices. XGBoost [6] implements a gradient-boosting decision tree algorithm on CPUs and GPUs. Muhsen Owaida et al. [31] bring XGBoost to FPGAs. Toby Sharp [39] implements decision trees and forests on GPUs. These frameworks only support a narrowed variety of models and solve the problem of portability to a certain extent. (3) Extension based on DL attempts to utilize DL frameworks to support CML models. TF-DF [43] is a decision forest library based on TensorFlow but is limited to CPUs. It's implemented ad-hoc, losing the portability of DL frameworks. Hummingbird [30] is a general-purpose solution based on PyTorch, adding support for GPUs. They utilize those abstractions in DL frameworks directly without digging into the features of CML, missing many optimization chances.", "7 CONCLUSION": "This paper presented the design and implementation of CMLCompiler, an open-source unified compiler for classical Machine Learning (CML) inference. CMLCompiler proposed two unified abstractions: operator representations and extended computational graphs 1 https://www.kaggle.com/c/avazu-ctr-prediction (ECGs). Operator representations convert CML operators into tensor formats, while an ECG organizes these converted operators in an optimization-friendly way. The CMLCompiler framework performs the conversion and graph optimization based on two unified abstractions, then outputs an optimized computational graph to deep learning compilers or frameworks. CMLCompiler also enables the hybrid deployment of CML and DL with a unified framework. Our implementations of CMLCompiler on top of TVM show the effectiveness and achieve up to 4.38x speedup on CPU, 3.31x speedup on GPU, and 5.09x speedup on IoT devices, compared to the stateof-the-art solutions - scikit-learn, Intel sklearn, and hummingbird. Our support for CML and DL mixed pipelines achieves up to 3.04x speedup compared with cross-framework implementations.", "REFERENCES": "[1] Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensorflow: A system for large-scale machine learning. In Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation , OSDI'16, page 265-283, USA, 2016. USENIX Association. [3] Prajjwal Bhargava, Aleksandr Drozd, and Anna Rogers. Generalization in nli: Ways (not) to go beyond simple heuristics, 2021. [2] Amazon. The total cost of ownership (tco) of amazon sagemaker. https://pages. awscloud.com/rs/112-TZM-766/images/Amazon_SageMaker_TCO_uf.pdf, 2020. [4] Leo Breiman. Random forests. Machine learning , 45(1):5-32, 2001. [6] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining , pages 785-794, 2016. [5] Chih-Chung Chang and Chih-Jen Lin. Libsvm: a library for support vector machines. ACM transactions on intelligent systems and technology (TIST) , 2(3):127, 2011. [7] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Meghan Cowan, Haichen Shen, Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin, and Arvind Krishnamurthy. Tvm: An automated end-to-end optimizing compiler for deep learning. In Proceedings of the 13th USENIX Conference on Operating Systems Design and Implementation , OSDI'18, page 579-594, USA, 2018. USENIX Association. [9] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. Wide&deeplearning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems , pages 7-10, 2016. [8] Tianqi Chen, Lianmin Zheng, Eddie Yan, Ziheng Jiang, Thierry Moreau, Luis Ceze, Carlos Guestrin, and Arvind Krishnamurthy. Learning to optimize tensor programs. Advances in Neural Information Processing Systems , 31, 2018. [10] Scott Cyphers, Arjun K. Bansal, Anahita Bhiwandiwalla, Jayaram Bobba, Matthew Brookhart, Avijit Chakraborty, William Constable, Christian Convey, Leona Cook, Omar Kanawi, Robert Kimball, Jason Knight, Nikolay Korovaiko, Varun Kumar Vijay, Yixing Lao, Christopher R. Lishka, Jaikrishnan Menon, Jennifer Myers, Sandeep Aswath Narayana, Adam Procter, and Tristan J. Webb. Intel ngraph: An intermediate representation, compiler, and executor for deep learning. CoRR , abs/1801.08058, 2018. [12] Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. [11] Jeffrey Dean and Sanjay Ghemawat. Mapreduce: simplified data processing on large clusters. Communications of the ACM , 51(1):107-113, 2008. [13] EasonLiao. Cudatree. https://github.com/EasonLiao/CudaTree, 2022. [15] Jerome H Friedman. Greedy function approximation: a gradient boosting machine. Annals of statistics , pages 1189-1232, 2001. [14] Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. Liblinear: A library for large linear classification. the Journal of machine Learning research , 9:1871-1874, 2008. [16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems , 27, 2014. [18] Kim Hazelwood, Sarah Bird, David Brooks, Soumith Chintala, Utku Diril, Dmytro Dzhulgakov, Mohamed Fawzy, Bill Jia, Yangqing Jia, Aditya Kalro, James Law, Kevin Lee, Jason Lu, Pieter Noordhuis, Misha Smelyanskiy, Liang Xiong, and Xiaodong Wang. Applied machine learning at facebook: A datacenter infrastructure perspective. In 2018 IEEE International Symposium on High Performance [17] H2O.ai. H2o: Scalable machine learning platform. https://github.com/h2oai/h2o-3, 2022. 11 Xu Wen, Wanling Gao, Anzheng Li, Lei Wang, Zihan Jiang, and Jianfeng Zhan [34] Susmita Ray. A quick review of machine learning algorithms. In 2019 International conference on machine learning, big data, cloud and parallel computing 12"}
