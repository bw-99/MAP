{
  "On Causally Disentangled State Representation Learning for Reinforcement Learning based Recommender Systems": "Siyu Wang The University of New South Wales Sydney, Australia siyu.wang5@unsw.edu.au Xiaocong Chen Data 61, CSIRO Eveleigh, Australia xiaocong.chen@data61.csiro.au Lina Yao Data 61, CSIRO Eveleigh, Australia The University of New South Wales Sydney, Australia lina.yao@data61.csiro.au",
  "ABSTRACT": "In Reinforcement Learning-based Recommender Systems (RLRS), the complexity and dynamism of user interactions often result in high-dimensional and noisy state spaces, making it challenging to discern which aspects of the state are truly influential in driving the decision-making process. This issue is exacerbated by the evolving nature of user preferences and behaviors, requiring the recommender system to adaptively focus on the most relevant information for decision-making while preserving generaliability. To tackle this problem, we introduce an innovative causal approach for decomposing the state and extracting C ausalI n D ispensable S tate Representations (CIDS) in RLRS. Our method concentrates on identifying the D irectly A ctionI nfluenced S tate Variables (DAIS) and A ctionI nfluence A ncestors (AIA), which are essential for making effective recommendations. By leveraging conditional mutual information, we develop a framework that not only discerns the causal relationships within the generative process but also isolates critical state variables from the typically dense and high-dimensional state representations. We provide theoretical evidence for the identifiability of these variables. Then, by making use of the identified causal relationship, we construct causal-indispensable state representations, enabling the training of policies over a more advantageous subset of the agent's state space. We demonstrate the efficacy of our approach through extensive experiments, showcasing our method outperforms state-of-the-art methods. interaction as an opportunity to learn and enhance the personalization of content, thereby maintaining alignment with the evolving preferences and behaviors of the users.",
  "KEYWORDS": "Reinforcement Learning, Recommendation, Causal state representation",
  "1 INTRODUCTION": "Recommender Systems (RS) are crucial in navigating the vast digital environment, tailoring suggestions to align with individual user preferences. The integration of Reinforcement Learning (RL) within RS, known as RLRS, has transformed the recommendation experience into a dynamic, sequential decision-making process. Unlike traditional systems that passively suggest content based on static data, RLRS actively engage with users, continuously adapting recommendations in response to real-time feedback. This approach seeks to maximize long-term user engagement by treating each In RLRS, the efficiency hinges on three essential components: State Representation, Policy Optimization, and Reward Formulation [1, 8]. While much of the current research in RLRS is centered on policy optimization [5, 7, 33, 39] and reward formulation [4, 9, 16], the role of state representation should not be understated. The state in RLRS is a composite of varied attributes: user characteristics (like age, gender, and recent activities), item properties (including price, category, and popularity), and contextual elements (such as time and location). Effectively distilling this rich tapestry of information poses a significant challenge. Neglecting key features might result in suboptimal recommendations, while incorporating too much detail could clutter the system with irrelevant data, diminishing its predictive accuracy. Recent advances in representation learning algorithms in RL aim to extract abstract features from high-dimensional data, which has proven beneficial in enhancing the efficiency of existing RL algorithms [12, 15, 17]. For instance, Zhang et al. [36] developed a method to learn representations by ignoring task-irrelevant information through the bisimulation metric, while Wang et al. [34] proposed a state abstraction technique in model-based RL by learning a dynamic model that minimizes dependencies between state variables and actions. Despite these advancements in RL, the exploration of state representation in RLRS is still limited. RLRS often involves complex, high-dimensional data and intricate causal relationships within the recommender system. Rather than merely condensing the state aggregation, we aim to discern and highlight the specific state dimensions that are causally critical for decision-making in RLRS, providing a more targeted and effective approach to recommendation processes. In RLRS, the agent's actions involve recommending items, while the rewards typically correspond to user feedback like clicks, purchases, or exits. However, rewards alone do not clearly indicate which aspects of the state influence user behavior, making it challenging to discern critical from irrelevant state dimensions for effective decision-making. To tackle this, we introduce CausalIndispensable State Representations (CIDS) in RLRS. CIDS leverages causal relationships between actions and state variables, as well as among different state dimensions, to identify key elements crucial for policy learning. CIDS focuses on two types of causal relationships. The first involves state dimensions directly influenced by actions, termed Directly Action-Influenced State Variables (DAIS). For instance, CIKM'24, October 21 - 25, 2024, Boise, Idaho, USA 2018. ACM ISBN 978-1-4503-XXXX-X/18/06...$15.00 https://doi.org/XXXXXXX.XXXXXXX CIKM'24, October 21 - 25, 2024, Boise, Idaho, USA Trovato and Tobin, et al. when a user's recent browsing history changes following specific item recommendations, it indicates a direct impact of the action on this state dimension. Such changes can extend to item features like popularity or category trends, offering valuable feedback on user preferences in response to recommendations. The second type of causal relationship in CIDS pertains to state dimensions that affect DAIS. Since DAIS are pivotal for policy learning, other dimensions influencing DAIS are also deemed critical. This relationship encompasses interactions between various state dimensions, such as how a user's browsing history might correlate with their likelihood of engaging with similar items, or how static attributes like age and gender could inform preferences in certain categories. This paper employs causal graphical models to uncover and identify these causal relationships within CIDS. We theoretically validate that CIDS can be determined using conditional dependence and independence relationships and can be learned from observations through conditional mutual information. Our main contributions are: · Reformulating the RLRS process using a causal graphical model to describe the interactions between actions and state dimensions. · Theoretically characterizing CIDS by leveraging conditional dependence and independence relationships. · Proposing a methodology to learn the underlying causal structure of CIDS from observations using conditional mutual information, thereby constructing CIDS for more efficient and effective recommendation policy learning. · Demonstrating our method's efficacy on both online simulators and offline datasets, showing improvements in the efficiency and performance of recommendation policy learning through the application of CIDS.",
  "2 PRELIMINARIES": "",
  "2.1 Reinforcement Learning based Recommender Systems": "Reinforcement Learning (RL) in Recommender Systems (RS) is focused on optimizing decision-making through ongoing user interaction, framed within a Markov Decision Process (MDP) modelled by the tuple ⟨S , A , R , P , 𝛾 ⟩ . Here, S denotes the state space, containing user data, historical interactions, item characteristics, and contextual elements; A represents the action space, including all candidate items; R : S × A → R is the reward function, related to the user feedback; P covers the transition probabilities of transitioning from one state to another; and 𝛾 is the discount factor, prioritizing immediate versus future rewards. In the MDP framework, the agent (RS) interacts with its environment over discrete time steps denoted by 𝑡 = 0 , 1 , 2 , . . . , 𝑛 . At each time step 𝑡 , the agent examines the current state 𝑠 𝑡 , which encompasses user preferences, past interactions, and item information, all contained within the state space S . The agent then selects an action 𝑎 𝑡 from the action space A( 𝑠 𝑡 ) , often comprising a set of item recommendations. This action prompts a transition to a subsequent state 𝑠 𝑡 + 1 , and the agent receives a corresponding reward 𝑟 𝑡 , reflecting the user's response and the effectiveness of the recommended items. The RS strives to establish a policy 𝜋 : S → A Figure 1: An illustrative causal graphical model for an MDP is depicted. The state 𝑠 𝑡 is decomposed into six different dimensions, denoted as 𝑠 𝑡 = ( 𝑠 1 𝑡 , . . . , 𝑠 6 𝑡 ) . The purple nodes signify DAIS, while the blue nodes symbolize AIA. The nodes enclosed within the gray area collectively represent CIDS, which contains both DAIS and AIA. that optimizes the cumulative discounted return, thus assessing the long-term effectiveness of its recommendations.",
  "2.2 Causal Graphical Model": "Let's formally define the causal graphical model followed by [25]. Consider a set of finitely many random variables denoted as X = ( 𝑋 1 , ..., 𝑋 𝑑 ) with an index set V : = { 1 , ..., 𝑑 } . These random variables have a joint distribution 𝑃 X and a density function 𝑝 ( x ) . A causal graphical model is represented by a Directed Acyclic Graph (DAG) G = ( V , E) , where V represents the nodes or vertices of the graph, and E represents the edges between the nodes. The edges E ⊆ V 2 satisfy the property that for any node 𝑣 ∈ V , ( 𝑣, 𝑣 ) ∉ E , meaning there are no self-loops in the graph. In a causal graph, a random variable 𝑋 𝑖 is considered a direct cause of 𝑋 𝑗 if and only if ( 𝑖, 𝑗 ) ∈ E and ( 𝑗, 𝑖 ) ∉ E . Hence, it is assumed that the causal graph is acyclic, meaning that there are no directed cycles in the graph. This ensures that there are no causal loops where the causal influence could propagate indefinitely. In a DAG, two disjoint subsets of vertices, denoted as A and B , are considered d-separated (see Definition A.1 in appendix) by a third disjoint subset S if all paths connecting nodes in A and B are blocked by S . This relationship is denoted as A ⊥ ⊥ 𝐺 B | S .",
  "3 METHODOLOGY": "",
  "3.1 Causal-indispensable State Representation": "To characterize a set of causal-indispensable state representations for RLRS, we consider decomposing 𝑠 𝑡 into 𝑑 different dimensions denoted as 𝑠 𝑡 = ( 𝑠 1 𝑡 , ..., 𝑠 𝑑 𝑡 ) . Given the causal graphical model for one-step MDP G = ( V , E) , where V = { 𝑎 𝑡 , 𝑠 1: 𝑑 𝑡 , 𝑠 1: 𝑑 𝑡 + 1 } represents the nodes of the graph, and E represents the edges describing the causal relationships between actions and different dimensions of states. It is commonplace that the action variable may not influence every dimension of the state variable, and there are structural relationships among different dimensions of 𝑠 𝑡 . For example, consider the causal On Causally Disentangled State Representation Learning for Reinforcement Learning based Recommender Systems CIKM'24, October 21 - 25, 2024, Boise, Idaho, USA graphical model shown in Figure 1, 𝑎 𝑡 -1 is a direct cause of 𝑠 1 𝑡 and 𝑠 2 𝑡 , and 𝑠 2 𝑡 has an impact on 𝑠 4 𝑡 + 1 , while there is no edge from 𝑎 𝑡 -1 to 𝑠 5 𝑡 . To reflect the different structural relationships between actions and different dimensions of states, and among different dimensions of states, we consider the decomposition of the state 𝑠 𝑡 as follows: Definition 3.1 (Causal Decomposition of State) . Given the causal graphical model, such as the model in Figure 1, that describes the causal relationship among the actions and dimensions of states within the MDP environment, the state can be decomposed into two subsets: C ausalI n D ispensable S tate Representations (CIDS) and causal-dispensable state representations, where the CIDS is defined as including the subsets of state dimensions satisfying one of the following causal relationships: (i) D irectly A ctionI nfluenced S tate Variables (DAIS): A state variable 𝑠 𝑖 𝑡 ∈ S belongs to DAIS 𝑡 if there exists a direct edge from 𝑎 𝑡 -1 ∈ A to 𝑠 𝑖 𝑡 in the causal graph. (ii) A ctionI nfluence A ncestors (AIA): A state variable 𝑠 𝑗 𝑡 ∈ S belongs to AIA 𝑡 if it is an ancestor of any state variable in DAIS in the causal graph. That is, there exists a directed path in the causal graph from 𝑠 𝑗 𝑡 to some 𝑠 𝑖 𝑡 + 1 ∈ DAIS 𝑡 + 1 , and there is no direct edge from any action variable 𝑎 𝑡 -1 ∈ A to 𝑠 𝑗 𝑡 . Correspondingly, a state variable 𝑠 𝑚 𝑡 ∈ S belongs to causal-dispensable state representations (CDS) if it is neither a part of DAIS nor AIA. That is, 𝑠 𝑚 𝑡 has no direct causal relationship with action variable 𝑎 𝑡 ∈ A or any state in DAIS. In RLRS, the state often includes a variety of user and item features. RL agents learn to choose appropriate actions according to the current state vector 𝑠 𝑡 to improve user satisfaction and engagement, in which some dimensions may be redundant for policy learning. In an RLRS, the policy dictates how recommendations are made based on the current state. Utilizing CIDS would mean that the policy learning is based on the most causally indispensable and sufficient state representations when the structural relationship is given. Figure 1 shows different types of state dimensions in the example causal structural relationship. The state variables in purple nodes belong to DAIS, as these are state variables that have a direct causal relationship with action variables (e.g. the historical interaction or the item popularity will be impacted after the action). In other words, they are the descendants of action variables in the causal graph. These state dimensions change directly in response to the actions taken by the agent. The state variables in blue nodes belong to AIA, as these are state variables that are ancestors of the DAIS in the causal graph. They do not directly interact with action variables but have an impact on the DAIS (e.g. user age or gender can influence preferences and, consequently, historical interaction). They form a preceding layer in the causal structure, influencing the states that are directly affected by the agent's actions. Then we demonstrate how the proposed CIDS can be determined by using the conditional dependence and independence relationship among the variables under the following assumptions [23]: A 1. Markov condition 1 and faithfulness for the underlying DAG. A 2. There is an arrow from 𝑠 𝑖 𝑡 -1 to 𝑠 𝑖 𝑡 . 1 Here we use the global version of Markov condition in Definition A.3(i) A 3. There are no long-range arrows, i.e. arrows from 𝑠 𝑖 𝑡 -𝑚 to 𝑠 𝑖 𝑡 for any 𝑚 > 1, and no backward arrows in time. A 4. There are no arrows between state variables at the same timestep, i.e., no arrow from 𝑠 𝑖 𝑡 to 𝑠 𝑗 𝑡 for all 𝑖 and 𝑗 . The assumption A1 allows us to have a one-to-one correspondence between d-separation statements in the graph G and the corresponding conditional independence statements in the distribution. The assumptions A2-A4 impose some restrictions on the connectivity of the graph with respect to the properties of MDP. Theorem 3.1. Under the assumptions A1-A4, 𝑠 𝑖 𝑡 + 1 ∈ DAIS 𝑡 + 1 if and only if 𝑎 𝑡 ̸ ⊥ ⊥ 𝑠 𝑖 𝑡 + 1 | DAIS 𝑡 . Proof. (Proof by Contradiction) Forward Direction ( ⇒ ): Suppose 𝑠 𝑖 𝑡 + 1 ∈ DAIS 𝑡 + 1 . By definition, there exists a direct edge from 𝑎 𝑡 to 𝑠 𝑖 𝑡 + 1 in the causal graph. To prove by contradiction, assume that 𝑎 𝑡 ⊥ ⊥ 𝑠 𝑖 𝑡 + 1 | DAIS 𝑡 . However, 𝑎 𝑡 ⊥ ⊥ 𝑠 𝑖 𝑡 + 1 | DAIS 𝑡 would violate the assumption A1, under which the direct edge from 𝑎 𝑡 to 𝑠 𝑖 𝑡 + 1 implies that 𝑎 𝑡 and 𝑠 𝑖 𝑡 + 1 are not conditionally independent given any set that doesn't include one of them. Therefore, we show that if 𝑠 𝑖 𝑡 + 1 ∈ DAIS 𝑡 + 1 , then 𝑎 𝑡 ̸ ⊥ ⊥ 𝑠 𝑖 𝑡 + 1 | DAIS 𝑡 . Backward Direction ( ⇐ ): Suppose 𝑎 𝑡 ̸ ⊥ ⊥ 𝑠 𝑖 𝑡 + 1 | DAIS 𝑡 , implying a direct or indirect influence from 𝑎 𝑡 to 𝑠 𝑖 𝑡 + 1 . For contradiction, assume 𝑠 𝑖 𝑡 + 1 ∉ DAIS 𝑡 + 1 , meaning there is no direct edge from 𝑎 𝑡 to 𝑠 𝑖 𝑡 + 1 . Considering assumptions A2-A4, the only permissible connection from 𝑎 𝑡 to 𝑠 𝑖 𝑡 + 1 under these constraints is a direct edge, since no long-range arrows, backward arrows, or arrows between state variables at the same timestep are permitted. Consequently, if 𝑎 𝑡 ̸ ⊥ ⊥ 𝑠 𝑖 𝑡 + 1 | DAIS 𝑡 , it implies the existence of a direct edge from 𝑎 𝑡 to 𝑠 𝑖 𝑡 + 1 , necessitating that 𝑠 𝑖 𝑡 + 1 is indeed a part of DAIS 𝑡 + 1 . This stands in contradiction to our initial assumption that 𝑠 𝑖 𝑡 + 1 ∉ DAIS 𝑡 + 1 . Therefore, it is established that if 𝑎 𝑡 ̸ ⊥ ⊥ 𝑠 𝑖 𝑡 + 1 | DAIS 𝑡 , then it must follow that 𝑠 𝑖 𝑡 + 1 ∈ DAIS 𝑡 + 1 . By contradiction in both directions, the theorem is proven. Under assumptions A1-A4, 𝑠 𝑖 𝑡 + 1 ∈ DAIS 𝑡 + 1 if and only if 𝑎 𝑡 ̸ ⊥ ⊥ 𝑠 𝑖 𝑡 + 1 | DAIS 𝑡 . □ Theorem 3.2. Under the assumptions A1-A4, 𝑠 𝑖 𝑡 -1 ∈ AIA 𝑡 -1 if and only if 𝑎 𝑡 -1 ̸ ⊥ ⊥ 𝑠 𝑖 𝑡 -1 | DAIS 𝑡 Proof. (Prove by Contradiction) Forward Direction ( ⇒ ): Suppose 𝑠 𝑖 𝑡 -1 ∈ AIA 𝑡 -1 , meaning there's a directed edge from 𝑠 𝑖 𝑡 -1 to some 𝑠 𝑘 𝑡 ∈ DAIS 𝑡 . Given 𝑠 𝑘 𝑡 ∈ DAIS 𝑡 , there is a directed edge from 𝑎 𝑡 -1 to 𝑠 𝑘 𝑡 . Hence, there is a path between 𝑎 𝑡 -1 and 𝑠 𝑖 𝑡 -1 ∈ AIA 𝑡 -1 : 𝑎 𝑡 -1 → 𝑠 𝑘 𝑡 ← 𝑠 𝑖 𝑡 -1 . Assume for contradiction that 𝑎 𝑡 -1 ⊥ ⊥ 𝑠 𝑖 𝑡 -1 | DAIS 𝑡 , which suggests that DAIS 𝑡 blocks every path between 𝑎 𝑡 -1 and 𝑠 𝑖 𝑡 -1 . However, this contradicts the path 𝑎 𝑡 -1 → 𝑠 𝑘 𝑡 ← 𝑠 𝑖 𝑡 -1 , which is not be blocked by DAIS 𝑡 (see Definition A.1). Therefore, we conclude that if 𝑠 𝑖 𝑡 -1 ∈ AIA 𝑡 -1 , then 𝑎 𝑡 -1 ̸ ⊥ ⊥ 𝑠 𝑖 𝑡 -1 | DAIS 𝑡 . CIKM'24, October 21 - 25, 2024, Boise, Idaho, USA Trovato and Tobin, et al. Backward Direction ( ⇐ ): Suppose 𝑎 𝑡 -1 ̸ ⊥ ⊥ 𝑠 𝑖 𝑡 -1 | DAIS 𝑡 and assume for contradiction that 𝑠 𝑖 𝑡 -1 ∉ AIA 𝑡 -1 . If 𝑠 𝑖 𝑡 -1 ∉ AIA 𝑡 -1 , it implies there is no directed path from 𝑠 𝑖 𝑡 -1 to any state in DAIS 𝑡 . However, 𝑎 𝑡 -1 ̸ ⊥ ⊥ 𝑠 𝑖 𝑡 -1 | DAIS 𝑡 indicates at least one path between 𝑎 𝑡 -1 and 𝑠 𝑖 𝑡 -1 that is not blocked by DAIS 𝑡 . Under assumptions A2-A4, we can see that the path between 𝑎 𝑡 -1 and 𝑠 𝑖 𝑡 -1 suggests existing a directed path from 𝑠 𝑖 𝑡 -1 to some state in DAIS 𝑡 . This directed path contradicts the assumption that 𝑠 𝑖 𝑡 -1 ∉ AIA 𝑡 -1 . Therefore, if 𝑎 𝑡 -1 ̸ ⊥ ⊥ 𝑠 𝑖 𝑡 -1 | DAIS 𝑡 , then 𝑠 𝑖 𝑡 -1 must be in AIA 𝑡 -1 . By proving by contradiction in each direction, we establish the theorem. Under the assumptions A1-A4, 𝑠 𝑖 𝑡 -1 ∈ AIA 𝑡 -1 if and only if 𝑎 𝑡 -1 ̸ ⊥ ⊥ 𝑠 𝑖 𝑡 -1 | DAIS 𝑡 □",
  "3.2 Causal-indispensable State Representation Learning": "When learning the CIDS in practice, a significant challenge arises from the unknown nature of the causal graphical model. Therefore, instead of learning CIDS directly from the causal graphical model, we rely on the collected transition data D = {( 𝑠 𝑙 𝑡 , 𝑎 𝑙 𝑡 , 𝑠 𝑙 𝑡 + 1 , 𝑟 𝑙 𝑡 )} . To tackle this, we initially define the causal structure of a DAG G , which describes the causal relationships within the MDP environment. This structure explicitly encodes the causal relationships between actions and various state dimensions, alongside the relationships among state dimensions themselves:  Here, 𝑠 𝑗 𝑡 + 1 is a dimension of the state at time 𝑡 + 1, and 𝑠 𝑡 + 1 = ( 𝑠 1 𝑡 + 1 , . . . , 𝑠 𝑑 𝑡 + 1 ) . The binary mask M captures the causal structure, and ⊙ denotes the element-wise multiplication. The matrix M 𝑠 → 𝑠 ∈ { 0 , 1 } 𝑑 × 𝑑 indicates causal relationships between dimensions of the state 𝑠 𝑡 and the next state 𝑠 𝑡 + 1 . Specifically, M ( 𝑖,𝑗 ) 𝑠 → 𝑠 = 1 implies an edge from 𝑠 𝑖 𝑡 to 𝑠 𝑗 𝑡 + 1 . Similarly, the matrix M 𝑎 → 𝑠 ∈ { 0 , 1 } 1 × 𝑑 represents causal relationships between the action 𝑎 𝑡 and the dimensions of the next state 𝑠 𝑡 + 1 . The notation M ( , 𝑗 ) 𝑎 → 𝑠 denotes the 𝑗 -th column of this matrix. We operate under the assumption that the causal structure linking 𝑠 𝑡 , 𝑠 𝑡 + 1 , and 𝑎 𝑡 is time-invariant without any unobserved confounders. Based on Definition 3.1, DAIS comprises a set of state variables that have direct edges from the action 𝑎 𝑡 -1 . We can represent DAIS using the causal structure mask M as: DAIS 𝑡 = M 𝑎 → 𝑠 ⊙ 𝑠 𝑡 . Similarly, AIA consists of state variables at time 𝑡 that have a causal influence on any state variable in DAIS 𝑡 + 1 at the next time step. Accordingly, AIA can be formulated as AIA 𝑡 = M 𝑠 → 𝑠 ⊙ 𝑠 𝑡 . As discussed in Section 3.1, DAIS 𝑡 and 𝑎 𝑡 -1 are not conditionally independent given DAIS 𝑡 -1 . In contrast, other state dimensions, except for DAIS 𝑡 , are conditionally independent given DAIS 𝑡 -1 . Similarly, for the Action-Influence Ancestors (AIA), AIA 𝑡 -1 and 𝑎 𝑡 -1 are not conditionally independent given DAIS 𝑡 . However, other state dimensions, excluding AIA 𝑡 , exhibit conditional independence given DAIS 𝑡 . We proceed to formalize the learning process CIDS using the principles of Conditional Mutual Information (CMI). We aim to maximize the CMI to learn the DAIS and AIA in each case. The learning process for DAIS can be formulated by maximizing the following objective:  where 𝐼 (·) denotes the Mutual Information. This equation captures the mutual information between DAIS 𝑡 + 1 and 𝑎 𝑡 conditioned on DAIS 𝑡 . Subsequently, AIA can be learned by maximizing the following:  which similarly utilizes mutual information measures to identify the most relevant state dimensions constituting AIA 𝑡 -1 . The mutual information 𝐼 ( DAIS 𝑡 + 1 ; 𝑎 𝑡 | DAIS 𝑡 ) can be expressed in terms of conditional entropy, which is defined as the difference between the conditional entropy of one variable given the conditioning variable and the conditional entropy of the same variable given both the conditioning variable and the second variable. Thus, it can be defined using conditional entropy as follows:  By leveraging the probabilistic predictive model of DAIS parameterized by 𝜃 𝐷 𝑖 . Then the conditional entropy of DAIS 𝑡 + 1 given DAIS 𝑡 is calculated as: 𝐻    The conditional entropy of DAIS 𝑡 + 1 given both 𝑎 𝑡 and DAIS 𝑡 is calculated similarly, but conditioning on both the previous action and DAIS 𝑡 : 𝐻 ( DAIS 𝑡 + 1 | 𝑎 𝑡 , DAIS 𝑡 )  For the AIA and the corresponding mutual information term, the expression can be reformulated in a similar way: 𝐼 ( AIA 𝑡 -1 ; 𝑎 𝑡 -1 | DAIS 𝑡 ) = 𝐻 ( AIA 𝑡 -1 | DAIS 𝑡 ) -𝐻 ( AIA 𝑡 -1 | 𝑎 𝑡 -1 , DAIS 𝑡 )   where 𝜃 𝐴 𝑖 are the parameters of the probabilistic predictive model of AIA. On Causally Disentangled State Representation Learning for Reinforcement Learning based Recommender Systems CIKM'24, October 21 - 25, 2024, Boise, Idaho, USA Theorem 3.3. (Identifiability of Causal Structure) Given the observable state 𝑠 𝑡 and action 𝑎 𝑡 , which form the equation in Equation (1) , the causal structure masks M 𝑠 → 𝑠 and M 𝑎 → 𝑠 are identifiable under the global Markov condition and faithfulness assumption. The proof of Theorem 3.3 is detailed in Appendix B. This theorem lays the theoretical groundwork necessary for identifying the causal structure masks from observed data.",
  "3.3 Objective Function": "The predictive models are designed with distinct objectives for learning the DAIS and AIA components, parameterized by 𝜃 𝐷 and 𝜃 𝐴 respectively. The objective function for each component is tailored to minimize loss while incorporating a regularization term to promote sparsity in the learned structural matrices. The objective function for the DAIS predictive model is given by:  where L DAIS is related to the learning of DAIS and 𝜆 1 is a hyperparameter that controls the sparsity of the action-to-state causal structure matrix. For the AIA predictive model, the objective function is:  where L AIA is associated with the learning of AIA and 𝜆 2 is a hyperparameter for the sparsity of the state-to-state causal structure matrix. The detailed formulations for L DAIS and L AIA are provided in Section 3.2, while the comprehensive expressions for both objective functions are available in Appendix C.",
  "3.4 Recommendation Policy Learning": "Upon establishing the DAIS and AIA established, we can construct the causal-indispensable state representation by combining these elements: CIDS = ( DAIS , AIA ) . To operationalize this, we define a causal structure matrix for CIDS such that M CIDS = { M 𝑠 → 𝑠 } ∨ { M 𝑎 → 𝑠 } . The CIDS at time 𝑡 can then be succinctly expressed as CIDS 𝑡 = M CIDS ⊙ 𝑠 𝑡 , which serves as the input for learning the recommendation policy. In this way, the recommendation policy chooses the action only with the causal-indispensable state dimensions. The detailed steps of this comprehensive learning approach are outlined in Algorithm 1, encompassing three critical phases: (i) data collection using a sub-optimal policy, (ii) learning of DAIS and AIA from the collected data, and (iii) learning of recommendation policy with CIDS.",
  "4 EXPERIMENTS": "In this section, we begin by performing experiments on an online simulator and recommendation datasets to highlight the remarkable performance of our methods. We then conduct an ablation study to demonstrate the effectiveness of the causal-indispensable state representation.",
  "4.1 Experimental Setup": "We introduce the experimental settings with regard to environments and state-of-the-art RL methods. The implementation details can be found in Appendix D. Algorithm 1 Training Procedure for Predictive Models and Policy Learning 1: Input: Dataset D , empty reply buffer B , initial neural networks parameters 𝜃 𝐷 and 𝜃 𝐴 , initial recommendation policy parameter 𝜃 rec , hyperparameters 𝜆 1 , 𝜆 2 2: for episode = 1, ..., E do 3: for t = 1, ..., T do 4: Sample a random minibatch of 𝐾 trajectories from 𝐷 5: // Optimize DAIS Predictive Model 6: Update 𝜃 𝐷 by minimizing L DAIS-model ( eq. (8)) with minibatch 7: // Optimize AIA Predictive Model 8: Update 𝜃 𝐴 by minimizing L AIA-model ( eq. (9)) with minibatch",
  "9: end for": "",
  "10: end for": "11: // Training of the recommendation policy 𝜋 12: for episode = 1, ..., E do 13: Receive initial observation state 𝑠 1 14: for t = 1, ..., T do 15: Calculate M CIDS based on M 𝑠 → 𝑠 and M 𝑎 → 𝑠 16: Observe state 𝑠 𝑡 and select action 𝑎 ∼ 𝜋 ( M CIDS ⊙ 𝑠 𝑡 ) Execute 𝑎 𝑡 in the environment 17: 18: Observe next state 𝑠 𝑡 + 1 and receive reward 𝑟 𝑡 19: Store transition ( M CIDS ⊙ 𝑠 𝑡 , 𝑎 𝑡 , 𝑠 𝑡 + 1 , 𝑟 𝑡 ) in B 20: Sample a random minibatch of 𝐾 transition from 𝐷 21: Update recommendation policy parameter 𝜃 rec 22: end for 23: end for",
  "4.1.1 Recommendation Environments.": "Online Evaluation. For online evaluation, we employ VirtualTaobao [26], a simulation platform that replicates an online retail environment. This platform leverages data from Taobao, one of China's largest online retail sites, using hundreds of millions of genuine data points. VirtualTaobao generates virtual customers and interactions, enabling our agent to be tested in a simulated \"live\" environment. Offline Evaluation. For offline evaluation, we use the following benchmark datasets: · MovieLens (100k 2 and 1M 3 ) : These datasets, derived from the MovieLens website, feature user ratings of movies. The ratings are on a 5-star scale, with each user providing at least 20 ratings. Movies and users are characterized by 23 and 5 features, respectively. · Douban-Book 4 : The Douban-Book dataset is a collection of user interactions and book information derived from the Douban website, a popular Chinese social networking service. This dataset primarily focuses on user ratings of books. 2 https://grouplens.org/datasets/movielens/100k/ 3 https://grouplens.org/datasets/movielens/1m/ 4 https://huggingface.co/datasets/larrylawl/douban-dushu CIKM'24, October 21 - 25, 2024, Boise, Idaho, USA Trovato and Tobin, et al. · Book-Crossing 5 : The Book-Crossing dataset originates from an online book club known for its book exchange and tracking services. This dataset encompasses user ratings, book information, and user-book interactions. 4.1.2 Baseline. In our experiments, we employ the following algorithms as the baseline: · Deep Deterministic Policy Gradient (DDPG) [18] : An off-policy method suitable for environments with continuous action spaces, employing a target policy network for action computation. · Soft Actor-Critic (SAC) [13] : An off-policy maximum entropy Deep RL approach, optimizing a stochastic policy with clipped double-Q method and entropy regularization. · Twin Delayed DDPG (TD3) [11] : An enhancement over DDPG, incorporating dual Q-functions, less frequent policy updates, and noise addition to target actions. · MACS [30] A method that introduces a counterfactual synthesis policy into the RL-based recommender systems, generating the counterfactual user interaction based on the casual view of MDP for data augmentation. · TPGR [3] : A model for large-scale interactive recommendations, combining RL and a binary tree structure. · PGPR [35] : An explainable recommendation model that integrates knowledge awareness with RL techniques. · DRR-att [20] : A model for interactive recommender systems that introduces an attention network into the state representation module of a deep reinforcement learning recommendation framework. The DRR-att is implemented on the DDPG. Note that the methods TPGR and PGPR use the knowledge graphs which are not available in our experiment, hence we removed the related part to conduct the experiments. 4.1.3 Evaluation Measures. The effectiveness of our model is assessed using different measures in online and offline environments: · Online Evaluation: VirtualTaobao uses click-through rate (CTR) as the indicator of effectiveness. The formula for CTR is as follows:  where 𝑚𝑎𝑥𝑖𝑚𝑢𝑚 _ 𝑟𝑒𝑤𝑎𝑟𝑑 represents the highest potential reward obtainable in a single step within an episode. · Offline Evaluation: For dataset evaluation, we utilize three widely recognized numerical criteria: Precision, Recall, and Accuracy.",
  "4.2 Overall Results": "Online Simulator. The performance comparison, as seen in Figure 2 exhibits that algorithms enhanced with CIDS, namely DDPGCIDS, SAC-CIDS, and TD3-CIDS, surpass the baseline methods significantly across the learning episodes. DDPG-CIDS, for instance, indicates a considerable uplift in CTR over the standard DDPG algorithm. This improvement highlights the impact of integrating 5 https://grouplens.org/datasets/book-crossing/ a causal understanding into the policy learning mechanism. SACCIDS similarly outperforms the conventional SAC model, which underscores the robustness of CIDS in environments with highdimensional action spaces. The TD3-CIDS also shows substantial gains, especially in the later stages of the learning curve, suggesting the effectiveness of CIDS in enhancing temporal-difference learning algorithms. Figure 3 details the 1-step CTR performance of all baselines and our DDPG-CIDS(since the method DRR-att is also implemented on the DDPG framework) in the VirtualTaobao simulation. The DDPG-CIDS again stands out, affirming its superior policy learning capacity by consistently achieving higher average CTRs compared to the other methods. DDPG SAC TD3 DDPG-CIDS SAC-CIDS TD3-CIDS 20000 40000 60000 80000 100000 Timestep Figure 2: Performance comparison of baseline algorithms and corresponding CIDS-enhanced methods in the VirtualTaobao simulation. Offline Dataset. Our experimental evaluation showcases the efficacy of the DDPG-CIDS framework across several datasets, including MovieLens-100k, MovieLens-1M, Douban-Book, and BookCrossing. We benchmarked against a range of state-of-the-art algorithms and highlighted the best and second-best results with bold and asterisks, respectively, as detailed in Table 1. Particularly on MovieLens100k, DDPG-CIDS surpassed all baselines, reinforcing the benefits of integrating CIDS into the recommendation mechanism. Its top scores in Recall, Precision, and Accuracy highlight the method's effectiveness. On the larger MovieLens-1M, DDPG-CIDS maintained top-tier results, suggesting scalability. DDPG-CIDS's superior performance on the Douban-Book and BookCrossing datasets confirmed its adaptability to various content types and user interactions. Overall, these results clearly manifest the advantages of incorporating CIDS into the recommendation policy learning framework. The boosted performance in Precision and Recall signifies that CIDS facilitates the identification of more relevant information, while the improved Accuracy reflects the overall reliability of the policy.",
  "4.3 Ablation Study": "In this section, we first delve into the ablation study designed to dissect the contributions of the different components of our framework, specifically focusing on CIDS and its constituents: DAIS and On Causally Disentangled State Representation Learning for Reinforcement Learning based Recommender Systems CIKM'24, October 21 - 25, 2024, Boise, Idaho, USA Table 1: Performance comparisons of our method with baselines on the MovieLens datasets, Douban-Book, and BookCrossing datasets. The best results are highlighted in bold and the second best use the symbol *. 5 DDPG SAC TD3 TPGR PGPR DDPG-CIDS 1 information needed for making decisions. In contrast, AIA represents state dimensions that do not directly interact with action variables but influence DAIS indirectly. Training with only the AIA representation reveals that a state representation lacking direct action-relevant information can detrimentally affect the learning of the recommendation policy. Hence, it is the combined effect of DAIS and AIA within the DDPG-CIDS framework that leads to superior recommendation performance, underscoring the importance of integrating comprehensive causal knowledge into the recommendation process. Model Name Figure 3: The 1-step CTR performance in the VirtualTaobao simulation is presented as the mean with error bars. AIA. Then we investigate the impact of the sparsity of the learned causal structure on recommendation policy learning. 4.3.1 Impact of CIDS, DAIS, and AIA on Recommendation Policy Learning. Our analysis is aimed at understanding the individual and combined effects of DAIS and AIA components on the policy learning process. We considered three variants: DDPG enhanced with only DAIS (DDPG-DAIS), only AIA (DDPG-AIA), and both DAIS and AIA (DDPG-CIDS). As depicted in Figure 4, DDPG-CIDS consistently outperforms the other variants, suggesting that the combination of DAIS and AIA is critical for capturing the complete causal structure necessary for robust policy learning. While the DDPG-DAIS variant demonstrates some improvements over the baseline DDPG, the DDPG-AIA variant does not perform as well, even falling behind the baseline performance. This observation can be attributed to the inherent characteristics of the state representations. DAIS focuses on the aspects of the state that are directly affected by the action, often containing the most important 4.3.2 Results with Different RL Frameworks. Further, we extended our ablation study to evaluate the impact of integrating CIDS into different RL frameworks: DDPG, SAC, and TD3. As illustrated in Figure 2 and Figure 4, the inclusion of CIDS enhances the performance of all backbone architectures. Notably, SAC-CIDS and TD3-CIDS demonstrate a marked improvement in CTR, highlighting our method's flexibility with different reinforcement learning algorithms. The consistent improvement boost across diverse frameworks validates the adaptability and effectiveness of the CIDS framework for RL-based recommender systems. Table 2: Mean and standard deviation of CTR across various 𝜆 1 values over different timestep 𝑡 in VirtualTaobao, presented as percentages. CIKM'24, October 21 - 25, 2024, Boise, Idaho, USA Trovato and Tobin, et al. Figure 4: Evaluation with different RL frameworks: (a) DDPG as the backbone, (b) SAC as the backbone, and (c) TD3 as the backbone. Ablation versions with only DAIS representation and AIA representation are also included in each backbone. (a) DDPG DDPG-Cios DDPG-DAIS DDPG-AIA š 40000 1o0o0o Timestep (b) SAC SAC-CIDS SAC-DAIS 6ooo 1o0o0o Timestep (c) T03 TD3-CIDS TD3-DAIS TD3-AIA 20000 Timestep 4.3.3 Impact of the Sparsity of Learned Causal Structure on Recommendation Policy Learning. The sparsity of the structural matrices in our model is regulated by the hyperparameters 𝜆 1 and 𝜆 2 , as described in Section 3.3. A greater value for these parameters induces a sparser causal structure. Specifically, we examine the role of 𝜆 1 , which influences the causal structure between actions and state dimensions, to assess its impact on the performance of the recommendation policy. This focus stems from observations in Section 4.3.1, which highlighted that the action-state causal structure significantly affects the recommendation policy, more so than the inter-state causal relationships. Table 2 summarizes the mean and standard deviation of the CTR across a spectrum of 𝜆 1 values, tracked over successive timesteps within the VirtualTaobao simulation. It becomes apparent from the data that the sparsity level, governed by 𝜆 1 , is a critical determinant in the policy's learning performance. Optimal sparsity can enhance policy learning, but an overly sparse structure (such as when 𝜆 1 is set to 9e-4) can be detrimental, possibly due to insufficient information for the policy to leverage. Conversely, a lower 𝜆 1 value may facilitate a more robust early learning phase by providing a richer informational context. However, too low a 𝜆 1 value may introduce noise through non-essential dimensions, thus impeding the policy's optimization process. that derives rewards from three user feedback sources: scores, opinions, and wireless signals. Moving beyond predefining a reward function, Chen et al. [9] introduced InvRec, utilizing inverse reinforcement learning to infer a reward function from user behaviors and directly learning the recommendation policy from these behaviors. InvRec employs inverse DRL as a generator to augment state-action pairs, offering a novel approach to the task. Addressing offline RL methods, Wang et al. [29] proposed CDT4Rec, which designed a new causal mechanism to estimate the reward function. Finally, Chen et al. [6] proposed a causal augmentation method for RLRS, providing a new perspective to address the exploration problem in RLRS.",
  "5 RELATED WORK": "",
  "DRL-based recommender system.": "DRL-based recommender systems model the interaction recommendation process as Markov Decision Processes (MDPs), utilizing deep learning to estimate the value function and tackle highdimensional MDPs [8, 22]. Recognizing the significance of negative feedback in understanding user preferences, Zhao et al. [40] introduced DEERS, which processes positive and negative signals separately at the input layer to avoid negative feedback overwhelming positive signals due to their sparsity. Chen et al. [5] incorporated knowledge graphs into DRL for interactive recommendation, employing a local knowledge network to enhance efficiency. Hong et al. [14] proposed NRRS, a model-based approach integrating nonintrusive sensing and reinforcement learning for personalized dynamic music recommendation. NRRS trains a user reward model Causal Recommendation. In recent years, the recommendation domain has witnessed significant advancements through the integration of causal inference techniques. These techniques, especially in de-biasing training data, have been transformative for the field. Bonner and Vasile [2] developed a domain adaptation algorithm, capitalizing on biased logged feedback to predict randomized treatment effects and address the challenge of random exposure. Further expanding this field, Liu et al. [19] presented KDCRec, a knowledge distillation framework aimed at addressing bias in recommender systems by extracting insights from uniformly distributed data. Zhang et al. [38] tackled the pervasive issue of popularity bias, devising a novel causal inference paradigm to adjust recommendation scores through targeted causal interventions. Additionally, Ding et al. [10] proposed CI-LightGCN, a Causal Incremental Graph Convolution method for updating graph convolutional networks in recommender systems, efficiently handling model updates with new data while maintaining recommendation accuracy. The application of counterfactual inference in recommender systems has also gained traction, being utilized for path-specific effects removal [31] and out-of-distribution (OOD) generalization [32]. Moreover, an increasing number of studies are adopting counterfactual reasoning for objectives such as providing explanations, enhancing model interpretability, and learning robust representations [21, 28, 37].",
  "6 CONCLUSION": "In this study, we address the challenge of high-dimensional and noisy state spaces in RLRS by introducing Causal-Indispensable On Causally Disentangled State Representation Learning for Reinforcement Learning based Recommender Systems CIKM'24, October 21 - 25, 2024, Boise, Idaho, USA State Representations (CIDS). Focusing on Directly Action-Influenced State Variables (DAIS) and Action-Influence Ancestors (AIA), our approach identifies key state components essential for effective recommendation policy learning. Utilizing conditional mutual information, CIDS effectively discerns causal relationships within the generative process, isolating crucial state variables. Theoretical evidence supports the identifiability of these variables, allowing for the construction of optimized state representations. This novel framework enables training on a refined subset of the state space, significantly enhancing recommendation accuracy and efficiency, as demonstrated in our extensive experimental evaluations. Future work could focus on incorporating potential confounders to further refine and strengthen the causal relationships within RLRS. Exploring the simultaneous training of both the representation and policy also presents a promising direction.",
  "A DEFINITIONS IN CAUSALITY": "Here we briefly mention some fundamental definitions [24, 25, 27], which we use in our paper to present and prove our methodology. Definition A.1 (d-Separation [25]) . In Directed Acyclic Graph (DAG) G , a path between two nodes, denoted as 𝑖 𝑛 and 𝑖 𝑚 , is considered blocked by a set S . This occurs when neither 𝑖 𝑛 nor 𝑖 𝑚 are included in S , and there exists a node 𝑖 𝑘 that satisfies one of the following two possibilities: (i) 𝑖 𝑘 ∈ 𝑆 and 𝑖 𝑘 -1 → 𝑖 𝑘 → 𝑖 𝑘 + 1 , 𝑖 𝑘 -1 ← 𝑖 𝑘 ← 𝑖 𝑘 + 1 , or 𝑖 𝑘 -1 ← 𝑖 𝑘 → 𝑖 𝑘 + 1 ; (ii) 𝑖 𝑘 and its descendants are not part of the blocking set S , and 𝑖 𝑘 -1 → 𝑖 𝑘 ← 𝑖 𝑘 + 1 . Definition A.2 (Structural Causal Models [24]) . AStructural Causal Model (SCM) M : = ( 𝑆, 𝑃 𝑈 ) is associated with a DAG G , which consists of a collection 𝑆 of 𝑛 structural assignments :  where 𝑉 = { 𝑋 1 , ..., 𝑋 𝑛 } is a set of endogenous variables, and 𝑷𝑨 𝑖 ⊆ { 𝑋 1 , ..., 𝑋 𝑛 } \\ { 𝑋 𝑖 } represent parents of 𝑋 𝑖 , which are also called direct causes of 𝑋 𝑖 . And 𝑈 = { 𝑈 1 , ..., 𝑈 𝑛 } are noise variables, determined by unobserved factors. We assume that noise variables are jointly independent. Correspondingly, 𝑃 𝑈 is the joint distribution over the noise variables. Each structural equation 𝑓 𝑖 is a causal mechanism that determines the value of 𝑋 𝑖 based on the values of 𝑷𝑨 𝑖 and the noise term 𝑈 𝑖 . Definition A.3 (Markov property [25]) . Given a DAG G , and a joint distribution 𝑃 𝑋 , the distribution is said to satisfy (i) global Markov property in relation to the DAG G if the condition A ⊥ ⊥ 𝐺 B | S implies that A ⊥ ⊥ B | S holds true for all distinct sets of vertices A , B , S . (ii) Markov factorization property in relation to the DAG G if the expression 𝑝 ( x ) = 𝑝 ( 𝑥 1 , 𝑥 2 , ..., 𝑥 𝑑 ) = ˛ 𝑑 𝑖 = 1 𝑝 ( 𝑥 𝑗 | pa G 𝑗 ) holds true. Definition A.4 (Causal Faithfulness [24, 25, 27]) . A distribution 𝑃 is faithful to a DAG G if no conditional independence relations other than the ones entailed by the Markov property are present.",
  "B PROOF OF THEOREM 3.3": "Proof. Firstly, the Markov condition ensures that for any pair of non-adjacent variables in the causal graph G = ( V , E) , denoted 𝑉 𝑖 and 𝑉 𝑗 , there is conditional independence between them given a set of other variables. This condition allows us to infer the absence of direct causal links between certain variables in the MDP. Secondly, the faithfulness assumption posits that all observed conditional independencies are indicative of true separations in the causal graph. This implies that if two variables are found to be conditionally independent given a set of other variables, there is indeed no direct causal path between them in the graph. Together, these conditions permit the identification of the binary structural masks M 𝑠 → 𝑠 and M 𝑎 → 𝑠 defined over the set V , which are identifiable through the examination of conditional independence relationships. □",
  "C OBJECTIVE FUNCTION": "The objective function for the DAIS predictive model is given by: L DAIS-model = -L DAIS + 𝜆 1 ∥ M 𝑎 → 𝑠 ∥ 1  (11) where L DAIS is the loss related to the learning of DAIS and 𝜆 1 is a hyperparameter that controls the sparsity of the action-to-state causal structure matrix. For the AIA predictive model, the objective function is: L = -L + ∥ M ∥ 𝜆  (12) where L AIA is the loss associated with the learning of AIA and 𝜆 2 is a hyperparameter for the sparsity of the state-to-state causal structure matrix.",
  "D IMPLEMENTATION DETAILS": "Initially, we train a sub-optimal recommendation policy utilizing the DDPG algorithm with default parameters as delineated in [18]. This policy undergoes training across 1,000,000 episodes, with the best-performing iteration saved as our expert policy. Utilizing this expert policy, we generate a dataset of expert trajectories within the environment. It should be noted that the expert's exposure to the environment is limited, with only a finite number of trajectories being sampled. These expert trajectories comprise the observed dataset employed for training the DAIS and AIA components. CIKM'24, October 21 - 25, 2024, Boise, Idaho, USA Trovato and Tobin, et al. For the DAIS and AIA predictive models, we opt for Multilayer Perceptron (MLP) networks. Both models feature a tri-layered fully connected architecture with each layer outputting 128 units. Following each hidden layer is a ReLU activation function. The hyperparameters 𝜆 𝑖 are set as follows: 𝜆 1 = 5 × 10 -4 and 𝜆 2 = 10 -4 . In training the recommendation policy, we fix the actor network's learning rate at 10 -4 and the critic network's at 10 -3 . The discount factor, denoted by 𝛾 , is established at 0.95, paired with a soft target update rate, 𝜏 , of 0.001. The network's hidden layer size is determined to be 128, and the replay buffer capacity is 10 6 entries. Weadhere to parameter configurations as specified in stable baselines3 6 or as originally reported in the respective baseline papers for all baseline methods.",
  "REFERENCES": "[1] MMehdi Afsar, Trafford Crump, and Behrouz Far. 2022. Reinforcement learning based recommender systems: A survey. Comput. Surveys 55, 7 (2022), 1-38. [2] Stephen Bonner and Flavian Vasile. 2018. Causal embeddings for recommendation. In Proceedings of the 12th ACM conference on recommender systems . 104-112. [3] Haokun Chen, Xinyi Dai, Han Cai, Weinan Zhang, Xuejian Wang, Ruiming Tang, Yuzhou Zhang, and Yong Yu. 2019. Large-scale interactive recommendation with tree-structured policy gradient. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 33. 3312-3320. [4] Shi-Yong Chen, Yang Yu, Qing Da, Jun Tan, Hai-Kuan Huang, and Hai-Hong Tang. 2018. Stabilizing reinforcement learning in dynamic environment with application to online recommendation. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 1187-1196. [5] Xiaocong Chen, Chaoran Huang, Lina Yao, Xianzhi Wang, Wenjie Zhang, et al. 2020. Knowledge-guided deep reinforcement learning for interactive recommendation. In 2020 International Joint Conference on Neural Networks (IJCNN) . IEEE, 1-8. [6] Xiaocong Chen, Siyu Wang, Lianyong Qi, Yong Li, and Lina Yao. 2023. Intrinsically motivated reinforcement learning based recommendation with counterfactual data augmentation. World Wide Web 26, 5 (2023), 3253-3274. [7] Xiaocong Chen, Siyu Wang, and Lina Yao. 2024. Maximum-Entropy Regularized Decision Transformer with Reward Relabelling for Dynamic Recommendation. arXiv preprint arXiv:2406.00725 (2024). [8] Xiaocong Chen, Lina Yao, Julian McAuley, Guanglin Zhou, and Xianzhi Wang. 2023. Deep reinforcement learning in recommender systems: A survey and new perspectives. Knowledge-Based Systems 264 (2023), 110335. https://doi.org/10. 1016/j.knosys.2023.110335 [9] Xiaocong Chen, Lina Yao, Aixin Sun, Xianzhi Wang, Xiwei Xu, and Liming Zhu. 2021. Generative inverse deep reinforcement learning for online recommendation. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management . 201-210. [10] Sihao Ding, Fuli Feng, Xiangnan He, Yong Liao, Jun Shi, and Yongdong Zhang. 2022. Causal Incremental Graph Convolution for Recommender System Retraining. IEEE Transactions on Neural Networks and Learning Systems (2022), 1-11. https://doi.org/10.1109/TNNLS.2022.3156066 [11] Scott Fujimoto, Herke Hoof, and David Meger. 2018. Addressing function approximation error in actor-critic methods. In International conference on machine learning . PMLR, 1587-1596. [12] Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum, and Marc G Bellemare. 2019. Deepmdp: Learning continuous latent space models for representation learning. In International Conference on Machine Learning . PMLR, 2170-2179. [13] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning . PMLR, 18611870. [14] Daocheng Hong, Yang Li, and Qiwen Dong. 2020. Nonintrusive-Sensing and Reinforcement-Learning Based Adaptive Personalized Music Recommendation. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . 1721-1724. [15] Biwei Huang, Chaochao Lu, Liu Leqi, José Miguel Hernández-Lobato, Clark Glymour, Bernhard Schölkopf, and Kun Zhang. 2022. Action-sufficient state representation learning for control with structural constraints. In International Conference on Machine Learning . PMLR, 9260-9279. [16] Ilya Kostrikov, Kumar Krishna Agrawal, Debidatta Dwibedi, Sergey Levine, and Jonathan Tompson. 2019. Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning. In 7th International 6 https://stable-baselines3.readthedocs.io/en/master/ Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . OpenReview.net. https://openreview.net/forum?id=Hk4fpoA5Km [17] Timothée Lesort, Natalia Díaz-Rodríguez, Jean-Franois Goudou, and David Filliat. 2018. State representation learning for control: An overview. Neural Networks 108 (2018), 379-392. [18] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2015. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971 (2015). [19] Dugang Liu, Pengxiang Cheng, Zhenhua Dong, Xiuqiang He, Weike Pan, and ZhongMing. 2020. A general knowledge distillation framework for counterfactual recommendation via uniform data. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . 831-840. [20] Feng Liu, Ruiming Tang, Xutao Li, Weinan Zhang, Yunming Ye, Haokun Chen, Huifeng Guo, Yuzhou Zhang, and Xiuqiang He. 2020. State representation modeling for deep reinforcement learning based recommendation. Knowledge-Based Systems 205 (2020), 106170. https://doi.org/10.1016/j.knosys.2020.106170 [21] Prashan Madumal, Tim Miller, Liz Sonenberg, and Frank Vetere. 2020. Explainable reinforcement learning through a causal lens. In Proceedings of the AAAI conference on artificial intelligence , Vol. 34. 2493-2500. [22] Tariq Mahmood and Francesco Ricci. 2007. Learning and adaptivity in interactive recommender systems. In Proceedings of the ninth international conference on Electronic commerce . 75-84. [23] Atalanti A Mastakouri, Bernhard Schölkopf, and Dominik Janzing. 2021. Necessary and sufficient conditions for causal feature selection in time series with latent common causes. In International Conference on Machine Learning . PMLR, 7502-7511. [24] Judea Pearl. 2009. Causality . Cambridge university press. [25] Jonas Peters, Dominik Janzing, and Bernhard Schlkopf. 2017. Elements of Causal Inference: Foundations and Learning Algorithms . The MIT Press. [26] Jing-Cheng Shi, Yang Yu, Qing Da, Shi-Yong Chen, and An-Xiang Zeng. 2019. Virtual-taobao: Virtualizing real-world online retail environment for reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 33. 4902-4909. [27] Peter Spirtes, Clark N Glymour, and Richard Scheines. 2000. Causation, prediction, and search . MIT press. [28] Juntao Tan, Shuyuan Xu, Yingqiang Ge, Yunqi Li, Xu Chen, and Yongfeng Zhang. 2021. Counterfactual explainable recommendation. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management . 1784-1793. [29] Siyu Wang, Xiaocong Chen, Dietmar Jannach, and Lina Yao. 2023. Causal decision transformer for recommender systems via offline reinforcement learning. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1599-1608. [30] Siyu Wang, Xiaocong Chen, Julian McAuley, Sally Cripps, and Lina Yao. 2023. Plug-and-Play Model-Agnostic Counterfactual Policy Synthesis for Deep Reinforcement Learning-Based Recommendation. IEEE Transactions on Neural Networks and Learning Systems (2023). [31] Wenjie Wang, Fuli Feng, Xiangnan He, Hanwang Zhang, and Tat-Seng Chua. 2021. Clicks can be cheating: Counterfactual recommendation for mitigating clickbait issue. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1288-1297. [32] Wenjie Wang, Xinyu Lin, Fuli Feng, Xiangnan He, Min Lin, and Tat-Seng Chua. 2022. Causal Representation Learning for Out-of-Distribution Recommendation. In Proceedings of the ACM Web Conference 2022 . 3562-3571. [33] Yu Wang. 2020. A hybrid recommendation for music based on reinforcement learning. In Advances in Knowledge Discovery and Data Mining: 24th PacificAsia Conference, PAKDD 2020, Singapore, May 11-14, 2020, Proceedings, Part I 24 . Springer, 91-103. [34] Zizhao Wang, Xuesu Xiao, Zifan Xu, Yuke Zhu, and Peter Stone. 2022. Causal Dynamics Learning for Task-Independent State Abstraction. In Proceedings of the 39th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 162) , Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (Eds.). PMLR, 23151-23180. https: //proceedings.mlr.press/v162/wang22ae.html [35] Yikun Xian, Zuohui Fu, S Muthukrishnan, Gerard de Melo, and Yongfeng Zhang. 2019. Reinforcement Knowledge Graph Reasoning for Explainable Recommendation. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval . ACM, 285-294. [36] Amy Zhang, Rowan Thomas McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. 2021. Learning Invariant Representations for Reinforcement Learning without Reconstruction. In International Conference on Learning Representations . https://openreview.net/forum?id=-2FCwDKRREu [37] Shengyu Zhang, Dong Yao, Zhou Zhao, Tat-Seng Chua, and Fei Wu. 2021. Causerec: Counterfactual user sequence synthesis for sequential recommendation. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval . 367-377. [38] Yang Zhang, Fuli Feng, Xiangnan He, Tianxin Wei, Chonggang Song, Guohui Ling, and Yongdong Zhang. 2021. Causal intervention for leveraging popularity bias in recommendation. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval . 11-20. CIKM'24, October 21 - 25, 2024, Boise, Idaho, USA On Causally Disentangled State Representation Learning for Reinforcement Learning based Recommender Systems [39] Yang Zhang, Chenwei Zhang, and Xiaozhong Liu. 2017. Dynamic scholarly collaborator recommendation via competitive multi-agent reinforcement learning. In Proceedings of the eleventh ACM conference on recommender systems . 331-335. [40] Xiangyu Zhao, Liang Zhang, Zhuoye Ding, Long Xia, Jiliang Tang, and Dawei Yin. 2018. Recommendations with negative feedback via pairwise deep reinforcement learning. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 1040-1048.",
  "keywords_parsed": [
    "Reinforcement Learning",
    "Recommendation",
    "Causal state representation"
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "Reinforcement learning based recommender systems: A survey"
    },
    {
      "ref_id": "b2",
      "title": "Causal embeddings for recommendation"
    },
    {
      "ref_id": "b3",
      "title": "Large-scale interactive recommendation with tree-structured policy gradient"
    },
    {
      "ref_id": "b4",
      "title": "Stabilizing reinforcement learning in dynamic environment with application to online recommendation"
    },
    {
      "ref_id": "b5",
      "title": "Knowledge-guided deep reinforcement learning for interactive recommendation"
    },
    {
      "ref_id": "b6",
      "title": "Intrinsically motivated reinforcement learning based recommendation with counterfactual data augmentation"
    },
    {
      "ref_id": "b7",
      "title": "Maximum-Entropy Regularized Decision Transformer with Reward Relabelling for Dynamic Recommendation"
    },
    {
      "ref_id": "b8",
      "title": "Deep reinforcement learning in recommender systems: A survey and new perspectives"
    },
    {
      "ref_id": "b9",
      "title": "Generative inverse deep reinforcement learning for online recommendation"
    },
    {
      "ref_id": "b10",
      "title": "Causal Incremental Graph Convolution for Recommender System Retraining"
    },
    {
      "ref_id": "b11",
      "title": "Addressing function approximation error in actor-critic methods"
    },
    {
      "ref_id": "b12",
      "title": "Deepmdp: Learning continuous latent space models for representation learning"
    },
    {
      "ref_id": "b13",
      "title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor"
    },
    {
      "ref_id": "b14",
      "title": "Nonintrusive-Sensing and Reinforcement-Learning Based Adaptive Personalized Music Recommendation"
    },
    {
      "ref_id": "b15",
      "title": "Action-sufficient state representation learning for control with structural constraints"
    },
    {
      "ref_id": "b16",
      "title": "Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning"
    },
    {
      "ref_id": "b17",
      "title": "State representation learning for control: An overview"
    },
    {
      "ref_id": "b18",
      "title": "Continuous control with deep reinforcement learning"
    },
    {
      "ref_id": "b19",
      "title": "A general knowledge distillation framework for counterfactual recommendation via uniform data"
    },
    {
      "ref_id": "b20",
      "title": "State representation modeling for deep reinforcement learning based recommendation"
    },
    {
      "ref_id": "b21",
      "title": "Explainable reinforcement learning through a causal lens"
    },
    {
      "ref_id": "b22",
      "title": "Learning and adaptivity in interactive recommender systems"
    },
    {
      "ref_id": "b23",
      "title": "Necessary and sufficient conditions for causal feature selection in time series with latent common causes"
    },
    {
      "ref_id": "b24",
      "title": "Causality"
    },
    {
      "ref_id": "b25",
      "title": "Elements of Causal Inference: Foundations and Learning Algorithms"
    },
    {
      "ref_id": "b26",
      "title": "Virtual-taobao: Virtualizing real-world online retail environment for reinforcement learning"
    },
    {
      "ref_id": "b27",
      "title": "Causation, prediction, and search"
    },
    {
      "ref_id": "b28",
      "title": "Counterfactual explainable recommendation"
    },
    {
      "ref_id": "b29",
      "title": "Causal decision transformer for recommender systems via offline reinforcement learning"
    },
    {
      "ref_id": "b30",
      "title": "Plug-and-Play Model-Agnostic Counterfactual Policy Synthesis for Deep Reinforcement Learning-Based Recommendation"
    },
    {
      "ref_id": "b31",
      "title": "Clicks can be cheating: Counterfactual recommendation for mitigating clickbait issue"
    },
    {
      "ref_id": "b32",
      "title": "Causal Representation Learning for Out-of-Distribution Recommendation"
    },
    {
      "ref_id": "b33",
      "title": "A hybrid recommendation for music based on reinforcement learning"
    },
    {
      "ref_id": "b34",
      "title": "Causal Dynamics Learning for Task-Independent State Abstraction"
    },
    {
      "ref_id": "b35",
      "title": "Reinforcement Knowledge Graph Reasoning for Explainable Recommendation"
    },
    {
      "ref_id": "b36",
      "title": "Learning Invariant Representations for Reinforcement Learning without Reconstruction"
    },
    {
      "ref_id": "b37",
      "title": "Causerec: Counterfactual user sequence synthesis for sequential recommendation"
    },
    {
      "ref_id": "b38",
      "title": "Causal intervention for leveraging popularity bias in recommendation"
    },
    {
      "ref_id": "b39",
      "title": "Dynamic scholarly collaborator recommendation via competitive multi-agent reinforcement learning"
    },
    {
      "ref_id": "b40",
      "title": "Recommendations with negative feedback via pairwise deep reinforcement learning"
    }
  ]
}