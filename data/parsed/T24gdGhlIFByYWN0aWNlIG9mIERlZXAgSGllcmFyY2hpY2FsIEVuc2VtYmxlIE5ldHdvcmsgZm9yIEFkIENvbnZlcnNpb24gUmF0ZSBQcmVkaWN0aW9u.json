{"On the Practice of Deep Hierarchical Ensemble Network for Ad Conversion Rate Prediction": "Jinfeng Zhuang Pinterest Inc. San Francisco, California, USA jzhuangl@pinterest.com Yinrui Li Pinterest Inc. San Francisco, California, USA yinruili@pinterest.com Runze Su Pinterest Inc. San Francisco, California, USA runzesu@pinterest.com Ke Xu Pinterest Inc. San Francisco, California, USA kxu@pinterest.com Ling Leng Pinterest Inc. San Francisco, California, USA lleng@pinterest.com Yixiong Meng Pinterest Inc. San Francisco, California, USA ymeng@pinterest.com Zhixuan Shao Pinterest Inc. San Francisco, California, USA zshao@pinterest.com Han Sun Pinterest Inc. San Francisco, California, USA hsun@pinterest.com Yang Tang Pinterest Inc. San Francisco, California, USA ytang@pinterest.com Zhifang Liu Pinterest Inc. San Francisco, California, USA zhifangliu@pinterest.com Aayush Mudgal Pinterest Inc. San Francisco, California, USA amudgal@pinterest.com Kungang Li Pinterest Inc. San Francisco, California, USA kungangli@pinterest.com Meng Qi Pinterest Inc. San Francisco, California, USA mengqi@pinterest.com Qifei Shen Pinterest Inc. San Francisco, California, USA qshen@pinterest.com Caleb Lu Pinterest Inc. San Francisco, California, USA klu@pinterest.com Jie Liu Pinterest Inc. San Francisco, California, USA jieliu@pinterest.com Hongda Shen Pinterest Inc. San Francisco, California, USA hshen@pinterest.com Figure 1: Illustration of the workflow of conversion ad recommendation with use behavior sequence modeling. 3. Ad Served 5. Ad Conversion 2. Ad Ranking Conversion Ads Recommendation System Organic Engagement Sequence Ads Engagement Sequence 4. Ad Engagement 8OM+ Search Query_ Sequence 6. Ads Measurement Matched Conversion Sequence 1.Ad Created Personalization Attributed Conversion Pin Sequence", "ABSTRACT": "The predictions of click through rate (CTR) and conversion rate (CVR) play a crucial role in the success of ad-recommendation systems. A Deep Hierarchical Ensemble Network (DHEN) has been proposed to integrate multiple feature crossing modules and has achieved great success in CTR prediction. However, its performance for CVR prediction is unclear in the conversion ads setting, where an ad bids for the probability of a user's off-site actions on a third party website or app, including purchase, add to cart, sign up, etc. Because of the ensemble nature, the degree of freedom of DHEN is essentially high, for example, 1) What feature-crossing modules (MLP, DCN, Transformer, to name a few) should be included in DHEN? 2) How deep and wide should DHEN be to achieve the best tradeoff between efficiency and efficacy? 3) What hyper-parameters to choose in each feature-crossing module? Orthogonal to the model architecture, the input personalization features also significantly impact model performance with a high degree of freedom. It is an important and interesting problem in the advertising industry on how to make DHEN work effectively with a diverse collection of personalization features for web-scale CVR prediction. In this paper, we attack this problem and present our contributions biased to the applied data science side, including: First, we propose a multitask learning framework with DHEN as the single backbone model architecture to predict all CVR tasks, with a detailed study on how to make DHEN work effectively in practice; Second, we build both on-site real-time user behavior sequences and off-site conversion event sequences for CVR prediction purposes, and conduct ablation study on its importance; Last but not least, we propose a self-supervised auxiliary loss to predict future actions in the input sequence, to help resolve the label sparseness issue in CVR prediction. Our method achieves state-of-the-art performance compared to previous single feature crossing modules with pre-trained user personalization features. It has been deployed in our Pinterest conversion ad recommendation system and has significantly boosted both user value and advertiser value by connecting online inspiration to real-world actions.", "CCS CONCEPTS": "\u00b7 Information systems \u2192 Information systems applications ; Computational advertising ; Social networking sites ;", "KEYWORDS": "Conversion Ads Ranking, Multi-task Neural Networks, Sequence Modeling, Personalization, Future Action Prediction Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia \u00a9 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-1331-6/2025/04...$15.00 https://doi.org/10.1145/3701716.3715252", "ACMReference Format:": "Jinfeng Zhuang, Yinrui Li, Runze Su, Ke Xu, Zhixuan Shao, Kungang Li, Ling Leng, Han Sun, Meng Qi, Yixiong Meng, Yang Tang, Qifei Shen, Zhifang Liu, Aayush Mudgal, Caleb Lu, Jie Liu, and Hongda Shen. 2025. On the Practice of Deep Hierarchical Ensemble Network for Ad Conversion Rate Prediction. In Companion Proceedings of the ACM Web Conference 2025 (WWW Companion '25), April 28-May 2, 2025, Sydney, NSW, Australia. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3701716.3715252", "1 INTRODUCTION": "With more than 500 million monthly active users, Pinterest has become an important visual inspiration platform for people to search, save, and shop the best ideas in the world for all of life's moments. The native advertising format of Pinterest presents merchants' advertising content to a user and helps a user decide what to purchase off-site, often with an image or video, title, and well-designed description. A particular ad type called conversion ad, also known as Optimized Cost Per Mille (oCPM) ad in the advertising industry, aims to optimize the rate of user's off-site conversion after impressed on the ad, including checkout, add to cart, sign up, etc. It is a key pillar of an online monetization system as well as a key component to bridge the gap between users' online inspiration and their off-site realization. Predicting the conversion rate (CVR) of an oCPM ad accurately and ranking the conversion ads on top of it can dominate the quality and efficiency of the advertising delivery funnel. Therefore, CVR prediction has been crucial, in general, for the success of the digital advertising business. Besides the importance, it is also a technically interesting problem in a social media's ecosystem, given that both the users' behavior patterns and the data format from the content side are diverse. It takes cutting-edge machine learning techniques to exploit the value in these rich input data, such that the user's value, merchant's value, and platform's business value are jointly maximized. CVR prediction is different from onsite engagement prediction in nature, e.g., click through rate (CTR) prediction, because: 1) the volume of labels is sparser; 2) the delay of collecting labels is longer; 3) the label itself is more noisy. Our high-level recipe for these challenges is better measurement of off-site conversions to improve label quality, and better user understanding to maximize the modeling power on given labels. We limit the scope of this paper to the latter: we optimize serving every user ad content that is highly personalized to the user's interests, tastes, goals and intent given a fixed mechanism of collecting conversion labels 1 . However, personalized CVR prediction is a challenging problem [16]. There is no golden standard for a one-fit-all solution to featurize the diverse data format, and there are many existing feature crossing modules that can handle a given collection of feature vectors. The degree of freedom is essentially high. In addition, how to squeeze the power of the sparse conversion labels has not been explored much in the past. Previously, from the personalization perspective, a pre-trained user embedding that can be plugged into retrieval and ranking models has achieved great success at Pinterest [4, 11, 17, 18, 30, 31, 41]. 1 Pinterest serves personalized ads and uses off-site conversion data for users only when allowed pursuant to user privacy choices and applicable laws. On the Practice of Deep Hierarchical Ensemble Network for Ad Conversion Rate Prediction WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia In particular, the PinnerFormer model [18, 30] that summarizes a user engagement sequence into an embedding vector in a Euclidean space has produced a step function change in home feed recommendation. However, PinnerSAGE or PinnerFormer is optimized for onsite engagement only, which is not optimized for the conversion probability over impression of the ad. Their training data do not include off-site conversion data 1 . From the model architecture perspective, many Deep Neural Networks (DNN) models have been proposed to handle feature crossing, biased to CTR prediction in the ad industry, e.g., Multilayer Perceptron [21], Transformer [23], DeepFM [8], DCN [24-26], FibiNet [13, 37], MaskNet [28], DIN [6, 39, 40], User Behavior Sequence Modeling [1, 2, 19, 20], just to name a few. In order to take advantage of the strengths of different models, a Deep Hierarchical Ensemble Network (DHEN) [35, 36] has been proposed to incorporate different feature crossing modules as a \"cocktail\" solution. It has been applied in Meta's CTR prediction successfully and achieved great business success. It is also technically interesting what modules and configurations make DHEN work in applications. With a focus on optimizing the CVR prediction with DHEN, we present the most important components in the practice of developing a personalized web-scale CVR prediction model. Our contributions and key value proposals include: \u00b7 Build a unified multitask model for all off-site conversion actions with DHEN as the backbone architecture. We conduct detailed analysis and ablation study on what modules are best working; \u00b7 Model user action sequence end-to-end in the CVR prediction task, and prove that it adds significant values in addition to pre-trained embeddings; \u00b7 Introduce a self-supervised future action prediction loss and it can benefit supervised tasks like CVR prediction. Wedeployed the DHEN model in Pinterest's ad recommendation system in 2024 and boosted top-line business metrics like Cost-PerAcquisition (CPA) significantly. The long-term gain in engineering is also significant: we build a framework where we can keep iterating new feature-crossing modules with a built-in self-supervised learning mechanism to address the label sparseness problem. The feature engineering paradigm is shifted to 1) add more types of sequence data to the input; 2) enrich the information per item in the sequence, from designing handcrafted and pre-trained features. We elaborate the key design decisions in section 2, the detailed model architecture in Section 3. Both off-line evaluation and online results are presented in Section 6.", "2 PROBLEM SETUP AND HIGH-LEVEL DESIGN": "In this section, we formulate the CVR prediction problem and elaborate the key design decisions.", "2.1 Problem Setup": "We start with a corpus of \"pins\" P = GLYPH<8> \ud835\udc43 1 , \ud835\udc43 2 , ..., \ud835\udc43 \ud835\udc41 GLYPH<9> , where \ud835\udc41 is a large-scale value on the order of billions, and a set of users U = GLYPH<8> \ud835\udc48 1 , \ud835\udc48 2 , ..., \ud835\udc48 \ud835\udc40 GLYPH<9> , where \ud835\udc40 is a large-scale value on the order of millions. Each pin here is classified into organic content and advertising content . We also have access to the sequence of both onsite actions (click, long click, download, hide, etc.) and off-site conversion actions (checkout, add to cart, sign up, etc.) where allowed pursuant to user privacy choices. Conversion rate prediction is essentially the modeling of the possibility that a user would convert for the advertiser, that is, take expected off-site actions such as checkout, when an ad content bid by this advertiser is presented in front of the user: \ud835\udc53 : U \u00d7 P \u2192 R + . This is usually formulated by the click-based conditional probability: P ( conv; \ud835\udc48,\ud835\udc43 ) = P ( ctr ) \u2217 P GLYPH<0> conv | ctr GLYPH<1> , where P ( ctr ) is the Click-Through Rate (CTR) and P ( conv | ctr ) is the probability of \ud835\udc48 converts after clicking \ud835\udc43 , respectively. It is also not uncommon for impression-based formulations to be used in industry; we omit this case without loss of generality.", "2.2 Multi-Head DNN with CTR Prediction": "As mentioned above, the CVR prediction problem naturally has orders of magnitude less labels than the CTR prediction. The top design choice is how to increase the number of labels. Our solution to this problem is to: \u00b7 Merge all conversion tasks together and learn a unified model to predict them in a single forward inference, instead of building one model per task; \u00b7 Include CTR prediction head in the training process for CVR modeling purpose. This head brings much training labels and will not be used in online inference; \u00b7 Then the final supervised objective function would be a weighted loss defined over each head. We use binary crossentropy (BCE) as the loss function for each head. Mixing all prediction tasks into a single Multi-task Learning (MTL) setting significantly increases the training data. The CTR head has proved very useful to regularize the parameter space and allows us to use a much larger model than CVR tasks only. We choose DHEN [36] as the architecture of the backbone model, by virtue of the ensemble of different feature-crossing modules. Historically, our model evolved through different single feature crossing modules. DHEN has been shown to be capable of taking advantage of each feature-crossing module. Figure 2 presents the overall architecture we used.", "2.3 Hand Crafted Features VS Sequence based User Features": "Feature engineering is an important pillar in the success of CVR prediction. Some representative examples include: 1) engagement counting features in a past rolling time window on a particularly entity, like ad campaign, advertiser, web domain, etc.; 2) predicted category features like interest or product category on a predefined taxonomy thesaurus; 3) pre-trained user embeddings and content embeddings. We argue that for many types of user feature, similar to [34], can be learned directly by a DNN end-to-end, if the crafted feature is extracted from the past user's behavior sequence. For example, the engagement count can simply be implemented as a sum-pooling module over a past time window. Predicted category features and pre-trained embedding features are also essentially derived from WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia Jinfeng Zhuang et al. the user's behavior sequence. Therefore, we focus on exploring what types of sequences to pass to the model and what properties to exploit per item in the sequence. We will discuss the sequence modeling methods in Section 3. Organic sequence VS ads specific sequence. It has been reported that the user onsite engagement sequence is a very strong feature to build user representations [17, 18]. When we zoom into the CVR prediction problem, the sequence data can be extended to the 3 types of sequences, respectively: \u00b7 \ud835\udc46 search : Search Query Sequence, consisting of user's past search queries at Pinterest; \u00b7 \ud835\udc46 org: Organic Content Engagement Sequence, consisting of the content \ud835\udc43 \ud835\udc42 that are not ads generated by advertisers; \u00b7 \ud835\udc46 ads : Ad-content engagement sequence. Note that we do not distinguish the type of the ad product, i.e., both traffic ads and conversion ads are included in \ud835\udc46 ads ; Both \ud835\udc46 org and \ud835\udc46 ads are a sequence of pins, which means that all the metadata and pre-trained signals are available on each item [9, 31, 33]. For completeness, we define the input signals for each pin by a union of ID level signals. Onsite sequence VS off-site sequence. oCPM ads rely on offsite conversion events to measure its performance. It involves User Match between conversion and user, and Attribution , to identify which ad presented to a user leads to their conversions. We define two off-site sequences that have stronger causality with CVR than onsite engagement sequences: \u00b7 \ud835\udc46 match : Matched Conversion Sequence, where each item is a pair of user ID and advertiser ID, together with the off-site action type. It is not known which ad is leading to conversion yet; \u00b7 \ud835\udc46 conv: Attributed Conversion Sequence, consisting of the ad content in which each ad is attributed to an off-site conversion action. Putting together, our input on the personalization features would be an ordered list of user behavior sequences:  We tried merging all sequences into a single one ordered by timestamp, but we found that keeping them separate is better and provides more flexibility. The open question here is how to use the power encoded in S to help predict the CVR.", "2.4 Pre-trained User Embedding VS End-to-end Sequence Modeling": "The pre-trained user embedding is very powerful as a generalpurpose personalization feature at Pinterest [4, 7, 17, 18]. The other view of personalization is to model the user representation endto-end in a particular recommendation model. It has been proven that it is possible to achieve significant gains [29] by modeling the sequence directly in the home feed recommendation. However, it is unknown how it can increase the value in the ad recommendation scenario. We present our sequence models in Section 3 and verify its impact in the experiment section. Figure 2: The Multitask DHEN model architecture for CVR prediction. We found empirically the best trade-off is two layer of feature crossing, where the first layer is MLP + Transformer, and the second layer is MLP + MaskNet. First Ensemble Layer MLP Transformer Feature Preprocessing Ensemble Layer Add & Norm MLP + Sigmoid Add To Cart Checkout Sign Up \u2026 Unified Dimension Projection Cross Entropy Loss Content Feature Context Feature User Feature \u2026 Second Ensemble Layer MLP MaskNet Ensemble Layer Add & Norm CTR Sequence Modeling", "2.5 Batch-mode VS Real-time Sequence": "This comparison is based on the implementation side. There is no difference in the modeling, as both can be handled by an identical architecture. Sequence data are both storage consuming and computationally heavy. It usually takes a separate service to aggregate real-time user actions. In practice, we find that it is a good strategy to start with the offline batch-mode sequence feature. Once it is launched, the gain from the freshness of real-time sequence is well justified and always provides additional value. In this paper, our sequence data contain both batch mode and real-time data.", "3 MULTITASK ENSEMBLE NETWORK WITH SELF-SUPERVISED AUXILIARY LOSS": "In this section, we present the forward architecture for the CVR prediction model. On the Practice of Deep Hierarchical Ensemble Network for Ad Conversion Rate Prediction WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia", "3.1 Deep Hierarchical Ensemble Network": "Figure 2 presents our backbone architecture following a DHEN [36] style of feature interaction. The DHEN model is essentially a cocktail solution that assembles different feature interaction modules for the prediction of CTR. We found that it works well empirically for CVR prediction as well, with a careful selection of feature-crossing modules. We present some details on each component as follows: Feature Preprocessing. We have four steps of preprocessing sequentially: 1) continuous feature normalization: we use min-max normalization to scale continuous features to [ 0 , 1 ] ; 2) batch normalization over pre-trained embedding features; 3) timestamp transformation: for the time at \ud835\udc56 , it will be \ud835\udc61\ud835\udc56\ud835\udc5a\ud835\udc52\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5a\ud835\udc5d [ \ud835\udc56 ] = \ud835\udc59\ud835\udc5c\ud835\udc54 ( \ud835\udc61\ud835\udc56\ud835\udc5a\ud835\udc52\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5a\ud835\udc5d [ \ud835\udc56 ]-\ud835\udc61\ud835\udc56\ud835\udc5a\ud835\udc52\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5a\ud835\udc5d [ \ud835\udc56 -1 ]+ 1 . 0 ) ; 4) categorical feature mapping to embedding vectors, which are learnable variables. Unified Dimension Projection. The input tensors are projected to 3 dimension tensors R \ud835\udc35 \u00d7 \ud835\udc3f \u00d7 \ud835\udc37 , where \ud835\udc35 is the batch size, \ud835\udc3f is number of tokens, and \ud835\udc37 is the dimension. The purpose of this layer is to make input generally compatible with the following feature-crossing modules. Feature-Crossing Modules. We evaluated an extensive set of feature-crossing modules. Before DHEN as the ensemble model, we iterated through MLP [21], Transformer [23], DeepFM [8], SENet [12], DIN [40], DCN [25], DCN V2 [26], MaskNet [28], and HSTU [34]. In practice, we found the ensemble approach with two layers of crossing modules achieves the best online performance within latency and infra cost budget, as presented in Figure 2. Final MLP + Sigmoid. It is a multilayer feedforward MLP plus activation function for each prediction head. Note that each head has its own MLP layers. Sequence Modeling. The user's past behavior sequences defined in Section 2.3 are particularly important. We need to encode user sequence features before they can be fed to the projection layer. In our deployed system, we concatenate the vector of each feature per item as the representation of this item, then pass it to the Transformer Encoder to model the sequence. The sequence feature is important for the model's performance because many hand-crafted features can be derived from a proper operator defined on the sequence. The adoption of Transformers is inspired by their success in language modeling. We further explore their power in solving the label sparseness issue.", "3.2 Self-Supervised Future Action Prediction": "To exploit the interest of the user encoded in the user behavior sequences S , we propose a joint learning between CVR prediction and self-supervised future action prediction. We add a self-supervised loss item \ud835\udc3d : S \u2192 R :  where \ud835\udc48 denotes a user and \ud835\udc46 is a behavior sequence of \ud835\udc48 , \ud835\udc43 denotes an ad, \ud835\udc4c \u2208 { 0 , 1 } is a binary label for a specific conversion action between this pair of ( \ud835\udc48,\ud835\udc43 ) , \ud835\udc53 : U\u00d7P \u21a6\u2192 R is the DNN model that predicts CVR, and BCE is the binary cross-entropy function. The sum of BCE is over different conversion action types, \ud835\udefc is a hyperparameter controlling the importance of the sequence modeling loss. Figure 3: The end-to-end sequence based personalization for conversion ads ranking. It introduces a self-supervised loss to predict future actions in input sequences, which helps to solve the label sparseness problem. DHEN Content Feature Context Feature \u2026 User Feature Feature Preprocessing Sequence Modeling P P P P \u2026 \u2026 \u2026 \u2026 Organic Engagement Sequence Ads Engagement Sequence Matched Conversion Sequence Attributed Conversion Sequence Personalization Self-Supervised Loss Supervised Loss \u2026 Search Query Sequence + Inspired by the idea of pre-trained user embedding [18], we enforce a self-supervised objective function, which defines a loss over the probability of generating target items in \ud835\udc46 from the items happening before some timestamp \ud835\udc61 0:  For web-scale applications, it is computationally prohibitive to model a probability distribution on P . We take advantage of the INFO-NCE loss [22] to mock it. Let x be the forward embedding of an ad \ud835\udc43 from the encoder, then the probability in (2) is approximated by:  where N \ud835\udc61 is a collection of sampled negative pins for position \ud835\udc61 .", "3.3 ParetoNet Parameter Search": "Based on the above proposal, there is a large search space for hyperparameters, with limited computational resources for both training and online serving. We shall find the Pareto front with the performance and cost trade-off. We use a simple, yet effective, neural architecture search (NAS) strategy for parameter search: (1) Define and build the essential baseline minimum viable product model; (2) Define the search space and dimensions to search for; (3) Randomly sample from the search space and train the model to get AUC and Throughput. Use 60 day's data for training and 3 day's data for evaluation for efficiency purpose; (4) Build a predictive model with search dimensions as input features and AUC/Throughput as prediction targets; (5) Sample random points from the search space, and use the predictive model to estimate the AUC and Throughput. (6) Identify the Pareto efficient candidates from Step 5 for optimal model building. WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia Jinfeng Zhuang et al. Figure 4: The Pareto front of model's performance with throughput / infra cost. Throughput (Inverse of Infra Cost) AUC Pareto Front Opportunity", "4 RELATED WORKS": "For the model architecture part, our work is related to sequence modeling and feature crossing. For the learning paradigm side, it resides in the intersection of Contrastive Learning (CL), Selfsupervised Learning (SSL), and Multi-task Learning (MTL).", "4.1 Feature Crossing in DNN": "Given the input features, a focus of DNN architecture design has been to cross the input features including embedding vectors to increase model's capacity. Crossing means multiplying two or more dimensions of input feature vectors. In general, any function that constructs an output dimension involving multiple input dimensions can be counted as a crossing. Some representative examples include [8, 12, 21, 23, 25, 26, 28, 34, 40], etc. We evolved through a single feature crossing module in the history of CVR prediction and found that DHEN is capable of taking different advantages of different feature crossing modules, leading to the final implementation of our model.", "4.2 User Behavior Sequence Modeling in Recommendation Systems": "Sequence modeling has been widely used in e-Commerce or in the prediction of CTR in ads [1, 2, 6, 19, 20, 39, 40]. Another category of modeling user interest is to enrich the input data [5, 15, 38]. These papers often use a variant of Transformer encoder or the attention mechanism to implement the forward user or item encoding. However, they are all built for CTR prediction, instead of CVR prediction. Therefore, they do not consider the label sparseness. The techniques in existing works that optimize long or life-time sequence are not necessary for our purposes because the off-site sequence is usually short. However, the trend in technique in these works on how to leverage the input data is inspirational to our future works, including: 1) use long user behavior sequence. TWIN [2] proposed a general retrieval component to construct a shorter relevant sequence first. This unlocks the inference efficiency of using a life-long user behavior sequence; 2) enrich per item information in the sequence; 3) user more complex data structure than sequence.", "4.3 CL, SSL, and MTL": "The auxiliary objective function of our multisequence modeling is a type of CL [14], which tries to push a positive off-site event far away from randomly sampled negative events in the embedding space. Because the auxiliary loss is constructed from the input sequence feature, instead of the conversion label, it is essentially an SSL method [32]. Finally, because the master conversion prediction model learns multiple conversion events simultaneously, and the SSL loss is also added to the supervised loss, our learning method is MTL [27]. This combination of CL, SSL and MTL is able to help the main learning task, conversion rate prediction, by exploring the user's interests contained in the sequence features. We are essentially formulating an autoregression problem as predicting the item in the attributed conversion sequence. It augments the labels by the past conversions per user.", "5 EXPERIMENTS AND RESULTS": "We present the empirical results and learnings on the practice of CVR prediction, biased towards the models that have been working in real production.", "5.1 Dataset": "Our experiments are conducted on a sample of Pinterest's largescale production dataset, which includes: \u00b7 Positives : All off-site ad insertions attributed conversions (that is, checkout) with full engagement labels, such as clicks and repins. \u00b7 Negatives : A downsampled set of 5% insertions without attributed conversion. The sampled dataset has approximately hundreds of million samples and tens of millions of unique users per day. The training periods for our models range from 60 to 150 days, ensuring sufficient data for robust model training and evaluation. PyTorch DistributedDataParallel is used in the trainer. Training data is randomly shuffled. The training mechanism follows a batch + incremental mode, where we train a warm-up model with 110 days of data. Then we train incrementally for 40 days loading the previous day's checkpoint to warm up.", "5.2 Evaluation Setup and Metrics": "We evaluated the performance of the model on the next day and calculated the average performance. Because CVR prediction is essentially a classification problem, we use ROC-AUC [10] and Precision-Recall AUC (PR-AUC) [3] as the metric. PR-AUC is supposed to be better because positive: negative \u226a 1 is highly skewed. However, we found that either of them can be a better metric than the other when evaluated in an online ad recommendation system. Average Cost Per Action (CPA) is the online evaluation metric, which is a standard metric in the advertising industry, calculated by dividing the total cost of conversions by the total number of conversions. For example, if one ad receives 2 conversions, one costing $2.00 and one costing $4.00, then the average CPA for those conversions is ( $2 . 00 + $4 . 00 )/ 2 = $3 . 00. The smaller the CPA on On the Practice of Deep Hierarchical Ensemble Network for Ad Conversion Rate Prediction WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia Figure 5: The best offline relative ROC-AUC lift of featurecrossing models compared to MLP baseline. 80% 60% 5 g 0.40% 0.00% DCN V2 MaskNet Transformer DHEN an ad platform, the more competitive and attractive its advertising effect for advertisers.", "5.3 From Single Feature-Crossing Module to Ensemble Module": "The first question we examine is whether an ensemble model is better than a single feature-crossing module. The baseline method is MLP as a single crossing module. This method is actually a strong baseline that has been in production for years. Starting from it, we tried many modules developed in recent years. Depending on the data set on which we train and evaluate, not all modules can lead to significant gains. Specifically, we found that the following four modules, among many architectures proposed for CTR prediction, are most effective for CVR prediction both offline and online: \u00b7 MLP: Multi-layer Perceptron as the baseline feature crossing module; \u00b7 DCN V2: Improved DCN with low-rank optimization [26]; \u00b7 MaskNet: Similar to DCN V2 but have parameters with better normalization [28]; \u00b7 Transformer: Standard version in [23]; \u00b7 DHEN: Two-layer ensemble model [36]. Offline Evaluation of Feature-Crossing Modules. Figure 5 presents the relative lift in AUC compared to MLP. All 3 single feature-crossing modules show clear offline gains. DHEN is the best by a large margin, with the same set of input features. This verifies that the ensemble model is capable of capturing the effectiveness of each crossing module. It can bring additional benefits and has been one of the most successful launches in production. Ablation on DHEN configurations. Due to the flexibility of DHEN, it is not clear how deep and how wide DHEN can lead to the best performance. In each layer, it can use any of the combinations of single feature-crossing modules. To evaluate this, using the best Transformer model as the baseline, we summarize the best DHEN tuning results with the following schema: \u00b7 { 1 , 2 , 3 } -Layer: DHEN with different number of layers; \u00b7 3-Layer-S: 3-layer DHEN use exactly the same crossing module per layer Table 1 presents the best AUC result for each setup using ParetoNet training. We draw several interesting observations. First, the best tuned single layer DHEN cannot beat the best multi-layer Transformer. Instead, it drops the AUC by 0. 18%. This Table 1: Best DHEN of different number of layers and different feature crossing modules per layer. \u2713 means a module is used on that layer, and \u2715 means a module is not used. \u00a7: Baseline is 4 layer Transformer as a single crossing module. means that going to multiple layers is still important for DHEN. It does not necessarily beat a carefully tuned single-crossing module. Second, there is no essential difference after two layers for DHEN, while the inference cost will increase. We deployed 2-Layer DHEN into production. Note that the MaskNet module in DHEN already contains 2 horizontal blocks, and the Transformer module inside DHEN already contains 2 layers with 4 heads, which means that the complexity of DHEN increases very quickly with the number of layers. We also found that training more epoches hurts 3-Layer DHEN's performance further, which means that over-fitting is probably the reason that more layers will not improve generalization performance any more. However, the gain from 3-Layer-S is marginal. Fixing the modules in each layer diminishes the gains of DHEN. That confirms that different crossing modules can help each other. Online CPA Reduction. The offline AUC metric does not necessarily map to online performance, because it couples with the ad delivery system with a large number of moving knobs. In order to make online results reliable, we also look at other important business metrics in addition to CPA. The online experiment fixes the ad campaign budget between the control group and the treatment group. The hypothesis for treatment is that the revenue should be close to neutral, but the CPA will decrease due to a better ranking result based on the CVR prediction. Table 2 presents the changes in the online metrics. We explain the meaning of each one and draw conclusions about its movement. For the most important business metric for oCPM ads, the CPA is significantly reduced by 2. 15%, while the total number of conversions (#conversion) increases by 1. 62%, and the conversion rate per impression of ads (iCVR) increases by 4.90%. This is a great success for a model update. We reduce the cost for advertisers to make platform's business more competitive. And it improves conversion volume at the same time. From a platform perspective, the overall CTR drops by 2.70%. However, note that this treatment group works only on the oCPM ad. The conversion ad CTR is actually neutral (0.08% statistically not significant). This learning is important: For oCPM ad campaigns, a significant drop in CTR but a significant increase in conversion volume can occur at the same time. The implication is strong in driving future directions: WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia Jinfeng Zhuang et al. Table 2: Business metric change in the treatment group during online A/B test for deploying DHEN. \u00b7 Business Wise: click on ad can be from entertainment purpose, it does not necessarily provide clear conversion value; \u00b7 Model Wise: entire space optimization can be promising, which means we optimize CTR and CVR together, instead of depending on engagement models predicting CTR. From a user experience perspective, the overall ad load drops by 0. 68%, which is usually a positive move. gCTR30 means \"good click lasting 30 seconds\", which follows similar trends to CTR. The change in ad hide rate is negligible. In summary, our practice leads to a significantly better CVR prediction model and increases advertiser value and platform value by a clear margin, without hurting user experience.", "5.4 Ablation of the Self-Supervised Loss": "Using the ParetoNet search of the parameters in Section 3.3, we are able to identify a few configurations for self-supervised loss, to gain insight into the importance of different factors. Specifically, we examine three areas: \u00b7 Prediction Goal: We predict only the future actions in the input sequences (Next Action Loss abbreviated as NAL), versus the randomly masked actions in the middle (Masked Language Modeling abbreviated as MLM); \u00b7 Data Sampling Ratio: how many positive actions (#Pos) to predict and how many negative actions (#Neg) to sample per positive; \u00b7 Importance of SSL: we can put different weight of the SSL term, split by organic sequences (OrgWeight) and ads sequences (AdsWeight). Table 3 presents the best ParetoNet results for PR-AUC with different configurations. First, the best result is achieved by NAL prediction, which means that predicting future actions and the most recent actions help more than predicting past actions. In terms of SSL data sampling, it is clear that more NAL predictions are more useful (i.e., #Pos is bigger). But different from pre-trained embedding learning, the ratio of negative per positive does not need to be a Table 3: Ablation study of factors impacting SSL performance. Embedding Figure 6: The relative AUC lift of adding each category of user features, with a baseline of removing all user-side features. #Pos #Neg OrgWeight AdsWeight AUC Lift NAL 90 20 0.0002 0.0001 2.18% NAL 90 20 0 0.0002 0.90% NAL 20 100 0.0005 0.0005 1.03% NAL 20 100 0.001 0.001 0.65% NAL 20 100 0.01 0.01 0.29% MLM 60 30 0.0002 0.0001 1.68% 50% 00% 509 0.00%6 Demographic Counting Categorical Pre-trained Sequence large number. A #Pos: #Neg = 1:20 produces the best results. This is probably explained by the fact that NAL is just a helper term in an objective function.", "5.5 Ablation of Different Category of User Features": "The user-side features are very important for the personalized ad recommendation. Although we derived a better model with DHEN, we hope to examine which types of features are most effective for the prediction. To this end, we divide the user-side features into 5 categories: Demographic (e.g., gender, age, location), Counting (e.g., onsite and off-site action count in a past time window), Categorical (e.g., predicted user interest, annotation), Pre-trained User Embedding , and Sequence . We completely remove all user-side features as a baseline. Then we evaluate AUC lift by adding a single category of user-side features. Figure 6 presents the AUC gains for each type of feature. Sequence features are most effective, which double confirms the effect of SSL based user modeling. Another important observation is that pre-trained embedding cannot replace other features, although they are often included in the input of pre-trained embedding learning. The most possible reason we believe is that pre-trained embedding has a different objective function from the CVR prediction task.", "6 CONCLUSION AND FUTURE WORKS": "The prediction of the conversion rate is of the utmost importance in an online advertising platform. We proposed applying an ensemble approach for the ad conversion rate prediction with an auxilary selfsupervised future action prediction task for the input sequences, biased to the applied data science perspective on what factors are most effective for the final model's performance. We conducted a detailed ablation study on both hyperparameters and user-side features. The proposed modeling approach achieved great offline On the Practice of Deep Hierarchical Ensemble Network for Ad Conversion Rate Prediction WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia gains and online business metric gains. Looking into the future, there are four important areas that call for cutting-edge applied science and techniques. 1) Entire space optimization: the CVR prediction is orthogonal to the engagement models in reality. It is shown that CVR can increase while CTR drops. Optimize engagement tasks such as CTR, gCTR30, favorite, etc. can probably improve the performance on the CVR. 2) Better sequence modeling: The user behavior sequence turns out to be the most important user-side feature. It deserves more exploration on how to maximize its effect, including even longer sequences, a generative AI modeling approach, and enriching the items in sequence. 3) Delayed feedback of conversion: CVR labels depend on the measurement of conversion in the third-party data. It usually has a delay in collecting this data. 4) Privacy-preserving machine learning: a trend in industry is to protect user data which will have a clear negative impact on the quantity and quality of training data. How to optimize models while respecting users' privacy is an important topic for oCPM ads.", "REFERENCES": "[1] Yue Cao, Xiaojiang Zhou, Jiaqi Feng, Peihao Huang, Yao Xiao, Dayao Chen, and Sheng Chen. 2022. Sampling Is All You Need on Modeling Long-Term User Behaviors for CTR Prediction. CoRR abs/2205.10249 (2022). arXiv:2205.10249 https://doi.org/10.48550/arXiv.2205.10249 [2] Jianxin Chang, Chenbin Zhang, Zhiyi Fu, Xiaoxue Zang, Lin Guan, Jing Lu, Yiqun Hui, Dewei Leng, Yanan Niu, Yang Song, and Kun Gai. 2023. TWIN: TWo-stage Interest Network for Lifelong User Behavior Modeling in CTR Prediction at Kuaishou. In DKK . 3785-3794. [3] Jesse Davis and Mark Goadrich. 2006. The relationship between Precision-Recall and ROC curves. In ICML , Vol. 148. 233-240. [4] Stephanie deWet and Jiafan Ou. 2019. Finding Users Who Act Alike: Transfer Learning for Expanding Advertiser Audiences. In KDD . 2251-2259. [5] Zhifang Fan, Dan Ou, Yulong Gu, Bairan Fu, Xiang Li, Wentian Bao, Xin-Yu Dai, Xiaoyi Zeng, Tao Zhuang, and Qingwen Liu. 2022. Modeling Users' Contextualized Page-wise Feedback for Click-Through Rate Prediction in E-commerce Search. In WSDM . 262-270. [6] Yufei Feng, Fuyu Lv, Weichen Shen, Menghan Wang, Fei Sun, Yu Zhu, and Keping Yang. 2019. Deep Session Interest Network for Click-Through Rate Prediction. In IJCAI . 2301-2307. [7] Mihajlo Grbovic and Haibin Cheng. 2018. Real-time Personalization using Embeddings for Search Ranking at Airbnb. In KDD , Yike Guo and Faisal Farooq (Eds.). ACM, 311-320. [8] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: A Factorization-Machine based Neural Network for CTR Prediction. In IJCAI , Carles Sierra (Ed.). 1725-1731. [9] William L. Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive Representation Learning on Large Graphs. In NeurIPS . 1024-1034. [10] James A. Hanley and Barbara J. McNeil. 1982. The meaning and use of the area under a receiver operating characteristic (ROC) curve. Radiology 143(1) (1982), 29-36. [11] Yi-Ping Hsu, Po-Wei Wang, Chantat Eksombatchai, and Jiajing Xu. 2024. Taming the One-Epoch Phenomenon in Online Recommendation System by Two-stage Contrastive ID Pre-training. In Proceedings of the 18th ACM Conference on Recommender Systems . 838-840. [12] Jie Hu, Li Shen, and Gang Sun. 2018. Squeeze-and-Excitation Networks. In CVPR . 7132-7141. [13] Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. 2019. FiBiNET: combining feature importance and bilinear feature interaction for click-through rate prediction. In Proceedings of the 13th ACM conference on recommender systems . 169-177. [14] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. 2020. A Survey on Contrastive Self-supervised Learning. CoRR abs/2011.00362 (2020). arXiv:2011.00362 [15] Wensen Jiang, Yizhu Jiao, Qingqin Wang, Chuanming Liang, Lijie Guo, Yao Zhang, Zhijun Sun, Yun Xiong, and Yangyong Zhu. 2022. Triangle Graph Interest Network for Click-through Rate Prediction. In WSDM . 401-409. [16] Kungang Li, Xiangyi Chen, Ling Leng, Jiajing Xu, Jiankai Sun, and Behnam Rezaei. 2024. Privacy Preserving Conversion Modeling in Data Clean Room. In Proceedings of the 18th ACM Conference on Recommender Systems . 819-822. [17] Aditya Pal, Chantat Eksombatchai, Yitong Zhou, Bo Zhao, Charles Rosenberg, and Jure Leskovec. 2020. PinnerSage: Multi-Modal User Embedding Framework for Recommendations at Pinterest. In KDD . 2311-2320. [18] Nikil Pancha, Andrew Zhai, Jure Leskovec, and Charles Rosenberg. 2022. PinnerFormer: Sequence Modeling for User Representation at Pinterest. In KDD . ACM, 3702-3712. [19] Qi Pi, Weijie Bian, Guorui Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Practice on Long Sequential User Behavior Modeling for Click-Through Rate Prediction. CoRR abs/1905.09248 (2019). arXiv:1905.09248 [20] Qi Pi, Xiaoqiang Zhu, Guorui Zhou, Yujing Zhang, Zhe Wang, Lejian Ren, Ying Fan, and Kun Gai. 2020. Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate Prediction. CoRR abs/2006.05639 (2020). arXiv:2006.05639 [21] F. Rosenblatt. 1958. The perceptron: A probabilistic model for information storage and organization in the brain. Psychological Review 65, 6 (1958), 386-408. https://doi.org/10.1037/h0042519 [22] A\u00e4ron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation Learning with Contrastive Predictive Coding. CoRR abs/1807.03748 (2018). arXiv:1807.03748 [23] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In NeurIPS . 6000-6010. [24] Fangye Wang, Hansu Gu, Dongsheng Li, Tun Lu, Peng Zhang, and Ning Gu. 2023. Towards deeper, lighter and interpretable cross network for ctr prediction. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management . 2523-2533. [25] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & Cross Network for Ad Click Predictions. In ADKDD . 12:1-12:7. [26] Ruoxi Wang, Rakesh Shivanna, Derek Zhiyuan Cheng, Sagar Jain, Dong Lin, Lichan Hong, and Ed H. Chi. 2021. DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems. In WWW . 17851797. [27] Yuhao Wang, Ha Tsz Lam, Yi Wong, Ziru Liu, Xiangyu Zhao, Yichao Wang, Bo Chen, Huifeng Guo, and Ruiming Tang. 2023. Multi-Task Deep Recommender Systems: A Survey. CoRR abs/2302.03525 (2023). arXiv:2302.03525 [28] Zhiqiang Wang, Qingyun She, and Junlin Zhang. 2021. MaskNet: Introducing Feature-Wise Multiplication to CTR Ranking Models by Instance-Guided Mask. CoRR abs/2102.07619 (2021). arXiv:2102.07619 [29] Xue Xia, Pong Eksombatchai, Nikil Pancha, Dhruvil Deven Badani, Po-Wei Wang, Neng Gu, Saurabh Vishwas Joshi, Nazanin Farahpour, Zhiyuan Zhang, and Andrew Zhai. 2023. TransAct: Transformer-based Realtime User Action Model for Recommendation at Pinterest. In KDD . ACM, 5249-5259. [30] Jiajing Xu, Andrew Zhai, and Charles Rosenberg. 2022. Rethinking Personalized Ranking at Pinterest: An End-to-End Approach. In RecSys '22 . ACM, 502-505. [31] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and Jure Leskovec. 2018. Graph Convolutional Neural Networks for WebScale Recommender Systems. In KDD . 974-983. https://doi.org/10.1145/3219819. 3219890 [32] Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Jundong Li, and Zi Huang. 2024. Self-Supervised Learning for Recommender Systems: A Survey. IEEE Trans. Knowl. Data Eng. 36, 1 (2024), 335-355. [33] Andrew Zhai, Hao-Yu Wu, Eric Tzeng, Dong Huk Park, and Charles Rosenberg. 2019. Learning a Unified Embedding for Visual Search at Pinterest. In KDD . 2412-2420. [34] Jiaqi Zhai, Lucy Liao, Xing Liu, Yueming Wang, Rui Li, Xuan Cao, Leon Gao, Zhaojie Gong, Fangda Gu, Michael He, Yinghai Lu, and Yu Shi. 2024. Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations. CoRR abs/2402.17152 (2024). https://doi.org/10.48550/ ARXIV.2402.17152 arXiv:2402.17152 [35] Buyun Zhang, Liang Luo, Yuxin Chen, Jade Nie, Xi Liu, Daifeng Guo, Yanli Zhao, Shen Li, Yuchen Hao, Yantao Yao, et al. 2024. Wukong: Towards a Scaling Law for Large-Scale Recommendation. arXiv preprint arXiv:2403.02545 (2024). [36] Buyun Zhang, Liang Luo, Xi Liu, Jay Li, Zeliang Chen, Weilin Zhang, Xiaohan Wei, Yuchen Hao, Michael Tsang, Wenjun Wang, Yang Liu, Huayu Li, Yasmine Badr, Jongsoo Park, Jiyan Yang, Dheevatsa Mudigere, and Ellie Wen. 2022. DHEN: A Deep and Hierarchical Ensemble Network for Large-Scale Click-Through Rate Prediction. CoRR abs/2203.11014 (2022). [37] Pengtao Zhang, Zheng Zheng, and Junlin Zhang. 2023. FiBiNet++: Reducing model size by low rank feature interaction layer for CTR prediction. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management . 4425-4429. [38] Zuowu Zheng, Changwang Zhang, Xiaofeng Gao, and Guihai Chen. 2022. HIEN: Hierarchical Intention Embedding Network for Click-Through Rate Prediction. In SIGIR . ACM, 322-331. [39] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu, and Kun Gai. 2018. Deep Interest Evolution Network for Click-Through Rate Prediction. CoRR abs/1809.03672 (2018). arXiv:1809.03672 http://arxiv.org/abs/ 1809.03672 [40] Guorui Zhou, Xiaoqiang Zhu, Chengru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep Interest Network for WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia Jinfeng Zhuang et al. Table 4: Hyperparameters of the three type of featurecrossing modules (MLP, MaskNet, Transformer) in DHEN. MaskNet #MaskBlock=2, horizontal MaskBlock layout #HiddenSize=256, Dropout=0.005 Transformer #Layer=2, #Head=4 #HiddenSize=256, #Forward MLP Size=512 DropOut=0 MLP Both MLP have the same output size 1024 Click-Through Rate Prediction. In KDD . 1059-1068. [41] Jinfeng Zhuang and Yu Liu. 2019. PinText: A Multitask Text Embedding System in Pinterest. In KDD . 2653-2661. https://doi.org/10.1145/3292500.3330671", "A HYPERPARAMETERS OF THE DEPLOYED DHEN MODEL": "A significant amount of time was spent trying to determine which feature-crossing module to use in DHEN and what the parameters are for each module. In case it helps the audience, we list the parameters of each module in Table 4. We list the important parameter choices used in production below: \u00b7 DHEN has two layers and each layer has 2 feature-crossing modules, where the ensemble modes are both sum; \u00b7 The final MLP module has 3 sequential layers [ 128 , 128 , 128 ] ; \u00b7 The unified dimension projection module maps each feature to an embedding with dimension 64; \u00b7 User behavior sequence length is cut off at 500; \u00b7 The total number of model parameters is 340M; \u00b7 Training is by AWS p4d.24xlarge with 8 Nvidia Tesla A100 GPUs. PyTorch DistributedDataParallel is used in the trainer. Training data is randomly shuffled; \u00b7 All models performance are sensitive to Learning Rate in training; The exact value is algorithm dependent."}
