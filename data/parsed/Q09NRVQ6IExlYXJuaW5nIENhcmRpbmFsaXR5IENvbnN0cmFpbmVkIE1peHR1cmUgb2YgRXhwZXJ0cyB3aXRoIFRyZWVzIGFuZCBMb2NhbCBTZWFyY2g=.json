{"title": "COMET: Learning Cardinality Constrained Mixture of Experts with Trees and Local Search", "authors": "Shibal Ibrahim; Wenyu Chen; Hussein Hazimeh; Natalia Ponomareva; Zhe Zhao; Rahul Mazumder", "pub_date": "2023-06-05", "abstract": "The sparse Mixture-of-Experts (Sparse-MoE) framework efficiently scales up model capacity in various domains, such as natural language processing and vision. Sparse-MoEs select a subset of the \"experts\" (thus, only a portion of the overall network) for each input sample using a sparse, trainable gate. Existing sparse gates are prone to convergence and performance issues when training with first-order optimization methods. In this paper, we introduce two improvements to current MoE approaches. First, we propose a new sparse gate: COMET, which relies on a novel tree-based mechanism. COMET is differentiable, can exploit sparsity to speed up computation, and outperforms state-of-the-art gates. Second, due to the challenging combinatorial nature of sparse expert selection, first-order methods are typically prone to low-quality solutions. To deal with this challenge, we propose a novel, permutation-based local search method that can complement first-order methods in training any sparse gate, e.g., Hash routing, Top-k, DSelect-k, and COMET. We show that local search can help networks escape bad initializations or solutions. We performed large-scale experiments on various domains, including recommender systems, vision, and natural language processing. On standard vision and recommender systems benchmarks, COMET+ (COMET with local search) achieves up to 13% improvement in ROC AUC over popular gates, e.g., Hash routing and Top-k, and up to 9% over prior differentiable gates e.g., DSelect-k. When Top-k and Hash gates are combined with local search, we see up to 100\u00d7 reduction in the budget needed for hyperparameter tuning. Moreover, for language modeling, our approach improves over the state-of-the-art MoEBERT model for distilling BERT on 5/7 GLUE benchmarks as well as SQuAD dataset.", "sections": [{"heading": "INTRODUCTION", "text": "The Sparse Mixture of Experts (Sparse-MoE) framework has led to state-of-the-art performance in various applications such as natural language processing (NLP) [2,13,14,47,58], vision [45,55], time-series analysis [25], multi-task learning [20,31,36], and multimodal learning [39]. Sparse-MoE consists of a set of trainable experts (neural networks) and a trainable sparse gate. The sparse gate in Sparse-MoE selects an appropriate subset of experts on a per-sample basis, which allows for faster computation [47] and enhances interpretability [14,25].\nThe literature on Sparse-MoE has traditionally focused on Topk gating, which selects \ud835\udc58 out of \ud835\udc5b experts using a Top-k operation [14,47,58]. Top-k gating is simple and efficient because it allows sparse training. However, as highlighted by prior literature [14,20,58], the non-continuous nature of Top-k makes it susceptible to stability and convergence issues. Alternative gating strategies exist in the literature, based on reinforcement learning [4] or postprocessing via linear assignment [8,34]. However, these strategies also face challenges in terms of efficiency and interpretability; see related work in Section 2 for more details. Random routing strategies [44,59] alternatively bypass learning of the gating function altogether. Although computationally efficient, these strategies lead to performance degradation [8]. Recent work [20] demonstrates that differentiable gating in Sparse-MoE can improve stability and performance compared to popular non-differentiable gates. However, it suffers from expert collapse in some cases as we observed in our experiments.\nIn this paper, we propose two new approaches for improving routing in Sparse-MoE. First, we introduce a novel differentiable sparse gate COMET 1 that improves over existing state-of-the-art sparse gates [14,20,44,47,58]. Second, we argue that the combinatorial nature of expert selection in Sparse-MoE presents a serious challenge for first-order methods. In particular, the performance of these methods is highly dependent on initialization, and they can get stuck in low-quality routing solutions. Thus, we propose a new permutation-based local search method for Sparse-MoEs, which can help first-order methods escape low-quality initializations or solutions. Our local search approach is general and can be applied to any sparse gate, including Top-k [47], Hash routing [44], DSelect-k [20], and our proposed gate COMET.\nCOMET. Our proposed COMET gate is the first decision-treebased selection mechanism for sparse expert selection -decision trees naturally perform per-sample routing (i.e., each sample follows a root-to-leaf path). Our gate has several advantages: (i) it is differentiable and can be optimized using first-order optimization methods e.g., stochastic gradient descent; (ii) it allows (partially) conditional training, i.e., dense-to-sparse training; (iii) it enforces a cardinality constraint, i.e., selects (at most) k out of the n experts; (iv) it has superior predictive performance over state-of-the-art gates such as Hash routing, Top-k, and DSelect-k.\nLocal Search. The learning problem underlying Sparse-MoEs is of combinatorial nature, which poses additional challenges compared to non-MoE machine learning models. Popularly used optimization methods, such as SGD, may lead to low-quality solutions in Sparse-MoE, as we demonstrate in our numerical experiments in Section 5. To this end, we propose a permutation-based local search method, which can help first-order methods escape bad initializations and lead to better sample routing for any sparse gate e.g., Top-k, Hash routing, DSelect-k and even COMET. To the best of our knowledge, we are the first to explore local search methods in the context of Sparse-MoE. We provide empirical evidence through ablation studies and large-scale experiments to demonstrate permutationbased local search (i) pushes learning towards better gate/expert initializations in early optimization stages (see Section 4.4); (ii) effectively reduces the budget needed for hyperparameter tuning by up to 100\u00d7 for some popular gates e.g., Hash Routing and Topk (see Section 5.1.3); (iii) leads to SOTA performance in terms of prediction and expert selection when combined with COMET, across various applications (see Section 5).\nContributions. As discussed earlier, it is well-known in the literature that popular sparse gates are challenging to train and may suffer from stability and performance issues. In this context, our contributions can be summarized as follows:\n\u2022 We propose COMET, a novel tree-based sparse gate that simultaneously has the following desirable properties: (a) differentiable, (b) allows (partially) conditional training i.e., dense-to-sparse training, and sparse inference, (c) satisfies per-sample cardinality constraint (selects at most \ud835\udc58 out of the \ud835\udc5b experts per-sample, where \ud835\udc58 is a user-specified parameter). ", "publication_ref": ["b1", "b12", "b13", "b46", "b57", "b44", "b54", "b24", "b19", "b30", "b35", "b38", "b46", "b13", "b24", "b13", "b46", "b57", "b13", "b19", "b57", "b3", "b7", "b33", "b43", "b58", "b7", "b19", "b13", "b19", "b43", "b46", "b57", "b46", "b43", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "RELATED WORK", "text": "Sparse-Mixture-of-Experts. The MoE framework was introduced by [27], and since then has been extensively studied -see e.g., [26,28,29]. More recently, [47] proposed a Sparse-MoE framework, based on the Top-k gate, and showed good performance on natural language processing tasks. It was further improved upon by [14,56,58]. However, Top-k gate does not optimize the core expert selection problem as pointed out by [8]. Additionally, as highlighted by prior literature [14,20,58], the non-continuous nature of Top-k makes it vulnerable to training stability and convergence issues.\nWith BASE Layers, [8,34] formulate Sparse-MoE as an assignment problem where they post-process the gate output for balanced expert selection. [4] formulates the expert selection as a reinforcement learning problem. Others [44,59] proposed random routing strategies that do not learn the gating function during training. These methods are also promising as they have been shown to outperform models that learn routing through Top-k, e.g., in Switch Transformers [14]. Lastly, [20] introduced DSelect-k, a differentiable gate based on binary encodings, which improves over Top-k in terms of stability and statistical performance.\nConditional Computation. In addition to the Sparse-MoE framework, there are other related works that also study conditional computation, i.e., the setup where only some parts of neural network are activated based on the input -see e.g., [4,5,23,50]. These works rely on heuristics where the training and inference models are different. More recently, [19] introduced conditional computation in differentiable (a.k.a. soft) trees [15,19,21,22,24,27]. Their proposal allows routing samples through small parts of the tree; thus allowing for conditional computation with customized algorithms. Our work builds upon this approach to solve the cardinality-constrained expert selection problem in Sparse-MoE. Note that [19] does not address sparse expert selection in Sparse-MoE.\nLocal Search and Permutation Learning. There is an extensive optimization literature on local search, e.g., [3,18]. However, such methods have not been used in Sparse-MoE. Here, we survey permutation learning methods that are most relevant to our proposal. This work uses differentiable relaxations of permutation via Sinkhorn operators [1,37]. These earlier works use these relaxations in other contexts e.g., ranking in [1] and sorting in [37]. We use permutation learning as a local search to complement first-order optimization methods to improve sample routing in Sparse-MoE.", "publication_ref": ["b26", "b25", "b27", "b28", "b46", "b13", "b55", "b57", "b7", "b13", "b19", "b57", "b7", "b33", "b3", "b43", "b58", "b13", "b19", "b3", "b4", "b22", "b49", "b18", "b14", "b18", "b20", "b21", "b23", "b26", "b18", "b2", "b17", "b0", "b36", "b0", "b36"], "figure_ref": [], "table_ref": []}, {"heading": "LEARNING SPARSE MIXTURE OF EXPERTS WITH DECISION TREES", "text": "Problem Setup of Sparse-MoE. We first review the Sparse-MoE objective. We assume that the task has an input space X \u2286 R \ud835\udc5d and an output space Y \u2286 R \ud835\udc62 . Denote the \ud835\udc5b-dimensional simplex by The goal of Sparse-MoE paradigm is to develop a gate that selects a convex combination of at most \ud835\udc58 out of the \ud835\udc5b experts. The output of the gate can be thought of as a probability vector \ud835\udc54 with at most \ud835\udc58 nonzero entries, where \ud835\udc54(\u2022) \ud835\udc56 is the weight assigned to the expert \ud835\udc53 \ud835\udc56 . The underlying optimization problem (also in [20]) is:\n\u0394 \ud835\udc5b = {\ud835\udc64 \u2208 R \ud835\udc5b : \ud835\udc56 \u2208 [\ud835\udc5b] \ud835\udc64 \ud835\udc56 = 1,\nmin \ud835\udc53 1 ,\u2022\u2022\u2022 ,\ud835\udc53 \ud835\udc5b ,\ud835\udc54 1 \ud835\udc41 (\ud835\udc65,\ud835\udc66) \u2208 D \u2113 \ud835\udc66, \ud835\udc56 \u2208 [\ud835\udc5b] \ud835\udc53 \ud835\udc56 (\ud835\udc65)\ud835\udc54(\ud835\udc65) \ud835\udc56 ,(1a)\ns.t. \u2225\ud835\udc54(\ud835\udc65)\u2225 0 \u2264 \ud835\udc58, \ud835\udc54(\ud835\udc65) \u2208 \u0394 \ud835\udc5b , \u2200\ud835\udc65 \u2208 X.(1b\n) \u2225\ud835\udc54(\u2022) \u2225 0 denotes the number of nonzero entries in the vector \ud835\udc54(\u2022), \u2113 (\u2022, \u2022) is the associated loss function such that \u2113 : Y \u00d7 X \u2192 R, and \ud835\udc41 denotes the size of training samples D = {(\ud835\udc65 \ud835\udc56 , \ud835\udc66 \ud835\udc56 ) \u2208 X \u00d7 Y} \ud835\udc41 \ud835\udc56=1 . The cardinality constraint in (1b) ensures that the gate selects at most \ud835\udc58 experts. Some popular gates e.g., Top-k impose exact cardinality constraint instead of an inequality constraint in (1b). However, the inequality constraint can allow for sparser expert selection as observed in prior work [20] and in our experiments (Section 5). Problem (1) is a combinatorial optimization problem that is not amenable to stochastic gradient descent due to the cardinality constraint in (1b). In the next sections, we discuss our formulation that ensures the cardinality constraint and the simplex constraints are satisfied despite optimization with gradient-based methods.\nThe rest of this section is organized as follows. In Section 3.1, we discuss a high-level overview of our novel tree-based framework, which equivalently sets up the cardinality-constrained objective in problem (1) as a weighted sum of decision trees. Next in Section 3.2, we provide background on a single decision tree that selects a single expert per-sample while (i) allowing for smooth optimization, and (ii) conditional computation support -routing samples to a single leaf. Later in Section 3.3, we dive deeper into our novel treebased framework that combines such trees to satisfy the cardinality constraint for \ud835\udc58 \u2265 1 without violating the simplex constraint. We additionally highlight important aspects regarding leaf parameterization and regularization. Next, in Section 3.4, we discuss how our method handles settings where experts are non-powers of 2. We then discuss in Section 3.5 an implementation of COMET for numerically stable training.", "publication_ref": ["b19", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Sparse-MoE with \ud835\udc58 decision trees", "text": "The cardinality constrained MoE objective (1) can be formulated equivalently using a set of decision trees. Classical decision trees are naturally suited to route each sample to a single leaf with a chain of hierarchical decisions. In the case of \ud835\udc58 = 1, we propose a single decision tree to route samples, where each leaf node is associated with an expert. In cases where \ud835\udc58 > 1, we instantiate \ud835\udc58 different decision trees and combine their output in a way that enforces the cardinality and simplex constraints in (1b).\nGiven that classical decision trees are not amenable to differentiable training with first-order methods, we use a variant [19] of differentiable (a.k.a. soft) decision trees [19,21,22,27,30]. We build upon this work to solve the cardinality-constrained problem (1). We first provide a summary of a single soft tree (with conditional computation support) in Section 3.2. This serves as a building block for selecting a single expert per-sample.", "publication_ref": ["b18", "b18", "b20", "b21", "b26", "b29", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries: Differential Decision Tree with Conditional Computation", "text": "In this section, we provide a brief summary of a variant [19] of a differentiable (a.k.a. soft) tree [21,24,27,29,30], which we use to enable single-expert selection in Sparse-MoEs. We extend it in the next section to solve the cardinality constrained problem for a general case \ud835\udc58 \u2265 1. Differentiable decision trees are similar to classical decision trees with hyperplane splits [38]. However, they route each sample to left and right with different proportions, i.e., each sample reaches all leaves. Traditionally, differentiable decision trees have been unamenable to conditional computation as they cannot route a sample exclusively to the left or to the right. Recent work [19] introduced a variant of the differentiable tree model that supports conditional computation. Here, we discuss a brief summary of this variant.\nWe denote a single tree by \ud835\udc63 : X \u2192 \u0394 \ud835\udc5b , which maps an input sample \ud835\udc65 \u2208 X to a probability vector \ud835\udc63 over \u0394 \ud835\udc5b . Here, \ud835\udc5b corresponds to the number of root-to-leaf paths (also equal to number of experts in the MoE paradigm). Let \ud835\udc63 be a binary tree with depth \ud835\udc51 -note our framework can naturally support cases where number of experts is non-powers of 2, see Section 3.4 for more details. Let I and L denote sets of the internal (split) nodes and the leaves of the tree, respectively. For any node \ud835\udc5e \u2208 I \u222a L, we define \ud835\udc47 (\ud835\udc5e) as its set of ancestors. Let {\ud835\udc65 \u2192 \ud835\udc5e} denote that a sample \ud835\udc65 \u2208 R \ud835\udc5d reaches \ud835\udc5e. Sample Routing. Following prior work [19,21,30], we will discuss sample routing using a probabilistic model. While sample routing is discussed using probability, differentiable trees are deterministic. Differentiable trees are based on hyperplane splits [38], where a linear combination of the features is used in making routing decisions. In particular, we assign a trainable weight vector \ud835\udc64 \ud835\udc5e \u2208 R \ud835\udc5d with each internal node, which parameterizes the node's hyperplane split. Let \u210e : R \u2192 [0, 1] be an activation function. Given a sample \ud835\udc65 \u2208 R \ud835\udc5d , the probability that internal node \ud835\udc5e routes \ud835\udc65 to the left is defined by \u210e(\ud835\udc64 \ud835\udc5e \u2022 \ud835\udc65).\nNow we summarize how to model the probability that \ud835\udc65 reaches a certain leaf \ud835\udc59 [19,21,30]. Let [\ud835\udc59 \ud835\udc5e] (resp. [\ud835\udc5e \ud835\udc59]) denote the event that leaf \ud835\udc59 belongs to the left (resp. right) subtree of node \ud835\udc5e \u2208 I. The probability that \ud835\udc65 reaches \ud835\udc59 is given by: Pr({\ud835\udc65 \u2192 \ud835\udc59 }) = \ud835\udc5e \u2208\ud835\udc47 (\ud835\udc59 ) \ud835\udc5f \ud835\udc5e,\ud835\udc59 (\ud835\udc65), where \ud835\udc5f \ud835\udc5e,\ud835\udc59 (\ud835\udc65) is the probability of node \ud835\udc5e routing \ud835\udc65 towards the subtree containing leaf \ud835\udc59, i.e., \ud835\udc5f \ud835\udc5e,\ud835\udc59 (\ud835\udc65)\n:= \u210e(\ud835\udc64 \ud835\udc5e \u2022 \ud835\udc65) 1[\ud835\udc59 \ud835\udc5e ] (1 -\u210e(\ud835\udc64 \ud835\udc5e \u2022 \ud835\udc65)) 1[\ud835\udc5e \ud835\udc59 ]\n. Note that the vector \ud835\udc63 (\ud835\udc65) given by\n\ud835\udc63 (\ud835\udc65) = [Pr({\ud835\udc65 \u2192 \ud835\udc59 1 }), \u2022 \u2022 \u2022 , Pr({\ud835\udc65 \u2192 \ud835\udc59 \ud835\udc5b })] \u2208 \u0394 \ud835\udc5b ,(2)\ndefines a per-sample probability distribution over the \ud835\udc5b leaves (or experts). Next, we discuss how the split probabilities {\u210e(\ud835\udc64 \ud835\udc5e \u2022 \ud835\udc65), 1 -\u210e(\ud835\udc64 \ud835\udc5e \u2022 \ud835\udc65)} can achieve binary state with a particular choice of activation function \u210e -this is crucial for achieving sparse expert selection (and conditional computation) in the Sparse-MoE paradigm.", "publication_ref": ["b18", "b20", "b23", "b26", "b28", "b29", "b37", "b18", "b18", "b20", "b29", "b37", "b18", "b20", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "Smooth-", "text": "Step Activation Function. The common choice for activation function \u210e in soft tree literature is a logistic function [15,21,29,30]. However, it can not perform hard routing i.e., output exact zeros. This implies that any sample \ud835\udc65 will reach every node in the tree with a positive probability, leading to a dense \ud835\udc63. [19] proposed a smooth-step activation function for a variant of soft trees -see Appendix A for details. Despite being continuously differentiable, smooth-step activation function can produce a sparse \ud835\udc63 (after an initial warm-up period of soft routing) for hard routing. This is crucial for a sparse expert selection in Sparse-MoE paradigm. Additionally, this choice of activation function also allows for (partially) conditional training with customized sparse backpropagation algorithms in soft trees (as shown in [19]), which is an important consideration for training large-scale Sparse-MoE models.\nFor cardinality-constrained Sparse-MoE learning with trees (not studied in [19]), the goal for each tree is to perform hard routing for all samples. Therefore, we add additional regularization on {\u210e(\ud835\udc64 \ud835\udc5e \u2022 \ud835\udc65), 1\u210e(\ud835\udc64 \ud835\udc5e \u2022 \ud835\udc65)} to encourage convergence of \ud835\udc63 to a onehot state (discussed in more detail in Section 3.3).", "publication_ref": ["b14", "b20", "b28", "b29", "b18", "b18", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Cardinality constraint with \ud835\udc58 trees", "text": "Next, we discuss how to achieve the cardinality constraint (\ud835\udc58 \u2265 1) in Sparse-MoE with decision trees in the presence of simplex constraint. This key ideas are given as follows:\n\u2022 We consider \ud835\udc58 decision trees, where each tree \ud835\udc57 selects a single expert via \ud835\udc63 ( \ud835\udc57 ) (\u2022) as defined in (2).\n\u2022 With the experts selected as above, we need to decide the relative weights assigned to each expert. This is done through auxiliary functions \ud835\udefc ( \ud835\udc57 ) (\u2022), where \ud835\udefc\n( \ud835\udc57 ) \ud835\udc56 is a linear function \ud835\udefd ( \ud835\udc57 ) \ud835\udc56 \u2022 \ud835\udc65 of the input. \ud835\udefc ( \ud835\udc57 ) \ud835\udc56\nreflects a linear weighting function (in the log space) for \ud835\udc56-th expert (or leaf) in \ud835\udc57-th tree. See Figure 1 as an example. Next, we define the prediction function for Sparse-MoE with \ud835\udc58 decision trees to form COMET.\nCOMET Prediction with \ud835\udc58 Out of \ud835\udc5b Experts. The prediction function for Sparse-MoE with \ud835\udc58 \u2265 1 is a weighted sum of the predictions of \ud835\udc56-th expert (or leaf) across \ud835\udc58 trees. To this end, we define the weight for \ud835\udc56-th expert as follows\n\ud835\udc54(\ud835\udc65; \ud835\udefc, \ud835\udc63) \ud835\udc56 = \ud835\udc57 \u2208 [\ud835\udc58 ] exp(\ud835\udefc ( \ud835\udc57 ) \ud835\udc56 (\ud835\udc65))\ud835\udc63 ( \ud835\udc57 ) \ud835\udc56 (\ud835\udc65) \ud835\udc57 \u2208 [\ud835\udc58 ] \ud835\udc56 \u2208 [\ud835\udc5b] exp(\ud835\udefc ( \ud835\udc57 ) \ud835\udc56 (\ud835\udc65))\ud835\udc63 ( \ud835\udc57 ) \ud835\udc56 (\ud835\udc65) ,(3)\nwhere\n\ud835\udc63 ( \ud835\udc57 )\n\ud835\udc56 (\ud835\udc65) is the probability that a sample \ud835\udc65 will reach expert \ud835\udc53 \ud835\udc56 in the \ud835\udc57-th tree. Using (3), the prediction function for Sparse-MoE with \ud835\udc58 \u2265 1 is given by \u0177 = \ud835\udc56 \u2208 [\ud835\udc5b] \ud835\udc53 \ud835\udc56 (\ud835\udc65)\ud835\udc54(\ud835\udc65; \ud835\udefc, \ud835\udc63) \ud835\udc56 .\nWe present the following proposition (proof in Appendix B):\nProposition 3.1. For any \ud835\udefc, if \ud835\udc63 ( \ud835\udc57 ) outputs a binary vector for every \ud835\udc57, the function \ud835\udc54(\ud835\udc65; \ud835\udefc, \ud835\udc63) satisfies the cardinality and simplex constraints in (1b).\nAccelerating Convergence of \ud835\udc63 ( \ud835\udc57 ) to One-Hot Encoding with Entropic Regularization. In the Sparse-MoE setup, the goal is to achieve a one-hot vector state for \ud835\udc63 ( \ud835\udc57 ) quickly -this ensures the cardinality constraint (i.e., to select at most \ud835\udc58 experts) is respected by the \ud835\udc58 trees. To encourage faster convergence towards a one-hot vector, we add a per-tree entropy regularizer, \ud835\udf06\u03a9(\ud835\udc63 ( \ud835\udc57 ) (\ud835\udc65)) to the loss objective, where \u03a9(\ud835\udc63 ( \ud835\udc57 ) \n(\ud835\udc65)) = -\ud835\udc56 \u2208 [\ud835\udc5b] \ud835\udc63 ( \ud835\udc57 ) \ud835\udc56 (\ud835\udc65) log(\ud835\udc63 ( \ud835\udc57 )\n\ud835\udc56 (\ud835\udc65)). Entropy regularizers are used in [20,37] to get binary representations .\nDense-to-Sparse Learning. COMET supports conditional training only partially. At the start of training, it uses all the available experts as \ud835\udc63 ( \ud835\udc57 ) is completely dense, so conditional training is not possible. As training proceeds, \ud835\udc63 ( \ud835\udc57 ) becomes sparser due to smoothstep activation function and entropic regularization, eventually achieving binary state. From this stage onwards, the gate satisfies the cardinality constraint per-sample, i.e, each sample gets routed to at most \ud835\udc58 experts. Hence, sparse training can proceed to refine the solution quality. Empirically, we observe that a small number of epochs are sufficient for the optimizer to reach the sparse training phase.", "publication_ref": ["b1", "b19", "b36"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Non-powers of 2", "text": "Typically, in Sparse-MoE, each expert is assigned to a separate machine for efficiency [14,58]. This may mean that the number of experts could be defined by the number of machines -machines may not necessarily be available in powers of 2. Our gate naturally handles cases where the number of experts are not chosen to be powers of 2. We propose merging child nodes at the leaf level. In such instances, we have imperfect binary decision trees (Fig. 4 in Appendix) with \ud835\udc5b nodes, with 2 \ud835\udc51 -\ud835\udc5b nodes in the (\ud835\udc51 -1)-th level, and 2\ud835\udc5b -2 \ud835\udc51 nodes in the \ud835\udc51-th level. Additional details are in Appendix C. In contrast to other differentiable gates (e.g., DSelect-k [20]), our proposed gate COMET does not require any additional regularization to encourage the simplex constraint in (1b).", "publication_ref": ["b13", "b57", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Stable numerical implementation", "text": "Next, we discuss a stable numerical implementation of COMET gate. COMET introduces additional exponential functions in the expert weights (or leaf nodes of the decision trees) -see (3). More exponential functions are known to cause instabilities in Sparse-MoE models. For example, [58] introduced router z-loss in Switch Transformers to encourage smaller logits. However, this may have a performance tradeoff. In our implementation of COMET, we can mitigate instability issues arising from additional exponential functions using the following approach: (i) convert root-to-leaf probabilities to the log-space, log \ud835\udc63\n( \ud835\udc57 ) \ud835\udc56 (\ud835\udc65), (ii) compute \ud835\udefc ( \ud835\udc57 ) \ud835\udc56 + log \ud835\udc63 ( \ud835\udc57 )\n\ud835\udc56 (\ud835\udc65), (iii) subtract the maximum, i.e., max \ud835\udc56,\ud835\udc57 (\ud835\udefc\n( \ud835\udc57 ) \ud835\udc56 + log \ud835\udc63 ( \ud835\udc57 )\n\ud835\udc56 (\ud835\udc65)) from each element, (iv) apply a two-way softmax operation to get \ud835\udc54(\ud835\udc65).", "publication_ref": ["b2", "b57"], "figure_ref": [], "table_ref": []}, {"heading": "LOCAL SEARCH", "text": "Expert selection is a challenging combinatorial problem that is known to be NP-hard. Although first-order heuristics can usually provide fast solutions, they rely heavily on initialization and are sometimes prone to arriving at low-quality solutions. To this end, we propose a permutation-based local search method that complements first-order methods in optimizing Sparse-MoEs. In both large-scale experiments and ablation studies, we see that the incorporation of local search can improve the performance of any gating method and can significantly reduce the number of tuning trials.\nOur approach derives inspiration from the local search methods commonly used along with the first-order methods to help escape local minima in sparse linear models [3,18]. We note that this is the first attempt in the literature to incorporate local search methods in the context of Sparse-MoE. Moreover, unlike common local search methods in literature, our proposed search method is differentiable. We want to highlight that our local search method is useful for any existing sparse gate, e.g., Hash routing, Top-k, and our proposed COMET. We hypothesize that our permutation-based approach can help navigate the optimization loss surface for various gates.\nThe rest of the section is organized as follows. In section 4.1, we formulate a refined cardinality-constrained Sparse-MoE objective with additional binary variables to add support for permutationbased local search. Then, in section 4.2, we provide background on permutation and its differentiable relaxation. Next in section 4.3, we outline our differentiable optimization approach for the refined Sparse-MoE objective and some additional practical considerations for computational efficiency. Later, in Section 4.4, we provide an ablation study to support our hypothesis that the local search can help escape bad initializations.", "publication_ref": ["b2", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Permutation-based Local Search", "text": "In this section, we formulate a refined objective for the cardinalityconstrained Sparse-MoE objective that adds support for permutationbased local search.\nLet us denote by S \ud835\udc5b the set of all permutations of the set [\ud835\udc5b]. Given any permutation \ud835\udf0e \u2208 S \ud835\udc5b , we permute the \ud835\udc5b experts accordingly and assign \ud835\udc56-th weight \ud835\udc54(\ud835\udc65) \ud835\udc56 to \ud835\udf0e (\ud835\udc56)-th expert instead of \ud835\udc56-th expert. With this permutation, the prediction for Sparse-MoE could be written as: \u0177 = \ud835\udc56 \u2208 [\ud835\udc5b] \ud835\udc53 \ud835\udf0e (\ud835\udc56 ) (\ud835\udc65)\ud835\udc54(\ud835\udc65) \ud835\udc56 . We note that due to symmetry between experts and weights, permuting the experts is essentially same as permuting the weights. To see this, we can write \ud835\udc56 \u2208 [\ud835\udc5b] \ud835\udc53 \ud835\udf0e (\ud835\udc56 ) (\ud835\udc65)\ud835\udc54(\ud835\udc65) \ud835\udc56 = \ud835\udc57 \u2208 [\ud835\udc5b] \ud835\udc53 \ud835\udc57 (\ud835\udc65)\ud835\udc54(\ud835\udc65) \ud835\udf0e -1 ( \ud835\udc57 ) , where \ud835\udf0e -1 is the inverse map of \ud835\udf0e, which is also a permutation.\nFor a permutation \ud835\udf0e, we can define a corresponding permutation matrix \ud835\udc77 \ud835\udf0e , by setting \ud835\udc43 \ud835\udf0e [\ud835\udc56, \ud835\udc57] = 1{\ud835\udf0e ( \ud835\udc57) = \ud835\udc56}, where 1{\u2022} is an indicator function. Then it is easy to see that\n\ud835\udc57 \u2208 [\ud835\udc5b] \ud835\udc53 \ud835\udc57 (\ud835\udc65)\ud835\udc54(\ud835\udc65) \ud835\udf0e -1 ( \ud835\udc57 ) = \ud835\udc57 \u2208 [\ud835\udc5b] \ud835\udc53 \ud835\udc57 (\ud835\udc65)(\ud835\udc77 \ud835\udf0e \ud835\udc54(\ud835\udc65)) \ud835\udc57 . The refined Sparse-MoE problem is min \ud835\udc53 1 ,\u2022\u2022\u2022 ,\ud835\udc53 \ud835\udc5b ,\ud835\udc54,\ud835\udc77 1 \ud835\udc41 (\ud835\udc65,\ud835\udc66) \u2208 D \u2113 \ud835\udc66, \ud835\udc56 \u2208 [\ud835\udc5b] \ud835\udc53 \ud835\udc56 (\ud835\udc65)(\ud835\udc77\ud835\udc54(\ud835\udc65)) \ud835\udc56 ,(4a)\ns.t. \u2225\ud835\udc54(\ud835\udc65)\u2225 0 \u2264 \ud835\udc58, \ud835\udc54(\ud835\udc65) \u2208 \u0394 \ud835\udc5b , \u2200\ud835\udc65 \u2208 X,(4b)\n\ud835\udc77 \u2208 P local \ud835\udc5b ,(4c)\nwhere P local \ud835\udc5b is a localized set of permutations in the full set of permutations, which we denote by P \ud835\udc5b . For example, one may only allow for P local \ud835\udc5b = P 2 , which only allows interchanging (swapping) two columns similar to \"swap\" operations shown to be useful in the sparse regression literature [18]. Besides optimizing the gates and experts, formulation (4) performs local search by optimizing over the permutation matrix. Specifically, the goal of local search here is to find a permutation \ud835\udc43 that leads to a better solution, i.e., one with a lower objective. Intuitively, if SGD is stuck at a lowquality solution, the permutation may be able to escape the solution by a better reordering of the experts. Standard local search, e.g., bruteforce search may be computationally expensive. Therefore, we resort to a differentiable method that can be optimized efficiently.", "publication_ref": ["b17"], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries: Permutation and a differentiable relaxation", "text": "In this section, we briefly summarize how the permutation learning problem is parameterized and later optimized. To parametrize the permutation matrix in the problem, a natural consideration is through the linear assignment problem [32]. To illustrate this, consider \ud835\udc5b people are to complete \ud835\udc5b tasks and a matrix \ud835\udc7c \u2208 R \ud835\udc5b\u00d7\ud835\udc5b \u22650 , the goal is to assign each task to one person so as to maximize the utility given that the utility of assigning task \ud835\udc57 to person \ud835\udc56 is \ud835\udc48 \ud835\udc56 \ud835\udc57 . This leads to the following optimization problem\n\ud835\udc40 (\ud835\udc7c ) = arg max \ud835\udc77 \u2208 P \ud835\udc5b \u27e8\ud835\udc77, \ud835\udc7c \u27e9 \ud835\udc39 := \ud835\udc56 \u2208 [\ud835\udc5b] \ud835\udc57 \u2208 [\ud835\udc5b]\n\ud835\udc43 \ud835\udc56 \ud835\udc57 \ud835\udc48 \ud835\udc56 \ud835\udc57 .\n(\n)5\nThe operator \ud835\udc40 here is called the Matching operator, which maps a nonnegative matrix \ud835\udc7c to a permutation matrix \ud835\udc77 . Problem ( 5) is a combinatorial optimization problem, which admits the following linear relaxation [6]:\nmax \ud835\udc69 \u2208 B \ud835\udc5b \u27e8\ud835\udc77, \ud835\udc7c \u27e9 \ud835\udc39 := \ud835\udc56 \u2208 [\ud835\udc5b] \ud835\udc57 \u2208 [\ud835\udc5b] \ud835\udc43 \ud835\udc56 \ud835\udc57 \ud835\udc48 \ud835\udc56 \ud835\udc57 ,(6)\nwhere B \ud835\udc5b denotes the set of double stochastic matrices\nB \ud835\udc5b = {\ud835\udc69 \u2208 R \ud835\udc5b\u00d7\ud835\udc5b : \ud835\udc56 \u2208 [\ud835\udc5b] \ud835\udc35 \ud835\udc56 \ud835\udc57 = 1, \ud835\udc57 \u2208 [\ud835\udc5b] \ud835\udc35 \ud835\udc56 \ud835\udc57 = 1, \ud835\udc35 \ud835\udc56 \ud835\udc57 \u2208 [0, 1]},\nwhich is a convex hull of the set of permutation matrices P \ud835\udc5b . However, this is still not a differentiable parametrization as problem (6) might end up with multiple solutions. To this end, Mena et al. [37] proposes a smooth version 2 of the permutation learning objective in (6):\n\ud835\udc46 (\ud835\udc7c /\ud835\udf0f) = arg max \ud835\udc69\u2208 B \ud835\udc5b \u27e8\ud835\udc69, \ud835\udc7c \u27e9 \ud835\udc39 -\ud835\udf0f \ud835\udc56,\ud835\udc57 \u2208 [\ud835\udc5b] \ud835\udc35 \ud835\udc56 \ud835\udc57 log \ud835\udc35 \ud835\udc56 \ud835\udc57 ,(7)\nand solves it using Sinkhorn operator \ud835\udc46 (\u2022) [1], defined by the following recursion:\n\ud835\udc46 0 (\ud835\udc7c ) = exp(\ud835\udc7c ),(8a)\n\ud835\udc46 \ud835\udc5f (\ud835\udc7c ) = T \ud835\udc50\ud835\udc5c\ud835\udc59 (T \ud835\udc5f\ud835\udc5c\ud835\udc64 (\ud835\udc46 \ud835\udc5f -1 (\ud835\udc7c ))),(8b)\n\ud835\udc46 (\ud835\udc7c ) = lim \ud835\udc5f \u2192\u221e \ud835\udc46 \ud835\udc5f (\ud835\udc7c ),(8c)\nwhere T \ud835\udc5f\ud835\udc5c\ud835\udc64 (\ud835\udc7c ) = \ud835\udc7c \u2298 (\ud835\udc7c 1 \ud835\udc5b 1 \ud835\udc47 \ud835\udc5b ), and T \ud835\udc50\ud835\udc5c\ud835\udc59 (\ud835\udc7c ) = \ud835\udc7c \u2298 (1 \ud835\udc5b 1 \ud835\udc47 \ud835\udc5b \ud835\udc7c ) are the row and column-wise normalization operators of a matrix, with \u2298 denoting the element-wise division and 1 \ud835\udc5b a column vector of ones. The sinkhorn procedure in (8) allows differentiable training with first-order methods, making it appealing as a local search method for Sparse-MoE.\nAs shown in [37], \ud835\udc40 (\ud835\udc7c ) can be obtained as lim \ud835\udf0f\u21920 + \ud835\udc46 (\ud835\udc7c /\ud835\udf0f), and thus lim \ud835\udf0f\u21920 + ,\ud835\udc5f \u2192\u221e \ud835\udc46 \ud835\udc5f (\ud835\udc7c /\ud835\udf0f). In practice, we set a max number of iterations \ud835\udc45 for normalization in (8b) as well as a small positive number \ud835\udf0f > 0, and use \ud835\udc46 \ud835\udc45 (\ud835\udc7c /\ud835\udf0f) to approximate the limit (8c). In this way, we are able to parametrize the permutation matrix \ud835\udc77 in (4) as a differentiable function \ud835\udc46 \ud835\udc45 (\ud835\udc7c /\ud835\udf0f) of learnable matrix \ud835\udc7c . However, additional considerations are needed to ensure that a hard permutation matrix can be achieved quickly in a few epochs -this is important in Sparse-MoE paradigm for computational reasons and a well-defined measure of sparsity. We discuss these in the next section.", "publication_ref": ["b31", "b5", "b36", "b5", "b0", "b7", "b36"], "figure_ref": [], "table_ref": []}, {"heading": "Practical considerations for optimization", "text": "Next, we discuss some empirical considerations for the end-to-end learning approach that are important for Sparse-MoE.\nNeed for a hard permutation matrix. We would like to have a hard permutation matrix at inference time and ideally during the course of training, for exact sparsity and computational efficiency considerations. First, the gate does not perform sparse inference if the learnt permutation matrix is not a hard matrix. For example, even if \ud835\udc54(\u2022) is sparse, the refined weights \ud835\udc77 \u2022 \ud835\udc54(\u2022) are not a sparse vector if \ud835\udc77 is not a binary matrix. This would result in a dense mixture of experts. Second, some sparse gates perform dense-to sparse-training (partially conditional training), e.g., DSelect-k, COMET, or variants of Top-k [41]. If the learnt permutation matrix is not hard, then sparse training cannot proceed in the later stages of optimization. To this end, we employ a two-stage optimization approach: (i) in the first stage, we simultaneously train the network (experts and gates) and the permutation for a small number of epochs. (ii) In the second phase, the permutation matrix is fixed and only the remaining network (experts and gate) is trained. Therefore, local search is only used in the early stages of training. Empirically, we observe that a small number of epochs (1 -10) is sufficient to learn a good permutation in the first stage and improve solution quality. Since local search is restricted to the first stage, the computational efficiency of gates that perform dense-to-sparse training is not affected by much -please refer to Appendex D.3 for additional discussion.\nIn the two-stage approach outlined above, there is a transition from a soft to a hard matrix between the two stages. As we mentioned earlier, we use \ud835\udc46 \ud835\udc45 (\ud835\udc7c /\ud835\udf0f) to approximate \ud835\udc40 (\ud835\udc7c ) as a limit of \ud835\udc45 \u2192 \u221e, \ud835\udf0f \u2192 0 + . In practice, the transition could be not continuous, as this approximation does not always reach a hard permutation matrix given that \ud835\udc45 is finite and \ud835\udf0f is nonzero. Therefore, at the transition point, we propose to convert the \"soft\" permutation matrix \ud835\udc46 \ud835\udc45 (\ud835\udc7c /\ud835\udf0f) to a hard one via the linear assignment problem given in (5), by invoking \ud835\udc7c as \ud835\udc46 \ud835\udc45 (\ud835\udc7c /\ud835\udf0f). In addition, empirically, small \ud835\udc45 can lead to numerical instabilities for small \ud835\udf0f [37]. Therefore, to decrease deviance of \ud835\udc46 \ud835\udc45 (\ud835\udc7c /\ud835\udf0f) from the closest hard permutation matrix, we introduce two schedulers on \ud835\udc45 and \ud835\udf0f that increase \ud835\udc45 for decreased \ud835\udf0f: (i) Ramp up (linearly) \ud835\udc45 from 20 to 150, (ii) Ramp down (linearly in log-scale) \ud835\udf0f from 10 -3 to 10 -7 .\nAlthough the above schedulers decrease the deviance between soft and its closest hard permutation matrix at the transition point, the method still appears to suffer from pseudo-convergence. In particular, we observed, some row-columns can converge to fractional entries i.e., a 2x2 sub-block having all entries with 0.5. Therefore, we introduce small separate row-wise and column-wise entropic regularizations to mitigate such degenerate cases: \ud835\udf01 \ud835\udc56 \u2208 [\ud835\udc5b] (\u03a9(S \ud835\udc45 (\ud835\udc7c /\ud835\udf0f) \ud835\udc56 ) + \u03a9(T \ud835\udc5f\ud835\udc5c\ud835\udc64 (S \ud835\udc45 (\ud835\udc7c /\ud835\udf0f)) \ud835\udc56 )), where \ud835\udf01 \u2265 0.\nImplicit localization. In the spirit of common local search approaches, a potential optimization approach could alternate between optimization of network (experts and gates) and permutation matrix. However, this is unnecessary because the differentiable relaxation of permutation is also amenable to first-order methods. Therefore, our approach jointly optimizes both the network and the permutation matrix. We noted earlier that the search space for permutation is \"localized\" out of the full set of permutation matrices P \ud835\udc5b . This localization is implicitly imposed through the smooth optimization of the permutation matrix via Sinkhorn. The permutation matrix learning relies on the initialization for \ud835\udc7c and at each gradient step the \ud835\udc7c (\ud835\udc61 ) is naturally expected to not deviate drastically from \ud835\udc7c (\ud835\udc61 -1) . Since the permutation matrix is updated for a limited number of steps in first stage, intuitively it cannot deviate significantly from the initial permutation matrix. This also defines an implicit neighborhood.", "publication_ref": ["b40", "b4", "b36"], "figure_ref": [], "table_ref": []}, {"heading": "Ablation study for local search", "text": "In this section, we provide an ablation study to provide evidence that the permutation-based local search can complement first-order optimization methods for routing in Sparse-MoE. The study highlights that local search can improve solution quality through escape out of bad initializations in the first stages of optimization for different types of routing strategies: (a) fixed gates, (b) trainable gates. We perform this study on a subsampled (200k) MovieLens dataset and use the same MoE architecture with 16 experts as the one described in Supplemental Section S1.2. We trained models for only 10 epochs without/with local search, where in the latter case we fixed the number of epochs for permutation learning to 5 epochs and \ud835\udf01 = 10 -5 . We used a batch size of 512 and learning rate of 2.5\u00d710 -5 . We repeat the training with 100 different random initializations and compute averages along with their standard errors.\nFixed Gates. In fixed gating strategies e.g., random hash routing (Hash-r), the samples are pre-assigned to experts. For example, in natural language processing tasks, tokens or words in vocabulary are clustered randomly [44] before training begins into groups and each group of words are assigned to a random expert in the set of experts. In our experiments on recommender systems, we randomly pre-assigned samples to experts based on user index for Hash-r (and Hash-r+). It is possible that the same group of users could be better aligned with another expert based on expert and user embedding initializations. Permutation-based local search can potentially find better assignment of each group to a more suited expert. We provide empirical evidence to demonstrate that local search indeed can find better loss. We report the average out-of-sample loss achieved by both Hash-r and Hash-r+ in Table 1. Learning permutation appears to help map each pre-assigned cluster of users to a more suitable expert based on expert initialization for second stage of optimization.\nTrainable Gates. For trainable gates, we also study the effect of local search on non-differentiable (Top-k) and differentiable gates (COMET ). We fixed \ud835\udc58 = 2 for both types of gates and followed the same training protocol for 10 epochs. For COMET (and COMET+), we fixed \ud835\udefe = 0.01 (for smooth-step) and \ud835\udf06 = 1 (for entropic regularization). For Top-k+and COMET+, we fixed the number of epochs for permutation learning as 5. We repeated this exercise for 100 different random initializations of the experts and gates. We report the average out-of-sample objective achieved by both types of gates in Table 1. We can observe that local search appears to complement first-order optimization methods by learning better initializations in the first stage of Sparse-MoE optimization for later learning.\nThe practical significance of local search achieving a better test objective across many initializations for various gates can be seen in terms of reducing hyperparameter tuning overhead as discussed in Section 5.1.3.", "publication_ref": ["b43"], "figure_ref": [], "table_ref": ["tab_1", "tab_1"]}, {"heading": "EXPERIMENTS", "text": "We study the performance of COMET and COMET+ in recommender systems and image datasets in Section 5.1 and COMET-BERT in natural language processing tasks in 5.2. We also study the effect of local search for various gates. We denote our methods in italics.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments on Recommender Systems and Image Datasets", "text": "We study the performance of COMET and COMET+ in recommender systems and image datasets. We compare with state-of-the-art gates and baselines including Softmax, Top-k, DSelect-k and Hash routing (Hash-r) on recommender systems (MovieLens [17], Jester [16], Books [57]) and image datasets (Digits [10,40], MultiMNIST [46],\nMultiFashionMNIST [20], CelebA [35]). We also include an ablation study in Section 5.1.2 that shows that COMET achieves good performance with much less trials than existing popular gates e.g., Hash routing and Top-k. Additionally, in Section 5.1.3, we show that Hash-r+,Top-k+, and COMET+ with local search can potentially achieve good performance with much less trials than Hash-r, Top-k and COMET respectively.", "publication_ref": ["b16", "b15", "b56", "b9", "b39", "b45", "b19", "b34"], "figure_ref": [], "table_ref": []}, {"heading": "Implementation.", "text": "We provide an open-source implementation of COMET and COMET+: https://github.com/mazumder-lab/COMET. Experimental Setup. Although our exposition in Section 3 was for a single-task setting, the same gate can also be used in multi-task learning -multi-task requires multi-gate MoE architecture [36],\nwhere each task has a separate trainable gate, but tasks have to select from a common set of experts. We briefly summarize the key aspects for each dataset. For MovieLens/Books/Jester we have two tasks: classification task predicts whether user watches/read/rates a particular movie/book/joke, regression problem predicts user's rating. Loss is the convex combination of the two binary crossentropy (for classification) and mean squared error (for regression) with task weights: {\ud835\udefc, 1 -\ud835\udefc }. We separately present results for two different \ud835\udefc's: \ud835\udefc \u2208 {0.1, 0.9}. For MultiMNIST/MultiFashionMNIST, there are two multi-class classification tasks, which are equally weighted. For CelebA, there are 10 binary classification problems, which are equally weighted. Lastly, for Digits dataset, we have a multi-class single-task classification cross-entropy objective. Full details about datasets and MoE architectures are in Supplement Section S1. We used Adam for optimization, and we tuned the key hyperparameters using random grid search. Note that for Hash-r+, COMET+ and Top-k+, we only allocate a very small portion of the epochs (1-10) for permutation learning. Full details about the hyperparameter tuning are given in Supplement Section S1.", "publication_ref": ["b35"], "figure_ref": [], "table_ref": []}, {"heading": "Performance of COMET and COMET+.", "text": "In Tables 2 and3, we report the (average) test loss and the average number of selected experts per sample across multiple recommender and vision datasets. The results indicate that COMET and COMET+ lead on many datasets, outperforming popular state-of-the-art gating methods e.g., Hash-r, Top-k and DSelect-k in test loss. Our proposed gate COMET can outperform standard routing techniques (without Table 3: Tess Loss (\u00d710 -2 , the smaller the better) and number of experts per sample (\ud835\udc5b/\ud835\udc60) for COMET, COMET+ and benchmarks gates across various image datasets. Asterisk(*) indicates statistical significance (p-value<0.05) over the best existing gate, using a one-sided unpaired t-test.  7 in Appendix E. We observe COMET+ can improve AUC by up to 13% over Hash routing and Top-k, and 9% over DSelect-k. We observe that Top-k gate does not uniformly outperform the Softmax across multiple datasets. However, Top-k+ significantly improves the performance of Top-k across multiple datasets. In fact with the permutation module, Top-k+ outperforms Softmax in all cases, so sparsity in gating seems to be beneficial on all these datasets.\nInference Sparsity. We see that COMET and COMET+ can sometimes lead to a smaller number of experts selected than that for Top-k. This leads to smaller number of FLOPs at inference time (see Appendix D.2). For some settings, DSelect-k appears to arrive at a sparser selection than COMET+; however, in these cases, DSelect-k loses significantly in terms of performance. We observed expert collapsing in DSelect-k in such cases.\nTiming Discussion. For cost complexity of COMET, please see Appendix D.1. Additionally, we discuss the computational aspects of the local search in Appendix D.3.  hyperparameter trials. This indicates that COMET is not too heavily dependent on a very restricted set of hyperparameter values. We visualize this for various datasets in Fig. 2. We see tuning reduction by a factor of 5\u00d7-100\u00d7 for COMET over popular gates.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": ["tab_2", "tab_8"]}, {"heading": "Reducing Hyperparameter", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Effect of Local Search on Hyperparameter", "text": "Tuning. Here, we study how local search can be beneficial in terms of hyperparameter tuning. We study this effect for Hash-r, Top-k and COMET. We visualize this in Fig. 3 for MovieLens for both Hash-r+, Top-k+and COMET+. We observe that we can achieve comparable performance with much smaller number of trials. We see tuning reduction by a factor of 3\u00d7-100\u00d7 for Hash-r+, 20\u00d7-100\u00d7 for Top-k+ and 2\u00d7-5\u00d7 for COMET+. This suggests that permutation-based local search helps escape out of bad initializations. Such favorable properties of local search in terms of reducing the hyperparameter tuning load for existing gates can be beneficial for Large Language Models.", "publication_ref": [], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Experiments on NLP Tasks", "text": "In this section, we consider a setting where a pretrained large model (non-MoE based) needs to be distilled for a more efficient inference while preserving or improving the best performance. Following [60], we study a distillation setting, where BERT [11] is distilled into its Sparse-MoE based variant. Specifically, the FFN layers are replaced with MoE layers -this can result in a \u223c2\u00d7 smaller number of (effective) parameters with per-sample sparse routing (for \ud835\udc58 = 1), thus allowing for more efficient inference.\nMovieLens (\ud835\udefc = 0.1)\nMovieLens (\ud835\udefc = 0.9)  Following [60], we use an importance-weight guided distillation strategy: (i) Finetune BERT on a downstream task. (ii) Compute importance weights in FFN layers to construct an MoE-based variant of BERT. (iii) Distill BERT into MoE-based variant on the downstream task with a layer-wise discrepancy loss. [60] used Hash routing in their MoEBERT model. We propose COMET-BERT (MoE based BERT model with COMET / COMET+ gating) and evaluate the performance on the GLUE benchmarks [49] and SQuAD benchmark [42]. More details about the benchmarks are given in Supplement Section S2.1.\n0\nImplementation. We implemented COMET-BERT in HuggingFace [54] and adapted the codebase of [60]. Unlike Hash routing, our gates can also cater to \ud835\udc58 \u2265 1. However, for consistent comparison in terms of inference, we set \ud835\udc58 = 1. Tuning details are outlined in Supplement Section S2.2. Code for COMET-BERT is available at https://github.com/mazumder-lab/COMET-BERT.\nResults. We report the performance metrics in Table 4 for 7 GLUE datasets and SQuAD dataset. COMET-BERT outperforms MoEBERT in 5/7 benchmarks on GLUE datasets. COMET-BERT also outperform MoEBERT significantly on SQuADv2.0. Notably, in 5 of these datasets (CoLA, MRPC, QNLI and MNLI, SQuAD v2.0), COMET-BERT achieves SOTA performance when distilling BERT, (when compared with all distillation methods in literature with same number of effective parameters for inference). We show that gates that learn sparse routing decisions per sample, e.g., Top-k, DSelect-k, COMET, significantly reduce the number of FLOPs (3\u00d7-6\u00d7) at inference time in comparison to dense gates e.g., Softmax. Additionally, we see that in all 4 cases, COMET has smaller number of FLOPs (1.1\u00d7-1.6\u00d7) than the highly popular Topk gate. We also outperform DSelect-k in some cases in number of FLOPs. While in some cases, we have larger number of FLOPs than DSelect-k, our AUC is higher (up to 9%) in these cases.", "publication_ref": ["b59", "b10", "b59", "b59", "b48", "b41", "b53", "b59"], "figure_ref": [], "table_ref": ["tab_6"]}, {"heading": "D.3 Effect of local search on computation", "text": "Inference. Note that the permutation matrix is global and not sample specific. At inference time, multiplying permutation matrix \ud835\udc77 with \ud835\udc54(\ud835\udc65) amounts to a reordering of the expert indiceshence, additional cost for this permutation is negligible compared to evaluation of \ud835\udc53 (\ud835\udc65) and \ud835\udc54(\ud835\udc65).\nTraining. In the first stage of COMET training (a few epochs \u223c 5), the training is dense (requiring all experts per sample). For COMET+, we also learn the permutation matrix during this stage. There is a small additional computational cost: (a) permutation matrix of size \ud835\udc5b \u00d7\ud835\udc5b, where \ud835\udc5b is the number of experts, e.g., 16; (b) cost of Sinkhorn operator which constitutes row/column sum normalizations. This cost is marginal compared to the cost of evaluating the experts \ud835\udc53 \u2032 \ud835\udc56 \ud835\udc60, each of which is an MLP/CNN. In the second stage of training, where the samples are being routed to a small \ud835\udc58 (= 2) subset of experts per-sample, there is no additional cost for COMET vs COMET+. To show an example, for MovieLens 200k, where we learn permutation matrix in first 5 epochs, the total time for 50 epochs (on 4 GPUs) is given by: 494s for COMET and 496s for COMET+. Note 50 epochs were sufficient to achieve convergence for both gates.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E TASK-SPECIFIC METRICS CORRESPONDING TO TABLES 2 AND 3", "text": "We provide task-specific metrics for all recommender systems and image datasets in Table 7. We observe COMET+ can give superior AUC performance by up to 13% over Hash routing and Top-k, and 9% over DSelect-k. SQuAD. We evaluate our sparse routing approaches on question answering dataset: SQuAD v2.0 [42]. This task is treated as a sequence labeling problem, where we predict the probability of each token being the start and end of the answer span. Statistics of the question answering dataset (SQuAD v2.0) are summarized in Table S2. Following [60], we followed the 3-step process as outlined in the MoEBERT codebasefoot_2 :\n\u2022 We finetuned BERT on each downstream task for a set of 50 random hyperparameter trials over the following set:\n-Learning Rate: Discrete uniform over the set {1 \u00d7 10 -5 , 2 \u00d7 10 -5 , 3 \u00d7 10 -5 , 5 \u00d7 10 -5 } -Batch size: Discrete uniform over the set {8, 16, 32, 64} -Weight Decay: Discrete uniform over the set {0, 0.01, 0.1} -Epochs: 10 Note that this step matched the performance numbers reported for BERT-base in Table 1 of [60]. We used the best model (for each dataset) for the remaining steps below. \u2022 Compute importance weights in FFN layers to construct an MoEBERT/COMET-BERT model, where FFN layers are replaced with MoE layers with the weight assignment strategy in [60]. \u2022 Distill BERT into MoEBERT or COMET-BERT on the downstream task with a layer-wise discrepancy loss. For MoEBERT, we used the optimal hyperparameters reported (based on \u223c 1000 trials per dataset) in Table 7 of Supplement in [60]. For COMET-BERT, we performed 100 tuning trials via random search with each COMET and COMET+ and picked the best results based on development datasets. The hyperparameters were randomly selected from the following sets:\n-Learning Rate: Discrete uniform over the set {1 \u00d7 10 -5 , 2 \u00d7 10 -5 , 3 \u00d7 10 -5 , 5 \u00d7 10 -5 } -Batch size: Discrete uniform over the set {8, 16, 32, 64} -Weight Decay: Discrete uniform over the set {0, 0.01, 0.1} -Distillation Regularization (\ud835\udf06 \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc56\ud835\udc59\ud835\udc59 in [60]): Discrete uniform over the set {1, 2, 3, 5}.\n-\ud835\udefe (for smooth-step for COMET ): Discrete uniform over the set {0.01, 0.1, 1.0}.\n-\ud835\udf06 (for entropy regularization for COMET ): Discrete uniform over the set {0.05, 0.1, 0.5, 1, 5, 10}.\n-Epochs: 50 for small datasets (CoLA, RTE, MRPC) and 25 for large datasets (SST-2, MNLI, QQP, QNLI, SQuADv2.0). Best model was recovered on development set on best checkpoint.", "publication_ref": ["b41", "b59", "b59", "b59", "b59", "b59"], "figure_ref": [], "table_ref": ["tab_8", "tab_10", "tab_1", "tab_8"]}, {"heading": "APPENDIX A SMOOTH-STEP ACTIVATION FUNCTION", "text": "In smooth routing, the internal nodes of a soft tree use an activation function \u210e in order to compute the routing probabilities. The common choice for \u210e -logistic (a.k.a. sigmoid) function in soft trees literature [15,21,29,30] -can not output exact zeros. This implies that any sample \ud835\udc65 will reach every node in the tree with a positive probability. Thus, computing the output of mixture of experts will require computation over every expert. The smoothstep activation function proposed in [19] can output exact zeros and ones, thus allowing for conditional computation. Let \ud835\udefe be a non-negative scalar parameter. The smooth-step function is:\nThe smooth-step function is continuously differentiable, similar to the logistic function. Additionally, it performs hard routing, i.e., outside [-\ud835\udefe/2, \ud835\udefe/2], the function produces exact zeros and ones. For cardinality-obeying Sparse-MoE learning with trees (not studied in [19]), the goal for each tree is to perform hard routing for all samples. Therefore, we propose additional regularization on {\u210e(\ud835\udc64 \ud835\udc5e \u2022 \ud835\udc65), 1\u210e(\ud835\udc64 \ud835\udc5e \u2022 \ud835\udc65)} to encourage convergence of \ud835\udc63 to a one-hot state (discussed in more detail in Section 3.3).", "publication_ref": ["b14", "b20", "b28", "b29", "b18", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "B PROOF FOR PROPOSITION 3.1", "text": "Proof. First off, it is straightforward to see that \ud835\udc54(\ud835\udc65; \ud835\udefc, \ud835\udc63) satisfies the simplex constraint in (1b): It remains to show that \u2225\ud835\udc54(\ud835\udc65; \ud835\udefc, \ud835\udc63)\u2225 0 \u2264 \ud835\udc58 under the given conditions. Recall the hierarchical binary encoding \ud835\udc63 ( \ud835\udc57 ) outputs a one-hot vector for each sample \ud835\udc65 as the routing decision. Let us denote by \u00ee \ud835\udc57 the expert number selected by \ud835\udc57-th tree. For now, let us assume that \u00ee \ud835\udc57 are different for any \ud835\udc57. Then, we have\ni.e., the weights are restricted on the \ud835\udc58 experts selected by the \ud835\udc58 trees, and the weights form a softmax activation of logits \ud835\udefc\nGiven that the \ud835\udc57-th tree selects \u00ee \ud835\udc57 -th expert, we know that if \ud835\udc56 \u2209 { \u00ee1 , \u00ee2 , . . . , \u00ee\ud835\udc58 }, \ud835\udc63 ( \ud835\udc57 ) \ud835\udc56 = 0 for any \ud835\udc57, and thus \ud835\udc54(\ud835\udc65; \ud835\udefc, \ud835\udc63) \ud835\udc56 = 0, and this means that the support of \ud835\udc54(\ud835\udc65; \ud835\udefc, \ud835\udc63) is contained in the set { \u00ee1 , \u00ee2 , . . . , \u00ee\ud835\udc58 }, which has at most \ud835\udc58 elements. Therefore, the cardinality constraint in (1b) holds. \u25a1", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C COMET FOR NON-POWERS OF 2", "text": "COMET has a natural way to cater to settings where number of experts are non-powers of 2. Recall we have \ud835\udc5b experts. Let \ud835\udc51 = \u2308log 2 \ud835\udc5b\u2309, then 2 \ud835\udc51 -1 < \ud835\udc5b \u2264 2 \ud835\udc51 . Let T be a full binary tree with depth \ud835\udc51 and 2 \ud835\udc51 leaf nodes. We collapse 2(2 \ud835\udc51 -\ud835\udc5b) leaf nodes to their parents in the (\ud835\udc51 -1)-th level. Each time we collapse two leaf nodes, we get a new node in the (\ud835\udc51 -1)-th level, and the total number of nodes reduce by one. Therefore, we get a tree with \ud835\udc5b nodes, with 2 \ud835\udc51 -\ud835\udc5b nodes in the (\ud835\udc51 -1)-th level, and 2\ud835\udc5b -2 \ud835\udc51 nodes in the \ud835\udc51-th level. This is visualized in Figure 4 for number of experts equal to 5.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D TIMING DISCUSSION D.1 Cost Complexity", "text": "We compare the cost of forward pass COMET vs Top-k. We consider a single-task, \ud835\udc5b experts (\ud835\udc56th expert is \ud835\udc53 \ud835\udc56 ), \ud835\udc5d features, desired sparsity \ud835\udc58 \u226a \ud835\udc5b, and some shared layers. Cost for COMET. For COMET, the cost of the gate is \ud835\udc42 (\ud835\udc58\ud835\udc5d\ud835\udc5b). The cost of the full model is \ud835\udc58 \ud835\udc4e O (\ud835\udc53 \ud835\udc56 ) +\ud835\udc42 (\ud835\udc58\ud835\udc5d\ud835\udc5b) + O (shared layers). After a very few epochs (e.g., 4-5), when COMET has reached the desired sparsity, we have \ud835\udc58 \ud835\udc4e \u2264 \ud835\udc58.\nNote that during inference, COMET has smaller cost: All rootto-leaf paths in COMET do not require evaluation, hence the gate cost reduces from \ud835\udc42 (\ud835\udc58\ud835\udc5d\ud835\udc5b) to \ud835\udc42 (\ud835\udc58\ud835\udc5d log(\ud835\udc5b)). Hence, due to fewer expert evaluations and smaller gate cost, COMET is more efficient at inference time than Top-k. Also see Table 6 for FLOP counts at inference time for different gating methods.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.2 FLOPs Comparison", "text": "We compare the FLOP counts of different gating methods (across different models/datasets) to compare inference speed-see Table 6 below. Inference FLOPs per sample -number of floating point", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "SUPPLEMENTARY MATERIAL FOR \"COMET: LEARNING CARDINALITY CONSTRAINED MIXTURE OF EXPERTS WITH TREES AND LOCAL SEARCH\" S1 ADDITIONAL DETAILS FOR SECTION 5.1 S1.1 Datasets", "text": "MovieLens. MovieLens [17] is a movie recommendation dataset containing records for \u223c 4, 000 movies and \u223c 6, 000 users. Following [51], for every user-movie pair, we construct two tasks. Task 1 is a binary classification problem for predicting whether the user will watch a particular movie. Task 2 is a regression problem to predict the user's rating (in {1, 2, \u2022 \u2022 \u2022 , 5}) for a given movie. We use 1.6 million samples for training and 200, 000 for each of the validation and testing sets.\nJester. Jester [16] is a joke recommendation dataset containing records for \u223c 74\ud835\udc58 users and \u223c 100 jokes. This gives a dataset of 7.4 million records. Similar to MovieLens above, for every user-joke pair, we construct two tasks. Task 1 is a binary classification problem for predicting whether the user will rate a particular joke. Task 2 is a regression problem to predict the user's rating (in [-10, 10]) for a given joke. We use 5.1 million samples for training and 1.1 million samples for each of the validation and testing sets.\nBooks. Books [57] is a book recommendation dataset containing records for \u223c 105\ud835\udc58 users and \u223c 340\ud835\udc58 books. We filter users and books with each atleast 5 records. This gives a subset of 18, 960 users and 31, 070 books. This gives a subset of 556,724 records. Similar to MovieLens above, for every user-book pair, we construct two tasks. Task 1 is a binary classification problem for predicting whether the user will read a particular book. Task 2 is a regression problem to predict the user's rating (in {1, 2, \u2022 \u2022 \u2022 , 10}) for a given book. We use 389,706 samples for training and 83,509 for each of the validation and testing sets.\nDigits. We use a mixture of MNIST [10] and SVHN [40] datasets. MNIST is a database of 70, 000 handwritten digits. SVHN is a much harder dataset of \u223c 600, 000 images obtained from house numbers in Google Street View images. We divided the dataset into training, validation and testing as follows: MNIST (#train: 50,000, #validation: 10,000, #test: 10,000) and SVHN (#train: 480,420, #validation: 75,000, #test: 75,000). We combined the corresponding splits to get the train, validation and test sets for the mixture.\nMultiMNIST/MultiFashionMNIST. We consider multi-task variants of MNIST/MultiFashionMNIST [10]. The datasets are constructed in a similar fashion as given in [20,46]: (i) uniformly sample two images from MNIST and overlay them on top of each other, and (ii) shift one digit towards the top-left corner and the other digit towards the bottom-right corner (by 4 pixels in each direction). This procedure leads to 36 \u00d7 36 images with some overlap between the digits. We consider two classification tasks: Task 1 is to classify the top-left item and Task 2 is to classify the bottom-right item. We use 100,000 samples for training, and 20, 000 samples for each of the validation and testing sets.\nCelebA. CelebA [35] is a large-scale face attributes dataset with more than 200, 000 celebrity images, each with 40 attribute annotations. The images in this dataset cover large pose variations and background clutter. We consider 10 of the face attributes in a multi-task learning setting. We use \u223c 160, 000 images for training, and \u223c 20, 000 for each of validation and testing.", "publication_ref": ["b16", "b50", "b15", "b56", "b9", "b39", "b9", "b19", "b45", "b34"], "figure_ref": [], "table_ref": []}, {"heading": "S1.2 Architectures", "text": "MovieLens. We consider a multi-gate MoE architecture, where each task is associated with a separate gate. The MoE architecture consists of a shared bottom subnetwork comprising two embedding layers (for users and movies). The 128-dimensional embeddings from both layers are concatenated and fed into an MoE Layer of 16 experts, where each expert is a ReLU-activated dense layer with 256 units, followed by a dropout layer (with a dropout rate of 0.5). For each of the two tasks, the corresponding convex combination of the experts is fed into a task-specific subnetwork. The subnetwork is composed of a dense layer (ReLU-activated with 256 units) followed by a single unit that generates the final output of the task.\nBooks/Jester. We consider a multi-gate MoE architecture, where each task is associated with a separate gate. The MoE architecture consists of a shared bottom subnetwork comprising two embedding layers (for users and books/jokes). The 64-dimensional embeddings from both layers are concatenated and fed into an MoE Layer of 9/16 experts, where each expert is a ReLU-activated dense layer with 128 units, followed by a dropout layer (with a dropout rate of 0.5). For each of the two tasks, the corresponding convex combination of the experts is fed into a task-specific subnetwork. The subnetwork is composed of a dense layer (ReLU-activated with 256 units) followed by a single unit that generates the final output of the task. subnetwork specific to each of the 2 tasks is composed of a stack of 3 dense layers: the first two have 50 ReLU-activated units and the third has 10 units followed by a softmax.\nCelebA. We use a multi-gate MoE with 6 experts. Each of the experts is a CNN that is composed (in order) of: (i) convolutional layer 1 (kernel size = 3, #filters = 4, ReLU-activated) followed by max pooling, (ii) convolutional layer 2 (kernel size=3, #filters = 4, ReLU-activated) followed by max pooling, (iii) convolutional layer 3 (kernel size=3, #filters = 4, ReLU-activated) followed by max pooling, and (iv) convolutional layer 4 (kernel size=3, #filters = 1, ReLU-activated) followed by max pooling, and (v) flatten layer. The subnetwork specific to each of the 2 tasks is composed of a dense layer followed by a sigmoid.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "S1.3 Hyperparameters and Tuning", "text": "We performed 500 tuning trials for each gate with a random search over the hyperparameter space described below (for each dataset). For each gate, we selected Top 5% of the trials based on validation loss. We report the (average) test loss for the Top 5% trials along with the standard errors in Tables 2 and3.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "MovieLens.", "text": "\u2022 Learning Rates: Uniform in the log range [5 \u00d7 10 -5 , 5 \u00d7 10 -4 ] for Adam.\n\u2022 Batch-size: 512.\n\u2022 Epochs: 100 with early stopping (patience=25) based on validation set.\n\u2022 \ud835\udefe: Discrete uniform in the set {0.01, 0.1, 1, 5, 10} for DSelect-k and COMET. \ud835\udefe is fixed to 10 for COMET+.\n\u2022 Entropy: Discrete uniform in the set {0.05, 0.1, 0.5, 1, 5, 10} for DSelect-k and COMET and COMET+.\n\u2022 Number of epochs for permutation learning: Discrete uniform in the set {1, \u2022 \u2022 \u2022 , 10} for COMET+ and Top-k+.\n\u2022 \ud835\udf01 (for permutation): 10 -4\n\u2022 \ud835\udc5b (number of experts): 16.\n\u2022 \ud835\udc58: 2 for all sparse (trainable) gates.\n\u2022 For Hash-r (and Hash-r+), users are randomly pre-allocated to experts (similar to how words in vocabulary are pre-allocated randomly in LLMs) \u2022 Number of tuning trials per gate: 500 Books.\n\u2022 Learning Rates: Uniform in the log range [5 \u00d7 10 -5 , 5 \u00d7 10 -4 ] for Adam.\n\u2022 Batch-size: 2048.\n\u2022 Epochs: 100 with early stopping (patience=25) based on validation set.\n\u2022 \ud835\udefe: Discrete uniform in the set {0.1, 0.5, 1, 5, 10} for DSelect-k and COMET. \ud835\udefe is fixed to 0.5 for COMET+.\n\u2022 Entropy: Discrete uniform in the set {1, 5, 10, 50, 100} for DSelect-k and COMET and COMET+.\n\u2022 Number of epochs for permutation learning: Discrete uniform in the set {1, \u2022 \u2022 \u2022 , 10} for COMET+ and Top-k+.\n\u2022 \ud835\udf01 (for permutation): 10 -4\n\u2022 \ud835\udc5b (number of experts): 9.\n\u2022 \ud835\udc58: 4 for all sparse (trainable) gates.\n\u2022 For Hash-r (and Hash-r+), users are randomly pre-allocated to experts (similar to how words in vocabulary are pre-allocated randomly in LLMs) \u2022 Number of tuning trials per gate: 500\nJester.\n\u2022 Learning Rates: Uniform in the log range [5 \u00d7 10 -5 , 5 \u00d7 10 -4 ] for Adam.\n\u2022 Batch-size: 2048.\n\u2022 Epochs: 100 with early stopping (patience=25) based on validation set. \u2022 \ud835\udc5b (number of experts): 16.\n\u2022 \ud835\udc58: 2 for all sparse (trainable) gates.\n\u2022 For Hash-r (and Hash-r+), users are randomly pre-allocated to experts (similar to how words in vocabulary are pre-allocated randomly in LLMs) \u2022 Number of tuning trials per gate: 500 Digits.\n\u2022 Learning Rates: Uniform in the log range [1 \u00d7 10 -5 , 5 \u00d7 10 -4 ] for Adam.\n\u2022 Batch-size: 512.\n\u2022 Epochs: 200 with early stopping (patience=25) based on validation set.\n\u2022 \ud835\udefe: Discrete uniform in the set {0.001, 0.01, 0. \u2022 \ud835\udc5b (number of experts): 6.\n\u2022 \ud835\udc58: 2 for all sparse (trainable) gates.\n\u2022 Number of tuning trials per gate: 500\nCelebA.\n\u2022 Learning Rates: Uniform in the log range [1 \u00d7 10 -4 , 1 \u00d7 10 -3 ] for Adam.\n\u2022 Batch-size: 512.\n\u2022 Epochs: 100 with early stopping (patience=25) based on validation set. GLUE. General Language Understanding Evaluation (GLUE) benchmark [49], is a collection of natural language understanding tasks. Following previous works on model distillation, we consider SST-2 [48], CoLA [52], MRPC [12], STSB [7], QQP, and MNLI [53] and exclude STS-B [7] and WNLI [33] in the experiments. The datasets are briefly summarized below:\n\u2022 SST-2 [48] is a binary single-sentence classification task that classifies movie reviews to positive or negative;\n\u2022 CoLA [52] is a linguistic acceptability task;\n\u2022 MRPC [12] is a paraphrase detection task;\n\u2022 QQP is a duplication detection task;\n\u2022 MNLI [53], QNLI [43], and RTE [9] are natural language inference tasks. Dataset details are summarized in Table S1.", "publication_ref": ["b48", "b47", "b51", "b11", "b6", "b52", "b6", "b32", "b47", "b51", "b11", "b52", "b42", "b8"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Ranking via Sinkhorn Propagation", "journal": "", "year": "2011", "authors": "Ryan Prescott; Adams ; Richard S Zemel"}, {"ref_id": "b1", "title": "Efficient Large Scale Language Modeling with Mixtures of Experts", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Mikel Artetxe; Shruti Bhosale; Naman Goyal"}, {"ref_id": "b2", "title": "Sparsity Constrained Nonlinear Optimization: Optimality Conditions and Algorithms", "journal": "SIAM Journal on Optimization", "year": "2013", "authors": "Amir Beck; Yonina C Eldar"}, {"ref_id": "b3", "title": "Conditional Computation in Neural Networks for faster models", "journal": "", "year": "2016", "authors": "Emmanuel Bengio; Pierre-Luc Bacon; Joelle Pineau; Doina Precup"}, {"ref_id": "b4", "title": "Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation", "journal": "", "year": "2013", "authors": "Yoshua Bengio; Nicholas L\u00e9onard; Aaron C Courville"}, {"ref_id": "b5", "title": "Introduction to linear optimization", "journal": "Athena Scientific", "year": "1997", "authors": "Dimitris Bertsimas; John N Tsitsiklis"}, {"ref_id": "b6", "title": "SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Daniel Cer; Mona Diab; Eneko Agirre; I\u00f1igo Lopez-Gazpio; Lucia Specia"}, {"ref_id": "b7", "title": "Unified Scaling Laws for Routed Language Models", "journal": "PMLR", "year": "2022", "authors": "Aidan Clark; Diego De; Las Casas; Aurelia Guy; Arthur Mensch; Michela Paganini; Jordan Hoffmann; Bogdan Damoc; Blake Hechtman; Trevor Cai; Sebastian Borgeaud; George Bm Van Den Driessche; Eliza Rutherford; Tom Hennigan; Matthew J Johnson; Albin Cassirer; Chris Jones; Elena Buchatskaya; David Budden; Laurent Sifre; Simon Osindero; Oriol Vinyals; Marc'aurelio Ranzato; Jack Rae; Erich Elsen; Koray Kavukcuoglu; Karen Simonyan"}, {"ref_id": "b8", "title": "The PASCAL Recognising Textual Entailment Challenge", "journal": "Springer", "year": "2006", "authors": "Ido Dagan; Oren Glickman; Bernardo Magnini"}, {"ref_id": "b9", "title": "The mnist database of handwritten digit images for machine learning research", "journal": "IEEE Signal Processing Magazine", "year": "2012", "authors": "Li Deng"}, {"ref_id": "b10", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "journal": "", "year": "2018", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b11", "title": "Automatically Constructing a Corpus of Sentential Paraphrases", "journal": "", "year": "2005", "authors": "William B Dolan; Chris Brockett"}, {"ref_id": "b12", "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts", "journal": "PMLR", "year": "2022", "authors": "Nan Du; Yanping Huang; Andrew M Dai; Simon Tong; Dmitry Lepikhin; Yuanzhong Xu; Maxim Krikun; Yanqi Zhou; Adams Wei Yu; Orhan Firat; Barret Zoph; Liam Fedus; P Maarten; Zongwei Bosma; Tao Zhou; Emma Wang; Kellie Wang; Marie Webster; Kevin Pellat; Kathleen Robinson; Toju Meier-Hellstern; Lucas Duke; Kun Dixon; Quoc Zhang; Yonghui Le; Zhifeng Wu; Claire Chen;  Cui"}, {"ref_id": "b13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity", "journal": "Journal of Machine Learning Research", "year": "2022", "authors": "William Fedus; Barret Zoph; Noam Shazeer"}, {"ref_id": "b14", "title": "Distilling a Neural Network Into a Soft Decision Tree", "journal": "", "year": "2017", "authors": "Nicholas Frosst; Geoffrey Hinton"}, {"ref_id": "b15", "title": "Eigentaste: A Constant Time Collaborative Filtering Algorithm", "journal": "Information Retrieval", "year": "2001", "authors": "Ken Goldberg; Theresa Roeder; Dhruv Gupta; Chris Perkins"}, {"ref_id": "b16", "title": "The MovieLens Datasets: History and Context", "journal": "ACM Trans. Interact. Intell. Syst", "year": "2015-12", "authors": "F ; Maxwell Harper; Joseph A Konstan"}, {"ref_id": "b17", "title": "Fast Best Subset Selection: Coordinate Descent and Local Combinatorial Optimization Algorithms", "journal": "Oper. Res", "year": "2020-09", "authors": "Hussein Hazimeh; Rahul Mazumder"}, {"ref_id": "b18", "title": "The tree ensemble layer: Differentiability meets conditional computation", "journal": "PMLR", "year": "2020", "authors": "Hussein Hazimeh; Natalia Ponomareva; Petros Mol; Zhenyu Tan; Rahul Mazumder"}, {"ref_id": "b19", "title": "DSelectk: Differentiable Selection in the Mixture of Experts with Applications to Multi-Task Learning", "journal": "", "year": "2021", "authors": "Hussein Hazimeh; Zhe Zhao; Aakanksha Chowdhery; Maheswaran Sathiamoorthy; Yihua Chen; Rahul Mazumder; Lichan Hong; Ed Chi"}, {"ref_id": "b20", "title": "End-to-End Learning of Decision Trees and Forests", "journal": "International Journal of Computer Vision", "year": "2019", "authors": "M Thomas; Julian F P Hehn; Fred A Kooij;  Hamprecht"}, {"ref_id": "b21", "title": "Flexible Modeling and Multitask Learning Using Differentiable Tree Ensembles", "journal": "Association for Computing Machinery", "year": "2022", "authors": "Shibal Ibrahim; Hussein Hazimeh; Rahul Mazumder"}, {"ref_id": "b22", "title": "Decision Forests, Convolutional Networks and the Models in-Between", "journal": "", "year": "2016", "authors": "Yani Ioannou; Duncan P Robertson; Darko Zikic; Peter Kontschieder; Jamie Shotton; Matthew Brown; Antonio Criminisi"}, {"ref_id": "b23", "title": "Soft decision trees", "journal": "", "year": "2012", "authors": "O T Ozan Irsoy; Ethem Yildiz;  Alpaydin"}, {"ref_id": "b24", "title": "Interpretable Mixture of Experts", "journal": "Transactions on Machine Learning Research", "year": "2023", "authors": "Aya Abdelsalam Ismail; Sercan O Arik; Jinsung Yoon; Ankur Taly; Soheil Feizi; Tomas Pfister"}, {"ref_id": "b25", "title": "Bias/Variance Analyses of Mixtures-of-Experts Architectures", "journal": "Neural Computation", "year": "1997", "authors": "Robert A Jacobs"}, {"ref_id": "b26", "title": "Adaptive Mixtures of Local Experts", "journal": "Neural Computation", "year": "1991", "authors": "Robert A Jacobs; Michael I Jordan; Steven J Nowlan; Geoffrey E Hinton"}, {"ref_id": "b27", "title": "On the identifiability of mixtures-of-experts", "journal": "Neural Networks", "year": "1999", "authors": "W Jiang; M A Tanner"}, {"ref_id": "b28", "title": "Hierarchical mixtures of experts and the EM algorithm", "journal": "", "year": "1993", "authors": "M I Jordan; R A Jacobs"}, {"ref_id": "b29", "title": "Deep Neural Decision Forests", "journal": "", "year": "2015", "authors": "Peter Kontschieder; Madalina Fiterau; Antonio Criminisi"}, {"ref_id": "b30", "title": "Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference", "journal": "", "year": "2021", "authors": "Sneha Kudugunta; Yanping Huang; Ankur Bapna; Maxim Krikun; Dmitry Lepikhin; Minh-Thang Luong; Orhan Firat"}, {"ref_id": "b31", "title": "The Hungarian method for the assignment problem", "journal": "Naval Research Logistics Quarterly", "year": "1955", "authors": "H W Kuhn"}, {"ref_id": "b32", "title": "The Winograd Schema Challenge", "journal": "AAAI Press", "year": "2012", "authors": "Hector J Levesque; Ernest Davis; Leora Morgenstern"}, {"ref_id": "b33", "title": "BASE Layers: Simplifying Training of Large, Sparse Models", "journal": "", "year": "2021", "authors": "Mike Lewis; Shruti Bhosale; Tim Dettmers; Naman Goyal; Luke Zettlemoyer"}, {"ref_id": "b34", "title": "Deep Learning Face Attributes in the Wild", "journal": "", "year": "2015", "authors": "Ziwei Liu; Ping Luo; Xiaogang Wang; Xiaoou Tang"}, {"ref_id": "b35", "title": "Modeling task relationships in multi-task learning with multi-gate mixture-ofexperts", "journal": "", "year": "1930", "authors": "Jiaqi Ma; Zhe Zhao; Xinyang Yi; Jilin Chen; Lichan Hong; Ed H Chi"}, {"ref_id": "b36", "title": "Learning Latent Permutations with Gumbel-Sinkhorn Networks", "journal": "", "year": "2018", "authors": "Gonzalo Mena; David Belanger; Scott Linderman; Jasper Snoek"}, {"ref_id": "b37", "title": "A System for Induction of Oblique Decision Trees", "journal": "J. Artif. Int. Res", "year": "1994-08", "authors": "K Sreerama; Simon Murthy; Steven Kasif;  Salzberg"}, {"ref_id": "b38", "title": "Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts", "journal": "", "year": "2022", "authors": "Basil Mustafa; Carlos Riquelme Ruiz; Joan Puigcerver; Rodolphe Jenatton; Neil Houlsby"}, {"ref_id": "b39", "title": "Reading Digits in Natural Images with Unsupervised Feature Learning", "journal": "", "year": "2011", "authors": "Yuval Netzer; Tao Wang; Adam Coates; A Bissacco; Bo Wu; A Ng"}, {"ref_id": "b40", "title": "EvoMoE: An Evolutional Mixture-of-Experts Training Framework via Dense-To-Sparse Gate", "journal": "", "year": "2021", "authors": "Xiaonan Nie; Xupeng Miao; Shijie Cao; Lingxiao Ma; Qibin Liu; Jilong Xue; Youshan Miao; Yi Liu; Zhi Yang; Bin Cui"}, {"ref_id": "b41", "title": "Know What You Don't Know: Unanswerable Questions for SQuAD", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Pranav Rajpurkar; Robin Jia; Percy Liang"}, {"ref_id": "b42", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text", "journal": "", "year": "2016", "authors": "Pranav Rajpurkar; Jian Zhang; Konstantin Lopyrev; Percy Liang"}, {"ref_id": "b43", "title": "Hash Layers For Large Sparse Models", "journal": "", "year": "2021", "authors": "Stephen Roller; Sainbayar Sukhbaatar; Arthur Szlam; Jason E Weston"}, {"ref_id": "b44", "title": "Scaling Vision with Sparse Mixture of Experts", "journal": "", "year": "2021", "authors": "Carlos Riquelme Ruiz; Joan Puigcerver; Basil Mustafa; Maxim Neumann; Rodolphe Jenatton; Andr\u00e9 Susano Pinto; Daniel Keysers; Neil Houlsby"}, {"ref_id": "b45", "title": "Dynamic Routing between Capsules", "journal": "", "year": "2017", "authors": "Sara Sabour; Nicholas Frosst; Geoffrey E Hinton"}, {"ref_id": "b46", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "journal": "", "year": "2017", "authors": "Noam Shazeer; Azalia Mirhoseini; * Krzysztof Maziarz; Andy Davis; Quoc Le; Geoffrey Hinton; Jeff Dean"}, {"ref_id": "b47", "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank", "journal": "", "year": "2013", "authors": "Richard Socher; Alex Perelygin; Jean Wu; Jason Chuang; Christopher D Manning; Andrew Ng; Christopher Potts"}, {"ref_id": "b48", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding", "journal": "", "year": "2019", "authors": "Alex Wang; Amanpreet Singh; Julian Michael; Felix Hill; Omer Levy; Samuel R Bowman"}, {"ref_id": "b49", "title": "SkipNet: Learning Dynamic Routing in Convolutional Networks", "journal": "", "year": "2018", "authors": "Xin Wang; Fisher Yu; Zi-Yi Dou; Trevor Darrell; Joseph E Gonzalez"}, {"ref_id": "b50", "title": "Small Towers Make Big Differences", "journal": "", "year": "2020", "authors": "Yuyan Wang; Zhe Zhao; Bo Dai; Christopher Fifty; Dong Lin; Lichan Hong; Ed H Chi"}, {"ref_id": "b51", "title": "Neural Network Acceptability Judgments", "journal": "Transactions of the Association for Computational Linguistics", "year": "2019", "authors": "Alex Warstadt; Amanpreet Singh; Samuel R Bowman"}, {"ref_id": "b52", "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Adina Williams; Nikita Nangia; Samuel Bowman"}, {"ref_id": "b53", "title": "Transformers: State-of-the-Art Natural Language Processing", "journal": "", "year": "2020", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; Remi Louf; Morgan Funtowicz; Joe Davison; Sam Shleifer; Clara Patrick Von Platen; Yacine Ma; Julien Jernite; Canwen Plu; Teven Xu; Sylvain Le Scao; Mariama Gugger; Quentin Drame; Alexander Lhoest;  Rush"}, {"ref_id": "b54", "title": "Residual Mixture of Experts", "journal": "", "year": "2022", "authors": "Lemeng Wu; Mengchen Liu; Yinpeng Chen; Dongdong Chen; Xiyang Dai; Lu Yuan"}, {"ref_id": "b55", "title": "Mixture-of-Experts with Expert Choice Routing", "journal": "", "year": "2022", "authors": "Yanqi Zhou; Tao Lei; Hanxiao Liu; Nan Du; Yanping Huang; Y Vincent; Andrew M Zhao; Zhifeng Dai; Quoc V Chen; James Le;  Laudon"}, {"ref_id": "b56", "title": "Improving Recommendation Lists through Topic Diversification", "journal": "Association for Computing Machinery", "year": "2005", "authors": "Cai-Nicolas Ziegler; Sean M Mcnee; Joseph A Konstan; Georg Lausen"}, {"ref_id": "b57", "title": "ST-MoE: Designing Stable and Transferable Sparse Expert Models", "journal": "", "year": "2022", "authors": "Barret Zoph; Irwan Bello; Sameer Kumar; Nan Du; Yanping Huang; Jeff Dean; Noam M Shazeer; William Fedus"}, {"ref_id": "b58", "title": "Taming Sparsely Activated Transformer with Stochastic Experts", "journal": "", "year": "2022", "authors": "Simiao Zuo; Xiaodong Liu; Jian Jiao; Jin Young; Hany Kim; Ruofei Hassan; Jianfeng Zhang; Tuo Gao;  Zhao"}, {"ref_id": "b59", "title": "MoEBERT: from BERT to Mixture-of-Experts via Importance-Guided Adaptation", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Simiao Zuo; Qingru Zhang; Chen Liang; Pengcheng He; Tuo Zhao; Weizhu Chen"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "\ud835\udc64 \u2265 0}. In the MoE framework, the prediction function has two components: (i) a set of \ud835\udc5b experts (parametrized by neural networks) \ud835\udc53 \ud835\udc56 : X \u2192 R \ud835\udc62 for any \ud835\udc56 \u2208 [\ud835\udc5b] := {1, 2, . . . , \ud835\udc5b}, and (ii) a gate \ud835\udc54 : X \u2192 \u0394 \ud835\udc5b that outputs weights in the probability simplex. Given a sample \ud835\udc65 \u2208 X, the corresponding output of the MoE is a convex combination of the experts with weights \ud835\udc54(\ud835\udc65): \ud835\udc56 \u2208 [\ud835\udc5b] \ud835\udc53 \ud835\udc56 (\ud835\udc65)\ud835\udc54(\ud835\udc65) \ud835\udc56 .", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: COMET for 8 experts. Note \ud835\udc67 \ud835\udc5e (\ud835\udc64 \ud835\udc5e \u2022 \ud835\udc65) denotes the binary state {0, 1} for \u210e(\ud835\udc64 \ud835\udc5e \u2022 \ud835\udc65) achieved due to smooth-step activation function and entropic regularization.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: Sensitivity of COMET to hyperparameter tuning.COMET can achieve the same level of performance as popular gates (e.g., Hash-r and Top-k) with significantly lesser number of hyperparameter trials. We see tuning reduction by 5\u00d7-100\u00d7 for COMET over Top-k and Hash routing.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: Effect of local search on hyperparameter tuning. Comparison of Hash-r+ vs Hash-r, Top-k+ vs Top-k and COMET+ vs COMET on MovieLens with two different task weight settings. Local search appears to achieve the same level of performance with much lesser number of hyperparameter trials. We see tuning reduction by a factor of 3\u00d7-100\u00d7 for Hash-r+, 20\u00d7-100\u00d7 for Top-k+ and 2\u00d7-5\u00d7 for COMET+.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Our local search method is general and can be applied to any gate, e.g., Hash routing, Top-k, and COMET. \u2022 We perform extensive experiments on recommender systems, vision and natural language processing tasks to highlight that COMET and COMET+ (COMET combined with local search) can give boosts in predictive performance. In particular, on recommender and image datasets, we observed that COMET+ can improve AUC performance by up to 13% over existing sparse gates e.g., Top-k and Hash routing. It can also reduce tuning by up to 100\u00d7 over popular gates e.g., Hash routing, and Topk. Similarly, in natural language processing applications, our COMET-BERT model (MoE based variant of BERT with COMET / COMET+ gating) can outperform state-of-the-art Hash-routingbased MoEBERT model [60] on 5/7 GLUE benchmarks as well as SQuAD dataset for distilling pre-trained BERT model [11].", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Test loss (\u00d710 -2 ) achieved for different gates without and with (marked with +) local search in early stages of optimization. Asterisk(*) indicates statistical significance (p-value<0.05) over the corresponding gate without permutation with a one-sided unpaired t-test.", "figure_data": "StrategySmoothnessGateTest Loss \u2193Hash-r57.434 \u00b1 0.025Pre-assigned -Hash-r+*  57.000 \u00b1 0.037Top-k53.345 \u00b1 0.033TrainableNon-differentiable DifferentiableTop-k+ COMET COMET+  *  52.017 \u00b1 0.005  *  53.140 \u00b1 0.031 52.034 \u00b1 0.007"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Tess Loss (\u00d710 -2 , the smaller the better) and number of experts per sample (\ud835\udc5b/\ud835\udc60) for COMET, COMET+ and benchmark gates across various recommender system datasets. Asterisk(*) indicates statistical significance (p-value<0.05) over the best existing gate, using a one-sided unpaired t-test.", "figure_data": "Dataset\ud835\udc5bModelTest Loss \u2193\ud835\udc5b/\ud835\udc60 \u2193Softmax244.47 \u00b1 0.149.00 \u00b1 0.00Hash-r247.43 \u00b1 0.141.00 \u00b1 0.00Books (\ud835\udefc = 0.1)9Hash-r+ Top-k247.33 \u00b1 0.23 247.87 \u00b1 0.171.00 \u00b1 0.00 4.00 \u00b1 0.00Top-k+247.88 \u00b1 0.144.00 \u00b1 0.00DSelect-k246.43 \u00b1 0.361.09 \u00b1 0.00COMET*  240.79 \u00b1 0.142.81 \u00b1 0.11COMET+240.82 \u00b1 0.193.03 \u00b1 0.08Softmax73.88 \u00b1 0.029.00 \u00b1 0.00Hash-r75.02 \u00b1 0.031.00 \u00b1 0.00Books (\ud835\udefc = 0.9)9Hash-r+ Top-k75.06 \u00b1 0.03 74.78 \u00b1 0.031.00 \u00b1 0.00 4.00 \u00b1 0.00Top-k+74.86 \u00b1 0.034.00 \u00b1 0.00DSelect-k75.98 \u00b1 0.131.07 \u00b1 0.00COMET73.62 \u00b1 0.032.94 \u00b1 0.10COMET+*  73.55 \u00b1 0.023.15 \u00b1 0.08Softmax42.26 \u00b1 0.0116.00 \u00b1 0.00Hash-r46.91 \u00b1 0.021.00 \u00b1 0.00MovieLens (\ud835\udefc = 0.9)16Hash-r+ Top-k46.84 \u00b1 0.03 41.83 \u00b1 0.021.00 \u00b1 0.00 2.00 \u00b1 0.00Top-k+41.74 \u00b1 0.022.00 \u00b1 0.00DSelect-k40.82 \u00b1 0.021.94 \u00b1 0.06COMET40.76 \u00b1 0.021.76 \u00b1 0.06COMET+*  40.69 \u00b1 0.021.66 \u00b1 0.06Softmax75.52 \u00b1 0.0216.00 \u00b1 0.00Hash-r79.41 \u00b1 0.021.00 \u00b1 0.00MovieLens (\ud835\udefc = 0.1)16Hash-r+ Top-k78.92 \u00b1 0.05 76.52 \u00b1 0.041.00 \u00b1 0.00 2.00 \u00b1 0.00Top-k+75.12 \u00b1 0.042.00 \u00b1 0.00DSelect-k73.91 \u00b1 0.051.94 \u00b1 0.03COMET73.91 \u00b1 0.041.94 \u00b1 0.03COMET+*  73.67 \u00b1 0.041.98 \u00b1 0.03Softmax68.17 \u00b1 0.0316.00 \u00b1 0.00Hash-r67.47 \u00b1 0.011.00 \u00b1 0.00Jester (\ud835\udefc = 0.1)16Top-k Top-k+68.38 \u00b1 0.05 68.00 \u00b1 0.072.00 \u00b1 0.00 2.00 \u00b1 0.00DSelect-k67.06 \u00b1 0.031.96 \u00b1 0.02COMET67.12 \u00b1 0.041.98 \u00b1 0.02COMET+*  66.91 \u00b1 0.032.00 \u00b1 0.00Softmax21.936 \u00b1 0.00216.00 \u00b1 0.00Hash-r22.083 \u00b1 0.0041.00 \u00b1 0.00Jester (\ud835\udefc = 0.9)16Top-k Top-k+21.958 \u00b1 0.007 21.961 \u00b1 0.0062.00 \u00b1 0.00 2.00 \u00b1 0.00DSelect-k21.930 \u00b1 0.0052.00 \u00b1 0.00COMET21.946 \u00b1 0.0052.00 \u00b1 0.00COMET+"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Even without local search, COMET is getting relatively good solutions. We hypothesize that the good performance of COMET is due to a combination of factors including differentiability, and \ud835\udc58-decision trees formulation. With local search, COMET+ can sometimes further enhance solution quality. We also provide taskspecific metrics (AUC/Accuracy/MSE) in Tables", "figure_data": "local search).Dataset\ud835\udc5bModelTest Loss \u2193\ud835\udc5b/\ud835\udc60 \u2193Softmax34.21 \u00b1 0.095.00 \u00b1 0.00Top-k33.82 \u00b1 0.092.00 \u00b1 0.00MultiFashionMNIST5Top-k+*  33.62 \u00b1 0.082.00 \u00b1 0.00DSelect-k35.49 \u00b1 0.101.00 \u00b1 0.00COMET33.70 \u00b1 0.091.49 \u00b1 0.07COMET+*  33.67 \u00b1 0.091.54 \u00b1 0.07Softmax35.10 \u00b1 0.326.00 \u00b1 0.00Top-k34.48 \u00b1 0.242.00 \u00b1 0.00CelebA6Top-k+34.54 \u00b1 0.232.00 \u00b1 0.00DSelect-k35.39 \u00b1 0.121.00 \u00b1 0.00COMET33.96 \u00b1 0.151.00 \u00b1 0.08COMET+*  33.93 \u00b1 0.161.00 \u00b1 0.08Softmax17.48 \u00b1 0.078.00 \u00b1 0.00Top-k17.46 \u00b1 0.092.00 \u00b1 0.00Digits8Top-k+17.29 \u00b1 0.082.00 \u00b1 0.00DSelect-k17.18 \u00b1 0.061.15 \u00b1 0.06COMET17.19 \u00b1 0.061.07 \u00b1 0.04COMET+17.08 \u00b1 0.091.06 \u00b1 0.04Softmax6.88 \u00b1 0.0616.00 \u00b1 0.00Top-k6.84 \u00b1 0.054.00 \u00b1 0.00MultiMNIST16Top-k+6.70 \u00b1 0.084.00 \u00b1 0.00DSelect-k6.64 \u00b1 0.073.40 \u00b1 0.09COMET6.48 \u00b1 0.073.49 \u00b1 0.09COMET+6.49 \u00b1 0.063.53 \u00b1 0.08"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Search with COMET. Here, we study how our differentiable COMET gate (that performs dense-tosparse training) can be beneficial in terms of hyperparameter tuning over popular gates such as Hash routing and Top-k. We perform a large set of tuning trials and perform a bootstrapping procedure (discussed in Appendix F) to see whether COMET helps in reducing the hyperparameter tuning overload. COMET can achieve the same level of performance as popular gates with much lesser number of", "figure_data": "BooksJesterTest Loss2.40 2.42 2.44 2.46 2.48 2.50 2.52050 100 150 200 250 number of trials Hash-r Top-k COMETTest Loss0.670 0.675 0.680 0.685 0.690 0.6950number of trials 50 100 150 200 250 Hash-r Top-k COMETDigitsMovieLens0.178Top-k COMET0.62 0.63Hash-r Top-k COMETTest Loss0.174 0.176Test Loss0.59 0.60 0.610.1720.57 0.58050 100 150 200 250 number of trials0.56050100 number of trials 150200250"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Performance metrics on the GLUE and SQuAD development sets. Models are trained without data augmentation. Both models have 66M (effective) parameters for inference. that a model performs per sample -is a standard measure to evaluate the inference speed for Sparse-MoE e.g., in[14] etc.", "figure_data": "GLUESQuADRTECoLAMRPCSST-2QNLIQQPMNLIv2.0AccMccF1AccAccF1/Accm/mmF1/EMMOEBERT 370.8 55.491.093.2 90.9 88.5/91.4 84.7 76.8/73.6COMET-BERT 71.1 57.0 91.393.0 91.2 88.4/--85.5 78.4/75.36 CONCLUSIONIn summary, we propose two new approaches for improving rout-ing in Sparse-MoE. First, we introduce a new differentiable gateCOMET, which relies on a novel tree-based sparse expert selectionmechanism. COMET allows optimization with first-order methods,offers explicit control over the number of experts to select, allows(partially) conditional training and sparse inference. Second, in thiswork, we argue that combinatorial nature of expert selection inSparse-MoE makes sparse routing optimization challenging withfirst-order methods. Thus, we propose a new local search methodthat can help any gate including ours (COMET ) escape \"bad\" ini-tializations. Our large-scale experiments on recommender systems,vision and natural language processing tasks show COMET andCOMET+: (i) achieve statistically significant improvements in pre-diction (up to 13% improvement in AUC) and expert selection overpopular sparse gates. (ii) reduce tuning up to a factor of 100\u00d7 toachieve the same level of performance as popular gates e.g., Top-k"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "#FLOPs per-sample at inference time for COMET against benchmarks (Softmax, Topk, DSelect-k) across various datasets.", "figure_data": "DatasetModelFLOPsSoftmax1195KTop-k402KBooksDSelect-k214KCOMET326\ud835\udc3eSoftmax2255KTop-k413KMovieLensDSelect-k399KCOMET362KSoftmax7.49MTop-k3.03MMultiFashionMNISTDSelect-k1.58MCOMET2.35MSoftmax22.02MTop-k8.82MCelebADSelect-k5.64MCOMET5.47M"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Test AUC/Accuracy/MSE for COMET+ and benchmark gates on recommender systems and image datasets. Randomly sample \ud835\udc60 (\ud835\udc60 \u2208 {1, 2, 5, 10, 15, \u2022 \u2022 \u2022 , 250}) trials from the bag of a larger set of 500 trials. \u2022 Find the trial with the best validation loss.\u2022 Compute the test loss for that trial.\u2022 Repeat this exercise for 1000 times.\u2022 Compute the average test loss across the best selected trials.", "figure_data": "Recommender SystemsTask 1Task 2Gate(Test AUC)(Test MSE)Softmax56.70\u00b10.162.6470\u00b10.0016Hash-r54.55\u00b10.072.6791\u00b10.0017BooksTop-k55.28\u00b10.072.6783\u00b10.0026(alpha=0.1)DSelect-k59.19\u00b10.362.6667\u00b10.0038COMET+68.18\u00b10.242.6063\u00b10.0018Softmax77.85\u00b10.012.6195\u00b10.0019Hash-r77.32\u00b10.052.7152\u00b10.0050BooksTop-k77.46\u00b10.032.6942\u00b10.0016(alpha=0.9)DSelect-k77.07\u00b10.092.7581\u00b10.0092COMET+77.95\u00b10.022.6158\u00b10.0021Softmax90.92\u00b10.010.7585\u00b10.0005Hash-r88.95\u00b10.020.8065\u00b10.0004MovieLensTop-k91.25\u00b10.010.7635\u00b10.0008(alpha=0.9)DSelect-k91.65\u00b10.020.7455\u00b10.0006COMET+91.70\u00b10.010.7437\u00b10.0006Softmax85.50\u00b10.030.7867\u00b10.0029Hash-r84.27\u00b10.070.8279\u00b10.0003MovieLensTop-k87.12\u00b10.040.8005\u00b10.0004(alpha=0.1)DSelect-k88.16\u00b10.070.7734\u00b10.0005COMET+88.02\u00b10.010.7707\u00b10.0005Softmax97.350\u00b10.0040.7460\u00b10.0003Hash-r97.323\u00b10.0030.7530\u00b10.0003JesterTop-k97.346\u00b10.0040.7456\u00b10.0006(alpha=0.9)DSelect-k97.361\u00b10.0040.7464\u00b10.0005COMET+97.362\u00b10.0040.7439\u00b10.0006Softmax97.22\u00b10.010.7380\u00b10.0003Hash-r97.01\u00b10.010.7301\u00b10.0002JesterTop-k97.38\u00b10.010.7412\u00b10.0005(alpha=0.1)DSelect-k97.45\u00b10.000.7273\u00b10.0003COMET+97.45\u00b10.000.7257\u00b10.0004Image DatasetsTest AccuracyGate(Averaged across tasks)Softmax87.99\u00b10.04Top-k88.03\u00b10.03MultiFashionMNISTDSelect-k87.42\u00b10.04COMET+88.12\u00b10.04Softmax83.84\u00b10.15Top-k83.95\u00b10.17CelebADSelect-k83.53\u00b10.06COMET+84.27\u00b10.08Softmax93.53\u00b10.12Top-k95.34\u00b10.04DigitsDSelect-k95.41\u00b10.03COMET+95.45\u00b10.04Softmax98.01\u00b10.03Top-k98.01\u00b10.02MultiMNISTDSelect-k98.03\u00b10.02COMET+98.07\u00b10.02F BOOTSTRAPPING PROCEDURE FORSTUDYING HYPERPARAMETER TUNINGWe performed 500 tuning trials and performed a bootstrappingprocedure as outlined below:\u2022"}, {"figure_label": "S1", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Summary of GLUE benchmark.", "figure_data": "Corpus Task#Train #Dev #Test #Label MetricsSingle-Sentence Classification (GLUE)CoLAAcceptability8.5k1k1k2Matthews correlationSST-2Sentiment67k8721.8k2AccuracyPairwise Text Classification (GLUE)MNLINatural Language Inference393k20k20k3AccuracyRTENatural Language Inference2.5k2763k2AccuracyQQPParaphrase364k40k391k2Accuracy/F1MRPCParaphrase3.7k4081.7k2Accuracy/F1QNLIQuestion Answering/Natural Language Inference 108k5.7k5.7k2Accuracy"}, {"figure_label": "S2", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Summary of SQuAD benchmark.", "figure_data": "CorpusTask#Train #Dev MetricsSQuAD v2.0 Question Answering/Reading Comprehension 130k11.9k F1/EM"}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u0394 \ud835\udc5b = {\ud835\udc64 \u2208 R \ud835\udc5b : \ud835\udc56 \u2208 [\ud835\udc5b] \ud835\udc64 \ud835\udc56 = 1,", "formula_coordinates": [3.0, 53.89, 178.72, 118.96, 9.86]}, {"formula_id": "formula_1", "formula_text": "min \ud835\udc53 1 ,\u2022\u2022\u2022 ,\ud835\udc53 \ud835\udc5b ,\ud835\udc54 1 \ud835\udc41 (\ud835\udc65,\ud835\udc66) \u2208 D \u2113 \ud835\udc66, \ud835\udc56 \u2208 [\ud835\udc5b] \ud835\udc53 \ud835\udc56 (\ud835\udc65)\ud835\udc54(\ud835\udc65) \ud835\udc56 ,(1a)", "formula_coordinates": [3.0, 97.82, 318.84, 196.76, 19.56]}, {"formula_id": "formula_2", "formula_text": "s.t. \u2225\ud835\udc54(\ud835\udc65)\u2225 0 \u2264 \ud835\udc58, \ud835\udc54(\ud835\udc65) \u2208 \u0394 \ud835\udc5b , \u2200\ud835\udc65 \u2208 X.(1b", "formula_coordinates": [3.0, 114.32, 343.54, 176.78, 9.2]}, {"formula_id": "formula_3", "formula_text": ":= \u210e(\ud835\udc64 \ud835\udc5e \u2022 \ud835\udc65) 1[\ud835\udc59 \ud835\udc5e ] (1 -\u210e(\ud835\udc64 \ud835\udc5e \u2022 \ud835\udc65)) 1[\ud835\udc5e \ud835\udc59 ]", "formula_coordinates": [4.0, 53.53, 251.8, 240.43, 21.07]}, {"formula_id": "formula_4", "formula_text": "\ud835\udc63 (\ud835\udc65) = [Pr({\ud835\udc65 \u2192 \ud835\udc59 1 }), \u2022 \u2022 \u2022 , Pr({\ud835\udc65 \u2192 \ud835\udc59 \ud835\udc5b })] \u2208 \u0394 \ud835\udc5b ,(2)", "formula_coordinates": [4.0, 86.55, 287.3, 208.03, 8.97]}, {"formula_id": "formula_5", "formula_text": "( \ud835\udc57 ) \ud835\udc56 is a linear function \ud835\udefd ( \ud835\udc57 ) \ud835\udc56 \u2022 \ud835\udc65 of the input. \ud835\udefc ( \ud835\udc57 ) \ud835\udc56", "formula_coordinates": [4.0, 158.65, 83.91, 205.98, 626.82]}, {"formula_id": "formula_6", "formula_text": "\ud835\udc54(\ud835\udc65; \ud835\udefc, \ud835\udc63) \ud835\udc56 = \ud835\udc57 \u2208 [\ud835\udc58 ] exp(\ud835\udefc ( \ud835\udc57 ) \ud835\udc56 (\ud835\udc65))\ud835\udc63 ( \ud835\udc57 ) \ud835\udc56 (\ud835\udc65) \ud835\udc57 \u2208 [\ud835\udc58 ] \ud835\udc56 \u2208 [\ud835\udc5b] exp(\ud835\udefc ( \ud835\udc57 ) \ud835\udc56 (\ud835\udc65))\ud835\udc63 ( \ud835\udc57 ) \ud835\udc56 (\ud835\udc65) ,(3)", "formula_coordinates": [4.0, 355.61, 187.27, 203.13, 43.04]}, {"formula_id": "formula_7", "formula_text": "\ud835\udc63 ( \ud835\udc57 )", "formula_coordinates": [4.0, 341.79, 239.32, 13.8, 9.83]}, {"formula_id": "formula_8", "formula_text": "(\ud835\udc65)) = -\ud835\udc56 \u2208 [\ud835\udc5b] \ud835\udc63 ( \ud835\udc57 ) \ud835\udc56 (\ud835\udc65) log(\ud835\udc63 ( \ud835\udc57 )", "formula_coordinates": [4.0, 388.19, 399.97, 118.71, 13.12]}, {"formula_id": "formula_9", "formula_text": "( \ud835\udc57 ) \ud835\udc56 (\ud835\udc65), (ii) compute \ud835\udefc ( \ud835\udc57 ) \ud835\udc56 + log \ud835\udc63 ( \ud835\udc57 )", "formula_coordinates": [5.0, 108.29, 198.13, 124.53, 13.12]}, {"formula_id": "formula_10", "formula_text": "( \ud835\udc57 ) \ud835\udc56 + log \ud835\udc63 ( \ud835\udc57 )", "formula_coordinates": [5.0, 157.13, 212.21, 47.14, 13.12]}, {"formula_id": "formula_11", "formula_text": "\ud835\udc57 \u2208 [\ud835\udc5b] \ud835\udc53 \ud835\udc57 (\ud835\udc65)\ud835\udc54(\ud835\udc65) \ud835\udf0e -1 ( \ud835\udc57 ) = \ud835\udc57 \u2208 [\ud835\udc5b] \ud835\udc53 \ud835\udc57 (\ud835\udc65)(\ud835\udc77 \ud835\udf0e \ud835\udc54(\ud835\udc65)) \ud835\udc57 . The refined Sparse-MoE problem is min \ud835\udc53 1 ,\u2022\u2022\u2022 ,\ud835\udc53 \ud835\udc5b ,\ud835\udc54,\ud835\udc77 1 \ud835\udc41 (\ud835\udc65,\ud835\udc66) \u2208 D \u2113 \ud835\udc66, \ud835\udc56 \u2208 [\ud835\udc5b] \ud835\udc53 \ud835\udc56 (\ud835\udc65)(\ud835\udc77\ud835\udc54(\ud835\udc65)) \ud835\udc56 ,(4a)", "formula_coordinates": [5.0, 325.78, 142.1, 232.96, 53.22]}, {"formula_id": "formula_12", "formula_text": "s.t. \u2225\ud835\udc54(\ud835\udc65)\u2225 0 \u2264 \ud835\udc58, \ud835\udc54(\ud835\udc65) \u2208 \u0394 \ud835\udc5b , \u2200\ud835\udc65 \u2208 X,(4b)", "formula_coordinates": [5.0, 382.07, 200.44, 176.67, 8.97]}, {"formula_id": "formula_13", "formula_text": "\ud835\udc77 \u2208 P local \ud835\udc5b ,(4c)", "formula_coordinates": [5.0, 394.72, 216.38, 164.02, 9.3]}, {"formula_id": "formula_14", "formula_text": "\ud835\udc40 (\ud835\udc7c ) = arg max \ud835\udc77 \u2208 P \ud835\udc5b \u27e8\ud835\udc77, \ud835\udc7c \u27e9 \ud835\udc39 := \ud835\udc56 \u2208 [\ud835\udc5b] \ud835\udc57 \u2208 [\ud835\udc5b]", "formula_coordinates": [5.0, 354.71, 510.76, 139.82, 16.58]}, {"formula_id": "formula_15", "formula_text": ")5", "formula_coordinates": [5.0, 552.4, 513.03, 6.34, 4.09]}, {"formula_id": "formula_16", "formula_text": "max \ud835\udc69 \u2208 B \ud835\udc5b \u27e8\ud835\udc77, \ud835\udc7c \u27e9 \ud835\udc39 := \ud835\udc56 \u2208 [\ud835\udc5b] \ud835\udc57 \u2208 [\ud835\udc5b] \ud835\udc43 \ud835\udc56 \ud835\udc57 \ud835\udc48 \ud835\udc56 \ud835\udc57 ,(6)", "formula_coordinates": [5.0, 375.71, 584.51, 183.03, 15.76]}, {"formula_id": "formula_17", "formula_text": "B \ud835\udc5b = {\ud835\udc69 \u2208 R \ud835\udc5b\u00d7\ud835\udc5b : \ud835\udc56 \u2208 [\ud835\udc5b] \ud835\udc35 \ud835\udc56 \ud835\udc57 = 1, \ud835\udc57 \u2208 [\ud835\udc5b] \ud835\udc35 \ud835\udc56 \ud835\udc57 = 1, \ud835\udc35 \ud835\udc56 \ud835\udc57 \u2208 [0, 1]},", "formula_coordinates": [5.0, 317.96, 608.55, 240.07, 20.44]}, {"formula_id": "formula_18", "formula_text": "\ud835\udc46 (\ud835\udc7c /\ud835\udf0f) = arg max \ud835\udc69\u2208 B \ud835\udc5b \u27e8\ud835\udc69, \ud835\udc7c \u27e9 \ud835\udc39 -\ud835\udf0f \ud835\udc56,\ud835\udc57 \u2208 [\ud835\udc5b] \ud835\udc35 \ud835\udc56 \ud835\udc57 log \ud835\udc35 \ud835\udc56 \ud835\udc57 ,(7)", "formula_coordinates": [6.0, 86.98, 102.21, 207.6, 16.58]}, {"formula_id": "formula_19", "formula_text": "\ud835\udc46 0 (\ud835\udc7c ) = exp(\ud835\udc7c ),(8a)", "formula_coordinates": [6.0, 117.96, 149.66, 176.63, 8.59]}, {"formula_id": "formula_20", "formula_text": "\ud835\udc46 \ud835\udc5f (\ud835\udc7c ) = T \ud835\udc50\ud835\udc5c\ud835\udc59 (T \ud835\udc5f\ud835\udc5c\ud835\udc64 (\ud835\udc46 \ud835\udc5f -1 (\ud835\udc7c ))),(8b)", "formula_coordinates": [6.0, 117.97, 163.52, 176.62, 10.89]}, {"formula_id": "formula_21", "formula_text": "\ud835\udc46 (\ud835\udc7c ) = lim \ud835\udc5f \u2192\u221e \ud835\udc46 \ud835\udc5f (\ud835\udc7c ),(8c)", "formula_coordinates": [6.0, 121.84, 179.89, 172.75, 13.38]}, {"formula_id": "formula_22", "formula_text": "0", "formula_coordinates": [9.0, 93.88, 454.72, 2.47, 7.34]}], "doi": "10.1145/3580305.3599278"}
