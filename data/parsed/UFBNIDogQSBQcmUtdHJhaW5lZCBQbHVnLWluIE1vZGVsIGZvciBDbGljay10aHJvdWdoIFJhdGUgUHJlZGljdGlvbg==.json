{
  "PPM : A Pre-trained Plug-in Model for Click-through Rate Prediction": "Yuanbo Gao ∗ Jingdong Group Beijing, China gaoyuanbo1@jd.com Feng Mei Jingdong Group Beijing, China meifeng6@jd.com Peng Lin ∗† Jingdong Group Beijing, China linpeng47@jd.com Dongyue Wang Jingdong Group Beijing, China wangdongyue@jd.com",
  "Xiwei Zhao": "Jingdong Group Beijing, China zhaoxiwei@jd.com Sulong Xu Jingdong Group Beijing, China xusulong@jd.com",
  "Jinghe Hu": "Jingdong Group Beijing, China hujinghe@jd.com",
  "ABSTRACT": "",
  "CCS CONCEPTS": "Click-through rate (CTR) prediction is a core task in recommender systems. Existing methods (IDRec for short) rely on unique identities to represent distinct users and items that have prevailed for decades. On one hand, IDRec often faces significant performance degradation on cold-start problem; on the other hand, IDRec cannot use longer training data due to constraints imposed by iteration efficiency. Most prior studies alleviate the above problems by introducing pre-trained knowledge(e.g. pre-trained user model or multi-modal embeddings). However, the explosive growth of online latency can be attributed to the huge parameters in the pre-trained model. Therefore, most of them cannot employ the unified model of end-to-end training with IDRec in industrial recommender systems, thus limiting the potential of the pre-trained model. Tothis end, we propose a p re-trained p lug-in CTR m odel, namely PPM. PPM employs multi-modal features as input and utilizes largescale data for pre-training. Then, PPM is plugged in IDRec model to enhance unified model's performance and iteration efficiency. Upon incorporating IDRec model, certain intermediate results within the network are cached, with only a subset of the parameters participating in training and serving. Hence, our approach can successfully deploy an end-to-end model without causing huge latency increases. Comprehensive offline experiments and online A/B testing at JD E-commerce demonstrate the efficiency and effectiveness of PPM. ∗ Both authors contributed equally to this research. † Corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WWW'24 Companion, May 13-17, 2024, Singapore, Singapore © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0172-6/24/05...$15.00 https://doi.org/10.1145/3589335.3648329 · Information systems → Recommender systems ; Personalization .",
  "KEYWORDS": "E-commerce recommendation systems, pre-trained model, CTR, end-to-end training",
  "ACMReference Format:": "Yuanbo Gao, Peng Lin, Dongyue Wang, Feng Mei, Xiwei Zhao, Sulong Xu, and Jinghe Hu. 2024. PPM : A Pre-trained Plug-in Model for Click-through Rate Prediction. In Companion Proceedings of the ACM Web Conference 2024 (WWW'24 Companion), May 13-17, 2024, Singapore, Singapore. ACM, New York, NY, USA, 8 pages. https://doi.org/10.1145/3589335.3648329",
  "1 INTRODUCTION": "As one of the largest B2C e-commerce platforms in China, JD.com 1 serves hundreds of millions of active customers. The accurate capturing of user interests assumes heightened significance as they browse the JD App. The traditional methods utilize unique identities to represent distinct users and items have been state-of-the-art and dominated the recommendation systems literature for decades [23]. However, when confronted with unfamiliar items that lack interaction features like clicks and orders, the efficacy of ID features in accurately characterizing both users and items falters in such scenarios [29]. Another issue is how to strike a balance between training data and iteration efficiency. Despite the abundance of artificial feedback data in industrial domains, it is customary to employ a limited subset of recent data for model training to improve iteration efficiency. Nevertheless, this could potentially diminish the model's performance by limiting the utilization of training data. The most common way to address these issues is to leverage pre-train knowledge, which can be broadly classified into two approaches, pre-trained multi-modal embedding and pre-trained user model [1, 25, 26]. The first approach combines IDRec with 1 https://www.jd.com/ WWW'24 Companion, May 13-17, 2024, Singapore, Singapore Yuanbo Gao et al. pre-trained multi-modal embeddings that are extracted from wellknown pre-trained models, such as BERT [6] and ResNet [10]. Subsequently, these modal features are fixed and regarded as supplementary attributes in the recommendation model to alleviate the cold-start problem [13, 35]. Despite their success, just using pretrained representations instead of loading the pre-trained model would greatly limit its ability. Therefore, an alternative approach directly loads the pre-trained user model that is obtained by selfsupervision learning from user historical behavior. The final ranking model can simply load the parameters of the pre-trained user model to benefit from the gains brought by large-scale training data. In spite of the accomplishments yielded by prior investigations on pre-trained knowledge in academic scenarios, the unified deployment between IDRec and the pre-trained user model, still encounters challenges and difficulties in industrial domain. The primary concern is an end-to-end deployment may result in an explosive increase in training resources and online latency, due to the huge amount of the pre-trained model's parameters in industrial recommendation system, an excessive latency may result in vacuous returns, severely impacting the user experience, while users browse the application. Thus, the online latency is strictly limited within a certain range. Although the method of caching user and item representations in advance is introduced to solve the online latency problem [18], the performance of the unified model cannot be guaranteed due to lack of the joint training. To create a delightful shopping experience, it is critical to accomplish a joint model between IDRec and the pre-trained model without incurring an increase in online latency. Thus, we propose a pre-trained plug-in CTR model (PPM), a novel framework that employs multi-modal features as input and utilizes large-scale data for pre-training. Then, this framework is plugged in IDRec model to mitigate the cold-start problem and accelerate iteration efficiency. Furthermore, during the joint training process, the pre-trained multi-modal features are cached in advance and fixed to address the issue of increased online latency. Meanwhile, the remaining essential parameters of PPM are trainable to further enhance the performance of the unified model. Experimental results show that PPM not only achieves better results on different network structures, but is also effective for cold-start problem. In addition, the performance of utilizing 50% of data with PPM basically equals that of 100% without PPM for the unified model, indicating that PPM can significantly improve iteration efficiency for an end-to-end model. The contributions of this paper are summarized as follows. · We propose a pre-trained plug-in CTR model (PPM) that achieves an end-to-end deployment with IDRec in industrial recommendation system; · Though caching pre-trained multi-modal features and the other vital parameters of PPM can be optimized during the end-to-end training, the model performance is improved without causing additional online latency increase; · Experimental results from offline evaluation and online A/B tests prove that PPM not only achieves better results, especially for the cold-start problem, but also can improve iteration efficiency for the unified ranking model. We organize the rest of the paper as follows. In Section 2, we introduce related works in brief. In Section 3, we present our proposed method, including PPM and Unified Ranking Model (URM). In Section 4 details the experiments, including mentioned questions we care about and relevant experimental results. Finally, we conclude this work in Section 5.",
  "2 RELATED WORK": "",
  "2.1 ID-based Recommendation": "A large number of recommendation models are built only based on discrete User ID and Item ID. Collaborative filtering is a typical method to model user preference by producing similarity matrix based on users' interaction histories, such as Item-based CF [2, 21] and User-based CF [31] and Unifing-CF [24]. Many previous works [4, 28] simply mapped the sparse IDs into dense vectors, through random initialized embedding matrix. Then deep neural networks are adopted to model users' preference, including DeepFM [9], Wide&Deep [3], GRU4Rec [12], SASRec [14] and etc [16, 34]. In the existing recommendation literature, ID-based methods have been well-established and dominated the RS field until now.",
  "2.2 Pre-trained Modal Embedding": "To solve cold-start problem in IDRec, many prior works adopted two-stage (TS) paradigm to amalgamate IDRec and pre-trained modal embedding. In TS paradigm, the frozen modality representations are extracted from well-known pre-trained models at first. These frozen modality representations are incorporated as auxiliary information for recommendation models, in the form of embeddings [11, 17]. He and McAuley [11] attempts to extract visual features using pre-trained deep networks for personalized ranking. Zheng et al. [32] utilize multi-modal encoder and propose a modal adaptation module for better amalgamation of texts and images representations. This approach is popular for industrial applications owing to its cost-effectiveness in terms of computation and training expenses.",
  "2.3 Pre-trained User Model": "Pre-trained models have made a great success in NLP and CV, which is first pre-trained on unlabeled corpus and then fine-tuned on downstream tasks via labeled data. In recent years, pre-train and fine-tune paradigm is also widely used in recommendation models. [25, 27] propose pre-training transformer-based recommendation model via predicting the masked item in user behavior sequence. Wu et al. [26] further proposed contrastive user behavior sequence matching task, combined with masked item prediciton task (MIP) to pre-train user models.",
  "3 METHODS": "In this section, we describe the architecture of our proposed model as depicted in Figure 1. The whole framework consists of two basic components, Pre-trained CTR Model and Unified Ranking Model. PPM : A Pre-trained Plug-in Model for Click-through Rate Prediction WWW'24 Companion, May 13-17, 2024, Singapore, Singapore Figure 1: Architecture of the proposed PPM and Unified Ranking Model (URM). ΒΕΡΤ ΡεσΝετ-101 ΣΚΥ ΣΗΟΠ Ι∆-βασεδ Ηαση Εµβεδδινγ (Τραιναβλε) ΧΙ∆ ΒΡΑΝ∆ Υσερ Ι∆ Ρεπρεσεντατιον Ιτεµ Ι∆ Ρεπρεσεντατιον Ιτεµ Μοδαλ Ρεπρεσεντατιον Πρε-τραινεδ Τρανσφορµερ Ι∆-βασεδ Τρανσφορµερ Εξπερτ 1 Χλιχκ Ορδερ Εξπερτ Ν Αππλε ιΠηονε 15 Προ 512ΓΒ Τρανσφορµερ 〈〈〈 𝑈 ! 𝑈 \" 𝑈 # Ποσιτιον & Ρεχενχψ Εµβεδδινγ 2× Κ ς Θ Τρανσφορµερ 2× Κ ς Θ Ποσιτιον & Ρεχενχψ Εµβεδδινγ Ποσιτιον & Ρεχενχψ Εµβεδδινγ Χλιχκ Νικον ∆7500 Βοδψ ΠΠΜ (Τραιναβλε) Ξιαοµι 13Τ 5Γ+4Γ ΛΤΕ Ιτεµ Μοδαλ Υσερ Μοδαλ Σεθυενχε Μοδαλιτψ Ενχοδερ Λαψερ (Χαχηεδ) 𝐼 ΣΚΥ: 887124 ΣΗΟΠ: 4891 ΧΙ∆: 944 ΒΡΑΝ∆: 309 ΣΚΥ: 3129523 ΣΗΟΠ: 9437 ΧΙ∆: 706 ΒΡΑΝ∆: 6591 ΣΚΥ: 103941 ΣΗΟΠ: 644 ΧΙ∆: 9 ΒΡΑΝ∆: 795 Ιτεµ Ι∆ Υσερ Ι∆ Σεθυενχε 〈〈〈 𝑈 ! 𝑈 \" 𝑈 # 𝐼 〈〈〈 𝑈 ! 𝑈 \" 𝑈 # 〈〈〈 𝑈 ! 𝑈 \" 𝑈 # 𝐼 〈〈〈 𝑈 ! 𝑈 \" 𝑈 # 𝐼 Πρε-τραινεδ ΧΤΡ Μοδελ 𝐼 𝑈 Υσερ Μοδαλ Ρεπρεσεντατιον 𝐼 𝑈 Υνιφιεδ Ρανκινγ Μοδελ Ι∆-βασεδ Σεθυεντιαλ Μοδυλε Βεηαϖιορ-Τρανσφορµερ Λαψερ 𝐼 Μυλτι-τασκ Μοδυλε 〈〈〈 ⊕ ⊕ Χονχατ + Αδδ ✖ + Εξπερτ 2 〈〈〈 ✖ + ✖ ✖ ✖ ✖ Γατε 1 Γατε 2 ✖ Προδυχτ Σχαλαρ ςεχτορ",
  "3.1 Pre-trained CTR Model": "The purpose of pre-trained CTR model mainly focus on building ID-free recommendation model with CTR as the supervision signal, including three layers, Modality Encoder Layer, BehaviorTransformer Layer and CTR Prediction Layer. information of the clicked item from user search catalog. The text-pairs are then encoded by the text model to get embedding-pair ℎ 𝑥 𝑖 , ℎ 𝑦 𝑖 . The training object is: 3.1.1 Modality Encoder Layer. The high quality of modality representation(image, text) are obtained by Modality Encoder Layer(ME). In this layer, we fine-tuned pre-trained model(BERT and ResNet) using e-commerce data via query matching task and entity prediction task to obtain the image and text representations for the given items. · Query Matching Task : Following SimCSE [8], we use contrastive query matching task to train text model with user feedbacks. Let 𝑥 𝑖 , 𝑦 𝑖 denotes a collection of text-pairs, each pair is formed by a search query and a sequence of textual  where 𝐵 denotes the batch size, 𝜏 is temperature coefficient. · Entity Prediction Task : We use the pre-trained vision model, ResNet-101 [10] as base model. To adapt general vision model for e-commerce recommendation, we fine-tuned ResNet-101 with product prediction task. Giving the image of the product 𝐼 𝑖 , our goal is to predict the key entity in the given image 𝑃 ( 𝐸 | 𝐼 𝑖 ) . The training object is:  WWW'24 Companion, May 13-17, 2024, Singapore, Singapore Yuanbo Gao et al. The fixed-length dense features extracted from the frozen text and vision model are concatenated to represent the modality features of the given items. 3.1.2 Behavior-Transformer Layer. To learn the user preference base on the historical interactions between user and items, we first extract the item representation from modality encoder layer, then we utilize bidirectional transformer to capture contextual information, shown in Fig 1(a). Since the transformer layer itself does not contain position information, the transformer in natural language processing uses cosine position encoding. However, the contextual products in the user sequence contain not only sequence information but also time interval information. The time interval between two user interactions may be only 1 second, or even weeks. We further use position coding to represent the absolute position of the product displayed on the user interface.  Both 𝐸 𝑝𝑜𝑠𝑖𝑡𝑖𝑜𝑛 and 𝐸 𝑟𝑒𝑐𝑒𝑛𝑐𝑦 are random initialized embeddings. Given a sequence of the previous clicked items' modality representation 𝑒 𝑖 ∈ 𝐸 from user catalog ℎ 𝑢 0 , the contextual representation of item 𝑖 is calculated through 𝐿 𝑢 layers of bidirectional transformer blocks:   where Trm denotes bidirectional transformer block, and the input 𝑞 , 𝑘 , 𝑣 are all ℎ 𝑢 . ℎ 𝑢 𝑖 denotes the output hidden representation at the 𝑖 -th transformer encoder layer. The output of last layer is utilized as user representation in our approach. 3.1.3 CTR Prediction Layer. By concatenating the representation of user and target item of the Transformer layer, we then use three fully connected layers to further learn the interactions among the concatenated features, which is standard practice in industrial recomendation systems. To predict whether a user will click the target item 𝑣 𝑡 , we model it as a binary classification problem, thus we use the sigmoid function as the output unit. To train the model, we use the cross-entropy loss  where D represent all the samples, and 𝑦 ∈ { 0 , 1 } is the label representing whether user have click an item or not, 𝑝 ( 𝑥 ) is the output of the network after the sigmoid unit, representing the predicted probability of sample 𝑥 being click.",
  "3.2 Unified Ranking Model": "The pre-trained CTR model is integrated into IDRec model for endto-end training. To accelerate the training process and minimize online latency, we initially cache the multi-modal features, enabling the training of the parameters within the Behavior-Transformer Layer and the CTR Prediction Layer, including ID-based sequential module (IDSM), PPM and Multi-task module. 3.2.1 ID-based Sequential Module. In the setting of sequential module, we are given an item set I and a users interaction sequence 𝑠 = { 𝑖 1 , 𝑖 2 , ..., 𝑖 𝑛 } in temporal order where 𝑛 is the length of 𝑠 and 𝑖 ∈ I . In IDSM, each item 𝑖 is represented by side information of ID type, such as stock keeping unit (SKU) id, shop id, brand id and category id, which are all converted to distinct ids and then encoded by item embedding tables. The bi-directional transformer layer (Trm) are utilized to encode user behavior sequences and model the relationship of user and item. In addition, Trm is not aware of the order of the input sequence. In order to make use of the sequential information of the input, we inject Positional and Recency Embeddings into the input.  | donates concatenation, the output of this module is user interest embeddings and item embeddings based on item's ID features. 3.2.2 PPM. The architecture of the module is the same as ID-based sequential module except the input. In this module, each item 𝑖 is represented by title and image, which are extracted by ME layer. In order to accelerate the training process and minimize online latency, this representations are cached in hdfs, while other parameters are initialized by preloading the pre-trained CTR model.  The output of this module is user interest embeddings and item embeddings based on item's multi-modal features. 3.2.3 Multi-task Module. Our main goal is to predict the probability of the user clicking, placing an order, or collecting (add to cart) the product. There are both correlations and differences between these tasks. For example, users will definitely click on a product before placing an order, but the products they click on are not necessarily for the purpose of purchase, may just be out of curiosity about a newly released mobile phone. We take advantage of Multigate Mixture-of-Experts (MMoE) to model the task relationships and learns task-specific functionalities to leverage shared representations [19]. Given 𝑁 experts and 𝐾 tasks, the output of task 𝑘 ∈ 𝐾 can be formulated as:  where 𝑓 𝑖 ( 𝑥 ) is the output of 𝑖 -th expert, and 𝑔 𝑘 denotes the selfgating combination weight of 𝑘 -th task, can be calculated as follow:  where 𝑈 𝐼 𝐷 , 𝐼 𝐼 𝐷 are the output of IDSM, 𝑈 𝑀𝑂 , 𝐼 𝑀𝑂 are the output of PPM and 𝐷 is other features. 𝑥 ∈ R 𝑁 , 𝑊 𝑘 ∈ R 𝑁 × 𝑁 and 𝑏 𝑘 ∈ R 𝑁 are learnable weight matrix and bias vector of the gating module. After calculating all gating networks and experts, we can obtain the prediction of task k finally:  where 𝑡 𝑘 denotes the tower network of task k. PPM : A Pre-trained Plug-in Model for Click-through Rate Prediction WWW'24 Companion, May 13-17, 2024, Singapore, Singapore",
  "4 EXPERIMENTS": "In this section, we first describe experimental settings and then conduct extensive experiments to answer the following research questions: · RQ1: How do the various state-of-art sequential recommendation models enhanced with the proposed PPM perform? · RQ2: How does each component of PPM perform in predicting the click and order tasks? · RQ3: Does the proposed URM advantage over the traditional CTR model without PPM in mitigating the cold-start issue?",
  "4.1 Dataset": "We leverage real-world production CTR datasets. Each sample in the datasets contains a sequence of user historical behaviors, the target item, and the ground truth label indicating whether or not the user clicked the target item. We use these datasets to evaluate the effectiveness of our proposed model. We first used about 6 months of data for model pre-training. Then, 1 weeks (Small) and 2 weeks (Large) of data were used for training the Unified Ranking model respectively. The details of the three training datasets are shown in Table 1. Table 1: Statistics of the CTR datasets used in the experiments.",
  "4.2 Experimental Settings": "4.2.1 Evaluation metrics. Following the previous work [22, 33, 36], we evaluate the ranking result using two widely used metrics: AUC and Precision@N. · AUC (Area Under the ROC Curve): AUC represents the probability that a random positive example is positioned in front of a random negative example. The higher, the better. · Precision@N : In an information retrieval system that retrieves a ranked list, the top-N documents are the first N in the ranking. Precision at N is the proportion of the top-N documents that are relevant [5]. In the online A/B test, we also employ UCTR (clicks per user per day) and UCVR (orders per user per day) to evaluate the performance of our model.  4.2.2 Compared Methods. We compare the proposed approach with the following baseline methods in Table 2. In this paper, we focus on comparing our method with existing classical user interest modeling methods that mainly model dynamic user behavior from historical interactions. · Wide&Deep [3]: Wide&Deep (W&D) trains a wide linear model and a deep neural model simultaneously for CTR prediction, combining the benefits of memorization and generalization. · DeepFM [9]: To combine the power of traditional FM and deep MLP, DeepFM replaces the LR in the wide network of Wide &Deep with FM to model the 2-order feature interactions. · DIN [20]: DIN is a deep model that employs an attentive neural network to activate related user behaviors with respect to corresponding targets. · DSIN [7]: This is a transformer-based model that uses the transformer and RNN to model the user's intra- and intersession interests separately. · MIAN [30]: MIAN is a deep CTR model that contains a multi-interaction and transformer layer to extract multiple representations of user behavior. 4.2.3 Implementation Details. All models are trained on GPUs(Tesla A100) using Tensorflow2. The batch size is set to 20000. The learning rates for Adam [15] with 𝜂 = 0.0004 to 0.0001. The model parameters are initialized with a Gaussian distribution (with a mean of 0 and a standard deviation of 0.01). The id embedding and multimodal embedding dimensions are set to 64. The layer number of all transformer-encoder and transformer-decoder is 2 and the number of multi-attention heads is 4.",
  "4.3 Performance Comparison (RQ1)": "Table 2 reports the experimental results of the baseline methods and our proposed model on real-world small and large CTR datasets. The best results are highlighted in boldface. Note that the improvement is the absolute AUC/P@2 improvement of models with PPM to the models without PPM. It can be observed that all recommendation models enhanced with PPM outperform the models without PPM, which demonstrates the effectiveness of our proposed model. Besides, we also have the following observations: · Among the conventional methods, W&D and DeepFM perform the worst. In contrast, attention-based methods have better performance for CTR prediction. This verifies the necessity of modeling contextual representation of user behavior sequence. · DSIN outperforms DIN and MAIN, owing to its effectiveness in extracting users' historical behaviors into session interests and modeling the dynamic evolution of session interests. · PPM model exhibits a greater improvement on the small dataset compared to the large dataset, further indicating that the enhancement brought by PPM model is more pronounced in scenarios with sparse data. · Except for URM, the effects of other models trained with PPM on a small dataset outperform the models without PPM trained on a large dataset. The effects are the essentially identical between small dataset and large dataset on URM (In our scenario, the deviations within 0.1% are fluctuations). This indicates that PPM can greatly accelerate the iteration efficiency of URM. WWW'24 Companion, May 13-17, 2024, Singapore, Singapore Yuanbo Gao et al. Table 2: The overall performance comparison with other baseline methods Table 3: The performance of contrast models in terms of AUC and P@2 for click and order tasks.",
  "4.4 Ablation Study (RQ2)": "In this section, we conduct experiments to evaluate the effectiveness of different components in URM. Specifically, we design four contrast models: Fine-tuned BERT and ResNet offer more appropriate representations for text and images in the e-commerce domain. · Base: URM without Query Matching Task and Entity Prediction Task (QM&EP), Pre-trained Plug-in Model (PPM). Thus, the multi-modal features are extracted from primal pre-trained BERT and ResNet by self-supervised learning. Then, the multi-modal features are concatenated with other ID-based features and encoded through a shared transformer. · Base+QM&EP: Base model with QM&EP (URM wo.PPM). · Base+QM&EP+PPM (random initialized): URM with random initialized Plug-in Model. · Base+QM&EP+PPM (frozen): URM with frozen Pre-trained Plug-in Model. · Base+QM&EP+PPM (finetune): This is our proposed unified ranking model (URM). Table 3 shows the performance of contrast models in terms of AUC and P@2 for click and order tasks. URM achieves the best performance in terms of AUC and P@2, which proves the effectiveness of each component of our model. Furthermore, we also have the following observations: · The model with QM&EP significantly outperforms the base model, which proves that fine-tuning modality encoder models with QM&EP tasks play an indispensable part in PPM. · We notice that the model with random initialized PPM outperform others without PPM, indicating that modeling IDbased features and modal-based features with separate transformers is an optimal choice in our scenario. · We discover that models with either frozen or fine-tuned PPM outperform the model with random initialized PPM, which suggests the effectiveness of our proposed pre-trained method for PPM. · We also observe that frozen PPM can degrade the performance of PPM, this may be because PPM needs to adapt for domain-specific data and learn user representations specialized for downstream tasks.",
  "4.5 Performance in Cold Start Setting (RQ3)": "Fig 2 illustrates the performances of our proposed URM and Base model in different settings. Both models are trained on a large dataset. We partitioned the test dataset into different groups based on the appearance frequency of the target item in the training dataset. The lower the appearance frequency of the target item, the more long-tail the test sample represents. The results demonstrate that our URM surpasses the Base model, particularly in long-tail items, thereby validating the effectiveness of URM in mitigating the cold-start issue through the proposed PPM. PPM : A Pre-trained Plug-in Model for Click-through Rate Prediction WWW'24 Companion, May 13-17, 2024, Singapore, Singapore Figure 2: The performances of our proposed URM and Base model in cold-start and warm settings. The shaded region in red represents the observed increase in 𝐴𝑈𝐶 for URM compared to the Base Model. 0-100 101-1000 1001-5000 5001-10000 10001-50000 50001-200000 200000+ Item Frequency 0.70 0.72 0.74 0.76 0.78 0.80 AUC Base URM 0.0000 0.0025 0.0050 0.0075 0.0100 0.0125 0.0150 0.0175 0.0200 AUC Improve",
  "4.6 Online A/B Testing": "The A/B test is conducted on the JD homepage recommendation service for 10 consecutive days, where the baseline model is our last online CTR model which the modality features are regarded as a kind of side information for item. Simultaneously, to make the online evaluation confident and fair, each method deployed for the A/B test has the same number of users. PPM contributes to a 1.09% increase in UCTR, a 0.28% increase in UCVR, and promotes a 10% increase in the exposure ratio of long-tail products. Now PPM has been deployed online and serves the main traffic of users. Figure 3 demonstrates the workflow of the deployed URM, including offline training and incremental updates. During offline training, we utilize large-scale data to train PPM. Then, 50% of the data, which are utilized to train traditional rank model without PPM, are employed to train URM with PPM. In order to ensure the timeliness of the ranking model, we timely update both PPM and URM. PPM is updated by using the past 2 days (T-2) and URM is updated by using the past 1 day (T-1) with the last PPM. After incremental updates, URM is deployed online to ensure users' shopping experience. Figure 3: The offline training and incremental update processes of PPM and URM. PPM URM PPM PPM URM Online Serving PPM Daily Update(T-2) Daily Update(T-1) Offline Training Incremental Updates For ALL For PPM For URM",
  "5 CONCLUSION": "In this paper, we propose a pre-trained plug-in CTR model (PPM) to boost the performance of IDRec in industrial recommender systems. PPM is built on the well-known pre-trained model, utilizing multimodal features (title and image of item) as input and CTR as a supervision signal for pre-training at first. Subsequently, it is loaded by the IDRec model and only a subset of crucial parameters to be trained concurrently. Both offline and online A/B testing results demonstrate the effectiveness of our approach without an increase in online latency.",
  "REFERENCES": "[1] Paul Baltescu, Haoyu Chen, Nikil Pancha, Andrew Zhai, Jure Leskovec, and Charles Rosenberg. 2022. ItemSage: Learning product embeddings for shopping recommendations at pinterest. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 2703-2711. [2] Oren Barkan and Noam Koenigstein. 2016. Item2vec: neural item embedding for collaborative filtering. In 2016 IEEE 26th International Workshop on Machine Learning for Signal Processing (MLSP) . IEEE, 1-6. [3] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems . 7-10. [4] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks for youtube recommendations. In Proceedings of the 10th ACM conference on recommender systems . 191-198. [5] Nick Craswell. [n. d.]. Precision at n. In Encyclopedia of Database Systems , LING LIU and M. TAMER ÖZSU (Eds.). Springer US, 2127-2128. https://doi.org/10. 1007/978-0-387-39940-9_484 [6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018). [7] Yufei Feng, Fuyu Lv, Weichen Shen, Menghan Wang, Fei Sun, Yu Zhu, and Keping Yang. 2019. Deep session interest network for click-through rate prediction. arXiv preprint arXiv:1905.06482 (2019). [8] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. Simcse: Simple contrastive learning of sentence embeddings. arXiv preprint arXiv:2104.08821 (2021). [9] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. arXiv preprint arXiv:1703.04247 (2017). [10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition . 770-778. [11] Ruining He and Julian McAuley. 2016. VBPR: visual bayesian personalized ranking from implicit feedback. In Proceedings of the AAAI conference on artificial intelligence , Vol. 30. [12] Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2015. Session-based recommendations with recurrent neural networks. arXiv preprint arXiv:1511.06939 (2015). [13] Yupeng Hou, Shanlei Mu, Wayne Xin Zhao, Yaliang Li, Bolin Ding, and Ji-Rong Wen. 2022. Towards universal sequence representation learning for recommender systems. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 585-593. [14] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recommendation. In 2018 IEEE international conference on data mining (ICDM) . IEEE, 197-206. [15] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014). [16] Youngjune Lee, Yeongjong Jeong, Keunchan Park, and SeongKu Kang. 2023. MvFS: Multi-view Feature Selection for Recommender System. arXiv preprint arXiv:2309.02064 (2023). [17] Chang Liu, Xiaoguang Li, Guohao Cai, Zhenhua Dong, Hong Zhu, and Lifeng Shang. 2021. Noninvasive self-attention for side information fusion in sequential recommendation. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 35. 4249-4256. [18] Qijiong Liu, Jieming Zhu, Quanyu Dai, and Xiao-Ming Wu. 2022. Boosting Deep CTR Prediction with a Plug-and-Play Pre-trainer for News Recommendation. In Proceedings of the 29th International Conference on Computational Linguistics . International Committee on Computational Linguistics, Gyeongju, Republic of Korea, 2823-2833. https://aclanthology.org/2022.coling-1.249 [19] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. 2018. Modeling task relationships in multi-task learning with multi-gate mixture-ofexperts. In Proceedings of the 24th ACM SIGKDD international conference on WWW'24 Companion, May 13-17, 2024, Singapore, Singapore Yuanbo Gao et al. knowledge discovery & data mining . 1930-1939. [20] Matthew Richardson, Ewa Dominowska, and Robert Ragno. 2007. Predicting clicks: estimating the click-through rate for new ads. In Proceedings of the 16th international conference on World Wide Web . 521-530. [21] Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. 2001. Item-based collaborative filtering recommendation algorithms. In Proceedings of the 10th international conference on World Wide Web . 285-295. [22] Qijie Shen, Hong Wen, Wanjie Tao, Jing Zhang, Fuyu Lv, Zulong Chen, and Zhao Li. 2022. Deep interest highlight network for click-through rate prediction in trigger-induced recommendation. In Proceedings of the ACM Web Conference 2022 . 422-430. [23] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer. In Proceedings of the 28th ACM international conference on information and knowledge management . 1441-1450. [24] Jun Wang, Arjen P De Vries, and Marcel JT Reinders. 2006. Unifying userbased and item-based collaborative filtering approaches by similarity fusion. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval . 501-508. [25] Chuhan Wu, Fangzhao Wu, Tao Qi, Jianxun Lian, Yongfeng Huang, and Xing Xie. 2020. PTUM: Pre-training User Model from Unlabeled User Behaviors via Selfsupervision. In Findings of the Association for Computational Linguistics: EMNLP 2020 , Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, Online, 1939-1944. https://doi.org/10.18653/v1/2020.findings-emnlp. 174 [26] Chuhan Wu, Fangzhao Wu, Yang Yu, Tao Qi, Yongfeng Huang, and Xing Xie. 2021. UserBERT: Contrastive User Model Pre-training. https://doi.org/10.48550/ arXiv.2109.01274 arXiv:2109.01274 [cs]. [27] Chaojun Xiao, Ruobing Xie, Yuan Yao, Zhiyuan Liu, Maosong Sun, Xu Zhang, and Leyu Lin. 2021. UPRec: User-Aware Pre-training for Recommender Systems. https://doi.org/10.48550/ARXIV.2102.10989 [28] Xinyang Yi, Ji Yang, Lichan Hong, Derek Zhiyuan Cheng, Lukasz Heldt, Aditee Ajit Kumthekar, Zhe Zhao, Li Wei, and Ed Chi (Eds.). 2019. Sampling-BiasCorrected Neural Modeling for Large Corpus Item Recommendations . [29] Fajie Yuan, Xiangnan He, Alexandros Karatzoglou, and Liguang Zhang. 2020. Parameter-efficient transfer from sequential behaviors for user modeling and recommendation. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval . 1469-1478. [30] Kai Zhang, Hao Qian, Qing Cui, Qi Liu, Longfei Li, Jun Zhou, Jianhui Ma, and Enhong Chen. 2021. Multi-interactive attention network for fine-grained feature learning in ctr prediction. In Proceedings of the 14th ACM international conference on web search and data mining . 984-992. [31] Zhi-Dan Zhao and Ming-Sheng Shang. 2010. User-based collaborative-filtering recommendation algorithms on hadoop. In 2010 third international conference on knowledge discovery and data mining . IEEE, 478-481. [32] Xiaoyang Zheng, Zilong Wang, Sen Li, Ke Xu, Tao Zhuang, Qingwen Liu, and Xiaoyi Zeng. 2023. MAKE: Vision-Language Pre-training based Product Retrieval in Taobao Search. [33] Zuowu Zheng, Changwang Zhang, Xiaofeng Gao, and Guihai Chen. 2022. HIEN: hierarchical intention embedding network for click-through rate prediction. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval . 322-331. [34] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Deep interest evolution network for click-through rate prediction. In Proceedings of the AAAI conference on artificial intelligence , Vol. 33. 5941-5948. [35] Feng Zhu, Yan Wang, Chaochao Chen, Jun Zhou, Longfei Li, and Guanfeng Liu. 2021. Cross-domain recommendation: challenges, progress, and prospects. arXiv preprint arXiv:2103.01696 (2021). [36] Jieming Zhu, Jinyang Liu, Shuai Yang, Qi Zhang, and Xiuqiang He. 2021. Open benchmarking for click-through rate prediction. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management . 2759-2769.",
  "keywords_parsed": [
    "E-commerce recommendation systems",
    " pre-trained model",
    " CTR",
    " end-to-end training"
  ]
}