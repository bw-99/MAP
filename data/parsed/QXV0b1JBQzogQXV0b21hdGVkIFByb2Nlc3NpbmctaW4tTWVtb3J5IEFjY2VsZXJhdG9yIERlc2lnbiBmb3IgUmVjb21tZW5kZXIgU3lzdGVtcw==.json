{
  "AutoRAC: Automated Processing-in-Memory Accelerator Design for Recommender Systems": "Feng Cheng Duke University Durham, NC, USA feng.cheng@duke.edu Tunhou Zhang Duke University Durham, NC, USA tunhou.zhang@duke.edu Junyao Zhang Duke University Durham, NC, USA junyao.zhang@duke.edu Jonathan Hao-Cheng Ku Duke University Durham, NC, USA jonathan.ku@duke.edu Yitu Wang Duke University Durham, NC, USA yitu.wang@duke.edu Hai 'Helen' Li Duke University Durham, NC, USA hai.li@duke.edu Xiaoxuan Yang University of Virginia Charlottesville, VA, USA xiaoxuan@virginia.edu Yiran Chen Duke University Durham, NC, USA Yiran.chen@duke.edu",
  "Abstract": "The performance bottleneck of deep-learning-based recommender systems resides in their backbone Deep Neural Networks. By integrating Processing-In-Memory (PIM) architectures, researchers can reduce data movement and enhance energy efficiency, paving the way for next-generation recommender models. Nevertheless, achieving performance and efficiency gains is challenging due to the complexity of the PIM design space and the intricate mapping of operators. In this paper, we demonstrate that automated PIM design is feasible even within the most demanding recommender model design space, spanning over 10 54 possible architectures. We propose AutoRAC, which formulates the co-optimization of recommender models and PIM design as a combinatorial search over mixed-precision interaction operations, and parameterizes the search with a one-shot supernet encompassing all mixed-precision options. We comprehensively evaluate our approach on three ClickThrough Rate benchmarks, showcasing the superiority of our automated design methodology over manual approaches. Our results indicate up to a 3.36 Ã— speedup, 1.68 Ã— area reduction, and 12.48 Ã— higher power efficiency compared to naively mapped searched designs and state-of-the-art handcrafted designs.",
  "CCS Concepts": "Â· Hardware â†’ Hardware-software codesign ; Emerging architectures ; Â· Information systems â†’ Recommender systems .",
  "Keywords": "AutoML, Deep Neural Networks, Hardware-Software Co-design, Processing in Memory, Recommender Systems, ReRAM This work is licensed under a Creative Commons Attribution 4.0 International License. GLSVLSI '25, New Orleans, LA, USA Â© 2025 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-1496-2/2025/06 https://doi.org/10.1145/3716368.3735229",
  "ACMReference Format:": "Feng Cheng, Tunhou Zhang, Junyao Zhang, Jonathan Hao-Cheng Ku, Yitu Wang, Xiaoxuan Yang, Hai 'Helen' Li, and Yiran Chen. 2025. AutoRAC: Automated Processing-in-Memory Accelerator Design for Recommender Systems. In Great Lakes Symposium on VLSI 2025 (GLSVLSI '25), June 30July 2, 2025, New Orleans, LA, USA. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/3716368.3735229",
  "1 Introduction": "Advances in recommender systems have focused on enhancing personalization, scalability, and diversity. The incorporation of deep learning techniques [2, 15, 17, 21] has significantly improved the ability to capture and anticipate user preferences, enabling more accurate and tailored recommendations. Processing-in-Memory (PIM) architectures [23, 24, 28] offer promising pathways for next generation recommender models. First, PIM embeds computation within memory units, reducing data movement and improving energy efficiency. Second, PIM leverages emerging memory technologies, such as Resistive Random-Access Memory (ReRAM) and PhaseChange Memory (PCM), which provide higher density and lower latency for large datasets. Consequently, PIM-based solutions are compelling candidates for addressing the challenges faced by modern recommender systems. Nevertheless, we posit that jointly optimizing the recommender system model and the PIM design can yield substantial improvements in system-level performance and efficiency. Our objective is to discover a PIM-friendly recommender model and to devise a dedicated PIM architecture for high-throughput, efficient inference. From the model-search perspective, a thorough exploration of a recommender system should encompass its operators, connections, embedding dimensions, and feature dimensions. From the PIM-design perspective, a chief obstacle arises from the unfixed operands in dot-product layers and factorization machine operations [3], necessitating additional crossbar programming steps during inference. Moreover, modifications to connections and operators during the search phase can significantly influence dataflow GLSVLSI '25, June 30-July 2, 2025, New Orleans, LA, USA Cheng, et al. Figure 1: Overview of AutoRAC framework with search space and evaluation criteria. Input â€¦ Block i Dense 2D Input Sparse 3D Input EFC 8-bit EFC 4-bit DP 8-bit DP 4-bit FC 8-bit FC 4-bit DSI FM Dense 2D Output Sparse 3D Output + + & + Concatenation & Addition + Fixed Connection Searchable Connection Sparse Op Dense Op ReRAM search space FC Block N Block 1 â€¦ ReRAM Configurations Operators, DAC Resolution, Crossbar Size, Memristor Precision, ADC Resolution Evaluation Criteria ï± Test Loss ï± Throughput ï± Area ï± Power and mapping efficiency. Finally, the precision requirements of recommender models make them sensitive to nonidealities in certain analog PIM implementations [26]. Â· We present novel mapping schemes for operations such as dot product and factorization machine, accompanied by a carefully designed pipeline to manage their interconnections effectively. Â· We extensively evaluate our proposed method on three CTR benchmarks, revealing that automated co-design achieves higher throughput, smaller area footprint, and better power efficiency compared to manual design.",
  "2 Background and Related Work": "Deep-Learning Recommender Systems. The remarkable success of deep learning has led to the broad adoption of DNNs [2, 15, 21] over traditional recommender designs [4, 16] for tasks such as Click-Through Rate (CTR) prediction. Recently, NAS [14, 29] has emerged as a powerful methodology for algorithm-level optimization in recommender systems, contributing to end-to-end DNN architecture design [18, 31], feature-interaction selection [13, 30], and embedding-table optimization [33]. However, these approaches do not fully exploit hardware-aware optimizations, overlooking potential efficiency gains that arise from co-designing models and hardware. AutoRAC addresses this gap by simultaneously exploring DNNbackbones and PIM hardware to provide holistic and practical solutions for recommender systems. The rise of Automated Machine Learning (AutoML), particularly Neural Architecture Search (NAS), has driven substantial innovations in designing recommender models by enabling algorithmlevel optimizations [14, 29] and hardware-aware adaptations [12]. While most NAS algorithms have been evaluated on relatively small-scale vision and language benchmarks, adapting NAS to recommender systems introduces unique challenges, especially when co-designing backbone architectures (e.g., DNNs) and specialized hardware (e.g., PIM). Two factors exacerbate these difficulties: first, recommender models require rigorous evaluation protocols, wherein even a minor shift (e.g., 0.2% in Log Loss, or 0.001) can be critical; second, the co-design of deep neural network architectures and PIM hardware for recommender systems remains underexplored, with specialized operations (e.g., factorization machines) requiring dedicated consideration. These issues highlight the necessity of a NAS approach tailored to unlock the potential of joint optimization for PIM-based recommender models. In this paper, we present a framework called AutoRAC, which employs NAS to accelerate recommender systems on PIM. We construct a comprehensive design space that encompasses recommender models and PIM systems, parameterized through a oneshot supernet covering all mixed-precision options. To navigate this complex space, we adopt an evolutionary algorithm. Additionally, we streamline the search space to be hardware-friendly and aligned with PIM dataflow, thereby promoting both efficiency and efficacy in the search process. Our empirical findings showcase the advantages of our NAS-based design strategy and underscore the importance of well-structured dataflow. More crucially, our work provides valuable insights into the interplay among neural architectures and hardware configurations in PIM-based recommender systems. We make the following contributions in this paper. Â· We propose AutoRAC, a holistic methodology for co-optimizing recommender-system architectures and PIM hardware, aiming to enhance overall system performance. Â· We introduce a wide-ranging design space that spans over 10 54 possible architectures, incorporating model structure, quantization, and PIM design, thereby demonstrating the feasibility of automated PIM design under demanding conditions. PIM Designs. PIM architectures harness crossbar-based structures in memory technologies such as ReRAM [27]. As shown in Figure 3a, analog voltages are applied to word lines (WLs) and multiplied by the conductances along each row (Ohm's Law). The resulting currents are then summed along each column (Kirchhoff's Current Law) and read out by circuitry connected to bit lines (BLs). These crossbar arrays thus naturally support matrix-vector multiplication (MVM) [5]. Prior efforts have leveraged PIM parallelism to accelerate recommender systems, achieving promising gains [22, 25]. Nonetheless, these works do not address the unique challenges of PIM-based recommender systems, including inefficient hand-crafted mapping and heuristic-driven hardware design. Consequently, integrating PIM design into a unified search space is vital for delivering end-to-end solutions. AutoRAC tackles this need by constructing an optimized PIM-based recommender system, complete with improved processing engines and automated mapping strategies, ultimately identifying optimal architectures based on defined search criteria. AutoRAC: Automated Processing-in-Memory Accelerator Design for Recommender Systems GLSVLSI '25, June 30-July 2, 2025, New Orleans, LA, USA Table 1: AutoRAC design space construction",
  "3 AutoRAC": "In this section, we introduce AutoRAC, a unified framework designed to jointly optimize DNN backbones and PIM hardware for recommender systems. Figure 1 presents an overview of the AutoRAC workflow. We first detail the AutoRAC design space in Section 3.1, discussing both the DNN backbone and the PIM architecture search. Next, Section 3.2 describes how we map DNN operators to PIM hardware under various design configurations. Subsequently, Section 3.3 outlines the composition of the PIM-based recommender system architecture, which emerges from the combined optimization of the DNN design space and operator mappings. Finally, Section 3.4 elaborates on the automated evolutionary search process that drives the co-design approach in AutoRAC.",
  "3.1 AutoRAC Design Space": "To achieve a holistic co-optimization strategy for recommender systems, AutoRAC expands on two primary axes: the DNN backbone design space and the PIM design space. Table 1 provides an overview of the configuration parameters across these two domains. Within the DNN design space, we target the selection of operators and interconnections vital for accurate and efficient recommendation. Within the PIM design space, we explore quantization techniques and ReRAM parameters that exploit the parallelism of PIM accelerators and effectively control hardware overhead. By encompassing a broad set of reasonable configurations while maintaining search tractability, AutoRAC increases the chance of discovering architectures that excel in both accuracy and efficiency. Â· Dense operators , such as FC and Dot-Product (DP), which output dense tensors. Recommender Model Design Space. We adopt a design space inspired by NASRec [31], adapting it for PIM-oriented dataflows. The model is composed of ğ‘ choice blocks followed by a final Fully-Connected (FC) layer. Each choice block ingests an arbitrary number of dense tensors, ğ‘‹ ğ‘‘ âˆˆ R ğµ Ã— dim ğ‘‘ , and sparse tensors, ğ‘‹ ğ‘  âˆˆ R ğµ Ã— ğ‘ ğ‘  Ã— dim ğ‘  , producing one dense output ğ‘Œ ğ‘‘ and one sparse output ğ‘Œ ğ‘  . The operators are categorized as follows: Â· Sparse operators , for instance, Embedded Fully-Connected (EFC), which preserve sparse output structure. Â· Dense-Sparse interaction operators , which fuse information across dense and sparse branches. For example, a Dense-to-Sparse Figure 2: Test Log Loss on Criteo versus weight bit-width. 5 10 15 20 25 30 -1.0% -0.8% -0.6% -0.4% -0.2% 0.0% Acceptable Accuracy Loss Accuracy Loss (%) BitWidth Merger (DSI) employs FC and reshaping to merge dense outputs into sparse features, while a Factorization Machine (FM) acts as a Sparse-to-Dense Merger. Â· Quantization Design Space. We begin with a 32-bit floating-point representation for both weights and activations, then progressively reduce bit-width. Empirical testing on the Criteo dataset reveals that accuracy remains relatively stable at higher precisions, but begins to degrade sharply if weights drop below 8 bits (see Figure 2). Moreover, lowering activation bit-width further complicates convergence for the supernet in large-scale recommendation tasks. To keep the design space tractable, AutoRAC restricts weight precision to 4 bits or 8 bits for FC, EFC, DSI, DP and FM operators. This choice strikes a balance between hardware efficiency and model fidelity. We exclude 6-bit weight quantization because it is not a power-of-two format, which, according to our experiments, reduces crossbar utilization and tends to hinder overall performance. We permit flexible connections between blocks, subject to the constraint that at least one operator is selected in the dense branch and one operator is selected in the sparse branch. This design ensures that a broad range of architectures, with differing operator orders and topological connections, is included in the search space. PIM Design Space. The PIM design space covers two main aspects: quantization and ReRAM configurations, both of which are crucial to maximizing speedup and energy efficiency in recommender systems. Â· ReRAM Design Space. We tailor the ReRAM design space to meet the stringent low-loss requirements typically required by recommender systems [10]. The configuration of this space is intuited by previous research [26], with adjustments to minimize the impact of the intrinsic nonidealities of ReRAM crossbar arrays. Options for the crossbar size include 16, 32, and 64, while memristor precision and Digital-to-Analog Converter (DAC) resolution are set to 1 or 2 bits. For the Analog-to-Digital Converter (ADC), precision options are 4, 6 and 8 bits. Notably, we only consider combinations of DAC and memristor precision that fall within the maximum ADC resolution range to avoid any loss during the analog-to-digital conversion process. Although this constraint may slightly reduce design space, it is a deliberate choice to ensure that the resulting models exhibit lower loss. The design space is summarized in Table 1. We include the model design space, quantization design space, and ReRAM design space in the table. To simplify the search process, we fixed the number of searchable blocks to ğ‘ = 7, encompassing as many as 2 Ã— 10 54 architectures characterized by significant heterogeneity. Due to the limited use of human-derived priors and this vast, unrestricted search space, exhaustive sampling-based approaches could require an extensive amount of time to identify a cutting-edge model. GLSVLSI '25, June 30-July 2, 2025, New Orleans, LA, USA Cheng, et al. Figure 3: (a) ReRAM crossbar for FC, EFC, and DP. (b) Transposed-write ReRAM crossbar for FM. A0,0[0] A0,0[1] A0,0[2] A0,0[3] A1,0[0] A1,0[1] A1,0[2] A1,0[3] A2,0[0] A2,0[1] A2,0[2] A2,0[3] A3,0[0] A3,0[3] A3,0[2] A3,0[3] B0[3], â€¦,B0[0] B1[3], â€¦,B1[0] B2[3], â€¦,B2[0] B3[3], â€¦,B3[0] Shared ADCs Shift & Add Write Driver A0,0[0] A0,0[1] A0,0[2] A0,0[3] A0,1[0] A0,1[1] A0,1[2] A0,1[3] A0,2[0] A0,2[1] A0,2[2] A0,2[3] A0,3[0] A0,3[3] A0,3[2] A0,3[3] B0[3], â€¦,B0[0] B1[3], â€¦,B1[0] B2[3], â€¦,B2[0] B3[3], â€¦,B3[0] Shared ADCs Shift & Add Write Driver (b) (a)",
  "3.2 Operators Mapping": "In this section, we illustrate the mapping between DNN operators and PIM hardware to design PIM-based recommender models. For ease of explanation, we set the batch size to one. DPLayer. The DP layer captures feature interactions by computing pairwise inner products across multi-modal inputs, accommodating both dense and sparse feature representations. Its design involves four components: an FC layer for mapping the dense dimension ( ğ‘‘ğ‘–ğ‘š ğ‘‘ ) to the sparse dimension ( ğ‘‘ğ‘–ğ‘š ğ‘  ), an EFC layer for projecting the number of sparse features ( ğ‘ ğ‘  ) to âˆšï¸ 2 Ã— ğ‘‘ğ‘–ğ‘š ğ‘‘ , a dedicated DP engine for inner products, and a final FC layer for projecting the concatenated result to the desired output dimension. FC Layer, EFC Layer, and DSI. FC layers and EFC layers are pivotal for generating dense representations and constructing sparse features in recommender systems. The DSI is functionally an FC layer followed by a reshaping operation, enabling the transition of feature representations from a dense format to a sparse one. In contrast, an EFC layer performs sparse computations along a middle dimension as ğ‘Œ ğ‘  = ğ‘Š ğ‘  ğ‘‹ ğ‘  , where ğ‘Š ğ‘  âˆˆ R N ğ‘–ğ‘› Ã— N ğ‘œğ‘¢ğ‘¡ is the weight matrix, ğ‘‹ ğ‘  âˆˆ R N ğ‘–ğ‘› Ã— dim ğ‘  is the input tensor, and ğ‘Œ ğ‘  âˆˆ R N ğ‘œğ‘¢ğ‘¡ Ã— dim ğ‘  is the output tensor. Both FC and EFC layers essentially perform vector-matrix or matrix-matrix multiplications, which map naturally onto ReRAM crossbars. As illustrated in Figures 4a and 4b, these layers are implemented in PIM by programming the weight matrix ğ‘Š onto the crossbar and sequentially feeding the bits of input vector ğ‘‹ to the word lines. The ReRAM crossbars then perform MVMs, producing the output tensor ğ‘Œ . For simplicity, Figure 4 omits the explicit visualization of individual bits. Initially, the dense input is reshaped via an FC layer to match the sparse dimension, and the sparse features are simultaneously reduced through an EFC layer to âˆšï¸ 2 Ã— ğ‘‘ğ‘–ğ‘š ğ‘‘ for balanced operator workloads. The outputs of these two transformations are merged into a single tensor ğ‘‹ âˆˆ R ( âˆš 2 Ã— ğ‘‘ğ‘–ğ‘š ğ‘‘ + 1 ) Ã— ğ‘‘ğ‘–ğ‘š ğ‘  , which undergoes pairwise inner products computed as Triu ( ğ‘‹ğ‘‹ ğ‘‡ ) . The flattened inner-product results are then passed through an FC layer, yielding the final output. Mapping of the FC and EFC submodules onto ReRAM crossbars remains consistent with the approach outlined for FC/EFC layers. Figure 4c highlights the additional steps required for the DP engine. Specifically, each output vector from the EFC and FC layers is buffered and programmed onto crossbars. Meanwhile, the EFC layer generates the next vector output, enabling a pipelined, overlap-friendly process that avoids unnecessary waiting. Because the sparse output from the EFC layer is inherently transposed, ğ‘‹ ğ‘‡ can be programmed directly. Once the sparse feature matrix is fully produced, each feature vector is loaded into the crossbars to compute partial dot-product results, which are concatenated into the DP layer's final output. This output subsequently flows into an FC layer, which projects it to the final dense dimension ğ‘‘ğ‘–ğ‘š d_out . We propose a novel PIM mapping for the FM operator, focusing on the square of the sum and the sum of the squares. Figure 4d provides an overview of this process. To calculate the square of the sum, the outputs of the EFC layer are programmed into the columns of a transposed ReRAM array [20] (Figure 3b). Unlike traditional crossbar architectures that program sparse outputs row by row, this transposed layout aligns spatially with the inputs and eliminates idle buffers. Once all sparse outputs are programmed, a vector of ones is supplied to the word lines to accumulate each column's sum. Next, element-wise multiplication is performed on these sum vectors using the MBSA [34] module (Figure 4e). First, the sum vector is programmed; then each bit is sent in parallel to the MBSA's AND gates. Iterating this process across every bit ultimately produces the square of the sum. Sparse-to-Dense FM Layer. The FM layer consists of two components: an FM engine for converting a 3D sparse representation into a dense vector, followed by an fully-connected layer for mapping this dense vector to the desired output size. The FM engine processes a batch of three-dimensional sparse tensors by computing GLYPH<0>Ë ğ‘› ğ‘– = 1 x ğ‘– GLYPH<1> 2 and Ë ğ‘› ğ‘– = 1 x 2 ğ‘– . Here, x ğ‘– denotes each embedding in the sparse feature set. The interaction term ix = GLYPH<0>Ë ğ‘› ğ‘– = 1 x ğ‘– GLYPH<1> 2 -Ë ğ‘› ğ‘– = 1 x 2 ğ‘– results from subtracting the sum of squares from the square of the sum, effectively capturing pairwise feature interactions. The sum of squares is computed in parallel by directly programming each vector output of the EFC layer onto the transposed crossbar. Because each word line receives an identical vector, each row of the crossbar naturally yields a squared value. These values are then summed along the bit lines, delivering the aggregate sum of squares. Critically, the operations for the sum of squares and the square of the sum can be performed concurrently, leveraging the transposed array's capability for full data pipelining. Afterwards, the difference between these two computations is fed into the final FC layer implemented on the ReRAM crossbars, projecting the result to the designated output dimension. This integrated procedure expedites throughput and reduces latency, culminating in an efficient factorization mechanism on PIM.",
  "3.3 Architecture Overview": "We illustrate the overall system architecture in Figure 4f. This design integrates memory tiles for storing embedding tables alongside computation tiles for operator execution. The memory tiles hold embedding tables in a static, read-only state, and an offline accessaware mechanism reorganizes embeddings by their frequency of occurrence, placing them in round-robin fashion across different banks to avoid conflicts. Meanwhile, the computation tiles are partitioned into three dedicated engines: the FM engine, DP engine, and MVMengine. Each engine hosts a crossbar array with its peripheral circuitry and I/O registers, mirroring the PIM functionality detailed in the preceding sections. Additionally, each tile contains a data buffer for intermediate outputs and a functional unit responsible for AutoRAC: Automated Processing-in-Memory Accelerator Design for Recommender Systems GLSVLSI '25, June 30-July 2, 2025, New Orleans, LA, USA Figure 4: Overview of AutoRAC mapping schemes and architecture design. dims â€¦ ReRAM Crossbars â€¦ â€¦ â€¦ â€¦ â€¦ Cycles Nout +1          2    1 Nout Buffer â€¦ â€¦ 1 Nout dims â€¦ â€¦ Nout (Nout +1)/2 ReRAM Crossbars Cycles 1 1 dimd_out â€¦ â€¦ DP Layer Cycles dims 2     1 Nout ReRAM Crossbars â€¦ Nin dims dims 2 1 EFC Layer â€¦ ReRAM Crossbars FC Layer Cycles 1 1 1 dimd_in dimd_out â€¦ dims â€¦ Nout 1 1 1 1 â€¦ Cycles 1 â€¦ Nout Transposed ReRAM Crossbars Cycles 1 dims â€¦ â€¦ dims â€¦ ReRAM Crossbars MBSA â€¦ â€¦ Square of Sum Sum of Square - - â€¦ FM = = dims ReRAM Crossbars Cycles 1 1 dimd_out â€¦ â€¦ FM Layer BL[0] BL[1] BL[2] BL[3] I ref SA SA SA SA AND B[i] A[3:0] * B[i] SHIFT & ADD A * B MBSA Cycles 1 (a) (b) (c) (d) Transposed ReRAM Crossbars (e) Memory Tile Memory Tile Memory Tile Compute Tile In Reg Peripherals FM Engine Out Regs â€¦ â€¦ Transposed ReRAM Crossbars In Reg Peripherals FM Engine Out Regs â€¦ â€¦ Transposed ReRAM Crossbars In Reg Peripherals FM Engine Out Regs â€¦ â€¦ Transposed ReRAM Crossbars In Reg Peripherals DP Engine Out Regs â€¦ â€¦ ReRAM Crossbars In Reg Peripherals DP Engine Out Regs â€¦ â€¦ ReRAM Crossbars In Reg Peripherals DP Engine Out Regs â€¦ â€¦ ReRAM Crossbars ReRAM Crossbars In Reg Peripherals FC Engine Out Regs ReRAM Crossbars In Reg Peripherals FC Engine Out Regs ReRAM Crossbars In Reg Peripherals FC Engine Out Regs Bus Local Data Buffer Scheduler & Controller Functional Unit Compute Tile In Reg Peripherals FM Engine Out Regs â€¦ â€¦ Transposed ReRAM Crossbars In Reg Peripherals DP Engine Out Regs â€¦ â€¦ ReRAM Crossbars ReRAM Crossbars In Reg Peripherals FC Engine Out Regs Bus Local Data Buffer Scheduler & Controller Functional Unit Compute Tile In Regs Peripherals FM Engine Out Regs â€¦ â€¦ Transposed ReRAM Crossbars In Regs Peripherals DP Engine Out Regs â€¦ â€¦ ReRAM Crossbars ReRAM Crossbars In Regs Peripherals FC Engine Out Regs Bus Local Data Buffer Scheduler & Controller Functional Unit (f) activation functions. A controller and a scheduler coordinate the data flow, ensuring all pipeline stages run smoothly and efficiently. from scratch, and the best-performing one is selected as the final architecture.",
  "3.4 Evolutionary Search": "The step-by-step search procedure is outlined in Algorithm 1. We adopt a regularized-evolution framework to identify the optimal subnet configuration within the AutoRAC design space. On the model side, each iteration selects one parent configuration, then applies a series of actions within a chosen block, such as swapping dense/sparse operators, modifying dense/sparse dimensions, adjusting block-to-block connections, or introducing dense-sparse interaction layers. Concurrently, the PIM design space is explored using a similar evolutionary process, but with mutations specialized to toggling among different ADC resolutions, DAC options, memristor precisions, and crossbar sizes. This targeted mutation scheme is crucial for uncovering architectures that align with performance and efficiency requirements in PIM-based recommender systems. By systematically switching these hardware parameters, we can comprehensively evaluate and refine the PIM design under diverse computational demands and constraints.",
  "4 Evaluation": "",
  "4.1 Experiment setup and Benchmark": "Experiment Setup. To model on-chip buffers, we use CACTI [1] at a 32 nm technology node. For ReRAM characterization, we follow the parameters in MNSIM2.0 [35] to obtain precise estimates of area, latency, and power consumption. We develop a behavioral simulator to further analyze end-to-end latency and throughput. Although our primary exploration and performance simulations are executed on an Intel Xeon Gold 6254 platform, we leverage an NVIDIA A5000 GPU to accelerate the co-exploration process. Recommender Model Benchmarks. Weconduct empirical evaluations on three widely used CTR benchmarks: Criteo [8], Avazu [7], and KDD Cup 2012 [6]. The datasets are preprocessed following the same protocol used in NASRec [31]. After preprocessing, each dataset is split into a training set (80%), a validation set (10%), and a test set (10%). During the AutoRAC search phase, we train a supernet on the training set and identify the top 15 subnets based on validation performance. These 15 subnets are each retrained",
  "4.2 Experiment Results": "Model Accuracy. Table 2 compares AutoRAC with both handcrafted and NAS-crafted baselines on three widely used CTR benchmarks: Criteo, Avazu, and KDD Cup 2012. Two central metrics, Log Loss (lower is better) and AUC (higher is better), are used to quantify predictive performance. On the Criteo dataset, AutoRAC achieves a Log Loss of 0.4397 and an AUC of 0.8116. Although these values may appear close to those of existing models, even a 0.001 reduction in Log Loss can yield notable gains in practical recommender systems. Notably, AutoRAC surpasses NASRec and outperforms hand-crafted approaches such as DLRM and DeepFM. For the Avazu dataset, AutoRAC sustains its strong results with a Log Loss of 0.3736 and an AUC of 0.7906, outperforming NASRec and highlighting its reliable predictive power across diverse user-item interactions. On the KDD Cup 2012 dataset, AutoRAC",
  "Algorithm 1 Best Subnet Config Search in AutoRAC": "Require: Design targets GLYPH<2> 1 throughput , area , power GLYPH<3> denoted by GLYPH<2> target 1 , target 2 , target 3 GLYPH<3> 1: ğ‘ğ‘™ğ‘™ _ ğ‘ğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘  â† random_search ( ğ‘ ğ‘¢ğ‘ğ‘’ğ‘Ÿğ‘›ğ‘’ğ‘¡ ) 2: for ğ‘”ğ‘’ğ‘›ğ‘’ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘œğ‘› â† 1 to ğ‘›ğ‘¢ğ‘š _ ğ‘”ğ‘’ğ‘›ğ‘’ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘  do 3: ğ‘ğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ â† Sample_and_select ( ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘’ğ‘Ÿğ‘–ğ‘œğ‘›, ğ‘ğ‘™ğ‘™ _ ğ‘ğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘  ) 4: for ğ‘â„ğ‘–ğ‘™ğ‘‘ â† 1 to ğ‘›ğ‘¢ğ‘š _ ğ‘â„ğ‘–ğ‘™ğ‘‘ğ‘Ÿğ‘’ğ‘› do 5: ğ‘â„ğ‘œğ‘–ğ‘ğ‘’ â† ğ‘ğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ 6: for ğ‘šğ‘¢ğ‘¡ğ‘ğ‘¡ğ‘–ğ‘œğ‘› â† 1 to ğ‘›ğ‘¢ğ‘š _ ğ‘šğ‘¢ğ‘¡ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘  do 7: ğ‘â„ğ‘œğ‘–ğ‘ğ‘’ â† Mutate ( ğ‘â„ğ‘œğ‘–ğ‘ğ‘’ ) 8: end for 9: test_loss â† finetune_and_eval_loss ( ğ‘â„ğ‘œğ‘–ğ‘ğ‘’ ) 10: metric â† hw_ea ( ğ‘â„ğ‘œğ‘–ğ‘ğ‘’ ) 11: ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘’ğ‘Ÿğ‘–ğ‘œğ‘› â† test_loss + Ë 3 ğ‘– = 1 ğœ† ğ‘– metric ğ‘– target ğ‘– 12: append ( ğ‘â„ğ‘œğ‘–ğ‘ğ‘’, ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘’ğ‘Ÿğ‘–ğ‘œğ‘› ) to ğ‘ğ‘™ğ‘™ _ ğ‘ğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘  13: end for 14: sort ğ‘ğ‘™ğ‘™ _ ğ‘ğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘  by ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘’ğ‘Ÿğ‘–ğ‘œğ‘› 15: remove last ğ‘›ğ‘¢ğ‘š _ ğ‘â„ğ‘–ğ‘™ğ‘‘ğ‘Ÿğ‘’ğ‘› entries 16: end for GLSVLSI '25, June 30-July 2, 2025, New Orleans, LA, USA Cheng, et al. further validates its effectiveness by posting a Log Loss of 0.1489 and an AUC of 0.8160, ranking first in both metrics. These outcomes collectively underscore the model's adaptability to varied data distributions and its capacity for capturing vital feature interactions through automated architecture search. Table 2: Performance of AutoRAC on CTR Tasks. Hardware Performance . Table 3 summarizes the hardware performance of AutoRAC in comparison with a CPU baseline, a naively mapped NASRec [32] design, and two handcrafted accelerators, RecNMP [9] and ReREC [22]. Three principal metrics are used in this comparison: speedup, power efficiency, and area savings. When measured against the CPU, AutoRAC achieves a 22.83 Ã— speedup while also improving power efficiency by 66.87 Ã— , indicating that significant acceleration can be realized by leveraging specialized PIM hardware alongside the automatically discovered DNN architecture. This high degree of hardware-software co-optimization effectively reduces memory transfers and accelerates computations inherent in recommender systems. In comparison with the naively mapped NASRec, AutoRAC demonstrates a 3.17 Ã— speedup and achieves 2.39 Ã— higher power efficiency, complemented by a 1.68 Ã— reduction in area. These improvements highlight the importance of searching for hardware-friendly operator configurations in tandem with the model design. By reducing mismatches between dataflow patterns and physical crossbar layouts, AutoRAC avoids many of the inefficiencies encountered in naive mappings. The comparison against state-of-the-art handcrafted designs shows that AutoRAC remains highly competitive. In relation to RecNMP, AutoRAC attains a 12.48 Ã— improvement in power efficiency and a 3.36 Ã— speedup. Compared with ReREC, AutoRAC displays a 1.57 Ã— gain in power efficiency and a 1.28 Ã— speedup. The results underscore how a systematic, search-based approach can either match or exceed manually optimized accelerators by jointly refining both algorithmic and architectural choices. Table 3: Hardware metrics of AutoRAC against baselines. Search Efficiency . Figure 5 illustrates the evolution of the performance criterion across 240 generations during the search. The criterion begins with a rapid decline of over 10% within the first 50 generations, indicating that the search strategy quickly identifies promising model-hardware configurations and discards less suitable ones. After this initial period of rapid improvement, the curve plateaus, suggesting that the search converges to top-performing candidates, with only incremental gains observed. Around the 150th generation, another period of gradual performance increase emerges, indicating that although the search has discovered strong solutions, further exploration may reveal moderately better architectures. After 200 generations, the curve stabilizes and shows minimal further decrease, signifying that the algorithm has effectively exploited the design space to discover high-quality solutions. Figure 5: Percentage drop of criterion (Lower is better). 0 50 200 250 100 150 17.5% 15.0% 12.5% 10.0% 7.5% 5.0% 2.5% 0.0% % Drop of Criterion Criterion Best Model Discovered . Figure 6 depicts the best-performing architecture discovered on the Criteo dataset. This architecture reveals several noteworthy design trends. The EFC layers are predominantly 8-bit, a choice that likely results from their comparatively smaller parameter size, allowing them to maintain better precision without incurring an excessive resource burden. The FC layers in the middle of the network typically use 4-bit precision, an allocation that effectively balances computational overhead and accuracy in intermediate stages. In contrast, the initial and final FC layers generally adopt 8-bit precision, indicating that retaining more fine-grained details in the early and late phases of the network is beneficial for preserving critical information. The DP layers do not show a strong preference for any particular bit-width, suggesting that the design of these interaction-oriented modules can flexibly align with either higher or lower precision. Overall, the architecture discovered by AutoRAC strikes a nuanced balance between 4-bit and 8-bit operators, demonstrating the effectiveness of automatically searching for models that adapt precision settings to the computational needs of different layers. Figure 6: Best model discovered from AutoRAC. Sparse in Dense in EFC 8-bit DP 8-bit EFC 8-bit DP 8-bit EFC 8-bit DP 4-bit EFC 8-bit FC 4-bit EFC 8-bit DP 4-bit EFC 8-bit FC 8-bit EFC 8-bit FC 8-bit d=768/s=48 1 d=32/s=32 2 d=256/s=32 4 d=32/s=48 6 d=512/s=64 7 d=256/s=32 5 d=512/s=48 3 Dense/Sparse block Dense/Sparse dimension d  /  s s Dense-Sparse Merger",
  "5 Conclusion": "This work demonstrates the practicality of automating processingin-memory (PIM) design for large-scale recommender models. The proposed framework, AutoRAC, casts the joint optimization of DNN architectures and PIM hardware as a mixed-precision search over a one-shot supernet. Experimental results show improvements of up to 3.4 Ã— in speed, 1.7 Ã— reduction in silicon area, and 12.5 Ã— higher power efficiency compared with naÃ¯ve mappings and stateof-the-art handcrafted baselines, underscoring the benefits of unified neural-architecture and hardware exploration.",
  "Acknowledgments": "The work was funded in part by National Science Foundation NSF2328805 and NSF-2112562. AutoRAC: Automated Processing-in-Memory Accelerator Design for Recommender Systems GLSVLSI '25, June 30-July 2, 2025, New Orleans, LA, USA",
  "References": "[1] Rajeev Balasubramonian, Andrew B. Kahng, Naveen Muralimanohar, Ali Shafiee, and Vaishnav Srinivas. 2017. CACTI 7: New Tools for Interconnect Exploration in Innovative Off-Chip Memories. ACM Trans. Archit. Code Optim. 14, 2 (2017), 14:1-14:25. [3] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. In Proceedings of the 26th International Joint Conference on Artificial Intelligence (Melbourne, Australia) (IJCAI'17) . AAAI Press, 1725-1731. [2] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and Hemal Shah. 2016. Wide & Deep Learning for Recommender Systems. In Proceedings of the 1st Workshop on Deep Learning for Recommender Systems (Boston, MA, USA) (DLRS 2016) . Association for Computing Machinery, New York, NY, USA, 7-10. https://doi.org/10.1145/2988450.2988454 [4] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural Collaborative Filtering. In Proceedings of the 26th International Conference on World Wide Web (Perth, Australia) (WWW'17) . International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, CHE, 173-182. https://doi.org/10.1145/3038912.3052569 [6] Kaggle. 2012. KDD Cup 2012 Track2. https://www.kaggle.com/c/kddcup2012track2/data. [5] Miao Hu et al. 2014. Memristor crossbar-based neuromorphic computing system: A case study. IEEE transactions on neural networks and learning systems 25, 10 (2014), 1864-1878. [7] Kaggle. 2014. Avazu CTR Prediction. https://www.kaggle.com/c/avazu-ctrprediction/data. [9] Liu Ke, Udit Gupta, Carole-Jean Wu, Benjamin Youngjae Cho, Mark Hempstead, Brandon Reagen, Xuan Zhang, David Brooks, Vikas Chandra, Utku Diril, Amin Firoozshahian, Kim Hazelwood, Bill Jia, Hsien-Hsin S. Lee, Meng Li, Bert Maher, Dheevatsa Mudigere, Maxim Naumov, Martin Schatz, Mikhail Smelyanskiy, and Xiaodong Wang. 2019. RecNMP: Accelerating Personalized Recommendation with Near-Memory Processing. arXiv:1912.12953 [cs.DC] [8] Kaggle. 2014. Criteo Display Ad Challenge. https://www.kaggle.com/c/criteodisplay-ad-challenge. [10] Zelong Li, Jianchao Ji, Yingqiang Ge, and Yongfeng Zhang. 2022. AutoLossGen: Automatic Loss Function Generation for Recommender Systems. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (Madrid, Spain) (SIGIR '22) . Association for Computing Machinery, New York, NY, USA, 1304-1315. https://doi.org/10.1145/3477495. 3531941 [12] Yujun Lin, Mengtian Yang, and Song Han. 2022. NAAS: Neural Accelerator Architecture Search. In Proceedings of the 58th Annual ACM/IEEE Design Automation Conference (San Francisco, California, United States) (DAC '21) . IEEE Press, 1051-1056. https://doi.org/10.1109/DAC18074.2021.9586250 [11] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong Sun. 2018. xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (London, United Kingdom) (KDD '18) . Association for Computing Machinery, New York, NY, USA, 1754-1763. https://doi.org/10.1145/3219819.3220023 [13] Bin Liu, Chenxu Zhu, Guilin Li, Weinan Zhang, Jincai Lai, Ruiming Tang, Xiuqiang He, Zhenguo Li, and Yong Yu. 2020. AutoFIS: Automatic Feature Interaction Selection in Factorization Models for Click-Through Rate Prediction. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (Virtual Event, CA, USA) (KDD '20) . Association for Computing Machinery, New York, NY, USA, 2636-2645. https://doi.org/10.1145/ 3394486.3403314 [15] Maxim Naumov et al. 2019. Deep learning recommendation model for personalization and recommendation systems. arXiv preprint arXiv:1906.00091 (2019). [14] Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2018. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055 (2018). [16] Steffen Rendle. 2012. Factorization machines with libfm. ACM Transactions on Intelligent Systems and Technology (TIST) 3, 3 (2012), 1-22. [17] Xiang-Rong Sheng, Liqin Zhao, Guorui Zhou, Xinyao Ding, Binding Dai, Qiang Luo, Siran Yang, Jingshan Lv, Chi Zhang, Hongbo Deng, and Xiaoqiang Zhu. 2021. One Model to Serve All: Star Topology Adaptive Recommender for Multi-Domain CTR Prediction. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management (Virtual Event, Queensland, Australia) (CIKM '21) . Association for Computing Machinery, New York, NY, USA, 4104-4113. https://doi.org/10.1145/3459637.3481941",
  "keywords_parsed": [
    "AutoML",
    " Deep Neural Networks",
    " Hardware-Software Co-design",
    " Processing in Memory",
    " Recommender Systems",
    " ReRAM This work is licensed under a Creative Commons Attribution 4.0 International License. GLSVLSI '25",
    " New Orleans",
    " LA",
    " USA Â© 2025 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-1496-2/2025/06 https://doi.org/10.1145/3716368.3735229"
  ]
}