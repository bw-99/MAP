{"Ads Supply Personalization via Doubly Robust Learning": "Wei Shi Meta Platforms, Inc. Sunnyvale, CA, USA weishi0079@meta.com Chen Fu Meta Platforms, Inc. Sunnyvale, CA, USA chenfu@meta.com Qi Xu Meta Platforms, Inc. Sunnyvale, CA, USA xuqi@meta.com Sanjian Chen Meta Platforms, Inc. Menlo Park, CA, USA sjchen@meta.com Jizhe Zhang Meta Platforms, Inc. Menlo Park, CA, USA jizhezhang@meta.com Zhigang Hua Meta Platforms, Inc. Sunnyvale, CA, USA zhua@meta.com Qinqin Zhu Meta Platforms, Inc. Menlo Park, CA, USA catherinezhu@meta.com Shuang Yang Meta Platforms, Inc. Sunnyvale, CA, USA shuangyang@meta.com", "Abstract": "Ads supply personalization aims to balance the revenue and user engagement, two long-term objectives in social media ads, by tailoring the ad quantity and density. In the industry-scale system, the challenge for ads supply lies in modeling the counterfactual effects of a conservative supply treatment (e.g., a small density change) over an extended duration. In this paper, we present a streamlined framework for personalized ad supply. This framework optimally utilizes information from data collection policies through the doubly robust learning. Consequently, it significantly improves the accuracy of long-term treatment effect estimates. Additionally, its low-complexity design not only results in computational cost savings compared to existing methods, but also makes it scalable for billion-scale applications. Through both offline experiments and online production tests, the framework consistently demonstrated significant improvements in top-line business metrics over months. The framework has been fully deployed to live traffic in one of the world's largest social media platforms.", "CCS Concepts": "\u00b7 Information systems \u2192 Personalization .", "Keywords": "Advertisement; Causal Learning; Doubly Robust Learning", "ACMReference Format:": "Wei Shi, Chen Fu, Qi Xu, Sanjian Chen, Jizhe Zhang, Qinqin Zhu, Zhigang Hua, and Shuang Yang. 2024. Ads Supply Personalization via Doubly Robust Learning. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management (CIKM '24), October 21-25, 2024, Boise, ID, USA. ACM, New York, NY, USA, 8 pages. https: //doi.org/10.1145/3627673.3680035 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). CIKM '24, October 21-25, 2024, Boise, ID, USA \u00a9 2024 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-0436-9/24/10 https://doi.org/10.1145/3627673.3680035", "1 Introduction": "Optimized revenue gain / Engagement loss Total Ad-load Total Engagement Total Ad-load Total Revenue Ad-load assignment policy Total Ad-load Total Revenue Total Engagement Individual revenue Individual engagement Individual revenue Individual engagement t 0 t 0 +T Individual revenue Individual engagement User A User B Individual revenue Individual engagement Time T Ad-load increase A Ad-load increase B Insensitive user Sensitive user", "Figure 1: Systematic overview of ads-supply personalization.": "Social advertising platforms generate revenue by ads and user engagement by organic content. Balancing user engagement and revenue generation is important. Typically, boosting ad delivery enhances short-term revenue but may lead to a decline in user engagement, impacting long-term revenue prospects adversely. Addressing the optimal trade-offs between these conflicting objectives represents a significant challenge in the realm of online advertising. Personalization is one of the important strategies that has proven to be effective in boosting revenue and engagement separately, typically by customizing content selection and ranking. Yet, the separate personalization strategy does not take into account the often-conflicting nature and intricate interplay between the two objectives (i.e, revenue and engagement ). To attain optimal tradeoffs, there is a crucial need for a joint personalization strategy that delves deeper into customizing the blending algorithm itself. Along this line, existing efforts have largely focused on ads allocation, specifically by customizing the positions of ads and organic content, for example, via constrained optimization [31] and Markov CIKM '24, October 21-25, 2024, Boise, ID, USA Wei Shi et al. decision process (MDP) [14, 15, 28, 30, 34]. However, there still exists a significant gap in research concerning the optimization of ad quantity and density, which directly influence top-line revenue and engagement metrics, making further exploration imperative from an industry perspective. In this paper, we examined a personalized strategy designed to control the level of ad-load to balance revenue and user engagement, termed as ads supply personalization . The effectiveness of personalization arises from tailoring ad load to individual users according to their perceived value of the ads content, rather than applying an one-size-fits-all configuration. Such heterogeneous ads perceptions are quantified by users responses (e.g, revenue gain and engagement loss) from an ad-load treatment (e.g, an increased load level). We define users with high revenue increase and low engagement loss as insensitive users. A selective ad-load increase with insensitive users well identified can significantly improve the cumulative revenue gain-to-engagement loss efficiency over a uniform ad-load increase as shown in Figure 1. In the billion-scale industrial applications, the key challenge of ads supply personalization is to build a model for users' long-term causal effects from an ad-load change. Long-term ads-supply effects are difficult to estimate because of the small-magnitude treatment and complicated observations in a long time window. Firstly, adload changes, the treatments, are usually small to be conservative since they directly impact top-line business metrics. As a result, the impact (i.e, to revenue and engagement metrics) is so tiny that distinguishing it from background noise becomes a real challenge. Secondly, to measure long-term impact, user responses are often monitored over an extended period (e.g., months) to ensure consistency, which introduces additional challenges and complexities. For example, observations on revenue and engagement can easily be overshadowed by various unrelated factors such as seasonality and ongoing changes made into production (e.g., the launch of new ranking models). Furthermore, ads supply personalization should be low-latency and less computationally intensive to ensure minimal additional infrastructure costs on top of the established ads and organic content delivery systems. Existing works either directly reuse estimated utilities [3, 31] like ads-CTR from ranking models or build the model on users' immediate and short-term signals [15, 16, 29]. Modeling long-term user effect, which is crucial for ads supply personalization, remains largely unexplored. These models for short-term causal effect modeling are also difficult to scale up as they are already computationally demanding due to deployments of dedicated deep neural networks (DNN) [16, 29]. In this paper, we present a streamlined framework based on doubly robust learner (DRL), which provides a lightweight solution to model the long-term causal effects in large-scale ads supply personalization. The improved design on utilization of information during data collection and modeling stages leads to substantial enhancements in performance and reductions in model complexity. Our framework can be easily integrated with separate ad and organic delivery systems, resulting in minimal increases in computational complexity. This framework offers a comprehensive optimization across stages of data collection, modeling, and deployment, grounded in a theory from causal inference. The contributions of our work are summarized as follows: \u00b7 Alightweight ads-supply personalization for long-term metrics. In this paper, we propose a new framework based on DRL to model the long-term causal effects from ad-load changes. The framework fully leverages the information of random-controlled trial (RCT) data to significantly improve the quality of treatment effect estimates at a reduced model complexity. We present detailed analysis under DRL formulation and design a lightweight implementation for the large-scale application correspondingly. \u00b7 Detailed industrial and practical experience. We conducted extensive empirical studies on the public and realworld benchmarks. The proposed system and the reported improvements have been fully deployed on with live traffic in one of the world's largest social media platforms.", "2 Preliminaries and Related Works": "", "2.1 Uplift Modeling": "Uplift models have been extensively used in heterogeneous treatment effect (HTE) or conditional average treatment effect (CATE), which is widely applicable in medical sciences, psychology, sociology, and economics. Following the Neyman-Rubin potential outcome framework [21], the observed responses are the outcome and their incremental changes of outcome are the treatment effect. There are three major categories of uplift models. (1) Causal Trees. Tree-based models divide the user population using different splitting criteria, such as distribution divergences [18] and expected responses [22, 35]. The extension of causal trees is causal forest [2]. (2) Meta-learner. Meta-learner [13] tackles the counterfactual problem by predicting the potential outcomes. (3) Deep Neural Network (DNN) causal models. Strong predictive performance and representation power of DNNs motivate researchers to work on deep causal models [1, 8, 11, 17, 23, 24, 33]. The consistency of treatment estimates from meta-learner and deep causal models highly rely on the consistent outcome models. We will show that it is not satisfactory to build a complex outcome model for the long-term treatment effects in our ads-supply personalization. Instead, we can build a doubly robust learner on RCT data.", "2.2 Doubly Robust Learning": "Doubly Robust Learning [9, 12] is a method for estimating CATE even if either the propensity score or outcome modeling cannot be satisfactorily modeled by parametric functions. Doubly robust learning works under the conditions of unconfoundedness [19], where all potential confounders are observed, and categorical treatments. The methodology has been developed primarily for average treatment effect (ATE) [10, 26, 27, 32]. Recently doubly robust approach is combined with machine learning to estimate CATE like DRL in [12]. DRL illustrates a application scenario that we can leverage a known propensity score to achieve an unbiased CATE estimates if the individual outcomes are difficult to estimate on their own but CATE is more structured. Therefore, we adopt the DRL methodology to avoid the resource intensive outcome model in ads-supply modeling and still obtain satisfactory CATE estimates. Ads Supply Personalization via Doubly Robust Learning CIKM '24, October 21-25, 2024, Boise, ID, USA", "2.3 Ads Allocation": "A related problem is the ads allocation optimization. The ads allocation not only determines whether to insert a ad in the organic content flow, but also determines the optimal position. A blending layer for ads and organic contents can be added and formulated as a constrained optimization problem [31], or multi-objective optimization [3]. Their focus is the optimization on positions based on existing utility signals like ad-CTR in [3]. Since the optimization is conducted at the session-level, the work of [31] also sets the guardrails including top slot and min gap to avoid disastrous experience in the user level. Sophisticated DNN modules been extensively explored in the ads allocation[14, 15, 28, 30, 34]. Our ads-supply personalization distinguish the above problem formulation in two folds: (1) we explicitly control the user-level ads quantity. The existing ads allocation either leave ads quantity as an uncontrolled confounder or use a pre-defined quantity threshold [3]. (2) We model the long-term metrics as utilities for optimization while above works either leverage existing utilities or immediate user signals [15].", "3 Problem Formulation": "", "3.1 Constrained Optimization": "One business goal is to optimize for the revenue gain at a fixed engagement loss budget. The ad-load increase is discretized into a binary treatment so that the constrained optimization problem falls into the category of 0-1 knapsack problem:  where \ud835\udc67 \ud835\udc56 \u2208 { 0 , 1 } is the treatment variable for \ud835\udc56 \ud835\udc61\u210e user. In this context, \ud835\udc67 \ud835\udc56 = 1 signifies that an ad-load increase is assigned to the user, while \ud835\udc67 \ud835\udc56 = 0 indicates the absence of such an increase. The terms \ud835\udf0f \ud835\udc5f and \ud835\udf0f \ud835\udc52 represent the treatment effects for revenue gain and engagement loss, respectively. As users have different tolerance to ad-load increase, quantified by their treatment effect ratio \ud835\udf0f \ud835\udc5f / \ud835\udf0f \ud835\udc52 , a personalized ad-load assignment will benefit the cumulative revenue gain. If \ud835\udf0f \ud835\udc5f and \ud835\udf0f \ud835\udc52 are known, the optimal solution turns out to be the greedy algorithm of selecting the largest \ud835\udf0f \ud835\udc5f / \ud835\udf0f \ud835\udc52 since \ud835\udf0f \ud835\udc52 is much smaller than the total budget [5]. The key challenge lies in building a model for estimating the treatment effects of \ud835\udf0f \ud835\udc5f and \ud835\udf0f \ud835\udc52 . Therefore, we adopt the uplift model, a category of models in causal inference with an emphasis on modeling heterogeneous treatment effects.", "3.2 Heterogeneous Treatment Effect Modeling": "Our primary modeling objective, \ud835\udf0f \ud835\udc5f and \ud835\udf0f \ud835\udc52 , are long lasting effects from a conservative ad-load change of small magnitudes. We formalize our modeling problem as a heterogeneous treatment effects (HTE), or conditional average treatment effect (CATE) estimate problem. We follow the potential outcome framework[20] in the causal inference. We have \ud835\udc5b independent and identically distributed samples ( \ud835\udc4b \ud835\udc56 , \ud835\udc4c \ud835\udc56 , \ud835\udc4a \ud835\udc56 ) , \ud835\udc56 = 1 , ..., \ud835\udc5b , where \ud835\udc4b \u2208 \ud835\udf12 are per-sample features, \ud835\udc4c \ud835\udc56 \u2208 R is the observed outcome, and \ud835\udc4a \ud835\udc56 \u2208 0 , 1 is the treatment assignment indicator. The potential outcomes \ud835\udc4c \ud835\udc56 ( 0 ) , \ud835\udc4c \ud835\udc56 ( 1 ) are the outcome we would have observed given the treatment \ud835\udc4a \ud835\udc56 = 0 or 1 respectively. Our objective of interest is to estimate CATE:  In our problem, we set up two separate CATE models, one for revenue and the other for engagement treatment effects. The specific meaning for each variable mentioned above is specified below. Outcome Y is the long-term user revenue or engagement metrics. These metrics are gathered over an extended period, during which numerous perturbations may occur. Consequently, the association between features and outcomes are challenging to model as they are dominated by the large noises. Treatment W is the user-level ad-load change that influences both revenue and engagement metrics. The ad-load change is discretized into a binary variable \ud835\udc4a \u2208 { 0 , 1 } . The ad-load changes has a clear semantic meaning like top slot and min gap in [31] and a small magnitude for conservative business decisions. Feature X is the pre-treatment user features like user characteristics and activity loggings. The feature values should not be impacted by the treatment after model launched online. Otherwise, a potential positive feedback loop could lead to a disaster in the ad-load assignment. CATE \ud835\udf0f is the long-term and incremental changes in revenue and engagement for a certain user cohort. The individual revenue-toengagement ratio, \ud835\udf0f \ud835\udc5f / \ud835\udf0f \ud835\udc52 , is defined as user's sensitivity to ads. The larger the ratio is, the more insensitive user is.", "3.3 Challenge for Long-term Effect Estimate": "Popular uplift models like meta-learners in [13] rely on outcome modeling for individual-level CATE estimates. However, our outcomes are long-term revenue and engagement metrics, whose association with features are very difficult to capture. This issue presents a dilemma: we must either accept an unsatisfactory CATE estimate or bear with the prohibitive complexity of the model. To address this challenge, we first consider the full utilization of information in both the data collection and modeling stages. Subsequently, we devise a strategy to create a lightweight CATE model that is compatible with the constrained online serving environment. Nuisance modeling Because we do not have the true labels for CATE due to the counterfactual problem, outcomes are first predicted. Outcome predictions are only used for estimating CATE, hence defined as the nuisance. Therefore, a straightforward CATE estimator is:  where \u02c6 \ud835\udc4c ( \ud835\udc4b \ud835\udc56 , \ud835\udc4a ) is the prediction from the outcome model. However, such method is not satisfactory when the outcomes are hard to estimate on their own. Furthermore, it is particularly inefficient to build a complex outcome model to estimate a more structured CATE function. Data collection Although many methods are designed to learn from observational data [13, 17, 25], using RCT data as training data can make the modeling easier. RCTs are the gold standard for ascertaining the efficacy and safety of a treatment so that they are applied in large-scale applications [16, 29]. However, the existing CIKM '24, October 21-25, 2024, Boise, ID, USA Wei Shi et al. works focus more on building capable outcome modeling while not fully leveraging the information from RCTs. As a result, the strategy for long-term CATE estimates should aim to (1) maximize the extraction of information from RCT data collection to enhance performance, and (2) separate a lightweight CATE model from nuisance models for online serving. The detailed methodology is outlined in the subsequent section.", "4 Methodology": "", "4.1 Doubly Robust Learner with RCT": "We collect training data from a RCT. Users are first divided into treatment and control groups. The labels collected in the RCT will be modified by a long-term (4 months+) projection. Usually, unconfoundedness is required to identify the treatment effect.  RCT data actually poses a stronger assumption than unconfoundedness:  Therefore, RCT provides us with additional information, true propensity score, besides the observed outcomes. Here propensity score refers to the probability of receiving treatment. We now denote the propensity score as \ud835\udc52 ( \ud835\udc65 ) = \ud835\udc5d where \ud835\udc5d is the overall treatment percentage in the RCT. As mentioned, the direct estimator in (3) is highly dependent on the consistency of outcome model \u02c6 \ud835\udc4c ( \ud835\udc65 ) . It is not consistent if the outcome model is misspecified. We can address such problem by fully harnessing the true propensity score. The doubly robust (DR) CATE estimator is constructed by:  where:  With a known \ud835\udc52 ( \ud835\udc65 ) = \ud835\udc5d , DR estimator guarantees the consistency even if the outcome model is misspecified. In essence, the DR estimator maximizes the use of information available in RCT data to lessen the demands on the outcome model. Building on the DR estimator, DRL provides a distinct CATE model separate from the outcome model. In addition to eliminating the outcome bias inherent in the DR estimator, the standalone CATE model in DRL is not burdened by the complexity of the outcome model during online serving. Figure 2 shows the DRL flow consisting of two predictive tasks. First, we split the training data into two subsets \ud835\udc37 \ud835\udc5b 1 and \ud835\udc37 \ud835\udc5b 2 . Then potential outcome predictions (nuisance) \u02c6 \ud835\udc4c 0 , \u02c6 \ud835\udc4c 1 are obtained by regression on the observed outcome \ud835\udc4c . With a known propensity score \ud835\udc52 ( \ud835\udc4b ) = \ud835\udc5d from RCT, we construct the pseudo-outcome by: D 1 n = (Y, X, W) 1 D 2 n = (Y, X, W) 2 Nuisance model Nuisance model Pseudo outcome Pseudo outcome Nuisance model CATE model CATE model M2 (X) M1 (X) Inference: M(X) = (M (X) + M2 (X)) / 2 1 Figure 2: The flow of doubly robust learner.  The first term on the right-hand side represents the integration of treatment information in the RCT data. This ensures the consistency of the CATE estimator, even in the presence of biased outcome predictions. At last, we construct the second predictive model by regressing \u02c6 \ud835\udf19 on \ud835\udc4b , resulting in a CATE model \ud835\udc40 ( \ud835\udc4b ) .", "4.2 A Lightweight Implementation": "Nuisance and CATE regressor Since the bias of outcome predictions is not carried over to CATE estimator in the combination of DRL and RCT, the requirement on nuisance model is largely reduced. We implement the outcome models with random forest, which significantly saves the computation power compared to DNN implementations in [16, 29]. We also implement the second-stage CATE regressor by random forest to satisfy the stability condition as outlined in [12]. High-dimensional feature consumption In order to augment the predictive capabilities of the CATE model based on random forest, we have also devised methods to process high-dimensional features. One such significant feature is the user embedding, which is generated from other large pre-trained models. We utilized unsupervised learning to segregate users into distinct clusters based on their embeddings. The cluster ID is then used as a categorical feature in the random forest model, leading to an enhancement in performance. Feature selection We need to choose features that directly contribute to CATE estimates from numerous candidates. For each feature, users are divided into different cohorts according to the feature value. Consequently, an AUUC value can be derived from ground-truth CATE from each cohorts. Such single-feature AUUC is used to filter out most of candidates. Lagrangian selecting criteria Although the ratio form, \ud835\udf0f \ud835\udc5f / \ud835\udf0f \ud835\udc52 , is Ads Supply Personalization via Doubly Robust Learning CIKM '24, October 21-25, 2024, Boise, ID, USA ideally optimal in (1) as a greedy selection criteria, it is not the case in the real-world. The ratio form amplifies the prediction noises and loses some information. Instead, we adopted the Lagrangian selecting criteria [7], which is a linear form. Furthermore, it is also compatible with DR estimator.  where we treat \ud835\udf06 as another hyper-parameter of the model and use parameter search to determine its value. In our actual implementation, we fuse separate revenue and engagement CATE regressors into a unified stage to generate the score \ud835\udc46 \ud835\udc56 . Furthermore, we apply clustering to raw scores to further reduce variance. An alternative way is to build a direct ranking model (DRM) in [7]. However, DRM demands non-trivial computing resources at the large scale, which is a less ideal candidate for the lightweight solution to our problem.", "5 EMPIRICAL EVALUATION": "In this section, we conduct experiments to evaluate our framework for large-scale industrial applications and present analysis for systematic trade-offs. Our experiments have the following objectives: (1) Evaluate the end-to-end improvement in CATE estimate accuracy in public datasets and business metrics in product datasets, brought about by our framework. (2) Demonstrate the rationale behind our system design through component-level experiments and showcase the adaptability of our framework with larger datasets. (3) Validate the long-term impacts of our framework on the products, both by a one-month pre-launch A/B test and Nmonth post-launch hold-out test (N \u2265 4).", "5.1 RCT Datasets": "We evaluate the methods in both public and product datasets. The selected benchmarks meet the following criteria: (1) They are both large-scale advertisement datasets, and (2) They have been collected through RCTs, where all covariates are solely related to the outcome. The RCT setting guarantees that the propensity score can be known trivially. The public benchmark is CRITEO-UPLIFT [6]. Our product dataset was collected through a RCT, recording users' long-term revenue and engagement metric with the pre-treatment feature values. The dataset contains tens of million user loggings with around twenty features. The ad-load change is a binary operation. We randomly split both two datasets into training data (80%) and test data (20%). In these two RCT-based datasets, propensity score is the ratio of treatment group size over the entire population.", "5.2 Baseline Models": "Wechoose five popular methods that have been applied to the largescale applications. They all rely on the outcome modeling to attain the treatment effect estimates. Here S/T/X-learner are based on random forest regressors. CEVAE and Dragonnet are DNN-based models. They represent different complexities of outcome models. Baseline methods are implemented by the CausalML package[4]. S-learner S-learner [13] is a single model to predict the outcome of individuals with the treatment indicator as a feature. T-learner T-learner [13] uses separate outcome models for treatment and control groups. It circumvents the bias towards zero in S-learner. X-learner X-learner [13] is a two-stage approach. The first stage is the outcome model like T-learner. The second stage regresses the pseudo-outcome derived from data labels and first-stage predictions. CEVAE CEVAE [17] uses Variational Autoencoders (VAE) to simultaneously estimate the unknown latent space summarizing the confounders and the causal effect. The network structure focuses on the confounder modeling, which is an important aspect in causal representations for observational studies. Dragonnet Dragonnet [25] exploits the sufficiency of the propensity score for estimation adjustment by training the network for both propensity score and outcome predictions.", "5.3 Evaluation Metrics": "We use AUUC to measure the performance of a CATE model and AUCC to measure the business gain by a combination of revenue and engagement CATE models. \u25e6 AUUC (Area under Uplift Curve) A common metric to evaluate heterogeneous effect predictions. This is used for CRITEOUPLIFT as it has only one objective. \u25e6 AUCC (Area under Cost Curve) [7] An AUUC variant to evaluate the Return on Investment (ROI) predictions. The cost curve plots the aggregated incremental value as the Y-axis and the aggregated cost as the X-axis. In our problem, revenue gain is the value and engagement loss is the cost. AUCC is used for our product data as it is a dual-objective optimization. An cost uplift curve example is shown in Figure 3. The more accurately we can identify insensitive users, the greater the efficiency we can achieve in terms of cumulative revenue gain relative to engagement loss. Figure 3: AUCC Definition in Ads Supply Personalization. 0% 0% 50% 100% 50% 100% Aggregated engagement loss Aggregated revenue gain User's sensitivity to ad-load High Very low Low Very high", "5.4 Performance": "We now demonstrate the effectiveness of our proposed methods by benchmarking all the methods on CRITEO-UPLIFT and the product CIKM '24, October 21-25, 2024, Boise, ID, USA Wei Shi et al. data. We simulate the product scenario, where outcome predictions are not satisfactory, by training a biased outcome model in CRITEO-UPLIFT benchmark. In the product data, we construct separate CATE models for both revenue and engagement metrics and obtain the resulting a ROI metric of AUCC. CRITEO-UPLIFT We intentionally modify the labels to inject bias into training data for the outcome model. Given the original data \ud835\udc37 = {( \ud835\udc4b,\ud835\udc4c )} , we first deterministically construct a subset according to a feature value filter, \ud835\udc37 \u2032 = {( \ud835\udc4b,\ud835\udc4c ) | \ud835\udc4b \u2208 \ud835\udc37, \ud835\udc53 0 ( \ud835\udc4b ) < \ud835\udefc } , where \ud835\udc53 0 is a linear function scalarizing the feature vector \ud835\udc4b . We randomly switch the binary label, \"visit\", from 1 to 0 with a probability of \ud835\udefd ( % ) in the subset. Then we train the outcome model on the entire training dataset with the modified labels. At last, the models trained on the biased training data are evaluated using the original test data. Table 1 presents the test AUUCs with bias injection at \ud835\udefd = 0% and \ud835\udefd = 100%. Here, \ud835\udefd = 0% corresponds to the original training dataset, while \ud835\udefd = 100% represents the biased training dataset. All methods have shown a consistent high AUUC at \ud835\udefd = 0%, indicating a well-performing outcome model on the original training data. In the case where outcomes are poorly estimated ( \ud835\udefd = 100%), our framework still maintains the high AUUC while baselines all suffer from a significant performance degradation. Table 1: AUUC Comparison on CRITEO-UPLIFT Product data We evaluate all the methods on the product data in AUCC metrics, as shown in Table 2. As the outcomes, long-term revenue and engagement metrics, are difficult to estimate well on their own, all baseline models show unsatisfactory AUCC performances. The proposed framework achieves 20% improvement over the second-best baseline despite the inevitable misspecified outcome models. The performance improvement stems from the use of propensity score information in a RCT data. Note that DR estimator in (6) is unbiased but has a larger variance than the direct estimator in (3). The impact of training label variances diminish as dataset size increases. As such, the improvement becomes more pronounced in large-scale applications. Figure 4 shows the cost uplift curves of all methods. Here the value and cost in the uplift curves are normalized revenue and engagement, correspondingly. The curve of our framework shows around 40% revenue gain improvements at most operating points. Fundamentally, our framework leverages the experimental cost of a large-scale RCT and turns it into accuracy improvement in CATE estimates and savings in model complexity. Although Dragonnet's training with the regularization yields the estimator satisfying the non-parametric estimating equation, it doesn't show doubly robustness in our product dataset. We also observed the instability during Dragonnet's training on product data. Table 2: AUCC Comparison on Product Data Figure 4: Cost Curves on Product Data. AUCC Proposed Framework T-learner S-learner CEVAE Dragonnet Random Basline Aggregated Inc Engagement An end-to-end training procedure to achieve doubly robustness in real-world datasets could be left for the future work.", "5.5 Component Analysis": "In this section, we conduct comprehensive experiments to investigate the influence of the propensity score and outcome model on end performance. The resulting insights inform the requirements for our data collection policy and the complexity of our outcome model. Finally, we examine the asymptotic performance of our framework. In some component analysis, the performance of the T-learner has been demonstrated for comparison purposes. Impact of outcome model bias We adjust the probability of label modification, \ud835\udefd for CRITEO-UPLIFT, to see how the magnitude of bias impacts different methods differently. As shown in Figure 5, Figure 5: AUUC changes over outcome model bias. AUUCs under different outcome bias T-learner Proposed Framework Ads Supply Personalization via Doubly Robust Learning CIKM '24, October 21-25, 2024, Boise, ID, USA T-learner's AUUC gradually degrades with the increasing outcome model bias. Our framework, on the contrary, maintains a constant AUUC. This AUUC comparison demonstrates that the doubly robust estimator becomes increasingly essential as the bias in the outcome model can directly affect the end result in its absence. Impact of propensity score bias We adjust the propensity score that is plugged into the doubly robust estimator for CRITEO-UPLIFT. The larger the deviation it has from the ground-truth value, the larger the bias is in the propensity score prediction. The groundtruth propensity score of CRITEO-UPLIFT is 0.85. We offset it by different magnitudes so that we can see how much performance degradation will be caused by the propensity score bias. As shown in Figure 6: AUUC changes over propensity score bias. AUUCs under_different propensity score bias consistent outcome biased outcome Propensity score offset the Figure 6, the performance degradation of the proposed method is trivial given the unbiased outcome model. The degradation is larger but still within 5%. Empirically, propensity score model do not have to be very accurate in our case. Such result suggests that we can further relax our RCT constraint for data collection, especially when the RCT cost is too high. The choice of outcome model Table 3 shows the AUCCs for the product data with different outcome models. We compare the results of using a constant zero, a random forest, and a MLP model. Improving the outcome model is still beneficial. A better outcome model reduces the variance of doubly robust estimator, leading to a better finite-sample performance in CATE estimates. We can choose an optimal point considering both model performance and complexity. Table 3: AUCCs with outcome model plugged in CATE model performance scaling with data size Figure 7 shows the AUCC trend over the increasing product dataset size. The X-axis is the normalized dataset size. T-learner's AUCC does not show an increasing trend as the dataset size grows larger. On the contrary, our framework's performance is improved as the dataset size scales. Such a comparison indicates that CATE estimates, when solely based on outcome predictions, do not improve with increasing data size. However, the CATE model derived from DR estimator can effectively leverage data scaling. Such scalability because more important as the available training data size grows Figure 7: AUCCs with increasing dataset sizes. < AUCCs under different dataset sizes T-learner Proposed Framework Normalized dataset size", "5.6 Online Result": "We conduct an online A/B test before product launch (one month) and a long-term hold-out test after launch (a couple of months). Long-term revenue gain-to-engagement loss efficiency metrics are used here. The high efficiency indicates that a platform is capable of sustaining profitability over an extended period. The table 4 highlights substantial improvements of +13.8% and +19.0% prior to launch, and enhancements of +12.4% and +20.2% observed at least 4 months post-launch. In the long term, our deployed work enhances monetization efficiency with live traffic. Table 4: Online Test", "6 Conclusions and Future Works": "In this paper, we develop a doubly robust learner based framework for ads-supply personalization to achieve a sustainable revenue and engagement win. By maximizing the usage of information from the dataset collect through RCT, we significantly improve the user treatment effect estimates, hence a better revenue-to-engagement trade-off. The optimal use of information largely reduces the model complexity. The lightweight framework has been seamlessly integrated with the established ads-delivery system. In terms of future directions, devising the method when RCT data are not representative is an interesting challenge. We can gradually extend our methodology from fully controlled data collection to partially controlled and then observational data. In the meantime, we observed the user features drift over time, which requires our future efforts in developing a continuous training for causal models. CIKM '24, October 21-25, 2024, Boise, ID, USA Wei Shi et al.", "References": "[1] Ahmed M Alaa, Michael Weisz, and Mihaela Van Der Schaar. 2017. Deep counterfactual networks with propensity-dropout. arXiv preprint arXiv:1706.05966 (2017). [2] Susan Athey and Guido Imbens. 2016. Recursive partitioning for heterogeneous causal effects. Proceedings of the National Academy of Sciences 113, 27 (2016), 7353-7360. [3] Carlos Carrion, Zenan Wang, Harikesh Nair, Xianghong Luo, Yulin Lei, Xiliang Lin, Wenlong Chen, Qiyu Hu, Changping Peng, Yongjun Bao, et al. 2021. Blending Advertising with Organic Content in E-Commerce: A Virtual Bids Optimization Approach. arXiv preprint arXiv:2105.13556 (2021). [4] Huigang Chen, Totte Harinen, Jeong-Yoon Lee, Mike Yung, and Zhenyu Zhao. 2020. Causalml: Python package for causal machine learning. arXiv preprint arXiv:2002.11631 (2020). [5] George B Dantzig. 1957. Discrete-variable extremum problems. Operations research 5, 2 (1957), 266-288. [6] Eustache Diemert, Artem Betlei, Christophe Renaudin, Massih-Reza Amini, Th\u00e9ophane Gregoir, and Thibaud Rahier. 2021. A large scale benchmark for individual treatment effect prediction and uplift modeling. arXiv preprint arXiv:2111.10106 (2021). [7] Shuyang Du, James Lee, and Farzin Ghaffarizadeh. 2019. Improve User Retention with Causal Learning. In The 2019 ACM SIGKDD Workshop on Causal Discovery . PMLR, 34-49. [8] Max H Farrell, Tengyuan Liang, and Sanjog Misra. 2021. Deep neural networks for estimation and inference. Econometrica 89, 1 (2021), 181-213. [9] Dylan J Foster and Vasilis Syrgkanis. 2023. Orthogonal statistical learning. The Annals of Statistics 51, 3 (2023), 879-908. [10] Adam N Glynn and Kevin M Quinn. 2010. An introduction to the augmented inverse propensity weighted estimator. Political analysis 18, 1 (2010), 36-56. [11] Fredrik Johansson, Uri Shalit, and David Sontag. 2016. Learning representations for counterfactual inference. In International conference on machine learning . PMLR, 3020-3029. [12] Edward H Kennedy. 2023. Towards optimal doubly robust estimation of heterogeneous causal effects. Electronic Journal of Statistics 17, 2 (2023), 3008-3049. [13] S\u00f6ren R K\u00fcnzel, Jasjeet S Sekhon, Peter J Bickel, and Bin Yu. 2019. Metalearners for estimating heterogeneous treatment effects using machine learning. Proceedings of the national academy of sciences 116, 10 (2019), 4156-4165. [14] Guogang Liao, Xiaowen Shi, Ze Wang, Xiaoxu Wu, Chuheng Zhang, Yongkang Wang, Xingxing Wang, and Dong Wang. 2022. Deep Page-Level Interest Network in Reinforcement Learning for Ads Allocation. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval . 2292-2296. [15] Guogang Liao, Ze Wang, Xiaoxu Wu, Xiaowen Shi, Chuheng Zhang, Yongkang Wang, Xingxing Wang, and Dong Wang. 2022. Cross dqn: Cross deep q network for ads allocation in feed. In Proceedings of the ACM Web Conference 2022 . 401409. [16] Dugang Liu, Xing Tang, Han Gao, Fuyuan Lyu, and Xiuqiang He. 2023. Explicit Feature Interaction-aware Uplift Network for Online Marketing. arXiv preprint arXiv:2306.00315 (2023). [17] Christos Louizos, Uri Shalit, Joris M Mooij, David Sontag, Richard Zemel, and Max Welling. 2017. Causal effect inference with deep latent-variable models. Advances in neural information processing systems 30 (2017). [18] Nicholas J Radcliffe and Patrick D Surry. 2011. Real-world uplift modelling with significance-based uplift trees. White Paper TR-2011-1, Stochastic Solutions (2011), 1-33. [19] Paul R Rosenbaum and Donald B Rubin. 1983. The central role of the propensity score in observational studies for causal effects. Biometrika 70, 1 (1983), 41-55. [20] Donald B Rubin. 1974. Estimating causal effects of treatments in randomized and nonrandomized studies. Journal of educational Psychology 66, 5 (1974), 688. [21] Donald B Rubin. 2005. Causal inference using potential outcomes: Design, modeling, decisions. J. Amer. Statist. Assoc. 100, 469 (2005), 322-331. [22] Yuta Saito, Hayato Sakata, and Kazuhide Nakata. 2020. Cost-effective and stable policy optimization algorithm for uplift modeling with multiple treatments. In Proceedings of the 2020 SIAM International Conference on Data Mining . SIAM, 406-414. [23] Patrick Schwab, Lorenz Linhardt, and Walter Karlen. 2018. Perfect match: A simple method for learning representations for counterfactual inference with neural networks. arXiv preprint arXiv:1810.00656 (2018). [24] Uri Shalit, Fredrik D Johansson, and David Sontag. 2017. Estimating individual treatment effect: generalization bounds and algorithms. In International conference on machine learning . PMLR, 3076-3085. [25] Claudia Shi, David Blei, and Victor Veitch. 2019. Adapting neural networks for the estimation of treatment effects. Advances in neural information processing systems 32 (2019). [26] Xiaoqing Tan, Shu Yang, Wenyu Ye, Douglas E Faries, Ilya Lipkovich, and Zbigniew Kadziola. 2022. When doubly robust methods meet machine learning for estimating treatment effects from real-world data: A comparative study. arXiv preprint arXiv:2204.10969 (2022). [27] Mark J Van Der Laan and Daniel Rubin. 2006. Targeted maximum likelihood learning. The international journal of biostatistics 2, 1 (2006). [28] Ze Wang, Guogang Liao, Xiaowen Shi, Xiaoxu Wu, Chuheng Zhang, Bingqi Zhu, Yongkang Wang, Xingxing Wang, and Dong Wang. 2022. Hybrid Transfer in Deep Reinforcement Learning for Ads Allocation. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management . 4560-4564. [29] Guojun Wu, Ge Song, Xiaoxiang Lv, Shikai Luo, Chengchun Shi, and Hongtu Zhu. 2023. DNet: Distributional Network for Distributional Individualized Treatment Effects. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 5215-5224. [30] Wanqi Xue, Qingpeng Cai, Ruohan Zhan, Dong Zheng, Peng Jiang, Kun Gai, and Bo An. 2022. ResAct: Reinforcing long-term engagement in sequential recommendation with residual actor. arXiv preprint arXiv:2206.02620 (2022). [31] Jinyun Yan, Zhiyuan Xu, Birjodh Tiwana, and Shaunak Chatterjee. 2020. Ads allocation in feed via constrained optimization. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 33863394. [32] Shu Yang and Yunshu Zhang. 2023. Multiply robust matching estimators of average and quantile treatment effects. Scandinavian Journal of Statistics 50, 1 (2023), 235-265. [33] Jinsung Yoon, James Jordon, and Mihaela Van Der Schaar. 2018. GANITE: Estimation of individualized treatment effects using generative adversarial nets. In International conference on learning representations . [34] Xiangyu Zhao, Changsheng Gu, Haoshenglun Zhang, Xiwang Yang, Xiaobing Liu, Jiliang Tang, and Hui Liu. 2021. Dear: Deep reinforcement learning for online advertising impression in recommender systems. In Proceedings of the AAAI conference on artificial intelligence , Vol. 35. 750-758. [35] Yan Zhao, Xiao Fang, and David Simchi-Levi. 2017. Uplift modeling with multiple treatments and general response types. In Proceedings of the 2017 SIAM International Conference on Data Mining . SIAM, 588-596."}
