{
  "Deep Ensemble Shape Calibration: Multi-Field Post-hoc Calibration in Online Advertising": "Shuai Yang Shopee Discovery Ads Beijing, China Hao Yang Shopee Discovery Ads Beijing, China Shuo Yuan Shopee Discovery Ads Beijing, China Zhuang Zou Shopee Discovery Ads Beijing, China Linhe Xu Shopee Discovery Ads Beijing, China",
  "Yifan Zeng": "Shopee Discovery Ads Beijing, China",
  "ABSTRACT": "In the e-commerce advertising scenario, estimating the true probabilities (known as a calibrated estimate) on Click-Through Rate (CTR) and Conversion Rate (CVR) is critical. Previous research has introduced numerous solutions for addressing the calibration problem. These methods typically involve the training of calibrators using a validation set and subsequently applying these calibrators to correct the original estimated values during online inference. However, what sets e-commerce advertising scenarios is the challenge of multi-field calibration. Multi-field calibration requires achieving calibration in each field. In order to achieve multi-field calibration, it is necessary to have a strong data utilization ability. Because the quantity of pCTR specified range for single field-value (such as user ID and item ID) sample is relatively small, which makes the calibrator more difficult to train. However, existing methods have difficulty effectively addressing these issues. To solve these problems, we propose a new method named D eep E nsemble S hape C alibration (DESC). In terms of business understanding and interpretability, we decompose multi-field calibration into value calibration and shape calibration. We introduce innovative basis calibration functions, which enhance both function expression capabilities and data utilization by combining these basis calibration functions. A significant advancement lies in the development of an allocator capable of allocating the most suitable calibrators to different estimation error distributions within diverse fields and values. We achieve significant improvements in both public and industrial datasets. In online experiments, we observe a +2.5% increase in CVR and +4.0% in GMV. (Gross Merchandise Volume). Our code is now available at: https://github.com/HaoYang0123/DESC.",
  "KEYWORDS": "Multi-Field Calibration, Basis Calibration Function, Field-aware Attention Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference acronym 'XX, June 03-05, 2018, Woodstock, NY © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-XXXX-X/18/06...$15.00 https://doi.org/XXXXXXX.XXXXXXX",
  "ACMReference Format:": "Shuai Yang, Hao Yang, Zhuang Zou, Linhe Xu, Shuo Yuan, and Yifan Zeng. 2018. Deep Ensemble Shape Calibration: Multi-Field Post-hoc Calibration in Online Advertising. In Proceedings of Make sure to enter the correct conference title from your rights confirmation emai (Conference acronym 'XX). ACM, New York, NY, USA, 9 pages. https://doi.org/XXXXXXX.XXXXXXX",
  "1 INTRODUCTION": "Estimating CTR and CVR is a crucial technology in e-commerce advertising domains [2-6, 10, 14, 22, 26-28, 30, 33, 35]. Accurate prediction of CTR (pCTR) and CVR (pCVR) is essential, as it necessitates precision not only in ranking but also in absolute values. However, recent studies [1, 7, 9, 16, 23] have revealed that numerous established machine learning techniques, particularly deep learning methods extensively applied in the fields of e-commerce advertising, often yield inadequately calibrated probability predictions. In previous research, there are many solutions to solve the calibration problem. These methods learn calibrator through the validation set, and then use the calibrator to correct the original estimated values in the online inference stage. These methods can be categorized into parameter methods (including Platt Scaling [25], Temperature scaling [9], Beta calibration [18], Gamma calibration [20] and Dirichlet calibration [17]), non-parameter methods (such as Histogram Binnning [31] and Isotonic Regression [32]) and hybrid methods. Hybrid methods encompass both non-field-aware [15, 19, 34] and field-aware approaches [13, 24, 29]. In the advertising scenarios, there are multiple fields (such as user group and item category), and if predictions of CTR (pCTR) or CVR (pCVR) are not calibrated in certain fields, such as item category, it can lead to a negative impact on the earnings of some advertisers. Simultaneously, in the e-commerce scenario, there is a particularly large number of fields to consider. This necessitates the calibration of each field, referred to as multi-field calibration . To achieve multi-field calibration, it is necessary to have a strong data utilization ability. Because the samples of the specified pCTR range for a single field value (such as user ID and item ID) are relatively small, which makes the calibrator more difficult to train. In terms of business understanding and interpretability, we decompose multi-field calibration into value calibration and shape calibration. Figure 1 shows shape miscalibration and value miscalibration. Value calibration is defined as no over- or under-estimation each value under concerned fields (such as average of pCTR should equal to the CTR for value \"women's shoes\" in the field \"category\", Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Shuai Yang, Hao Yang, Zhuang Zou, Linhe Xu, Shuo Yuan, and Yifan Zeng CTR PCTR PCTR PCTR pCTR = CTR Shape Miscalibrated Value  Miscalibrated Shape & Value Calibrated Calibrated Calibrated Calibrated Figure 1: Examples to show the shape miscalibration and value miscalibration Figure 2: Examples to show significant variations in value miscalibration among different fields and values. 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 1.0 1.5 2.0 pCTR/CTR Value miscalibration over 24 hours cat_0 cat_1 cat_2 cat_3 cat_4 cat_5 0.6 0.8 1.0 1.2 1.4 pCTR/CTR Value miscalibration across Categories Figure 3: Examples to show significant variations in shape miscalibration among different values within the same field. 0.011 0.019 0.027 0.035 0.043 Un-calibrated pCTR 0.2 0.4 0.6 0.8 1.0 1.2 1.4 pCTR/CTR Calibrated pCTR value=\"Food\",field=\"category\" 0.011 0.019 0.027 0.035 0.043 Un-calibrated pCTR 0.2 0.4 0.6 0.8 1.0 1.2 1.4 Calibrated pCTR value=\"Book\",field=\"category\" Shape miscalibration across values in the e-commerce advertising context, the number of fields can range from dozens to even hundreds). From the advertising perspective, value calibration ensures that the ECPM (Effective Cost Per Mille) and GMV (Gross Merchandise Value) of different items are not over- or under estimated. Figure 2 illustrates the significant inconsistency of over- and under-estimation across different values. Shape calibration is defined as no over- or under-estimation for each subset of the pCTR within the specified range. Explaining from the advertising perspective, shape calibration ensures that some already popular items are not excessively exposed or suppressed. Take Figure 3 as an example, for the field \"Category\" with values \"Food\" and \"Book\", the calibrated pCTR/CTR is 1 in any pCTR interval, whereas the distribution (shape) of the uncalibrated pCTR/CTR will be different. However, the existing methods cannot simultaneously fulfill both value calibration and shape calibration. For parameter methods, non-parameter methods and non-fieldaware methods, they involve the training of a single calibration function to address the issue of over- or under-estimation for each subset of the pCTR globally but overlook the biases across different fields. For field-aware methods, they have different emphases, and cannot simultaneously fulfill both value calibration and shape calibration. NeuralCalib/Field-aware Calibration (FAC) [24] places its focus on modeling estimation value biases across various field values, rather than addressing shape miscalibrations. Multiple Boosting Calibration Tree (MBCT) [13] effectively enhances the binning of field values, giving less emphasis to shape miscalibrations. AdaCalib (Ada) [29] can only model shape calibration and value calibration under the condition of a single field. An analysis of the aforementioned field-aware methods reveals several key observations. Firstly, within a single field, there exists multiple values. In some cases, the allocation of samples to each field value may be limited, particularly in scenarios involving user ID, item ID and certain sparse cross-features. Consequently, training a calibrator under such limited samples can be challenging. Additionally, field-aware methods often employ a binning strategy, followed by the generation of calibration parameters based on information from two adjacent bins. This approach can lead to a sample isolation issue wherein the parameters of a bin are exclusively influenced by the samples within that bin and its neighboring bins, with no updates from samples in other bins. As a result, these methods can result in suboptimal data utilization. To solve these above problems, we propose a new method named D eep E nsemble S hape C alibration ( DESC ). There are four contributions in our work: · We redefine the multi-field calibration: perform shape calibration and value calibration at the same time. · We propose the novel basis calibration functions, which can simultaneously improve function expression ability and data utilization through the combination of basis calibration functions. · Wemakeabreakthrough in putting forward an allocator that can allocate the most suitable shape calibrators for different estimation error distributions on various fields and values. · Our proposed DESC method outperforms other methods significantly across both calibration and non-calibration metrics, as demonstrated on two public datasets and one industrial dataset. Additionally, in online experiments, we observe a notable increase of +2.5% in CVR and +4.0% in GMV.",
  "2 RELATED WORKS": "With the growing emphasis on improving the reliability and accuracy of machine learning models, several research approaches have emerged. Numerous calibration methods focus on acquiring a mapping function to convert predicted probabilities into observed posterior probabilities, referred to as post-hoc calibration. These studies can be broadly classified into three categories: non-parameter, parameter, and hybrid methods.",
  "2.1 Non-parametric Methods": "Non-parametric methods do not rely on any assumptions regarding the distribution of estimates. Examples of non-parametric methods encompass Histogram Binning (HB)[31] and Isotonic Regression (IR) [32]. HB involves sorting the estimated values and then dividing them into bins of equal frequencies or intervals. Building upon this, IR introduces the additional constraint of order preservation. These approach can introduce instability in the bin boundary values.",
  "2.2 Parametric Methods": "Parameter methods often rely on specific probability distribution assumptions for deriving mapping functions. Platt Scaling [25], extensively employed in binary classification calibration, assumes a Gaussian distribution with equal variances for both positive and Multi-Field Post-hoc Calibration on Online Advertising Conference acronym 'XX, June 03-05, 2018, Woodstock, NY negative classes [20]. For multi-class tasks, Temperature Scaling [9] extends this approach. Beta calibration [18], Gamma calibration [20] and Dirichlet calibration [17] rely on their respective probability distributions for calibration. Parameter methods are highly reliant on the strong distribution assumption, which can lead to suboptimal performance if the assumption does not hold in practical scenarios.",
  "2.3 Hybrid Methods": "Hybrid methods integrate non-parametric and parametric approaches. Based on whether they are applied at the field-level, we further categorize hybrids into two groups: non-field-aware and field-aware methods. 2.3.1 Non-field-aware Methods. Non-field-aware methods encompass three notable approaches: Scaling-binning [19], Smooth Isotonic Regression (SIR) [15], and Ensemble Temperature Scaling (ETS) [34]. Scaling-binning combines the techniques of Platt Scaling and Histogram Binning. SIR employs linear interpolation based on isotonic regression, while ETS combines multiple temperature scalings for calibration. Notably, all of these methods utilize raw predicted scores as input without taking field information into account. 2.3.2 Field-aware Methods. Field-aware methods integrate supplementary features to formulate calibration functions. NeuralCalib (FAC) [24] introduces an auxiliary module but it does not address the challenge of shape miscalibration variance among different field values. AdaCalib (Ada) [29] learns an isotonic function based on posterior statistics and selects the most appropriate bin number for a single field value. However, AdaCalib does not encompass all fields. MBCT [13] employs trees to uncover more effective calibration across fields. However, calibration trees are unable to handle sparse fields, such as user ID and item ID.",
  "3 CALIBRATION PROBLEM FORMULATION": "In online advertising systems, using CTR as an illustration (CVR follows the same principles), we have trained a neural predictor, denoted as 𝑓 𝑢𝑛𝑐𝑎𝑙𝑖𝑏 , on a training dataset D 𝑡𝑟𝑎𝑖𝑛 . This dataset includes all field values as inputs ( 𝑥 ) and click responses ( 𝑦 ), where 𝑦 = 0 represents non-click events and 𝑦 = 1 signifies click events. The neural predictor is capable of forecasting the likelihood of a click using the following formula.  To address the under- and over-estimation issues associated with the predicted scores generated by 𝑓 𝑢𝑛𝑐𝑎𝑙𝑖𝑏 , we must train an additional calibrator, denoted as 𝑓 𝑐𝑎𝑙𝑖𝑏 , while considering 𝑛 distinct fields ( 𝑍 1, 𝑍 2, ..., 𝑍 𝑛 ) in a validation dataset. Our objective is for 𝑓 𝑐𝑎𝑙𝑖𝑏 to predict the conditional expectation E [ 𝑦 | 𝑥 ] . Subsequently, the theoretical calibration error of 𝑓 𝑐𝑎𝑙𝑖𝑏 with respect to the groundtruth under the 𝑙 𝑝 -norm is defined as:  The calibrator 𝑓 𝑐𝑎𝑙𝑖𝑏 is considered to be perfectly calibrated when the theoretical calibration error 𝑇𝐶𝐸 𝑝 ( 𝑓 𝑐𝑎𝑙𝑖𝑏 ) is zero. However, in fact perfect calibration is impossible in practice. Only approximate and asymptotic grouped calibrations are possible for finite and specific partitions of samples [11]. To test the performance of different calibrators, we will explain some related calibration metrics in EXPERIMENTS section.",
  "4 METHODS": "Under the requirement of multi-field calibration, both shape calibration and value calibration need to be performed simultaneously. Therefore, within the entire DESC architecture, we have designed separate modules, namely Shape Calibrator and Value Calibrator (Figure 4), to achieve shape calibration and value calibration. The final calibrated score is the product of these two parts:  The input 𝑥 consists of all 𝑛 field values 𝑍 and non-calibrated scores ˆ 𝑝 𝑢𝑛𝑐𝑎𝑙𝑖𝑏 . S ( 𝑥 ) and V ( 𝑥 ) refer to the shape calibrated score (the output of Shape Calibrator) and value calibrated score (the output of Value Calibrator), respectively. The training loss function is the negative log-likelihood function.  In Section 4.1, we present the Shape Calibrator module, which is responsible for achieving shape calibration. In Section 4.2, we discuss the Value Calibrator module, which is designed to accomplish value calibration. In Section 4.3, we elaborate on the deployment of DESC in an online setting, outlining the necessary procedures and considerations.",
  "4.1 Shape Calibrator": "For multi-field calibration, the goal of Shape Calibrator is to ensure that: given input features 𝑥 , we need to reduce the problems of over- and under-estimation across all intervals of pCTR (shape miscalibration). In section 4.1.1, we pre-define a variety of basis functions to accommodate different shape requirements. Section 4.1.2 deals with the allocation of appropriate shape functions given specific field conditions. This section offers separate discussions on how shape allocation is managed for regular fields and sparse fields. In section 4.1.3, when dealing with multiple fields, there may be conflicts in terms of the influence of different fields on calibration. Thus, we introduce a multi-field fusion mechanism named MultiField Shape Ensemble. Collectively, sections 4.1.1 and 4.1.2 are referred to as the Single Field Shape Calibrator (SFSC), as depicted in Figure 4b. 4.1.1 Shape Pre-Define. In this section, we pre-define some basis calibration functions. Then, we merge the basis calibration functions into shape functions to enhance their expressive ability. For the variable 𝑡 , 𝑡 is between 0 and 1, basis calibration function B( 𝑡 ) satisfies the following conditions: 1. The function is monotonically non-decreasing and continuous in range ( 0 , 1 ) . 2. When 𝑡 approaches 0 + , the limit of B( 𝑡 ) is 0, and when 𝑡 approaches 1 -, the limit of B( 𝑡 ) is 1 (Equation 5).  We pre-define 𝑚 basis functions consisting of 𝑝 power functions, 𝑙 logarithmic functions and 𝑠 scaling functions (shown in Equation 6). These functions exhibit different shape characteristics, and their shapes vary when the hyper-parameters ( e.g. the coefficients in these basis calibration functions) are different. We choose predefined functions over Multi-Layer Perceptron (MLP) because, through data research, we have found that predefined functions Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Shuai Yang, Hao Yang, Zhuang Zou, Linhe Xu, Shuo Yuan, and Yifan Zeng Figure 4: (a) Overall architecture of DESC, its input includes the non-calibrated pCTR and the original fields, and the final output is the calibrated pCTR. It is end-to-end trainable. (b) Single Field Shape Calibrator takes one field and the non-calibrated pCTR as inputs and outputs the calibrated shape score for this field.",
  "(a) Overall Architecture": "Value Calibrator Shape Calibrator SFSC SFSC 0.1 0.7 0.2 pCTR lookup table bucketid MLP field i emb (emb) emb",
  "(b) S ingle F ield S hape C alibrator": "Embedding Augment Softmax Softmax Global Shape Attention SFSC pCTR fields value dot-product inner- product Multi-Field Shape Ensemble Shape Pre-define attention scores sum sigmoid inverse sigmoid can address the issue of over- or underestimation for each subset of pCTR within the specified range (shape calibration). Additionally, predefined functions have a lower parameter count and superior performance. Compared to segmented linear functions in traditional calibration methods, including SIR, FAC, Ada, using basis calibration functions doesn't require data segmentation. In other words, shape learning can utilize all samples for training, significantly enhancing the data utilization. Basis calibration functions include, but are not limited to 𝑙𝑜𝑔 , 𝑒𝑥𝑝 and 𝑠𝑐𝑎𝑙𝑖𝑛𝑔 . By analyzing the data, we find that the combination of 𝑙𝑜𝑔 , 𝑒𝑥𝑝 and 𝑠𝑐𝑎𝑙𝑖𝑛𝑔 functions can satisfactorily fulfill our requirements with ease.      ℎ 𝑖 , 𝑣 𝑖 and 𝑎 𝑖 ( ℎ 𝑖 , 𝑣 𝑖 , 𝑎 𝑖 ∈ R + ) are parameters of basis calibration functions. For each type of basic calibration function, we pre-define these parameters using equally spaced floats (e.g., [ 0 . 1 , 0 . 3 , 0 . 5 , 0 . 7 ] ), which can be set as trainable. However, B( 𝑡 ) can only represent simple shapes, and complex shapes can be composed of simple shapes, as shown in Figure 5. Therefore, we need to combine these basis calibration functions through weighted summation to create shape function S 𝑖 ( 𝑥 ) capable of representing complex shapes for 𝑖 -th field, as shown in Equation 11, where 𝛼 𝑗 𝑖 refers to the attention weight of 𝑗 -th basis function for 𝑖 -th field, which will be explained in next section.  Figure 5: Complex shape can be composed of simple shapes. 0.0 0.5 1.0 0.0 0.5 1.0 S(x) 0.0 0.5 1.0 0.0 0.5 1.0 =  0.5 * scaling ( x ; 0.5) 0.0 0.5 1.0 0.0 0.5 1.0 +  0.5 * scaling ( x ; 3.8) 4.1.2 Shape Allocation. In shape allocation, we allocate suitable shape functions based on feature values. These features include two parts: pCTR bucket feature and original field features. PCTR bucket feature categorizes pCTR into intervals. For example, 0 to 0.001 is bucket 1, and 0.001 to 0.003 is bucket 2. Introducing pCTR bucket feature allows for better shape allocation within different pCTR intervals. The specific steps are as follows: Step1. We pre-define the embedding size for each feature. Each one-hot feature (including pCTR bucket feature 𝑏𝑢𝑐𝑘𝑒𝑡 ˆ 𝑝 𝑢𝑛𝑐𝑎𝑙𝑖𝑏 and original field 𝑍 𝑖 ) is projected into a fixed-size dense embedding, such as b ˆ puncalib and ei . The bucket size of 𝑏𝑢𝑐𝑘𝑒𝑡 ˆ 𝑝 𝑢𝑛𝑐𝑎𝑙𝑖𝑏 is assigned as 100. Step2. We concatenate embedding vectors into a Multi-Layer Perceptron (MLP), followed by a softmax operation, the output 𝛼 𝑖 size of softmax is 1 × 𝑚 , with 𝛼 1 𝑖 to 𝛼 𝑚 𝑖 , same as the number of pre-defined basis functions.  Step3. Finally, we obtain the output value of the Shape Calibrator S 𝑖 ( 𝑥 ) for 𝑍 𝑖 shown in the previous Equation 11. In the shape allocation stage, the expressive ability of embedding directly affects the ability of allocating shapes. For sparse fields, we enhance the expressive ability of embeddings by using self-attention. The specific formula is shown in following formula, where 𝑑 is the dimension of e and 𝑛 is the number of fields.  That means: for more similar fields, their miscalibration distributions will be similar. If the semantics between two embeddings ei and ej is similar, then the corresponding weight is also relatively large.  Multi-Field Post-hoc Calibration on Online Advertising Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Next, we concatenate the original embedding ei , the enhanced output embedding X ( ei ) , and the pCTR bucket embedding b ˆ puncalib . Then, following the same process as in the shape allocation stage, we generate the attention for shape allocation 𝛼 𝑖 and perform the final fusion. 4.1.3 Multi-Field Shape Ensemble. Different calibration values for different fields can conflict with each other. For example, there is a scenario where 19 to 26-year-old users demand for women's shoes. Under the item category field of \"women's shoes\", the noncalibrated model behaves 30% over-estimation of pCTR in the 0.01 to 0.03 range. However, under the user age field of \"19 to 26\", there's a 20% under-estimation of pCTR in the same 0.01 to 0.03 range. Consequently, we not only need to perform shape calibration on individual field but also need to globally harmonize the outputs of shape calibrators for different fields. This helps reduce calibration errors at a global level. We use Global Shape Attention to combine the output results obtained from different fields (Figure 4a). Global Shape Attention, denoted as Ψ , is derived from the Global Shape Attention Generator module (details will be elaborated in section 4.2.2). The size of Ψ is 1 × 𝑛 , with Ψ 𝑖 ∈ [ 0 , 1 ] , and ˝ 𝑛 𝑖 = 1 Ψ 𝑖 = 1. The fusion formula is as follows, where there are a total of 𝑛 fields, and S 𝑖 ( 𝑥 ) represents the output of the shape calibrator for 𝑖 -th field.  We finally obtain the output of Global Shape Calibrator: S ( 𝑥 ) .",
  "4.2 Value Calibrator": "The goal of the Value Calibrator is to ensure that, for each sample 𝑥 , there is no overall over- or under-estimation (value calibration). We use all fields for Value Calibrator to achieve the best overall performance, as described in section 4.2.1. Considering all fields allows for the excellent allocation of shapes for each field, we use global shape attention depicted in section 4.2.2. 4.2.1 Global Field Value Calibrator. Global Field Value Calibrator encompasses all the necessary information of the fields concerned. We train a neural network to fix the field-level miscalibration or biases by utilizing all necessary features[24]. These input fields are projected into fixed-size dense representations, which are fed into a neural network (the form of the neural network is not restricted, here we use MLP). Then we get the middle output ℎ𝑖𝑑𝑑𝑒𝑛𝐿𝑎𝑦𝑒𝑟 and the final output V ( 𝑥 ) for the Global Field Value Calibrator. Finally, by combining V ( 𝑥 ) and S ( 𝑥 ) , we obtain the final calibrated output ˆ 𝑝 𝑐𝑎𝑙𝑖𝑏 = S ( 𝑥 ) · V ( 𝑥 ) .   4.2.2 Global Shape Attention Generator. The Global Field Value Calibrator not only produces the final output but also yields valuable intermediate representation. In this context, we use the intermediate representation ℎ𝑖𝑑𝑑𝑒𝑛𝐿𝑎𝑦𝑒𝑟 as input of Equation 18, which can effectively learn information from all fields. After passing through a MLP and softmax layer, it produces Global Shape Attention Ψ with a size equal to the number of fields 𝑛 . In the Shape Calibration stage, the output produced by each field 𝑍 𝑖 is multiplied by its corresponding score Ψ 𝑖 and summed to obtain the final Shape Figure 6: Real-Time calibration system used DESC method for CXR (CTR/CVR) task in our industrial advertising system. Matching Logs Ranking Model Plugin DESC Ranking features training Save checkpoint Result request Calibration Score.",
  "4.3 Online Deployment": "The overall framework of the online service using the DESC calibration method in our industrial advertising system is shown in Figure 6. When a real-time user request comes, all candidate items are recalled and predicted with pCTRs by a non-calibration model (ranking model). Then DESC model calibrates the pCTRs considering different field values, including user, item, and context features. In detail, the input 𝑥 of DESC has two parts: different field values and its non-calibrated score ˆ 𝑝 𝑢𝑛𝑐𝑎𝑙𝑖𝑏 . DESC uses a light neural network to output the calibrated score ˆ 𝑝 𝑐𝑎𝑙𝑖𝑏 , which is then used to sort the final ranks for all candidate items. We have integrated the DESC method as a plugin into the ranking model to reduce maintenance costs while only adding a little more time (about 2ms for one user's request) for online inference.",
  "5 EXPERIMENTS": "",
  "5.1 Experimental Setup": "5.1.1 Datasets. To validate the effectiveness of our proposed DESC method, we conduct experiments on two public datasets ( AliCCP and CRITEO for CTR prediction tasks) and one industrial dataset ( Shopee for CVR prediction task). CRITEO display advertising data 1 , which consists of 46 million samples. We split the samples into 28 million samples as training set ( D 𝑡𝑟𝑎𝑖𝑛 ), 9 million samples as validation set ( D 𝑑𝑒𝑣 ) and other 9 million samples as testing set ( D 𝑡𝑒𝑠𝑡 ). AliCCP (Alibaba Click and Conversion Prediction) [21] 2 contains 80 million samples. We split these samples with the proportion 2:1:1 to D 𝑡𝑟𝑎𝑖𝑛 , D 𝑑𝑒𝑣 and D 𝑡𝑒𝑠𝑡 . To test DESC method on CVR prediction task, we collect the conversion logs from the Shopee 's online advertising system. It contains 100 million samples, where the first 60 million for D 𝑡𝑟𝑎𝑖𝑛 , the next 20 million for D 𝑑𝑒𝑣 and the last 20 million for D 𝑡𝑒𝑠𝑡 . 5.1.2 Competing Methods. Several representative calibration methods are used as competitors. We tested several competitive methods, including parametric methods, non-parametric methods, non-fieldaware hybrid methods and field-aware hybrid methods. Parametric method contains Platt Scaling (PS) [25]. Non-parametric method contains Histogram Binning (HB) [31] and Istonic Regression (IR) [32]. Non-field-aware method contains Smooth Isotonic Regression 1 https://www.kaggle.com/c/criteo-display-ad-challenge 2 https://tianchi.aliyun.com/datalab/dataSet.html?dataId=408 Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Shuai Yang, Hao Yang, Zhuang Zou, Linhe Xu, Shuo Yuan, and Yifan Zeng (SIR) [15] and Ensemble Temperature Scaling (ETS) [34]. Fieldaware method contains FAC [24], Ada [29] and MBCT [13]. We use DeepFM [10] to train the non-calibrated models with all fields in D 𝑡𝑟𝑎𝑖𝑛 , and predict the non-calibrated scores in D 𝑑𝑒𝑣 and D 𝑡𝑒𝑠𝑡 . DeepFM consists of both the fully connected part and the FM part. We utilized the open-source DeepFM \"from deepctr.models import DeepFM\" and incorporated all features into DeepFM. All calibration methods are trained using samples of D 𝑑𝑒𝑣 and are tested using samples of D 𝑡𝑒𝑠𝑡 . 5.1.3 Parameter Configuration. For all neural calibrators, including FAC, Ada and DESC, we use Adam as the optimizer with a learning rate of 1e-3 and batch size of 16,384. For the embedding size of field value 𝑑 , both FAC and Ada are set as 256 while for DESC, it is 128 because it obtains a better result. For FAC, we set the number of bins to 100 while for Ada, the candidate set of bin numbers is set {2, 4, 8} following in [29]. For DESC, the number of basis functions 𝑚 is 48 (the numbers of the three types of basis functions are both 16), and the numbers of fields 𝑛 are 26, 23 and 10 for Criteo, AliCCP and Industrial dataset, respectively. For MBCT, we set the same hyperparameters according to the paper. 5.1.4 Compared Metrics. Two commonly used metrics, F-RCE (Field RCE) and F-ECE (Field ECE) for each field 𝑍 𝑖 are used. To calculate the F-RCE of 𝑖 -th field (shown in Equation 19), we use the testing dataset D 𝑡𝑒𝑠𝑡 (for simplicity, D refers to D 𝑡𝑒𝑠𝑡 here) consisting of ( 𝑥 𝑗 , 𝑦 𝑗 ) , where 𝑥 𝑗 and 𝑦 𝑗 mean the input features with different fields and clicked label of 𝑗 -th sample. The subset of D 𝑧 has the samples with the same value 𝑧 for 𝑖 -th field. F-RCE of 𝑖 -th field evaluates the deviation level of each sample's calibrated probability ˆ 𝑝 𝑗 𝑐𝑎𝑙𝑖𝑏 considering 𝑗 -th field. Another metric is the F-ECE of 𝑖 -th field. As shown in Equation 20, it can be calculated for each subset 𝐷 𝑧 of samples with the same value in 𝑖 -th field. We can calculate the subset of F-ECE (shown in Equation 21) by partitioning predictions into 𝑀 equally-spaced bins and taking a weighted average of the difference between bins' accuracy and confidence. In detail, firstly we sort the samples by the non-calibrated scores ˆ 𝑝 𝑗 𝑢𝑛𝑐𝑎𝑙𝑖𝑏 . Then all samples are grouped into 𝑀 interval bins. For all samples ( 𝐵 𝑚 ) in 𝑚 -th bin, we calculate the accuracy and confidence, where the accuracy is the average of labels and the confidence is the average of calibrated scores.     As explained that the traditional field-aware calibration methods, such as FAC [24] and Ada [29], only consider one field to calibrate, these methods do not take into account the influence of other fields on the calibration results. We found that the calibration results from these models only training on one field perform poorly on other fields. So we use the Multi-Field RCE (MF-RCE) and Multi-Field ECE (MF-ECE) on all fields to compare different methods as follows:   where 𝑛 is the number of fields. We use F-RCE and F-ECE as the primary empirical metrics to compare the calibration errors of different calibration methods on one field and MF-RCE and MF-ECE to test on all fields. Besides, we also report the overall ranking performance by using AUC and Log-loss metrics. Since PCOC (Predicted Click Over Click) [8, 12] and ECE (Expected Calibration Error) [13] indicators can be achieved well for all competing methods, and F-ECE and MF-ECE, as compared to the traditional ECE, can provide a finer-grained representation of calibration error in the context of field-aware problems, PCOC and ECE are not listed in the main results.",
  "5.2 Main Results": "5.2.1 Results on One Field. As other hybrid field-aware based methods only consider one field to calibrate, we compare our proposed method in one field in the same way. We select the field of \"C2\" (577 unique values) in Criteo, \"853\" (39,979 unique values) in AliCCP and \"item ID\" (5 million unique values) in Industrial data to perform our experiments. As shown in Table 1, the AUCs of all calibrators are higher than the uncalibrator while all other calibration-related metrics (F-ECE, F-RCE) of calibrators are smaller than the uncalibrator, which shows that the calibration methods can improve the accuracy of pCTR while maintaining a certain increase in ranking performance. Specially, by fusing multiple fields in shape allocation and shape augmentation, our proposed DESC can achieve much smaller calibration errors compared to other competitors even in one field evaluation. It's worth explaining that DESC performs much better than other competitors in industrial data than public data. The reason is that DESC behaves better in calibration performance in this scene where the data sparsity is more serious in practical industrial data. 5.2.2 Results on All Fields. As DESC can consider the effects of different fields on calibration, we need to evaluate its performance on all fields. As shown in Table 2, the conclusion is consistent with the smallest calibration errors on all three datasets.",
  "5.3 In-Depth Analysis": "5.3.1 Model Structure Ablation Experiment for DESC. We perform some model structure ablation experiments to validate the significance and effectiveness. (1) Ablation Experiment of Shape Calibrator. To evaluate the effectiveness of the Shape Calibrator and Value Calibrator, we delete the corresponding module and left the other parts unchanged. Firstly, we delete the Shape Calibrator sub-module, the AUC reduces to 0.6602 while the metrics of MF-RCE and MF-ECE@3 on all fields are slightly larger than DESC with 0.1434 and 0.0120 (first line on Table. 3). (2) Ablation Experiment of Value Calibrator. Secondly, we delete another sub-module, the Value Calibrator. The phenomenon is almost the same, with 0.6761 of AUC, and 0.1398 and 0.0119 of MF-RCE and MF-ECE@3 on all fields (second line on Table. 3). (3) Ablation Experiment of Multi-Field Shape Ensemble. In METHODS section, we have emphasized that shape calibration Multi-Field Post-hoc Calibration on Online Advertising Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Table 1: Results of different methods for calibrating CTR predictive models on public and industrial datasets for one field. Table 2: Results of different methods for calibrating CTR predictive models on public and industrial datasets for all fields. Table 3: Ablation study of DESC on AliCCP. 0.2 0.4 0.6 0.8 1.0 Sampling Ratio 0.0100 0.0105 0.0110 0.0115 0.0120 0.0125 0.0130 MF-ECE@3 FAC Ada MBCT DESC needs to consider multiple fields because different fields and values can have numerous different shapes. Then, we only use meanpooling to compute the final predicted result considering different fields ( Ψ 𝑖 is all the same with 1 / 𝑛 in Equation 15). The results are still inferior as shown in the third line in Table. 3. (4) Ablation Experiment of pCTR Bucket Feature. The pCTR bucket feature is concatenated with the field embedding to calculate the shape allocation weights in DESC (Equation 14). Then we delete the pCTR bucket feature (fourth line on Table. 3) to verify that different pCTR values can also have effects on the calibration shape. (5) Ablation Experiment of Embedding Augmentation. To enhance the expressive ability of embeddings, the self-attention mechanism is used. Then we delete this part by using the original Equation 12 rather than Equation 14. The fifth line on Table. 3 shows that the embedding augmentation mechanism can enhance the representational power of embedding. 5.3.2 DESC exhibits a stronger data utilization capability. In this part, we analyze the data utilization capabilities of different calibrators from both a global perspective (overall down sampling) and an individual perspective (field value sample quantity). Figure 7: Down sampling analysis in CRITEO. (1) Overall Down Sampling We further conducted experiments to compare the calibration performance of four different hybrid field-aware methods (FAC, Ada, MBCT and DESC) across various sampling ratios. As shown in Figure 7, DESC exhibits a distinct lower MF-ECE in the context of varying sampling ratios. In a global perspective, this demonstrates that DESC exhibits a stronger data utilization capability. (2) Field Value Sample Quantity. Weanalysis the performance differences among MBCT, FAC, Ada and DESC at sample quantity aspect. We set the quantity of samples with a specific field value (for the convenience of display, we use 𝑙𝑜𝑔 10 ( 𝑥 ) ) as the x-axis and 𝐸𝐸𝑅 𝐶𝑎𝑙𝑖𝑏 ( E xcept Calibration E rror R atio) as the y-axis (the smaller 𝐸𝐸𝑅 𝐶𝑎𝑙𝑖𝑏 is, the better DESC is compared with other calibrators), as shown in Figure 8.  ECE (Expected Calibration Error) [13] is used as the calibration error to explain the results. The ECE value ranges from ( 0 , +∞] . Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Shuai Yang, Hao Yang, Zhuang Zou, Linhe Xu, Shuo Yuan, and Yifan Zeng Figure 8: Performance differences for the sample quantity of the field value. 3.50 3.75 4.00 4.25 4.50 4.75 5.00 1 3.50 3.75 4.00 4.25 4.50 4.75 5.00 3.50 3.75 4.00 4.50 4.75 5.00 Less Sample Quantity) More The higher ECE value indicates higher calibration error and worse performance. We can find that for the samples with small sized of number, 𝐸𝐸𝑅 is less than 1, which means, from an individual perspective, that DESC possesses a stronger data utilization capability. 5.3.3 DESC Can Adapt to Complex Miscalibration Shapes. We analysis the performance differences among MBCT, FAC, Ada and DESC at shape complexity. From the perspective of pCTR, we divide the estimated pCTR values into bins either with equal frequency or equal interval, and each bin can calculate the PCOC (Predicted Click Over Click) [8, 12] to describe the over- and under-estimation within that bin. For some cases, the over- and under-estimation within each bin are close, while in other cases, there is a significant difference in over- and under-estimation for each bin. We define the \"Miscalibration Complexity\" metric to characterize the disparity in over- and under-estimation across different pCTR bins. We selected all field values for a subset of fields. For field 𝑧 with a value 𝑣 , the samples are divided into 𝑄 ( 𝑄 > 1) bins. The miscalibration complexity is denoted as D 𝑍,𝑣 as follows:  In general, the higher the miscalibration complexity, the higher the disparity in over- and under-estimation within the bins. For a calibrator, a higher miscalibration complexity indicates a greater calibration challenge. On one hand, it requires higher precision in shaping the accuracy of the calibration, and on the other hand, an excessive reliance on posterior data can compromise the generalization capability of the calibrator. We still use the ECE metric to characterize the calibration error of different calibrators under the specific field value. We compare MBCT, FAC, Ada, and DESC and obtain the 𝐸𝐸𝑅 𝐶𝑎𝑙𝑖𝑏 . For each calibrator and each field value, there is a corresponding D 𝑍,𝑣 value as the x-axis and 𝐸𝐸𝑅 𝐶𝑎𝑙𝑖𝑏 as the y-axis. As shown in Figure 9, we can observe that in the high D 𝑍,𝑣 range, all 𝐸𝐸𝑅 𝐶𝑎𝑙𝑖𝑏 values are less than 1, which indicates that DESC always performs better in conditions of complex shapes compared to other methods.",
  "5.4 Online Results": "To prove the effectiveness of the proposed method, we further conduct real-world online experiments. We feed item ID with 5 million unique values, category ID with 5,000 unique values, bidding type, 1 0.02 0.04 0.06 0.08 0.10 0.02 0.04 0.06 0.08 0.10 0.02 0.04 0.06 0.08 0.10 Miscalibration Complexity ~ Complex Easy Figure 9: Performance differences for miscalibration complexity aspect. time, raw predict score and other features into DESC. Then the model is deployed for online pCVR calibration on Shopee's advertising system. For the online A/B test, we build two experimental buckets, each with ten percent of the traffic. One bucket was configured with SIR [15] as the control group, while the other was configured with DESC as the experimental group. We use the two significant important business metrics CVR (Conversion Rate) and GMV (Gross Merchandise Volume) to evaluate our calibration effectiveness. The online results over a 7-day period indicated that DESC brings +2.5% on CVR and +4.0% on GMV compared to SIR. These improvements verify the effectiveness and business values of DESC in practical advertising system.",
  "6 DISCUSSION": "Wepropose a new calibration approach named DESC. Firstly, we put forward multiple types of functions ( e.g. , power function, scaling function, logarithmic function) as the basis functions. Secondly, we allocate the appropriate basis functions to combine the shape function given the specific field and specific value. Finally, all fields are concatenated and used to calculate the weight for the calibration value from each field, which can further improve the accuracy of pCTR on multiple fields. Both offline and online experiments verify that DESC achieves significant improvement. In future work, we should explore the performance of our proposed method on other machine learning tasks except CXR task. Also, we need to study the reasons of miscalibration error in these machine learning tasks, especially for CXR task, and find new technology ( e.g. data augment, multi-tasking learning) to further improve the performance.",
  "REFERENCES": "[1] Antonio Bella, Cèsar Ferri, José Hernández-Orallo, and María José RamírezQuintana. 2010. Calibration of machine learning models. In Handbook of Research on Machine Learning Applications and Trends: Algorithms, Methods, and Techniques . IGI Global, 128-146. [2] Weijie Bian, Kailun Wu, Lejian Ren, Qi Pi, Yujing Zhang, Can Xiao, Xiang-Rong Sheng, Yong-Nan Zhu, Zhangming Chan, Na Mou, et al. 2022. CAN: feature co-action network for click-through rate prediction. In Proceedings of the fifteenth ACM international conference on web search and data mining . 57-65. [3] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems . 7-10. [4] James Davidson, Benjamin Liebald, Junning Liu, Palash Nandy, Taylor Van Vleet, Ullas Gargi, Sujoy Gupta, Yu He, Mike Lambert, Blake Livingston, et al. 2010. Multi-Field Post-hoc Calibration on Online Advertising Conference acronym 'XX, June 03-05, 2018, Woodstock, NY The YouTube video recommendation system. In Proceedings of the fourth ACM conference on Recommender systems . 293-296. [5] Chao Deng, Hao Wang, Qing Tan, Jian Xu, and Kun Gai. 2021. Calibrating user response predictions in online advertising. In Machine Learning and Knowledge Discovery in Databases: Applied Data Science Track: European Conference, ECML PKDD 2020, Ghent, Belgium, September 14-18, 2020, Proceedings, Part IV . Springer, 208-223. [6] Chao Du, Zhifeng Gao, Shuo Yuan, Lining Gao, Ziyan Li, Yifan Zeng, Xiaoqiang Zhu, Jian Xu, Kun Gai, and Kuang-Chih Lee. 2021. Exploration in online advertising systems with deep uncertainty-aware learning. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining . 2792-2801. [7] Shai Feldman, Stephen Bates, and Yaniv Romano. 2023. Calibrated multipleoutput quantile regression with representation learning. Journal of Machine Learning Research 24, 24 (2023), 1-48. [8] Thore Graepel, Joaquin Quinonero Candela, Thomas Borchert, and Ralf Herbrich. 2010. Web-scale bayesian click-through rate prediction for sponsored search advertising in microsoft's bing search engine. Omnipress. [9] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. 2017. On calibration of modern neural networks. In International conference on machine learning . PMLR, 1321-1330. [10] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. arXiv preprint arXiv:1703.04247 (2017). [11] Chirag Gupta, Aleksandr Podkopaev, and Aaditya Ramdas. 2020. Distributionfree binary classification: prediction sets, confidence intervals and calibration. Advances in Neural Information Processing Systems 33 (2020), 3711-3723. [12] Xinran He, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Yanxin Shi, Antoine Atallah, Ralf Herbrich, Stuart Bowers, et al. 2014. Practical lessons from predicting clicks on ads at facebook. In Proceedings of the eighth international workshop on data mining for online advertising . 1-9. [13] Siguang Huang, Yunli Wang, Lili Mou, Huayue Zhang, Han Zhu, Chuan Yu, and Bo Zheng. 2022. MBCT: Tree-Based Feature-Aware Binning for Individual Uncertainty Calibration. In Proceedings of the ACM Web Conference 2022 . 22362246. [14] Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. 2019. FiBiNET: combining feature importance and bilinear feature interaction for click-through rate prediction. In Proceedings of the 13th ACM Conference on Recommender Systems . 169-177. [15] Xiaoqian Jiang, Melanie Osl, Jihoon Kim, and Lucila Ohno-Machado. 2011. Smooth isotonic regression: a new method to calibrate predictive models. AMIA Summits on Translational Science Proceedings 2011 (2011), 16. [16] Kevin B Korb. 1999. Calibration and the evaluation of predictive learners. In Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence . Citeseer, 73-77. [17] Meelis Kull, Miquel Perello Nieto, Markus Kängsepp, Telmo Silva Filho, Hao Song, and Peter Flach. 2019. Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with dirichlet calibration. Advances in neural information processing systems 32 (2019). [18] Meelis Kull, Telmo Silva Filho, and Peter Flach. 2017. Beta calibration: a wellfounded and easily implemented improvement on logistic calibration for binary classifiers. In Artificial Intelligence and Statistics . PMLR, 623-631. [19] Ananya Kumar, Percy S Liang, and Tengyu Ma. 2019. Verified uncertainty calibration. Advances in Neural Information Processing Systems 32 (2019). [20] Wonbin Kweon, SeongKu Kang, and Hwanjo Yu. 2022. Obtaining Calibrated Probabilities with Personalized Ranking Models. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 36. 4083-4091. [35] Han Zhu, Junqi Jin, Chang Tan, Fei Pan, Yifan Zeng, Han Li, and Kun Gai. 2017. Optimized cost per click in taobao display advertising. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining . 2191-2200.",
  "keywords_parsed": [
    "Multi-Field Calibration",
    " Basis Calibration Function",
    " Field-aware Attention Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise",
    " or republish",
    " to post on servers or to redistribute to lists",
    " requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference acronym 'XX",
    " June 03-05",
    " 2018",
    " Woodstock",
    " NY © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-XXXX-X/18/06...$15.00 https://doi.org/XXXXXXX.XXXXXXX"
  ]
}