{"CT4Rec: Simple yet Effective Consistency Training for Sequential Recommendation": "Chong Liu \u2217 Tencent Inc. China nickcliu@tencent.com Xiaoyang Liu \u2217 OPPO Inc. China liuxiaoyang@oppo.com Rongqin Zheng Tencent Inc. China leonezheng@tencent.com Lixin Zhang Tencent Inc. China lixinzhang@tencent.com Xiaobo Liang Soochow University China xbliang3@stu.suda.cn Juntao Li \u2020 Soochow University China ljt@suda.edu.cn Lijun Wu Microsoft Research Asia China lijunwu@microsoft.com Min Zhang Soochow University China minzhang@suda.edu.cn Leyu Lin Tencent Inc. China goshawklin@tencent.com", "ABSTRACT": "Sequential recommendation methods are increasingly important in cutting-edge recommender systems. Through leveraging historical records, the systems can capture user interests and perform recommendations accordingly. State-of-the-art sequential recommendation models proposed very recently combine contrastive learning techniques for obtaining high-quality user representations. Though effective and performing well, the models based on contrastive learning require careful selection of data augmentation methods and pretext tasks, efficient negative sampling strategies, and massive hyper-parameters validation. In this paper, we propose an ultrasimple alternative for obtaining better user representations and improving sequential recommendation performance. Specifically, we present a simple yet effective C onsistency T raining method for sequential Rec ommendation (CT4Rec) in which only two extra training objectives are utilized without any structural modifications and data augmentation. Experiments on three benchmark datasets and one large newly crawled industrial corpus demonstrate that our proposed method outperforms SOTA models by a large margin and with much less training time than these based on contrastive learning. Online evaluation on real-world content recommendation system also achieves 2.717% improvement on the click-through rate and 3.679% increase on the average click number per capita. Further exploration reveals that such a simple method KDD '23, August 6-10, 2023, Long Beach, CA, USA https://doi.org/10.1145/3580305.3599798 has great potential for CTR prediction. Our code is available at https://github.com/ct4rec/CT4Rec.git.", "CCS CONCEPTS": "\u00b7 Information systems \u2192 Information retrieval .", "KEYWORDS": "Recommender Systems, Sequential Recommendation, Consistency Training", "ACMReference Format:": "Chong Liu, Xiaoyang Liu, Rongqin Zheng, Lixin Zhang, Xiaobo Liang, Juntao Li, Lijun Wu, Min Zhang, and Leyu Lin. 2023. CT4Rec: Simple yet Effective Consistency Training for Sequential Recommendation. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '23), August 6-10, 2023, Long Beach, CA, USA. ACM, New York, NY, USA, 13 pages. https://doi.org/10.1145/3580305.3599798", "1 INTRODUCTION": "Recommendation systems have been extensively applied in online platforms nowadays, e.g., Amazon [30], Google [1, 5, 22] and Facebook [19]. Due to the dynamic interactions between users and items, it is essential to capture evolving user interests from users' historical records. To accurately represent user interests and make an appropriate recommendation, many efforts have been paid to study sequential recommendation methods [15, 24, 45, 50]. Generally, the sequential recommendation task aims to characterize user representation from users' historical behaviors and predict the expected item accordingly. In viewing the great success of deep learning for sequential dependency modeling and representation learning, many methods based on deep neural networks [14] have been introduced and proposed to solve this task, covering RNN-Based frameworks [15, 32, 44], different CNNs blocks and structures [45, 46], Graph Neural Networks (GNNs) [37, 50, 51], and also model variants [24, 27, 43] relied on the powerful multi-head self-attention [47]. Though performing well, these methods might suffer from the data sparsity problem [24, 42, 63] for sequential recommendation, especially for models built on the multi-head self-attention mechanism, where only one single item prediction loss is used to optimize the full model parameters for capturing all possible correlations in input interaction sequences. To address the above challenge, various self-supervised learning strategies are introduced [55, 62]. Among these, the recently introduced contrastive learning (CL) objective [53] achieves very promising results. Through combing with effective data augmentation strategies and cooperating with the vanilla sequential prediction objective, the CL-based method can learn better sequence-level user representations and enhance the performance of sequential recommendations. However, the effectiveness of CL-based approaches is subject to the correlated data augmentation methods, pretext tasks, efficient negative sampling, and hyper-parameters selection (e.g., the temperature in NCE and InfoNCE losses) [20]. To mitigate the above drawbacks, many efforts have been made to simplify contrastive learning. [8] propose a very simple yet effective contrastive learning scheme that utilizes dropout noise as data augmentation to construct high-quality positive samples for sequence-level representation learning. Although such a scheme is effective for unsupervised representation learning, adapting it into the sequential recommendation task will still encounter the dilemma in which a higher proportion of CL objective in model training will lead to better representations that can easily distinguish positive and negative samples but might result in worse item prediction performance and vice versa. In other words, there is a noticeable gap between the discrimination of positive and negative samples and the task objective for the sequential recommendation. Thus, the key to obtaining better user representations mainly for the item prediction task is designing more training strategies that can address the inherent issues of sequential recommendation models. Inspired by the recent observation on the multi-head attention model that a very simple regularization strategy imposed on the output space of supervised tasks yields striking performance improvement [29] (achieving SOTA on many challenging tasks), we propose to thoroughly explore the effect of consistency training for the sequential recommendation task. We first introduce the simple bidirectional KL divergence regularization into the output space to constrain the inconsistency between two forward passes with different dropouts. Unlike previous machine translation, summarization, and natural language understanding tasks, we argue that the introduced consistency regularization merely in the output space is not enough for the data sparsity setting of sequential recommendation. We then design a novel and simple regularization objective in the representation space. Unlike previous studies that utilize cosine [8] and L2 [35, 64] distance to regularize the representation space, we propose to regularize the distributed probability of each user representation over others. Thus, we can extend and leverage the effective bidirectional KL loss to regularize the representation inconsistency. Experiments on three public benchmarks and one newly collected large-scale corpus indicate that our proposed simple consistency training for sequential recommendation (CT4Rec) outperforms state-of-the-art methods based on contrastive learning by a large margin. Extensive experiments further prove that our proposed consistency training can be easily extended to the data side. The Online A/B test also confirms the effectiveness of our method. Besides, we extend our consistency training method to the CTR prediction task, and experiments conducted on two newly constructed industrial datasets further prove the effectiveness of our method. In a nutshell, we mainly have the following contributions: \u00b7 We propose a simple (with only two bidirectional KL losses) yet very effective consistency training method for sequential recommendation systems. To the best of our knowledge, this is the first work to thoroughly study the effect of consistency training from different perspectives and with a unified training objective for the sequential recommendation task. \u00b7 Our proposed consistency training method can be easily extended to other inconsistency scenarios and tasks, e.g., data augmentation and CTR prediction. \u00b7 Extensive experiments on four offline datasets show the effectiveness of our proposed CT4Rec over SOTA models based on contrastive learning with much better performance and faster convergence time. The Online A/B test also shows significant improvement over the strong ensemble model.", "2 RELATED WORK": "Early sequential recommendation (SR) methods are usually based on Markov Chain (MC), including adopting first-order MC [40] and high-order MCs [12, 13]. As for current deep learning approaches, they can be generally divided into four categories, i.e., RNN-based [15, 16, 21, 26, 31, 32, 38, 44], CNN-based [45, 46, 52], attentionbased [24, 27, 43, 56, 57, 60] and GNN-based [25, 37, 48, 50, 51, 61] methods. Concretely, GRU4Rec [15] applies RNN to SR and many variants have been proposed based on GRU4Rec by adding data augmentation GRU4Rec+ [44], hierarchical RNN [38] and attention module [2, 26, 34]. However, RNN-based methods usually exhibit worse performance than CNN-based and attention-based methods for the data sparsity setting. Caser [45] proposes a convolutional sequence embedding recommendation model, and [46] use 3-dimensional CNNs to achieve character-level encoding of input data. Meanwhile, attention mechanisms [47] is used to model user behavior sequences and have achieved outstanding performance [7, 23, 24, 27, 33, 43]. Besides, many researches [37, 50, 51] combine attention mechanisms and GNNs to solve the SR task. Memory networks [3, 18], data augmentation [9, 54] and session-based [17] models are also utilized to improve the performance of SR. Recently, the combination of contrastive learning and attention mechanisms [53, 62] is widely used in SR and achieves great success. CauseRec [59] performs contrastive learning by contrasting the counterfactual with the observational. StackRec [49] utilizes stacking and fine-tuning to develop a deep but easy-to-train SR model. ICAI-SR [58] focuses on the complex relations between items and categorical attributes in SR. Unlike previous researches, we only introduce two consistency training objectives in the representation and output spaces without any structure modifications, extra data, and heuristic patterns from tasks. Though extremely simple, our method outperforms recent SOTA models by a large margin and is more efficient for model training. There are only a few related researches in the fields of natural language process [8] and machine learning [29, 35, 64]. Specifically, [8] introduce dropout as the alternative of data augmentation into contrasting learning for sequence representation learning while we focus on the inconsistency introduced by dropout in the data Cbasic SA FF SA LDR LRD Lbasic CTARec P(d Click Sequence units dropped units N x N x sparsity SR task. Our proposed method is also extremely simple compared with the paradigm of combining contrastive learning with the traditional item prediction objective. [35, 64] mainly focus on the gap between training and testing and utilize L2 for regularizing the representation space, which is less effective in the data sparsity setting, represented by the marginal to none performance improvements in Section 5. Different from introducing a regularization objective in the output space to constrain the randomness of sub-models brought by dropout [29], we focus on the consistency training of the data sparsity SR task from both the representation and output space. We also propose a simple yet effective regularization strategy in the representation space to compensate and align the output space consistency loss.", "3 THE CT4REC MODEL": "The overall structure of our model is illustrated in Figure 1. Before elaborating our proposed CT4Rec, we first present some necessary notations to describe the sequential item prediction task. Let U = ( \ud835\udc62 1 , \ud835\udc62 2 , ..., \ud835\udc62 | U| ) denote a set of users, and V = ( \ud835\udc63 1 , \ud835\udc63 2 , ..., \ud835\udc63 | V | ) denote a set of items. The sequence for user \ud835\udc62 \u2208 U is denoted as \ud835\udc60 \ud835\udc62 = ( \ud835\udc63 ( \ud835\udc62 ) 1 , \ud835\udc63 ( \ud835\udc62 ) 2 , ..., \ud835\udc63 ( \ud835\udc62 ) \ud835\udc61 , ..., \ud835\udc63 ( \ud835\udc62 ) | \ud835\udc60 \ud835\udc62 | ) , where \ud835\udc63 ( \ud835\udc62 ) \ud835\udc61 \u2208 V is the item that user \ud835\udc62 interacts at time step \ud835\udc61 and | \ud835\udc60 \ud835\udc62 | is the length of sequence \ud835\udc60 \ud835\udc62 . Given the historical sequence \ud835\udc60 \ud835\udc62 , the task of sequential recommendation is to predict the probability of all alternative items to be interacted by user \ud835\udc62 at time step | \ud835\udc60 \ud835\udc62 | + 1, which is formulated as \ud835\udc43 ( \ud835\udc63 ( \ud835\udc62 ) | \ud835\udc60 \ud835\udc62 | + 1 = \ud835\udc63 | \ud835\udc60 \ud835\udc62 ) .", "3.1 Backbone Model": "Since our proposed consistency training method does not involve structural modification and extra data utilization, we apply it to the widely used SASRec model [24]. Following the original setting in SASRec, the transformer encoder contains three parts, i.e., an embedding layer, the stacked multi-head self-attention blocks, and a prediction layer. Then, we can obtain user representation \ud835\udc94 \ud835\udc96 = \ud835\udc53 ( \ud835\udc60 \ud835\udc62 ) , where \ud835\udc53 (\u00b7) indicates the transformer encoder. To learn the relation between users and items in sequential recommendation, a similarity function, e.g., inner product, is applied to measure distances between user representation and item representation. Thus, for user representation \ud835\udc94 \ud835\udc96,\ud835\udc95 of user \ud835\udc62 at time step \ud835\udc61 , we can get a similarity distribution P( \ud835\udc94 \ud835\udc96,\ud835\udc95 ) = P( \ud835\udc63 ( \ud835\udc62 ) \ud835\udc61 + 1 | \ud835\udc94 \ud835\udc96,\ud835\udc95 ) to predict the item that user \ud835\udc62 will interact at time step \ud835\udc61 + 1. Then, the basic loss function with positive item \ud835\udc97 + \ud835\udc95 + 1 and randomly sampled negative items \ud835\udc97 -\ud835\udc95 + 1 \u2208 V is denoted as: , where \ud835\udf14 refers to all trainable parameters of the model.", "3.2 Consistency Training": "Since the over-fitting problem extensively exists in deep neural network models, regularization methods, including dropout, are widely used to alleviate this problem. Commonly, dropout can reduce over-fitting and co-adapting by randomly removing a certain rate of units in the whole deep neural network. Also, dropout can be treated as a method to generate and combine exponentially submodels, which always effectively enhances model performance. Considering the above advantages and the randomness of dropout, we propose our CT4Rec based on dropout to regularize both the LRD softmax sottmax Softmax Softmax Item Embeddings Input units dropped units sim(s2,, (a) (b) output space and the representation space of models. Inspired by recent studies on dropout [29], we enhance the user representation from the perspective of reducing the model inconsistency and gap between training and testing. Concretely, we forward twice with different dropouts and learn the consistency between these two representations for each user, i.e., each user interaction sequence su passing the forward network twice and obtain two representations \ud835\udc94 \ud835\udc85 1 \ud835\udc96,\ud835\udc95 and \ud835\udc94 \ud835\udc85 2 \ud835\udc96,\ud835\udc95 . Since dropout randomly removes units in a model, the two representations are actually generated from two sub-models of the same model. Regularized Dropout Loss (RD). We first apply a regularized dropout loss to constrain the output space of sub-models from dropout. Considering two representations \ud835\udc94 \ud835\udc85 1 \ud835\udc96,\ud835\udc95 and \ud835\udc94 \ud835\udc85 2 \ud835\udc96,\ud835\udc95 for user \ud835\udc62 , as mentioned in Function 1, we can get two similarity distributions P( \ud835\udc94 \ud835\udc85 1 \ud835\udc96,\ud835\udc95 ; \ud835\udf14 ) and P( \ud835\udc94 \ud835\udc85 2 \ud835\udc96,\ud835\udc95 ; \ud835\udf14 ) . Then, we introduce a bidirectional KLdivergence loss to regularize the above two distributions: As shown in Figure 2(a), the dropped units of the left model to generate user presentation \ud835\udc94 \ud835\udc85 1 \ud835\udc96,\ud835\udc95 are different from that of the right model to generate \ud835\udc94 \ud835\udc85 2 \ud835\udc96,\ud835\udc95 . Therefore, the similarity distributions P( \ud835\udc94 \ud835\udc85 1 \ud835\udc96,\ud835\udc95 ; \ud835\udf14 ) and P( \ud835\udc94 \ud835\udc85 2 \ud835\udc96,\ud835\udc95 ; \ud835\udf14 ) are also varied for the same input sequence su . Distributed Regularization Loss (DR). To better regularize the representation space, we propose a distributed regularization method in which each user is represented by its correlations with other users rather than directly utilizing user representations for consistency regularization. In this paper, we compare users in each mini-batch, e.g., \ud835\udc5b users ( \ud835\udc62 1 , \ud835\udc62 2 , ..., \ud835\udc62 \ud835\udc5b ) and two representations for each user generated by dropout denoted as ( \ud835\udc94 \ud835\udc85 1 \ud835\udc96 1 , \ud835\udc94 \ud835\udc85 1 \ud835\udc96 2 , ..., \ud835\udc94 \ud835\udc85 1 \ud835\udc96 \ud835\udc8f ) and ( \ud835\udc94 \ud835\udc85 2 \ud835\udc96 1 , \ud835\udc94 \ud835\udc85 2 \ud835\udc96 2 , . . . , \ud835\udc94 \ud835\udc85 2 \ud835\udc96 \ud835\udc8f ) . As shown in Figure 2(b), for user \ud835\udc62 1, we calculate the similarities between \ud835\udc94 \ud835\udc85 \ud835\udc8b \ud835\udc96 1 and all the other user representation Then, a bidirectional KL-divergence loss is applied to regularize the two distributions P \ud835\udc62 ( \ud835\udc94 \ud835\udc85 1 \ud835\udc96 1 ; \ud835\udf14 ) and P \ud835\udc62 ( \ud835\udc94 \ud835\udc85 2 \ud835\udc96 1 ; \ud835\udf14 ) , defined as: Final Objective. As shown in Figure 1, we train the above two objectives together with the task-specific loss in the backbone model. The task-specific loss and final training objective are as: where \ud835\udefc and \ud835\udefd are the coefficient weights to control L \ud835\udc45\ud835\udc37 and L \ud835\udc37\ud835\udc45 . Thus, our CT4Rec can control the influence of dropout and constrain the model space. Compared with Equation 2, our CT4Rec only adds two losses L \ud835\udc45\ud835\udc37 and L \ud835\udc37\ud835\udc45 with model structures unchanged, which can also be widely applied on various model structures. We present the training algorithm in the Appendix A.2.", "4 EXPERIMENTS": "", "4.1 Datasets": "We conduct extensive experiments on three public benchmark datasets that are widely used in recent literature [53] and a new large-scale dataset collected from a real-world recommendation scenario, i.e., PC-WeChat Top Stories. These datasets are very different in domains, platforms, and data scale, where their detailed statistics are presented in Table 1. \u00b7 Amazon: a series of datasets comprise product reviews, which are crawled from one of the largest E-Commerce platforms, i.e., Amazon.com . As introduced in SASREC [24, 36], these datasets are separated by top-level product categories. We follow one of the most recent researches [53] to utilize the Beauty and Sports categories for comparison. \u00b7 Yelp: it is a widely acknowledged dataset for the business recommendation, which is collected from the Yelp platform 1 . Following [63], we leverage the data after January 1st, 2019, and treat business categories as attributes. \u00b7 WeChat: this dataset is constructed from WeChat platform for PC Top Stories recommendation (denoted as WeChat to distinguish datasets from other platforms), which consists of interaction records from 7th to 13rd, June 2021. Each interaction provides positive feedback (i.e., click) of an item from a user. Concretely, we collect 9.5 million interactions from 0.74 million users on 0.21 million items and regard data from the first few days as the train set and the rest for testing.", "4.2 Baselines": "To verify the effectiveness of our proposed method, we introduce four representative baselines and three very recent methods. \u00b7 GRU4Rec [15]. It utilizes GRU modules to model user action sequences for the session-based recommendation. We consider each user's click sequence as a session. \u00b7 SASRec [24]. It applies the multi-head self-attention mechanism to solve the sequential recommendation task, which is commonly treated as one of the state-of-the-art baselines. In this paper, we utilize SASRec as the backbone of our CT4Rec. \u00b7 TiSASRec [27]. Based on SASRec, it further introduces time interval aware self-attention mechanism to encode the user's interaction sequence, where the positions and time interval between any two items are considered. \u00b7 BERT4Rec [43]. It uses the deep bidirectional self-attention mechanism to model user interaction history in the sequential recommendation and trains the model like BERT [6]. \u00b7 CL4SRec [53]. It generates different views of the same user interaction sequence by using data augmentation methods and adds contrastive learning objective to the original objective of SASRec for sequential recommendation tasks. \u00b7 CLRec. [62] design a queue-based contrastive learning method named CLRec to de-bias deep candidate generation in the recommendation system and further propose Multi-CLRec for multi-intention aware bias reduction. In this work, we only compare the CLRec for fairness. \u00b7 StackRec [49]. It first uses a stacking operation on the pretrained layers/blocks to transfer knowledge from a shallow model to a deep model and then utilizes iterative stacking to obtain a deeper but easier-to-train recommendation model.", "4.3 Settings": "All models are implemented based on TensorFlow. For baselines with official codes, we utilize the implementations provided by authors. As for models without open-accessible codes from the original paper, we prefer the well-tested version from the opensource community. Specifically, we use code from https://github. com/Songweiping/GRU4Rec_TensorFlow as the implementation of GRU4Rec [15]. We implement CL4SRec and CLRec based on the model descriptions and experimental settings of the correlated papers since there is no official code or popular implementation from the open-source community. For fair comparisons, the embedding dimension size is set to 50, and all models are optimized by Adam. Recall that our proposed CT4Rec method does not modify the model architecture and increases the model scale of the backbone model. Instead, it only involves two effective consistency regularization strategies. Thus, we follow the backbone method, i.e., SASRec [24], to implement our CT4Rec. We use two self-attention layers and set the head number to 2. The maximum sequence length is 50 for all datasets. We optimize the parameters with the learning rate of 0.001 and the batch size as 128. The dropout rate of turning off neurons is set to 0.5. To verify the effect of each component and their combination, we fix the structure and other hyper-parameters of the model and only adjust the values of \ud835\udefc and \ud835\udefd , where \ud835\udefc and \ud835\udefd are selected from { 0 . 1 , 0 . 3 , 0 . 5 , 1 . 0 , 2 . 0 , 3 . 0 } . Other details for reproducing our experiments can be found in our anonymous code.", "4.4 Offline Evaluation": "4.4.1 Evaluation Protocols. Following many previous studies [14, 24, 53, 63], we employ the leave-one-out strategy to evaluate model performance. Specifically, for each user, we take the last interacted item for test. Similar to [24, 63], we randomly sample 500 items from the whole dataset for each positive item, and rank them by similarity scores. The model performances are evaluated by top-k Normalizedrand Discounted Cumulative Gain (NDCG@ \ud835\udc58 ) and top-k Hit Ratio (HR@ \ud835\udc58 ), which are both commonly used in top-k recommendation systems. Specifically, we report HR@ \ud835\udc58 and NDCG@ \ud835\udc58 with \ud835\udc58 = { 5 , 10 , 20 } for all datasets. 4.4.2 Experimental Results. As shown in Table 2, our proposed CT4Rec outperforms all other baseline methods, including multiple representative models and state-of-the-art sequential recommendation solutions, on three benchmark datasets and one large industrial corpus. Compared with other strong methods that utilize data augmentation or/and contrastive learning to obtain better user representations and mitigate the incompatibility between the single item prediction task and the vast amounts of parameters in data sparsity scenarios [24], our CT4Rec is simpler and more effective without requiring any augmented data or delicate training strategies in which only two extra objectives are introduced. The performance advance of CT4Rec over SOTA models based on contrastive learning is confirmed by the significant improvements of HR@ \ud835\udc58 , and NDCG@ \ud835\udc58 scores over CLRec and CL4SRec. We also observe that the training objective in recent baselines performs much better than the original binary cross-entropy in SASRec , which is demonstrated by the results that SASRec* increase offline scores by a large margin over SASRec . Notice that our CT4Rec is implemented by introducing two training objectives into SASRec* . We Table 2: Model performance of baselines and our proposed CT4Rec on four offline datasets, where '*' refers to modifying the original binary cross-entropy loss in SASRec with the training objective in recent baselines, e.g., CLRec, CL4SRec, StackRec. Our CT4Rec is implemented on SASRec*. Improv. and Improv.* refer to the relative improvement of CT4Rec over SASRec and SASRec*, respectively. The performance improvement over baselines is statistically significant with \ud835\udc5d < 0 . 01 , in which we present the experimental results of extra two runs with different random seeds in the Appendix A.4. compare CT4Rec with SASRec* to calibrate the effect of our consistency training method. The universal enhancement of CT4Rec over SASRec* on four datasets and all HR@ \ud835\udc58 and NDCG@ \ud835\udc58 (relative improvements ranging from 5.34% to 16.50%) verify the superiority of our proposed simple method.", "4.5 Online Evaluation": "4.5.1 Evaluation Protocols. We further perform an online A/B test that resembles [11] to evaluate our CT4Rec in a real-world system. Commonly, an online recommendation system is divided into four stages, including matching, pre-ranking, ranking, and re-ranking. We have deployed CT4Rec for PC Wechat Top Stories recommendation in the matching stage. To calibrate the effect of CT4Rec for the online system, CT4Rec is implemented as an additional channel in the current matching module with the rest of the system unchanged, where the existing matching module refers to an ensemble model that combines multiple methods, e.g., rule-based, reinforcementbased, sequence-based, DSSM, self-distillation. In the online A/B test, we utilize two metrics, i.e., click-through rate (CTR) and average click number per capita (ACN), to evaluate model performance. The online test lasts for 7 days and involves nearly 3 million users. 4.5.2 Experimental Results. Theperformance improvement of CT4Rec on real-world recommendation service is reported in Table 3 with the significance level \ud835\udc5d < 0 . 01, which can be seen that only implementing CT4Rec as an additional channel in the matching module with the rest of the system unchanged can yield very impressive performance improvement over the original ensemble model. Our method increases CTR and ACN metrics by 2.717% and 3.679%, respectively. This indicates that our proposed simple method is not only effective for offline benchmarks and evaluation metrics but also generalizes well to the real-world online system. Since CT4Rec restricts the uncertainty of sub-models without involving extra model structure, the computation cost of CT4Rec for online serving is identical to the original backbone model. Thus, CT4Rec can significantly enhance online performance without adding online computation costs, which is a definite advantage for online serving, especially considering the machine costs.", "5 ANALYSIS": "Wehave demonstrated the impressive performance of our proposed simple CT4Rec on offline benchmarks and the online A/B test. In 0 205 0 205 0 205 0 200 0 200 0 200 0 18 80195 80195 80195 8 0 17 0 190 0 190 0 190 0 16 0 185 0 185 0 185 0 180 0 180 0 180 0 15 0 1 03 05 10 20 30 0 1 03 05 10 20 30 0 1 0.3 05 10 20 30 0 1 0 2 03 0 4 05 0.6 031 031 031 0 28 0 27 0 30 0 30 0 26 0 29 0 29 0 29 0 25 0 28 0 28 0 24 01 05 10 20 30 01 03 05 10 20 30 01 03 05 10 20 30 01 0 2 03 0 4 0.6 (a) CTARec(basic + RD) (b) CT4Reclbasic + DR) (c) CTARec(basic + cosine) (d) CTARec(basic +L2) this section, we launch extensive experiments to understand and analyze CT4Rec from different perspectives. For convenience, these studies are performed on Beauty . More specifically, we mainly focus on: 1) the effect of each introduced objective ( Ablation Study ), 2) the influence of several important hyper-parameters ( Hyper-Parameter Analysis ), 3) the extension of CT4Rec to data augmentation ( Extension to Data Augmentation ), 4) the change of training process and cost resulted by CT4Rec ( Training and Cost Analysis ) SASRec : 032 0 20 0 18 2 028 5 026 0 16 0 24 0 14 0 22 0 2 08 09 0 2 03 0 4 05 dropout rate dropout rate", "5.1 Ablation Study": "We perform an ablation study to explore the effect of two objectives in Section 3, i.e., regularized dropout (RD) and distributed regularization (DR), where the results are illustrated in Figure 3. RDObjective. As shown in the left sub-figure (a), the introduced RD objective achieves significant performance improvement over the backbone SASRec* model. Compared with DR objective and its variants, RD loss contributes most to the performance increase, which concludes that launching consistency regularization in the model output space is the most beneficial to SASRec* , which is similar to the observations in other tasks [29]. The possible reason might be that RD consistency regularization affects directly on the same space of the model output probability distribution. DR Objective. We can see that the unsupervised DR loss can also yield substantial performance gains, which points out that unsupervised consistency regularization in user representation space for data sparsity sequential recommendation task is also essential. We also adapt two typical unsupervised strategies in previous studies [8, 35, 64] to regularize user representations, including cosine similarity and L2 distance. As shown in the right two sub-figures in Figure 3, these two methods have not brought meaningful improvement upon the backbone method, which further proves that our designed DR objective is more preferable for consistency regularization in the representation space.", "5.2 Hyper-Parameter Analysis": "We mainly consider several critical hyper-parameters in this part, including \ud835\udefc and \ud835\udefd in Equation 7, and the dropout rate. The Effect of \ud835\udefc . Figure 3 also gives the results of different \ud835\udefc values. Considering that there are many feasible combinations of the ( \ud835\udefc , \ud835\udefd ) grid, we temporarily remove the DR objective and only examine the influence of \ud835\udefc for RD. It can be observed that a small value of \ud835\udefc can bring meaningful performance improvement. With the increase of \ud835\udefc , the performance improvement further increases in which the best result is achieved when \ud835\udefc = 2 . 0. These results further confirm that the consistency regularization directly performed on the output space is very effective even when it only takes a small proportion for the final training objective. With a further increase of \ud835\udefc , the model will pay more attention to the consistency of model outputs, which will dilute the original item prediction objective, resulting in worse performance (e.g., \ud835\udefc = 2 . 0 v.s., \ud835\udefc = 3 . 0). The Influence of \ud835\udefd . Similar to the analysis of \ud835\udefc , we only study the impact of \ud835\udefd for each single unsupervised regularization loss (i.e., DR, cosine, and L2). Different from \ud835\udefc , the DR loss has no influence on the overall performance when the value of \ud835\udefd <0.3. This is probably because the consistency regularization on the user representations is overwhelmed and adapted when passing to the output space. With the increase of \ud835\udefd , DR loss gradually produces a more important role and consistently improves model performance until \ud835\udefd >2.0. And also, a large \ud835\udefd value will force the model to focus on the consistency of user representation rather than perform the item prediction task. For the cosine objective, different \ud835\udefd values have a limited influence on evaluation results. As for the L2 loss, a large \ud835\udefd value will cause inferior performance. Analysis on Dropout Rates. Besides the above analysis, we also study how the dropout rate affects the effectiveness of our CT4Rec since different dropout rates will lead to varying degrees of inconsistency. As demonstrated in Figure 4, dropout rates have a significant influence on the performance of the backbone model, and our proposed consistency training is applicable and effective when the dropout rate<0.7. However, our proposed CT4Rec will lead to a negative effect when the dropout rate>0.8. We speculate that the reason behind this might be the data sparsity issue and irreconcilable inconsistency.", "5.3 Extension to Data Augmentation": "CT4Rec SASRec: CL4SRec 0.250 0.225 0.225 8 0.200 2 0.200 0.175 0.175 0.150 0.150 0.125 0.125 105 120 135 150 400 800 1200 1600 2000 0.350 0.350 0.325 0.325 0.300 0,300 8 0.275 8 0.275 0.250 0.225 0.225 0.200 0.200 105 120 135 150 400 800 1200 1600 2000 epoch umels) The above studies prove the effectiveness of each component of our CT4Rec and its capability of leveraging more consistency regularization signals. We then study its generalization ability to other inconsistency scenarios. More concretely, we utilize our consistency training method to regularize the inconsistency brought by data other than the above-mentioned model-side inconsistency, i.e., we replace dropout with data augmentation methods to create two different user representations, and the corresponded task outputs for each user so as to conduct consistency regularization. To align with recent sequential recommendation methods based on constrastive learning, we leverage two easily implemented augmentation strategies from CL4SRec [24], i.e., Reorder and Mask . In doing so, we can directly compare the effects of different training objectives (besides the item prediction one for the sequential recommendation) on the augmented data, including unsupervised methods (i.e., CL, cosine, L2, DR), the supervised regularization in the output space (RD), and the combination of DR and RD (CT4Rec). Table 4 presents the experimental results for the data augmentation setting. We can observe that: 1) Our introduced DR objective is the most effective compared with other single methods, i.e., the consistency regularization on the representation space is the most preferable for than data augmentation scenario, which is in contrast with the observation for the dropout setting. We speculate that the labelinvariant data augmentation methods can lead to permuted and perturbed representation variants, which need more consistency regularization, while the label-invariant strategy does not deteriorate the inconsistency in the output space. 2) The combination of the consistency regularization in the representation space and the output space (CT4Rec) still performs the best with consistent and significant performance over other training objectives.", "5.4 Training and Cost Analysis": "Since our CT4Rec does not modify the model structure of the backbone model or introduce extra augmented data, we mainly analyze the changes in the training process. We plot the curves of HR@10and NDCG@10 scores on the valid set along the training epoch number and time (seconds) for SASRec* , CL4SRec and our CT4Rec models, shown in Figure 5. At the early training stage, the backbone SASRec* model converges quickly with the same training epochs (around 15 epochs) and model performance as CT4Rec, but our CT4Rec can continuously improve the performance on the welltrained SASRec* model. It concludes that our consistency training objectives do not lead to more training epochs on the backbone SASRec* . But for CL4SRec that combines contrastive learning objective with SASRec* , it converges with much more training epochs and only moderate performance improvement. As for convergence time (seconds), our model indeed is slower than the backbone model since our consistency training objectives need an extra forward process (dropout) in each training step but achieves a much superior performance. Compared with CL4SRec , our CT4Rec is much more efficient and effective with a much better final optimum and less convergence time even when we haven't calculated the time cost of data augmentation and negative sampling for CL4SRec . Through the analysis, we can conclude that: 1) CT4Rec is more efficient and effective than the method based on contrastive learning even without counting its time cost of data augmentation and negative sampling; 2) CT4Rec indeed introduces extra training time for the backbone model, which can be mitigated by early stop insomuch as our CT4Rec can quickly surpass the backbone model in the early stage and with a much better final convergence performance.", "6 EXTENSION TO CTR PREDICTION": "Besides the application of CT4Rec on the matching stage, we further explore the effectiveness of our C onsistency T raining for the CTR prediction task denoted as CT4CTR, serving on the ranking stage. Concretely, for each instance ( \ud835\udc99 , \ud835\udc66 ) \u2208 D , \ud835\udc99 denotes a multi-filed feature vector input, and label \ud835\udc66 \u2208 { 0 , 1 } indicates whether the user clicks the item. The CTR prediction is to obtain the probability \u02c6 \ud835\udc66 that a user will click a certain item in a given context. Weutilize a widely used structure DeepFM in this section, which simply applies two components (i.e., FM component and deep component) and still achieves promising performance in the industry. For each input \ud835\udc99 , we forward it twice in the deep component of DeepFM with different dropouts and learn the consistency between these two sub-models. Similar to Sec. 3, two regularization methods (i.e. RD loss, DR loss) are utilized to constrain the two submodels generated from dropout. Concretely, input \ud835\udc99 passes the deep component twice and obtains \u02c6 \ud835\udc66 \ud835\udc51 1 \ud835\udc37\ud835\udc41\ud835\udc41 and \u02c6 \ud835\udc66 \ud835\udc51 2 \ud835\udc37\ud835\udc41\ud835\udc41 with different dropouts. Thus, with the unchanged part \u02c6 \ud835\udc66 \ud835\udc39\ud835\udc40 , the final prediction can be defined as \u02c6 \ud835\udc66 \ud835\udc51 1 and \u02c6 \ud835\udc66 \ud835\udc51 2 and a two-dimensional distribution P( \ud835\udc99 \ud835\udc51 \ud835\udc56 ) = ( \u02c6 \ud835\udc66 \ud835\udc51 \ud835\udc56 , 1 -\u02c6 \ud835\udc66 \ud835\udc51 \ud835\udc56 ) can be formulated to calculate RD loss. Meanwhile, we compare the deep representations of instances in each mini-batch,e.g., ( \ud835\udc99 \ud835\udc85 1 1 , \ud835\udc99 \ud835\udc85 1 2 , . . . , \ud835\udc99 \ud835\udc85 1 \ud835\udc8f ) and ( \ud835\udc99 \ud835\udc85 2 1 , \ud835\udc99 \ud835\udc85 2 2 , . . . , \ud835\udc99 \ud835\udc85 2 \ud835\udc8f ) . Similar to the matching task, for instance, \ud835\udc99 1 , we can calculate the similarity \ud835\udc60\ud835\udc56\ud835\udc5a ( \ud835\udc99 \ud835\udc85 \ud835\udc8b 1 , \ud835\udc99 \ud835\udc85 \ud835\udc8b \ud835\udc8a ) , \ud835\udc56 = 2 , 3 , . . . , \ud835\udc5b , so as to obtain similarity distribution between deep representations denoted as P \ud835\udc60 ( \ud835\udc99 \ud835\udc85 \ud835\udc8b 1 ) . Then, similar to Eq. 5, a bidirectional KL-divergence loss to regularize the two distributions P \ud835\udc60 ( \ud835\udc99 \ud835\udc85 1 1 ) and P \ud835\udc60 ( \ud835\udc99 \ud835\udc85 2 1 ) can be denoted as DR loss. We compare CT4CTR with: (1) LR [41], a simple baseline model for CTR prediction, which only models the linear combination of raw features. (2) FM [39]. Since LR fails to capture non-linear feature interactions, the factorization machine (FM) has been proposed to model second-order feature interactions. It is notable that FM only has a linear time complexity in terms of the number of features. (3) Wide&Deep[4]. With the development of deep models, Google achieves great improvement by combining a wide (or shallow) network and a deep network. This is a general learning framework that can achieve the advantages of both wide networks and deep networks. (4) DeepFM [10], which extends Wide&Deep by substituting LR with FM to precisely model second-order feature interactions. (5) xDeepFM[28], which captures high-order interactions by its core module, Compressed Interaction Network (CIN). CIN takes an outer product of a stacked feature matrix in a vector-wise way. (6) AutoInt[42], which automatically models the high-order interactions of input features by using self-attention networks. To evaluate the performance of CT4CTR, we build two private industrial datasets from the WeChat ecosystem: Wechat-Video and Wechat-Article, and both of them are collected from a real-world recommendation scenario, i.e., WeChat Subscriptions. We choose them because they are sampled from real click logs in production, and both have tens of millions of samples, making the results meaningful to industrial practitioners. We split Train and Test sets in chronological order, and Table 5 summarizes the detailed statistics. As shown in Table 6, our proposed CT4CTR outperforms all other baseline methods in CTR prediction on AUC, including multiple state-of-the-art CTR prediction solutions, on two large industrial corpora. Besides, we have deployed CT4CTR on a real-world recommendation system that serves nearly one billion users with dramatically high online machine costs. In the online A/B test, we achieve +1.203% on the Video scenario and +2.341% on the Article scenario, as for the CTR metric. Moreover, it is a remarkable success that CT4CTR can enhance online performance without involving extra computation costs and machine costs, which is essential for online serving. Besides, the improvement of such a mature system that already has stable and advanced online models is extremely challenging, which further proves the effectiveness of CT4CTR.", "7 CONCLUSION AND FUTURE WORK": "In this paper, we proposed a simple yet very effective consistency training method for the sequential recommendation task, namely CT4Rec, which only involves two bidirectional KL losses. We first introduce a top-performed regularization in the output space by minimizing the bidirectional KL loss of two different outputs. We then design a novel consistency training term in the representation space by minimizing the distributed probability of two user representations. Extensive experiments and analysis demonstrate its effectiveness, efficiency, generalization ability, and compatibility. Besides experiments on the recall task in recommendation systems, further exploration reveals that the introduced consistency training strategies (i.e., DR and RD) are still very effective for the CTR prediction task. In the near future, we will thoroughly study the pros and cons of consistency training for the CTR task.", "REFERENCES": "[1] Minmin Chen, Alex Beutel, Paul Covington, Sagar Jain, Francois Belletti, and Ed H Chi. 2019. Top-k off-policy correction for a REINFORCE recommender system. In Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining . 456-464. [2] Tong Chen, Hongzhi Yin, Hongxu Chen, Rui Yan, Quoc Viet Hung Nguyen, and Xue Li. 2019. Air: Attentional intention-aware recommender systems. In 2019 IEEE 35th International Conference on Data Engineering (ICDE) . IEEE, 304-315. [3] Xu Chen, Hongteng Xu, Yongfeng Zhang, Jiaxi Tang, Yixin Cao, Zheng Qin, and Hongyuan Zha. 2018. Sequential recommendation with user memory networks. In Proceedings of the eleventh ACM international conference on web search and data mining . 108-116. [4] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems . 7-10. [5] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks for youtube recommendations. In Proceedings of the 10th ACM conference on recommender systems . 191-198. [6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) . 4171-4186. [7] Xinyan Fan, Zheng Liu, Jianxun Lian, Wayne Xin Zhao, Xing Xie, and Ji-Rong Wen. 2021. Lighter and better: low-rank decomposed self-attention networks for next-item recommendation. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1733-1737. [8] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple Contrastive Learning of Sentence Embeddings. arXiv preprint arXiv:2104.08821 (2021). [9] Chuan Guo, Ali Mousavi, Xiang Wu, Dan Holtmann-Rice, Satyen Kale, Sashank Reddi, and Sanjiv Kumar. 2019. Breaking the glass ceiling for embedding-based classifiers for large output spaces. (2019). [10] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. arXiv preprint arXiv:1703.04247 (2017). [11] Xiaobo Hao, Yudan Liu, Ruobing Xie, Kaikai Ge, Linyao Tang, Xu Zhang, and Leyu Lin. 2021. Adversarial Feature Translation for Multi-domain Recommendation. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining . 2964-2973. [12] Ruining He, Wang-Cheng Kang, and Julian McAuley. 2017. Translation-based recommendation. In Proceedings of the eleventh ACM conference on recommender systems . 161-169. [13] Ruining He and Julian McAuley. 2016. Fusing similarity models with markov chains for sparse sequential recommendation. In 2016 IEEE 16th International Conference on Data Mining (ICDM) . IEEE, 191-200. [14] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural collaborative filtering. In Proceedings of the 26th international conference on world wide web . 173-182. [15] Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2015. Session-based recommendations with recurrent neural networks. arXiv preprint arXiv:1511.06939 (2015). [16] Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural computation 9, 8 (1997), 1735-1780. [17] Liang Hu, Longbing Cao, Shoujin Wang, Guandong Xu, Jian Cao, and Zhiping Gu. 2017. Diversifying Personalized Recommendation with User-session Context.. In IJCAI . 1858-1864. [18] Jin Huang, Wayne Xin Zhao, Hongjian Dou, Ji-Rong Wen, and Edward Y Chang. 2018. Improving sequential recommendation with knowledge-enhanced memory networks. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval . 505-514. [19] Jui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip Pronin, Janani Padmanabhan, Giuseppe Ottaviano, and Linjun Yang. 2020. Embeddingbased retrieval in facebook search. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 2553-2561. [20] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. 2021. A survey on contrastive self-supervised learning. Technologies 9, 1 (2021), 2. [21] How Jing and Alexander J Smola. 2017. Neural survival recommender. In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining . 515-524. [22] Manas R Joglekar, Cong Li, Mei Chen, Taibai Xu, Xiaoming Wang, Jay K Adams, Pranav Khaitan, Jiahui Liu, and Quoc V Le. 2020. Neural input search for large scale recommendation models. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 2387-2397. [23] Taegwan Kang, Hwanhee Lee, Byeongjin Choe, and Kyomin Jung. 2021. Entangled Bidirectional Encoder to Autoregressive Decoder for Sequential Recommendation. In Proceedings of the 44th International ACM SIGIR Conference on Research you need. In Advances in neural information processing systems . 5998-6008. [48] Chenyang Wang, Min Zhang, Weizhi Ma, Yiqun Liu, and Shaoping Ma. 2020. Make it a chorus: knowledge-and time-aware item modeling for sequential recommendation. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . 109-118. [49] Jiachun Wang, Fajie Yuan, Jian Chen, Qingyao Wu, Min Yang, Yang Sun, and Guoxiao Zhang. 2021. StackRec: Efficient Training of Very Deep Sequential Recommender Models by Iterative Stacking. In Proceedings of the 44th International ACM SIGIR conference on Research and Development in Information Retrieval . 357-366. [50] Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, and Tieniu Tan. 2019. Session-based recommendation with graph neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 33. 346-353. [51] Chengfeng Xu, Pengpeng Zhao, Yanchi Liu, Victor S Sheng, Jiajie Xu, Fuzhen Zhuang, Junhua Fang, and Xiaofang Zhou. 2019. Graph Contextualized SelfAttention Network for Session-based Recommendation.. In IJCAI , Vol. 19. 39403946. [52] Chengfeng Xu, Pengpeng Zhao, Yanchi Liu, Jiajie Xu, Victor S Sheng S. Sheng, Zhiming Cui, Xiaofang Zhou, and Hui Xiong. 2019. Recurrent convolutional neural network for sequential recommendation. In The world wide web conference . 3398-3404. [53] Fei Sun Xu Xie, Zhaoyang Liu, Shiwen Wu, Jinyang Gao, Bolin Ding, and Bin Cui. 2021. Contrastive Learning for Sequential Recommendation. (2021). [54] Tiansheng Yao, Xinyang Yi, Derek Zhiyuan Cheng, Felix Yu, Ting Chen, Aditya Menon, Lichan Hong, Ed H Chi, Steve Tjoa, Jieqi Kang, et al. 2020. Self-supervised Learning for Large-scale Item Recommendations. arXiv preprint arXiv:2007.12865 (2020). [55] Tiansheng Yao, Xinyang Yi, Derek Zhiyuan Cheng, Felix Yu, Ting Chen, Aditya Menon, Lichan Hong, Ed H Chi, Steve Tjoa, Jieqi Kang, et al. 2020. Self-supervised learning for deep models in recommendations. arXiv e-prints (2020), arXiv-2007. [56] Haochao Ying, Fuzhen Zhuang, Fuzheng Zhang, Yanchi Liu, Guandong Xu, Xing Xie, Hui Xiong, and Jian Wu. 2018. Sequential recommender system based [64] Konrad Zolna, Devansh Arpit, Dendi Suhubdy, and Yoshua Bengio. 2018. Fraternal Dropout. In International Conference on Learning Representations .", "A APPENDIX": "", "A.1 Model structure of CT4CTR": "The model structure of CT4CTR is shown in Figure 6. CT4CTR takes user features and item features as input and outputs click probabilities for each user and item pair in the ranking stage of recommendation. For the deep component of DeepFM, input features are transformed into vector representations via the embedding layer and deep neural network with different hidden dropout masks. In addition, Distributed Regularization Loss and Regularized Dropout Loss are introduced to restrain these representations generated by different dropout masks. Shura CDR CRp ONN P(z\") Constraint", "A.2 Training Algorithm": "The whole training process of CT4Rec is presented in Algorithm 1. As shown in Line 3-5, we obtain two user representations \ud835\udc94 \ud835\udc85 1 \ud835\udc96 \ud835\udc8a ,\ud835\udc95 and \ud835\udc94 \ud835\udc85 2 \ud835\udc96 \ud835\udc8a ,\ud835\udc95 by going forward the model twice for each user sequence \ud835\udc60 \ud835\udc62 \ud835\udc56 ,\ud835\udc61 . Line 6-7 calculate the L \ud835\udc53 \ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc59 according to the loss function 7, and update the model parameters. The training process will continue until convergence.", "A.3 More Backbone": "To verify the universal influence of CT4Rec, we also apply it to GRU4Rec [15] denoted as CT4Rec \ud835\udc3a and evaluate its performance on four datasets. Table 8 shows that CT4Rec brings significant improvement compared with the original GRU4Rec on all datasets and all HR@ \ud835\udc58 and NDCG@ \ud835\udc58 scores (relative improvements ranging from 3.48% to 11.51%), which indicates that our method can be widely applied to different model structures and achieve enhancement.", "A.4 More Experimental Results": "Here, we present the experimental results of extra two runs with different random seeds."}
