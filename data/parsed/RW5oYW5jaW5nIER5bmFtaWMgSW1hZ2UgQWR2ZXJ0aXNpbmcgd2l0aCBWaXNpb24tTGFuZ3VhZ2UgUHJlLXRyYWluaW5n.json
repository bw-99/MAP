{
  "Enhancing Dynamic Image Advertising with Vision-Language Pre-training": "Zhoufutu Wen ∗ Baidu Search Ads, Baidu Inc. Beijing, China wenzhoufutu01@baidu.com Xinyu Zhao ∗† Peking University Beijing, China xinyuzhao@stu.pku.edu.cn Zhipeng Jin Baidu Search Ads, Baidu Inc. Beijing, China jinzhipeng@baidu.com",
  "Yi Yang ‡": "Baidu Search Ads, Baidu Inc. Beijing, China yangyi15@baidu.com Wei Jia Baidu Search Ads, Baidu Inc. Beijing, China jiawei04@baidu.com Xiaodong Chen Baidu Search Ads, Baidu Inc. Beijing, China chenxiaodong@baidu.com Shuanglong Li Baidu Search Ads, Baidu Inc. Beijing, China lishuanglong@baidu.com Lin Liu Baidu Search Ads, Baidu Inc. Beijing, China liulin03@baidu.com (b) Existing method: task-specific training Knowledge Distillation (c) Ours: unified pre-training + fine-tuning Pre-training datasets ITC+ITM+MLM +Click Log ITM ITC Yoga training class Query Candidate images 1. Retrieval 2. Relevance modeling 3. Image ad generation (a) Dynamic Image Advertising Pre-trained Model Fine-tuned Relevance Model Fine-tuned Retrieval Model Ad System Log +Relevance Label ITM Image-Text Relevance Model Query-image pairs ITC Image-Text Retrieval Model Query-image click data Query-image pairs Figure 1: Dynamic Image Advertising illustration: (a) First, candidate images are retrieved based on their distance from query. Then, the images are re-ranked according to query-image relevance. (b) Previous solution is training task-specific models separately. (c) We adopt vision-language pre-training and fine-tuning to unify retrieval and relevance models.",
  "ABSTRACT": "In the multimedia era, image is an effective medium in search advertising. Dynamic Image Advertising (DIA), a system that matches queries with ad images and generates multimodal ads, is introduced to improve user experience and ad revenue. The core of DIA is a query-image matching module performing ad image retrieval and relevance modeling. Current query-image matching suffers from limited and inconsistent data, and insufficient cross-modal interaction. Also, the separate optimization of retrieval and relevance models affects overall performance. To address this issue, we propose a vision-language framework consisting of two parts. First, Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '23, July 23-27, 2023, Taipei, Taiwan © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-9408-6/23/07...$15.00 https://doi.org/10.1145/3539618.3591844 we train a base model on large-scale image-text pairs to learn general multimodal representation. Then, we fine-tune the base model on advertising business data, unifying relevance modeling and retrieval through multi-objective learning. Our framework has been implemented in Baidu search advertising system 'Phoneix Nest'. Online evaluation shows that it improves cost per mille (CPM) and click-through rate (CTR) by 1.04% and 1.865%.",
  "CCS CONCEPTS": "· Information systems → Image search .",
  "KEYWORDS": "cross-modal retrieval, search advertising, image retrieval",
  "ACMReference Format:": "Zhoufutu Wen, Xinyu Zhao, Zhipeng Jin, Yi Yang, Wei Jia, Xiaodong Chen, Shuanglong Li, and Lin Liu. 2023. Enhancing Dynamic Image Advertising with Vision-Language Pre-training. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '23), July 23-27, 2023, Taipei, Taiwan. ACM, New York, NY, USA, 6 pages. https://doi.org/10.1145/3539618.3591844 SIGIR '23, July 23-27, 2023, Taipei, Taiwan Zhoufutu Wen et al.",
  "1 INTRODUCTION": "In the era of rich media, multimodal advertisement (ad) especially image ad is becoming a popular form in search advertising. Baidu, as one of the leading search companies, introduces Dynamic Image Advertising (DIA), a system that pairs queries with ad images and generating multimodal ads, see Figure 1. The core of the system is a query-image matching module. Given a query, it first retrieves candidate images via approximate nearest neighbor (ANN) search, and then computes query-image similarity to rerank images for subsequent procedures such as click-through rate (CTR) prediction. A powerful query-image matching module not only improve user experience, but also increase ad clicks and revenue for advertisers and search engine. However, two aspects make query-image matching a challenging task: the multimodal and multi-objective problems. Regarding the multimodal feature, the queries and ad images are in two drastically different feature spaces, which need to be reconciled through cross-modal representation learning. Previous solution is to train models specifically for relevance and retrieval task, affecting overall performance. And the small models suffering from limited multimodal data and insufficient cross-modal interaction. In recent years, Vision-Language Pretraining (VLP) [11] achieves good performance in cross-modal tasks. Its main idea is self-supervised learning on large-scale unlabeled multimodal data to generate latent representations for downstream tasks. VLP is compatible with query-image matching, because there is a large number of query-image pairs available, and learning cross-modal representation helps measuring query-image similarity. As for the multi-objective aspect, due to the commercial nature of advertising, the system goal is to generate ads that are not only relevant to the query, but also higher in CTR. Previous matching module consists of two single-objective models: a retrieval model to recall high-CTR ad images, and a relevance model to rank queryimage similarity. Single-objective training may result in presenting ads that are either not relevant or not attractive. Also, developing separate models increases training and maintenance cost. Given the existing problems, in this paper, we propose a visionlanguage framework for query-image matching, composing of two parts. First, to learn general multimodal representations, we train a base model on large-scale multimodal datasets. Then, we further train the base model to build a relevance model and a multitask retrieval model. In the online and offline evaluation, our framework shows good representing ability and profitability. The contributions of this paper are as follows: · To improve image matching for diverse queries, on multi-domain multimodal datasets of 20B image-text pairs, we train a base model, incorporating dual encoders optimized by momentum contrastive loss (ITC), and a fusion encoder optimized by masked language modeling (MLM) and image-text matching (ITM). With the base model, image diversity ratio increases by over 3% and irrelevant case ratio on long-tail queries declines by 10%. ∗ Equal contribution. ‡ Corresponding author. † Work done during an internship at Baidu Search Ads. · To transfer the multimodal representation learning ability to advertising scenario, we further train the base model on queryimage pairs with relevance labels. Our relevance model outperforms previous one in AUC by over 4%. · To improve ad quality and profitability simultaneously, we unify knowledge distillation from the relevance model and further contrastive learning on ad click data. After launching the multitask retrieval model, CPM and CTR of our search advertising system grow by 1.04% and 1.865%",
  "2 RELATED WORK": "Inspired by the success of pre-training in NLP, VLP has progressed rapidly in recent years and has developed several sturctures. Some models consist only of encoders. According to how the two modalities interact, encoder models can be divided into two types. One deeply fuses vision and language embeddings through transformer layers, called fusion encoder, which is suitable for vision-language understanding tasks [2, 11, 18]. Another type, called dual encoder model, first encodes vision and language embeddings independently and then aligns them by metric learning, which makes it more efficient in vision-language retrieval tasks [10, 20]. Recently, there are studies combining different types of encoders and even decoders to exploit their strengths for different tasks [1, 15, 16, 23, 26]. Besides, image feature extraction has been the computational bottleneck of VLP. As vision transformer [5] has been proposed, many studies use it as vision backbone in VLP instead of object detectors and convolutional networks (CNN), which enhances model scalability and training efficiency [1, 11, 16]. Ad relevance modeling, an essential part in search advertising, is to understand search intent and then match relevant and profitable ads. Previous studies introduce commercial metrics such as CPM and CTR as additional optimization goals besides query-ad similarity [6, 31]. Recently, VLP has also been applied to multimodal search and search advertising. Trained on multimodal fashion data, FashionBERT and Kaleido-BERT are proposed to solve ITR in the ecommerce industry [7, 34]. AdsCVLR models the relevance among query, ad image and ad text [33]. Baidu, as a leading search engine provider in China, has also applied VLP to improve the online advertising platform and released a series of works [27, 29, 30]. However, existing works focus on relevance modeling ability and train multimodal models with only domain data. There is still room to better utilize general VLP model and data, and balance ad relevance and CTR in advertising scenarios.",
  "3 METHODS": "With VLP, we can improve query-image matching for long-tail, low-frequency queries. And we need a unified model to balance ad quality and profitability. Therefore, we propose a vision-language framework for image advertising. As shown in Figure 2, it follows the 'pre-training and then fine-tuning' paradigm, composing of three models: pre-trained base model, fine-tuned relevance model, and multitask retrieval model.",
  "3.1 Pre-training": "The pre-trained model consists of two single-modal encoders and a multimodal fusion encoder. The vision and language encoders are trained to align two modalities, while the fusion encoder helps capture fine-grained cross-modal interaction. We use ViT-B/16 [5] SIGIR '23, July 23-27, 2023, Taipei, Taiwan Enhancing Dynamic Image Advertising with Vision-Language Pre-training Figure 2: Overview of our vision-language framework. (a) We jointly pre-train two single-modal encoders with contrastive loss, and a multimodal encoder that fuses modalities through image-to-text cross attention with masked language modeling and image-text matching loss. Then, we fine-tune the single-modal encoders on two tasks, with the image part fixed (not updating parameters): (b) classification of relevant query-image pairs; (c) computing query-image cosine similarity to discriminate clicked pairs and fit relevance teacher model output simultaneously. Classification Image Encoder Text Encoder MLP Image Encoder Text Encoder Multimodal Encoder ITC ITM MLM image text image text (a) Pre-training Fine-tuning Relevance Model (b) Relevance model (c) Multitask learning Contrastive Learning + Knowledge Distillation Image Encoder image text Text Encoder Weights not updating (fixed) 12x x12 x6 12x x3 12x x12 12x Transformer layer number x2 and RoBERTa 𝑏𝑎𝑠𝑒 [19] as the vision and language encoders, processing embeddings of each modality separately with self-attention. The single-modal encoders are trained on multi-view Image-Text Contrastive learning (ITC) [21]. The contrastive loss, for which we use InfoNCE [22], aims to maximize the contrast between the similarity of positive pairs and that of negative pairs. Our multi-view ITC loss combines bidirectional cross-modal contrastive losses and two single-modal contrastive losses. The cross-modal contrastive learning helps align the text and image feature spaces, while the single-modal loss is to enhance the ability to distinguish the semantics within each modality. Following [9], we enhance negative sampling in ITC with momentum encoders, which are the exponential moving average version of single-modal encoders. Then, the output of the language encoder is fed into the fusion encoder, implemented with RoBERTa 𝑏𝑎𝑠𝑒 . It computes cross-modal attention that receives visual features as key and value inputs, producing multimodal language representation conditioned on the vision modality. The fusion encoder is trained on multimodal masked language modeling (MLM), and image-text matching (ITM). Our multimodal MLM is to predict masked language tokens based on both language context and image. Similar to BERT [4], the objective function of MLM is the cross-entropy loss between the prediction and the ground truth. ITM is to classify whether an image matches with a text (positive) or not (negative). The objective function of ITM is the cross-entropy loss between the classifier output and the ground truth label. We improve ITM with hard negative mining following [16], which encourages model to learn from meaningful negative pairs with high similarity.",
  "3.2 Fine-tuning": "To adapt our base model with general representing ability to different roles in the query-image matching module, we design two fine-tuning strategies, obtaining a relevance model and a multitask retrieval model. In fine-tuning, we stop updating the parameters of the vision encoder, to facilitate computation and prevent overfitting. · Relevance Model. To ensure ad quality, we expect high-relevant images to be selected. But a large part of pre-training data are noisy image-text pairs crawled from the Web that are not necessarily relevant. Besides, ad queries are shorter and more abstract than texts in general datasets. Therefore, we further train our base model on business data with relevance labels, transferring its representing ability to advertising domain data. The relevance model is fine-tuned on the dual encoders of our base model. Before fine-tuning, we use the base model to temporarily replace the retrieval model generating candidate image embeddings. In fine-tuning, candidate images and queries are fed into the relevance model. We place a MultiLayer Perceptron (MLP) on top of the dual encoders as classifier, which computes two-dimensional logits. We minimize the cross-entropy loss between the logits and relevance labels. · Multitask Retrieval Model. In query-image matching module, the input candidate images of the relevance model are determined by the retrieval model. To ensure ad quality and mine the commercial value of image ad and, we need to preserve the knowledge of query-image relevance, while encouraging high-CTR ad images to be favored by the retrieval model. Thus, we design a retrieval model with two objectives: modeling image commercial value on click data while learning query-ad relevance from the relevance teacher model. The multitask retrieval model is also fine-tuned from the dual encoders of our base model. The dual encoders learn a cosine similarity between query and image embeddings, jointly optimized by contrastive learning and knowledge distillation. For contrastive learning, we minimize the bidirectional cross-modal contrastive loss, to distinguish clicked query-image pairs (positive) from the others (negative). Different from the ITC in base model, we use inbatch negative sampling to facilitate computation. For knowledge distillation, we minimize the Euclidean distance between the cosine similarity and the classifier output of relevance model.",
  "4 EXPERIMENTS": "· Datasets. For pre-training base model, we gather 20B image-text pairs, including general image-text pairs, and advertising domain data from Baidu DIA system which are query-image pairs clicked more than once from January 2021 to July 2022. During pre-training, we perform data augmentation with RandAugment [3]. The dataset for fine-tuning relevance model is about 500K query-image pairs from the DIA system that are annotated with relevance degree SIGIR '23, July 23-27, 2023, Taipei, Taiwan Zhoufutu Wen et al. Table 1: Zero-shot performance on text-to-image retriveal on public and private datasets. Table 2: Experiment result of image retrieval between taskspecifically trained model and pre-trained model. ∈ { 0 , 1 , 2 } , where 0 is negative and 1 or 2 is positive. The data for multitask fine-tuning consists of 1.3B query-image pairs with at least 2 clicks in the past six months from Baidu DIA system, including about 2K unique images. · Evaluation of the base model. To evaluate the representing ability of our base model (ours 𝑏𝑎𝑠𝑒 ), we compare it with the basesize Chinese CLIP (CN-CLIP), the SOTA model in Chinese crossmodal retrieval [25]. We experiment on both Chinese public benchmark datasets and our business datasets. The public datasets include MSCOCOCN [17], Flickr30k-CN [13], Wukong [8]. Three private datasets are from different domains: search, advertising, and e-commerce. We evaluate model performance by text-to-image Recall@ 𝐾 , 𝐾 ∈ { 1 , 5 , 10 } , which measures whether the ground truth is included in the top K results. As shown in Table 1, the overall performance of our base model (ours) is comparable to CN-CLIP in public datasets, with a distinct advantage in business domain datasets, proving that it is powerful in commercial scenarios, while maintaining expressiveness for general data. The results on public datasets also show the influence of data distribution in multimodal pre-training. Pre-trained on translated descriptive image-text pairs (Visual Genome [12] and MSCOCO), CN-CLIP performs better on MSCOCO-CN and Flickr30K-CN that share the same data source, while our base model gains advantage on Wukong based on original noisy Chinese web content as our pre-training data. To evaluate model performance on image retrieval, we index a dataset of 860K images by our base model (ours 𝑏𝑎𝑠𝑒 ) and the previous retrieval model (previous 𝑟𝑒𝑡𝑟𝑖𝑒𝑣𝑎𝑙 ). Then, we randomly select 10K queries, conducting ANN search to retrieve their top-15 similar images from each index, to examine the ratio of retrieved images to all images (Diversity Ratio). In addition, we perform query-image matching on 1K long-tail queries on the two indexes, and invite annotators to label the query-image relevance (0 for irrelevant, 1 for relevant, and 2 for highly relevant). We record the ratio of pairs labeled as irrelevant (Irrelevant Ratio). As shown in Table 3: Comparison between relevance models. Table 4: Ablation study of multitask fine-tuning. Table 5: Online A/B testing result. Tabel 2, our base model surpasses previous task-specific model in both image retrieval on general queries and long-tail queries. In addition, we visualize the query-image matching results with our proposed framework in Appendix A. · Evaluation of the relevance model. To evaluate our relevance model (ours 𝑟𝑒𝑙𝑒𝑣𝑎𝑛𝑐𝑒 ), we compare it with the previous relevance model (previous 𝑟𝑒𝑙𝑒𝑣𝑎𝑛𝑐𝑒 ) [28]. They are tested on 10K query-image pairs, with AUC (area under the curve) as the metric. the previous model is trained to match a triple <query, image description, image>. While our relevance model separately encodes text query and visual content all by transformer. As shown in Table 3, the better performance of our relevance model demonstrates that it is possible to directly model vision-language interaction without image description as a medium. We also perform human evaluation on the relevance of query-image pairs showed online, where our relevance model outperforms previous one by 7%. · Evaluation of the multitask retrieval model. For ablation study, we compare our retrieval model fine-tuned with additional relevance knowledge distillation (retrieval w/ KD) to our base model (base) and a retrieval model fine-tuned with contrastive learning only (retrieval w/o KD). They are tested on 10K query-image click data. The evaluation metrics are Recall@10 and Relevance Degree (Rel@10), which is the average relevance score for the top 10 results. As shown in Table 4, the single-task retrieval model improves the recall of clicked ad images, but affects the relevance. With multitask fine-tuning, clicked ad recall and image relevance increase simultaneously. · Online Experiments. We compare the CPM, CTR, and 97th percentile tail latency (P97 latency) before and after lauching the proposed framework in Baidu search advertising system for 15 days. As shown in Table 5, the CPM and CTR increase by 1.04% and 1.865% respectively, which is a substantial gain considering it is observed on the system main traffic. Given that the new queryimage matching module is similar to the previous one in terms of computational cost and model size, the P97 latency shows no Enhancing Dynamic Image Advertising with Vision-Language Pre-training SIGIR '23, July 23-27, 2023, Taipei, Taiwan discernible increase, verifying our framework improves ad revenue without sacrificing system response time.",
  "5 CONCLUSION": "In the multimedia era, image ad become a powerful form of search advertising. We propose a vision-language framework for queryimage matching in image advertising. We first train a base model with powerful general representing ability, and then fine-tune it on business data to adapt to image advertising scenarios. Furthermore, we jointly optimize the base model to balance ad relevance and CTR. Offline and online experiments demonstrate its profitability and effectiveness in improving the quality of image ads.",
  "COMPANY PORTRAIT": "Baidu is a leading search engine platform and tech company in China with a strong AI foundation, encompassing AI infrastructure, core AI capabilities, as well as an open AI platform. Baidu provides online marketing services and non-marketing value added services, as well as products and services from new AI initiatives.",
  "PRESENTER BIO": "Zhoufutu Wen is an algorithm engineer at Baidu Search Ads. His research interests include multimodal content understanding, crossmodal information retrieval. Wen currently works on the construction of vision-language big model and its applications.",
  "REFERENCES": "[1] Hangbo Bao, Wenhui Wang, Li Dong, Qiang Liu, Owais Khan Mohammed, Kriti Aggarwal, Subhojit Som, Songhao Piao, and Furu Wei. 2022. VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts. In Advances in Neural Information Processing Systems (NeurIPS) . [2] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. 2019. UNITER: UNiversal Image-TExt Representation Learning. In Proceedings of the European Conference on Computer Vision (ECCV) . [3] Ekin Dogus Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V. Le. 2020. Randaugment: Practical automated data augmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) . [4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, (NAACL-HLT) . [5] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In Proceedings of the 9th International Conference on Learning Representations (ICLR) . [6] Miao Fan, Jiacheng Guo, Shuai Zhu, Shuo Miao, Mingming Sun, and Ping Li. 2019. MOBIUS: Towards the Next Generation of Query-Ad Matching in Baidu's Sponsored Search. In Proceedings of the 25th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD) . [7] Dehong Gao, Linbo Jin, Ben Chen, Minghui Qiu, Yi Wei, Y. Hu, and Haozhe Jasper Wang. 2020. FashionBERT: Text and Image Matching with Adaptive Loss for Cross-modal Retrieval. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR) . [8] Jiaxi Gu, Xiaojun Meng, Guansong Lu, Lu Hou, Minzhe Niu, Xiaodan Liang, Lewei Yao, Runhu Huang, W. Zhang, Xingda Jiang, Chunjing Xu, and Hang Xu. 2022. Wukong: A 100 Million Large-scale Chinese Cross-modal Pre-training Benchmark. In Advances in Neural Information Processing Systems (NeurIPS) . [9] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. 2020. Momentum Contrast for Unsupervised Visual Representation Learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . [10] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021. Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision. In Proceedings of the 38th International Conference on Machine Learning (ICML) . [11] Wonjae Kim, Bokyung Son, and Ildoo Kim. 2021. ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision. In Proceedings of the 38th International Conference on Machine Learning (ICML) . [12] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. 2017. Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations. International Journal of Computer Vision 123, 1 (2017), 32-73. https://doi.org/10.1007/s11263-016-0981-7 [13] Weiyu Lan, Xirong Li, and Jianfeng Dong. 2017. Fluency-Guided Cross-Lingual Image Captioning. In Proceedings of the 25th ACM international conference on Multimedia (MM) . [14] Chaozhuo Li, Bochen Pang, Yuming Liu, Hao Sun, Zheng Liu, Xing Xie, Tianqi Yang, Yanling Cui, Liangjie Zhang, and Qi Zhang. 2021. AdsGNN: BehaviorGraph Augmented Relevance Modeling in Sponsored Search. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR) . [15] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In Proceedings of the International Conference on Machine Learning (ICML) . [16] Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Gotmare, Shafiq R. Joty, Caiming Xiong, and Steven C. H. Hoi. 2021. Align before Fuse: Vision and Language Representation Learning with Momentum Distillation. In Advances in Neural Information Processing Systems (NeurIPS) . [17] Xirong Li, Chaoxi Xu, Xiaoxu Wang, Weiyu Lan, Zhengxiong Jia, Gang Yang, and Jieping Xu. 2018. COCO-CN for Cross-Lingual Image Tagging, Captioning, and Retrieval. IEEE Transactions on Multimedia 21 (2018), 2347-2360. [18] Xiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu, Pengchuan Zhang, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, and Jianfeng Gao. 2020. Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks. In Proceedings of the European Conference on Computer Vision (ECCV) . [19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv preprint arXiv:1907.11692 (2019). [20] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models From Natural Language Supervision. In Proceedings of the 38th International Conference on Machine Learning (ICML) . [21] Bin Shan, Weichong Yin, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. 2022. ERNIE-ViL 2.0: Multi-view Contrastive Learning for Image-Text Pre-training. arXiv preprint arXiv:2209.15270 (2022). [22] Aäron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation Learning with Contrastive Predictive Coding. arXiv preprint arXiv:1807.03748 (2018). [23] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. 2022. SimVLM: Simple Visual Language Model Pretraining with Weak Supervision. In Proceedings of the 10th International Conference on Learning Representations (ICLR) . [24] Zhirong Xu, Shiyang Wen, Junshan Wang, Guojun Liu, Liang Wang, Zhi-Xin Yang, Lei Ding, Yan Zhang, Di Zhang, Jian Xu, and Bo Zheng. 2022. AMCAD: Adaptive Mixed-Curvature Representation based Advertisement Retrieval System. In IEEE 38th International Conference on Data Engineering (ICDE) . [25] Antoine Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, and Chang Zhou. 2022. Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese. arXiv preprint arXiv:2211.01335 (2022). [26] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. 2022. CoCa: Contrastive Captioners are Image-Text Foundation Models. arXiv preprint arXiv:2205.01917 (2022). [27] Tan Yu, Zhipeng Jin, Jie Liu, Yi Yang, Hongliang Fei, and Ping Li. 2022. Boost CTR Prediction for New Advertisements via Modeling Visual Content. In IEEE International Conference on Big Data (Big Data) . [28] Tan Yu, Xiaokang Li, Jianwen Xie, Ruiyang Yin, Qing Xu, and Ping Li. 2021. MixBERT for Image-Ad Relevance Scoring in Advertising. In Proceedings of the 30th ACM International Conference on Information and Knowledge Management (CIKM) . [29] Tan Yu, Xuemeng Yang, Yan Jiang, Hongfang Zhang, Weijie Zhao, and Ping Li. 2021. TIRA in Baidu Image Advertising. In IEEE 37th International Conference on Data Engineering (ICDE) . [30] Tan Yu, Yi Yang, Yi Li, Lin Liu, Hongliang Fei, and Ping Li. 2021. Heterogeneous Attention Network for Effective and Efficient Cross-modal Retrieval. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR) . [31] Jianjin Zhang, Zheng Liu, Weihao Han, Shitao Xiao, Rui Zheng, Yingxia Shao, Hao Sun, Hanqing Zhu, Premkumar Srinivasan, Denvy Deng, Qi Zhang, and Xing Xie. 2022. Uni-Retriever: Towards Learning the Unified Embedding Based Retriever in Bing Sponsored Search. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD) . SIGIR '23, July 23-27, 2023, Taipei, Taiwan Zhoufutu Wen et al. [32] Jason Zhu, Yanling Cui, Yuming Liu, Hao Sun, Xue Li, Markus Pelger, Liangjie Zhang, Tianqi Yan, Ruofei Zhang, and Huasha Zhao. 2021. TextGNN: Improving Text Encoder via Graph Neural Network in Sponsored Search. In Proceedings of the Web Conference 2021 (WWW) . [33] Yongjie Zhu, Chunhui Han, Yu-Wei Zhan, Bochen Pang, Zhaoju Li, Hao Sun, Si Li, Boxin Shi, Nan Duan, Weiwei Deng, Ruofei Zhang, Liangjie Zhang, and Qi Zhang. 2022. AdsCVLR: Commercial Visual-Linguistic Representation Modeling in Sponsored Search. In Proceedings of the 30th ACM International Conference on Multimedia (MM) . [34] Mingchen Zhuge, Dehong Gao, Deng-Ping Fan, Linbo Jin, Ben Chen, Haoming Zhou, Minghui Qiu, and Ling Shao. 2021. Kaleido-bert: Vision-language pre-training on fashion domain. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . Query: 多走路可以瘦腿吗 (Does Walking Reduce Thigh Fat) 1. (ours) (previous) Query: 15w 贷 5 年利息 (5-year ￥ 150,000 mortgage loan interest) 2. (ours) (previous) 15 $ Query: 老人机如何弄按键声音 (How to turn on button click sound on elderly-friendly cell phone) 3. (ours) (previous) Query: 搜一下初一的数学题 (Search seventh grade math problems) 4. Figure A1: Top-5 matched images for example queries. Queries are in Chinese and translated to English for reading convenience. (ours) denotes images retrieved by our proposed framework. (previous) denotes image retrieved by previous query-image matching module. (ours) (previous)",
  "A CASE STUDY": "We compare the query-to-image matching between our framework and previous method in Figure A1. For each query, we show the top-5 matched images. The results of our framework are more relevant to user search intent. For example, for the fourth query to search math problems, previous module includes a image of English words and some blurred images in the results, while our method retrieves clearer images about math equations. In the image matched for the third query, the previous module misidentifies the remote control as a cell phone. And our framework outperform previous one in terms of image diversity taking the second query as example, which can possibly improve image ad attractiveness when displaying several similar ads. Examining the detail of queries and images, it can be found out that our framework trained with close vision-language interaction is able to capture fine-grained information from queries and retrieve corresponding images. For instance, for the first query, the images our framework match not only contain the information about 'thigh' as previous module does, but also capture the information about 'walking'.",
  "keywords_parsed": [
    "cross-modal retrieval",
    "search advertising",
    "image retrieval"
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts"
    },
    {
      "ref_id": "b2",
      "title": "UNITER: UNiversal Image-TExt Representation Learning"
    },
    {
      "ref_id": "b3",
      "title": "Randaugment: Practical automated data augmentation with a reduced search space"
    },
    {
      "ref_id": "b4",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
    },
    {
      "ref_id": "b5",
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
    },
    {
      "ref_id": "b6",
      "title": "MOBIUS: Towards the Next Generation of Query-Ad Matching in Baidu's Sponsored Search"
    },
    {
      "ref_id": "b7",
      "title": "FashionBERT: Text and Image Matching with Adaptive Loss for Cross-modal Retrieval"
    },
    {
      "ref_id": "b8",
      "title": "Wukong: A 100 Million Large-scale Chinese Cross-modal Pre-training Benchmark"
    },
    {
      "ref_id": "b9",
      "title": "Momentum Contrast for Unsupervised Visual Representation Learning"
    },
    {
      "ref_id": "b10",
      "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision"
    },
    {
      "ref_id": "b11",
      "title": "ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision"
    },
    {
      "ref_id": "b12",
      "title": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"
    },
    {
      "ref_id": "b13",
      "title": "Fluency-Guided Cross-Lingual Image Captioning"
    },
    {
      "ref_id": "b14",
      "title": "AdsGNN: BehaviorGraph Augmented Relevance Modeling in Sponsored Search"
    },
    {
      "ref_id": "b15",
      "title": "BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation"
    },
    {
      "ref_id": "b16",
      "title": "Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"
    },
    {
      "ref_id": "b17",
      "title": "COCO-CN for Cross-Lingual Image Tagging, Captioning, and Retrieval"
    },
    {
      "ref_id": "b18",
      "title": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks"
    },
    {
      "ref_id": "b19",
      "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
    },
    {
      "ref_id": "b20",
      "title": "Learning Transferable Visual Models From Natural Language Supervision"
    },
    {
      "ref_id": "b21",
      "title": "ERNIE-ViL 2.0: Multi-view Contrastive Learning for Image-Text Pre-training"
    },
    {
      "ref_id": "b22",
      "title": "Representation Learning with Contrastive Predictive Coding"
    },
    {
      "ref_id": "b23",
      "title": "SimVLM: Simple Visual Language Model Pretraining with Weak Supervision"
    },
    {
      "ref_id": "b24",
      "title": "AMCAD: Adaptive Mixed-Curvature Representation based Advertisement Retrieval System"
    },
    {
      "ref_id": "b25",
      "title": "Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese"
    },
    {
      "ref_id": "b26",
      "title": "CoCa: Contrastive Captioners are Image-Text Foundation Models"
    },
    {
      "ref_id": "b27",
      "title": "Boost CTR Prediction for New Advertisements via Modeling Visual Content"
    },
    {
      "ref_id": "b28",
      "title": "MixBERT for Image-Ad Relevance Scoring in Advertising"
    },
    {
      "ref_id": "b29",
      "title": "TIRA in Baidu Image Advertising"
    },
    {
      "ref_id": "b30",
      "title": "Heterogeneous Attention Network for Effective and Efficient Cross-modal Retrieval"
    },
    {
      "ref_id": "b31",
      "title": "Uni-Retriever: Towards Learning the Unified Embedding Based Retriever in Bing Sponsored Search"
    },
    {
      "ref_id": "b32",
      "title": "TextGNN: Improving Text Encoder via Graph Neural Network in Sponsored Search"
    },
    {
      "ref_id": "b33",
      "title": "AdsCVLR: Commercial Visual-Linguistic Representation Modeling in Sponsored Search"
    },
    {
      "ref_id": "b34",
      "title": "Kaleido-bert: Vision-language pre-training on fashion domain"
    }
  ]
}