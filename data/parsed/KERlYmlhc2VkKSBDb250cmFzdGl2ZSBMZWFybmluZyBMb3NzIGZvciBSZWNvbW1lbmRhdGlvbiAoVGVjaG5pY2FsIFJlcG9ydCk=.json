{"title": "Debiased) Contrastive Learning Loss for Recommendation (Technical Report", "authors": "Ruoming Jin; Dong Li", "pub_date": "", "abstract": "In this paper 1 , we perform a systemic examination of the recommendation losses, including listwise (softmax), pairwise(BPR), and pointwise (mean-squared error, MSE, and Cosine Contrastive Loss, CCL) losses through the lens of contrastive learning. We introduce and study both debiased InfoNCE and mutual information neural estimator (MINE), for the first time, under the recommendation setting. We also relate and differentiate these two losses with the BPR loss through the lower bound analysis. Furthermore, we present the debiased pointwise loss (for both MSE and CCL) and theoretically certify both iALS and EASE, two of the most popular linear models, are inherently debiased. The empirical experimental results demonstrate the effectiveness of the debiased losses and newly introduced mutual-information losses outperform the existing (biased) ones.", "sections": [{"heading": "INTRODUCTION", "text": "Machine learning, including recommender systems [1,27], has played a pivotal role in transforming the way humans interact with and explore information on the internet [57], which has been ongoing for several years and continues to evolve with the integration of AI-generated content [32], significantly impacting the lives of individuals [59]. As recommendation algorithms continue to boom [1,61], largely thanks to deep learning models, many models' real effectiveness and claimed superiority has been called into question, such as various deep learning models vs. simple linear and heuristic methods [13], and the potential \"inconsistent\" issue of using itemsampling based evaluation [24,26,[28][29][30], among others. A major problem in existing recommendation research is their \"hyper-focus\" on evaluation metrics with often weak or not-fully-tuned baselines to demonstrate the progress of their newly proposed model [13]. However, recent research has shown that properly evaluating (simple) baselines is indeed not an easy task [47].\nTo deal with the aforementioned challenges, aside from standardized benchmarks and evaluation criteria, research communities have devoted efforts to developing and optimizing simple and stronger baseline models, such as the latest effort on finetuning iALS [45,46] and SimpleX (CCL) [37]. In the meantime, researchers [23] have started to theoretically analyze and compare different models, such as matrix factorization (iALS) vs. low-rank regression (EASE). By simplifying and unifying different models under a similar closed form, they observe how different approaches \"shrink\" singular values differently and thus provide a deep understanding and explain how and why the models behave differently, and the exact factor leads to a performance gap. However, the above work is limited only to linear models, and their regularization landscapes [31].\nTaking the cue from the aforementioned research, this paper aims to rigorously analyze and compare the loss functions, which play a key role in designing recommendation models [37]. This paper is also particularly driven by the emergence of contrastive learning loss [7,9,40] has shown great success in various learning tasks, such as computer vision and NLP, etc. Even though there are a few research works [55,63] in adopting contrastive learning paradigms into individual recommendation models, a systematic understanding of the recommendation loss function from the lens of contrastive learning is still lacking. For instance, how recommendation models can take advantage of (debiased) contrastive loss [9] (including mutual information-based losses [3]? How pairwise BPR [43] (Bayesian Pairwise Ranking) loss relate to these recent contrastive losses [3,9,40]? How can we debias the \ud835\udc3f2 (mean-squared-error, MSE) or \ud835\udc3f1 (mean-absolute-error, MAE) loss function with respect to the contrastive learning loss? Following this, does the well-known linear models, such as iALS [20,45,46] and EASE [51], needs to be debiased with respect to contrastive learning losses?\nTo answer these questions, we made the following contribution to this paper:\n\u2022 (Section 2) We revisit the softmax loss (and BPR loss) using the latest debiased contrastive loss [9], which provides a natural debias approach instead of typically rather sophisticated debias methods through propensity score [64]. We also introduce a mutual information neural estimator (MINE) loss [3] to the recommendation setting. To the best of our knowledge, this is the first study to study MINE for recommendation models. Experimental results demonstrate its (surprising) performance compared with other known losses. Finally, through the lower bound analysis, we are able to relate the contrastive learning losses and BPR loss. \u2022 (Section 3) We generalize the debiased contrastive loss [9] to the pointwise loss (MSE [20] and CCL [37]) in recommendation models and design the new debiased pointwise loss. We then examine the well-known linear models with pointwise loss, including iALS and EASE; rather surprisingly, our theoretical analysis reveals that both are inherently debiased under the reasonable assumption with respect to the contrastive learning losses. \u2022 (Section 4) We experimentally validate the debiased InfoNCE and point-wise losses indeed perform better than the biased (commonly used) alternatives. We also show the surprising performance of the newly introduced mutual informationbased recommendation loss and its refinement (MINE and MINE+). Finally, Section 5 reviews the related work, and Section 6 concludes. This paper towards to build a loss function theory for a recommendation based on contrastive learning.", "publication_ref": ["b0", "b26", "b56", "b31", "b58", "b0", "b60", "b12", "b23", "b25", "b27", "b28", "b29", "b12", "b46", "b44", "b45", "b36", "b22", "b30", "b36", "b6", "b8", "b39", "b54", "b62", "b8", "b2", "b42", "b2", "b8", "b39", "b19", "b44", "b45", "b50", "b8", "b63", "b2", "b8", "b19", "b36"], "figure_ref": [], "table_ref": []}, {"heading": "CONTRASTIVE RECOMMENDATION LOSS", "text": "In this section, we introduce Debiased InfoNCE [40] and Mutual-Information based Loss (MINE) [3] to the recommendation settings. Note that, the study in this section, is not to claim those losses are new (they stem from the latest contrastive learning research [2,9]), but to bring them into the recommendation community. Indeed, to our surprise, those losses from contrastive learning have not been fully explored and leveraged by the recommendation models. To reemphasize, our goals are two folds in this section: 1) demonstrating the benefits of these contrastive learning inspired losses are useful for recommendation models; 2) utilizing the lens of contrastive learning to carefully reexamine the losses recommendation community have designed before towards better understanding and unifying them. Notation: We use U and I to denote the user and item sets, respectively. For an individual user \ud835\udc62 \u2208 U, we use I + \ud835\udc62 to denote the set of items that have been interacted (such as Clicked/Viewed or Purchased) by the user \ud835\udc62, and I\\ I + \ud835\udc62 to represent the remaining set of items that have not been interacted by user \ud835\udc62. Similarly, U + \ud835\udc56 consists of users interacting with item \ud835\udc56, and U \\ U + \ud835\udc56 includes remaining users. Often, we denoted \ud835\udc5f \ud835\udc62\ud835\udc56 = 1 if item \ud835\udc62 is known to interact with of item \ud835\udc56 and \ud835\udc5f \ud835\udc62\ud835\udc56 = 0 if such interaction is unknown.\nGiven this, most of the recommendation systems utilize either matrix factorization (shallow encoder) or deep neural architecture to produce a latent user vector \ud835\udc63 \ud835\udc62 and a latent item vector \ud835\udc63 \ud835\udc56 , then use their inner product (< \ud835\udc63 \ud835\udc62 , \ud835\udc63 \ud835\udc56 >) or cosine (< \ud835\udc63 \ud835\udc62 /||\ud835\udc63 \ud835\udc62 ||, \ud835\udc63 \ud835\udc56 /||\ud835\udc63 \ud835\udc56 || >) to produce a similarity measure \u0177\ud835\udc62\ud835\udc56 . The loss function is then used to produce a (differentiable) measure to quantify and encapsulate all such \u0177\ud835\udc62\ud835\udc56 .", "publication_ref": ["b39", "b2", "b1", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Basic (Sampled) Softmax and InfoNCE loss", "text": "Softmax Loss: In recommendation models, a softmax function is often utilized to transform the similarity measure into a probability measure [11,63]:\n\ud835\udc5d (\ud835\udc56 |\ud835\udc62) = exp( \u0177\ud835\udc62\ud835\udc56 )\n\ud835\udc57 \u2208I exp( \u0177\ud835\udc62 \ud835\udc57 ) . Given this, the maximal likelihood estimation (MLE) can be utilized to model the fit of the data through the likelihood (\u03a0 \ud835\udc62 \u2208 U,\ud835\udc56 \u2208 I + \ud835\udc62 \ud835\udc5d (\ud835\udc56 |\ud835\udc62)), and the negative log-likelihood serves as the loss function:\nL \ud835\udc60\ud835\udc5c \ud835\udc53 \ud835\udc61 = -E \ud835\udc62 log \u2211\ufe01 \ud835\udc56 \u2208 I + \ud835\udc62 \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62\ud835\udc56 ) \ud835\udc57 \u2208 I \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62 \ud835\udc57 )(1)\nNote that in the earlier research, the sigmoid function has also adopted for transforming similarity measure to probability: \ud835\udc5d (\ud835\udc56 |\ud835\udc62) = \ud835\udf0e ( \u0177\ud835\udc62\ud835\udc56 ). However, it has shown to be less effective [44]. Furthermore, the loss function is a binary cross-entropy where the negative sample part is eliminated as being treated as 0. Finally, the main challenge here is that the denominator in the loss sums over all possible items in I, which is often impossible in practice and thus requires approximation. In the past, the sampled softmax approaches typically utilized the important sampling for estimation [22]:\nL \ud835\udc60\ud835\udc60\ud835\udc5c \ud835\udc53 \ud835\udc61 = -E \ud835\udc62 log \u2211\ufe01 \ud835\udc56 \u2208 I + \ud835\udc62 \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62\ud835\udc56 ) \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62\ud835\udc56 ) + \ud835\udc41 \ud835\udc57=1;\ud835\udc57\u223c\ud835\udc5d - \ud835\udc62 \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62 \ud835\udc57 )/\ud835\udc5d - \ud835\udc62 ( \ud835\udc57)(2)\nwhere, \ud835\udc5d - \ud835\udc62 is a predefined negative sampling distribution and often is implemented as \ud835\udc5d, which is proportional to the item popularity. It has been shown that sampled softmax performs better than NCE [15] and negative sampling ones [8,43] when the item number is large [63].", "publication_ref": ["b10", "b62", "b43", "b21", "b14", "b7", "b42", "b62"], "figure_ref": [], "table_ref": []}, {"heading": "Contrastive Learning Loss (InfoNCE):", "text": "The key idea of contrastive learning is to contrast semantically similar (positive) and dissimilar (negative) pairs of data points, thus pulling the similar points closer and pushing the dissimilar points apart. There are several contrastive learning losses have been proposed [2,7,40], and among them, InfoNCE loss is one of the most well-known and has been adopted in the recommendation as follows:\nL \ud835\udc56\ud835\udc5b\ud835\udc53 \ud835\udc5c = -E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 log \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62\ud835\udc56 ) \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62\ud835\udc56 ) + \ud835\udc41 \ud835\udc57=1;\ud835\udc57\u223c\ud835\udc5d - \ud835\udc62 \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62 \ud835\udc57 )(3)\nwhere \ud835\udc5d + \ud835\udc62 (\ud835\udc5d - \ud835\udc62 ) is the positive (negative) item distribution for user \ud835\udc62. Note that unlike the sampled softmax, the correction from the sampling distribution is not present in the InfoNCE loss. In fact, this loss does not actually approximate the softmax probability measure \ud835\udc5d (\ud835\udc56 |\ud835\udc62) while it corresponds to the probability that item \ud835\udc56 is the positive item, and the remaining points are noise, and \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62\ud835\udc56 ) measures the density ration (\ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62\ud835\udc56 ) \u221d \ud835\udc5d (\ud835\udc56 |\ud835\udc62)/\ud835\udc5d (\ud835\udc56)) [40]. Again, the overall InfoNCE loss is the cross-entropy of identifying the positive items correctly, and it also shows maximize the mutual information between user \ud835\udc62 and item \ud835\udc56 (\ud835\udc3c (\ud835\udc56, \ud835\udc62)) and minimize \ud835\udc3c ( \ud835\udc57, \ud835\udc62) for item \ud835\udc57 and \ud835\udc62 being unrelated [40].\nIn practice, as the true negative labels (so does all true positive labels) are not available, negative items \ud835\udc57 are typically drawn uniformly (or proportional to their popularity) from the entire item set I or I\\ I + \ud835\udc62 , i.e, \ud835\udc57 \u223c \ud835\udc5d \ud835\udc62 :\nL \ud835\udc56\ud835\udc5b\ud835\udc53 \ud835\udc5c = -E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 log \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62\ud835\udc56 ) \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62\ud835\udc56 ) + \ud835\udc41 \ud835\udc57=1;\ud835\udc57\u223c\ud835\udc5d \ud835\udc62 \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62 \ud835\udc57 )(4)\nNote that this loss L \ud835\udc56\ud835\udc5b\ud835\udc53 \ud835\udc5c in practice is sometimes treated as a softmax estimation, despite is not a proper estimator for the softmax loss [11,40]. In addition, the L \ud835\udc56\ud835\udc5b\ud835\udc53 \ud835\udc5c approximation easily leads to sampling bias: when sampling a large number of supposed negative items, they would include some positive items i.e., those top \ud835\udc3e items in recommendation setting. It has been known such sampling bias can empirically lead to a significant performance drop for machine learning tasks [6,8].", "publication_ref": ["b1", "b6", "b39", "b39", "b39", "b10", "b39", "b5", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Leveraging Debiased Contrastive InfoNCE", "text": "Can we reduce the gap between the approximation L \ud835\udc56\ud835\udc5b\ud835\udc53 \ud835\udc5c and ideal InfoNCE loss L \ud835\udc56\ud835\udc5b\ud835\udc53 \ud835\udc5c and mitigate sampling bias? In [9], a novel debiased estimator is proposed while only accessing positive items and unlabeled training data. Below, we demonstrate how this approach can be adopted in the recommendation setting.\nGiven user \ud835\udc62, let us consider the implicit (binary) preference probability (class distribution) of \ud835\udc62: \ud835\udf0c \ud835\udc62 (\ud835\udc50), where \ud835\udf0c \ud835\udc62 (\ud835\udc50 = +) = \ud835\udf0f + \ud835\udc62 represents the probability of user \ud835\udc62 likes an item, and \ud835\udf0c \ud835\udc62 (\ud835\udc50 = -) = \ud835\udf0f - \ud835\udc62 = 1-\ud835\udf0f + \ud835\udc62 corresponds to being not interested. In the recommendation setting, we may consider \ud835\udf0c \ud835\udc62 (+) as the fraction of positive items, i.e., those top \ud835\udc3e items, and \ud835\udf0c \ud835\udc62 (-) are the fraction of true negative items. Let the item class joint distribution be \ud835\udc5d \ud835\udc62 (\ud835\udc56, \ud835\udc50) = \ud835\udc5d \ud835\udc62 (\ud835\udc56 |\ud835\udc50)\ud835\udf0c \ud835\udc62 (\ud835\udc50). Then \ud835\udc5d + \ud835\udc62 (\ud835\udc56) = \ud835\udc5d \ud835\udc62 (\ud835\udc56 |\ud835\udc50 = +) is the probability of observing \ud835\udc56 as a positive item for user \ud835\udc62 and \ud835\udc5d - \ud835\udc62 ( \ud835\udc57) = \ud835\udc5d \ud835\udc62 ( \ud835\udc57 |\ud835\udc50 = -) the probability of a negative example. Given this, the distribution of an individual item of user \ud835\udc62 is:\n\ud835\udc5d \ud835\udc62 (\ud835\udc56) = \ud835\udf0f + \ud835\udc62 \u2022 \ud835\udc5d + \ud835\udc62 (\ud835\udc56) + \ud835\udf0f - \ud835\udc62 \u2022 \ud835\udc5d - \ud835\udc62 (\ud835\udc56)(5)\nFor implicit recommendation, we can assume the known positive items I + \ud835\udc62 are being uniformly sampled according to \ud835\udc5d + \ud835\udc62 . For \ud835\udf0f + \ud835\udc62 , we can count all the existing positive items (\ud835\udc5f \ud835\udc62\ud835\udc56 = 1), and the unknown top \ud835\udc3e items as being positive, thus, \ud835\udf0f + \ud835\udc62 = (| I + \ud835\udc62 | + \ud835\udc3e)/|I|. Alternatively, we can assume the number of total positive items for \ud835\udc62 is proportional to the number of existing (known) positive items, i.e, \ud835\udf0f + \ud835\udc62 = (1 + \ud835\udefc)| I + \ud835\udc62 |/|I|, where \ud835\udefc \u2265 0. As mentioned earlier, the typical assumption of \ud835\udc5d \ud835\udc62 (the probability distribution of selection an item is also uniform). Then, we can represent \ud835\udc5d - \ud835\udc62 using the total and positive sampling as: \ud835\udc5d - \ud835\udc62 (\ud835\udc56) = 1/\ud835\udf0f - \ud835\udc62 (\ud835\udc5d \ud835\udc62 (\ud835\udc56) -\ud835\udf0f + \ud835\udc62 \u2022 \ud835\udc5d + \ud835\udc62 (\ud835\udc56)). Given this, let us consider the following ideal (debiased) InfoNCE loss:\nL \ud835\udc3c\ud835\udc5b\ud835\udc53 \ud835\udc5c = -E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 log exp( \u0177\ud835\udc62\ud835\udc56 ) exp( \u0177\ud835\udc62\ud835\udc56 ) + \ud835\udf06 E \ud835\udc57 \u223c\ud835\udc5d - \ud835\udc62 exp( \u0177\ud835\udc62 \ud835\udc57 ) = -E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 log exp( \u0177\ud835\udc62\ud835\udc56 ) exp( \u0177\ud835\udc62\ud835\udc56 ) + \ud835\udf06 1 \ud835\udf0f -E \ud835\udc61 \u223c\ud835\udc5d\ud835\udc62 [exp( \u0177\ud835\udc62\ud835\udc61 ) ] -\ud835\udf0f + E \ud835\udc57 \u223c\ud835\udc5d + \ud835\udc62 [exp( \u0177\ud835\udc62 \ud835\udc57 ) ](6)\nNote that when \ud835\udf06 = \ud835\udc41 , the typical InfoNCE loss L \ud835\udc3c\ud835\udc5b\ud835\udc53 \ud835\udc5c (Eq. ( 3)) converges into L \ud835\udc3c\ud835\udc5b\ud835\udc53 \ud835\udc5c when \ud835\udc41 \u2192 \u221e. Now, we can use \ud835\udc41 samples from \ud835\udc5d \ud835\udc62 (not negative samples), an \ud835\udc40 positive samples from \ud835\udc5d + \ud835\udc62 to empirically estimate L \ud835\udc3c\ud835\udc5b\ud835\udc53 \ud835\udc5c following [9]:\nL \ud835\udc37\ud835\udc52\ud835\udc4f\ud835\udc56\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc51 \ud835\udc3c\ud835\udc5b\ud835\udc53 \ud835\udc5c = -E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 ;\ud835\udc57 \u223c\ud835\udc5d\ud835\udc62 ;\ud835\udc58\u223c\ud835\udc5d + \ud835\udc62 log exp( \u0177\ud835\udc62\ud835\udc56 ) exp( \u0177\ud835\udc62\ud835\udc56 ) + \ud835\udf06\ud835\udc53 ( { \ud835\udc57 } \ud835\udc41 1 , {\ud835\udc58 } \ud835\udc40 1 )\n, \ud835\udc64\u210e\ud835\udc52\ud835\udc5f\ud835\udc52\n\ud835\udc53 ( { \ud835\udc57 } \ud835\udc41 1 , {\ud835\udc58 } \ud835\udc40 1 ) = max 1 \ud835\udf0f - \ud835\udc62 1 \ud835\udc41 \ud835\udc41 \u2211\ufe01 \ud835\udc57 =1;\ud835\udc57 \u223c\ud835\udc5d\ud835\udc62 \ud835\udc52\ud835\udc65\ud835\udc5d (\ud835\udc53 \ud835\udc62 \ud835\udc57 ) -\ud835\udf0f + \ud835\udc62 1 \ud835\udc40 \ud835\udc40 \u2211\ufe01 \ud835\udc58=1;\ud835\udc58\u223c\ud835\udc5d + \ud835\udc62 \ud835\udc52\ud835\udc65\ud835\udc5d (\ud835\udc53 \ud835\udc62\ud835\udc58 ) , \ud835\udc52 1/\ud835\udc61 (7)\nHere \ud835\udc53 is constrained to be greater than its theoretical minimum \ud835\udc52 -1/\ud835\udc61 to prevent the negative number inside log. We will experimentally study this debiased InfoNCE in Section 4.", "publication_ref": ["b8", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Connection with MINE and BPR", "text": "In the following, we first introduce Mutual Information Neural Estimator (MINE) loss and then discuss some common lower bounds for InfoNCE and MINE, BPR as well as their relationship. Later in the Section 4, we evaluate MINE against existing losses for recommendation and show it performs surprisingly well. To the best of our knowledge, this work is the first to study and apply MINE [3] to the recommendation setting.\nThe Mutual Information Neural Estimator (MINE) loss is introduced in [3], which can directly measure the mutual information between user and item:\n\ud835\udc3c (\ud835\udc62, \ud835\udc56) = \ud835\udc60\ud835\udc62\ud835\udc5d (\ud835\udc63 \ud835\udc62 ;\ud835\udc63 \ud835\udc56 ) E \ud835\udc5d \ud835\udc62,\ud835\udc56 [ \u0177\ud835\udc62\ud835\udc56 ] -log E \ud835\udc5d \ud835\udc62 \u2297\ud835\udc5d \ud835\udc56 [\ud835\udc52 \u0177\ud835\udc62\ud835\udc56 ](8)\nHere, the positive \u0177\ud835\udc62\ud835\udc56 items sampled \ud835\udc5b times according to the joint user-item distribution \ud835\udc5d \ud835\udc62,\ud835\udc56 . The negative items are sampled according to the empirical marginal item distribution of \ud835\udc5d \ud835\udc56 , which are proportional to their popularity (or in practice, sampled uniformly).\nA simple adaption of MINE to the recommendation setting can simply exclude the positive item \ud835\udc56 from the denominator of In-foNCE [40]:\nL \ud835\udc5a\ud835\udc56\ud835\udc5b\ud835\udc52 = -E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 \u0177\ud835\udc62\ud835\udc56 -log E \ud835\udc57\u223c\ud835\udc5d \ud835\udc56 [exp \u0177\ud835\udc62 \ud835\udc57 ](9)\nIts empirical loss can be written as (the factor of 1/\ud835\udc41 will not affect the optimization):\nL \ud835\udc5a\ud835\udc56\ud835\udc5b\ud835\udc52 = -E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 \u0177\ud835\udc62\ud835\udc56 -log \ud835\udc41 \u2211\ufe01 \ud835\udc57=1;\ud835\udc57\u223c\ud835\udc5d \ud835\udc56 [exp \u0177\ud835\udc62 \ud835\udc57 ](10)\nLower Bounds: Given this, we can easily observe (based on Jensen's inequality):\nL \ud835\udc56\ud835\udc5b\ud835\udc53 \ud835\udc5c = -E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 log \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62\ud835\udc56 ) \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62\ud835\udc56 ) + \ud835\udc41 \ud835\udc57=1;\ud835\udc57\u223c\ud835\udc5d \ud835\udc62 \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62 \ud835\udc57 ) \u2265 L \ud835\udc5a\ud835\udc56\ud835\udc5b\ud835\udc52 = E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 log( \ud835\udc41 \u2211\ufe01 \ud835\udc57=1;\ud835\udc57\u223c\ud835\udc5d \ud835\udc56 \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62 \ud835\udc57 -\u0177\ud835\udc62\ud835\udc56 )) \u2248 E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 log \ud835\udc41 E \ud835\udc57\u223c\ud835\udc5d \ud835\udc56 \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62 \ud835\udc57 -\u0177\ud835\udc62\ud835\udc56 )) \u2265 E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 E \ud835\udc57\u223c\ud835\udc5d \ud835\udc56 \u0177\ud835\udc62 \ud835\udc57 -\u0177\ud835\udc62\ud835\udc56 + log \ud835\udc41 \u2265 E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 E \ud835\udc57\u223c\ud835\udc5d \ud835\udc56 \u0177\ud835\udc62 \ud835\udc57 -\u0177\ud835\udc62\ud835\udc56(11)\nAlternative, by using LogSumExp lower-bound [2], we also observe:\nE \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 \ud835\udc41 max \ud835\udc57=1;\ud835\udc57\u223c\ud835\udc5d \ud835\udc56 0, \u0177\ud835\udc62 \ud835\udc57 -\u0177\ud835\udc62\ud835\udc56 + log(\ud835\udc41 + 1) \u2265 L \ud835\udc56\ud835\udc5b\ud835\udc53 \ud835\udc5c \u2265 L \ud835\udc5a\ud835\udc56\ud835\udc5b\ud835\udc52 = E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 log( \ud835\udc41 \u2211\ufe01 \ud835\udc57=1;\ud835\udc57\u223c\ud835\udc5d \ud835\udc56 \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62 \ud835\udc57 -\u0177\ud835\udc62\ud835\udc56 )) \u2265 E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 \ud835\udc41 max \ud835\udc57=1;\ud835\udc57\u223c\ud835\udc5d \ud835\udc56 \u0177\ud835\udc62 \ud835\udc57 -\u0177\ud835\udc62\ud835\udc56(12)\nNote that the second lower bound is tighter than the first one. We also observe the looser bound is also shared by the well-known pairwise BPR loss, which can be written as:\nL \ud835\udc4f\ud835\udc5d\ud835\udc5f = E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 \ud835\udc41 \u2211\ufe01 \ud835\udc57=1;\ud835\udc57\u223c\ud835\udc5d \ud835\udc56 -log \ud835\udf0e ( \u0177\ud835\udc62\ud835\udc56 -\u0177\ud835\udc62 \ud835\udc57 ) = E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 \ud835\udc41 \u2211\ufe01 \ud835\udc57=1;\ud835\udc57\u223c\ud835\udc5d \ud835\udc56 log(1 + \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62 \ud835\udc57 -\u0177\ud835\udc62\ud835\udc56 ))(13)\nFollowing above analysis, we observe:\nL \ud835\udc56\ud835\udc5b\ud835\udc53 \ud835\udc5c = E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 log(1 + \ud835\udc41 \u2211\ufe01 \ud835\udc57=1;\ud835\udc57\u223c\ud835\udc5d \ud835\udc56 \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62 \ud835\udc57 -\u0177\ud835\udc62\ud835\udc56 )) \u2248 E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 log(1 + \ud835\udc41 E \ud835\udc57\u223c\ud835\udc5d \ud835\udc56 \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62 \ud835\udc57 -\u0177\ud835\udc62\ud835\udc56 )) \u2265 E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 E \ud835\udc57\u223c\ud835\udc5d \ud835\udc56 log(1 + \ud835\udc41 \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62 \ud835\udc57 -\u0177\ud835\udc62\ud835\udc56 )) \u2265 E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 E \ud835\udc57\u223c\ud835\udc5d \ud835\udc56 log(1/\ud835\udc41 + \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62 \ud835\udc57 -\u0177\ud835\udc62\ud835\udc56 )) + log \ud835\udc41 \u2265 E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 E \ud835\udc57\u223c\ud835\udc5d \ud835\udc56 \u0177\ud835\udc62 \ud835\udc57 -\u0177\ud835\udc62\ud835\udc56 ) + log \ud835\udc41 \u2265 E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 E \ud835\udc57\u223c\ud835\udc5d \ud835\udc56 \u0177\ud835\udc62 \ud835\udc57 -\u0177\ud835\udc62\ud835\udc56 )(14)\nThus, the BPR loss shares the same lower bound \ud835\udc38 \ud835\udc62 \ud835\udc38 \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 E \ud835\udc57\u223c\ud835\udc5d \ud835\udc56 \u0177\ud835\udc62 \ud835\udc57 -\u0177\ud835\udc62\ud835\udc56 ) as the InfoNCE and MINE, which aims to maximize the average difference between a positive item score ( \u0177\ud835\udc62\ud835\udc56 ) and a negative item score ( \u0177\ud835\udc62 \ud835\udc57 ).\nHowever, the tighter lower bound of BPR diverges from the InfoNCE and MINE loss using the LogSumExp lower bound:\nL \ud835\udc4f\ud835\udc5d\ud835\udc5f = E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 \ud835\udc41 \u2211\ufe01 \ud835\udc57=1;\ud835\udc57\u223c\ud835\udc5d \ud835\udc56 log(1 + \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62 \ud835\udc57 -\u0177\ud835\udc62\ud835\udc56 )) \u2265 E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 \ud835\udc41 \u2211\ufe01 \ud835\udc57=1;\ud835\udc57\u223c\ud835\udc5d \ud835\udc56 \ud835\udc5a\ud835\udc4e\ud835\udc65 (0, \u0177\ud835\udc62 \ud835\udc57 -\u0177\ud835\udc62\ud835\udc56 ) \u221d E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 E \ud835\udc57\u223c\ud835\udc5d \ud835\udc56 \ud835\udc5a\ud835\udc4e\ud835\udc65 (0, \u0177\ud835\udc62 \ud835\udc57 -\u0177\ud835\udc62\ud835\udc56 ) (15)\nNote that the bound in Eq. ( 15) only sightly improves over the one in Eq. ( 14) and cannot reflect the bounds shared by InfoNCE and MINE, which aims to minimize the largest gain of a negative item (random) over a positive item (max \ud835\udc41 \ud835\udc57=1;\ud835\udc57\u223c\ud835\udc5d \ud835\udc56 \u0177\ud835\udc62 \ud835\udc57 -\u0177\ud835\udc62\ud835\udc56 ). We also experimentally validate using the first bound E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 E \ud835\udc57\u223c\ud835\udc5d \ud835\udc56 \u0177\ud835\udc62 \ud835\udc57 -\u0177\ud835\udc62\ud835\udc56 leads to poor performance, indicating this can be a potentially underlying factor the relatively inferior performance of BRP comparing with other losses, such as softmax, InfoNCE, and MINE (Section 4). MINE+ loss for Recommendation: Utimately, we consider the following refined MINE loss format for recommendation which as we will show in Section 4, can provide an additional boost comparing with the Vanilla formula (Eq. ( 10)):\nL \ud835\udc5a\ud835\udc56\ud835\udc5b\ud835\udc52+ = -E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 \u0177\ud835\udc62\ud835\udc56 /\ud835\udc61 -\ud835\udf06 log \ud835\udc41 \u2211\ufe01 \ud835\udc57=1;\ud835\udc57\u223c\ud835\udc5d \ud835\udc56 [exp( \u0177\ud835\udc62 \ud835\udc57 /\ud835\udc61](16)\nHere, \u0177\ud835\udc62\ud835\udc56 uses the cosine similarity between user and item latent vectors, \ud835\udc61 serves as the temperature (similar to other contrastive learning loss), and finally, \ud835\udf06 helps balance the weight between positive scores and negative scores.", "publication_ref": ["b2", "b2", "b39", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "DEBIASED POINTWISE LOSS", "text": "Besides the typical listwise (softmax, InfoNCE) and pairwise (BPR) losses, another type of recommendation loss is the pointwise loss. Most of the linear (non-deep) models are based on pointwise losses, including the well-known iALS [20,46], EASE [51], and latest CCL [37]. Here, these loss functions aim to pull the estimated score \u0177\ud835\udc62\ud835\udc56 closer to its default score \ud835\udc5f \ud835\udc62\ud835\udc56 . In all the existing losses, when the interaction is known, \ud835\udc56 \u2208 I + \ud835\udc62 , \ud835\udc5f \ud835\udc62\ud835\udc56 = 1; otherwise, the unlabeled (interaction unknown) items are treated as \ud835\udc5f \ud835\udc62\ud835\udc56 = 0.\nFollowing the analysis of debiased contrastive learning, clearly, some of the positive items are unlabeled, and making them closer to 0 is not the ideal option. In fact, if the optimization indeed reaches the targeted score, we actually do not learn any useful recommendations. A natural question is whether we can improve the pointwise losses following the debiased contrastive learning paradigm [9]. In addition, we will investigate if the popular linear methods, such as iALS and EASE, are debiased or not.", "publication_ref": ["b19", "b45", "b50", "b36", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Debiased MSE and CCL", "text": "Let us denote the generic single pointwise loss function \ud835\udc59 + ( \u0177\ud835\udc62\ud835\udc56 ) and \ud835\udc59 -( \u0177\ud835\udc62\ud835\udc56 ), which measure how close the individual positive (negative) item score to their ideal target value. MSE single pointwise loss: Mean-Squared-Error (MSE) is one of the most widely used recommendation loss functions. Indeed, all the earlier collaborative filtering (Matrix factorization) models and pointwise losses were based on MSE (with different regularization). Some recent linear models, such as SLIM [39] and EASE [51], are also based on MSE. Its single pointwise loss function is denoted as:\n\ud835\udc59 + \ud835\udc5a\ud835\udc60\ud835\udc52 ( \u0177\ud835\udc62\ud835\udc56 ) = (1 -\u0177\ud835\udc62\ud835\udc56 ) 2 \ud835\udc59 - \ud835\udc5a\ud835\udc60\ud835\udc52 ( \u0177\ud835\udc62\ud835\udc56 ) = ( \u0177\ud835\udc62\ud835\udc56 ) 2(17)\nCCL single pointwise loss: The cosine contrastive loss (CCL) is recently proposed loss and has shown to be very effective for recommendation []. The original Contrastive Cosine Loss (CCL) [37] can be written as:\nL \ud835\udc36\ud835\udc36\ud835\udc3f = E \ud835\udc62 \u2211\ufe01 \ud835\udc56 \u2208 I + \ud835\udc62 (1 -\u0177\ud835\udc62\ud835\udc56 ) + \ud835\udc64 \ud835\udc41 \ud835\udc41 \u2211\ufe01 \ud835\udc57 \u2208 I \ud835\udc45\ud835\udc52\ud835\udc3f\ud835\udc48 ( \u0177\ud835\udc62 \ud835\udc57 -\ud835\udf16)(18)\nwhere \u0177\ud835\udc62 \ud835\udc57 is cosine similarity, \ud835\udc45\ud835\udc52\ud835\udc3f\ud835\udc48 (\ud835\udc65) = \ud835\udc5a\ud835\udc4e\ud835\udc65 (0, \ud835\udc65) is the activation function, \ud835\udc64 is the negative weight, \ud835\udc41 is the number of negative samples and \ud835\udf16 is margin. Its single pointwise loss thus can be written as:\n\ud835\udc59 + \ud835\udc50\ud835\udc50\ud835\udc59 ( \u0177\ud835\udc62\ud835\udc56 ) = 1 -\u0177\ud835\udc62\ud835\udc56 \ud835\udc59 - \ud835\udc50\ud835\udc50\ud835\udc59 ( \u0177\ud835\udc62\ud835\udc56 ) = \ud835\udc45\ud835\udc52\ud835\udc3f\ud835\udc48 ( \u0177\ud835\udc62\ud835\udc56 -\ud835\udf16)(19)\nGiven this, we introduce the ideal pointwise loss for recommendation models (following the basic idea of (debiased) contrastive learning) : Definition 1. (Ideal Pointwise Loss) The ideal pointwise loss function for recommendation models is the expected single pointwise loss for all positive items (sampled from \ud835\udf0c \ud835\udc62 (+) = \ud835\udf0f + ) and for negative items (sampled from \ud835\udf0c \ud835\udc62 (-) = \ud835\udf0f -):\nL \ud835\udc5d\ud835\udc5c\ud835\udc56\ud835\udc5b\ud835\udc61 = E \ud835\udc62 \ud835\udf0f + \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 \ud835\udc59 + ( \u0177\ud835\udc62\ud835\udc56 ) + \ud835\udf06\ud835\udf0f - \ud835\udc62 E \ud835\udc57 \u223c\ud835\udc5d - \ud835\udc62 \ud835\udc59 -( \u0177\ud835\udc62 \ud835\udc57 )(20)\nWhy this is ideal? When the optimization reaches the potential minimal, we have for all positive items \ud835\udc56 and all the negative items \ud835\udc57 reaching all the minimum of individual loss (\ud835\udc59 + and \ud835\udc59 -). Here, \ud835\udf06 help adjusts the balance between the positive and negative losses, similar to iALS [20,46] and CCL [37]. However, since \ud835\udc5d + \ud835\udc62 and \ud835\udc5d - \ud835\udc62 is unknown, we utilize the same debiasing approach as InfoNCE [40], also in Section 2.2, for the debiased MSE: Definition 2. (Debiased Ideal pointwise loss)\nL \ud835\udc37\ud835\udc52\ud835\udc4f\ud835\udc56\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc51 \ud835\udc5d\ud835\udc5c\ud835\udc56\ud835\udc5b\ud835\udc61 = E \ud835\udc62 \ud835\udf0f + \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 \ud835\udc59 + ( \u0177\ud835\udc62\ud835\udc56 ) + \ud835\udf0f - \ud835\udc62 \ud835\udf06 \u2022 E \ud835\udc57\u223c\ud835\udc5d - \ud835\udc62 \ud835\udc59 -( \u0177\ud835\udc62\ud835\udc56 ) = E \ud835\udc62 \ud835\udf0f + \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 \ud835\udc59 + ( \u0177\ud835\udc62\ud835\udc56 ) + \ud835\udf06 E \ud835\udc57\u223c\ud835\udc5d \ud835\udc62 \ud835\udc59 -( \u0177\ud835\udc62\ud835\udc56 ) -\ud835\udf0f + \ud835\udc62 E \ud835\udc58\u223c\ud835\udc58 + \ud835\udc62 \ud835\udc59 -( \u0177\ud835\udc62 \ud835\udc57 )(21)\nIn practice, assuming for each positive item, we sample \ud835\udc41 positive items and \ud835\udc40 negative items, then the empirical loss can be written as:\nL \ud835\udc37\ud835\udc52\ud835\udc4f\ud835\udc56\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc51 \ud835\udc5d\ud835\udc5c\ud835\udc56\ud835\udc5b\ud835\udc61 = E \ud835\udc62 \ud835\udc38 \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 \ud835\udf0f + \ud835\udc62 \ud835\udc59 + ( \u0177\ud835\udc62\ud835\udc56 )+ \ud835\udf06 1 \ud835\udc41 \ud835\udc41 \u2211\ufe01 \ud835\udc57=1;\ud835\udc57\u223c\ud835\udc5d \ud835\udc62 \ud835\udc59 -( \u0177\ud835\udc62 \ud835\udc57 ) -\ud835\udf0f + \ud835\udc62 1 \ud835\udc40 \ud835\udc40 \u2211\ufe01 \ud835\udc58=1;\ud835\udc58\u223c\ud835\udc5d + \ud835\udc62 \ud835\udc59 -( \u0177\ud835\udc62\ud835\udc58 )(22)\nAs an example, we have the ideal L \ud835\udc36\ud835\udc36\ud835\udc3f , debiased L \ud835\udc51\ud835\udc52\ud835\udc4f\ud835\udc56\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc51 \ud835\udc36\ud835\udc36\ud835\udc3f , empirical debiased CCL L \ud835\udc37\ud835\udc52\ud835\udc4f\ud835\udc56\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc51 \ud835\udc50\ud835\udc50\ud835\udc59 losses as follows:\n\uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 L \ud835\udc36\ud835\udc36\ud835\udc3f = E \ud835\udc62 \ud835\udf0f + \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 (1 -\u0177\ud835\udc62\ud835\udc56 ) + \ud835\udf06\ud835\udf0f - \ud835\udc62 E \ud835\udc57\u223c\ud835\udc5d - \ud835\udc62 [\ud835\udc45\ud835\udc52\ud835\udc3f\ud835\udc48 ( \u0177\ud835\udc62 \ud835\udc57 -\ud835\udf16)] L \ud835\udc51\ud835\udc52\ud835\udc4f\ud835\udc56\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc51 \ud835\udc36\ud835\udc36\ud835\udc3f = E \ud835\udc62 \ud835\udf0f + \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 (1 -\u0177\ud835\udc62\ud835\udc56 ) +\ud835\udf06 E \ud835\udc57\u223c\ud835\udc5d \ud835\udc62 [\ud835\udc45\ud835\udc52\ud835\udc3f\ud835\udc48 ( \u0177\ud835\udc62 \ud835\udc57 -\ud835\udf16)] -\ud835\udf0f + \ud835\udc62 E \ud835\udc58\u223c\ud835\udc5d + \ud835\udc62 [\ud835\udc45\ud835\udc52\ud835\udc3f\ud835\udc48 ( \u0177\ud835\udc62\ud835\udc58 -\ud835\udf16)] L \ud835\udc37\ud835\udc52\ud835\udc4f\ud835\udc56\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc51 \ud835\udc50\ud835\udc50\ud835\udc59 = E \ud835\udc62 \ud835\udc38 \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 \ud835\udf0f + \ud835\udc62 (1 -\u0177\ud835\udc62\ud835\udc56 )+ \ud835\udf06 1 \ud835\udc41 \ud835\udc41 \ud835\udc57=1;\ud835\udc57\u223c\ud835\udc5d \ud835\udc62 \ud835\udc45\ud835\udc52\ud835\udc3f\ud835\udc48 ( f \ud835\udc62 \ud835\udc57 -\ud835\udf16) -\ud835\udf0f + \ud835\udc62 1 \ud835\udc40 \ud835\udc40 \ud835\udc58=1;\ud835\udc58\u223c\ud835\udc5d + \ud835\udc62 \ud835\udc45\ud835\udc52\ud835\udc3f\ud835\udc48 ( f \ud835\udc62 \ud835\udc57 -\ud835\udf16)\nWe will study how (empirical) debiased MSE and CCL in the experimental result section.", "publication_ref": ["b38", "b50", "b36", "b19", "b45", "b36", "b39"], "figure_ref": [], "table_ref": []}, {"heading": "iALS, EASE and their Debiasness", "text": "Here, we investigate how the debiased MSE loss will impact the solution of two (arguably) most popular linear recommendation models, iALS [20,46] and EASE [51]. Rather surprisingly, we found the solvers of both models can absorb the debiased loss under their existing framework with reasonable conditions (details below). In other words, both iALS and EASE can be considered to be inherently debiased.\nTo obtain the aforementioned insights, we first transform the debiased loss into the summation form being used in iALS and EASE.\nL \ud835\udc37\ud835\udc52\ud835\udc4f\ud835\udc56\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc51 \ud835\udc5a\ud835\udc60\ud835\udc52 \u2248 E \ud835\udc62 \ud835\udf0f + \ud835\udc62 | I + \ud835\udc62 | \u2211\ufe01 \ud835\udc56 \u2208I + \ud835\udc62 ( \u0177\ud835\udc62\ud835\udc56 -1) 2 + \ud835\udf06 1 | I | \u2211\ufe01 \ud835\udc61 \u2208I \u01772 \ud835\udc62\ud835\udc61 - \ud835\udf0f + \ud835\udc62 | I + \ud835\udc62 | \u2211\ufe01 \ud835\udc5e \u2208I + \ud835\udc62 \u01772 \ud835\udc62\ud835\udc5e = \u2211\ufe01 \ud835\udc62 1 | I | \ud835\udc50 \ud835\udc62 \u2211\ufe01 \ud835\udc56 \u2208I + \ud835\udc62 ( \u0177\ud835\udc62\ud835\udc56 -1) 2 where, \ud835\udc50 \ud835\udc62 = | I | | I + \ud835\udc62 | \ud835\udf0f + \ud835\udc62 + \ud835\udf06 1 | I | \u2211\ufe01 \ud835\udc61 \u2208I \u01772 \ud835\udc62\ud835\udc61 - 1 | I | \ud835\udc50 \ud835\udc62 \u2211\ufe01 \ud835\udc5e \u2208I + \ud835\udc62 \u01772 \ud835\udc62\ud835\udc5e \u221d \u2211\ufe01 \ud835\udc62 \ud835\udc50 \ud835\udc62 \u2211\ufe01 \ud835\udc56 \u2208I + \ud835\udc62 ( \u0177\ud835\udc62\ud835\udc56 -1) 2 + \ud835\udf06 \u2211\ufe01 \ud835\udc61 \u2208I \u01772 \ud835\udc62\ud835\udc61 -\ud835\udc50 \ud835\udc62 \u2211\ufe01 \ud835\udc5e \u2208I + \ud835\udc62 \u01772 \ud835\udc62\ud835\udc5e = \u2211\ufe01 \ud835\udc62 \u2211\ufe01 \ud835\udc56 \u2208I + \ud835\udc62 [\ud835\udc50 \ud835\udc62 ( \u0177\ud835\udc62\ud835\udc56 -1) 2 -\ud835\udc50 \ud835\udc62 \ud835\udf06 \u01772 \ud835\udc62\ud835\udc5e ] + \ud835\udf06 \u2211\ufe01 \ud835\udc61 \u2208I \u01772 \ud835\udc62\ud835\udc61\nDebiased iALS: Following the original iALS paper [21], and the latest revised iALS [45,46], the objective of iALS is given by [46]:\nL \ud835\udc56\ud835\udc34\ud835\udc3f\ud835\udc46 = \u2211\ufe01 (\ud835\udc62,\ud835\udc56 ) \u2208\ud835\udc46 ( \u0177 (\ud835\udc62, \ud835\udc56) -1) 2 + \ud835\udefc 0 \u2211\ufe01 \ud835\udc62 \u2208\ud835\udc48 \u2211\ufe01 \ud835\udc56 \u2208\ud835\udc3c \u0177 (\ud835\udc62, \ud835\udc56) 2 + \ud835\udf06 \u2211\ufe01 \ud835\udc62 \u2208\ud835\udc48 (|I + \ud835\udc62 | + \ud835\udefc 0 |I|) \ud835\udf08 ||w \ud835\udc62 || 2 + \u2211\ufe01 \ud835\udc56 \u2208\ud835\udc3c (|\ud835\udc48 + \ud835\udc56 | + \ud835\udefc 0 |U|) \ud835\udf08 ||h \ud835\udc56 || 2(23)\nwhere \ud835\udf06 is global regularization weight (hyperparameter) and \ud835\udefc 0 is unobserved weight (hyperparameter). \ud835\udf07 is generally set to be 1. Also, w \ud835\udc62 and h \ud835\udc56 are user and item vectors for user \ud835\udc62 and item \ud835\udc56, respectively. Now, we consider applying the debiased MSE loss to replace the original MSE loss (the first line in L \ud835\udc56\ud835\udc34\ud835\udc3f\ud835\udc46 ) with the second line unchanged:\nL \ud835\udc37\ud835\udc52\ud835\udc4f\ud835\udc56\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc51 \ud835\udc56\ud835\udc34\ud835\udc3f\ud835\udc46 = \u2211\ufe01 \ud835\udc62 \u2208\ud835\udc48 \u2211\ufe01 \ud835\udc56 \u2208I + \ud835\udc62 [\ud835\udc50 \ud835\udc62 ( \u0177\ud835\udc62\ud835\udc56 -1) 2 -\ud835\udc50 \ud835\udc62 \u2022 \ud835\udefc 0 \u2022 \u01772 \ud835\udc62\ud835\udc56 ] + \ud835\udefc 0 \u2211\ufe01 \ud835\udc61 \u2208\ud835\udc3c \u01772 \ud835\udc62\ud835\udc61 + \ud835\udf06 \u2211\ufe01 \ud835\udc62 \u2208\ud835\udc48 ( | I + \ud835\udc62 | + \ud835\udefc 0 | I | ) \ud835\udf08 | |w \ud835\udc62 | | 2 + \u2211\ufe01 \ud835\udc56 \u2208\ud835\udc3c ( | U + \ud835\udc56 | + \ud835\udefc 0 | U | ) \ud835\udf08 | |h \ud835\udc56 | | 2\nThen we have the following conclusion:\nTheorem 1. For any debiased iALS loss L \ud835\udc37\ud835\udc52\ud835\udc4f\ud835\udc56\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc51 \ud835\udc56\ud835\udc34\ud835\udc3f\ud835\udc46 with parameters \ud835\udefc 0 and \ud835\udf06 with constant \ud835\udc50 \ud835\udc62 for all users, there are original iALS loss with parameters \ud835\udefc \u2032 0 and \ud835\udf06 \u2032 , which have the same closed form solutions (up to a constant factor) for fixing item vectors and user vectors, respectively. (proof in Appendix B) Debiased EASE: EASE [51] has shown to be a simple yet effective recommendation model and evaluation bases (for a small number of items), and it aims to minimize:\nL \ud835\udc52\ud835\udc4e\ud835\udc60\ud835\udc52 = ||\ud835\udc4b -\ud835\udc4b\ud835\udc4a || 2 \ud835\udc39 + \ud835\udf06||\ud835\udc4a || 2 \ud835\udc39 \ud835\udc60.\ud835\udc61 . \ud835\udc51\ud835\udc56\ud835\udc4e\ud835\udc54(\ud835\udc4a ) = 0\nIt has a closed-form solution [51]:\n\ud835\udc43 = (\ud835\udc4b \ud835\udc47 \ud835\udc4b + \ud835\udf06\ud835\udc3c ) -1 \ud835\udc4a * = \ud835\udc3c -\ud835\udc43 \u2022 \ud835\udc51\ud835\udc40\ud835\udc4e\ud835\udc61 (\ud835\udc51\ud835\udc56\ud835\udc4e\ud835\udc54(1 \u2298 \ud835\udc43))(24)\nwhere \u2298 denotes the elementwise division, and \ud835\udc51\ud835\udc56\ud835\udc4e\ud835\udc54 vectorize the diagonal and \ud835\udc51\ud835\udc40\ud835\udc4e\ud835\udc61 transforms a vector to a diagonal matrix.\nTo apply the debiased MSE loss into the EASE objective, let us first further transform L \ud835\udc37\ud835\udc52\ud835\udc4f\ud835\udc56\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc51 \ud835\udc5a\ud835\udc60\ud835\udc52 into parts of known positive items and parts of unknown items:\nL \ud835\udc37\ud835\udc52\ud835\udc4f\ud835\udc56\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc51 \ud835\udc5a\ud835\udc60\ud835\udc52 = \u2211\ufe01 \ud835\udc62 \u2211\ufe01 \ud835\udc56 \u2208 I + \ud835\udc62 [\ud835\udc50 \ud835\udc62 ( \u0177\ud835\udc62\ud835\udc56 -1) 2 -\ud835\udc50 \ud835\udc62 \ud835\udf06 \u01772 \ud835\udc62\ud835\udc5e ] + \ud835\udf06 \u2211\ufe01 \ud835\udc61 \u2208 I \u01772 \ud835\udc62\ud835\udc61 = \u2211\ufe01 \ud835\udc62 \u2211\ufe01 \ud835\udc56 \u2208 I + \ud835\udc62 [\ud835\udc50 \ud835\udc62 ( \u0177\ud835\udc62\ud835\udc56 -1) 2 -\ud835\udc50 \ud835\udc62 \ud835\udf06 \u01772 \ud835\udc62\ud835\udc56 ] + \ud835\udf06 \u2211\ufe01 \ud835\udc61 \u2208 I + \ud835\udc62 \u01772 \ud835\udc62\ud835\udc61 + \ud835\udf06 \u2211\ufe01 \ud835\udc5d \u2208 I\\I + \ud835\udc62 \u01772 \ud835\udc62\ud835\udc61 = \u2211\ufe01 \ud835\udc62 \u2211\ufe01 \ud835\udc56 \u2208 I + \ud835\udc62 [\ud835\udc50 \ud835\udc62 ( \u0177\ud835\udc62\ud835\udc56 -1) 2 + \ud835\udc50 \ud835\udc62 \u2211\ufe01 \ud835\udc5d \u2208 I\\I + \ud835\udc62 \u01772 \ud835\udc62\ud835\udc61 + \ud835\udf06(1 -\ud835\udc50 \ud835\udc62 ) \u2211\ufe01 \ud835\udc62 \u2211\ufe01 \ud835\udc56 \u2208 I + \ud835\udc62 \u01772 \ud835\udc62\ud835\udc56 + (\ud835\udf06 -\ud835\udc50 \ud835\udc62 ) \u2211\ufe01 \ud835\udc5d \u2208 I\\I + \ud835\udc62 \u01772 \ud835\udc62\ud835\udc61 = || \u221a\ufe01 \ud835\udc36 \ud835\udc62 (\ud835\udc4b -\ud835\udc4b\ud835\udc4a )|| 2 \ud835\udc39 -\ud835\udf06||\ud835\udc4b \u2299 \u221a\ufe01 \ud835\udc36 \ud835\udc62 -\ud835\udc3c\ud835\udc4b\ud835\udc4a || 2 \ud835\udc39 -||(1 -\ud835\udc4b ) \u2299 \u221a\ufe01 \ud835\udc36 \ud835\udc62 -\ud835\udf06\ud835\udc3c\ud835\udc4b\ud835\udc4a || 2 \ud835\udc39 Note that \u2211\ufe01 \ud835\udc62 \u2211\ufe01 \ud835\udc56 \u2208 I \u01772 \ud835\udc62\ud835\udc56 = ||\ud835\udc4b\ud835\udc4a || 2 \ud835\udc39 (25\n)\nTo find the closed-form solution, we further restrict \ud835\udf06 = 1 and consider \ud835\udc50 \ud835\udc62 as a constant (always > 1), thus, we have the following simplified format:\nL \ud835\udc37\ud835\udc52\ud835\udc4f\ud835\udc56\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc51 \ud835\udc5a\ud835\udc60\ud835\udc52 =|| \u221a\ufe01 \ud835\udc36 \ud835\udc62 (\ud835\udc4b -\ud835\udc4b\ud835\udc4a )|| 2 \ud835\udc39 -|| \u221a\ufe01 \ud835\udc36 \ud835\udc62 -\ud835\udc3c\ud835\udc4b\ud835\udc4a || 2 \ud835\udc39 = ||\ud835\udc4b -\ud835\udc4b\ud835\udc4a || 2 \ud835\udc39 -\ud835\udefc ||\ud835\udc4b\ud835\udc4a || 2 \ud835\udc39(26)\nwhere, \ud835\udefc = \ud835\udc50 \ud835\udc62 -1. Note that if we only minimize this objective function L \ud835\udc37\ud835\udc52\ud835\udc4f\ud835\udc56\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc51 \ud835\udc5a\ud835\udc60\ud835\udc52 , we have the closed form:\n\ud835\udc4a = ((1 -\ud835\udf06)\ud835\udc4b \ud835\udc47 \ud835\udc4b ) -1 \ud835\udc4b \ud835\udc47 \ud835\udc4b .", "publication_ref": ["b19", "b45", "b50", "b20", "b44", "b45", "b45", "b50"], "figure_ref": [], "table_ref": []}, {"heading": "Now, considering the debiased version of EASE:", "text": "L \ud835\udc37\ud835\udc52\ud835\udc4f\ud835\udc56\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc51 \ud835\udc52\ud835\udc4e\ud835\udc60\ud835\udc52 = ||\ud835\udc4b -\ud835\udc4b\ud835\udc4a || 2 \ud835\udc39 -\ud835\udefc ||\ud835\udc4b\ud835\udc4a || 2 \ud835\udc39 + \ud835\udf06||\ud835\udc4a || 2 \ud835\udc39 \ud835\udc60.\ud835\udc61 . \ud835\udc51\ud835\udc56\ud835\udc4e\ud835\udc54(\ud835\udc4a ) = 0\nIt has the following closed-form solution (by similar inference as EASE [51]):\n\ud835\udc4a = 1 1 -\ud835\udefc (\ud835\udc3c -P \u2022 \ud835\udc51\ud835\udc40\ud835\udc4e\ud835\udc61 ( \u00ec 1 \u2298 \ud835\udc51\ud835\udc56\ud835\udc4e\ud835\udc54( P)), \ud835\udc64\u210e\ud835\udc52\ud835\udc5f\ud835\udc52 P = (\ud835\udc4b \ud835\udc47 \ud835\udc4b + \ud835\udf06 1 -\ud835\udefc \ud835\udc3c ) -1 (27)\nNow, we can make the following observation: Theorem 2. For any debiased EASE loss L \ud835\udc37\ud835\udc52\ud835\udc4f\ud835\udc56\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc51 \ud835\udc52\ud835\udc4e\ud835\udc60\ud835\udc52 with parameters \ud835\udefc and \ud835\udf06 with constant \ud835\udc50 \ud835\udc62 > 1 for all users, there are original EASE loss with parameter, \ud835\udf06 \u2032 , which have the same closed form solutions EASE (up to constant factor).\nProof Sketch: Following above analysis and let \ud835\udf06 \u2032 = \ud835\udf06 (1-\ud835\udefc )\ud835\udc50 \ud835\udc62 . \u25a1 These results also indicate the sampling based approach to optimize the debiased \ud835\udc40\ud835\udc46\ud835\udc38 loss may also be rather limited. In the experimental results, we will further validate this conjecture on debiased MSE loss. ", "publication_ref": ["b50"], "figure_ref": [], "table_ref": []}, {"heading": "EXPERIMENT", "text": "In this section, we experimentally study the most widely used loss functions in the existing recommendation models and their corresponding debiased loss as well as newly proposed MINE loss under recommendation settings. This could help better understand and compare the resemblance and discrepancy between different loss functions, and understand their debiasing benefits. Specifically, we would like to answer the following questions:\n\u2022 Q1. How does the bias introduced by the existing negative sampling impact model performance and how do our proposed debiased losses perform compared to traditional biased ones? \u2022 Q2. How does our proposed MINE objective function in Section 2 compare to widely used ones? \u2022 Q3. What is the effect of different hyperparameters on the various debiased loss functions?", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Setup", "text": "4.1.1 Datasets. We use three datasets, Amazon-Books, Yelp2018 and Gowalla commonly used by a number of recent studies [16,37,56,58]. We obtain the publicly available processed data and follow the same setting as [16,37,56].", "publication_ref": ["b15", "b36", "b55", "b57", "b15", "b36", "b55"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation Metrics.", "text": "Consistent with benchmarks [16,37,56], we evaluate the performance by \ud835\udc45\ud835\udc52\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc59@20 and \ud835\udc41 \ud835\udc37\ud835\udc36\ud835\udc3a@20 over all items [25].\n4.1.3 Models. This paper focuses on the study of loss functions, which is architecture agnostic. Thus, we evaluate our loss functions on top of the simplest one -Matrix Factorization (MF) in this study. By eliminating the influence of the architecture, we can compare the power of various loss functions fairly. Meantime, we compare the performance of the proposed methods with a few classical machine learning models as well as advanced deep learning models, including, iALS [46], MF-BPR [43], MF-CCL [37], YouTubeNet [11], NeuMF [17], LightGCN [16], NGCF [56], CML [19], PinSage [60], GAT [54], MultiVAE [35], SGL-ED [58]. Those comparison can help establish that the loss functions with basic MF can produce strong and robust baselines for recommendation community to evaluate more advanced and sophisticated models.", "publication_ref": ["b15", "b36", "b55", "b24", "b45", "b42", "b36", "b10", "b16", "b15", "b55", "b18", "b59", "b53", "b34", "b57"], "figure_ref": [], "table_ref": []}, {"heading": "Loss functions.", "text": "As the core of this paper, we focus on some most widely used loss functions and their debiased counterparts, including InfoNCE [40], MSE [21,46], and state-of-the-art CCL [37], as well as new proposed MINE [3].  and reduced by 0.5 on the plateau, early stopped until it arrives at 1\ud835\udc52 -6. For the cases where negative samples get involved, we would set it as 800 by default. We search the global regularization weight between 1\ud835\udc52 -9 to 1\ud835\udc52 -5 with an increased ratio of 10.\n4\nFor the exact hyperparameter settings of the model, we would present it in Appendix A and our anonymous code is available at https://anonymous.4open.science/r/KDD-2023-Anonymous-5F83/ README.md ", "publication_ref": ["b39", "b20", "b45", "b36", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Q1. (Biased) Loss vs Debiased Loss", "text": "Here, we seek to examine the negative impact of bias introduced by negative sampling and assess the effectiveness of our proposed debiased loss functions. We utilize Matrix Factorization (MF) as the backbone and compare the performance of the original (biased) version and the debiased version for the loss functions of \ud835\udc36\ud835\udc36\ud835\udc3f [37], \ud835\udc40\ud835\udc46\ud835\udc38 [21,46], and \ud835\udc3c\ud835\udc5b\ud835\udc53 \ud835\udc5c\ud835\udc41\ud835\udc36\ud835\udc38 [40]. Table 2 shows the biased loss vs debiased loss (on top of the MF) results.\nThe results show that the debiased methods consistently outperform the biased ones for the CCL and InfoNCE loss functions in all cases. In particular, the debiased method exhibits remarkable improvements of up to over 4% for the CCL loss function on the \ud835\udc34\ud835\udc5a\ud835\udc4e\ud835\udc67\ud835\udc5c\ud835\udc5b -\ud835\udc35\ud835\udc5c\ud835\udc5c\ud835\udc58\ud835\udc60 dataset. This highlights the adverse effect of bias in recommendation quality and demonstrates the success of applying contrastive learning based debiasing methods in addressing this issue. In addition, despite the robustness with respect to the biased MSE loss for iALS and EASE (as being proof in Section 3, the debiased MSE still provides slight gain over the existing MSE loss. Finally, we note that the performance gained by the debiasing losses is also consistent with the results over other machine learning tasks as shown in [9].", "publication_ref": ["b36", "b20", "b45", "b39", "b8"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Q2. Performance of MINE \\ MINE+", "text": "Next, we delve into the examination of the newly proposed Mutual Information Neural Estimator (MINE) loss function. As we mentioned before, to our best knowledge, this is the first study utilizing and evaluating MINE loss in recommendation settings. We compare MINE with other loss functions, all set MF as the backbone, as displayed in Table 2,. Our results show the surprising effectiveness of MINE-type losses. In particular, the basic MINE is shown close to 5% average lifts over the (biased) InfoNCE loss, a rather stand loss in recommendation models. MINE+ demonstrates comparable or superior performance compared to the state-of-the-art CCL loss function. Furthermore, MINE+ consistently achieves better results than InfoNCE with a maximum improvement of 14% (with an average improvement over 11%.\nThese findings highlight the potential of MINE as an effective objective function for recommendation models and systems. Noting that our MINE+ loss is model agnostic, which can be applied to any representation-based model. But using it with the basic MF already provides strong and robust baselines for evaluating recommendation models (and systems).\nFurthermore, we compare MF-MINE+ as well as our debiased MF-CCL model with some advanced machine learning and deep learning models in Table 3. We can find these two methods perform best or second best in most cases, affirming the superiority of the MINE+ loss and debiased CCL.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Q3. Hyperparameter analysis", "text": "Here, we analyze the impacts of three key hyperparameters in the debiased framework: negative weight \ud835\udf06, number of negative samples, and number of positive samples as shown in Figs. 1 and2.\nThe left side of Fig. 1 reveals that, in general, increasing the number of negative samples leads to better performance in models. The MSE loss requires fewer negative samples to reach a stable Table 3: Perfomance comparison to widely used models. We highlight the top-3 best models in each column. Results of models marked with * are duplicated from [37] for consistency. For a fair comparison of the MF-based models marked with * * are reproduced by ourselves with thorough parameter searching detailed in Appendix A.", "publication_ref": ["b36"], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "Model Yelp", "text": "Gowalla Amazon-Books Recall@20 NDCG@20 Recall@20 NDCG@20 Recall@20 NDCG@20 Deep Learning Based YouTubeNet* [11] 6  plateau but performs worse when the number of negative samples increases. The right side of Fig. 1 shows that the number of positive samples in the debiased formula has a minor impact on performance compared to negative samples. In Fig. 2, we illustrate how the negative weight affects the performance of the debiased CCL, MSE, and InfoNCE losses. In general, we would expect the \u2229 shaped curve for quality metric (\ud835\udc45\ud835\udc52\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc59@20 here) for the parameters, which is consistent with our curves. The optimal negative weight for these loss functions locates at different orders of magnitude due to their intrinsic differences of loss functions. In general, the negative weights of MSE and CCLE are relatively small and the negative weights of InfoNCE are in the order of hundreds and related to the number of negative samples (Appendix A).\nIn Fig. 3, we investigate how temperature can affect the performance of \ud835\udc40\ud835\udc3c \ud835\udc41 \ud835\udc38+ loss function. In general, it would achieve optimal around 0.5 for all datasets (see Appendix A). to optimize user and item embeddings alternatively. Due to its effectiveness and efficiency, there is a large number of works that take the MSE or its variants as their objectives, spanning from Matrix Factorization (MF) based models [18,45,46], to regression-based models, including SLIM [39], EASE [51,52], etc. He et al. treat collaborative filtering as a binary classification task and apply the pointwise objective -Binary Cross-Entropy Loss (BCE) onto it [17]. CML utilized the pairwise hinge loss onto the collaborative filtering scenario [19]. MultiVAE [35] utilizes multinomial likelihood estimation. Rendle et al. proposed a Bayesian perspective pairwise ranking loss -BPR, in the seminal work [43]. YouTubeNet posed a recommendation as an extreme multiclass classification problem and apply the Softmax cross-entropy (Softmax) [11,22]. Recently, inspired by the largely used contrastive loss in the computer vision area, [37] proposed a first-order based Cosine Contrastive Loss (CCL), where they maximize the cosine similarity between the positive pairs (between users and items) and minimize the similarity below some manually selected margin.", "publication_ref": ["b10", "b17", "b44", "b45", "b38", "b50", "b51", "b16", "b18", "b34", "b42", "b10", "b21", "b36"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Contrastive Learning", "text": "Recently, Contrastive Learning (CL) has become a prominent optimizing framework in deep learning [7,40]. The motivation behind CL is to learn representations by contrasting positive and negative pairs as well as maximize the positive pairs, including data augmentation [7] and multi-view representations [53], etc. Chuang et al.\nproposed a new unsupervised contrastive representation learning framework, targeting to minimize the bias introduced by the selection of the negative samples. This debiased objective consistently improved its counterpart -the biased one in various benchmarks [9]. Lately, by introducing the margin between positive and negative samples, Belghazi et al. present a supervised contrastive learning framework that is robust to biases [2]. In addition, a mutual information estimator -MINE [3], closely related to the contrastive learning loss InfoNCE [40], has demonstrated its efficiency in various optimizing settings. [55] study the alignment and uniformity of user, item embeddings from the perspective of contrastive learning in the recommendation. CLRec [63] design a new contrastive learning loss which is equivalent to using inverse propensity weighting to reduce exposure bias of a large-scale system.", "publication_ref": ["b6", "b39", "b6", "b52", "b8", "b1", "b2", "b39", "b54", "b62"], "figure_ref": [], "table_ref": []}, {"heading": "Negative Sampling Strategies for Item Recommendation", "text": "In most real-world scenarios, only positive feedback is available which brings demand for negative signals to avoid trivial solutions during training recommendation models. Apart from some nonsampling frameworks like iALS [21], Multi-VAE [35], the majority of models [5,8,16,17,41,43] would choose to sample negative items for efficiency consideration. Uniform sampling is the most popular one [43] which assumes uniform prior distribution. Importance sampling is another popular choice [33], which chooses negative according to their frequencies. Adaptive sampling strategy keep tracking and picking the negative samples with higher scores [42,62]. Another similar strategy is SRNS [14], which tries to find and optimize false negative samples In addition, NCE [15] approach for negative sampling has also been adopted in the recommendation system [8].", "publication_ref": ["b20", "b34", "b4", "b7", "b15", "b16", "b40", "b42", "b42", "b32", "b41", "b61", "b13", "b14", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Bias and Debias in Recommendation", "text": "In the recommender system, there are various different bias [6], including item exposure bias, popularity bias, position bias, selection bias, etc. These issues have to be carefully handled otherwise it would lead to unexpected results, such as inconsistency of online and offline performance, etc. Marlin et al. [38] validate the existence of selection bias by conducting a user survey, which indicates the discrepancy between the observed rating data and all data. Since users are generally exposed to a small set of items, unobserved data doesn't mean negative preference, this is so-called exposure bias [36]. Position bias [10] is another commonly encountered one, where users are more likely to select items that display in a desirable place or higher position on a list. Steck et al. [50] propose an unbiased metric to deal with selection bias. iALS [21] put some weights on unobserved data which helps improve the performance and deal with exposure bias. In addition, the sampling strategy would naturally diminish the exposure bias during model training [6,43]. EXMF [34] is a user exposure simulation model that can also help reduce exposure bias. To reduce position bias, some models [4,12] make assumptions about user behaviors and estimate true relevance instead of directly using clicked data. [48] proposed an unbiased pairwise BPR estimator and further provide a practical technique to reduce variance. Another widely used method to correct bias is the utilization of propensity score [49,64] where each item is equiped with some position weight.", "publication_ref": ["b5", "b37", "b35", "b9", "b49", "b20", "b5", "b42", "b33", "b3", "b11", "b47", "b48", "b63"], "figure_ref": [], "table_ref": []}, {"heading": "CONCLUSION", "text": "In this paper, we conduct a comprehensive analysis of recommendation loss functions from the perspective of contrastive learning. We introduce a list of debiased losses and new mutual informationbased loss functions -MINE and MINE+ to recommendation setting. We also show how the point-wise loss functions can be debiased and certified both iALS and EASE are inherently debiased. The empirical experimental results demonstrate the debiased losses and new information losses outperform the existing (biased) ones. In the future, we would like to investigate how the loss functions work with more sophisticated neural architectures, and help discover more effective recommendation models by leveraging these losses.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "ACKNOWLEDGEMENT", "text": "This research was partially funded by the National Science Foundation through grants IIS-2142675, IIS-2142681, and III-2008557. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A REPRODUCIBILITY", "text": "In Table 4, we list the details of the hyperparameters for reproducing the results of \ud835\udc40\ud835\udc3c \ud835\udc41 \ud835\udc38+ in Tables 2 and3.\nIn Table 5, we list the details of the hyperparameters for reproducing the results of debiased \ud835\udc36\ud835\udc36\ud835\udc3f in Table 2.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_4", "tab_2", "tab_5", "tab_2"]}, {"heading": "B PROOF B.1 Proof of Theorem 1", "text": "Proof. To derive the alternating least square solution, we have:\n\u2022 Fixing item vectors, optimizing: Interestingly by choosing the right \ud835\udefc 0 and \ud835\udf06, the above solution is in fact the same solution (up to constant factor) for the original L \ud835\udc56\ud835\udc34\ud835\udc3f\ud835\udc46 [20]. Following the above analysis and let \ud835\udefc \u2032  \nL \ud835\udc62 = || \u221a \ud835\udc50 \ud835\udc62 (\ud835\udc3b\n\u00db \u2190 X \u22a4 X + \u039b + \u03a9 -1 X \u22a4 X \u2022 dMat(1 + \u03b2) + \u03a9 \u2022 dMat( \u03b2 -\u03b3) V V\u22a4 V -1 V\u22a4 \u2190 \u00db\u22a4 X \u22a4 X + \u039b + \u03a9 \u00db -1 \u00db\u22a4 X \u22a4 X \u2022 dMat(1 + \u03b2) + \u03a9 \u2022 dMat( \u03b2 -\u03b3) \u03b2 \u2190 diag X \u22a4 X \u00db V\u22a4 -diag X \u22a4 X + diag(\u03a9) \u2299 diag \u00db V\u22a4 + \u03b3 diag (X \u22a4 X) + diag(\u03a9 -\u039b) \u03b3 \u2190 \u03b3 + diag \u00db V\u22a4 - \u03b2(35)\nD EASE-DEBIASED ", "publication_ref": ["b19"], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Additional support came from a collaborative research agreement between Kent State University and iLambda Inc.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Recommender Systems: The Textbook", "journal": "Springer Publishing Company, Incorporated", "year": "2016", "authors": "C Charu;  Aggarwal"}, {"ref_id": "b1", "title": "Unbiased Supervised Contrastive Learning", "journal": "", "year": "2023", "authors": "Alberto Carlo; Benoit Barbano; Enzo Dufumier; Marco Tartaglione; Pietro Grangetto;  Gori"}, {"ref_id": "b2", "title": "Mutual information neural estimation", "journal": "PMLR", "year": "2018", "authors": "Mohamed Ishmael Belghazi; Aristide Baratin; Sai Rajeshwar; Sherjil Ozair; Yoshua Bengio; Aaron Courville; Devon Hjelm"}, {"ref_id": "b3", "title": "A Dynamic Bayesian Network Click Model for Web Search Ranking", "journal": "Association for Computing Machinery", "year": "2009", "authors": "Olivier Chapelle; Ya Zhang"}, {"ref_id": "b4", "title": "Revisiting Negative Sampling VS. Non-Sampling in Implicit Recommendation", "journal": "ACM Trans. Inf. Syst", "year": "2022-03", "authors": "Chong Chen; Weizhi Ma; Min Zhang; Chenyang Wang; Yiqun Liu; Shaoping Ma"}, {"ref_id": "b5", "title": "Bias and Debias in Recommender System: A Survey and Future Directions", "journal": "ACM Trans. Inf. Syst", "year": "2022-10", "authors": "Jiawei Chen; Hande Dong; Xiang Wang; Fuli Feng; Meng Wang; Xiangnan He; \u2020 "}, {"ref_id": "b6", "title": "A simple framework for contrastive learning of visual representations", "journal": "PMLR", "year": "2020", "authors": "Ting Chen; Simon Kornblith; Mohammad Norouzi; Geoffrey Hinton"}, {"ref_id": "b7", "title": "On Sampling Strategies for Neural Network-Based Collaborative Filtering", "journal": "Association for Computing Machinery", "year": "2017", "authors": "Ting Chen; Yizhou Sun; Yue Shi; Liangjie Hong"}, {"ref_id": "b8", "title": "Debiased Contrastive Learning", "journal": "", "year": "2020-12-06", "authors": "Ching-Yao Chuang; Joshua Robinson; Yen-Chen Lin; Antonio Torralba; Stefanie Jegelka"}, {"ref_id": "b9", "title": "A Study of Position Bias in Digital Library Recommender Systems", "journal": "", "year": "2018", "authors": "Andrew Collins; Dominika Tkaczyk; Akiko Aizawa; J\u00f6ran Beel"}, {"ref_id": "b10", "title": "Deep Neural Networks for YouTube Recommendations", "journal": "Association for Computing Machinery", "year": "2016", "authors": "Paul Covington; Jay Adams; Emre Sargin"}, {"ref_id": "b11", "title": "An Experimental Comparison of Click Position-Bias Models", "journal": "Association for Computing Machinery", "year": "2008", "authors": "Nick Craswell; Onno Zoeter; Michael Taylor; Bill Ramsey"}, {"ref_id": "b12", "title": "Are We Really Making Much Progress? A Worrying Analysis of Recent Neural Recommendation Approaches", "journal": "", "year": "2019", "authors": "Maurizio Ferrari Dacrema; Paolo Cremonesi; Dietmar Jannach"}, {"ref_id": "b13", "title": "Simplify and Robustify Negative Sampling for Implicit Collaborative Filtering", "journal": "", "year": "2020", "authors": "Jingtao Ding; Yuhan Quan; Quanming Yao; Yong Li; Depeng Jin"}, {"ref_id": "b14", "title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "journal": "PMLR", "year": "2010", "authors": "Michael Gutmann; Aapo Hyv\u00e4rinen"}, {"ref_id": "b15", "title": "LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation", "journal": "Association for Computing Machinery", "year": "2020", "authors": "Xiangnan He; Kuan Deng; Xiang Wang; Yan Li; Yongdong Zhang; Meng Wang"}, {"ref_id": "b16", "title": "Neural Collaborative Filtering", "journal": "ACM", "year": "2017-04-03", "authors": "Xiangnan He; Lizi Liao; Hanwang Zhang; Liqiang Nie; Xia Hu; Tat-Seng Chua"}, {"ref_id": "b17", "title": "Fast Matrix Factorization for Online Recommendation with Implicit Feedback", "journal": "Association for Computing Machinery", "year": "2016", "authors": "Xiangnan He; Hanwang Zhang; Min-Yen Kan; Tat-Seng Chua"}, {"ref_id": "b18", "title": "Collaborative Metric Learning", "journal": "CHE", "year": "2017", "authors": "Cheng-Kang Hsieh; Longqi Yang; Yin Cui; Tsung-Yi Lin; Serge Belongie; Deborah Estrin"}, {"ref_id": "b19", "title": "Collaborative filtering for implicit feedback datasets", "journal": "", "year": "2008", "authors": "Y Hu; Y Koren; C Volinsky"}, {"ref_id": "b20", "title": "Collaborative Filtering for Implicit Feedback Datasets", "journal": "", "year": "2008", "authors": "Yifan Hu; Yehuda Koren; Chris Volinsky"}, {"ref_id": "b21", "title": "On Using Very Large Target Vocabulary for Neural Machine Translation", "journal": "The Association for Computer Linguistics", "year": "2015-07-26", "authors": "S\u00e9bastien Jean; Kyunghyun Cho; Roland Memisevic; Yoshua Bengio"}, {"ref_id": "b22", "title": "Towards a Better Understanding of Linear Models for Recommendation", "journal": "Association for Computing Machinery", "year": "2021", "authors": "Ruoming Jin; Dong Li; Jing Gao; Zhi Liu; Li Chen; Yang Zhou"}, {"ref_id": "b23", "title": "On Estimating Recommendation Evaluation Metrics under Sampling", "journal": "", "year": "2021-05", "authors": "Ruoming Jin; Dong Li; Benjamin Mudrak; Jing Gao; Zhi Liu"}, {"ref_id": "b24", "title": "On Sampled Metrics for Item Recommendation", "journal": "Association for Computing Machinery", "year": "2020", "authors": "Walid Krichene; Steffen Rendle"}, {"ref_id": "b25", "title": "On sampled metrics for item recommendation", "journal": "Commun. ACM", "year": "2022", "authors": "Walid Krichene; Steffen Rendle"}, {"ref_id": "b26", "title": "Impression-Informed Multi-Behavior Recommender System: A Hierarchical Graph Attention Approach", "journal": "", "year": "2023", "authors": "Dong Li; Divya Bhargavi; Vidya Sagar; Ravipati "}, {"ref_id": "b27", "title": "On sampling top-k recommendation evaluation", "journal": "", "year": "2020", "authors": "Dong Li; Ruoming Jin; Jing Gao; Zhi Liu"}, {"ref_id": "b28", "title": "On Item-Sampling Evaluation for Recommender System", "journal": "ACM Trans. Recomm. Syst", "year": "2023-10", "authors": "Dong Li; Ruoming Jin; Zhenming Liu; Bin Ren; Jing Gao; Zhi Liu"}, {"ref_id": "b29", "title": "Towards Reliable Item Sampling for Recommendation Evaluation", "journal": "AAAI Press", "year": "2023", "authors": "Dong Li; Ruoming Jin; Zhenming Liu; Bin Ren; Jing Gao; Zhi Liu"}, {"ref_id": "b30", "title": "On the regularization landscape for the linear recommendation models", "journal": "", "year": "2022", "authors": "Dong Li; Zhenming Liu; Ruoming Jin; Zhi Liu; Jing Gao; Bin Ren"}, {"ref_id": "b31", "title": "Generation-Augmented Query Expansion For Code Retrieval", "journal": "", "year": "2022", "authors": "Dong Li; Yelong Shen; Ruoming Jin; Yi Mao; Kuan Wang; Weizhu Chen"}, {"ref_id": "b32", "title": "Personalized Ranking with Importance Sampling", "journal": "Association for Computing Machinery", "year": "2020", "authors": "Defu Lian; Qi Liu; Enhong Chen"}, {"ref_id": "b33", "title": "Modeling User Exposure in Recommendation", "journal": "CHE", "year": "2016", "authors": "Dawen Liang; Laurent Charlin; James Mcinerney; David M Blei"}, {"ref_id": "b34", "title": "Variational Autoencoders for Collaborative Filtering", "journal": "ACM", "year": "2018-04-23", "authors": "Dawen Liang; Rahul G Krishnan; Matthew D Hoffman; Tony Jebara"}, {"ref_id": "b35", "title": "A General Knowledge Distillation Framework for Counterfactual Recommendation via Uniform Data", "journal": "Association for Computing Machinery", "year": "2020", "authors": "Dugang Liu; Pengxiang Cheng; Zhenhua Dong; Xiuqiang He; Weike Pan; Zhong Ming"}, {"ref_id": "b36", "title": "SimpleX: A Simple and Strong Baseline for Collaborative Filtering", "journal": "Association for Computing Machinery", "year": "2021", "authors": "Kelong Mao; Jieming Zhu; Jinpeng Wang; Quanyu Dai; Zhenhua Dong; Xi Xiao; Xiuqiang He"}, {"ref_id": "b37", "title": "Collaborative Filtering and the Missing at Random Assumption", "journal": "AUAI Press", "year": "2007-07-19", "authors": "Benjamin M Marlin; Richard S Zemel; Sam T Roweis; Malcolm Slaney"}, {"ref_id": "b38", "title": "SLIM: Sparse Linear Methods for Top-N Recommender Systems", "journal": "", "year": "2011", "authors": "Xia Ning; George Karypis"}, {"ref_id": "b39", "title": "Representation learning with contrastive predictive coding", "journal": "", "year": "2018", "authors": "Aaron Van Den Oord; Yazhe Li; Oriol Vinyals"}, {"ref_id": "b40", "title": "Item recommendation from implicit feedback", "journal": "Springer", "year": "2021", "authors": "Steffen Rendle"}, {"ref_id": "b41", "title": "Improving Pairwise Learning for Item Recommendation from Implicit Feedback", "journal": "Association for Computing Machinery", "year": "2014", "authors": "Steffen Rendle; Christoph Freudenthaler"}, {"ref_id": "b42", "title": "BPR: Bayesian Personalized Ranking from Implicit Feedback", "journal": "AUAI Press", "year": "2009", "authors": "Steffen Rendle; Christoph Freudenthaler; Zeno Gantner; Lars Schmidt-Thieme"}, {"ref_id": "b43", "title": "Neural Collaborative Filtering vs. Matrix Factorization Revisited", "journal": "Association for Computing Machinery", "year": "2020", "authors": "Steffen Rendle; Walid Krichene; Li Zhang; John Anderson"}, {"ref_id": "b44", "title": "iALS++: Speeding up Matrix Factorization with Subspace Optimization", "journal": "", "year": "2021", "authors": "Steffen Rendle; Walid Krichene; Li Zhang; Yehuda Koren"}, {"ref_id": "b45", "title": "Revisiting the Performance of IALS on Item Recommendation Benchmarks", "journal": "Association for Computing Machinery", "year": "2022", "authors": "Steffen Rendle; Walid Krichene; Li Zhang; Yehuda Koren"}, {"ref_id": "b46", "title": "On the Difficulty of Evaluating Baselines: A Study on Recommender Systems", "journal": "", "year": "2019", "authors": "Steffen Rendle; Li Zhang; Yehuda Koren"}, {"ref_id": "b47", "title": "Unbiased Pairwise Learning from Biased Implicit Feedback", "journal": "Association for Computing Machinery", "year": "2020", "authors": "Yuta Saito"}, {"ref_id": "b48", "title": "Unbiased Recommender Learning from Missing-Not-At-Random Implicit Feedback", "journal": "Association for Computing Machinery", "year": "2020", "authors": "Yuta Saito; Suguru Yaginuma; Yuta Nishino; Hayato Sakata; Kazuhide Nakata"}, {"ref_id": "b49", "title": "Training and Testing of Recommender Systems on Data Missing Not at Random", "journal": "Association for Computing Machinery", "year": "2010", "authors": "Harald Steck"}, {"ref_id": "b50", "title": "Embarrassingly Shallow Autoencoders for Sparse Data", "journal": "", "year": "2019", "authors": "Harald Steck"}, {"ref_id": "b51", "title": "Autoencoders that don't overfit towards the Identity", "journal": "", "year": "2020-12-06", "authors": "Harald Steck"}, {"ref_id": "b52", "title": "Contrastive Multiview Coding", "journal": "Springer", "year": "2020-08-23", "authors": "Yonglong Tian; Dilip Krishnan; Phillip Isola"}, {"ref_id": "b53", "title": "Graph attention networks", "journal": "", "year": "2018", "authors": "Petar Veli\u010dkovi\u0107; Guillem Cucurull; Arantxa Casanova; Adriana Romero; Pietro Lio; Yoshua Bengio"}, {"ref_id": "b54", "title": "Towards Representation Alignment and Uniformity in Collaborative Filtering", "journal": "Association for Computing Machinery", "year": "2022", "authors": "Chenyang Wang; Yuanqing Yu; Weizhi Ma; Min Zhang; Chong Chen; Yiqun Liu; Shaoping Ma"}, {"ref_id": "b55", "title": "Neural Graph Collaborative Filtering", "journal": "ACM", "year": "2019-07-21", "authors": "Xiang Wang; Xiangnan He; Meng Wang; Fuli Feng; Tat-Seng Chua"}, {"ref_id": "b56", "title": "CellPAD: Detecting Performance Anomalies in Cellular Networks via Regression Analysis", "journal": "", "year": "2018", "authors": "Jun Wu; P C Patrick; Qi Lee; Lujia Li; Jianfeng Pan;  Zhang"}, {"ref_id": "b57", "title": "Self-supervised Graph Learning for Recommendation", "journal": "ACM", "year": "2021-07-11", "authors": "Jiancan Wu; Xiang Wang; Fuli Feng; Xiangnan He; Liang Chen; Jianxun Lian; Xing Xie"}, {"ref_id": "b58", "title": "BotTriNet: A Unified and Efficient Embedding for Social Bots Detection via Metric Learning", "journal": "", "year": "2023", "authors": "Jun Wu; Xuesong Ye; Yanyuet Man"}, {"ref_id": "b59", "title": "Graph Convolutional Neural Networks for Web-Scale Recommender Systems", "journal": "ACM", "year": "2018-08-19", "authors": "Rex Ying; Ruining He; Kaifeng Chen; Pong Eksombatchai; William L Hamilton; Jure Leskovec"}, {"ref_id": "b60", "title": "Deep Learning Based Recommender System: A Survey and New Perspectives", "journal": "ACM Comput. Surv", "year": "2019-02", "authors": "Shuai Zhang; Lina Yao; Aixin Sun; Yi Tay"}, {"ref_id": "b61", "title": "Optimizing Top-n Collaborative Filtering via Dynamic Negative Item Sampling", "journal": "Association for Computing Machinery", "year": "2013", "authors": "Weinan Zhang; Tianqi Chen; Jun Wang; Yong Yu"}, {"ref_id": "b62", "title": "Contrastive Learning for Debiased Candidate Generation in Large-Scale Recommender Systems", "journal": "Association for Computing Machinery", "year": "2021", "authors": "Chang Zhou; Jianxin Ma; Jianwei Zhang; Jingren Zhou; Hongxia Yang"}, {"ref_id": "b63", "title": "Unbiased Implicit Recommendation and Propensity Estimation via Combinational Joint Learning (RecSys '20)", "journal": "Association for Computing Machinery", "year": "2020", "authors": "Ziwei Zhu; Yun He; Yin Zhang; James Caverlee"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Effect of number of positive and negative samples on \ud835\udc3a\ud835\udc5c\ud835\udc64\ud835\udc4e\ud835\udc59\ud835\udc59\ud835\udc4e", "figure_data": ""}, {"figure_label": "23", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :Figure 3 :23Figure 2: Effect of negative weight on \ud835\udc3a\ud835\udc5c\ud835\udc64\ud835\udc4e\ud835\udc59\ud835\udc59\ud835\udc4e", "figure_data": ""}, {"figure_label": "23021232112234", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "2 ( 30 ) 2 \ud835\udc39 + ||\u039b 1 2 2 \ud835\udc39 ( 32 )||\u039b 1 2||\u039b 1 2 2 \ud835\udc39+ 2 \ud835\udc39 ( 34 )23021232112234convex. (\ud835\udc65, \ud835\udc67 are vectors) min \ud835\udc53 (\ud835\udc65) + \ud835\udc54(\ud835\udc67) \ud835\udc60.\ud835\udc61 \ud835\udc34\ud835\udc65 + \ud835\udc35\ud835\udc67 = \ud835\udc50 (29) Its equivalent to optimize:L \ud835\udf0c (\ud835\udc65, \ud835\udc66, \ud835\udc67) = \ud835\udc53 (\ud835\udc65) + \ud835\udc54(\ud835\udc67) + \ud835\udc66 \ud835\udc47 (\ud835\udc34\ud835\udc65 + \ud835\udc35\ud835\udc67 -\ud835\udc50) + (\ud835\udf0c/2)||\ud835\udc34\ud835\udc65 + \ud835\udc35\ud835\udc67 -\ud835\udc50 || 2The alternating solution is:\ud835\udc65 \ud835\udc58+1 := argmin \ud835\udc65 \ud835\udc3f \ud835\udf0c \ud835\udc65, \ud835\udc67 \ud835\udc58 , \ud835\udc66 \ud835\udc58 \ud835\udc67 \ud835\udc58+1 := argmin \ud835\udc67 \ud835\udc3f \ud835\udf0c \ud835\udc65 \ud835\udc58+1 , \ud835\udc67, \ud835\udc66 \ud835\udc58 \ud835\udc66 \ud835\udc58+1 := \ud835\udc66 \ud835\udc58 + \ud835\udf0c \ud835\udc34\ud835\udc65 \ud835\udc58+1 + \ud835\udc35\ud835\udc67 \ud835\udc58+1 -\ud835\udc50(31)C.2 Low-Rank EDLAEIn[52], equation 10 is to optimize:||\ud835\udc4b -\ud835\udc4b \u2022 {\ud835\udc48\ud835\udc49 \ud835\udc47 -\ud835\udc51\ud835\udc40\ud835\udc4e\ud835\udc61 \ud835\udc51\ud835\udc56\ud835\udc4e\ud835\udc54(\ud835\udc48\ud835\udc49 \ud835\udc47 ) }|| \u2022 {\ud835\udc48\ud835\udc49 \ud835\udc47 -\ud835\udc51\ud835\udc40\ud835\udc4e\ud835\udc61 \ud835\udc51\ud835\udc56\ud835\udc4e\ud835\udc54(\ud835\udc48\ud835\udc49 \ud835\udc47 ) }||This can be re-written as:||\ud835\udc4b -\ud835\udc4b\ud835\udc48\ud835\udc49 \ud835\udc47 + \ud835\udc4b \u2022 \ud835\udc51\ud835\udc40\ud835\udc4e\ud835\udc61 (\ud835\udefd)|| 2 \ud835\udc39 + \u2022 (\ud835\udc48\ud835\udc49 \ud835\udc47 -\ud835\udc51\ud835\udc40\ud835\udc4e\ud835\udc61 (\ud835\udefd))|| 2 \ud835\udc39 \ud835\udc60.\ud835\udc61 . \ud835\udefd = \ud835\udc51\ud835\udc56\ud835\udc4e\ud835\udc54(\ud835\udc48\ud835\udc49 \ud835\udc47 )(33)Using augmented Lagrangian:||\ud835\udc4b -\ud835\udc4b\ud835\udc48\ud835\udc49 \ud835\udc47 + \ud835\udc4b \u2022 \ud835\udc51\ud835\udc40\ud835\udc4e\ud835\udc61 (\ud835\udefd)|| 2 \ud835\udc39 + \ud835\udc48\ud835\udc49 \ud835\udc47 || -||\ud835\udc51\ud835\udc40\ud835\udc4e\ud835\udc61 (\ud835\udefd)|| 2\ud835\udefe \ud835\udc47 \u03a9(\ud835\udefd -\ud835\udc51\ud835\udc56\ud835\udc4e\ud835\udc54(\ud835\udc48\ud835\udc49 \ud835\udc47 )) + ||\u03a9 1 2 (\ud835\udefd -\ud835\udc51\ud835\udc56\ud835\udc4e\ud835\udc54(\ud835\udc48\ud835\udc49 \ud835\udc47 ))||\ud835\udefe is the vector of Lagrangian multipliers, amd \u03a9 = \u039b + \ud835\udf14\ud835\udc3c where \ud835\udf14 > 0 is a scalar.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "(Debiased) Contrastive Learning Loss for Recommendation (Technical Report) Conference acronym 'XX, 2023, Woodstock, NY", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Statistics of the datasets.", "figure_data": "DatasetUser # Item # Interaction #Yelp201831,668 38,0481, 561, 406Gowalla29,858 40,9811, 027, 370Amazon-Books 52,643 91,5992, 984, 108"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "The performance of different loss functions w.r.t its debiased one on MF (Matrix Factorization). The unit of the metric values is %. We also present and highlight the relative improvement (RI) of the debiased loss function with its vanilla (biased) counterpart.", "figure_data": "YelpGowallaAmazon-BooksLossAverage RI%Recall@20 NDCG@20 Recall@20 NDCG@20 Recall@20 NDCG@20biased6.915.6718.1714.615.274.22-CCLdebiased6.985.7118.4214.975.494.40-RI%1.01%0.71%1.38%2.46%4.17%4.27%2.33%biased5.964.9514.2712.393.362.68-MSEdebiased6.055.0014.9812.643.352.61-RI%1.51%1.01%4.98%2.02%-0.30%-2.61%1.10%biased6.545.3616.4513.434.603.56-InfoNCEdebiased6.665.4516.5713.724.653.66-RI%1.83%1.68%0.73%2.16%1.09%2.81%1.72%MINE6.565.3716.9314.285.003.93-MINE (ours)RI% (w.r.t baised infoNCE)0.31%0.19%2.92%6.33%8.70%10.39%4.80%MINE+7.125.8618.1315.315.184.05-RI% (w.r.t baised infoNCE)8.87%9.33%10.21%14.00%12.61%13.76%11.46%"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Hyperparameter details of \ud835\udc40\ud835\udc3c \ud835\udc41 \ud835\udc38+ loss", "figure_data": "Yelp2018 Gowalla Amazon-Booksnegative weight1.11.21.1temperature0.50.40.4regularization1e-81e-81e-8number of negative800800800"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Hyperparameter details of debiased \ud835\udc36\ud835\udc36\ud835\udc3f loss", "figure_data": "Yelp2018 Gowalla Amazon-Booksnegative weight0.40.70.6margin0.90.90.4regularization-9-9-9number of negative800800800number of positive102050"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "\ud835\udc62 \ud835\udc46 ) \ud835\udc47 w \ud835\udc62 -\u221a \ud835\udc50 \ud835\udc62 y \ud835\udc46 || 2 + || \u221a \ud835\udefc 0 \ud835\udc3b \ud835\udc47 w \ud835\udc62 || 2 + || \u221a\ufe01 \ud835\udf06 \ud835\udc62 \ud835\udc3c \u2022 w \ud835\udc62 || 2 -|| \u221a \ud835\udc50 \ud835\udc62 \ud835\udefc 0 (\ud835\udc3b \ud835\udc62 \ud835\udc46 ) \ud835\udc47 w \ud835\udc62 || 2 , \ud835\udc64\u210e\ud835\udc52\ud835\udc5f\ud835\udc52 \ud835\udf06 \ud835\udc62 = \ud835\udf06(|I + \ud835\udc62 | + \ud835\udefc 0 |I|) \ud835\udf08 Note that the observed item matrix for user \ud835\udc62 as \ud835\udc3b \ud835\udc62 \ud835\udc46 : \ud835\udc3b is the entire item matrix for all items. \ud835\udc66 \ud835\udc46 are all 1 column (the same row dimension as \ud835\udc3b \ud835\udc62 \ud835\udc46 . \u2022 Fixing User Vectors, optimizing: \u221a\ufe01 \ud835\udc36 \ud835\udc62 y \ud835\udc46 || 2 + || \u221a \ud835\udefc 0 \ud835\udc4a \ud835\udc47 h \ud835\udc56 || 2 + || \u221a\ufe01 \ud835\udf06 \ud835\udc56 \ud835\udc3c \u2022 h \ud835\udc56 || 2 \ud835\udc56 || 2 , \ud835\udc64\u210e\ud835\udc52\ud835\udc5f\ud835\udc52 \ud835\udf06 \ud835\udc56 = \ud835\udf06(|U + \ud835\udc56 | + \ud835\udefc 0 |U|) \ud835\udf08 Here, \ud835\udc4a \ud835\udc56 \ud835\udc46 are observed user matrix for item \ud835\udc56, and \ud835\udc4a are the entire user matrix, \ud835\udc36 \ud835\udc62 = \ud835\udc51\ud835\udc56\ud835\udc4e\ud835\udc54(\ud835\udc50 \ud835\udc62 ) a |U| \u00d7 |U| diagonal matrix with \ud835\udc50 \ud835\udc62 on the diagonal. Solving L \ud835\udc62 and L \ud835\udc56 , we have the following closed form solutions: w * \ud835\udc62 = \ud835\udc50 \ud835\udc62 (1 -\ud835\udefc 0 )\ud835\udc3b \ud835\udc62 \ud835\udc46 (\ud835\udc3b \ud835\udc62 \ud835\udc46 ) \ud835\udc47 + \ud835\udefc 0 \ud835\udc3b\ud835\udc3b \ud835\udc47 + \ud835\udf06 \ud835\udc62 \ud835\udc3c \ud835\udefc 0 )\ud835\udc4a \ud835\udc56 \ud835\udc46 \ud835\udc36 \ud835\udc62 (\ud835\udc4a \ud835\udc56 \ud835\udc46 ) \ud835\udc47 + \ud835\udefc 0 \ud835\udc4a\ud835\udc4a \ud835\udc47 + \ud835\udf06 \ud835\udc56 \ud835\udc3c \u221a\ufe01 \ud835\udc36 \ud835\udc62 y \ud835\udc46 Assuming \ud835\udc50 \ud835\udc62 be a constant for all users, we get w * \ud835\udc62 \u221d \ud835\udc3b \ud835\udc62 \ud835\udc46 (\ud835\udc3b \ud835\udc62 \ud835\udc46 ) \ud835\udc47 + \ud835\udefc 0 (1 -\ud835\udefc 0 )\ud835\udc50 \ud835\udc62 \ud835\udc3b\ud835\udc3b \ud835\udc47 + \ud835\udf06 \ud835\udc62 (1 -\ud835\udefc 0 )\ud835\udc50 \ud835\udc62 \ud835\udc3c", "figure_data": "-1\u2022 \ud835\udc3b \ud835\udc62 \ud835\udc46 \u2022 y \ud835\udc46h  *  \ud835\udc56 \u221d \ud835\udc4a \ud835\udc56 \ud835\udc46 (\ud835\udc4a \ud835\udc56 \ud835\udc46 ) \ud835\udc47 +\ud835\udefc 0 (1 -\ud835\udefc 0 )\ud835\udc50 \ud835\udc62\ud835\udc4a\ud835\udc4a \ud835\udc47 +\ud835\udf06 \ud835\udc56 (1 -\ud835\udefc 0 )\ud835\udc50 \ud835\udc62\ud835\udc3c-1\u2022 \ud835\udc4a \ud835\udc56 \ud835\udc46 \u2022 y \ud835\udc46\ud835\udc62 \ud835\udc46 =\uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0h 1 h \ud835\udc60 \u2022 \u2022 \u2022 h | I + \ud835\udc62 |\uf8f9 \uf8fa \uf8fb \uf8fa \uf8fa \uf8fa\u2208 R \ud835\udc58 \u00d7 | I + \ud835\udc62 | , \ud835\udc60 \u2208 I + \ud835\udc62(28)L \ud835\udc56 = ||(\ud835\udc4a \ud835\udc56 \ud835\udc46\u221a\ufe01 \ud835\udc36 \ud835\udc62 )-||\u221a \ud835\udefc 0 (\u221a\ufe01 \ud835\udc36 \ud835\udc62 \ud835\udc4a \ud835\udc56 \ud835\udc46 )-1\u2022 \ud835\udc3b \ud835\udc62 \ud835\udc46 \u2022\u221a \ud835\udc50 \ud835\udc62 y \ud835\udc46h  -1\u2022 \ud835\udc4a \ud835\udc56 \ud835\udc46 \u2022"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "\ud835\udc62[\ud835\udc50 \ud835\udc62 ( \u0177\ud835\udc62\ud835\udc56 -1)2 -\ud835\udc50 \ud835\udc62 \ud835\udf06 \u01772 \ud835\udc62\ud835\udc56 ] + \ud835\udf06 \ud835\udc62 [\ud835\udc50 \ud835\udc62 ( \u0177\ud835\udc62\ud835\udc56 -1) 2 + \ud835\udc50 \ud835\udc62 \u2211\ufe01 \ud835\udc39 For Regression based \ud835\udc4a = \ud835\udc48\ud835\udc49 \ud835\udc47 \ud835\udc3f = || \u221a\ufe01 \ud835\udc36 \ud835\udc62 (\ud835\udc4b -\ud835\udc4b\ud835\udc48\ud835\udc49 \ud835\udc47 )|| 2 \ud835\udc39 -\ud835\udf06||\ud835\udc4b \u2299 \u221a\ufe01 \ud835\udc36 \ud835\udc62 -\ud835\udc3c\ud835\udc4b\ud835\udc48\ud835\udc49 \ud835\udc47 || 2 \ud835\udc39 -||(1 -\ud835\udc4b ) \u2299 \u221a\ufe01 \ud835\udc36 \ud835\udc62 -\ud835\udf06\ud835\udc3c\ud835\udc4b\ud835\udc48\ud835\udc49 \ud835\udc47 || 2", "figure_data": "L \ud835\udc37\ud835\udc52\ud835\udc4f\ud835\udc56\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc51 \ud835\udc5a\ud835\udc60\ud835\udc52=\u2211\ufe01\u2211\ufe01\u2211\ufe01\u01772 \ud835\udc62\ud835\udc61\ud835\udc62\ud835\udc56 \u2208 I +\ud835\udc61 \u2208 I=\u2211\ufe01\u2211\ufe01\u2211\ufe01\u01772 \ud835\udc62\ud835\udc61 + \ud835\udf06\u2211\ufe01\u01772 \ud835\udc62\ud835\udc61\ud835\udc62\ud835\udc56 \u2208 I +\ud835\udc61 \u2208 I + \ud835\udc62\ud835\udc5d \u2208 I\\I + \ud835\udc62=\u2211\ufe01\u2211\ufe01\u01772\ud835\udc62\ud835\udc56 \u2208 I +\ud835\udc5d \u2208 I\\I + \ud835\udc62"}], "formulas": [{"formula_id": "formula_0", "formula_text": "\ud835\udc5d (\ud835\udc56 |\ud835\udc62) = exp( \u0177\ud835\udc62\ud835\udc56 )", "formula_coordinates": [2.0, 119.19, 695.14, 70.39, 11.49]}, {"formula_id": "formula_1", "formula_text": "L \ud835\udc60\ud835\udc5c \ud835\udc53 \ud835\udc61 = -E \ud835\udc62 log \u2211\ufe01 \ud835\udc56 \u2208 I + \ud835\udc62 \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62\ud835\udc56 ) \ud835\udc57 \u2208 I \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62 \ud835\udc57 )(1)", "formula_coordinates": [2.0, 370.16, 124.44, 188.58, 26.17]}, {"formula_id": "formula_2", "formula_text": "L \ud835\udc60\ud835\udc60\ud835\udc5c \ud835\udc53 \ud835\udc61 = -E \ud835\udc62 log \u2211\ufe01 \ud835\udc56 \u2208 I + \ud835\udc62 \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62\ud835\udc56 ) \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62\ud835\udc56 ) + \ud835\udc41 \ud835\udc57=1;\ud835\udc57\u223c\ud835\udc5d - \ud835\udc62 \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62 \ud835\udc57 )/\ud835\udc5d - \ud835\udc62 ( \ud835\udc57)(2)", "formula_coordinates": [2.0, 322.89, 259.7, 235.85, 26.17]}, {"formula_id": "formula_3", "formula_text": "L \ud835\udc56\ud835\udc5b\ud835\udc53 \ud835\udc5c = -E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 log \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62\ud835\udc56 ) \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62\ud835\udc56 ) + \ud835\udc41 \ud835\udc57=1;\ud835\udc57\u223c\ud835\udc5d - \ud835\udc62 \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62 \ud835\udc57 )(3)", "formula_coordinates": [2.0, 342.35, 436.68, 216.39, 24.78]}, {"formula_id": "formula_4", "formula_text": "L \ud835\udc56\ud835\udc5b\ud835\udc53 \ud835\udc5c = -E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 log \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62\ud835\udc56 ) \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62\ud835\udc56 ) + \ud835\udc41 \ud835\udc57=1;\ud835\udc57\u223c\ud835\udc5d \ud835\udc62 \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62 \ud835\udc57 )(4)", "formula_coordinates": [2.0, 342.6, 645.16, 216.14, 24.4]}, {"formula_id": "formula_5", "formula_text": "\ud835\udc5d \ud835\udc62 (\ud835\udc56) = \ud835\udf0f + \ud835\udc62 \u2022 \ud835\udc5d + \ud835\udc62 (\ud835\udc56) + \ud835\udf0f - \ud835\udc62 \u2022 \ud835\udc5d - \ud835\udc62 (\ud835\udc56)(5)", "formula_coordinates": [3.0, 118.9, 337.34, 175.69, 11.14]}, {"formula_id": "formula_6", "formula_text": "L \ud835\udc3c\ud835\udc5b\ud835\udc53 \ud835\udc5c = -E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 log exp( \u0177\ud835\udc62\ud835\udc56 ) exp( \u0177\ud835\udc62\ud835\udc56 ) + \ud835\udf06 E \ud835\udc57 \u223c\ud835\udc5d - \ud835\udc62 exp( \u0177\ud835\udc62 \ud835\udc57 ) = -E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 log exp( \u0177\ud835\udc62\ud835\udc56 ) exp( \u0177\ud835\udc62\ud835\udc56 ) + \ud835\udf06 1 \ud835\udf0f -E \ud835\udc61 \u223c\ud835\udc5d\ud835\udc62 [exp( \u0177\ud835\udc62\ud835\udc61 ) ] -\ud835\udf0f + E \ud835\udc57 \u223c\ud835\udc5d + \ud835\udc62 [exp( \u0177\ud835\udc62 \ud835\udc57 ) ](6)", "formula_coordinates": [3.0, 54.44, 489.98, 242.11, 59.13]}, {"formula_id": "formula_7", "formula_text": "L \ud835\udc37\ud835\udc52\ud835\udc4f\ud835\udc56\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc51 \ud835\udc3c\ud835\udc5b\ud835\udc53 \ud835\udc5c = -E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 ;\ud835\udc57 \u223c\ud835\udc5d\ud835\udc62 ;\ud835\udc58\u223c\ud835\udc5d + \ud835\udc62 log exp( \u0177\ud835\udc62\ud835\udc56 ) exp( \u0177\ud835\udc62\ud835\udc56 ) + \ud835\udf06\ud835\udc53 ( { \ud835\udc57 } \ud835\udc41 1 , {\ud835\udc58 } \ud835\udc40 1 )", "formula_coordinates": [3.0, 54.44, 611.74, 221.94, 20.31]}, {"formula_id": "formula_8", "formula_text": "\ud835\udc53 ( { \ud835\udc57 } \ud835\udc41 1 , {\ud835\udc58 } \ud835\udc40 1 ) = max 1 \ud835\udf0f - \ud835\udc62 1 \ud835\udc41 \ud835\udc41 \u2211\ufe01 \ud835\udc57 =1;\ud835\udc57 \u223c\ud835\udc5d\ud835\udc62 \ud835\udc52\ud835\udc65\ud835\udc5d (\ud835\udc53 \ud835\udc62 \ud835\udc57 ) -\ud835\udf0f + \ud835\udc62 1 \ud835\udc40 \ud835\udc40 \u2211\ufe01 \ud835\udc58=1;\ud835\udc58\u223c\ud835\udc5d + \ud835\udc62 \ud835\udc52\ud835\udc65\ud835\udc5d (\ud835\udc53 \ud835\udc62\ud835\udc58 ) , \ud835\udc52 1/\ud835\udc61 (7)", "formula_coordinates": [3.0, 54.2, 638.44, 272.29, 31.8]}, {"formula_id": "formula_9", "formula_text": "\ud835\udc3c (\ud835\udc62, \ud835\udc56) = \ud835\udc60\ud835\udc62\ud835\udc5d (\ud835\udc63 \ud835\udc62 ;\ud835\udc63 \ud835\udc56 ) E \ud835\udc5d \ud835\udc62,\ud835\udc56 [ \u0177\ud835\udc62\ud835\udc56 ] -log E \ud835\udc5d \ud835\udc62 \u2297\ud835\udc5d \ud835\udc56 [\ud835\udc52 \u0177\ud835\udc62\ud835\udc56 ](8)", "formula_coordinates": [3.0, 356.43, 217.07, 202.31, 13.95]}, {"formula_id": "formula_10", "formula_text": "L \ud835\udc5a\ud835\udc56\ud835\udc5b\ud835\udc52 = -E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 \u0177\ud835\udc62\ud835\udc56 -log E \ud835\udc57\u223c\ud835\udc5d \ud835\udc56 [exp \u0177\ud835\udc62 \ud835\udc57 ](9)", "formula_coordinates": [3.0, 360.2, 341.13, 198.54, 14.73]}, {"formula_id": "formula_11", "formula_text": "L \ud835\udc5a\ud835\udc56\ud835\udc5b\ud835\udc52 = -E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 \u0177\ud835\udc62\ud835\udc56 -log \ud835\udc41 \u2211\ufe01 \ud835\udc57=1;\ud835\udc57\u223c\ud835\udc5d \ud835\udc56 [exp \u0177\ud835\udc62 \ud835\udc57 ](10)", "formula_coordinates": [3.0, 353.8, 405.77, 204.94, 25.82]}, {"formula_id": "formula_12", "formula_text": "L \ud835\udc56\ud835\udc5b\ud835\udc53 \ud835\udc5c = -E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 log \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62\ud835\udc56 ) \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62\ud835\udc56 ) + \ud835\udc41 \ud835\udc57=1;\ud835\udc57\u223c\ud835\udc5d \ud835\udc62 \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62 \ud835\udc57 ) \u2265 L \ud835\udc5a\ud835\udc56\ud835\udc5b\ud835\udc52 = E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 log( \ud835\udc41 \u2211\ufe01 \ud835\udc57=1;\ud835\udc57\u223c\ud835\udc5d \ud835\udc56 \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62 \ud835\udc57 -\u0177\ud835\udc62\ud835\udc56 )) \u2248 E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 log \ud835\udc41 E \ud835\udc57\u223c\ud835\udc5d \ud835\udc56 \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62 \ud835\udc57 -\u0177\ud835\udc62\ud835\udc56 )) \u2265 E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 E \ud835\udc57\u223c\ud835\udc5d \ud835\udc56 \u0177\ud835\udc62 \ud835\udc57 -\u0177\ud835\udc62\ud835\udc56 + log \ud835\udc41 \u2265 E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 E \ud835\udc57\u223c\ud835\udc5d \ud835\udc56 \u0177\ud835\udc62 \ud835\udc57 -\u0177\ud835\udc62\ud835\udc56(11)", "formula_coordinates": [3.0, 331.8, 468.86, 226.94, 97.46]}, {"formula_id": "formula_13", "formula_text": "E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 \ud835\udc41 max \ud835\udc57=1;\ud835\udc57\u223c\ud835\udc5d \ud835\udc56 0, \u0177\ud835\udc62 \ud835\udc57 -\u0177\ud835\udc62\ud835\udc56 + log(\ud835\udc41 + 1) \u2265 L \ud835\udc56\ud835\udc5b\ud835\udc53 \ud835\udc5c \u2265 L \ud835\udc5a\ud835\udc56\ud835\udc5b\ud835\udc52 = E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 log( \ud835\udc41 \u2211\ufe01 \ud835\udc57=1;\ud835\udc57\u223c\ud835\udc5d \ud835\udc56 \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62 \ud835\udc57 -\u0177\ud835\udc62\ud835\udc56 )) \u2265 E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 \ud835\udc41 max \ud835\udc57=1;\ud835\udc57\u223c\ud835\udc5d \ud835\udc56 \u0177\ud835\udc62 \ud835\udc57 -\u0177\ud835\udc62\ud835\udc56(12)", "formula_coordinates": [3.0, 330.91, 594.69, 227.83, 75.34]}, {"formula_id": "formula_14", "formula_text": "L \ud835\udc4f\ud835\udc5d\ud835\udc5f = E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 \ud835\udc41 \u2211\ufe01 \ud835\udc57=1;\ud835\udc57\u223c\ud835\udc5d \ud835\udc56 -log \ud835\udf0e ( \u0177\ud835\udc62\ud835\udc56 -\u0177\ud835\udc62 \ud835\udc57 ) = E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 \ud835\udc41 \u2211\ufe01 \ud835\udc57=1;\ud835\udc57\u223c\ud835\udc5d \ud835\udc56 log(1 + \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62 \ud835\udc57 -\u0177\ud835\udc62\ud835\udc56 ))(13)", "formula_coordinates": [4.0, 99.65, 103.06, 194.93, 59.33]}, {"formula_id": "formula_15", "formula_text": "L \ud835\udc56\ud835\udc5b\ud835\udc53 \ud835\udc5c = E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 log(1 + \ud835\udc41 \u2211\ufe01 \ud835\udc57=1;\ud835\udc57\u223c\ud835\udc5d \ud835\udc56 \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62 \ud835\udc57 -\u0177\ud835\udc62\ud835\udc56 )) \u2248 E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 log(1 + \ud835\udc41 E \ud835\udc57\u223c\ud835\udc5d \ud835\udc56 \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62 \ud835\udc57 -\u0177\ud835\udc62\ud835\udc56 )) \u2265 E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 E \ud835\udc57\u223c\ud835\udc5d \ud835\udc56 log(1 + \ud835\udc41 \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62 \ud835\udc57 -\u0177\ud835\udc62\ud835\udc56 )) \u2265 E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 E \ud835\udc57\u223c\ud835\udc5d \ud835\udc56 log(1/\ud835\udc41 + \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62 \ud835\udc57 -\u0177\ud835\udc62\ud835\udc56 )) + log \ud835\udc41 \u2265 E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 E \ud835\udc57\u223c\ud835\udc5d \ud835\udc56 \u0177\ud835\udc62 \ud835\udc57 -\u0177\ud835\udc62\ud835\udc56 ) + log \ud835\udc41 \u2265 E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 E \ud835\udc57\u223c\ud835\udc5d \ud835\udc56 \u0177\ud835\udc62 \ud835\udc57 -\u0177\ud835\udc62\ud835\udc56 )(14)", "formula_coordinates": [4.0, 84.34, 188.43, 210.24, 125.87]}, {"formula_id": "formula_16", "formula_text": "L \ud835\udc4f\ud835\udc5d\ud835\udc5f = E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 \ud835\udc41 \u2211\ufe01 \ud835\udc57=1;\ud835\udc57\u223c\ud835\udc5d \ud835\udc56 log(1 + \ud835\udc52\ud835\udc65\ud835\udc5d ( \u0177\ud835\udc62 \ud835\udc57 -\u0177\ud835\udc62\ud835\udc56 )) \u2265 E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 \ud835\udc41 \u2211\ufe01 \ud835\udc57=1;\ud835\udc57\u223c\ud835\udc5d \ud835\udc56 \ud835\udc5a\ud835\udc4e\ud835\udc65 (0, \u0177\ud835\udc62 \ud835\udc57 -\u0177\ud835\udc62\ud835\udc56 ) \u221d E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 E \ud835\udc57\u223c\ud835\udc5d \ud835\udc56 \ud835\udc5a\ud835\udc4e\ud835\udc65 (0, \u0177\ud835\udc62 \ud835\udc57 -\u0177\ud835\udc62\ud835\udc56 ) (15)", "formula_coordinates": [4.0, 85.77, 397.13, 208.81, 79.25]}, {"formula_id": "formula_17", "formula_text": "L \ud835\udc5a\ud835\udc56\ud835\udc5b\ud835\udc52+ = -E \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 \u0177\ud835\udc62\ud835\udc56 /\ud835\udc61 -\ud835\udf06 log \ud835\udc41 \u2211\ufe01 \ud835\udc57=1;\ud835\udc57\u223c\ud835\udc5d \ud835\udc56 [exp( \u0177\ud835\udc62 \ud835\udc57 /\ud835\udc61](16)", "formula_coordinates": [4.0, 69.54, 634.14, 225.05, 25.82]}, {"formula_id": "formula_18", "formula_text": "\ud835\udc59 + \ud835\udc5a\ud835\udc60\ud835\udc52 ( \u0177\ud835\udc62\ud835\udc56 ) = (1 -\u0177\ud835\udc62\ud835\udc56 ) 2 \ud835\udc59 - \ud835\udc5a\ud835\udc60\ud835\udc52 ( \u0177\ud835\udc62\ud835\udc56 ) = ( \u0177\ud835\udc62\ud835\udc56 ) 2(17)", "formula_coordinates": [4.0, 404.0, 406.73, 154.74, 23.84]}, {"formula_id": "formula_19", "formula_text": "L \ud835\udc36\ud835\udc36\ud835\udc3f = E \ud835\udc62 \u2211\ufe01 \ud835\udc56 \u2208 I + \ud835\udc62 (1 -\u0177\ud835\udc62\ud835\udc56 ) + \ud835\udc64 \ud835\udc41 \ud835\udc41 \u2211\ufe01 \ud835\udc57 \u2208 I \ud835\udc45\ud835\udc52\ud835\udc3f\ud835\udc48 ( \u0177\ud835\udc62 \ud835\udc57 -\ud835\udf16)(18)", "formula_coordinates": [4.0, 344.64, 491.49, 214.1, 27.01]}, {"formula_id": "formula_20", "formula_text": "\ud835\udc59 + \ud835\udc50\ud835\udc50\ud835\udc59 ( \u0177\ud835\udc62\ud835\udc56 ) = 1 -\u0177\ud835\udc62\ud835\udc56 \ud835\udc59 - \ud835\udc50\ud835\udc50\ud835\udc59 ( \u0177\ud835\udc62\ud835\udc56 ) = \ud835\udc45\ud835\udc52\ud835\udc3f\ud835\udc48 ( \u0177\ud835\udc62\ud835\udc56 -\ud835\udf16)(19)", "formula_coordinates": [4.0, 397.49, 571.42, 161.25, 25.51]}, {"formula_id": "formula_21", "formula_text": "L \ud835\udc5d\ud835\udc5c\ud835\udc56\ud835\udc5b\ud835\udc61 = E \ud835\udc62 \ud835\udf0f + \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 \ud835\udc59 + ( \u0177\ud835\udc62\ud835\udc56 ) + \ud835\udf06\ud835\udf0f - \ud835\udc62 E \ud835\udc57 \u223c\ud835\udc5d - \ud835\udc62 \ud835\udc59 -( \u0177\ud835\udc62 \ud835\udc57 )(20)", "formula_coordinates": [4.0, 356.06, 693.74, 202.62, 15.63]}, {"formula_id": "formula_22", "formula_text": "L \ud835\udc37\ud835\udc52\ud835\udc4f\ud835\udc56\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc51 \ud835\udc5d\ud835\udc5c\ud835\udc56\ud835\udc5b\ud835\udc61 = E \ud835\udc62 \ud835\udf0f + \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 \ud835\udc59 + ( \u0177\ud835\udc62\ud835\udc56 ) + \ud835\udf0f - \ud835\udc62 \ud835\udf06 \u2022 E \ud835\udc57\u223c\ud835\udc5d - \ud835\udc62 \ud835\udc59 -( \u0177\ud835\udc62\ud835\udc56 ) = E \ud835\udc62 \ud835\udf0f + \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 \ud835\udc59 + ( \u0177\ud835\udc62\ud835\udc56 ) + \ud835\udf06 E \ud835\udc57\u223c\ud835\udc5d \ud835\udc62 \ud835\udc59 -( \u0177\ud835\udc62\ud835\udc56 ) -\ud835\udf0f + \ud835\udc62 E \ud835\udc58\u223c\ud835\udc58 + \ud835\udc62 \ud835\udc59 -( \u0177\ud835\udc62 \ud835\udc57 )(21)", "formula_coordinates": [5.0, 61.55, 203.51, 233.03, 40.84]}, {"formula_id": "formula_23", "formula_text": "L \ud835\udc37\ud835\udc52\ud835\udc4f\ud835\udc56\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc51 \ud835\udc5d\ud835\udc5c\ud835\udc56\ud835\udc5b\ud835\udc61 = E \ud835\udc62 \ud835\udc38 \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 \ud835\udf0f + \ud835\udc62 \ud835\udc59 + ( \u0177\ud835\udc62\ud835\udc56 )+ \ud835\udf06 1 \ud835\udc41 \ud835\udc41 \u2211\ufe01 \ud835\udc57=1;\ud835\udc57\u223c\ud835\udc5d \ud835\udc62 \ud835\udc59 -( \u0177\ud835\udc62 \ud835\udc57 ) -\ud835\udf0f + \ud835\udc62 1 \ud835\udc40 \ud835\udc40 \u2211\ufe01 \ud835\udc58=1;\ud835\udc58\u223c\ud835\udc5d + \ud835\udc62 \ud835\udc59 -( \u0177\ud835\udc62\ud835\udc58 )(22)", "formula_coordinates": [5.0, 85.28, 316.51, 209.3, 48.49]}, {"formula_id": "formula_24", "formula_text": "\uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 L \ud835\udc36\ud835\udc36\ud835\udc3f = E \ud835\udc62 \ud835\udf0f + \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 (1 -\u0177\ud835\udc62\ud835\udc56 ) + \ud835\udf06\ud835\udf0f - \ud835\udc62 E \ud835\udc57\u223c\ud835\udc5d - \ud835\udc62 [\ud835\udc45\ud835\udc52\ud835\udc3f\ud835\udc48 ( \u0177\ud835\udc62 \ud835\udc57 -\ud835\udf16)] L \ud835\udc51\ud835\udc52\ud835\udc4f\ud835\udc56\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc51 \ud835\udc36\ud835\udc36\ud835\udc3f = E \ud835\udc62 \ud835\udf0f + \ud835\udc62 E \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 (1 -\u0177\ud835\udc62\ud835\udc56 ) +\ud835\udf06 E \ud835\udc57\u223c\ud835\udc5d \ud835\udc62 [\ud835\udc45\ud835\udc52\ud835\udc3f\ud835\udc48 ( \u0177\ud835\udc62 \ud835\udc57 -\ud835\udf16)] -\ud835\udf0f + \ud835\udc62 E \ud835\udc58\u223c\ud835\udc5d + \ud835\udc62 [\ud835\udc45\ud835\udc52\ud835\udc3f\ud835\udc48 ( \u0177\ud835\udc62\ud835\udc58 -\ud835\udf16)] L \ud835\udc37\ud835\udc52\ud835\udc4f\ud835\udc56\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc51 \ud835\udc50\ud835\udc50\ud835\udc59 = E \ud835\udc62 \ud835\udc38 \ud835\udc56\u223c\ud835\udc5d + \ud835\udc62 \ud835\udf0f + \ud835\udc62 (1 -\u0177\ud835\udc62\ud835\udc56 )+ \ud835\udf06 1 \ud835\udc41 \ud835\udc41 \ud835\udc57=1;\ud835\udc57\u223c\ud835\udc5d \ud835\udc62 \ud835\udc45\ud835\udc52\ud835\udc3f\ud835\udc48 ( f \ud835\udc62 \ud835\udc57 -\ud835\udf16) -\ud835\udf0f + \ud835\udc62 1 \ud835\udc40 \ud835\udc40 \ud835\udc58=1;\ud835\udc58\u223c\ud835\udc5d + \ud835\udc62 \ud835\udc45\ud835\udc52\ud835\udc3f\ud835\udc48 ( f \ud835\udc62 \ud835\udc57 -\ud835\udf16)", "formula_coordinates": [5.0, 54.16, 437.58, 245.1, 113.95]}, {"formula_id": "formula_25", "formula_text": "L \ud835\udc37\ud835\udc52\ud835\udc4f\ud835\udc56\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc51 \ud835\udc5a\ud835\udc60\ud835\udc52 \u2248 E \ud835\udc62 \ud835\udf0f + \ud835\udc62 | I + \ud835\udc62 | \u2211\ufe01 \ud835\udc56 \u2208I + \ud835\udc62 ( \u0177\ud835\udc62\ud835\udc56 -1) 2 + \ud835\udf06 1 | I | \u2211\ufe01 \ud835\udc61 \u2208I \u01772 \ud835\udc62\ud835\udc61 - \ud835\udf0f + \ud835\udc62 | I + \ud835\udc62 | \u2211\ufe01 \ud835\udc5e \u2208I + \ud835\udc62 \u01772 \ud835\udc62\ud835\udc5e = \u2211\ufe01 \ud835\udc62 1 | I | \ud835\udc50 \ud835\udc62 \u2211\ufe01 \ud835\udc56 \u2208I + \ud835\udc62 ( \u0177\ud835\udc62\ud835\udc56 -1) 2 where, \ud835\udc50 \ud835\udc62 = | I | | I + \ud835\udc62 | \ud835\udf0f + \ud835\udc62 + \ud835\udf06 1 | I | \u2211\ufe01 \ud835\udc61 \u2208I \u01772 \ud835\udc62\ud835\udc61 - 1 | I | \ud835\udc50 \ud835\udc62 \u2211\ufe01 \ud835\udc5e \u2208I + \ud835\udc62 \u01772 \ud835\udc62\ud835\udc5e \u221d \u2211\ufe01 \ud835\udc62 \ud835\udc50 \ud835\udc62 \u2211\ufe01 \ud835\udc56 \u2208I + \ud835\udc62 ( \u0177\ud835\udc62\ud835\udc56 -1) 2 + \ud835\udf06 \u2211\ufe01 \ud835\udc61 \u2208I \u01772 \ud835\udc62\ud835\udc61 -\ud835\udc50 \ud835\udc62 \u2211\ufe01 \ud835\udc5e \u2208I + \ud835\udc62 \u01772 \ud835\udc62\ud835\udc5e = \u2211\ufe01 \ud835\udc62 \u2211\ufe01 \ud835\udc56 \u2208I + \ud835\udc62 [\ud835\udc50 \ud835\udc62 ( \u0177\ud835\udc62\ud835\udc56 -1) 2 -\ud835\udc50 \ud835\udc62 \ud835\udf06 \u01772 \ud835\udc62\ud835\udc5e ] + \ud835\udf06 \u2211\ufe01 \ud835\udc61 \u2208I \u01772 \ud835\udc62\ud835\udc61", "formula_coordinates": [5.0, 318.59, 99.72, 239.28, 145.84]}, {"formula_id": "formula_26", "formula_text": "L \ud835\udc56\ud835\udc34\ud835\udc3f\ud835\udc46 = \u2211\ufe01 (\ud835\udc62,\ud835\udc56 ) \u2208\ud835\udc46 ( \u0177 (\ud835\udc62, \ud835\udc56) -1) 2 + \ud835\udefc 0 \u2211\ufe01 \ud835\udc62 \u2208\ud835\udc48 \u2211\ufe01 \ud835\udc56 \u2208\ud835\udc3c \u0177 (\ud835\udc62, \ud835\udc56) 2 + \ud835\udf06 \u2211\ufe01 \ud835\udc62 \u2208\ud835\udc48 (|I + \ud835\udc62 | + \ud835\udefc 0 |I|) \ud835\udf08 ||w \ud835\udc62 || 2 + \u2211\ufe01 \ud835\udc56 \u2208\ud835\udc3c (|\ud835\udc48 + \ud835\udc56 | + \ud835\udefc 0 |U|) \ud835\udf08 ||h \ud835\udc56 || 2(23)", "formula_coordinates": [5.0, 318.13, 284.12, 241.43, 61.85]}, {"formula_id": "formula_27", "formula_text": "L \ud835\udc37\ud835\udc52\ud835\udc4f\ud835\udc56\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc51 \ud835\udc56\ud835\udc34\ud835\udc3f\ud835\udc46 = \u2211\ufe01 \ud835\udc62 \u2208\ud835\udc48 \u2211\ufe01 \ud835\udc56 \u2208I + \ud835\udc62 [\ud835\udc50 \ud835\udc62 ( \u0177\ud835\udc62\ud835\udc56 -1) 2 -\ud835\udc50 \ud835\udc62 \u2022 \ud835\udefc 0 \u2022 \u01772 \ud835\udc62\ud835\udc56 ] + \ud835\udefc 0 \u2211\ufe01 \ud835\udc61 \u2208\ud835\udc3c \u01772 \ud835\udc62\ud835\udc61 + \ud835\udf06 \u2211\ufe01 \ud835\udc62 \u2208\ud835\udc48 ( | I + \ud835\udc62 | + \ud835\udefc 0 | I | ) \ud835\udf08 | |w \ud835\udc62 | | 2 + \u2211\ufe01 \ud835\udc56 \u2208\ud835\udc3c ( | U + \ud835\udc56 | + \ud835\udefc 0 | U | ) \ud835\udf08 | |h \ud835\udc56 | | 2", "formula_coordinates": [5.0, 325.67, 447.2, 220.52, 49.24]}, {"formula_id": "formula_28", "formula_text": "L \ud835\udc52\ud835\udc4e\ud835\udc60\ud835\udc52 = ||\ud835\udc4b -\ud835\udc4b\ud835\udc4a || 2 \ud835\udc39 + \ud835\udf06||\ud835\udc4a || 2 \ud835\udc39 \ud835\udc60.\ud835\udc61 . \ud835\udc51\ud835\udc56\ud835\udc4e\ud835\udc54(\ud835\udc4a ) = 0", "formula_coordinates": [5.0, 380.06, 615.02, 114.24, 22.46]}, {"formula_id": "formula_29", "formula_text": "\ud835\udc43 = (\ud835\udc4b \ud835\udc47 \ud835\udc4b + \ud835\udf06\ud835\udc3c ) -1 \ud835\udc4a * = \ud835\udc3c -\ud835\udc43 \u2022 \ud835\udc51\ud835\udc40\ud835\udc4e\ud835\udc61 (\ud835\udc51\ud835\udc56\ud835\udc4e\ud835\udc54(1 \u2298 \ud835\udc43))(24)", "formula_coordinates": [5.0, 378.56, 657.72, 180.18, 24.97]}, {"formula_id": "formula_30", "formula_text": "L \ud835\udc37\ud835\udc52\ud835\udc4f\ud835\udc56\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc51 \ud835\udc5a\ud835\udc60\ud835\udc52 = \u2211\ufe01 \ud835\udc62 \u2211\ufe01 \ud835\udc56 \u2208 I + \ud835\udc62 [\ud835\udc50 \ud835\udc62 ( \u0177\ud835\udc62\ud835\udc56 -1) 2 -\ud835\udc50 \ud835\udc62 \ud835\udf06 \u01772 \ud835\udc62\ud835\udc5e ] + \ud835\udf06 \u2211\ufe01 \ud835\udc61 \u2208 I \u01772 \ud835\udc62\ud835\udc61 = \u2211\ufe01 \ud835\udc62 \u2211\ufe01 \ud835\udc56 \u2208 I + \ud835\udc62 [\ud835\udc50 \ud835\udc62 ( \u0177\ud835\udc62\ud835\udc56 -1) 2 -\ud835\udc50 \ud835\udc62 \ud835\udf06 \u01772 \ud835\udc62\ud835\udc56 ] + \ud835\udf06 \u2211\ufe01 \ud835\udc61 \u2208 I + \ud835\udc62 \u01772 \ud835\udc62\ud835\udc61 + \ud835\udf06 \u2211\ufe01 \ud835\udc5d \u2208 I\\I + \ud835\udc62 \u01772 \ud835\udc62\ud835\udc61 = \u2211\ufe01 \ud835\udc62 \u2211\ufe01 \ud835\udc56 \u2208 I + \ud835\udc62 [\ud835\udc50 \ud835\udc62 ( \u0177\ud835\udc62\ud835\udc56 -1) 2 + \ud835\udc50 \ud835\udc62 \u2211\ufe01 \ud835\udc5d \u2208 I\\I + \ud835\udc62 \u01772 \ud835\udc62\ud835\udc61 + \ud835\udf06(1 -\ud835\udc50 \ud835\udc62 ) \u2211\ufe01 \ud835\udc62 \u2211\ufe01 \ud835\udc56 \u2208 I + \ud835\udc62 \u01772 \ud835\udc62\ud835\udc56 + (\ud835\udf06 -\ud835\udc50 \ud835\udc62 ) \u2211\ufe01 \ud835\udc5d \u2208 I\\I + \ud835\udc62 \u01772 \ud835\udc62\ud835\udc61 = || \u221a\ufe01 \ud835\udc36 \ud835\udc62 (\ud835\udc4b -\ud835\udc4b\ud835\udc4a )|| 2 \ud835\udc39 -\ud835\udf06||\ud835\udc4b \u2299 \u221a\ufe01 \ud835\udc36 \ud835\udc62 -\ud835\udc3c\ud835\udc4b\ud835\udc4a || 2 \ud835\udc39 -||(1 -\ud835\udc4b ) \u2299 \u221a\ufe01 \ud835\udc36 \ud835\udc62 -\ud835\udf06\ud835\udc3c\ud835\udc4b\ud835\udc4a || 2 \ud835\udc39 Note that \u2211\ufe01 \ud835\udc62 \u2211\ufe01 \ud835\udc56 \u2208 I \u01772 \ud835\udc62\ud835\udc56 = ||\ud835\udc4b\ud835\udc4a || 2 \ud835\udc39 (25", "formula_coordinates": [6.0, 53.8, 131.44, 237.36, 189.65]}, {"formula_id": "formula_31", "formula_text": ")", "formula_coordinates": [6.0, 291.16, 304.71, 3.42, 4.09]}, {"formula_id": "formula_32", "formula_text": "L \ud835\udc37\ud835\udc52\ud835\udc4f\ud835\udc56\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc51 \ud835\udc5a\ud835\udc60\ud835\udc52 =|| \u221a\ufe01 \ud835\udc36 \ud835\udc62 (\ud835\udc4b -\ud835\udc4b\ud835\udc4a )|| 2 \ud835\udc39 -|| \u221a\ufe01 \ud835\udc36 \ud835\udc62 -\ud835\udc3c\ud835\udc4b\ud835\udc4a || 2 \ud835\udc39 = ||\ud835\udc4b -\ud835\udc4b\ud835\udc4a || 2 \ud835\udc39 -\ud835\udefc ||\ud835\udc4b\ud835\udc4a || 2 \ud835\udc39(26)", "formula_coordinates": [6.0, 81.53, 373.06, 213.05, 27.55]}, {"formula_id": "formula_33", "formula_text": "\ud835\udc4a = ((1 -\ud835\udf06)\ud835\udc4b \ud835\udc47 \ud835\udc4b ) -1 \ud835\udc4b \ud835\udc47 \ud835\udc4b .", "formula_coordinates": [6.0, 123.41, 434.92, 100.06, 10.43]}, {"formula_id": "formula_34", "formula_text": "L \ud835\udc37\ud835\udc52\ud835\udc4f\ud835\udc56\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc51 \ud835\udc52\ud835\udc4e\ud835\udc60\ud835\udc52 = ||\ud835\udc4b -\ud835\udc4b\ud835\udc4a || 2 \ud835\udc39 -\ud835\udefc ||\ud835\udc4b\ud835\udc4a || 2 \ud835\udc39 + \ud835\udf06||\ud835\udc4a || 2 \ud835\udc39 \ud835\udc60.\ud835\udc61 . \ud835\udc51\ud835\udc56\ud835\udc4e\ud835\udc54(\ud835\udc4a ) = 0", "formula_coordinates": [6.0, 85.2, 471.9, 175.66, 22.46]}, {"formula_id": "formula_35", "formula_text": "\ud835\udc4a = 1 1 -\ud835\udefc (\ud835\udc3c -P \u2022 \ud835\udc51\ud835\udc40\ud835\udc4e\ud835\udc61 ( \u00ec 1 \u2298 \ud835\udc51\ud835\udc56\ud835\udc4e\ud835\udc54( P)), \ud835\udc64\u210e\ud835\udc52\ud835\udc5f\ud835\udc52 P = (\ud835\udc4b \ud835\udc47 \ud835\udc4b + \ud835\udf06 1 -\ud835\udefc \ud835\udc3c ) -1 (27)", "formula_coordinates": [6.0, 93.81, 540.47, 200.78, 40.06]}, {"formula_id": "formula_36", "formula_text": "4", "formula_coordinates": [6.0, 317.96, 668.55, 3.22, 8.04]}, {"formula_id": "formula_37", "formula_text": "L \ud835\udc62 = || \u221a \ud835\udc50 \ud835\udc62 (\ud835\udc3b", "formula_coordinates": [12.0, 53.98, 418.9, 51.06, 14.16]}, {"formula_id": "formula_38", "formula_text": "\u00db \u2190 X \u22a4 X + \u039b + \u03a9 -1 X \u22a4 X \u2022 dMat(1 + \u03b2) + \u03a9 \u2022 dMat( \u03b2 -\u03b3) V V\u22a4 V -1 V\u22a4 \u2190 \u00db\u22a4 X \u22a4 X + \u039b + \u03a9 \u00db -1 \u00db\u22a4 X \u22a4 X \u2022 dMat(1 + \u03b2) + \u03a9 \u2022 dMat( \u03b2 -\u03b3) \u03b2 \u2190 diag X \u22a4 X \u00db V\u22a4 -diag X \u22a4 X + diag(\u03a9) \u2299 diag \u00db V\u22a4 + \u03b3 diag (X \u22a4 X) + diag(\u03a9 -\u039b) \u03b3 \u2190 \u03b3 + diag \u00db V\u22a4 - \u03b2(35)", "formula_coordinates": [13.0, 55.09, 97.82, 278.69, 99.74]}], "doi": "10.1145/1526709.1526711"}
