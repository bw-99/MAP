{
  "LTP-MMF: Towards Long-term Provider Max-min Fairness Under Recommendation Feedback Loops": "CHEN XU, Gaoling School of Artificial Intelligence, Renmin University of China xc_chen@ruc.edu.cn XIAOPENG YE, Gaoling School of Artificial Intelligence, Renmin University of China xpye@ruc.edu.cn JUN XU ‚àó , Gaoling School of Artificial Intelligence, Renmin University of China junxu@ruc.edu.cn XIAO ZHANG, Gaoling School of Artificial Intelligence, Renmin University of China zhangx89@ruc.edu.cn WEIRAN SHEN, Gaoling School of Artificial Intelligence, Renmin University of China shenweiran@ruc.edu.cn JI-RONG WEN, Gaoling School of Artificial Intelligence, Renmin University of China jrwen@ruc.edu.cn Multi-stakeholder recommender systems involve various roles, such as users, and providers. Previous work pointed out that max-min fairness (MMF) is a better metric to support weak providers. However, when considering MMF, the features or parameters of these roles vary over time, how to ensure long-term provider MMF has become a significant challenge. We observed that recommendation feedback loops (named RFL) will influence the provider MMF greatly in the long term. RFL means that recommender systems can only receive feedback on exposed items from users and update recommender models incrementally based on this feedback. When utilizing the feedback, the recommender model will regard the unexposed items as negative. In this way, the tail provider will not get the opportunity to be exposed, and its items will always be considered negative samples. Such phenomena will become more and more serious in RFL. To alleviate the problem, this paper proposes an online ranking model named Long-Term Provider Max-min Fairness (named LTP-MMF). Theoretical analysis shows that the long-term regret of LTP-MMF enjoys a sub-linear bound. Experimental results on three public recommendation benchmarks demonstrated that LTP-MMF can outperform the baselines in the long term. CCS Concepts: ¬∑ Information systems ‚Üí Recommender systems . Additional Key Words and Phrases: Max-min Fairness, Provider Fairness, Recommender System ‚àó Jun Xu is the corresponding author. Authors' addresses: Chen Xu, Gaoling School of Artificial Intelligence, Renmin University of China xc_chen@ruc.edu.cn; Xiaopeng Ye, Gaoling School of Artificial Intelligence, Renmin University of China xpye@ruc.edu.cn; Jun Xu, Gaoling School of Artificial Intelligence, Renmin University of China junxu@ruc.edu.cn; Xiao Zhang, Gaoling School of Artificial Intelligence, Renmin University of China zhangx89@ruc.edu.cn; Weiran Shen, Gaoling School of Artificial Intelligence, Renmin University of China shenweiran@ruc.edu.cn; Ji-Rong Wen, Gaoling School of Artificial Intelligence, Renmin University of China jrwen@ruc.edu.cn. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ¬© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM 1046-8188/2024/1-ART1 https://doi.org/10.1145/3695867 ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:2 Chen Xu, Xiaopeng Ye, Jun Xu, Xiao Zhang, Weiran Shen, and Ji-Rong Wen",
  "ACMReference Format:": "Chen Xu, Xiaopeng Ye, Jun Xu, Xiao Zhang, Weiran Shen, and Ji-Rong Wen. 2024. LTP-MMF: Towards Longterm Provider Max-min Fairness Under Recommendation Feedback Loops. ACM Trans. Inf. Syst. 1, 1, Article 1 (January 2024), 30 pages. https://doi.org/10.1145/3695867",
  "1 INTRODUCTION": "In multi-stakeholder recommender systems (RS), different roles are involved, including users, providers, recommender models, etc [2]. In recent years, provider fairness has received increasing attention, including provider demographic parity fairness [4, 22, 39, 48], proportion fairness [40, 58] and max-min fairness [19, 59]. Max-min fairness (MMF) aims to support worst-off providers and it is proposed based on distributive justice concept [31, 59]. Worse-off providers, who occupy the majority of the platform, cannot survive with the necessary support. Supporting the weak providers will increase the stability of the recommender market and make a good ecosystem [20, 43]. In this paper, we focused on the amortized max-min fairness [5, 9, 19, 59], where fairness accumulated across a series of rankings. In real scenarios, the components of recommender systems are not static. When conducting provider fairness, how to improve long-term performance under a time-changing environment is receiving increasing attention [21, 22, 38]. Previous work [21, 22, 38] aimed to conduct long-term fairness under time-vary content providers, item popularity, and provider features, respectively. Different from these studies, we observed that recommendation feedback loops (RFL) [14] will influence the provider max-min fairness greatly in the long term. As shown in Figure 1(a). The users and the RS are in RFL, where the model recommends a list of items to one user in each loop. Then, the user gives feedback, and the model will be updated incrementally according to the feedback. When updating the recommendation model, items without exposures are often viewed as negative samples (known as exposure/self-selection bias [14]). Next, we will conduct an empirical study to describe how the RFL influences max-min fairness performance. Figure 1(b) illustrates a simulation of how the unfairness increases in the repeated interactions between RS and users. We simulated 100 interactions between 512 users and an optimal fairness model, which can always obtain the optimal objective of the trade-off between predicting user preference and the max-min fairness objective. The preference estimation is updated using the users' feedback at the end of each interaction. Every point in Figure 1(b) represents the long-term lowest exposures among all providers (abbreviated as Lowest Exposures) during the entire interaction process, and different lines represent the simulation performance under different exposure sizes (i.e. ranking size). The larger the exposure size becomes, the small providers will get more exposure opportunities [41]. From Figure 1(b), one can easily observe that the magnitude of the exposure size (i.e. the impact of exposure bias) will significantly affect the long-term performance of the max-min fairness performance. The reason is that provider who has the lowest exposures will not get the opportunity to be exposed in such loop because his/her items will always be considered as negative samples. In RFL, providers with few exposures will get fewer and fewer opportunities to be exposed, intensifying the unfairness over time. Although existing studies [27, 49, 50] propose to utilize reinforcement learning frameworks to alleviate the exposure bias amplification, how to optimize the long-term objective under RFL subject to fairness constraints is still a challenging problem. In this paper, we propose an online ranking model named Optimizing Long-Term Provider Max-min Fairness (LTP-MMF) to address the issue. Intuitively, LTP-MMF exploits the objective while exploring the feedback of unexposed items. Specifically, LTP-MMF formulates the provider fair recommendation problem as a repeated resource allocation problem under batched bandit settings. Each item is considered a bandit's ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2024. LTP-MMF: Towards Long-term Provider Max-min Fairness Under Recommendation Feedback Loops 1:3 data fairness model users feedback items update loops (a) Feedback loops. 0 20 40 60 80 100 interactions 2000 2500 3000 3500 4000 4500 5000 5500 6000 worst-off exposure exposure size 5 exposure size 10 exposure size 15 exposure size 20 oracle (b) Simulation of feedback loops. Fig. 1. (a) The feedback loops of the interaction between the fairness model and users.(b)Simulations of the long-term lowest exposures among all provider (abbreviated as Lowest Exposures) arm. At each round, when a user comes to access the RS, LTP-MMF will choose ùëò items (i.e. arms) to generate a recommendation ranking list. For each arm, LTP-MMF first utilizes the Matrix Factorization (MF) model [26] to generate the predicted accuracy rewards from the side of users. Then, LTP-MMF will get provider exposures as the fairness reward in terms of the ranking list. Finally, LTP-MMF adds an exploration term toward the accuracy and fairness rewards by utilizing the Upper Confidence Bound (UCB) algorithm. Such exploration terms under fairness constraints help the model to access the feedback of unexposed items of small providers, improving the fairness performance in the long term. As for the parameters of predicting rewards on the user's side, it has a serious problem of long update time. Therefore, we first collect enough users' feedback and then update them incrementally in a batched style. Theoretical analysis shows that the regret of LTP-MMF enjoys a sub-linear bound and the batch size is a trade-off coefficient between accuracy and fairness in the long term. We summarize the major contributions of this paper as follows: (1) In this paper, we analyze the importance of the long-term provider max-min fairness when considering the recommendation feedback loops; (2) We formulate the long-term provider fair recommendation problem as a repeated resource allocation problem under a batched bandit setting and a ranking model called LTP-MMF is proposed. Theoretical analysis shows that the regret of LTP-MMF can be bounded; (3) Extensive experiments on three public datasets demonstrated that LTP-MMF steadily outperforms the baselines in the long term. Moreover, we verified that LTP-MMF is computationally efficient in the online inference phase.",
  "2 RELATED WORK": "The fairness problem in multi-stakeholder recommender systems has become a hot research topic [2, 3]. According to different stakeholders, the fairness problem can be divided into customer fairness (C-fairness) and provider fairness (P-fairness) [13, 40]. In a recent publication, the stream of provider fairness is defined as ensuring that the item exposures of each provider are relatively similar to each other [18, 23, 29, 40, 43, 44, 58, 59, 61]. In generative recommender systems, provider fairness can also entail generating non-discriminatory item content for providers based on large language models [16, 34]. In this paper, our primary focus lies on mainstream provider fairness, which emphasizes maintaining as much equality as possible in the item exposures across different providers. ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:4 Chen Xu, Xiaopeng Ye, Jun Xu, Xiao Zhang, Weiran Shen, and Ji-Rong Wen Whendealing with provider fairness, most methods are conducted under re-ranking scenarios [19, 23, 29, 40, 43, 58, 59, 61]. For example, FairRec [43] and its extension FairRec+ [12] proposed an offline recommender model to guarantee equal frequency for all items in a series of ranking lists. Welf [19] proposed the Frank-Welf algorithm to solve the provider fair problem. Some work [7, 23, 29, 40, 58] proposed a Linear Programming (LP)-based method to ensure the group fairness. P-MMF [59], FairSync [61] proposed an online mirror gradient descent to improve the worst-off provider's exposures in the dual space. In this paper, our main objective is to mitigate the influence of feedback loops in mirror gradient descent approaches [59, 61], thereby achieving long-term provider fairness. Another line of research proposed that the recommendation system is dynamic, the attributes, features, and parameters can change as time goes on [14, 21, 22, 38]. Previous studies aimed to improve the long-term performance of fairness. Ge et al. [22] focused on the time-vary recommendation attributes and formulated the long-term problem as a Constrained Markov Decision Process. Mladenov et al. [38] proposed that content providers may leave if they cannot obtain enough support and formulated the problem as a constrained matching problem. Akpinar et al. [4] studied the provider population distributions that will change in the long term and proposed an intervention adjustment method. There are also some models [10, 37] proposed to utilize reinforcement learning techniques to optimize long-term fairness. However, these long-term-based fairness methods did not consider the impact of recommendation feedback loop (i.e. exposure bias) in the long term. In recent years, many debiasing approaches [35, 63] are proposed to break the feedback loop. For example, [63] proposed to utilize the influence function to remedy the bias from the loop, and Liu et al. [35] proposed to utilize uniform data to debias from the loop. At the same time, there are also some reinforcement learning methods to break the loop. Exploitation and exploration in bandit settings were proposed to solve the problem. Li et al. [32], Wang et al. [52, 56] all utilized Upper Confidence Bound (UCB) to explore the unexposed items. Other models [55, 64] also utilized policy gradient methods to break the feedback loop. However, they do not consider provider max-min fairness constraints in the feedback loop. In the causal inference literature [24, 33, 47, 60], some methods try to alleviate exposure bias in RS. For example, some work [33, 47] suggested utilizing user-social networks to mitigate exposure bias, as users who are closely connected are more likely to share the same items with each other. Some work [24] proposed to utilize propensity score to mitigate exposure bias. DEPS [60] considers the user-social network and item side propensity score together. However, these methods do not account for feedback loop scenarios and cannot be applied to fairness-aware RS.",
  "3 FORMULATION": "In this section, we first define the notations in multi-stakeholder recommender systems. Then, we give the formal definition of amortized max-min fairness accordingly. Finally, we formulate the interactions between users and the recommender model as a bandit problem.",
  "3.1 Multi-Stakeholders Recommender Systems": "A multi-stakeholder recommender system consists of different participants, including users, item providers, etc. Let U , I , and P be the set of users, items, and providers, respectively. Each item ùëñ ‚àà I is associated with a unique provider ùëù ‚àà P . The set of items associated with a specific provider ùëù is denoted by I ùëù . In the ranking phase, when a specific user ùë¢ ‚àà U accesses the recommender system, for each user-item pair ( ùë¢, ùëñ ) , the click rate ùë† ùë¢,ùëñ = ùëÉ ( ùëê ùë¢,ùëñ = 1 ) of the user ùë¢ on the item ùëñ is estimated through a ranking model based on the user's historical click feedback. We denote such feedback by ùëê ùë¢,ùëñ ‚àà { 0 , 1 } , which represents whether or not the user clicked the item. The vector ùíî ùë¢ = [ ùë† ùë¢, 1 , ùë† ùë¢, 2 , ¬∑ ¬∑ ¬∑ , ùë† ùë¢, | I | ] ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2024. LTP-MMF: Towards Long-term Provider Max-min Fairness Under Recommendation Feedback Loops 1:5 Fig. 2. Sequential item ranking process of LTP-MMF u 1 u 2 accuracy module ùùÖùùÖ ‚Ä¶ u T users arrive sequentially fairness module ùùÖùùÖ ùë≠ùë≠ predicted accuracy reward ùêãùêãK F (u ) log data user feedback model updating every T users update ùùÖùùÖ every T users UCB module exploration term predicted accuracy-fairness reward recommendaion list ‚Ä¶ is the click rates of ùë¢ to all items. In the ranking phase, the items are ranked according to the estimated click rates. In provider-fair recommendation, we should consider the user-side utilities and provider-side fairness together. Formally, we define the true user-side utility of exposing item list ùêø ùêæ ( ùë¢ ) to ùë¢ as the summation of the preference scores in the list, denoted by ùëî ( ùêø ùêæ ( ùë¢ )) = Àù ùëñ ‚àà ùêø ùêæ ( ùë¢ ) ùë† ùë¢,ùëñ , where ùêø ùêæ ( ùë¢ ) contains the ùêæ items exposed to ùë¢ . Following the literature convention [40, 43, 59], we define the fairness vector as e , for a specific provider ùëù , e ùëù ‚àà R + denotes the exposure of provider ùëù . In this paper, we mainly focus on provider max-min fairness [8, 19, 59], which aims to improve the exposure opportunities of worst-off providers. Formally, ùëü ( ùíÜ ) = min ùëù ‚àà| P| ( ùíÜ / ùõæ ) , where ùú∏ is the weighting vector of providers. In this paper, we aim to compute a fair list ùêø ùêπ ùêæ ( ùë¢ ) ‚àà I ùêæ , which well balances the user utilities ùëî (¬∑) and provider fairness metric ùëü ( ùíÜ ) under RFL in the long-term.",
  "3.2 Amortized Provider Max-Min Fairness": "In real-world applications, the users arrive at the recommender system sequentially. Assume that at time ùë° user ùë¢ ùë° arrives. The recommender system needs to consider long-term provider exposure during the entire time horizon from ùë° = 0 to ùëá . Our task can be formulated as a resource allocation problem with amortized fairness [5, 59]. Specifically, the optimal utility of the recommender system can be defined as an amortized fairness function [5, 9, 59], which is the accumulated exposures over periods from 0 to ùëá . In this case, e ùëù can be seen as the total number of exposed items of provider ùëù , accumulated over the period 0 to ùëá . ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:6 Chen Xu, Xiaopeng Ye, Jun Xu, Xiao Zhang, Weiran Shen, and Ji-Rong Wen Formally, when trading off the user utilities and provider fairness, we have the following mathematical program:  where ùú∏ ‚àà R | P | denotes the weights of different providers, i.e., weighted MMF [8, 59]. In amortized fairness, ùíÜ ‚àà R ùëá √ó|P| is the exposure vector, i.e., ùíÜ ùë°,ùëù denotes the utility of provider ùëù at time ùë° . x ùë° ‚àà { 0 , 1 } | I | is the decision vector for user ùë¢ ùë° . Specifically, for each item ùëñ , x ùë°ùëñ = 1 if it is in the fair ranking list ùêø ùêπ ùêæ ( ùë¢ ùë° ) , otherwise, x ùë°ùëñ = 0. The max-min fairness regularizer min (¬∑) in the objective function suggests that we should improve the exposure opportunities of worst-off providers. The first constraint in Equation (1) ensures that the recommended lists are of size ùêæ . The second constraint in Equation 1 suggests that the exposures of each provider ùëù are the accumulated exposures of the corresponding items over all periods. In general, we think time-separable fairness would be preferred under scenarios with weak timeliness. For example, recommending items with long service life (e.g., games and clothes, etc.). Although here we have already given a linear programming solution Eq.(1) to the problem, it can only solve small-scale problems in an offline way. In online recommendation systems, for each user ùë¢ ùë° access, the model needs to generate a fair ranking list ùêø ùêπ ùêæ ( ùë¢ ùë° ) from large-scale item corpus immediately. This means we have no idea about the information after ùë° . Next, we will discuss how to use MMF in the online recommendation problem.",
  "3.3 Bandit with Provider Fairness": "We first define some notations for the problem. For any symmetric matrix ùë® ‚àà R ùëÄ √ó ùëÄ and vector ùíô ‚àà R ùëÄ , let ùíô ùëñ denote the ùëñ -th element of the vector and ùë® ùëñ denote the ùëñ -th column of the matrix ùë® . Define ‚à• ùíô ‚à• ùë® = ‚àö ùíô ‚ä§ ùë®ùíô . We also define the weighted ‚Ñì 2 norm to be ‚à• ùíô ‚à• 2 ùíö 2 = Àù ùëÄ ùëñ = 1 ùíô 2 ùëñ ùíö 2 ùëñ . To help the fairness model break the feedback loop, we formulate the provider-fair problem as a context bandit process [53]. As shown in Figure 2, at each iteration, a batch of users { ùë¢ ùë° } ùëá ùë° = 1 arrive sequentially. For each user ùë¢ ùë° and each item ùëñ , the accuracy module ùúã takes user features and item context as input and predicts the accuracy reward ÀÜ ùë† ùë¢ ùë° ,ùëñ , ‚àÄ ùëñ ‚àà I . Then the fairness module ùúã ùêπ takes the accuracy reward as input and generates a predicted accuracy-fairness reward according to the provider exposures. Finally, the exploration module gives the exploration term according to previous recommendations to explore unexposed items. After getting the reward of each bandit arm (i.e., item), LTP-MMF chooses ùêæ items with the highest rewards to generate an item list ùêø ùêπ ùêæ ( ùë¢ ùë° ) . Then the user gives their feedback { ùëê ùë¢ ùë° ,ùëñ , ‚àÄ ùëñ ‚àà ùêø ùêπ ùêæ ( ùë¢ ùë° )} , which is stored in the log. When a batch of users finishes their access, the recommendation results ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2024. LTP-MMF: Towards Long-term Provider Max-min Fairness Under Recommendation Feedback Loops 1:7 and rewards are collected. In this paper, we propose to formulate such a process as a batched bandit which can be represented by a 7-tuple < S , L , ùúã, ùúã ùêπ , ùëÖ, ùëÅ,ùëá > : Context space S : denotes a hidden context space that summarizes the embedding space of both users and items. Action space L : denotes a given action space and each action corresponds to selecting ùêæ items (arms): ùêø ùêπ ùêæ ( ùë¢ ) ‚àà L . Accuracy module ùúã : takes user and item context as input and predicts the accuracy reward if the user is recommended with this item, i.e., ÀÜ ùë† ùë¢,ùëñ = ùúã ( ùë¢, ùëñ ) . Fairness module ùúã ùêπ : An online algorithm ùúã ùêπ produces a real-time decision vector x ùë° ‚àà { 0 , 1 } | I | based on the current user ùë¢ ùë° and the history H ùë° -1 = { ùë¢ ùë† , x ùë† } ùë° -1 ùë† = 1 . ùúã ùêπ takes the estimated user-item preference score and item-provider relations as input and outputs an accuracy-fairness reward if the user is recommended with this item, i.e. ÀÜ ùëì ùë¢,ùëñ = ùúã ùêπ ( ùë¢, ÀÜ ùë† ùë¢,ùëñ , H ùë° -1 ) . Reward ùëÖ : is defined as a linear combination of the two types of feedback defined in Eq. (1): ùëÖ = 1 ùëá ùëî ( ùíô ùë° ) + ùúÜùëü ( ùíÜ ) . Total data number ùëÅ : the process of a batched fairness-aware bandit is partitioned into ‚åä ùëÅ / ùëá ‚åã episodes. Within each episode, the platform first updates the ranking policy ùúã using the collected user feedback and then applies the re-ranking policy ùúã ùêπ with users for time horizon ùëá using the updated ranking policy ùúã . Batch size ùëá : is the number of steps (time horizons) in each episode. That is, in each episode, the platform makes recommendations for each user and collects log data B = { ùíô ùë° , ùëê ùë¢ ùë° ,ùëñ , ùëñ ‚àà ùêø ùêπ ùêæ ( ùë¢ ùë° )} , ùíÜ ùë° } ùëá ùë° = 1 . Finally, a new accuracy predicted ùúã is trained on B at the beginning of the next episode. Within each batch, the estimated reward is defined as ÀÜ ùëÖ = 1 ùëá ÀÜ ùëî ( ùíô ùë° ) + ùúÜùëü ( ùíÜ ) , where ÀÜ ùëî (¬∑) , ùëü (¬∑) are based on the estimated user-item score ÀÜ ùë† . The long-term regret of the LTP-MMF is defined as the expectation over all log data B :",
  "4 OUR APPROACH: LTP-MMF": "In this section, we propose a novel recommender model which we call LTP-MMF. The model aims to optimize the long-term accuracy-fairness trade-off in recommendation loops. The whole procedure can be formulated as a batched bandit process in Figure 2.",
  "4.1 Overall Architectures": "In this section, we will introduce the overall relations between the bandit settings discussed in Section 3.3. In the bandit problem, the overall reward ùëÖ will be divided into three parts. In the following three sections, we employ three modules-the Accuracy Module, the Fairness Module, and the UCB module-to compute three distinct parts of the reward, respectively.",
  "4.2 Accuracy Module: Matrix Factorization": "For each ( ùë¢, ùëñ ) , we can define their hidden embeddings as ùíó ùë¢ , ùíó ùëñ ‚àà R ùëë , where ùëë is the pre-defined dimension. The matrix factorization model takes two embeddings to determine the estimated preference score ÀÜ ùë† ùë¢,ùëñ :  where the random noise ùúñ is drawn from a zero-mean Gaussian distribution N( 0 , ùúé 2 ) . The MF [30] model uses a coordinate descent algorithm built upon the ridge regression to ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:8 Chen Xu, Xiaopeng Ye, Jun Xu, Xiao Zhang, Weiran Shen, and Ji-Rong Wen estimate the unknown parameter ùíó ùë¢ for each user and the unknown hidden feature ùíó ùëñ for each item. Specifically, the objective function of the ridge regression can be written as follows  where where ùúÜ ùë¢ and ùúÜ ùëñ are the trade-off parameters for the ‚Ñì 2 regularization. In real-world applications, the users arrive at the recommender system sequentially. Assume that at time ùë° user ùë¢ ùë° arrives, the recommender model recommendes list ùêø ùêæ ( ùë¢ ùë° ) to ùë¢ ùë° based on estimated scores and receives the user click feedback { ùëê ùë¢ ùë° ,ùëñ , ùëñ ‚àà ùêø ùë° ùêæ ( ùë¢ )} . The closed-form estimation of ùíó ùëñ , ùíó ùë¢ at time ùë° can be derived as ÀÜ ùíó ùë¢,ùë° = ( ùë® ùë¢,ùë° ) -1 ùíÉ ùë¢,ùë° and ÀÜ ùíó ùëñ,ùë° = ( ùë™ ùëñ,ùë° ) -1 ùíÖ ùëñ,ùë° , where",
  "4.3 Fairness Module": "In this section, we will take the most state-of-the-art dual gradient descent method P-MMF [59] as our fairness module. It is important to note that our methods can be readily applied to various score-based re-ranking methods [5, 40, 61], as our model LTP-MMF can easily substitute the fairness reward of the fairness module with other fairness rewards. we aim to describe how to trade off user accuracy and provider fairness in an online fashion. Given the ranking scores from accuracy reward ùë† ùë¢,ùëñ , we can consider its dual problem: Theorem 1 (Dual Problem). The dual problem of Equation (1) can be written as:  where ùë¥ ‚àà R | I | √ó | P | is the item-provider adjacency matrix with ùëÄ ùëñùëù = 1 indicating item ùëñ ‚àà I ùëù , ùëî ‚àó (¬∑) , ùëü ‚àó (¬∑) the conjugate functions:  with X = { x ùë° | x ùë° ‚àà { 0 , 1 } ‚àß Àù ùëñ ‚ààI x ùë°ùëñ = ùêæ } , and D = { ùùÅ | ùëü ‚àó (-ùùÅ ) < ‚àû} is the feasible region of dual variable ùùÅ . Moreover, the feasible region of the dual problem is  Ô£≥ Ô£æ where P ùë† is the power set of P , i.e., the set of all the subsets of P . Lemma 1. The conjugate function ùëü ‚àó (¬∑) has a closed form:  and the optimal dual variable is:  ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2024. LTP-MMF: Towards Long-term Provider Max-min Fairness Under Recommendation Feedback Loops 1:9 Remark 1. Following the practice in [59], we also try to optimize the integral linear programming from the dual space. It involves a vast variable ùíô 's space of size ùëá √ó |I| to dual variable ùúá 's size |P| ‚â™ ùëá √ó |I| , and thanks to the sparsity of M , the computation of M ùùÅ is highly efficient. Note that any other dual transformation way [5, 61] can be also applied in LTP-MMF. The proofs of Theorem 1 and Lemma 1 are provided in Appendix A.1. From Theorem 1, we can have a new non-integral decision variable ùùÅ ‚àà R | P | . In practice, we usually have |P| ‚â™ |I| . Besides, due to ùë¥ 's sparsity, it is very efficient to compute ùë¥ùùÅ , which aims to project the variable ùùÅ from the provider space onto the item spaces. According to Theorem 1, the accuracy-fairness reward of a user-item pair is obtained through the conjunction function ùëî ‚àó ( ùë¥ùùÅ ) : ÀÜ ùëì ùë¢,ùëñ = ÀÜ ùë† ùë¢,ùëñ -ùë¥ ‚ä§ ùëñ ùùÅ . For each time step ùë° in a batch, we utilize the mirror momentum gradient descent [5, 45] to learn the dual variable ùùÅ ùë° : first, we evaluate the conjugate function of the max-min regularizer ùëü ‚àó (-ùùÅ ùë° ) according to Lemma 1. Then we can get the subgradient of the estimated dual function as  Finally, we utilize g ùë° = ùë¥ x ùë° + ùíÜ ùë° to update the dual variable by performing the online descent. Therefore, the dual variable will move towards the directions of the providers with fewer exposures, and the primal variable x ùë° will move to a better solution. Note that the projection step can be efficiently solved using QP solvers [5] since D is coordinatewisely symmetric.  where e ùùÅ satisfies:",
  "4.4 UCB for Accuracy-fairness Reward": "Upper Confidence Bound (UCB) [53, 54] has proven to be an effective strategy to estimate the confidence of predicted rewards during exploration. In fairness-aware recommendations, the system not only should exploit the accuracy-fairness objective in the ranking process but also should explore less exposed items to avoid giving too few exposures to some providers. Next, we will bound the corresponding upper confidence bounds of the predicted accuracyfairness reward ÀÜ ùëì ùë¢,ùëñ . Theorem 2 (Confidence Radius of Ranking). The parameter ùíó ùë¢ , ùíó ùëñ is ùëû -linear to the optimizer. For any ùúé > 0 , with probability at least 1 -ùúé , the confidence radius ‚ñ≥ ùëì ùë° ùë¢,ùëñ of user-item preference score at time ùë° satisfies:  We can decompose the confidence radius into three terms: ¬∑ ùíó ùë¢ and ùíó ùëñ bias terms ‚à• ÀÜ ùíó ùë¢,ùë° -ùë£ ‚àó ùë¢ ‚à• ùë® ùë¢,ùë° , ‚à• ÀÜ ùíó ùëñ,ùë° -ùë£ ‚àó ùëñ ‚à• ùë™ ùëñ,ùë° and their upper bound at time ùë° are denoted by ùõº ùë° , ùõΩ ùëñ,ùë° , respectively. ¬∑ ùíó ùë¢ and ùíó ùëñ variance terms ‚à• ÀÜ ùíó ùë¢,ùë° ‚à• ùë® -1 ùë¢,ùë° , ‚à• ÀÜ ùíó ùëñ,ùë° ‚à• ùë™ -1 ùëñ,ùë° . ¬∑ collaborative variance terms ‚à• ÀÜ ùíó ùë¢,ùë° -ùë£ ‚àó ùë¢ ‚à• ùë® -1 ùë¢,ùë° , ‚à• ÀÜ ùíó ùëñ,ùë° -ùë£ ‚àó ùëñ ‚à• ùë™ -1 ùëñ,ùë° and their upper bound at time ùë° are defined as ùê∂ ùë° = ( ùëû + ùúñ ùëû ) ùë° , where for any ùúñ ùëû > 0 and ùíó ùë¢ , ùíó ùëñ are ùëû -linear ( 0 < ùëû < 1 to the optimizer. ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:10 Chen Xu, Xiaopeng Ye, Jun Xu, Xiao Zhang, Weiran Shen, and Ji-Rong Wen",
  "Algorithm 1: Online learning of LTP-MMF": "Input: User arriving order { ùë¢ ùëñ } ùëÅ ùëñ = 1 , batch size ùëá , ranking size ùêæ . Ranking parameters: User embedding { ùíó ùë¢ ‚àà R ùëë , ‚àÄ ùë¢ ‚àà U} , item embedding { ùíó ùëñ ‚àà R ùëë , ‚àÄ ùëñ ‚àà I} . Confidence level ùúé , and semi-positive matrix ùë® ùë¢ = ùúÜ ùë¢ ùë∞ , ùíÉ ùë¢ = 0 , ùë™ ùëñ = ùúÜ ùëñ ùë∞ , ùíÖ ùëñ = 0 . Re-ranking parameters: Item-provider adjacent matrix M , maximum resources ùú∏ and the coefficient ùúÜ . Output: The decision variables { x ùëñ , ùëñ = 1 , 2 , . . . , ùëÅ } . 1: for ùëõ = 1 , ¬∑ ¬∑ ¬∑ , ‚åä ùëÅ / ùëá ‚åã do 2: Initialize dual solution ùùÅ 1 = 0, remaining resources ùú∑ 1 = ùú∏ , and momentum gradient g 0 = 0, training buffer B = ‚àÖ . 3: for ùë° = 1 , ¬∑ ¬∑ ¬∑ , ùëá do 4: User ùë¢ ùëõùëá + ùë° arrives, we abbreviate ùë¢ ùëõùëá + ùë° as ùë¢ . 5: fairness model->users edge in Figure 1(a): lines 6-11. 6: Compute estimated score ùë† ùë¢,ùëñ = ùíó ‚ä§ ùë¢ ùíó ùëñ , ‚àÄ ùëñ ‚àà I . 7: Compute the UCB ‚ñ≥ ùíî following Eq. (10): ‚ñ≥ ùëì ùëõ ùë¢,ùëñ = ùõº ùëõ (‚à• ùë£ ùëñ ‚à• ùë® -1 ùë¢ + ùê∂ / 2 ) + ùõΩ ùëõ (‚à• ùë£ ùë¢ ‚à• ùë™ -1 ùëñ + ùê∂ / 2 ) . 8: m ùëù = ( 1 -ùë∞ ( ùú∑ ùë°ùëù > 0 ) ‚àó 1000 . 0 ) 9: // Rewards for exploit-explore trade-off: 10: ùëü ùë¢,ùëñ = ÀÜ ùë† ùë¢,ùëñ / ùëá -M ‚ä§ ùëñ ( ùùÅ ùë° + m ) + ‚ñ≥ ùëì ùëõ ùë¢,ùëñ 11: yields output variable ùíô 12: x ùë° = arg max x ùë° ‚ààX GLYPH<2> ùíì ‚ä§ ùë¢ ùíô ùë° GLYPH<3> 13: users->data edge in Figure 1(a): lines 13-20. 14: B = B ‚à™ {( ùë¢, ùëñ, ùëê ùë¢,ùëñ ) // Receive user's feedback: 15: // Update the remaining resources: 16: ùú∑ ùë° + 1 = ùú∑ ùë° -ùë¥ ‚ä§ x ùë° , e ùë° = arg max et ‚â§ ùú∑ ùë° ùëü ‚àó (-ùùÅ t ) 18: g ùë° = ùõº e g ùë° + ( 1 -ùõº ) g ùë° -1 17: e g ùë° = -M ‚ä§ x ùë° + e ùë° , 19: // Update the dual variable by gradient descent ùùÅ h i 20: 2 ùë° + 1 = arg min ùùÅ ‚ààD ‚ü® g ùë° , ùùÅ ‚ü© + ùúÇ ‚à• ùùÅ - ùùÅ ùë° ‚à• ùú∏ 2 21: end for 22: data->fairness model edge in Figure 1(a) lines 22-27. 23: for ( ùë¢, ùëñ, ùëê ùë¢,ùëñ ) ‚àà B do 24: // Update accuracy module: 25: ùë® ùë¢ = ùë® ùë¢ + ùíó ùëñ ùíó ‚ä§ ùëñ , ùë™ ùëñ = ùë™ ùëñ + ùíó ùë¢ ùíó ‚ä§ ùë¢ 26: ùíó ùë¢ = ùë® -1 ùë¢ ùíÉ ùë¢ , ùíó ùë¢ = ùíó ùë¢ /‚à• ùíó ùë¢ ‚à• 2 27: ùíÉ ùë¢ = ùíÉ ùë¢ + ùíó ùëñ ùëê ùë¢,ùëñ , ùíÖ ùëñ = ùíÖ ùëñ + ùíó ùë¢ ùëê ùë¢,ùëñ 28: ùíó ùëñ = ùë™ -1 ùëñ ùíÖ ùëñ , ùíó ùëñ = ùíó ùëñ /‚à• ùíó ùëñ ‚à• 2 29: end for 30: end for The bias bound (i.e. exploration weight) ùõº ùë¢ , ùõº ùëñ are defined as    ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2024. LTP-MMF: Towards Long-term Provider Max-min Fairness Under Recommendation Feedback Loops 1:11 The confidence radius can be bounded by ùëÇ GLYPH<18> [ 1 -( ùëû + ùúñ ùëü ) ùë° ] ‚àö ln ( ùë° ) ‚àö ùë° GLYPH<19> , which is a sub-linear decreasing ‚àöÔ∏Å ‚àö function of t when ùë° becomes large. ùëÇ (( 1 - ( ùëû + ùúñ ùëü ) ùë° ) ln ( ùë° )) is the bias growth rate and ùëÇ ( 1 / ùë° ) is the variance converge rate. Remark 2. From Theorem 2, we can see the exploration term is ‚ñ≥ ùëì ùë° ùë¢,ùëñ = ùõº ùë° (‚à• ÀÜ ùíó ùëñ,ùë° ‚à• ùë® -1 ùë¢,ùë° + ùê∂ ùë° / 2 ) + ùõΩ ùë° (‚à• ÀÜ ùíó ùë¢,ùë° ‚à• ùë™ -1 ùëñ,ùë° + ùê∂ ùë° / 2 ) . The exploration term has three parts: ¬∑ Bias terms ‚à• ÀÜ ùíó ùë¢,ùë° -ùë£ ‚àó ùë¢ ‚à• ùë® ùë¢,ùë° , ‚à• ÀÜ ùíó ùëñ,ùë° -ùë£ ‚àó ùëñ ‚à• ùë™ ùëñ,ùë° is bounded by ùõº ùë° , ùõΩ ùë° , which captures the user preference shift with fairness. Intuitively, the presence of more shifts indicates that previous estimations may not be accurate, thus resulting in a larger degree of exploration. ¬∑ Variance terms ‚à• ÀÜ ùíó ùë¢,ùë° ‚à• ùë® -1 ùë¢,ùë° , ‚à• ÀÜ ùíó ùëñ,ùë° ‚à• ùë™ -1 ùëñ,ùë° captures the variance of user and item embedding estimation. Intuitively, the presence of more variance indicates that previous estimations may not be accurate, thus resulting in a larger degree of exploration. ¬∑ Similarly, the collaborate variance term ùê∂ ùë° captures the variance of user-item interaction. Intuitively, the presence of more variance indicates that previous estimations may not be accurate, thus resulting in a larger degree of exploration. Those three parts together form the exploration term to balance exploration in fairness constraints with accuracy. The proof of theorem 2 is deferred to Appendix A.3. Equation 10 measures the estimation uncertainty of parameter ùíó ùë¢ , ùíó ùëñ in ranking module. The exploration and exploitation trade-off is balanced by the prediction confidence bound of both the user side and the item side. From Theorem 2, we can also observe that the confidence radius is a trade-off between the bias term and the variance term. With more observations, the bias will increase but the variance will decrease more rapidly so that the radius bound will become smaller (i.e. the bound is a sub-linear decreasing function of t when ùë° becomes large).",
  "4.5 Our Algorithm: LTP-MMF": "In each iteration of the algorithm, we first conduct online recommendations of user accuracy and provider max-min fairness. with fixed ranking model for a batch of users { ùë¢ ùë° } ùëá ùë° = 1 . Then we will collect these users' feedback on the recommended items. Finally, the accuracy module parameters are updated using the collected users' feedback. Algorithm 1 illustrates our main algorithm. In lines 6-11, the algorithm describes the process of fair-aware RS model giving recommendation results to users (fairness model->user edge in Figure 1). Specifically, when user ùë¢ comes to the recommender system, in line 6, LTP-MMF first estimates the accuracy score (accuracy Module). Then in line 7, the LTP-MMF computes the UCB bound (UCB module). in line 10, we will also add the final score with the fairness term (fairness module). These three modules together form the total reward: (1) For the error from the ranking model, we modify the ranking score according to its upper confidence bound. (2) For the max-min fairness objective, we apply the primal-dual theory to adjust the score with the dual variable ùùÅ ùë° . Intuitively, for ùùÅ ùë° , when the values of dual variables are higher, the algorithm naturally recommends fewer items related to the corresponding provider. (3) ùíé ensures that the algorithm only recommends items from providers with remaining resources. Note that in line 9, the formulation is linear with respect to x ùë° . Therefore, it is efficient to compute x ùë° through a topùêæ sort algorithm in constant time. Finally, in line 12, we will adapt the top-K sorting operation to get the final output variable ùíô according to the reward combined with these three components. In line 14, the algorithm outlines the procedure by which users generate data for model training, ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:12 Chen Xu, Xiaopeng Ye, Jun Xu, Xiao Zhang, Weiran Shen, and Ji-Rong Wen specifically focusing on the user->data edge as depicted in Figure 1. In lines 15-28, the algorithm outlines the process of model updating (data->fairness model edge in Figure 1). In lines 17-21, we aim to update the parameters of the fairness module utilizing the dual mirror descent algorithm. In lines 23-29, we utilize the user behavior data to update the parameters of the accuracy module. Theorem 3 (Regret Bound). Assume that the function ‚à• ¬∑ ‚à• 2 ùú∏ 2 is ùúé -strong convex and there exists a constant ùê∫, ùêø ‚àà R + such that ‚à• e ùëî ùë° ‚à• < ùê∫, ‚à• ùùÅ ‚à• ùõæ ‚â§ ùêø . Then, the regret can be bounded as follows:  where the regret bound of fairness module Regret ( ùúã ùêπ ) is the regret of the dual fairness model, which can be bounded as  where ¬Ø ùëü is the upper bound of MMF regularzier, and in practice, ¬Ø ùëü ‚â§ 1 . The regret bound of accuracy module Regret ( ùúã ) can be bounded as  where    Remark 3 (Accuracy-fairness regret trade-off). Setting the learning rate as ùúÇ = ùëÇ ( ùëá -1 / 2 ) , we can obtain a fairness regret Regret( ùúã ùêπ ) upper bound of order ùëÇ ( ùëÅ / ‚àö ùëá ) . The accuracy regret Regret( ùúã ) is ùëÇ ( ‚àöÔ∏É ùëÅùëá ln ( ùëÅ ùëá )) . The larger batch size ùëá , the less error raised of accuracy in the long term, but the more bias caused by the fairness module ùúã ùêπ . Remark 4 (Sublinear Long-term Regret). Overall, the long-term regret of LTP-MMF can be obtained from order ùëÇ ( ùëÅ ln ùëÅ ) . Moreover, we can observe that the more we tend to consider fairness, the larger ùêø will become, leading to larger regret in the long term.",
  "4.6 Discussion": "We will summarize the key contribution of our method as follows. Firstly, our chosen accuracy module is the widely represented two-tower model of recommender systems [62], which utilizes the dot product between two learned user item embeddings. However, it can be readily replaced with any other two-tower recommender system models [25, 46, 51] by simply substituting the learned embeddings for the user-item embeddings ùë£ ùëñ and ùë£ ùë¢ . Secondly, as mentioned in the Fairness Module, the fairness module can be also replaced by any score-based re-ranking methods [5, 40, 61]. Secondly, Our UCB exploration term exhibits wide adaptability, and our theoretical analysis also provides guidance for the application of other two-tower and score-based fairness models. Our method uniquely integrates the UCB technique to effectively balance accuracy estimation and fairness optimization within the feedback loop scenarios of RS, offering the literature a notable example of addressing the accuracy-fairness trade-off in such long-term contexts. ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2024. LTP-MMF: Towards Long-term Provider Max-min Fairness Under Recommendation Feedback Loops 1:13 Table 1. Statistics of the datasets.",
  "5 EXPERIMENT": "We conducted experiments to show the effectiveness of the proposed LTP-MMF in the long term. The source code and experiments have been shared at github https://github.com/XuChen0427/LTPMMF.",
  "5.1 Experimental settings": "5.1.1 Datasets. The experiments were conducted on four large-scale, publicly available recommendation datasets, including: Yelp 1 : a large-scale businesses recommendation dataset. We only utilized the clicked data, which is simulated as the 4-5 star rating samples. Amazon-Beauty/Amazon-Baby : Two subsets (beauty and digital music domains) of Amazon Product dataset 2 . We only utilized the clicked data, which is simulated as the 4-5 star rating samples. Also, the brands are considered as providers. Steam 3 [28]: We used the data for gamed played for more than 10 hours in our experiments. The publishers of games are considered as providers. As a pre-processing step, the users, items, and providers who interacted with less than 5 items/users were removed from all datasets to avoid the extremely sparse cases. We removed providers associated with fewer than 5 items, which also included providers lacking a brand name, resulting in the removal of their associated items as well. We will give a more detailed provider-removing ratio in Table 1. Table 1 lists some statistics of the four datasets. 5.1.2 Evaluation. We sorted all the interactions according to the time and used the first 80% of the interactions as the training data to train the base model (i.e., BPR [46]). The remaining 20% of interactions were used as the test data for evaluation. Based on the trained base model, we can obtain a preference score ùë† ùë¢,ùëñ for each user-item pair ( ùë¢, ùëñ ) . The chronological interactions in the test data were split into interaction sequences where the horizon length was set to ùëá . We calculated the metrics separately for each sequence, and the averaged results are reported as the final performances. As for the evaluation metrics, the performances of the models were evaluated from three aspects: user-side preference, provider-side fairness, and the trade-off between them. As for the user-side accuracy, following the practices in [58], we utilized the CTR@K, which measures the averaged click rate in the long term:  1 https://www.yelp.com/dataset 2 http://jmcauley.ucsd.edu/data/amazon/ 3 http://cseweb.ucsd.edu/~wckang/Steam_games.json.gz ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:14 Chen Xu, Xiaopeng Ye, Jun Xu, Xiao Zhang, Weiran Shen, and Ji-Rong Wen As for provider fairness, we directly utilized the definition of MMF in Section 3 as the metric:  Ô£≥ Ô£æ where I (¬∑) is the indicator function. As for the trade-off performance, we used the online objective traded-off to measure the fairness:  where ùúÜ ‚â• 0 is the trade-off coefficient. 5.1.3 Baselines. The following representative provider fair re-ranking models were chosen as the baselines: FairRec [43] and FairRec+ [12] aimed to guarantee at least Maximin Share (MMS) of the provider exposures. CPFair [40] formulated the trade-off problem as a knapsack problem and proposed a greedy solution. We also chose the following MMF models: Welf [19] use the Frank-Wolfe algorithm to maximize the Welfare functions of worst-off items. However, it is developed under off-line settings; RAOP [5] is a state-of-the-art online resource allocation method. We applied it to the recommendation by regarding the items as the resources and users as the demanders. Fairco [39]: added a regularizer that measures the exposure gaps between the target provider and the worst providers. We also compared the proposed with one heuristic MMF baseline: ùêæ -neighbor : at each time step ùë° , only the items associated to the topùêæ providers with the least cumulative exposure are recommended. At the same time, we also compared three bandit baselines, which aim to remedy the bias from the feedback loops of RS. Note that, to make a fair comparison, all the bandit algorithms are updated based on the batched log. hLinUCB [53]: it utilized UCB for learning hidden features for both users and items. EXP3 [11] aimed to choose actions according to a distribution constructed by the exponential weights. BLTS [17] will sample a reward based on a normal distribution with the estimated reward variance. However, these bandit baselines did not consider provider fairness in the long term. Meanwhile, we also compare the baseline Dyn [4] of optimizing long-term fairness in feedback loops. However, Dyn addresses long-term fairness concerns by considering the dynamic nature of user social networks, where changes in social connections may lead users to belong to different social groups over time. Therefore, we apply their re-ranking methods to our framework by treating specific user groups as providers and users within those groups as items belonging to the respective provider. To ensure a fair comparison, we substitute their dynamic environment, which involved changing social networks, with dynamic user-item score estimation. Table 2. List of critical parameters related to fairness in Table 3. To ensure a fair comparison, if the baseline includes a linear trade-off coefficient, we uniformly set ùúÜ = 0 . 5 . For other parameters, we tuned them within the following ranges for each dataset. ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2024. LTP-MMF: Towards Long-term Provider Max-min Fairness Under Recommendation Feedback Loops 1:15 Table 3. Performance comparisons between LTP-MMF and the baselines. The experimental settings are listed in Table 2. The bold numbers denote the performance of our models, and the underlined numbers denote the best-performing baselines. ‚àó : improvements over the best baseline are statistically significant ( ùë° -test, ùëù -value< 0.05). ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:16 Chen Xu, Xiaopeng Ye, Jun Xu, Xiao Zhang, Weiran Shen, and Ji-Rong Wen 60 62 64 66 68 70 72 74 CTR@10(%) 0 10 20 30 40 50 MMF@10(%) Œª =0.001 Œª =0.005 Œª =0.01 LTP-MMF P-MMF BLTS CPFair FairCo FairRec+ Welf (a) Yelp 50 51 52 53 54 55 CTR@10(%) 0 10 20 30 40 50 60 MMF@10(%) Œª =0.001 Œª =0.005 Œª =0.01 Œª =0.1 LTP-MMF P-MMF BLTS CPFair FairCo FairRec+ Welf 49 50 51 52 53 54 55 56 CTR@10(%) 0 10 20 30 40 50 MMF@10(%) Œª =0.001 Œª =0.005 Œª =0.01 Œª =0.1 LTP-MMF P-MMF BLTS CPFair FairCo FairRec+ Welf (b) Amazon Beauty 30 40 50 60 70 80 CTR@10(%) 0 10 20 30 40 50 60 MMF@10(%) Œª =0.001 Œª =0.005 Œª =0.01 LTP-MMF P-MMF BLTS CPFair FairCo FairRec+ Welf (c) Amazon Baby (d) Steam Fig. 3. Pareto frontier in Four Dataset 5.1.4 Implementation details. As for the hyper-parameters in all models, the learning rate was tuned among [ 1 ùëí -2 , 1 ùëí -3 ]/ ùëá 1 / 2 , and the momentum coefficient ùõº was tuned among [ 0 . 2 , 0 . 5 ] . For the maximum resources (i.e., the weights) ùú∏ , following the practices in [58, 59], we set ùú∏ based on the number of items provided by the providers:  where ùúÇ is the factor controlling the richness of resources. In all the experiments, we set ùúÇ = 1 + 1 /|P| . We implemented LTP-MMF with PyTorch [42]. The experiments were conducted with a single NVIDIA GeForce RTX 3090.",
  "5.2 Main Experiments": "In this section, we conducted the experiments on four large-scale datasets with the LTP-MMF and other baselines. Due to variations in dataset characteristics, we will provide a comprehensive list of critical parameters related to fairness performance ranges for our model LTP-MMF and other baselines in Table 2. To make a fair comparison, the other important variables such as ùõæ for evaluation and the learning rate are all listed in Section 5.1.4 in the revised version. ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2024. LTP-MMF: Towards Long-term Provider Max-min Fairness Under Recommendation Feedback Loops 1:17 0.01 0.1 1.0 5.0 10.0 50.0 100.0 ‚àö Œªu , ‚àö Œªi 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 Metric CTR@10 MMF@10 r@10 Fig. 4. Exploration Weight 0 25 50 75 100 125 150 175 200 Interactions 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 CTR@10 LTP-MMF LTP-MMF w/o exploration term (a) CTR Fig. 5. Ablation study for the exploration of LTP-MMF. 0 25 50 75 100 125 150 175 200 Interactions 0.35 0.40 0.45 0.50 0.55 0.60 0.65 MMF@10 LTP-MMF LTP-MMF w/o exploration term (b) MMF 5.2.1 Overall performance. Table 3 reports the experimental results of LTP-MMF and the baselines on all three datasets to investigate the long-term performance. Underlined numbers mean the best-performed baseline and ‚àó means the improvements over the best baseline are statistically significant ( ùë° -test, ùëù -value< 0.05). To make fair comparisons, all the baselines were tuned and used ùëü @ ùêæ as the evaluation metric. Note that similar experiment phenomena have also been observed on other ùúÜ values. From the reported results, we found that LTP-MMF outperformed all of the bandit baselines including hLinUCB, EXP3, and BLTS, which verified that LTP-MMF can serve the provider fairness well in the long term. We also observed that LTP-MMF outperformed all the MMF-based baselines, indicating that LTP-MMF can better remedy the bias in the RFL compared to other provider-fair baselines. The reason is that LTP-MMF can explore the feedback of unexposed items, avoiding always treating unexposed items as negative ones. Due to exposure bias, larger exposure sizes tend to decrease bias, as they offer more items to be exposed to users. However, in modern Recommender Systems, such as YouTube [15], the ranking size exposed to users is typically limited to single-digit items. Moreover, in the example depicted in Figure 1, we can observe that as the ranking size increases, the severity of exposure bias diminishes. ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:18 Chen Xu, Xiaopeng Ye, Jun Xu, Xiao Zhang, Weiran Shen, and Ji-Rong Wen Fig. 6. (a) illustrates the long-term performance and (b) illustrates the long-term regret w.r.t. batch size ùëá 200 400 600 800 1000 Batch size T 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Metric CTR@10 MMF@10 r@10 (a) Performance 200 400 600 800 1000 Batch size T 0.10 0.15 0.20 0.25 0.30 0.35 Regret Regret( œÄ F ) Regret( œÄ ) Regret( œÄ , œÄ F ) (b) Regret From the empirical studies presented in Table 2, we can observe that the average improvements over the best baselines are 5.5%, 2.7%, and 1.8% for ranking sizes of 5, 10, and 20, respectively. This means that the smaller the magnitude of the exposure size (i.e. the larger the exposure bias in RFL), the more improvements LTP-MMF increases. It indicates the effectiveness of LTP-MMF especially when the ranking size is small. Finally, it is evident that our model LTP-MMF significantly outperforms the long-term fairness baseline Dyn, underscoring the effectiveness of our methods in optimizing long-term fairness amidst the changing user preference estimations influenced by the feedback loops of RS. 5.2.2 Pareto frontier. Figure 3 shows the Pareto frontiers [36] of CTR@K and MMF@K. The Pareto frontiers were drawn by tuning the accuracy-fairness trade-off coefficient ùúÜ ‚àà [ 1 ùëí -3 , 1 ] with best (CTR@K, MMF@K) long-term performances. In the experiment, we selected the baselines of P-MMF, CPFair, fairco, Welf, FairRec+, and BLTS, which achieved relatively good performances among fairness baselines and bandit baselines. From the Pareto frontiers, we can see that the proposed LTP-MMF Pareto dominated all the baselines (i.e., the LTP-MMF curves are at the upper right corner) expect P-MMF [59] on the Amazon-Baby and Amazon-Beauty, indicating that LTP-MMF can achieve better user accuracy (i.e., CTR@K) with the same provider fairness (MMF@K) level. The results demonstrate that LTP-MMF can better trade-off accuracy and fairness in the long term. Moreover, we can observe the changes in user accuracy and provider fairness with trade-off coefficient ùúÜ = [ 0 . 001 , 0 . 01 ] . From Figure 3(d), we can observe that when ùúÜ increases from 0 . 001 to 0 . 01, the user accuracy (i.e., CTR@K) only decreases 0.09% but provider fairness (i.e. MMF@K) increases 220%. The result indicated that we can achieve better MMF in the long term while sacrificing little accuracy in LTP-MMF.",
  "5.3 Experiment Analysis": "We also conducted experiments to analyze LTP-MMF on the Steam dataset under top-10 settings. 5.3.1 Ablation study on exploration term. To investigate the impact of exploration term (i.e., the upper-confidence-bound ‚ñ≥ ùëì ùë¢,ùëñ in Eq. (10)), we conduct an ablation study shown in Figure 5. Specifically, we showed the CTR@10 and MMF@10 of two LTP-MMF variations, including the variations without exploration term (denoted as 'LTP-MMF w/o exploration term'), and the complete LTPMMF. From Figure 5, we found that in the first 25 interactions, the exploration term of LTP-MMF is ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2024. LTP-MMF: Towards Long-term Provider Max-min Fairness Under Recommendation Feedback Loops 1:19 0 25 50 75 100 125 150 175 200 Interactions 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 r@10 LTP-MMF P-MMF BLTS CPFair FairCo FairRec+ Welf Fig. 7. The periodical performance of LTP-MMF and other top baselines Fig. 8. The online inference time per user w.r.t. the number of total items. 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 Item size 1e4 0 2 4 6 8 10 12 14 16 Inference Time (ms) Fairness Inference Time Accuracy Inference Time Exploration Time Fig. 9. Ablation studies of the individual impact of each component. We report the overall performance ùëü @ ùêæ on Steam datasets under different ranking sizes ùêæ . top-5 top-10 top-20 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 r@K w/o fairness module w/o UCB module full modules ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:20 Chen Xu, Xiaopeng Ye, Jun Xu, Xiao Zhang, Weiran Shen, and Ji-Rong Wen dominant, leading to relatively poor performance. However, in the rest of the periods, LTP-MMF explores more trustful feedback of items, leading to better estimation of user preference. Therefore, LTP-MMF has better performance in terms of both user accuracy and provider fairness in the long term compared to 'LTP-MMF w/o exploration term'. The experiment also verified the importance of giving the exposures fairly to the providers in the feedback loop. At the same time, we also conduct the experiment to investigate the long-term impact of different exploration weights (i.e. ‚àö ùúÜ ùëñ , ‚àö ùúÜ ùë¢ in Eq. 11) shown in Figure 4. From the curve, we find that the performance improved when ‚àö ùúÜ ùëñ , ‚àö ùúÜ ùë¢ ‚àà [ 0 . 01 , 1 ] and then they dropped between [ 1 , 100 ] , respectively. The reason is that the more exploration, the less bias will be in the long term. However, too much exploration will inevitably hurt the exploit reward, hurting the long-term performance. The opposite also holds. Therefore, we need to balance the exploration-exploit in the long term. 5.3.2 Ablation study of different fair-aware modules. To investigate the impact of different fairaware modules (fairness module and UCB module), we conduct an ablation study shown in Figure 9. Specifically, we showed the r@K of two LTP-MMF variations, including the variations without fairness module (denoted as 'w/o fairness module'), without UCB module (denoted as 'w/o UCB module'), and the complete LTP-MMF (denoted as 'full modules'). From Figure 9, We can observe that when dropping out of the fairness and UCB modules, the overall performance decreases under different ranking sizes, indicating the effectiveness of the different fairness-aware modules. 5.3.3 Impact of the batch size ùëá . According to Theorem 3, the batch size ùëá balances the regret of user accuracy and provider fairness. In this experiment, Figure 6 (a) studied how CTR@K, MMF@K, and r@K changed when the batch size ùëá was set to different values from [ 64 , 1024 ] . Figure (b) studied how regret of accuracy (Regret( ùúã )), regret of fairness (Regret( ùúã ùêπ )) and regret of LTP-MMF (Regret( ùúã, ùúã ùêπ )) changed when the batch size ùëá was set from [ 64 , 1024 ] . From the curves shown in Figure 6 (a), we found the accuracy performance CTR@K improved while the fairness performance MMF@K dropped when the batch size becomes smaller. Similarly, in Figure 6 (b), the regret of accuracy Regret( ùúã ) dropped while the regret of fairness Regret( ùúã ùêπ ) dropped when the batch size becomes smaller. The results verified the theoretical analysis that small ùëá (e.g., ùëá = 64) results in more bias in fairness estimation while large ùëá (e.g., ùëá = 1024) results in large bias in accuracy estimation in the long term. Moreover, The overall performance ùëü @ ùêæ improved and regret of LTP-MMF (Regret( ùúã, ùúã ùêπ )) dropped when ùëá ‚àà [ 64 , 512 ] and then ùëü @ ùêæ , Regret( ùúã, ùúã ùêπ ) dropped and improved between [ 512 , 1024 ] , respectively. Therefore, it is important to balance the accuracy and fairness in real applications through batch size ùëá . 5.3.4 Periodical performance. Figure 7 reports the experimental results of LTP-MMF and performing baselines to investigate how the LTP-MMF performs for each period. Note that similar experiment phenomena have also been observed on other ùúÜ and topùëò values. To make fair comparisons, all the baselines were tuned ùúÜ = 1 as the evaluation metric. From the ùëü @10 curve, we found that although the performance gaps between LTP-MMF and other baselines are not obvious in the beginning (interaction 0), the LTP-MMF can well explore the feedback of unexposed items, leading to a huge improvement over other baselines in the long term (interaction 200). Moreover, in most periods, LTP-MMF can steadily outperform other baselines, verifying the effectiveness of LTP-MMF. 5.3.5 Online inference time. We experimented with investigating the online inference time of LTPMMF. Figure 8 reports the curves of inference time (ms) of the accuracy module, fairness module, and exploration term computation per user access w.r.t. item size. We can see that LTP-MMF with ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2024. LTP-MMF: Towards Long-term Provider Max-min Fairness Under Recommendation Feedback Loops 1:21 GPU versions needs only within 6ms to calculate user-item scores ùë† ùë¢,ùëñ and exploration term ‚ñ≥ ùëì ùë¢,ùëñ . The reason is that this operation only needs matrix multiplication and the inverse of the matrix can be stored in the training (offline) phase, leading to low latency in the online phase. Moreover, the inference time for the fairness module can also be maintained as 14-16ms even when the item size becomes larger. We conclude that LTP-MMF can be adapted to online scenarios efficiently because of its low latency, even when the item size grows rapidly.",
  "6 CONCLUSION": "We proposed a novel ranking model called LTP-MMF that aims to consider provider max-min fairness in the long term. Firstly, we formulated the provider fair recommendation as a repeated resource allocation problem under a batched bandit setting. LTP-MMF applies the exploration term to break the loop while exploiting the fairness-aware rewards. Our theoretical analysis showed that the regret of LTP-MMF can be bounded. Experiments on four available datasets demonstrated that LTP-MMF can conduct ranking in an effective and efficient way.",
  "ACKNOWLEDGMENTS": "This work was funded by the National Key R&D Program of China (2023YFA1008704), the National Natural Science Foundation of China (No. 62376275 & No. 62106273 & No. 72192805), Engineering Research Center of Next-Generation Intelligent Search and Recommendation, Ministry of Education, Major Innovation & Planning Interdisciplinary Platform for the 'Double-First Class' Initiative, Renmin University of China, fund for building world-class universities (disciplines) of Renmin University of China. Supported by the Outstanding Innovative Talents Cultivation Funded Programs 2024 of Renmin University of China. ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:22 Chen Xu, Xiaopeng Ye, Jun Xu, Xiao Zhang, Weiran Shen, and Ji-Rong Wen",
  "A APPENDIX": "",
  "A.1 Proof of Theorem 1": "Proof. For max-min fairness, we have the regularizer as ùëü ( e ) = min ùëù ‚ààP GLYPH<0> e ùëù / ùú∏ ùëù GLYPH<1> , we can easily proof that the exposure vector e can be represented as the dot-product between decision varible x ùë° and the item-provider adjacent matrix A : e = Àù ùëá ùë° = 1 ( A ‚ä§ x ùë° ) . Then we treat the e as the auxiliary variable, and the ideal objective can be written as:   where X = { x ùë° | x ùë° ‚àà 0 , 1 ‚àß Àù ùëñ ‚ààI x ùë°ùëñ = ùêæ } . Then we move the constraints to the objective using a vector of Lagrange multipliers ùùÅ ‚àà R | P | :  where D = ùùÅ | ùëü ‚àó (-ùùÅ ) < ‚àû} is the feasible region of dual variable ùùÅ . According to the Lemma 1 in the Balseiro et al. [5], we have D is convex and positive orthant is inside the recession cone of D . We let the variable z ùëù = ( e ùëù / ùú∏ ùëù -1 ) , we have:   Ô£∞ Ô£ª  Let ùë† ( z ) = min ùëù z ùëù and v = ( ùùÅ ‚äô ùú∏ )/ ùúÜ , ‚äô is the hadamard product. Then we define ùë† ‚àó ( v ) = max z ‚â§ 0 GLYPH<0> ùë† ( z ) + z ùëá v GLYPH<1> . We firstly show that if Àù ùëù ‚ààS v ùëù ‚â• -1 , ‚àÄS ‚àà P ùë† , then ùë† ‚àó ( v ) = 0 and z = 0 is the optimal solution, otherwise ùë† ‚àó ( v ) = ‚àû . We can equivalently write D = { v | Àù ùëù ‚ààS v ùëù ‚â• -1 , ‚àÄS ‚àà P ùë† } . We firstly show that ùë† ‚àó ( v ) = ‚àû for v ‚àâ D . Suppose that there exists a subset S ‚àà P ùë† such that Àù ùëù ‚ààS v ùëù < -1. For any ùëè > 1, we can get a feasible solution:  Then, because such solution is feasible and ùë† ( z ) = -ùëè , we obtain that  Let ùëè ‚Üí‚àû , we have ùë† ‚àó ( v ) ‚Üí ‚àû . Then we show that ùë† ‚àó ( ùùÅ ) = 0 for v ‚àà D . Note that z = 0 is feasible. Therefore, we have ùë† ‚àó ( v ) ‚â• ùë† ‚àó ( 0 ) = 0 . ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2024. LTP-MMF: Towards Long-term Provider Max-min Fairness Under Recommendation Feedback Loops 1:23 Then we have z ‚â§ 0 and without loss of generality, that the vector z is sorted in increasing order, i.e., z 1 ‚â§ z 2 , ¬∑ ¬∑ ¬∑ , ‚â§ z | P | . The objective value is  ‚ñ° ‚ñ°",
  "A.2 Proof of Lemma 1": "Proof. From the proof of Theorem 1, we have  and ùë† ‚àó ( ùùÅ ) = 0 for v ‚àà D . Therefore, we have",
  "A.3 Proof of Theorem 2": "Proof. Our proof will take the following four steps: Bias Term Bound Firstly, we will bound the bias term ‚à• ÀÜ ùíó ùë¢,ùë° -ùíó ‚àó ùë¢ ‚à• ùë® ùë¢,ùë° , ‚à• ÀÜ ùíó ùëñ,ùë° -ùíó ‚àó ùëñ ‚à• ùë™ ùëñ,ùë° : we define the bias term upper bound as ùõº ùë° , ùõΩ ùë° respectively. We will bound the bias term as follows: by taking the gradient of the objective function respect to ùë£ ùë¢ , ùë£ ùëñ , we have,  where ùúñ ùëó is the Gaussian noise at time ùëó . Without loss of generality, in ranking task, we can always scale the user-item score ùë† ùë¢,ùëñ , so that the l2-norm of ùë£ ùë¢ , ùë£ ùëñ can be bounded by a constant factor:  Therefore, we can bound the function norm of the above equation as  Since the ùëû -linearly convergent to the optimizer of parameter ùíó ùë¢ , ùíó ùëñ in [53], we have for every ùúñ ùëû > 0 and 0 < ùëû < 1, we have  Therefore, by applying the self-normalized vector-valued martingales [1], we have for any ùúé > 0, with probability at least 1 -ùúé ,   ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:24 Chen Xu, Xiaopeng Ye, Jun Xu, Xiao Zhang, Weiran Shen, and Ji-Rong Wen Similarly, we have   ‚àö . Therefore, the bias term ùõº ùë° , ùõΩ ùë° are comparable with ùëÇ (( 1 - ( ùëû + ùúñ ùëü ) ùë° ) ln ùë° ) for every ùúñ ùëü > 0. Variance Term Bound Next, we will bound the variance term Then we prove the converge rate of variance term: ‚à• ÀÜ ùíó ùë¢,ùë° ‚à• 2 , ‚à• ÀÜ ùíó ùëñ,ùë° ‚à• 2 as follows: ùë® ùë¢,ùë° ùë™ ùëñ,ùë°  since we can always scale the l2-norm of ùë£ ùë¢ , ùë£ ùëñ by any rate in ranking tasks (we only care the relative score). Thus we can easily obtain that  Collaborative Variance Term Bound Next, we will bound the variance term of collaborative term ‚à• ÀÜ ùíó ùë¢,ùë° -ùíó ‚àó ùë¢ ‚à• 2 ùë® -1 ùë¢,ùë° , ‚à• ÀÜ ùíó ùëñ,ùë° -ùíó ‚àó ùëñ ‚à• 2 ùë™ -1 ùëñ,ùë° as follows:  where ùëû, ùúñ ùëû follows the Eq. (21). Similarly, we have  Let's abbreviate the upper bound of collaborative error term ( ùëû + ùúñ ùëû ) ùë° as ùê∂ Upper Confidence Bound Finally, the upper confidence bound of user-item score can have easily by putting the aforementioned term together:  We can easily have the upper bound of user-item score have the converge term  which is a decrease function of ùë° when the ùë° becomes large. ‚ñ°",
  "A.4 Proof of Theorem 3": "Proof. Firstly, in practice, we normalize the user-item preference score ùë† ùë¢,ùëñ to [ 0 , 1 ] . Therefore, Àù ùëá ùë° = 1 ùëî ( x ùë° )/ ùëá ‚â§ ùêæ . In max-min regularizer ùëü ( e ) . Let's abbreviate its upper bound to ¬Ø ùëü . In practice, ¬Ø ùëü ‚â§ 1 We have  ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2024. LTP-MMF: Towards Long-term Provider Max-min Fairness Under Recommendation Feedback Loops 1:25 We consider the stopping time ùúè of Algorithm 1 as the first time the provider will have the maximum exposures, i.e.  Note that is ùúè a random variable. Similarly, following the proven idea of Balseiro et al. [5], first, we analyze the primal performance of the objective function. Second, we bound the complementary slackness term by the momentum gradient descent. Finally, We conclude by putting it to achieve the final regret bound. Primal performance proof : Consider a time ùë° < ùúè , the recommender action will not violate the resource constraint. Therefore, we have:  and we have e ùë° = arg max e ‚â§ ùú∏ { ùëü ( e ) + ùùÅ ‚ä§ e / ùúÜ }  We make the expectations for the current time step ùë° for the primal functions:  Consider the process ùëç ùë° = Àù ùëá ùëó = 1 ùùÅ ùë° ùëó (-ùë¥ ‚ä§ x ùë° + e ùë° ) -E GLYPH<2> ùùÅ ùëá ùë° (-ùë¥ ‚ä§ x ùë° + e ùë° ) GLYPH<3> is a martingale process. The Optional Stopping Theorem in martingale process [57] implies that E [ ùëç ùúè ] = 0. Consider the variable ùë§ ùë° ( ùùÅ ùë° ) = ùùÅ ùëá ùë° (-A ‚ä§ x ùë° + e ùë° ) , we have  Moreover, in MMF, the dual function ùëä ùê∑ùë¢ùëéùëô is convex proofed in Theorem 1, we have  where f ùùÅ ùúè = Àù ùúè ùë° = 1 ùùÅ ùë° / ùúè . Next, we will bound the bias of the primal performance due to the estimation error of the ranking model. From the proof of theorem 1, we can bound the ùëä ùê∑ùë¢ùëéùëô ( ùùÅ ùë° ) as follows: at iteration ùëõ , for any ùúé > 0, with probability at least 1 -ùúé ,  Complementaryslacknessproof Thenweaimtoproofthecomplementaryslackness Àù ùëá ùë° = 1 ùë§ ùë° ( ùùÅ ùë° )-ùë§ ùë° ( ùùÅ ) is bounded. Suppose there exists ùê∫,ùë†.ùë°. the gradient norm is bounded ‚à• e g ùë° ‚à• ‚â§ ùê∫ . Then we have:  ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:26 Chen Xu, Xiaopeng Ye, Jun Xu, Xiao Zhang, Weiran Shen, and Ji-Rong Wen where the project function ‚à• ùùÅ -ùùÅ ùë° ‚à• 2 ùú∏ is ùúé -strongly convex. Next, we prove the inequality in Equation. According to the Theorem 1 in [6], we have  and  We have ‚à• ùùÅ ùë° -ùùÅ 0 ‚à• 2 ùú∏ 2 ‚â§ ùêø 2 according to the Cauchy-Schwarz' inequality. The results follows. Let ùëÄ = ùêø 2 ùúÇ + ùê∫ 2 ( 1 -ùõº ) ùúé ùúÇ ( ùëá -1 ) + ùê∫ 2 2 ( 1 -ùõº ) 2 ùúéùúÇ . We now choose a proper ùùÅ , s.t. the complementary stackness can be further bounded. For ùùÅ = ÀÜ ùùÅ + ùúÉ , where ùúÉ ‚àà R | ùëÉ | is non-negative to be determined later and ÀÜ ùùÅ = arg max ùùÅ -ùùÅ ‚ä§ ( Àù ùëá ùëñ = 1 A ‚ä§ x ùë° )/ ùúÜ . According to the constraint e = Àù ùëá ùëñ = 1 A ‚ä§ x ùë° , we have that  Note that in proof of Theorem 1, the feasible region D is recession cone, therefore, ùùÅ ‚àà D . Therefore, we have  For each iteration ùëõ , we have  Put them together: For each batch ùëá at iteration ùëõ , we obtain that  Let's abbreviate ‚ñ≥ ùëü ( ùëõ ) = Àù ùúè ùë° = 1 Àù ùëñ, ùíô ùë°ùëñ ‚ñ≥ ùëì ùëõ ùë¢ ùë° ,ùëñ ùêø . Therefore, combining Eq. (10,12,13) the regret ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2024. LTP-MMF: Towards Long-term Provider Max-min Fairness Under Recommendation Feedback Loops 1:27 Regret ( ùëõ ) of iteration ùëõ , can be bounded as:  Let ùê∂ = ùêæ + ùúÜ ¬Ø ùëü + ùúÜùêæ , then setting the ùúÉ = ùê∂ min ùëù ùú∏ ùëù u ùëù , where u ùëù is the p-th unit vector. We have  Then the Regret ( ùëõ ) ‚â§ ùëÄ + ùê∂ /( min ùëù ùú∏ ùëù ) , when we set ùúÇ = ùëÇ ( ùëá -1 / 2 ) , the Regret( ùúã ùêπ ) is comparable with ùëÇ ( ùëá -1 / 2 ) . According to our algorithm in Algorithm of LTP-MMF, the user will interact with the system in ùëÅ / ùëá iterations. Following the Lemma 11 in Abbasi-Yadkori et al. [1], the error Regret ( ùúã ) raised of accuracy module is bounded as  where and  Fromtheconclusion, we can see that the regret of accuracy module is comparable with ùëÇ ( ‚àöÔ∏É ùëÅùëá ln ùëÅ ùëá ) . Finally, the total regret can be bounded as  Setting the learning rate as ùúÇ = ùëÇ ( ùëá -1 / 2 ) , we can obtain a fairness regret Regret( ùúã ùêπ ) upper bound of order ùëÇ ( ùëÅ ‚àö ùëá ) . Overall, the long-term regret of LTP-MMF can be obtained of order ùëÇ ( ùëÅ ln ùëÅ ) . ‚ñ°  ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:28 Chen Xu, Xiaopeng Ye, Jun Xu, Xiao Zhang, Weiran Shen, and Ji-Rong Wen",
  "REFERENCES": "[1] Yasin Abbasi-Yadkori, D√°vid P√°l, and Csaba Szepesv√°ri. 2011. Improved algorithms for linear stochastic bandits. Advances in neural information processing systems 24 (2011). [2] Himan Abdollahpouri, Gediminas Adomavicius, Robin Burke, Ido Guy, Dietmar Jannach, Toshihiro Kamishima, Jan Krasnodebski, and Luiz Pizzato. 2020. Multistakeholder recommendation: Survey and research directions. User Modeling and User-Adapted Interaction 30, 1 (2020), 127-158. [3] Himan Abdollahpouri and Robin Burke. 2019. Multi-stakeholder recommendation and its connection to multi-sided fairness. arXiv preprint arXiv:1907.13158 (2019). [4] Nil-Jana Akpinar, Cyrus DiCiccio, Preetam Nandy, and Kinjal Basu. 2022. Long-term Dynamics of Fairness Intervention in Connection Recommender Systems. arXiv preprint arXiv:2203.16432 (2022). [5] Santiago Balseiro, Haihao Lu, and Vahab Mirrokni. 2021. Regularized online allocation problems: Fairness and beyond. In International Conference on Machine Learning . PMLR, 630-639. [6] Santiago R. Balseiro, Haihao Lu, Vahab Mirrokni, and Balasubramanian Sivan. 2022. From Online Optimization to PID Controllers: Mirror Descent with Momentum. https://doi.org/10.48550/ARXIV.2202.06152 [7] Omer Ben-Porat and Rotem Torkan. 2023. Learning with Exposure Constraints in Recommendation Systems. In Proceedings of the ACM Web Conference 2023 . 3456-3466. [8] Dimitris Bertsimas, Vivek F Farias, and Nikolaos Trichakis. 2011. The price of fairness. Operations research 59, 1 (2011), 17-31. [9] Asia J Biega, Krishna P Gummadi, and Gerhard Weikum. 2018. Equity of attention: Amortizing individual fairness in rankings. In The 41st international acm sigir conference on research & development in information retrieval . 405-414. [10] Ilai Bistritz, Tavor Baharav, Amir Leshem, and Nicholas Bambos. 2020. My fair bandit: Distributed learning of max-min fairness with multi-player bandits. In International Conference on Machine Learning . PMLR, 930-940. [11] Ilai Bistritz, Zhengyuan Zhou, Xi Chen, Nicholas Bambos, and Jose Blanchet. 2019. Online EXP3 learning in adversarial bandits with delayed feedback. In Advances in Neural Information Processing Systems 32 . 11349-11358. [12] Arpita Biswas, Gourab K Patro, Niloy Ganguly, Krishna P Gummadi, and Abhijnan Chakraborty. 2021. Toward Fair Recommendation in Two-sided Platforms. ACM Transactions on the Web (TWEB) 16, 2 (2021), 1-34. [13] Robin Burke, Nasim Sonboli, and Aldo Ordonez-Gauger. 2018. Balanced neighborhoods for multi-sided fairness in recommendation. In Conference on fairness, accountability and transparency . PMLR, 202-214. [14] Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan He. 2020. Bias and Debias in Recommender System: A Survey and Future Directions. CoRR abs/2010.03240 (2020). [15] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks for youtube recommendations. In Proceedings of the 10th ACM conference on recommender systems . 191-198. [16] Yashar Deldjoo. 2024. Understanding Biases in ChatGPT-based Recommender Systems: Provider Fairness, Temporal Stability, and Recency. arXiv preprint arXiv:2401.10545 (2024). [17] Maria Dimakopoulou, Zhengyuan Zhou, Susan Athey, and Guido Imbens. 2019. Balanced linear contextual bandits. In Proceedings of the 33rd AAAI Conference on Artificial Intelligence . 3445-3453. [18] Karlijn Dinnissen and Christine Bauer. 2023. Amplifying Artists' Voices: Item Provider Perspectives on Influence and Fairness of Music Streaming Platforms. In Proceedings of the 31st ACM Conference on User Modeling, Adaptation and Personalization (Limassol, Cyprus) (UMAP '23) . Association for Computing Machinery, New York, NY, USA, 238-249. https://doi.org/10.1145/3565472.3592960 [19] Virginie Do, Sam Corbett-Davies, Jamal Atif, and Nicolas Usunier. 2021. Two-sided fairness in rankings via Lorenz dominance. Advances in Neural Information Processing Systems 34 (2021), 8596-8608. [20] KJ Erickson. 2018. In Their Own Words: Why Sellers Are Fed Up With Amazon. https://medium.com/public-market/intheir-own-words-why-sellers-are-fed-up-with-amazon-e97da44f7f18 [21] Andres Ferraro, Xavier Serra, and Christine Bauer. 2021. Break the loop: Gender imbalance in music recommenders. In Proceedings of the 2021 Conference on Human Information Interaction and Retrieval . 249-254. [22] Yingqiang Ge, Shuchang Liu, Ruoyuan Gao, Yikun Xian, Yunqi Li, Xiangyu Zhao, Changhua Pei, Fei Sun, Junfeng Ge, Wenwu Ou, et al. 2021. Towards long-term fairness in recommendation. In Proceedings of the 14th ACM International Conference on Web Search and Data Mining . 445-453. [23] Elizabeth G√≥mez, Ludovico Boratto, and Maria Salam√≥. 2022. Provider fairness across continents in collaborative recommender systems. Information Processing & Management 59, 1 (2022), 102719. [24] Xiangnan He, Yang Zhang, Fuli Feng, Chonggang Song, Lingling Yi, Guohui Ling, and Yongdong Zhang. 2023. Addressing confounding feature issue for causal recommendation. ACM Transactions on Information Systems 41, 3 (2023), 1-23. [25] Bal√°zs Hidasi, Massimo Quadrana, Alexandros Karatzoglou, and Domonkos Tikk. 2016. Parallel recurrent neural network architectures for feature-rich session-based recommendations. In Proceedings of the 10th ACM conference on recommender systems . 241-248. ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2024. LTP-MMF: Towards Long-term Provider Max-min Fairness Under Recommendation Feedback Loops 1:29 [26] Yifan Hu, Yehuda Koren, and Chris Volinsky. 2008. Collaborative filtering for implicit feedback datasets. In 2008 Eighth IEEE international conference on data mining . Ieee, 263-272. [27] Rolf Jagerman, Ilya Markov, and Maarten de Rijke. 2019. When people change their mind: Off-policy evaluation in non-stationary recommendation environments. In Proceedings of the twelfth ACM international conference on web search and data mining . 447-455. [28] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recommendation. In 2018 IEEE international conference on data mining (ICDM) . IEEE, 197-206. [29] Saeedeh Karimi, Hossein A. Rahmani, Mohammadmehdi Naghiaei, and Leila Safari. 2023. Provider Fairness and Beyond-Accuracy Trade-offs in Recommender Systems. arXiv:2309.04250 [cs.IR] [30] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization techniques for recommender systems. Computer 42, 8 (2009), 30-37. [31] Julian Lamont and Christi Favor. 2017. Distributive Justice. In The Stanford Encyclopedia of Philosophy (Winter 2017 ed.), Edward N. Zalta (Ed.). Metaphysics Research Lab, Stanford University. [32] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. 2010. A contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th international conference on World wide web . 661-670. [33] Qian Li, Xiangmeng Wang, Zhichao Wang, and Guandong Xu. 2023. Be Causal: De-Biasing Social Network Confounding in Recommendation. 17, 1, Article 14 (feb 2023), 23 pages. https://doi.org/10.1145/3533725 [34] Xinyi Li, Yongfeng Zhang, and Edward C Malthouse. 2023. A preliminary study of chatgpt on news recommendation: Personalization, provider fairness, fake news. arXiv preprint arXiv:2306.10702 (2023). [35] Dugang Liu, Pengxiang Cheng, Zhenhua Dong, Xiuqiang He, Weike Pan, and Zhong Ming. 2020. A general knowledge distillation framework for counterfactual recommendation via uniform data. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . 831-840. [36] Alexander V Lotov and Kaisa Miettinen. 2008. Visualizing the Pareto frontier. In Multiobjective optimization . Springer, 213-243. [37] Debmalya Mandal and Jiarui Gan. 2022. Socially fair reinforcement learning. arXiv preprint arXiv:2208.12584 (2022). [38] Martin Mladenov, Elliot Creager, Omer Ben-Porat, Kevin Swersky, Richard Zemel, and Craig Boutilier. 2020. Optimizing long-term social welfare in recommender systems: A constrained matching approach. In International Conference on Machine Learning . PMLR, 6987-6998. [39] Marco Morik, Ashudeep Singh, Jessica Hong, and Thorsten Joachims. 2020. Controlling Fairness and Bias in Dynamic Learning-to-Rank. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, China) (SIGIR '20) . Association for Computing Machinery, New York, NY, USA, 429-438. https://doi.org/10.1145/3397271.3401100 [40] Mohammadmehdi Naghiaei, Hossein A Rahmani, and Yashar Deldjoo. 2022. Cpfair: Personalized consumer and producer fairness re-ranking for recommender systems. arXiv preprint arXiv:2204.08085 (2022). [41] Harrie Oosterhuis and Maarten de Rijke. 2020. Policy-aware unbiased learning to rank for top-k rankings. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . 489-498. [42] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017. Automatic differentiation in PyTorch. (2017). [43] Gourab K Patro, Arpita Biswas, Niloy Ganguly, Krishna P Gummadi, and Abhijnan Chakraborty. 2020. Fairrec: Two-sided fairness for personalized recommendations in two-sided platforms. In Proceedings of The Web Conference 2020 . 1194-1204. [44] Tao Qi, Fangzhao Wu, Chuhan Wu, Peijie Sun, Le Wu, Xiting Wang, Yongfeng Huang, and Xing Xie. 2022. ProFairRec: Provider fairness-aware news recommendation. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1164-1173. [45] Ning Qian. 1999. On the momentum term in gradient descent learning algorithms. Neural networks 12, 1 (1999), 145-151. [46] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2012. BPR: Bayesian personalized ranking from implicit feedback. arXiv preprint arXiv:1205.2618 (2012). [47] Paras Sheth, Ruocheng Guo, Kaize Ding, Lu Cheng, K Sel√ßuk Candan, and Huan Liu. 2022. Causal disentanglement with network information for debiased recommendations. In International Conference on Similarity Search and Applications . Springer, 265-273. [48] Ashudeep Singh and Thorsten Joachims. 2018. Fairness of exposure in rankings. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 2219-2228. [49] Wenlong Sun, Sami Khenissi, Olfa Nasraoui, and Patrick Shafto. 2019. Debiasing the human-recommender system feedback loop in collaborative filtering. In Companion Proceedings of The 2019 World Wide Web Conference . 645-651. [50] Adith Swaminathan, Akshay Krishnamurthy, Alekh Agarwal, Miro Dudik, John Langford, Damien Jose, and Imed Zitouni. 2017. Off-policy evaluation for slate recommendation. Advances in Neural Information Processing Systems 30 ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:30 Chen Xu, Xiaopeng Ye, Jun Xu, Xiao Zhang, Weiran Shen, and Ji-Rong Wen (2017). [51] Yong Kiam Tan, Xinxing Xu, and Yong Liu. 2016. Improved recurrent neural networks for session-based recommendations. In Proceedings of the 1st workshop on deep learning for recommender systems . 17-22. [52] Huazheng Wang, Qingyun Wu, and Hongning Wang. 2016. Learning hidden features for contextual bandits. In Proceedings of the 25th ACM international on conference on information and knowledge management . 1633-1642. [53] Huazheng Wang, Qingyun Wu, and Hongning Wang. 2016. Learning hidden features for contextual bandits. In Proceedings of the 25th ACM international on conference on information and knowledge management . 1633-1642. [54] Huazheng Wang, Qingyun Wu, and Hongning Wang. 2017. Factorization bandits for interactive recommendation. In Thirty-First AAAI Conference on Artificial Intelligence . [55] Xiting Wang, Yiru Chen, Jie Yang, Le Wu, Zhengtao Wu, and Xing Xie. 2018. A reinforcement learning framework for explainable recommendation. In 2018 IEEE international conference on data mining (ICDM) . IEEE, 587-596. [56] Xin Wang, Steven CH Hoi, Chenghao Liu, and Martin Ester. 2017. Interactive social recommendation. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management . 357-366. [57] David Williams. 1991. Probability with martingales . Cambridge university press. [58] Yao Wu, Jian Cao, Guandong Xu, and Yudong Tan. 2021. Tfrom: A two-sided fairness-aware recommendation model for both customers and providers. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1013-1022. [59] Chen Xu, Sirui Chen, Jun Xu, Weiran Shen, Xiao Zhang, Gang Wang, and Zhenhua Dong. 2023. P-MMF: Provider Max-min Fairness Re-ranking in Recommender System. In Proceedings of the ACM Web Conference 2023 . 3701-3711. [60] Chen Xu, Jun Xu, Xu Chen, Zhenghua Dong, and Ji-Rong Wen. 2022. Dually Enhanced Propensity Score Estimation in Sequential Recommendation. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management (Atlanta, GA, USA) (CIKM '22) . Association for Computing Machinery, New York, NY, USA, 2260-2269. https://doi.org/10.1145/3511808.3557299 [61] Chen Xu, Jun Xu, Yiming Ding, Xiao Zhang, and Qi Qi. 2024. FairSync: Ensuring Amortized Group Exposure in Distributed Recommendation Retrieval. arXiv:2402.10628 [cs.IR] [62] Hong-Jian Xue, Xinyu Dai, Jianbing Zhang, Shujian Huang, and Jiajun Chen. 2017. Deep matrix factorization models for recommender systems.. In IJCAI , Vol. 17. Melbourne, Australia, 3203-3209. [63] Jiangxing Yu, Hong Zhu, Chih-Yao Chang, Xinhua Feng, Bowen Yuan, Xiuqiang He, and Zhenhua Dong. 2020. Influence function for unbiased recommendation. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . 1929-1932. [64] Xiangyu Zhao, Long Xia, Liang Zhang, Zhuoye Ding, Dawei Yin, and Jiliang Tang. 2018. Deep reinforcement learning for page-wise recommendations. In Proceedings of the 12th ACM Conference on Recommender Systems . 95-103. ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2024.",
  "keywords_parsed": [
    "None"
  ]
}