{
  "Recent Advances of Foundation Language Models-based Continual Learning: A Survey": "YUTAO YANG, JIE ZHOU ∗ , XUANWEN DING, TIANYU HUAI, SHUNYU LIU, QIN CHEN, YUAN XIE, and LIANG HE, School of Computer Science and Technology, East China Normal University, China Recently, foundation language models (LMs) have marked significant achievements in the domains of natural language processing (NLP) and computer vision (CV). Unlike traditional neural network models, foundation LMs obtain a great ability for transfer learning by acquiring rich commonsense knowledge through pre-training on extensive unsupervised datasets with a vast number of parameters. Despite these capabilities, LMs still struggle with catastrophic forgetting, hindering their ability to learn continuously like humans. To address this, continual learning (CL) methodologies have been introduced, allowing LMs to adapt to new tasks while retaining learned knowledge. However, a systematic taxonomy of existing approaches and a comparison of their performance are still lacking. In this paper, we delve into a comprehensive review, summarization, and classification of the existing literature on CL-based approaches applied to foundation language models, such as pre-trained language models (PLMs), large language models (LLMs) and vision-language models (VLMs). We divide these studies into offline and online CL, which consist of traditional methods, parameter-efficient-based methods, instruction tuning-based methods and continual pre-training methods. Additionally, we outline the typical datasets and metrics employed in CL research and provide a detailed analysis of the challenges and future work for LMs-based continual learning. CCS Concepts: · Computing methodologies → Natural language processing . Additional Key Words and Phrases: Continual Learning, Foundation Language Models, Pre-trained Language Models, Large Language Models, Vision-Language Models, Survey",
  "ACMReference Format:": "Yutao Yang, Jie Zhou, Xuanwen Ding, Tianyu Huai, Shunyu Liu, Qin Chen, Yuan Xie, and Liang He. 2018. Recent Advances of Foundation Language Models-based Continual Learning: A Survey. In Proceedings of Make sure to enter the correct conference title from your rights confirmation emai (Conference acronym 'XX). ACM, New York, NY, USA, 38 pages. https://doi.org/XXXXXXX.XXXXXXX",
  "1 INTRODUCTION": "Recent advancements in foundation language models (LMs) have set new benchmarks in both natural language processing (NLP) [126, 222, 230] and computer vision (CV) [147, 169, 183]. Foundation LMs encompass three primary categories: Pre-trained Language Models (PLMs) [126], Large Language Models (LLMs) [222], and Vision-Language Models (VLMs) [40]. These models are pre-trained on large, unlabeled datasets to capture rich semantic information, which is then fine-tuned for specific tasks or domains. This strategy not only enhances performance across various applications but also significantly improves the flexibility and adaptability of these models. ∗ Corresponding authors. Authors' Contact Information: Yutao Yang; Jie Zhou, jzhou@cs.ecnu.edu.cn; Xuanwen Ding; Tianyu Huai; Shunyu Liu; Qin Chen; Yuan Xie; Liang He, School of Computer Science and Technology, East China Normal University, Shanghai, China. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. Manuscript submitted to ACM Manuscript submitted to ACM 1 2 Yutao Yang et al. Fig. 1. Comparison between traditional CL and Foundation language models (LMs)-Based CL. Traditional CL Text cannot predict before learning Text Task 1 Task i Task N … … … … Instruction: Create a story with keywords Input: Explorers, Island Output: Explorers landed on an island … Instruction: Extract characters from article Input: John Doe joins the climate summit … Output: John Doe Instruction: Translate sentences into French Input: The quick brown fox jumps … Output: Le renard brun rapide saute … Foundation LMs Adapter 1 Zero/few-shot prediction without training Model 1 Model i Model N … … Task 1 Task i Task N … … Neural Network Foundation Language Models (LMs)-Based CL Adapter 2 Adapter 3 However, despite their strengths, foundation LMs face challenges in dynamic environments where tasks evolve over time. A key issue is 'catastrophic forgetting\" [84], where models lose previously learned knowledge when adapting to new information. Unlike human learning, which is inherently continuous and adaptive [32], foundation LMs generally require retraining to incorporate new data. Effective learning in such environments demands not only the ability to accelerate learning on new tasks (forward transfer) but also to improve performance on previous tasks by integrating newly acquired knowledge (backward transfer). While multi-task learning (MTL) and transfer learning (TL) offer potential solutions, MTL requires all task data to be available upfront, and TL focuses on limited tasks, making both approaches impractical for dynamic, real-world applications. Continual learning (CL) [172, 182], also known as lifelong learning [134] or incremental learning [228], offers an effective solution to these challenges. It aims to develop systems capable of continuously learning and updating without forgetting past knowledge. Recent advancements in CL methodologies have substantially enhanced the adaptability and knowledge retention capabilities of foundation LMs [28, 89, 123]. Notable successes have been documented in diverse downstream tasks, such as aspect-based sentiment analysis [77], dialogue generation [157], text classification [149], visual question answering [137, 217] and so on. Luo et al. [113] conduct an empirical study on catastrophic forgetting (CF) in large language models (LLMs) during continual instruction tuning. The aforementioned works underscore the potential of continual learning to significantly boost the performance of foundation LMs. In the domain of continual learning, there has been a shift from traditional methods to those incorporating foundation LMs (Figure 1). First, foundation LMs has specialized transfer capability to quickly adapt to downstream tasks with only a few samples. Consequently, it is crucial to mitigate the degradation of both the zero-shot transfer and history task abilities while facilitating the acquisition of new skills. Second, due to the substantial number of parameters in foundation LMs, it is crucial to employ parameter-efficient techniques [52], such as prompt tuning [107] and adapters [129], to update parameters without comprehensive retraining. Third, the foundation LMs possess the capability to follow instructions through instructional learning [37, 133], enabling more dynamic and context-aware interactions. This review systematically organizes continual learning strategies and technologies into two main categories: offline continual learning and online continual learning (Figure 2). We begin by defining and explaining the different settings for these two types of continual learning. Offline continual learning includes domain/task/class-incremental CL, while Manuscript submitted to ACM Recent Advances of Foundation Language Models-based Continual Learning: A Survey 3 Fig. 2. Taxonomy of foundation language models for continual learning. Foundation LMs-based CL Offline Continual Learning (§5) Domain-Incremental Learning (§5.1) PLMs-based DIL (§5.1.1) LFPT5 [140], B-CL [79], ELLE [142], AdapterCL [117], RMR_ DSE [92], DEMIX [44] , CLASSIC [77], Pretr [28], CPT [74], C-PT [232], CL-KD [19], PlugLM [26], AEWC [90], DAS [78] LLM-based DIL (§5.1.2) COPF [214], LAMOL [166], RVAE_LAMOL [179], Adapt-Retrieve-Revise [220], Lifelong-MoE [24], DACP [198], CPPO [215], EcomGPT-CT [114], LLM-CL [36], AMA [97] VLMs-based DIL (§5.1.3) S-Prompt [186], VQACL [217], SC-MLLM [101], DIKI [168] Task-Incremental Learning (§5.2) PLMs-based TIL (§5.2.1) PP [149], CTR [76], MeLL [178], LINC [99], ERDA [139], PCLL [223], BiHNet-Reg [70], ConTinTin [204], HMI [118], ACM [219], DYNAINST [127], Conure [210], TERACON [83], EMR [180], ERNIE 2.0 [167], RecyclableTuning [141], RecAdam [23] LLMs-based TIL (§5.2.2) Conpet [165], InstructAlign [15], Continual-T0 [158], DynaMind [39], ELM [64], O-LoRA [184], JARe [135], Robocoder [95], Eureka [115], COPF [213] VLMs-based TIL (§5.2.3) Medical AI [203], CTP [231], ZSCL [226], MoE-Adapters4CL [207], TRIPLET [137], AwoForget [225], SND [208] Class-Incremental Learning (§5.3) PLMs-based CIL (§5.3.1) EPI [188], IDBR [61], PAGeR [174], ENTAILMENT [197], ExtendNER [128], PLE [94], DE&E [193], SRC [104] VLMs-based CIL (§5.3.2) MoE-Adapters4CL [207], VLM-PL [82], Adaptation-CLIP [105], PROOF [229], LGCL [81], ZSCL [226], CLAP [68], GMM [17], RAPF [60], STAR-Prompt [124], DIKI [168], SND [208], AwoForget [225] Online Continual Learning (§6) Hard Task Boundary (§6.1) PLMs-based HTB (§6.1.1) MBPA++ [33], Meta-MBPA++ [189], OML-ER [55], TPEM [44], CID [103], ProgModel [163] VLMs-based HTB (§6.1.2) PEGP [138] Blurry Task Boundary (§6.2) PLMs-based BTB (§6.2.1) MBPA++ [33], Meta-MBPA++ [189], OML-ER [55], TPEM [44], CID [103], S6 [93] VLMs-based BTB (§6.2.2) DKR [30], SIT [181], OLiVia-Nav [130], G-NoCL [160] online continual learning is further divided into methods that address hard task boundaries and those that manage blurry task boundaries (Section 4). To further clarify the relationship between these strategies and model architectures, we group the methods based on three major model types: Pre-trained Language Models (PLMs), Large Language Models (LLMs), and Vision-Language Models (VLMs). These categories reflect the distinct requirements for continual learning in each architecture and emphasize the role these models play in shaping learning strategies. Additionally, we classify the methods into four key approaches: traditional continual learning methods, continual pre-training methods, parameter-efficient tuning methods, and instruction-based methods (Section 5, 6). This categorization highlights the different techniques used to address challenges like catastrophic forgetting and to improve knowledge transfer in continual learning scenarios. Finally, we static the main datasets from various perspectives (Section7) and review the key metrics to evaluate the forgetting and transferring of the models (Section 8). The main contributions of this survey paper can be summarized as follows: · We thoroughly review the existing literature on foundation LMs-based CL approaches, which integrate foundation LMs with CL to learn new knowledge without retraining the models. It is quite different from traditional CL since foundation LMs have great abilities of transfer learning, zero-shot and instruction following with huge parameters. · We give the definitions of different settings and categorize these studies into various classes to better understand the development of this domain. In addition to the traditional methods like replay, regularization and parameterisolation-based algorithms, we also summarize the works about continual pre-training methods, parameter-efficient tuning methods and instruction tuning-based methods. · We provide the characters of existing datasets for CL and present the main metrics to evaluate the performance of preventing forgetting and knowledge transfer. Furthermore, we discuss the most challenging problems of foundation LMs-based CL and point out promising future research directions in this field. Manuscript submitted to ACM 4 Yutao Yang et al.",
  "2 RELATED SURVEYS": "",
  "2.1 Continual Learning": "Early examinations have provided broad coverage, as observed in surveys such as Parisi et al. [134]. Recently, Wang et al. [182] conduct a comprehensive survey that categorizes five key strategies in CL: regularization-based, replaybased, optimization-based, representation-based, and architecture-based approaches. This survey reflects an effort to organize and understand the diverse methodologies employed in the field. Notably, there is also a growing focus on class-incremental setting [6, 120, 228] and replay-based approaches [53]. Recent surveys [164, 194, 224] provide comprehensive overviews of continual pre-training, continual domain-adaptive pre-training, continual instruction tuning, and continual alignment. In particular, Shi et al. [164] provides an in-depth overview of CL, specifically in LLMs, introducing vertical CL (adapting from general to specific domains) and horizontal CL (adapting across time and domains). Additionally, one notable approach in continual alignment is Reinforcement Learning from Human Feedback (RLHF) [195], which incorporates human feedback to optimize model outputs for better task adaptation.",
  "2.2 Continual Learning for Computer Vision": "In the realm of computer vision (CV), De et al. [32] concentrate on task incremental classification and offer significant contributions including a taxonomy, a new framework for balancing stability and plasticity, and an extensive comparison of continual learning methods against baselines. Qu et al. [143] present a comprehensive examination of continual learning, highlighting its vital role in the accumulation of knowledge from sequential data streams. This research explores multiple methods including regularization, knowledge distillation, memory-based approaches, and more, categorizing them by their characteristics and applications in CV. Moreover, Mai et al. [119] focus on the realm of online continual learning within the image classification task. This study evaluates the efficacy of state-of-the-art methods across diverse memory and data configurations. Masana et al. [120] conduct a comprehensive evaluation of class-incremental methods for image classification, involving large-scale datasets and various network architectures. Belouadah et al. [6] focus more on class-incremental learning algorithms specifically for visual tasks, defining key properties of these algorithms, formalizing the class-incremental learning problem, and providing an evaluation framework for thorough analysis.",
  "2.3 Continual Learning for NLP": "Biesialska et al. [7] address the challenge of continual learning within Natural Language Processing (NLP), wherein conventional architectures struggle to accommodate new tasks without compromising previously acquired knowledge. In a similar vein, Ke et al. [75] offer a focused survey on continual learning within the NLP domain, providing a comprehensive examination of various continual learning settings, methodologies, and challenges. This work presents an in-depth analysis of state-of-the-art approaches and extends original CL settings to be more general and up-to-date. Gogoulou et al. [45] study the pros and cons of updating a language model when new data comes from new languages the case of CL under language shift. They feed various languages into the model to examine the impact of pre-training sequence and linguistic characteristics on both forward and backward transfer effects across three distinct model sizes.",
  "2.4 Continual Learning for Other Domains": "Recent surveys, surveys like [91, 161, 216] explore advancements in incremental learning across other domains. Zhang et al. [216] focus on neural recommendation systems, particularly introducing the Incremental Update Recommendation Systems (IURS) to bridge the gap between academic research and industrial applications. They discuss the unique Manuscript submitted to ACM Recent Advances of Foundation Language Models-based Continual Learning: A Survey 5 . . . . . . Continual Learner Task i Task N Task 1 . . . . . . Domain 1 Domain i Domain N Task - ID . . . . . . Task - ID . . . . . . Task - ID . . . . . . Domain 1 Domain i Domain N Task 1 Task i Task N Task-Incremental Learning Task 1 Task i Task N Class-Incremental Learning Domain-Incremental Learning Training Testing Fig. 3. The setting of different offline continual learning tasks, including task-incremental learning, class-incremental learning and domain-incremental learning. The samples with different classes (domains) are marked with various shapes (colors). challenges of IURS compared to traditional Batch Update Recommendation Systems (BURS) and offer a detailed review of existing literature and evaluation methods. Shaheen et al. [161] offer a comprehensive overview of the CL methods in real-world contexts, highlighting efficient learning algorithms for handling large sequential datasets within resource constraints and exploring the applicability of CL to autonomous systems. Lesort et al. [91] investigate CL in robotics, defining it as a dynamic learning paradigm where both data distribution and objectives evolve. They also emphasize the challenges in evaluating CL algorithms and introduce a new framework with tailored metrics for assessing CL methods. Continual learning across domains aims to mitigate catastrophic forgetting by integrating new tasks while preserving knowledge from prior ones [94, 128]. However, domains like NLP, CV, and robotics exhibit significant differences. CV methods [190, 191] typically focus on classification, while NLP involves more complex tasks such as information extraction and text generation. Robotics tasks evolve sequentially, relying on decision-making and state transitions [3, 84]. NLP models, primarily based on PLMs or LLMs, excel in few-shot and transfer learning, whereas CV tasks commonly utilize traditional neural networks (e.g., ResNet). Recently, Vision-Language Models (VLMs) have been introduced in CV to enhance continual learning. Additionally, the pre-trained transformers in NLP enables parameter-efficient strategies like adapter and prompt tuning, allowing adaptation to new tasks with minimal retraining [74, 117]. This paper centers on the advancements in CL as applied to foundation LMs, which have obtained success in the fields of NLP and multimodal. We categorize existing works into offline and online CL based on PLMs, LLMs, and VLMs.",
  "3 TYPICAL FOUNDATION LANGUAGE MODEL": "Pre-trained Language Models (PLMs). PLMs are trained on large-scale textual data using techniques such as Masked Language Modeling (MLM) and Next Sentence Prediction (NSP) for various tasks, including text classification, sentiment analysis, and named entity recognition. Notable examples of PLMs include ELMo [152], BERT [34], early versions of the GPT series [145], and RoBERTa [108]. PLM-based methods aim to mitigate catastrophic forgetting when learning new tasks or domains within a closed-world setting. Continual learning for PLMs focuses on parameter-efficient tuning and replay-based strategies to reduce resource consumption and prevent catastrophic forgetting. Approaches like adapter-based tuning, prompt-based methods, and knowledge distillation are popular because they update the Manuscript submitted to ACM 6 Yutao Yang et al. Fig. 4. The setting of different online continual learning tasks, including hard task boundary arriving and blurry task boundary arriving. The samples with different classes (domains) are marked with various shapes (colors). Hard Task Boundary Blurry Task Boundary Task 1 Task 2 Task 3 model with few parameters. This ensures that PLMs remain feasible to deploy on standard hardware while keeping computational costs low. Large Language Models (LLMs). LLMs are significantly larger in scale than PLMs, often containing billions or even trillions of parameters. These models are trained on vast datasets using language modeling techniques and excel in tasks such as generation, instruction learning, and in-context learning. LLMs are typically exposed to a broader range of complex tasks, from domain-specific corpora to open-domain challenges. Prominent examples include OpenAI's GPT-3 [11], GPT-4 [1], LLaMA [170], and others. Due to the substantial memory and computational demands associated with LLMs, continual learning strategies for LLMs often focus on optimizing memory efficiency (e.g., orthogonal low-rank adaptation, recyclable tuning) and selectively updating parameters (e.g., Mixture-of-Experts, dynamic task selection), as retraining or fine-tuning the entire model is typically cost-prohibitive. Additionally, LLMs require techniques to mitigate catastrophic forgetting while managing task diversity and retaining foundation knowledge. Vision-Language Models (VLMs). VLMs are designed to process and integrate multimodal information, such as text and vision, by building on PLMs or LLMs for tasks like image captioning, visual question answering, and visual reasoning. Notable examples include CLIP [144] and ALIGN [69]. VLMs must effectively handle multimodal data, which necessitates continual learning methods that can efficiently manage cross-modal learning. Continual learning for VLMs often emphasizes alignment across multiple modalities using specialized modules such as cross-modal fusion techniques, attention mechanisms, and lightweight adapters. Additionally, methods that focus on retaining modality-specific knowledge and enabling parameter-efficient fine-tuning (e.g., Adaptation-CLIP [105], MoE-Adapters4CL [207]) are essential due to the high costs associated with continual learning across both vision and language domains.",
  "4 SETTINGS AND LEARNING MODES OF CL": "",
  "4.1 Basic Formulation": "Continual learning is an advanced method in machine learning. Within this framework, the model is sequentially trained across a diverse array of tasks denoted as 𝑡 within the set 𝑇 = { 1 , 2 , ..., 𝑁 } , where each task 𝑡 is associated with its individual dataset 𝑋 𝑡 = {( 𝑥 ( 𝑡 ) 𝑖 , 𝑦 ( 𝑡 ) 𝑖 )} | 𝑋 𝑡 | 𝑖 = 1 . Here, 𝑥 ( 𝑡 ) 𝑖 represents an individual training example, and 𝑦 ( 𝑡 ) 𝑖 denotes the corresponding class label for task 𝑡 , while | 𝑋 𝑡 | indicates the total number of samples in task 𝑡 . However, the data distributions between any two tasks 𝑡 and 𝑡 ′ are distinct ( 𝑝 ( 𝑋 𝑡 ) ≠ 𝑝 ( 𝑋 𝑡 ′ ) for all 𝑡 ≠ 𝑡 ′ ). This distinction presents a fundamental challenge in managing the diversity of data distributions across multiple tasks. This setup necessitates the model to learn new knowledge while retaining past information. Manuscript submitted to ACM Recent Advances of Foundation Language Models-based Continual Learning: A Survey 7 Continual learning encompasses two principal paradigms: offline and online continual learning. These paradigms define how data arrives and how the model updates its knowledge over time. · Offline Continual Learning: This setting involves learning across a series of tasks, with each task fully presented before handling the next task. For each task 𝑡 , the model trains on the entire dataset 𝐷 𝑡 through multiple epochs. The model progresses to task 𝑡 + 1 only upon achieving the desired proficiency on task 𝑡 . · Online Continual Learning: This setting operates within a dynamic framework wherein the model learns knowledge from a stream of data points or mini-batches presented sequentially. Additionally, the model lacks access to the entire dataset for a given task. This setting closely mirrors real-world scenarios characterized by continuous data flow, compelling the model to adapt in real time.",
  "4.2 Typical Scenarios": "4.2.1 Offline Continual Learning. Offline CL (Figure 3) comprises three principal scenarios, each distinguished by distinct characteristics: Domain-Incremental Learning (e.g., LFPT5 [140], LAMOL [166], S-Prompt [186]), Task-Incremental Learning (e.g., PP [149], Conpet [165], ZSCL [226]), and Class-Incremental Learning (e.g., EPI [188], IDBR [61], GMM [17]). · Domain-Incremental Learning (DIL): The model aims to process diverse data distributions. Specifically, in DIL, while the data distributions 𝑝 ( 𝑋 𝑡 ) in task 𝑡 and 𝑝 ( 𝑋 𝑡 ′ ) in task 𝑡 ′ are different, their task types and class labels remain consistent. The task identities (task IDs) are not required. · Task-Incremental Learning (TIL): The model is designed to handle a series of tasks, each with unique objectives. The classes within these tasks may or may not be disjoint. The boundaries of each task are clear, and task IDs are provided during both the training and testing phases. · Class-Incremental Learning (CIL): The model is designed to continually learn new class information while retaining knowledge of previously learned classes. For tasks 𝑡 and 𝑡 ′ , while they might share the same task type (such as classification), their class sets 𝐶 𝑡 and 𝐶 𝑡 ′ are distinct. Moreover, the task IDs are only available during training. In summary, DIL concentrates on adapting the model to the shifts in input data distributions while maintaining consistency in tasks and classes. TIL necessitates the model's ability to learn and retain task-specific knowledge over successive tasks. On the other hand, CIL highlights the gradual integration of new classes into the model's recognition capabilities without compromising knowledge of previously learned classes. 4.2.2 Online Continual Learning. In online continual learning (Figure 4), the existing researches are categorized into two configurations based on the arrival pattern of tasks: \"Hard Task Boundary\" (e.g., MBPA++ [33], PEGP [138]) and \"Blurry Task Boundary\" (e.g., SIT [181], G-NoCL [160]): · Hard Task Boundary: The arrival of tasks follows a strictly structured and sequential process. Data from the preceding task is completely processed before transitioning to the next task, ensuring no overlap of data between tasks. · Blurry Task Boundary: The distinction between tasks is less clear, similar to real-world scenarios. Data from different tasks are intermixed, making it difficult to pinpoint when one task ends and another begins. In both setups, the main challenge lies in achieving the balance of learning new data while preserving previously gained knowledge, often termed as catastrophic forgetting. Numerous approaches, such as experience replay [142, 166], elastic weight consolidation (EWC) [84], and progressive neural networks [76, 79], have emerged to address this issue. Each method comes with its unique strengths and weaknesses upon the task arrival configuration. Manuscript submitted to ACM 8 network. First, the lookup (addressing) stage would compute the matching degree between x and each key. In the second stage, x would be transformed 4.1 Differential Plug-in Memory In this paper, we view n-th knowledge d n = 1 2 d n Section 5.1. We omit the interleaving dense layers to make this figure simple and clear. We need to decide how to expand and initialize new exImplicit Regularization via Distillation from Old ExTo alleviate catastrophic forgetting, numerous methods have been proposed for continual learning. One of the most effective methods is to leverage a buffer of exemplars from old tasks to perform either a rehearsal or a distillation when tuning the whole network to learn new tasks (see Non-Prompting Fig. 5. Frameworks in DIL: PlugLM (PLM-based) [26], Lifelong-MoE (LLM-based) [24], S-Prompts (VLM-based) [186].",
  "Yutao Yang et al.": "Self-Attention Layer FFN Layer Knowledge Attention MIPS [MASK] is the first transformer-based PLM. End-to-End Backprop. Knowledge Encoder Wikipedia S2ORC PubMed Keys Values Figure 1: Overview of our PlugLM. We replace FFN in PLM with a Differentiable Plug-in key-value Memory (DPM) by which PLM could store and leverage knowledge in an explainable manner. (a) PlugLM Lifelong Language Pretraining with Distribution-Specialized Experts Expert & Expert 9 (;<=) 𝒑 (\"%&) 𝒑 (\") 𝒙 (\") Expert 9 (;<=) @& Expert 9 (;) Top2-Gating 9 (;<=) ⋯⋯ ⋯ Expert/Gating Expansion Expert/Gating Freeze Output Regularization 𝒑 (\"@&) 𝒑 (\"%J) Lifelong Pretraining Stream of Corpus Distribution 𝒙 (\"%&) 𝒙 (\"%J) 𝒙 (\"@&) Top2-Gating 9 (;) 𝓜 Inputs MoE Outputs 𝜽\" 𝜽N:\"%& Figure 2: Overview of our lifelong pretraining method for the MoE model ( M ): 1) When pretraining on each data distribution ( x ( t ) ), we expand the number of experts and gatings (from E ( t - 1) to E ( t ) ) for larger model capacity; 2) We freeze the pretrained old experts and gatings; 3) We further regularize the MoE on the output level to avoid the catastrophic forgetting. Embedding, dense, and attention layers (omitted in this figure) are shared across all data distributions. See details of our method in Section 3 and pretraining settings in (b) Lifelong-MoE s s-1 s-2 s-1 s-2 s-3 Prompts Classifiers Pretrained & Fixed Data Stream Current Session s L2P DyTox Non-Prompting Methods Proposed S-Prompts Session s-1 Session s+1 Extractor s-3 Extractor Extractor Extractor Feature Space Methods Image Tokens Instance Selection Session s-2 Session s-3 s-3 s-2 s-1 s s-3 s-2 s-1 s s-3 s-2 s-1 s s s s-1 s-2 s-3 Previous Methods Proposed S-Prompts Incremental Update (Dependent) Incremental Insert (Independent) Instance features of class +/- in Session s-1 Previous trained & Now Fixed Incremental Tuned / Learnable Instance features of class +/- in Session s Domain centers of s-1 Domain centers of s Figure 1: Comparison of the proposed S-Prompts paradigm against non-prompting methods, and prompting methods (L2P [66], DyTox [14]). Exiting methods generally learn sequential tasks/sessions/domains dependently, producing a single feature space where the classes from different domains are less separable (more forgetting or worse transferring). In contrast, the proposed S-Prompts learn the tasks independently. This paradigm leads to one subspace for every domain, making the classes more separable (less/no forgetting and better transferring). (c) S-Prompts n as consecutive tokens from unla- , ..., t t , t | | beled corpora as in Guu et al. (2020). For each n n d However, it is often more desired that the exemplars of old tasks are not stored for better data security perts/Gatings We try to find possible ways to implicitly perts and gatings. We empirically observed that randomly Methods in Fig.1). Both the rehearsal and the distillation typically decrease the forgetting degree. n , by the weighted sum of values according to the { } formance, potentially due to mismatched gradient direc- MemoryNetwork",
  "distribution of the matching degree in the first stage. We can formally define it as: we get its dense representation h n from a knowledge encoder KnowEncoder ( · ) : 5 OFFLINE CONTINUAL LEARNING": "initializing expanded experts and gatings leads to poor per- tions and magnitudes from new experts/gatings and pre- h Therefore, inspired by the trained dense/attention layers. AttnPooling = ( ( d ) + EToken x K /latticetop softmax (2) ) = ) ( ( V x n n EPos d (3) n ( )) is to initialize each new expert and gating dimension from 'Net2WiderNet' approach (Chen et al., 2015), a better way ed ones, helping both the preservation of old knowl-",
  "where K , V ∈ R d 2 × d 1 . Comparing equation (1) and (2), we could find that the FFN is an unnormalized version of MemoryNetwork. The keys in where AttentivePooling function (Xu et al., 2021; Cheng et al., 2023a) corresponds to a trainable pre 5.1 Domain-Incremental Learning": "FFN are pattern detectors and would be activated edge and the warming-up for the subsequent pretraining. pattern detector aggregating information from a se- quence of input. And EToken and EPos denote token embedding and positional embedding. regularize parameters, including the newly expanded ex- Therefore, our focus is shifted to the challenging in this paper. exemplar-free DIL perts, gating dimensions, embeddings, and dense/attention and privacy. Moreover, storing a large amount of exemplars might run out available memories. layers. Inspired by (Li & Hoiem, 2017), we choose to distill One of the most promising solutions to the exemplar-free DIL problem is to learn a set of prompts 4 denoting the model as the knowledge from old experts and gatings. Specifically, over transformers that achieve the state-of-the-arts in a wide range of areas. domain-specific knowledge is stored by a prompt pool, and hence a rehearsal buffer is no longer perplexity loss , we minimize the combination of M In this solution, the mandatory to mitigate forgetting. KL divergence Perp L KL For instance, two recent prompting methods [14, 66] 5 learning task/domain-specific prompts dependently across domains L (for the next-token prediction) and the aim at of outputs from two models: In [14], the prompts are added task by task, and the newly added prompts share the same task- transformer using a distillation loss to keep a balance between the new prompting and the old ones. the num- in an incremental update fashion. attention learning with the old prompts. This requests a joint tuning on all the prompts and the entire In [66], a pool of prompts is initialized at the right beginning, and a set of the prompts from the A vanilla expansion strategy would be to (1) KL Perp λ = + pool are selected for each instance. The instance-level prompts generally share task-level knowledge Then we duplicate manner (Geva et al., 2021; Sukhbaatar et al., 2019). ber of experts in order to fully leverage and inherit all the use two independent mapping functions to project L L L",
  "only when certain patterns occur in the input. This explains how FFN stores knowledge in a key-value 5.1.1 PLMs-based DIL.": "among any similar instances, by jointly tuning the selected prompts and the whole classifier, with - pretrained knowledge. However, this will lead to an expo- d (2) 1 i t 0: Perp +1 t 0: i , θ , θ = x ( log P ( x )) , θ the pre-trained transformer being fixed. While using different prompting approaches, [14, 66] both PlugLM 4 The overall architecture of PlugLM is illustrated in Figure 1. Because FFN is essentially a key-value memory network (Geva et al., 2021; Dai et al., 2022a; Meng et al., 2022), PlugLM creatively decouples the knowledge storage from model parameters by replacing 2 FFN with a Differential Plug-in key-value Memory, DPM (§4.1) and conducting knowledge retrieval in DPM with knowledge attention (§4.2) for explicit knowledge usage instead of storing all knowledge implicitly in the model parameters. In §4.3, we detailedly explain how PlugLM is trained in both pre-training and finetuning stages. 2 Because different layers in transformer capture different knowledge, the lower layer for shallow patterns while the upper layers for more semantic ones (Geva et al., 2021; ? ), we only consider replacing FFN in Top-L layers with DPM while keeping FFN in the lower layers untouched to encode the intrinsic language understanding knowledge as detailed in §5.4. k n = W k · h n + b k (4) v n = W v · h n + v k (5) where W k , W v , b k and v k are trainable parameters. And DPM is a triplet of 〈 D , K , V 〉 : D = { d 1 , d 2 , ..., d | D | } (6) K = { k 1 , k 2 , ..., k | D | } (7) V = { v 1 , v 2 , ..., v | D | } (8) 4.2 Memory Fusion For hidden states h ∈ R l × d from Self-Attn, FFN would transform h with unnormalized key-value memory as in Equation (1). Our key insight is that instead of interacting with unnamed vectors in FFN, we conduct Maximum Inner Product Search (MIPS) to retrieve knowledge in natural language from 〈 D , K , V 〉 where each triplet corresponds to one knowledge along with its key and 14290 work, we choose to partially expand the number of experts and gating dimensions. We study differ expansion choices, and will show that by expanding a limited number of experts for each data distribution we can achieve competitive performance without further introducing extra model size. That means, we selectively expand (and only expand) the experts when necessary to accommodate incoming new data distribution that is not covered by the older corpora. We do not increase the number of dense layers. 3.3. Expert/Gating Regularization The purpose of our expert/gating expansion is to enlarge the model capacity for incoming new data distributions. At this moment, pretrained experts and gatings store the knowledge about previous distributions. Continuous training will still erase these pretrained knowledge and overfit on the new data, which is not desired. In this section, we propose two approaches to effectively preserve old knowledge. L KL = -∑ x i ∈ X M ( x i , θ 0: t -1 , θ d ) log ( M ( x i , θ 0: t -1 , θ t , θ d )) . (3) θ d indicates parameters for dense layers that are shared across distributions, θ 0: t -1 indicates parameters for old experts and gatings, and θ t for parameters of newly expanded experts and gating dimensions. x is the embedding of the current token and X represents the whole corpus of current data distribution. This auxiliary loss L KL will implicitly avoid the model parameters from being updated too far from pretrained ones. It is multiplied with a scaling factor λ to control its impact to the original pretraining loss value, and we will study different λ s. Explicit Regularization via Partial Experts and Gatings Freezing To explicitly preserve pretrained knowledge, an intuitive way is to completely freeze neurons specifically responsible for previous data distributions, and only allow parameters for the current distribution to be updated. In our method, the dense/attention layers are always being optimized, since they are trained to fit all data distributions. 4 paradigms generally result in a tug-of-war (or a zero-sum game) , in which one side's gain is equivalent to the other's loss, as studied in [56, 33]. In other words, these paradigms generally accumulate new knowledge in the same feature space, which is very likely to mix up the subspaces of old/new knowledge resulting in less separable classes (more forgetting or worse transferring) (Fig.1). In this paper, we explore a rule-breaking idea to instead play a win-win game, i.e., learning the prompts independently across domains so that the prompting can achieve the best for each domain. The excellent generalization capability of the transformers and the strong transfer learning ability of the recently appeared prompting mechanisms allow for the realization of this idea. In particular, we introduce a new learning paradigm that learns the prompts independently domain by domain, and incrementally inserts the learned prompts into a pool. As shown in Fig.1, compared to the other prompting and non-prompting methods, the new paradigm suggests merely tuning the current domain4 For simplicity and consistency, we use 'prompt' to represent 'prompt token' for context/domain knowledge encoding, and we utilize 'token' to denote those normal tokens on images and classes, throughout the paper. 5 A concurrent work [12] learns class-level prompts dependently on transformers for the CIL problem. 2 Traditional Methods. Continual Learning methods are increasingly used with Pre-trained Language Models (PLMs), adopting various techniques such as replay, regularization, and parameter-isolation. Driven by regularization, RMR_DSE [92] optimizes recall and uses domain drift estimation to manage embedding space shifts across domains, effectively reducing catastrophic forgetting in seq2seq tasks under memory constraints. CL-KD [19] uses a Teacher-Student model to transfer language knowledge efficiently. AEWC [90] is an improved EWC algorithm, where the importance can be computed in an online manner. DEMIX [50] is a parameter-isolation approach, utilizing a collection of domain-specific experts for dynamic domain adaptation. It offers superior performance in both in-domain and out-of-domain tasks, providing strong generalization to unseen domains and promising improved adaptability and scalability in multi-domain settings. Lastly, PlugLM [26] (Figure 5a) features a differentiable plug-in memory (DPM) for domain-adaptive training, with a flexible key-value structure that decouples knowledge storage from model parameters, enabling more effective knowledge management in domain-adaptive training. ∑ nentially increasing model size, which is not scalable. In our ∈ follow a commonly-respected principle that the prompting should keep sharing across tasks/domains. i X x L - |M n to the key space and value space: h However, the sharing-driven dependent prompt tuning paradigm as well as many of the non-prompting Continual Pre-training Methods. Pretr [28] formalizes continual pre-training, where models are continuously pretrained on sequential data streams before fine-tuning for downstream tasks. DAS [78] offers an effective solution for continual domain-adaptive pre-training (DAP-training) of LMs. By integrating soft-masking, KL-divergence loss, and contrastive learning, it mitigates catastrophic forgetting and enhances knowledge transfer between domains. Parameter-Efficient Tuning Methods. Due to the huge parameters of LMs, parameter-efficient tuning methods like adaptors [56, 136] and p-tuning [107] are used for domain-incremental CL [77, 79, 117, 232]. The adapter architecture incorporates a skip-connection to minimize the number of parameters. A notable exemplar of this approach is AdapterCL [117], which employs residual adapters tailored for task-oriented dialogue systems. This makes it ideal for real-world deployment with frequent domain updates without the need for full retraining. In a related vein, B-CL [79] sets a new standard in Aspect-Based Sentiment Classification by utilizing CL adapters within capsule network architectures, demonstrating that backward knowledge transfer significantly boosts performance while preserving task-specific information. CLASSIC [77] addresses catastrophic forgetting by leveraging adapters in BERT, employing a contrastive CL strategy that facilitates knowledge transfer across tasks without requiring task identifiers during testing. Furthermore, CPT [74] incorporates CL-plugins in RoBERTa, effectively improving end-task performance by managing domain-specific knowledge transfer, particularly in scenarios with limited labeled data. Prompt tuning [107], or P-tuning, introduces trainable prompts into the sequence of input word embeddings, while the LM remains frozen. LFPT5 [140] integrates prompt tuning, pseudo-labeled samples, and KL divergence to excel in lifelong few-shot language learning (LFLL). The use of pseudo-labeled samples and KL divergence helps maintain label Manuscript submitted to ACM · · Recent Advances of Foundation Language Models-based Continual Learning: A Survey 9 consistency between models, though task transferability requires careful attention. Similarly, C-PT [232] enhances knowledge transfer between tasks in dialogue systems through prompt initialization, query fusion, and memory replay, outperforming baselines like EWC, Replay, and AdapterCL, while being efficient in parameter usage and memory. Instruction Tuning-based Methods. Instruction tuning-based methods involve transforming a given task into natural language instructions. ELLE [142] incorporates expanding streaming data into PLMs with two key components: (1) function-preserved model expansion, which enhances knowledge acquisition efficiency by changing the width and depth of PLM, and (2) pre-trained domain prompts, which enhance downstream task adaptation by utilizing domain-specific knowledge from the pre-training phase.",
  "5.1.2 LLMs-based DIL.": "Traditional Methods. In Lifelong Language Learning (LLL), a key challenge is training models on sequential NLP tasks while preserving knowledge from previous tasks. LAMOL [166] tackles this by generating pseudo-samples from past tasks during new task training, effectively reducing knowledge loss without additional memory or computational overhead. RVAE_LAMOL [179] builds on this by utilizing a variational autoencoder (RVAE) to map tasks into a unified semantic space, while introducing Alternate Lag Training (ALT) and an identity task to improve stability and task-specific sample generation. Efficiently and stably aligning with dynamic human preferences is challenging, especially when minimizing retraining costs. CPPO [215] addresses this issue by integrating sample-wise weighting into the PPO algorithm, effectively balancing policy learning and knowledge retention, particularly in adaptive tasks like summarization and question-answering. Similarly, COPF [214] reduces reliance on human feedback by regularizing the current policy with past optimal policies. It is well-suited for handling unlabeled data, making it ideal for real-world applications with limited labeled datasets. Furthermore, AMA [97] addresses the \"alignment tax\"-the trade-off between maximizing alignment rewards and mitigating forgetting-by interpolating between pre- and post-RLHF model weights. This interpolation identifies optimal layer-wise weight combinations, balancing \"alignment tax\" across various RLHF algorithms. Continual Pre-training Methods. Large language models (LLMs) excel in open-domain tasks but face significant challenges in domain-specific applications, such as insufficient domain knowledge, limited capacity to leverage that knowledge, and inadequate adaptation to specialized data formats. To overcome these limitations, continual pre-training [25, 114, 198] has emerged as a promising approach, enabling LLMs to better fit domain-specific tasks. For example, AdaptLLM [25] transforms raw corpora into reading comprehension texts, but it was found that while pre-training on raw data enhances domain knowledge, it hampers the model's ability to answer questions effectively. As an alternative to full retraining, Domain-Adaptive Continual Pre-training (DACP) leverages large domain-specific corpora to continually train models, although it involves high computational costs. DACP [198] introduces Efficient Task-Specific (ETS-DACP) and Task-Agnostic (ETA-DACP) strategies to optimize performance by focusing either on task-specific foundational models or by selecting the most informative domain samples. Given the high cost of training from scratch and limited annotated data, EcomGPT-CT [114] explores continual pre-training on unlabeled general and E-commerce corpora, employing a data-mixing strategy to better handle semi-structured data. Experimental results across multiple tasks show continual pre-training and data-mixing strategies improve few-shot and zero-shot performance. Manuscript submitted to ACM 10 Yutao Yang et al. Parameter-Efficient Tuning Methods. Lifelong-MoE [24] (Figure 5b) uses a Mixture-of-Experts (MoE) architecture, expanding capacity by adding new experts while freezing prior experts and gating mechanisms. It surpasses existing methods with better few-shot performance on 19 NLP tasks. Instruction Tuning-based Methods. Adapt-Retrieve-Revise (ARR)[220] aims at reducing hallucinations in specialized domains such as the Chinese legal domain. It consists of three steps: adapting a 7-billion-parameter language model for initial responses, retrieving corroborative evidence from an external knowledge base, and integrating these to refine the final response with GPT-4. Furthermore, LLM-CL [36], designed for aspect-based sentiment analysis, employs task-specific prompts and independently learns both domain-shared and domain-specific knowledge. 5.1.3 VLMs-based DIL. Vision-language models (VLMs) excel in domain-incremental learning, particularly through methods like S-Prompt [186] (Figure 5c), which independently learns prompts across diverse domains using pre-trained VLMs. This approach introduces innovative image and language-image prompt acquisition techniques, utilizing a unified cross-entropy loss during training and a k-nearest neighbors (K-NN) method to identify domains during inference. In visual question answering, VQACL [217] enhances multimodal tasks through a dual-level task sequence. This framework uses a compositionality test to assess generalization to new skill-concept combinations and employs a representation learning strategy that differentiates sample-specific (SS) features, capturing unique input attributes, from sample-invariant (SI) features, which retain essential characteristics. DIKI [168] preserves pre-trained knowledge by using a residual attention mechanism to inject new information into a frozen backbone, minimizing interference. A distribution-aware calibration scheme ensures smooth integration of new knowledge while maintaining the VLM's zero-shot capabilities. SC-MLLM [101] is a robotic manipulation framework that utilizes an exponential moving average (EMA) for continuous learning, reducing the risk of catastrophic forgetting.",
  "5.2 Task-Incremental Learning": "",
  "5.2.1 PLMs-based TIL.": "Traditional Methods. Drawing inspiration from neurobiological mechanisms, HMI [118] (Figure 6a) integrates compressed representations of prior training instances, providing selective guidance for generating training samples. Unlike traditional Continual Relation Learning (CRL) that rely on large labeled datasets. Continual Few-Shot Relation Learning (CFRL) focuses on learning new relational patterns with minimal labeled data. ERDA [139] tackles this challenge by selecting informative samples from unlabeled data and enforcing relational constraints in the embedding space. As for Lifelong Relation Extraction, EMR [180] utilizes a working memory mechanism to selectively replay stored samples, helping the model retain prior knowledge while learning new tasks. Learning user representations is fundamental for personalized recommender systems, but current approaches often train separate models for each task, which leads to inefficiency. To improve this, Conure [210] introduces a framework that manages multiple tasks by pruning less critical parameters to accommodate new, task-specific ones. This method allows for positive transfer learning and preserves essential parameters to avoid knowledge loss. However, learning task-specific user representations for each task is impractical. Recent studies focus on universal user representations, which capture generalized user traits relevant to multiple tasks. TERACON [83] addresses this challenge by employing task-specific soft masks to isolate parameters and ensure effective knowledge retention. Given that PLMs often suffers catastrophic forgetting when applied to CL scenarios, CTR [76] tackles this by utilizing innovative techniques such as CL-plugins and task masking, which significantly improve knowledge transfer while Manuscript submitted to ACM Recent Advances of Foundation Language Models-based Continual Learning: A Survey 11 [GEN] Context Question Answer [ANS] [EOS] GPT-2 Answer [GEN] Context Question Answer [ANS] [EOS] GPT-2 Context Question Answer [ANS] [CLS] Context Question BERT h c h' encode (PQ) decode (PQ) store retrieve [GEN] Context Question Answer [ANS] [EOS] GPT-2 Answer [GEN] Context Question Answer [ANS] [EOS] GPT-2 Context Question Answer [ANS] LM (b) HMI-LAMOL (a) LAMOL QA QA LM M 1 M 2 M i ・・・ Hippocampus Module Figure 1: (a) is the overview of the LAMOL framework. The top is the learning QA to solve tasks and the bottom is the learning LM to generate pseudo-samples. (b) is the proposed HMI implemented on LAMOL. We introduce a hippocampus module illustrated on the left. (a) HMI External Resource Inference Engine Knowledge Metabolism Short-term Memory Time Decay Long-term Memory Time Persist Operators Coordinator Searcher Browser Responder Discriminator More Operators 🤔 : The 1995 Tooheys 1000 driver who has second-to-last in the Tooheys Top 10 was born where? 🗣 : The driver who finished second-to-last in the 1995 Tooheys Top 10 is Tony Longhurst. He was born in Sydney, Australia. <Operator, Args> Results Context Knowledge Query Results Memory Manager Knowledge Retrieval Knowledge Embedding Knowledge Representation Feedback Score Update Query Results LLMs Figure 1: The system overview of DynaMind. Coordinator The 1995 Tooheys 1000 driver who has second-to-last in the Tooheys Top 10 was born where? (Vaswani et al., 2017). To uphold the close align- ment with the LLMs' semantic space, we utilized the embeddings generated by LLMs as the knowl- (b) DynaMind Con$nual Task ! Task ! + # How many skiers? Is the TV on? Is this a laptop ? Inference Training ① :Decoupled Visual / Textual / Fusion Prompts :Decoupled Expert / General Prompts · · · · · · · · · · · · Is this a laptop? Input query-key match key-prompt pair :Visual / Textual / Fusion Keys / / / / / Query Learning ② ③ Input features ! ! ! !\"# F ① Modality-Interaction with Matrix ! ! ② Task-Interaction: Classifier Textual Encoder Visual Encoder Fusion Encoder = × + × + × ( ⨀ ) - cos , ③ Query and Match: ( ) Classifier Textual Encoder Visual Encoder Fusion Encoder Classifier Textual Encoder Visual Encoder Fusion Encoder : Figure 3: The TRIPLET framework. Left: during training, the pre-trained encoders are frozen, and parameters in classifier, decoupled prompts, task-specific keys and interaction matrix are learnable. At task t + 1 , we train the decoupled prompts (including three aspects, i.e ., modality-wise, layer-wise and complementary). We further apply three interaction strategies (within the light blue colored rectangle) to model modality-wise prompt interaction, task-wise prompt interaction, and in- teraction between input features and prompt keys. Right: during inference, we first calculate multi-modal representations (c) TRIPLET Driver who finished pal memory indexing (HMI) for improving generative reply. To remember training samples with a small data usage, we introduce a hippocampus modsentence representations (Xu et al., 2018; Liu et al., 2019), sentiment analysis (Chen et al., 2015; Xia et al., 2017), composition language learning (Li Searcher List of drivers who participated in the 1995 Tooheys 1000 race. edge representation. For OpenAI series models, we can directly retrieve the embedding through their official APIs 4 . For other open-source models, we with the query function, which are used to match the most similar multi-modal keys. Then decoupled E-Prompts paired with matched keys, together with decoupled G-Prompts, are appended to the inputs (or features) for answer generation. Basically, Eq. (1) would be modified with: extract task-specific knowledge. For example, the visual Fig. 6. Frameworks in TIL: HMI (PLM-based) [118], DynaMind (LLM-based) [39], TRIPLET (VLM-based) [137]. ( v ) ( v ) ( v ) ˆ y ( v , q ) = FT P ; VT ([ P ; v ]); TT ([ P , q ]) [0] , { { }} ( f ) ( v ) ( q ) prompt P = G ; E is composed of G-prompt ( v ) ( v ) K ule that encodes training samples into compressed memory engrams using BERT (Devlin et al., 2019) and product quantization (PQ) (Jégou et al., 2011), and stores them to generate conditioned samples during the replay step (Fig. 1b). This method makes it possible to generate specific training samples from previously learned tasks. Weevaluated HMI on two different CL scenarios using the original LAMOL as a baseline. The first scenario is a sequence of different types of tasks, for which we used five natural language understanding (NLU) tasks from DecaNLP (McCann et al., 2018). The other scenario is a sequence of different domains in the same task, for which we used five text classification datasets and single-pass setting, et al., 2020b), relation learning (Han et al., 2020), dialogue systems (Lee, 2017; Madotto et al., 2021), text classification, and question-answering (QA) (de Masson d'Autume et al., 2019; Wang et al., 2020). Regularization-based methods aim to constrain changes in model parameters important for previous tasks. Various methods have been proposed to estimate the importance of each parameter. For example, elastic weight consolidation (EWC) (Kirkpatrick et al., 2017) uses the Fisher information matrix. Synaptic intelligence (SI) (Zenke et al., 2017) estimates importance from the contribution to loss changes. Memory-aware synapses (MAS) (Aljundi et al., 2018) computes the sensitivity of Browser second-to-last in the Top 10. Coordinator Birthplace of the driver who finished second-to-last in the Tooheys Top 10. Figure 2: The pipeline of example \" The 1995 Tooheys 1000 driver who has second-to-last in the Tooheys Top 10 was born where? \". ries models 3 . 3.2 Memory Manager Memory Manager plays a pivotal role in storing and organizing the memories of DynaMind. It consists of five interconnected modules: Knowledge Representation, Knowledge Retrieval, Longchoose the last hidden state as the representation. Knowledge Retrieval utilizes vector search to construct an index on DynaMind's memory, enabling the rapid identification and retrieval of the most relevant knowledge. By leveraging the Langchain library (Chase, 2022), DynaMind can effortlessly access and shift between distinct vector indexing schemes. This module evaluates the relevance of knowledge within the given context, empowering DynaMind to access pertinent knowledge throughout the reasoning process efficiently. In DynaMind's memory, every piece of knowledge is represented as a triple <Context, Key, Value>. The \"Context\" component houses the contextual data relating to the knowledge. The \"Key\" F ( ([ ]) ) (2) where P ( v ) , P ( q ) , P ( f ) are the vision, question, and fusion prompt, respectively. Selective Deep Decoupling We then disentangle prompts in a layer-wise format, and attaching it to selective layers. Rather than keeping attaching prompts to all the selected multi-head attention (MHA) layers [37], in this paper, we add prompts to some MHA layers in a replacing schema, which is more memory-efficient. Given a transformer T containing K layers, T ([ P ; x ]) = ( L K ◦ L K -1 · · ·◦ L 0 )([ P ; x ]) could be decomposed layer-by-layer: ¯ h P k = α k · h P k +(1 -α k ) · P k , [ h CLS k +1 ; h P k +1 ; h x k +1 ] = L k ([ h CLS k ; ¯ h P k ; h x k ]) , (3) where [ h CLS 0 ; ¯ h P 0 ; h x 0 ] = [ CLS , P 0 , x ] are the raw inputs, and the output of L is regarded as model output. Moreover, G shared for all tasks and E-prompt E t specialized for the t -th task . When the t -th task comes, we train the prompt P ( m ) t = { G ( m ) ; E ( m ) t } where m = v, q, f . In our implementation, we combine all the three aforementioned decoupling designs. That is, we have three sets of prompts for three modalities, where each set of prompts contains layer-wise deep-prompts and each layer-wise deep prompt contains a G-prompt and a set of E-prompts. In summary, all the learnable prompts include: P ( m ) = { G ( m ) k ∈ R L G × D } ⋃ { E ( m ) t,k ∈ R L E × D } , m = v, q, f, (4) with subscripts t for tasks, k for the k -th MHA layers, L G / L E for G / E-Prompt's length, D for embedding dimension. reducing forgetting. This approach proves particularly effective on challenging benchmarks, such as the 20News dataset, where task dissimilarity often causes severe forgetting in other models. RecAdam [23] extends the concepts of EWC by applying a regularization term to the pre-trained weights, utilizing an annealing coefficient to progressively incorporate the importance of prior knowledge. 4.2.2 Prompt Interaction 931 which is considered as an ideal scenario for CL. The results indicate that HMI consistently outperforms LAMOL and improves robustness to training task order and amount of replay samples. We also investigated the balance of previously learned tasks in generated samples and found that HMI enables the generation of even old task samples, which indicates the controllability of sample generation with HMI. Furthermore, we explored the potential of further improvement of HMI with different sample selection strategies for replay. 2 Related Work CL, which involves learning from a stream of tasks without catastrophic forgetting, is a long-standing issue in machine learning. In the NLP, CL has been studied for diverse tasks, for example, word and parameters on the basis of the gradient of model outputs. Architecture-based methods dynamically change the network structure to assign model parameters for each task. Progressive neural networks (PNN) (Rusu et al., 2016) freeze the current parameters and add a new column of the network when training a new task. Instead of extending the network, PackNet (Mallya and Lazebnik, 2018) applies network pruning using dynamic filters to separate the neurons used for each task. Replay-based methods mitigate catastrophic forgetting by retraining for previous tasks when training for a new one. MbPA++ (de Masson d'Autume et al., 2019) introduces an episodic memory that stores real samples of previous tasks to use for experience replay and local adaptation. Meta-MbPA term Memory, Short-term Memory, and Knowledge Metabolism. Memory Manager works hand in hand with Inference Engine , engaging in frequent interactions to ensure a continuous acquisition and updating of the knowledge necessary for inferencing. This section provides an overview of the functionalities within Memory Manager and elaborates on their interactions with Inference Engine . Knowledge Representation is responsible for encoding knowledge in a format that can be efficiently processed by DynaMind. Typically, there are several common methods used for knowledge representation, such as Bag of Words (BOW) (Zhang et al., 2010),Word2Vec (Mikolov et al., 2013), knowledge graph (Luu et al., 2014, 2016), or fine-tuning a Transformer encoder 3 signifies the vectorized representation of the \"Context\", while the \"Value\" embodies the specific substance of the knowledge. Long-term Memory acts as a permanent knowledge repository within DynaMind, accumulating and retaining large amounts of information over time. The repository holds a collection of various knowledge from a variety of sources, such as previous interactions, external databases, and the Internet. DynaMind harnesses the power of longterm memory to enhance its reasoning ability by integrating past experience and acquired knowledge. Furthermore, users can actively manipulate the knowledge engaged in the inference by explicitly updating the long-term memory, thereby granting them heightened control over DynaMind's cog4 α k ∈ { 0 , 1 } is a predefined switch that controls whether using the output prompt feature h P k or the k -th layer-specific prompt P k as input. Complementary Decoupling Following the complementary design principle [37], each prompt is further split into two parts: a General Prompt (G-Prompt) to extract taskinvariant knowledge, and an Expert Prompt (E-Prompt) to With the proposed decoupled prompts, then we need interaction strategies to train them all together. We first have Query-and-Match Strategy to match between input features and related task-specific prompts. We further introduce Modality-Interaction Strategy and Task-Interaction Strategy to promote interactions between prompts. The former 2956 The diversity of text distributions and intents across domains poses challenges for scaling user intent detection (UIC) in industrial applications. MeLL [178] proposes a solution by leveraging global and local memory networks to maintain cross-task representations, which allows the model to continually adapt to new tasks without significant loss of performance. MeLL also employs an LRU policy for efficient memory management and controls parameter growth, making it scalable for real-world applications. https://huggingface.co/tiiuae/falcon-40b https://platform.openai.com/ Recent advancements in conversational AI focus on overcoming the limitations of traditional chatbots, which rely on static knowledge bases and extensive manual data annotation. Lifelong INteractive learning in Conversation (LINC) introduces a dynamic learning framework that mirrors human cognitive processes, enabling chatbots to integrate and utilize knowledge in real-time [98, 99, 121]. Structured with an Interaction Module, Task Learner, and Knowledge Store, LINC enhances chatbots by improving real-time information extraction, handling erroneous inputs, and refining conversational skills, thereby boosting linguistic and interactive capabilities. Continual Pre-training Methods. Continual pre-training represents a paradigm where PLMs are progressively enhanced by assimilating new knowledge from expanding datasets. ERNIE 2.0 [167] illustrates this approach by incrementally constructing pre-training tasks, enabling the model to capture increasingly complex lexical, syntactic, and semantic nuances. Unlike traditional fixed-task training, it employs a continual multi-task learning framework to enhance its capabilities. Advancing this field further, RecyclableTuning [141] introduces two strategies: initialization-based and distillation-based. The former builds on fine-tuned weights from existing PLMs, while the latter reuses outdated weights to preserve knowledge and improve efficiency in future models. Parameter-Efficient Tuning Methods. Expounding upon the crucial need for more efficacious knowledge integration, ACM[219] dynamically adjusts transformer architectures and leverages pseudo-experience replay to enhance knowledge transfer. Continual Learning of Few-Shot Learners (CLIF) addresses a gap between rapid generalization methods (such as few-shot learning) and traditional continual learning approaches, which are not inherently designed for such generalization. BiHNet-Reg [70] provides a solution by generating task-specific adapter weights through bi-level task representations and incorporating regularization. Instruction Tuning-based Methods. PCLL [223], designed for task-oriented dialogue systems, uses a conditional variational autoencoder with prompts to generate pseudo samples that capture task-specific distributions. It incorporates distillation to reduce noise in these samples, outperforming baselines like Experience Replay and HAT in intent detection and slot filling. PP [149] mitigates catastrophic forgetting and enables forward transfer without data replay or excessive task-specific parameters. This method appends new soft prompts to previously learned ones, keeping the base model Manuscript submitted to ACM 12 Yutao Yang et al. unchanged. It surpasses LPT5, IDBR, and MBPA++ on both standard and custom continual learning benchmarks. Furthermore, ConTinTin [204] provides a framework for sequentially mastering tasks through textual instructions, supporting both forward and backward transfer of knowledge. DYNAINST [127], designed for lifelong in-context instruction learning, enhances a PLM's generalization by combining parameter regularization with experience replay. Its Dynamic Instruction Replay, using Dynamic Instance Selection (DIS) and Dynamic Task Selection (DTS), optimizes memory and computation by selectively replaying relevant instances and tasks. 5.2.2 LLMs-based TIL. Recent attention has focused on the convergence of LLMs with CL methods, exemplified by significant contributions [135, 185]. Benefiting from vast corpora and advanced hardware infrastructure, LLMs showcase remarkable capabilities in language comprehension and generation. However, challenges arise in scenarios involving sequential tasks, where LLMs often exhibit a decline in performance known as catastrophic forgetting. Traditional Methods. DynaMind [39] (Figure 6b) stands as a pioneering framework that enhances LLM output precision through the integration of memory mechanisms and modular operators. The framework consists of three key components: a memory module that stores and updates learned knowledge, a modular operator that processes incoming data, and a continual learning (CL) module that dynamically adjusts LLM parameters as new knowledge is introduced. In parallel, JARe [135] incorporates Dynamic Task-related Knowledge Retrieval (DTKR) to enable adaptive adjustments of language models for specific downstream tasks. By leveraging task distributions within vector space, JARe aims to optimize the continual learning process, streamlining task-specific knowledge adaptation. Parameter-Efficient Tuning Methods. Large language models (LLMs) face significant challenges that limit their practical applications, such as high computational and memory demands, and vulnerability to catastrophic forgetting. These issues highlight the need for more efficient and robust training and deployment strategies. ConPET [165], designed for the continual adaptation of LLMs across diverse tasks, leverages parameter-efficient tuning (PET) strategies to enhance both efficiency and performance. ConPET operates in two modes: Static ConPET, optimized for moderate-scale tasks, and Dynamic ConPET, tailored to large-scale tasks. Additionally, ELM [64] introduces a compact expert adapter trained for each task, with a retrieval mechanism that selects the most suitable expert LLM for new tasks. Furthermore, O-LoRA [184] preserves task-specific knowledge by using low-rank approximations of previous task gradients, while the orthogonality constraint ensures task-specific subspaces remain distinct, minimizing interference from new tasks. This method demonstrates robustness across various task sequences. Instruction Tuning-based Methods. Continual-T0 [158] incorporates rehearsal techniques alongside instruction tuning of LLMs, showing strong performance across 70 datasets. However, its generalization to underrepresented languages remains limited. In response, InstructAlign [15] proposes aligning new languages with those previously learned, leveraging abundant linguistic resources to mitigate catastrophic forgetting. The key innovation lies in enhancing language adaptation for instruction-tuned LLMs, particularly by incorporating underrepresented languages. Unlike traditional methods focused on single-task learning, RoboCoder [95] enables robots to tackle a wide range of increasingly complex tasks by continuously refining and updating action codes based on environmental feedback. EUREKA [115] operates by leveraging LLMs to generate reward functions from the environment's source code without requiring pre-defined templates or task-specific prompts. Through its evolutionary approach and in-context learning capabilities, it continually refines the rewards to achieve superior performance across diverse RL tasks. Manuscript submitted to ACM Recent Advances of Foundation Language Models-based Continual Learning: A Survey 13 5.2.3 VLMs-based TIL. The long-term sustainability of pre-trained visual-language models (VLMs) is increasingly under scrutiny due to their dependence on continually expanding datasets. Although these models demonstrate robust performance across a diverse range of downstream tasks, the incessant growth of real-world data poses substantial challenges to the sustainability of traditional offline training methodologies. Traditional Methods. CTP [231] employs topology preservation and momentum contrast to maintain consistent relationships within sample mini-batches across tasks, thereby preserving the distribution of prior embeddings. ZSCL [226] addresses the challenge of zero-shot transfer degradation in VLMs by employing a label-free dataset for distillation in the feature space, alongside weight regularization in the parameter space. Additionally, ZSCL has been adapted for the CIL setting, expanding its applicability to a wider range of continual learning scenarios. SND [208] tackles catastrophic forgetting and zero-shot performance degradation in VLMs by leveraging a dual-teacher model. The pre-trained VLM retains zero-shot generalization, while the fine-tuned VLM preserves task-specific knowledge. A teacher selection mechanism quantifies discrepancies between their feature representations. Similarly, AwoForget [225] employs a graph-based multi-modal proximity distillation framework to mitigate forgetting in VLMs. It constructs a cross-modal graph to represent relationships between image-text pairs and adopts a dual-teacher distillation approach for maintaining both zero-shot and task-specific knowledge. This method also extends its applicability to CIL. Parameter-Efficient Methods. MoE-Adapters4CL [207] introduces a parameter-efficient continual learning method to mitigate long-term forgetting in incremental learning of VLMs. Their approach involves dynamically extending the pre-trained CLIP model to accommodate new tasks by integrating a Mixture-of-Experts (MoE) adapter. Specifically, MoE consists of several LoRA adapter experts and routers, where the router calculates gating weights and uses the 𝑇𝑜𝑝𝐾 function to select the 𝑘 most relevant experts for learning the current task. To maintain the zero-shot recognition capabilities of the visual-language model, a Distribution Discriminative Automatic Selector (DDAS) is further introduced, which can automatically route in-distribution and out-of-distribution inputs to the MoE adapters and the original CLIP, respectively. Furthermore, the MoE-Adapters4CL framework has also been adapted for use in the CIL setting. Instruction Tuning-based Methods. By decoupling prompts and prompt interaction strategies, TRIPLET [137] (Figure 6c) effectively captures complex interactions between modalities. This includes specific designs for visual, textual, and fused prompts, as well as how to interact between different tasks through these prompts and retain crucial information, thereby reducing catastrophic forgetting. Decoupled prompts are designed to separate prompts in terms of multimodality, layer-wise, and complementary, with each type of prompt containing learnable parameters intended to capture modality-specific knowledge from pre-trained VLMs and training data.",
  "5.3 Class-Incremental Learning": "",
  "5.3.1 PLMs-based CIL.": "Traditional Methods. ExtendNER [128] (Figure 7a) introduces a continual learning framework for Named Entity Recognition (NER) that reduces re-annotation efforts. It uses knowledge distillation, where a \"teacher\" NER model helps a \"student\" model learn to recognize new entity types while retaining knowledge of previous ones. SRC [104] focuses on sentence representation learning by initializing corpus-independent encoders and refining them through Boolean operations on conceptor matrices. IDBR [61] improves continual text classification by disentangling task-generic and task-specific representations. It uses regularization techniques and auxiliary tasks like next sentence prediction to enhance knowledge retention and generalization, outperforming many existing methods across various task sequences. Manuscript submitted to ACM 14 Yutao Yang et al. CE loss KD loss e n c o d e r l i n e a r B- I- O       LOC     LOC Reuters reported in New York Teacher e n c o d e r l i n e a r B- I- B- I- O       LOC     LOC     ORG   ORG Reuters reported in New York B- I- B- I- O       LOC     LOC     ORG   ORG ExtendNER (a) ExtendNER model: the teacher encoder layers Figure 2: ExtendNER CoNLL-03 PER 6,532 1,829 1,597 LOC 7,125 1,832 1,664 ORG 6,271 1,325 1,654 MISC 3,398 916 698 Total 23,326 5,902 5,613 Table 1: Distribution of entity labels in CoNLL-03. OntoNotes Training Validation Test ORG 24,163 3,798 2,002 PER 22,035 3,163 2,134 GPE 21,938 3,649 2,546 DATE 18,791 3,208 1,787 ! \" ! # \" \" $ FeedForward Down-Projection ReLU Up-Projection Add & Norm Embedding Layer (EL) PLE Training Set Query PLE % # : Teacher PLE + % # : PLE Memory EL + % #$% : PLE % #$& : PLE … Class Prototypes * Query Representation sim( ⋅ ) score … Pseudo Samples ! !\"# ! $%$ Replay Distill Dynamic Weighting a) Teacher Knowledge Transfer (TKT) b) Pseudo Samples Replay (PSR) × L d) Inference Multi-Head Attention Adapter Prefix Prefix Add & Norm c) Dynamic Weighting Replay (DWR) Figure 2: Overview of the proposed framework. The left side shows the structure of our PLE, which consists of a frozen PrLM and the Prefix ( P k , P v ) and Continual Adapter inserted into each layer. The trainable parameters are in green. The top right shows the process of learning for a new task T i with three strategies: Teacher Knowledge Transfer (TKT), Pseudo Samples Replay (PSR), and Dynamic Weighting Replay (DWR). The lower right shows the (b) PLE Validation Training Test (grey, solid border) are copied to build the new model. A new output layer (green, dashed border) substitutes the old one CARD 10,901 1,720 1,005 NORP 9,341 1,277 990 distance-based classification pipeline at inference. Summary . Existing works in few-shot intent detection mainly focus on the data scarcity and ignore advantages and defeats the goal of few-shot learning (Gao et al., 2021). After learning i , the model et al. 2022) classifies images by multiplying the encoded image with text features extracted using the text encoder in a frozen CLIP model using the labels of all categories seen model has encountered. This set of prompts is given to the text encoder to obtain the text features B t . Finally, we perform a matrix multiplication between A i and B t to compute Fig. 7. Frameworks in CIL: ExtendNER (PLM-based) [128], PLE (PLM-based) [94], Adaptation-CLIP (VLM-based) [105]. Adapter ! !\"# ! ! (∆!,%) ! ! ' Train Prune Update Image encoder Dog Cat … Text encoder Frozen Frozen Classifier Prediction Update Latent Feature Adapted Feature Figure 2: Illustration of our proposed method. We add a Linear Adapter layer (in green) after the CLIP Image Encoder which generates projected features. The text feature is generated by encoding class labels using the CLIP Text Encoder. During training the Image and Text encoders are frozen and we only update the Linear Adapter. To obtain better continual learning performance, we propose a parameter retention method to mitigate forgetting in the Adapter (see Parameter Retention Section). (c) Adaptation-CLIP the ability to learn consecutive tasks, which is es- Total 107,169 10,464 16,815 T is evaluated separately on the test set of seen tasks. up to the current task as prompts. The prompts are manually the output logits. We use the cross-entropy loss to update the sential for the online dialogue systems. The works The setup is aligned with the real scenario, where determined and combined with the class name and the sim- ilarity score between the input image and different prompts parameters of the Linear Adapter layer: D with the additional dimensions for the new entity type. If a teacher and the student outputs for the entity types in ; new token is annotated in the new training data, it will contribute to the cross entropy loss. Otherwise, the teacher soft targets will be used for the KD loss. extended to be a matrix with dimension h × (2 n +2 m +1) in order to accommodate the new entity types 3 . Again, we adopt KD to prevent catastrophic forgetting with two loss functions: ( i ) the KL divergence between the Table 2: Distribution of entity labels in OntoNotes. distributed among the admissible transitions starting from the tag p . 4 Experimental Evaluation Without loss of generality, we consider the case where the in continual intent detection do not consider the data scarcity of emerging new intents. The newly proposed FSCIL-ID setting is also not aligned with the online dialogue systems. In contrast to those works, our work aims to recognize continually emerging new intents from multiple domains with very few labeled examples. 3 Methodology 3.1 Problem Formulation In the CFID setting, given a sequence of n tasks {T 1 , T 2 , ..., T n } , each task T i contains its own training set D i train , development set D i dev , and test set the data privacy of different users is protected while the task information is available. 3.2 Overall Framework As shown in Figure 2, the framework of the proposed method consists of one main module and three strategies of continual learning: 1) The lightweight PLE is responsible for extracting semantic features of the input utterances. 2) The TKT aims to transfer task-specific knowledge to the current model. 3) The PSR first selects two key samples per class and encodes them through the frozen embedding layer (EL) to generate pseudo samples and save them into the Memory. 4) The from all classes is used for classification. This method does not require storing any exemplars or updating any parameters. It outperforms most state-of-the-art methods with only zero-shot evaluation. Continual-CLIP performs no adaptation ability on incoming task data and in the next section, we propose how CLIP pre-trained models can be updated and further improved for class incremental learning. CLIP with Adaptation Starting from the zero-shot evaluation protocol proposed in Continual-CLIP (Thengane et al. 2022), we aim to augment the original architecture with new modules that enable better adaptation to more downstream tasks. By learning on the training set of each task and updating the parameters of these modules, we can achieve a better trade-off between stability L CE ( x , y, t ; W ) = -1 n n ∑ i =1 y i · log A i B t . (3) Adaptation via a Self-attention Adapter. We can use a self-attention module in place of the single Linear Adapter layer described above. Specifically, we apply three separate linear transformations to the output I i of the CLIP image encoder, resulting in the query ( Q ), key ( K ), and value ( V ) matrices: Q = W q I i , K = W k I i , V = W v I i , (4) where W q , W k , and W v are learned weight matrices. Next, we compute the self-attention weights using the dot product between the query and key matrices, scaled by a factor √ M : α = softmax ( QK T √ ) , (5) Instruction Tuning-based Methods. Instead of retaining examples, PAGeR [174] uses PLMs to generate intent-specific utterances for new tasks while maintaining performance on previous ones. It selectively preserves relevant contexts as prompts, ensuring efficient memory usage. By utilizing PLMs, PAGeR is effective in real-world scenarios where new intents and relations are continuously introduced, offering a memory-efficient alternative to traditional methods. test contains a series of sam- . Each dataset D i DWRis responsible for balancing the learning of and plasticity. M E { D i /negationslash T ( ii ) the cross-entropy between the student output and the gold annotation in D new for the entity types in E new . More formally, for each token in a sentence with associated category y , the model will compute either the KL divergence or the cross-entropy loss, depending on the value of y . In particular, when y = O , the KL divergence is computed between the teacher output distribution and the student output distribution (the green circles in Figure 2): L Ex KL = KL ( p M i E i , p M i +1 E i ) . Instead, if y = O (i.e., the token was labeled as one of the entity types in E new ) the model will compute the cross-entropy loss (the red diamonds in Figure 2), i.e., L Ex CE = CE ( y, p M i +1 E new ) . Again, the model is trained on the weighted sum of the two losses, i.e., L Ex = αL Ex KL + βL Ex CE . The final tags are obtained by the Viterbi algorithm (Vintsyuk 1968) applied as a post-process for the n + m entity types. The emission probabilities are given by the M i +1 outputs, i.e., p M i +1 E i +1 , while the transition probabilities have been hard-coded to prevent impossible transitions. The transition probabilities are represented as a matrix T r of dimensions (2 n + 2 m + 1) × (2 n + 2 m + 1) , where i.e., E = { e i +1 } . This allows us to test the capability of our approach to learn one entity type at a time. Thus, at step i +1 , we train the model M i +1 to recognize the entity types { e 1 , ..., e i +1 } , given the teacher M i . 4.1 Datasets To evaluate our approach, we used two well-known NER datasets: CoNLL-03 English NER (Tjong Kim Sang and De Meulder 2003) and OntoNotes (Hovy et al. 2006). CoNLL-03. The English version of this dataset contains roughly 22k sentences pulled from news stories that have been annotated w.r.t. 4 named entity types: Person (PER), Location (LOC), Organization (ORG), and Miscellaneous (MISC). The distribution of the entity types across the official training, validation, and test sets are given in Table 1. OntoNotes. The English version of this dataset includes about 144k sentences drawn from a variety of texts, such as news, transcribed telephone conversations, and weblogs. Although the dataset has been annotated for 18 entity types, we restrict our investigation to the 6 most represented types to ensure a sufficient number of examples for training. The 6 types we considered are: Organization (ORG), Person 335 we describe its few-shot nature that there are only K ∈ { 5 , 10 } samples for each class in the training set. There are also a few (e.g., 10) samples for each class in the development set. This is because using a larger development set brings significant 3.3 Prefix-guided Lightweight Encoder (PLE) PLE serves as the main module to alleviate catastrophic forgetting caused by over-parameterization. As a sub-module of PLE, the Continual Adapter is a full continual learning lightweight module t as F image and the text encoder as F text . For the image modality, we input the images into the image encoder to extract the latent embedding I i = F image ( x i t ) ∈ R M . Subsequently, we pass it through a Linear Adapter layer A i = g W ( I i ) parameterized by weight matrix W ∈ R M × M to compute an adapted image feature with more capacity. For the text modality, we amalgamate a manually specified, fixed set of prompts ('a photo of [CLS]') where [CLS] takes the value of all class names C t = { c 1 , c 2 , ..., c t } that the current A i = αV. (6) After obtaining the adapted image features A i , we use Eq. 3 to calculate the logits by multiplying the image features with the text features and use the cross-entropy loss to update parameters. Unlike the previous approach, in this Self-attention Adapter we need to update three linear projection matrices instead of just one. This results in three times the number of learnable parameters. Parameter-Efficient Tuning Methods. In task-oriented dialogue systems, continuous few-shot intent detection (CFID) focuses on recognizing new intents with a few examples. To address this, PLE [94] (Figure 7b) employs a parameterefficient tuning approach that integrates a Continual Adapter module with a frozen PLM and a Prefix-guided Attention mechanism. This design effectively reduces forgetting while minimizing the number of trainable and stored parameters, making it an ideal choice for real-world applications where computational efficiency is critical. Similarly, EPI [188] allocates task-specific private parameters alongside a shared model, ensuring precise retrieval and superior performance in continual learning. It reduces storage needs through random static masking and consistently outperforms rehearsalfree methods, even competing with rehearsal-based ones on far-domain datasets. For environments with sequentially presented data, DE&E [193] uses a mixture of binary class-specific experts, dynamically selecting the best expert for each input, facilitating incremental learning by combining expert outputs for final classification. }| new , where ples ( x , y ) y | | dataset is annotated only for one new entity type D is the ground-truth new tasks and replaying past tasks. Assume we are at task t , intent class of the input utterance i =1 i i +1 i , e i x Finally, we compute the weighted sum of the value matrix i . In particular, (PER), (GPE), (DATE), Date Cardi- Geo-Political Entity",
  "Itag following an O tag or an I-X tag following an I-Y tag, where X and Y are two different entity types. Otherwise, 5.3.2 VLMs-based CIL.": "Nationalities nal and Religious Political Group → r if p, r r ) = 0 ( p T is an impossible transition, e.g., an r , where is the number of allowed tran- ) = p ) N ( ( p, r 1 N p ( ) p 4.2 Experimental Setup 3 Notice that the additional 2 m is for accommodating the B-X and I-X categories for each new entity type X in E new . To test our approach in the CL setting, we divided the official training and validation sets of CoNLL-03 and OntoNotes into four and six disjoint subsets, D 1 , D 2 , . . . , respectively: 13573 Traditional Methods. VLM-PL [82] leverages a VLM to enhance the pseudo-labeling process, integrating new object classes into a detection model without compromising previously learned categories. GMM [17] leverages LLMs for class-incremental learning. This innovative approach entails the generation of labels for images by employing an adapted generative model. PROOF [229] maps pre-trained features into a new feature space designed to preserve prior knowledge. To enhance the use of cross-modal information, it incorporates a fusion module with an attention mechanism, allowing the model to retain essential features when adapting to new tasks. CLAP [68] offers a new method for adapting VLMs to novel tasks without losing previously learned knowledge. This approach employs a Variational Inference framework to probabilistically model the distribution of visual-guided text features, enhancing fine-tuning reliability by accounting for uncertainties in visual-textual interactions. , i.e., the probability mass is equally sitions starting from Parameter-Efficient Tuning Methods. Adaptation-CLIP [105] (Figure 7c) employs three strategies for continual learning: a linear adapter, a self-attention adapter, and prompt tuning. The adapters add layers to the image encoder while freezing the rest of the model, and prompt tuning enhances the text encoder by integrating new and prior prompts to maintain task comprehension. DIKI [168] prevents the disruption of pre-trained knowledge by using a residual attention mechanism that injects new knowledge into a frozen backbone, minimizing interference. A distribution-aware calibration scheme is employed to maintain the zero-shot capabilities of VLMs. RAPF [60] addresses class-incremental learning challenges using a pre-trained CLIP model with a linear adapter. It minimizes interference by identifying Manuscript submitted to ACM (NORP). Dataset statistics are shown in Table 2. (CARD), α Adaptation via a Linear Adapter. the input image is x , and denote the CLIP image encoder V, using the self-attention weights : ( i ) Recent Advances of Foundation Language Models-based Continual Learning: A Survey 15 < l a t e x i s h 1 _ b 6 4 = \" F P E N K V g Z 8 m f d r u k 5 + Y > A B 3 c S I 9 v W p H L C q J o w X M 2 7 Q / D n O z G U 0 R T j y < l a t e x i s h 1 _ b 6 4 = \" F P E N K V g Z 8 m f d r u k 5 + Y > A B 3 c S I 9 v W p H L C q J o w X M 2 7 Q / D n O z G U 0 R T j y < l a t e x i s h 1 _ b 6 4 = \" F P E N K V g Z 8 m f d r u k 5 + Y > A B 3 c S I 9 v W p H L C q J o w X M 2 7 Q / D n O z G U 0 R T j y < l a t e x i s h 1 _ b 6 4 = \" F P E N K V g Z 8 m f d r u k 5 + Y > A B 3 c S I 9 v W p H L C q J o w X M 2 7 Q / D n O z G U 0 R T j y < l a t e x i s h 1 _ b 6 4 = \" L c f O H X P I Q w E m 3 j q n / 7 o > A B 9 V D S g M F T W + d v y u G 5 Y 0 k J R 8 N C z U Z p r K 2 < l a t e x i s h 1 _ b 6 4 = \" L c f O H X P I Q w E m 3 j q n / 7 o > A B 9 V D S g M F T W + d v y u G 5 Y 0 k J R 8 N C z U Z p r K 2 < l a t e x i s h 1 _ b 6 4 = \" L c f O H X P I Q w E m 3 j q n / 7 o > A B 9 V D S g M F T W + d v y u G 5 Y 0 k J R 8 N C z U Z p r K 2 < l a t e x i s h 1 _ b 6 4 = \" L c f O H X P I Q w E m 3 j q n / 7 o > A B 9 V D S g M F T W + d v y u G 5 Y 0 k J R 8 N C z U Z p r K 2 < l a t e x i s h 1 _ b 6 4 = \" k B W p E V 3 z 0 9 J d P D f c Z n > A H N S 8 I Y X o g L j y 2 + R r 5 u w F C / O v M 7 G m Q U T K q < l a t e x i s h 1 _ b 6 4 = \" k B W p E V 3 z 0 9 J d P D f c Z n > A H N S 8 I Y X o g L j y 2 + R r 5 u w F C / O v M 7 G m Q U T K q < l a t e x i s h 1 _ b 6 4 = \" k B W p E V 3 z 0 9 J d P D f c Z n > A H N S 8 I Y X o g L j y 2 + R r 5 u w F C / O v M 7 G m Q U T K q < l a t e x i s h 1 _ b 6 4 = \" k B W p E V 3 z 0 9 J d P D f c Z n > A H N S 8 I Y X o g L j y 2 + R r 5 u w F C / O v M 7 G m Q U T K q < l a t e x i s h 1 _ b 6 4 = \" 5 k K Y 2 U A 9 m W J + 0 p N R d I Z Q > B H c S 8 E n v j g q D o P F w T f X / 7 V C O 3 r G u M L y z < l a t e x i s h 1 _ b 6 4 = \" 5 k K Y 2 U A 9 m W J + 0 p N R d I Z Q > B H c S 8 E n v j g q D o P F w T f X / 7 V C O 3 r G u M L y z < l a t e x i s h 1 _ b 6 4 = \" 5 k K Y 2 U A 9 m W J + 0 p N R d I Z Q > B H c S 8 E n v j g q D o P F w T f X / 7 V C O 3 r G u M L y z < l a t e x i s h 1 _ b 6 4 = \" 5 k K Y 2 U A 9 m W J + 0 p N R d I Z Q > B H c S 8 E n v j g q D o P F w T f X / 7 V C O 3 r G u M L y z < l a t e x i s h 1 _ b 6 4 = \" W 0 9 j r L R m 8 T o F H + O K C A > X c V 7 S g N B J 2 f q z Q M u G w k 5 d 3 E D Y v / y p n U Z P I < l a t e x i s h 1 _ b 6 4 = \" W 0 9 j r L R m 8 T o F H + O K C A > X c V 7 S g N B J 2 f q z Q M u G w k 5 d 3 E D Y v / y p n U Z P I < l a t e x i s h 1 _ b 6 4 = \" W 0 9 j r L R m 8 T o F H + O K C A > X c V 7 S g N B J 2 f q z Q M u G w k 5 d 3 E D Y v / y p n U Z P I < l a t e x i s h 1 _ b 6 4 = \" W 0 9 j r L R m 8 T o F H + O K C A > X c V 7 S g N B J 2 f q z Q M u G w k 5 d 3 E D Y v / y p n U Z P I < l a t e x i s h 1 _ b 6 4 = \" u o 9 T J U C I Q 5 n v y k Y q m 8 + M > A B H c Z N S E 3 W r p L F f R O K g 2 d 7 w P X G z 0 D j / V < l a t e x i s h 1 _ b 6 4 = \" u o 9 T J U C I Q 5 n v y k Y q m 8 + M > A B H c Z N S E 3 W r p L F f R O K g 2 d 7 w P X G z 0 D j / V < l a t e x i s h 1 _ b 6 4 = \" u o 9 T J U C I Q 5 n v y k Y q m 8 + M > A B H c Z N S E 3 W r p L F f R O K g 2 d 7 w P X G z 0 D j / V < l a t e x i s h 1 _ b 6 4 = \" u o 9 T J U C I Q 5 n v y k Y q m 8 + M > A B H c Z N S E 3 W r p L F f R O K g 2 d 7 w P X G z 0 D j / V < l a t e x i s h 1 _ b 6 4 = \" u o 9 T J U C I Q 5 n v y k Y q m 8 + M > A B H c Z N S E 3 W r p L F f R O K g 2 d 7 w P X G z 0 D j / V < l a t e x i s h 1 _ b 6 4 = \" u o 9 T J U C I Q 5 n v y k Y q m 8 + M > A B H c Z N S E 3 W r p L F f R O K g 2 d 7 w P X G z 0 D j / V < l a t e x i s h 1 _ b 6 4 = \" u o 9 T J U C I Q 5 n v y k Y q m 8 + M > A B H c Z N S E 3 W r p L F f R O K g 2 d 7 w P X G z 0 D j / V < l a t e x i s h 1 _ b 6 4 = \" u o 9 T J U C I Q 5 n v y k Y q m 8 + M > A B H c Z N S E 3 W r p L F f R O K g 2 d 7 w P X G z 0 D j / V < l a t e x i s h 1 _ b 6 4 = \" u X U q 0 C p K O 2 A 3 w N r j S m 8 R Q > B + H c Z M E I z 9 v W 5 g y V 7 G L d k J / F D n o Y f T P < l a t e x i s h 1 _ b 6 4 = \" u X U q 0 C p K O 2 A 3 w N r j S m 8 R Q > B + H c Z M E I z 9 v W 5 g y V 7 G L d k J / F D n o Y f T P < l a t e x i s h 1 _ b 6 4 = \" u X U q 0 C p K O 2 A 3 w N r j S m 8 R Q > B + H c Z M E I z 9 v W 5 g y V 7 G L d k J / F D n o Y f T P < l a t e x i s h 1 _ b 6 4 = \" u X U q 0 C p K O 2 A 3 w N r j S m 8 R Q > B + H c Z M E I z 9 v W 5 g y V 7 G L d k J / F D n o Y f T P < l a t e x i s h 1 _ b 6 4 = \" R 9 G Y k C L v 2 M S N 7 p F V 8 f q 0 I > A B + X c D J 3 r P U d m u K o w T Z 5 Q H j / W g n y E O z < l a t e x i s h 1 _ b 6 4 = \" R 9 G Y k C L v 2 M S N 7 p F V 8 f q 0 I > A B + X c D J 3 r P U d m u K o w T Z 5 Q H j / W g n y E O z < l a t e x i s h 1 _ b 6 4 = \" R 9 G Y k C L v 2 M S N 7 p F V 8 f q 0 I > A B + X c D J 3 r P U d m u K o w T Z 5 Q H j / W g n y E O z < l a t e x i s h 1 _ b 6 4 = \" R 9 G Y k C L v 2 M S N 7 p F V 8 f q 0 I > A B + X c D J 3 r P U d m u K o w T Z 5 Q H j / W g n y E O z < l a t e x i s h 1 _ b 6 4 = \" J k U T S M X n f C 2 F 8 H A r K I > c Z 7 g N B y 3 G q E z m 0 5 L w Q Y V 9 D / P O + R W v u o d p < l a t e x i s h 1 _ b 6 4 = \" J k U T S M X n f C 2 F 8 H A r K I > c Z 7 g N B y 3 G q E z m 0 5 L w Q Y V 9 D / P O + R W v u o d p < l a t e x i s h 1 _ b 6 4 = \" J k U T S M X n f C 2 F 8 H A r K I > c Z 7 g N B y 3 G q E z m 0 5 L w Q Y V 9 D / P O + R W v u o d p < l a t e x i s h 1 _ b 6 4 = \" J k U T S M X n f C 2 F 8 H A r K I > c Z 7 g N B y 3 G q E z m 0 5 L w Q Y V 9 D / P O + R W v u o d p j j j j < l a t e x i s h 1 _ b 6 4 = \" F P E N K V g Z 8 m f d r u k 5 + Y > A B 3 c S I 9 v W p H L C q J o w X M 2 7 Q / D n O z G U 0 R T j y < l a t e x i s h 1 _ b 6 4 = \" F P E N K V g Z 8 m f d r u k 5 + Y > A B 3 c S I 9 v W p H L C q J o w X M 2 7 Q / D n O z G U 0 R T j y < l a t e x i s h 1 _ b 6 4 = \" F P E N K V g Z 8 m f d r u k 5 + Y > A B 3 c S I 9 v W p H L C q J o w X M 2 7 Q / D n O z G U 0 R T j y < l a t e x i s h 1 _ b 6 4 = \" F P E N K V g Z 8 m f d r u k 5 + Y > A B 3 c S I 9 v W p H L C q J o w X M 2 7 Q / D n O z G U 0 R T j y < l a t e x i s h 1 _ b 6 4 = \" L c f O H X P I Q w E m 3 j q n / 7 o > A B 9 V D S g M F T W + d v y u G 5 Y 0 k J R 8 N C z U Z p r K 2 < l a t e x i s h 1 _ b 6 4 = \" L c f O H X P I Q w E m 3 j q n / 7 o > A B 9 V D S g M F T W + d v y u G 5 Y 0 k J R 8 N C z U Z p r K 2 < l a t e x i s h 1 _ b 6 4 = \" L c f O H X P I Q w E m 3 j q n / 7 o > A B 9 V D S g M F T W + d v y u G 5 Y 0 k J R 8 N C z U Z p r K 2 < l a t e x i s h 1 _ b 6 4 = \" L c f O H X P I Q w E m 3 j q n / 7 o > A B 9 V D S g M F T W + d v y u G 5 Y 0 k J R 8 N C z U Z p r K 2 < l a t e x i s h 1 _ b 6 4 = \" V 9 5 Y w k n F r z c B L y q g S o M f 0 U > A 7 Z N 8 E I v W p H Q j d u m 3 + K P X G C R D 2 / J T O < l a t e x i s h 1 _ b 6 4 = \" V 9 5 Y w k n F r z c B L y q g S o M f 0 U > A 7 Z N 8 E I v W p H Q j d u m 3 + K P X G C R D 2 / J T O < l a t e x i s h 1 _ b 6 4 = \" V 9 5 Y w k n F r z c B L y q g S o M f 0 U > A 7 Z N 8 E I v W p H Q j d u m 3 + K P X G C R D 2 / J T O < l a t e x i s h 1 _ b 6 4 = \" V 9 5 Y w k n F r z c B L y q g S o M f 0 U > A 7 Z N 8 E I v W p H Q j d u m 3 + K P X G C R D 2 / J T O base model and stored in the memory. At certain intervals, we sample examples from the memory Online Continual Learning with Category Names Only via Data Generation 5 x memory y experience replay memory x ˆ y model model local adaptation retrieve training inference Figure 1: An illustration of our model and how it interacts with the key-value memory module during training (left) and inference (right). During training, newly seen examples are used to update the (a) MBPA++ Dog Cat Task t Task t+1 Query Generate DALL.E-2 SDXL CogView2 DeepFloyd Trainable Deer Bird [Concept] Prompt Rewrites Base Prompt Meta Prompt Fig. 2: G-NoCL framework: In an online CL setting, the learner f θ receives new concepts which is passed through a prompt refiner module ψ to obtain fine-grained prompts. These prompts are subsequently used to generate data from the set of gen- (b) G-NoCL adaptation). We use the fine-tuned model to make a prediction and then discard it-keeping the base and perform gradient updates on the base model (experience replay). During inference, we retrieve examples whose keys are similar to a test example under consideration to fine-tune the model (local erators G . The data generated by each generator are ensembled through ensembler ∆ , and they are used for training the model f θ . Fig. 8. Frameworks in Online Continual Learning: MBPA++ (PLM-based HTB/BTB) [33], G-NoCL (VLM-base BTB) [160]. generative models, yet the focus remains on supervised learning, overlooking model for other predictions. exploration in a sequential training regime where distribution shift is common. into the model. We show that a 1% experience replay to learning new examples ratio is sufficient. Such a process bears some similarity to memory consolidation in human learning (McGaugh, 2000). In local adaptation, we follow Memory-based Parameter Adaptation (MbPA; Sprechmann et al., 2018) and use examples retrieved from memory to update model parameters used to make a prediction of a particular test example. Our setup is different from a typical lifelong learning setup. We assume that the model only makes one 3 Approach In contrast to previous works that assumed the abundance of well-curated annotated data, we explore the problem setup of online continual learning in which only new concepts are streamed to the learner f θ without any data. We aim to address the absence of data by proposing a Generative Name only Continual neighboring categories and applying hinge loss, while generating old class features from a Gaussian distribution to aid classification. STAR-Prompt [124] introduces a two-level prompting mechanism to balance stability and plasticity in large models like CLIP and ViT, reducing catastrophic forgetting while enabling continuous learning. our training nor test examples have dataset identifying information (e.g., a dataset identity, a dataset pass over the training examples, similar to Chaudhry et al. (2019). However, we also assume neither Learning ( G-NoCL ) framework. The G-NoCL framework is composed of four ψ descriptor). We argue that our lifelong language learning setup-where a model is presented with a stream of examples without an explicit identifier about which dataset (distribution) the examples come from-is a realistic setup to learn a general linguistic intelligence model. 1 Our experiments focus on lifelong language learning on two tasks-text classification and question answering. 2 Our main contributions in this paper are: · We introduce a lifelong language learning setup where the model needs to learn from a stream of examples from many datasets (presented sequentially) in one pass, and no dataset boundary or dataset identity is given to the model. Generators G (Sec. 3.2), (iii) an Ensembler represented by ∆ (Sec. 3.3), and (iv) the learner f θ . When a new concept is introduced, for which f θ needs to be learned, a generator g ∈ G generates images related to the given concept. However, generative models have limitations in terms of the finite diversity of their output [76,105]. Therefore, we try to maximize the output diversity of the generative model through the Prompt Refiner Module ψ . Furthermore, to enhance the diversity of generated images, we employ an ensemble approach by combining the outputs of a set of generators G through a complexity-aware ensembler ∆ . Instruction Tuning-based Methods. LGCL [81] introduces two key innovations: a refined prompt pool key query mechanism and category-level language guidance. The former leverages CLS features to optimize prompt selection with dynamic task-level mappings, improving accuracy and robustness, while the latter aligns output features with specific language representations in the vision transformer, enhancing task handling and model performance. integral components: (i) the Prompt Refiner Module (Sec. 3.1), (ii) a set of • We present an episodic memory model (§2) that augments an encoder-decoder model with a",
  "for sparse experience replay and local adaptation. · Weleverage progress in unsupervised pretraining to obtain good memory key representations 6 ONLINE CONTINUAL LEARNING": "memory module. Our memory is a key-value memory that stores previously seen examples and discuss strategies to manage the space complexity of the memory module.",
  "· Wecompare our proposed method to baseline and state-of-the-art continual learning methods 6.1 Hard Task Boundary": "and demonstrate its efficacy on text classification and question answering tasks (§4). 2 Model We consider a continual (lifelong) learning setup where a model needs to learn from a stream of training examples { x t , y t } T t =1 . We assume that all our training examples in the series come from multiple datasets of the same task (e.g., a text classification task, a question answering task), and each dataset comes one after the other. Since all examples come from the same task, the same model can be used to make predictions on all examples. A crucial difference between our continual learning 6.1.1 PLMs-based HTB. The Hard Task Boundary (HTB) setting has been developed to enable continuous knowledge acquisition by learning models from a dynamically changing stream of textual data, without the need for dataset identifiers. For example, ProgModel [163] have implemented HTB in slot filling, TAP-SLDA [125] have utilized it in audio classification, and AOS [173] have explored its use in automatic speech recognition. Contrast this with a more common setup where the model learns in a multitask setup (Ruder, 2017; McCann 1 et al., 2018). 2 McCann et al. (2018) show that many language processing tasks (e.g., classification, summarization, natural language inference, etc.) can be formulated as a question answering problem. 2 Traditional Methods. Continual learning (CL) methodologies, particularly pertinent to online scenarios, encompass a variety of approaches. These include parameter-isolation-based methods [33, 189], replay-based methods [55] and regularization-based methods [103, 173]. MBPA++ [33] (Figure 8a) introduces a framework for lifelong language learning, enabling a pre-trained model to learn continually from textual examples without requiring labeled datasets. It employs an episodic memory system with sparse experience replay and local adaptation techniques to prevent catastrophic forgetting. Extending this framework, Meta-MBPA++ [189] integrates three core lifelong learning principles, enhancing performance in text classification and question-answering tasks while using only 1% of the typical memory usage. CID [103] is devised for lifelong intent detection. This method uses cosine normalization, hierarchical knowledge distillation, and inter-class margin loss to tackle the challenges of data imbalances in the lifelong intent detection task, aiming to mitigate the negative impacts associated with these imbalances. 6.1.2 VLMs-based HTB. PEGP [138] utilizes various parameter-efficient tuning methods and an orthogonal gradient projection mechanism to prevent catastrophic forgetting in continual learning. Using backbone networks like ViT and CLIP, PEGP ensures that gradient updates during training are orthogonal to the feature subspace of previous tasks, preserving old knowledge while learning new ones. Manuscript submitted to ACM 16 Yutao Yang et al.",
  "6.2 Blurry Task Boundary": "6.2.1 PLMs-based BTB. MBPA++ [33] and Meta-MBPA++ [189] exemplify models capable of adapting to environments with indistinct task boundaries. TPEM [44] adopts a tripartite approach within an encoder-decoder framework, utilizing pruning, expanding, and masking techniques. Pruning preserves essential information from previous tasks, expansion increases model capacity for new tasks, and masking mitigates interference from prior task weights, thereby enhancing learning efficiency. Online meta-learning (OML) [67] and the neuromodulatory meta-learning algorithm (ANML) [5] are initially designed to learn sequences of tasks during the testing phase. Holla et al. [55] adapts these algorithms for conventional continual learning, focusing on performance evaluation on previous tasks. Their enhanced versions, OML-ER and ANML-ER, incorporate an episodic memory module for experience replay. The S6 framework addresses the challenges of continual relation learning in real-world scenarios, where data arrives in a streaming manner with noisy labels and shifting distributions. 6.2.2 VLMs-based BTB. DKR [30] is devised to mitigate the propagation of incorrect information in foundation LMs. It operates by initially leveraging an existing model to identify and exclude obsolete or erroneous knowledge when confronted with new data. Subsequently, a rectification process is employed to amend these inaccuracies while preserving valid data associations. SIT [181] addresses the challenge of asymmetry in Online Lifelong Learning (OLL) with VLMs, where the image encoder is tuned on the current batch of data, while the text encoder can access features from previously seen classes. SIT reformulates the loss function by restricting the text features in the comparison to only those present in the current batch, ensuring that both image and text feature updates remain balanced. OLiVia-Nav [130] is an innovative architecture designed for social navigation in human-centered environments like hospitals and offices, integrating VLMs with an online lifelong learning framework. The system employs a novel distillation approach, Social Context Contrastive Language Image Pre-training (SC-CLIP), to transfer the social reasoning capabilities of large VLMs into lightweight models, enabling real-time encoding of social and environmental context. Generative Name only Continual Learning (G-NoCL) [160] (Figure 8b) is a framework designed to overcome the challenges of manual annotation and web-scraped data in continual learning. The core of the model is the Prompt Refiner Module, which transforms basic concept names into diverse, context-rich prompts.",
  "7 DATASETS": "In this section, we review the typical datasets for offline and online continual learning for different tasks (Table 1).",
  "7.1 Offline Datasets for NLP": "",
  "7.1.1 Datasets for Classification.": "Text Classification. The most typical task for continual learning is text classification. The foundational text classification benchmark encompasses five text classification datasets introduced by [218], including AG News, Amazon Reviews, Yelp Reviews, DBpedia, and Yahoo Answers [166]. Particularly, the AG News dataset has 4 classes for news classification; the Amazon and Yelp dataset has 5 classes for sentiment analysis; the DBpedia dataset has 14 classes for Wikipedia text classification; and the Yahoo dataset has 10 classes for Q&A classification. Building upon this, Razdaibiedina et al. [149] developed a novel continual learning (CL) benchmark. This benchmark not only utilizes the foundational text classification benchmark but also integrates additional datasets from the GLUE benchmark [177], SuperGLUE benchmark [176], and the IMDB dataset [116]. DE&E [193] uses three common text classification data sets with different Manuscript submitted to ACM Recent Advances of Foundation Language Models-based Continual Learning: A Survey 17 Table 1. The statistics information of the existing CL datasets. #D/T/C means the number of domains/tasks/classes for DIL, TIL and CIL, respectively. Manuscript submitted to ACM 18 Yutao Yang et al. characteristics-News-groups, BBC News, and Consumer Finance Complaints2. Such datasets can be used to evaluate the models on tasks with different difficulty levels. The datasets introduced in [188] further are categorized into two groups based on the domain relevance between tasks: far-domain and near-domain. The far-domain group comprises two text classification tasks, which are foundational benchmarks [218] divided into topic classification (AG News, Yahoo Answers, DBpedia) and sentiment classification (Yelp, Amazon Reviews). In contrast, the near-domain group uses the Web of Science (WOS) [86] and 20 Newsgroups [87], which are restructured according to their high inter-task relevance. Intent Classification. Some studies focus on intent classification tasks, where the classes are quite different in various domains or scenarios. The dataset, as introduced in PAGeR [174], aims to tackle the lifelong intent detection problem by combining three public intent classification datasets (CLINC150 [88], HWU64 [106], BANKING77 [18]), one text classification dataset (Stackoverflow S20 [200]), and two public multidomain dialog intent detection datasets (SGD [148], MWOZ [12]). Moreover, FewRel [51] is also incorporated to tackle the lifelong relation extraction problem. This integration is intended to simulate real-world applications by encompassing a broad spectrum of domains and query distributions, thereby facilitating the development of more robust and versatile intent detection systems. Conversely, the dataset compiled in PLE [94] consolidates nine well-regarded intent detection datasets, including CLINC150 [88] and HWU64 [106], among others, arranged in a fixed random sequence to form a standardized benchmark. The dataset described by MeLL [178] specifically addresses intent detection within two distinct contexts: task-oriented dialogues (TaskDialog-EUIC) and real-world e-commerce interactions (Hotline-EUIC). TaskDialog-EUIC integrates data from Snips [29], TOP semantic parsing [49], and Facebook's Multilingual Task Oriented Dataset [156] into 90 tasks with overlapping label sets, amounting to over ten thousand samples. Fine-grained Sentiment Analysis. Ke et al. [79] develop a task incremental learning dataset for aspect-based sentiment classification (ABSC). This dataset aggregates reviews from four distinct sources, thereby enhancing its diversity and applicability across multiple domains. The sources include the L5Domains dataset by Hu et al. [59], the Liu3Domains dataset [102], the Ding9Domains dataset [35], and the SemEval14 dataset. 7.1.2 Datasets for Generation. Diverse datasets function as crucial benchmarks for exploring various dimensions of language and code generation. A particularly significant dataset highlighted in the work by Continual-T0 [157] focuses on English language generation tasks, including text simplification and empathetic dialogue generation, among others [8, 16]. In a subsequent study, Luo et al. [113] conduct an analysis of catastrophic forgetting on Bloomz [153] using Continual T0 datasets. The dataset, introduced in LAMOL [166], integrates elements from both DecaNLP [122] and the foundational text classification benchmark [218]. Moreover, the dataset devised in RVAE_LAMOL [179], employs three tasks from DecaNLP: the English Wizard of Oz (WOZ) for goal-oriented dialogue, QA-SRL for semantic role labeling in a SQuAD-style format, and SST, which is a binary version of the Stanford Sentiment Treebank. The dataset introduced in COPF [214] represents a pioneering effort in applying both TIL and DIL within the context of benchmarks that utilize existing human preferences. Specifically, the TIL framework in this dataset mandates that the model sequentially acquires knowledge from three distinct tasks. These include the question-answering task utilizing the HH-RLHF dataset [4], the summarization task based on the Reddit TL, DR dataset with human feedback [175], and the positive film review generation task using the IMDB dataset [116]. Meanwhile, the DIL framework requires the model to adapt to three distinct segments from the SHP dataset, as described by Ethayarajh et al. [42]. Manuscript submitted to ACM Recent Advances of Foundation Language Models-based Continual Learning: A Survey 19 The dataset described in ACM [219] explores sequence generation and categorizes tasks into 'similar\" and 'dissimilar\" groups based on their characteristics. Tasks classified as similar, including E2ENLG [132] and four domains (restaurant, hotel, TV, laptop) from RNNLG [192], demonstrate shared patterns and are tested across four sequence orders, comprising a total of five tasks. In contrast, dissimilar tasks such as WikiSQL [227], CNN/DailyMail [159], and MultiWOZ [12] exhibit significant distributional shifts from previously encountered tasks. The CODETASKCL dataset, explored by Yadav et al. [201], encompasses a diverse array of code-centric tasks, including code generation [63], summarization [62], translation [111], and refinement [171] across various programming languages. This dataset significantly enhances the breadth of language processing applications within technical fields. 7.1.3 Datasets for Information Extraction. The dataset introduced in ExtendNER [128], exemplifies a continual learning approach to Named Entity Recognition (NER). This dataset amalgamates the CoNLL-03 English NER [151] and OntoNotes [57], covering a broad spectrum of entity types and sources. This hybrid dataset is structured to challenge the adaptability and generalization capabilities of NER systems across varied contexts. Unlike the static nature of text in NER tasks, the Schema-Guided Dialog (SGD) [148] dataset, utilized in C-PT [232], serves the Dialog State Tracking aspect of IE, which involves maintaining the context of a dialog over time. The SGD dataset features 44 services across 19 domains, each treated as a separate task, and is designed to evaluate models on their ability to manage and extract information across conversational turns. Lastly, the lifelong SimpleQuestions and lifelong FewRel datasets, devised in [180] is crafted for the task of relation extraction. It merges elements from the SimpleQuestions [10] and FewRel [51] to form a lifelong learning benchmark that confronts the challenges of relation detection in a few-shot context. 7.1.4 Datasets for Continual Pre-training. In the realm of continual pre-training for LLMs, the development and utilization of specialized benchmarks play a pivotal role in evaluating and enhancing the effectiveness of continual learning systems. The dataset, introduced in CPT [74], primarily focuses on the continual post-training of LMs across a series of domain-specific, unlabeled datasets. It provides a rigorous test environment by using diverse corpora such as Yelp Restaurant Reviews [199], AI and ACL Papers [109], and AGNews articles [218]. Its main objective is to gauge how well an LM can incrementally integrate domain-specific knowledge without forgetting previously learned information, thereby enhancing its few-shot learning capabilities in these domains. Contrary to the datasets employed in CPT [74], which evaluate domain-specific adaptability and incremental learning, the CKL benchmark [66] is meticulously designed to measure the LM's ability to retain timeless knowledge, update obsolete information, and acquire new knowledge. It comprises subsets like INVARIANTLAMA, UPDATEDLAMA, and NEWLAMA, which are crafted to probe specific types of knowledge that an LM may encounter in its learning trajectory. Whereas the aforementioned two datasets assess more controlled dimensions of knowledge integration and retention, the dataset introduced in ELLE [142] focuses on the dynamic scenario of accumulating streaming data from diverse sources in a lifelong learning context. This dataset mirrors the real-world challenge of a LM that must continuously adapt to new data inflows from multiple domains, including BOOKCORPUS [233], NEWS ARTICLES [211], AMAZON REVIEWS [54], BIOMEDICAL PAPERS [109] and COMPUTER SCIENCE PAPERS [109]. The benchmark evaluates the LM's capacity to effectively integrate new information from these varied sources over time, highlighting the essential need for LMs to evolve in response to continual data growth and shifts in data distribution. Jin et al. [71] construct data streams to represent two prevalent types of domain shifts observed in practical scenarios. The first, a Domain-incremental Paper Stream, simulates the sequential evolution of research areas within academic papers, encompassing diverse disciplines such as biomedical and computer science. The second, a Chronologically-ordered Tweet Stream, models the temporal progression of tweets over time. Manuscript submitted to ACM 20 Yutao Yang et al. 7.1.5 Datasets for Hybrid Tasks. An increasing number of datasets are adopting a hybrid task approach that integrates multiple learning paradigms and task types, aimed at enhancing the adaptability of models. AdapterCL [117] introduces a dataset for task-oriented dialogue systems, which incorporates four task-oriented datasets: TaskMaster 2019 [14], TaskMaster 2020 [14], Schema Guided Dialogue [148], and MultiWoZ [12]. Continual Instruction Tuning Benchmark (CITB) [221] extends the concept of continual learning by focusing on instruction-based NLP tasks. Built on the comprehensive SuperNI [187] dataset, it includes over 1,600 tasks across diverse NLP categories. The ConTinTin [204] is an adaptation of the NATURAL-INSTRUCTIONS dataset, specifically restructured to facilitate a CL framework. The dataset, used in Conure [210], consists of Tencent TL (TTL) [209] and Movielens (ML). The TTL dataset is designed to address three item recommendation tasks and three user profiling tasks, whereas the ML dataset exclusively focuses on three item recommendation tasks. Furthermore, Kim et al. [83] introduced the proprietary NAVER Shopping dataset, which builds upon the previously mentioned datasets. The NAVER Shopping dataset features six tasks: two for search query prediction, two for purchased item category prediction, and two for user profiling, all designed to meet real-world industry requirements. Finally, the TRACE dataset, introduced by Wang et al. [185], is specifically designed to bridge the existing gap in the evaluation of LLMs within the CL framework, encompassing a wide range of complex and specialized tasks.",
  "7.2 Online Datasets for NLP": "7.2.1 Datasets for Classification. The foundational text classification benchmark, as introduced by Zhang et al. [218] has traditionally been applied in offline CL settings. Recent advancements have adapted this benchmark for online CL, notably in studies such as MBPA++ [33] and OML-ER [55] 7.2.2 Datasets for Generation. The dataset, used in MBPA++ [33], comprises three distinct question-answering collections: SQuAD 1.1 [146], TriviaQA [72], and QuAC [27]. SQuAD 1.1 is a reading comprehension dataset based on Wikipedia articles, designed to assess the ability to derive answers from structured text. TriviaQA consists of questionanswer pairs developed by trivia enthusiasts, accompanied by corroborative evidence sourced from both the web and Wikipedia, testing the model's capability to handle diverse information sources. QuAC adopts a dialog-style format in which a student queries about information in a Wikipedia article and a teacher responds using text directly from the article, challenging the model's interactive response generation. 7.2.3 Datasets for Information Extraction. The lifelong relation extraction benchmark, used in OML-ER [55], is structured by Wang et al. [180] based on FewRel. Unlike the original application by Wang et al., the benchmark in OML-ER is adapted for online continuous learning scenarios. 7.2.4 Datasets for Other Tasks. Hu et al. [58] compile the Firehose dataset, consisting of 110 million tweets from over 920,000 users between January 2013 and September 2019. This dataset is split into FIREHOSE 10M and FIREHOSE 100M. TemporalWiki [65] addresses temporal misalignment by serving as a lifelong benchmark that trains and evaluates LMs using consecutive snapshots of Wikipedia and Wikidata. This methodology assists in assessing an LM's capacity to both retain previously acquired knowledge and assimilate new information over time.",
  "7.3 Offline CL Datasets for Multi-modal Tasks": "The P9D dataset [231] consists of over one million image-text pairs from e-commerce data, organized into nine industry sector-based training tasks. It includes 1,014,599 training pairs, 2,846 for cross-modal retrieval tests, and 4,615 query Manuscript submitted to ACM Recent Advances of Foundation Language Models-based Continual Learning: A Survey 21 Fig. 9. Illustration of calculating metrics. ... ... ... ... ... ... ... ... ... ... ... ... ... Test task Training Task T1 T1 T2 T3 TN-1 TN T2 T3 T4 TN-1 TN T4 ... Training on task T k individually, test on T k Training on task {T 1 , ..., T k }, test on T i>k Training on task {T 1 , ..., T k }, test on T i<=k Training on task {T 1 , ..., T N }, test on T i<=N R0,1 R0,2 R0,3 R0,4 R0,N-1 R0,N R1,1 R1,2 R1,3 R1,4 R1,N-1 R1,N R2,1 R2,2 R2,3 R2,4 R2,N-1 R2,N R3,1 R3,2 R3,3 R3,4 R3,N-1 R3,N R4,1 R4,2 R4,3 R4,4 R4,N-1 R4,N RN-1,1 RN-1,2 RN-1,3 RN-1,4 RN-1,N-1 RN-1,N RN,1 RN,2 RN,3 RN,4 RN,N-1 RN,N pairs with 46,855 gallery pairs for multi-modal retrieval. Qian et al. [137] introduce two novel benchmarks for continual learning, namely CL-TDIUC and CL-VQA2.0, which are derived from the TDIUC [73] and VQA2.0 [47], respectively. These benchmarks are categorized into three scenarios: the Continual Vision Scenario, which deals with new visual scenes; the Continual Language Scenario, focusing on new questions in existing scenes; and the Continual VisionLanguage Scenario, addressing changes in both questions and visuals. DKR [30] comprises five benchmark datasets: MS-COCO Caption (MS-COCO) [96], Flickr30K [206], IAPR TC-12 [48], ECommerce-T2I (EC) [202], and RSICD [112]. Furthermore, two experimental scenarios are established. The first scenario involves a sequential processing of the datasets, specifically MS-COCO, Flickr30K, IAPR TC-12, EC, and RSICD, in that order. The second scenario, which builds on the approach proposed by Ni et al. [131], partitions the EC dataset into five sub-datasets for the training phase. The model's performance is subsequently tested on the Flickr30K, MS-COCO, and EC datasets. LAION-400M [155] and LAION-5B [154] are both CLIP-filtered, large-scale, open-access datasets designed to support multimodal models like CLIP, but they differ significantly in scale and scope. LAION-400M contains 400 million image-text pairs primarily in English, while LAION-5B is a much larger and more diverse dataset with 5.85 billion pairs, including multilingual and language-agnostic examples. CC3M [162] and CC12M [20] are large-scale datasets for vision-and-language pre-training, differing in scale, filtering, and diversity. CC3M has 3.3 million image-text pairs with strict filtering for high-precision captions, while CC12M, with 12.4 million pairs, relaxes these filters to capture a broader range of concepts, especially long-tail categories. COYO-700M [13] contains 747 million image-text pairs, designed to train various models with additional meta-attributes. Datacomp [43] is a benchmark designed to advance the creation of multimodal datasets, particularly for image-text models such as CLIP.",
  "8 METRICS": "In this section, we review the principal metrics commonly used to evaluate continual learning. These metrics can be categorized into three main types: (1) overall performance, which assesses the algorithm's effectiveness across all tasks; (2) memory stability, which measures the extent to which an algorithm retains previously acquired knowledge; and (3) learning plasticity, which evaluates the algorithm's capacity to acquire new skills or knowledge. Each of these metrics provides insights into different aspects of the algorithm's performance in a continual learning context. To begin, we establish the notation (Figure 9) used throughout the learning and evaluation phases of the model. Once the model completes a learning task, denoted as 𝑇 𝑖 , it evaluates its performance on a test set that encompasses all 𝑁 Manuscript submitted to ACM 22 Yutao Yang et al. tasks, where 𝑁 is the total number of tasks in the set 𝑇 . This evaluation is represented by a matrix 𝑅 ∈ R 𝑁 × 𝑁 , wherein each element 𝑅 𝑖,𝑗 indicates the model's test classification accuracy on task 𝑇 𝑗 after training on task 𝑇 𝑖 .",
  "8.1 Overall Performance.": "The metric termed 'Last\" [110, 226] evaluates the overall performance of a continual learning (CL) method upon the completion of all tasks. Specifically, it computes the average score from the last row in the performance matrix 𝑅 .  Also, Zheng et al. [226] devise the 'Avg\" score metric, which computes the mean accuracy across all datasets and timestamps.  « ‹ In the seminal works of Rebuffi et al. [150] and Douillard et al. [38], the concept of Average Incremental Accuracy (AIA) is introduced. This metric is specifically designed to quantify the historical performance across different tasks. It calculates the average performance for each task by considering the lower triangular portion of the matrix 𝑅 , effectively capturing the evolving competence of the system as new tasks are learned.  « ‹ The metric, termed Transfer, is derived by computing the average of the performance values for tasks that are represented in the upper-right triangle of matrix 𝑅 . This approach uniformly weights each dataset by averaging their performance across different tasks, thereby assessing the preservation of zero-shot transfer capabilities. Prior to commencing learning on task 𝑇 𝑖 , no fine-tuning is performed on tasks that precede 𝑇 𝑖 .  « ‹ Moreover, Chaudhry et al. [22] devise a metric known as Learning Curve Area (LCA), which quantifies the speed of learning in a model. Qin et al. [142] propose two metrics designed to evaluate pre-trained language models (PLMs) based on their performance within learned domains: Average Perplexity ( 𝐴𝑃 ) and Average Increased Perplexity ( 𝐴𝑃 + ).",
  "8.2 Memory Stability.": "Memory stability is typically evaluated by backward transfer [110] and forgetting measure [21]. Backward Transfer (BWT) emerges as a pivotal concept extensively documented in the literature, notably by Lopez et al. [110] and Wu et al. [196]. BWT measures the performance degradation on previously mastered tasks after the model is trained on new tasks. This performance degradation phenomenon is often referred to as 'forgetting\".  Additionally, Chaudhry et al. [21] introduce the Forgetting Measure (FM), a metric designed to quantify the extent of forgetting a model experiences for a specific task. A lower FM indicates better retention of previous tasks. Davari et al. Manuscript submitted to ACM Recent Advances of Foundation Language Models-based Continual Learning: A Survey 23 [31] propose linear probes (LP) to assess representation forgetting via an optimal linear classifier trained on the frozen activations of a base network. Representation forgetting is quantified by evaluating the change in Language Processing (LP) performance before and after the introduction of a new task. Kemker et al. [80] introduce three metrics, where Ω base assesses retention of initial learning, Ω new measures recall of new tasks, and Ω all evaluates overall proficiency in maintaining old knowledge and acquiring new information. Additionally, researchers [85] devise a novel metric, termed the Knowledge Loss Ratio (KLR), quantifies knowledge degradation using principles from information theory.",
  "8.3 Learning Plasticity.": "Evaluating learning plasticity can be effectively accomplished through two key metrics: forward transfer (FWT) [110] and intransigence measure (IM) [21]. Forward Transfer (FWT) [110] assesses the beneficial effects on the performance of subsequent tasks following a model's training on prior tasks.  where 𝑅 0 ,𝑖 denotes the performance metric associated with training on task 𝑖 independently. Higher values of FWT indicate superior model performance. Intransigence measure (IM), as defined by Chaudhry et al. [21], quantifies a model's inability to learn new tasks by comparing the performance difference of a task when trained jointly with other tasks versus. Moreover, Koh et al. [85] introduce novel metrics, known as Knowledge Gain Ratio (KGR), which quantifies the capacity to acquire new knowledge by calculating knowledge gain.",
  "8.4 Metrics for Continual Pre-training.": "CKL [66] introduces FUAR, which quantitatively measures the efficiency of each CKL method. It calculates the instances of time-invariant knowledge that a model forgets to learn or update one instance of new knowledge.",
  "8.5 Online CL-Specific Metrics.": "Near-future accuracy (NFA) [2] is introduced as a novel evaluation metric for OCL problems. Unlike traditional evaluation methods that assess models on immediately subsequent samples, NFA evaluates models on samples slightly further into the future, using a minimal shift 𝑆 . Such operation can mitigate label correlation effects, which can adversely impact the accuracy of model adaptability assessments. Yogatama et al. [205] proposed a novel online codelength, inspired by prequential encoding [9], to quantify how quickly an existing model can adapt to a new task.",
  "9 ANALYSIS AND DISCUSSION": "In this section, we compare the performance and characteristics of typical foundation model-based continual learning methods (Table 2 and Table 3). The effectiveness of each continual learning method is shaped by the specific task environment, resource constraints, and application goals. Replay-based methods like LAMOL, Meta-MBPA++, PAGeR, and COPF excel in scenarios where memory efficiency and maintaining a balance between old and new knowledge are essential. These methods strike a delicate balance between retaining previously acquired knowledge and adapting to new tasks. Parameter isolation is key to minimizing interference between highly distinct tasks. By allocating dedicated parameters to each task, these approaches prevent overlap and ensure that task-specific learning is preserved. Techniques such as AdapterCL, B-CL, Manuscript submitted to ACM 24 Yutao Yang et al. Table 2. Comparison of typical continual learning methods, where Reg. and Para. mean Regularization and Parameter-isolation. Manuscript submitted to ACM Recent Advances of Foundation Language Models-based Continual Learning: A Survey 25 Table 3. A summary of the performance of various Foundation LMs. A-RG: Average of ROUGE-1, -2 and -L scores; F1: F1 score; Acc.: Accuracy; MF1: Macro-F1; AMF1: averaged Macro-F1; JGA: Joint Goal Accuracy; SER: Slot Error Rate; AA: average accuracy; AIA: average incremental accuracy; FM: forgetting measure. Methods Dataset F1 Acc A-RG MF1 JGA SER BLEU AA AIA FM FWT BWT AMF1 LFPT5 [140] AGNews, Amazon, DBPedia, Yahoo - 52.71 - - - - - - - - - - - SLM(T5) [135] AGNews, Amazon, DBpedia, Yahoo - - - - - - - 73.10 - - - - - PP(T5) [149] AGNews, Amazon, DBpedia, Yahoo - - - - - - - 75.10 - - - - - MBPA++ [33] AGNews, Amazon, DBpedia, Yahoo, Yelp - - - - - - - 70.60 - - - - - IDBR [61] AGNews, Amazon, DBpedia, Yahoo, Yelp - - - - - - - 73.19 - - - - - EPI [188] AGNews, Amazon, DBpedia, Yahoo, Yelp - - - - - - - 74.43 - - - - - OML-ER [55] AGNews, Amazon, DBpedia, Yahoo, Yelp - - - - - - - 75.70 - - - - - O-LoRA[184] AGNews, Amazon, DBpedia, Yahoo, Yelp - - - - - - - 75.80 - - - - - LAMOL [166] AGNews, Amazon, DBPedia, Yahoo, Yelp - - - - - - - 76.50 - - - - - PP(Bert) [149] AGNews, Amazon, DBpedia, Yahoo, Yelp - - - - - - - 77.90 - - - - - Meta-MBPA++ [189] AGNews, Amazon, DBpedia, Yahoo, Yelp - - - - - - - 77.30 - - - - - SLM(Bert) [135] AGNews, Amazon, DBpedia, Yahoo, Yelp - - - - - - - 79.10 - - - - - B-CL [79] HL5Domains, Liu3Domains, Ding9Domains, SemEval14 - 88.29 - 81.40 - - - - - - - - - CLASSIC [77] HL5Domains, Liu3Domains, Ding9Domains, SemEval14 - 90.22 - 85.12 - - - - - - - - - CTR [76] HL5Domains, Liu3Domains, Ding9Domains, SemEval14 - 89.47 - 83.62 - - - - - - - - - LLM-CT [36] HL5Domains, Liu3Domains, Ding9Domains, SemEval14 - 94.91 - 91.43 - - - - - - - - - PP(Bert) [149] MNLI, CB, WiC, COPA, QQP, BoolQ, RTE, IMDB, Yelp, Ama- zon, SST2, DBpedia, AGNews, Yelp, Amazon, Yahoo - - - - - - - 69.3 - - - - - O-LoRA[184] MNLI, CB, WiC, COPA, QQP, BoolQ, RTE, IMDB, Yelp, Ama- zon, SST2, DBpedia, AGNews, Yelp, Amazon, Yahoo - - - - - - - 69.6 - - - - - PP(T5) [149] MNLI, CB, WiC, COPA, QQP, BoolQ, RTE, IMDB, Yelp, Ama- zon, SST2, DBpedia, AGNews, Yelp, Amazon, Yahoo - - - - - - - 79.5 - - - - - OML-ER [55] FewRel - 69.5 - - - - - - - - - - - DYNAMIC CONPET[165] FewRel - 88.62 - - - - - 88.62 - - - - - PAGeR [174] FewRel - - - - - - - 91.30 - - - - - CPPO [215] IMDB, HH-RLHF, Reddit TL, DR - - - - - - - 77.40 84.20 2.9 - -2.9 - COPF [214] IMDB, HH-RLHF, Reddit TL, DR - - - - - - - 86.30 85.10 -0.60 - 0.60 - LAMOL [166] SST, QA-SRL, WOZ - - - - - - - 81.00 - - - - - RVAE_LAMOL [179] SST, QA-SRL, WOZ - - - - - - - 81.60 - - - - - PLE(5-shot regime) [94] CLINC150, ATIS, HWU64, BANKING77, MTOP, SNIPS, LEYZER, MSLU, TOP - - - - - - - 85.61 - - - - - PLE(10-shot regime) [94] CLINC150, ATIS, HWU64, BANKING77, MTOP, SNIPS, LEYZER, MSLU, TOP - - - - - - - 88.16 - - - - - MBPA++ [33] QuAC, TrWeb, TrWik, SQuAD - - - - - - - - - - - - 62.40 Meta-MBPA++ [189] QuAC, TrWeb, TrWik, SQuAD - - - - - - - - - - - - 64.90 CPT [74] Yelp Restaurant, AI Papers, ACL Papers, AGNews - 52.59 - 46.41 - - - - - - - - - Continual DAP-training [78] Yelp Restaurant, Amazon Phone, Amazon Camera, ACL Papers, AI Papers, PubMed Papers - 81.91 - 77.93 - - - - - - - - - DYNAMIC CONPET[165] OntoNotes - 84.47 - - - - - 85.83 - - - - - LFPT5 [140] CoNLL03, OntoNotes 47.59 - - - - - - - - - - - - AdapterCL [117] TM19, TM20, SGD, MultiWoZ - 90.50 - - 35.10 31.78 16.76 - - - - - - RMR_ DSE [92] MultiWoZ-2.0 - - - - - 48.79 39.86 - - - - - - DEMIX [50] In-Car Assistant, Multi-WOZ 2.1, CamRest 45.84 - - - - - 11.96 - - - - - - LFPT5 [140] CNNDM, WikiHow, XSum - - 17.05 - - - - - - - - - - C-PT [232] Schema-Guided Dialog dataset (SGD) - - - - 61.20 - - - - - 13.70 0.50 - Lifelong-MoE [24] TriviaQA 20.22 - - - - - - - - - - - - Lifelong-MoE [24] WMT16 - - - - - - 19.16 - - - - - - CTR [76] 10 DSC datasets - 89.31 - 88.75 - - - - - - - - - CTR [76] 20News - 95.25 - 95.23 - - - - - - - - - MeLL [178] TaskDialog-EUIC - - - - - - - 93.79 - - - - 93.42 MeLL [178] Hotline-EUIC - - - - - - - 96.73 - - - - 93.41 PCLL [223] HWU, BANKING, CLINC, SNIPS, TOP - - - - - - - 90.25 - - - - - PCLL [223] SNIPS, AITS, DSTC, MIT-MOVIE, MIT-RESTAURANT - - - - - - - - - - - - 74.48 SLM(LLaMA) [135] Medical, MMLU, Finance - - - - - - - 82.3 - - - - - EPI [188] Web of Science (WOS) - - - - - - - 77.83 - - - - - IDBR [61] Yahoo, AGNews, Yelp - - - - - - - 72.53 - - - - - and EPI are particularly effective in environments with highly variable tasks, where reducing interference is critical. Regularization techniques, on the other hand, enhance stability in structured task settings. These methods constrain model updates to protect knowledge from previous tasks, making them ideal in situations with well-defined task boundaries. Regularization-based approaches like RMR_DSE, CLASSIC, and EWC mitigate catastrophic forgetting by limiting model changes, thereby preserving prior learning. These methods are particularly effective in domainincremental learning, where tasks evolve gradually, and long-term stability is vital. First, most continual learning (CL) methods are built on PLMs like BERT, RoBERTa, and BART, or smaller-scale LLMs such as LLaMA. Models leveraging LLMs often outperform PLM-based ones, thanks to their greater capacity for Manuscript submitted to ACM 26 Yutao Yang et al. handling complex tasks and enhanced adaptability. Second, the majority of CL methods are tailored for offline settings, where the full target dataset is assumed to be available for training. However, in real-world scenarios, data typically arrives in a streaming fashion, making it challenging to anticipate distribution shifts and effectively manage dynamic data. Third, a significant hurdle in the field is the lack of unified benchmarks and standardized metrics for evaluating CL algorithms. This fragmentation complicates the comparison of methods across tasks and domains. To propel the field forward, there is a critical need for benchmark datasets and robust metrics that can consistently gauge the performance of CL models, particularly in dynamic, data-streaming environments. These benchmarks should span diverse tasks and modalities to ensure comprehensive evaluation across various domains.",
  "10 CHALLENGES AND FURTHER WORK": "Autonomous Continual Learning. Most existing studies in the domain of continual learning assume static datasets with known distributions in a relatively closed environment. Moreover, these studies mainly focus on simple tasks (e.g., text classification, sentiment analysis and intent classification) with clear labels. These assumptions do not hold in realworld applications, where environments continually evolve and introduce novel stimuli. A key challenge is developing continual learning models that can autonomously detect and adapt to data distribution shifts, thereby improving the applicability of AI in dynamic real-world scenarios. Liu et al. [100] recently proposed the SOLA framework to address these limitations by facilitating autonomous adaptation in AI systems. Despite this progress, significant challenges remain in enabling these systems to adjust to new, dynamic environments without ongoing human oversight. Bridging Machine Learning and Cognitive Science in Continual Learning. While continual learning aims to mimic human-like learning, a gap persists in how well these methods align with cognitive science findings on human continuous learning. In human cognition, processes such as rehearsal, memory consolidation, and adaptive forgetting are key to learning in dynamic environments. In contrast, machine learning models often face challenges like catastrophic forgetting, a phenomenon without a clear biological equivalent. Bridging these fields could enhance the design of continual learning algorithms by drawing on cognitive science insights, potentially leading to more robust, adaptive, and human-like learning systems. Learning Knowledge from Conversation. Traditional AI systems are typically trained on static data sets, which starkly contrasts with human conversational learning that dynamically updates knowledge through interaction [99]. The challenge for AI lies in transitioning from static data learning to more dynamic, conversational engagements. The future direction in this area could involve the development of models that mimic human conversational learning processes, capable of context adaptation, new concept inference, and dynamic knowledge application within ongoing interactions. Multi-modal Continual Learning. Continual learning research has predominantly concentrated on natural language processing tasks such as sentiment analysis and text classification. Recent studies have begun exploring basic multimodal tasks, such as text-to-image retrieval, text-image classification, and visual question answering. The integration of diverse data types-textual, visual, and auditory-poses a substantial challenge. Future studies should expand to more complex multi-modal datasets and strive to devise methodologies that effectively synthesize these varied modalities, thereby enhancing the model's capability to maintain continuous learning across different sensory inputs. Privacy Protection in Continual Learning. Privacy protection in continual learning systems poses a significant challenge, particularly as these systems are designed to continuously update and refine their models based on incoming data streams. Unlike traditional static machine learning models, continual learning systems frequently access and process sensitive data across different contexts and time periods, raising substantial concerns about data confidentiality Manuscript submitted to ACM Recent Advances of Foundation Language Models-based Continual Learning: A Survey 27 and user privacy. Effective privacy-preserving mechanisms must be integrated into the architecture of these systems to ensure that they do not inadvertently expose or misuse personal data. Techniques such as differential privacy [41], federated learning [212], and secure multi-party computation [46] offer promising solutions by allowing models to learn from decentralized data sources without needing to access the actual data directly. Continual Alignment. In dynamic real-world environments, human preferences and requirements are ever-evolving, making continual alignment crucial for maintaining AI system effectiveness. Traditional alignment methods often assume static preferences, which is insufficient in changing contexts. Integrating Reinforcement Learning from Human Feedback (RLHF) into continual learning presents a unique challenge, as models must adapt to new preferences while retaining previously learned ones. Future research should prioritize developing methods like CPPO and COPF that balance integrating new feedback with preserving past alignment, ensuring long-term adaptability to human needs. Robust Continual Learning. The existing studies mainly focus on designing a continual learning model to improve the performance of forgetting and transferring with various metrics while the robustness of continual learning systems is not well studied. It is critical, especially in applications where safety and reliability are paramount. The main challenges include evaluating the robustness of these systems against adversarial attacks or when faced with drastically changing environments. Future research could focus on developing evaluation metrics for robustness in continual learning and designing systems that maintain performance reliability over time despite environmental changes. Large-Scale and High-Quality Datasets and Benchmarks. As discussed in Section 7, most of the datasets are constructed by merging the existing datasets. This often results in datasets that lack diversity and real-world complexity, which hampers the development of robust and adaptable continual learning models. The creation of large-scale, high-quality datasets that accurately reflect real-world complexities represents a critical challenge. Moving forward, the development of such datasets and benchmarks will be essential not only for assessing the efficacy of continual learning algorithms but also for pushing the limits of what these algorithms can achieve in practical settings.",
  "11 CONCLUSIONS": "This survey provides an in-depth exploration of continual learning (CL) methodologies tailored for foundation language models (LMs), such as pre-trained language models (PLMs), large language models (LLMs), and vision-language models (VLMs). By integrating the dynamic adaptability of CL with the robust foundational capabilities of LMs, this field promises to significantly advance the state of artificial intelligence. We categorize existing research into offline and online continual learning paradigms, offering a clear distinction between the settings and methodologies used within these frameworks. Offline CL is discussed in terms of domain-incremental, task-incremental, and class-incremental learning. Meanwhile, online CL is analyzed with a focus on the delineation between hard and blurry task boundaries, providing insights into how these approaches handle real-time data streams. Our review of the literature not only clarifies the current landscape of CL approaches for foundation LMs but also emphasizes the innovative integration of continual pre-training, parameter-efficient tuning, and instruction tuning methods that are specifically designed to leverage the vast capabilities of foundation LMs. Furthermore, we highlight the main characteristics of datasets used in this domain and the metrics that effectively measure both the mitigation of catastrophic forgetting and the enhancement of knowledge transfer. This work hopes to inspire further research that will ultimately lead to more robust, efficient, and intelligent systems capable of lifelong learning. Manuscript submitted to ACM 28 Yutao Yang et al.",
  "REFERENCES": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023). [2] Hasan Abed Al Kader Hammoud, Ameya Prabhu, Ser-Nam Lim, Philip HS Torr, Adel Bibi, and Bernard Ghanem. 2023. Rapid Adaptation in Online Continual Learning: Are We Evaluating It Right?. In ICCV . 18852-18861. [3] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars. 2018. Memory aware synapses: Learning what (not) to forget. In ECCV . 139-154. [4] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 (2022). [5] Shawn Beaulieu, Lapo Frati, Thomas Miconi, Joel Lehman, Kenneth O Stanley, Jeff Clune, and Nick Cheney. 2020. Learning to continually learn. arXiv preprint arXiv:2002.09571 (2020). [6] Eden Belouadah, Adrian Popescu, and Ioannis Kanellos. 2021. A comprehensive study of class incremental learning algorithms for visual tasks. Neural Networks 135 (2021), 38-54. [7] Magdalena Biesialska, Katarzyna Biesialska, and Marta R Costa-jussà. 2020. Continual Lifelong Learning in Natural Language Processing: A Survey. In COLING . 6523-6541. [8] Raad Bin Tareaf. 2017. Tweets Dataset - Top 20 most followed users in Twitter social platform. [9] Léonard Blier and Yann Ollivier. 2018. The description length of deep learning models. NeurIPS 31 (2018). [10] Antoine Bordes, Nicolas Usunier, Sumit Chopra, and Jason Weston. 2015. Large-scale simple question answering with memory networks. arXiv preprint arXiv:1506.02075 (2015). [11] Tom B Brown. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165 (2020). [12] Paweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Inigo Casanueva, Stefan Ultes, Osman Ramadan, and Milica Gašić. 2018. MultiWOZ-a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling. arXiv preprint arXiv:1810.00278 (2018). [13] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. 2022. Coyo-700m: Image-text pair dataset. [14] Bill Byrne, Karthik Krishnamoorthi, Chinnadhurai Sankar, Arvind Neelakantan, Daniel Duckworth, Semih Yavuz, Ben Goodrich, Amit Dubey, Andy Cedilnik, and Kyu-Young Kim. 2019. Taskmaster-1: Toward a realistic and diverse dialog dataset. arXiv preprint arXiv:1909.05358 (2019). [15] Samuel Cahyawijaya, Holy Lovenia, Tiezheng Yu, Willy Chung, and Pascale Fung. 2023. InstructAlign: High-and-Low Resource Language Alignment via Continual Crosslingual Instruction Tuning. In WSSANLP . 55-78. [16] Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-snli: Natural language inference with natural language explanations. NeurIPS 31 (2018). [17] Xusheng Cao, Haori Lu, Linlan Huang, Xialei Liu, and Ming-Ming Cheng. 2024. Generative Multi-modal Models are Good Class-Incremental Learners. arXiv preprint arXiv:2403.18383 (2024). [18] Iñigo Casanueva, Tadas Temčinas, Daniela Gerz, Matthew Henderson, and Ivan Vulić. 2020. Efficient intent detection with dual sentence encoders. arXiv preprint arXiv:2003.04807 (2020). [19] Giuseppe Castellucci, Simone Filice, Danilo Croce, and Roberto Basili. 2021. Learning to Solve NLP Tasks in an Incremental Number of Languages. In ACL . 837-847. [20] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. 2021. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR . 3558-3568. [21] Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajanthan, and Philip HS Torr. 2018. Riemannian walk for incremental learning: Understanding forgetting and intransigence. In ECCV . 532-547. [22] Arslan Chaudhry, Marc'Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. 2019. Efficient Lifelong Learning with A-GEM. In ICLR . [23] Sanyuan Chen, Yutai Hou, Yiming Cui, Wanxiang Che, Ting Liu, and Xiangzhan Yu. 2020. Recall and learn: Fine-tuning deep pretrained language models with less forgetting. arXiv preprint arXiv:2004.12651 (2020). [24] Wuyang Chen, Yanqi Zhou, Nan Du, Yanping Huang, James Laudon, Zhifeng Chen, and Claire Cui. 2023. Lifelong language pretraining with distribution-specialized experts. In ICML . PMLR, 5383-5395. [25] Daixuan Cheng, Shaohan Huang, and Furu Wei. 2023. Adapting large language models via reading comprehension. arXiv preprint arXiv:2309.09530 (2023). [26] Xin Cheng, Yankai Lin, Xiuying Chen, Dongyan Zhao, and Rui Yan. 2023. Decouple knowledge from paramters for plug-and-play language modeling. In ACL . 14288-14308. [27] Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer. 2018. QuAC: Question Answering in Context. In EMNLP . 2174-2184. [28] Andrea Cossu, Tinne Tuytelaars, Antonio Carta, Lucia Passaro, Vincenzo Lomonaco, and Davide Bacciu. 2022. Continual pre-training mitigates forgetting in language and vision. arXiv preprint arXiv:2205.09357 (2022). [29] Alice Coucke, Alaa Saade, Adrien Ball, Théodore Bluche, Alexandre Caulier, David Leroy, Clément Doumouro, Thibault Gisselbrecht, Francesco Caltagirone, Thibaut Lavril, et al. 2018. Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces. arXiv preprint arXiv:1805.10190 (2018). Manuscript submitted to ACM Recent Advances of Foundation Language Models-based Continual Learning: A Survey 29 [30] Zhenyu Cui, Yuxin Peng, Xun Wang, Manyu Zhu, and Jiahuan Zhou. 2024. Continual Vision-Language Retrieval via Dynamic Knowledge Rectification. In AAAI , Vol. 38. 11704-11712. [31] MohammadReza Davari, Nader Asadi, Sudhir Mudur, Rahaf Aljundi, and Eugene Belilovsky. 2022. Probing representation forgetting in supervised and unsupervised continual learning. In CVPR . 16712-16721. [32] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Aleš Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. 2021. A continual learning survey: Defying forgetting in classification tasks. TPAMI 44, 7 (2021), 3366-3385. [33] Cyprien de Masson D'Autume, Sebastian Ruder, Lingpeng Kong, and Dani Yogatama. 2019. Episodic memory in lifelong language learning. NeurIPS 32 (2019). [34] Jacob Devlin. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018). [35] Xiaowen Ding, Bing Liu, and Philip S Yu. 2008. A holistic lexicon-based approach to opinion mining. In WSDM . 231-240. [36] Xuanwen Ding, Jie Zhou, Liang Dou, Qin Chen, Yuanbin Wu, Chengcai Chen, and Liang He. 2024. Boosting Large Language Models with Continual Learning for Aspect-based Sentiment Analysis. In EMNLP . [37] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. A survey on in-context learning. arXiv preprint arXiv:2301.00234 (2022). [38] Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas Robert, and Eduardo Valle. 2020. Podnet: Pooled outputs distillation for small-tasks incremental learning. In ECCV . 86-102. [39] Mingzhe Du, Anh Tuan Luu, Bin Ji, and See-kiong Ng. 2023. From Static to Dynamic: A Continual Learning Framework for Large Language Models. arXiv preprint arXiv:2310.14248 (2023). [40] Yifan Du, Zikang Liu, Junyi Li, and Wayne Xin Zhao. 2022. A Survey of Vision-Language Pre-Trained Models. In IJCAI . 5436-5443. [41] Cynthia Dwork. 2006. Differential privacy. In International colloquium on automata, languages, and programming . 1-12. [42] Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. 2022. Understanding dataset difficulty with V-usable information. In ICML . 5988-6008. [43] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. 2024. Datacomp: In search of the next generation of multimodal datasets. NeurIPS 36 (2024). [44] Binzong Geng, Fajie Yuan, Qiancheng Xu, Ying Shen, Ruifeng Xu, and Min Yang. 2021. Continual Learning for Task-oriented Dialogue System with Iterative Network Pruning, Expanding and Masking. In ACL . 517-523. [45] Evangelia Gogoulou, Timothée Lesort, Magnus Boman, and Joakim Nivre. 2023. A study of continual learning under language shift. arXiv preprint arXiv:2311.01200 (2023). [46] Oded Goldreich. 1998. Secure multi-party computation. Manuscript. Preliminary version 78, 110 (1998), 1-108. [47] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In CVPR . 6904-6913. [48] Michael Grubinger, Paul Clough, Henning Müller, and Thomas Deselaers. 2006. The iapr tc-12 benchmark: A new evaluation resource for visual information systems. In International workshop ontoImage , Vol. 2. [49] Sonal Gupta, Rushin Shah, Mrinal Mohit, Anuj Kumar, and Mike Lewis. 2018. Semantic parsing for task oriented dialog using hierarchical representations. arXiv preprint arXiv:1810.07942 (2018). [50] Suchin Gururangan, Mike Lewis, Ari Holtzman, Noah A Smith, and Luke Zettlemoyer. 2022. DEMix Layers: Disentangling Domains for Modular Language Modeling. In NAACL . 5557-5576. [51] Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao, Zhiyuan Liu, and Maosong Sun. 2018. FewRel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation. arXiv preprint arXiv:1810.10147 (2018). Zeyu Han, Chao Gao, Jinyang Liu, Sai Qian Zhang, et al. 2024. Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey. [52] arXiv preprint arXiv:2403.14608 (2024). [53] Tyler L Hayes, Giri P Krishnan, Maxim Bazhenov, Hava T Siegelmann, Terrence J Sejnowski, and Christopher Kanan. 2021. Replay in deep learning: Current approaches and missing biological elements. Neural computation 33, 11 (2021), 2908-2950. [54] Ruining He and Julian McAuley. 2016. Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering. In WWW . 507-517. [55] Nithin Holla, Pushkar Mishra, Helen Yannakoudakis, and Ekaterina Shutova. 2020. Meta-learning with sparse experience replay for lifelong language learning. arXiv preprint arXiv:2009.04891 (2020). [56] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. In ICML . 2790-2799. [57] Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. OntoNotes: The 90% Solution. In NAACL . 57-60. [58] Hexiang Hu, Ozan Sener, Fei Sha, and Vladlen Koltun. 2022. Drinking from a firehose: Continual learning with web-scale natural language. TPAMI 45, 5 (2022), 5684-5696. [59] Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In SIGKDD . 168-177. [60] Linlan Huang, Xusheng Cao, Haori Lu, and Xialei Liu. 2024. Class-Incremental Learning with CLIP: Adaptive Representation Adjustment and Parameter Fusion. arXiv preprint arXiv:2407.14143 (2024). [61] Yufan Huang, Yanzhe Zhang, Jiaao Chen, Xuezhi Wang, and Diyi Yang. 2021. Continual Learning for Text Classification with Information Disentanglement Based Regularization. In ACL . 2736-2746. Manuscript submitted to ACM 30 Yutao Yang et al. [62] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Codesearchnet challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436 (2019). [63] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2018. Mapping language to code in programmatic context. arXiv preprint arXiv:1808.09588 (2018). [64] Joel Jang, Seungone Kim, Seonghyeon Ye, Doyoung Kim, Lajanugen Logeswaran, Moontae Lee, Kyungjae Lee, and Minjoon Seo. 2023. Exploring the benefits of training expert language models over instruction tuning. In ICML . 14702-14729. [65] Joel Jang, Seonghyeon Ye, Changho Lee, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, and Minjoon Seo. 2022. TemporalWiki: A Lifelong Benchmark for Training and Evaluating Ever-Evolving Language Models. In EMNLP . 6237-6250. [66] Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, Stanley Jungkyu Choi, and Minjoon Seo. 2022. Towards Continual Knowledge Learning of Language Models. In ICLR . [67] Khurram Javed and Martha White. 2019. Meta-learning representations for continual learning. NeurIPS 32 (2019). [68] Saurav Jha, Dong Gong, and Lina Yao. 2024. CLAP4CLIP: Continual Learning with Probabilistic Finetuning for Vision-Language Models. arXiv preprint arXiv:2403.19137 (2024). [69] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021. Scaling up visual and vision-language representation learning with noisy text supervision. In International conference on machine learning . PMLR, 4904-4916. [70] Xisen Jin, Bill Yuchen Lin, Mohammad Rostami, and Xiang Ren. 2021. Learn Continually, Generalize Rapidly: Lifelong Knowledge Accumulation for Few-shot Learning. In EMNLP . 714-729. [71] Xisen Jin, Dejiao Zhang, Henghui Zhu, Wei Xiao, Shang-Wen Li, Xiaokai Wei, Andrew Arnold, and Xiang Ren. 2022. Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora. In NAACL . 4764-4780. [72] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. In ACL . 1601-1611. [73] Kushal Kafle and Christopher Kanan. 2017. An analysis of visual question answering algorithms. In ICCV . 1965-1973. [74] Zixuan Ke, Haowei Lin, Yijia Shao, Hu Xu, Lei Shu, and Bing Liu. 2022. Continual Training of Language Models for Few-Shot Learning. In 10205-10216. [75] Zixuan Ke and Bing Liu. 2022. Continual learning of natural language processing tasks: A survey. arXiv preprint arXiv:2211.12701 (2022). [76] Zixuan Ke, Bing Liu, Nianzu Ma, Hu Xu, and Lei Shu. 2021. Achieving Forgetting Prevention and Knowledge Transfer in Continual Learning. In NeurIPS , Vol. 34. 22443-22456. [77] Zixuan Ke, Bing Liu, Hu Xu, and Lei Shu. 2021. CLASSIC: Continual and Contrastive Learning of Aspect Sentiment Classification Tasks. In EMNLP . [78] Zixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi, Gyuhak Kim, and Bing Liu. 2023. Continual Pre-training of Language Models. In ICLR . [79] Zixuan Ke and Hu Xu. 2021. Adapting BERT for Continual Learning of a Sequence of Aspect Sentiment Classification Tasks. In NAACL . [80] Ronald Kemker, Marc McClure, Angelina Abitino, Tyler Hayes, and Christopher Kanan. 2018. Measuring catastrophic forgetting in neural networks. In AAAI , Vol. 32. [81] Muhammad Gul Zain Ali Khan, Muhammad Ferjad Naeem, Luc Van Gool, Didier Stricker, Federico Tombari, and Muhammad Zeshan Afzal. 2023. Introducing language guidance in prompt-based continual learning. In ICCV . 11463-11473. [82] Junsu Kim, Yunhoe Ku, Jihyeon Kim, Junuk Cha, and Seungryul Baek. 2024. VLM-PL: Advanced Pseudo Labeling approach Class Incremental Object Detection with Vision-Language Model. arXiv preprint arXiv:2403.05346 (2024). [83] Sein Kim, Namkyeong Lee, Donghyun Kim, Minchul Yang, and Chanyoung Park. 2023. Task Relation-aware Continual User Representation Learning. In SIGKDD . 1107-1119. [84] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. 2017. Overcoming catastrophic forgetting in neural networks. NAS 114, 13 (2017), 3521-3526. [85] Hyunseo Koh, Minhyuk Seo, Jihwan Bang, Hwanjun Song, Deokki Hong, Seulki Park, Jung-Woo Ha, and Jonghyun Choi. 2022. Online Boundary-Free Continual Learning by Scheduled Data Prior. In ICLR . [86] Kamran Kowsari, Donald E Brown, Mojtaba Heidarysafa, Kiana Jafari Meimandi, Matthew S Gerber, and Laura E Barnes. 2017. Hdltex: Hierarchical deep learning for text classification. In ICMLA . 364-371. [87] Ken Lang. 1995. Newsweeder: Learning to filter netnews. In Machine learning proceedings . 331-339. [88] Stefan Larson, Anish Mahendran, Joseph J Peper, Christopher Clarke, Andrew Lee, Parker Hill, Jonathan K Kummerfeld, Kevin Leach, Michael A Laurenzano, Lingjia Tang, et al. 2019. An evaluation dataset for intent classification and out-of-scope prediction. arXiv preprint arXiv:1909.02027 (2019). [89] Kuan-Ying Lee, Yuanyi Zhong, and Yu-Xiong Wang. 2023. Do pre-trained models benefit equally in continual learning?. In WCACV . 6485-6493. [90] Sungjin Lee. 2017. Toward continual learning for conversational agents. arXiv preprint arXiv:1712.09943 (2017). [91] Timothée Lesort, Vincenzo Lomonaco, Andrei Stoian, Davide Maltoni, David Filliat, and Natalia Díaz-Rodríguez. 2020. Continual learning for robotics: Definition, framework, learning strategies, opportunities and challenges. Information fusion 58 (2020), 52-68. [92] Dingcheng Li, Zheng Chen, Eunah Cho, Jie Hao, Xiaohu Liu, Fan Xing, Chenlei Guo, and Yang Liu. 2022. Overcoming catastrophic forgetting during domain adaptation of seq2seq language generation. In NAACL . 5441-5454. [93] Guozheng Li, Peng Wang, Qiqing Luo, Yanhe Liu, and Wenjun Ke. 2023. Online noisy continual relation learning. In AAAI , Vol. 37. 13059-13066. [94] Guodun Li, Yuchen Zhai, Qianglong Chen, Xing Gao, Ji Zhang, and Yin Zhang. 2022. Continual few-shot intent detection. In COLING . 333-343. Manuscript submitted to ACM EMNLP . Recent Advances of Foundation Language Models-based Continual Learning: A Survey 31 [95] Jingyao Li, Pengguang Chen, Sitong Wu, Chuanyang Zheng, Hong Xu, and Jiaya Jia. 2024. RoboCoder: Robotic Learning from Basic Skills to General Tasks with Large Language Models. arXiv preprint arXiv:2406.03757 (2024). [96] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In ECCV . 740-755. [97] Yong Lin, Hangyu Lin, Wei Xiong, Shizhe Diao, Jianmeng Liu, Jipeng Zhang, Rui Pan, Haoxiang Wang, Wenbin Hu, Hanning Zhang, Hanze Dong, Renjie Pi, Han Zhao, Nan Jiang, Heng Ji, Yuan Yao, and Tong Zhang. 2024. Mitigating the Alignment Tax of RLHF. arXiv:2309.06256 [cs.LG] [98] Bing Liu. 2020. Learning on the job: Online lifelong and continual learning. In AAAI , Vol. 34. 13544-13549. [99] Bing Liu and Sahisnu Mazumder. 2021. Lifelong and continual learning dialogue systems: learning during conversation. In AAAI , Vol. 35. 15058-15063. [100] Bing Liu, Sahisnu Mazumder, Eric Robertson, and Scott Grigsby. 2023. AI Autonomy: Self-initiated Open-world Continual Learning and Adaptation. AI Magazine (2023). [101] Jiaming Liu, Chenxuan Li, Guanqun Wang, Lily Lee, Kaichen Zhou, Sixiang Chen, Chuyan Xiong, Jiaxin Ge, Renrui Zhang, and Shanghang Zhang. 2024. Self-Corrected Multimodal Large Language Model for End-to-End Robot Manipulation. arXiv preprint arXiv:2405.17418 (2024). [102] Qian Liu, Zhiqiang Gao, Bing Liu, and Yuanlin Zhang. 2015. Automated rule selection for aspect extraction in opinion mining. In IJCAI . [103] Qingbin Liu, Xiaoyan Yu, Shizhu He, Kang Liu, and Jun Zhao. 2021. Lifelong intent detection via multi-strategy rebalancing. arXiv preprint arXiv:2108.04445 (2021). [104] Tianlin Liu, Lyle Ungar, and João Sedoc. 2019. Continual Learning for Sentence Representations Using Conceptors. In NAACL . 3274-3279. [105] Xialei Liu, Xusheng Cao, Haori Lu, Jia-wen Xiao, Andrew D Bagdanov, and Ming-Ming Cheng. 2023. Class Incremental Learning with Pre-trained Vision-Language Models. arXiv preprint arXiv:2310.20348 (2023). [106] Xingkun Liu, Arash Eshghi, Pawel Swietojanski, and Verena Rieser. 2021. Benchmarking natural language understanding services for building conversational agents. In International Workshop on Spoken Dialogue Systems . 165-183. [107] Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2022. P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks. In ACL . 61-68. [108] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019). [109] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. 2020. S2ORC: The Semantic Scholar Open Research Corpus. In ACL . 4969-4983. [110] David Lopez-Paz and Marc'Aurelio Ranzato. 2017. Gradient episodic memory for continual learning. NeurIPS 30 (2017). [111] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al. 2021. Codexglue: A machine learning benchmark dataset for code understanding and generation. arXiv preprint arXiv:2102.04664 (2021). [112] Xiaoqiang Lu, Binqiang Wang, Xiangtao Zheng, and Xuelong Li. 2017. Exploring models and data for remote sensing image caption generation. IEEE Transactions on Geoscience and Remote Sensing 56, 4 (2017), 2183-2195. [113] Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. 2023. An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning. arXiv e-prints (2023). [114] Shirong Ma, Shen Huang, Shulin Huang, Xiaobin Wang, Yangning Li, Hai-Tao Zheng, Pengjun Xie, Fei Huang, and Yong Jiang. 2023. EcomGPT-CT: Continual pre-training of e-commerce large language models with semi-structured data. arXiv preprint arXiv:2312.15696 (2023). [115] Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2024. Eureka: Human-Level Reward Design via Coding Large Language Models. In ICLR . [116] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning Word Vectors for Sentiment Analysis. In ACL . 142-150. [117] Andrea Madotto, Zhaojiang Lin, Zhenpeng Zhou, Seungwhan Moon, Paul Crook, Bing Liu, Zhou Yu, Eunjoon Cho, and Zhiguang Wang. 2020. Continual learning in task-oriented dialogue systems. arXiv preprint arXiv:2012.15504 (2020). [118] Aru Maekawa, Hidetaka Kamigaito, Kotaro Funakoshi, and Manabu Okumura. 2023. Generative Replay Inspired by Hippocampal Memory Indexing for Continual Language Learning. In EACL . 930-942. [119] Zheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe, Hyunwoo Kim, and Scott Sanner. 2022. Online continual learning in image classification: An empirical survey. Neurocomputing 469 (2022), 28-51. [120] Marc Masana, Xialei Liu, Bartłomiej Twardowski, Mikel Menta, Andrew D Bagdanov, and Joost Van De Weijer. 2022. Class-incremental learning: survey and performance evaluation on image classification. TPAMI 45, 5 (2022), 5513-5533. [121] Sahisnu Mazumder and Bing Liu. 2024. Lifelong and Continual Learning Dialogue Systems . Springer Nature. [122] Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. [n. d.]. The natural language decathlon: Multitask learning as question answering. arXiv 2018. arXiv preprint arXiv:1806.08730 ([n. d.]). [123] Sanket Vaibhav Mehta, Darshan Patil, Sarath Chandar, and Emma Strubell. 2021. An Empirical Investigation of the Role of Pre-training in Lifelong Learning. (2021). [124] Martin Menabue, Emanuele Frascaroli, Matteo Boschini, Enver Sangineto, Lorenzo Bonicelli, Angelo Porrello, and Simone Calderara. 2024. Semantic Residual Prompts for Continual Learning. arXiv preprint arXiv:2403.06870 (2024). Manuscript submitted to ACM 32 Yutao Yang et al. Manuscript submitted to ACM Recent Advances of Foundation Language Models-based Continual Learning: A Survey 33 Manuscript submitted to ACM 34 Yutao Yang et al. Manuscript submitted to ACM Recent Advances of Foundation Language Models-based Continual Learning: A Survey 35 [212] Chen Zhang, Yu Xie, Hang Bai, Bin Yu, Weihong Li, and Yuan Gao. 2021. A survey on federated learning. KBS 216 (2021), 106775. [213] Han Zhang, Lin Gui, Yu Lei, Yuanzhao Zhai, Yehong Zhang, Yulan He, Hui Wang, Yue Yu, Kam-Fai Wong, Bin Liang, et al. 2024. COPR: Continual Human Preference Learning via Optimal Policy Regularization. arXiv preprint arXiv:2402.14228 (2024). [214] Han Zhang, Lin Gui, Yuanzhao Zhai, Hui Wang, Yu Lei, and Ruifeng Xu. 2023. Copf: Continual learning human preference through optimal policy fitting. arXiv preprint arXiv:2310.15694 (2023). [215] Han Zhang, Yu Lei, Lin Gui, Min Yang, Yulan He, Hui Wang, and Ruifeng Xu. 2024. CPPO: Continual Learning for Reinforcement Learning with Human Feedback. In ICLR . [216] Peiyan Zhang and Sunghun Kim. 2023. A Survey on Incremental Update for Neural Recommender Systems. arXiv preprint arXiv:2303.02851 (2023). [217] Xi Zhang, Feifei Zhang, and Changsheng Xu. 2023. Vqacl: A novel visual question answering continual learning setting. In CVPR . 19102-19112. [218] Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. NeurIPS 28 (2015). [219] Yanzhe Zhang, Xuezhi Wang, and Diyi Yang. 2022. Continual Sequence Generation with Adaptive Compositional Modules. In ACL . 3653-3667. [220] Yating Zhang, Yexiang Wang, Fei Cheng, Sadao Kurohashi, et al. 2023. Reformulating Domain Adaptation of Large Language Models as AdaptRetrieve-Revise. arXiv preprint arXiv:2310.03328 (2023). [221] Zihan Zhang, Meng Fang, Ling Chen, and Mohammad-Reza Namazi-Rad. 2023. Citb: A benchmark for continual instruction tuning. arXiv preprint arXiv:2310.14510 (2023). [222] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223 (2023). [223] Yingxiu Zhao, Yinhe Zheng, Zhiliang Tian, Chang Gao, Jian Sun, and Nevin L. Zhang. 2022. Prompt Conditioned VAE: Enhancing Generative Replay for Lifelong Learning in Task-Oriented Dialogue. In EMNLP . 11153-11169. [224] Junhao Zheng, Shengjie Qiu, Chengming Shi, and Qianli Ma. 2024. Towards Lifelong Learning of Large Language Models: A Survey. arXiv preprint arXiv:2406.06391 (2024). [225] Mengyu Zheng, Yehui Tang, Zhiwei Hao, Kai Han, Yunhe Wang, and Chang Xu. [n. d.]. Adapt without Forgetting: Distill Proximity from Dual Teachers in Vision-Language Models. ([n. d.]). [226] Zangwei Zheng, Mingyuan Ma, Kai Wang, Ziheng Qin, Xiangyu Yue, and Yang You. 2023. Preventing zero-shot transfer degradation in continual learning of vision-language models. In ICCV . 19125-19136. [227] Victor Zhong, Caiming Xiong, and Richard Socher. 2017. Seq2sql: Generating structured queries from natural language using reinforcement learning. arXiv preprint arXiv:1709.00103 (2017). [228] Da-Wei Zhou, Qi-Wei Wang, Zhi-Hong Qi, Han-Jia Ye, De-Chuan Zhan, and Ziwei Liu. 2023. Deep class-incremental learning: A survey. arXiv preprint arXiv:2302.03648 (2023). [229] Da-Wei Zhou, Yuanhan Zhang, Jingyi Ning, Han-Jia Ye, De-Chuan Zhan, and Ziwei Liu. 2023. Learning without forgetting for vision-language models. arXiv preprint arXiv:2305.19270 (2023). [230] Jie Zhou, Pei Ke, Xipeng Qiu, Minlie Huang, and Junping Zhang. 2023. ChatGPT: potential, prospects, and limitations. FITEE (2023), 1-6. [231] Hongguang Zhu, Yunchao Wei, Xiaodan Liang, Chunjie Zhang, and Yao Zhao. 2023. Ctp: Towards vision-language continual pretraining via compatible momentum contrast and topology preservation. In ICCV . 22257-22267. [232] Qi Zhu, Bing Li, Fei Mi, Xiaoyan Zhu, and Minlie Huang. 2022. Continual Prompt Tuning for Dialog State Tracking. In ACL . 1124-1137. [233] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In ICCV . 19-27. Manuscript submitted to ACM 36 Yutao Yang et al.",
  "A DETAILS OF METRICS": "",
  "A.1 Overall Performance.": "Moreover, Chaudhry et al. [22] devise a metric known as Learning Curve Area (LCA), which quantifies the speed of learning in a model. It first defines an average 𝑏 -shot performance, where 𝑏 represents the number of mini-batches, subsequent to the completion of training across all 𝑇 tasks as follows:  𝐿𝐶𝐴 at 𝛽 is the area of the convergence curve 𝑍 𝑏 as a function of 𝑏 ∈ [ 0 , 𝛽 ] :  The Learning Curve Area (LCA) provides insights into model learning dynamics. 𝐿𝐶𝐴 0 measures the average 0-shot performance, similar to forward transfer ([110]). 𝐿𝐶𝐴 𝛽 , quantifying the area under the 𝑍 𝑏 curve, evaluates both average 0-shot performance and learning speed. Although two models may achieve similar 𝑍 𝑏 or 𝐴 𝑇 values, they can differ significantly in 𝐿𝐶𝐴 𝛽 due to variations in learning rates. This metric is crucial for identifying models that quickly learn from few examples, particularly when 𝛽 is small. Qin et al. [142] propose two metrics designed to evaluate pre-trained language models (PLMs) based on their performance within learned domains: Average Perplexity ( 𝐴𝑃 ) and Average Increased Perplexity ( 𝐴𝑃 + ). The aforementioned metrics are utilized to assess key capabilities of PLMs, such as instruction following and safety, as discussed in Wang et al. [185].",
  "A.2 Memory Stability.": "Chaudhry et al. [21] introduce the Forgetting Measure (FM), a metric designed to quantify the extent of forgetting a model experiences for a specific task. The forgetting for a given task 𝑇 𝑗 after sequential training on tasks up to 𝑇 𝑁 is quantified as the difference between the highest proficiency ( 𝑚𝑎𝑥 ( 𝑅 𝑙,𝑗 ) ) achieved on task 𝑇 𝑗 during initial training and its proficiency ( 𝑅 𝑁,𝑗 ) after subsequent learning phases:  For the purpose of quantifying forgetting in previous tasks, the function 𝑓 𝑗 is defined within the interval [-1 , 1 ] for 𝑗 < 𝑁 . Furthermore, to account for the number of tasks previously encountered, the Forgetting Measure (FM) at the 𝑁 -th task represents the mean level of forgetting across all preceding tasks:  A lower FM indicates better retention of previous tasks. Here, the expansion or 𝑅 𝑗,𝑗 serves as a more effective quantifier of retained knowledge concerning past tasks, as opposed to using max . Nonetheless, max remains a valuable estimator for assessing the extent of forgetting that occurs throughout the learning process. Manuscript submitted to ACM Recent Advances of Foundation Language Models-based Continual Learning: A Survey 37 Davari et al. [31] propose a method named linear probes (LP) to assess representation forgetting. This approach measures the effectiveness of learned representations via an optimal linear classifier trained on the frozen activations of a base network. Representation forgetting is quantified by evaluating the change in Language Processing (LP) performance before and after the introduction of a new task. Formally, for each model ( 𝑓 𝜃 𝑖 ) at time step 𝑖 of a task sequence, the classifier ( 𝑊 ∗ 𝑖 ) is optimized as: 𝑊 ∗ 𝑖 = arg min 𝑊 𝑖 L( 𝑊 𝑖 ; 𝑓 𝜃 𝑖 ( 𝑋 𝑖 ) , 𝑌 𝑖 ) , where L , 𝑋 𝑖 , and 𝑌 𝑖 represent the objective function, input data, and labels for task 𝑖 , respectively. The degree of representational forgetting between two model states, 𝜃 𝑎 and 𝜃 𝑏 , where 𝜃 𝑏 is derived later in the sequence, is evaluated by calculating the difference in scores: 𝑆𝑐𝑜𝑟𝑒 ( 𝑊 𝑎 𝑓 𝜃 𝑎 ( 𝑋 𝑎 ) , 𝑌 𝑎 ) -𝑆𝑐𝑜𝑟𝑒 ( 𝑊 𝑏 𝑓 𝜃 𝑏 ( 𝑋 𝑎 ) , 𝑌 𝑎 ) , where 𝑆𝑐𝑜𝑟𝑒 represents the performance metric, such as accuracy, on the task. Kemker et al. [80] introduce three metrics designed to CF: Ω base , Ω new, and Ω all . Ω base assesses retention of initial learning, Ω new measures recall of new tasks, and Ω all evaluates overall proficiency in maintaining old knowledge and acquiring new information.    where 𝑁 represents the total number of sessions, 𝛼 new ,𝑖 is the test accuracy after learning session 𝑖 , 𝛼 base ,𝑖 denotes the accuracy on the initial session after 𝑖 sessions, and 𝛼 all ,𝑖 refers to the test accuracy across all test data for classes encountered up to point 𝑖 . The ideal performance ( 𝛼 ideal ) is defined as the offline MLP accuracy on the base set. To facilitate comparative analysis across different datasets, Ω base and Ω all are normalized by 𝛼 ideal . Consequently, unless a model surpasses 𝛼 ideal , normalized results will range from 0 to 1, enabling consistent cross-dataset comparisons. Additionally, researchers [85] devise a novel metric, termed the Knowledge Loss Ratio (KLR), quantifies knowledge degradation using principles from information theory.",
  "A.3 Learning Plasticity.": "Intransigence measure (IM), as defined by Chaudhry et al. [21], quantifies a model's inability to learn new tasks. This measure is calculated by comparing the performance difference of a task when trained jointly with other tasks versus when trained in a continual learning setting. Then the intransigence for the 𝑁 -th task can be defined as:  where 𝑅 ∗ 𝑁 represents the accuracy achieved on the held-out dataset of the 𝑁 -th task, 𝑅 𝑁,𝑁 indicates the accuracy on the 𝑁 -th task upon completion of training in an incremental sequence up to and including task 𝑁 . Note, 𝐼 𝑀 𝑁 ∈ [-1 , 1 ] , and lower values indicate superior performance.",
  "A.4 Metrics for Continual Pre-training.": "CKL [66] introduces a novel metric, named FUAR (FORGOTTEN / (UPDATED + ACQUIRED) RATIO), which quantitatively measures the efficiency of each CKL method. It calculates the number of instances of time-invariant knowledge Manuscript submitted to ACM 38 Yutao Yang et al. that a model forgets in order to learn or update one instance of new knowledge. When FUAR is equal to 1.0, it signifies an equilibrium where one time-invariant knowledge instance is forgotten on average to obtain a new or updated knowledge instance. Formally, FUAR is defined as:   where 𝑇 represents an arbitrary task, and ( 𝐷 𝑖 ) 𝑁 𝑖 = 0 is a sequence of corpora for LM pretraining. Gap( 𝑇, 𝐷 𝑎 , 𝐷 𝑏 ) is Score ( 𝑇 ) of 𝐿𝑀 𝑎 - Score ( 𝑇 ) of 𝐿𝑀 𝑏 , where 𝐿𝑀 𝑎 is pretrained on 𝐷 𝑎 . ❚ 𝐹 = ( 𝑇 𝐹 𝑖 ) 𝑁 -1 𝑖 = 0 measures forgetting of invariant-knowledge from ( 𝐷 𝑖 ) 𝑁 -1 𝑖 = 0 . If no task is from 𝐷 𝑖 , 𝑇 𝐹 𝑖 is \"n.d. \" (not defined). 𝑇 𝑈 𝑁 and 𝑇 𝐴 𝑁 from 𝐷 𝑁 measure update and acquisition of new knowledge, respectively.",
  "A.5 Online CL-Specific Metrics.": "Near-future accuracy (NFA) [2] is introduced as a novel evaluation metric for OCL problem. Unlike traditional evaluation methods that assess models on immediately subsequent samples, NFA evaluates models on samples slightly further into the future, using a minimal shift 𝑆 . Such operation can mitigate label correlation effects, which can adversely impact the accuracy of model adaptability assessments. The smallest shift 𝑆 is selected to ensure that the test sample aligns closely with the distribution of recently observed training data. The calculation of NFA involves first checking if the model correctly predicts the label of a future sample, which can be expressed as 𝑎 𝑡 = ✶ { 𝑓 𝜃 𝑡 ( 𝑥 𝑡 + 1 + 𝑆 ) = 𝑦 𝑡 + 1 + 𝑆 } . Subsequently, the running average is updated using the formula 𝐴 𝑅𝐴 𝑡 = 1 𝑡 ( 𝐴 𝑅𝐴 𝑡 -1 · ( 𝑡 -1 ) + 𝑎 𝑡 ) . Yogatama et al. [205] proposed a novel online codelength ( ℓ ( 𝐷 ) ), inspired by prequential encoding [9], to quantify how quickly an existing model can adapt to a new task.  where | 𝑌 | is the number of possible labels (classes), and 𝜃 𝐷 𝑖 represents a particular subset of the dataset 𝐷 . Similar to the approach in Latent Contextual Allocation (LCA) [22], the concept of online codelength is associated with the area under the learning curve. Manuscript submitted to ACM",
  "keywords_parsed": [
    "None"
  ]
}