{"FLIP: Fine-grained Alignment between ID-based Models and Pretrained Language Models for CTR Prediction": "Hangyu Wang \u2217 hangyuwang@sjtu.edu.cn Shanghai Jiao Tong University Shanghai, China Jianghao Lin \u2217 chiangel@sjtu.edu.cn Shanghai Jiao Tong University Shanghai, China Xiangyang Li lixiangyang34@huawei.com Huawei Noah's Ark Lab Shenzhen, China", "Bo Chen": "", "Chenxu Zhu": "chenbo116@huawei.com Huawei Noah's Ark Lab Shanghai, China zhuchenxu1@huawei.com Huawei Noah's Ark Lab Shanghai, China Weinan Zhang \u2020 wnzhang@sjtu.edu.cn Shanghai Jiao Tong University Shanghai, China", "ABSTRACT": "Click-through rate (CTR) prediction plays as a core function module in various personalized online services. The traditional ID-based models for CTR prediction take as inputs the one-hot encoded ID features of tabular modality , which capture the collaborative signals via feature interaction modeling. But the one-hot encoding discards the semantic information included in the textual features. Recently, the emergence of Pretrained Language Models (PLMs) has given rise to another paradigm, which takes as inputs the sentences of textual modality obtained by hard prompt templates and adopts PLMs to extract the semantic knowledge. However, PLMs often face challenges in capturing field-wise collaborative signals and distinguishing features with subtle textual differences. In this paper, to leverage the benefits of both paradigms and meanwhile overcome their limitations, we propose to conduct Fine-grained feature-level ALignment between ID-based Models and Pretrained Language Models (FLIP) for CTR prediction. Unlike most methods that solely rely on global views through instance-level contrastive learning, we design a novel jointly masked tabular/language modeling task to learn fine-grained alignment between tabular IDs and word tokens. Specifically, the masked data of one modality ( i.e. , IDs and tokens) has to be recovered with the help of the other modality, which establishes the feature-level interaction and alignment via sufficient mutual information extraction between dual modalities. Moreover, we propose to jointly finetune the ID-based model and PLM by \u2217 Both authors contributed equally to this research. \u2020 Corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. RecSys '24, October 14-18, 2024, Bari, Italy \u00a9 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0505-2/24/10...$15.00 https://doi.org/10.1145/3640457.3688106 Ruiming Tang tangruiming@huawei.com Huawei Noah's Ark Lab Shenzhen, China", "Yong Yu": "yyu@sjtu.edu.cn Shanghai Jiao Tong University Shanghai, China adaptively combining the output of both models, thus achieving superior performance in downstream CTR prediction tasks. Extensive experiments on three real-world datasets demonstrate that FLIP outperforms SOTA baselines, and is highly compatible with various ID-based models and PLMs. The code is available 12 .", "CCS CONCEPTS": "\u00b7 Information systems \u2192 Recommender systems .", "KEYWORDS": "Fine-grained Alignment, Pretrained Language Model, CTR prediction, Recommender Systems", "ACMReference Format:": "Hangyu Wang, Jianghao Lin, Xiangyang Li, Bo Chen, Chenxu Zhu, Ruiming Tang, Weinan Zhang, and Yong Yu. 2024. FLIP: Fine-grained Alignment between ID-based Models and Pretrained Language Models for CTR Prediction. In 18th ACM Conference on Recommender Systems (RecSys '24), October 14-18, 2024, Bari, Italy. ACM, New York, NY, USA, 11 pages. https: //doi.org/10.1145/3640457.3688106", "1 INTRODUCTION": "Click-through rate (CTR) prediction is the core component of various personalized online services ( e.g. , web search [12, 16, 39], recommender systems [20, 77]). It aims to estimate a user's click probability towards each target item, given a particular context [40, 78, 84]. Recently, the emergence of Pretrained Language Models (PLMs) [54] has facilitated the acquisition of extensive knowledge and enhanced reasoning abilities, hence introducing a novel paradigm for predicting CTR directly through natural language [38]. On the one hand, the traditional ID-based models for CTR prediction adopt the one-hot encoding to convert the input data into ID features, which we refer to as tabular data modality . An 1 PyTorch version: https://github.com/justarter/FLIP. 2 MindSpore version: https://github.com/mindspore-lab/models/tree/master/research/ huawei-noah/FLIP. RecSys '24, October 14-18, 2024, Bari, Italy Hangyu Wang et al. I nformation Extraction Gender Age Title Genre female 18-25 The Flash action Gender is [MASK]. Age is 18-25. Title is The Flash. Genre is action. PLM I D-based Model Gender is female. Age is 18-25. Title is The Flash. Genre is action. PLM I D-based Model I nformation Extraction Gender is female. Age is 18-25. Title is The Flash. Genre is action. (b) Feature-level Masked Language Modeling (MLM) Gender Age Title Genre female [MASK] The Flash action Gender Age Title Genre female 18-25 The Flash action Gender Age Title Genre female 18-25 The Flash action Gender is female. Age is 18-25. Title is The Flash. Genre is action. PLM I D-based Model I nfoNCE Loss (a) Instance-level Contrastive Learning (ICL) [CLS] Token Representation Final Representation (c) Feature-level Masked Tabular Modeling (MTM) Figure 1: Three cross-modal pretraining tasks. Task (a) provides coarse-grained instance-level alignment via contrastive learning, while tasks (b) and (c) achieve fine-grained featurelevel alignment through jointly masked modality modeling. example is shown as follows:  Deriving from POLY2 [5] and FM [61], the key idea of these models is to capture the complex high-order feature interaction patterns across multiple fields by different operators ( e.g. , product [55, 74], convolution [42, 80], and attention [65, 79]). On the other hand, the development of PLMs has ushered in another modeling paradigm that utilizes the PLM as a text encoder or recommender directly. The input data is first transformed into textual sentences via hard prompt templates, which could be referred to as textual data modality . In this way, the semantic information among original data is well preserved as natural languages ( e.g. , gender feature as text 'male' instead of ID code '[0,1]'). Then, PLMs [13, 46] encode and understand the textual sentences, and thus turn CTR prediction into either a binary text classification problem [43, 50] or a sequence-to-sequence task [18, 19]. However, both of the above models ( i.e. , ID-based and PLMs) possess inherent limitations. ID-based models utilize the one-hot encoding that discards the semantic information included in the textual features and fails to capture the semantic correlation between feature descriptions [35, 37]. In addition, ID-based models rely heavily on the user interactions and may struggle in scenarios with sparse interactions [63]. The aforementioned issues can be effectively alleviated by PLMs. PLMs excel in understanding the context and meaning behind textual features, and use their knowledge and reasoning capabilities to achieve robust performance in sparse-interaction situations [75, 86]. Furthermore, PLMs also have certain limitations. PLMs struggle to understand field-wise collaborative signals because their input data is formulated as textual sentences, which are broken down into subword tokens, thus splitting the field-wise features [2, 60]. Additionally, PLMs may fail to recognize subtle differences between different feature descriptions [56, 64] because it is hard to distinguish features that exhibit little textual variations in natural language [23, 83] ( e.g. , in terms of the movie name, 'The Room' and 'Room' are two very similar movies literally). Fortunately, the ID-based model can perceive field-wise collaborative signals with various model structures and distinguish each different feature with the distinctive ID encodings. To this end, it is natural to bridge two paradigms to leverage the benefits of both modalities while overcoming their limitations. Moreover, the fine-grained feature-level alignment between tabular IDs and word tokens is critical, enabling ID-based models to perceive semantic information corresponding to each feature ID and allowing PLMs to clearly distinguish features with similar text but different IDs. However, most existing methods [35, 60] align both modalities by instance-level contrastive learning, as shown in Figure 1(a). They only rely on the global view and lack supervision to encourage fine-grained alignment between IDs and tokens, potentially causing representation degeneration problems [17, 29, 53]. In this paper, we propose to conduct Fine-grained Feature-level ALignment between ID-based Models and Pretrained Language Models ( FLIP ) for CTR prediction. FLIP is a model-agnostic framework that adopts the common pretrain-finetune scheme [13, 40]. For pretraining objectives, as illustrated in Figure 1(b) and 1(c), we propose to build jointly masked tabular and language modeling, where the masked features ( i.e. , IDs or tokens of specific features) of one modality are recovered with the help of another modality. This is motivated by the fact that both tabular and textual data convey almost the same information of the original raw data, but only in different formats. In order to accomplish the masked feature reconstruction, each single model (ID-based model or PLM) is required to seek and exploit the valuable knowledge embedded in the other model that corresponds to the masked features, thereby achieving fine-grained feature-level cross-modal interactions. Then, as for finetuning , we propose a simple yet effective adaptive finetuning approach to combine the predictions from both models for downstream CTR estimation. The main contributions of this paper are as follows: \u00b7 We highlight the importance of utilizing fine-grained featurelevel alignment between tabular IDs and word tokens, in order to explore the potential of enhancing the performance of existing recommender systems. \u00b7 We propose a model-agnostic framework FLIP, where the jointly masked tabular and language modeling tasks are involved. In these tasks, the masked features of one modality have to be recovered with the help of another modality, thus learning finegrained feature-level cross-modal interactions. FLIP also employs an adaptive finetuning approach to combine the predictions from the ID-based model and PLM for improving performance. \u00b7 Extensive experiments on three real-world public datasets demonstrate the superiority of FLIP, compared with existing baseline models. Moreover, we validate the model compatibility of FLIP in terms of both ID-based models and PLMs.", "2 PRELIMINARIES": "", "2.1 ID-based Models for CTR Prediction": "The traditional CTR prediction is modeled as a binary classification task [20, 55], whose dataset is presented as {( x \ud835\udc56 , \ud835\udc66 \ud835\udc56 )} \ud835\udc41 \ud835\udc56 = 1 , where FLIP: Fine-grained Alignment between ID-based Models and Pretrained Language Models for CTR Prediction RecSys '24, October 14-18, 2024, Bari, Italy \ud835\udc66 \ud835\udc56 \u2208 { 1 , 0 } is the label indicating user's actual click behavior, and x \ud835\udc56 = [ \ud835\udc65 \ud835\udc56, 1 , \ud835\udc65 \ud835\udc56, 2 , . . . , \ud835\udc65 \ud835\udc56,\ud835\udc39 ] is categorical tabular data with \ud835\udc39 different fields. The goal of CTR prediction is to estimate the click probability \ud835\udc43 ( \ud835\udc66 \ud835\udc56 = 1 | x \ud835\udc56 ) based on input x \ud835\udc56 . Generally, ID-based models adopt the 'Embedding & Feature Interaction' paradigm [36, 65]: (1) the input x \ud835\udc56 is first transformed into one-hot vectors, which are then mapped to low-dimensional embeddings via an embedding layer. (2) Next, Feature Interaction (FI) Layer is used to process the embeddings, compute complex feature interactions and generate a dense representation v \ud835\udc56 . (3) Finally, the prediction layer (usually a MLP module) estimates the click probability \u02c6 \ud835\udc66 \ud835\udc56 \u2208 [ 0 , 1 ] based on the dense representation v \ud835\udc56 . The ID-based model is trained with the binary cross-entropy (BCE) loss in an end-to-end manner:", "2.2 PLMs for CTR Prediction": "As PLMs have shown remarkable success in a wide range of tasks due to their extensive knowledge and reasoning capabilities [3, 46], researchers now tend to leverage their proficiency in natural language understanding and world knowledge modeling to solve the CTR prediction task [38, 44]. Different from ID-based models, (1) PLMs first transform the input data x \ud835\udc56 into the textual sentence x \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 \ud835\udc56 via hard prompt templates. (2) Then the textual sentence x \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 \ud835\udc56 is converted into meaningful lexical tokens through tokenization, which are embedded into a vector space. (3) Next, PLMs utilize their transformer-based neural networks to process the token embeddings obtained from the previous step, generating the contextually coherent representation w \ud835\udc56 . (4) Finally, we can either add a randomly initialized classification head (usually MLP) on the representation w \ud835\udc56 to perform binary classification and predict the click label \ud835\udc66 \ud835\udc56 \u2208 { 0 , 1 } [35, 43], or add a language modeling head to do causal language modeling tasks and predict the likelihood of generating the next keyword ( e.g. , 'yes' or 'no') through a verbalizer [2, 18].", "3 METHODOLOGY": "", "3.1 Overview of FLIP": "Figure 2 depicts the architecture of FLIP, which consists of three stages: modality transformation , modality alignment pretraining and adaptive finetuning . Firstly, FLIP transforms the raw data from tabular modality into textual modality. Then, in the modality alignment pretraining, we employ the jointly masked language/tabular modeling task to learn fine-grained modality alignments. Lastly, we propose a simple yet effective adaptive finetuning strategy to further enhance the performance on CTR prediction. Hereinafter, we omit the detailed structure of PLMs and ID-based models, since FLIP serves as a model-agnostic framework and is compatible with various backbone models.", "3.2 Modality Transformation": "Standard PLMs take sequences of words as inputs [13, 57]. Consequently, the modality transformation aims to convert the tabular data x \ud835\udc61\ud835\udc4e\ud835\udc4f \ud835\udc56 into textual data x \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 \ud835\udc56 via hard prompt templates. Previous works [24, 35, 43] have suggested that sophisticated templates ( e.g. , with more vivid descriptions) for textual data construction might mislead the PLM and make it fail to grasp the key information in the texts. Hence, we adopt the following simple yet effective transformation template:  where \ud835\udc5a \ud835\udc53 is the name of \ud835\udc53 -th field ( e.g. , gender ), \ud835\udc63 \ud835\udc56,\ud835\udc53 denotes the feature value of \ud835\udc53 -th field for input \ud835\udc65 \ud835\udc61\ud835\udc4e\ud835\udc4f \ud835\udc56 ( e.g. , female ), and \u2295 indicates the concatenation operator. An illustrative example is given in Figure 2 (Stage 1), where we first construct a descriptive sentence \ud835\udc61 \ud835\udc56,\ud835\udc53 for each feature field, and then concatenate them to obtain the final textual data. In this way, wepreserve the semantic knowledge of both field names and feature values with minimal preprocessing and no information loss. Both tabular data and textual data can be considered to contain almost the same information of raw data, albeit in different modalities.", "3.3 Modality Alignment Pretraining": "As shown in Figure 2 (Stage 2), after obtaining the paired texttabular data ( x \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 \ud835\udc56 , x \ud835\udc61\ud835\udc4e\ud835\udc4f \ud835\udc56 ) from the same raw input, we first perform field-level data masking to obtain the corrupted version of input pair, ( \u02c6 x \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 \ud835\udc56 , \u02c6 x \ud835\udc61\ud835\udc4e\ud835\udc4f \ud835\udc56 ) . Then, the PLM \u210e PLM and ID-based model \u210e ID encode the input pair to obtain the dense representations ( w \ud835\udc56 , \u02c6 w \ud835\udc56 ) and ( v \ud835\udc56 , \u02c6 v \ud835\udc56 ) for textual and tabular modalities, respectively. Next, we apply three different pretraining objectives to achieve both featurelevel and instance-level alignments between PLMs and ID-based models: \u00b7 Feature-level Masked Language Modeling (MLM) requires the model to recover the original tokens from the corrupted textual context with the help of complete tabular data. \u00b7 Feature-level Masked Tabular Modeling (MTM)requires the model to recover the original feature IDs from the corrupted tabular context with the help of complete textual data. \u00b7 Instance-level Contrastive Learning (ICL) draws positive samples together ( i.e. , different modalities of the same input) and pushes apart negative sample pairs. The jointly masked modality modeling task learns fine-grained modality interactions by the way of mask-and-predict, while ICL aligns the modalities from the perspective of the global consistency. 3.3.1 Field-level Data Masking . We first perform the field-level data masking strategy for textual and tabular data, respectively. For textual data , we propose to mask tokens at the field level. We first randomly select a certain ratio \ud835\udc5f \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 of the feature fields to be corrupted, and then mask all the consecutive tokens that constitute the entire text of the corresponding feature value. We denote the index set of masked tokens as I \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 . Note that this is quite different from the random token-level masking strategy for common language models ( e.g. , BERT [13]). For example, suppose the sentence tokenization from occupation field is ['occupation', 'is', 'college', 'student']. The outcome of field-level masking should be ['occupation', 'is', [MASK] , [MASK] ]. But the outcome of token-level masking might be [ [MASK] , 'is', 'college', 'student'] or ['occupation', 'is', 'college', [MASK] ]. Obviously, token-level masked tokens can be easily inferred solely based RecSys '24, October 14-18, 2024, Bari, Italy Hangyu Wang et al. User ID Gender Occupation Movie ID Title Genre 217 female college student 12 The Flash action Textual Data Tabular Data User ID is 217. Gender is female. Occupation is college student. Movie ID is 12. Title is The Flash. Genre is action. User ID is 217. Gender is female. Occupation is college student. Movie ID is 12. Title is The Flash. Genre is action. Stage 1:  Modality Transformation I D-based Model Prediction Head Tabular Data PLM Prediction Head Textual Data I D-based Model Prediction Head Tabular Data PLM Prediction Head Textual Data FLIP Stage 3:  Adaptive Finetuning FLIP FLIP ID PLM Figure 2: The overall framework of our proposed FLIP. User ID Gender Occupation Movie ID Title Genre 217 female college student 12 The Flash action Masked Textual Data Tabular Data User ID is 217. Gender is [MASK]. Occupation is college student. Movie ID is 12. Title is [MASK] [MASK]. Genre is action. User ID Gender Occupation Movie ID Title Genre 217 [MASK] college student 12 [MASK] action Masked Tabular Data Textual Data User ID is 217. Gender is female. Occupation is college student. Movie ID is 12. Title is The Flash. Genre is action. PLM I D-based Model I nstance-level Contrastive Learning Module Stage 2:  Modality Alignment Pretraining Masked Language Modeling (MLM) MTM Module MLM Module Prediction Layer Original Token Original Token Prediction Layer Original Feature Original Feature Cross-Attention Unit Key & Value Query Masked Tabular Modeling (MTM) Field-level Data Masking on the textual context, thus limiting the cross-modal interactions and losing the generalization ability [45]. For tabular data , following previous works [40, 72], we uniformly sample a certain ratio \ud835\udc5f \ud835\udc61\ud835\udc4e\ud835\udc4f of the fields, and then replace the corresponding feature with an additional <MASK> feature, which is set as a special feature in the embedding table of ID-based model. The <MASK> feature is not field-specific, but shared by all feature fields to prevent introducing prior knowledge about the masked field [40]. Similarly, we denote the index set of masked fields as I \ud835\udc61\ud835\udc4e\ud835\udc4f . After the field-level data masking, we obtain the corrupted version for both modalities, i.e. , ( \u02c6 x \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 \ud835\udc56 , \u02c6 x \ud835\udc61\ud835\udc4e\ud835\udc4f \ud835\udc56 ) . It is worth noting that the selected fields to be masked do not necessarily have to be the same for textual and tabular modalities, since they serve as two independent parallel workflows. 3.3.2 Data Encoding . We employ the PLM \u210e PLM and ID-based model \u210e ID to encode the input pairs from textual and tabular modalities, respectively:  Here, w \ud835\udc56 = [ \ud835\udc64 \ud835\udc56,\ud835\udc59 ] \ud835\udc3f \ud835\udc59 = 1 \u2208 R \ud835\udc3f \u00d7 \ud835\udc37 \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 is the set of hidden states from the last layer of the PLM, where \ud835\udc3f is the number of tokens for x \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 \ud835\udc56 and \ud835\udc37 \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 is the hidden size of the PLM. Note that \ud835\udc64 \ud835\udc56, 1 is the [CLS] token vector that represents the overall textual input. \u02c6 w \ud835\udc56 = [ \u02c6 \ud835\udc64 \ud835\udc56,\ud835\udc59 ] \ud835\udc3f \ud835\udc59 = 1 \u2208 R \ud835\udc3f \u00d7 \ud835\udc37 \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 also satisfies the notations above. v \ud835\udc56 , \u02c6 v \ud835\udc56 \u2208 R \ud835\udc37 \ud835\udc61\ud835\udc4e\ud835\udc4f are the representations produced by ID-based model. 3.3.3 Masked Language Modeling . As shown in Figure 2 (Stage 2), the MLM module takes the text-tabular pair ( \u02c6 w \ud835\udc56 , v \ud835\udc56 ) as input, and attempts to recover the masked tokens. We denote the index set of masked tokens as I \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 . For each masked token with index \ud835\udc59 \u2208 I \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 , we concatenate the corresponding token vector \u02c6 \ud835\udc64 \ud835\udc56,\ud835\udc59 with the 'reference answer' v \ud835\udc56 , and then feed them through the prediction layer to obtain the estimated distribution:  where \u2295 is the concatenation operation, \ud835\udc49 is the vocabulary size, and \ud835\udc54 PLM is a two-layer MLP. Finally, similar to common masked language modeling [13], we leverage the cross-entropy loss for pretraining optimization:  where \ud835\udc65 \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 \ud835\udc56,\ud835\udc59 is the original \ud835\udc59 -th token. 3.3.4 Masked Tabular Modeling . Likewise, the MTM module takes the text-tabular pair ( w \ud835\udc56 , \u02c6 v \ud835\udc56 ) as input, and aims to recover the masked features. To dynamically capture the essential knowledge from the corresponding tokens of the masked features, we design the cross-attention unit to aggregate the tabular representation \u02c6 v \ud835\udc56 with the 'reference answer' w \ud835\udc56 :  where Q \u2208 R \ud835\udc37 \ud835\udc61\ud835\udc4e\ud835\udc4f \u00d7 \ud835\udc37 \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 is the trainable cross-modal attention matrix, and \u221a \ud835\udc37 \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 is the scaling factor [66]. FLIP: Fine-grained Alignment between ID-based Models and Pretrained Language Models for CTR Prediction RecSys '24, October 14-18, 2024, Bari, Italy For each masked feature with index \ud835\udc53 \u2208 I \ud835\udc61\ud835\udc4e\ud835\udc4f , we maintain an independent MLP network \ud835\udc54 ( \ud835\udc53 ) ID followed by a softmax function to compute the distribution \ud835\udc5d \ud835\udc56,\ud835\udc53 \u2208 R \ud835\udc40 over the candidate features:  where \ud835\udc40 is the size of the entire feature space. Finally, we adopt the cross-entropy loss on all the masked features:  where \ud835\udc65 \ud835\udc61\ud835\udc4e\ud835\udc4f \ud835\udc56,\ud835\udc53 is the original feature of \ud835\udc53 -th field. However, the loss above is actually impractical and inefficient since it has to calculate the softmax function over the entire feature space in Eq. 8, where \ud835\udc40 is usually at million level for real-world applications. To this end, we adopt noise contrastive estimation (NCE) [21, 40, 49]. NCE transforms a multi-class classification task into a binary classification task, where the model is required to distinguish positive features ( i.e. , masked features) from noise features. Specifically, for each masked feature \ud835\udc65 \ud835\udc61\ud835\udc4e\ud835\udc4f \ud835\udc56,\ud835\udc53 of \ud835\udc53 -th field, we sample \ud835\udc3e noise features from the entire feature space according to their frequency distribution in the training set. Then, we utilize the binary cross-entropy (BCE) loss for MTM optimization:  where \ud835\udf0e is the sigmoid function, and \ud835\udc61 , \ud835\udc58 are the indices of the positive and noise features respectively. 3.3.5 Instance-level Contrastive Learning (ICL) . In addition to the feature-level alignment through the masked modality modeling, we also introduce contrastive learning to explicitly learn instance-level consistency between two modalities. The contrastive objective draws the representations of matched text-tabular pairs together and pushes apart those non-matched pairs [30, 56]. Here, we utilize the [CLS] token vector \ud835\udc64 \ud835\udc56, 1 to represent the textual input x \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 \ud835\udc56 . For dimensional consistency, we employ two separate linear layers to project the [CLS] token vector \ud835\udc64 \ud835\udc56, 1 and tabular representation v \ud835\udc56 into \ud835\udc51 -dimensional vectors, \ud835\udc67 \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 \ud835\udc56 and \ud835\udc67 \ud835\udc61\ud835\udc4e\ud835\udc4f \ud835\udc56 , respectively. Next, we adopt InfoNCE [51] to compute the ICL loss:  where \ud835\udc35 is the batch size, \ud835\udf0f is the temperature hyperparameter, and the similarity function sim (\u00b7) is measured by dot product. Finally, by putting the three objectives together, the overall loss for the modality alignment pretraining stage is:", "3.4 Adaptive Finetuning": "After the pretraining stage, the PLM and ID-based model have learned fine-grained multimodal representations. As depicted in Figure 2 (Stage 3), in this stage, we adaptively finetune the two models jointly on the downstream CTR prediction task with supervised click signal to achieve superior performance. FLIP places a randomly initialized linear layer on the ID-based model, and another layer upon the PLM, so that these two models can output the estimated probability \u02c6 \ud835\udc66 ID \ud835\udc56 and \u02c6 \ud835\udc66 PLM \ud835\udc56 respectively.  And the final click probability is estimated by a weighted sum of outputs from both models:  where \ud835\udefc \u2208 is a learnable parameter to adaptively balance the outcomes from two models. To avoid performance collapse mentioned in previous works [4, 62], we apply BCE objectives over the jointly estimated click probability \u02c6 \ud835\udc66 , as well as the solely estimated click probability \u02c6 \ud835\udc66 ID \ud835\udc56 and \u02c6 \ud835\udc66 PLM \ud835\udc56 for model optimization:  To delve deeper into the influence of fine-grained alignment on the single model, we define two variants: FLIP ID and FLIP PLM. The former solely finetunes the ID-based model with loss L \ud835\udc35\ud835\udc36\ud835\udc38 ( \ud835\udc66, \u02c6 \ud835\udc66 ID ) , while the latter solely finetunes the PLM with loss L \ud835\udc35\ud835\udc36\ud835\udc38 ( \ud835\udc66, \u02c6 \ud835\udc66 PLM ) . It is worth noting that FLIP is expected to achieve the superior performance since it explicitly combines the predictions from two models, while FLIPID and FLIPPLM could more clearly reveal the effect of fine-grained alignment on a single model.", "4 EXPERIMENT": "", "4.1 Experiment Setup": "4.1.1 Datasets. We conduct experiments on three real-world public datasets: MovieLens-1M, BookCrossing, GoodReads. All of the selected datasets contain user and item information. The information is unencrypted original text, thus preserving the real semantic information. \u00b7 MovieLens-1M [22] is a movie recommendation dataset with user-movie ratings ranging from 1 to 5. Following the previous work [35, 65], We consider samples with ratings greater than 3 as positive, samples with ratings less than 3 as negative, and remove samples with ratings equal to 3 ( i.e. , neutral). \u00b7 BookCrossing [88] is a book recommendation dataset and possesses user-book ratings ranging from 0 to 10. We consider samples with scores greater than 5 as positive, and the rest as negative. \u00b7 GoodReads [67, 68] is a book recommendation dataset which contains user-book ratings ranging from 1 to 5. We take samples with ratings greater than 3 as positive, and the rest as negative. Following previous works [33, 35], we sort all samples in chronological order and take the first 90% samples as the training set and the remaining as the testing set. The training set for the pretraining RecSys '24, October 14-18, 2024, Bari, Italy Hangyu Wang et al. Table 1: Statistics of processed datasets. and finetuning stage are the same [40]. All our experimental results are obtained on the testing set. The statistics of the processed datasets are shown in Table 1. 4.1.2 Evaluation Metrics. Weuse commonly adopted metrics, AUC (Area Under the ROC Curve) and Logloss (binary cross-entropy loss) as the evaluation metrics. Notably, a slightly higher AUC or a lower Logloss ( e.g. , 0.001 ) can be considered as a significant improvement in CTR prediction [20, 34, 36]. 4.1.3 Baselines. The baseline methods can be mainly classified into three categories: (1) ID-based models: AFM [79], PNN [55], Wide&Deep [9], DCN [73], DeepFM [20], xDeepFM [36], AFN [10], AutoInt [65] and DCNv2 [74], (2) PLM-based models: CTR-BERT [50], P5 [18] and PTab [43], (3) ID+PLM models that combine ID-based model and PLM: CTRL [35], MoRec [81]. 4.1.4 Implementation Details. All of our experiments are performed on 8 NVIDIA Tesla V100 GPUs with PyTorch [52]. In the modality alignment pretraining stage, we set the text and tabular mask ratio \ud835\udc5f \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 and \ud835\udc5f \ud835\udc61\ud835\udc4e\ud835\udc4f both to 15%. Unless specified otherwise, we adopt TinyBERT [26] as the PLM, and DCNv2 [74] as the ID-based model. During pretraining, the model is trained for 30 epochs with the AdamW[47] optimizer and a batch size of 1024. The learning rate is initialized as 5e-5 followed by a cosine decay strategy. The number of noise features \ud835\udc3e is 25 in Eq. 10. The temperature \ud835\udf0f is 0.7 in Eq. 11. In the adaptive finetuning stage, we adopt the Adam [27] optimizer with learning rate selected from {1e-5,5e-5,1e-4,5e-4,1e-3,5e-3}. The finetuning batch size is 256 for MovieLens-1M and BookCrossing, and is 2048 for GoodReads. For all ID-based models in the baselines or FLIP framework, the embedding size is fixed to 32, and the size of DNN layers is [300,300,128]. The structure parameters and the size of PLMs in baselines are set according to their original papers. We also apply grid search to baseline methods for optimal performance.", "4.2 Performance Comparison": "We compare the recommendation performance of FLIP with three categories of baselines. We also include variants FLIP ID and FLIP PLM to reveal the effect of fine-grained alignment on a single model. The results are shown in Table 2, from which we can observe that: \u00b7 FLIP outperforms all baselines from all three categories ( i.e. , IDbased, PLM-based, and ID+PLM) significantly, which confirms the excellence of our proposed fine-grained feature-level alignment and adaptive finetuning approach for CTR prediction. \u00b7 FLIPID surpasses all ID-based baselines, while FLIP PLM outperforms all PLM-based baselines. These phenomena prove that fine-grained modality alignment can leverage the benefits of both models and boost their own performance. \u00b7 CTRL and MoRec generally outperform other baselines due to their integration of ID-based models and PLMs. However, they either overlook the fine-grained alignment between IDs and tokens or ignore cross-modal interactions, thereby degrading the performance. FLIP addresses these shortcomings by bridging IDbased models and PLMs through jointly masked tabular/language modeling, which enables fine-grained feature-level interactions between dual modalities, thus resulting in superior performance.", "4.3 Compatibility Analysis": "FLIP serves as a model-agnostic framework that is compatible with various backbone models. In this section, we investigate the model compatibility in terms of different ID-based models and PLMs. 4.3.1 Compatibility with ID-based models. We apply FLIP to three different ID-based models, including DeepFM, AutoInt and DCNv2, while keeping TinyBERT as the PLM. The results are listed in Table 3. We can obtain the following observations: \u00b7 First and foremost, FLIP ID consistently surpasses the corresponding vanilla ID-based model by a large margin without altering the model structure or increasing the inference cost. This indicates that ID-based models of various structures can all acquire useful semantic information from PLMs through the fine-grained alignment pretraining, thereby improving performance. \u00b7 By jointly tuning the ID-based model and PLM, FLIP achieves the best performance across various ID-based backbone models significantly, demonstrating the superior compatibility of FLIP in terms of ID-based models. 4.3.2 Compatibility with PLMs. Similarly, we keep DCNv2 as the ID-based model, and select PLMs of different sizes, including TinyBERT (14.5M), RoBERTa-Base (125M) [46] , and RoBERTa-Large (355M) [46]. The results are in Table 4, from which we find that: \u00b7 As the size of PLM grows, the performance of FLIPPLM continuously increases and even achieves a better AUC 0.7972 on BookCrossing with RoBERTa-Large compared with the vanilla DCNv2. A larger model size would lead to larger model capacity and better language understanding ability, thus benefiting the final predictive performance. \u00b7 While increasing the PLM's size is expected to yield more notable performance improvements, the advantages of scaling up gradually taper off. For instance, the improvement from RoBERTa-Base to RoBERTa-large is significantly smaller than the improvement from TinyBERT to RoBERTa-Base. \u00b7 FLIP and FLIPID outperform the DCNv2 model consistently and significantly, highlighting FLIP's ability to adapt seamlessly to different PLM sizes and architectures.", "4.4 Ablation Study": "We conduct ablation experiments for better understanding the contributions of different components in our proposed FLIP. Firstly, we evaluate the impact of pretraining objectives ( i.e. , MLM , MTM , ICL ) by eliminating them from the pretraining stage. Note that removing all three objectives means that the pretraining stage does not exist. The results are reported in Table 5. \u00b7 As we can observe, the optimal performance is achieved when three losses are deployed simultaneously, and removing each loss will degrade performance, while eliminating all losses results FLIP: Fine-grained Alignment between ID-based Models and Pretrained Language Models for CTR Prediction RecSys '24, October 14-18, 2024, Bari, Italy Table 2: The overall performance of different models from three categories ( i.e. , ID-based, PLM-based, and ID+PLM). For each type of models, the best result is given in bold, and the second-best value is underlined. Rel.Impr denotes the relative AUC improvement rate of our method against each baseline within each category. The symbol ' \u2217 ' indicates statistically significant improvement of FLIP over the best baseline with \ud835\udc5d -value < 0.001. Table 3: The compatibility w.r.t. different ID-based models. The PLM is fixed as TinyBERT. N/A means to train the vanilla ID-based model from scratch. For each ID-based model, the best result is in bold, and the second-best is underlined. Table 4: The compatibility w.r.t. different PLMs. The ID-based model is fixed as DCNv2. For each type of PLM, the best result is in bold, and the second-best is underlined. in the lowest performance. These phenomena demonstrate that each component contributes to the final performance. \u00b7 Removing ICL (w/o ICL ) obtains better performance than removing MLM&MTM (w/o MLM&MTM ), indicating that joint modeling for MLM&MTM tasks can learn meaningful cross-modal alignments even without ICL. Next, to further investigate the effect of jointly masked modality modeling, we design the following two variants: \u00b7 w/o Field-level Masking : We replace the field-level masking for textual data with the common random token-level masking, as discussed in Section 3.3.1. \u00b7 w/o Joint Reconstruction : The reconstruction of one masked modality depends only on itself and no longer relies on the help from the other modality. Table 5: The results of ablation study. The results are shown in Table 5. The performance drops, especially on the Logloss metric, when we either remove the field-level masking strategy or eliminate the joint reconstruction. Such a phenomenon demonstrates the importance of fine-grained feature-level RecSys '24, October 14-18, 2024, Bari, Italy Hangyu Wang et al. 0.15 0.3 0.6 Textual Mask Ratio r text 0.8618 0.8619 0.8620 0.8621 0.8622 0.8623 AUC MovieLens with r tab =0.15 AUC -0.3810 -0.3800 -0.3790 -0.3780 -0.3770 -0.3760 Negative Logloss (NLL) NLL 0.15 0.3 0.6 Tabular Mask Ratio r tab 0.8580 0.8600 0.8620 0.8640 AUC MovieLens with r text =0.15 AUC -0.3840 -0.3820 -0.3800 -0.3780 -0.3760 Negative Logloss (NLL) NLL 0.15 0.3 0.6 Textual Mask Ratio r text 0.8040 0.8050 0.8060 0.8070 AUC BookCrossing with r tab =0.15 AUC -0.5030 -0.5020 -0.5010 -0.5000 -0.4990 Negative Logloss (NLL) NLL 0.15 0.3 0.6 Tabular Mask Ratio r tab 0.8030 0.8040 0.8050 0.8060 0.8070 AUC BookCrossing with r text =0.15 AUC -0.5060 -0.5040 -0.5020 -0.5000 -0.4980 Negative Logloss (NLL) NLL Figure 3: The hyperparameter study on textual mask ratio \ud835\udc5f \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 (left column) and tabular mask ratio \ud835\udc5f \ud835\udc61\ud835\udc4e\ud835\udc4f (right column) on MovieLens-1M (top) and BookCrossing (bottom) datasets. 0.05 0.1 0.3 0.5 0.7 1.0 Temperature \u03c4 0.8605 0.8610 0.8615 0.8620 0.8625 0.8630 0.8635 AUC MovieLens AUC -0.3820 -0.3800 -0.3780 -0.3760 Negative Logloss (NLL) NLL 0.05 0.1 0.3 0.5 0.7 1.0 Temperature \u03c4 0.8050 0.8055 0.8060 0.8065 0.8070 BookCrossing AUC -0.5030 -0.5020 -0.5010 -0.5000 -0.4990 -0.4980 Negative Logloss (NLL) NLL AUC Figure 4: The hyperparameter study on the temperature \ud835\udf0f . alignment, which ensures the feature-level communication and interaction between ID-based models and PLMs for dual modalities.", "4.5 Hyperparameter Study": "4.5.1 The Impact of Mask Ratio \ud835\udc5f \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 and \ud835\udc5f \ud835\udc61\ud835\udc4e\ud835\udc4f . Since there are two independent mask ratio for textual and tabular modalities, we fix the mask ratio on the one side and alter the mask ratio on the other side from { 0 . 15 , 0 . 3 , 0 . 6 } . The results are in Figure 3, from which we observe that the best performance is generally achieved when both mask ratios are relatively small ( i.e. , 0.15). The reason is that excessive masking might lead to ambiguity in the target modality data, which hurts the model pretraining [13, 46]. So we set the \ud835\udc5f \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 and \ud835\udc5f \ud835\udc61\ud835\udc4e\ud835\udc4f both to 15%. 4.5.2 The Impact of Temperature Hyperparameter \ud835\udf0f . The temperature \ud835\udf0f controls the sharpness of estimated distribution. We select \ud835\udf0f from { 0 . 05 , 0 . 1 , 0 . 3 , 0 . 5 , 0 . 7 , 1 . 0 } , and report the results in Figure 4. When the temperature gradually grows, the performance increases first and then decreases, with the optimal temperature choice between 0 . 5 and 0 . 7. As suggested in previous works [8, 69], a too small temperature would only concentrate on the nearest sample pairs with top scores, and a too large temperature might lead to a flat distribution where all negative sample pairs receive almost the same amount of punishment. Both of them will hurt the final performance [35, 69]. Therefore, we choose \ud835\udf0f = 0 . 7 in our approach.", "4.6 Analysis on Fine-grained Alignment": "4.6.1 Feature-level Alignment. We conduct case studies to further explore the fine-grained feature-level alignment established during Figure 5: Visualization of similarities between the sample representations of masked textual and tabular data. \"Text\ud835\udc53 \" and \"Tab\ud835\udc53 \" denote that we mask the \ud835\udc53 -th field of the input data of textual or tabular modalities, respectively. Tab-1 Tab-2 Tab-3 Tab-4 Tab-5 Tab-6 Tab-7 Tab-8 Text-1 Text-2 Text-3 Text-4 Text-5 Text-6 Text-7 Text-8 MovieLens 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 Tab-1 Tab-2 Tab-3 Tab-4 Tab-5 Tab-6 Tab-7 Tab-8 Text-1 Text-2 Text-3 Text-4 Text-5 Text-6 Text-7 Text-8 BookCrossing 0.5 0.6 0.7 0.8 0.9 1.0 (a) FLIPw/o \ud835\udc40\ud835\udc3f\ud835\udc40 & \ud835\udc40\ud835\udc47\ud835\udc40 (b) FLIPw/o \ud835\udc40\ud835\udc3f\ud835\udc40 (c) FLIPw/o \ud835\udc40\ud835\udc47\ud835\udc40 Figure 6: The visualization of feature ID embeddings learned by different model variants on MovieLens-1M. We use SVD to project the feature embedding matrix into 2D data. (d) FLIP pretraining. For a dual-modality input pair ( x \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 \ud835\udc56 , x \ud835\udc61\ud835\udc4e\ud835\udc4f \ud835\udc56 ) , we first perform field-level data masking to mask out each field respectively, resulting in \ud835\udc39 pairs of corrupted inputs {( x \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 \ud835\udc56, ( \ud835\udc53 ) , x \ud835\udc61\ud835\udc4e\ud835\udc4f \ud835\udc56, ( \ud835\udc53 ) )} \ud835\udc39 \ud835\udc53 = 1 , where x \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 \ud835\udc56, ( \ud835\udc53 ) and x \ud835\udc61\ud835\udc4e\ud835\udc4f \ud835\udc56, ( \ud835\udc53 ) denote that we solely mask the \ud835\udc53 -field of the textual or tabular data. Then we employ the PLM and ID-based model to encode them into \ud835\udc39 pairs of normalized sample representations {( z \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 \ud835\udc56, ( \ud835\udc53 ) , z \ud835\udc61\ud835\udc4e\ud835\udc4f \ud835\udc56, ( \ud835\udc53 ) )} \ud835\udc39 \ud835\udc53 = 1 . Next, we compute the mutual similarity scores (measured by dot product) over each cross-modal representation pair, and visualize the heat map in Figure 5. We can observe that the similarity score varies a lot for crossmodal input pairs with different masked fields, and the top-similar score is achieved for pairs with the same masked field ( i.e. , on the diagonal). This indicates that FLIP can perceive the changes among field-level features for both modalities, and further maintain a oneto-one feature-level correspondence between the two modalities. 4.6.2 Visualization. we investigate the impact of the fine-grained alignment on the feature ID embedding learning. Following previous work [53], we adopt SVD [28] decomposition to project the learned ID embeddings from the ID-based model into 2D data. In Figure 6, we visualize the ID embeddings learned by four variants FLIPw/o \ud835\udc40\ud835\udc3f\ud835\udc40 & \ud835\udc40\ud835\udc47\ud835\udc40 , FLIPw/o \ud835\udc40\ud835\udc3f\ud835\udc40 , FLIPw/o \ud835\udc40\ud835\udc47\ud835\udc40 and FLIP. Note that FLIPw/o \ud835\udc40\ud835\udc3f\ud835\udc40 & \ud835\udc40\ud835\udc47\ud835\udc40 indicates the instance-level contrastive FLIP: Fine-grained Alignment between ID-based Models and Pretrained Language Models for CTR Prediction RecSys '24, October 14-18, 2024, Bari, Italy learning (ICL) variant without the MLM and MTM objectives. We have the following observations: \u00b7 The feature ID embeddings learned by FLIPw/o \ud835\udc40\ud835\udc3f\ud835\udc40 & \ud835\udc40\ud835\udc47\ud835\udc40 ( i.e. , ICL) collapse into a narrow cone, suffering from severe representation degeneration problem. In contrast, FLIP obtains feature ID embeddings with more distributed latent patterns, thus better capturing the feature diversity. This highlights the effectiveness of fine-grained alignment in promoting representation learning and mitigating the representation degeneration. \u00b7 Comparing FLIP with FLIPw/o \ud835\udc40\ud835\udc3f\ud835\udc40 or FLIPw/o \ud835\udc40\ud835\udc47\ud835\udc40 , we find that removing either MLM or MTM objective makes the learned embeddings more indistinguishable, demonstrating the necessity of dual alignments between modalities for learning effective representations.", "5 RELATED WORK": "", "5.1 ID-based Models for CTR Prediction": "CTR prediction serves as a core function module in personalized online services, including online advertising, recommender systems, etc [82]. ID-based models follow the common design paradigm: embedding layer, feature interaction (FI) layer, and prediction layer. These models take as input one-hot features of tabular data modality, and employ various interaction functions to capture collaborative signals among features. Due to the significance of FI in CTR prediction, numerous studies focus on designing novel structures for the FI layer to capture more informative and complex feature interactions. Wide&Deep [9] combines a wide network and a deep network to achieve the advantages of both. DCN [73] and DCNv2 [74] improve Wide&Deep by replacing the wide part with a cross network to learn explicit high-order feature interactions. DeepFM [20] combines DNN and FM, and xDeepFM [36] extends DeepFM by using a compressed interaction network (CIN) to capture feature interactions in a vector-wise way. Furthermore, explicit or implicit interaction operators are designed to improve performance, such as the productive operator [55], the logarithmic operator [10] and the attention operator [65, 79].", "5.2 PLMs for CTR Prediction": "Pretrained Language Models (PLMs) have demonstrated exceptional success in a wide range of tasks, owing to their extensive knowledge and strong reasoning abilities [3, 41, 57, 58]. Inspired by these achievements, the application of PLMs for recommender systems has received more attention [6, 7, 14, 15, 32, 38, 44, 70, 76, 87, 87]. Different from one-hot encoding in ID-based models, this line of research needs to convert the raw data into textual modality, thus retaining the original semantic information of features. Recent efforts to adapt PLMs for CTR prediction have yielded several breakthroughs [2, 11, 25, 31, 48, 71, 75, 81, 85]. For instance, CTR-BERT [50] leverages a two-tower structure with a user BERT and item BERT for final CTR prediction. PTab [43] pretrains a BERT model by masked language modeling, and then finetunes it on downstream tasks. P5 [18] uniformly converts different recommendation tasks into text generation tasks with T5 backbone [59]. However, PLMs encounter challenges in capturing field-wise collaborative signals and discerning features with subtle textual differences. Recent works [35, 60, 81] have tried to address this problem by adding text features into the ID-based model or integrating ID information into the PLM. However, they either overlook cross-modal interactions or depend solely on coarse-grained ICL, which is insufficient to capture fine-grained feature-level modality interactions. Therefore, we propose to conduct fine-grained feature-level alignment between ID-based models and PLMs via the jointly masked tabular/language modeling, which learns fine-grained interactions between tabular IDs and word tokens by the way of mask-and-predict. In the finetuning stage, we also propose to jointly tune both models, and thus leverage the benefits of both textual and tabular modalities to achieve superior CTR prediction performance.", "6 CONCLUSION": "In this paper, we propose FLIP, a model-agnostic framework that achieves the fine-grained alignment between ID-based models and PLMs. We view tabular data and transformed textual data as dual modalities, and design a novel fine-grained modality alignment pretraining task. Specifically, the joint reconstruction for masked language/tabular modeling (with specially designed masking strategies) and cross-modal contrastive learning are employed to accomplish feature-level and instance-level alignments, respectively. Furthermore, we propose to jointly finetune the ID-based model and PLM to achieve superior performance by adaptively combining the outputs of both models. Extensive experiments show that FLIP outperforms state-of-the-art baselines on three real-world datasets, and is highly compatible with various ID-based models and PLMs.", "ACKNOWLEDGMENTS": "The Shanghai Jiao Tong University team is partially supported by National Natural Science Foundation of China (62177033, 62076161) and Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102). The work is also sponsored by Huawei Innovation Research Program. We thank MindSpore [1] for the partial support of this work.", "REFERENCES": "[1] 2020. MindSpore. https://www.mindspore.cn/ [2] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. Tallrec: An effective and efficient tuning framework to align large language model with recommendation. arXiv preprint arXiv:2305.00447 (2023). [3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877-1901. [4] Peter B\u00fchlmann. 2012. Bagging, boosting and ensemble methods. Handbook of computational statistics: Concepts and methods (2012), 985-1022. [5] Yin-Wen Chang, Cho-Jui Hsieh, Kai-Wei Chang, Michael Ringgaard, and Chih-Jen Lin. 2010. Training and testing low-degree polynomial data mappings via linear SVM. Journal of Machine Learning Research 11, 4 (2010). [6] Junyi Chen. 2023. A Survey on Large Language Models for Personalized and Explainable Recommendations. arXiv e-prints , Article arXiv:2311.12338 (Nov. 2023), arXiv:2311.12338 pages. arXiv:2311.12338 [cs.IR] [7] Jin Chen, Zheng Liu, Xu Huang, Chenwang Wu, Qi Liu, Gangwei Jiang, Yuanhao Pu, Yuxuan Lei, Xiaolong Chen, Xingmei Wang, et al. 2023. When large language models meet personalization: Perspectives of challenges and opportunities. arXiv preprint arXiv:2307.16376 (2023). [8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A simple framework for contrastive learning of visual representations. In International conference on machine learning . PMLR, 1597-1607. [9] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems . 7-10. RecSys '24, October 14-18, 2024, Bari, Italy Hangyu Wang et al. [55] Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, and Jun Wang. 2016. Product-based neural networks for user response prediction. In 2016 IEEE FLIP: Fine-grained Alignment between ID-based Models and Pretrained Language Models for CTR Prediction RecSys '24, October 14-18, 2024, Bari, Italy 16th international conference on data mining (ICDM) . IEEE, 1149-1154. [56] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning . PMLR, 8748-8763. [57] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training. (2018). [58] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog 1, 8 (2019), 9. [59] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research 21, 1 (2020), 5485-5551. [60] Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, and Chao Huang. 2023. Representation Learning with Large Language Models for Recommendation. arXiv preprint arXiv:2310.15950 (2023). [61] Steffen Rendle. 2010. Factorization machines. In ICDM . [62] Robert E Schapire. 2013. Explaining adaboost. In Empirical Inference: Festschrift in Honor of Vladimir N. Vapnik . Springer, 37-52. [63] Andrew I Schein, Alexandrin Popescul, Lyle H Ungar, and David M Pennock. 2002. Methods and metrics for cold-start recommendations. In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval . 253-260. [64] Yaya Shi, Xu Yang, Haiyang Xu, Chunfeng Yuan, Bing Li, Weiming Hu, and Zheng-Jun Zha. 2022. EMScore: Evaluating video captioning via coarse-grained and fine-grained embedding matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 17929-17938. [65] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, and Jian Tang. 2019. Autoint: Automatic feature interaction learning via selfattentive neural networks. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management . 1161-1170. [66] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems . 5998-6008. [67] Mengting Wan and Julian McAuley. 2018. Item recommendation on monotonic behavior chains. In Proceedings of the 12th ACM conference on recommender systems . 86-94. [68] Mengting Wan, Rishabh Misra, Ndapandula Nakashole, and Julian McAuley. 2019. Fine-Grained Spoiler Detection from Large-Scale Review Corpora. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics . 26052610. [69] Feng Wang and Huaping Liu. 2021. Understanding the behaviour of contrastive loss. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition . 2495-2504. [70] Hangyu Wang, Jianghao Lin, Bo Chen, Yang Yang, Ruiming Tang, Weinan Zhang, and Yong Yu. 2024. Towards Efficient and Effective Unlearning of Large Language Models for Recommendation. arXiv preprint arXiv:2403.03536 (2024). [71] Lei Wang and Ee-Peng Lim. 2023. Zero-Shot Next-Item Recommendation using Large Pretrained Language Models. arXiv preprint arXiv:2304.03153 (2023). [72] Peng Wang, Jiang Xu, Chunyi Liu, Hao Feng, Zang Li, and Jieping Ye. 2020. Masked-field Pre-training for User Intent Prediction. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management . 27892796. [73] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network for ad click predictions. In Proceedings of the ADKDD'17 . 1-7. [74] Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong, and Ed Chi. 2021. Dcn v2: Improved deep & cross network and practical lessons for web-scale learning to rank systems. In Proceedings of the Web Conference 2021 . 1785-1797. [75] Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, and Chao Huang. 2023. LLMRec: Large Language Models with Graph Augmentation for Recommendation. arXiv preprint arXiv:2311.00423 (2023). [76] Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, et al. 2023. A Survey on Large Language Models for Recommendation. arXiv preprint arXiv:2305.19860 (2023). [77] Yunjia Xi, Jianghao Lin, Weiwen Liu, Xinyi Dai, Weinan Zhang, Rui Zhang, Ruiming Tang, and Yong Yu. 2023. A Bird's-eye View of Reranking: from List Level to Page Level. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining . 1075-1083. [78] Yunjia Xi, Weiwen Liu, Jianghao Lin, Jieming Zhu, Bo Chen, Ruiming Tang, Weinan Zhang, Rui Zhang, and Yong Yu. 2023. Towards Open-World Recommendation with Knowledge Augmentation from Large Language Models. arXiv preprint arXiv:2306.10933 (2023). [79] Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua. 2017. Attentional factorization machines: learning the weight of feature interactions via attention networks. In Proceedings of the 26th International Joint Conference on Artificial Intelligence . 3119-3125. [80] Xin Xin, Bo Chen, Xiangnan He, Dong Wang, Yue Ding, and Joemon M Jose. 2019. CFM: Convolutional Factorization Machines for Context-Aware Recommendation.. In IJCAI , Vol. 19. 3926-3932. [81] Zheng Yuan, Fajie Yuan, Yu Song, Youhua Li, Junchen Fu, Fei Yang, Yunzhu Pan, and Yongxin Ni. 2023. Where to go next for recommender systems? id-vs. modality-based recommender models revisited. arXiv preprint arXiv:2303.13835 (2023). [82] Shuai Zhang, Lina Yao, and Aixin Sun. 2017. Deep learning based recommender system: A survey and new perspectives. arXiv preprint arXiv:1707.07435 (2017). [83] Wenxuan Zhang, Hongzhi Liu, Yingpeng Du, Chen Zhu, Yang Song, Hengshu Zhu, and Zhonghai Wu. 2023. Bridging the Information Gap Between DomainSpecific Model and General LLM for Personalized Recommendation. arXiv preprint arXiv:2311.03778 (2023). [84] Weinan Zhang, Jiarui Qin, Wei Guo, Ruiming Tang, and Xiuqiang He. 2021. Deep learning for click-through rate estimation. IJCAI (2021). [85] Yang Zhang, Fuli Feng, Jizhi Zhang, Keqin Bao, Qifan Wang, and Xiangnan He. 2023. Collm: Integrating collaborative embeddings into large language models for recommendation. arXiv preprint arXiv:2310.19488 (2023). [86] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223 (2023). [87] Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, and Ji-Rong Wen. 2023. Large language models for information retrieval: A survey. arXiv preprint arXiv:2308.07107 (2023). [88] Cai-Nicolas Ziegler, Sean M McNee, Joseph A Konstan, and Georg Lausen. 2005. Improving recommendation lists through topic diversification. In Proceedings of the 14th international conference on World Wide Web . 22-32."}
