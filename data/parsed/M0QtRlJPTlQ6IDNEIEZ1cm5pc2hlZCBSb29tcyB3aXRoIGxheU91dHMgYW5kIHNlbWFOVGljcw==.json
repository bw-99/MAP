{"title": "3D-FRONT: 3D Furnished Rooms with layOuts and semaNTics", "authors": "Huan Fu; Bowen Cai; Lin Gao; Lingxiao Zhang; Jiaming Wang; Cao Li; Qixun Zeng; Chengyue Sun; Rongfei Jia; Binqiang Zhao; Hao Zhang; Alibaba Group", "pub_date": "2021-05-14", "abstract": "Figure 1: 3D-FRONT is a new, large-scale, and comprehensive repository of synthetic indoor scenes with professionally and distinctively designed layouts, a large number (18,968) of rooms populated with 3D furniture objects that are stylistically compatible and endowed with high-quality textures. All freely available to the academic community and beyond.", "sections": [{"heading": "Introduction", "text": "The computer vision community has invested much effort into the study of 3D indoor scenes, from 3D reconstruction, visual SLAM, and navigation, to scene understanding, affordance analysis, and generative modeling. With datadriven and learning-based approaches receiving more and more attention in recent years, there has been a steady accumulation of indoor scene datasets [25,34,41,3,17,5,8,16,21,45,22] to drive the deep learning revolution that has redefined the landscape of indoor scene processing.\nExisting 3D scene datasets all fall into two broadly categories: acquired (via scanning and reconstruction) vs. de-Table 1: Comparison between prominent 3D indoor scene datasets, where \"#3DFRs\" represents the number of rooms or scenes populated with 3D furniture objects, \"N/A\" = \"not available\", \"N/R\" = \"not reported\", \"Raw Mesh\" denotes machine reconstructed meshes, and \"Raw PCD\" refers to reconstructed point clouds. For model textures, \"Rec. from Scan\" is the result of reconstruction from raw RGB-D data, while \"Amateur\" and \"Professional\" refer to who designed the textures. The \"3D structures\" annotatd by Structured3D [45] contain information on primitives including 3D boxes and their relations. signed (i.e., synthetic scenes created by humans). In terms of data volume, the largest repository is ScanNet [8] which consists of 2.5M RGB-D images from 1,513 scanned real scenes acquired by commodity sensors, in 707 distinct spaces. The 3D scenes, including textured 3D objects, were recovered by state-of-the-art 3D reconstruction techniques from the raw scans, which are typically noisy and incomplete. As a result, the reconstructed meshes are often of low quality, both in geometric fidelity and texture quality.\nIn the world of synthetic 3D indoor scene datasets, the recent exit by SUNCG [32] has left an apparent void in the community. Most recently, Structured3D [45] and Open-Room [22] have emerged as promising alternatives. In addition to providing professionally designed room layouts, Structured3D [45] aims to provide large-scale photorealistic scene images with rich 3D structure annotations. However, the actual 3D furniture objects populating the scenes are not included in the dataset. OpenRoom [22] replaces detected objects in a set of 1,068 scanned scenes from ScanNet [8] with CAD models from ShapeNet [6]. A major contribution of this dataset is to provide ground-truth annotations of complex material parameters for the CAD objects. However, the dataset has not been released at this point and according to the authors' account, only 2.5K CAD models were annotated with material properties.\nIn this paper, we introduce 3D-FRONT (3D Furnished Rooms with layOuts and semaNTics), a new, large-scale, and comprehensive repository of synthetic 3D indoor scenes. It contains professionally and distinctively designed layouts spanning 31 scene categories, object semantics (e.g., category, style, and material labels), and a large number (18,968) of rooms populated with 3D furniture objects. Most importantly, these 3D furniture objects are all endowed with high-quality textures, thanks to 3D-FUTURE [13], a recently released dataset of quality 3D furniture used in industrial productions. Furthermore, the selection of furniture objects from 3D-FUTURE to populate the scenes in 3D-FRONT has been inpired by expert interior designs. Specifically, the selection is based on a recommender system learned from the expert designs, while taking into account of furniture styles both in terms of geometry and texture. As a result, the furnished rooms in 3D-FRONT consist of stylistically compatible objects adhering to the design inspirations.\nIn Table 1, we present essential information for the current public release of 3D-FRONT and compare to other prominent indoor scene datasets. As we can see, the most compelling feature of our dataset is the large number of 3D furnished rooms, which far surpasses all the other publicly available datasets. Style compatibility, as well as the high texture quality, of the furniture objects in each scene (see middle of Figure 1) is another unique attribute of 3D-FRONT. On top of all these, the total number of rooms with professionally designed layouts is much larger than 18,968; it is close to 45,000. Last but not least, we share Trescope, a light-weight rendering tool, with the community so that the users of 3D-FRONT can easily capture their desired 2D renderings and annotations to guide their image-driven learning tasks. We will continuously improve 3D-FRONT by releasing an industrial rendering engine (AceRay) and providing much enriched texture and 3D geometry contents.\nWe anticipate that 3D-FRONT, being as comprehensive as it is, will enable and further drive a whole suite of AIpowered and data-driven scene analysis and modeling applications. We demonstrate two applications which cannot be well supported by other publicly available datasetsthese applications are best served by having a large number of high-quality textured mesh models with style consistency, a unique feature of 3D-FRONT. One such application is learning to texture 3D objects in indoor scenes. In another, by learning the layout of 3D furniture in each room with [38], we can coherently predict and arrange functional furniture for an empty room.", "publication_ref": ["b4", "b33", "b16", "b7", "b15", "b21", "b7", "b21", "b21", "b7", "b5", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Over the past years, a large number of RGB-D benchmarks have been constructed and made publicly available [19,2,25,34,41,31,17,5,8,16,24,21,45,22,3,30,33,40,10,15,43]. Current 3D scene datasets are mainly collected based on scanning and reconstruction or human creation. These datasets thus fall into two broadly categories: Acquired vs. Designed. Acquired Scenes. To construct \"Acquired\" datasets, researchers capture RGB-D videos, reconstruct the scene meshes, and manually label the frames or the reconstructed scenes. For example, NYU-Depth v2 [25] gathered 464 short RGB-D sequences from different rooms via Kinect, where 1,449 images are selected and labeled with pixellevel annotations. SUN RGB-D [31] collected 10,335 RGB-D images and provided more 2.5D annotations, such as 2D polygons and 3D bounding boxes correspondences, room layouts, and scene categories. These datasets may lack the physical relationship between the frames and the scene space's real 3D structure. To address the issue, SUN3D [41] developed an interactive reconstruction pipeline to recover the 3D scene structures for 254 different spaces in 41 buildings, in which 8 scenes are provided with semantic labels for 3D point clouds and camera poses. Sce-neNN [17] improved the pipeline by recovering mesh surfaces instead of point clouds for 100 scans. Further, one of the largest \"Scanned\" datasets, i.e., ScanNet [8], has been established. It reconstructed 1,513 rooms based on 2.5M RGB-D views, and labeled rich 3D annotations, including estimated 3D camera poses, surface reconstructions, semantic segmentation, and 2D-3D alignments.\nSince 3D scene reconstruction with fine geometric and textures details is still a challenging problem with the depth cameras on the shelf such as Kinect, the mesh qualities in these scene dataset are usually not as good as the synthetic data. Besides, some of the 3D annotations may be unreliable or imprecise due to the reconstruction error, such as camera pose and 2D-3D alignment.\nDesigned Scenes. Another type of scene dataset is from the human creation with professional design software as this 3D-FRONT. In addition to 3D-FRONT, there is one synthetic (designed) dataset that shares both the layout and the well-posited 3D CAD mesh models, i.e., SceneNet [16] with providing 59 scenes. Several other synthetic benchmarks share 2D and 2.5D contents based on designed synthetic scenes. For example, InteriorNet [21] released 15k sequences and 5M images, which are rendered from their large-scale scene packages. Further, Structure3D [45] provided 21,835 panoramic images with the corresponding structure annotations, such as panoramic layouts, depth, surface normal. Recently, Li et al. [22] built OpenRooms, a synthetic benchmark based on ScanNet, and planned to share rendered images with their high-quality SVBRDF and spatially-varying lighting. Also, Hypersim [28] presented a photorealistic synthetic dataset for holistic indoor scene understanding, focusing on providing per-pixel depth and disentangled illuminance and reflectance properties over scene images designed by professional artists.\nThese large-scale synthetic datasets have not made the completed scene packages, including the floorplans' mesh, the large amount of involved CAD models with fine geometric and texture details, and the layout with design ideas, publicly available. In contrast, 3D-FRONT shares everything that is used to construct houses, from real layouts to interior design ideas and involved objects. The holiest repository of indoor scene packages enables a robot to navigate in them. It also allows the researchers to render whatever information they need for new subjects studying.", "publication_ref": ["b18", "b4", "b33", "b30", "b16", "b7", "b15", "b23", "b21", "b29", "b14", "b42", "b4", "b30", "b16", "b7", "b15", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "Building 3D-FRONT", "text": "Creating a large-scale 3D scene repository such as 3D-FRONT is a non-trivial task. Our 3D-FRONT project has been built on a large volume (about 60K) of professionally designed houses and 1M 3D CAD meshes. While we are unable to publish all these meshes, due to copyright restrictions, all the models and learning algorithms employed during the data collection progress have been trained on the large database. As shown in Figures 2 and3, we start from some house collections, create room suites, optimize the layout, verify the created interior designs, and finally assign qualified camera viewpoints. In the following, we will detail the pipeline as well as the techniques involved.", "publication_ref": [], "figure_ref": ["fig_0", "fig_1"], "table_ref": []}, {"heading": "Room Suite Creation", "text": "Given a synthetic house and its professional design ideas, we automatically create room suites for the scenes. Here, the design ideas for a room consist of the category labels of the objects, and their positions, orientations, sizes, and styles. Taking a bedroom as an example, we first randomly select a seed object, e.g., a bed, from a 3D model pool according to the required size and style. We then recurrently identify the visually matched furniture according to the room suite thus far until the room is filled.  We mainly rely on the Furnishing Suite Composition (FSC) approach in 3D-FUTURE [13] to create visually compatible suites. Specifically, leveraging on the largescale expert scene designs, we carry out two tasks, i.e., mask prediction and suite compatibility scoring, to model visual compatibility. The first task predicts the masked (removed) furniture given other objects in a suite. And the second task evaluates the compatibility score of the input suite. We utilize a textured image to represent each object (furniture). The two tasks optimize a visual embedding network (VEN) [29] and two transformer architectures [36,9], so that the trained VEN can extract informative visual feature for each object. With the learned visual representation and the given attributes, including category, style, color, material, and size, for each object. We train gradient boosting decision trees (GBDT) [12] to infer decision rules based on these information, and post a logistic regression (LR) layer to estimate the comparability scores of the room suites. These two techniques are integrated as the GBDT-LR model. 3D-FUTURE [13] first adopts the visual embedding ex-tracted from VEN to perform a primary ranking, then employs the trained GBDT-LR model to re-rank the selected candidates for online recommendation. We improve the primary ranking stage by considering graph auto-encoder techniques [7]. In detail, we define an undirected graph G = {V, E}, and learn a graph auto-encoder (GAE) for visual compatibility prediction following [7]. The graph nodes are all the involved objects in the designed house database. Each node is represented with a feature vector extracted from VEN. Each edge's weight is equal to 1 if the two objects are visually appealing, and 0 otherwise. With the graph, we first learn a graph convolutional network (GCN) [18] as an encoder to propagate neighborhood information to obtain new representations, depending on the connections. Then, we adopt a fully connected layer as a decoder to reconstruct the weight matrix. When building 3D-FRONT, we use the trained models to perform recommendation from the 3D-FUTURE pool [13].", "publication_ref": ["b28", "b35", "b8", "b6", "b6", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Layout Optimization and Verification", "text": "We observed that with the room suites constructed using the techniques described so far, placing objects into the corresponding rooms according to their suggested positions and orientations, various layout artifacts still remained. For example, a bed may overlap with its nearby nightstand in the 3D space. Other examples are highlighted in red boxes in Figure 2. One of the main reasons is that it is difficult to find a visually matched furniture based on concurrent room suite that, at the same time, has the same size required by the design ideas -there is potential conflict between style and size compatibilities. To this end, we apply the layout optimization algorithm proposed in [39].\nSpecifically, we start from the initially created designs, and slightly modify the object positions in the room suites in order to satisfy several layout constraints in [39], including pairwise distance, focal point distance, distance to wall, accessibility, and collision. These constraints were con-structed based on polling statistics of the design rules from our synthetic house database. Since the intial layouts often provide a good starting point, we only optimize the defined energy function in up to 50 iterations. On average, the optimization only takes 10s for each room.\nWe further verify the created designs and remove the unsatisfied ones to ensure dataset quality. To facilitate the reviewing step, we develop a light-weight renderer Trescope that enables the reviewers to browse the synthetic houses online in an interactive manner. Note that, Trescope supports offline benchmark rendering on local machines for 3D-FRONT. The renderer will be shared so that users of 3D-FRONT can capture their desired renderings such as images, depth, normal, and segmentation.", "publication_ref": ["b38", "b38"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Viewpoint Generation", "text": "The viewpoint generation stage aims to assign several cameras to each scene, and ensure most of the cameras have practical viewpoints. For the purpose, we recast this problem as a similarity measure problem, thus mainly transfer knowledge from expert ideas. Specifically, we are provided with many excellent scenes with suggested camera viewpoints by expert designers (about 5,000 rooms in our database). Given a scene, we choose a \"center\" object as the origin to build the world coordinate. We compute the normalized distances to define an object group (like a graph), and convert its mask projection as a feature vector to represent the scene structure. With this method, we can compute cosine similarity to perform scene retrieval, thus generate practical camera viewpoints for new scenes. Note that, for the created scenes, we are allowed to extract different scene features by selecting different \"center\" objects. This would guarantee the diversity of the generated viewpoints.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Validation and Assessment", "text": "In this section, we offer several means to validate and assess the way our dataset was built and the quality and utility of the data. Applications are discussed in Section 5. Evaluation of recommender system. We collected 8,000 scene designs and their design logs from the online Topping Homestyler platformfoot_0 for our evaluation. We discuss several metrics, including Area Under The Curve (AUC) [11], 1-N Average Rank (1-N Avg Rank), N-1 Average Rank (N-1 Avg Rank), 1-N Hit@10, and 1-N Hit@20, to show the superiority of the designs in 3D-FRONT. These metrics are calculated based on experts' online logs. To explain these measurements, we take a room suite (Bed, Nightstand, Chair) \u21d4 (A, B, C) as an example, where a designer chooses the objects A, B, and C in order. (B and C). Here, Nightstand and Chair are the required categories, and B and C are the specific objects. N-1 denotes that we recommend each object given the other two. Hit@K calculates the TopK recall accuracy. For (A, B) \u2192 Chair, a correct recommendation in TopK means that C ranks less than K. We refer to the supplementary materials for more details about these metrics. The qualitative scores are reported in Table 2. Generally, incorporating GAE [7] with the original FSC [13] would yield improvements on all metrics. We point out that both FSC and its improved version (FSC+GAE) can generate high-quality room suites, though it seems that the performance numbers of 1-N Hit@10 (33.6%\u223c36.1%) and 1-N Avg Rank (41.6\u223c37.3) are not significant. But it should note that our 3D pool contains more than 1M models. The vast collection makes the visual compatibility inspired recommendation task extremely challenging, though we have filtered out invalid items in the retrieval sequences according to the fine-grained category labels. It's also worth to mention that, after layout optimization and verification, our AI created designs (room suites + professional design ideas) have been used for VR shopping by eCommerce merchants. The rate of our high-quality designs (or customer preferred designs) is 88%, while it is only 71% for designs from ordinal-level designers. The comparison may not fair for ordinal-level designers since we reuse professional design ideas. However, it strongly supports that the shared scene User study. We conduct a series of user studies, on Amazon Mechanical Turk (AMT), to assess the quality of the data provided by 3D-FRONT, in comparison with SUNCG [32]. The quality criteria considered include those related to scene layouts (in terms of plausibility, design quality, and richness of texture) and individual objects (in terms of texture quality and preferability), as well as style compatibility. We refer to the supplemental material for more details on each study. As for the user study setting, we randomly sampled 90 pairs of scenes and 30 pairs of 3D models from 3D-FRONT and SUNCG based on scene type and model category. Each pair was labeled by 20 master-level annotators in AMT. Thus, the scene and model scores are calculated using 1,800 and 600 feedback, respectively.\nFrom the scores reported in Table 3, we see that for each quality criterion assessed, the majority of Turkers (between 60% and 70%), preferred data presented by 3D-FRONT. We believe that higher-quality datasets would not only lead to improved performance of algorithms which are trained on these datasets, but also enable new applications. It should also be evident that most, if not all, applications in the computer vision and graphics community which had utilized SUNCG, would also be well supported by 3D-FRONT.\nProperties of 3D-FRONT. One of the most desirable features of our dataset is that it publically shares all the essential data that would enable the modeling of high-quality indoor scene, from layout semantics down to stylistic and texture details of individual objects. While the layout ideas are directly sourced from professional designs, the interior designs are transferred from expert creations followed by a post verification process. Figure 4 shows some additional house examples from our dataset. 3D-FRONT enables a variety of AI-powered tasks related to 3D scenes, including data-driven designing studies, such as floorplan synthesis, interior scene synthesis, and scene suites compatibility prediction, that other scene datasets do not support adequately. It also benefits the study of 3D scene understanding subjects, such as SLAM, 3D scene reconstruction, and 3D scene segmentation.\nFigures 5 and6 reveal some relevant statistics related to our dataset, with more that can be found in the supplemental material. Further, we assign selected practical camera viewpoints to furnished the scenes and release Trescope, a light-weight rendering tool compatible with 3D-FRONT. These would allow users of 3D-FRONT to easily render images and annotations to support their 2D vision studies. Last but not least, we will continuously improve 3D-FRONT by adding more features. A certain plan is to share much en- ", "publication_ref": ["b6"], "figure_ref": ["fig_2", "fig_3"], "table_ref": ["tab_0", "tab_1"]}, {"heading": "Applications", "text": "We present two applications, interior scene synthesis and object texturing in scene contexts, to demonstrate the utility of our dataset. This only represents a small sampler of applications that can benefit from 3D-FRONT.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Interior Scene Synthesis", "text": "The main goal of current scene synthesis methods [38,27,20,37,44] is to coherently predict and arrange functional furniture shapes. The extensive professional layout designs provided by 3D-FRONT may be immensely valuable to support the development of learning-based methods for this synthesis task.\nOur demonstration uses the state-of-the-art neural scene synthesis method of Wang et al. [38], where each 3D scene is represented in an orthographic top-down view, which constitutes depth, room mask, wall mask, object mask, and orientation. Their method trains a deep convolutional neural network to iteratively capture scene priors, so as to decide whether to add a next object, what category of object to add and where, and finally insert an instance of that object category with estimated rotation into the scene. Following [38], we conduct our experiment on two scene types, i.e., bedroom (Bedroom, MasterBedRoom, and Sec-ondBedRoom) and living room (LivingRoom and Living-DiningRoom), and remove the rooms whose width or length is larger than 6 meters. As a result, we obtain 6,230 bedrooms and 645 living rooms, with 6,070 / 485 rooms for training and 160 / 160 rooms for evaluation. We refer to [38] for more details on training and test settings.\nWe evaluate diversity of the synthesized results using converge (COV) and minimum matching distance (MMD) [1] measured by Chamfer Distance (CD) or Earth-Mover Distance (EMD) between scenes synthesized by models trained on 3D-FRONT and on SUNCG, respectively. The results were generated from empty rooms in the combined test set of 3D-FRONT and SUNCG. For each synthesized scene, we randomly sample 100K points and calculate these metrics against the ground truth. Recall that lower MMD and higher COV indicate better synthesis ability of a method. Quantitative comparisons in Table 4 show the dataset advantage of 3D-FRONT over SUNCG. A qualitative comparison is shown in Figure 7.\nIn addition, we conduct a user study on AMT where Turkers were asked to choose scenes synthesized by [38], from randomly choosen empty rooms, that are deemed to be more \"plausible\"; see supplemental materials for details. From the user feedback, we find that layouts synthesized by the model trained on 3D-FRONT were chosen 64.8% of the time (vs. 35.2% for SUNCG). All these results strongly demonstrate the utility of our new dataset, over other alternatives, for the important scene synthesis task.", "publication_ref": ["b26", "b0"], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "Texturing 3D Models in Indoor Scenes", "text": "The quality, richness, and compatibility of object textures in an indoor scene can greatly enhance its realism. The textured 3D models available from 3D-FRONT fulfill these very characteristics, and we expect our dataset to benefit the development of many data-driven scene texture synthesis algorithms. In comparison to texturing a single 3D object [26,14,23], doing the same to an object in the context of an indoor scene must take into account that scene context to ensure both quality and visual compatibility.\nWe extend a recent generative model for textured meshes, TM-Net [14], to the 3D scene texturing task. TM-Net represents 3D shape parts with their structural deformable boxes, thus enables to generate part-level structural texture atlases for the given untextured 3D shapes. When applying it to the 3D scene configuration, we enforce the texture coherence between 3D objects by randomly choosing a shape in the scene, extracting its texture's VGG feature, and finally using the feature to guide the generation of other objects' textures in the training setting. After training the generative models, we synthesize texture for a random shape, and use it as a condition for other objects' texture generation to keep the consistency.\nWe conduct a simple experiment to validate the advantage of 3D-FRONT, as training data for TM-Net, for the generation of chair textures given table textures as a condition or guidance. Comparisons are made to ShapeNet [6], which also contains textured 3D models and can serve as the training data. Figure 8 presents some qualitative results where the table-chair settings were sampled from dining rooms. TM-Net trained on 3D-FRONT tends to generate richer and more diverse textures, as can be verified by both a quantitative test and a user study. Specifically, the model trained on 3D-FRONT yields a LPIPS [42] score of 0.289, which outperforms its ShapeNet counterpart, which has a score of 0.215, where we recall that LPIPS is a measure of the diversity of generated textures. Our user study on AMT, where users were asked to select which generated textures were \"richer\", also shows that results by TM-Net trained on 3D-FRONT were selected 61.1% of the time (vs. 38.9% for ShapeNet); see supplemental material for more details.", "publication_ref": ["b25", "b13", "b22", "b13", "b5", "b41"], "figure_ref": ["fig_6"], "table_ref": []}, {"heading": "Conclusion and future work", "text": "We present 3D-FRONT, a new large-scale dataset of synthetic 3D indoor scenes. Up to now, there have been a variety of 3D scene datasets established to serve different purposes. Some focus on photorealistic renderings of artistcreated scenes, possibly with instance segmentations and per-pixel material and illumination ground truth data, while others acquire large volumes of raw scans of the world to drive research in 3D scene reconstruction and modeling. Compared to these efforts, 3D-FRONT offers the largest publicly available collection of professional designed room layouts instanced with high-quality textured CAD meshes.\nOne of our intentions was to fill a void in the vision and graphics community after SUNCG became unavailable. Yet, our dataset surpasses SUNCG in three aspects: professional vs. amateur layout designs, CAD model quality, and style compatibility. We demonstrate that these distinctive features enable several data-driven applications which were not well supported by other datasets. In the future, we will continuously improve 3D-FRONT by releasing an industrial render engine (AceRay) and providing much enriched texture and 3D geometry contents. Figure 9: Metrics in \"Validation and Assessment\". \"1-N\" here means 1-7 because there are seven required objects in the room. When recommending a single sofa, the designer selected the 16th sofa from the retrieval sequence. Thus, the single sofa ranks 16 in this recommendation step. Hit@K refers to TopK recall accuracy [35].", "publication_ref": ["b34"], "figure_ref": [], "table_ref": []}, {"heading": "Metrics in \"Validation and Assessment\"", "text": "We have briefly explained some metrics in Sec. 4 (Validation and Assessment) in the main paper. Here, we present an example in Figure 9 to make them more clear.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Other Statistics", "text": "In Figure 10, we show the distribution of number of objects per room. At this time, 3D-FRONT's rooms are furnished by the functional furniture.\nIn Figure 11, we report the distribution of annotated object labels for 3D-FRONT's scenes corresponded to the 3D-FUTURE [13] 3D CAD model categories.\nIn Figure 13, we show the distribution of object categories conditioned on different room types. The area of the square denotes the frequency of a given object category that appears in a certain room type. The frequency is normalized for each object category. It strongly implies the relationships between objects and rooms. For example, categories such as children cabinet, bunk bed, and kids bed are more likely to appear in a kid room, while bookcase and desk are more likely to appear in a study room. We can learn rich design knowledge from the distribution.\nIn Figure 12, we present the physical sizes over the rooms and houses. The 3D-FRONT dataset are measured in real-world spatial dimensions (units are in meters).", "publication_ref": [], "figure_ref": ["fig_7", "fig_8", "fig_10", "fig_9"], "table_ref": []}, {"heading": "User Studies", "text": "We perform user studies to show the quality of 3D-FRONT using Amazon Mechanical Turk (AMT). The compared datasets are 3D-FRONT, SUNCG [32], and ShapeNet [6]. The questions and the scores are shown in Figure 14. The tasks are explained below.\nDataset: Scene Quality. We have 90 pairs of scenes randomly sampled from SUNCG and 3D-FRONT based on scene types (LivingRoom, DiningRoom, and Bed Room) in our questionnaire. Each scene type contains 30 pairs. We study layout plausibility, design quality, texture quality, and style compatibility in this task. We have collected 20 questionnaires from master-level annotators in AMT. That means each scene pair has been labeled by 20 annotators. Thus, the final scores are calculated using 1,800 feedback.\nDataset: Model Quality. We have 30 pairs of furniture models randomly sampled from SUNCG and 3D-FRONT based on categories in our questionnaire. We study texture quality and model's visual quality in this task. We have collected 20 questionnaires from master-level annotators in AMT. Thus, the final scores are calculated using 600 feedback.  Application: Layout Synthesis. We randomly sampled 60 rooms from 3D-FRONT (36) and SUNCG (24). For each room, we synthesizing a pair of layouts, with one produced by the model trained on 3D-FRONT and the other generated by the model trained on SUNCG. We study layout plausibility in this task. We have collected 20 questionnaires from master-level annotators in AMT. Thus, the final score is cal-culated using 1,200 feedback.\nApplication: Texture Synthesis. We have selected 5 DiningRoom corners from 3D-FRONT and textured their chairs and tables using the learned texture synthesis models (3D-FRONT vs. ShapeNet). For each corner, we have perform texture synthesis three times with random noises. We study texture diversity in this task. We have collected Figure 14: User Studies. We perform the listed user studies using Amazon Mechanical Turk (AMT). 3D-FRONT's scores are reported in the figure. From the dataset comparisons, we see that for each quality criterion assessed, the majority of Turkers (between 60% and 70%), preferred data presented by 3D-FRONT. See Sec. 8 for the experimental settings. 20 questionnaires from master-level annotators in AMT. Thus, the final score is calculated using 100 feedback.", "publication_ref": ["b5", "b35", "b23"], "figure_ref": ["fig_2", "fig_2"], "table_ref": []}, {"heading": "More House & Room Examples", "text": "In Figure 15, Figure 16, Figure 17, Figure 18, and Figure 19, we present more house and room examples to demonstrate the quality of 3D-FRONT.     ", "publication_ref": [], "figure_ref": ["fig_11", "fig_12", "fig_13", "fig_14", "fig_15"], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Learning representations and generative models for 3d point clouds", "journal": "PMLR", "year": "2018", "authors": "Panos Achlioptas; Olga Diamanti; Ioannis Mitliagkas; Leonidas Guibas"}, {"ref_id": "b1", "title": "Joint 2d-3d-semantic data for indoor scene understanding", "journal": "", "year": "2017", "authors": "Iro Armeni; Sasha Sax; Silvio Amir R Zamir;  Savarese"}, {"ref_id": "b2", "title": "3d semantic parsing of large-scale indoor spaces", "journal": "", "year": "2016", "authors": "Iro Armeni; Ozan Sener; Helen Amir R Zamir; Ioannis Jiang; Martin Brilakis; Silvio Fischer;  Savarese"}, {"ref_id": "b3", "title": "Scan2CAD: Learning cad model alignment in rgb-d scans", "journal": "", "year": "2019-06", "authors": "Armen Avetisyan; Manuel Dahnert; Angela Dai; Manolis Savva; Angel X Chang; Matthias Niessner"}, {"ref_id": "b4", "title": "Matterport3d: Learning from rgb-d data in indoor environments", "journal": "", "year": "2017", "authors": "Angel Chang; Angela Dai; Thomas Funkhouser; Maciej Halber; Matthias Niessner; Manolis Savva; Shuran Song; Andy Zeng; Yinda Zhang"}, {"ref_id": "b5", "title": "ShapeNet: An information-rich 3d model repository", "journal": "", "year": "2015", "authors": "Thomas Angel X Chang; Leonidas Funkhouser; Pat Guibas; Qixing Hanrahan; Zimo Huang; Silvio Li; Manolis Savarese; Shuran Savva; Hao Song;  Su"}, {"ref_id": "b6", "title": "Context-aware visual compatibility prediction", "journal": "", "year": "2019", "authors": "Guillem Cucurull; Perouz Taslakian; David Vazquez"}, {"ref_id": "b7", "title": "Scannet: Richly-annotated 3d reconstructions of indoor scenes", "journal": "", "year": "2017", "authors": "Angela Dai; X Angel; Manolis Chang; Maciej Savva; Thomas Halber; Matthias Funkhouser;  Nie\u00dfner"}, {"ref_id": "b8", "title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2018", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b9", "title": "Example-based synthesis of 3d object arrangements", "journal": "ACM Transactions on Graphics (TOG)", "year": "2012", "authors": "Matthew Fisher; Daniel Ritchie; Manolis Savva; Thomas Funkhouser; Pat Hanrahan"}, {"ref_id": "b10", "title": "Case studies in the use of roc curve analysis for sensor-based estimates in human computer interaction", "journal": "", "year": "2005", "authors": "James Fogarty; Ryan S Baker; Scott E Hudson"}, {"ref_id": "b11", "title": "Greedy function approximation: a gradient boosting machine", "journal": "Annals of statistics", "year": "2001", "authors": " Jerome H Friedman"}, {"ref_id": "b12", "title": "3d-future: 3d furniture shape with texture", "journal": "", "year": "2020", "authors": "Huan Fu; Rongfei Jia; Lin Gao; Mingming Gong; Binqiang Zhao; Steve Maybank; Dacheng Tao"}, {"ref_id": "b13", "title": "Tm-net: Deep generative networks for textured meshes", "journal": "", "year": "2020", "authors": "Lin Gao; Tong Wu; Yu-Jie Yuan; Ming-Xian Lin; Yu-Kun Lai; Hao Zhang"}, {"ref_id": "b14", "title": "The robotrix: An extremely photorealistic and very-large-scale indoor dataset of sequences with robot trajectories and interactions", "journal": "IEEE", "year": "2018", "authors": "Alberto Garcia-Garcia; Pablo Martinez-Gonzalez; Sergiu Oprea; John ; Alejandro Castro-Vargas; Sergio Orts-Escolano; Jose Garcia-Rodriguez; Alvaro Jover-Alvarez"}, {"ref_id": "b15", "title": "Understanding real world indoor scenes with synthetic data", "journal": "", "year": "2016", "authors": "Ankur Handa; Vijay Viorica Patraucean; Simon Badrinarayanan; Roberto Stent;  Cipolla"}, {"ref_id": "b16", "title": "Scenenn: A scene meshes dataset with annotations", "journal": "IEEE", "year": "2016", "authors": "Binh-Son Hua; Quang-Hieu Pham; Duc ; Thanh Nguyen; Minh-Khoi Tran; Lap-Fai Yu; Sai-Kit Yeung"}, {"ref_id": "b17", "title": "Semi-supervised classification with graph convolutional networks", "journal": "", "year": "", "authors": "N Thomas; Max Kipf;  Welling"}, {"ref_id": "b18", "title": "Semantic labeling of 3d point clouds for indoor scenes", "journal": "", "year": "2011", "authors": "Abhishek Hema S Koppula; Thorsten Anand; Ashutosh Joachims;  Saxena"}, {"ref_id": "b19", "title": "Grains: Generative recursive autoencoders for indoor scenes", "journal": "ACM Transactions on Graphics (TOG)", "year": "2019", "authors": "Manyi Li; Akshay Gadi Patil; Kai Xu; Siddhartha Chaudhuri; Owais Khan; Ariel Shamir; Changhe Tu; Baoquan Chen; Daniel Cohen-Or; Hao Zhang"}, {"ref_id": "b20", "title": "Interiornet: Mega-scale multisensor photo-realistic indoor scenes dataset", "journal": "", "year": "2018", "authors": "Wenbin Li; Sajad Saeedi; John Mccormac; Ronald Clark; Dimos Tzoumanikas; Qing Ye; Yuzhong Huang; Rui Tang; Stefan Leutenegger"}, {"ref_id": "b21", "title": "Openrooms: An end-toend open framework for photorealistic indoor scene datasets", "journal": "", "year": "2020", "authors": "Zhengqin Li; Ting-Wei Yu; Shen Sang; Sarah Wang; Sai Bi; Zexiang Xu; Hong-Xing Yu; Kalyan Sunkavalli; Milo\u0161 Ha\u0161an; Ravi Ramamoorthi"}, {"ref_id": "b22", "title": "GeLaTO: Generative Latent Textured Objects", "journal": "", "year": "2020", "authors": "Ricardo Martin-Brualla; Rohit Pandey; Sofien Bouaziz; Matthew Brown; Dan B Goldman"}, {"ref_id": "b23", "title": "Scenenet rgb-d: Can 5m synthetic images beat generic imagenet pre-training on indoor segmentation", "journal": "", "year": "2017", "authors": "John Mccormac; Ankur Handa; Stefan Leutenegger; Andrew J Davison"}, {"ref_id": "b24", "title": "Indoor segmentation and support inference from rgbd images", "journal": "", "year": "2012", "authors": "Pushmeet Kohli; Nathan Silberman; Derek Hoiem; Rob Fergus"}, {"ref_id": "b25", "title": "Learning to generate textures on 3d meshes", "journal": "", "year": "2019", "authors": "Amit Raj; Cusuh Ham; Connelly Barnes; Vladimir Kim; Jingwan Lu; James Hays"}, {"ref_id": "b26", "title": "Fast and flexible indoor scene synthesis via deep convolutional generative models", "journal": "", "year": "2019", "authors": "Daniel Ritchie; Kai Wang; Yu-An Lin"}, {"ref_id": "b27", "title": "Hypersim: A photorealistic synthetic dataset for holistic indoor scene understanding", "journal": "", "year": "2020", "authors": "Mike Roberts; Nathan Paczan"}, {"ref_id": "b28", "title": "Mobilenetv2: Inverted residuals and linear bottlenecks", "journal": "", "year": "2018", "authors": "Mark Sandler; Andrew Howard; Menglong Zhu; Andrey Zhmoginov; Liang-Chieh Chen"}, {"ref_id": "b29", "title": "PiGraphs: Learning Interaction Snapshots from Observations", "journal": "ACM Transactions on Graphics (TOG)", "year": "2016", "authors": "Manolis Savva; Angel X Chang; Pat Hanrahan; Matthew Fisher; Matthias Nie\u00dfner"}, {"ref_id": "b30", "title": "Sun rgb-d: A rgb-d scene understanding benchmark suite", "journal": "", "year": "2015", "authors": "Shuran Song; Jianxiong Samuel P Lichtenberg;  Xiao"}, {"ref_id": "b31", "title": "Semantic scene completion from a single depth image", "journal": "", "year": "2017", "authors": "Shuran Song; Fisher Yu; Andy Zeng; Angel X Chang; Manolis Savva; Thomas Funkhouser"}, {"ref_id": "b32", "title": "The replica dataset: A digital replica of indoor spaces", "journal": "", "year": "2019", "authors": "Julian Straub; Thomas Whelan; Lingni Ma; Yufan Chen; Erik Wijmans; Simon Green; Jakob J Engel; Raul Mur-Artal; Carl Ren; Shobhit Verma"}, {"ref_id": "b33", "title": "A benchmark for the evaluation of rgb-d slam systems", "journal": "IEEE", "year": "2012", "authors": "J\u00fcrgen Sturm; Nikolas Engelhard; Felix Endres; Wolfram Burgard; Daniel Cremers"}, {"ref_id": "b34", "title": "Pix3d: Dataset and methods for single-image 3d shape modeling", "journal": "", "year": "2018", "authors": "Xingyuan Sun; Jiajun Wu; Xiuming Zhang; Zhoutong Zhang; Chengkai Zhang; Tianfan Xue; Joshua B Tenenbaum; William T Freeman"}, {"ref_id": "b35", "title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b36", "title": "Planit: Planning and instantiating indoor scenes with relation graph and spatial prior networks", "journal": "ACM Transactions on Graphics (TOG)", "year": "2019", "authors": "Kai Wang; Yu-An Lin; Ben Weissmann; Manolis Savva; Angel X Chang; Daniel Ritchie"}, {"ref_id": "b37", "title": "Deep convolutional priors for indoor scene synthesis", "journal": "ACM Transactions on Graphics (TOG)", "year": "2018", "authors": "Kai Wang; Manolis Savva; Angel X Chang; Daniel Ritchie"}, {"ref_id": "b38", "title": "Fast and scalable position-based layout synthesis", "journal": "IEEE Transactions on Visualization and Computer Graphics", "year": "2018", "authors": "Tomer Weiss; Alan Litteneker; Noah Duncan; Masaki Nakada; Chenfanfu Jiang; Lap-Fai Yu; Demetri Terzopoulos"}, {"ref_id": "b39", "title": "Gibson env: Real-world perception for embodied agents", "journal": "", "year": "2018", "authors": "Fei Xia; Zhiyang Amir R Zamir; Alexander He; Jitendra Sax; Silvio Malik;  Savarese"}, {"ref_id": "b40", "title": "Sun3d: A database of big spaces reconstructed using sfm and object labels", "journal": "", "year": "2013", "authors": "Jianxiong Xiao; Andrew Owens; Antonio Torralba"}, {"ref_id": "b41", "title": "The unreasonable effectiveness of deep features as a perceptual metric", "journal": "", "year": "2018", "authors": "Richard Zhang; Phillip Isola; Alexei A Efros; Eli Shechtman; Oliver Wang"}, {"ref_id": "b42", "title": "Physically-based rendering for indoor scene understanding using convolutional neural networks", "journal": "", "year": "2017", "authors": "Yinda Zhang; Shuran Song; Ersin Yumer; Manolis Savva; Joon-Young Lee; Hailin Jin; Thomas Funkhouser"}, {"ref_id": "b43", "title": "Deep generative modeling for scene synthesis via hybrid representations", "journal": "ACM Transactions on Graphics (TOG)", "year": "2020", "authors": "Zaiwei Zhang; Zhenpei Yang; Chongyang Ma; Linjie Luo; Alexander Huth; Etienne Vouga; Qixing Huang"}, {"ref_id": "b44", "title": "Structured3d: A large photorealistic dataset for structured 3d modeling", "journal": "", "year": "2019", "authors": "Jia Zheng; Junfei Zhang; Jing Li; Rui Tang; Shenghua Gao; Zihan Zhou"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure2: Pipeline of building 3D-FRONT. We start from an empty house with professional design ideas, create the room suites, optimize the layouts (e.g., to resolve artifacts highlighed in the red boxes), and finally verify the furnished rooms.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: Viewpoint Generation. Each scene is associated with several natural camera views to facilitate rendering.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: House Examples in 3D-FRONT. The left column shows the top-down views of three houses. The middle column presents several rooms contained in these houses, including bedrooms, living rooms, dining rooms, etc. The interior design ideas at the right column summarize the textured objects involved in the rooms and their high-quality 3D CAD models. designs in 3D-FRONT are really with high-quality.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 5 :5Figure 5: Statistics of room numbers per house. 3D-FRONT contains 6,813 distinct houses constructed by 44,427 rooms. There are 6.5 rooms per house on average. riched texture and 3D geometry contents.", "figure_data": ""}, {"figure_label": "64", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 6 :Table 4 :64Figure 6: Distribution of the room scenes available in 3D-FRONT, organized by type. There are 44,427 rooms in total. A large percentage of rooms (indicated by dark color) in the top part are diversely furnished (18,968). These rooms, such as bedrooms, living rooms, dinging rooms, and study rooms, are the activity spaces where people tend to spend most of their times living indoors. MMD-MMD-COV-COV-CD \u2193 EMD \u2193 CD \u2191 EMD \u2191 SUNCG [32] 0.3642 1.1490 45.65 46.72 3D-FRONT 0.3371 1.1049 50.01 52.91Table 4: Evaluting diversity of scenes synthesized by models trained on 3D-FRONT vs. SUNCG.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 7 :7Figure 7: Interior Scene Synthesis. Several scenes produced by a state-of-the-art network trained on SUNCG (a) and 3D-FRONT (b), respectively. The results were synthesized from randomly chosen empty rooms. In each set, the first row is for bedrooms and the second row for living rooms. The 3D-FRONT results tend to show a richer variety of objects and more plausible scene layouts.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 8 :8Figure 8: Texturing 3D models in indoor scenes. The default textures (b) were provided by the 3D-FRONT dataset. In (c) and (d), we show chair textures generated by TM-Net, conditioned on given textures for the table. The network was trained on ShapeNet (c) or 3D-FRONT (d).", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 10 :10Figure 10: Distribution of number of objects per room.", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 11 :11Figure 11: Distribution of annotated instances for 3D-FRONT's scenes corresponded to 3D-FUTURE's model categories.", "figure_data": ""}, {"figure_label": "12", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 12 :12Figure 12: Distribution of object categories conditioned on different room types.", "figure_data": ""}, {"figure_label": "13", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 13 :13Figure 13: Distribution of physical sizes (in meters 2 ) per room (Left) and house (Right) of the 3D-FRONT dataset.", "figure_data": ""}, {"figure_label": "15", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure 15 :15Figure 15: House Examples -Part 1. Zoom in for better view.", "figure_data": ""}, {"figure_label": "16", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "Figure 16 :16Figure 16: House Examples -Part 2. Zoom in for better view.", "figure_data": ""}, {"figure_label": "17", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "Figure 17 :17Figure 17: House Examples -Part 3. Zoom in for better view.", "figure_data": ""}, {"figure_label": "18", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "Figure 18 :18Figure 18: Room Examples -Part 1. Zoom in for better view.", "figure_data": ""}, {"figure_label": "19", "figure_type": "figure", "figure_id": "fig_15", "figure_caption": "Figure 19 :19Figure 19: Room Examples -Part 2. Zoom in for better view.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "", "figure_caption": "", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "1-N Avg Rank means that we recurrently perform (A) \u2192 Nightstand and (A, B) \u2192 Chair, respectively, and compute the average rank Evaluating the Pipeline. \u2191: higher is better. \u2193: lower is better. We perform recommendation based on a extremely large 3D pool (about 1M models). When calculating these scores, invalid items in the retrieval sequences have been filtered out based on fine-grained category labels.", "figure_data": "MetricsFSC [13]FSC + GAE [7]AUC0.7660.772\u21911-N Hit@1033.6%36.1%1-N Hit@2061.3%64.3%\u21931-N Avg Rank N-1 Avg Rank41.6 26.737.3 24.1Questions3D-FRONTPlausible Layout62.5%SceneDesign Quality Richer Texture69.2% 70.0%Style Compatibility65.4%3D ModelRicher Texture Preferable65.4% 61.5%"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "User", "figure_data": ""}], "formulas": [], "doi": ""}
