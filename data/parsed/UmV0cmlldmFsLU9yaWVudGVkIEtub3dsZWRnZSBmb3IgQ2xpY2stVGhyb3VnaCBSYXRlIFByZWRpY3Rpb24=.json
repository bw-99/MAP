{"Retrieval-Oriented Knowledge for Click-Through Rate Prediction": "Huanshuo Liu \u2217 National University of Singapore Singapore, Singapore liuhsh35@mail2.sysu.edu.cn Bo Chen Huawei Noah's Ark Lab Shenzhen, China chenbo116@huawei.com Menghui Zhu Huawei Noah's Ark Lab Shenzhen, China zhumenghui1@huawei.com Jianghao Lin Shanghai Jiao Tong University Shanghai, China chiangel@sjtu.edu.cn Jiarui Qin Huawei Noah's Ark Lab Shenzhen, China qinjr@apex.sjtu.edu.cn Yang Yang Huawei Noah's Ark Lab Shanghai, China yangyang590@huawei.com Hao Zhang Huawei Noah's Ark Lab Singapore, Singapore hzhang26@outlook.com Ruiming Tang \u2020 Huawei Noah's Ark Lab Shenzhen, China tangruiming@huawei.com", "Abstract": "Click-through rate (CTR) prediction is crucial for personalized online services. Sample-level retrieval-based models, such as RIM, have demonstrated remarkable performance. However, they face challenges including inference inefficiency and high resource consumption due to the retrieval process, which hinder their practical application in industrial settings. To address this, we propose a universal plug-and-play retrieval-oriented knowledge ( ROK ) framework that bypasses the real retrieval process. The framework features a knowledge base that preserves and imitates the retrieved & aggregated representations using a decomposition-reconstruction paradigm. Knowledge distillation and contrastive learning optimize the knowledge base, enabling the integration of retrievalenhanced representations with various CTR models. Experiments on three large-scale datasets demonstrate ROK's exceptional compatibility and performance, with the neural knowledge base serving as an effective surrogate for the retrieval pool. ROK surpasses the teacher model while maintaining superior inference efficiency and demonstrates the feasibility of distilling knowledge from nonparametric methods using a parametric approach. These results highlight ROK's strong potential for real-world applications and its ability to transform retrieval-based methods into practical solutions. Our implementation code is available to support reproducibility 1 .", "CCS Concepts": "", "\u00b7 Information systems \u2192 Recommender systems ; Nearestneighbor search ; Users and interactive retrieval .": "This work is done during the internship at Huawei Noah's Ark Lab. https://github.com/HSLiu-Initial/ROK.git \u2217 \u2020 Ruiming Tang is the corresponding author. 1 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CIKM '24, October 21-25, 2024, Boise, ID, USA. \u00a9 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0436-9/24/10...$15.00 https://doi.org/10.1145/3627673.3679842", "Keywords": "Information Retrieval; Recommender Systems; Knowledge Distillation; Contrastive Learning", "ACMReference Format:": "Huanshuo Liu, Bo Chen, Menghui Zhu, Jianghao Lin, Jiarui Qin, Hao Zhang, Yang Yang, and Ruiming Tang. 2024. Retrieval-Oriented Knowledge for Click-Through Rate Prediction. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management (CIKM '24), October 21-25, 2024, Boise, ID, USA. ACM, New York, NY, USA, 11 pages. https: //doi.org/10.1145/3627673.3679842", "1 Introduction": "Click-through rate (CTR) prediction is a key component of many personalized online services, such as recommender systems [39, 41] and web search [9, 11, 22, 24]. It aims to estimate the probability of a user's click given a particular context [45]. The CTR models can be mainly classified into two categories. The first category is the feature interaction-based methods. The core idea of these methods is to capture the high-order feature interactions across multiple fields with different operators ( e.g., product [4, 13, 29, 38], convolution [23, 43], and attention [33, 42]). The second category is the user behavior modeling methods that leverage different architectures ( e.g., RNN [14, 15], CNN [35], attention [47, 48], memory bank [25, 30]) to extract informative knowledge from user behavior sequences for final CTR prediction. Due to the problem of distribution shift and the daily generation of vast amounts of interaction data in industrial applications, using all the data for training becomes impractical. This results in a significant waste of historical data, as traditional methods cannot effectively utilize it. To address this issue, retrieval-based methods have been proposed. UBR4CTR [28] and SIM [26] retrieve useful behaviors from the user's behavior history ( i.e., clicked items), reducing the potential noise in user behavior sequences. Subsequent studies [20] improve these methods' efficiency using hashing functions [6] and parallel retrieval execution [2] during inference. Recent studies [10, 27] further expand these retrieval-based methods from item-level retrieval to sample-level retrieval, making them applicable to general CTR prediction settings. Instead of retrieving similar items from the user history , RIM [27] adopts the idea of \ud835\udc58 nearest neighbor ( \ud835\udc58 NN) and designs a sample-centric retrieval method, which aggregates the relevant data samples retrieved from the search pool ( e.g., the whole training dataset). Although sample-level retrieval-based methods brings impressive performance enhancement, they have to perform instancewise comparisons between the target data sample and candidate samples in the search pool (usually at the million or even billion level), leading to extreme inefficiency problems during inference , as illustrated in the left part of Figure 1. While follow-up studies [21, 46] have attempted to enhance retrieval mechanisms through vector retrieval techniques, they have instead created further challenges and failed to meet the necessary inference speed for deployment. Moreover, sample-level retrieval-based methods faces several other significant challenges. These include high resource consumption due to the linear increase in retrievers with request volume, as well as a divergent inference process that requires an inference-retrieval-inference cycle. This necessitates significant optimizations, which makes it challenging for industrial applications. This work introduces the Retrieval-Oriented Knowledge (ROK) framework to address the problems of sample-level retrieval-based methods by bypassing the real retrieval process. The framework operates in two stages: Retrieval-Oriented Knowledge Construction and Knowledge Utilization. In the first stage, we pre-train a samplelevel retrieval-based method, such as RIM [27]. To bypass the real retrieval process, ROK learns a neural network-based Knowledge Base to store the retrieval-oriented knowledge from this pre-trained method. Specifically, ROK imitates the aggregated representations from the pre-trained retrieval-based method via a Retrieval Imitation module, as illustrated in Figure 1. The Knowledge Base, built on a novel decomposition-reconstruction paradigm , where a Retrieval-Oriented Embedding Layer captures the feature-wise embedding and a Knowledge Encoder reconstructs the instance-wise aggregated representations ( i.e., retrieval-enhanced representation). In this way, instead of the time-consuming retrieval, efficient inference of neural networks can be adopted to get the approximated aggregated representations. Additionally, we introduce a Contrastive Regularization module to ensure learning stability and prevent model collapse. At the Knowledge Utilization stage, ROK designs two approaches (i.e., instance-wise and feature-wise) to integrate the retrieval-enhanced representations with various CTR models, thus providing high inference efficiency. The main contributions of this paper are as follows: \u00b7 ROK is a novel framework that leverages knowledge distillation to efficiently utilize large amounts of historical data and retrievaloriented knowledge for training and lightweight deployment. To the best of our knowledge, ROK is the first such framework. It transforms sample-level retrieval-based methods into a practical solution by using a neural network-based Knowledge Base to store retrieval-enhanced representations, eliminating the need for time-intensive retrieval during inference. This demonstrates the feasibility of distilling knowledge from non-parametric models and learning their inherent knowledge using a parametric approach. \u00b7 ROK optimizes the Knowledge Base using a combination of knowledge distillation and contrastive learning. This approach allows for the seamless integration of retrieval-enhanced representations with various CTR models at both the instance and feature levels, enhancing their performance and compatibility. \u00b7 Extensive experiments across three large-scale datasets demonstrate ROK's exceptional compatibility and performance. The results show that ROK significantly enhances the performance of various CTR methods, indicating that the neural Knowledge Base serves as an effective and compact surrogate for the search pool. Although the knowledge distillation foundation initially suggests that ROK may only approach the teacher model's performance, the introduction of contrastive regularization enables ROK to surpass the sample-level retrieval-based teacher model. This highlights ROK's strong potential for real-world applications. Target Sample Backbone Models Search Pool (Historical Samples) Retrieved Relevent Samples Traditional Framework Target Sample Backbone Models Search Pool (Historical Samples) Text Learn Retrieval-Oriented Knowledge ROK Knowledge Base Features Forward Retrieve Learn Representation", "2 Related Work": "", "2.1 CTR Prediction": "For click-through rate (CTR) prediction, models can essentially be divided into two main categories: those focusing on feature interaction and those centering on user behavior modeling. The first category is the feature interaction-based methods, evolving from foundational works such as POLY2 [3] and Factorization Machines (FM) [31]. With the integration of Deep Neural Networks (DNNs), a variety of sophisticated deep feature interaction models have been proposed. These models aim to capture high-order feature interactions across different fields by employing various operations, such as the product operation [4, 13, 19, 29, 38], convolution [23, 43], and attention mechanisms [33, 42]. The key innovation of these models lies in their ability to identify and utilize complex interactions between a multitude of features to enhance the predictive performance of CTR models. Furthermore, user behavior modeling is another core technique for CTR prediction that mines user preferences from historical interaction behaviors meticulously. To better extract informative knowledge from a user's behavior sequence, various network structures have been utilized, including Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs), Attention Networks, and Memory Networks. GRU4Rec [15] designs Gated Recurrent Units (GRUs) to capture the preference-evolving relationship, while Caser [35] leverages the horizontal and vertical convolution to model skip behaviors at both the union-level and point-level. Moreover, the attention mechanism is the most popular method for Retrieval-Oriented Embedding Layer Query Sample Retrieved Sample Knowledge Encoder Knowledge Encoder Projector Negative Cosine Similarity Stop Gradient Retrieval-Oriented Embedding Layer Query Sample Knowledge Encoder Multi-value Features Retrieval-Oriented Embedding Learnable Embedding Backbone Model Sample-level Retrieval-based Model (a) Retrieval-Oriented Knowledge Construction Embedding Layer Mean Square Error Representations Features Forward Concatenation Multi-value Aggregation Instance-wise Feature-wise Aggregation (b) Knowledge Utilization Parameter Sharing Query Sample Search Engine Interaction & Prediction Layer Sample/Fetch Search Pool Fetch ROK Sample pX zt X modeling item dependencies, and several influential works have been proposed, including SASRec [18], DIN [48], DIEN [47], and BERT4Rec [34]. Among these, DIN and DIEN leverage the targetattention network to identify important historical items, while SASRec and BERT4Rec apply the self-attention network with Transformer architecture to excavate behavior dependencies. Additionally, memory-based methods [16] are also proposed to store user behavior representations in a read-write manner.", "2.2 Retrieval-Augmented Recommendation": "To further enhance the performance of CTR prediction, retrievalaugmented recommendation is proposed, where the most relevant information is retrieved from historical data. Specifically, UBR4CTR [28] and SIM [30] are designed to retrieve beneficial behaviors from the user's extremely long historical behavior sequence. UBR4CTR deploys the search engine method, while SIM uses hard search and soft search approaches. To make the search procedure end-toend, ETA [7] is proposed by leveraging the SimHash algorithm to map user behavior into a low-dimensional space, hence achieving learnable retrieval. Moreover, recent works further extend retrievalaugmented recommendation from item-level retrieval to samplelevel retrieval by retrieving informative samples. RIM [27] is the first to deploy this method, leveraging the search engine to retrieve several relevant samples from the search pool and perform neighbor aggregation. PET [10] and DERT [46] have respectively made improvements in the interaction and retrieval mechanisms of neighboring samples. PET constructs a hypergraph over the retrieved data samples and performs message propagation to improve the target data representations for final CTR prediction. DERT utilizes vector retrieval to speed up the retrieval process.", "3 Preliminary": "", "3.1 CTR Prediction": "In CTR prediction, each data sample is denoted as \ud835\udc60 \ud835\udc61 = ( \ud835\udc65 \ud835\udc61 , \ud835\udc66 \ud835\udc61 ) , where \ud835\udc65 \ud835\udc61 = { \ud835\udc50 \ud835\udc61 \ud835\udc56 } \ud835\udc39 \ud835\udc56 = 1 , \ud835\udc39 is the number of discrete features 2 , and \ud835\udc66 \ud835\udc61 is the label. Thus a dataset with \ud835\udc41 samples can be expressed as \ud835\udc47 = { \ud835\udc60 \ud835\udc61 } \ud835\udc41 \ud835\udc61 = 1 . The goal of the CTR prediction is to estimate the click probability of a specific sample: \u02c6 \ud835\udc66 \ud835\udc61 = \ud835\udc3a ( \ud835\udc65 \ud835\udc61 ; \ud835\udf03 ) , where \ud835\udc3a is the CTR model with learnable parameters \ud835\udf03 . Apart from the traditional training/validation/testing splits for CTR prediction, retrieval-based methods further require a search pool \ud835\udc47 \ud835\udc5d\ud835\udc5c\ud835\udc5c\ud835\udc59 , which might overlap with the training set \ud835\udc47 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b according to different settings. The search pool \ud835\udc47 \ud835\udc5d\ud835\udc5c\ud835\udc5c\ud835\udc59 is constructed to provide useful knowledge for downstream CTR prediction towards the target sample \ud835\udc65 \ud835\udc61 , which can be formulated as: where \ud835\udc45 ( \ud835\udc65 \ud835\udc61 ) is the retrieved knowledge of \ud835\udc65 \ud835\udc61 . For item-level retrieval [26, 28], the knowledge \ud835\udc45 ( \ud835\udc65 \ud835\udc61 ) is the retrieved \ud835\udc58 user behaviors. For sample-level retrieval [10, 27], the knowledge \ud835\udc45 ( \ud835\udc65 \ud835\udc61 ) is the retrieved \ud835\udc58 nearest data samples. As for our proposed ROK , the knowledge \ud835\udc45 ( \ud835\udc65 \ud835\udc61 ) is the learned retrieval-enhanced representation. After obtaining the click prediction \u02c6 \ud835\udc66 \ud835\udc61 , the parameters \ud835\udf03 are optimized by minimizing the binary cross-entropy (BCE) loss:", "3.2 Sample-level Retrieval-based Methods": "For illustration purposes, we abstract the sample-level retrieval approach in the left part of Figure 2. The target sample \ud835\udc65 \ud835\udc61 will be considered as a query to retrieve the top-K neighboring samples { \ud835\udc60 1 , \u00b7 \u00b7 \u00b7 , \ud835\udc60 \ud835\udc3e } from the search pool \ud835\udc47 \ud835\udc5d\ud835\udc5c\ud835\udc5c\ud835\udc59 through the search engine. After retrieval, an aggregation layer is used to aggregate the features and labels of the retrieved samples, resulting in a dense knowledge representation r \ud835\udc61 . For example, RIM [27] deploys an attentive aggregation layer, while PET [10] constructs a hypergraph over the retrieved samples, and performs message propagation to aggregate the representations. Finally, the aggregated representation r \ud835\udc61 is sent to the prediction module together with the target sample representation. The prediction module usually contains components for feature interaction modeling [4, 13, 33] or user behavior modeling [47, 48].", "4 Methodology": "", "4.1 Overview of ROK": "Despite their superior performance, sample-level retrieval-based methods suffer from challenges, including inference inefficiency and high resource consumption, due to the time-consuming online retrieval process. To this end, we propose a novel Retrieval-Oriented Knowledge (ROK) framework, where a knowledge base is built to imitate the aggregated retrieval knowledge r \ud835\udc61 . As shown in Figure 2, ROK consists of two stages: (1) Retrieval-Oriented Knowledge Construction, and (2) Knowledge Utilization. In the first stage of ROKretrieval-oriented knowledge construction -we pre-train a sample-level retrieval-based model 3 , followed by designing a delicate knowledge base with a retrieval-oriented embedding layer and a knowledge encoder. This is designed to imitate the retrieval & aggregation results ( i.e., r \ud835\udc61 ) of the pre-trained model by directly generating the final aggregated representation z \ud835\udc65 \ud835\udc61 through the knowledge encoder. In this way, when online serving, the time-consuming retrieval process could be replaced by a simpler and faster forward propagation of a neural network ( i.e., knowledge base). We adopt the mean square error loss for knowledge imitation and design a contrastive regularization loss to stabilize the learning process and prevent collapse [17, 32]. In the knowledge utilization stage, we propose to integrate the plug-and-play retrieval-enhanced representations into arbitrary backbone CTR models in both instance-wise and feature-wise manner. Hence, we retain the sample-level retrieval capacity, while avoiding the online overhead brought by retrieval & aggregation operations. Next, we will first elaborate on the network structure design of the knowledge base for knowledge imitation. Then, we give the detailed training strategies for the two stages ( i.e., retrieval-oriented knowledge construction and knowledge utilization).", "4.2 Structure Design of Knowledge Base": "For sample-level retrieval-based methods [10, 27], the overhead of inference inefficiency and high resource consumption is mainly caused by the retrieval & aggregation operations, which are uncacheable and unavoidable during the online inference if a brandnewdata sample comes. To this end, we manage to distill knowledge from non-parametric retrieval process using a parametric approach. To be specific, we propose a novel decomposition-reconstruction paradigm , where the aggregated representation r \ud835\udc61 is first decomposed into feature-level embeddings, and then reconstructed via a neural encoder. As illustrated in 3, our proposed knowledge base comprises two key modules: (1) retrieval-oriented embedding layer \ud835\udc53 , and (2) knowledge encoder \ud835\udc54 . Initially, we decompose the query \ud835\udc65 \ud835\udc61 into feature-wise embeddings via the retrieval-oriented embedding layer \ud835\udc53 . Then, the obtained feature-wise embeddings are fed into the knowledge encoder \ud835\udc54 to reconstruct the instance-wise aggregated representation z \ud835\udc65 \ud835\udc61 = \ud835\udc54 ( \ud835\udc53 ( \ud835\udc65 \ud835\udc61 )) . Specifically, a learnable embedding layer functions as the retrieval-oriented embedding layer. The architecture of knowledge encoder \ud835\udc54 can be chosen arbitrarily ( e.g., transformer [37]). For simplicity, we adopt multi-layer perceptron (MLP) in this paper. This approach reduces the time complexity of sample-level retrieval to O( 1 ) . Since we introduce the retrieval-oriented embedding layer \ud835\udc53 , we will cut down the embedding size of \ud835\udc53 from \ud835\udc51 to \ud835\udc51 / 2 for fair comparison in space complexity. With the help of the learned knowledge base, online retrieval & aggregation operations can be avoided. 56 Decomposition Retrieval-Oriented Embedding Layer Reconstruction Knowledge Encoder Instance-level aggregated sample Features Representations Forward What needs to be stored Learn Search Pool Feature-level embeddings", "4.3 Retrieval-Oriented Knowledge Construction": "To construct the knowledge base, we introduce two modules: Retrieval Imitation and Contrastive Regularization. 4.3.1 Retrieval Imitation After we obtain the reconstructed representation z \ud835\udc65 \ud835\udc61 from the knowledge base for query sample \ud835\udc65 \ud835\udc61 . We use mean square error (MSE) loss to train the knowledge base, enabling it to imitate the aggregated representation r \ud835\udc61 : This knowledge imitation approach acts as a knowledge distillation process, where we distill knowledge from a non-parametric retrieval process using a parametric approach. By extracting and injecting the retrieval-oriented knowledge from the pre-trained model into the learnable knowledge base, we enable implicit sample-level retrieval capability. 4.3.2 Contrastive Regularization Besides, we have intricately integrated a contrastive regularization loss to ensure stability in the learning process and avert potential collapse [17, 32]. Notably, sample-level retrieval-based methods [10, 27] ingeniously produces positive samples during the retrieval process. This can be perceived as a unique data augmentation technique, offering a localized positive perspective for contrastive learning. Consequently, ROK employs the SimSiam [8] framework, utilizing a free negative samples scheme for regularization. While the traditional sample-level retrieval-based methods predominantly harness global features from the aggregated neighboring samples, our use of contrastive regularization allows for a more detailed extraction of local features from these samples. As shown in Figure 2(a), the most correlated neighboring sample \ud835\udc60 \ud835\udc61 is selected from the \ud835\udc3e retrieved samples { \ud835\udc60 1 , \u00b7 \u00b7 \u00b7 , \ud835\udc60 \ud835\udc3e } by the sample-level retrieval-based methods. Then, the target sample \ud835\udc65 \ud835\udc61 and selected neighboring sample \ud835\udc60 \ud835\udc61 are fed into the knowledge base to obtain the reconstructed representations z \ud835\udc65 \ud835\udc61 and z \ud835\udc60 \ud835\udc61 , respectively. To prevent collapse in the absence of negative samples [44], a projector \u210e is employed to generate the projected representation p \ud835\udc65 \ud835\udc61 , p \ud835\udc60 \ud835\udc61 from z \ud835\udc65 \ud835\udc61 , z \ud835\udc60 \ud835\udc61 . The cosine similarity between projected representation p \ud835\udc65 \ud835\udc61 and reconstructed representations z \ud835\udc60 \ud835\udc61 is defined as: where \u2225\u00b7\u2225 2 is the \ud835\udc59 2 norm. Besides, the gradient of z \ud835\udc60 \ud835\udc61 is stopped when computing the loss [8, 44]. To avoid spatial bias, motivated by the Jensen-Shannon (JS) divergence, a symmetric loss is adopted and the final contrastive regularization loss is: Hence, The overall objective for ROK in the retrieval-oriented knowledge construction stage is: where \ud835\udefc adjusts the ratio of the two loss terms. After the retrievaloriented knowledge construction stage, the knowledge base will be retained for the follow-up knowledge utilization stage.", "4.4 Knowledge Utilization": "In the knowledge utilization stage, the plug-and-play retrievalenhanced representations from the learned knowledge base can be integrated into various backbone CTR models. When integrated with backbone models, all parameters of ROK are frozen-a strategy that outperforms others, as detailed in Section 5.5.1. As illustrated in Figure 2(b), we present two approaches for knowledge utilization, which are mutually compatible to boost the final CTR prediction: \u00b7 Feature-wise retrieval-enhanced methodleverages the retrievaloriented embedding layer \ud835\udc53 to obtain the retrieval-enhanced feature-wise embeddings for the target sample. Specifically, this method integrates the retrieval-oriented embedding layer \ud835\udc53 with the original learnable feature embedding of the backbone model to form the final embedding layer 4 . \u00b7 Instance-wise retrieval-enhanced method employs the knowledge base to generate a retrieval-enhanced instance-level aggregated representation z \ud835\udc65 \ud835\udc61 for each sample. Typically, various backbone models produce their unique representations-for example, RIM's aggregated representations and DIEN's interest states. These unique representations are then concatenated with the feature embeddings and the combined results are fed into the interaction and MLP (Multi-Layer Perceptron) layers. where \ud835\udf19 is utilized to extract the unique representations of the backbone model.", "5 Experiments": "In this section, we present the experimental settings and corresponding results in detail. The experiments are conducted on three large-scale datasets, including Tmall, Taobao, and Alipay. To gain more insights into ROK, we endeavor to address the following research questions (RQs) in this section. \u00b7 RQ1: How does ROK's performance compare to that of traditional CTR models and the teacher model? \u00b7 RQ2: How compatible is ROK with other backbone models? \u00b7 RQ3: How does the knowledge captured by ROK contribute to improving performance? \u00b7 RQ4: How do the chosen update strategy and positive sample selecting strategy affect ROK's performance? \u00b7 RQ5: How does the specified hyperparameter influence ROK's performance?", "5.1 Experimental Settings": "5.1.1 Datasets. The evaluations are conducted on three widelyrecognized public datasets: Tmall, Taobao, and Alipay. Comprehensive statistics for these datasets are presented in Table 1. To be specific, we tally the counts of users, items, samples, fields, and categories for each dataset. It's worth noting that the number of features refers to the count of unique feature values. Following RIM [27], we organize the data such that the oldest instances constitute the search pool, the most recent instances comprised the test set, and the instances in between are assigned to the training set. The retrieval-based methods [27, 28] retrieve neighboring samples from the search pool. For user behavior modeling methods [47, 48] (e.g., DIEN), the sequential features (e.g., user behavior) are also generated from the search pool. 5.1.2 Evaluation Metrics. To measure performance, we utilize the commonly adopted metrics AUC and log-loss, which reflect pairwise ranking performance and point-wise likelihood, respectively. A significance test contrasting the two top-performing methods for each metric is undertaken, with results of significance denoted by an asterisk ( \u2217 ). We follow previous work [48] to introduce Rel. Impr. metric to quantify the relative improvement of models, which is defined as below: where the base model is the backbone model of ROK in each dataset. 5.1.3 Compared Baselines. For baselines, we compare ROK with a mid-tier performance backbone model against traditional models and retrieval-based models. For traditional CTR models, GBDT[5] is a frequently used tree-based model, and DeepFM [13] is a popular deep-learning model that focuses on feature interactions. HPMN [30] and MIMN [25] utilize memory network architectures, while DIN [48] and DIEN [47] are attention-based CTR models designed for pinpointing user interests. Additionally, FATE [40] is a model for learning representations of tabular data which facilitates interactions among samples within a minibatch. In item-level retrieval-based models, SIM [26] and UBR [28] extract pertinent user behaviors from a comprehensive set of user-generated data. Regarding the sample-level retrieval-based model, RIM [27] retrieves relevant data instances based on raw input features. We do not include DERT [46] in our comparison because it is a special case of RIM that introduces a new dense retrieval setting. Moreover, our primary goal is to develop a method that eliminates the need for retrieval during inference, thereby improving efficiency and enabling real-world deployment. Therefore, we focus more on comparing the performance of ROK with the teacher model i.e., RIM [27] to determine whether ROK would cause a significant performance degradation and whether it could enhance the backbone model.", "5.2 Overall Performance Comparison: RQ1": "In this section, we compare the overall experimental results presented in Table 2 and make the following observations. First, ROK significantly enhances the performance of the backbone model, with AUC improvements of 10.08%, 34.96%, and 16.85% on the three datasets, respectively, demonstrating its superior performance. Second, retrieval-based methods [27, 28] considerably outperform user behavior modeling methods [47, 48], benefiting from their advanced knowledge retrieval capabilities. Furthermore, sample-level retrieval-based methods [27] exhibit better performance than itemlevel retrieval-based methods [26, 28] due to more comprehensive retrieval strategies. Finally, ROK notably surpasses the performance of item-level retrieval-based methods methods, such as SIM [26] and UBR [28], and even outperforms our teacher model, RIM [27], which belongs to the sample-level retrieval-based methods. Our primary goal was to develop a method that eliminates the need for retrieval during inference, thereby improving efficiency and enabling real-world deployment. Initially, we expected that our approach would merely approximate the performance of the teacher model while preserving extraordinary inference efficiency, given that it is based on knowledge distillation. However, through the introduction of contrastive regularization, our method not only matches but exceeds the performance of the teacher model. This demonstrates the strong potential of our approach for real-world applications, as it can effectively learn and utilize the knowledge in the search pool without the need to perform real retrieval during inference. Our results highlight the effectiveness of knowledge distillation combined with contrastive regularization in capturing and leveraging the information from the search pool, enabling efficient and high-performing models suitable for deployment in practical scenarios.", "5.3 Compatibility Analysis: RQ2": "In this section, we begin by contrasting the compatibility of our novel approach, ROK, with conventional CTR models, as delineated in Table 3. Upon examining the data, two observations arise: (i) The application of ROK markedly amplifies the performance of the backbone models. To illustrate, by integrating ROK with DeepFM [13], DIN [48], and DIEN [47], there is a notable enhancement in performance metrics, with an average AUC increase of 4.30%, 8.01%, and 5.59%, coupled with an average decrease in log-loss by 11.80%, 8.51%, and 8.22% across three datasets, respectively. This highlights the significant benefits of incorporating the retrieval-oriented knowledge from ROK into the backbone models and indicates that the neural knowledge model successfully serves as a compact surrogate for the search pool. (ii) ROK, exhibiting model-agnosticism, seamlessly integrates with a diverse array of backbone CTR models, irrespective of whether they focus on feature interaction or behavior modeling. This underscores that the knowledge garnered by ROK is beneficial across diverse models, highlighting the compatibility of ROK. The feature of model-agnosticism is crucial since, regardless of the optimal model for a given scenario, ROK consistently boosts its capabilities. Thus, instead of being bound to one model, we can choose the ideal one for each situation and leverage ROK to optimize its performance.", "5.4 Comparison of the learned knowledge of ROK and RIM: RQ3": "In this section, we delve into the comparative analysis of the knowledge quality acquired by ROK and RIM. For a detailed assessment, we formulate two simple variants (LR and MLP) for each model to fully demonstrate the effect of knowledge: \u00b7 For ROK, we design the ROK(LR) and ROK(MLP) variants. Both take as input the concatenation of the original feature embeddings x \ud835\udc61 , with ROK's knowledge component z \ud835\udc65 \ud835\udc61 . \u00b7 Similarly, for RIM [27], we introduced the RIM(LR) and RIM(MLP) variants. Their input comprises the concatenation of the original feature embeddings, x \ud835\udc61 , with RIM's aggregated features and labels, r \ud835\udc61 . The detailed performance comparison of these variants is tabulated in Table 4 and we make two observations. First, significantly, the results indicate that the ROK variants surpass the RIM variants in performance across both datasets. This distinction in outcomes emphasizes the superior quality of knowledge learned by ROK, especially as these models apply this intrinsic knowledge directly for predictions. It is compelling to note that while ROK leverages the retrieval imitation strategy (detailed in Section 4.3.1) to imitate the aggregated features and labels of RIM, it extracts richer and more profound knowledge from the data. We believe that the contrastive regularization methodology is pivotal in driving this enhanced performance for ROK. Second, ROK(MLP) in Tmall even outperforms DeepFM [13] and some user behavior modeling methods , which shows another way to improve the pre-ranking model effect. To delve deeper into the inherent knowledge of ROK and explain its superiority over RIM, we employed t-SNE visualization [36]. Figure 4 illustrates the knowledge distribution patterns of both ROK and RIM in the Alipay dataset. For this visualization, we randomly selected 10 , 000 samples from both the training and testing sets for each model. We have two observations. First, ROK's knowledge distribution exhibits a more pronounced clustering effect compared to RIM. This enhanced clustering can be attributed to the contrastive regularization employed by ROK. The contrastive regularization emphasizes the local features of neighboring samples and mitigates the noise by selectively considering the most relevant sample, as shown in section 4.3.2. Specifically, representations of analogous samples are pulled closer, while those of dissimilar samples are pushed apart, leading to a more distinct and meaningful clustering. In contrast, RIM's retrieved neighboring samples may include some incidental noise that correlates with the quantity of these samples, and its aggregation mechanism only captures the global features of the neighboring samples. Second, the visualization of ROK's representations reveals a high degree of similarity between the training and test datasets, demonstrating the good generalization ability of the decomposition-reconstruction paradigm. This is significant because a major concern when using neural networks as a knowledge base is their generalization ability, as the training process is guided by the retrieved examples of sample-level retrieval-based methods, while the testing phase requires the model to apply its knowledge independently. 100 75 50 25 0 25 50 75 100 t-SNE Dimension 1 75 50 25 0 25 50 75 t-SNE Dimension 2 t-SNE visualization of RIM representations 80 60 40 20 0 20 40 60 80 t-SNE Dimension 1 75 50 25 0 25 50 75 t-SNE Dimension 2 t-SNE visualization of ROK representations (a) Alipay (Train) 75 50 25 0 25 50 75 t-SNE Dimension 1 100 75 50 25 0 25 50 75 100 t-SNE Dimension 2 t-SNE visualization of RIM representations 80 60 40 20 0 20 40 60 80 t-SNE Dimension 1 75 50 25 0 25 50 75 t-SNE Dimension 2 t-SNE visualization of ROK representations (b) Alipay (Test)", "5.5 Ablation & Hyper-parameter Study": "5.5.1 Update Strategy: RQ4. In this section, we undertake comprehensive ablation experiments for ROK. We begin by examining the parameter update strategy employed for the knowledge base during the knowledge utilization phase. The strategies under consideration include: fixing the knowledge base ( Fix ), updating only the upper knowledge encoder \ud835\udc54 ( Upd \ud835\udc54 ), and updating both the knowledge encoder and the retrieval-oriented embedding layer \ud835\udc53 ( Upd \ud835\udc53 + \ud835\udc54 ). Furthermore, we consider the positive sample selection strategy described in Section 4.3.2. Specifically, we select the most related neighboring sample by default and compare its performance to randomly selecting a sample from the retrieved samples ( random ). Figure 5 depicts the results obtained on the Tmall, Taobao and Alipay datasets. We have two observations: First, randomly selecting a sample as the positive sample diminishes performance, suggesting that even within the small retrieved set of samples, noise is still present. Second, a key observation is that the strategy of fixing the knowledge base ( Fix ) yields the best performance. This can be attributed to the primary goal of ROK: to derive superior representations. However, updating with the backbone models may adversely impact the acquired knowledge. This characteristic is highly desirable and aligns well with our expectations, further enhancing the practicality of ROK. It indicates that once ROK has been fully trained, it can function as an independent module, eliminating the need for subsequent modifications and reinforcing its role as an authentic knowledge base. 5.5.2 Hyperparameter Study: RQ5. In this section, we delve into the study of hyperparameter \ud835\udefc pertaining to the balance between the contrastive regularization loss and retrieval imitation loss, represented as \ud835\udefc . As illustrated in Figure 6, for both datasets, when \ud835\udefc equals either 0 or 1-representing the extreme scenarios-the performance of ROK deteriorates. This observation underscores the significance of both retrieval imitation loss and contrastive regularization loss to the model's efficacy.", "6 Industry Application": "", "6.1 Training Efficiency": "Training ROK involves pre-training a sample-level retrieval-based model, creating retrieval-oriented knowledge, and applying this knowledge to backbone CTR models. Despite these steps, the extra training time is reasonable compared to training a standalone model. The training duration is detailed in Table 5. Two observations stand out. Firstly, incorporating ROK into the backbone model significantly reduces training time, thanks to fewer parameters needing updates, partly because ROK's parameters are frozen and the embedding size is halved. This leads to faster convergence. Second, the total time of Phase 1 and Phase 2 is close to the training duration of the DIEN without ROK. The relatively time-consuming part of the whole training process would be the RIM's retrieval mechanism. However, this is mitigated by using large bulk sizes and caching, which decreases retrieval times. For further efficiency, extending the knowledge base update frequency beyond that of the backbone model could be beneficial. Table 5: Training duration comparison for two phases, total time and backbone model on Tmall, Taobao, and Alipay, measured in minutes. Phase 1 includes data retrieval for training and testing tests, pre-training RIM, and Retrieval-Oriented Knowledge Construction. Phase 2 includes Knowledge Utilization, with DIEN as the backbone model. Additionally, DIEN training times without ROK integration are provided. For Phase 1, data retrieval times are specified separately. \u0000\u0000\u0000 \u0000\u0000\u0000\u0000 g \u0000\u0000\u0000\u0000 f + g \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000\u0000 g \u0000\u0000\u0000\u0000 f + g \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 (b) Taobao \u0000\u0000\u0000 \u0000\u0000\u0000\u0000 g \u0000\u0000\u0000\u0000 f + g \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 (c) Alipay \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 (a) Taobao \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 (b) Alipay", "6.2 Inference Efficiency": "Deploying sample-level retrieval-based methods is complex and costly due to several key factors like extreme inefficiency problems during inference, high resource consumption and divergent inference process that requires an inference-retrieval-inference cycle. Our method, ROK, introduces a knowledge base that improves retrieval efficiency, reducing time complexity to O( 1 ) . We assessed backbone models-DeepFM, DIEN, and DIN-integrated with ROK and compared them to UBR [28], sparse sample-level retrievalbased methods, and dense sample-level retrieval-based methods. As shown in Table 6, retrieval-based methods have significantly longer inference times, making them impractical for real-time requirements. In contrast, ROK improves backbone models with minimal additional inference time, making it suitable for online deployment. Table 3 and Table 6 demonstrate that ROK consistently improves the AUC of all backbone models across the tested datasets while only slightly increasing the inference time by 0.12 ms to 0.25 ms per sample, showcasing its ability to enhance performance without significantly impacting speed. ROK's retrieval-free design ensures compatibility with existing systems, requiring only one inference process, thus avoiding data transfer overhead and enhancing efficiency. This combination of accuracy and rapid inference makes ROK a superior solution for modern online services.", "7 Conclusion": "The ROK framework tackles the critical challenges faced by samplelevel retrieval-based methods, such as inference inefficiency and high resource consumption. By introducing a neural network-based Knowledge Base, ROK effectively distills and stores retrieval-oriented knowledge from pre-trained retrieval-based models. This innovative approach enables the seamless integration of retrieval-enhanced representations with various CTR models at both the instance and feature levels, bypassing the need for time-consuming retrieval during inference. Extensive experiments on three large-scale datasets demonstrate ROK's exceptional compatibility and performance, significantly enhancing the performance of various CTR methods. Remarkably, the introduction of contrastive regularization allows ROK to surpass the performance of the sample-level retrieval-based teacher model, highlighting its strong potential for real-world applications. The ROK framework successfully transforms samplelevel retrieval-based methods into practical, efficient, and scalable solutions for industrial deployment. By eliminating the need for resource-intensive retrieval during inference and demonstrating the feasibility of distilling knowledge from non-parametric models using a parametric approach, ROK opens up new possibilities for leveraging vast amounts of historical data in CTR prediction.", "8 Acknowledgement": "We thank MindSpore [1] for the partial support of this work, which is a new deep learning computing framework.", "References": "[1] 2020. MindSpore. https://www.mindspore.cn/ [2] Yue Cao, Xiaojiang Zhou, Jiaqi Feng, Peihao Huang, Yao Xiao, Dayao Chen, and Sheng Chen. 2022. Sampling Is All You Need on Modeling Long-Term User Behaviors for CTR Prediction. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management . 2974-2983. [3] Yin-Wen Chang, Cho-Jui Hsieh, Kai-Wei Chang, Michael Ringgaard, and Chih-Jen Lin. 2010. Training and testing low-degree polynomial data mappings via linear SVM. Journal of Machine Learning Research 11, 4 (2010). [4] Bo Chen, Yichao Wang, Zhirong Liu, Ruiming Tang, Wei Guo, Hongkun Zheng, Weiwei Yao, Muyu Zhang, and Xiuqiang He. 2021. Enhancing explicit and implicit feature interactions via information sharing for parallel deep CTR models. In Proceedings of the 30th ACM international conference on information & knowledge management . 3757-3766. [5] Ming-Syan Chen, Jiawei Han, and Philip S. Yu. 1996. Data mining: an overview from a database perspective. IEEE Transactions on Knowledge and data Engineering 8, 6 (1996), 866-883. [6] Qiwei Chen, Changhua Pei, Shanshan Lv, Chao Li, Junfeng Ge, and Wenwu Ou. 2021. End-to-end user behavior retrieval in click-through rateprediction model. arXiv preprint arXiv:2108.04468 (2021). [7] Qiwei Chen, Yue Xu, Changhua Pei, Shanshan Lv, Tao Zhuang, and Junfeng Ge. 2022. Efficient Long Sequential User Data Modeling for Click-Through Rate Prediction. arXiv:2209.12212 [cs.IR] [8] Xinlei Chen and Kaiming He. 2021. Exploring simple siamese representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition . 15750-15758. [9] Xinyi Dai, Jianghao Lin, Weinan Zhang, Shuai Li, Weiwen Liu, Ruiming Tang, Xiuqiang He, Jianye Hao, Jun Wang, and Yong Yu. 2021. An adversarial imitation click model for information retrieval. In Proceedings of the Web Conference 2021 . 1809-1820. [10] Kounianhua Du, Weinan Zhang, Ruiwen Zhou, Yangkun Wang, Xilong Zhao, Jiarui Jin, Quan Gan, Zheng Zhang, and David Wipf. 2022. Learning enhanced representations for tabular data via neighborhood propagation. arXiv preprint arXiv:2206.06587 (2022). [11] Lingyue Fu, Jianghao Lin, Weiwen Liu, Ruiming Tang, Weinan Zhang, Rui Zhang, and Yong Yu. 2023. An F-shape Click Model for Information Retrieval on Multiblock Mobile Pages. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining . 1057-1065. [12] Huifeng Guo, Bo Chen, Ruiming Tang, Weinan Zhang, Zhenguo Li, and Xiuqiang He. 2021. An embedding learning framework for numerical features in ctr prediction. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining . 2910-2918. [13] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. arXiv preprint arXiv:1703.04247 (2017). [14] Bal\u00e1zs Hidasi and Alexandros Karatzoglou. 2018. Recurrent neural networks with top-k gains for session-based recommendations. In Proceedings of the 27th ACM international conference on information and knowledge management . 843-852. [15] Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2015. Session-based recommendations with recurrent neural networks. arXiv preprint arXiv:1511.06939 (2015). [16] Xunqiang Jiang, Binbin Hu, Yuan Fang, and Chuan Shi. 2020. Multiplex memory network for collaborative filtering. In Proceedings of the 2020 SIAM International Conference on Data Mining . SIAM, 91-99. [17] Li Jing, Pascal Vincent, Yann LeCun, and Yuandong Tian. 2021. Understanding dimensional collapse in contrastive self-supervised learning. arXiv preprint arXiv:2110.09348 (2021). [18] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recommendation. In 2018 IEEE international conference on data mining (ICDM) . IEEE, 197-206. [19] Xiangyang Li, Bo Chen, HuiFeng Guo, Jingjie Li, Chenxu Zhu, Xiang Long, Sujian Li, Yichao Wang, Wei Guo, Longxia Mao, et al. 2022. IntTower: the Next Generation of Two-Tower Model for Pre-Ranking System. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management . 3292-3301. [20] Xiaochen Li, Jian Liang, Xialong Liu, and Yu Zhang. 2022. Adversarial Filtering Modeling on Long-term User Behavior Sequences for Click-Through Rate Prediction. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1969-1973. [21] Yushen Li, Jinpeng Wang, Tao Dai, Jieming Zhu, Jun Yuan, Rui Zhang, and ShuTao Xia. 2024. RAT: Retrieval-Augmented Transformer for Click-Through Rate Prediction. In Companion Proceedings of the ACM on Web Conference 2024 (WWW '24) . ACM. https://doi.org/10.1145/3589335.3651550 [22] Jianghao Lin, Weiwen Liu, Xinyi Dai, Weinan Zhang, Shuai Li, Ruiming Tang, Xiuqiang He, Jianye Hao, and Yong Yu. 2021. A Graph-Enhanced Click Model for Web Search. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1259-1268. [23] Bin Liu, Ruiming Tang, Yingzhi Chen, Jinkai Yu, Huifeng Guo, and Yuzhou Zhang. 2019. Feature generation by convolutional neural network for click-through rate prediction. In The World Wide Web Conference . 1119-1129. [24] Huanshuo Liu, Hao Zhang, Zhijiang Guo, Kuicai Dong, Xiangyang Li, Yi Quan Lee, Cong Zhang, and Yong Liu. 2024. CtrlA: Adaptive Retrieval-Augmented Generation via Probe-Guided Control. arXiv preprint arXiv:2405.18727 (2024). [25] Qi Pi, Weijie Bian, Guorui Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Practice on long sequential user behavior modeling for click-through rate prediction. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 2671-2679. [26] Qi Pi, Guorui Zhou, Yujing Zhang, Zhe Wang, Lejian Ren, Ying Fan, Xiaoqiang Zhu, and Kun Gai. 2020. Search-based user interest modeling with lifelong sequential behavior data for click-through rate prediction. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management . 2685-2692. [27] Jiarui Qin, Weinan Zhang, Rong Su, Zhirong Liu, Weiwen Liu, Ruiming Tang, Xiuqiang He, and Yong Yu. 2021. Retrieval & Interaction Machine for Tabular Data Prediction. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining . 1379-1389. [28] Jiarui Qin, Weinan Zhang, Xin Wu, Jiarui Jin, Yuchen Fang, and Yong Yu. 2020. User behavior retrieval for click-through rate prediction. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . 2347-2356. [29] Yanru Qu, Bohui Fang, Weinan Zhang, Ruiming Tang, Minzhe Niu, Huifeng Guo, Yong Yu, and Xiuqiang He. 2018. Product-based neural networks for user response prediction over multi-field categorical data. ACM Transactions on Information Systems (TOIS) 37, 1 (2018), 1-35. [30] Kan Ren, Jiarui Qin, Yuchen Fang, Weinan Zhang, Lei Zheng, Weijie Bian, Guorui Zhou, Jian Xu, Yong Yu, Xiaoqiang Zhu, et al. 2019. Lifelong sequential modeling with personalized memorization for user response prediction. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval . 565-574. [31] Steffen Rendle. 2010. Factorization machines. In 2010 IEEE International conference on data mining . IEEE, 995-1000. [32] Kendrick Shen, Robbie M Jones, Ananya Kumar, Sang Michael Xie, Jeff Z HaoChen, Tengyu Ma, and Percy Liang. 2022. Connect, not collapse: Explaining contrastive learning for unsupervised domain adaptation. In International Conference on Machine Learning . PMLR, 19847-19878. [33] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, and Jian Tang. 2019. Autoint: Automatic feature interaction learning via selfattentive neural networks. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management . 1161-1170. [34] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer. In Proceedings of the 28th ACM international conference on information and knowledge management . 1441-1450. [35] Jiaxi Tang and Ke Wang. 2018. Personalized top-n sequential recommendation via convolutional sequence embedding. In Proceedings of the eleventh ACM international conference on web search and data mining . 565-573. [36] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. Journal of machine learning research 9, 11 (2008). [37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017). [38] Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong, and Ed Chi. 2021. Dcn v2: Improved deep & cross network and practical lessons for web-scale learning to rank systems. In Proceedings of the Web Conference 2021 . 1785-1797. [39] Yuhao Wang, Xiangyu Zhao, Bo Chen, Qidong Liu, Huifeng Guo, Huanshuo Liu, Yichao Wang, Rui Zhang, and Ruiming Tang. 2023. PLATE: A prompt-enhanced paradigm for multi-scenario recommendations. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1498-1507. [40] Qitian Wu, Chenxiao Yang, and Junchi Yan. 2021. Towards open-world feature extrapolation: An inductive graph learning approach. Advances in Neural Information Processing Systems 34 (2021), 19435-19447. [41] Yunjia Xi, Jianghao Lin, Weiwen Liu, Xinyi Dai, Weinan Zhang, Rui Zhang, Ruiming Tang, and Yong Yu. 2023. A Bird's-eye View of Reranking: from List Level to Page Level. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining . 1075-1083. [42] Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua. 2017. Attentional factorization machines: Learning the weight of feature interactions via attention networks. arXiv preprint arXiv:1708.04617 (2017). [43] Xin Xin, Bo Chen, Xiangnan He, Dong Wang, Yue Ding, and Joemon M Jose. 2019. CFM: Convolutional Factorization Machines for Context-Aware Recommendation.. In IJCAI , Vol. 19. 3926-3932. [44] Chaoning Zhang, Kang Zhang, Chenshuang Zhang, Trung X Pham, Chang D Yoo, and In So Kweon. 2022. How does simsiam avoid collapse without negative samples? a unified understanding with self-supervised contrastive learning. arXiv preprint arXiv:2203.16262 (2022). [45] Weinan Zhang, Jiarui Qin, Wei Guo, Ruiming Tang, and Xiuqiang He. 2021. Deep learning for click-through rate estimation. arXiv preprint arXiv:2104.10584 (2021). [46] Lei Zheng, Ning Li, Xianyu Chen, Quan Gan, and Weinan Zhang. 2023. Dense Representation Learning and Retrieval for Tabular Data Prediction. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 3559-3569. [47] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Deep interest evolution network for click-through rate prediction. In Proceedings of the AAAI conference on artificial intelligence , Vol. 33. 5941-5948. [48] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through rate prediction. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1059-1068."}
