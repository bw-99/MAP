{
  "Contrastive Self-supervised Learning in Recommender Systems: A Survey": "MENGYUAN JING, Shanghai Jiao Tong University, China YANMIN ZHU ∗ , Shanghai Jiao Tong University, China TIANZI ZANG, Shanghai Jiao Tong University, China KE WANG, Shanghai Jiao Tong University, China Deep learning-based recommender systems have achieved remarkable success in recent years. However, these methods usually heavily rely on labeled data (i.e., user-item interactions), suffering from problems such as data sparsity and cold-start. Self-supervised learning, an emerging paradigm that extracts information from unlabeled data, provides insights into addressing these problems. Specifically, contrastive self-supervised learning, due to its flexibility and promising performance, has attracted considerable interest and recently become a dominant branch in self-supervised learning-based recommendation methods. In this survey, we provide an up-to-date and comprehensive review of current contrastive self-supervised learning-based recommendation methods. Firstly, we propose a unified framework for these methods. We then introduce a taxonomy based on the key components of the framework, including view generation strategy, contrastive task, and contrastive objective. For each component, we provide detailed descriptions and discussions to guide the choice of the appropriate method. Finally, we outline open issues and promising directions for future research. CCS Concepts: · Information systems → Recommender systems . Additional Key Words and Phrases: contrastive learning, self-supervised learning, unsupervised learning, survey, deep learning",
  "ACMReference Format:": "Mengyuan Jing, Yanmin Zhu, Tianzi Zang, and Ke Wang. 2018. Contrastive Self-supervised Learning in Recommender Systems: A Survey. J. ACM 37, 4, Article 111 (August 2018), 39 pages. https://doi.org/XXXXXXX. XXXXXXX",
  "1 INTRODUCTION": "Recommender systems, as the most effective way to alleviate information overloading, have been an indispensable tool in daily life [20, 140]. They are intensively employed in a broad range of online services such as e-commerce platforms, social media, and music platforms. Owing to the ability to effectively capture the user-item relationships, deep learning techniques have been widely used in recommender systems [17, 40]. Despite their effectiveness, most deep learning-based methods ∗ corresponding author Authors' addresses: Mengyuan Jing, jingmy@sjtu.edu.cn, Shanghai Jiao Tong University, No.800 Dongchuan Road, Minhang District, Shanghai, Shanghai, China, 200240; Yanmin Zhu, yzhu@sjtu.edu.cn, Shanghai Jiao Tong University, No.800 Dongchuan Road, Minhang District, Shanghai, Shanghai, China, 200240; Tianzi Zang, zangtianzi@sjtu.edu.cn, Shanghai Jiao Tong University, No.800 Dongchuan Road, Minhang District, Shanghai, Shanghai, China, 200240; Ke Wang, onecall@ sjtu.edu.cn, Shanghai Jiao Tong University, No.800 Dongchuan Road, Minhang District, Shanghai, Shanghai, China, 200240. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. © 2018 Association for Computing Machinery. 0004-5411/2018/8-ART111 $15.00 https://doi.org/XXXXXXX.XXXXXXX 111 J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:2 M. Jing et al. focus on supervised learning settings. The recommendation model is trained with abundant labeled data (i.e., user-item interactions). However, user-item interaction records are very sparse compared to the interaction space [2, 38]. Hence, these methods usually suffer from the problem of data sparsity [104]. Meanwhile, these methods are prone to the problem of over-fitting and generalization error [56]. Self-supervised learning (SSL) [56], as a novel learning paradigm, provides new insights to overcome aforementioned problems. The basic idea of SSL is to acquire transferable knowledge from the data itself without the need for manually annotated labels. This is achieved by solving auxiliary tasks (named pretext tasks). The acquired knowledge is then used in downstream tasks. Due to its efficiency, SSL has been widely used in many fields such as computer vision (CV) [13, 42, 68], natural language processing (NLP) [26, 61] and graph learning [88, 108]. Inspired by the success of SSL in other fields, there is growing interest in applying SSL to the area of recommendation. Existing SSL-based recommendation methods can be classified into generative, contrastive, and predictive methods [135]. However, generative self-supervised learning is memory-consuming when trained on large-scale datasets. Predictive self-supervised learning often requires domain knowledge to generate labels, leading to increased costs and reduced generalization performance. In contrast, contrastive self-supervised learning (CL for brevity) has lightweight models and flexible designs since it is independent of the encoder structure and typically requires no domain knowledge. As a result, CL-based methods have witnessed significant growth in recent years, emerging as the dominant approach among SSL-based recommendation methods. Furthermore, the number of publications related to CL-based recommendations exceeds 50% of the total number of SSL-based recommendation publications in the ACM Digital Library 1 . Considering this increasing trend, we aim to provide a timely and comprehensive review to summarize these CL-based methods in this paper. Although there have been several reviews [44, 49] on contrastive learning, they mainly focus on methods in CV and NLP without reviewing CL-based recommendation methods. However, due to the uniqueness of the recommendation, it is difficult to apply existing CL-based methods from other fields to recommendation. Specifically, in CV/NLP, models usually deal with dense input data and treat each data instance as isolated. However, in recommender systems, the input data are extremely sparse (e.g., one-hot ID and categorical features of users/items) and there is homophily between users or items. Moreover, various recommendation tasks are unique to recommender systems, such as bundle recommendation and multi-behavior recommendation. Furthermore, several reviews on SSL-based graph learning [57, 106, 117] also include some CLbased recommendation methods. However, these reviews only provide a limited introduction and lack in-depth discussions on recommendation. Therefore, these reviews cannot provide sufficient insights into CL-based recommendation. Considering the unique characteristics of recommendation, a comprehensive survey is necessary to thoroughly review CL-based recommendation methods. In the field of recommendation, the most relevant survey is [135]. This survey reviews SSL-based recommendation methods, including some CL-based methods. Compared to [135], our survey purely focuses on CL-based recommendation and provides a more comprehensive and detailed analysis of this topic. Specifically, our survey has the following differences. Firstly, we present a more rational, fine-grained, and comprehensive taxonomy. For instance, we add model-based augmentation methods and methods without augmentation to view generation strategies and categorize contrastive tasks based on the characteristics of the contrastive instances. Secondly, we provide in-depth analyses of different options for key components of CL-based recommendation methods, guiding the selection of these components. This critical discussion is not present in [135]. 1 https://dl.acm.org/ J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. Contrastive Self-supervised Learning in Recommender Systems: A Survey 111:3 Finally, because of the increasing popularity of CL-based recommendation methods, we provide a more up-to-date review that summarizes recently published studies that were not included in [135]. To sum up, the key contributions of this paper are summarized as follows: · We propose a general framework to unify the CL-based methods for recommendation. Based on the framework, we review existing research according to three key components: view generation, pretext task, and contrastive objective. · We provide an up-to-date and comprehensive review of CL-based recommendation methods. We provide detailed descriptions and discussions for each key component to guide the choice of the appropriate method. We also introduce the relevant background knowledge to help readers easily understand CL-based recommendation. · We identify the limitations of existing research and propose promising future directions for CL-based recommendation to inspire new research. Paper Collection. We first adopt Google Scholar as the main search engine to collect related papers. Then, we search for related work from top-tier conferences and journals, such as SIGIR, KDD, WWW, AAAI, IJCAI, WSDM, CIKM, NuerIPS, ICML, TKDE, TOIS, etc. Specifically, we search with keywords including \"self-supervised\", \"contrastive\" in combination with \"recommend\", \"collaborative filtering\". To prevent omissions of relevant work, we further look through the references of each paper. Survey Organization. The remainder of the survey is organized as follows. In Section 2, we introduce background knowledge. We then introduce the unified framework and taxonomy in Section 3. Section 4, Section 5 and Section 6 are the main contents, which review contrastive learning in recommender systems. In Section 7, we discuss the open problems and future directions. Finally, we conclude the survey in Section 8.",
  "2 BACKGROUND": "In this section, we introduce essential background knowledge about CL-based recommendation. First, we provide the definitions of relevant concepts. Then, we give a brief introduction to contrastive learning. At last, we introduce training strategies used in CL-based recommendation methods. In addition, we summarize the notations used in this survey in Table. 1.",
  "2.1 Term Definitions": "2.1.1 Supervised Learning, Unsupervised Learning, and Self-supervised Learning. Supervised learning refers to a learning paradigm that trains models with manually annotated labels. In contrast, unsupervised learning indicates the learning paradigm that trains models without using manually annotated labels. Self-supervised learning can be viewed as a subset of unsupervised learning as it requires no manually annotated labels. However, unlike other unsupervised learning methods (e.g., clustering) that concentrate on mining data patterns, self-supervised learning aims to generate supervision signals from the data itself, and models are still trained in supervised settings. 2.1.2 Pretext Tasks Versus Downstream Tasks. Pretext tasks are pre-designed tasks to be solved by models (e.g., node self-discrimination [104]). By learning the objective functions of the pretext tasks, models learn more generalized representations from unlabeled data, thus benefiting downstream tasks. Downstream tasks refer to tasks used to evaluate the quality of representations learned by models. Specifically, in recommender systems, downstream tasks are the recommendation tasks such as sequential recommendation and social recommendation. In general, solving downstream tasks requires manually annotated labels. J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:4 M. Jing et al. Table 1. Key notations. Fig. 1. Pipeline of contrastive learning. Notations Discriptions U The set of users I The set of items B The mini-batch h , c , g , z The learned representations 𝑓 𝜃 The encoder to learn representations 𝑝 𝜔 The pretext decoder 𝑞 𝜙 The downstream decoder 𝜃 , 𝜔 , 𝜙 , 𝜉 , 𝜓 Learnable parameters 𝜆, 𝜌, 𝜖 The hyperparameter T Data-based view generation strategy L The location matrix A The adjacent matrix of graph X The feature matrix H The representation matrix G The graph V / E The set of the graph nodes/edges 𝑠 𝑢 The interaction sequence of user 𝑢 MI Mutual information function | | Concatenation operation ◦ The Hadamard product | · | The length of a set Data View Data View Encoder f  Encoder f  Pretext Decoder p  Contrastive Loss  (2)   (1)  Original Data  con ",
  "2.2 Contrastive Learning": "The core idea of contrasting learning (CL) is to maximize agreement between different views, where the agreement is usually measured by Mutual Information (MI). The general pipeline of CL is shown in Fig.1. In specific, two different data views are generated using view generation strategies. Then, representations in different views are generated by an encoder, which is usually shared by the two views. Finally, the model is optimized by contrastive loss to maximize the agreement between positive pairs and minimize the agreement between negative pairs. In general, positive pairs are the same instances from different views, while negative pairs are different instances from different views. Formally, contrastive self-supervised learning can be formulated as:  J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. Contrastive Self-supervised Learning in Recommender Systems: A Survey 111:5 (b) Pre-training and Fine-tuning. Data Labels Downstream Decoder q  Pretext Decoder p  Contrastive Loss con  Recommendation Loss rec  Encoder f  (a) Joint Learning. Data for Fine-tuning Data for Pre-training Labels Downstream Decoder q  Pretext Decoder p  Encoder f  Contrastive Loss Recommendation Loss rec  Encoder f  Parameters init  con  Fig. 2. Two types of training strategies for CL-based recommendation. where ˜ D ( 1 ) and ˜ D ( 2 ) are two generated data views. 𝑓 𝜃 (·) is the (shared) encoder to learn representations of instances in different views. 𝑝 𝜔 (·) is the pretext decoder that estimates the agreement between two instances. L 𝑐𝑜𝑛 denotes the contrastive loss.",
  "2.3 Training Strategy": "Currently, CL-based recommendation methods employ two typical training strategies: Pre-training and Fine-tuning, and Joint Learning. The detailed workflow of them is shown in Fig. 2. 2.3.1 Pre-training and Fine-tuning (P&F). In this strategy, the model is trained in two stages. In the pre-training stage, the encoder 𝑓 𝜃 (·) is first pre-trained with contrastive tasks. In addition, the pre-trained parameter 𝜃 𝑖𝑛𝑖𝑡 is then used as the initialization parameter for the encoder 𝑓 𝜃 𝑖𝑛𝑖𝑡 (·) . In the fine-tuning stage, the pre-trained encoder 𝑓 𝜃 𝑖𝑛𝑖𝑡 (·) is fine-tuned with the downstream decoder 𝑞 𝜙 (·) supervised by the recommendation task. The formulation of this strategy can be defined as:  where L 𝑐𝑜𝑛 is the contrastive loss and L 𝑟𝑒𝑐 is the recommendation loss. 𝑞 𝜙 is the downstream decoder. 2.3.2 Joint Learning (JL). In this strategy, the encoder 𝑓 𝜃 (·) is jointly trained with the pretext tasks and downstream tasks (i.e., recommendation tasks). Moreover, the encoder is usually shared by pretext and recommendation tasks. This strategy can be considered a type of multi-tasking learning strategy, in which the contrastive pretext task is the auxiliary task to regularize the recommendation task. The loss function consists of both contrastive loss and recommendation loss. The learning J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:6 M. Jing et al. objective can be formalized as:  where 𝜆 is a trade-off hyperparameter that controls the contribution of L 𝑐𝑜𝑛 . 2.3.3 Discussion. Compared to JL, P&F has better generalizability. Specifically, for different recommendation tasks, P&F only requires fine-tuning, while JL requires re-training. Additionally, when data for the target recommendation task is limited, P&F allows for pre-training with data from other recommendation tasks. However, due to the two-step training process, P&F is more complex compared to JL. Moreover, since the pre-trained model is trained without labels, it may not explicitly learn the features of a specific task, which could result in compromised performance on certain tasks. When recommendation tasks are determined, JL usually achieves better recommendation performance and is easier to be implemented. Therefore, most of the existing CL-based approaches adopt JL. However, JL requires high computational resources since multiple tasks need to be trained simultaneously. Additionally, careful balancing of loss functions for different tasks is necessary to avoid instability or performance degradation. To summarize, if the primary goal is to improve recommendation performance on a specific task, it is recommended to choose JL. Conversely, if the goal is to achieve good performance on different recommendation tasks, P&F is the better choice.",
  "3 TAXONOMY": "In this section, we first propose a unified framework of CL-based recommendation methods. Then we introduce our proposed taxonomy with three perspectives.",
  "3.1 Unified Framework": "As introduced in Section 2, the general framework of CL-based methods is first to perform view generation strategies to obtain multiple views and then maximize the agreement of positive pairs in these views by conducting the contrastive pretext task. Specifically, given the data D , 𝐾 data views { ˜ D ( 𝑘 ) } 𝐾 𝑘 = 1 are obtained through 𝐾 data-based augmentations {T 𝑘 (·)} 𝐾 𝑘 = 1 , which can be formulated as:  Then, encoders { 𝑓 𝜃 𝑘 (·)} 𝐾 𝑘 = 1 are applied to generate representations { h 𝑘 } 𝐾 𝑘 = 1 for each data view. Formally, we have  In addition, { h 𝑘 } 𝐾 𝑘 = 1 may have different scales depending on the type of pretext tasks. For example, it can be a representation of an item or a representation of a sequence that consists of multiple items. During training, contrastive learning is to maximize the agreement between representations of positive pairs ( h 𝑖 , h 𝑗 ) in two views. Moreover, the mutual information MI GLYPH<0> h 𝑖 , h 𝑗 GLYPH<1> is usually applied to measure the agreement. The contrastive objective can be defined as:  where 𝜆 ∈ { 0 , 1 } , if the mutual information between h 𝑖 and h 𝑗 is calculated then 𝜆 = 1, otherwise 𝜆 = 0. Since it is difficult to directly calculate mutual information, mutual information estimators are usually used instead. The estimation is calculated based on the discriminator 𝑝 𝜔 (·) (i.e., the pretext J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. Contrastive Self-supervised Learning in Recommender Systems: A Survey 111:7 Fig. 3. Taxonomy of contrastive learning-based recommendation. \"JS\" refers to Jensen-Shannon. \"InfoNCE\" loss refers to the loss proposed by Oord et al. [68], where \"NCE\" stands for Noise-Contrastive Estimator. \"BYOL\" (Bootstrap Your Own Latent) loss refers to the contrastive loss proposed by Grill et al. [28]. Pretext Task Cross-Scale Contrasting Same-Scale Contrasting Local-Local Contextual-Contextual Global-Global Local-Global Contextual-Global Local-Contextual Objective Non-bound Bound JS Estimator InfoNCE Triplet Loss BYOL Loss View Generation Data-based Model-based With Augmentation Without Augmentation decoder). Moreover, projection heads [18] can be optionally applied to { h 𝑘 } 𝐾 𝑘 = 1 , defined as:  where 𝑔 𝜉 𝑖 (·) is a projection head, which can be the Multi-Layer Perceptron (MLP) or linear projection. For the sake of convenience, we treat the projection head as part of the pretext decoder 𝑝 𝜔 (·) . Then 𝑓 𝜃 ∗ (·) and 𝑝 𝜔 ∗ (·) can be obtained by learning Eq.(6). Furthermore, by utilizing 𝑓 𝜃 ∗ (·) , the generated representations can be used for recommendation tasks. The recommendation task can be formulated as:  where 𝑦 denotes the labels. L 𝑟𝑒𝑐 is the supervised loss for recommendation tasks such as the cross-entropy (CE) loss.",
  "3.2 Proposed Taxonomy": "The differences among contrastive learning methods lie in three key components: view generation strategies, pretext tasks, and contrastive objectives. A CL-based recommendation method can be determined by specifying these components. Note that the encoder 𝑓 𝜃 (·) is not included in our taxonomy, as it is not the focus of CL-based recommendation and is determined by the specific recommendation tasks. Therefore, we propose a taxonomy based on these components as shown in Fig. 3. Table.2 shows representative works of CL-based recommendation. J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:8 M. Jing et al. Table 2. A summary of CL-based recommendation methods. For alphabets in \"Pretext Task\", L means Local; C means Contextual; G means Global. For acronyms used, \"BPR\" refers to Bayesian Personalized Ranking loss; \"JL\" refers to Joint Learning; \"P&F\" refers to Pre-training and Fine-tuning; \"CF\" refers to Collaborative Filtering; \"KG\" refers to Knowledge Graph. Continued on next page J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. Contrastive Self-supervised Learning in Recommender Systems: A Survey 111:9 Table 2. A summary of CL-based recommendation methods. (Continued) View Generation is the design of how to generate contrastive views. Depending on whether the augmentation is needed, we classify the view generation strategies into view generation with augmentation and without augmentation. Pretext Task is the design of how to obtain supervision signals. Depending on the scale of the instances being contrasted, we classify the pretext tasks into same-scale contrasting and cross-scale contrasting. Contrastive Objective is the design of how to measure mutual information. Depending on whether an estimation of lower-bound of mutual information is provided, we classify the contrastive objectives into bound objective and non-bound objective. Note that the selection of these components depends on the characteristics of the input data and downstream tasks. For instance, sequence-based augmentation methods may not be suitable for graph data. For sequential recommendation, the pretext tasks generally contrast sequence representations as the primary objective is to learn high-quality sequence representations. It is worth noting that the selection of components is not entirely independent. Although the same pretext task can be performed with different view generation strategies or contrastive objectives, some may not be effective. For instance, feature-based augmentation methods may not be effective when the pretext task aims to model the sequential relationships of items. Therefore, when designing a CL-based recommendation method, we can first design the pretext task based on the specific recommendation task and then select view generation strategies and contrastive objectives accordingly.",
  "4 VIEW GENERATION": "Recent works [51, 86, 96] in other fields have shown that contrastive learning relies heavily on view generation, as generating multiple views facilitates models to explore richer underlying semantic information. In practice, if multiple data views naturally exist, such as interaction views and social networks in social recommendation, pretext tasks can be performed directly on these views. In addition, multiple views are not available in many scenarios, so augmentations are needed to generate contrastive views from the original data [13, 26, 32]. Therefore, we divide existing view generation strategies into view generation with augmentation and without augmentation. J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:10 M. Jing et al. Fig. 4. Graph-based augmentation. Edge Perturbation Graph Diffusion Subgraph Sampling Original Graph",
  "4.1 With Augmentation": "Augmentation strategies can be categorized into data-based augmentation and model-based augmentation. The former generates views based on the data, while the latter is based on the model (i.e., the encoder). 4.1.1 Data-based Augmentation. Based on the type of data to be augmented, we classify data-based augmentation into graph-based augmentation, sequence-based augmentation, and feature-based augmentation. Graph-based Augmentation. This strategy (shown in Fig.4) performs augmentations on the graph (e.g., interaction graph and social graph) to generate multiple views. Note that since the augmentations of node attributes in graphs are similar to feature-based augmentation, under this subcategory we only present the augmentations of the graph structure (shown in Fig. 4). Formally, given a graph G = (V , E) , graph-based augmentation transforms the adjacent matrix A of G , i.e., T = T( A ) . Edge perturbation. This strategy [31, 50, 52, 60, 63, 91, 92, 104, 111, 124, 125] generates graph views through randomly adding or dropping edges. It can be defined as:  where L is the location matrix. If L 𝑖 𝑗 = 1, the edge between 𝑖 and 𝑗 will be perturbed. Specifically, if A 𝑖 𝑗 = 1 , L 𝑖 𝑗 = 1, the edge between 𝑖 and 𝑗 will be dropped. If A 𝑖 𝑗 = 0 , L 𝑖 𝑗 = 1, an edge will be added between 𝑖 and 𝑗 . L can be randomly sampled [104, 124] or manually set. Furthermore, L can also be calculated adaptively [43, 45, 46] to keep important edges while perturbing possibly unimportant ones. Graph Diffusion. The graph diffusion [62, 139] incorporates the global information to the original graph by creating new edges between nodes. It can be formulated as:  where Θ 𝑘 is the weighting coefficient. T denotes the generalized transition matrix. For example, SMIN [62] generates a substructure-aware adjacent matrix and injects it into the user-item interaction graph. Subgraph Sampling. This strategy samples a node subset and corresponding edges to generate a subgraph as the data view. Existing methods usually obtain the node subset V ′ by uniform J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. Contrastive Self-supervised Learning in Recommender Systems: A Survey 111:11 Fig. 5. Sequence-based augmentation. Item Masking Item Shuffling Item Cropping Item Substituting Item Insterting Original Sequence 1 i 2 i 3 i 4 i 5 i 1 i 2 i 3 i 4 i 5 i 1 i 2 i 5 i 3 i 4 i 1 i 2 i 6 i 3 i 4 i 5 i 1 i 6 i 3 i 4 i 5 i 1 i 4 i 2 i 3 i 5 i sampling, ego-net sampling and knowledge-based sampling. Uniform sampling [31, 35, 76, 104, 138] uniformly samples a certain portion of nodes and corresponding edges to augment the views. Node dropout belongs to uniform sampling. For example, SGL [104] randomly drops a portion of nodes, which is denoted as V 𝑑 . Therefore, the sampled node subset can be obtained by V ′ = V - V 𝑑 . Ego-net sampling [9] samples the 𝐿 -hop neighbors of each node in a graph, also known as the 𝐿 -ego net. Therefore, the node subset can be represented as V ′ = { 𝑗 | 𝑑 ( 𝑖, 𝑗 ) ≤ 𝐿 } , where 𝑑 ( 𝑣 𝑖 , 𝑣 𝑗 ) is the shortest distance between node 𝑖 and 𝑗 . Knowledge-based sampling [92, 133] incorporates domain knowledge when sampling subgraph. For example, MHCN [133] designs three types of triangular motifs based on underlying semantics. Motifs specify high-order relations like \"having a mutual friend\". Sequence-based Augmentation. This strategy (shown in Fig.5) performs augmentations on the user interaction sequences. Formally, give the interaction sequence 𝑠 𝑢 of user 𝑢 , it can be formulated as ˜ 𝑠 𝑢 = T( 𝑠 𝑢 ) . Item Shuffling. The item shuffling [53, 59, 89, 94, 97, 116] randomly shuffle a continuous subsequence of the interaction sequence to generate the augmented sequence:  where [ 𝑖 𝑢,𝑘 , · · · , 𝑖 𝑢,𝑘 + 𝑙 𝑠 -1 ] is shuffled as [ ˜ 𝑖 𝑢,𝑘 , · · · , ˜ 𝑖 𝑢,𝑘 + 𝑙 𝑠 -1 ] . 𝑙 𝑐 = ⌈ 𝜌 𝑠 | 𝑠 𝑢 |⌉ is the length of the subsequence and 𝜌 𝑠 ∈ [ 0 , 1 ] . Item Cropping. The item cropping [53, 54, 59, 94, 97, 116] randomly chooses a continuous subsequence of the interaction sequence and can be represented as:  where 𝑙 𝑐 = ⌈ 𝜌 𝑐 | 𝑠 𝑢 |⌉ is the length of the subsequence and 𝜌 𝑐 ∈ [ 0 , 1 ] is the hyperparameter. Item Masking. This strategy [23, 53, 54, 59, 89, 94, 97, 116] randomly chooses a portion of items in the interaction sequence and replaces them with a [mask] token, which can be formulated as:  where ˜ 𝑖 𝑢,𝑘 = [ mask ] if 𝑖 𝑢,𝑘 is masked, otherwise ˜ 𝑖 𝑢,𝑘 = 𝑖 𝑢,𝑘 . Item Substituting. [53, 59] As dropout-based augmentation methods such as item masking may exacerbate the problem of data sparsity and cold-start, item substituting and item inserting are proposed. The item substituting randomly replaces a portion of items in the sequence with other J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:12 M. Jing et al. Fig. 6. Feature-based Augmentation. Feature Shuffling Feature Dropout Original Features items, which can be formulated as:  where ˜ 𝑖 𝑢,𝑘 replaces 𝑖 𝑢,𝑘 . Moreover, CoSeRec [59] substitutes items with highly correlated items to maintain the item correlations in the sequences. Item Inserting. [53, 59] Fewer interactions are recorded in the interaction sequence than the complete behavior of the user, as interaction data from other sources may be missing. Therefore, the comprehensive user preferences and item correlations cannot be captured. To complete the sequence, CoSeRec [59] proposes the item inserting to generate the augmented sequence. Firstly, it randomly samples a portion of items in the sequence. Then, items that correlated to sampled items are inserted around them:  where 𝑖 𝑢,𝑘 is the sampled item and ˜ 𝑖 𝑢,𝑘 is the item related to it. Feature-based Augmentation. Feature-based augmentation (shown in Fig.6) performs augmentations on the feature vectors, which can be categorical features or feature representations (e.g., embeddings). Given feature matrix X , the augmented view is represented as ˜ X = T( X ) . Feature Dropout. The feature dropout (masking) [66, 90, 129] masks/drops a portion of the features and is formulated as:  where L is the masking matrix that indicates the masking locations. If the 𝑗 -th feature of 𝑖 is masked/dropped, then L 𝑖 𝑗 = 1, otherwise L 𝑖 𝑗 = 0. Similar to edge perturbation, L can be uniformly sampled or manually assigned. ◦ is the Hadamard product. Feature Shuffling. The feature shuffling [6, 113, 125, 133] perturbs the feature matrix by row or column. It can be formulated as: T( X ) = X [ 𝑖𝑑𝑥 𝑟 , 𝑖𝑑𝑥 𝑐 ] (17) where 𝑖𝑑𝑥 𝑟 and 𝑖𝑑𝑥 𝑐 are the shuffled row index and the shuffled column index, respectively. 4.1.2 Model-based Augmentation. Model-based augmentation strategies (shown in Fig.7) generate views by perturbing the model (i.e., encoder). It is worth noting that unlike data-based augmentation strategies, which first generate different data views and then generate representations for each data view, model-based strategies directly generate different representations for the original data to perform the pretext task. It can be formulated as:  where 𝑓 𝜃 (·) and 𝑓 ′ 𝜃 ′ (·) are the encoder and perturbed encoder, respectively. h , h ′ are representations output by 𝑓 𝜃 (·) and 𝑓 ′ 𝜃 ′ (·) , respectively. D is the original data. Message Dropout. This strategy[23, 58, 63, 73, 90] randomly masks the neurons in the layers for a certain dropout ratio [26]. Then, by applying different dropout masks, multiple views can be J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. Contrastive Self-supervised Learning in Recommender Systems: A Survey 111:13 Fig. 7. Model-based augmentation. Message Dropout Parameter Noise Layer 2 Dropout Layer 1 Embedding Noise Layer 3 Layer 2 Layer 1 Layer 3 Layer 2 Layer 1 Layer Dropout Encoder Complementing Layer 3 Layer2 Layer 1 Layer 3 Layer 2 Layer 1 (1)  (2)  (3)  (1)  (2)  (3)  Layer 3 Layer 2 Layer 1 obtained with the same input data. For example, DuoRec [73] applies two different dropout masks on the Transformer-based model to generate two different views. Embedding Noise. This strategy [130, 131, 134] generates different views by adding different noises to original embeddings. Unlike feature-based augmentations that only perturb input embeddings or the final representations, this strategy adds noise to the embeddings at different layers of the encoder. It can be formulated as:  where E 𝑙 is the original embedding and Δ 𝑙 is the perturbation noise at the 𝑙 -th layer. In SimGCL [134], the Δ 𝑙 ∼ 𝑈 ( 0 , 1 ) is the random uniform noise. In RocSE [130], Δ 𝑙 = 𝜖 · 𝑓 norm ( 𝑓 shuffle ( E 𝑙 )) , where 𝑓 shuffle and 𝑓 norm are the random shuffling and normalization operations, respectively. 𝜖 is a hyperparameter. Parameter Noise. This strategy [109] adds noises to the parameters of the encoder, which is formulated as:  where 𝜃 𝑙 and 𝜃 ′ 𝑙 are the original parameters and perturbed parameters of 𝑙 -th layer, respectively. The Δ 𝑙 is the random noise, that can be sampled from the Gaussian distribution. 𝜖 is a hyper-parameter. Architecture Perturbation. Unlike the above strategies that perturb learnable parameters in the model, some works generate different views by changing the model architecture. For example, SRMA [58] proposes Layer Dropout and Encoder Complementing . Specifically, the Layer Dropout randomly drops a portion of layers in the model during training to enable contrastive learning between shallow features and deep features. The Encoder Complementing uses a pre-trained encoder to generate representations. These representations are combined with the representations generated by the original encoder for contrastive learning. MA-GCL [27] proposes to perturb the architecture of graph neural network (GNN) encoders by varying the number and permutations of propagation and transformation operators.",
  "4.2 Without Augmentation": "The key idea of contrastive learning is to maximize the agreement between different views. Thus, if multiple views naturally exist, these views can be contrasted directly without additional augmentations. For example, in cross-domain recommendation, the two domains can be considered as two views. Therefore, some methods such as CCDR [115] and ML-SAT [142], directly perform contrastive learning between these domains. For knowledge graph-based recommendation, Some methods [91, 146] use the knowledge graph as a contrastive view. For multi-behavior recommendation, views can be constructed based on the auxiliary behavior data. For example, S-MBRec [29] treats each type of behavior as a view. Specifically, HMG-CR [121] build different hyper meta-graphs J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:14 M. Jing et al. Table 3. Comparison between different view generation strategies. based on the hyper meta-paths constructed using the distance between auxiliary behavior and target behavior buy . In bundle recommendation, user-item interaction and user-bundle interaction can also be contrasted [63].",
  "4.3 Discussion": "Table. 3 shows the comparison between different view generation strategies. In specific, most existing CL-based recommendation methods adopt data-based augmentation strategies due to their ease of implementation. However, data-based augmentations are usually selected by manual trial-and-errors, which significantly limits the generalizability of these methods. In addition, some data-based augmentations destroy the semantic information of the original data, potentially harming recommendation performance [134]. Strategies without augmentation do not require trial-and-errors. These strategies typically use domain knowledge to build auxiliary views, which preserves the semantics of the data. However, domain knowledge is expensive and cannot be applied to other domains. Furthermore, since the views are fixed during model training, strategies without augmentation lack the introduction of randomness that helps to learn noise-invariant representations. Compared to other strategies, model-based augmentations have better generalizability because they vary the learned representations without considering the original data. Although model-based augmentations require no trial-and-error and domain knowledge, settings such as the dropout ratio of messages/layers still require manual tuning. This limits their generalizability to some extent. Additionally, designing architecture-based perturbations is challenging. Furthermore, many works [23, 63, 90, 97] adopt hybrid methods by combining multiple view generation strategies. In this way, the advantages of different strategies can be combined. However, some disadvantages may still exist. For instance, combining strategies without augmentation with data-based augmentations can be helpful in introducing randomness but data-based augmentation still requires manual trial-and-errors. To summarize, selecting the appropriate strategy for view generation requires considering various factors. Here, we provide guidance for strategy selection based on typical issues in recommender systems. For the cold-start problem, strategies that drop data should be avoided as it exacerbates the problem [59]. Instead, views can be generated by adding/substituting interactions or using model-based strategies. Incorporating other data, like knowledge graphs or data from different recommendation domains, can also help mitigate data sparsity. To tackle the noise issue, databased augmentation strategies are often more effective than other strategies as the noise mainly exists in the data. Perturbing data can make models more robust, thus mitigating the impact of noise [79]. Additionally, constructing a denoised data view based on metrics such as edge reliability degrees [85] can be helpful. Similarly, for addressing bias, a debiased view can also be constructed. Introducing other side information is also beneficial in reducing bias [11]. J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. Contrastive Self-supervised Learning in Recommender Systems: A Survey 111:15 Fig. 8. Illustration of same-scale contrasting. L-L Contrasting C-C Contrasting G-G Contrasting",
  "5 PRETEXT TASK": "The goal of contrastive learning is to maximize the agreement between positive pairs (i.e., instances with same semantic information) and minimize the agreement between negative pairs (i.e., instances with unrelated semantic information). According to the scale of instances, we classify existing contrastive pretext tasks into two categories: same-scale contrasting and cross-scale contrasting. Specifically, there are three contrastive scales: local, contextual, and global. The local scale usually represents the minimum granularity of the input data, while the global scale represents the maximum. For instance, in graph (sequence) data, the local scale represents the node (item/feature), and the global scale represents the whole graph (sequence). The contextual scale is between the local and global scales and represents the subgraph (subsequence).",
  "5.1 Same-Scale Contrasting": "Depending on the different scales being contrasted, same-scale contrasting (shown in Fig.8) can be further divided into three sub-types: local-local (L-L) contrasting, contextual-contextual (C-C) contrasting, and global-global (G-G) contrasting. Considering the unique characteristics of the recommendation tasks, we present existing methods based on their recommendation tasks. 5.1.1 Local-Local Contrasting. Methods under this category mainly discriminate the local representations (i.e., representation of users/items) and can be formulated as  where h 𝑖 and h 𝑗 are the representation of instance 𝑖 and 𝑗 in different views respectively. Furthermore, these representations are generated by encoder 𝑓 𝜃 (·) , which is usually shared by different views. Graph-based Collaborative Filtering. Depending on the types of graphs being contrasted, methods can be categorized into contrasting on user-item graph and contrasting on different graphs . (i) Contrasting on User-Item Graph. As only one graph exists, methods under this category should perform augmentations on the user-item interaction graph to generate different views. SGL [104] first applies contrastive learning to graph-based recommendation. Given a user-item interaction graph G . It first generates two different graph views ˜ G ( 1 ) = T(G) and ˜ G ( 2 ) = T(G) . T is the data-based view generation strategy. Moreover, it utilizes three data-based augmentations including node dropout, edge dropout, and random walk (apply edge dropout at each layer). Then, it utilizes LightGCN [39] as graph encoder 𝑓 𝜃 (·) to generate node representations H ( 1 ) = 𝑓 𝜃 ( ˜ G ( 1 ) ) and H ( 2 ) = 𝑓 𝜃 ( ˜ G ( 2 ) ) . Afterward, it performs the node self-discrimination task. Specifically, it makes the representations of the same node (i.e., the positive pair) in different views similar while J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:16 M. Jing et al. making representations of different nodes (i.e., the negative pairs) in different views dissimilar. The contrastive loss of the user side can be formulated as:  where h ( 1 ) 𝑢 ∈ H ( 1 ) and h ( 2 ) 𝑢 ∈ H ( 2 ) are representations of user 𝑢 . 𝑝 𝜔 (·) is the cosine similarity with a temperature parameter 𝜏 . 𝑝 𝜔 ( z ( 1 ) 𝑢 , z ( 2 ) 𝑢 ) = ( z ( 1 ) 𝑢 ) 𝑇 z ( 2 ) 𝑢 / 𝜏 and z ( 1 ) 𝑢 = h ( 1 ) 𝑢 /|| h ( 1 ) 𝑢 | | . In addition, for efficiency, the in-batch negative sampling can be adopted, i.e., considering only different nodes of the same batch B instead of using all other nodes as negative samples. Therefore, 𝑁𝑒𝑔 is defined as  Note that ( h ( 1 ) 𝑢 , h ( 2 ) 𝑣 ) is the inter-view negative pairs. The loss of the item side L 𝑖𝑡𝑒𝑚 𝑐𝑜𝑛 can be obtained in the same way. The contrastive loss is L 𝑐𝑜𝑛 = L 𝑢𝑠𝑒𝑟 𝑐𝑜𝑛 + L 𝑖𝑡𝑒𝑚 𝑐𝑜𝑛 . Finally, SGL adopts a joint learning strategy to optimize the contrastive loss and recommendation loss. Based on the framework of SGL, several works are proposed. The main difference with SGL is in the view generation strategies. DCL [60] perturbs the edges in 𝐿 -ego net of each node to obtain views. GDCL [139] generate new graph view using graph diffusion. Moreover, it constructs the intra-view negative pairs and the Eq.(23) can be rewritten as  LightGCL [7] proposes a singular value decomposition (SVD)-based graph augmentation strategy to effectively distill global collaborative signals. In specific, SVD is first performed on the adjacency matrix. Then, the list of singular values is truncated to retain the largest 𝐾 values and truncated matrices are used to reconstruct the adjacency matrix. The node contrastive learning is performed between the reconstructed graph and the original graph. RGCL [76] also performs edge contrastive learning. It maximizes the MI between the review representation and the corresponding interaction representation. SimGCL [134], XSimGCL [131], and RocSE [130] generate views by adding uniform noises to node representations. Moreover, to reduce the computational complexity, XSimGCL [131] replaces the final-layer contrast with cross-layer contrasting. It only utilizes one Convolutional Network (GCN)-based encoder and contrasts embeddings of different layers:  where h is the node representation and h 𝑙 ∗ is the representation at the 𝑙 ∗ layer. SimRec [110] proposes contrastive knowledge distillation by incorporating contrastive learning into knowledge distillation. It adopts a GCN-based encoder as the teacher model and an MLP-based model as the student model to generate node (user/item) representations. Furthermore, it maximizes the MI between the representations of the same node learned from the teacher model and the student model. RGCF [85] constructs contrastive views based on the edge reliability degree. It first obtains the node structural feature by aggregating its one-hop neighbor representations. Then reliability J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. Contrastive Self-supervised Learning in Recommender Systems: A Survey 111:17 degree is calculated based on the similarity of the structural feature  where h 𝑠 𝑢 and h 𝑠 𝑖 are the structure feature of user 𝑢 and item 𝑖 in the user-item interaction graph. 𝑠 𝑢,𝑖 is the reliability degree of the edge between 𝑢 and 𝑖 . Then, RGCF constructs a denoised graph and a diversity graph. The denoised graph is constructed by dropping edges with lower reliability degrees while the diversity graph is constructed by randomly adding edges with higher degrees. It maximizes the MI between representations of the same user in the two graphs. DCCF [74] constructs two relation graphs to perform contrastive tasks. Specifically, it first obtains general representation h 𝑧 and intent-aware representation h 𝑟 for each node. Then two graph relation matrices G 𝑧 and G 𝑟 are generated using them. The calculation of G 𝑧 can be formulated as:  where A is the original user-item interaction graph. G 𝑟 can be obtained similarly. The node selfdiscrimination task is performed on these three graphs. Furthermore, the augmented graph views become learnable in this process. GCARec [46], LDA_GCL [43], and AdaGCL [45] also use a learnable strategy for generating graph views. Specifically, GCARec applies an MLP to obtain preserving probabilities for edges and uses them to generate graph views on these probabilities by sampling edges. LDA_GCL also generates views by obtaining probabilities but requires pre-trained models to generate representations for calculating probabilities. AdaGCL utilizes a generative model and a denoised model to generate graph views. Similar to ADaGCL, VGCL [126] generates graph views based on a generative model, but instead of generating graphs, it directly obtains node representations for contrastive learning. VGCL also maximizes the MI between the representations of nodes in the same cluster. (ii) Contrasting on Different Graphs. In addition to the user-item interaction graph, some works construct other graphs using interaction data. That is, views are usually generated without augmentation. MCLSR [99] constructs three graphs based on the interaction sequences, including a user-item relation graph, an item-item relation graph, and a user-user relation graph. HCCF [111] constructs two views, including a user-item interaction graph and a learnable hypergraph. The node selfdiscrimination same as SGL is performed in MCLSR and HCCF. SGCCL [50] constructs a user-user graph G 𝑢𝑢 and an item-item graph G 𝑖𝑖 and performs edge/feature dropout to augment them. Node self-discrimination is conducted on the ˜ G 𝑢𝑢 and ˜ G 𝑖𝑖 . LWC_KD [98] incorporates contrastive knowledge distillation into incremental learning. In each time block, a user-item graph, user-user graph, and item-item graph are constructed. It proposes layer-wise structure-aware contrastive learning, which contrasts node representations of the same layer 𝑘 between adjacent time blocks. It can be formulated as:  where 𝑡 denotes the time block. h 𝑡 -1 𝑖,𝑘 is the representation of node 𝑖 . N 𝑡 -1 𝑖 denotes the one-hop neighbors of 𝑖 at 𝑡 -1. 𝑁𝑒𝑔 𝑡 -1 denotes nodes randomly selected from the unconnected nodes. MPT [35] extends PT-GNN [36] which performs reconstruction tasks and can only model the J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:18 M. Jing et al. intra-correlations. It leverages contrastive tasks to capture the inter-correlations within the data. Specifically, it samples subgraphs/paths for each user. Node dropout/substitution is applied to augment subgraphs/paths. The MI between representations of the same user in augmented subgraphs/paths is maximized. Knowledge Graph-based Recommendation. Apart from interaction data, the knowledge graph (KG) is also utilized for CL-based recommendation, as it can bring rich semantic information. Generally, CL-based recommendation methods using KG generate views by manual design. MCCLK [146] constructs three graph views, including user-item graph G 𝑢𝑖 , item-entity graph G 𝑖𝑒 and user-item-entity graph G 𝑢𝑖𝑒 . It maximizes the MI between the user representations h ( 𝑢𝑖 ) 𝑢 , h ( 𝑢𝑖𝑒 ) 𝑢 in G 𝑢𝑖 and G 𝑢𝑖𝑒 and between the item representations h ( 𝑢𝑖 ) 𝑖 , h ( 𝑖𝑒 ) 𝑖 in G 𝑢𝑖 and G 𝑖𝑒 . Furthermore, it generates a new representation for each item  where | | is the concatenation operation. Then the MI between h ′ 𝑖 and h ( 𝑢𝑖𝑒 ) 𝑖 is maximized by performing node-self discrimination. KACL [91] maximizes the MI between representations of the same item in the augmented user-item graph and the augmented knowledge graph. Moreover, the augmented graphs are generated by automatically dropping unimportant edges. KG can also be used to guide the generation of different user-item graph views. For example, KGCL [124] performs stochastic augmentation on the knowledge graph to generate two different views ˜ G ( 1 ) 𝑘 and ˜ G ( 2 ) 𝑘 . The the item consistency is 𝑐 𝑖 = cos ( h ( 1 ) 𝑖 , h ( 2 ) 𝑖 ) . cos (·) is the cosine similarity function. h ( 1 ) 𝑖 and h ( 2 ) 𝑖 are item representations in ˜ G ( 1 ) 𝑘 and ˜ G ( 2 ) 𝑘 , respectively. Then, the user-item interaction graph G 𝑢𝑖 is augmented using knowledge-guided data augmentation. Specifically, edge dropout is performed based on the probability calculated as follows:  where 𝑝 𝑢𝑖 is the dropout probability of edge ( 𝑢, 𝑖 ) in G 𝑢𝑖 . 𝑝 𝜏 is the threshold. 𝑝 ′ 𝑢𝑖 is an intermediate variable that is integrated with the mean value 𝜇 𝑝 ′ . 𝑝 𝑎 is a strength controller. With the 𝑝 𝑢𝑖 , masking vectors M 𝑖 ∈ { 0 , 1 } are generated based on the Bernoulli distribution [65]. The augmented graphs is ˜ G ( 𝑖 ) 𝑢𝑖 = (V , E ◦ M 𝑖 ) . Moreover, KGCL performs both intra-view contrasting and interview contrasting. KGRec [122] generates rationale scores of knowledge triplets based on attention mechanism to augment the KG and user-item graph. Besides, it only performs inter-view contrasting. Multi-behavior Recommendation. For enhancing user intention modeling, multi-behavior recommendation methods incorporate multiple types of user behavior data, which can be used to build contrastive views. S-MBRec [29] adopts a star-style contrastive task, i.e., it only performs contrastive learning between the target behavior (usually the buy) and each auxiliary behavior. It samples positive samples based on the similarity under target behavior. The similarity is calculated by point-wise mutual information [127].  J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. Contrastive Self-supervised Learning in Recommender Systems: A Survey 111:19 where I( 𝑢 ) is items that user 𝑢 has interacted. I is the item set and |I| is the number of items. If similarity 𝑃𝑀𝐼 ( 𝑢,𝑢 ′ ) > 𝑡 , ( 𝑢,𝑢 ′ ) are considered as positive pairs. 𝑡 is the threshold. The positive samples of the items are selected in a similar way. Moreover, negative samples are selected randomly. MMCLR [107] constructs a graph view (user-item graph) G and a sequence view (multi-behavior sequence) S . For each view, different behavior representations of the same users (e.g., h 𝑔 𝑢,𝑏 1 and h 𝑔 𝑢,𝑏 2 ) are treated as positive pairs. It also maximizes the MI between overall representations of the same user (e.g., h 𝑔 𝑢 and h 𝑠 𝑢 ) in different views. KMCLR [120] maximizes the MI between different behaviors of the same user. In addition, it performs knowledge-aware contrastive learning. It leverages a knowledge graph to guide the augmentation of the user-item graph under the target behavior G 𝑏 𝑡 𝑢𝑖 . It first calculates the consistency 𝑐 𝑖 of each item 𝑖 like KGCL. Then the edge dropout probability is obtained by  where h 𝑢 and h 𝑖 are the user representation and item representation in G 𝑏 𝑡 𝑢𝑖 , respectively. Min _ Max (·) is the min-max normalization function. 𝑎 and 𝑏 are hyperparameters that control the value interval of 𝑝 𝑢𝑖 . Then it performs edge dropout in a way similar to KGCL. Moreover, KMCLR adopts the node self-discrimination task. Social Recommendation. For social recommendation, social networks are utilized to improve recommendation performance. Similar to KG-based recommendation, views can be generated based on manual design. SEPT [132] constructs three graph views based on the interaction data and social network, including a preference view G 𝑟 , friend view G 𝑓 , and sharing view G 𝑠 . G 𝑟 is the user-item interaction graph. Other views are constructed based on two types of triangle motifs. Moreover, it leverages tri-training [145] to predict positive samples for each view. The Eq.(22) is changed as follows  where P 𝑣 𝑢 + is the set of predict positive samples. h 𝑣 𝑢 is the user representation in view 𝑣 . 𝑝 𝜔 is the cosine similarity with temperature parameter 𝜏 . ˜ h 𝑝 is the representation of user 𝑝 in ˜ G , which is obtained by performing random edge dropout on the joint graph of the user-item interaction graph and the social network. HGCL_S [12] constructs three types of graphs: user-item graph, user-user graph, and item-item graph. It performs the node self-discrimination task on the corresponding graphs. Moreover, when generating node representations in the user-user graph and item-item graph, HGCL_S utilizes a meta-network. Cross-domain Recommendation. Different domains in cross-domain recommendation can be considered as different views. For cross-domain recommendation, two types of contrastive tasks can be conducted, i.e., single-domain contrasting and cross-domain contrasting. CCDR [115] performs both single-domain contrasting and cross-domain contrasting. For the single-domain contrasting, it samples two subgraphs of each node to generate two node representations. Then the MI between these two representations is maximized. For the cross-domain contrasting, It maximizes the MI between the representations of the same node in the source domain and the target domain. Besides, to extract more cross-domain knowledge between unaligned nodes, J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:20 M. Jing et al. CCDR maximizes the MI between the representation of an aligned node in the source domain and the representations of its target-domain neighbors. ML-SAT [142] studies the multi-scenario problem, which can be viewed as a multi-domain recommendation problem. Moreover, it only performs the cross-domain contrasting between two different scenarios. It treats the representations of the same users/items in different domains as positive pairs and representations of other users/items in both scenarios as negative pairs. DR-MTCDR [31] only performs sing-domain contrasting. It augments the user-item graph in each domain by edge/node dropout. For each domain, it generates 𝐾 channel node representations. For each channel, MI between representations of the same node is maximized. Group Recommendation. Group recommendation aims to recommend items that can fulfill the preferences of a collective of users. To improve group recommendation performance, S 2 -HHGR [138] builds a hierarchical hypergraph based on the user-item, group-item, and user-group interactions. It applies double-scale node dropout strategies including coarse- and fine-grained dropout on the hypergraph. The former drops users in all groups. The latter only drops some nodes in a specific group while other groups still contain these users. It performs node self-discrimination on the coarse- and fine-grained user representations as follows:  where h ′ 𝑢 and h ′′ 𝑢 are the coarse-grained user representation and fine-grained user representation, respectively. 𝑛 is the number of negative samples. SGGCF [52] performs user node dropout and edge dropout on the user-item-group graph. Moreover, representations of the same nodes in original and augmented graphs are viewed as positive pairs. Besides, it performs cross-layer contrasting. The MI between initial node embedding and 𝑙 -th ( 𝑙 is an even number) layer embedding is maximized. Sequential Recommendation. DCRec [123] constructs sequential and collaborative views based on interaction sequences. It builds an item transition graph and an item co-interaction graph as collaborative views. Additionally, DCRec adjusts the strength of contrastive regularization by disentangling user conformity and actual interest, further enhancing its performance, which is formulated as:  where 𝑠 𝑢 is the interaction sequence of user 𝑢 . h 𝑠 𝑖 , h 𝑡 𝑖 , h 𝑐 𝑖 are representations of item 𝑖 in the sequence view, item transition graph and item co-interaction graph, respectively. 𝜔 𝑢,𝑖 is the conformity degree of ( 𝑢, 𝑖 ) . Session-based Recommendation. Based on the session data, COTREC [112] constructs two graph views, including item view (item-item graph) and session view (session-session graph). Inspired by SEPT, it utilizes co-training [14] to predict the positive and negative samples for each session. Note that the positive and negative samples are items. Furthermore, it maximizes the agreement between the representation of the last item in the session and representations of the predicted positive samples. The agreement between the representation of the last item and representations of the negative samples is minimized. J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. Contrastive Self-supervised Learning in Recommender Systems: A Survey 111:21 Bunndle Recommendation. Based on user-item interaction, user-bundle interaction, and item-bundle affiliation, CrossCBR [63] constructs a bundle view (user-bundle graph) and an item view (user-item graph and bundle-item graph). It performs edge dropout and message dropout on these views and generates user representation and bundle representations. Moreover, MI between representations of the same user/bundle in corresponding views is maximized. 5.1.2 Contextual-Contextual Contrasting. For contextual-contextual contrasting, discrimination is performed on the contextual representations. It can be formulated as:  where c 𝑖 and c 𝑗 are contextual representations denoting data with similar contextual information. HIN-based Recommendation. CHEST [92] extracts subgraphs from heterogeneous information networks (HIN) by considering path relevance and then applies data-based augmentations to them. Augmented subgraphs generated from the same original subgraphs are considered positive samples, while subgraphs connecting the same user to other items are considered negative samples. Furthermore, it employs generative pretext tasks, such as masked node/edge prediction, to capture local information. The model leverages curriculum learning [4] by pre-training in a progressive manner, with contrastive pretext tasks serving as the advanced course and generative pretext tasks as the elementary course. KGIC [147] constructs local and non-local graphs by combining user-item interactions and the knowledge graph. Moreover, intra-view contrasting and inter-view contrasting are performed among these graphs. Multi-behavior Recommendation. Tocapture different user behavior patterns, HMG-CR [121] performs the hyper meta-graph discrimination task. It first constructs different hyper meta-graphs {G ( 𝑖 ) 𝑢 } 𝐾 𝑖 = 1 for each user based on hyper meta-paths. In specific, the hyper meta-path is constructed based on the distance to the target behavior. Then representations of these hyper meta-graphs are generated through different encoders, i.e., c ( 𝑖 ) 𝑢 = 𝑓 𝑖 𝜃 (G ( 𝑖 ) 𝑢 ) . Furthermore, it treats the representations of adjacent hyper meta-graphs ( c ( 𝑖 -1 ) 𝑢 , c ( 𝑖 ) 𝑢 ) as negative pairs. The positive samples are generated by feeding the current hyper meta-graph into the encoder of the adjacent hyper meta-graph. For example, the positive sample of c ( 𝑖 ) 𝑢 is c 𝑝 = 𝑓 𝑖 -1 𝜃 (G ( 𝑖 ) 𝑢 ) . Sequential Recommendation. GCL4SR [141] obtains subgraphs based on uniform sampling. Specifically, it first constructs a transition graph based on all user interaction sequences. For each sequence node, it randomly samples two different subgraphs. The subgraphs of the same sequence are positive pairs, and those of different sequences are viewed as negative pairs. Unlike other works that perform contrasting on the graph, MISS [30] performs it on the feature vectors, which consist of categorical and sequential features. It leverages two CNN-based models to extract multiple interests contained in each feature vector. Moreover, it makes representations of the same interest similar and representations of different interests dissimilar. 5.1.3 Global-Global Contrasting. Methods under this category discriminate the global representations, which can be represented as:  where g 𝑖 and g 𝑗 are the global representation. Moreover, global-global contrasting is typically used in sequential recommendation and session-based recommendation, where g represents a sequence or a session. J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:22 M. Jing et al. Sequential Recommendation. We introduce the methods for sequential recommendation according to the view generation strategies. Generally, existing methods adopt data-based augmentation and model-based augmentation. (i) Methods using Data-based Augmentation. CL4SRec [116] utilizes three types of sequence-based augmentation: sequence cropping, sequence shuffling, and item masking. Given an interaction sequence of user 𝑢 , it applies augmentation T that is randomly sampled from three augmentations to generate different sequence views. ˜ 𝑠 ( 1 ) 𝑢 = T( 𝑠 𝑢 ) and ˜ 𝑠 ( 2 ) 𝑢 = T( 𝑠 𝑢 ) . SASRec [48] is used as sequence encoder 𝑓 𝜃 (·) to generate sequence (global-level) representations g ( 1 ) 𝑢 = 𝑓 𝜃 ( ˜ 𝑠 ( 1 ) 𝑢 ) and g ( 2 ) 𝑢 = 𝑓 𝜃 ( ˜ 𝑠 ( 2 ) 𝑢 ) . It performs the sequence self-discrimination task. Specifically, it makes the representations of augmented sequences from the same sequence (i.e., positive pairs) to be similar and those from different sequences (i.e., negative pairs) to be dissimilar.  where 𝑝 𝜔 (·) is the cosine similarity with temperature parameter 𝜏 . CL4SRec adopts in-batch negative sampling and the 𝑁𝑒𝑔 is defined as  Similar to CL4SRec, H 2 SeqRec [54] contrasts sequence representations. Moreover, the model is pre-trained with contrastive tasks. CoSeRec [59] and ContraRec [89] adopt the same framework and objective as CL4SRec. Moreover, CoSeRec proposes two robust augmentation strategies, i.e., item substituting and item inserting . In addition to the augmented sequence from the same sequence, ContraRec also treats the sequences that have the same target item as positive samples. Based on CoseRec, TiCoseRec [21] proposes five data augmentation based on time intervals to generate uniform sequences. Moreover, a uniform (un-uniform) sequence is one in which the standard deviation value of its time interval series is relatively small (large). IOCRec [53] generates 𝐾 intention representations for each augmented sequence. It maximizes MI between representations of the same sequence with the same intent, which can be formulated as:  where { g ( 1 ) 𝑢,𝑘 } 𝐾 𝑘 = 1 and { g ( 2 ) 𝑢,𝑘 } 𝐾 𝑘 = 1 denote the intention representations of augmentation sequences. 𝑝 𝜔 (·) is the dot product. N is the set of negative samples, including different intention representations of the same user and all intention representations of different users. MCCM [93] incorporates contrastive learning into news recommendation. It augments sequences by performing item masking/substituting. Moreover, the masking/substituting probability is calculated based on the frequency of news.  where 𝑝 max and 𝑝 min are the predefined boundaries of the probability. count ( 𝑖 ) is the frequency of news 𝑖 in the dataset. I is the news set. It performs sequence self-discrimination similar to CL4SRec. CCL [5] augments sequences by leveraging a data generator based on the mask-and-fill operation. In specific, it first masks a portion of items in each sequence. Then the data generator recovers the original sequence. The original sequence and recovered sequences from the same user are treated as J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. Contrastive Self-supervised Learning in Recommender Systems: A Survey 111:23 positive pairs and other recovered sequences from different users are negative samples. Moreover, CCL utilizes curriculum learning to conduct contrastive learning via an easy-to-difficult process. In addition to interaction data, MIC [66] utilizes attributes of user/items. It constructs user/item sequences, which consist of user/item attributes and interaction records. Then, feature dropout is applied to generate different user/item sequences. Moreover, it treats k-nearest neighbors of the user/item as positive samples. It also clusters the users/items using k-means++ and treats users/items that are from different clusters as negative samples. EC4SRec [94] further incorporates explanation methods into data-based augmentation. It obtains the importance scores of items to guide the augmentation based on explanation methods. The importance scores of items in sequence 𝑠 𝑢 are calculated as  where 𝐹 𝑒 is any explanation method. 𝑓 𝜃 is the sequence encoder. 𝑦 𝑢 is the prediction probability for the next item. 𝑠𝑐𝑜𝑟𝑒 ( 𝑠 𝑢 ) = [ 𝑠𝑐𝑜𝑟𝑒 ( 𝑖 𝑢, 1 ) , · · · , 𝑠𝑐𝑜𝑟𝑒 ( 𝑖 𝑢, | 𝑠 𝑢 | )] and 𝑠𝑐𝑜𝑟𝑒 ( 𝑖 𝑢, 1 ) is the important score of item 𝑖 𝑢, 1. Then, it crops/masks/shuffles the items with the lowest scores to generate positive samples. It also adopts supervised positive sampling to sample sequences like ContraRec and takes the sequences with higher important scores among them as the positive samples. To generate negative samples, it masks the items with the highest scores. The cropped items also form negative samples. In addition to making positive samples from the same users similar, it also makes negative samples from different users more similar than positive samples from all users. (ii) Methods using Model-based Augmentation. DuoRec [73] obtains different sequence representations based on message dropout. Specifically, the sequence is fed twice with different dropout masks in the Transformer-based encoder. Then it performs sequence self-discrimination on these representations. Similar to ContraRec, DuoRec also uses supervised positive sampling, where an interaction sequence with the same target item is randomly selected as a positive sample. Several methods also apply dropout to generate different sequence representations. Besides message dropout, CBiT [23] also uses item masking. Specifically, it first generates 𝐾 different sequences by item masking. Then, these sequences are fed into bidirectional Transformers with different dropout masks to generate representations. In addition, CBiT proposes multi-pair contrastive learning. All 𝐾 augmented sequences are treated as positive samples. It defined the loss as  where N is the set of negative samples, which are the sequence representations of different users. ContrastVAE [97] incorporates contrastive learning into the Variational AutoEncoder. Besides message dropout and data-based augmentations, it utilizes the variational dropout that introduces a learnable Gaussian dropout rate during the sampling step. The selection of negative and positive pairs is similar to CL4SRec. CLUE [18] also adopts both message dropout and sequence-based augmentations. The main difference is that CLUE does not use negative samples. FDSA_CL [37] constructs feature sequences consisting of item features. It maximizes the MI between the representations of feature sequences and interaction sequences. In addition, it applies message dropout to generate 𝐾 representations for each feature sequence. EMKD [24] combines contrastive knowledge distillation with ensemble modeling. Specifically, it first generates 𝐾 sequences by item masking. Then, it uses 𝑁 networks that have the same structure but different initializations as an ensemble of sequence encoders. Each parallel network is a bidirectional Transformer encoder. EMKD conducts both intra- and inter-network contrasting. The intra-network contrasting is conducted by maximizing the MI between representations of the J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:24 M. Jing et al. original sequence and the augmented sequence generated by the same network. The inter-network contrasting is conducted by maximizing the MI between sequence representations generated by the different networks. MCLRec [71] utilizes a learnable model-based augmentation method for view generation. It employs two different MLP-based augmenters to obtain representations to perform contrastive learning. Specifically, it first generates two sequences using data-based augmentation methods such as item masking and obtains their original representations through a shared encoder 𝑓 𝜃 (·) . Then, these representations are fed into the augmenters to obtain augmented representations. MCLRec consequently contrasts the four representations. Additionally, it performs a meta-learning strategy to train the augmenters. Session-based Recommendation. DHCN [113] constructs two hypergraphs (i.e., G ℎ and G 𝑙 ) to represent intra- and inter-session information, respectively. In DHCN, representations of the same session (e.g., g ℎ 𝑠 and g 𝑙 𝑠 ) is the positive pair. Moreover, it perturbs the representations matrix H ℎ with row- and column-wise shuffling. The negative samples are the perturbed representation ˜ g ℎ 𝑠 ∈ ˜ H of the same session. ˜ H is the perturbed representation matrix. It maximizes MI between positive pairs and minimizes MI between negative pairs by  where 𝑝 𝜔 (·) is the dot product and 𝜎 (·) is the sigmoid function. OD-Rec [114] incorporates contrastive learning into knowledge distillation for session-based recommendation. Specifically, it maximizes the mutual information between the representations of the same session 𝑠 that learned from the teacher model and student model (i.e., g 𝑡𝑒𝑎 𝑠 , and g 𝑠𝑡𝑢 𝑠 ). The negative pairs are the ( g 𝑡𝑒𝑎 𝑠 , g 𝑡𝑒𝑎 𝑠 ′ ) . CGL [69] constructs a global graph based on the similarity of sessions. In the graph, each session node is connected with their 𝑀 most similar sessions. It maximizes the MI between a session node and its neighbors and minimizes the MI of unconnected sessions. Moreover, the session representation is obtained by aggregating the representations of items in it. Feature-based Recommendation. CFM [129] adopts feature dropout to augment feature vectors. The vectors generated from the same feature vector are treated as positive pairs, while those generated from different feature vectors are treated as negative pairs. Moreover, to make the pretext task more difficult, it masks/drops the related features. CL4CTR [90] performs feature-based augmentation strategies to generate two different feature vectors of each sample. It minimizes the expected distance between representations of the same samples and does not use negative samples. Besides using feature representations, CLCRec [103] also generates item collaborative representations based on the interaction data. Therefore, it can be viewed as hybrid recommendation [1]. The feature representation and collaborative representation of the same item are treated as positive pairs and those of different items are treated as negative pairs.",
  "5.2 Cross-Scale Contrasting": "In the cross-scale contrasting (shown in Fig.9), the contrasting is conducted across different scales. Furthermore, according to the scale, we further divide this branch of methods into three sub-types: local-contextual (L-C) contrasting, local-global (L-G) contrasting, and contextual-global (C-G) contrasting. 5.2.1 Local-Contextual Contrasting. The local-contextual contrasting can be formulated as:  where h 𝑖 is the local representation and c 𝑗 is the contextual representation. J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. Contrastive Self-supervised Learning in Recommender Systems: A Survey 111:25 Fig. 9. Illustration of cross-scale contrasting. L-C Contrasting L-G Contrasting C-G Contrasting Graph-based Collaborative Filtering. To capture contextual information, NCL [55] proposes a prototype-contrastive objective. In specific, for each item/user, the positive sample is the prototype of the cluster it belongs to, and the negative sample is the prototype of other clusters. The prototype is the representation of the cluster center. Moreover, the prototype-contrastive objective is learned with Expectation-Maximization (EM) algorithm. NCL also performs cross-layer contrasting. For each user/item, corresponding representations output from the even-numbered layer GNN are treated as positive samples. Sequential Recommendation. ICL [16] relies on a framework similar to that of NCL [55]. The difference is that NCL clusters the representations of users or items, while ICL clusters the representations of sequences. Specifically, in ICL, the representation of a sequence can be viewed as a local view of the cluster it belongs to. In addition, each prototype represents the user intent. Besides, ICL also places contrasting between sequences like sequential recommendation methods in global-global contrasting. Cross-domain Recommendation. SITN [80] only performs cross-domain contrasting. Like ICL, it clusters user sequence representations to represent the user interests and maximizes the MI between the representations of users and clusters. Specifically, the positive pairs are representations of a user and its corresponding cluster in different domains. The negative samples are new clusters. Additionally, SITN also maximizes the MI of user representations in different domains. Social Recommendation. In addition to using clustering algorithms, contextual information is also modeled based on human prior knowledge. MHCN [133] designs three types of triangle motifs based on social relations and models them with a multi-channel hypergraph encoder. Moreover, a multi-channel hypergraph is proposed to model the information. In each channel, MHCN hierarchically maximizes the mutual information between the user representation, the user-centered sub-hypergraph representation, and the hypergraph representation. SMIN [62] constructs the context by generating a substructure-aware adjacent matrix based on the addition operations of different order adjacent matrices. Group Recommendation. CubeRec [15] maximizes the MI between the representation of group intersection and the representations of users belonging to that intersection. Specifically, it generates a hypercube representation for each group. The group intersection representations can be viewed as contextual representations. It considers the representations of users within the intersection and the representation of the intersection as positive pairs. As such, CubeRec enhances the representations, allowing for better modeling of the common interests among different groups. J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:26 M. Jing et al. 5.2.2 Local-Global Contrasting. This branch of methods conducts contrastive tasks between the local representation and global representation, which can be presented as  where h 𝑖 is the local representation and g 𝑗 is the global representation. Graph-based Collaborative Filtering. EGLN [125] places the contrasting across the edge representation (i.e., the concatenation of representations of its connected nodes) and the global graph representations (i.e., the average of all edge representations). Specifically, the positive samples of g 1 (representation of G ( 1 ) ) are the edge representations in the graph G ( 1 ) and the negative samples are the edge representations in the augmented graph G ( 2 ) . HGCL [6] constructs node-type specific homogeneous graphs to preserve the heterogeneity. Following DGI [88], it maximizes the MI between a node representation and corresponding graph presentation. Moreover, to incorporate the relationship between different node types, HGCL also designs a cross-type contrasting object. For each node type pair ( 𝑡 1 , 𝑡 2 ) , given the node-type specific homogeneous graph G ( 𝑡 2 ) , the positive samples of it are the node representations in G ( 𝑡 2 ) , and the negative samples are the node representation in the augmented graph of G ( 𝑡 1 ) . Group Recommendation. For group recommendation, there are typically two views: the user view and the group view, to perform contrastive tasks. GroupIM [75] maximizes the MI between the representations of group members and the representations of groups. In specific, it treats the user representation h 𝑖 and group representation g 𝑗 as positive pairs, where user 𝑖 belongs to group 𝑗 . Negative samples h ˜ 𝑢 are sampled from non-member user representations. Moreover, GroupIM introduces a preference-biased negative user sampling distribution PN( ˜ 𝑢 | 𝑗 ) . This distribution gives a higher likelihood to non-member users who have interacted with similar items as the target group 𝑗 . The sampling distribution is defined as:  where x ˜ 𝑢 and x 𝑗 are the interacted items of user ˜ 𝑢 and group 𝑗 , respectively. I(·) is the indicator function. 𝜂 is the hyperparameter controlling the sampling bias, and |U| is the number of users. 5.2.3 Contextual-Global Contrasting. These methods contrast the contextual representation with global representation, which can be defined as:  Graph-based Collaborative Filtering. For each edge ( 𝑖, 𝑗 ) , BiGI [9] performs ego-net sampling to get two subgraphs centered at 𝑖 and 𝑗 , respectively. Then, it adopts attention mechanism to obtain two contextual representations. The contextual representation of this edge s 𝑖 𝑗 is the concatenation of contextual representations of 𝑖 and 𝑗 . Specifically, positive samples of g 1 are the contextual representations of edges in G ( 1 ) and negative samples are the contextual representation of edges in the augmented graph G ( 2 ) . MMSSL [102] generates multiple modal-specific representations of users. Moreover, it maximizes MI between the modality-specific representation and the overall representation of the same user. The negative samples of a modality-specific representation c 𝑚 𝑢 are both the modality-specific representation c 𝑚 𝑢 ′ and overall representation g 𝑚 𝑢 ′ of different users. Cross-domain Recommendation. C 2 DSR [8] obtains cross-domain sequences through merging single-domain sequences in chronological order. Then, it generates two augmented cross-domain sequences based on item substituting. Based on the self-attention mechanism, it generates item representations in the sequences. The representations of cross-domain sequences are obtained by J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. Contrastive Self-supervised Learning in Recommender Systems: A Survey 111:27 \" \" Table 4. Comparison between different pretext tasks. aggregating the representations of items in each domain. Similarly, the representations of singledomain sequences are obtained. It maximizes the MI between the single-domain sequences and the original cross-domain sequences and minimizes the MI between the single-domain sequences and the augmented cross-domain sequences. Sequential Recommendation. SSI [136] leverages contrastive learning to capture global consistency in sequential recommendation. Specifically, it samples a subsequence from the interaction sequence and masks the corresponding items in that sequence. It then maximizes the MI between the representation of the subsequence and that of the entire sequence. Negative samples are subsequences sampled from other sequences. SESRec [77] contrasts the recommendation sequence with the search sequence. It divides each sequence into a positive subsequence and a negative subsequence. Moreover, subsequences are generated based on similarity scores, which are obtained through the co-attention technique [118]. The higher-scoring elements of the sequence are assigned to the positive subsequence while the lower-scoring elements are assigned to the negative subsequence. Anchor is generated from the original sequence. It makes anchors similar to positive sequences and different from negative sequences through the triplet loss. Moreover, based on the belonging relationships in interaction sequences, S 3 -Rec [143] and TCPSRec [84] conduct multiple contrastive tasks. Specifically, S 3 -Rec devises four objectives, including sequence-item, sequence-attribute, item-attribute, and sequence-subsequence mutual information maximization. TCPSRec performs item-sequence and item-subsequence contrasting as well as subsequence-subsequence contrasting at both coarse- and fine-grained periodicity levels. In TCPSRec, the subsequences are generated by dividing the interaction sequence when the time interval is greater than a threshold.",
  "5.3 Discussion": "Table.4 shows the comparison between different pretext tasks. Most existing methods utilize the same-scale contrasting, as it is only necessary to generate representations of the same scale. In contrast, cross-scale contrasting requires generating the corresponding representations for all the different scales. Furthermore, since existing CL-based methods usually use the shared encoder to generate representations, methods that adopt cross-scale contrasting usually require an additional module (i.e., summary function) to generate large-scale representations after generating small-scale representations. Take local-global contrasting in the graph-based recommendation as an example, it needs to learn the representation of each node first and then aggregate these representations to generate the graph representation using a readout function. The contextual contrasting also tends to have high complexity, because it needs to design the corresponding strategy (i.e., context extraction) to decide which part of the data to generate the contextual representation. In addition, compared to same-scale contrasting, which usually aims to identify different instances, cross-scale contrasting focuses on modeling the belonging relationship between small and large scales. Considering the complexity, in the cross-scale contrasting, the negative samples are usually J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:28 M. Jing et al. selected from the small-scale representations. Moreover, cross-scale contrasting can introduce more information into the small-scale representations, but this may also introduce more noises, i.e., irrelevant information. The objective of CL-based recommendation is to enhance recommendation performance by incorporating contrastive pretext tasks as auxiliary tasks. Therefore, the choice of pretext tasks depends on the specific recommendation tasks. For instance, in sequential recommendation where the goal is to learn sequence representations, utilizing (sub)sequence-level contrasting will perform better compared to solely relying on item-level contrasting. In recommendation tasks with inherent multiple views like KG-based recommendation, employing both inter- and intra-view contrasting can further enhance model performance [124, 147]. This is because, under the supervision of the CL, auxiliary views such as knowledge graphs can acquire better representations. In addition, we can address common issues by designing the sampling strategy for negative and positive samples. For example, to solve the data noise problem, one approach is to divide the data into noise-free data and noisy data and then treat them as positive and negative data for the original data, respectively [72]. When sampling, focusing more on tailed samples can help mitigate the bias problem [11]. Likely, for the cold-start problem, paying more attention to users/items that have a few interactions can be helpful.",
  "6 CONTRASTIVE OBJECTIVE": "As introduced in Section. 3.1, the contrastive objective is to maximize the mutual information (MI) between different views. Specifically, given representations ( h 𝑖 , h 𝑗 ) of instances ( 𝑖, 𝑗 ) , the MI between them can be represented as:  where 𝐾𝐿 (·) is the Kullback-Leibler (KL) divergence. Contrastive learning aims to maximize the agreement between positive pairs and minimize the agreement between negative pairs. Moreover, the positive pair comes from the joint distribution 𝑃 ( h 𝑖 , h 𝑗 ) and the negative pair comes from the product of marginal distributions 𝑃 ( h 𝑖 ) 𝑃 ( h 𝑗 ) . Depending on whether an estimation of lower-bound of mutual information is provided, we classify the contrastive objective into bound objective and non-bound objective.",
  "6.1 Bound Objective": "As calculating MI directly is difficult, lower-bounds are derived to estimate it [42], such as the Donsker-Varadhan estimator MI 𝐷𝑉 [3, 22], the Jensen-Shannon estimator MI 𝐽 𝑆 [67], and the noise-contrastive estimator (InfoNCE) MI 𝑁𝐶𝐸 [33, 68]. Therefore, MI can be maximized by maximizing the lower-bound. Moreover, of these three estimators, only MI 𝐽 𝑆 and MI 𝑁𝐶𝐸 are currently used for CL-based recommendation. 6.1.1 Jensen-Shannon Estimator. Compared to the DV estimator, the Jensen-Shannon (JS) estimator enables a more efficient estimation of MI. It replaces the Kullback-Leibler divergence with the Jensen-Shannon divergence. The contrastive loss based on it can be defined as  h 𝑖 and h 𝑗 are sampled from distribution 𝑃 , and h ′ 𝑗 is sampled from distribution ˜ 𝑃 . 𝑝 𝜔 (·) is the discriminator (i.e., pretext decoder), which generates the agreement score of h 𝑖 and h 𝑗 . Moreover, there may be a projection head 𝑔 𝜉 (·) in 𝑝 𝜔 (·) , which map representation h 𝑖 to z 𝑖 . Specifically, 𝑔 𝜉 (·) J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. Contrastive Self-supervised Learning in Recommender Systems: A Survey 111:29 can be a linear mapping, MLP, or identical mapping. The 𝑝 𝜔 (·) can be inner product z 𝑇 𝑖 z 𝑗 , the cosine similarity z 𝑇 𝑖 z 𝑗 /(|| z 𝑖 | | | | z 𝑗 | |) , or bi-linear transformation z 𝑇 𝑖 Wz 𝑗 . 6.1.2 InfoNCE Estimator. InfoNCE is the most popular MI lower-bound adopted in CL-based methods for recommendation. The contrastive loss based on it can be formulated as     where 𝐾 is the set of samples that consists of 𝑁 random variables identically and independently distributed from ˜ 𝑃 . Generally, 𝑝 𝜔 (·) is the cosine similarity with a temperature parameter 𝜏 , i.e., 𝑝 𝜔 ( z 𝑖 , z 𝑗 ) = z 𝑖 z 𝑗 / 𝜏 and z 𝑖 = h 𝑖 /|| h 𝑖 | | . This is also known as the NT-Xent [78] loss. In practice, InfoNCE is calculated on a mini-batch B whose size is 𝑁 + 1. Specifically, for each instance 𝑖 in B , the rest 𝑁 instances are considered as negative samples. The loss based on InfoNCE can be",
  "6.2 Non-Bound Objective": "In addition to the lower-bound MI estimators mentioned above, some other objectives are used to optimize contrastive learning, i.e., triplet loss and BYOL loss. However, they have not proven to be the lower-bound of MI and thus minimizing it does not guarantee to maximize mutual information. 6.2.1 Triplet Loss. The triplet loss does not minimize the agreement of the negative pairs but only makes the agreement of the positive pairs greater than that of the negative pairs. It is defined as:  where 𝜖 is the margin value. h 𝑖 and h 𝑗 are sampled from distribution 𝑃 , and h ′ 𝑗 is sampled from ˜ 𝑃 . The discriminator 𝑝 𝜔 can calculated the agreement by 𝑝 𝜔 ( h 𝑖 , h 𝑗 ) = sigmoid ( h 𝑖 , h 𝑗 ) or 𝑝 𝜔 ( h 𝑖 , h 𝑗 ) = | | ( h 𝑖 -h 𝑗 )|| . 6.2.2 BYOL Loss. This objective is proposed by BYOL [28]. It only maximizes the agreement of positive pairs and does not use negative samples. It is defined as:  where h 𝑖 and h 𝑗 are sampled from 𝑃 . 𝑝 𝜓 (·) is an online predictor. As it does not use negative samples to prevent collapse, other designs are needed. For example, BYOL [28] utilizes momentum encoders, stop gradient, etc.",
  "6.3 Discussion": "Table.5 shows the comparison between different contrastive objectives. Among all the contrastive objectives, InfoNCE is the most widely used due to its good performance. Moreover, both InfoNCE and JS estimate MI based on lower-bound, and Poole et al. [70] demonstrates that InfoNCE has a lower variance of the estimated MI than JS. However, InfoNCE requires a large number of negative samples and thus a large batch size during training. This leads to high computational and time complexity. In contrast, JS can achieve better performance when the batch size is small. J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:30 M. Jing et al. Table 5. Comparison between different contrastive objectives. Triplet loss and BYOL loss are also independent of the large batch size. However, they lack theoretical support, i.e. no theory proves that maximizing them will achieve the goal of maximizing mutual information. Moreover, triplet loss just makes the agreement of negative pairs smaller than that of positive pairs. Therefore, selecting informative positive/negative samples that are difficult to discriminate can lead to better performance, whereas using random or easy samples leads to poor performance. Additionally, triplet loss can be sensitive to the choice of margin value, hence it requires careful adjustment. BYOL loss is the most efficient since it does not require negative samples. However, BYOL loss does not contain the uniformity proposed by Wang and Isola [95], which suggests that normalized representations should be uniformly distributed over the unit hypersphere. Hence, it easily encounters the problem of collapse. Therefore, if BYOL loss is used, additional design is usually required to prevent it. Overall, InfoNCE is generally a good choice. If the batch size is limited, JS may be a better alternative. Triplet loss can be adopted when positive and negative pairs should not be absolutely discriminated. For the bias issue, Yu et al. [134] demonstrate that InfoNCE can implicitly alleviate the popularity bias by making representations uniformly distributed across the unit hypersphere. This implies JS and Triplet loss can also partially mitigate the popularity. However, they use much fewer negative samples than InfoNCE, and thus may not mitigate bias as effectively. For the noise issue, re-weighting strategies [79] can be used to assign a higger weight to reliable (noise-free) data in the loss. In addition, there are some contrastive recommendation loss functions such as BC loss [137], SSM loss [82, 105], and CCL loss [64]. However, these works utilize contrastive learning in a supervised manner. In specific, they treat interacted user-item pairs as positive pairs and non-interacted ones as negative pairs. As we focus on contrastive self-supervised learning, we will not discuss these supervised approaches in detail.",
  "7 OPEN ISSUES AND FUTURE DIRECTIONS": "While contrastive learning-based recommendation methods have achieved great success, there are still some open issues. In this section, we discuss these issues and outline some potential future research directions.",
  "7.1 View Generation": "View generation is a key component of CL-based methods. However, unlike in computer vision, where various data augmentation methods (e.g., resize, rotation, color distortion, etc.) are available, the way of generating views for CL-based recommendation is still not well explored. Specifically, most existing CL-based recommendation methods are limited to randomly removing some interactions or disrupting the order of the interaction sequence. Moreover, these methods are often based on intuitive designs and may not be applicable to downstream recommendation tasks [134]. J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. Contrastive Self-supervised Learning in Recommender Systems: A Survey 111:31 Therefore, designing more effective view generation strategies is a promising future direction. Generally, the view generation strategies need to have the following properties: (1) Adaptability, the generated views should be adaptive to different tasks, as different tasks may use different types of data and require different information. (2) Efficiency, view generation strategies should not have high computational or time complexity. Moreover, dynamically updating the augmentation strategy during training is also a promising direction.",
  "7.2 Pretext Task": "By solving pretext tasks, the model acquires the knowledge from data for downstream tasks. Therefore, extracting useful knowledge is an important issue. For example, CGI [100] proposes an information bottleneck-based method that enables representations to capture the minimum sufficient information for the recommended task. However, it is designed for graph-based recommendation and is difficult to apply to other recommendation tasks. Moreover, as different tasks can capture different information, learning with multiple different pretext tasks can further improve recommendation performance. It is also worthwhile to further investigate the adaptive combination of different pretext tasks for the specific recommended task. In contrastive pretext tasks, negative samples are essential, but obtaining informative negative samples is challenging. The commonly used uniform sampling strategy, which obtains negative samples by random sampling, suffers from false negatives. Besides, easy negative samples may degrade the performance of contrastive learning as they provide little information. Therefore, effective negative sampling strategies deserve further investigation. Some works [19, 47] explore this problem in computer vision. However, these methods are specifically designed for image data and are difficult to apply to recommendation methods. Moreover, since current methods require a large number of negative samples, efficient negative sample strategies also need to be explored.",
  "7.3 Contrastive Objective": "Most CL-based recommendation methods use InfoNCE as their objective function due to its simplicity and effectiveness. Although great success has been achieved, two issues need further exploration. First, the measurement of mutual information in InfoNCE is based on KL divergence. Therefore, it suffers from problems stemming from KL divergence (e.g., asymmetrical estimation and unstable training). Hence, better mutual information measurement is required but a few works [25] investigate this issue. Fan et al. [25] propose the Wasserstein discrepancy measurement based on the 2-Wasserstein distance to measure mutual information. In addition, it has only been applied to sequential recommendation, and its applicability to other recommendation tasks needs to be further explored. Second, current methods use mutual information to measure the agreement. However, mutual information has several shortcomings. Besides being hard to estimate, mutual information can also lead to suboptimal representations [87]. Therefore, exploring alternative measures of the agreement, such as V -information proposed by Xu et al. [119], is a promising direction.",
  "7.4 Miscellaneous": "7.4.1 Meeting Real-World Recommendation. Most existing CL-based recommendation models are trained offline. However, in real-world recommendation scenarios, such as online shopping and news recommendation, large-scale interaction data are continuously generated and user preferences are dynamic. Offline trained models may suffer from the problem of information asymmetry as they rely only on historical user interaction data to make recommendations. Hence, exploring online learning strategies in CL-based recommendation to quickly capture dynamic preference trends would be a potential direction. Moreover, conversational recommendation [81, 144] is also proposed to address the information asymmetry. Specifically, the model makes recommendations based on J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:32 M. Jing et al. the multi-turn interaction (e.g., dialogues) with users. By leveraging real-time user feedback, the users' current preferences can be modeled. Combining contrastive learning with conversational recommendation methods would be an interesting direction to explore. In addition, there are various types of data noise in recommender systems, like random clicks and false interactions, which influence the effectiveness of recommendation models. Consequently, recommendation denoising has gained considerable attention. However, current CL-based recommendation methods are mainly implicit denoising. They achieve denoising by learning perturbationinvariant representations to enhance the robustness of the model. Explicit strategies to address the issue of data noise are rarely explored. One potential approach is to construct a denoised data view to perform contrastive learning. Moreover, as denoising may impair recommendation diversity, leveraging contrastive learning to balance diversity and accuracy when denoising can also be a promising direction. For instance, RGCF [85] incorporates contrastive learning into recommendation denoising by maximizing the mutual information between the denoised graph and the diversity graph. Apart from denoising, recommendation debiasing has also been a hot research topic in recent years. DCRec [123] investigates the combination of contrastive learning and debiasing techniques. It addresses the popularity bias by disentangling user conformity and interest to adjust the contrastive regularization strength. Moreover, some existing debiased recommendation models can also be used as backbones in combination with contrastive learning to address the popularity bias issue. For example, UnKD [10], which proposes an unbiased knowledge distillation approach, can be combined with contrastive learning by incorporating contrastive learning into knowledge transfer or by treating partition groups as contrastive views. Additionally, there are various other biases, such as selection bias, and incorporating contrastive learning to mitigate these biases is also worth exploring. Additionally, in recommender systems, data is often multi-modal, including video, text, images, etc. These modalities contain rich information and are useful for improving recommendation performance, particularly when interaction data is sparse. Therefore, deriving useful knowledge from multi-modal data through contrastive learning is a promising direction. However, only a limited number of studies [34, 83, 102] have explored this area. 7.4.2 Learning with Advanced Techniques. With the rapid development of deep learning, many advanced techniques can be used to improve the performance of CL-based recommendation. One such technique is Knowledge Distillation (KD) [41], which is proposed to address the trade-off between high cost and model performance. Typically, KD first trains a large teacher model using the training set. Then a small student model is trained under supervision from the soft labels generated by the teacher model. Thus, there are naturally two views, namely the teacher view and the student view in KD. It can be easily combined with contrastive learning. Recently, several studies [24, 98, 110] have proposed to incorporate contrastive learning into KD and achieved promising recommendation performance. However, these explorations are still at an early stage, and there is still significant research potential in combining KD with contrastive learning. Moreover, some works [112, 132] leverage semi-supervised learning to obtain more supervision signals. In specific, SEPT [132] and COTREC [112] use tri-training and co-training to obtain more informative samples, respectively. CML [101] unifies contrastive learning and meta-learning by capturing meta-knowledge through contrastive learning. CCL [5] incorporates curriculum learning [4] into contrastive learning. Additionally, there are still many new techniques that can be utilized for CL-based recommendation. For example, there are various choices for view generation strategies, contrastive tasks, J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. Contrastive Self-supervised Learning in Recommender Systems: A Survey 111:33 encoders, etc. in CL-based methods. Therefore, it is a promising direction to use Automated Machine Learning (AutoML) [128] to automatically select the appropriate method to reduce human effort.",
  "8 CONCLUSION": "In this survey, we present a comprehensive and systematic review of recent works in contrastive self-supervised learning-based recommendation. We first propose a unified framework and then introduce a taxonomy based on its key components, which include view generation strategy, pretext task, and contrastive objective. For each component, we provide detailed descriptions and discussions to guide the choice of the appropriate method. Finally, we discuss open issues and promising research directions for contrastive self-supervised learning-based recommendation in the future. We hope that this survey can provide both junior and experienced researchers with a comprehensive understanding of contrastive self-supervised learning-based recommendation and inspire future research in this area.",
  "ACKNOWLEDGMENTS": "This research is supported in part by National Science Foundation of China (No. 62072304), Shanghai Municipal Science and Technology Commission (No. 21511104700), the Shanghai East Talents Program, the Oceanic Interdisciplinary Program of Shanghai Jiao Tong University (No. SL2020MS032), and Zhejiang Aoxin Co. Ltd.",
  "REFERENCES": "[1] Gediminas Adomavicius and Alexander Tuzhilin. 2005. Toward the Next Generation of Recommender Systems: A Survey of the State-of-the-Art and Possible Extensions. IEEE Trans. Knowl. Data Eng. 17, 6 (2005), 734-749. [2] Immanuel Bayer, Xiangnan He, Bhargav Kanagal, and Steffen Rendle. 2017. A generic coordinate descent framework for learning from implicit feedback. In Proceedings of the 26th International Conference on World Wide Web . 1341-1350. [3] Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and Devon Hjelm. 2018. Mutual information neural estimation. In International conference on machine learning . PMLR, 531-540. [4] Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. 2009. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning . 41-48. [5] Shuqing Bian, Wayne Xin Zhao, Kun Zhou, Jing Cai, Yancheng He, Cunxiang Yin, and Ji-Rong Wen. 2021. Contrastive curriculum learning for sequential user behavior modeling via data augmentation. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management . 3737-3746. [6] Desheng Cai, Shengsheng Qian, Quan Fang, Jun Hu, Wenkui Ding, and Changsheng Xu. 2023. Heterogeneous Graph Contrastive Learning Network for Personalized Micro-Video Recommendation. IEEE Transactions on Multimedia 25 (2023), 2761-2773. [7] Xuheng Cai, Chao Huang, Lianghao Xia, and Xubin Ren. 2023. LightGCL: Simple Yet Effective Graph Contrastive Learning for Recommendation. In The Eleventh International Conference on Learning Representations, ICLR 2023 . OpenReview.net. [8] Jiangxia Cao, Xin Cong, Jiawei Sheng, Tingwen Liu, and Bin Wang. 2022. Contrastive Cross-Domain Sequential Recommendation. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management, Atlanta, GA, USA, October 17-21, 2022 , Mohammad Al Hasan and Li Xiong (Eds.). ACM, 138-147. [9] Jiangxia Cao, Xixun Lin, Shu Guo, Luchen Liu, Tingwen Liu, and Bin Wang. 2021. Bipartite graph embedding via mutual information maximization. In Proceedings of the 14th ACM International Conference on Web Search and Data Mining . 635-643. [10] Gang Chen, Jiawei Chen, Fuli Feng, Sheng Zhou, and Xiangnan He. 2023. Unbiased Knowledge Distillation for Recommendation. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining, WSDM 2023 . ACM, 976-984. [11] Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan He. 2023. Bias and Debias in Recommender System: A Survey and Future Directions. ACM Trans. Inf. Syst. 41, 3 (2023), 67:1-67:39. [12] Mengru Chen, Chao Huang, Lianghao Xia, Wei Wei, Yong Xu, and Ronghua Luo. 2023. Heterogeneous Graph Contrastive Learning for Recommendation. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining . 544-552. J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:34 M. Jing et al. J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. Contrastive Self-supervised Learning in Recommender Systems: A Survey 111:35 [58] Zhiwei Liu, Yongjun Chen, Jia Li, Man Luo, S Yu Philip, and Caiming Xiong. 2021. Self-supervised Learning for Sequential Recommendation with Model Augmentation. (2021). J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:36 M. Jing et al. J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. Contrastive Self-supervised Learning in Recommender Systems: A Survey 111:37 J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:38 M. Jing et al. J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. Contrastive Self-supervised Learning in Recommender Systems: A Survey 111:39 J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.",
  "keywords_parsed": [
    "None"
  ]
}