{"Differentiable Retrieval Augmentation via Generative Language Modeling for E-commerce Query Intent Classification": "Chenyu Zhao \u2020 zhaochenyu8@jd.com JD.com Yunjiang Jiang yunjiangster@gmail.com JD.com", "\u2020": "Yiming Qiu qiuyiming3@jd.com JD.com Han Zhang zhanghan33@jd.com JD.com", "ABSTRACT": "Retrieval augmentation, which enhances downstream models by a knowledge retriever and an external corpus instead of by merely increasing the number of model parameters, has been successfully applied to many natural language processing (NLP) tasks such as text classification, question answering and so on. However, existing methods that separately or asynchronously train the retriever and downstream model mainly due to the non-differentiability between the two parts, usually lead to degraded performance compared to end-to-end joint training. In this paper, we propose D ifferentiable R etrieval A ugmentation via G enerative l AN guage modeling ( Dragan ), to address this problem by a novel differentiable reformulation. We demonstrate the effectiveness of our proposed method on a challenging NLP task in e-commerce search, namely query intent classification. Both the experimental results and ablation study show that the proposed method significantly and reasonably improves the state-of-the-art baselines on both offline evaluation and online A/B test.", "CCS CONCEPTS": "\u00b7 Information systems \u2192 Information retrieval ; Information retrieval query processing ;", "KEYWORDS": "Retrieval Augmentation; Query Intent Classification; E-commerce", "ACMReference Format:": "Chenyu Zhao \u2020 , Yunjiang Jiang \u2020 , Yiming Qiu, Han Zhang, and Wen-Yun Yang \u2217 . 2023. Differentiable Retrieval Augmentation via Generative Language Modeling for E-commerce Query Intent Classification. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management (CIKM '23), October 21-25, 2023, Birmingham, United Kingdom. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3583780.3615210 \u2020 Both authors contribute equally \u2217 corresponding author Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CIKM '23, October 21-25, 2023, Birmingham, United Kingdom. \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0124-5/23/10...$15.00 https://doi.org/10.1145/3583780.3615210", "Wen-Yun Yang": "wenyun.yang@gmail.com \u2217 JD.com", "1 INTRODUCTION": "In the recent few years, large natural language processing (NLP) models [7, 8, 21, 28, 30] have emerged as a breakthrough approach to many long-standing NLP problems, such as question answering [34, 37], text classification [23, 32], entity extraction [5, 22], semantic retrieval [16, 38, 39] and so on. The number of parameters in those large NLP models have exploded from tens of millions [8], hundreds of millions [30], to a stunning hundreds of billions [25]. However, it is still worth re-thinking if the formidable scale of the models is the final destiny of NLP, or broadly artificial intelligence (AI) [36]. At the same time, some researchers have started another direction of research, namely retrieval augmentation, which essentially utilizes external plugin modules, e.g. , a retrieval module, to increase model capacity, instead of merely increasing the number of model parameters. As two of most representative works, ORQA [19] proposes a static knowledge retriever that can retrieve from external corpus to help question-answering task, and REALM [11] enhances downstream task by language model pre-training with a knowledge retriever. These retrieval augmentation approaches provide more appealing solutions to existing NLP problems, since they allow external corpus to be used as plugin module, which is much more scalable and flexible than model parameters. In this paper, we explore retrieval augmentation for a challenging problem in e-commerce search, namely query intent classification. In a typical e-commerce system, both items and queries are categorized into multi-level hierarchical structure of categories [27] in order to be utilized later in multiple scenarios, such as product search, targeted marketing, inventory management, sales analysis and so on. Despite playing a vital role in search pipeline, query intent classification remains one of the most challenging tasks in e-commerce search, since user queries are usually short, polysemous, and lack of training labels especially for long tail queries. As shown in Table 1, we can see a few query examples that are nearly impossible to classify correctly without leveraging external data sources: 1) query 'lomography' is quite a niche brand in photography, which occurs only a handful of times in the whole websites and zero times in our training data. 2) Query 'BCD-253WDPDU1' and 'huawei HD98SOKA' correspond to specific refrigerator and television models, respectively. These queries are literally meaningless, but they make sense if we can retrieve some items with matched tokens to 'augment ' the query. However, existing retrieval augmentation methods, though with initial successes, still suffer from a few deficiencies: 1) the retrieval index update is non-differentiable, which prevents the model to CIKM '23, October 21-25, 2023, Birmingham, United Kingdom. Chenyu Zhao, Yunjiang Jiang, Yiming Qiu, Han Zhang, & Wen-Yun Yang Table 1: Examples of hard queries that need retrieval augmentation. be trained in end-to-end manner. 2) The model training is too expensive, mostly due to the non-differentiable index building. Thus the index has to be re-built many times during the retrieval model training. 3) Retrieving the whole document seems unnecessary and it introduces more computation burden for downstream models. Thus, as an analogy to human memory, we are very interested in exploring if the retrieval module can just generate some useful 'fragments' instead of the whole document, since we believe that's how our human thinks. To address the above deficiencies, in this paper we propose a differentiable retrieval augmentation model to jointly learn a neural knowledge retriever and a knowledge-augmented classifier for query intent classification.", "2 RELATED WORK": "", "2.1 Query Intent Classification": "Over the last decade, deep learning approaches have been widely applied in query intent classification due to their capability of semantic representation [6, 12], for example, transfer learning [14, 31], external knowledge utilization (such as Wikipedia [15], search content [1] and knowledge base [35]), and retrieval augmentation at a small scale [2]. Recently, BERT [8] based methods significantly improve a lot of NLP tasks, including query classification [6] and document classification [4].", "2.2 Retrieval Augmentation": "Weston et al. [3] first introduces retrieval augmentation technique into dialogue response generation domain, and Li et al. [20] provides a survey about applying retrieval augmentation for text generation from some retrieved references instead of generating from scratch. Recent advances in language model pre-training have shown to be significantly effective in most NLP tasks, including BERT [8], T5 [29] and so on, where the knowledge is encoded implicitly as parameters of the neural network. On the other hand, REALM [11] learns a parameterized retriever to facilitate the main model where the knowledge is encoded explicitly in the external corpus. RECO [17] enhances vision-text models by incorporating a retrieval model to refine their embeddings with cross-modal retrieved information.", "3 METHOD": "Weformulate the query intent classification problem as two sequential tasks: a neural knowledge retriever and a knowledge-augmented classifier . The former task aims to retrieve supplementary knowledge for the query and the latter task achieves the goal of query intent classification based on the augmented knowledge and input query. In this section, we begin by introducing the details of these two tasks, followed by an empirical and novel training technique, i.e. , fragment generation. Finally, we provide a comprehensive overview of the training stages and explain the joint training for the proposed Dragan model.", "3.1 Query Intent Classification": "For a standard classification problem, we take some input \ud835\udc65 and learn a distribution \ud835\udc5d ( \ud835\udc66 | \ud835\udc65 ) over all possible class label \ud835\udc66 . With retrieval augmentation, we have two key components: the neural knowledge retriever , which models the generative probability \ud835\udc5d ( \ud835\udc67 | \ud835\udc65 ) , for some intermediate text \ud835\udc67 , and the knowledge-augmented classifier , which models \ud835\udc5d ( \ud835\udc66 | \ud835\udc67, \ud835\udc65 ) . Thus, the overall log likelihood of \ud835\udc66 given \ud835\udc65 can be defined as follows  Neural Knowledge Retriever. Previously, REALM [11] propose knowledge retrieval as an embedding retrieval problem with inner product as the similarity measure. The results are promising but the indexing part is cumbersome - the embedding index has to be asynchronously updated during the training, because the retrieval is based on inner product and there is no easier way to find \ud835\udc58 nearest neighbors other than rebuilding the embedding index. Here we explore another approach to knowledge augmentation based on text generation by replacing the retrieval probability \ud835\udc5d ( \ud835\udc67 | \ud835\udc65 ) with text generative probability, which consequentially gets rid of the embedding index and its cumbersome updating. Formally, the differentiable retriever can be defined as  where \ud835\udc5b is the length of text \ud835\udc67 , \ud835\udc5d ( \ud835\udc64 \ud835\udc56 + 1 | \ud835\udc64 \ud835\udc56 ) is learned by a transformerbased model, and \ud835\udc5d ( \ud835\udc67 | \ud835\udc65 ) is the text generative probability instead of retrieval probability, which can be learned jointly and synchronously with the following classification step. However, one may notice that it incurs one another challenge - the text generator usually involves arg max operator when decoding tokens, which is nondifferentiable. Next we present a technique to circumvent it. Knowledge-augmented Classifier. With the input \ud835\udc65 and the generated text \ud835\udc67 , we can get the label prediction by the probability \ud835\udc5d ( \ud835\udc66 | \ud835\udc67, \ud835\udc65 ) . Since it is prohibitively expensive to compute the summation in Equation (1) due to the exponential search space of Z , instead we propose to use a subset of the full space \u02dc Z = { \ud835\udc67 1 , \ud835\udc67 2 , \u00b7 \u00b7 \u00b7 , \ud835\udc67 \ud835\udc58 } \u2282 Z to approximate it.  In practice, we use the top\ud835\udc58 set of documents \ud835\udc67 \ud835\udc56 by the generative model for the given query \ud835\udc65 . As \ud835\udc58 \u226a |Z| , it is feasible to calculate the gradient for Equation (3) efficiently by enumerating all \ud835\udc67 \ud835\udc56 in \u02dc Z as follows  Differentiable Retrieval Augmentation via Generative Language Modeling CIKM '23, October 21-25, 2023, Birmingham, United Kingdom. Figure 1: Illustration of the proposed method where three networks work together to produce the final classification result. Input Query Start Position Predictor Neural Knowledge Generator Knowledge-Augmented Classifier Position Synthetic Fragments Predictions e.g. (0,5) [0]: lomography lomo instant aotomat polaroid camera retro red leather limited [5]: camera retro red leather limited edition with three lenses & photography e.g. photography e.g. e.g. lomography Label Classifier Loss Back propagation Forward propagation Input data Models While these approximate gradients may differ significantly from the true gradients \u2207 log \ud835\udc5d ( \ud835\udc66 | \ud835\udc65 ) , Eq. (3) implies that optimizing \u02dc \ud835\udc5d ( \ud835\udc66 | \ud835\udc65 ) approximately optimizes \ud835\udc5d ( \ud835\udc66 | \ud835\udc65 ) as well. Note also that pre-training the generator \ud835\udf03 \ud835\udc54 ensures that the approximation in Eq. (3) holds from the beginning. Pre-training the classifier \ud835\udf03 \ud835\udc50 is less necessary, though in practice it also gives small improvement.", "3.3 Model Training Details": "", "3.2 Fragment Generation": "In practice, straightforward using generative model as retrieval augmentation can be highly inefficient: 1) the sequential decoding process during text generation cannot be parallelized [10]. 2) With full length of e-commerce item title, which is normally around 100 characters [26], the final classification model also suffers from the \ud835\udc42 ( \ud835\udc5b 2 ) complexity of the self attention layer in the transformer models [33]. Besides efficiency, longer text generation can also lead to semantic drift and degeneration [13, 24], affecting downstream classification accuracy. Thus, we come up with the following fragment generation method to replace the full item title generation. Formally, we first choose a set \u02dc S of fragment start positions \ud835\udc60 according to input text \ud835\udc65 by a transformer encoder, which shares same parameters as the generator encoder. Instead of directly obtaining the top\ud835\udc58 start positions, we sample some start positions based on their probabilities. This approach helps us explore long-tail start positions that may be useful for classification. Gumbel-softmax [18] is applied to allow pass-through back-propagation. \ud835\udc5d ( \ud835\udc60 | \ud835\udc65 ) = Softmax ( Encoder ( x )) -log (-log ( U ( 0 , 1 ))) . Next, we generate a fixed number of fragment tokens step by step starting from position \ud835\udc60 . The generation probability \ud835\udc5d ( \ud835\udc67 | \ud835\udc65 ) in Equation (1) can be rewritten as  and the overall log likelihood can be rewritten as  The gradient with respect to generator and classifier parameters can be rewritten similarly, without impact on the differentiability. In practice, we find the fragment generation is good enough for our latency requirements, with better accuracy compared to full text generation (see Experiment, Section 4.3). Our training process involves two stages: pre-training and joint training. In the first stage, we utilize a dataset of user-clicked log (query, title, category) to pre-train the generation model. This model generates fragments of product titles based on the query and a randomly selected start position, on top of which we pre-train the classification model. In the second stage, we jointly fine-tune the generation model with the classification model. Specifically, we first sample start positions based on the predicted probabilities and employ a beam search decoder [9] to predict fragment tokens based on the sampled start positions. In our setting, we choose top-5 start positions, and for each start position, we use beam size of 3, thus resulting in a total of 15 augmented fragments for each query. Finally we concatenate the input query with the augmented fragments for classification. Note that the generation model continues to update throughout the joint training stage. Moreover, to prevent over-fitting, we randomly initialize the last dense layer of pre-trained classification model. Our vocabulary consists of around 9,000 frequently used Chinese characters and word pieces from the English alphabet, along with other special tokens. We do not use the conventional BERT vocabulary of around 20,000 tokens, since smaller vocabulary significantly improves the decoder efficiency and GPU memory usage.", "4 EXPERIMENT": "", "4.1 Setup": "4.1.1 Dataset. As shown in Table 3, we use a publicly available dataset [27] for the sake of reproducibility, which is collected from user click log in a major e-commerce search platform. 4.1.2 Evaluation Metrics. We use precision (P), recall (R), f1 score (F1) as our evaluation metrics, where the precision measures the accuracy of predicted query categories, the recall measures the proportion of correctly predicted categories. 4.1.3 Baseline Methods. Wecompareourproposed method Dragan with the below four baselines: \u00b7 ORQA [19] stands for the model released by Google research, which contains an inverse cloze task(ICT) embedding retrieval model and a downstream model. In our setting, the downstream model is a BERT classifier. Note that the retrieval model does not update in the joint training stage. CIKM '23, October 21-25, 2023, Birmingham, United Kingdom. Chenyu Zhao, Yunjiang Jiang, Yiming Qiu, Han Zhang, & Wen-Yun Yang Table 2: Start positions and generated title fragments of different queries.", "Table 3: Dataset statistics.": "", "Table 5: Comparison between model variants.": "Table 4: Comparative results with baseline methods. * The (x, y) notation stands for x-layer retriever and y-layer classifier. \u00b7 REALM [11] stands for a pre-training method evolved from ORQA model, where the retrieval task is jointly trained with a token prediction task. Here we also apply the same BERT model for classification, as the proposed Dragan method. \u00b7 Query-only stands for the BERT model trained directly on the dataset without any pre-training. \u00b7 RSC [27] stands for the model pre-trained with randomly clipped item titles and fine-tuned on queries.", "4.2 Results": "Table 4 shows the comparative results between the proposed Dragan with the above baseline methods, from where we can observe that our 12-layer Dragan model improves all baselines by a large margin in terms of F1 metrics. Moreover, since we aim at a more practical scenario where we can serve the model online even with CPU machines, we also include the results of Dragan with a 4-layer generator and a 2-layer classifier. Surprisingly, this much smaller Dragan still significantly improves the other baseline methods, especially on long-tail queries.", "4.3 Ablation Studies": "In Table 5, we compare the different variants of the proposed model: 1) full title stands for generating the full item title instead of just fragments introduced in Section 3.1, 2) equal probability stands for using equal probabilities for all fragments, instead of using learnable generation probabilities, 3) fixed generator stands for freezing the generator parameters while training the classifier, which means no joint-training. We can make a few observations from the comparative results: 1) using fragment generation improves full title generation by 2.2% and 2.7% in F1 metric for overall and long-tail evaluation, which may be caused by the noisy tokens in the full item title. Additionally, the inference time of fragment generation is much shorter than the full title generation, which enables us to serve it online. 2) Using learned Figure 2: Start position distribution on overall evaluation. 0 10 20 30 40 50 60 70 Start position 0.00 0.01 0.02 0.03 0.04 0.05 0.06 Frequency(normalized) probability improves the equal probability by 2.3% and 2.5% in F1 metric for overall and long-tail evaluation . 3) Joint training of the generator and classifier significantly improves the fixed generator, especially on long-tail evaluation. This demonstrates the necessity of joint training, enabled by our proposed differentiable retriever. In Figure 2, we present the start position distribution of the overall evaluation set, which shows that most of the start positions are concentrated in the range of 10-30. Table 2 shows that the model tends to predict the positions where the product words are located, thus most generated fragments contain the product words. This is highly intuitive as it aligns with how humans understand a query.", "4.4 Online Performance": "We conduct A/B test on a leading e-commerce search system, using 20% of the entire site traffic during a period of 30 days. The online query intent classification baseline is the RSC [27] model. Due to the confidential business information protection, we only report the relative improvements, where the gross merchandise value (GMV), the number of unique order items per user (UCVR), and the clickthrough rate (CTR) are significantly improved by 0.13%, 0.28% and 0.08%, respectively.", "5 CONCLUSION": "We have introduced a differentiable generator-based retrieval augmentation approach that can be jointly trained with any downstream tasks, which fixes a major weakness in existing retrieval augmentation methods. We apply the proposed method to an ecommerce query intent classification problem, and achieve significant improvements over previous methods for both offline and online evaluations. At last, we want to emphasize that the proposed approach is general enough to be applied to other NLP tasks. Differentiable Retrieval Augmentation via Generative Language Modeling", "REFERENCES": "[1] Azin Ashkan, Charles LA Clarke, Eugene Agichtein, and Qi Guo. 2009. Classifying and characterizing query intent. In European conference on information retrieval . Springer, 578-586. [2] Andrei Z Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, and Tong Zhang. 2007. Robust classification of rare queries using web knowledge. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval . 231-238. [3] Deng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xiaojiang Liu, and Shuming Shi. 2019. Retrieval-guided dialogue response generation via a matching-to-generation framework. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) . 1866-1875. [4] Fengyu Cai, Wanhao Zhou, Fei Mi, and Boi Faltings. 2021. SLIM: Explicit SlotIntent Mapping with BERT for Joint Multi-Intent Detection and Slot Filling. arXiv preprint arXiv:2108.11711 (2021). [5] Yuan Chang, Lei Kong, Kejia Jia, and Qinglei Meng. 2021. Chinese named entity recognition method based on BERT. In 2021 IEEE International Conference on Data Science and Computer Application (ICDSCA) . IEEE, 294-299. [6] Qian Chen, Zhu Zhuo, and Wen Wang. 2019. Bert for joint intent classification and slot filling. arXiv preprint arXiv:1902.10909 (2019). [7] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 (2022). [8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018). [9] Markus Freitag and Yaser Al-Onaizan. 2017. Beam search strategies for neural machine translation. arXiv preprint arXiv:1702.01806 (2017). [10] Alex Graves. 2013. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850 (2013). [11] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. REALM: Retrieval-augmented language model pre-training. arXiv preprint arXiv:2002.08909 (2020). [12] Homa B Hashemi, Amir Asiaee, and Reiner Kraft. 2016. Query intent detection using convolutional neural networks. In International Conference on Web Search and Data Mining, Workshop on Query Understanding . [13] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751 (2019). [14] Jeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. arXiv preprint arXiv:1801.06146 (2018). [15] Jian Hu, Gang Wang, Fred Lochovsky, Jian-tao Sun, and Zheng Chen. 2009. Understanding user's query intent with wikipedia. In Proceedings of the 18th international conference on World wide web . 471-480. [16] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. 2013. Learning deep structured semantic models for web search using clickthrough data. In Proceedings of the 22nd ACM international conference on Information & Knowledge Management . 2333-2338. [17] Ahmet Iscen, Mathilde Caron, Alireza Fathi, and Cordelia Schmid. 2023. RetrievalEnhanced Contrastive Vision-Text Models. arXiv preprint arXiv:2306.07196 (2023). [18] Eric Jang, Shixiang Gu, and Ben Poole. 2016. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144 (2016). [19] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. arXiv preprint arXiv:1906.00300 (2019). [20] Huayang Li, Yixuan Su, Deng Cai, Yan Wang, and Lemao Liu. 2022. A Survey on Retrieval-Augmented Text Generation. arXiv preprint arXiv:2202.01110 (2022). [21] Junyang Lin, Rui Men, An Yang, Chang Zhou, Ming Ding, Yichang Zhang, Peng Wang, Ang Wang, Le Jiang, Xianyan Jia, et al. 2021. M6: A chinese multimodal pretrainer. arXiv preprint arXiv:2103.00823 (2021). CIKM '23, October 21-25, 2023, Birmingham, United Kingdom. [22] Y. Lin, L. Hong, L. Yi, X. Li, and M. W. Anwar. 2015. Biomedical Named Entity Recognition based on Deep Neutral Network. International Journal of Hybrid Information Technology 8, 8 (2015), 279-288. [23] Zhibin Lu, Pan Du, and Jian-Yun Nie. 2020. VGCN-BERT: augmenting BERT with graph embedding for text classification. In European Conference on Information Retrieval . Springer, 369-382. [24] Stephen Merity, Nitish Shirish Keskar, and Richard Socher. 2017. Regularizing and optimizing LSTM language models. arXiv preprint arXiv:1708.02182 (2017). [25] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL] [26] Yiming Qiu, Kang Zhang, Han Zhang, Songlin Wang, Sulong Xu, Yun Xiao, Bo Long, and Wen-Yun Yang. 2021. Query Rewriting via Cycle-Consistent Translation for E-Commerce Search. In 2021 IEEE 37th International Conference on Data Engineering (ICDE) . IEEE, 2435-2446. [27] Yiming Qiu, Chenyu Zhao, Han Zhang, Jingwei Zhuo, Tianhao Li, Xiaowei Zhang, Songlin Wang, Sulong Xu, Bo Long, and Wen-Yun Yang. 2022. Pre-training Tasks for User Intent Detection and Embedding Retrieval in E-commerce Search. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management . 4424-4428. [28] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training. (2018). [29] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683 (2019). [30] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research 21, 1 (2020), 5485-5551. [31] Michael Skinner and Surya Kallumadi. 2019. E-commerce Query Classification Using Product Taxonomy Mapping: A Transfer Learning Approach.. In eCOM@ SIGIR . [32] Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang. 2019. How to fine-tune bert for text classification?. In China national conference on Chinese computational linguistics . Springer, 194-206. [33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017). [34] Zhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh Nallapati, and Bing Xiang. 2019. Multi-passage BERT: A Globally Normalized BERT Model for Open-domain Question Answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) . 5878-5882. [35] Zhongyuan Wang, Kejun Zhao, Haixun Wang, Xiaofeng Meng, and Ji-Rong Wen. 2015. Query understanding through knowledge-based conceptualization. In Twenty-Fourth International Joint Conference on Artificial Intelligence . [36] Wired. 2023. OpenAI's CEO Says the Age of Giant AI Models Is Already Over. https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giantai-models-is-already-over/. [37] Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li, and Jimmy Lin. 2019. End-to-End Open-Domain Question Answering with BERTserini. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations) . 72-77. [38] Han Zhang, Hongwei Shen, Yiming Qiu, Yunjiang Jiang, Songlin Wang, Sulong Xu, Yun Xiao, Bo Long, and Wen-Yun Yang. 2021. Joint learning of deep retrieval model and product quantization based embedding index. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1718-1722. [39] Han Zhang, Songlin Wang, Kang Zhang, Zhiling Tang, Yunjiang Jiang, Yun Xiao, Weipeng Yan, and Wen-Yun Yang. 2020. Towards personalized and semantic retrieval: An end-to-end solution for e-commerce search via embedding learning. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . 2407-2416."}
