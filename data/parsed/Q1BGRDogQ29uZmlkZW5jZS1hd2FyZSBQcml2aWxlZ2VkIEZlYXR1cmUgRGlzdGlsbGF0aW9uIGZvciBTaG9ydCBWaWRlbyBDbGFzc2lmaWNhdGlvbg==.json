{"CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification": "Jinghao Shi ByteDance San Jose, CA, USA jinghao.shi@bytedance.com Xiang Shen ByteDance Bellevue, WA, USA xiang.shen@bytedance.com Kaili Zhao ByteDance San Jose, CA, USA kaili.zhao@bytedance.com Xuedong Wang ByteDance San Jose, CA, USA xuedong.wang@bytedance.com Vera Wen ByteDance San Jose, CA, USA vera.wen@bytedance.com Yifan Wu ByteDance San Jose, CA, USA yifan.wu@bytedance.com Zixuan Wang ByteDance San Jose, CA, USA zixuan.wang1@bytedance.com Zhixin Zhang ByteDance Beijing, China zhangzhixin.01@bytedance.com", "ABSTRACT": "Dense features, customized for different business scenarios, are essential in short video classification. However, their complexity, specific adaptation requirements, and high computational costs make them resource-intensive and less accessible during online inference. Consequently, these dense features are categorized as 'Privileged Dense Features'. Meanwhile, end-to-end multi-modal models have shown promising results in numerous computer vision tasks. In industrial applications, prioritizing end-to-end multimodal features, can enhance efficiency but often leads to the loss of valuable information from historical privileged dense features. To integrate both features while maintaining efficiency and manageable resource costs, we present Confidence-aware Privileged Feature Distillation (CPFD), which empowers features of an end-toend multi-modal model by adaptively distilling privileged features during training. Unlike existing privileged feature distillation (PFD) methods, which apply uniform weights to all instances during distillation, potentially causing unstable performance across different business scenarios and a notable performance gap between teacher model (Dense Feature enhanced multimodal-model DFX-VLM) and student model (multimodal-model only X-VLM), our CPFD leverages confidence scores derived from the teacher model to adaptively mitigate the performance variance with the student model. We conducted extensive offline experiments on five diverse tasks demonstrating that CPFD improves the video classification F1 score by 6.76% compared with end-to-end multimodal-model (X-VLM) and by 2.31% with vanilla PFD on-average. And it reduces the performance gap by 84.6% and achieves results comparable to Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. https://doi.org/10.1145/3627673.3680045 teacher model DF-X-VLM. The effectiveness of CPFD is further substantiated by online experiments, and our framework has been deployed in production systems for over a dozen models.", "CCS CONCEPTS": "\u00b7 Computing methodologies \u2192 Machine learning ; \u00b7 Information systems \u2192 Data mining .", "KEYWORDS": "Short-form Video Classification; Privileged Feature Distillation; Multi-Modal Classification", "ACMReference Format:": "Jinghao Shi, Xiang Shen, Kaili Zhao, Xuedong Wang, Vera Wen, Zixuan Wang, Yifan Wu, and Zhixin Zhang. 2024. CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management (CIKM '24), October 21-25, 2024, Boise, ID, USA. ACM, New York, NY, USA, 8 pages. https://doi.org/10.1145/3627673.3680045", "1 INTRODUCTION": "Recent years have witnessed the rapid expansion of short video platforms such as TikTok, Youtube Shorts, and Reels. And short video classification is an essential task for short video platforms with a wide range of applications. This task involves tagging videos based on predefined taxonomies, providing insights for user research and being utilized in recommender systems. Additionally, video classification models are instrumental in content moderation, helping to detect inappropriate content. For instance, Qi et al. [12] curated a dataset from multiple short video platforms focusing on fake news and developed a multi-modal fake news detection model. Das et al. [2] created a multi-modal dataset as a benchmark for hate video classification. Binh et al. [1] developed a fusion model to identify inappropriate videos for young children using video titles and metadata. Video classification predicts video category based on a predefined taxonomy and take a sequence of video frames as input. Generally, text information such as title and sticker provides critical information to improve video classification, thus will be integrated to vision features using a multi-modal classification architectures. The performance of the model usually relies on extensive pre-training of foundation models such as CLIP [13], BLIP [7], and X-VLM [20] and followed by fine-tuning with task-specific data. Besides, dense features are also crucial for prediction tasks in industrial applications. These dense features come from different sources. Content-understanding models such as video classification models looking for the existence of specific semantics/objects or text classification models focusing on specific keys words can be one type of source. User interaction features such as likes, shares, and downloads is another type of source. In this work, we enhance the X-VLM model with these dense features to form a Dense Featureenhanced X-VLM (DF-X-VLM) to empower the model. However, these dense features are usually inaccessible or prohibitively expensive to access for online services. For example, when predicting the probability of the video containing inappropriate information, the count of user reports only exists after the video accumulates some views, but not immediately available upon video creation. Additionally, maintaining these historical features developed in the previous iterations require significant resources, including memory space for storing the features as well as computational power for inference. Those features are only available during training time but not available during inference and thus we define them as \"privileged dense features\". On the contrary, features that are available in both training and inference periods such as video frames are defined as non-privileged features. The existence of privileged features could be a severe issue in online machine learning systems at scale. XuandYanget al. [18, 19] proposed privileged feature distillation (PFD) to address the issue under the deep neural network (DNN) model framework involving dense privileged features and dense non-privileged features. The PFD approach distill a student model from a teacher model armed with privileged features. However, prior efforts have not integrated PFD into a multi-modal framework. In response, we introduce PFD in multi-modal classification. We employ X-VLM as a student model to learn from DF-X-VLM as a teacher model to harness the information within the dense features without incurring the cost of using those features during online inference. Conventional PFD methods treat the distillation process uniformly across all instances or adapt solely to the presence of privileged information. However, it is recognized that the performance discrepancies exist across samples due to variations in teacher behavior [23] for general distillation scenario, indicating the distillation performance could be significantly influenced by teacher confidence and thus cause the inherent limitation for conventional previous methods. In the context of distilling multi-modal video classification, we also observe such obvious discrepancies. To be more specific, we found that the performance of student varies a lot in different business tasks especially when the teacher model is empowered with a large number of privileged features. The paper introduces Confidence-aware Privileged Features Distillation for multi-modal video classification, a refined approach to the traditional knowledge distillation process, where the student model not only learns from the output of the teacher model but also leverages additional insights derived from the teacher's confidence levels. The core idea is to condition the distillation process on the teacher's confidence in its prediction and incorporate uncertainty learning. The shift not only facilitates a deeper understanding and utilization of privileged information but also addresses the performance variances and dependencies associated with traditional distillation and privileged feature approaches. We summarize our contributions as follows: \u00b7 We introduce the PFD approach to utilize dense features in a multi-modal classification framework for the first time. \u00b7 We propose the CPFD algorithm to further address the performance discrepancies introduced by traditional PFD approach. \u00b7 We validate the effectiveness of the CPFD approach in production data and deploy the model in production.", "2 RELATED WORK": "", "2.1 Vision-Language Models": "Video classification has been traditionally viewed as a vision-only task which aims to predict the class of a given input sequence of video frames. Recent work utilized additional available text and audio information to construct a multi-modal classification framework and achieve better performance. The video classification model usually follows pre-training fine-tuning paradigm and benefits from pre-training vision-language foundation models. CLIP CLIP [13] represents a groundbreaking milestone in vision-language pretraining. , achieving remarkable success in simultaneously understanding images and text via contrastive learning approach. Following this, there has been a surge in the development of Vision-Language Pretraining (VLP) models [3, 16], which have notably achieved SoTA performance across a range of vision-language benchmarks, such as visual question answering and image captioning assignments.ALBEF [8] incorporates contrastive loss and cross-modal attention to enhance the alignment of image and text representations, facilitating more grounded vision and language representation learning. BLIP [7] proposed multimodal mixture of encoderdecoder and jointed pretrained by 3 tasks on data precessed by captioning and filtering. X-VLM [20] performed multi-grained vision language pre-training to align the visual concepts in the images and the associated text under multi-granularity.", "2.2 Privileged Features Distillation (PFD)": "A privileged feature is one that is available during model training, but not available at test/online time. Such features naturally arise in merchandised recommendation systems; for instance, 'user clicked this item' as a feature is predictive of 'user purchased this item' in the offline data, but is clearly not available during online serving. Privileged features distillation (PFD) refers to a natural idea: train a teacher model using all features (including privileged ones) and then use it to train a student model that does not use the privileged features[9][19]. Lopez-Paz et al. [9] marries the concepts of distillation and privileged information and proposes the initial idea of PFD for the first time. Then Chen et al [18] implemented PFD first time in e-commerce recommendation systems and showcasing how distilling knowledge from privileged features like dwell time can significantly enhance model performance in real-world scenarios. Shuo et al. [19] also explored PFD within the learning-to-rank framework, highlighting its efficacy over traditional methods and analyzing the intricate dynamics between the predictive power of privileged features and student model performance. Similarly, Xiaoqiang et al[4] addresses the challenge of distilling ranking ability for CTR prediction by proposing a calibration-compatible listwise distillation approach. Besides, PFD has also been studied in other areas such as cardinality estimation [11], noisy-label [10] and entity-relation extraction [24] Inspired by all previous works, we also study PFD but in the domain of content moderation and feed quality. We study how privileged features can help contentunderstand and content-moderation models", "2.3 Confidence-aware": "Although there is no previous methodology which shares the exact same definition of Confidence-aware with this project, there are some similar ideas which inspire our method. In Knowledge Distillation, the confidence of a teacher is used to adaptively assign sample-wise reliability for each teacher prediction with the help of ground-truth labels [21]. Similarly, the reliability of teacher predictions is used to identify prime samples for distillation and introduce adaptive weighting [23]. In the noisy label domain, Co-teaching designed a two teacher paradigm and used confidence to select the possible clean labels, letting them teach each other[5]. JoCoR also applied a similar approach to utilize the teacher's confidence/loss to adjust the distillation process [17]. Besides, work in imitation learning used the idea of confidence-aware, though not specific for the purpose of distillation[22]. And Meta-Weight-Net designed a weighting function mapping from training loss to sample weight, which is similar to CPFD where we map teacher loss to weight [15].", "3 METHODOLOGY": "In this section, we first provide an overview of common short video classification methods, then explain the application of Privileged Feature Distillation (PFD) for short video classification, and introduce our proposed Confidence-aware Privileged Feature Distillation (CPFD), showing its advantages over conventional PFD.", "3.1 Video Classification Overview": "Short video classification requires multimodal understanding. In addition to vision content, text information such as title, sticker, ocr and even audio text also plays an critical role in comprehensively understanding videos. In the industry, video classification methodologies are generally categorized into two types: End-to-end multimodal model and Fusion models. End-to-end Multimodal models have been one of the most popular areas in machine learning. The proposal of famous works such as ALBEF, BLIP, X-VLM and so on has greatly boosted the application of Multimodal models in video classification model. In this paper, we employed X-VLM[20] as the model architecture because of its outstanding performance in short video classification. The other type is fusion models which build separate models for each modality to produce dense features with different focus Prediction MLP Fusion Concatl DF Branch MLP (Late Fusion) XVLM (Early Fusion)| Feat [Feat 2 [Feat Images Texts and granularity, and then train fusion models on these dense features. These dense features may focus on understanding content in each modality or specific characteristics, even including noncontent-related features such as \"video view\" (VV) or \"video action rate. \" Below, we elaborate on the exploration of efficient learning for dense and multimodal features, leading to our final proposals: confidence-aware privilege features (CPFD).", "3.2 Dense Feature enhanced Multimodal Model (DF-X-VLM)": "It is a natural idea to integrate the multimodal model with the dense feature to use their joint power and make the model address different challenges across multiple content spaces. We introduce DF-X-VLM. It contains two branchs as illustrated by Fig. 1: one branch utilizes dense features as inputs (DF-Branch) and the other processes sequence of images and text tokens (X-VLM Branch).The DF branch to designed to memorize the bad contents while the X-VLM branch aims to generalize it. In other words, The DF branch delineate a clear decision boundary of the easy cases and the larger capacity X-VLM branch address the remaining hard cases.", "3.3 Privileged Feature Distillation (PFD) for Video Classification": "DF-X-VLM approach is an effective method to boost performance However, we still want to use privilege feature distillation (PFD) to train an end-to-end student X-VLM model from DF-X-VLM teacher model for two main reasons: Privileged Features Distillation Privileged Features Distillation General Distillation For Video Classification (ours) Teacher Student Teacher Student Teacher Student DF-XVLM XVLM Privileged Features Regular Features Privileged Features Regular Features Multimodal Features Dense Features -\ud835\udc5e \ud835\udc60 is the predicted probability distribution generated by the student. (1) Model Performance : The X-VLM and DF branches use different coordinate systems and thus the decision boundary is drawn across different content spaces. Unifying the coordinate system may boost performance. (2) Online Pipeline Effciency Dense features not only lead to more resource consumption, but also result in long chained dependencies which harm online stability. Besides, an end-toend X-VLM model without any dense feature dependencies makes more rapid scaling across different business scenarios. Privileged Features Distillation (PFD) follows a natural distillation strategy: train a 'teacher' model using all features (including privileged ones) and then use it to train a 'student' model that does not use the privileged features. A comparison between General PFD, General Distillation and PFD for video classification is shown in Fig. 2. A privileged feature is one that is available during model training, but not available at test/online time. In the context of multimodal video classification, we define all historical dense features that is expensive to use as \"privileged features\". See more detailed definition in Section 1. For actual implementation, we distill an X-VLM model (student) from a DF-X-VLM model (teacher): \u00b7 The student model will be trained using a paramatrized combination of classification loss and distillation loss, which is a common practice in knowledge distillation: \u00b7 Where: -\ud835\udefc represents the scaling factor balancing two losses -\ud835\udc47 is the temperature parameter -\ud835\udc5e \ud835\udc61 is the soft target probability distribution generated by the teacher. -\ud835\udc66 is the hard label. Note that though we adopt X-VLM and DF-X-VLM as our use case, the PFD framework is suitable for any model architecture. With that said, the teacher and student can be any model architecture and are totally independent of one another.", "3.4 Confidence-aware Privileged Feature Distillation (CPFD)": "Though PFD proves to be an effective method, there are still performance gaps between teacher and student, especially when the teacher model becomes more complicated. Below we analyze the causes of those performance gaps and propose a refined approach to address the gaps. \u00b7 In traditional distillation framework, the student learning is often uniformly influenced by the teacher's outputs across all examples. It does not take into account the varying levels of certainty or confidence the teacher possesses in its own predictions. \u00b7 To analyze how the confidence of the teacher influences the student's behavior, we quantified the teacher's confidence through the distribution of teacher loss associated with different student predictions (Fig. 3 and Table 1). They highlights the varying impact of prediction accuracy on teacher loss, illustrating significant differences between correct and incorrect predictions by the student. We can conclude that the student prone to make mistakes when the teacher loss is high. And it is extremely obvious for false negative cases. 3.4.1 Redesign PFD to Incorporate Teacher Confidence. Based on the above analysis, we propose Confidence-aware Privileged Feature Distillation (CPFD) as shown in Fig 4. CPFD is a refined approach to the traditional knowledge distillation process, where the student model not only learns from the output of the teacher model but also leverages additional insights derived from the teacher's confidence levels. The core idea behind this methodology is to condition the distillation process on how certain the teacher is about its predictions and incorporating uncertainty learning. Student: Correct Label: 0 Student: Correct Label: 1 Mean: 0.0057 Mean: 0.8154 Teacher_loss Teacher_loss Student: Incorrect Label: 0 Student: Incorrect Label: 1 Mean: 0.2298 Mean: 4.1737 Teacher loss Teacher loss The proposed confidence-aware mechanism aims to tailor the student's learning experience by weighting the distillation loss according to the teacher's confidence, thereby mimicking the teacher more closely when it is more certain, and relying more on the ground truth when the teacher's confidence is low. This approach stems from the observation that not all knowledge from the teacher is equally reliable. By modulating the distillation process based on teacher confidence, the student can potentially avoid learning from the teacher's less reliable cues, which may lead to a more robust and accurate student model. The standard loss function for vanilla PFD is as below: The combination of classification loss and distillation loss is a common practice in knowledge distillation. In the context of video understanding, a new angle to interpret the loss combination is to take it as a multi-teacher distillation. \u00b7 The \"distillation teacher\" who is easier to learn from but also tends to have information loss. Moreover, the teacher is not same-level confident in all cases. To be more specific, the teacher can be good at some aspects while bad at others \u00b7 The \"label teacher\" who is hard to learn from but has no information loss. However, this teacher would also suffer from noise-label problems, which is quite common in scenarios of video understanding. And this raises the natural questions to traditional PFD: To what extent we should trust each teacher? And the answer is we should adaptively trust them. Based on this, we adjusted the standard loss function to dynamically control the \ud835\udefc who balance the distillation loss and classification loss. For actual implementation of CPFD, we used teacher's loss to represent confidence, which is used to control the \ud835\udefc , representing how much we would learn from the teacher on this sample. A comparison between CPFD and PFD is shown in Fig. 4 3.4.2 Implementing CPFD. To operationalize this concept, we designed several mapping mechanisms to allow the loss to control the \ud835\udefc . The detailed mapping function and distribution are outlined in Table 2 and its distribution are shown Fig. 5.", "4 EXPERIMENTS": "In this section, we conduct experiments to evaluate both the offline and online A/B performance of the video classification models trained by PFD and CPFD.", "4.1 Experiment Setting": "Data. Since the proposed CPFD mainly focus on the industrial video classification models, we conduct the experiments on real-world industrial datasets. The dataset spans five diverse classification tasks in different domains. For each classification task, we collected 5 to 20 millions of videos in the past 3 months. The labels under different taxonomies are human labeled. We extract 5%-10% data as the evaluation dataset and leave the rest as training dataset based on the available amount of data of each task. Model. As mentioned, we employ X-VLM architecture [20] as our primary model for video classification. The model takes two type of inputs: visual and text. For visual input, we extract and concatenate video frames. For text input, we concatenate the video title with the overlay text within the video. The implementation is modified based on the code base released by X-VLM (https://github.com/zengyan97/X-VLM). Training. Training commences from a in-house checkpoint pretrained on over 5 billion short video data. We start with an initial learning rate of 1e-5, which decays over time. Models are trained on a single 8-A100 node, with training time ranging from 1 to 5 days depending on the data volume. Baselines. To verify the power of the proposed CPFD, Our proposed CPFD is benchmarked against the following models: \u00b7 X-VLM . Normally trained X-VLM model without any distillation. It is basically a reproduction of the paper X-VLM with some minor modification to adapt to business need. \u00b7 DF-X-VLM . Dense Feature enhance X-VLM as described in Section 3.2 and Fig. 1. \u00b7 PFD-X-VLM . A X-VLM student model distilled from teacher DF-X-VLM using vanilla PFD method. To ensure a fair comparison, all methods are implemented on same code base, model starting checkpoint, learning rate, optimizer and other hyper-parameters based on empirical observations.", "4.2 Offline Evaluation Results": "Offline Evaluation Metrics. To show a comprehensive comparison between proposed method and baselines, we compare ROC alpha MAP(loss) Default fixed alpha Mapped adaptive alpha alpha-0.5 loss=le-8 alpha-0.88 Cofident Cofident Sample Sample alpha-0.5 loss=0.8 alpha-0.59 Semi-Cofident Semi-Cofident Sample Sample Unconfident alpha=0.5 Unconfident loss=8 alpha-le-8 Sample Sample Vanilla PFD CPFD 1.0 Thresholding Negative Sigmoid Tanh 0.8 Exponential 0.6 0.4 0.2 0.0 10 Teacher Loss AUC, PR AUC and F1 score. PR AUC is particularly informative for evaluating binary classifiers on imbalanced datasets[14] Task Task1 Task2 Task3 Task4 Task5 Cost Ratio 4.08 3.5 9 4.5 3.92 4.2.1 Performance Comparison. Results are detailed in Table 3, from which we draw two main conclusions: \u00b7 Proposed CPFD consistently outperforms other methods across all metrics and all tasks. Specifically, CPFD improves video classification F1 score by 6.76% over the standard XVLM and by 2.31% over PFD. \u00b7 Relatively, CPFD significantly reduced the performance F1 score gap between X-VLM and DF-X-VLM by 84.6% and that between PFD-X-VLM and DF-X-VLM by 62.2%. CPFD performs very closely with the teacher model DF-X-VLM, indicating minimum information loss removing the DF branch through distillation. 4.2.2 Ablation Study. In this section, we study the influence of different mapping function designs and temperature setting during distillation Mapping Functions. We evaluated four empirically derived mapping functions, as shown in Table 2. The four mapping functions including the thresholds and parameters we provided here are purely empirical, inspired by some popular activation function based on some basic facts: \u00b7 The output range needs to between 0 and 1 \u00b7 The function should be a monotonically increasing function with the loss input. We experimented with all four mapping functions on Task1,2 and 3. he following observations can be drawn based on results in Table 6 \u00b7 There is no definitive best performer, though Exponential Decay functions yields slightly better outcomes due to varying data distributions. \u00b7 The performance of different mapping design is relatively stable and consistently outperforms X-VLM and PFD-X-VLM \u00b7 Mapping functions and their parameters should be selected according to the specific data distribution and problem context. While we present several promising options here, there may be more optimal mapping function designs that could further enhance performance Table 6: Comparison of Model AUC with Different Mapping Functions across Tasks. Temperature. We also ablated the important parameter Temperature during distillation [6]. Due to space limitations, we only display the experimental results for Task1. The results in Table 7. show that T=1 works the best in our setting. So to make a fair comparison, we used 1 as the default temperature across all our experiments. 4.2.3 Computational Cost. We calculate the required online resources to inference the dense features compared with the inference cost of X-VLM models among all 5 different tasks and present the results in Table 4. On average, the DF-X-VLM model spends 5 times more resources compared with the X-VLM model among the 5 tasks while some of the features are counted repeated when used in different tasks. It shows that CPFD is a better option due to comparable classification performance and lower computation resources. In addition to resource consumption, CPFD also significantly improve the online service stability. It liberates the model from the dependency on specific features during online inference, allowing reliance solely on raw inputs such as video, text, or audio and mitigating online-offline model performance difference. Further more, it largely enhanced the model capability by granting the flexibility to leverage a broader range of privileged features during training, without the complications associated with online service and deployment. Lastly, it reduces operational costs and simplifies the expansion of applications.", "4.3 Online Experiments": "We conduct an online A/B experiment to further evaluate the effectiveness of the proposed approach. Specifically, we deployed 4 different models targeting Task 3 in the production system and the models are evaluated in Table 5. We employ the X-VLM model as the control group and conduct the experiment for 5 consecutive days. We set specific model score thresholds for each model and created 4 different model based rules. New videos published with model scores higher than the thresholds will be recalled. Human reviewers will review all the recalled videos and decide if the videos are accurate. We adjust the thresholds to minimize the differences of total videos recalled per day between the baseline model and other models. During the experiment, we evaluate the results by the hit rate as the number of positive videos after human review within recalled videos by the number recalled videos. The result is presented in Table 5. From the result, we could observe PFD X-VLM significantly improves performance compared with X-VLM. CPFD XVLM further improves the performance and achieve same hit rate as the teacher model DF-X-VLM, again indicating almost no information loss during distillation", "5 CONCLUSION": "In this paper, we introduce a Confidence-aware Privileged Feature Distillation (CPFD) approach tailored for video classification. This method effectively utilizes information from privileged dense features without incurring additional inference costs. Experimented on real-world datasets, both offline and online evaluation results consolidate the effectiveness of CPFD. This framework not only enhances classification performance but also maintains operational efficiency. It has been successfully integrated into production systems with multiple deployed models.", "ACKNOWLEDGMENTS": "We extend our gratitude to Zeya Wang for his invaluable assistance during the initial stages of this project. We also thank Zhiqian Chen and Kenan Xiao for their contributions to the offline experiments. Special thanks go to Ardalan Mehrani, Zhongrong Zuo, and Chengkai Jin for their insightful technical discussions.", "REFERENCES": "[1] Le Binh, Rajat Tandon, Chingis Oinar, Jeffrey Liu, Uma Durairaj, Jiani Guo, Spencer Zahabizadeh, Sanjana Ilango, Jeremy Tang, Fred Morstatter, et al. 2022. Samba: Identifying Inappropriate Videos for Young Children on YouTube. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management . 88-97. [2] Mithun Das, Rohit Raj, Punyajoy Saha, Binny Mathew, Manish Gupta, and Animesh Mukherjee. 2023. Hatemm: A multi-modal dataset for hate video classification. In Proceedings of the International AAAI Conference on Web and Social Media , Vol. 17. 1014-1023. [3] Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang Zhu, Pengchuan Zhang, Lu Yuan, Nanyun Peng, et al. 2022. An empirical study of training end-to-end vision-and-language transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 18166-18176. [4] Xiaoqiang Gui, Yueyao Cheng, Xiang-Rong Sheng, Yunfeng Zhao, Guoxian Yu, Shuguang Han, Yuning Jiang, Jian Xu, and Bo Zheng. 2024. Calibrationcompatible Listwise Distillation of Privileged Features for CTR Prediction. In Proceedings of the 17th ACM International Conference on Web Search and Data Mining . 247-256. [5] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. 2018. Co-teaching: Robust training of deep neural networks with extremely noisy labels. Advances in neural information processing systems 31 (2018), 8536-8546. [6] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 (2015). [7] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning . PMLR, 12888-12900. [8] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. 2021. Align before fuse: Vision and language representation learning with momentum distillation. Advances in neural information processing systems 34 (2021), 9694-9705. [9] David Lopez-Paz, L\u00e9on Bottou, Bernhard Sch\u00f6lkopf, and Vladimir Vapnik. 2015. Unifying distillation and privileged information. arXiv preprint arXiv:1511.03643 (2015). [10] Guillermo Ortiz-Jimenez, Mark Collier, Anant Nawalgaria, Alexander Nicholas D'Amour, Jesse Berent, Rodolphe Jenatton, and Efi Kokiopoulou. 2023. When does privileged information explain away label noise?. In International Conference on Machine Learning . PMLR, 26646-26669. [11] Pranav S Page, Anand S Siyote, Vivek S Borkar, and Gaurav S Kasbekar. 2023. Node Cardinality Estimation in the Internet of Things Using Privileged Feature Distillation. arXiv preprint arXiv:2310.18664 (2023). [12] Peng Qi, Yuyan Bu, Juan Cao, Wei Ji, Ruihao Shui, Junbin Xiao, Danding Wang, and Tat-Seng Chua. 2023. Fakesv: A multimodal benchmark with rich social context for fake news detection on short video platforms. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 37. 14444-14452. [13] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning . PMLR, 8748-8763. [14] Takaya Saito and Marc Rehmsmeier. 2015. The precision-recall plot is more informative than the ROC plot when evaluating binary classifiers on imbalanced datasets. PloS one 10, 3 (2015), e0118432. [15] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. 2019. Meta-weight-net: Learning an explicit mapping for sample weighting. Advances in neural information processing systems 32 (2019), 12 pages. [16] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. 2021. SimVLM: Simple Visual Language Model Pretraining with Weak Supervision. In International Conference on Learning Representations . [17] Hongxin Wei, Lei Feng, Xiangyu Chen, and Bo An. 2020. Combating noisy labels by agreement: A joint training method with co-regularization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition . 13726-13735. [18] Chen Xu, Quan Li, Junfeng Ge, Jinyang Gao, Xiaoyong Yang, Changhua Pei, Fei Sun, Jian Wu, Hanxiao Sun, and Wenwu Ou. 2020. Privileged features distillation at taobao recommendations. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 2590-2598. [19] Shuo Yang, Sujay Sanghavi, Holakou Rahmanian, Jan Bakus, and Vishwanathan SVN. 2022. Toward understanding privileged features distillation in learning-torank. Advances in Neural Information Processing Systems 35 (2022), 26658-26670. [20] Yan Zeng, Xinsong Zhang, and Hang Li. 2022. Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts. In International Conference on Machine Learning . PMLR, 25994-26009. [21] Hailin Zhang, Defang Chen, and Can Wang. 2022. Confidence-aware multiteacher knowledge distillation. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) . IEEE, 4498-4502. [22] Songyuan Zhang, Zhangjie Cao, Dorsa Sadigh, and Yanan Sui. 2021. Confidenceaware imitation learning from demonstrations with varying optimality. Advances in Neural Information Processing Systems 34 (2021), 12340-12350. [23] Youcai Zhang, Zhonghao Lan, Yuchen Dai, Fangao Zeng, Yan Bai, Jie Chang, and Yichen Wei. 2020. Prime-aware adaptive distillation. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XIX 16 . Springer, 658-674. [24] Xiaoyan Zhao, Min Yang, Qiang Qu, Ruifeng Xu, and Jieke Li. 2023. Exploring privileged features for relation extraction with contrastive student-teacher learning. IEEE Transactions on Knowledge and Data Engineering 35, 8 (2023), 7953-7965."}
