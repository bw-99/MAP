{"A Model-Agnostic Framework for Recommendation via Interest-aware Item Embeddings": "", "AMIT KUMAR JAISWAL, University of Surrey, United Kingdom": "YU XIONG, University of Surrey, United Kingdom Item representation holds significant importance in recommendation systems, which encompasses domains such as news, retail, and videos. Retrieval and ranking models utilise item representation to capture the user-item relationship based on user behaviours. While existing representation learning methods primarily focus on optimising item-based mechanisms, such as attention and sequential modelling. However, these methods lack a modelling mechanism to directly reflect user interests within the learned item representations. Consequently, these methods may be less effective in capturing user interests indirectly. To address this challenge, we propose a novel Interest-aware Capsule network (IaCN) recommendation model, a model-agnostic framework that directly learns interest-oriented item representations. IaCN serves as an auxiliary task, enabling the joint learning of both item-based and interest-based representations. This framework adopts existing recommendation models without requiring substantial redesign. We evaluate the proposed approach on benchmark datasets, exploring various scenarios involving different deep neural networks, behaviour sequence lengths, and joint learning ratios of interest-oriented item representations. Experimental results demonstrate significant performance enhancements across diverse recommendation models, validating the effectiveness of our approach. CCS Concepts: \u00b7 Information systems \u2192 Recommender systems ; \u00b7 Computing methodologies \u2192 Neural networks . Additional Key Words and Phrases: Recommender System, Capsule Network, Interest-aware, Embedding", "ACMReference Format:": "Amit Kumar Jaiswal and Yu Xiong. 2023. A Model-Agnostic Framework for Recommendation via Interest-aware Item Embeddings. In Seventeenth ACM Conference on Recommender Systems (RecSys '23), September 18-22, 2023, Singapore, Singapore. ACM, New York, NY, USA, 8 pages. https://doi.org/10.1145/3604915.3610660", "1 INTRODUCTION AND PRIOR WORK": "The field of recommendation systems has witnessed significant advancements in recent years, primarily due to the rapid progress of deep learning techniques. Notable successes have been observed in various domains, including news recommendation, e-commerce, video recommendation, and advertisement. In the context of recommendation systems, it is essential to consider user interaction behaviours as they convey valuable information about the user's interests. These behaviours indicate not only the individual items within the sequences but also the user's singular or multiple interests. Users often exhibit diverse interests, engaging with products from distinct categories such as sports, food, and clothing. Identifying these interests becomes challenging as they are embedded within the interactive behaviours, rendering direct capture of such (intrinsic) interests set forth a formidable task. The primary objective in recommendation systems revolves around effectively targeting the diverse interests of varied users. Existing deep learning models focussed on click-through rate (CTR) prediction such as Wide and Deep models [1] combine the training of wide linear models and deep neural networks, thereby harnessing the advantages of both memorisation and generalisation for recommender Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). Manuscript submitted to ACM systems as opposed to self-attention based models [3]. Deep Crossing [14] with follow up deep and cross models [15, 16], Product-network in network (PIN) [12], DeepFM [5], adaptive factorization network [2] and xDeepFM [7] aim to extract low-order and explicit high-order features by incorporating a product layer, and traditional factorization machine module. In the case of deep interest network (DIN) [19], an attention mechanism is employed to enhance the pooling weights of similar items. However, the deep interest evolution network (DIEN) [18] introduces a sequential model to capture the sequential characteristics instead of relying solely on item embeddings as in DIN. DIEN utilises the hidden states of the GRU as inputs for attention and employs attentional update GRU (AUGRU) as a replacement for the conventional attention model. Overall, deep neural networks play a crucial role in depicting user interests based on past user-item interactions [8, 10], utilising item embedding vectors. To address the issue pertaining to the dispersion of user interests, a method known as the dynamic routing capsule network [13] proposes to mitigate the limitations of traditional convolutional neural networks (CNNs), which successfully achieves a more comprehensive understanding of the relationships between objects [4]. Building upon the concept of dynamic routing, [6] propose the multi-interest network with dynamic routing to handle the diverse interests of users in retrieval systems. This method adopts a strategy of representing a single user using multiple vectors, each encoding distinct aspects of the user's diverse interests. By employing this multi-vector representation approach, [6] enhances the capacity to capture and account for the various facets of user interests in the retrieval process. Nonetheless, current representation learning methods primarily concentrate on optimising the item-based mechanism that connects user interactive behaviour sequences with candidate items. Consequently, they overlook the crucial factor of label diffusion within each item present in the user interactive behaviour sequences. This oversight weakens the impact of user interests during the backpropagation phase of the training model. Additionally, as network layers become deeper and the dimension of the embedding layer increases, the introduction of new methods often necessitates the redesign of the entire model architecture or the inclusion of additional datasets or information. Hence, it becomes imperative to develop a method that can enhance model performance while maintaining generalisability, eliminating the need for extensive architectural modifications or the introduction of supplementary information. In this work, a novel framework, Interest-aware Capsule network (IaCN), is proposed that can effectively learn item representations based on user interests. Our proposed framework directly incorporates the concept of user interests to enhance the modelling process. To ensure the generalisability of the framework, an auxiliary task to cater users' multiple interests based on [11] has been proven effective in improving model performance. By jointly learning item-based and interest-based item representations, the framework enables the integration of diverse user interests into the entire model. Consequently, the interest-based item representations generated by IaCN are shared with the original model, thereby enriching the overall information captured by the model. Our Contributions: To summarise, we make the following contributions: \u00b7 We propose a novel framework, the User Interest-aware Capsule network, which aims to facilitate the learning of interest-based item representations. By incorporating IaCN as an auxiliary task, this framework can be seamlessly integrated with existing recommendation models. \u00b7 We propose an approach that combines a joint learning method and hyperparameter optimisation within the IaCN framework. Through extensive experimentation on benchmark datasets, our results demonstrate significant improvements across various existing recommendation models.", "2 THE PROPOSED MODEL": "Within the context of recommendation systems, \ud835\udc56 represents the identification of an item, and the set of user interactive behaviour sequences, represented by S \ud835\udc62 , encompasses the sequences of items that a user \ud835\udc62 has either clicked on or viewed. The symbol \ud835\udc62 \ud835\udc5d corresponds to the user's basic profile information. The identification of a candidate item \ud835\udc56 \ud835\udc61 , while \ud835\udc5f \ud835\udc61 represents the associated information of said candidate item, obtained from the recommendation system. The function \ud835\udc53 ( \ud835\udc56 \ud835\udc61 , \ud835\udc5f \ud835\udc61 ) represents the feature function that captures the characteristics of the candidate item information derived from the combination of the candidate item \ud835\udc56 \ud835\udc61 and the item-related information \ud835\udc5f \ud835\udc61 . The representation of user interests is typically performed to facilitate the learning of the function \ud835\udc53 ( S \ud835\udc62 , \ud835\udc62 \ud835\udc5d ) , which encompasses both the user profile and the user's interactive behaviour sequences. Therefore, the formulation of user interests can be expressed as follows, where I denote the representation vector that is learned from the user information S \ud835\udc62 and \ud835\udc62 \ud835\udc5d . The dimension of the user interest vector is denoted by \u210e , while \ud835\udc3e represents the number of dimensions in the user vector. Specifically, the vector i 1 \ud835\udc62 represents one of the multiple interest vectors of the user, and I represents the collection of these interest vectors. In addition, the representation of candidate items aims to learn the function \ud835\udc53 ( \ud835\udc56 \ud835\udc61 , \ud835\udc5f \ud835\udc61 ) , which involves the item Output Auxilary network MUP Concat 8 Flatien SUM Pooing Interest Caosules Active Unit Active Unt Item Embedd ngs Pooling Layer Pooling Layer Pcoling Layer Pooling Layer Pooling Layer Poolng Layer Pooling Layer Pooling Laycr Pocing Layer Pooling Laycr Pooling Layer Embeddings Embeodings Layer Item Item Item Item Candicate Item Interest-aware Capsule Network Item Context User profile Deep Interest Network id and its associated information. The candidate item embeddings can be obtained through \ud835\udc50 \ud835\udc61 = \ud835\udc53 ( \ud835\udc56 \ud835\udc61 , \ud835\udc5f \ud835\udc61 ) | \ud835\udc50 \ud835\udc61 \u2208 R \u210e , where vector \ud835\udc50 \ud835\udc61 represents the embedding representation learned from the target item id \ud835\udc56 \ud835\udc61 and its corresponding associated information \ud835\udc5f \ud835\udc61 . Specifically, the vector \ud835\udc50 \ud835\udc61 consistently refers to the vector obtained from the embeddings and pooling layer in the Interest-aware Capsule Networks, as discussed in the subsequent section. The purpose of a recommendation system is to promote 'desirable' items to users navigating a website or web application. Consequently, it is essential to establish a scoring mechanism that measures the relationship between a candidate item and the user's interests. This score can be defined as R score (I \ud835\udc62 , \ud835\udc50 \ud835\udc61 ) = \ud835\udc50 \ud835\udc47 \ud835\udc61 i \ud835\udc3e \ud835\udc62 . By calculating the value of R score , we can quantitatively assess the proximity between user interests and candidate items. Ultimately, the recommendation system will utilise the collection of users multiple interests to identify and present the most desirable items to the user.", "2.1 Interest-aware Capsule Network": "To overcome certain limitations associated with convolutional neural networks, capsules have been introduced as an alternative that employs vector-based representations instead of scalars to encode appearance features [13]. This transition enables improved preservation of spatial relationships among complete objects and their constituent parts. Additionally, capsules incorporate the dynamic routing mechanism, which enables the weighing of part contributions to the entire object in a varying manner at each inference step. Users' diverse interests are typically concealed within their interactive behaviour sequences and profile information. Capsules [13] offer a means to encode appearance feature representations by employing vectors instead of scalars, accomplished through the assembly of a group of neurons. Through dynamic routing in capsule networks, the weights of different capsules are learned, enabling the encoding of relationships between parts and wholes. Notably, capsules demonstrate a superior comprehension of object relationships compared to CNNs [4]. In the domain of recommendation systems, multi-interest with dynamic routing network [6] leverages dynamic routing of capsules to automatically capture high-level multiple user interests, resulting in commendable performance within the e-commerce retrieval system. Building upon this foundation, we propose an item representation approach utilising user interest-aware capsule networks (IaCN) to enhance the performance of CTR prediction models. We follow the notion of the dynamic routing mechanism for Behaviour-to-Interest routing [6], employed by capsules for the purpose of learning multi-interest representations based on user profile information and interactive behaviour sequences. The input to each capsule, i \ud835\udc57 = \u2225 \ud835\udc60 \ud835\udc57 \u2225 2 /( 1 + \u2225 \ud835\udc60 \ud835\udc57 \u2225 2 )( \ud835\udc60 \ud835\udc57 /\u2225 \ud835\udc60 \ud835\udc57 \u2225) , here, i \ud835\udc57 represents the output, while \ud835\udc60 \ud835\udc57 encompasses all the inputs of capsule \ud835\udc57 . The formulation of components are, Subsequently, by employing dynamic routing, the aim is to extract high-level abstract interests from the raw user features. The function \ud835\udc4e \ud835\udc56 \ud835\udc57 represents the activation operation (Softmax) applied to the input \ud835\udc4f \ud835\udc56 \ud835\udc57 . The Behaviour to Interest approach selectively combines the user's viewing sequences into vectors representing multiple interests in an adaptive manner. Based on the routing logits ( R score ), the variable \ud835\udc4f \ud835\udc56 \ud835\udc57 can be expressed as, \ud835\udc4f \ud835\udc56 \ud835\udc57 = \ud835\udc62 \ud835\udc47 \ud835\udc57 \ud835\udc40\ud835\udc50 \ud835\udc56 | \ud835\udc56 \u2208 \ud835\udc3c \ud835\udc50 \u2200{ 0 , 1 , . . . , \ud835\udc5b } , \ud835\udc57 \u2208 { 0 , 1 , . . . , \ud835\udc3e } , where \ud835\udc50 \ud835\udc56 \u2208 R \u210e represents an embedding vector of item \ud835\udc56 derived from the user's interactive behaviour sequences. The variable \ud835\udc63\ud835\udc50 \ud835\udc56 denotes one specific item embedding vector from these sequences. Moreover, \ud835\udc62 \ud835\udc57 \u2208 R \u210e refers to the capsule vector representing the user's interests, where \ud835\udc57 ranges from 1 to \ud835\udc3e . The hyperparameter \ud835\udc3e represents the number of user interests. The matrix \ud835\udc40 \u2208 R \u210e \u00d7 \u210e , facilitates the bilinear mapping and serves as a linkage between the user's capsule interests and the viewed sequences. Meanwhile, \ud835\udc4f \ud835\udc56 \ud835\udc57 represents the connection between the user's interest and the item, ensuring they are mapped to the same vector space. While multi-interest dynamic routing network [6] captures multi-interest capsule vectors from user interactive behaviour sequences and profile information, it also introduces label-aware attention using the scaled dot product to assess the relationship between user interests and item information. Within the label-aware attention layer (as shown in Figure 1), the candidate item serves as the query, the user interest capsule acts as the key and value, and the candidate item embedding vector is represented within the interest capsule space. The label-aware attention layer incorporates the item embedding vector along with the item embeddings from the original recommendation model to incorporate user-diverse, interest-based item representations into the primary model. The primary model is specifically designed to accommodate the requirements of the particular task at hand, such as a ranking model (DIN and DIEN). The formulation of the scaled dot product can be expressed as i \ud835\udc62 = I \ud835\udc62 Softmax(pow( I \ud835\udc47 \ud835\udc62 \ud835\udc50 \ud835\udc56 , \ud835\udc5d )) , where \ud835\udc5d refers to the user profile information. As a result, the probability \ud835\udc43 ( \ud835\udc50 \ud835\udc56 | i \ud835\udc62 ) is derived, and the softmax activation function is applied to select the most desirable option. The training loss is formulated as L IaCN = \u02dd \ud835\udc62,\ud835\udc56 log \ud835\udc43 ( \ud835\udc50 \ud835\udc56 | i \ud835\udc62 ) , where L IaCN represents the loss incurred by the user interest-aware capsule network. It is crucial to ensure compatibility between the user's interests and the candidate item. To achieve this, the item embedding vector and the user's interest capsule vector are required to share a common vector space, which is established based on the representation of the user's interests. This alignment is of significant importance when aiming to construct interest-driven item representations.", "2.2 Augmentation of Item Embeddings with User Interests": "The embedding representation, which is based on deep learning, garners significant attention in practical applications [1, 9, 17]. Within the realm of recommendation systems, each specific model utilises its unique method for generating embeddings. As described in Section 1, several approaches have been introduced, focussing on extracting multiple interests represented by item embeddings through the design of network structures. Furthermore, the influence of label diffusion into individual items within user interactive behaviour sequences diminishes the strength of user interests during the backpropagation process of the training model. While dynamic routing capsule networks can address this issue to some extent, integrating them specifically into recommendation models requires complex redesigning of the model architecture, rendering it a challenging and infrequently utilised approach. Based on DIEN [18], the inclusion of an auxiliary task assumes a vital role in enhancing model performance. To avoid the need for redesigning the intricate architecture of the main recommendation model (DIN [19]), an auxiliary task is introduced to facilitate improved learning of item representations. Consequently, we propose a framework known as the User Interest-aware Capsule Network as an auxiliary task. Furthermore, the interest-based item embeddings derived from IaCN are shared with DIN [19]. Based on the formulation of \ud835\udc4f \ud835\udc56 \ud835\udc57 , which employs the scaled dot product to measure the distance among interests and items, the item embedding vector is influenced by the user interest capsule vectors. Additionally, the auxiliary task incorporates the item embeddings expressed through user interest capsules into the main framework by sharing the item embedding vector. Within the main framework (as delineated in Figure 1). The item embeddings are illustrated as it consists of two distinct components, E = E original \u2295 E auxiliary , where the concatenation operator \u2295 is employed to combine the item embedding vectors, denoted as E original and E auxiliary , respectively. The former corresponds to the item embedding vector in the primary target recommendation model task, while the latter is associated with the auxiliary task designed as IaCN. This framework enables the expansion of the original model's item embeddings by incorporating user interest capsules, while maintaining the original model architecture and preserving its inherent properties. As a result, this framework can be seamlessly integrated into a wide range of general recommendation models. The composition of the item embedding vector in the main recommendation model involves a combination of the original model's item embeddings and the auxiliary model's item embeddings. While the original item embeddings remain unaffected by the auxiliary task, the auxiliary item embedding is influenced by both tasks, with this influence being controlled through a hyperparameter. Consequently, the overall loss function for the entire model can be formulated as L = L DIN + \ud835\udf06 L IaCN , where L represents the overall loss of the complete model, L DIN denotes the loss of the main model (deep interest network [19], and L IaCN corresponds to the loss of the user interest-aware capsule network. The hyperparameter \ud835\udf06 is used to fine-tune the balance between the loss and the auxiliary task loss. Given L IaCN , the label-aware attention layer necessitates positive samples for constructing the loss function. Consequently, negative samples corresponding to label items are masked to ensure that the loss function operates effectively during the training of the complete model. Throughout the model training process, the item embedding E receives two components of backpropagating gradients, one from the main model and the other from the auxiliary model. The original item embedding E original solely receives the gradient from the main model \ud835\udc54 \ud835\udc52 main , while the auxiliary item embedding E auxiliary receives gradients from both the auxiliary model (IaCN) \ud835\udc54 \ud835\udc52 main and the main original model ( \ud835\udc54 \ud835\udc52 auxiliary_main ) to facilitate its alignment ( E auxiliary ) with the main model. The update of the auxiliary model gradient is performed according to the illustrated formulation, The selection of the appropriate hyperparameter \ud835\udeff is detailed in the subsequent section.", "3 EXPERIMENTS": "This section presents the experimental evaluation of the user interest-aware capsule network, which is introduced as an auxiliary task. The experiments are conducted using a real-world dataset from Amazon, and we detail the results and analysis in the following paragraphs.", "3.1 Datasets": "We employ the Amazon customer reviews, also known as product reviews dataset. The dataset utilised in this study comprises 192,403 consumer reviews obtained from Amazon electronics products, such as HDMI cables and Bluetooth speakers. A total of 63,001 products from 801 categories were covered, resulting in a dataset containing a total of 1,689,188 samples. The dataset encompasses essential product information, including details about each product, its corresponding category, and a list of viewed products. Another comprehensive collection of consumer reviews from Amazon books, encompassing 603,668 reviews in total are employed. These reviews pertain to 367,982 books, covering a wide range of 1,600 categories. In addition to the consumer reviews, the dataset provides essential product information, including details about the book's category and the list of viewed products.", "3.2 Implementation Details and Results": "We conduct experiments on the Amazon dataset encompassing electronics and books, and compare the performance of our model with other extensively employed baseline models within the industrial domain. Baselines: We employ a set of baseline models for comparison, which are a) Wide and Deep [1], it integrates a deep neural network (DNN) component as the 'Deep' part and a linear model component as the 'Wide' part. This integration aims to leverage the strengths of both models, facilitating a balance between model generalisation and memorisation. b) DIN [19] approach employs an attention mechanism to model the relationship between user interactive behaviour sequences and candidate items, enabling the determination of the contribution made by each item in the historical visiting sequences to the target item. And, c) DIEN [18] follows the architectures of DIN, where this approach incorporates gated recurrent units (GRUs) to effectively capture the evolutionary interests of users, while leveraging attention mechanisms. To explore the performance enhancements, we introduce an auxiliary network (IaCN) that facilitates the extraction of multiple interests from the user's data. In this auxiliary network, we incorporate a layer of dynamic interest capsules, denoted as \ud835\udc3e = [ log 2 | \ud835\udc3c \ud835\udc50 \ud835\udc62 |] to effectively extract the user's interests. The interest-based item embedding vector generated by the IaCN model is shared with the primary recommendation model, and its gradient is updated using a small ratio \ud835\udeff =0.3. The overall loss is composed of both the main model loss and the auxiliary model loss, with the hyperparameter \ud835\udf06 set to 1 to appropriately weight the losses. The Adam optimizer is employed with a learning rate of 1 \ud835\udc52 -4. Each experiment is replicated five times, and the standard deviation (SD) of the model's using the area under the curve (AUC) performance metric is computed to validate the effectiveness of our proposed framework. Table 1 presents the AUC of various baseline models. For the Amazon datasets, the maximum length of the list containing consumer-viewed electronic products is 10, while the maximum length for consumer-viewed books is 20. Notably, the framework incorporating the interest-aware capsule network as an auxiliary component consistently enhanced the performance of all model architectures, highlighting the advantages of this approach. Importantly, the main recommendation model retains its original structure while benefiting from the framework. Moreover, as the length of viewed items increases, the framework exhibits improved efficacy, particularly in the case of longer interactive behaviour sequences ( \ud835\udc59 ). To assess the performance of our proposed framework relative to the benchmark, we conducted experiments considering varying lengths of user-viewed sequences. The experimental results are presented in Table 2, indicating that DIEN with IaCN outperforms the original DIEN model in terms of AUC. Consequently, when dealing with long click sequences from users, the user interests capsule network demonstrates enhanced efficiency and accuracy in extracting diverse user interests, as verified through various experimental setups.", "4 CONCLUSION AND FUTURE WORK": "In this work, we propose a comprehensive framework aimed at improving the performance of recommendation models through the utilisation of interest-based item embedding representations. To achieve this, a novel framework is introduced, which directly learns interest-based item representations by incorporating a user interest-aware capsule network. Furthermore, the user interest-aware capsule network is incorporated as an auxiliary component, facilitating the joint learning of both item-based item representations and interest-based item representations within the framework. Our framework can be readily applied to recommendation model architectures without the need for additional data or significant modifications to the model structure. Furthermore, the framework exhibits a notable impact on longer sequences of viewed items and a wider range of user interests. Through experimental evaluations on industrial datasets, it is evident that incorporating the user interest-aware capsule network as an auxiliary component outperforms other approaches in terms of click-through rate prediction tasks. Looking ahead, we intend to extend this work towards developing an explainable recommendation system that leverages the interest-aware capsule network to elucidate users' interests.", "REFERENCES": "[1] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems . 7-10. [2] Weiyu Cheng, Yanyan Shen, and Linpeng Huang. 2020. Adaptive factorization network: Learning adaptive-order feature interactions. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 34. 3609-3616. [3] Yuan Cheng and Yanbo Xue. 2021. Looking at CTR Prediction Again: Is Attention All You Need?. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1279-1287. [4] Marzieh Edraki, Nazanin Rahnavard, and Mubarak Shah. 2020. Subspace capsule network. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 34. 10745-10753. [5] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. In Proceedings of the 26th International Joint Conference on Artificial Intelligence . 1725-1731. [6] Chao Li, Zhiyuan Liu, Mengmeng Wu, Yuchi Xu, Huan Zhao, Pipei Huang, Guoliang Kang, Qiwei Chen, Wei Li, and Dik Lun Lee. 2019. Multi-interest network with dynamic routing for recommendation at Tmall. In Proceedings of the 28th ACM international conference on information and knowledge management . 2615-2623. [7] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong Sun. 2018. xdeepfm: Combining explicit and implicit feature interactions for recommender systems. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1754-1763. [8] Zheng Liu, Jianxun Lian, Junhan Yang, Defu Lian, and Xing Xie. 2020. Octopus: Comprehensive and elastic user representation for the generation of recommendation candidates. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . 289-298. [9] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu, Alisson G Azzolini, et al. 2019. Deep learning recommendation model for personalization and recommendation systems. arXiv preprint arXiv:1906.00091 (2019). [10] Aditya Pal, Chantat Eksombatchai, Yitong Zhou, Bo Zhao, Charles Rosenberg, and Jure Leskovec. 2020. Pinnersage: Multi-modal user embedding framework for recommendations at pinterest. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 2311-2320. [11] Qi Pi, Weijie Bian, Guorui Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Practice on long sequential user behavior modeling for click-through rate prediction. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 2671-2679. [12] Yanru Qu, Bohui Fang, Weinan Zhang, Ruiming Tang, Minzhe Niu, Huifeng Guo, Yong Yu, and Xiuqiang He. 2018. Product-based neural networks for user response prediction over multi-field categorical data. ACM Transactions on Information Systems (TOIS) 37, 1 (2018), 1-35. [13] Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. 2017. Dynamic routing between capsules. Advances in neural information processing systems 30 (2017). [14] Ying Shan, T Ryan Hoens, Jian Jiao, Haijing Wang, Dong Yu, and JC Mao. 2016. Deep crossing: Web-scale modeling without manually crafted combinatorial features. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining . 255-262. [18] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Deep interest evolution network for click-through rate prediction. In Proceedings of the AAAI conference on artificial intelligence , Vol. 33. 5941-5948. [19] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through rate prediction. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1059-1068."}
