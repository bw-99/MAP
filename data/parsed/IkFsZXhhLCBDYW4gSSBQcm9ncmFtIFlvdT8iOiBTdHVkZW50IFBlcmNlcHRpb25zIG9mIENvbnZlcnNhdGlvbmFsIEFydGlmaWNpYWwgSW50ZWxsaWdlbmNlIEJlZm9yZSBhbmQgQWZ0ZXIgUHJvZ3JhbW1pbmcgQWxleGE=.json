{"title": "\"Alexa, Can I Program You?\": Student Perceptions of Conversational Artificial Intelligence Before and After Programming Alexa", "authors": "Jessica Van Brummelen; Viktoriya Tabunshchyk; Tommy Heng", "pub_date": "2021-02-02", "abstract": "Fig. 1. Example conversations from students' Alexa skill designs, including a \"Meme Maker\" and \"Language Game\". Growing up in an artificial intelligence-filled world, with Siri and Amazon Alexa often within arm's-or speech's-reach, could have significant impact on children. Conversational agents could influence how students anthropomorphize computer systems or develop a theory of mind. Previous research has explored how conversational agents are used and perceived by children within and outside of learning contexts. This study investigates how middle and high school students' perceptions of Alexa change through programming their own conversational agents in week-long AI education workshops. Specifically, we investigate the workshops' influence on student perceptions of Alexa's intelligence, friendliness, aliveness, safeness, trustworthiness, human-likeness, and feelings of closeness. We found that students felt Alexa was more intelligent and felt closer to Alexa after the workshops. We also found strong correlations between students' perceptions of Alexa's friendliness and trustworthiness, and safeness and trustworthiness. Finally, we explored how students tended to more frequently use computer science-related diction and ideas after the workshops. Based on our findings, we recommend designers carefully consider personification, transparency, playfulness and utility when designing CAs for learning contexts.", "sections": [{"heading": "INTRODUCTION", "text": "With children asking Google to buy them more toys [6], cheating on homework with Alexa [53], and playing voice-based pranks on parents [6], conversational agents (CAs) have potential to not only influence children's play-but also how they grow and develop [55]. For instance, researchers theorize that interacting with an agent can change people's understanding of agency concepts and Authors' address: Jessica Van Brummelen, jess@csail.mit.edu; Viktoriya Tabunshchyk, vikt@mit.edu; Tommy Heng, theng@ mit.edu, Massachusetts Institute of Technology, 77 Massachusetts Ave, Cambridge, MA, USA, 02139.\ntheir Theory of Mind (ToM) [21,29,54]. Other research has shown engaging with CAs can change people's behavior [1,9,20,49] and have positive effects on information retention [4].\nConsidering the impact agents have on human understanding and behavior, how prevalent these systems are becoming [43,55], and how opaque their operations can be to humans [13,34,42], a growing body of research suggests it is important for people of all ages to understand AI [32,42,56]. Furthermore, researchers are investigating how to best teach AI literacy concepts to students, including those as young as preschoolers [61]. For instance, one study leverages 3-5th grade students familiarity with CAs to teach AI literacy concepts [31]. Other works utilize AI ethics discussions [12], interactive, collaborative learning environments [60], and gesture recognition tools [66] to engage students in learning AI. In this work, we use a constructionist approach, in which students program their own CAs, to teach AI concepts to 6-12th grade students [40,59].\nAnother aspect of AI education research includes students' perception of AI systems themselves, including personification of such systems, emotions the systems evoke, and students' conceptions of how the systems work. For example, one study examines preschool-and kindergarten-aged students' perceptions of \"thinking machines\" during an AI learning activity, emphasizing the importance of early childhood AI literacy and ToM development [61]. Other studies investigate children and family's perceptions of CAs [34], how interaction modalities influence children's perceptions of CAs [13], children's perceptions of maze-solving agents' intelligence [14], and whether children categorize CAs as animate objects or artifacts [65]. Yet other studies emphasize the importance of adults' perceptions and conceptions of AI, especially in decision-making and policy [22,27,50]. To our knowledge, few studies investigate middle and high school students' perceptions of AI [36,45], despite teenage years being critical in ethical perspective development [10], a key component of AI literacy [56]. Furthermore, to our knowledge, no studies investigate how middle and high school students' perceptions of CAs change through programming CAs.\nWe posit that understanding students' perceptions and feelings towards such agents can help researchers better facilitate student learning. For instance, feelings of closeness with teachers have been shown to affect students' academic performance [2,5,63], which may also be the case when agents take on the teacher role. Another study indicates that the avatar used for pedagogical feedback-giving agents affect students' emotional attachment and satisfaction with the learning process [48], alluding to the potential for students' perception of agents to affect learning. Furthermore, research suggests understanding students' preconceptions and mental models can improve teaching [15,51]. By understanding students' feelings and conceptions about agents, we expect we can create better digital learning environments.\nThis study investigates 6-12th grade students' perceptions and conceptions of Amazon Alexa in a learning environment described in [59]. In contrast to [59], which investigates students' AI literacy, this study investigates how a programming and learning intervention, in which students develop their own CAs, affects student perspectives of AI. Our main research question is as follows:\nRQ: How does building Alexa skills and learning about conversational AI in a remote workshop affect students' perceptions and conceptions of AI, conversational AI, and Alexa?\nBy better understanding students' perspectives on agents and how these perspectives can be changed, we contribute to ongoing research to develop more human-centered, socially useful agents-especially for K-12 education. To this end, we present four design considerations for K-12 education agents and development tools based on our findings. Specifically, we look at students' conceptions of how AI and conversational AI work, and perceptions of Alexa in terms of friendliness, human-likeness, aliveness, safeness, trustworthiness, intelligence (generally and relative to themselves), and how close they feel to Alexa.", "publication_ref": ["b5", "b52", "b5", "b54", "b20", "b28", "b53", "b0", "b8", "b19", "b48", "b3", "b42", "b54", "b12", "b33", "b41", "b31", "b41", "b55", "b60", "b30", "b11", "b59", "b65", "b39", "b58", "b60", "b33", "b12", "b13", "b64", "b21", "b26", "b49", "b35", "b44", "b9", "b55", "b1", "b4", "b62", "b47", "b14", "b50", "b58", "b58"], "figure_ref": [], "table_ref": []}, {"heading": "RELATED WORK", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conceptions of artificial intelligence and theory of mind", "text": "The working definition of AI in research has changed over the years-from having a sharp focus on logical, symbolic representations of concepts and actions to a marked concentration on modelling extensive interconnected computation machines called \"neural networks\" [64]. In the media, AI has been depicted in many different ways-as killer robots, android caretakers, and superintelligent, disembodied voices [19]. Despite the somewhat frivolous portrayals, people's understanding of AI and how it works has serious implications-from policy-making to day-to-day assessments of whether a self-driving vehicle is safe to trust one's life with [22,35].\nToM research in AI investigates how to develop AI systems with human-like cognition, as well as how people understand AI as agents with mental states [17]. In this research, we focus on the latter, or \"Theory of Artificial Mind\" (ToAM) [54]. Understanding people's conceptions of AI, including anthropomorphization of AI technology, conceptions of specific technologies, like CAs, and emotional reactions to AI systems, is important for teaching AI literacy. Through better understanding students' perceptions and ToAM, we can likely better teach students about AI [21] and therefore better reach our research community's goal of equipping people to live in an AI-filled world [22,32,56].\nChildren have been observed to anthropomorphize AI systems [13,14,65]; however, their understanding of the actual \"aliveness\" of such systems is inconsistent across populations and seems to vary with age [24,47,61,65]. Other anthropomorphic aspects of AI systems have also been investigated for different purposes. For instance, a number of studies examine how children (3-10 years old) perceive agents' intelligence-generally and relative to their intelligence-with the purpose of inspiring critical thinking [13,14]. Another study investigates 5-6 year old children's perception of CAs' friendliness, aliveness, trustworthiness, safeness, and funniness, in addition to intelligence to develop CA design recommendations [34]. Researchers investigated similar anthropomorphic aspects, including how sociable, mutual-liking, attractive, human, close, and intelligent children (10-12 years old) perceive agents to be, in order to improve learning interventions [36]. We investigate related anthropomorphic aspects in middle to high school students' ToAM.\nResearch also shows that interaction with AI artifacts can influence people's ToM and perceptions of AI. For instance, observing and constructing robot behavior influenced students' ToAM, enabling them to better explain the AI systems' behavior [54]. Another study showed that interacting with a pedagogical agent influenced students' understanding of the key ToM concept of agency, allowing them to better predict behavior. The same study linked students' prior understanding of agency to better learning [21]. In this paper, we investigate how AI literacy workshops involving programming a CA influences students' ToAM, including perceptions of anthropomorphic qualities and understanding of AI behavior.", "publication_ref": ["b63", "b18", "b21", "b34", "b16", "b53", "b20", "b21", "b31", "b55", "b12", "b13", "b64", "b23", "b46", "b60", "b64", "b12", "b13", "b33", "b35", "b53", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "Conversational agents and education", "text": "Many studies investigate how CAs can best embody the teaching role [11,28,38,41]. Some such studies show that interacting with agents can positively affect learning and students' ToM [11,21]. In this study, however, we take a constructionist approach, and instead of placing agents in the teaching role, we empower students to learn about AI through developing their own CAs [40,59].\nConstructionism has been shown to be effective in teaching K-12 students AI concepts. For example researchers have taught students AI ethics through constructing paper prototypes [3], machine learning (ML) concepts through developing gesture-based models [66], and AI programming concepts through creating projects with AI cloud services [23]. Our study teaches students Long and Magerko's AI literacy competencies through developing CAs [32,59].\nCertain studies specifically investigate whether constructionist activities change student conceptions and perceptions of AI agents. For example, a series of studies showed constructing a robot's behavior enabled kindergarten students to conceptualize an agent's rule-based behavior [37], shifted students' perspectives from technological to psychological [30], and shifted students' language from anthropomorphic to technological [26]. Through an activity with the same constructionist programming environment, it was shown 5-and 7-year-old students' conceptions of ToAM developed, and the students were able to better understand robots' behavior [54]. A study with programming and ML training activities showed 4-6-year-old students' understanding of ToM and perceptions of robots changed throughout the experiment [61]. In this work, we investigate whether students' ToAM and perceptions of AI in middle and high school change through a constructionist CA programming activity and workshop.", "publication_ref": ["b10", "b27", "b37", "b40", "b10", "b20", "b39", "b58", "b2", "b65", "b22", "b31", "b58", "b36", "b29", "b25", "b53", "b60"], "figure_ref": [], "table_ref": []}, {"heading": "METHODS", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Participants", "text": "We conducted our workshops with 47 students separated into two groups of 12 and 35. For each group, the students' teachers observed the workshops and provided feedback to the three teaching researchers [59]. The teachers were recruited through an Amazon Future Engineers call to Title I schools. Each teacher chosen for the workshops was asked to recruit 5 or 6 of their students. We targeted Title I schools because they have high concentrations of children from low-income families [52], and we wanted to provide opportunities for enrichment that they may not normally receive. We developed middle and high school level AI curriculum and thus targeted middle and high school students. The students' mean age was 14.78 (range 11-18, SD=1.91), with 19 self-identifying as male, 27 self-identifying as female, and 1 student that did not complete the questionnaire.", "publication_ref": ["b58", "b51"], "figure_ref": [], "table_ref": []}, {"heading": "Procedure", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Programming agents.", "text": "To accomplish our goal of studying student perceptions of conversational AI before and after programming Alexa, we developed an interface within MIT App Inventor for creating Alexa skills [57]. This lowered the barrier of entry to programming CAs, as the students could use visual block-based coding to develop skills. As described in [59], once a student creates a skill on the interface, the backend translates their blocks into JSON and Javascript code to be sent to Alexa's API to build and enable the skill on the student's Amazon Developer account. This allows the students to interact and have a conversation with the Alexa skill either on an Alexa-enabled device (e.g., iOS Alexa App or Amazon Echo) or an online simulated Alexa device (e.g., MIT App Inventor Alexa Testing simulator or Amazon Developer Console).", "publication_ref": ["b56", "b58"], "figure_ref": [], "table_ref": []}, {"heading": "Workshop outline.", "text": "This section provides a brief overview of the learning intervention, which is described in-depth in [59]. The intervention occurred over two sessions, which both involved five consecutive days of 2.5 hour long Zoom sessions. The first day began with an introduction to the MIT App Inventor interface [62] to accustom students to block-based coding. Then the students were given a chance to interact freely with Alexa, writing down the questions they asked during the interaction. In the first week, students were each provided with a complimentary Echo Dot. This was not feasible for the second week of workshops due to an increased number of students, so students either used the Alexa app on their mobile devices, an online Alexa simulator (within MIT App Inventor or otherwise), or Alexa devices they previously owned. Overall, 19 students used an Alexa device, 17 used the Alexa app, 10 used an online simulator, and one did not specify.\nThe second day involved introducing students to key AI and conversational AI concepts, discussing AI ethics, and completing a tutorial walk-through to create an Alexa skill that would respond to basic greetings. On the third day, students completed a tutorial to develop a calculator skill, in which Alexa could be asked, \"What's number A multiplied by number B\", or something similar. Next, we taught students about ML in more depth, including discussing the difference between a rule-based CA developed on the first day and the ML-based CAs developed on the second and third days. Finally, students engaged in an AI text generation activity.\nOn the fourth day, students developed a skill that enabled Alexa to read out text entered into MIT-App-Inventor-developed mobile apps. Students then brainstormed ideas for skills for their personal projects. Students spent the final day developing their projects and presenting them to the rest of the class.", "publication_ref": ["b58", "b61"], "figure_ref": [], "table_ref": []}, {"heading": "Questionnaires", "text": "Various questionnaires inspired by the perception of AI questions in [14] and [34] were given to students during the learning intervention. On the first day, students recorded their interactions with Alexa, impressions of the CA, and demographics information. At the start of the second day, students completed a questionnaire assessing their initial feeling towards and understanding of Alexa, AI and conversational AI. The questions were divided into two sets, which we refer to as the Persona and Conception questions.\nThe Persona questions assessed students' sentiments about Alexa on a 7-point Likert scale. The questions stated, \"Alexa is...\" followed by \"intelligent\", \"friendly\", \"alive\", \"safe\", \"trustworthy\", \"human-like\", and \"smarter than me\". The final Persona question asked how close students felt to Alexa using the Inclusion of the Other in the Self scale [18]. The Conception questions assessed students' understanding of AI and conversational AI through asking, \"Describe in your own words what AI is\" and \"Describe in your own words what conversational AI is (e.g., chatbots, like Alexa or Google home, use conversational AI)\". At the end of the final day, students completed the Persona and Conception questions again. Additional questionnaires were given at the end of the second, third, and fourth days to assess specific AI literacy competencies, as discussed and analyzed in [59].", "publication_ref": ["b13", "b33", "b17", "b58"], "figure_ref": [], "table_ref": []}, {"heading": "Data Analysis", "text": "This study builds on the study presented in [59]. Thus, certain data analyzed in this study (e.g., demographics) is necessarily the same; however, this study focuses on data not analyzed in [59], including the questionnaire responses to the Persona questions and students' reported interactions with Alexa. The responses to the Conception questions were analyzed in both studies, however using different methods and through different lenses. This study investigates students' conceptions of AI through a word frequency analysis as well as analyses of changes in number of tags (as described below). The study in [59] assessed students' AI literacy before and after the learning intervention.\nTo investigate the responses to qualitative questions, a reflexive, open-coding approach to thematic analysis [7] was performed by three researchers. The three researchers independently completed familiarization and code-generation stages. After several discussions, the three researchers came to a consensus on codes for the questionnaire responses. Codes and respective representative quotations can be found in [58]. Researchers generally constructed codes inductively or with respect to ideas from literature, including the Big AI Ideas [56]. It is important to note that responses often involved multiple ideas and were thus tagged with more than one code.\nFor the quantitative questions (e.g., Likert scale Persona questions) asked on both pre-and postquestionnaires, the Wilcoxon Signed-Rank Test was employed to measure changes. Additionally, we used the Kendall Tau method to create pairwise correlation matrices. We analyzed the correlation coefficients using Cohen (2013)'s definition for correlation effect strength for behavioral and education psychology [8]. To test the validity of the strength of the coefficients, we compared Kendall Tau p-values to an alpha of 0.05. For the word frequency analysis, we used the NLTK library [33] to remove stop-words, tokenize and lemmatize qualitative responses. Additionally, to better visualize non-obvious concepts, we filtered out words directly from the questions, including 'AI', 'artificial', 'intelligence', and 'conversational'. Word clouds were generated using [39].", "publication_ref": ["b58", "b58", "b58", "b6", "b57", "b55", "b7", "b32", "b38"], "figure_ref": [], "table_ref": []}, {"heading": "RESULTS", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Student interactions with Alexa", "text": "To understand the types of interactions students had with Alexa prior to the intervention, we coded the phrases they reported saying to Alexa during the interaction activity. We found most of the phrases fell into one of five categories listed in Tab. 1. The Information Updates category involved real-time events; the Action Commands category involved built-in Alexa applications; the Personal Questions category involved questions about Alexa; the Jokes category involved asking Alexa to say a joke; and the Other category involved questions and phrases that were often humorous (e.g., \"Are dragons real?\") or impossible to fully answer (e.g., \"What are all the numbers of pi?\"), or generally fell outside of the other categories (e.g., \"Hello\"). Note that prior to the activity, we asked Alexa to tell us a joke, which may have contributed to a large number of students also asking Alexa for jokes.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Perceptions of Alexa pre-and post-workshop", "text": "By comparing pre-and post-survey answers to the Persona questions (see Fig. 2), we found significant differences in how students felt about Alexa's intelligence and how close they felt they were to Alexa. After the intervention, students felt Alexa was more intelligent ( x = 6.0, \ud835\udc40\ud835\udc5c = 6, |\ud835\udc4d | = 2.78, \ud835\udc5d = 0.003) and felt closer to Alexa ( x = 3.5, \ud835\udc40\ud835\udc5c = 4, |\ud835\udc4d | = 2.75, \ud835\udc5d = 0.003). We did not find any evidence of significant differences in how students felt about Alexa being friendly, alive, safe, trustworthy, human-like or smarter than themselves before and after the intervention.\nPrior to the intervention, students generally reported Alexa as being highly intelligent ( x = 5.6, \ud835\udc40\ud835\udc5c = 6), highly friendly ( x = 6.0, \ud835\udc40\ud835\udc5c = 7), not very alive ( x = 2.9, \ud835\udc40\ud835\udc5c = 1), highly safe ( x = 5.4, \ud835\udc40\ud835\udc5c = 6), moderately to highly trustworthy ( x = 5.3, \ud835\udc40\ud835\udc5c = 4, 5), and moderately human-like ( x = 4.2, \ud835\udc40\ud835\udc5c = 5). They also reported feeling Alexa was much smarter than themselves ( x = 6.1, \ud835\udc40\ud835\udc5c = 7), and feeling not particularly close to Alexa ( x = 3.0, \ud835\udc40\ud835\udc5c = 2). The results were similar after the intervention (other than the changes in intelligence and closeness described above).", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Correlations between perceptions of Alexa", "text": "We found strong (\ud835\udc5f \u2265 0.5 [8]) correlations between student reports of Alexa's safeness and trustworthiness on both the pre-and post-test, as well as between Alexa's friendliness and trustworthiness on the post-test. There was also a strong correlation between trustworthiness reported on the pre-test and safeness reported on the post-test. Student reports of Alexa's friendliness and trustworthiness on the pre-test and between the pre-and post-tests were moderately (\ud835\udc5f \u2265 0.3 [8]) correlated.\nOther moderate correlations included student reports of Alexa's intelligence and trustworthiness, friendliness and safeness, trustworthiness and feelings of closeness, human-likeness and aliveness, human-likeness and feelings of closeness, as well as aliveness and feelings of closeness. In the post-test, student reports of Alexa's intelligence and feeling Alexa was smarter than them, as well as Alexa's trustworthiness and feeling Alexa was smarter than them were moderately correlated. Additionally, there was a moderate correlation between students with more experience programming prior to the intervention and reports of Alexa's human-likeness on both the pre-and post-test. Our full correlation analysis is shown in Fig. 3.", "publication_ref": ["b7", "b7"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Student diction when describing AI", "text": "To visualize students' understanding of AI and conversational AI, we analyzed word frequency and created word clouds based on answers to two questions. Fig. 4 shows the word frequency analyses of students' answers to, \"Describe in your own words what AI is\", prior to and after the intervention. Fig. 5 shows the analyses of answers to, \"Describe in your own words what conversational AI is (e.g., chatbots, like Alexa or Google Home, use conversational AI)\".", "publication_ref": [], "figure_ref": ["fig_2", "fig_3"], "table_ref": []}, {"heading": "Conceptions of AI and conversational AI", "text": "To better understand students' qualitative answers when conceptualizing AI and conversational AI, we performed a graphical exploration of tag frequency from our thematic analysis. The graph in Fig. 6 shows the change in tag frequency from pre-to post-test. Since the number of participants who completed the pre-test differed from the post-test, the number of tags (\ud835\udc61 \ud835\udc5d\ud835\udc5f\ud835\udc52 and \ud835\udc61 \ud835\udc5d\ud835\udc5c\ud835\udc60\ud835\udc61 ) were normalized over the number of responses (\ud835\udc5b \ud835\udc5d\ud835\udc5f\ud835\udc52 = 45 and \ud835\udc5b \ud835\udc5d\ud835\udc5c\ud835\udc60\ud835\udc61 = 38), and reported as normalized percent change, \ud835\udc36 % (Eq. 1). This is presented as an exploratory, graphical analysis for high-level insights rather than statistical analysis.   ", "publication_ref": [], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "DISCUSSION", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Perceptions of Alexa's persona", "text": "Prior to the study, we hypothesized students would feel Alexa was less intelligent after learning how to program it, as they would better understand how it works; however, students felt Alexa was more intelligent after the intervention (|\ud835\udc4d | = 2.78, \ud835\udc5d = 0.003). This could have been for multiple reasons. Perhaps by successfully learning fundamental AI literacy concepts [59], students realized Alexa was more complex than they initially thought and thus perceived it to be more \"intelligent\" (as in the Dunning-Kruger effect [16]). This is supported by the relative increase in AI literacy concepts (which are comparatively complex) in the post-test responses to the Conception questions (Fig. 6), and the relative decrease in pre-programming concepts (which are comparatively simplistic). Students also generally felt Alexa was smarter than themselves (both before and after the intervention). This is consistent with previous studies of students aged 3-10 [13, 34]. The Dunning-Kruger concept may also explain why there were relatively fewer tags identifiedlikely indicating fewer ideas presented by students-in the post-test than in the pre-test for many of the qualitative answers to the Conception questions. For example, as shown in Fig. 6, there were relatively fewer tags for the majority of the tag categories in the post-test responses about conversational AI. Perhaps students became \"less ignorant of their ignorance\" [16] about Alexa through the intervention, and therefore felt less qualified to answer the qualitative questions and thus presented fewer ideas in the post-test. Nevertheless, one limitation of this study was that students responded to the post-test at the end of the workshops, so they may have had less energy than when they responded to the pre-test, alternatively explaining the relatively fewer ideas presented.\nWe also hypothesized that students would personify Alexa less after understanding the logic behind how it works, and therefore rate its \"aliveness\", \"human-likeness\", \"friendliness\", and their feelings of closeness to it as less than prior to the intervention. However, there was no significant evidence for any change, except that they felt closer to Alexa (|\ud835\udc4d | = 2.75, \ud835\udc5d = 0.003) after the intervention. Students' increased feelings of closeness could be due to \"boundary dissolution\", which is a type of closeness where two agents (usually human) no longer function completely autonomously, but rather function dependently [25]. In this case, an apparent \"boundary dissolution\" due to Alexa initially seeming to function independently, but seeming to function dependently on students' programming efforts after the intervention, could have caused students' increased feelings of closeness.\nAlternatively, perhaps having programming experience fundamentally increases feelings of closeness to Alexa, seeing as students' prior programming experience was moderately correlated with closeness. Furthermore, prior programming experience and human-likeness, as well as closeness and human-likeness were moderately correlated. One explanation could be that as students learned to program, they felt Alexa had human-like, logical reasoning, and thus felt closer to it (because of its human-like traits).\nStudents' perceptions of Alexa's friendliness and trustworthiness were strongly correlated, as well as trustworthiness and safeness, and to a lesser extent, intelligence and trustworthiness, friendliness and safeness, and closeness and trustworthiness. Although these correlations do not necessitate causation, it is important to consider the implications of potential causation when designing CAs. For instance, if a CA was purposefully designed to seem friendly and intelligent, users may associate this with trustworthiness and safeness, despite the potential for the CA to provide incorrect information (intentionally or not). Nevertheless, this could also provide positive opportunities, including how students may learn better if they feel a pedagogical agent is friendly and intelligent, and thus also trustworthy and safe. This is discussed in more depth below.", "publication_ref": ["b58", "b15", "b15", "b24"], "figure_ref": ["fig_4", "fig_4"], "table_ref": []}, {"heading": "Conceptions of AI and conversational AI", "text": "From the pre-/post-test comparison of word frequency in responses describing AI (Fig. 4) and conversational AI (Fig. 5), as well as the change in tag frequency analysis (Fig. 6), students' conceptions seemed to shift towards more accurate understandings. For instance, the diction for describing AI seemed to shift towards computer-science-related terminology, including program, learn, and information. This trend is consistent with other literature, in which students describe AI with more computer science vocabulary after developing AI projects [45]. Furthermore, the emergence of the word learn in post-test responses suggests a better understanding of AI systems' ability to adapt and update with training. For instance, one student's response described AI as \"a program that learns and uses the learning for other problems\". Furthermore, as shown in Fig. 6, there was a relative increase in references to concepts from the Big AI Ideas [56], including Learning and Representation and reasoning, and a relative decrease in simplistic explanations of the AI acronym (e.g., \"AI is artificial intelligence\") and of how AI is \"like a human\", indicating better understanding.\nIn the conversational AI responses (Fig. 5), human remains the most frequent word in the pre-and post-test, suggesting student understanding of how human interaction is central to conversational AI's purpose. For the conversational AI descriptions shown in Fig. 6, Learning and the concept of natural language (NL) responses and understanding increased, indicating better understanding. Furthermore, there was a relative decrease in simplistic explanations of conversational AI being \"like a robot\", mimicking humans, having pre-programmed responses, and being something that \"help[s] humans\". Despite these indications of better understanding, there was a slight relative increase in vague or shallow answers for both the descriptions of AI and conversational AI, and a relative decrease in the Big AI idea of representation and reasoning for conversational AI. Overall, however, it seemed as if students' conceptions improved through the workshops, especially considering the evidence for increased understanding of AI literacy concepts presented in [59].", "publication_ref": ["b44", "b55", "b58"], "figure_ref": ["fig_2", "fig_3", "fig_4", "fig_4", "fig_3", "fig_4"], "table_ref": []}, {"heading": "Design Considerations", "text": "Based on the results, we present design considerations for engaging students in learning experiences with CAs.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Personification.", "text": "As shown in Tab. 1, students asked Alexa many personal questions (e.g., \"Alexa, do you like Siri?\" and \"What's your favorite color?\"), which would typically be asked of humans rather than computer systems. Alexa's often humorous responses (e.g., \"I like ultraviolet. It glows with everything\") could have contributed to students' perception of personified traits, like friendliness, intelligence and trustworthiness, which were all rated highly. As discussed, personified traits in CAs could play a role in effective teaching interventions [48], especially since feelings of closeness and trust can enhance human teaching and learning experiences [2,5,63].\nWe recommend pedagogical CA developers cautiously consider personification in their designs. Although personification could engage students in effective learning experiences, it could also increase their feelings of trust disproportionately with the actual trustworthiness of the device. For example, students could perceive the device as always providing unbiased, correct answers, despite AI systems often being biased [46]. Thus, we further recommend considering transparency in CA design.", "publication_ref": ["b47", "b1", "b4", "b62", "b45"], "figure_ref": [], "table_ref": []}, {"heading": "Transparency.", "text": "Students also seemed to test the limits of Alexa, asking impossible or difficult questions as encapsulated by the Other category in Tab. 1. For example, students asked Alexa to turn itself off, to tell them all the (infinite) digits of \ud835\udf0b, and to provide the answer to 0 0 . These behaviors could be linked to trying to understand the system's inner workings. Thus, we recommend developing CAs with the ability to explain themselves, and furthermore, provide transparency in terms of their abilities (e.g., being able to explain AI bias). This is especially important when considering the correlations between CAs' friendliness and perceived trustworthiness, and students' potential increase in awareness of ignorance in how CAs work, as discussed above. This recommendation also aligns with other child-CA interaction research, which suggests designing transparent AI systems with respect to children's level of understanding [61].", "publication_ref": ["b60"], "figure_ref": [], "table_ref": []}, {"heading": "5.3.3", "text": "Playfulness. Similar to the behavior of \"testing\" Alexa described above, students asked Alexa playful questions like, \"How much wood would a wood chuck chuck if a wood chuck would chuck wood?\" and \"Are dragons real?\". These questions illustrate students'-even middle and high school students'-innate desire to play. Play can be hugely beneficial in learning environments, especially from a constructionist perspective [40,44]; thus, we recommend considering playful learning experiences when developing CAs. For example, in our study students had the opportunity to develop their own CA projects. Students came up with many different playful (as well as serious) ideas [59]. One very playful idea included a CA \"Meme Maker\", which according to the developer, \"help[ed] everyone get a quick laugh because as the old saying goes laughter is the best This same student cited their favorite part of the workshop as \"improving [their] coding ability and learning more about [CAs]\". 5.3.4 Utility. Many student projects' purposes were to provide utility, with 34% being mental and physical health-, 29% being educational-, 21% being productivity-and 8% being accessibility-related CAs [59]. Utility was also reflected in students' interactions with Alexa, as Information updates and Action commands were the most common interactions reported. With students evidently being interested in CAs' utility, we recommend designing CAs with useful features to provide entry points to CA engagement and potential learning moments. For example, students might naturally engage with a CA in figuring out what the weather is like tomorrow, which would provide an opportunity to teach students about APIs and databases, and how CAs provide such answers.", "publication_ref": ["b39", "b43", "b58", "b58"], "figure_ref": [], "table_ref": []}, {"heading": "LIMITATIONS AND FUTURE WORK", "text": "One limitation of this study includes its generalizability. We engaged middle and high school students in remote workshops in which they used MIT App Inventor to program Amazon Alexa; however, the results may not generalize to other environments or grade bands. Furthermore, since we held workshops on two different weeks with slight differences, this could have affected the results. Thus, future work may include larger follow-up studies with students in different grade bands in different environments.\nThere are also limitations associated with thematic analyses. For instance, we may have missed certain themes within the data, despite following the approach to analysis described in [7]. Furthermore, the amount of ideas presented by students in the pre-versus post-tests could have been influenced by the time of day each test was presented. Nevertheless, we believe the thematic data (as well as the word frequency data) are useful for exploratory, graphical analysis. Further research should statistically analyse students' conceptions of CAs and investigate how these conceptions affect the effectiveness of learning interventions.", "publication_ref": ["b6"], "figure_ref": [], "table_ref": []}, {"heading": "CONCLUSIONS", "text": "Through the programming and learning intervention, students' perceptions of Alexa changed in how they viewed its intelligence and how close they felt to it, and students' conceptions tended towards describing AI systems using more computer science terminology and AI literacy concepts. Based on these results, we presented four design recommendations, including considering personification, transparency, playfulness and utility when designing CAs for engaging students in learning experiences. This study contributes to AI literacy research aiming to develop students' understanding of AI to be more accurate and healthy, ToAM research aiming to understand students' perception of AI, and CA research aiming to develop more useful, effective interactions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "SELECTION AND PARTICIPATION OF CHILDREN", "text": "Children aged 11-18 (Mean=14.78, SD=1.91) were selected by their teachers to participate in the middle/high school workshops. Teachers were selected from those that responded to an Amazon Future Engineers call to Title I schools and signed a consent form. Selected students of the age of 18 were given similar student consent forms to sign, and those under the age of 18 were given assent forms and consent forms to be signed by their legal guardians before participating. The university's IRB approved the study protocol and consent/assent forms, which communicated how the data would be aggregated and anonymized. Given the wide age range, teachers assigned some of their older students to be mentors to younger students in case they fell behind.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Are you convinced? A Wizard of Oz study to test emotional vs. rational persuasion strategies in dialogues", "journal": "Computers in Human Behavior", "year": "2016", "authors": "Rachel F Adler; Francisco Iacobelli; Yehuda Gutstein"}, {"ref_id": "b1", "title": "Socioemotional and Academic Adjustment Among Children with Learning Disorders: The Mediational Role of Attachment-Based Factors", "journal": "The Journal of Special Education", "year": "2004", "authors": "Michal Al-Yagon; Mario Mikulincer"}, {"ref_id": "b2", "title": "Constructionism, ethics, and creativity: Developing primary and middle school artificial intelligence education", "journal": "", "year": "2019", "authors": "Safinah Ali; Blakeley H Payne; Randi Williams; Hae Won Park; Cynthia Breazeal"}, {"ref_id": "b3", "title": "Embodied Conversational Agents: Effects on Memory Performance and Anthropomorphisation", "journal": "Springer", "year": "2003", "authors": "Robbert-Jan Beun; Eveliene De Vos; Cilia Witteman"}, {"ref_id": "b4", "title": "The teacher-child relationship and children's early school adjustment", "journal": "Journal of school psychology", "year": "1997", "authors": "H Sondra; Gary W Birch;  Ladd"}, {"ref_id": "b5", "title": "40 Tweets About Parenting With Today's Technology", "journal": "", "year": "2021-01-27", "authors": "Caroline Bologna"}, {"ref_id": "b6", "title": "Thematic Analysis", "journal": "Springer", "year": "2019", "authors": "Virginia Braun; Victoria Clarke; Nikki Hayfield; Gareth Terry"}, {"ref_id": "b7", "title": "Statistical Power Analysis for the Behavioral Sciences", "journal": "Elsevier Science", "year": "2013", "authors": "J Cohen"}, {"ref_id": "b8", "title": "Co-constructing intersubjectivity with artificial conversational agents: People are more likely to initiate repairs of misunderstandings with agents represented as human", "journal": "Computers in Human Behavior", "year": "2016", "authors": "Kevin Corti; Alex Gillespie"}, {"ref_id": "b9", "title": "What is Positive Youth Development?", "journal": "The ANNALS of the American Academy of Political and Social Science", "year": "2004", "authors": "William Damon"}, {"ref_id": "b10", "title": "The effects of multiple-pedagogical agents on learners' academic success, motivation, and cognitive load", "journal": "Computers & Education", "year": "2017", "authors": "Serkan Dincer; Ahmet Doganay"}, {"ref_id": "b11", "title": "Decoding Design Agendas: An Ethical Design Activity for Middle School Students", "journal": "Association for Computing Machinery", "year": "2020", "authors": "Daniella Dipaola; Blakeley H Payne; Cynthia Breazeal"}, {"ref_id": "b12", "title": "Initial Explorations in Child-Agent Interaction", "journal": "Association for Computing Machinery", "year": "2017", "authors": "Stefania Druga; Randi Williams; Cynthia Breazeal; Mitchel Resnick"}, {"ref_id": "b13", "title": "How Smart Are the Smart Toys? Children and Parents' Agent Interaction and Intelligence Attribution", "journal": "Association for Computing Machinery", "year": "2018", "authors": "Stefania Druga; Randi Williams; Hae Won Park; Cynthia Breazeal"}, {"ref_id": "b14", "title": "Bibliography: Students' and teachers' conceptions and science education", "journal": "Diakses pada tanggal", "year": "2009", "authors": "Reinders Duit"}, {"ref_id": "b15", "title": "Chapter five -The Dunning-Kruger Effect: On Being Ignorant of One's Own Ignorance", "journal": "Advances in Experimental Social Psychology", "year": "2011", "authors": "David Dunning"}, {"ref_id": "b16", "title": "Artificial Intelligence & Theory of Mind", "journal": "", "year": "2016", "authors": "Benjamin Erb"}, {"ref_id": "b17", "title": "Measuring the closeness of relationships: a comprehensive evaluation of the 'Inclusion of the Other in the Self' scale", "journal": "PloS one", "year": "2015", "authors": "Simon G\u00e4chter; Chris Starmer; Fabio Tufano"}, {"ref_id": "b18", "title": "Do androids dream of electric copyright? Comparative analysis of originality in artificial intelligence generated works", "journal": "Intellectual property quarterly", "year": "2017", "authors": "Andres Guadamuz"}, {"ref_id": "b19", "title": "Can AI artifacts influence human cognition? The effects of artificial autonomy in intelligent personal assistants", "journal": "International Journal of Information Management", "year": "2021", "authors": "Qian Hu; Yaobin Lu; Zhao Pan; Yeming Gong; Zhiling Yang"}, {"ref_id": "b20", "title": "The interrelationship between concepts about agency and students' use of teachable-agent learning technology", "journal": "Cognitive research: principles and implications", "year": "2019", "authors": "Christopher Brett Jaeger; Alicia M Hymel; Gautam Daniel T Levin; Natalie Biswas; John Paul;  Kinnebrew"}, {"ref_id": "b21", "title": "If Asimo thinks, does Roomba feel? The legal implications of attributing agency to technology", "journal": "Journal of Human-Robot Interaction (Symposium on Robotics Law and Policy)", "year": "2016-12", "authors": "Christopher Brett; Jaeger ; Daniel Levin"}, {"ref_id": "b22", "title": "AI programming by children using Snap! block programming in a developing country", "journal": "", "year": "2018", "authors": "R Km Kahn; E Megasari;  Piantari;  Junaeti"}, {"ref_id": "b23", "title": "Robovie, you'll have to go into the closet now\": Children's social and moral relationships with a humanoid robot", "journal": "Developmental psychology", "year": "2012", "authors": "Takayuki Peter H Kahn; Hiroshi Kanda; Nathan G Ishiguro; Rachel L Freier; Brian T Severson; Jolina H Gill; Solace Ruckert;  Shen"}, {"ref_id": "b24", "title": "Psychological Closeness: EXAMPLES OF CLOSENESS CONCEPTUALIZATION REFERENCES", "journal": "The American Behavioral Scientist", "year": "1984-07", "authors": "Thomas Kreilkamp"}, {"ref_id": "b25", "title": "Chais] Kindergarten Children's Perceptions of \"Anthropomorphic Artifacts\" with Adaptive Behavior", "journal": "Interdisciplinary Journal of E-Learning and Learning Objects", "year": "2012-01", "authors": "Asi Kuperman; David Mioduser"}, {"ref_id": "b26", "title": "Life on the Road: Exposing Drivers' Tendency to Anthropomorphise In-Vehicle Technology", "journal": "Springer International Publishing", "year": "2019", "authors": "David R Large; Gary Burnett"}, {"ref_id": "b27", "title": "Designing learning by teaching agents: The Betty's Brain system", "journal": "International Journal of Artificial Intelligence in Education", "year": "2008", "authors": "Krittaya Leelawong; Gautam Biswas"}, {"ref_id": "b28", "title": "A transition model for cognitions about agency", "journal": "", "year": "2013", "authors": "D T Levin; J A Adams; M M Saylor; G Biswas"}, {"ref_id": "b29", "title": "? Kindergarten children's explanations of an autonomous robot's adaptive functioning", "journal": "International Journal of Technology and Design Education", "year": "2008", "authors": "T Sharona; David Levy;  Mioduser"}, {"ref_id": "b30", "title": "Zhorai: Designing a Conversational Agent for Children to Explore Machine Learning Concepts", "journal": "", "year": "2020-04", "authors": "Phoebe Lin; Jessica Van Brummelen; Galit Lukin; Randi Williams; Cynthia Breazeal"}, {"ref_id": "b31", "title": "What is AI Literacy? Competencies and Design Considerations", "journal": "Association for Computing Machinery", "year": "2020", "authors": "Duri Long; Brian Magerko"}, {"ref_id": "b32", "title": "Nltk: The natural language toolkit", "journal": "", "year": "2002", "authors": "Edward Loper; Steven Bird"}, {"ref_id": "b33", "title": "Hey Google, Do Unicorns Exist? Conversational Agents as a Path to Answers to Children's Questions", "journal": "Association for Computing Machinery", "year": "2019", "authors": "B Silvia; Anne Marie Lovato; Ellen A Piper;  Wartella"}, {"ref_id": "b34", "title": "Interacting with Autonomous Vehicles: Learning from Other Domains", "journal": "Association for Computing Machinery", "year": "2018", "authors": "Alexander Meschtscherjakov; Manfred Tscheligi; Bastian Pfleging; Sadeghian Shadan; Wendy Borojeni; Philippe Ju; Andreas Palanque; Bilge Riener; Andrew L Mutlu;  Kun"}, {"ref_id": "b35", "title": "Supporting Interest in Science Learning with a Social Robot", "journal": "Association for Computing Machinery", "year": "2019", "authors": "Joseph E Michaelis; Bilge Mutlu"}, {"ref_id": "b36", "title": "Making sense by building sense: Kindergarten children's construction and understanding of adaptive robot behaviors", "journal": "International Journal of Computers for Mathematical Learning", "year": "2010", "authors": "David Mioduser; Sharona T Levy"}, {"ref_id": "b37", "title": "How to Integrate Emotions in Dialogues With Pedagogic Conversational Agents to Teach Programming to Children", "journal": "", "year": "2020", "authors": "Elizabeth Katalina Morales-Urrutia; Jose Miguel Oca\u00f1a; Diana P\u00e9rez-Mar\u00edn"}, {"ref_id": "b38", "title": "WordCloud for Python documentation", "journal": "", "year": "2020", "authors": "Andreas Mueller"}, {"ref_id": "b39", "title": "Situating constructionism", "journal": "Constructionism", "year": "1991", "authors": "Seymour Papert; Idit Harel"}, {"ref_id": "b40", "title": "An exploratory study on how children interact with pedagogic conversational agents", "journal": "Behaviour & Information Technology", "year": "2013", "authors": "Diana P\u00e9rez-Mar\u00edn; Ismael Pascual-Nieto"}, {"ref_id": "b41", "title": "Learning Machine Learning with Personal Data Helps Stakeholders Ground Advocacy Arguments in Model Mechanics", "journal": "Association for Computing Machinery", "year": "2020", "authors": "Yim Register; Amy J Ko"}, {"ref_id": "b42", "title": "Global Conversational AI Market Forecast to 2024: Integration of Advanced AI Capabilities Adding Value to the Conversational AI Offering", "journal": "Research and Markets", "year": "2019", "authors": ""}, {"ref_id": "b43", "title": "Playful Learning", "journal": "Journal for Education in the Built Environment", "year": "2009", "authors": "Louis Rice"}, {"ref_id": "b44", "title": "Evaluation of an Online Intervention to Teach Artificial Intelligence With LearningML to 10-16-Year-Old Students", "journal": "", "year": "2021", "authors": "Juan David Rodr\u00edguez-Garc\u00eda; Jes\u00fas Moreno-Le\u00f3n; Marcos Rom\u00e1n-Gonz\u00e1lez; Gregorio Robles"}, {"ref_id": "b45", "title": "Managing Bias in AI", "journal": "Association for Computing Machinery", "year": "2019", "authors": "Drew Roselli; Jeanna Matthews; Nisha Talagala"}, {"ref_id": "b46", "title": "Do computers have brains? What children believe about intelligent artifacts", "journal": "British Journal of Developmental Psychology", "year": "1995", "authors": "Michael Scaife; Mike Van Duuren"}, {"ref_id": "b47", "title": "A Configurational View on Avatar Design-The Role of Emotional Attachment, Satisfaction, and Cognitive Load in Digital Learning", "journal": "", "year": "2019", "authors": "Sofia Sch\u00f6bel; Andreas Janson; Abhay Mishra"}, {"ref_id": "b48", "title": "The influence of conversational agents on socially desirable responding", "journal": "", "year": "2018", "authors": "Mark Ryan M Schuetzler; Justin Grimes; Jay F Scott Giboney;  Nunamaker"}, {"ref_id": "b49", "title": "Impressions of computer and human agents after interaction: Computer identity weakens power but not goodness impressions", "journal": "International Journal of Human-Computer Studies", "year": "2014", "authors": "B Daniel;  Shank"}, {"ref_id": "b50", "title": "A Computational Study of Commonsense Science: An Exploration in the Automated Analysis of Clinical Interview Data", "journal": "Journal of the Learning Sciences", "year": "2013", "authors": "Bruce Sherin"}, {"ref_id": "b51", "title": "The Elementary and Secondary Education Act (ESEA), as Amended by the Every Student Succeeds Act (ESSA): A Primer", "journal": "Congressional Research Service", "year": "2019", "authors": "Rebecca R Skinner"}, {"ref_id": "b52", "title": "Mom busts 9-year-old son using Alexa to cheat on homework", "journal": "", "year": "2019", "authors": "Hannah Sparks"}, {"ref_id": "b53", "title": "The Influence of Constructing Robot's Behavior on the Development of Theory of Mind (ToM) and Theory of Artificial Mind (ToAM) in Young Children", "journal": "Association for Computing Machinery", "year": "2015", "authors": "Karen Spektor; - Precel; David Mioduser"}, {"ref_id": "b54", "title": "Exploring Conversational Agents for Children's Linguistic Assessment", "journal": "Association for Computing Machinery", "year": "2020", "authors": "Micol Spitale; Silvia Silleresi; Giulia Cosentino; Francesca Panzeri; Franca Garzotto"}, {"ref_id": "b55", "title": "Envisioning AI for K-12: What Should Every Child Know about AI", "journal": "", "year": "2019-07", "authors": "David Touretzky; Christina Gardner-Mccune; Fred Martin; Deborah Seehorn"}, {"ref_id": "b56", "title": "Tools to Create and Democratize Conversational Artificial Intelligence", "journal": "", "year": "2019", "authors": "Jessica Van Brummelen"}, {"ref_id": "b57", "title": "", "journal": "", "year": "2020-09-09", "authors": "Jessica Van Brummelen; Tommy Heng; Viktoriya Tabunshchyk"}, {"ref_id": "b58", "title": "Teaching Tech to Talk: K-12 Conversational Artificial Intelligence Literacy Curriculum and Development Tools", "journal": "", "year": "2021", "authors": "Jessica Van Brummelen; Tommy Heng; Viktoriya Tabunshchyk"}, {"ref_id": "b59", "title": "SmileyCluster: Supporting Accessible Machine Learning in K-12 Scientific Discovery", "journal": "Association for Computing Machinery", "year": "2020", "authors": "Xiaoyu Wan; Xiaofei Zhou; Zaiqiao Ye; Chase K Mortensen; Zhen Bai"}, {"ref_id": "b60", "title": "A is for Artificial Intelligence: The Impact of Artificial Intelligence Activities on Young Children's Perceptions of Robots", "journal": "Association for Computing Machinery", "year": "2019", "authors": "Randi Williams; Hae Won Park; Cynthia Breazeal"}, {"ref_id": "b61", "title": "Democratizing computing with App Inventor", "journal": "GetMobile: Mobile Computing and Communications", "year": "2015", "authors": "David Wolber; Harold Abelson; Mark Friedman"}, {"ref_id": "b62", "title": "Gender-typicality of activity offerings and child-teacher relationship closeness in German \"Kindergarten\". Influences on the development of spelling competence as an indicator of early basic literacy in boys and girls", "journal": "Learning and Individual Differences", "year": "2014", "authors": "Ilka Wolter; Michael Gl\u00fcer; Bettina Hannover"}, {"ref_id": "b63", "title": "Artificial Intelligence Is a House Divided: A decades-old rivalry has riven the field. It's time to move on", "journal": "The Chronicle", "year": "2021-01-20", "authors": "Michael Wooldridge"}, {"ref_id": "b64", "title": "What Are You Talking To?: Understanding Children's Perceptions of Conversational Agents", "journal": "Association for Computing Machinery", "year": "2020", "authors": "Ying Xu; Mark Warschauer"}, {"ref_id": "b65", "title": "Youth Making Machine Learning Models for Gesture-Controlled Interactive Media", "journal": "Association for Computing Machinery", "year": "2020", "authors": "Abigail Zimmermann-Niefield; Shawn Polson; Celeste Moreno; R Benjamin Shapiro"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Fig. 2 .2Fig. 2. Students' perceptions of Alexa prior to the workshops (in blue) and after (in green).", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Fig. 3 .3Fig. 3. Correlation matrices before (top) and after (bottom) the intervention. Lighter colors correspond to higher coefficients.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Fig. 4 .4Fig. 4. Word frequency analyses from \"Describe in your own words what AI is\" prior to (left) and after the intervention (right).", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Fig. 5 .5Fig. 5. Word frequency analyses from \"Describe in your own words what conversational AI is\" prior to (left) and after the intervention (right). Notice the relative increase in the words, conversation, back, and able, likely having to do with agents' abilities to have back and forth conversation.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Fig. 6 .6Fig. 6. The change in instances of tags from pre-to post-test for Conception questions. The change was calculated according to Eq. 1.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Types of questions asked by students to Alexa prior to the conversational AI programming intervention.", "figure_data": "TypeExample utterancesInstancesInformation updates What time is it?, How is the weather for Wednesday?, How is the traffic? 31 (26%)Action commandsSet a 15-minute timer, Play my Custom Spotify Playlist, Remind me that30 (25%)I have a meeting at 1:00 pm, What's 0 times 0?OtherHello, Learn my voice, Are dragons real?, What are all the numbers of24 (20%)pi?JokesTell me a joke, Can you tell me a joke?17 (14%)Personal questionsWhat's your favorite color?, When were you made?, What's your favorite16 (14%)video game?, How was your day?"}], "formulas": [], "doi": "10.1016/j.chb.2015.12.011"}
