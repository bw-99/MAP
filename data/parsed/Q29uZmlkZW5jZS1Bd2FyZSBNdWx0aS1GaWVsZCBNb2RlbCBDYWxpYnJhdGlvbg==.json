{
  "Confidence-Aware Multi-Field Model Calibration": "Yuang Zhao 2 ∗ , Chuhan Wu 1 ∗ , Qinglin Jia 1 ∗ , Hong Zhu 4 , Jia Yan 3 , Libin Zong 3 , 1 Noah's Ark Lab, Huawei Linxuan Zhang 2 † , Zhenhua Dong 1 † , Muyu Zhang 3 3 Huawei Petal Cloud Technology Co., Ltd. 4 Consumer Cloud Service Interactive Media BU, Huawei 2 Tsinghua University zhaoya22@mails.tsinghua.edu.cn,lxzhang@tsinghua.edu.cn {wuchuhan1,jiaqinglin2,zhuhong8,yanjia9,zonglibin,dongzhenhua,zhangmuyu}@huawei.com",
  "ABSTRACT": "Accurately predicting the probabilities of user feedback, such as clicks and conversions, is critical for advertisement ranking and bidding. However, there often exist unwanted mismatches between predicted probabilities and true likelihoods due to the rapid shift of data distributions and intrinsic model biases. Calibration aims to address this issue by post-processing model predictions, and fieldaware calibration can adjust model output on different feature field values to satisfy fine-grained advertising demands. Unfortunately, the observed samples corresponding to certain field values can be seriously limited to make confident calibrations, which may yield bias amplification and online disturbance. In this paper, we propose a confidence-aware multi-field calibration method, which adaptively adjusts the calibration intensity based on confidence levels derived from sample statistics. It also utilizes multiple fields for joint model calibration according to their importance to mitigate the impact of data sparsity on a single field. Extensive offline and online experiments show the superiority of our method in boosting advertising performance and reducing prediction deviations.",
  "CCS CONCEPTS": "· Information systems → Computational advertising ; Recommender systems .",
  "KEYWORDS": "Model Calibration, Confidence-aware, Multi-field, Online Advertising",
  "ACMReference Format:": "Yuang Zhao, Chuhan Wu, Qinglin Jia, Hong Zhu, Jia Yan, Libin Zong, Linxuan Zhang, Zhenhua Dong, Muyu Zhang. 2018. Confidence-Aware MultiField Model Calibration. In Proceedings of Make sure to enter the correct conference title from your rights confirmation emai (Conference acronym 'XX). ACM, New York, NY, USA, 8 pages. https://doi.org/XXXXXXX.XXXXXXX ∗ Equal contribution. † Corresponding authors. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference acronym 'XX, June 03-05, 2018, Woodstock, NY © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/18/06 https://doi.org/XXXXXXX.XXXXXXX Figure 1: An example of multi-field model calibration. Ad ID Slot ID Predicted CTR True CTR True CTR Predicted CTR=0.15, Calibrated CTR= ？ Field #Click #Impression CTR Ad ID 20 100 [0.133, 0.289] Slot ID 50 500 [0.076, 0.129] Feature: Ad ID, Slot ID, User ID…",
  "1 INTRODUCTION": "Estimating the real probabilities of user actions, such as clickthrough rate (CTR) and conversion rate (CVR), is a fundamental task in online advertising [3, 4, 41, 43]. In an ideal system, modelpredicted probabilities are expected to reflect the true likelihoods of click and conversion, e.g., the real CTR should match the predicted CTR under sufficiently many ad impressions [23, 33]. Unfortunately, over- or under-estimation phenomena of model predictions are prevalent in real-world recommender systems for various reasons, including the limitation of binary targets [2, 6], model inductive biases [1, 7, 47], and rapid shifts of online data distributions [13, 35, 42, 45]. This discrepancy often causes wasted advertising expenditure on ineffective ads, which damages the ROI of advertising campaigns and user experience [10]. Pioneering studies like NeuCalib [36] and AdaCalib [46] have focused on field-aware calibration by learning additional calibration neural networks to adjust original model predictions. However, samples corresponding to certain feature values are often too sparse to learn an unbiased and confident calibrating network. As shown in Fig. 1, the calculated CTR is 0.1 if an ad slot accumulates 50 clicks out of 500 impressions, while the 95% confidence interval of CTR is [0.076, 0.129] (calculated by the Wilson interval formula [48]). Such a wide range indicates strong uncertainty in field-aware calibration. Furthermore, according to our industrial practice, the volumes of click and impression data w.r.t. most field values are smaller than the example above, which poses a severe challenge to the confidence of calibration models. In addition, different fields may have diverse probability score distributions (Fig. 1 left), and ad platforms may Model calibration post-processes the predicted scores based on the observation of real data to align the gaps between predicted and real probabilities. Extensive studies have been conducted on global calibration, such as binning [31, 49, 50] and scaling [9, 18, 38], which adjust the predicted probabilities according to the posterior information of all observed samples. However, global calibration suffers from the bias counteraction problem, e.g., the overall bias can be very small if scores for one advertiser are over-estimated while those for another advertiser are under-estimated [36]. Therefore, field-aware calibration is more practical in real-world scenarios, which can calibrate samples with the same value in a certain target field (e.g., ad ID) and alleviate biases in different sample subsets [46]. Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Zhao and Wu, et al. select multiple fields as the business targets. Most existing methods mainly focus on single field calibration, and it is impractical to directly merge multiple fields due to the ultra sparsity of samples satisfying multiple field value conditions. The main contributions of this paper are summarized as follows: In this paper, we propose a confidence-aware multi-field calibration method named ConfCalib , which can perform fine-grained calibration on multiple feature fields and meanwhile keep robustness under data sparsity via adaptive adjustment of the confidence level. Concretely, we assume that user feedback follows a binomial distribution and use Wilson intervals to compute confidence-aware calibrated scores with a dynamic adjustment of calibration intensity. Moreover, we propose a simple yet effective multi-field fusion method to synthesize the calibration results on different fields, in order to further eliminate the influence of data sparsity. Compared with methods based on neural networks, our ConfCalib performs more robustly in scenarios with severe data sparsity. Extensive offline experiments in different datasets and online A/B test on our advertising platform demonstrate the effectiveness of our method. · We propose a confidence-aware model calibration method to perform adaptive calibration on recommendation model predictions based on field-wise data sparsity. · Experiments on different offline datasets and the A/B testing on our advertising platform verify the superiority of our method. · We introduce a simple yet effective joint calibration method on multiple fields to improve both calibration performance and robustness to data sparsity.",
  "2 RELATED WORK": "Model calibration is a classic problem in machine learning, which is proposed to eliminate the discrepancy between the predicted probabilities of classification models and the true likelihood [34, 39, 44]. It has been widely explored to promote accurate probability prediction in various applications, such as ad auctions [12, 26], weather forecasting [5, 8], and personalized medicine [15]. Scaling-based methods [9, 11, 18, 19, 22, 28, 38, 40] fit parametric mapping functions for the predicted scores. Platt Scaling [38] adopts a linear mapping, assuming that the scores of each class follow normal distributions with the same variance. Beta calibration [19], Gaussian calibration [22] and Gamma calibration [22] further assume more complicated data distributions and derive corresponding score mapping functions to adapt more general situations. Dirichlet calibration [18] generalizes Beta calibration to the multi-class classification by assuming a Dirichlet distribution. The above methods are limited to a few occasions due to the specific distribution assumptions. Temperature Scaling [9] considers that Calibration methods calibrate trained prediction models during training or inference phase. Methods integrated into training usually conduct calibration through well-designed loss functions, including AvUC [17], MMCE [21], Focal loss [24, 29] and SB-ECE [16], etc. Besides, label smoothing, which can be interpreted as a modified primary loss, also serves to model calibration [25, 30, 37]. This paper focus on post-hoc methods that calibrate model predictions by post-processing. According to the characteristics of different mapping functions, existing calibration methods can be divided into three categories: scaling-based, binning-based, and hybrid methods. the miscalibration of models comes from the overfitting of negative log-likelihood (NLL) during training, then proposes a temperature multiplier for logits of all classes to reduce test NLL and eliminate miscalibration. Although these methods have solid theoretical validity, they are difficult to deal with complex practical scenarios. Hybrid methods [6, 14, 20, 36, 46, 51] combine scaling and binning methods, which are superior in calibration performance and prevalence in industry. Scaling-binning [20] first fits a mapping function for the sorted samples and then employs histogram binning calibration on the mapped scores. Smoothed Isotonic Regression (SIR) [6] uses isotonic regression on bins and fits a monotonic piecewise linear calibration function. Recently, some works have focused on field-wise calibration, which aims to reduce the calibration error in a specific feature field. Neural Calibration [36] learns a neural isotonic line-plot scaling function and an auxiliary network directly taking features as input to capture feature field information. AdaCalib [46] extends the single global mapping function of Neural Calibration to different ones for each value of a certain feature field and dynamically adjusts the number of bins by neural networks. These neural methods may suffer from data sparsity in specific application scenarios where network training is hard to converge. Additionally, existing field-wise calibration methods simply consider one field and ignore potential relations among fields, meaning there is still a lack of research on multi-field calibration methods. Binning-based methods [27, 31, 32, 49, 50] divide samples into several bins after sorting them by predicted scores. Histogram Binning [49] adopts an equal-frequency binning and directly uses the mean positive rate in each bin as the calibrated scores. Bayesian Binning [31] considers multiple binning schemes and combines them using a derived Bayesian score to yield more robust calibrated predictions. To maintain the original order of model predictions, Isotonic Regression [50] adjusts the binning bounds by minimizing the residual between calibrated predictions and labels, hence ensuring monotonicity of the posterior probability in bins.",
  "3 METHODOLOGY": "The overall framework of our multi-field confidence-aware calibration method is shown in Fig. 2. It first performs confidence-aware calibration on multiple target feature fields independently and then fuses the scores calibrated on different feature fields into a unified one. The details of our method are introduced as follows.",
  "3.1 Problem Definition": "First, we briefly introduce the problem setting in this paper, namely field-level calibration on the binary classification problem. Suppose we have a trained binary classification model 𝑓 (·) to provide predictions ˆ 𝑝 = 𝑓 ( 𝑥 ) on a dataset { 𝑥 𝑖 , 𝑦 𝑖 } 𝑁 𝑖 = 0 , where 𝑥 𝑖 denotes the features with multiple fields and 𝑦 𝑖 denotes the binary label. We generally consider biases on the level of sample subsets since the true likelihood of a specific sample is unobservable. For a specific sample subset, the average predicted probability usually deviates from the observed positive sample rate, thus requiring calibrations. Assume there are 𝐾 target feature fields and the value of each field is denoted as 𝑧 𝑗 ( 𝑗 = 1 , 2 , . . . , 𝐾 ) , the calibration model fits a transformation function 𝑔 (· ; 𝑧 1 , . . . , 𝑧 𝐾 ) and gives calibrated probability ˆ 𝑝 calib = 𝑔 ( ˆ 𝑝 ; 𝑧 1 , . . . , 𝑧 𝐾 ) according to sample statistics on target Confidence-Aware Multi-Field Model Calibration Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Figure 2: The overall framework of our ConfCalib method. Compute Deviation …… Field Name Field Value User ID 1000 Ad ID 101 … … Slot ID 15 … … Log Database Target Field 1 Target Field 𝐾 Number of Positive Samples 𝑃1 Number of Negative Samples 𝑁1 Number of Positive Samples 𝑃𝐾 Number of Negative Samples 𝑁𝐾 Predicted Probability 𝑦 Calibrate Deviation ො 𝑦1 Calibrated Score ො 𝑦 Adaptive Fusion Calibrated Probability Compute Deviation Calibrate Deviation ො 𝑦𝐾 Calibrated Probability Predicted Probability 𝑦 …… Score 0.1 fields. The goal of field-level calibration on a certain field is to minimize the field-level calibration error, such as Field-RCE [36]. For multi-field calibration, we can employ metrics like ECE [31] and MVCE [14] to measure the global calibration error. The above metrics will be detailed in Section 4.1.4. where 𝑧 is the deviation score (e.g., 𝑧 = 1 . 96 for 95% confidence).",
  "3.2 Confidence-aware Calibration": "Next, we introduce our confidence-aware calibration method. In practical scenarios, field-level calibration faces severe data sparsity issues due to the limited time window for data collection and intrinsic sparsity of certain user feedback, such as conversions. Consequently, calibrations on feature fields with numerous values are neither accurate nor confident. The main problem caused by data sparsity is that the observed probability is not credible enough to reflect the true data distribution. In this case, complete trust in limited observations will result in overfitting and harm generalization. In summary, it is necessary to preserve some trust in model predictions while referencing observations under data sparsity. The most commonly used CI formula is the normal interval. However, due to the limitation of the central limit theorem, it is unreliable with a small sample size or an extreme probability close to 0 or 1, which is quite common for user feedback prediction in recommender systems. Therefore, we adopt the Wilson confidence interval [48], which can be safely employed under the special circumstances above. The formula is Most user feedback, like clicks and conversions, can be naturally modeled using binomial distributions. Given the observed positive sample ratio 𝑝 and the number of total samples 𝑛 , we can use a confidence interval formula to compute the confidence interval (CI) ( 𝑝 -, 𝑝 + ) of the real feedback probability under a certain confidence level. Conversely, given a certain predicted feedback probability, we can obtain the corresponding confidence level by regarding the predicted value as the bound of CI. In other words, there is a mapping between the predicted probability and the confidence level, which is the core of our confidence-aware calibration method.  In order to perform field-level calibration, our method operates at the level of sample subsets, which can be obtained by splitting samples based on different values of the target feature field. Besides, field-wise binning 1 , i.e., further dividing each subset into several bins by order of their predicted scores, is optional. For each sample subset, we need the statistics, including the number of samples 𝑛 , the ratio of positive samples 𝑝 , and the average predicted score ˆ 𝑝 . Regard ˆ 𝑝 as a certain bound 2 of CI and substitute these variables into Eq. (1), we can solve the deviation score 𝑧 ∈ [ 0 , +∞) using numerical algorithms, such as the bisection method. A larger 𝑧 indicates a greater bias between model predictions and current observations, but a higher confidence level as well. We can directly use 𝑝 as the calibrated score, i.e., change 𝑧 to 0, to entirely eliminate biases. However, this will minimize the confidence level and result in poor generalization under data sparsity. Therefore, we intend to make the calibrated score between original 𝑝 and predicted ˆ 𝑝 , thus corresponding to a relatively high confidence. To this end, we propose a non-linear transformation function 𝑔 ( 𝑧 ) to reduce the deviation. Intuitively, 𝑔 ( 𝑧 ) should satisfy the following requirements: (1) passing through the origin: 𝑔 ( 0 ) = 0; (2) monotonic: ∀ 𝑧 1 ≤ 𝑧 2 , 𝑔 ( 𝑧 1 ) ≤ 𝑔 ( 𝑧 2 ) ; (3) bounded: ∃ 𝐶 ∈ R + , 𝑔 (+∞) ≤ 𝐶 . Thus, we devise a variant of the sigmoid function as follows:  where 𝜆 is a hyperparameter used to provide an upper bound and control the calibration intensity manually. A larger calibration intensity can result in a smaller 𝑧 , meaning more belief in observations and less belief in original predictions. Obviously, we have 𝑔 (+∞) = 𝜆 so that a large deviation is calibrated to a small one (e.g., the deviation corresponding to 95% confidence level if 𝜆 = 1 . 96), and minor deviations are reduced almost linearly. By replacing the original 𝑧 in Eq. (1) with the transformed value 𝑔 ( 𝑧 ) , we can obtain a new CI, then one of its bounds is the desired confidence-aware calibrated score ˆ 𝑝 ′ . In this way, given a test sample located in the 1 Binning is optional in our method since it introduces heavier data sparsity. 2 Let 𝑝 + = ˆ 𝑝 if ˆ 𝑝 > 𝑝 , else 𝑝 -= ˆ 𝑝 . Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Zhao and Wu, et al. Figure 3: The curves of calibrated scores w.r.t. different predicted scores under different numbers of observed samples. bserved CVR=0 pCVR-0.005 pCVR-0.01 pCVR=0.05 pCVR=0. Observed CVR erved CVR-0.02 pCVR-0.005 pCVR-0.01 pCVR=0. Observed CVR pCVR-0.005 pCVR=0.01 pCVR-0.05 pCVR=0.1 Observed CVR pCVR=0.005 pCVR-0.01 pCVR-0.05 pCVR-0.1 Observed CVR same subset, we compute a field-wise calibrated score 𝑝 calib by scaling its original predicted probability 𝑝 pred , i.e.,  fusion method allows us to conduct joint calibration based on the marginal distribution of each field independently instead of dealing with the joint distribution, thus avoiding further sparsity of samples. Fig. 3 shows the curves of calibrated probabilities w.r.t variant predicted probabilities under different observed sample numbers and positive sample rates, taking the conversion rate prediction task as an example. We uniformly set 𝜆 = 2 for the z-score transformation function. The curves demonstrate that the predicted probability is calibrated towards the observed one, and the proximity to the observation increases as the number of samples increases. Overall, our confidence-aware method adjusts model-predicted probabilities by resetting the confidence level, which considers the observation and prediction together to avoid violent value changes and overfitting limited observations. Consequently, it keeps robust under severe data sparsity, especially circumstances with few observed samples and zero positive samples.",
  "3.3 Multi-field Joint Calibration": "Previous field-level calibration methods only calibrate and evaluate errors in a certain field, ignoring other fields. However, the fieldlevel calibration error in different fields may vary greatly, which is ignored by single-field calibration. In addition, the biases in different fields, as reflections of the global bias from different perspectives, have a certain degree of commonality. Therefore, data sparsity faced by a certain field is expected to be alleviated through calibrations on other fields. A naive way to extend calibration from a single field to multiple fields is to split sample subsets based on the combination of feature values of all the fields. Nevertheless, it will cause severer data sparsity since fewer samples satisfy multiple field value conditions. Accordingly, prior single-field calibration methods cannot be directly extended to multi-field calibration. In contrast, our method demonstrates a better extensibility among fields. The confidenceaware calibration in a single field in Section 3.2 is actually a scaling operation for a specific sample, thus allowing a generalization to multi-field joint calibration. Assume that there are 𝐾 target fields to calibrate. For each sample, we can obtain 𝐾 scaling multipliers 𝑚 1 , 𝑚 2 , . . . , 𝑚 𝐾 , where 𝑚 𝑖 = ˆ 𝑝 𝑖 ′ / ˆ 𝑝 𝑖 for the 𝑖 -th field. We adaptively fuse these multipliers via the weighted geometric mean, that is  where 𝑤 1 , 𝑤 2 , . . . , 𝑤 𝐾 is weights satisfying ˝ 𝐾 𝑖 = 1 𝑤 𝑖 = 1. The fusion weights can be precisely obtained by grid search. The proposed",
  "3.4 Online Deployment": "Fig. 4 shows the online deployment diagram of ConfCalib in our advertising platform. On the offline side, the prediction model's training and ConfCalib's fitting are paralleled and completely independent. At fixed intervals, the calibration module pulls the latest online samples from the log database and refits calibration mappings. This interval is usually set relatively shorter (e.g., 30 minutes) than the prediction model update period to better capture the dynamic shift of online data. On the online side, prediction and calibration are conducted in sequence. First, the prediction model makes predictions for candidate objects, then the calibration model calibrates the predicted probabilities and outputs the calibrated probabilities for subsequent use. Recent methods designed for advertising platforms, such as NeuCalib and AdaCalib, involve auxiliary neural networks. These calibration models are coupled with original prediction models during the training phase, thus increasing the complexity of online deployment. On the contrary, our ConfCalib relies entirely on posterior probabilities of online samples and fits parameters without training networks. Therefore, it can be directly inserted into the pipeline without interfering with the training stage of prediction models, resulting in lower deployment costs. Figure 4: Online deployment diagram of ConfCalib. Log Database candidates Predict Calibrate 𝑝pred 𝑝calib Training Dataset (Long window) Calibration Dataset (Short window) train Prediction model Calibration model fit update every 𝑚 minutes online offline update every 𝑛 hours Confidence-Aware Multi-Field Model Calibration Conference acronym 'XX, June 03-05, 2018, Woodstock, NY",
  "4 EXPERIMENTS": "",
  "4.1 Experimental Setup": "4.1.1 Settings. The pipeline of our experiment is as follows. Each dataset is conventionally split into three parts in chronological order, i.e., training, validation, and test set. Firstly, we train a base neural network on the training set as the prediction model and fix it. Then, the base model makes predictions for validation set samples to fit (or train) calibration models. Finally, we use the test set to evaluate the performance of calibration methods. For all the datasets and methods, the prediction model is a fully connected network with four layers. Hyperparameters of baselines are set according to the original papers or carefully tuned. Results of neural methods are the average results of five random experiments with means and standard deviations reported. For our ConfCalib, we set the number of bins as {50, 10} and 𝜆 in Eq. (2) as {1.0, 1.6} for Avazu and our industrial dataset. Table 1: Data statistics of the public and industrial datasets. 4.1.2 Datasets. We conduct experiments on our industrial dataset and a public dataset Avazu 3 . Data statistics are shown in Table 1. Avazu is a popular dataset on mobile ad CTR prediction with 40.4 million samples in 10 days ordered by the impression time. We split the dataset by day in a ratio of 7:1:2 for training, validation, and test. For our multi-field joint calibration method, the target fields are set to \"site_id\" (3480) 4 , \"app_id\" (4864) and \"app_domain\" (305). For other methods considering a single field, the target field is set to \"site_id\". The industrial dataset is collected from the click logs on our ad platform in 10 consecutive days, and the prediction target is conversion, with a split ratio of 7:1:2 by day. For our method, the target fields are \"advertiser_id\" (529), \"app_size\" (82), and \"app_id\" (56). For other methods, the target field is \"advertiser_id\". The ratios of validation sets are relatively small while splitting, which is more in line with the short time window of online data collection. 4.1.3 Baselines. We choose several representative methods of the three categories as baselines for comparison. The scaling-based methods include naive scaling (multiply all predicted scores by a value 𝑘 to make their mean equal to the positive sample ratio), Platt Scaling (PlattScaling) [38], Gaussian Calibration (GaussCalib) [22] and Gamma Calibration (GammaCalib) [22]. The binning-based methods include Histogram Binning (HistoBin) [49] and Isotonic Regression (IsoReg) [50]. The hybrid methods include Smoothed Isotonic Regression (SIR) [6], Neural Calibration (NeuCalib) [36] and AdaCalib [46], with the latter two methods based on neural networks. The number of bins for NeuCalib and AdaCalib is set according to the initial settings when proposed. 4.1.4 Evaluation Metrics. Referring to prior methods, we continue to use field-level relative calibration error (Field-RCE or F-RCE) [36] 3 https://kaggle.com/competitions/avazu-ctr-prediction 4 Represent the number of possible values of the corresponding field. to evaluate the calibration error on a certain field, which formula is  where D is the dataset, Z is the possible value set of the target field and D 𝑧 is the subset of samples with the target field value 𝑧 satisfying ˝ 𝑧 ∈Z |D 𝑧 | = |D| . Intuitively, a lower Field-RCE indicates lower biases along the target field. Furthermore, we use the mean Field-RCE on multiple fields (Multi-Field-RCE or MF-RCE) to measure the multi-field calibration performance. Meanwhile, we adopt expected calibration error (ECE) [31] and multi-view calibration error (MVCE) [14] to evaluate the global calibration error. ECE arranges all predicted values in order and divides them equally into several subsets, i.e., a binning operation. Then it is calculated as  where 𝑇 is the number of bins and D 𝑡 is the subset of samples in the 𝑡 -th bin. MVCE iteratively shuffles the dataset and calculates ECE until the accumulative mean ECE converges. The formula is  where 𝑅 is the number of shuffle operations, 𝑇 is the number of bins, and D 𝑟,𝑡 is the subset of samples in the 𝑡 -th bin after the 𝑟 -th shuffle. MVCE allows us to inspect the calibration error from a more global perspective beyond the level of feature fields, thereby highlighting the effectiveness of multi-field calibration. Additionally, we also report AUC and LogLoss to evaluate the ranking performance after calibration since our method and primary baselines are not globally isotonic.",
  "4.2 Results": "4.2.1 Main Results. The main results on each dataset are shown in Table 2. Our method shows better calibration performance compared with baselines. It achieves lower calibration errors on both field-wise and global metrics compared to the uncalibrated model and almost all the baselines. Besides, our method also achieves higher AUC, indicating an improvement in ranking ability. Scaling and binning-based baselines show poor performances. They reduce global errors to some extent through global calibration while powerless against field-wise calibration errors. Field-level calibration methods such as NeuCalib and AdaCalib achieve low calibration errors on the single target field among baselines. However, they are still inferior to our method, especially on multi-field metrics like Multi-Field-RCE and global metrics like ECE, demonstrating the effectiveness of our multi-field joint calibration. The two neural methods show large variance, which means unstable learning on sparse data. On the contrary, our method only relies on posterior statistical information and maintains a stable calibration effect. Moreover, AdaCalib obtains a lower AUC than the uncalibrated model on our industrial dataset, indicating that complex neural calibration methods may be counterproductive under data sparsity. 4.2.2 Ablation Study. We report the results of several variants of our ConfCalib on Avazu CTR task in Fig. 5. Concretely, we Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Zhao and Wu, et al. Table 2: The performance of different methods in terms of prediction accuracy and miscalibration degrees. ' ↑ ' means higher is better, ' ↓ ' means lower is better. Figure 5: Results of the ablation study on several variants of ConfCalib. respectively remove the confidence-aware calibration (i.e., turn into a field-wise naive calibration), the nonlinear z-score mapping function (replaced by a single linear mapping), and the multi-field joint calibration from the original ConfCalib. The removal of our confidence-aware calibration results in increase in field-wise calibration errors and decrease in AUC. After replacing the mapping function with a single linear function 𝑔 ( 𝑧 ) = 𝜆𝑧 , AUC and all calibration error metrics deteriorate. This validates that the bounded mapping can provide proper calibration intensity compared with simple scaling. Finally, removing the joint calibration on multiple fields significantly increases both field-wise and global calibration errors and causes a large decline in AUC.",
  "4.3 Recalibration on Learning-Based Baselines": "Table 3: Results of recalibration on learning-based baselines. Part of the baselines are learning-based, such as GaussCalib, GammaCalib, NeuCalib, and AdaCalib. Since our ConfCalib does not require extra model training and has a low deployment cost, it can recalibrate the calibrated output of other methods for further improvement. We experiment on Avazu CTR task. First, we fit our method and baselines to be recalibrated separately on the validation set. Then, baselines make calibrated predictions for the test set. Finally, our method conducts recalibration on the calibrated outputs. Table 3 shows that recalibration achieves further improvement compared with the original results of other methods. Profiting from multi-field joint calibration, recalibration using ConfCalib significantly reduces Multi-Field-RCE and the global calibration error MVCE. Besides, recalibration on methods that strictly keep the order of original predictions, such as GaussCalib and GammaCalib, can provide an improvement in AUC as well.",
  "4.4 Robustness Analysis against Data Sparsity": "One primary advantage of ConfCalib is its robustness against data sparsity. Due to the limited time window for online data collection, samples available to fit calibration models can be very few. Under such circumstances, methods based on neural networks may be Confidence-Aware Multi-Field Model Calibration Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Figure 6: Results of robustness analysis of different data sparsity levels. Figure 7: Fusion weights in multi-field joint calibration. \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 ineffective or even counterproductive. Here, we reduce the validation dataset used for calibration in Avazu with various sample rates to simulate scenarios with varying degrees of data sparsity. The calibration performances w.r.t different sparsity degrees are shown in Fig. 6. As the number of samples for fitting the calibration model decreases, the performances of NeuCalib, AdaCalib get worse evidently, while our ConfCalib is little affected and exhibits strong robustness against data sparsity.",
  "4.5 Analysis of Multi-field Fusion Weights": "The multi-field joint calibration of ConfCalib obtains fusion weights for multiple target fields by minimizing the calibration error on single or multiple fields. Here, we select three target fields, \"ad_type\", \"consume_frequency\", and \"price\" in our industrial dataset, and optimize Field-RCE of each field and Multi-Field-RCE using grid search to acquire four groups of fusion weights separately, as shown in Fig. 7. Each column takes Field-RCE of the corresponding field as the optimization object. We can see that each field makes effects with different weights when optimizing Multi-Field-RCE of all three fields. When optimizing Field-RCE on a single field, the weights of other fields are not zero, indicating that joint calibration on multiple fields can also reduce the single-field calibration error. The weight of \"ad_type\" is generally the highest, suggesting that this field can better reflect global biases. Nevertheless, when Field-RCE on \"ad_type\" is minimized, the weight of the other field \"consume_frequency\" is the highest instead. This indicates that calibration on a single field is insufficient, and multi-field calibration can provide effective assistance.",
  "4.6 Hyperparameter Analysis": "Our method has a single hyperparameter 𝜆 in the z-score mapping function Eq. (2) to control the calibration strength manually. Its main effect is to select a relatively higher confidence level when observed samples are adequate, thus making the calibrated prediction closer to the observation. Here, we discuss the influence of different settings of 𝜆 . We set 𝜆 = 0 , 0 . 2 , 0 . 4 . . . , 3 . 0 and experiment on the Figure 8: Calibration errors w.r.t hyperparameter 𝜆 . industrial dataset. Results are shown in Fig. 8. Overall, the variation of field-wise calibration errors like Field-RCE and Multi-Field-RCE is within a small range as 𝜆 changes. For global calibration error metrics, the change of ECE is relatively violent when 𝜆 is significant, while MVCE remains almost the same. When 𝜆 = 0, the awareness of confidence is removed, which means we disregard the model prediction and believe entirely in the observation. We achieve a local optimum at this point. As 𝜆 increases, the calibration intensity gradually decreases, and the global optimal 𝜆 is around 1.6.",
  "4.7 Online A/B Test": "We conduct one week's online A/B test separately on two industrial scenarios to evaluate ConfCalib's effectiveness in practical applications. In each scenario, we randomly split users into two groups, each with tens of millions of users daily. One group receives recommendations from ConfCalib, while the other gets recommendations from a well-crafted baseline. On Huawei's online advertising platform, ConfCalib achieves a relative improvement of 2.42% in CVR compared with the baseline. On Top-grossing of Huawei AppGallery, ConfCalib obtains relative improvement of 32.6% and 49.1% in CTR and revenue. Top-grossing is a small-scale scenario where data distribution shifts frequently, and ConfCalib can better adapt this variability to achieve a significant improvement.",
  "5 CONCLUSION": "In this paper, we propose a confidence-aware multi-field model calibration method, ConfCalib, to address the common phenomenon of data sparsity in model calibration for advertising recommendation. ConfCalib dynamically adjusts calibration intensity based on the confidence level obtained from the observed data distribution. Moreover, it considers biases along multiple feature fields and introduces joint calibration to capture various bias patterns and further alleviate the impact of data sparsity. We conduct sufficient offline and online experiments. Results demonstrate that ConfCalib outperforms prior methods and exhibits strong robustness against data sparsity. ConfCalib is currently running on Huawei's advertising platform and AppGallery and has achieved continuous online revenue improvement. Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Zhao and Wu, et al.",
  "REFERENCES": "[1] Yu Bai, Song Mei, Huan Wang, and Caiming Xiong. 2021. Don't just blame over-parametrization for over-confidence: Theoretical analysis of calibration in binary classification. In ICML . PMLR, 566-576. [3] Olivier Chapelle, Eren Manavoglu, and Romer Rosales. 2014. Simple and scalable response prediction for display advertising. ACM Transactions on Intelligent Systems and Technology (TIST) 5, 4 (2014), 1-34. [2] Antonio Bella, Cèsar Ferri, José Hernández-Orallo, and María José RamírezQuintana. 2010. Calibration of machine learning models. In Handbook of Research on Machine Learning Applications and Trends: Algorithms, Methods, and Techniques . IGI Global, 128-146. [4] Junxuan Chen, Baigui Sun, Hao Li, Hongtao Lu, and Xian-Sheng Hua. 2016. Deep ctr prediction in display advertising. In ACM MM . 811-820. [6] Chao Deng, Hao Wang, Qing Tan, Jian Xu, and Kun Gai. 2021. Calibrating user response predictions in online advertising. In ECML-PKDD . Springer, 208-223. [5] Morris H DeGroot and Stephen E Fienberg. 1983. The comparison and evaluation of forecasters. Journal of the Royal Statistical Society: Series D (The Statistician) 32, 1-2 (1983), 12-22. [7] Jakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi, Mohsin Ali, Jongseok Lee, Matthias Humt, Jianxiang Feng, Anna Kruspe, Rudolph Triebel, Peter Jung, Ribana Roscher, et al. 2023. A survey of uncertainty in deep neural networks. Artificial Intelligence Review 56, Suppl 1 (2023), 1513-1589. [9] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. 2017. On calibration of modern neural networks. In ICML . PMLR, 1321-1330. [8] Tilmann Gneiting and Adrian E Raftery. 2005. Weather forecasting with ensemble methods. Science 310, 5746 (2005), 248-249. [10] Yuyao Guo, Haoming Li, Xiang Ao, Min Lu, Dapeng Liu, Lei Xiao, Jie Jiang, and Qing He. 2022. Calibrated Conversion Rate Prediction via Knowledge Distillation under Delayed Feedback in Online Advertising. In CIKM . 3983-3987. [12] Xinran He, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Yanxin Shi, Antoine Atallah, Ralf Herbrich, Stuart Bowers, et al. 2014. Practical lessons from predicting clicks on ads at facebook. In AdKDD . 1-9. [11] Kartik Gupta, Amir Rahimi, Thalaiyasingam Ajanthan, Thomas Mensink, Cristian Sminchisescu, and Richard Hartley. 2021. Calibration of Neural Networks using Splines. In ICLR . https://openreview.net/forum?id=eQe8DEWNN2W [13] Rui Huang, Andrew Geng, and Yixuan Li. 2021. On the importance of gradients for detecting distributional shifts in the wild. NeurIPS 34 (2021), 677-689. [15] Xiaoqian Jiang, Melanie Osl, Jihoon Kim, and Lucila Ohno-Machado. 2012. Calibrating predictive model estimates to support personalized medicine. JAMIA 19, 2 (2012), 263-274. [14] Siguang Huang, Yunli Wang, Lili Mou, Huayue Zhang, Han Zhu, Chuan Yu, and Bo Zheng. 2022. MBCT: Tree-Based Feature-Aware Binning for Individual Uncertainty Calibration. In WWW . 2236-2246. [16] Archit Karandikar, Nicholas Cain, Dustin Tran, Balaji Lakshminarayanan, Jonathon Shlens, Michael C Mozer, and Becca Roelofs. 2021. Soft calibration objectives for neural networks. NeurIPS 34 (2021), 29768-29779. [18] Meelis Kull, Miquel Perello Nieto, Markus Kängsepp, Telmo Silva Filho, Hao Song, and Peter Flach. 2019. Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with dirichlet calibration. NeurIPS 32 (2019). [17] Ranganath Krishnan and Omesh Tickoo. 2020. Improving model calibration with accuracy versus uncertainty optimization. NeurIPS 33 (2020), 18237-18248. [19] Meelis Kull, Telmo Silva Filho, and Peter Flach. 2017. Beta calibration: a wellfounded and easily implemented improvement on logistic calibration for binary classifiers. In AISTATS . PMLR, 623-631. [21] Aviral Kumar, Sunita Sarawagi, and Ujjwal Jain. 2018. Trainable calibration measures for neural networks from kernel mean embeddings. In ICML . PMLR, 2805-2814. [20] Ananya Kumar, Percy S Liang, and Tengyu Ma. 2019. Verified uncertainty calibration. NeurIPS 32 (2019). [22] Wonbin Kweon, SeongKu Kang, and Hwanjo Yu. 2022. Obtaining Calibrated Probabilities with Personalized Ranking Models. In AAAI , Vol. 36. 4083-4091. [24] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. 2017. Focal loss for dense object detection. In ICCV . 2980-2988. [23] Kuang-chih Lee, Burkay Orten, Ali Dasdan, and Wentong Li. 2012. Estimating conversion rate in display advertising from past erformance data. In KDD . 768776. [25] Bingyuan Liu, Ismail Ben Ayed, Adrian Galdran, and Jose Dolz. 2022. The devil is in the margin: Margin-based label smoothing for network calibration. In CVPR . 80-88. [27] Aditya Krishna Menon, Xiaoqian J Jiang, Shankar Vembu, Charles Elkan, and Lucila Ohno-Machado. 2012. Predicting accurate probabilities with a ranking loss. In ICML , Vol. 2012. NIH Public Access, 703. [26] H Brendan McMahan, Gary Holt, David Sculley, Michael Young, Dietmar Ebner, Julian Grady, Lan Nie, Todd Phillips, Eugene Davydov, Daniel Golovin, et al. 2013. Ad click prediction: a view from the trenches. In KDD . 1222-1230. [28] Azadeh Sadat Mozafari, Hugo Siqueira Gomes, Wilson Leão, Steeven Janny, and Christian Gagné. 2018. Attended temperature scaling: a practical approach for calibrating deep neural networks. arXiv preprint arXiv:1810.11586 (2018). [30] Rafael Müller, Simon Kornblith, and Geoffrey E Hinton. 2019. When does label smoothing help? NeurIPS 32 (2019). [29] Jishnu Mukhoti, Viveka Kulharia, Amartya Sanyal, Stuart Golodetz, Philip Torr, and Puneet Dokania. 2020. Calibrating deep neural networks using focal loss. NeurIPS 33 (2020), 15288-15299. [31] Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. 2015. Obtaining well calibrated probabilities using bayesian binning. In AAAI , Vol. 29. [33] Alexandru Niculescu-Mizil and Rich Caruana. 2005. Predicting good probabilities with supervised learning. In ICML . 625-632. [32] Brian Neelon and David B Dunson. 2004. Bayesian isotonic regression and trend analysis. Biometrics 60, 2 (2004), 398-406. [34] Jeremy Nixon, Michael W Dusenberry, Linchuan Zhang, Ghassen Jerfel, and Dustin Tran. 2019. Measuring Calibration in Deep Learning.. In CVPR workshops , Vol. 2. [36] Feiyang Pan, Xiang Ao, Pingzhong Tang, Min Lu, Dapeng Liu, Lei Xiao, and Qing He. 2020. Field-aware calibration: a simple and empirically strong method for reliable probabilistic predictions. In WWW . 729-739. [35] Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek. 2019. Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift. NeurIPS 32 (2019). [37] Gabriel Pereyra, George Tucker, Jan Chorowski, Łukasz Kaiser, and Geoffrey Hinton. 2017. Regularizing neural networks by penalizing confident output distributions. arXiv preprint arXiv:1701.06548 (2017). [39] Geoff Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, and Kilian Q Weinberger. 2017. On fairness and calibration. NeurIPS 30 (2017). [38] John Platt et al. 1999. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in large margin classifiers 10, 3 (1999), 61-74. [40] Amir Rahimi, Amirreza Shaban, Ching-An Cheng, Richard Hartley, and Byron Boots. 2020. Intra order-preserving functions for calibration of multi-class neural networks. NeurIPS 33 (2020), 13456-13467. [42] Zheyan Shen, Jiashuo Liu, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, and Peng Cui. 2021. Towards out-of-distribution generalization: A survey. arXiv preprint arXiv:2108.13624 (2021). [41] Matthew Richardson, Ewa Dominowska, and Robert Ragno. 2007. Predicting clicks: estimating the click-through rate for new ads. In WWW . 521-530. [43] Hongyan Tang, Junning Liu, Ming Zhao, and Xudong Gong. 2020. Progressive layered extraction (ple): A novel multi-task learning (mtl) model for personalized recommendations. In RecSys . 269-278. [45] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip Yu. 2022. Generalizing to unseen domains: A survey on domain generalization. TKDE (2022). [44] Yoav Wald, Amir Feder, Daniel Greenfeld, and Uri Shalit. 2021. On calibration and out-of-domain generalization. NeurIPS 34 (2021), 2215-2227. [46] Penghui Wei, Weimin Zhang, Ruijie Hou, Jinquan Liu, Shaoguo Liu, Liang Wang, and Bo Zheng. 2022. Posterior Probability Matters: Doubly-Adaptive Calibration for Neural Predictions in Online Advertising. In SIGIR . 2645-2649. [48] Edwin B Wilson. 1927. Probable inference, the law of succession, and statistical inference. JASA 22, 158 (1927), 209-212. [47] Jonathan Wenger, Hedvig Kjellström, and Rudolph Triebel. 2020. Non-parametric calibration for classification. In AISTATS . PMLR, 178-190. [49] Bianca Zadrozny and Charles Elkan. 2001. Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers. In ICML , Vol. 1. 609-616. [51] Jize Zhang, Bhavya Kailkhura, and T Yong-Jin Han. 2020. Mix-n-match: Ensemble and compositional methods for uncertainty calibration in deep learning. In ICML . PMLR, 11117-11128. [50] Bianca Zadrozny and Charles Elkan. 2002. Transforming classifier scores into accurate multiclass probability estimates. In KDD . 694-699.",
  "keywords_parsed": [
    "Model Calibration",
    "Confidence-aware",
    "Multi-field",
    "Online Advertising"
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "Don't just blame over-parametrization for over-confidence: Theoretical analysis of calibration in binary classification"
    },
    {
      "ref_id": "b3",
      "title": "Simple and scalable response prediction for display advertising"
    },
    {
      "ref_id": "b2",
      "title": "Calibration of machine learning models"
    },
    {
      "ref_id": "b4",
      "title": "Deep ctr prediction in display advertising"
    },
    {
      "ref_id": "b6",
      "title": "Calibrating user response predictions in online advertising"
    },
    {
      "ref_id": "b5",
      "title": "The comparison and evaluation of forecasters"
    },
    {
      "ref_id": "b7",
      "title": "A survey of uncertainty in deep neural networks"
    },
    {
      "ref_id": "b9",
      "title": "On calibration of modern neural networks"
    },
    {
      "ref_id": "b8",
      "title": "Weather forecasting with ensemble methods"
    },
    {
      "ref_id": "b10",
      "title": "Calibrated Conversion Rate Prediction via Knowledge Distillation under Delayed Feedback in Online Advertising"
    },
    {
      "ref_id": "b12",
      "title": "Practical lessons from predicting clicks on ads at facebook"
    },
    {
      "ref_id": "b11",
      "title": "Calibration of Neural Networks using Splines"
    },
    {
      "ref_id": "b13",
      "title": "On the importance of gradients for detecting distributional shifts in the wild"
    },
    {
      "ref_id": "b15",
      "title": "Calibrating predictive model estimates to support personalized medicine"
    },
    {
      "ref_id": "b14",
      "title": "MBCT: Tree-Based Feature-Aware Binning for Individual Uncertainty Calibration"
    },
    {
      "ref_id": "b16",
      "title": "Soft calibration objectives for neural networks"
    },
    {
      "ref_id": "b18",
      "title": "Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with dirichlet calibration"
    },
    {
      "ref_id": "b17",
      "title": "Improving model calibration with accuracy versus uncertainty optimization"
    },
    {
      "ref_id": "b19",
      "title": "Beta calibration: a well-founded and easily implemented improvement on logistic calibration for binary classifiers"
    },
    {
      "ref_id": "b21",
      "title": "Trainable calibration measures for neural networks from kernel mean embeddings"
    },
    {
      "ref_id": "b20",
      "title": "Verified uncertainty calibration"
    },
    {
      "ref_id": "b22",
      "title": "Obtaining Calibrated Probabilities with Personalized Ranking Models"
    },
    {
      "ref_id": "b24",
      "title": "Focal loss for dense object detection"
    },
    {
      "ref_id": "b23",
      "title": "Estimating conversion rate in display advertising from past performance data"
    },
    {
      "ref_id": "b25",
      "title": "The devil is in the margin: Margin-based label smoothing for network calibration"
    },
    {
      "ref_id": "b27",
      "title": "Predicting accurate probabilities with a ranking loss"
    },
    {
      "ref_id": "b26",
      "title": "Ad click prediction: a view from the trenches"
    },
    {
      "ref_id": "b28",
      "title": "Attended temperature scaling: a practical approach for calibrating deep neural networks"
    },
    {
      "ref_id": "b30",
      "title": "When does label smoothing help?"
    },
    {
      "ref_id": "b29",
      "title": "Calibrating deep neural networks using focal loss"
    },
    {
      "ref_id": "b31",
      "title": "Obtaining well calibrated probabilities using bayesian binning"
    },
    {
      "ref_id": "b33",
      "title": "Predicting good probabilities with supervised learning"
    },
    {
      "ref_id": "b32",
      "title": "Bayesian isotonic regression and trend analysis"
    },
    {
      "ref_id": "b34",
      "title": "Measuring Calibration in Deep Learning."
    },
    {
      "ref_id": "b36",
      "title": "Field-aware calibration: a simple and empirically strong method for reliable probabilistic predictions"
    },
    {
      "ref_id": "b35",
      "title": "Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift"
    },
    {
      "ref_id": "b37",
      "title": "Regularizing neural networks by penalizing confident output distributions"
    },
    {
      "ref_id": "b39",
      "title": "On fairness and calibration"
    },
    {
      "ref_id": "b38",
      "title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods"
    },
    {
      "ref_id": "b40",
      "title": "Intra order-preserving functions for calibration of multi-class neural networks"
    },
    {
      "ref_id": "b42",
      "title": "Towards out-of-distribution generalization: A survey"
    },
    {
      "ref_id": "b41",
      "title": "Predicting clicks: estimating the click-through rate for new ads"
    },
    {
      "ref_id": "b43",
      "title": "Progressive layered extraction (ple): A novel multi-task learning (mtl) model for personalized recommendations"
    },
    {
      "ref_id": "b45",
      "title": "Generalizing to unseen domains: A survey on domain generalization"
    },
    {
      "ref_id": "b44",
      "title": "On calibration and out-of-domain generalization"
    },
    {
      "ref_id": "b46",
      "title": "Posterior Probability Matters: Doubly-Adaptive Calibration for Neural Predictions in Online Advertising"
    },
    {
      "ref_id": "b48",
      "title": "Probable inference, the law of succession, and statistical inference"
    },
    {
      "ref_id": "b47",
      "title": "Non-parametric calibration for classification"
    },
    {
      "ref_id": "b49",
      "title": "Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers"
    },
    {
      "ref_id": "b51",
      "title": "Mix-n-match: Ensemble and compositional methods for uncertainty calibration in deep learning"
    },
    {
      "ref_id": "b50",
      "title": "Transforming classifier scores into accurate multiclass probability estimates"
    }
  ]
}