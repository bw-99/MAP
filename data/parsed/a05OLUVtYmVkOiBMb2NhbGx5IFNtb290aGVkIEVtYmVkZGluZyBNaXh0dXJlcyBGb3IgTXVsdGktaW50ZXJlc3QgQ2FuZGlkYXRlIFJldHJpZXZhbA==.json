{"title": "kNN-Embed: Locally Smoothed Embedding Mixtures For Multi-interest Candidate Retrieval", "authors": "Ahmed El-Kishky; Thomas Markovich; Kenny Leung; Frank Portman; Aria Haghighi \u22c6\u22c6; Ying Xiao \u22c6\u22c6", "pub_date": "2023-08-05", "abstract": "Candidate retrieval is the first stage in recommendation systems, where a light-weight system is used to retrieve potentially relevant items for an input user. These candidate items are then ranked and pruned in later stages of recommender systems using a more complex ranking model. As the top of the recommendation funnel, it is important to retrieve a high-recall candidate set to feed into downstream ranking models. A common approach is to leverage approximate nearest neighbor (ANN) search from a single dense query embedding; however, this approach this can yield a low-diversity result set with many near duplicates. As users often have multiple interests, candidate retrieval should ideally return a diverse set of candidates reflective of the user's multiple interests. To this end, we introduce kNN-Embed, a general approach to improving diversity in dense ANN-based retrieval. kNN-Embed represents each user as a smoothed mixture over learned item clusters that represent distinct \"interests\" of the user. By querying each of a user's mixture component in proportion to their mixture weights, we retrieve a high-diversity set of candidates reflecting elements from each of a user's interests. We experimentally compare kNN-Embed to standard ANN candidate retrieval, and show significant improvements in overall recall and improved diversity across three datasets. Accompanying this work, we open source a large Twitter follow-graph dataset1, to spur further research in graph-mining and representation learning for recommender systems.", "sections": [{"heading": "Introduction", "text": "Recommendation systems for online services such as e-commerce or social networks present users with suggestions in the form of ranked lists of items [5].", "publication_ref": ["b4"], "figure_ref": [], "table_ref": []}, {"heading": "Related Works", "text": "Traditionally, techniques for candidate retrieval rely on fast, scalable approaches to search large collections for similar sparse vectors [3,1]. Approaches apply indexing and optimization strategies to scale sparse similarity search. One such strategy builds a static clustering of the entire collection of items; clusters are retrieved based on how well their centroids match the query [25,20]. These methods either (1) match the query against clusters of items and rank clusters based on similarity to query or (2) utilize clusters as a form of item smoothing.\nFor embedding-based recommender systems [28], large-scale dense similarity search has been applied for retrieval. Some approaches proposed utilize hashing-based techniques such as mapping input and targets to discrete partitions and selecting targets from the same partitions as inputs [26]. With the advent of fast approximate nearest-neighbor search [21,13], dense nearest neighbor has been applied by recommender systems for candidate retrieval [5].\nWhen utilizing graph-based embeddings for recommender systems [8], some methods transform single-mode embeddings to multiple modes by clustering user actions [23]. Our method extends upon this idea by incorporating nearest neighbor smoothing to address the sparsity problem of generating mixtures of embeddings for users with few engagements.\nSmoothing via k-nearest-neighbor search has been applied for better language modeling [16] and machine translation [15]. We smooth low-engagement user representations by leveraging engagements from similar users.", "publication_ref": ["b2", "b0", "b24", "b19", "b27", "b25", "b20", "b12", "b4", "b7", "b22", "b15", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "kNN-Embed", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries", "text": "Let U = {u 1 , u 2 , . . . u n } be the set of source entities (i.e., users in a recommender system) and I = {i 1 , i 2 , . . . i m } be the set of target entities (i.e., items in a recommender system). Let G constitute a bipartite graph representing the engagements between users (U) and items (I). For each user and item, we define a \"relevance\" variable in {0, 1} indicating an item's relevance to a particular user. An item is considered relevant to a particular user if a user, presented with an item, will engage with said item. Based on the engagements in G, each user, u j , is associated with a d-dimensional embedding vector u j \u2208 R d ; similarly each target item i k is associated with an embedding vector i k \u2208 R d . We call these the unimodal embeddings, and assume that they model user-item relevance p(relevance|u j , i k ) = f (u j , i k ) for a suitable function f .\nGiven the input user-item engagement graph, our goal is to learn mixtures of embeddings representations of users that better capture the multiple interests of a user as evidenced by higher recall in a candidate retrieval task.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Unimodal User and Item Embeddings:", "text": "While kNN-Embed presupposes a set of co-embedded user and item embeddings and is agnostic to the exact embedding technique used (the only constraint is that the embeddings must satisfy p(i k |u j ) = g(u j T i k ) for monotone g), for completeness we describe a simple approach we applied to co-embed users and items into the same space. We form a bipartite graph G of users and items, where an edge represents relevance (e.g., user follows content producer). We seek to learn an embedding vector (i.e., vector of learnable parameters) for each user (u j ) and item (i k ) in this bipartite graph; we denote these learnable embeddings for users and items as u j and i k respectively. A user-item pair is scored with a scoring function of the form f (u j , i k ). Our training objective seeks to learn u and i parameters that maximize a log-likelihood constructed from the scoring function for (u, i) \u2208 G and minimize for (u, i) / \u2208 G. For simplicity, we apply a dot product comparison between user and item representations. For a user-item pair e = (u j , i), this is defined by:\nf (e) = f (u j , i k ) = u j \u22ba i k(1)\nAs seen in Equation 1, we co-embed users and items by scoring their respective embedded representations via dot product and perform edge (or link) prediction. We consume the input bipartite graph G as a set of user-item pairs of the form (u, i) which represent positive engagements between a user and item. The embedding training objective is to find user and item representations that are useful for predicting which users and items are linked via an engagement. While a softmax is a natural formulation to predict a user-item engagement, it is impractical due to the cost of computing the normalization over a large vocabulary of items. Following previous methods [22,10], negative sampling, a simplification of noise-contrastive estimation, can be used to learn the parameters u and i. We maximize the following negative sampling objective:\narg max u,i e\u2208G \uf8ee \uf8f0 log \u03c3(f (e)) + e \u2032 \u2208N (e) log \u03c3(-f (e \u2032 )) \uf8f9 \uf8fb(2)\nwhere:\nN (u, i) = {(u, i \u2032 ) : i \u2032 \u2208 I} \u222a {(u \u2032 , i) : u \u2032 \u2208 U }. Equation 2 represents\nthe log-likelihood of predicting a binary \"real\" (edges in the network) or \"fake\" (negatively sampled edges) label. To maximize the objective, we learn u and i parameters to differentiate positive edges from negative, unobserved edges. Negative edges are sampled by corrupting positive edges via replacing either the user or item in an edge pair with a negatively sampled user or item. Following previous approaches, negative sampling is performed both uniformly and proportional to node prevalence in the training graph [4,18].", "publication_ref": ["b21", "b9", "b3", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Smoothed Mixture of Embeddings", "text": "To use embeddings for candidate retrieval, we need a method of selecting relevant items given the input user. Ideally, we would like to construct a full distribution over all items for each user p(i k |u j ) and draw samples from it. The sheer number of items makes this difficult to do efficiently, especially when candidate retrieval strategies are meant to be light-weight. In practice, the most common method is to greedily select the top few most relevant items using an ANN search with the unimodal user embedding as query. A significant weakness of this greedy selection is that, by its nature, ANN search will return items that are similar not only to the user embedding, but also to each other; this drastically reduces the diversity of the returned items. This reduction in diversity is a side-effect of the way embeddings are trained -typically, the goal of training embeddings is to put users and relevant items close in Euclidean space; however, this also places similar users close in space, as well as similar items. We will repeatedly exploit this \"locality implies similarity\" property of embeddings in this paper to resolve this diversity issue.\nClustering Items: Since neighboring items are similar in the embedding space, if we apply a distance-based clustering to items, we can arrive at groupings that represent individual user preferences well. As such, we first cluster items using spherical k-means [6] where cluster centroids are placed on a high-dimensional sphere with radius one. Given these item clusters, instead of immediately collapsing the distribution p(i k |u j ) to a few items as ANN search does, we can write the full distribution p(i k |u j ) as a mixture over item clusters:\np(i k |u j ) = c p(c|u j ) \u2022 p(i k |u j , c)\nwhere in each cluster, we learn a separate distribution over the items in the cluster p(i k |u j , c). Thus, we are modeling each user's higher level interests p(c|u), and then within each interest c, we can apply an efficient ANN-search strategy as before. In effect, we are interpolating between sampling the full preference distribution p(i k |u j ) and greedily selecting a few items in an ANN.", "publication_ref": ["b5"], "figure_ref": [], "table_ref": []}, {"heading": "Mixture of Embeddings via Cluster Engagements:", "text": "After clustering target entities, we learn p(c|u j ) through its maximum likelihood estimator (MLE):\np mle (c|u j ) = count(u i , c)/ c \u2032 \u2208Mj count(u j , c \u2032 )(3)\nwhere, count(u j , c) is the number of times u j has a relevant item in cluster c. For computational efficiency, we take M j to be u j 's top m most relevant clusters. We normalize these counts to obtain a proper cluster-relevance distribution.\nNearest Neighbor Smoothing: Unfortunately, we typically have few user-item engagements on a per-user basis; thus, while the MLE is unbiased and asymptotically efficient, it can also be high variance. To this end, we introduce a smoothing technique that once again exploits locality in the ANN search, this time for users. Figure 1 illustrates identifying k nearest-neighbors (K j ) to the query user u j 's, and leveraging the information from the neighbors' cluster engagements to augment the user's cluster relevance. We compute this distribution over item clusters by averaging the MLE probability for each nearest neighbor (item clusters that are not engaged with by a retrieved neighbor have zero probability).\np k N N (c|u j ) = 1 |K j | u \u2032 \u2208Kj p mle (c|u \u2032 )(4)\nWe apply Jelinik-Mercer smoothing to interpolate between a user's MLE distribution with the aggregated nearest neighbor distribution [12].\np smoothed (c|u j ) = (1 -\u03bb)p mle (c|u j ) + \u03bbp k N N (c|u j ),(5)\nwhere \u03bb \u2208 [0, 1] represents how much smoothing is applied. It can be manually set or tuned on a downstream extrinsic task. Fig. 1: Example of retrieving two candidates. In an ANN, items 4 and 5 would be deterministically returned for user 1. In our proposed kNN-Embed, even though the distances to cluster 2 are larger, smoothing means that we will sometimes return items from that cluster, yielding more diverse items. Note in this case, we don't even require that user 1 has previously relevant items in cluster 2.\nSampling within Clusters Within each cluster there are many ways to retrieve items on a per user basis. A simple, but appealing, strategy is to represent each user as a normalized centroid of their relevant items in that cluster:\ncentroid(c, u j ) = m\u2208R(c,uj ) i m \u2225 m\u2208R(c,uj ) i m \u2225 ,(6)\nwhere R(c, u j ) is the set of relevant items for user u j in cluster c. However, since we are applying smoothing to the cluster probabilities p(c|u j ), it may be case that u j has zero relevant items in a given cluster. Hence, we smooth the user centroid using neighbor infomation to obtain the final user representation u c j :\nu c j = (1 -\u03bb) centroid(c, u j ) + \u03bb |K j | u \u2032 \u2208Kj p mle (c|u \u2032 ) centroid(c, u \u2032 )(7)\nEquation 7 shows the kNN-smoothed user-specific embedding for cluster c. This embedding takes the user-specific cluster representations from Equation 6, and performs a weighted averaging proportionate to each user's contribution to p smoothed (c|u j ). The final vector is once again normalized to unit norm.", "publication_ref": ["b11"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation Datasets and Metrics", "text": "We evaluate on three datasets which we describe below:", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "HEP-TH Citation Graph:", "text": "This paper citation network is collected from Arxiv preprints from the High Energy Physics category [9]. The dataset consists of: 34,546 papers and 421,578 citations.", "publication_ref": ["b8"], "figure_ref": [], "table_ref": []}, {"heading": "DBLP Citation Graph:", "text": "This paper citation network is collected from DBLP [24] and consists of 5,354,309 papers and 48,227,950 citation relationships.\nTwitter Follow Graph: We curate Twitter user-follows-user (available via API) by first selecting a number of 'highly-followed' users that we refer to as 'content producers'; these content producers serve as 'items' in our recommender systems terminology. We then sampled users that follow these content producer accounts. All users are anonymized with no other personally identifiable information (e.g., demographic features) present. Additionally, the timestamp of each follow edge was mapped to an integer that respects date ordering, but does not provide any information about the date that follow occurred. In total, we have 261M edges and 15.5M vertices, with a max-degree of 900K and a min-degree of 5. We hope that this dataset will be of useful to the community as a test-bed for large-scale retrieval research.", "publication_ref": ["b23"], "figure_ref": [], "table_ref": []}, {"heading": "Metrics:", "text": "We evaluate kNN-Embed on three aspects: (1) the recall (2) diversity and (3) goodness of fit of retrieved candidates. Below, we formalize these metrics.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Recall@K:", "text": "The most natural (and perhaps most important) metric for computing the efficacy of various candidate retrieval strategies is Recall@K. This metric is given by considering a fixed number of top candidates yield by a retrieval system (up to size K) and measuring what percent of these candidates are heldout relevant candidates. The purpose of most candidate retrieval systems is to collect a high-recall pool of items for further ranking, and thus recall is a relevant metric to consider. Additionally, recall provides an indirect way to measure diversity -to achieve high recall, one is obliged to return a large fraction of all relevant documents, which simple greedy ANN searches can struggle with.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Diversity:", "text": "To evaluate the diversity among the retrieved candidates, we measure the spread in the embeddings of the retrieved candidates by calculating the average distance retrieved candidates are from their centroid. The underlying idea is that when 'locality implies similarity'; as a corollary, if candidates are further in Euclidean distance, then they are likely to be different. As such, for a given set of candidates C, we compute diversity D as follows:\nD(C) = 1 |C| i k \u2208C \u2225i k -\u00ee\u2225(8)\nwhere C denotes the set of retrieved candidates and \u00ee = i k \u2208C i k /|C| is the mean of the unimodal embeddings of the retrieved candidates.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Goodness of Fit:", "text": "In addition to diversity of retrieved items, we need to ensure that a user's mixture representation is an accurate model of their intereststhat is the mixture of embeddings identifies points in the embedding space where relevant items lie. Thus, we compare held out relevant items to the user's mixture representation we use to query. We measure this \"goodness of fit\" by computing the Earth Mover's Distance (EMD) [19] between a uniform distribution over a user's relevant items and the user's mixture distribution. The EMD measures the distance between two probability distributions over a metric space [17,7]. We measure the distance between a user's cluster distribution (e.g., Equation 3 and Equation 4), to a uniform distribution over a held-out set of relevant items: p(i|u j ) over a Euclidean space. We compute EMD by soft assigning all held-out relevant items of a user to clusters, minimizing the sum of item-cluster distances, with the constraint that the sum over soft assignments matches p(c|u j ). As seen in Figure 2, with standard unimodal representations, a single embedding vector is compared to the held-out items and the goodness of fit is the distance between the item embeddings and the singular user embedding. In comparison, for mixture representations (Figure 2, each user multiple user embeddings who each have fractional probability mass that in total sums to 1. The goodness of fit is then the distance achieved by allocating the mass in each item to the closest user embedding cluster with available probability mass. Observing unimodal representations in Fig. 2, a single unimodal embedding is situated in the embedding space and compared to held-out relevant items.\nAs shown, some held-out items are close to the unimodal embedding, while others are further away. In contrast, for mixture representations, each user has multiple user-embeddings and each of these embeddings lies close to a cluster of relevant items. The intuition is that if a user has multiple item clusters they are interested in, multiple user embeddings can better capture these interests.  ", "publication_ref": ["b18", "b16", "b6"], "figure_ref": ["fig_0", "fig_0", "fig_0"], "table_ref": []}, {"heading": "Experiments", "text": "Experimental Setup: For our underlying ANN-based candidate retrieval system, we start by creating a bipartite graph between source entities and target entities for each dataset, with each edge representing explicit relevance between items (e.g., citing paper cites cited paper or user follows content producer). We then learn unimodal 100-dimensional embeddings for users and items by training over 20 epochs and cluster them via spherical k-means over 20 epochs [2].\nEvaluation Task: We evaluate three candidate retrieval strategies -baseline ANN with unimodal embeddings (which is how most ANN-based candidate retrieval systems work), mixture of embeddings with no smoothing [23], and mixture of embeddings with smoothing (i.e., kNN-Embed). For each strategy, we compute the Recall@K, diversity, and fit in a link prediction task.", "publication_ref": ["b1", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Research Hypotheses:", "text": "We explore two research hypotheses (as well as achieve some understanding of the hyperparameters): (1) Unimodal embeddings miss many relevant items due to the similarity of retrieved items. Mixtures yield more diverse and higher recall candidates. ( 2) Smoothing, by using information from neighboring users, further improves the recall of retrieved items. Approach R@10 R@20 R@50 R@10 R@20 R@50 R@10 R@20 R@50 Unimodal 20.0% 30.0% 45.7% 9.4% 13.9% 21.6% 0.58% Recall of unimodal vs mixture vs kNN-Embed-higher is better. HEP-TH (\u03bb = 0.8, 2000 clusters, 5 embeddings). DBLP (\u03bb = 0.8, 10000 clusters, 5 embeddings). Twitter-Follow (\u03bb = 0.8, 40000 clusters, 5 embeddings).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Recall:", "text": "In Table 1, we report results when evaluating recall on citation prediction tasks. Results support the first hypothesis that unimodal embeddings may miss relevant items if they don't lie close to the user in the shared embedding space. Mixture of embeddings with no smoothing, yields a 14% relative improvement in R@10 for for HEP-TH, and 16% relative improvement for DBLP. Our second hypothesis (2) posits that data sparsity can lead to sub-optimal mixtures of embeddings, and that nearest-neighbor smoothing can mitigate this. Our experiments support this hypothesis, as we see a 25% relative improvement for HEP-TH in R@10, and 35% for DBLP and when using kNN-Embed. We see similar significant improvements over baselines in R@20 and R@50. For Twitter-Follow, the improvements in recall are dramatic -534% in relative terms going from unimodal embeddings to a mixture of embeddings in R@10. We suspect this significant improvement is because Twitter-Follow simultaneously has a much higher average degree than HEP-TH and DBLP and the number of unique nodes is much larger. It is a more difficult task to embed so many items, from many different interest clusters, in close proximity to a user. As such, we see a massive improvement by explicitly querying from each user's interest clusters. Applying smoothing provides an additional 74% in relative terms, and similar behaviours are observed in R@20 and R@50.", "publication_ref": ["b1"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Diversity:", "text": "We apply Equation 8 to retrieved candidates and measure the spread of retrieved candidates' embedding vectors. As seen in Table 2, the candidates from unimodal retrieval are less diverse than candidates retrieved via multiple queries from mixture representations. This verifies our first research hypothesis that unimodal embeddings may retrieve many items that are clustered closely together as a by-product of ANN retrieval (i.e., diversity and recall is low). However, multiple queries from mixtures of embeddings broadens the search spatially; retrieved items are from different clusters, which are more spread out from each other. kNN-Embed (i.e., smooth mixture retrieval) results in slightly less diverse candidates than unsmoothed mixture retrieval. We posit that this is due to the high-variance of the maximum likelihood estimator of the p mle (c|u j ) multinomial (Equation 3). While this high-variance may yield more diverse candidates, this yields less relevant candidates as seen in Table 1 where kNN-Embed consistently yields better recall than unsmoothed mixture retrieval. While high diversity is necessary for high recall, it is insufficient on its own. ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_4", "tab_2"]}, {"heading": "Goodness of Fit:", "text": "We evaluate how well unimodal, mixture, and smoothed mixture embeddings model a user's interests. The main idea is that the better fit a user representation is, the closer it will be to the distribution of held out relevant items for that user. As seen in Table 3, the results validate the idea that unimodal user embeddings do not model user interests as well as mixtures over multiple embeddings. Multiple embeddings yield a significant EMD improvement over a single embedding vector when evaluated on held-out items. Smoothing further decreases the EMD which we posit is due to the smoothed embedding mixtures being lower-variance estimates as they leverage engagement data from similar users in constructing the representations. These results suggest that the higher recall of smoothed mixtures is due to better user preferences modeling. ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "Hyper-parameter Sensitivity Analysis:", "text": "We focus on recall as the sine qua non of candidate retrieval problems and analyze hyper-parameters on HEP-TH. In Figure 3a, we vary the smoothing parameter \u03bb (same parameter for both the mixture probabilities and the cluster centroids) and see heavy smoothing improves performance significantly. This likely stems from the sparsity of HEP-TH where most papers have only a few citations. In Figure 3b, we vary the number of embeddings (i.e., the mixture size) and notice improved performance saturating at six mixture components. Out of all the hyperparameters, this seems to be the critical one in achieving high recall. In practice, latency constraints can be considered when selecting the number of embeddings per user, explicitly making the trade-off between diversity and latency.   ", "publication_ref": [], "figure_ref": ["fig_3", "fig_3"], "table_ref": []}, {"heading": "Conclusions", "text": "We present kNN-Embed, a method of transforming single user dense embeddings, into mixtures of embeddings, with the goal of better modeling user interests, increasing retrieval recall and diversity. This multi-embedding scheme represents a source entity with multiple distinct topical affinities by globally clustering items and aggregating the source entity's engagements with clusters. Recognizing that user-item engagements may often be sparse, we propose a nearest-neighbor smoothing to enrich these mixture representation. Our smoothed mixture representation better models user preferences retrieving a diverse set of candidate items reflective of a user's multiple interests. This significantly improves recall on candidate retrieval tasks on three datasets including Twitter-Follow, a dataset we curate and release to the community.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions", "journal": "", "year": "2006", "authors": "A Andoni; P Indyk"}, {"ref_id": "b1", "title": "k-means++: The advantages of careful seeding", "journal": "", "year": "2006", "authors": "D Arthur; S Vassilvitskii"}, {"ref_id": "b2", "title": "Scaling up all pairs similarity search", "journal": "WWW", "year": "2007", "authors": "R Bayardo; Y Ma; R Srikant"}, {"ref_id": "b3", "title": "Translating embeddings for modeling multi-relational data", "journal": "NeurIPS", "year": "2013", "authors": "A Bordes; N Usunier; A Garcia-Duran; J Weston; O Yakhnenko"}, {"ref_id": "b4", "title": "Deep neural networks for youtube recommendations", "journal": "", "year": "2016", "authors": "P Covington; J Adams; E Sargin"}, {"ref_id": "b5", "title": "Concept decompositions for large sparse text data using clustering", "journal": "Machine learning", "year": "2001", "authors": "I Dhillon; D Modha"}, {"ref_id": "b6", "title": "Massively multilingual document alignment with crosslingual sentence-mover's distance", "journal": "", "year": "2020", "authors": "A El-Kishky; F Guzm\u00e1n"}, {"ref_id": "b7", "title": "Twhin: Embedding the twitter heterogeneous information network for personalized recommendation", "journal": "KDD", "year": "2022", "authors": "A El-Kishky; T Markovich; S Park"}, {"ref_id": "b8", "title": "Overview of the 2003 kdd cup", "journal": "", "year": "2003", "authors": "J Gehrke; P Ginsparg; J Kleinberg"}, {"ref_id": "b9", "title": "word2vec explained: deriving mikolov et al.'s negativesampling word-embedding method", "journal": "", "year": "2014", "authors": "Y Goldberg; O Levy"}, {"ref_id": "b10", "title": "Embedding-based retrieval in facebook search", "journal": "", "year": "2020", "authors": "J Huang; A Sharma; S Sun; L Xia"}, {"ref_id": "b11", "title": "Interpolated estimation of markov source parameters from sparse data", "journal": "PRIP", "year": "1980", "authors": "F Jelinek"}, {"ref_id": "b12", "title": "Billion-scale similarity search with gpus", "journal": "BigData", "year": "2019", "authors": "J Johnson; M Douze; H J\u00e9gou"}, {"ref_id": "b13", "title": "Candidate generation with binary codes for large-scale top-n recommendation", "journal": "", "year": "2019", "authors": "W Kang; J Mcauley"}, {"ref_id": "b14", "title": "Nearest neighbor machine translation", "journal": "ICLR", "year": "2021", "authors": "U Khandelwal; A Fan; D Jurafsky; L Zettlemoyer; M Lewis"}, {"ref_id": "b15", "title": "Generalization through memorization: Nearest neighbor language models", "journal": "ICLR", "year": "2020", "authors": "U Khandelwal; O Levy; D Jurafsky; L Zettlemoyer; M Lewis"}, {"ref_id": "b16", "title": "From word embeddings to document distances", "journal": "", "year": "2015", "authors": "M Kusner; Y Sun; N Kolkin; K Weinberger"}, {"ref_id": "b17", "title": "Pytorch-biggraph: A large-scale graph embedding system", "journal": "MLSys", "year": "2019", "authors": "A Lerer; L Wu; J Shen; T Lacroix; L Wehrstedt; A Bose; A Peysakhovich"}, {"ref_id": "b18", "title": "The earth mover's distance is the mallows distance: Some insights from statistics", "journal": "", "year": "2001", "authors": "E Levina; P Bickel"}, {"ref_id": "b19", "title": "Cluster-based retrieval using language models", "journal": "", "year": "2004", "authors": "X Liu; B Croft"}, {"ref_id": "b20", "title": "Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs", "journal": "TPAMI", "year": "2018", "authors": "Y Malkov; D Yashunin"}, {"ref_id": "b21", "title": "Distributed representations of words and phrases and their compositionality", "journal": "NeurIPS", "year": "2013", "authors": "T Mikolov; I Sutskever; K Chen; G Corrado; J Dean"}, {"ref_id": "b22", "title": "Pinnersage: multi-modal user embedding framework for recommendations at pinterest", "journal": "", "year": "2020", "authors": "A Pal; C Eksombatchai; Y Zhou"}, {"ref_id": "b23", "title": "Arnetminer: extraction and mining of academic social networks", "journal": "", "year": "2008", "authors": "J Tang; J Zhang; L Yao; J Li; L Zhang; Z Su"}, {"ref_id": "b24", "title": "Document clustering: An evaluation of some experiments with the cranfield 1400 collection", "journal": "", "year": "1975", "authors": "C Van R\u0133sbergen; W Bruce"}, {"ref_id": "b25", "title": "Label partitioning for sublinear ranking", "journal": "", "year": "", "authors": "J Weston; A Makadia; H Yee"}, {"ref_id": "b26", "title": "Practical diversified recommendations on youtube with determinantal point processes", "journal": "", "year": "2018", "authors": "M Wilhelm; A Ramanathan; A O Bonomo"}, {"ref_id": "b27", "title": "Collaborative knowledge base embedding for recommender systems", "journal": "", "year": "2016", "authors": "F Zhang; N Yuan; D Lian; X Xie; W Ma"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Fig. 2 :2Fig. 2: Goodness of fit of unimodal representation vs mixture representation.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Finally, in Figure we vary the number of k-means clusters; recall peaks at k = 2500 and then decreases. HEP-TH is a small dataset with only 34,546 items; it is likely that generating a very large number of clusters leads to excessively fine-grained and noisy sub-divisions of the items.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "(a) Varying lambda -R@50. (b) Varying mixtures R@20. (c) Varying clusters R@20.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Fig. 3 :3Fig. 3: We analyze the effect of three important hyper-parameters: (1) the \u03bb smoothing (2) the number of embeddings in the mixture (3) the number of clusters for candidate retrieval in the HEP-TH dataset.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Recall of Retrieved Candidates", "figure_data": "HEP-THDBLPTwitter-Follow"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": "1.02% 2.06%Mixture22.7% 33.4% 49.3% 10.9% 16.1% 25.1% 3.70% 5.53% 8.79%kNN-Embed"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Diversity of Retrieved Candidates", "figure_data": "HEP-THDBLPTwitter-FollowApproachD@10 D@20 D@50 D@10 D@20 D@50 D@10 D@20 D@50Unimodal0.490.540.610.430.460.510.380.400.43Mixture0.580.630.680.510.560.600.560.540.58kNN-Embed0.540.600.660.460.520.570.470.520.55"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Goodness of fit between user and held-out items as measured by earth mover's distance over a Euclidean embedding space. Lower EMD is better.", "figure_data": "ApproachHEP-TH DBLP Twitter-FollowUnimodal0.8970.8891.018Mixture0.8380.8300.952kNN-Embed0.8110.8080.940"}], "formulas": [{"formula_id": "formula_0", "formula_text": "f (e) = f (u j , i k ) = u j \u22ba i k(1)", "formula_coordinates": [4.0, 255.5, 140.16, 225.09, 10.91]}, {"formula_id": "formula_1", "formula_text": "arg max u,i e\u2208G \uf8ee \uf8f0 log \u03c3(f (e)) + e \u2032 \u2208N (e) log \u03c3(-f (e \u2032 )) \uf8f9 \uf8fb(2)", "formula_coordinates": [4.0, 201.77, 304.72, 278.82, 34.15]}, {"formula_id": "formula_2", "formula_text": "N (u, i) = {(u, i \u2032 ) : i \u2032 \u2208 I} \u222a {(u \u2032 , i) : u \u2032 \u2208 U }. Equation 2 represents", "formula_coordinates": [4.0, 168.42, 351.15, 312.17, 10.87]}, {"formula_id": "formula_3", "formula_text": "p(i k |u j ) = c p(c|u j ) \u2022 p(i k |u j , c)", "formula_coordinates": [5.0, 236.41, 215.17, 142.54, 19.61]}, {"formula_id": "formula_4", "formula_text": "p mle (c|u j ) = count(u i , c)/ c \u2032 \u2208Mj count(u j , c \u2032 )(3)", "formula_coordinates": [5.0, 212.65, 352.54, 267.94, 22.13]}, {"formula_id": "formula_5", "formula_text": "p k N N (c|u j ) = 1 |K j | u \u2032 \u2208Kj p mle (c|u \u2032 )(4)", "formula_coordinates": [5.0, 233.19, 551.8, 247.4, 26.8]}, {"formula_id": "formula_6", "formula_text": "p smoothed (c|u j ) = (1 -\u03bb)p mle (c|u j ) + \u03bbp k N N (c|u j ),(5)", "formula_coordinates": [5.0, 197.78, 621.4, 282.81, 10.32]}, {"formula_id": "formula_7", "formula_text": "centroid(c, u j ) = m\u2208R(c,uj ) i m \u2225 m\u2208R(c,uj ) i m \u2225 ,(6)", "formula_coordinates": [6.0, 230.41, 392.43, 250.18, 26.85]}, {"formula_id": "formula_8", "formula_text": "u c j = (1 -\u03bb) centroid(c, u j ) + \u03bb |K j | u \u2032 \u2208Kj p mle (c|u \u2032 ) centroid(c, u \u2032 )(7)", "formula_coordinates": [6.0, 163.25, 489.65, 317.34, 26.8]}, {"formula_id": "formula_9", "formula_text": "D(C) = 1 |C| i k \u2208C \u2225i k -\u00ee\u2225(8)", "formula_coordinates": [7.0, 255.96, 534.63, 224.63, 27.47]}], "doi": ""}
