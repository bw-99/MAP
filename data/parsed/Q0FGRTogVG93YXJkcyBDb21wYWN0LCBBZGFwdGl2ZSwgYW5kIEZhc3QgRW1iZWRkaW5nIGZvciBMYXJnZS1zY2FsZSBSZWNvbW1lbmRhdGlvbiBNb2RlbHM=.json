{"CAFE: Towards Compact, Adaptive, and Fast Embedding for Large-scale Recommendation Models": "HAILIN ZHANG \u2217 , Peking University, China ZIRUI LIU \u2217 , Peking University, China BOXUAN CHEN, Peking University, China YIKAI ZHAO, Peking University, China TONG ZHAO, Peking University, China TONG YANG \u2020 , Peking University, China BIN CUI \u2020 , Peking University, China Recently, the growing memory demands of embedding tables in Deep Learning Recommendation Models (DLRMs) pose great challenges for model training and deployment. Existing embedding compression solutions cannot simultaneously meet three key design requirements: memory efficiency, low latency, and adaptability to dynamic data distribution. This paper presents CAFE , a C ompact, A daptive, and F ast E mbedding compression framework that addresses the above requirements. The design philosophy of CAFE is to dynamically allocate more memory resources to important features (called hot features), and allocate less memory to unimportant ones. In CAFE, we propose a fast and lightweight sketch data structure, named HotSketch, to capture feature importance and report hot features in real time. For each reported hot feature, we assign it a unique embedding. For the non-hot features, we allow multiple features to share one embedding by using hash embedding technique. Guided by our design philosophy, we further propose a multi-level hash embedding framework to optimize the embedding tables of non-hot features. We theoretically analyze the accuracy of HotSketch, and analyze the model convergence against deviation. Extensive experiments show that CAFE significantly outperforms existing embedding compression methods, yielding 3 . 92% and 3 . 68% superior testing AUC on Criteo Kaggle dataset and CriteoTB dataset at a compression ratio of 10000 \u00d7 . The source codes of CAFE are available at GitHub [75]. CCS Concepts: \u00b7 Computing methodologies \u2192 Artificial intelligence ; \u00b7 Information systems \u2192 Online advertising ; \u00b7 Theory of computation \u2192 Sketching and sampling . Additional Key Words and Phrases: Embedding, Deep Learning Recommendation Model, Sketch \u2217 Both authors contributed equally to this research. \u2020 Bin Cui and Tong Yang are the corresponding authors. Authors' addresses: Hailin Zhang, z.hl@pku.edu.cn, School of Computer Science & Key Lab of High Confidence Software Technologies, Peking University, Beijing, China; Zirui Liu, zirui.liu@pku.edu.cn, School of Computer Science & Key Lab of High Confidence Software Technologies, Peking University, Beijing, China; Boxuan Chen, 2100012923@stu.pku.edu.cn, School of Electronics Engineering and Computer Science, Peking University, Beijing, China; Yikai Zhao, zyk@pku.edu.cn, School of Computer Science & Key Lab of High Confidence Software Technologies, Peking University, Beijing, China; Tong Zhao, zhaotong@pku.edu.cn, School of Computer Science, Peking University, Beijing, China; Tong Yang, yangtongemail@ gmail.com, School of Computer Science & Key Lab of High Confidence Software Technologies, Peking University, Beijing, China; Bin Cui, bin.cui@pku.edu.cn, School of Computer Science & Key Lab of High Confidence Software Technologies & Institute of Computational Social Science, Peking University (Qingdao), Peking University, Beijing, China. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \u00a9 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM 2836-6573/2024/2-ART51 https://doi.org/10.1145/3639306", "ACMReference Format:": "Hailin Zhang, Zirui Liu, Boxuan Chen, Yikai Zhao, Tong Zhao, Tong Yang, and Bin Cui. 2024. CAFE: Towards Compact, Adaptive, and Fast Embedding for Large-scale Recommendation Models. Proc. ACM Manag. Data 2, N1 (SIGMOD), Article 51 (February 2024), 28 pages. https://doi.org/10.1145/3639306", "1 INTRODUCTION": "", "1.1 Background and Motivation": "In recent years, embedding techniques are widely applied in various fields in database community, such as cardinality estimation [31, 39], query optimization [4, 82], language understanding [27], entity resolution [12, 13], document retrieval [23], graph learning [29, 69], and advertising recommendation [47], to learn the semantic representations of categorical features. Among these fields, Deep Learning Recommendation Models (DLRMs) are one of the most important applications of embedding techniques: they account for 35% of Amazon's revenue in 2018 [7, 60, 66], and consume more than 50% training and 80% inference cycles at Meta's data centers in 2020 [21, 50]. As shown in Figure 1, a typical DLRM vectorizes categorical features into learnable embeddings, and then feeds these embeddings into downstream neural networks along with other numerical features [5, 22, 44, 51, 56, 73]. Recently, with the exponential increase of categorical features in DLRM, the memory requirements of embedding tables have also skyrocketed, which creates formidable storage challenges in various applications [49, 72]. Therefore, it is highly desired to devise a framework that can effectively compress the embedding tables into limited storage space without compromising model accuracy. In this paper, we focus on compressing the embedding tables of extremely large-scale DLRMs. Numerical Features Categorical Features Input Sample 0.3 0.7 0.6 0 /0 / 1 Processed PricelScorel . Gender User ID = 4 Interest Reading [0.12, 0.34 , 0.42] Embedding Lookup Hash(IDs) [0.24, 0.52, [0.12 0.34 0.42] FC Layers [0,2,4] [0.74, 0.24, [0.74,0.24 0.82] [0.34 0.64 , 34] [0.75, 0.42, 0.17] DNN [0.75, 0.42, Model Interaction Layers FC Layers Prediction (e.g. click or not) DLRM has two training paradigms: offline training and online training. (1) In offline training, the training data is collected in advance, and the model is deployed for use after the entire training process. (2) In online training, the training data is generated in real time, and the model simultaneously updates parameters and serves requests. This paper mainly focuses on the scenario of online training as it is more difficult. Generally, compression methods for online training can be directly applied to offline training. Embedding compression in online training has three important design requirements, which are as follows: \u00b7 Memory efficiency. For extremely large-scale DLRMs, it is challenging to maintain model quality within memory constraints. While distributed instances can help manage large-scale embedding tables, they come with a significant communication overhead [48, 63]. Furthermore, training and deployment of embedding tables often occur on edge or end devices with smaller storage capacities, making the memory issue even worse [52]. On the other hand, since model quality directly impacts profits, even a small change of 0.001 in DLRM's AUC (area under the ROC curve) is considerable [19]. Existing compression methods often lead to severe model degradation when memory constraints are small [79], emphasizing the need for memory-efficient compression methods that maintain model quality. \u00b7 Low latency. Low latency is a vital requirement in practical applications, as latency is a key metric of service quality [20]. Embedding compression methods must be fast enough not to introduce significant latency. \u00b7 Adaptability to dynamic data distribution. In online training, the data distribution is not fixed as in offline training. We calculate the KL divergence (an asymmetric measure of the distance between distributions) between the feature distributions on each day within three common public datasets, and plot the heatmaps in Figure 2. In each heatmap, the block in row \ud835\udc56 , column \ud835\udc57 shows the KL divergence between the distributions on day \ud835\udc56 and day \ud835\udc57 . There is a significant difference between the feature distributions, and generally the greater the number of days between, the greater the difference. Existing advanced compression methods often exploit feature frequencies explicitly [17, 74] or capture feature importance implicitly [30, 81], which are inspired by the observation that feature popularity distributions are highly skewed, fitting zipfian [72] or powerlaw distributions [77]. However, most of them rely on fixed data distributions and cannot be applied to dynamic data distributions, demanding new adaptive compression method suitable for online training. Avazu 1.60 Criteo 0.10 CriteoTB 0.36 0.08 0.30 1.20 0.06 0.80 0.18 0.04 0.12 0.40 0.02 0.06 00 0.00 0.00", "1.2 Limitations of Prior Art": "Existing embedding compression methods can be generally categorized into two types: row compression and column compression. As column compression primarily aims at enhancing model quality rather than compressing to a specific memory limit, our focus is on row compression, including hash-based and adaptive methods. Hash-based methods. These methods utilize simple hash functions to map features into embeddings with collisions [58, 64, 68]. They restrict the number of embedding vectors to fit within the memory budget, causing different features to potentially share an embedding vector when a hash collision occurs. Despite their simplicity and convenience, which have resulted in widespread industry use, these methods are not very memory-efficient. Pre-determined hash functions distort the semantic information of features, often leading to a substantial decline in model accuracy. For each feature, the gradients of other hash-collided features will be updated to the same embedding, resulting in deviations from the original convergence direction. Integrating feature frequency information [74] can enhance hash-based methods' model quality in offline training but cannot be applied to online training. Adaptive methods. To accommodate online training, adaptive methods distinguish and track important features throughout the training process. AdaEmbed [34] logs the importance scores of all features, dynamically reallocates embedding vectors for critical features, and discards embeddings of less important features. While it can adapt to data distribution, its compression ratio is constrained by the storage of importance scores, which increases linearly with the total number of features. Thus, it cannot compress embedding tables to a small memory budget and still needs distributed training for large models, resulting in low memory efficiency. It also needs to sample and check data to determine whether to migrate embeddings, which can increase overall latency. In summary, existing methods fail to meet all three critical requirements for DLRM: memory efficiency, low latency, and adaptability. In this paper, we aim to propose an embedding compression method that is memory-efficient, adaptive, and ensures low latency.", "1.3 Our Proposed Method": "We introduce CAFE , a C ompact, A daptive, and F ast E mbedding compression method, which, to our knowledge, is the first to satisfy all three design requirements. (a) Memory efficiency: CAFE allocates unique embedding vectors to important features and shared embedding vectors to less important features, thereby preserving model quality. A light-weight sketch, HotSketch, distinguishes these features, with its memory usage being linear to the number of important features, enabling high compression ratios. Consequently, CAFE manages to maintain good model quality within tight memory constraints. (b) Low latency: CAFEentails only several hash processes and potentially one additional embedding lookup, incurring negligible time overhead beyond the standard embedding layers and thus maintaining low latency during serving. (c) Adaptability to dynamic data distribution: CAFE incorporates an embedding migration process that takes effect when a feature's importance score changes, ensuring that vital features are always identified even when data distribution changes during online training. On Criteo dataset, compared to existing methods, CAFE improves the model AUC by 1 . 79% and reduces the training loss by 2 . 31% on average. To achieve a high compression ratio without compromising model quality, we utilize a sketch structure to distinguish and record important features from a highly skewed Zipf distribution. Sketches are a class of probabilistic data structures for processing large-scale data streams, and are naturally suitable for handling streamed features in online training. Specifically, we extend SpaceSaving Sketch [59], an advanced sketch algorithm with small error, to design HotSketch, a less memory-consuming structure to store important DLRM features with a theoretically guaranteed error bound. Being a light-weight data structure, HotSketch incurs negligible time overhead, facilitating fast training and inference. Since HotSketch's memory usage is only linear to the number of important features, CAFE can compress to any given memory constraints. With HotSketch, we allocate unique embeddings to a handful of important features and shared embeddings to a vast majority of long-tail features, achieving memory efficiency. To adapt CAFE to online training, where important features can change dynamically, we enable features to migrate between unique and shared embedding tables. If a feature's importance score exceeds a relative threshold in HotSketch, it is deemed important and allocated a unique embedding. Conversely, if a feature's importance score drops below a relative threshold, its unique embedding migrates to the shared embedding table. To further optimize CAFE, we divide features into more groups by importance scores. While the most critical features are still allocated unique embeddings, other features are assigned a varying number of hashed embedding vectors. This multi-level design further improves the model AUC by 0 . 08% on Criteo dataset.", "1.4 Main Contribution": "\u00b7 We introduce CAFE, a compact, adaptive, and fast embedding compression method. \u00b7 Wepropose HotSketch, a light-weight sketch structure to discern and record features' importance scores. \u00b7 We provide a theoretical analysis of HotSketch's effectiveness, and elucidate how CAFE's design contributes to the convergence of compressed DLRMs. \u00b7 We evaluate CAFE on representative DLRM datasets, achieving 3 . 92%, 3 . 68%, 5 . 16% higher testing AUC and 4 . 61%, 3 . 24%, 11 . 21% lower training loss at 10000 \u00d7 compression ratio compared to existing method.", "2 PRELIMINARY": "In this section, we elaborate on the architecture of DLRMs in Section 2.1 and provide a formal definition of the embedding compression problem in Section 2.2.", "2.1 DLRM": "Figure 1 illustrates the overall architecture of DLRM. Each dataset of DLRM has several categorical feature fields and numerical feature fields. For example, in Figure 1, gender, user ID and interest are categorical fields, while price and score are numerical fields. Each field has a certain number or a certain range of possible values, called features. Categorical and numerical features are transformed into representations using embedding vectors and fully-connected layers, respectively. The representations are then fed into interaction layers and fully-connected layers for final predictions. The prediction may be a category for classification tasks such as click-through-rate and conversion-rate prediction, or a score for regression tasks such as score prediction. There are many variants of DLRM, such as WDL [5], DCN [61], DIN [83]; while they all utilize the same embedding layer, they explore different forms of interaction layers and neural network layers to enhance model performance. The size of a DLRM does not depend on the model structure, but on the number of unique categorical features in the dataset. The model parameters of DLRMs can be divided into two parts: the embedding table and the neural network. The former contains embeddings for all categorical features, i.e. , one embedding per feature if uncompressed. The latter is a network that interacts these embeddings and outputs predictions. The number of parameters in the embedding table depends on the dataset: if there are \ud835\udc5b unique categorical features in the dataset, and the dimension of embeddings is \ud835\udc51 , then the number of parameters is \ud835\udc5b \u00d7 \ud835\udc51 . In DLRMs, the size of the neural network part (just a few layers of matrix multiplication) is negligible compared to large embedding tables. Based on previous research works [25, 38, 49, 65, 80], we consider DLRMs with more than 100 million parameters as large-scale, and DLRMs with more than 10 billion parameters as extremely large-scale. In DLRMs, categorical features are viewed as one-hot vectors where only the \ud835\udc56 -th position is set to 1 and the rest are set to 0, facilitating the retrieval of the corresponding row vector from the embedding table. Each input data, sampled from distribution D , contains categorical features \ud835\udc65 \ud835\udc50\ud835\udc4e\ud835\udc61 , numerical features \ud835\udc65 \ud835\udc5b\ud835\udc62\ud835\udc5a , and a label \ud835\udc66 . We denote \ud835\udc38 as the embedding tables and \ud835\udc53 as the other neural network layers, then the process of minimizing the loss can be formulated as follows: In each iteration, after the forward pass, an optimizer such as Adam [28] is applied to update the embedding table and other parameters. Frequently used notations in this paper are detailed in Table 1.", "2.2 Embedding Compression": "Embedding compression is mainly conducted within a memory constraint. Denoting \ud835\udc40 as the memory budget of the embedding table, M as the memory function mapping an embedding table to corresponding memory usage, and \ud835\udc38 \u2217 as the compressed embedding table, the optimization of DLRM within a memory constraint is formulated as follows: The memory function excludes the memory usage of neural networks since it is fixed and negligible compared to the memory usage of embedding tables. We define compression ratio as the multiple of the original memory to the compressed memory, to reflect the degree of compression: \ud835\udc36\ud835\udc45 = M( \ud835\udc38 ) M( \ud835\udc38 \u2217 ) . In practical applications, using a compression ratio of 10 \u00d7 can reduce the cost of distributed deployment, 100 \u00d7 to 1000 \u00d7 can allow for single-device deployment, and an extreme compression ratio of 10000 \u00d7 can enable DLRMs on edge devices. For online training, the fixed data distribution D in the above definition can be modified to a variable distribution D \ud835\udc61 , which is continuously evolving over time \ud835\udc61 .", "3 CAFE DESIGN": "", "3.1 CAFE Overview": "Rationale: We design CAFE, an efficient embedding framework that is simultaneously compact, adaptive, and fast. The key idea of CAFE is to dynamically distinguish important features (called hot features) from unimportant ones (called non-hot features), and allocate more resources to hot features. Specifically, we define the importance score of a feature using the L2-norm of its gradient, which is proven to have good theoretical properties in Section 3.5.2 and also in previous works [18, 26, 34]. We further experimentally demonstrate the effectiveness of gradient norms in Section 5.3. We observe that in most training data, the feature importance follows a highly skewed distribution, where most features have small importance scores and only a small fraction of hot features are very important. For example, Figure 3 illustrates that the feature importance distributions in Criteo dataset and CriteoTB dataset are highly consistent with Zipf distributions of parameters 1.05 and 1.1, respectively. Therefore, if we can allocate more memory to the embeddings of hot features and less memory to those of non-hot features, it is possible to significantly improve the model quality under the same memory usage of embedding tables. (a) Criteo. 10 10 10-6 10 Gradient Norm Zipf(1.05) 10' 10 Feature ID 105 (b) CriteoTB. 10-2 10 - 1076 1 10-8 Gradient Norm Zipf(1.1) 103 10 Feature ID 101 103 As shown in Figure 4, in CAFE, we propose a novel sketch algorithm, called HotSketch, to capture feature importance and report top\ud835\udc58 hot features in real time (Section 3.2). In each training iteration, we first fetch data samples from the input training data, and query each feature from these samples in HotSketch. For each feature, HotSketch reports its current importance score, and if its score exceeds a predefined threshold, we regard it as a hot feature. We then lookup the embeddings for hot and non-hot features respectively. In CAFE, for each hot feature, we allocate a unique embedding, and we store the pointer to this embedding in HotSketch. For the non-hot features, we use hash embedding tables where multiple features can share one embedding. We will discuss how to migrate embeddings between the tables of hot and non-hot features (Section 3.3). Guided by our design philosophy, we further propose a multi-level hash embedding framework to better embrace the skewed feature importance distribution (Section 3.4). Afterwards, we feed the embeddings into the downstream neural network for prediction and get the gradient norm for each feature. Finally, we update the importance of these features in HotSketch using their gradient norms. CAFE 8 Uncompressed Lookup 'embeddings Embedding Table Input Data respectively Prediction Fetch data Hot Feed embeddings features into downstream samples neural network HotSketch Migrate DNN embeddings Use HotSketch to Hash Embedding Table distinguish features with different Non-hot importances features Update feature importance with gradient norm", "3.2 The HotSketch Algorithm": "Rationale: We design HotSketch to capture hot features with high importance scores in a single pass, which is essentially a problem of finding top\ud835\udc58 frequent items (features) in streaming data. Currently, Space-Saving [45] is the most recognized algorithm for solving top\ud835\udc58 problem. It maintains frequent items in sorted doubly linked list and uses a hash table to index this list. However, this hash table not only doubles the memory usage but also imposes time inefficiency due to numerous memory accesses caused by pointer operations. Based on the idea of Space-Saving, we propose HotSketch, which removes the hash table while still maintaining the \ud835\udc42 ( 1 ) time complexity. We theoretically prove that our HotSketch well inherits the theoretical results of Space-Saving (Section 3.5.1), and empirically validate the performance of HotSketch (Section 5.6). Datastructure: Asdepicted in Figure 5, HotSketch consists of an array of \ud835\udc64 buckets B[ 1 ] , \u00b7 \u00b7 \u00b7 , B[ \ud835\udc64 ] . We use a hash function \u210e (\u00b7) to map each feature into one bucket. Each bucket contains \ud835\udc50 slots. Each slot stores a feature ID and its importance score. Insertion: For each incoming feature \ud835\udc53 \ud835\udc56 associated with an importance score \ud835\udc60 \ud835\udc56 , we first calculate the hash function to locate a bucket B[ \u210e ( \ud835\udc53 \ud835\udc56 )] , termed as the hashed bucket of \ud835\udc53 \ud835\udc56 . Then, we check bucket B[ \u210e ( \ud835\udc53 \ud835\udc56 )] and encounter three possible scenarios: (1) \ud835\udc53 \ud835\udc56 is recorded in B[ \u210e ( \ud835\udc53 \ud835\udc56 )] . We add \ud835\udc60 \ud835\udc56 to its importance score. (2) \ud835\udc53 \ud835\udc56 is not recorded in B[ \u210e ( \ud835\udc53 \ud835\udc56 )] and there exists an empty slot in B[ \u210e ( \ud835\udc53 \ud835\udc56 )] . We insert \ud835\udc53 \ud835\udc56 into the empty slot by setting this slot to ( \ud835\udc53 \ud835\udc56 , \ud835\udc60 \ud835\udc56 ) . (3) \ud835\udc53 \ud835\udc56 is not recorded in B[ \u210e ( \ud835\udc53 \ud835\udc56 )] and B[ \u210e ( \ud835\udc53 \ud835\udc56 )] is full. We locate the feature with the smallest score ( \ud835\udc53 \ud835\udc5a\ud835\udc56\ud835\udc5b , \ud835\udc60 \ud835\udc5a\ud835\udc56\ud835\udc5b ) , replace \ud835\udc53 \ud835\udc5a\ud835\udc56\ud835\udc5b with \ud835\udc53 \ud835\udc56 , and add \ud835\udc60 \ud835\udc56 to \ud835\udc60 \ud835\udc5a\ud835\udc56\ud835\udc5b . In other words, we set the slot ( \ud835\udc53 \ud835\udc5a\ud835\udc56\ud835\udc5b , \ud835\udc60 \ud835\udc5a\ud835\udc56\ud835\udc5b ) to ( \ud835\udc53 \ud835\udc56 , \ud835\udc60 \ud835\udc5a\ud835\udc56\ud835\udc5b + \ud835\udc60 \ud835\udc56 ) . Figure 5 shows an example of insertion. Discussion: HotSketch has the following advantages: (1) HotSketch has fast insertion speed. It processes each feature in a one-pass manner and has an \ud835\udc42 ( 1 ) time complexity. In addition, HotSketch avoids complicated pointer operations and has only one memory access. (2) HotSketch is memory-efficient. It does not store pointers, and there are no empty slots in HotSketch after a brief cold start. (3) HotSketch is hardware-friendly and can be accelerated with multi-threading and SIMD, thereby achieving superior data parallelism. HotSketch Feature Score +0.18 Feature Score B[h(f;)] 6.4 6.58 fs 0.12 0.12 Feature recorded in one slot, increment its score_ Feature Score +0.42 Feature Score h(f2) B[h(f2)] 3.4 3.4 0.42 Feature not recorded, insert into empty slot. h(fs) [Feature Score +0.21 Feature Score B[h(f3)] 2.32 2.53 4.8 4.8 Feature not recorded, evict least important feature", "3.3 Migration Strategy": "During the online training of DLRMs, the distribution of feature importance fluctuates with data distribution changes, meaning that the hot features are not constant throughout the training process. Since HotSketch already records the feature importance during training, it can naturally support dynamic hot features by embeddings migration between uncompressed and hash embedding tables. In HotSketch, we set a threshold to distinguish hot features, and the entry and exit of hot features occur throughout the training process. Almost every feature that reaches HotSketch for the first time is considered a non-hot feature with a low importance score. When a non-hot feature's importance score surpasses the threshold, it transitions into a hot feature, and its embedding migrates from the shared table to the uncompressed table as initialization, ensuring the feature's representation remains smooth throughout the training process. Conversely, when a hot feature's importance score drops below the threshold (by eviction or decay), it becomes a non-hot feature, and its embedding is discarded from the uncompressed table. Considering that the newly migrated non-hot feature is no longer important, its original exclusive embedding is simply ignored and the shared embedding is used instead. The threshold is meticulously set, allowing HotSketch to always saturate with hot features and adapt to distribution changes. If the importance scores alter rapidly, we decay the scores periodically. During training, it's vital to maintain an appropriate migration frequency. If the migration occurs too frequently, the learning process may not be smooth enough due to the replacement of embeddings, and the migration will generate substantial delay. Conversely, if the migration occurs too infrequently, HotSketch cannot capture changes in the distribution, leading to a decline in model quality. By setting a suitable threshold in HotSketch, a moderate migration frequency can enable the model to adapt to changes in distribution without negatively impacting convergence and latency.", "3.4 Multi-level Hash Embedding": "In HotSketch, features are categorized into hot and non-hot features, with the latter outnumbering the former, considering typical compression ratios ranging from 10 \u00d7 to 1000 \u00d7 . A substantial number of non-hot features are treated identically in HotSketch, sharing a hash embedding table with the same rate of collisions. Given that these features' importance scores also conform to a highly skewed Zipf distribution, it's logical to further segregate non-hot features based on their importance scores and assign different hash embedding tables to them. Therefore, we integrate multi-level hash embedding, as shown in Figure 6. With multi-level hash embedding, we partition non-hot features into more refined categories of different importance levels, and assign to them different number of embeddings from multiple tables. For simplicity, we focus on 2-level hash embedding, where non-hot features are divided into medium features and cold features. We expand the functionality of HotSketch to identify medium features by estimating their importance scores. Since medium features are more significant, they reference 2 embedding vectors from 2 distinct hash embedding tables, while cold features only look up a single embedding vector. This design draws inspiration from prior hash-based methods [74] that also adopt multiple embedding vectors to enrich representations and boost model quality. We illustrate the multi-level embedding process using an example in Figure 6. (1) Input features \ud835\udc53 1 , \ud835\udc53 2 , \ud835\udc53 3 are fed into HotSketch. Among them \ud835\udc53 1 has a score larger than the hot threshold, \ud835\udc53 2 has a score above the medium threshold, and \ud835\udc53 3 has a score lower than the thresholds, so they are classified as hot, medium, and cold features respectively. (2) Hot and cold features look up the embedding vectors as before. (3) Medium feature \ud835\udc53 2 looks up two embedding vectors from two hash embedding tables, and obtains the final embedding through a pooling process. To ensure that the Hot feature Embedding Uncompressed Embedding Table Features f , f2, and f3 Multi-level Hash Embedding Table Input Data Pool Medium HotSketch feature DNN Non-hot 92(f2) features 9 (f3) [Feature Score f2, and f3 Cold 6.4 feature 1.2 Report feature importance Multi-level Hash Embedding training process remains smooth, the hash function is combined with hash tables. When a feature is migrated between middle and cold classes, it always retrieves the same embedding vector from the first embedding table. For pooling operation, in practice, we find that simple summation of embeddings performs well, since a feature's embedding vectors are always updated in the same direction. Thedesign of the multi-level hash embedding is based on the observation that a unique embedding is a comprehensive representation with no information loss, whereas, for hash embeddings, the larger the number of embeddings involved, the fewer the collisions and the more information a feature can retain. Through experiments detailed in Section 5.4, we find that multi-level hash embedding performs better, with a reduction of 0 . 25% in training loss and an increase of 0 . 08% in testing AUC.", "3.5 Theoretical Analysis": "", "3.5.1 Accuracy of HotSketch .": "In this section, we theoretically analyze the performance of HotSketch in finding hot features. We derive the probability that a hot feature with a large importance score is recorded in HotSketch. Theorem 3.1. Given a data stream with \ud835\udc5b features, and suppose their importance score vector is \ud835\udc4e = { \ud835\udc4e 1 , \ud835\udc4e 2 , \u00b7 \u00b7 \u00b7 , \ud835\udc4e \ud835\udc5b } , where \ud835\udc4e 1 \u2a7e \ud835\udc4e 2 \u2a7e \u00b7 \u00b7 \u00b7 \u2a7e \ud835\udc4e \ud835\udc5b . Suppose that our HotSketch has \ud835\udc64 buckets, and each bucket contains \ud835\udc50 cells. Without distribution assumption, for a hot feature with a total score larger than \ud835\udefe \u2225 \ud835\udc4e \u2225 1 , it can be held in HotSketch with probability at least: Pr > 1 -1 -\ud835\udefe ( \ud835\udc50 -1 ) \ud835\udefe\ud835\udc64 . Proof. The expected score sum of the other features \u02c6 \ud835\udc53 entering the same bucket is: \ud835\udc38 h \u02c6 \ud835\udc53 i = ( 1 -\ud835\udefe ) \u2225 \ud835\udc4e \u2225 \ud835\udc64 . By following the properties of SpaceSaving algorithm, if the score \u02c6 \ud835\udc53 of the other features entering the bucket is no more than ( \ud835\udc50 -1 ) \ud835\udefe \u2225 \ud835\udc4e \u2225 1 , then the feature must be held in the bucket. Using Markov inequality, we have Pr GLYPH<16> \u02c6 \ud835\udc53 > ( \ud835\udc50 -1 ) \ud835\udefe \u2225 \ud835\udc4e \u2225 1 GLYPH<17> \u2a7d 1 -\ud835\udefe ( \ud835\udc50 -1 ) \ud835\udefe\ud835\udc64 , which means that Pr > 1 -1 -\ud835\udefe ( \ud835\udc50 -1 ) \ud835\udefe\ud835\udc64 . \u25a1 Lemma 3.2. Given a data stream with score vector \ud835\udc4e = { \ud835\udc4e 1 , \ud835\udc4e 2 , \u00b7 \u00b7 \u00b7 , \ud835\udc4e \ud835\udc5b } , where \ud835\udc4e 1 \u2a7e \ud835\udc4e 2 \u2a7e \u00b7 \u00b7 \u00b7 \u2a7e \ud835\udc4e \ud835\udc5b . Suppose that \ud835\udc4e follows a Zipfian distribution with parameter \ud835\udc67 , meaning that \ud835\udc4e \ud835\udc56 = \ud835\udc4e 1 \ud835\udc56 \ud835\udc67 . Suppose our HotSketch has \ud835\udc64 buckets, and each bucket contains \ud835\udc50 cells. Suppose we would like to check whether the \ud835\udc58 \u2032 hottest features can be hashed into the buckets. Then the mathematical expectation of the score sum of the non-hot features entering each bucket is: \ud835\udc38 [ \u02c6 \ud835\udc53 ] \u2a7d \u2225 \ud835\udc4e \u2225 1 \u00b7 \ud835\udc58 \u2032 1 -\ud835\udc67 \ud835\udc64 with probability at least 3 -\ud835\udc58 \u2032 \ud835\udc64 for \ud835\udc67 > 1 and \ud835\udc5b \u2192+\u221e . Proof. The probability that the \ud835\udc58 \u2032 hottest features are not hashed into this bucket is: GLYPH<0> 1 -1 \ud835\udc64 GLYPH<1> \ud835\udc58 \u2032 = GLYPH<0> GLYPH<0> 1 -1 \ud835\udc64 GLYPH<1> \ud835\udc64 GLYPH<1> \ud835\udc58 \u2032 \ud835\udc64 > 3 -\ud835\udc58 \u2032 \ud835\udc64 . When \ud835\udc64 \u2a7e 6, GLYPH<0> 1 -1 \ud835\udc64 GLYPH<1> \ud835\udc64 increases monotonically with \ud835\udc64 . The expected score sum of the non-hot features entering this bucket is: for \ud835\udc67 > 1 and \ud835\udc5b \u2192+\u221e . \u25a1 Theorem3.3. Given a data stream with score vector \ud835\udc4e = { \ud835\udc4e 1 , \ud835\udc4e 2 , \u00b7 \u00b7 \u00b7 , \ud835\udc4e \ud835\udc5b } , where \ud835\udc4e 1 \u2a7e \ud835\udc4e 2 \u2a7e \u00b7 \u00b7 \u00b7 \u2a7e \ud835\udc4e \ud835\udc5b . Suppose that \ud835\udc4e follows a Zipfian distribution with parameter \ud835\udc67 . Suppose that our HotSketch has \ud835\udc64 buckets, and each bucket contains \ud835\udc50 cells. Let \ud835\udc58 \u2032 = \ud835\udf02\ud835\udc64 . Then for a hot feature with a score larger than \ud835\udefe \u2225 \ud835\udc4e \u2225 1 , it can be held in the sketch with probability at least: Pr > \ud835\udc60\ud835\udc62\ud835\udc5d \ud835\udf02 > 0 GLYPH<16> 3 -\ud835\udf02 \u00b7 GLYPH<16> 1 -\ud835\udf02 ( \ud835\udc50 -1 ) \ud835\udefe ( \ud835\udf02\ud835\udc64 ) \ud835\udc67 GLYPH<17> GLYPH<17> for \ud835\udc67 > 1 and \ud835\udc5b \u2192+\u221e . Proof. The condition C that none of the \ud835\udc58 \u2032 hottest features collide with this item holds with probability at least 3 -\ud835\udc58 \u2032 \ud835\udc64 . By following the properties of SpaceSaving algorithm, if the scores \u02c6 \ud835\udc53 of the other features entering the bucket is no more than ( \ud835\udc50 -1 ) \ud835\udefe \u2225 \ud835\udc4e \u2225 1 , then the feature must be held in the bucket. Using Markov inequality, we have Then we have Let \ud835\udc58 \u2032 = \ud835\udf02\ud835\udc64 , we have And we have the probability that this feature must be held greater than Corollary 3.4. The larger the parameter \ud835\udc50 , \ud835\udc64 , \ud835\udc67 , and \ud835\udefe , the larger the probability that the feature with score larger than \ud835\udefe \u2225 \ud835\udc4e \u2225 1 be held in the sketch. The larger \ud835\udc50 and \ud835\udc64 means the larger memory used by sketch, the larger \ud835\udc67 means the more skew the data stream is, and the larger \ud835\udefe means the hotter the feature is. Corollary 3.5. To let the feature with score larger than \ud835\udefe \u2225 \ud835\udc4e \u2225 1 be held with maximum probability in a fixed memory budget, the more skew the data stream is, the less cells per bucket should be used. Specifically, we recommend to use \ud835\udc50 = 1 + 1 \ud835\udc67 -1 . Proof. With a fixed memory budget \ud835\udc40 = \ud835\udc50\ud835\udc64 , to minimize \ud835\udf02 ( \ud835\udc50 -1 ) \ud835\udefe ( \ud835\udf02\ud835\udc64 ) \ud835\udc67 , we should maximize ( \ud835\udc50 -1 ) \ud835\udc64 \ud835\udc67 = GLYPH<0> \ud835\udc40 \ud835\udc64 -1 GLYPH<1> \ud835\udc64 \ud835\udc67 . As it has a derivative function the optimal \ud835\udc64 should be \ud835\udc67 -1 \ud835\udc67 \ud835\udc40 , and the optimal \ud835\udc50 \u2217 should be \u25a1 Discussion: From Corollary 3.5, we can see that under fixed memory usage ( \ud835\udc40 = \ud835\udc50\ud835\udc64 ), the optimal \ud835\udc50 is affected by data distribution. Under non-skewed data distribution (small \ud835\udc67 ), we should use larger \ud835\udc50 and smaller \ud835\udc64 to better approximate the results of basic Space-Saving. Under highly skewed data distribution (large \ud835\udc67 ), we should use smaller \ud835\udc50 and larger \ud835\udc64 to lower the impact of hash collisions between hot features. This might be because under highly skewed data, using small \ud835\udc50 can already guarantee us to find hot features with high probability. In this scenario, the performance of HotSketch is mainly affected by hash collisions between hot features. We surprisingly find that this corollary is consistent with our experimental results in Figure 18(a). 1e-05 2e-05 5e-05 1e-04 2e-04 5e-04 1e-03 \u03b3 (hotness) 1.1 1.4 1.7 2.0 z (skewness) 0.5 0.6 0.7 0.8 0.9 1.0 P robability Experimental analysis (Figure 7): Although we cannot directly obtain the analytical solution of Pr from Theorem 3.3, we can give the numerical solution of Pr under different \ud835\udefe and \ud835\udc67 by numerical simulation. In our simulation, we set \ud835\udc64 = 10000 and \ud835\udc50 = 4. We can see that larger \ud835\udc67 goes with higher Pr, showing that HotSketch is more suitable for capturing top\ud835\udc58 features on skewed data distribution. In addition, larger \ud835\udefe also goes with higher Pr, showing that hotter features have larger probability of being captured by HotSketch. The results are consistent with our design goal.", "3.5.2 Convergence Analysis against Deviation .": "As mentioned in Section 1.2, in hash-based methods, there will be deviations that can hinder the convergence of embeddings. CAFE aims to minimize the deviation of embedding gradients, which indeed reflects the deviation of embedding parameters. In this section, we analyze how this deviation affects the convergence of SGD algorithm. We study the following (non-convex) empirical risk minimization problem: where \ud835\udefc is learning rate, \ud835\udc54 \ud835\udc56 \ud835\udc61 = \u2207 \ud835\udc53 \ud835\udc56 ( \ud835\udf03 \ud835\udc56 \ud835\udc61 ) is the standard gradient without compression, e \ud835\udc54 \ud835\udc56 \ud835\udc61 is the practical gradient with compression. We make the assumptions below following [2, 16]. Assumptions. For \u2200 \ud835\udc56 \u2208 { 1 , 2 , ..., \ud835\udc41 } , \ud835\udf03, \ud835\udf03 \u2032 \u2208 R \ud835\udc37 , we make the following assumptions: (1. \ud835\udc3f -Lipschitz) \u2225\u2207 \ud835\udc53 \ud835\udc56 ( \ud835\udf03 ) - \u2207 \ud835\udc53 \ud835\udc56 ( \ud835\udf03 \u2032 )\u2225 < \ud835\udc3f \u2225 \ud835\udf03 -\ud835\udf03 \u2032 \u2225 ; (2. Bounded moment) E [\u2225\u2207 \ud835\udc53 \ud835\udc56 ( \ud835\udf03 )\u2225] < \ud835\udf0e 0 , E [\u2225\u2207 \ud835\udc53 ( \ud835\udf03 )\u2225] < \ud835\udf0e 0 ; (3. Bounded variance) E [\u2225\u2207 \ud835\udc53 \ud835\udc56 ( \ud835\udf03 ) - \u2207 \ud835\udc53 ( \ud835\udf03 )\u2225] < \ud835\udf0e ; (4. Existence of global minimum) \u2203 \ud835\udc53 \u2217 \ud835\udc60.\ud835\udc61 .\ud835\udc53 ( \ud835\udf03 ) \u2265 \ud835\udc53 \u2217 . Theorem 3.6. Suppose we run SGD optimization with CAFE on DLRMs satisfying the assumptions above, with \ud835\udf16 \ud835\udc61 = \u2225 e \ud835\udc54 \ud835\udc56 \ud835\udc61 -\ud835\udc54 \ud835\udc56 \ud835\udc61 \u2225 as the deviation of embedding gradients. Assume the learning rate \ud835\udefc satisfies \ud835\udefc < 1 \ud835\udc3f . After \ud835\udc47 steps, for \ud835\udf03 \ud835\udc47 which is randomly selected from { \ud835\udf03 0 , \ud835\udf03 1 , ..., \ud835\udf03 \ud835\udc47 -1 } , we have: The proof is in the supplementary file on our GitHub page [75]. As \ud835\udc47 increases, with a proper learning rate \ud835\udefc = \ud835\udc42 ( 1 \u221a \ud835\udc47 ) , the first two terms at the right hand side of above inequality tend to 0, and the convergence of SGD is mainly influenced by the deviation \ud835\udf16 \ud835\udc61 . In the scenario of compression, there is no bound for this deviation; yet the design of CAFE is proposed to minimize the deviation. Since we assign those importance features with exclusive embedding parameters, their parameters have little deviation; for features sharing embeddings with each other, the deviation is introduced by the hash collisions. Generally, we cannot directly obtain the deviation of gradients, but according to the \ud835\udc3f -Lipschitz assumption, the deviation of gradients is bounded by the deviation of weights. As non-hot features share embeddings with each other, the deviation of weights comes from other features' gradients, which may disturb the learning direction. Based on this observation, CAFE choose to use gradient norm as the importance of features. For less important features, their gradient norms are relatively small, which limits the deviation of weights to some extent.", "4 IMPLEMENTATION": "We implement CAFE as a plug-in embedding layer module based on PyTorch. It can directly replace the original Embedding module in any PyTorch-based recommendation models with minor changes. Usage examples can be found on our GitHub page [75]. We consider extending CAFE to other frameworks (TensorFlow, Hetu [46], etc.) in the future. CAFE Backend: We implement the HotSketch algorithm in C++ to reduce the overall latency, and implement the rest of CAFE using PyTorch operators. For HotSketch, we set the number of bucket in HotSketch to the pre-determined number of hot features, with 4 slots per bucket. We use one sketch structure for all feature fields instead of one sketch per field, because the distribution of hot features across fields is unclear, which is better handled directly with importance scores. Fault Tolerance: We register all HotSketch's states as buffers in CAFE's PyTorch module, so that the states can be saved and loaded alongside model parameters. This simple design requires no additional modifications and enables DLRM with CAFE to use checkpoints for training and inference. When training resumes with checkpoints, parameters and states are reloaded simultaneously. Memory Management: We place the whole HotSketch structure on CPU, since it is not computeintensive. Built upon PyTorch operators, CAFE's embedding module can run on any accelerators (including CPU, GPU) where PyTorch is supported.", "5 EXPERIMENTAL RESULTS": "In this section, we conduct experiments on four widely used recommendation datasets and compare CAFE with existing embedding memory compression methods. We experimentally show that CAFE satisfies all three requirements. We also design experiments to reflect the effectiveness of HotSketch.", "5.1 Experimental Settings": "5.1.1 Models and Datasets. We conduct the experiments on three representative recommendation models DLRM [51] 1 , WDL [5], and DCN [61]. These models are popular in both academia and industry. All models follow the architecture discussed in Section 2.1, with slight differences in the neural network part. In DLRM, a cross layer performs dot operations between embeddings, producing cross terms for subsequent fully-connected (FC) layers; in WDL, embeddings are fed into a wide network (1 FC layer) and a deep network (several FC layers), and finally the results are summed together for predictions; in DCN, cross layers multiply the embeddings with their projected vectors, producing element-level cross terms for subsequent FC layers. Since our method is essentially an embedding layer plugin, the conclusions can be generalized to other recommendation models with little effort. We set the configurations of the models as in the original paper. We train on three large-scale datasets Avazu [62], Criteo [33], KDD12 [1], and an extremely large-scale dataset CriteoTB [32]. Criteo Kaggle Display Advertising Challenge Dataset (Criteo) [33] and Criteo Terabytes Click Logs (CriteoTB) [32] contain 7 and 24 days of ads click-through rate (CTR) prediction data respectively, which are adopted in MLPerf [54]. Each data sample has 13 numerical fields and 26 categorical fields. For CriteoTB, we set the field's maximum cardinality to 4 \ud835\udc52 7, the same as in the MLPerf configuration. Avazu Click-Through Rate Prediction Dataset (Avazu) [62] and KDD Cup 2012, Track 2 (KDD12) [1] are another two widely-used CTR datasets. They have no numerical field. Avazu contains 10 days of CTR data with 22 categorical fields. KDD12 has no temporal information, and has 11 categorical fields. For each dataset, we use the appropriate embedding dimension based on the benchmarks [51, 84] or our experiments on the uncompressed models. The statistics of the datasets are listed in Table 2. Since the numerical field is not our focus, we omit it from the table. In Section 5.5, we construct a new dataset with a more significant shift in data distribution to further validate CAFE's ability to adapt to changes in data distribution. 5.1.2 Baselines. Wecompare CAFE with Hash Embedding [64], Q-R Trick [58], and AdaEmbed [34]. Hash embedding is a simple baseline using only one hash function, providing a lower bound for all compression methods. Q-R Trick is an improved hash-based method, using multiple hash functions and complementary embedding tables to reduce the overall collisions. AdaEmbed is an adaptive method, recording all features' importance scores and dynamically allocates embedding vectors only for important features. We also compare with uncompressed embedding tables. In Section 5.2.4, we compare CAFE with a column compression method MDE [17]. If not specified, the hyperparameters of the baselines are the same as in the original paper or code. 5.1.3 Hardware Environment. We conduct all experiments on NVIDIA RTX TITAN 24 GB GPU cards. Since we focus on embedding compression with large compression ratios, we do not incur distributed training or inference. 5.1.4 Metrics. We employ training loss and testing AUC (area under the ROC curve) to measure model quality. Specifically, we use the data samples except the last day as the training set, and the data samples of the last day as the testing set. We use the testing AUC on the last day as the metric for offline training, and the average loss during training as the metric for online training. We train one epoch on the training set in chronological order, which is common in industry. Since KDD12 has no temporal information, we randomly shuffle the data and select 90% for training and the rest for testing. For memory usage, besides embedding tables, we also consider the memory of additional structures to achieve a fair judgment on memory efficiency. We use latency and throughput to measure the speed of each method.", "5.2 End-to-end Comparison": "In this section, we compare CAFE with baseline methods in an end-to-end manner. For large-scale datasets, we train with compression ratios ranging from 2 \u00d7 to 10000 \u00d7 , while for the CriteoTB dataset, we train with compression ratios ranging from 10 \u00d7 to 10000 \u00d7 , ensuring the model fits in the memory. 5.2.1 Metrics v.s. Compression Ratios. We conduct the main experiments on DLRM. The testing AUC and the training loss of Criteo and CriteoTB under different compression ratios are plotted in Figure 8, representing the performance of offline and online training respectively. For KDD12, we only plot the testing AUC in Figure 10(a) since it does not contain temporal information for online training. For Avazu, given the significant changes in distributions between days as shown in Figure 2, we focus on the online training performance and plot the training loss in Figure 10(b). Only CAFE and Hash can compress the embedding tables to extreme 10000 \u00d7 compression ratio, while Q-R Trick can only compress to around 500 \u00d7 due to its complementary index design, and AdaEmbed can only compress to 5 \u00d7 in Avazu and Criteo with dimension 16, 20 \u00d7 in KDD12 with dimension 64, and 50 \u00d7 in CriteoTB with dimension 128. Compared to Hash and Q-R Trick, CAFE is always closer to ideal result that uses uncompressed embedding tables, showing excellent memory efficiency. When varying the compression ratio, on Criteo dataset CAFE improves the testing AUC by 1 . 79% and 0 . 55% compared to Hash and Q-R Trick respectively on average; on CriteoTB dataset the improvement is 1 . 304% and 0 . 427%; on KDD12 dataset the improvement is 1 . 86% and 3 . 80%. CAFE also reduces the training loss by 2 . 31%, 0 . 72% on Criteo dataset, 1 . 35%, 0 . 59% on CriteoTB dataset, and 3 . 34%, 0 . 76% on Avazu dataset compared to Hash and Q-R Trick, exhibiting better performance for both offline and online training. The training loss of Hash fluctuates with the increase of CR on KDD12, which may be due to the instability of the Hash method and a certain degree of randomness in its embedding sharing. The improvement of CAFE over Hash is greater with larger compression ratio. Compared to Hash, at 10000 \u00d7 compression ratio, CAFE improves 3 . 92%, 3 . 68%, and 5 . 16% testing AUC on Criteo, CriteoTB, KDD12; CAFE reduces 4 . 61%, 3 . 24%, and 11 . 21% training loss on Criteo, CriteoTB, Avazu. Compared to AdaEmbed, CAFE reaches nearly the same testing AUC and training loss on Criteo dataset, achieves an increase of 0 . 04% testing AUC and a decrease of 0 . 12% training loss on CriteoTB dataset, achieves an increase of 0 . 82% testing AUC on KDD12 dataset and a decrease of 0 . 83% training loss on Avazu dataset. AdaEmbed can distinguish hot features with no errors, but it uses much memory for storing importance information of all features, with less memory for embedding vectors compared to CAFE, leading to comparable results at small compression ratios. Idea 0.79 2 0.78 0.77 Hash Q-R Trick 0.76 AdaEmbed 0 - CAFE (ours) 0.75 103 Compression Ratio Idea 0.79 0.78 Hash Q-R Trick 0.77 AdaEmbed CAFE (ours) 0.76 101 102 Compression Ratio 0,49 Hash Q-R Trick 0,48 AdaEmbed 8 CAFE (ours) 0,47 0.46 Idcal Compression Ratio Hash Q-R Trick 0.128 AdaEmbed : CAFE (ours) 0.126 0.124 Compression Ratio 5.2.2 Metrics v.s. Iterations. We check the convergence process of different methods. Figure 9 shows the metrics on Criteo and CriteoTB throughout iterations during training. Figure 10(c) shows the training loss on Avazu throughout iterations. We do not plot uncompressed embeddings trained on CriteoTB because the model cannot be held in our limited memory space. In Figure 9(a)-9(d), the testing AUC curves tend to increase because the model continues to learn during training and the data distribution gradually approaches the distribution of the last day testing data. CAFE has consistently better AUC during training compared to Hash and Q-R Trick. However, CAFE does not show better performance at the beginning of training compared to AdaEmbed, mainly because CAFE has a cold-start process to populate HotSketch, where all features are initially non-hot features. As training progresses, CAFE gradually achieves an AUC comparable to or better than AdaEmbed. In Figure 9(e)-9(h), and 10(c), the training loss curves fluctuate due to changes in data distributions. CAFE always has a closer training loss to ideal result than Hash and Q-R Trick on Criteo and Avazu datasets, showing better online training ability. The training curves of CAFE and AdaEmbed roughly coincide, since they are both designed for online training. The CriteoTB dataset is large enough to adequately train various methods, resulting in the loss curves of different methods being indistinguishable. 5.2.3 Experiments on WDL and DCN. We use another two models, WDL [5] and DCN [61], to experiment on the extremely large-scale dataset CriteoTB. The results are shown in Figure 11. Similar to DLRM, CAFE consistently outperforms Hash and Q-R Trick at different compression 0.79 0.78 Idcal Hash 0.77 Q-R Trick CAFE (ours) 300000 Iterations 0.80 0.79 0.78 Hash Q-R Trick 0.77 CAFE (ours 0.5 Iterations 1e6 Ideal Hash Q-R Trick 0,48 CAFE (ours) 0.46 0,44 250000 Iterations 0.52 Ideal AdaEmbed CAFE (ours) 8 0.48 0,46 0,44 0.42 100000 150000 250000 300000 Iterations 0.800 Ideal 0.785 AdaEmbed CAFE (ours) 100000 200000 300000 Iterations 0.800 0.795 0.790 0.785 780 AdaEmbed CAFE (ours) 0.5 Iterations Ie6 (c) AUC v.s. iter (Criteo 5 \u00d7 ). 0.16 Hash 0,15 Q-R Trick CAFE (ours) 8 014 0.11 0.10 20 Iterations 1e6 0,16 AdaEmbed 0.15 CAFE (ours) 8 014 0,13 0.11 0.10 20 Iterations 1e6 Ideal 0.75 0.70 AdaEmbed CAFE (ours) 101 102 103 10' Compression Ratio 28 0./7 2 0.76 Hash 0.74 Q-R Trick AdaEmbed 0.73 CAFE (ours) 102 Compression Ratio Hash Q-R Trick AdaEmbed : CAFE (ours) 0,42 0.40 101 102 103 104 Compression Ratio Ideal 0.45 Hash g AdaEmbed CAFE (ours) 0.35 50000 100000 150000 200000 250000 Iterations Hash 0.134 Q-R Trick AdaEmbed CAFE (ours) 0.130 0.128 Compression Ratio 0.76 Hash Q-R Trick 0.74 AdaEmbed CAFE (ours) 102 103 10' Compression Ratio (c) DCN, AUC v.s. CR. Hash Q-R Trick 0.132 AdaEmbed 8 CAFE (ours) 0.130 0.128 0.126 102 103 Compression Ratio ratios in both testing AUC and training loss. AdaEmbed is the most advanced compression method for small compression ratios, and CAFE achieves comparable performance to AdaEmbed. The training loss of Hash is not stable in WDL, possibly due to the instability of the Hash method itself and a certain degree of randomness in its embedding sharing. 5.2.4 Comparison with Column Compression. We also compare CAFE with MDE [17], a method that compresses columns of embedding tables instead of rows as in CAFE and other baselines. It introduces frequency information to allocate different embedding dimensions for different features, and then uses a trainable matrix to project the raw embeddings to the same final dimension. Since MDE does not compress the rows, and each feature needs to be assigned at least one dimension, the overall compression ratio is limited by the original embedding dimension. We plot the results in Figure 12. We also include a simple row compression baseline Hash for comparison. MDE's performance is comparable to Hash on Criteo, but it drops dramatically on CriteoTB. To reduce the number of projection matrices, MDE simply uses the feature cardinality of the field to derive the frequency instead of using the actual frequency, which does not effectively utilize important features. It also significantly reduces the rank of the embedding matrix at large compression ratios, causing the embedding to lose semantic information. According to the experimental results, CAFE always outperforms MDE. 802 2 0.801 0.800 Hash MDE 0.799 CAFE (ours) Compression Ratio 802 0.800 0.798 0.796 Hash MDE 0,794 CAFE (ours) Compression Ratio 0.455 Hash MDE CAFE (ours) 8 0.454 0.453 0.452 Compression Ratio Hash 0.1245 MDE CAFE (ours) 0.1240 0,1235 0.1230 Compression Ratio (a) Latency. Hash AdaEmbed Q-R Trick CAFE (ours) MDE Train Test Hash AdaEmbed U 600 Q-R Trick CAFE (ours) MDE 400 200 Train Test (b) Throughput. 5.2.5 Latency and Throughput. We test the latency and throughput of each method in Figure 13. The experiments are conducted on CriteoTB dataset with a compression ratio of 10 \u00d7 . We use 2048 batch size for training and 16384 batch size for inference, which is common in real applications. As the data loading time and the DNN computing time is the same across different methods, the difference lies in the operations of the embedding layer. Hash requires only an additional modulo operation compared to uncompressed embedding operations, and is therefore the fastest method in both training and inference. Q-R Trick is also fast, because it only additionally introduces hash processes and the aggregation of embedding vectors. Although MDE introduces matrix multiplication, it requires fewer memory accesses to obtain the embedding parameters, resulting in low latency and high throughput. AdaEmbed and CAFE incur reallocation or migration of embeddings, which are inevitable for dynamic adjustments, leading to higher latency and lower throughput. AdaEmbed regularly samples thousands of data to determine whether to reallocate, which introduces a large time overhead. In contrast, CAFE determines the migration in HotSketch with negligible time overhead. Compared to AdaEmbed, CAFE has 33 . 97% lower training latency and 1 . 20% lower inference latency. Through the further experimental results in Section 5.6, we can see that HotSketch's \ud835\udc42 ( 1 ) operation time only accounts for a small fraction of the overall time. 5.2.6 Comparison with Offline Separation. Wealso compare CAFE with an offline feature separation version on Criteo dataset. The offline separation version collects all data and makes statistics, separates hot and non-hot features according to frequency instead of gradient norms, and assigns embedding tables respectively. It uses the same embedding memory as in CAFE for hot and nonhot features. As shown in Figure 14(a), two versions achieve nearly the same testing AUC under several compression ratios. Compared to CAFE, the offline version has no errors in distinguishing hot features, but it can only use frequency, resulting in comparable performance. Figure 14(b) and Figure 14(c) show the testing AUC and the training loss throughout the training process at 1000 \u00d7 compression ratio. At the beginning of training, the offline version has better testing AUC and training loss, because CAFE has a cold-start process to fill in the slots. When the training process becomes stable, the two training loss curves almost completely coincide. The offline version, however, cannot be used in practical applications. First, it cannot adapt to online training, where the frequency information is unknown without recording. Second, in offline training, memory storage and additional data traversal process are required for statistics, causing much overhead. In contrast, CAFE naturally supports online and offline training without storing all importance information, so it can be directly applied in the industry. idea 0,800 0.795 0.790 0.785 0.780 0 - CAFE (ours) 101 102 10' 10' Compression Ratio 800 0.795 0.790 ; 0.785 Ideal 0.780 Offline CAFE (ours) 0.775 Iterations (a) AUC v.s. CR. (b) AUC v.s. iter (1000 \u00d7 ). (c) Loss v.s. iterations (1000 \u00d7 ). Ideal Offline 0.50 CAFE (ours) 8 0.48 0.46 0,44 0.42 50000 250000 Iterations 79321 9387 79487 79531 79517 79508 ; 78545 0.780 785 0.790 0.795 0.800 Test AUC 100 79461 300 79488 500 79531 700 79504 900 79499 0.7940 0.7945 0.7950 0.7955 0.7960 Test AUC 79531 CAFE 5 Field 76547 79504 0.76 0.77 0.78 0.79 0.80 0.81 Test AUC Freq (a) Memory for hot features. (b) Threshold of hot features. (c) Decay of scores. 79392 79515 8 79531 79517 wlo 79518 793 0.794 0.795 0.796 Test AUC (d) Design details.", "5.3 Configuration Sensitivity": "In this section, we study the impact of configurations in CAFE. We test different configurations on the Criteo dataset with a fixed compression ratio of 1000 \u00d7 , as shown in Figure 15. Memory for hot features. Given a limited memory constraint, we need to split the memory into three parts: sketch structure, hot features, and non-hot features. We define the term \"hot percentage\" as the percentage of memory occupied by sketch structure and hot features, while the rest is used for non-hot features. Since HotSketch stores 4 times the slots of the number of hot features, with each slot 3 attributes, the ratio of memory usage between HotSketch and \ud835\udc51 dimension exclusive embeddings is 12 : \ud835\udc51 . In the Criteo dataset, the dimension is set to 16, so HotSketch occupies 3 / 7 of memory in hot percentage. Figure 15(a) shows the testing AUC under different hot percentages, where 'loo' means 'leave-one-out', leaving only one embedding for non-hot features. A small hot percentage has low memory overhead of HotSketch, and allocate more memory for non-hot features, while a large hot percentage allocate more memory for hot features. As hot percentage gradually increases from 0.4 to 1, the testing AUC first rises then drops. When the hot percentage is small, enlarging hot percentage enables more hot features, contributing to model quality; when the hot percentage reaches 0.7, CAFE reaches the best testing AUC; when the hot percentage exceeds 0.7, HotSketch brings much overhead, and collisions of non-hot features increase dramatically, making the testing AUC drop. At the extreme case \"leave-one-out\", all the non-hot features share only one embedding, leading to very bad model performance. In practice, we find that setting hot percentage to around 0.7 is good enough for nearly all compression ratios. Threshold of hot features. Hot features are distinguished in HotSketch if their importance scores exceed the threshold. We test different thresholds, and the experimental results are shown in Figure 15(b). The testing AUC is bad when the threshold is set too high or too low. If the threshold is set too high, the memory space allocated for hot features cannot be saturated, resulting in waste of memory and more non-hot features sharing hash embeddings. If the threshold is set too low, the entry and exit of features will be too frequent, leading to unstable training process. When threshold is set to 500, CAFE reaches the best model AUC. Decay of scores. The decay coefficient in HotSketch determines the exit of features. All the importance scores in HotSketch, after a certain number of iterations, will be multiplied the decay coefficient to adapt to temporal variation of data distribution. We test different decay coefficients in Figure 15(c). In general, the smaller the coefficient, the easier it is for hot features to drop out as non-hot features. In experiments, we find that 0.98 is a proper value for decay coefficient in Criteo dataset, while smaller or larger decay coefficient both have poor performance. When the decay coefficient is too small, hot features cannot stay long in HotSketch, makes HotSketch not saturated and hot features mis-classified to non-hot features. When the decay coefficient is too large, features continuously occupy slots in HotSketch, even if they are no longer hot features. Other design details. We experiment on other design details of HotSketch. Currently we maintain only one exclusive embedding table for all fields, instead of maintaining one embedding table per field. This design makes hot features more flexible, distributed among fields only according to importance scores rather than cardinality. Figure 15(d) shows that maintaining only one exclusive embedding table leads to a substantial increase in model AUC. We also check the effect of using frequency information as importance score, with a worse testing AUC than gradient norm. Although frequency is a good indicator of feature importance, it has been proved theoretically and experimentally that gradient norm is better.", "5.4 Multi-level Hash Embedding": "In this section, we study the effect of multi-level hash embedding. The experimental results are shown in Figure 16, where CAFE-ML means CAFE combined with multi-level hash embedding. Under different compression ratios, CAFE-ML always performs better than CAFE, achieving 0 . 08% better testing AUC and reducing 0 . 25% training loss. CAFE-ML performs especially well with smaller compression ratios, causing nearly no degradation at 100 \u00d7 compression ratio. This is because CAFEML allocates more memory for multi-level hash embedding tables at small compression ratios, making the representation of medium features more precise. (a) AUC v.s. CR. Idea 0.800 0.795 0.790 0.785 CAFE 0.780 CAFE-ML 101 10? 103 104 Compression Ratio (b) Loss v.s. CR. CAFE CAFE-ML 0.465 0.460 0.455 Ideal 10' 10? 103 Compression Ratio", "5.5 Performance on Processed Dataset": "In this section, we construct a new dataset with a more significant shift in data distribution to further validate CAFE's ability to adapt to changes in data distribution. Keeping the testing data unchanged, we select the training data of days 1,4,7,...,22 from CriteoTB to form CriteoTB-1/3 dataset. As shown in Figure 2, generally the greater the number of days between, the greater the difference between feature distributions. Therefore, CriteoTB-1/3 has a more significant shift in data distribution. The results are shown in Figure 17. Although all methods exhibit slight performance degradation compared to CriteoTB, CAFE and AdaEmbed can adapt to changing data distributions and achieve relatively good results. Figure 17(c) shows that CAFE and AdaEmbed have almost the same training loss throughout the training process. However, Figure 17(a) and 17(b) indicate that CAFE actually outperforms AdaEmbed with a slight improvement, demonstrating stronger online training capabilities. 0,786 Hash Q-R Trick 0.784 AdaEmbed 0.782 Compression Ratio 0.12575 0.12550 8 0.12525 Hash 5 0.12500 Q-R Trick AdaEmbed 0.12475 CAFE (ours) 0.12450 Compression Ratio (a) AUC v.s. CR. (b) Loss v.s. CR. (c) Loss v.s. iterations (50 0.145 Hash 0,140 Q-R Trick AdaEmbed 8 0.135 CAFE (ours) 0.130 0.125 0,120 0.115 100000 200000 300000 400000 60000o Iterations \u00d7 ).", "5.6 HotSketch Performance": "Impact of the number of slots per bucket (Figure 18(a), 18(b)): We record the recall and the throughput of HotSketch with different number of slots per bucket. The experiments use the number of hot features on Criteo dataset (1000 \u00d7 ) as \ud835\udc58 . In Figure 18(a), recall generally increases as memory becomes larger. According to Corollary 3.5, the best number of slots per bucket locates at 11 to 21 given a Zipf distribution of parameter 1.05 to 1.1. Therefore, \ud835\udc50 = 8 and \ud835\udc50 = 16 exhibit a better recall than \ud835\udc50 = 4 and \ud835\udc50 = 32. The throughput of serialized Insert (write) and Query (read) shown in Figure 18(b) is on the order of 1 \ud835\udc52 7, greater than that of DLRM. Considering that we can parallelize operations in practice, the sketch time is only a small fraction in training and inference. Throughput drops as the number of slots increases, because more time is spent doing comparisons within buckets. Trading-off recall and throughput, we adopt 4 slots per bucket in our implementation, as we find it to be good enough for model quality. Finding real-time top\ud835\udc58 features (Figure 18(c),18(d)): We conduct experiments to evaluate the performance of HotSketch on finding two types of real-time hot features in online training: the up-to-date top\ud835\udc58 features, and the top\ud835\udc58 features in previous time window. These two types of top\ud835\udc58 features change with data distribution during the online training process, and thus can effectively reflect HotSketch's capability to adapt dynamic workloads. The experiments are conducted on Criteo using 6 days of online training data, with a sliding window size of 0.5 day. Figure 18(c) and 18(d) show the real-time Recall Rate of HotSketch during online training under different compression ratios. HotSketch always achieves > 90% Recall Rate on finding these two types of top\ud835\udc58 features, meaning that it can well catch up with the changing data distribution. (a) Recall. 950 925 0.850 c=32 825 600 800 1000 1200 1400 Memory (KB) (b) Throughput. c=8 c=16 c=32 1 Insert Query 0.96 0.94 0.90 Sliding-window Top-k 0.88 Up-to-date Top-k Days Sliding-window Top-k .88 Up-to-date Top-k Days", "6 RELATED WORK": "", "6.1 Embedding Compression": "Numerous compression techniques have been proposed for embedding tables, which can be broadly divided into two categories: row compression and column compression [76]. Row compression methods, including hash-based methods, adaptive methods, and CAFE, reduce the number of rows in embedding tables. Column compression methods, including quantization, pruning, and dimension reduction, compress each unique feature's representation, thereby reducing the number of columns (or the number of bits) in embedding tables. Since two categories are primarily orthogonal, methods of different categories can be further combined in DLRMs. Rowcompressionmethods: These methods aim to reduce the number of rows in embedding tables. Initial attempts to accommodate large numbers of embeddings within a limited memory space came from hash-based methods [58, 64, 68], which are widely used in real-world applications. They use simple hash functions to map categorical features onto a limited number of embeddings, resulting in different features sharing the same embedding vector in the event of hash collisions. However, hash-based methods do not provide theoretical bounds, which can lead to significant degradation in model quality. AdaEmbed [34] is an adaptive method that identifies and records important features. It dynamically reallocates embeddings for important features during online training, and achieves good model accuracy over time. However, its compression ratio is constrained by the storage of importance information, which scales linearly with the number of features. AdaEmbed's sampling and migration strategy also incurs much latency in online training. Column compression methods: Methods of this category aim to compress the representation for each unique feature, thereby reducing the number of columns (or the number of bits) in embedding tables. They borrow techniques from traditional deep learning compression , such as quantization [36, 67], pruning [10, 30], and dimension reduction [17, 42, 81]. Except for simple quantization and rule-based dimension reduction, most of these methods incorporate learnable structures to implicitly capture the importance of features, achieving similar or even better model accuracy compared to an uncompressed model. Nevertheless, they are unable to compress the embedding tables to small memory constraints during training. Specifically, quantization has a fixed compression ratio according to the data type; for example, if INT4 is used for compression, the compression ratio is fixed at 8 \u00d7 compared to FLOAT32. Generally, pruning and dimension reduction compress the embedding tables only at inference time, requiring additional memory to store extra structures during training. They are seldom used in industry, as the memory bottleneck during training is more severe due to activations and optimizer states. Most of these methods can only support offline training because they require collected data for multi-stage training, including pre-training, finetuning, and re-training.", "6.2 Sketching Algorithm": "Sketch is an excellent probabilistic data structure that can approximately record the statistics of data streams by maintaining a summary. Thanks to their small memory overhead and fast processing speed, sketches are widely applied in the realm of streaming data mining [8], database [6, 24, 41, 57], and network measurement and management [71, 78] to perform various tasks, such as frequency estimation [8, 9, 14], finding top\ud835\udc58 frequent items [35, 43, 45, 70], and mining special patterns in streaming data [40]. Existing sketches can be classified into two categories: counter-based sketches and KV-based sketches. Counter-based sketches: Typical counter-based sketches include CM [8], CU [14], Count [3], ASketch [55], and more [9, 15, 37, 53]. The data structures of these sketches usually consist of multiple arrays, each containing many counters. Each array is associated with one hash function that maps items into a specific counter in it. For example, the most popular CM sketch [8] comprises \ud835\udc51 counter arrays \ud835\udc36 1 , \u00b7 \u00b7 \u00b7 , \ud835\udc36 \ud835\udc51 . For each incoming item \ud835\udc52 , it is hashed into \ud835\udc51 counters in the CM sketch \ud835\udc36 1 [ \u210e 1 ( \ud835\udc52 )] , \u00b7 \u00b7 \u00b7 , \ud835\udc36 \ud835\udc51 [ \u210e \ud835\udc51 ( \ud835\udc52 )] with each of the \ud835\udc51 counters incremented by one. To query item \ud835\udc52 , CM sketch returns the minimum counter among \ud835\udc36 1 [ \u210e 1 ( \ud835\udc52 )] , \u00b7 \u00b7 \u00b7 , \ud835\udc36 \ud835\udc51 [ \u210e \ud835\udc51 ( \ud835\udc52 )] . However, the CM sketch has overestimated errors due to hash collisions. Other sketches propose various strategies to reduce this error. For instance, CU sketch [14] only increments the minimum counter among \ud835\udc36 1 [ \u210e 1 ( \ud835\udc52 )] , \u00b7 \u00b7 \u00b7 , \ud835\udc36 \ud835\udc51 [ \u210e \ud835\udc51 ( \ud835\udc52 )] , and Count sketch [3] adds \ud835\udc60 ( \ud835\udc52 ) \u2208 {+ 1 , -1 } to each mapped counter to achieve unbiased estimation. Despite these improvements, existing counter-based sketches are not memory efficient for finding top\ud835\udc58 items. They do not distinguish between frequent and infrequent items, where infrequent items are useless for reporting top\ud835\udc58 items, and recording infrequent items only increases the error of frequent items. Moreover, they need multiple memory accesses per insertion, resulting in unsatisfactory insertion speed. KV-based sketches: Common key-value-based sketches include Space-Saving [45], Unbaised Space-Saving [59], Lossy Counting [11], HeavyGuardian [70], and more [35, 71, 78]. These sketches maintain the KV pairs of frequent items in their data structures. For instance, Space-Saving [45] and Unbiased Space-Saving [59] use a data structure called Stream-Summary to record frequent items, which is essentially a doubly-linked list of fixed size, indexed by a hash table. When StreamSummary is full and an unrecorded item arrives, Space-Saving replaces the least frequent item with the incoming one. Based on Space-Saving, Unbiased Space-Saving [59] replaces the least frequent item with a certain probability, so as to achieve unbiased estimation. Unfortunately, Space-Saving and Unbiased Space-Saving are not memory and time efficient because of the extra hash table and complex pointer operations. Another type of KV-based sketches, such as HeavyGuardian [70] and WavingSketch [35], uses a bucket array data structure, where each bucket stores multiple KV pairs. These sketches provide satisfactory accuracy for reporting top\ud835\udc58 items and only require one memory access per insertion, ensuring fast insertion speed.", "7 CONCLUSION": "In this paper, we propose CAFE, a compact, adaptive, and fast embedding compression method that fulfills three essential design requirements: memory efficiency, low latency, and adaptability to dynamic data distribution. We introduce a light-weight sketch structure, HotSketch, to identify and record the importance scores of features. It incurs negligible time overhead, and its memory consumption is significantly lower than the original embedding tables. By assigning exclusive embeddings to a small set of important features and shared embeddings to other less important features, we achieve superior model quality within a limited memory constraint. To adapt to dynamic data distribution during online training, we incorporate an embedding migration strategy based on HotSketch. We further optimize CAFE with multi-level hash embedding, creating finergrained importance groups. Experimental results demonstrate that CAFE outperforms existing methods, with 3 . 92%, 3 . 68% higher testing AUC and 4 . 61%, 3 . 24% lower training loss at 10000 \u00d7 compression ratio on Criteo Kaggle and CriteoTB datasets, exhibiting superior performance in both offline training and online training. The source codes of CAFE are available at GitHub [75].", "ACKNOWLEDGMENTS": "This work is supported by National Key R&D Program of China (2022ZD0116315), National Natural Science Foundation of China (U22B2037, U23B2048, 62372009), PKU-Tencent joint research Lab.", "REFERENCES": "[1] Aden and Yi Wang. 2012. KDD Cup 2012, Track 2. https://kaggle.com/competitions/kddcup2012-track2. [2] Zeyuan Allen-Zhu. 2017. Natasha: Faster Non-Convex Stochastic Optimization via Strongly Non-Convex Parameter. In Proceedings of the 34th International Conference on Machine Learning (ICML) . [3] Moses Charikar, Kevin C. Chen, and Martin Farach-Colton. 2002. Finding Frequent Items in Data Streams. In Automata, Languages and Programming, 29th International Colloquium (ICALP) . [4] Tianyi Chen, Jun Gao, Hedui Chen, and Yaofeng Tu. 2023. LOGER: A Learned Optimizer towards Generating Efficient and Robust Query Execution Plans. Proceedings of the VLDB Endowment 16, 7 (2023), 1777-1789. [5] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and Hemal Shah. 2016. Wide & Deep Learning for Recommender Systems. In Proceedings of the 1st Workshop on Deep Learning for Recommender Systems (DLRS@RecSys) . [6] Monica Chiosa, Thomas Preu\u00dfer, and Gustavo Alonso. 2021. SKT: A One-Pass Multi-Sketch Data Analytics Accelerator. Proceedings of the VLDB Endowment 14, 11 (2021), 2369-2382. [7] Michael Chui, James Manyika, Mehdi Miremadi, Nicolaus Henke, Rita Chung, Pieter Nel, and Sankalp Malhotra. 2018. Notes from the AI frontier: Insights from hundreds of use cases. McKinsey Global Institute 2 (2018). https://www.mckinsey.com/west-coast/~/media/McKinsey/Featured%20Insights/Artificial%20Intelligence/Notes% 20from%20the%20AI%20frontier%20Applications%20and%20value%20of%20deep%20learning/Notes-from-the-AIfrontier-Insights-from-hundreds-of-use-cases-Discussion-paper.pdf [8] Graham Cormode and S. Muthukrishnan. 2005. An improved data stream summary: the count-min sketch and its applications. Journal of Algorithms 55, 1 (2005), 58-75. [9] Fan Deng and Davood Rafiei. 2007. New estimation algorithms for streaming data: Count-min can do more. Webdocs. Cs. Ualberta. Ca (2007). [10] Wei Deng, Junwei Pan, Tian Zhou, Deguang Kong, Aaron Flores, and Guang Lin. 2021. DeepLight: Deep Lightweight Feature Interactions for Accelerating CTR Predictions in Ad Serving. In Proceedings of the 14th ACM International Conference on Web Search and Data Mining (WSDM) . [11] Xenofontas A. Dimitropoulos, Paul Hurley, and Andreas Kind. 2008. Probabilistic lossy counting: an efficient algorithm for finding heavy hitters. ACM SIGCOMM Computer Communication Review 38, 1 (2008), 5. [12] Yue Ding, Yuhe Guo, Wei Lu, Hai-Xiang Li, Meihui Zhang, Hui Li, An-Qun Pan, and Xiaoyong Du. 2023. ContextAware Semantic Type Identification for Relational Attributes. Journal of Computer Science and Technology 38, 4 (2023), 927-946. [13] Muhammad Ebraheem, Saravanan Thirumuruganathan, Shafiq R. Joty, Mourad Ouzzani, and Nan Tang. 2018. Distributed Representations of Tuples for Entity Resolution. Proceedings of the VLDB Endowment 11, 11 (2018), 1454-1467. [14] Cristian Estan and George Varghese. 2002. New directions in traffic measurement and accounting. ACM SIGCOMM Computer Communication Review 32, 4 (2002), 323-336. [15] Yao-Chung Fan and Arbee L. P. Chen. 2008. Efficient and robust sensor data aggregation using linear counting sketches. In 22nd IEEE International Symposium on Parallel and Distributed Processing (IPDPS) . [16] Fangcheng Fu, Yuzheng Hu, Yihan He, Jiawei Jiang, Yingxia Shao, Ce Zhang, and Bin Cui. 2020. Don't Waste Your Bits! Squeeze Activations and Gradients for Deep Neural Networks via TinyScript. In Proceedings of the 37th International Conference on Machine Learning (ICML) . [17] Antonio A. Ginart, Maxim Naumov, Dheevatsa Mudigere, Jiyan Yang, and James Zou. 2021. Mixed Dimension Embeddings with Application to Memory-Efficient Recommendation Systems. In IEEE International Symposium on Information Theory (ISIT) . [18] Siddharth Gopal. 2016. Adaptive Sampling for SGD by Exploiting Side Information. In Proceedings of the 33nd International Conference on Machine Learning (ICML) . [19] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: A Factorization-Machine based Neural Network for CTR Prediction. In Proceedings of the 26th International Joint Conference on Artificial Intelligence (IJCAI) . [20] Udit Gupta, Samuel Hsia, Vikram Saraph, Xiaodong Wang, Brandon Reagen, Gu-Yeon Wei, Hsien-Hsin S. Lee, David Brooks, and Carole-Jean Wu. 2020. DeepRecSys: A System for Optimizing End-To-End At-Scale Neural Recommendation Inference. In Proceedings of the 47th Annual International Symposium on Computer Architecture (ISCA) . [21] Udit Gupta, Carole-Jean Wu, Xiaodong Wang, Maxim Naumov, Brandon Reagen, David Brooks, Bradford Cottel, Kim M. Hazelwood, Mark Hempstead, Bill Jia, Hsien-Hsin S. Lee, Andrey Malevich, Dheevatsa Mudigere, Mikhail Smelyanskiy, Liang Xiong, and Xuan Zhang. 2020. The Architectural Implications of Facebook's DNN-Based Personalized Recommendation. In IEEE International Symposium on High Performance Computer Architecture (HPCA) . [22] Teng-Yue Han, Pengfei Wang, and Shaozhang Niu. 2023. Multimodal Interactive Network for Sequential Recommendation. Journal of Computer Science and Technology 38, 4 (2023), 911-926. [23] Ruihong Huang, Shaoxu Song, Yunsu Lee, Jungho Park, Soo-Hyung Kim, and Sungmin Yi. 2020. Effective and Efficient Retrieval of Structured Entities. Proceedings of the VLDB Endowment 13, 6 (2020), 826-839. [24] Yesdaulet Izenov, Asoke Datta, Florin Rusu, and Jun Hyung Shin. 2021. COMPASS: Online Sketch-based Query Optimization for In-Memory Databases. In Proceedings of the International Conference on Management of Data (SIGMOD) . [25] Biye Jiang, Chao Deng, Huimin Yi, Zelin Hu, Guorui Zhou, Yang Zheng, Sui Huang, Xinyang Guo, Dongyue Wang, Yue Song, Liqin Zhao, Zhi Wang, Peng Sun, Yu Zhang, Di Zhang, Jinhui Li, Jian Xu, Xiaoqiang Zhu, and Kun Gai. 2019. Xdl: an industrial deep learning framework for high-dimensional sparse data. In Proceedings of the 1st International Workshop on Deep Learning Practice for High-Dimensional Sparse Data . [26] Angelos Katharopoulos and Fran\u00e7ois Fleuret. 2018. Not All Samples Are Created Equal: Deep Learning with Importance Sampling. In Proceedings of the 35th International Conference on Machine Learning (ICML) . [27] Hyeonji Kim, Byeong-Hoon So, Wook-Shin Han, and Hongrae Lee. 2020. Natural language to SQL: Where are we today? Proceedings of the VLDB Endowment 13, 10 (2020), 1737-1750. [28] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In 3rd International Conference on Learning Representations (ICLR) . [29] Adrian Kochsiek and Rainer Gemulla. 2021. Parallel Training of Knowledge Graph Embedding Models: A Comparison of Techniques. Proceedings of the VLDB Endowment 15, 3 (2021), 633-645. [30] Shuming Kong, Weiyu Cheng, Yanyan Shen, and Linpeng Huang. 2023. AutoSrh: An Embedding Dimensionality Search Framework for Tabular Data Prediction. IEEE Transactions on Knowledge and Data Engineering 35, 7 (2023), 6673-6686. [31] Suyong Kwon, Woohwan Jung, and Kyuseok Shim. 2022. Cardinality Estimation of Approximate Substring Queries using Deep Learning. Proceedings of the VLDB Endowment 15, 11 (2022), 3145-3157. [50] Maxim Naumov, John Kim, Dheevatsa Mudigere, Srinivas Sridharan, Xiaodong Wang, Whitney Zhao, Serhat Yilmaz, Changkyu Kim, Hector Yuen, Mustafa Ozdal, Krishnakumar Nair, Isabel Gao, Bor-Yiing Su, Jiyan Yang, and Mikhail Smelyanskiy. 2020. Deep Learning Training in Facebook Data Centers: Design of Scale-up and Scale-out Systems. CoRR abs/2003.09518 (2020). [51] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu, Alisson G. Azzolini, Dmytro Dzhulgakov, Andrey Mallevich, Ilia Cherniavskii, Yinghai Lu, Raghuraman Krishnamoorthi, Ansha Yu, Volodymyr Kondratenko, Stephanie Pereira, Xianjie Chen, Wenlin Chen, Vijay Rao, Bill Jia, Liang Xiong, and Misha Smelyanskiy. 2019. Deep Learning Recommendation Model for Personalization and Recommendation Systems. CoRR abs/1906.00091 (2019). [52] Niketan Pansare, Jay Katukuri, Aditya Arora, Frank Cipollone, Riyaaz Shaik, Noyan Tokgozoglu, and Chandru Venkataraman. 2022. Learning Compressed Embeddings for On-Device Inference. In Proceedings of Machine Learning and Systems (MLSys) . [53] Guillaume Pitel and Geoffroy Fouquier. 2015. Count-Min-Log sketch: Approximately counting with approximate counters. In International Symposium on Web AlGorithms . [54] NVIDIA AI platform. 2020. MLPerf Benchmark. https://mlperf.org. [55] Pratanu Roy, Arijit Khan, and Gustavo Alonso. 2016. Augmented Sketch: Faster and More Accurate Stream Processing. In Proceedings of the International Conference on Management of Data (SIGMOD) . [56] Pengyang Shao, Le Wu, Lei Chen, Kun Zhang, and Meng Wang. 2022. FairCF: fairness-aware collaborative filtering. Science China Information Sciences 65, 12 (2022). [57] Benwei Shi, Zhuoyue Zhao, Yanqing Peng, Feifei Li, and Jeff M. Phillips. 2021. At-the-time and Back-in-time Persistent Sketches. In Proceedings of the International Conference on Management of Data (SIGMOD) . [58] Hao-Jun Michael Shi, Dheevatsa Mudigere, Maxim Naumov, and Jiyan Yang. 2020. Compositional Embeddings Using Complementary Partitions for Memory-Efficient Recommendation Systems. In Proceedings of the 26th ACM SIGKDD Conference on Knowledge Discovery & Data Mining (KDD) . [59] Daniel Ting. 2018. Data Sketches for Disaggregated Subset Sum and Frequent Item Estimation. In Proceedings of the International Conference on Management of Data (SIGMOD) . [60] Corinna Underwood. 2019. Use cases of recommendation systems in business-current applications and methods. Emerj (2019). https://emerj.com/ai-sector-overviews/use-cases-recommendation-systems/ [61] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & Cross Network for Ad Click Predictions. In Proceedings of the ADKDD'17 . [62] Steve Wang and Will Cukierski. 2014. Avazu Click-Through Rate Prediction. https://kaggle.com/competitions/avazuctr-prediction. [63] Zehuan Wang, Yingcan Wei, Minseok Lee, Matthias Langer, Fan Yu, Jie Liu, Shijie Liu, Daniel G. Abel, Xu Guo, Jianbing Dong, Ji Shi, and Kunlun Li. 2022. Merlin HugeCTR: GPU-accelerated Recommender System Training and Inference. In Proceedings of the 16th ACM Conference on Recommender Systems (RecSys) . [64] Kilian Q. Weinberger, Anirban Dasgupta, John Langford, Alexander J. Smola, and Josh Attenberg. 2009. Feature hashing for large scale multitask learning. In Proceedings of the 26th International Conference on Machine Learning (ICML) . [65] Minhui Xie, Kai Ren, Youyou Lu, Guangxu Yang, Qingxing Xu, Bihai Wu, Jiazhen Lin, Hongbo Ao, Wanhong Xu, and Jiwu Shu. 2020. Kraken: memory-efficient continual learning for large-scale real-time recommendations. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC) . [66] Xing Xie, Jianxun Lian, Zheng Liu, Xiting Wang, Fangzhao Wu, Hongwei Wang, and Zhongxia Chen. 2018. Personalized recommendation systems: Five hot research topics you must know. Microsoft Research Lab-Asia (2018). https: //www.microsoft.com/en-us/research/lab/microsoft-research-asia/articles/personalized-recommendation-systems/ [67] Zhiqiang Xu, Dong Li, Weijie Zhao, Xing Shen, Tianbo Huang, Xiaoyun Li, and Ping Li. 2021. Agile and Accurate CTR Prediction Model Training for Massive-Scale Online Advertising Systems. In Proceedings of the International Conference on Management of Data (SIGMOD) . [68] Bencheng Yan, Pengjie Wang, Jinquan Liu, Wei Lin, Kuang-Chih Lee, Jian Xu, and Bo Zheng. 2021. Binary Code based Hash Embedding for Web-scale Applications. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management (CIKM) . [69] Renchi Yang, Jieming Shi, Xiaokui Xiao, Yin Yang, Juncheng Liu, and Sourav S. Bhowmick. 2020. Scaling Attributed Network Embedding to Massive Graphs. Proceedings of the VLDB Endowment 14, 1 (2020), 37-49. [70] Tong Yang, Junzhi Gong, Haowei Zhang, Lei Zou, Lei Shi, and Xiaoming Li. 2018. HeavyGuardian: Separate and Guard Hot Items in Data Streams. In Proceedings of the 24th ACM SIGKDD Conference on Knowledge Discovery & Data Mining (KDD) . [71] Tong Yang, Jie Jiang, Peng Liu, Qun Huang, Junzhi Gong, Yang Zhou, Rui Miao, Xiaoming Li, and Steve Uhlig. 2018. Elastic sketch: adaptive and fast network-wide measurements. In Proceedings of the 2018 ACM SIGCOMM Conference . [72] Chunxing Yin, Bilge Acun, Carole-Jean Wu, and Xing Liu. 2021. TT-Rec: Tensor Train Compression for Deep Learning Recommendation Models. In Proceedings of Machine Learning and Systems (MLSys) . [73] Zhiyang Yuan, Wenguang Zheng, Peilin Yang, Qingbo Hao, and Yingyuan Xiao. 2023. Evolving Interest with Feature Co-action Network for CTR Prediction. Data Science and Engineering 8, 4 (2023), 344-356. [74] Caojin Zhang, Yicun Liu, Yuanpu Xie, Sofia Ira Ktena, Alykhan Tejani, Akshay Gupta, Pranay Kumar Myana, Deepak Dilipkumar, Suvadip Paul, Ikuhiro Ihara, Prasang Upadhyaya, Ferenc Huszar, and Wenzhe Shi. 2020. Model Size Reduction Using Frequency Based Double Hashing for Recommender Systems. In Proceedings of the 14th ACM Conference on Recommender Systems (RecSys) . [75] Hailin Zhang, Zirui Liu, and Boxuan Chen. 2023. Source codes related to CAFE. https://github.com/HugoZHL/CAFE. [76] Hailin Zhang, Penghao Zhao, Xupeng Miao, Yingxia Shao, Zirui Liu, Tong Yang, and Bin Cui. 2023. Experimental Analysis of Large-scale Learnable Vector Storage Compression. CoRR abs/2311.15578 (2023). [77] Jia-Dong Zhang and Chi-Yin Chow. 2015. GeoSoCa: Exploiting Geographical, Social and Categorical Correlations for Point-of-Interest Recommendations. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR) . [78] Yinda Zhang, Zaoxing Liu, Ruixin Wang, Tong Yang, Jizhou Li, Ruijie Miao, Peng Liu, Ruwen Zhang, and Junchen Jiang. 2021. CocoSketch: high-performance sketch-based measurement over arbitrary partial key query. In Proceedings of the 2021 ACM SIGCOMM Conference . [79] Weijie Zhao, Deping Xie, Ronglai Jia, Yulei Qian, Ruiquan Ding, Mingming Sun, and Ping Li. 2020. Distributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems. In Proceedings of Machine Learning and Systems (MLSys) . [80] Weijie Zhao, Jingyuan Zhang, Deping Xie, Yulei Qian, Ronglai Jia, and Ping Li. 2019. AIBox: CTR Prediction Model Training on a Single Node. In Proceedings of the 28th ACM International Conference on Information & Knowledge Management (CIKM) . [81] Xiangyu Zhao, Haochen Liu, Hui Liu, Jiliang Tang, Weiwei Guo, Jun Shi, Sida Wang, Huiji Gao, and Bo Long. 2021. AutoDim: Field-aware Embedding Dimension Searchin Recommender Systems. In Proceedings of the Web Conference (WWW) . [82] Yue Zhao, Gao Cong, Jiachen Shi, and Chunyan Miao. 2022. QueryFormer: A Tree Transformer Model for Query Plan Representation. Proceedings of the VLDB Endowment 15, 8 (2022), 1658-1670. [83] Guorui Zhou, Xiaoqiang Zhu, Chengru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep Interest Network for Click-Through Rate Prediction. In Proceedings of the 24th ACM SIGKDD Conference on Knowledge Discovery & Data Mining (KDD) . [84] Jieming Zhu, Jinyang Liu, Shuai Yang, Qi Zhang, and Xiuqiang He. 2021. Open Benchmarking for Click-Through Rate Prediction. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management (CIKM) . Received July 2023; revised October 2023; accepted November 2023"}
