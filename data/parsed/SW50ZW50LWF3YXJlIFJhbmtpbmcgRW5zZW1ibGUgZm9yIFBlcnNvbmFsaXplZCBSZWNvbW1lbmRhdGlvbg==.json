{"Intent-aware Ranking Ensemble for Personalized Recommendation": "", "Jiayu Li": "", "Zhefan Wang": "jy-li20@mails.tsinghua.edu.cn DCST, Tsinghua University Beijing, China Peijie Sun sun.hfut@gmail.com DCST, Tsinghua University Beijing, China", "Weizhi Ma": "", "Yangkun Li": "mawz@tsinghua.edu.cn AIR, Tsinghua University Beijing Academy of Artificial Intelligence Beijing, China liyangkun17@gmail.com DCST, Tsinghua University Beijing, China", "Zhoutian Feng": "wzf19@mails.tsinghua.edu.cn DCST, Tsinghua University Beijing, China", "Min Zhang \u2217": "z-m@tsinghua.edu.cn DCST, Tsinghua University BNRist Beijing, China", "Daiyue Xue": "fengzhoutian@hotmail.com Meituan Inc. Beijing, China", "ABSTRACT": "Ranking ensemble is a critical component in real recommender systems. When a user visits a platform, the system will prepare several item lists, each of which is generally from a single behavior objective recommendation model. As multiple behavior intents, e.g., both clicking and buying some specific item category, are commonly concurrent in a user visit, it is necessary to integrate multiple singleobjective ranking lists into one. However, previous work on rank aggregation mainly focused on fusing homogeneous item lists with the same objective while ignoring ensemble of heterogeneous lists ranked with different objectives with various user intents. In this paper, we treat a user's possible behaviors and the potential interacting item categories as the user's intent. And we aim to study how to fuse candidate item lists generated from different objectives aware of user intents. To address such a task, we propose an Intent-aware ranking Ensemble Learning (IntEL) model to fuse multiple single-objective item lists with various user intents, in which item-level personalized weights are learned. Furthermore, we theoretically prove the effectiveness of IntEL with point-wise, pair-wise, and list-wise loss functions via error-ambiguity decomposition. Experiments on two large-scale real-world datasets also show significant improvements of IntEL on multiple behavior objectives simultaneously compared to previous ranking ensemble models. \u2217 Min Zhang is the corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '23, July 23-27, 2023, Taipei, Taiwan. \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-9408-6/23/07...$15.00 https://doi.org/10.1145/XXXXXX.XXXXXX xuedaiyue@meituan.com Meituan Inc. Beijing, China", "CCS CONCEPTS": "\u00b7 Information systems \u2192 Recommendersystems ; Personalization .", "KEYWORDS": "Ranking ensemble, User intents, Personalized recommendation", "ACMReference Format:": "Jiayu Li, Peijie Sun, Zhefan Wang, Weizhi Ma, Yangkun Li, Min Zhang, Zhoutian Feng, and Daiyue Xue. 2023. Intent-aware Ranking Ensemble for Personalized Recommendation. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '23), July 23-27, 2023, Taipei, Taiwan. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/XXXXXX.XXXXXX", "1 INTRODUCTION": "Users typically have various intents when using recommender systems. For instance, when shopping online, users may intend to buy snacks or browse clothes. Generally, we call the users' behaviors the behavior intents and their interacted item categories the item category intents. Multiple behavior intents may be concurrent in a visit, and users need distinct items with different item category intents. Therefore, user intents are essential to recommender systems for recommendation list generation. In this paper, we follow the definition of user intents by Chen et al. [10] as a combination of user behavior and item category, such as booking an item with a hotel category or clicking an item in a phone category. From the systems' viewpoint, since users usually have diverse intents, multiple item lists will be generated when a user visits the platform. These lists generally come from recommendation models optimized with different behavior objectives, such as clicking, consuming, or browsing duration. Existing research has made promising achievements with a single objective, such as predicting Click Through Rate (CTR) [4, 20, 48] and Conversion Rate (CVR) [14, 32]. SIGIR '23, July 23-27, 2023, Taipei, Taiwan. Jiayu Li, Peijie Sun, Zhefan Wang, Weizhi Ma, Yangkun Li, Min Zhang, Zhoutian Feng, and Daiyue Xue user Requirement: Get a new phone charger Final Item List intent buy Recommender System behavior category charger click phone Single-objective Ranking lists Clicking Purchasing p 1 h 1 c 1 c 1 w 1 c 2 p 1 Int1 Int2 click headset Int3 headset 1 charger 1 charger 2 phone 1 watch 1 behavior objective w 1 c 2 Intent-aware Ranking Ensemble h 1 \u221a \u221a \u221a \u221a \u00d7 Figure 1: An example of fusing two single-objective item lists into a final list aware of multiple concurrent user intents. However, as multiple intents of a user may appear in a visit, it is crucial to aggregate multiple heterogeneous single-objective ranking lists aware of the user's current intents. An example of the intent-aware ranking ensemble on an online shopping platform is shown in Figure 1. A user needs to buy a phone charger, and she also wants to browse new products about phones and headsets. The system has two single-objective ranking lists ready when she visits the platform. These lists are produced by two recommendation models optimized with users' consumption and clicking histories, respectively. To satisfy the user's diverse intents at once, an intent-aware ranking ensemble model is adopted to aggregate two ranking lists for a final display, where items are reordered according to both basic ranking lists and the user's intents. Thus, charger 1 , charger 2 , phone 1 , and headset 1 are placed at the front of the final list, satisfying users' preference better than both single-objective ranking lists. Therefore, intent-aware ranking ensemble is important for promoting recommendation performance. However, there have been few attempts to combine heterogeneous single-objective ranking lists (Hereinafter referred to as basic lists) considering user intents. In industry, a common strategy is simply summing basic lists with pre-defined list-level weights, which ignores users' personalized preference. While in academia, existing studies are not adequate to handle ranking ensemble for personalized recommendation. Widely-explored unsupervised rank aggregation methods [3, 21, 23] are mostly studied in information retrieval tasks rather than recommendation scenario. Recently, supervised methods [1, 2, 30] have been proposed to combine different item lists in recommendation. Nevertheless, these studies focused on combining homogeneous item lists optimized for the same behavior, not the heterogeneous rank lists for different objectives. Users' intents are also overlooked in the ranking ensemble stage. To aggregate basic lists aware of user intents, we aim to learn different weights for different basic lists and item categories to sum basic lists' scores. However, it is challenging since numerous weights should be assigned for all items in all basic lists, which may be hard to learn. Therefore, we first prove its effectiveness theoretically. Unlike previous studies, we aim to assign ensemble weights at item level rather than list level. We prove the effectiveness of this form of ranking ensemble and verify that the loss of the ensemble list can be smaller than the loss of any basic models with point-wise, pair-wise, and list-wise loss functions. An ambiguity term is derived from the proof and used for optimization loss. With theoretical guarantees, another challenge in practice is to infer users' intents and integrate the intents into ranking ensemble of heterogeneous basic lists. To address this challenge, we propose an Intent-aware ranking Ensemble Learning (IntEL) method for personalized ensemble of multiple single-objective ranking lists adaptively. A sequential model is adopted to predict users' intents. And a ranking ensemble module is designed to integrate basic list scores, item categories, and user intents. Thus, the learnable ranking ensemble model can adaptively adjust the integration of multiple heterogeneous lists with user intents. Weconducted experiments on a public-available online shopping recommendation dataset and a local life service dataset. Our method, IntEL, is compared with various ensemble learning baselines and shows significant improvements. The main contributions of this work are as follows: \u00b7 To our knowledge, it is the first work that aims to generalize ranking ensemble learning with item-level weights on multiple heterogeneous item lists. We theoretically prove the effectiveness of ranking ensemble in this new setting. \u00b7 A novel intent-aware ranking ensemble learning model, IntEL, is proposed to fuse multiple single-objective recommendation lists aware of user intents adaptively. In the model, ambiguity loss, ranking loss, and intent loss have been proposed and integrated. \u00b7 Experiments on two large-scale real-world recommendation datasets indicate IntEL is significantly superior to previous ranking ensemble models on multiple objectives.", "2 RELATED WORK": "", "2.1 Ranking Ensemble": "Ranking ensemble, i.e., fusing multiple ranking lists for a consensus list, has been long discussed in IR scenario [17, 19] and proved to be NP-hard even with small collections of basic lists [16]. In general, rank aggregation includes unsupervised and supervised methods. Unsupervised methods only rely on the rankings. For instance, Borda Count [5] computed the sum of all ranks. MRA [18] adopted the median of rankings. Comparisons among basic ranks were also used, such as pair-wise similarity in Outrank [19]and distance from null ranks in RRA [21]. Recently, researchers have paid attention to supervised rank aggregation methods. For example, the Evolutionary Rank Aggregation (ERA) [30] was optimized with genetic programming. Differential Evolution algorithm [1, 2] and reinforcement learning [47] were also adopted for rank aggregation optimization. However, these rank aggregation methods only utilized the rank or scores of items in basic lists without considering item contents and users in recommendation. Another view on the fusion problem comes from ensemble learning. It is a traditional topic in machine learning [35], which has been successfully applied to various tasks [29, 39, 42]. A basic theory in ensemble learning is error-ambiguity (EA) decomposition analysis [22], which proves better performance can be achieved with aggregated results with good and diverse basic models. It was proved in classification and regression with diverse loss functions [6, 45]. Liu et al. [24] generalized EA decomposition to model-level weights Intent-aware Ranking Ensemble for Personalized Recommendation SIGIR '23, July 23-27, 2023, Taipei, Taiwan. in ranking ensemble with list-wise loss, where different items in a list shared the same weights. The differences between the previous studies and our method are mainly twofold: First, rather than calculate a general weight for each basic model, we extend to assign item-level weights considering item category and user behavior intents. We theoretically prove the effectiveness of this extension. Second, we aim to combine heterogeneous lists generated for different behavior objectives and simultaneously improve performance on multiple objectives.", "2.2 Multi-Intent Recommendation": "Since we aggregate ranking lists aware of users' multiple intents, we briefly introduce recent methods on multi-intents and multiinterests in recommender systems. Existing studies focused on capturing dynamic intents in the sequential recommendation [10, 25, 26, 40, 43]. For instance, AIR [10] predicted intents and their migration in users' historical interactions. Wang et al. [40] modeled users' dynamic implicit intention at the item level to capture item relationships. MIND [34] and ComiRec [9] adopted dynamic routing from historical interactions to capture users' multi-intents and diversity. TimiRec [41] distilled target user interest from predicted distribution on multi-interest of the users. With the development of contrastive learning, implicit intent representations were also applied as constraints on contrastive loss [11, 15]. Previous studies usually mixed 'intent' and 'interest' and paid attention to intent on item contents in single-behavior scenarios. However, we follow [10] to consider both behavior intents and item category intents. Moreover, instead of learning user preference for each intent, we utilize intents as guidance for fusing user preference with different behavior objectives.", "2.3 Multi-Objective Recommendation": "Another brunch of related but different work is the multi-objective recommendation. It mainly contains two groups of studies. One group provides multiple recommendation lists for different objectives with shared information among objectives, such as MMOE [28] and PLE [38], where different lists are evaluated on corresponding objectives separately. The other group tried to promote the model performance on a target behavior objective with the help of other objectives, such as MB-STR [46] predicting users' click preferences. However, instead of generating multiple lists or specifying a target behavior, we fuse a uniform list on which multiple objectives are evaluated simultaneously. Some studies that tried to jointly optimize ranking accuracy and other goals are also called multi-objective recommendation, such as fairness [44], diversity [8], etc. They sought to promote other metrics while maintaining utility on some behavior. But we aim to concurrently promote performance on multiple objectives by aggregating various recommendation lists.", "3 PRELIMINARIES": "", "3.1 Ranking Ensemble Learning Definition": "Let F = { \ud835\udc53 1 , \ud835\udc53 2 , ..., \ud835\udc53 \ud835\udc3e } be \ud835\udc3e basic models that are trained for \ud835\udc3e different objectives (such as click, buy, and favorite, etc.), I( \ud835\udc62, \ud835\udc50 ) = { \ud835\udc56 1 , \ud835\udc56 2 , ..., \ud835\udc56 \ud835\udc41 } be the union set of \ud835\udc3e recommended basic item lists for user \ud835\udc62 in session environment context \ud835\udc50 (e.g., time and location), Table 1: Notations. \ud835\udc62 and \ud835\udc50 denote user and context, respectively. and \ud835\udc46 \ud835\udc58 \ud835\udc5b ( \ud835\udc62, \ud835\udc50 ) = \ud835\udc53 \ud835\udc58 ( \ud835\udc56 \ud835\udc5b , \ud835\udc62, \ud835\udc50 ) be the predicted score given by basic model \ud835\udc58 on item \ud835\udc5b . The goal of ranking ensemble learning is to learn a weighted ensemble score \ud835\udc46 \ud835\udc52\ud835\udc5b\ud835\udc60 \ud835\udc5b ( \ud835\udc62, \ud835\udc50 ) for each item \ud835\udc56 \ud835\udc5b in I ,  Where \ud835\udc64 \ud835\udc58 \ud835\udc5b ( \ud835\udc62, \ud835\udc50 ) \u2208 R denotes the weight of the \ud835\udc58 -th basic model for item \ud835\udc56 \ud835\udc5b . The weights are learnable with the help of side information, e.g., user intents and item categories. The items in I are sorted according to \ud835\udc46 \ud835\udc52\ud835\udc5b\ud835\udc60 \ud835\udc5b , and are compared with a ground truth order of ranking \ud835\udf0b \ud835\udc62,\ud835\udc50 = { \ud835\udf0b 1 , \ud835\udf0b 2 , ..., \ud835\udf0b \ud835\udc41 } , which is sorted to users' interactions with a pre-defined priority of user feedback, e.g., Buy>Click>Examine. The priority can be defined by business realities and will not influence the model learning strategy. The definition of ranking ensemble learning is similar to previous work [1, 24, 31], except that we conduct ensemble on heterogeneous basic models with different objectives, which makes the problem more difficult. The main notations are shown in Table 1.", "3.2 User Intent Definition": "When aggregating basic models optimized with different objectives, users' intent about behaviors and item categories are both essential. Therefore, we define a user's intent in a visit as a probability distribution of item categories and behaviors,  Where \ud835\udc3c and \ud835\udc35 indicate the item category intents and behavior intents, respectively. The types of categories and behaviors vary with recommendation scenarios. For instance, in online shopping, \ud835\udc3c can be product class, and \ud835\udc35 may include clicking and buying. In music recommender systems, \ud835\udc3c may be music genre, while \ud835\udc35 can contain listening and purchasing albums. In experiments, user intents \ud835\udc3c\ud835\udc5b\ud835\udc61 are predicted from users' historical interactions and environment context.", "3.3 Ranking Losses": "Three representative losses are generally leveraged in the recommendation scenario, namely point-wise, pair-wise, and list-wise loss. We will theoretically and empirically illustrate the effectiveness of ranking ensemble with three losses in the following sections. For a given user \ud835\udc62 under session context \ud835\udc50 with multi-level ground truth ranking \ud835\udf0b \ud835\udc62,\ud835\udc50 = { \ud835\udf0b 1 , \ud835\udf0b 2 , ...\ud835\udf0b \ud835\udc41 } on an item set I = SIGIR '23, July 23-27, 2023, Taipei, Taiwan. Jiayu Li, Peijie Sun, Zhefan Wang, Weizhi Ma, Yangkun Li, Min Zhang, Zhoutian Feng, and Daiyue Xue { \ud835\udc56 1 , \ud835\udc56 2 , ..., \ud835\udc56 \ud835\udc41 } , the loss of score list S ( \ud835\udc62, \ud835\udc50 ) = { \ud835\udc46 1 , \ud835\udc46 2 , ..., \ud835\udc46 \ud835\udc41 } is defined as ( \ud835\udc62 and \ud835\udc50 are omitted): \u00b7 Point-wise Loss As \ud835\udf0b is a multi-level feedback based on a group of user feedback, the Mean Squared Error (MSE) loss is utilized as a representative point-wise loss,  \u00b7 Pair-wise Loss Weleverage the Bayesian Personalized Ranking (BPR) loss [33]. Following the negative sampling strategy for multi-level recommendation [27], a random item from one level lower is paired with a positive item at each level,  Where \ud835\udc3f is the number of interaction levels (e.g., buy, click, and exposure), \ud835\udc41 + is the number of positive items of all levels, \ud835\udc3c + \ud835\udc59 and \ud835\udc3c -\ud835\udc59 are positive and one-level-lower negative item set for level \ud835\udc59 , and \ud835\udf0e is the sigmoid function. \u00b7 List-wise Loss Following [24], we adopt the Plackett-Luce (P-L) model as the likelihood function of ranking predictions,  Where \ud835\udf0b \ud835\udc5b indicates the \ud835\udc5b -th item sorted by ground truth \ud835\udf0b . The corresponding list-wise loss function is", "4 THEORETICAL EFFECTIVENESS OF RANKING ENSEMBLE LEARNING": "To prove the effectiveness of our proposed item-level ranking ensemble learning in Eq.1, we aim to prove that the loss of ensemble learning scores S \ud835\udc52\ud835\udc5b\ud835\udc60 = { \ud835\udc46 \ud835\udc52\ud835\udc5b\ud835\udc60 \ud835\udc5b } can be smaller than any of the loss of basic-model scores S \ud835\udc58 = { \ud835\udc46 \ud835\udc58 \ud835\udc5b } for point-wise, pair-wise, and listwise loss, i.e. \ud835\udc59 ( \ud835\udf0b, S \ud835\udc52\ud835\udc5b\ud835\udc60 ) \u2264 \u02dd \ud835\udc3e \ud835\udc58 = 1 w \ud835\udc58 \ud835\udc59 ( \ud835\udf0b, S \ud835\udc58 ) , \u2200 w \ud835\udc58 , \ud835\udc59 \u2208 { \ud835\udc59 \ud835\udc5a , \ud835\udc59 \ud835\udc4f , \ud835\udc59 \ud835\udc5d -\ud835\udc59 } . In this way, we can claim that there exist some combinations of weights w \ud835\udc58 to achieve results better than all basic models. Inspired by previous studies in ensemble learning, error ambiguity (EA) decomposition [22] provides an upper bound for ensemble loss \ud835\udc59 ( \ud835\udf0b, S \ud835\udc52\ud835\udc5b\ud835\udc60 ) , which helps conduct the above proof. For basic lists with loss { \ud835\udc59 ( \ud835\udf0b, S \ud835\udc58 )} , EA decomposition tries to split ensemble loss \ud835\udc59 ( \ud835\udf0b, S \ud835\udc52\ud835\udc5b\ud835\udc60 ) into a weighted sum of basic-model loss ( \u02dd \ud835\udc58 \ud835\udc64 \ud835\udc58 \ud835\udc59 ( \ud835\udf0b, S \ud835\udc58 ) , \u2200 \ud835\udc64 \ud835\udc58 minus a positive ambiguity term \ud835\udc34 1 of basic models, so that the upper bound of \ud835\udc59 ( \ud835\udf0b, S \ud835\udc52\ud835\udc5b\ud835\udc60 ) is controlled by both basic-model losses and ambiguity. It was recently proved in ranking tasks with the same weights for a basic list (i.e., \ud835\udc64 \ud835\udc58 \ud835\udc5b = \ud835\udc64 \ud835\udc58 \ud835\udc5a , \u2200 \ud835\udc5b = \ud835\udc5a ) [24]. However, different weights should be assigned for different items in our setting. Therefore, we need to verify whether EA decomposition is still available. To summarize, we try to prove that loss functions can be rewritten as \ud835\udc59 ( \ud835\udf0b, S \ud835\udc52\ud835\udc5b\ud835\udc60 ) \u2264 \u02dd \ud835\udc58 \u02dd \ud835\udc5b \ud835\udc64 \ud835\udc58 \ud835\udc5b \ud835\udc59 \ud835\udc58 \ud835\udc5b ( \ud835\udf0b, S \ud835\udc58 \ud835\udc5b ) -\ud835\udc34, \u2200 \ud835\udc64 \ud835\udc58 \ud835\udc5b for point-wise, pair-wise, list-wise loss in the following. 1 The ambiguity \ud835\udc34 is sometimes called the diversity in EA decomposition. We use ambiguity to denote it to avoid confusion with the term item diversity in recommendation. )", "4.1 Point-wise Loss": "Theorem 1 (Generalized EA Decomposition Theory for Point-wise Loss). Given a set of score lists { \ud835\udc46 \ud835\udc58 \ud835\udc5b | \ud835\udc58 \u2208 { 1 , 2 , ..., \ud835\udc3e } , \ud835\udc5b \u2208 { 1 , ..., \ud835\udc41 }} from \ud835\udc3e basic models on \ud835\udc41 items, and a weighted ensemble model \ud835\udc46 \ud835\udc52\ud835\udc5b\ud835\udc60 \ud835\udc5b = \u02dd \ud835\udc41 \ud835\udc5b = 1 \ud835\udc64 \ud835\udc58 \ud835\udc5b \ud835\udc46 \ud835\udc58 \ud835\udc5b with \ud835\udc64 \ud835\udc58 \ud835\udc5b \u2265 0 and \u02dd \ud835\udc3e \ud835\udc58 = 1 \ud835\udc64 \ud835\udc58 \ud835\udc5b = 1 , the MSE loss of the \ud835\udc5b -th ensemble score \ud835\udc46 \ud835\udc52\ud835\udc5b\ud835\udc60 \ud835\udc5b can be decomposed into two parts,  where \ud835\udc34 \ud835\udc58 \ud835\udc5b indicates the ambiguity term,  Proof. For each basic-model score \ud835\udc46 \ud835\udc58 \ud835\udc5b , we expand the MSE loss \ud835\udc59 \ud835\udc5a ( \ud835\udf0b \ud835\udc5b , \ud835\udc46 \ud835\udc58 \ud835\udc5b ) in Eq. 3 around point \ud835\udc46 \ud835\udc52\ud835\udc5b\ud835\udc60 \ud835\udc5b by Taylor expansion with Lagrange type reminder ( \ud835\udf0b \ud835\udc5b is removed when the meaning is clear),  Where \u02dc \ud835\udc46 \ud835\udc52\ud835\udc5b\ud835\udc60 \ud835\udc5b is an interpolation point between \ud835\udc46 \ud835\udc52\ud835\udc5b\ud835\udc60 \ud835\udc5b and \ud835\udc46 \ud835\udc58 \ud835\udc5b . Define \ud835\udc34 \ud835\udc58 \ud835\udc5b as Eq.8, we weighted sum losses of all basic models as follows,  The first equation is due to \u02dd \ud835\udc3e \ud835\udc58 = 1 \ud835\udc64 \ud835\udc58 \ud835\udc5b = 1 and \ud835\udf15 2 \ud835\udc59 \ud835\udc5a /( \ud835\udf15\ud835\udc46 ) 2 = 2, and the second equation is due to Eq. 1. Therefore,  Proof done. \u25a1 Since the ambiguity \ud835\udc34 \ud835\udc58 \ud835\udc5b in Eq. 8 is positive and \ud835\udc64 \ud835\udc58 \ud835\udc5b \u2265 0, Eq. 7 follows the form of EA decomposition. Ranking ensemble with itemlevel weights for point-wise loss is effective theoretically, since the ensemble loss is smaller than weighted sum of basic model losses with any \ud835\udc64 \ud835\udc58 \ud835\udc5b as long as \ud835\udc64 \ud835\udc58 \ud835\udc5b \u2265 0 and \u02dd \ud835\udc3e \ud835\udc58 = 1 \ud835\udc64 \ud835\udc58 \ud835\udc5b = 1. For brevity, we will omit statements of score lists and ensemble formulas in the following theorems.", "4.2 Pair-wise Loss": "Theorem 2 (Generalized EA Decomposition Theory for Pair-wise Loss). When \ud835\udc64 \ud835\udc58 \ud835\udc5b \u2265 0 , \u02dd \ud835\udc3e \ud835\udc58 = 1 \ud835\udc64 \ud835\udc58 \ud835\udc5b = 1 , and | \ud835\udc64 \ud835\udc58 \ud835\udc5a -\ud835\udc64 \ud835\udc58 \ud835\udc5b | \u2264 \ud835\udeff, \u2200 \ud835\udc5a,\ud835\udc5b , the BPR loss of a pair of ensemble scores \ud835\udc46 \ud835\udc52\ud835\udc5b\ud835\udc60 \ud835\udc5b and \ud835\udc46 \ud835\udc52\ud835\udc5b\ud835\udc60 \ud835\udc5a can be decomposed into  Intent-aware Ranking Ensemble for Personalized Recommendation SIGIR '23, July 23-27, 2023, Taipei, Taiwan. Where \ud835\udc34 \ud835\udc58 \ud835\udc5b\ud835\udc5a is the ambiguity of scores generated from basic models,  \ud835\udc67 \u2217 \ud835\udc5b\ud835\udc5a = \ud835\udc46 \u2217 \ud835\udc5b -\ud835\udc46 \u2217 \ud835\udc5a denotes the differences between scores. Due to space limitation, we only show key steps in the proof: Proof. Let \ud835\udc67 \ud835\udc58 \ud835\udc5b\ud835\udc5a = \ud835\udc46 \ud835\udc58 \ud835\udc5b -\ud835\udc46 \ud835\udc58 \ud835\udc5a and \ud835\udc59 \ud835\udc4f ( \ud835\udc67 \u2217 \ud835\udc5b\ud835\udc5a ) = \ud835\udc59 \ud835\udc4f ( \ud835\udc46 \u2217 \ud835\udc5b , \ud835\udc46 \u2217 \ud835\udc5a ) in Eq.4, we expand \ud835\udc59 \ud835\udc4f ( \ud835\udc67 \ud835\udc58 \ud835\udc5b\ud835\udc5a ) around \ud835\udc67 \ud835\udc52\ud835\udc5b\ud835\udc60 \ud835\udc5b\ud835\udc5a by Taylor expansion,  Where \u02dc \ud835\udc67 \ud835\udc52\ud835\udc5b\ud835\udc60 \ud835\udc5b\ud835\udc5a is an interpolation point between \ud835\udc67 \ud835\udc52\ud835\udc5b\ud835\udc60 \ud835\udc5b\ud835\udc5a and \ud835\udc67 \ud835\udc58 \ud835\udc5b\ud835\udc5a . With the limitation that \u02dd \ud835\udc3e \ud835\udc58 = 1 \ud835\udc64 \ud835\udc58 \ud835\udc5b = 1 and | \ud835\udc64 \ud835\udc58 \ud835\udc5b -\ud835\udc64 \ud835\udc58 \ud835\udc5a | \u2264 \ud835\udeff , the weighted sum of \ud835\udc35 \ud835\udc58 \ud835\udc5b\ud835\udc5a is limited by  Sum both sides of Eq.14 with weights, we get  Proof done. \u25a1 The range limitation of \ud835\udc64 \ud835\udc58 \ud835\udc5b leads to \ud835\udeff \u2264 1. And in pair-wise loss, the order rather than the values of scores matters. So the second term in Eq. 12 ( \ud835\udeff \u02dd \ud835\udc3e \ud835\udc58 = 1 \ud835\udc46 \ud835\udc58 \ud835\udc5a < \u02dd \ud835\udc3e \ud835\udc58 = 1 \ud835\udc46 \ud835\udc58 \ud835\udc5a ) can be arbitrarily small. Meanwhile, the ambiguity \ud835\udc34 \ud835\udc58 \ud835\udc5b\ud835\udc5a is semi-positive. Therefore, Eq.12 follows the form of EA decomposition, and our ranking ensemble method with pair-wise loss is effective theoretically.", "4.3 List-wise Loss": "Theorem 3 (Generalized EA Decomposition Theory for List-wise Loss). When \ud835\udc64 \ud835\udc58 \ud835\udc5b \u2265 0 , \u02dd \ud835\udc3e \ud835\udc58 = 1 \ud835\udc64 \ud835\udc58 \ud835\udc5b = 1 , and | \ud835\udc64 \ud835\udc58 \ud835\udc5b -\ud835\udc64 \ud835\udc58 \ud835\udc5a | \u2264 \ud835\udeff for any \ud835\udc5a and \ud835\udc5b , the list-wise loss of ensemble scores S \ud835\udc52\ud835\udc5b\ud835\udc60 = { \ud835\udc46 \ud835\udc52\ud835\udc5b\ud835\udc60 1 , \ud835\udc46 \ud835\udc52\ud835\udc5b\ud835\udc60 2 , ..., \ud835\udc46 \ud835\udc52\ud835\udc5b\ud835\udc60 \ud835\udc5b } (sorted with \ud835\udf0b ) can be decomposed as  Where \ud835\udc64 \ud835\udc58 max denotes the maximum of all weights in list \ud835\udc58 , \ud835\udc34 \ud835\udc58 \ud835\udc5b is the ambiguity at position \ud835\udc5b ,  \ud835\udc46 max sum is defined as  \ud835\udc67 \u2217 \ud835\udc5b\ud835\udc5a = \ud835\udc46 \u2217 \ud835\udc5b -\ud835\udc46 \u2217 \ud835\udc5a denotes the differences between scores. Due to space limitation, we only show key steps in the proof: Proof. We define the score difference \ud835\udc67 \ud835\udc5b : \ud835\udc41 = [ \ud835\udc67 \ud835\udc5b + 1 , ..., \ud835\udc67 \ud835\udc41 ] = [ \ud835\udc46 \ud835\udc5b -\ud835\udc46 \ud835\udc5b + 1 , ..., \ud835\udc46 \ud835\udc5b -\ud835\udc46 \ud835\udc41 ] and the logarithm pseudo-sigmoid function,  For each basic model of a list of items S \ud835\udc58 = { \ud835\udc46 \ud835\udc58 \ud835\udc5b | \ud835\udc5b \u2208 { 1 , 2 , ..., \ud835\udc41 }} , the PL loss is \ud835\udc59 \ud835\udc5d -\ud835\udc59 ( S \ud835\udc58 ) = \u02dd \ud835\udc41 \ud835\udc5b = 1 \ud835\udc54 \ud835\udc5b ( \ud835\udc67 \ud835\udc58 \ud835\udc5b : \ud835\udc41 ) . We expand \ud835\udc54 \ud835\udc5b ( \ud835\udc67 \ud835\udc58 \ud835\udc5b : \ud835\udc41 ) around point \ud835\udc67 \ud835\udc52\ud835\udc5b\ud835\udc60 \ud835\udc5b : \ud835\udc41 by Taylor expansion with Lagrange type reminder,  With the limitation that \u02dd \ud835\udc3e \ud835\udc58 = 1 \ud835\udc64 \ud835\udc58 \ud835\udc5b = 1, \ud835\udc64 \ud835\udc58 \ud835\udc5b \u2265 0, and | \ud835\udc64 \ud835\udc58 \ud835\udc5b -\ud835\udc64 \ud835\udc58 \ud835\udc5a | < \ud835\udeff, \u2200 \ud835\udc5b,\ud835\udc5a , the weighted sum of \ud835\udc35 \ud835\udc58 \ud835\udc5b on \ud835\udc3e basic models will be  Therefore, sum from \ud835\udc5b = 1 to \ud835\udc5b = \ud835\udc41 , we get   Because in the list-wise optimization, the order rather than the values of scores matters, \ud835\udc46 \ud835\udc52\ud835\udc5b\ud835\udc60 \ud835\udc5b can be arbitrarily small. Meanwhile, the ambiguity term \ud835\udc34 \ud835\udc58 \ud835\udc5b is semi-positive. Therefore, Eq.17 conforms to the EA decomposition theory, and our ranking ensemble method with list-wise loss is effective.", "4.4 Ensemble Loss for Model Training": "The above theorems guarantee our proposed ranking ensemble learning method in theory for three representative loss functions. With EA decomposition theory, we prove that the loss of ensemble list is smaller than any weighted sum combination of losses of basic lists: \ud835\udc59 \ud835\udc52\ud835\udc5b\ud835\udc60 ( \ud835\udf0b, \ud835\udc46 \ud835\udc52\ud835\udc5b\ud835\udc60 ) \u2264 \u02dd \ud835\udc58 \ud835\udc64 \ud835\udc58 \ud835\udc59 \ud835\udc58 ( \ud835\udf0b, \ud835\udc46 \ud835\udc58 ) -\ud835\udc34 + \u0394 , \u2200 \ud835\udc64 \ud835\udc58 \u2264 0 , \u02dd \ud835\udc3e \ud835\udc58 = 1 \ud835\udc64 \ud835\udc58 = 1, where \ud835\udc34 is a positive ambiguity term, and \u0394 is arbitrarily small. Therefore, the ensemble loss \ud835\udc59 \ud835\udc52\ud835\udc5b\ud835\udc60 ( \ud835\udf0b, \ud835\udc46 \ud835\udc52\ud835\udc5b\ud835\udc60 ) (i.e., differences between ensemble list and ground truth) is possible to be smaller than any basic list loss with suitable weights { \ud835\udc64 \ud835\udc58 \ud835\udc5b } , and larger ambiguity \ud835\udc34 will lead to a smaller bound of ensemble loss. Thus, it can be effective for our ranking ensemble task. In practice, since basic lists are fixed (so \ud835\udc59 \ud835\udc58 ( \ud835\udf0b, \ud835\udc46 \ud835\udc58 ) are constants), we aim to minimize the ensemble ranking loss \ud835\udc59 \ud835\udc52\ud835\udc5b\ud835\udc60 ( \ud835\udf0b, \ud835\udc46 \ud835\udc52\ud835\udc5b\ud835\udc60 ) and maximize the ambiguity \ud835\udc34 . Therefore, the loss function for ranking ensemble learning, \ud835\udc59 \ud835\udc52\ud835\udc59 , is defined as follows,  where \ud835\udc59 \ud835\udc52\ud835\udc5b\ud835\udc60 ( \ud835\udf0b, S \ud835\udc52\ud835\udc5b\ud835\udc60 ) can be any of the \ud835\udc59 \ud835\udc5a ( \ud835\udf0b, S \ud835\udc52\ud835\udc5b\ud835\udc60 ) , \ud835\udc59 \ud835\udc4f ( \ud835\udf0b, S \ud835\udc52\ud835\udc5b\ud835\udc60 ) , and \ud835\udc59 \ud835\udc5d -\ud835\udc59 ( \ud835\udf0b, S \ud835\udc52\ud835\udc5b\ud835\udc60 ) , and \ud835\udc34 indicates the ambiguity term. For BPR and P-L loss, there exists an interpolation \u02dc \ud835\udc46 \ud835\udc52\ud835\udc5b\ud835\udc60 \ud835\udc5b = \ud835\udc46 \ud835\udc52\ud835\udc5b\ud835\udc60 \ud835\udc5b + \ud835\udf03 ( \ud835\udc46 \ud835\udc58 \ud835\udc5b -\ud835\udc46 \ud835\udc52\ud835\udc5b\ud835\udc60 \ud835\udc5b ) in \ud835\udc34 . To simplify the calculation, we let \ud835\udf03 \u2192 0 without loss of generality. SIGIR '23, July 23-27, 2023, Taipei, Taiwan. Jiayu Li, Peijie Sun, Zhefan Wang, Weizhi Ma, Yangkun Li, Min Zhang, Zhoutian Feng, and Daiyue Xue Figure 2: Overall framework of the Intent-aware Ranking Ensemble Learning (IntEL) model, where \ud835\udc59 \ud835\udc52\ud835\udc5b\ud835\udc60 and \ud835\udc59 \ud835\udc5f\ud835\udc52\ud835\udc50 are generated by Eq.24 and Eq.29, respectively. i 1 i 2 \u2026 i N Item Candidates S 1 1 S 1 2 \u2026 S 1 N S 2 1 S 2 2 \u2026 S 2 N S K 1 S K 2 \u2026 S K N \u2026 User Intent Item Category I 1 I 2 \u2026 I N Intent-aware Ranking Ensemble Module w 1 1 w 1 2 \u2026 w 1 N \u2026 w 2 1 w 2 2 \u2026 w 2 N w K 1 w K 2 \u2026 w K N f 1 Basic-list Scores f 2 f K Weighted Sum Ambiguity A Ranking Loss l ens (\u03c0,S ens ) S ens 1 S ens 2 \u2026 S ens N Item 1 Item 2 \u2026 Item N Intent Predictor H T-t \u2026 H T-2 H T-1 User History At Time T Environment Context Intent Loss l int Ensemble Learning loss l el Final Loss l rec (e.g., time, location\u2026)", "5 INTENT-AWARE RANKING ENSEMBLE METHOD": "", "5.1 Overall Framework": "After we proved the effectiveness of item-level weights { \ud835\udc64 \ud835\udc58 \ud835\udc5b } for ranking ensemble with three different loss functions, we need to design a neural network for learning the weights \ud835\udc64 \ud835\udc58 \ud835\udc5b . As shown in Section 3.2, users' intents about behaviors and item categories help aggregate the basic lists, while intents are not available in advance. Therefore, an intent predictor and an intent-aware ranking ensemble module are designed for our method. The main framework of our Intent-aware Ensemble Learning (IntEL) method is shown in Figure 2. For a user \ud835\udc62 at time \ud835\udc47 , user intents \ud835\udc3c\ud835\udc5b\ud835\udc61 are predicted with an intent predictor from her historical interactions and current environment context. Then, with \ud835\udc41 candidate items generated from \ud835\udc3e basic models, an intent-aware ranking ensemble module is adopted to integrate basic list scores, item categories, and the predicted user intents. The output of the ensemble module is item-level weights { \ud835\udc64 \ud835\udc58 \ud835\udc5b } for each item \ud835\udc5b and basic model \ud835\udc58 . Eventually, weighted sum of all basic list scores constructs the ensemble scores { \ud835\udc46 \ud835\udc52\ud835\udc5b\ud835\udc60 \ud835\udc58 } for a final list. Since we focus on the ranking ensemble learning problem, a straight-forward sequential model is used for intent prediction in Section 5.2, and we pay more attention to the design of ensemble module in Section 5.3. IntEL is optimized with a combination of ranking loss \ud835\udc59 \ud835\udc52\ud835\udc5b\ud835\udc60 ( \ud835\udf0b, \ud835\udc46 ) , ambiguity loss \ud835\udc34 , and intent prediction loss \ud835\udc59 \ud835\udc56\ud835\udc5b\ud835\udc61 . Details about the model learning strategy will be discussed in Section 5.4.", "5.2 User Intent Predictor": "As defined in Section 3.2, user intent describes a multi-dimensional probability distribution \ud835\udc3c\ud835\udc5b\ud835\udc61 over different item categories and behaviors at each user visit. The goal of the intent predictor is to generate an intent probability distribution for each user visit. We predict Figure 3: Structure of the intent-aware ranking ensemble module. S 1 1 S 1 2 \u2026 S 1 N S 2 1 S 2 2 \u2026 S 2 N S K 1 S K 2 \u2026 S K N \u2026 User Intent Item Category Basic-list Scores Self-Attention Self-Attention Embedding Intent-aware Cross Attention Intent-aware Cross Attention Projection w 1 1 w 1 2 \u2026 w 1 N \u2026 w 2 1 w 2 2 \u2026 w 2 N w K 1 w K 2 \u2026 w K N I 1 I 2 \u2026 I N Item 1 Item 2 \u2026 Item N (a) Ranking Ensemble Module X Multihead Attention Feed Forward Dropout Add & Norm Att Q K V X (t+1) \u00d7 T Embedding X (t) (b) Self-Attention Intent Embedding Self-Att Embedding Feed Forward W Q K V Product Attention Score Weighted Sum Intent-aware Embedding (c) Cross-Attention intents with users' historical interactions and environment context, as both historical habits and current situations will influence users' intents. For a user \ud835\udc62 at time \ud835\udc47 , her historical interactions from \ud835\udc47 -\ud835\udc61 to \ud835\udc47 -1 and environment context (such as timestamp and location) at \ud835\udc47 are adopted to predict her intent at \ud835\udc47 , where \ud835\udc61 is a pre-defined time window. Enviroment context is encoded into embedding \ud835\udc50 ( \ud835\udc62,\ud835\udc47 ) with a linear context encoder. Two sequential encoders are utilized to model historical interactions at user visit (i.e., session) level and item level. Session-level history helps learn users' habits about past intents, while item-level interactions express preferences about item categories in detail. At the session level, the intents and context of each historical session are embedded with two linear encoders, respectively. Then two embeddings are concatenated and encoded with a sequential encoder to form an embedding \u210e \ud835\udc60 ( \ud835\udc62,\ud835\udc47 ) . At the item level, 'intent' of each positive historical interaction can also be represented by its behavior type and item category. Then item-level 'intent's are embedded with the same intent encoder as sessionlevel, and fed into a sequential encoder to form item-level history \u210e \ud835\udc56 ( \ud835\udc62,\ud835\udc47 ) . The sequential encoder can be any sequential model, such as GRU [12], transformer [37], etc. Finally, context \ud835\udc50 ( \ud835\udc62,\ud835\udc47 ) , sessionlevel \u210e \ud835\udc60 ( \ud835\udc62,\ud835\udc47 ) , and item-level \u210e \ud835\udc56 ( \ud835\udc62,\ud835\udc47 ) are concated for a linear layer to predict intent \u02c6 \ud835\udc3c\ud835\udc5b\ud835\udc61 ( \ud835\udc62,\ud835\udc47 ) ( \ud835\udc62 and \ud835\udc47 are omitted),  Where W \ud835\udc3c and \ud835\udc4f \ud835\udc3c are linear mapping parameters.", "5.3 Design of Ensemble Module": "The structure of the intent-aware ranking ensemble module is shown in Figure 3(a). Since the final weights { \ud835\udc64 \ud835\udc58 \ud835\udc5b } should be learned from both behavior and item categories aware of user intents, the predicted intents, single-behavior-objective basic list scores, and categories of items in basic lists are adopted as inputs. Firstly, lists of item scores { \ud835\udc46 \ud835\udc58 \ud835\udc5b | \ud835\udc58 \u2208 { 1 , 2 , ..., \ud835\udc3e } , \ud835\udc5b \u2208 { 1 , 2 , ..., \ud835\udc41 }} are fed into a self-attention layer to represent the relationship among item scores in the same basic list. Item categories { \ud835\udc3c \ud835\udc5b | \ud835\udc5b \u2208 Intent-aware Ranking Ensemble for Personalized Recommendation SIGIR '23, July 23-27, 2023, Taipei, Taiwan. { 1 , 2 , ..., \ud835\udc41 }} are also encoded with a self-attention layer to capture the intra-list category distributions. The self-attention structure consists of a linear layer to embed scores { \ud835\udc46 \ud835\udc58 \ud835\udc5b } (or categories { \ud835\udc3c \ud835\udc5b } ) into \ud835\udc51 \ud835\udc52 -dimensional representations S \u2208 R \ud835\udc41 \u00d7 \ud835\udc51 \ud835\udc52 (or I \u2208 R \ud835\udc41 \u00d7 \ud835\udc51 \ud835\udc52 ) and \ud835\udc47 layers of multi-head attentions, which follow the cross-relation attention layer proposed by Wang et al. [40], as shown in Figure 3(b). Secondly, user intent \ud835\udc3c\ud835\udc5b\ud835\udc61 is embedded into \ud835\udc51 \ud835\udc56\ud835\udc5b\ud835\udc61 dimension with a linear projection \ud835\udc3c\ud835\udc5b\ud835\udc61 \ud835\udc51 = W \ud835\udc56 \ud835\udc3c\ud835\udc5b\ud835\udc61 \u2208 R \ud835\udc41 \u00d7 \ud835\udc51 \ud835\udc56\ud835\udc5b\ud835\udc61 . Then the influences of user intent on representations of scores and features are obtained with cross-attention layers,   Where the projection matrix W \ud835\udc44 \u2208 R \ud835\udc51 \ud835\udc52 \u00d7 \ud835\udc51 \ud835\udc56\ud835\udc5b\ud835\udc61 is shared between two intent-aware attention modules. Since behavior intents and category intents are associated when users interact with recommenders, we use the holistic user intents to guide the aggregations of both basic list scores and item categories rather than splitting the intents into two parts. Finally, weights { \ud835\udc64 \ud835\udc58 \ud835\udc5b } should be generated from all information. Intent-aware score embeddings A \ud835\udc60 , intent-aware item category embeddings A \ud835\udc56 , and intent embedding \ud835\udc3c\ud835\udc5b\ud835\udc61 \ud835\udc51 are concatenated and projected into space of R \ud835\udc3e to get the weight matrix W \u2208 R \ud835\udc41 \u00d7 \ud835\udc3e ,  Where W \ud835\udc64 \u2208 R \ud835\udc3e \u00d7( 2 \ud835\udc51 \ud835\udc52 + \ud835\udc51 \ud835\udc56\ud835\udc5b\ud835\udc61 ) is a trainable projection matrix. The output matrix W = { \ud835\udc64 \ud835\udc58 \ud835\udc5b } is used as the weights for summing basic model scores as in Eq. 1.", "5.4 Model Learning Strategy": "Since an end-to-end framework is to train the intent predictor module and intent-aware ranking ensemble module, joint learning of two modules is utilized for model optimization. To optimize ranking ensemble results according to theorems via EA decomposition in Section 4, ensemble learning loss \ud835\udc59 \ud835\udc52\ud835\udc59 consists of \ud835\udc59 \ud835\udc52\ud835\udc5b\ud835\udc60 ( \ud835\udf0b, S \ud835\udc52\ud835\udc5b\ud835\udc60 ) and \ud835\udc34 as in Eq.24. Meanwhile, accurate user intents will guide the ranking ensemble, so an intent prediction loss is also used for model training. Since user intents are described by multidimensional distributions, KL-divergence [13] loss \ud835\udc59 \ud835\udc56\ud835\udc5b\ud835\udc61 is adopted to measure the distance between true intents \ud835\udc3c\ud835\udc5b\ud835\udc61 and predicted intents \u02c6 \ud835\udc3c\ud835\udc5b\ud835\udc61 . The final recommendation loss \ud835\udc59 \ud835\udc5f\ud835\udc52\ud835\udc50 is a weighted sum  Where \ud835\udc59 \ud835\udc52\ud835\udc5b\ud835\udc60 ( \ud835\udf0b, S \ud835\udc52\ud835\udc5b\ud835\udc60 ) is the ranking ensemble loss, \ud835\udc34 is the ambiguity term, and \ud835\udc59 \ud835\udc56\ud835\udc5b\ud835\udc61 is the intent prediction loss. \ud835\udefc and \ud835\udefe are hyperparameters to adjust the weights of ambiguity and intent loss, respectively.", "6 EXPERIMENTS": "", "6.1 Experimental Setup": "6.1.1 Dataset. Experiments are conducted on a public online shopping recommendation dataset Tmall 2 and a private local life service recommendation dataset LifeData . 2 https://tianchi.aliyun.com/dataset/dataDetail?dataId=42 Table 2: Datasets statistics in ranking ensemble experiments. Tmall is a multi-behavior dataset from the IJCAI-15 competition, which contains half-year user logs with Click, Add-to-favorite (Fav.), and Buy interactions on Tmall online shopping platform. We employ the data in September for ensemble learning and exclude items and users with less than 3 positive interactions. Following the data processing strategy by Shen et al. [36], we treat a user's interactions within a day as a session. Three-week interactions before the ensemble dataset are used for the generation of basic-model scores, which will be discussed in Section 6.1.2. LifeData comes from a local life service App, where the recommender provides users with nearby businesses such as restaurants or hotels. Users may click or buy items on the platform. One-month interactions of a group of anonymous users are sampled, and users and items with less than 3 positive interactions are filtered. A user's each visit (i.e., entering the App) is defined as a session, and sessions with positive interactions are retained. Basic models are optimized for each of the behaviors, which will be introduced in Section 6.1.2. Ranking ensemble is conducted at session level, and interactions in most \ud835\udc61 = 20 historical sessions are considered in the intent predictor. Detailed statistics are shown in Table 2, which includes the dataset for ensemble learning only while excluding the data used for basic model generation. Moreover, training data for basic models have no overlap with ensemble learning data. 6.1.2 Basic-model Score and Intent Generation. In IntEL, basic scores are pre-generated and fixed during ranking ensemble. For Tmall , we adopted DeepFM [20] as basic models to train three models with Click, Fav., and Buy objectives separately. In each session, we select the top 30 items predicted by each basic model to construct three item sets, and take the union of them, plus positive interactions of the session, to form the basic item lists for reranking. Please refer to our public repository for details about basic model training strategy 3 . For LifeData , two basic score lists are used for ranking ensemble, which are sorted by predicted clicking probability and buying probability provided by the platform, respectively. As for intents, in Tmall , | \ud835\udc35 | = 3, and we merge categories with less than 50 items, resulting in category | \ud835\udc3c | = 357. In LifeData , | \ud835\udc3c | = 6 and | \ud835\udc35 | = 2. Hence, the dimension for intent \ud835\udc3c\ud835\udc5b\ud835\udc61 is 1071 for Tmall and 12 for LifeData . Intent ground truth \ud835\udc3c\ud835\udc5b\ud835\udc61 probability is calculated from all positive interactions in each session. 6.1.3 Baseline Methods. We compare IntEL against basic models and several ranking ensemble baselines as follows, 1. Single \ud835\udc4b\ud835\udc4b\ud835\udc4b : Use one of the basic models' scores to rank the item list. \ud835\udc4b\ud835\udc4b\ud835\udc4b indicates Click, Fav., and Buy, respectively. 2. RRA [21]: An unsupervised ensemble method, where items are sorted with their significance in basic-model lists. 3. Borda [5]: An unsupervised ensemble method to take the average ranks of all basic models as the final ranking. SIGIR '23, July 23-27, 2023, Taipei, Taiwan. Jiayu Li, Peijie Sun, Zhefan Wang, Weizhi Ma, Yangkun Li, Min Zhang, Zhoutian Feng, and Daiyue Xue Table 3: Main differences between two datasets. Pos. indicates positive interactions. 4. \ud835\udf06 Rank [7]: A gradient-based optimization method used for learning2rank task. We regard items as documents, basic-model scores and item categories as document features, and MLP as a backbone model. 5. ERA [30]: An evolutionary method to aggregate some basiclist features with Genetic Algorithm (GA), where fitness function is calculated on validation set. 6. aWELv [24]: A personalized ranking ensemble method to assign weights at basic model level, i.e., \ud835\udc64 \ud835\udc58 \ud835\udc5b = \ud835\udc64 \ud835\udc58 \ud835\udc5a for any \ud835\udc5b,\ud835\udc5a . We adopt the list-wise training loss following [24]. 7. aWELv+Int/IntEL : Two variations of aWELv considering user intents. Intents are predicted as a feature for aWELv+Int. The IntEL module is used for predicting list-level weights for aWELv+IntEL. Our methods are shown as IntEL-MSE , IntEL-BPR , and IntELPL with three different kinds of loss functions. 6.1.4 Experimental settings. We split both datasets along time: the last week is the test set, and the last three days from the training set is the validation set. The priority for the mutli-level ground truth \ud835\udf0b are Buy>Favorite>Click>Examine for Tmall , and Buy>Click>Examine for LifeData . As for evaluation, we adopt NDCG@3, 5, and 10 to evaluate the ensemble list \ud835\udc46 \ud835\udc52\ud835\udc5b\ud835\udc60 on the multilevel ground truth \ud835\udf0b (i.e., all) and each behavior objective. We implement IntEL model in PyTorch , and the code of IntEL and all baselines are released 3 . Each experiment is repeated with 5 different random seeds and average results are reported. All models are trained with Adam until convergence with a maximum of 100 epochs. For a fair comparison, the batch size is set to 512 for all models. We tuned the parameters of all methods over the validation set, where the learning rate are tuned in the range of [ 1 \ud835\udc52 -4 , 1 \ud835\udc52 -2 ] and all embedding size are tuned in { 16 , 32 , 64 } . Specifically, for IntEL, we found that it has stable performance when GRU [12] with embedding=128 is used for the intent predictor, and self-attention with \ud835\udc47 = 2 for Tmall and \ud835\udc47 = 1 for LifeData. The ambiguity loss weight \ud835\udefc is set to 1e-5, 1e-5, and 1e-4 for IntEL-MSE, IntEL-BPR, and IntEL-PL. Hyper-parameter details are released 3 .", "6.2 Overall Performance": "The overall performances on Tmall and LifeData are shown in Table 4 and Table 5, respectively. We divide all models into four parts: The first part evaluates on each single-objective basic model's scores. The second is unpersonalized baseline ensemble models, and the third contains personalized baselines: aWELv and its two variants with user intents. The last part shows our method IntEL with three loss functions. From the results, we have several observations: First, our proposed IntEL achieves the best performance on all behavior objectives in both datasets. IntEL with three loss functions, i.e., IntEL-MSE, IntEL-BPR, and IntEL-PL, outperform the best baselines on most metrics significantly. Although the two datasets are 3 https://github.com/JiayuLi-997/IntEL-SIGIR2023. Figure 4: Ablation study. Performance comparison between IntEL and its variant, i.e., without: Intent modeling (-Int), item categories (-I), basic score lists (-S), cross-attention (Cross), and self-attention (-Self). -Int -I -S -Cross -Self IntEL (b) Prediction on Tmall 0.30 0.32 0.34 0.36 0.38 0.40 0.42 All-NDCG@3 ERA 0.3913 0.3833 0.3099 0.3958 0.3463 0.4257 -Int -I -S -Cross -Self IntEL (b) Prediction on LifeData 0.40 0.41 0.42 0.43 0.44 aWELv+IntEL 0.4123 0.4253 0.4333 0.4270 0.4318 0.4378 quite different, as shown in Table 3, IntEL has stable, great ensemble results on both datasets. Second, IntEL with different loss functions show different performances on two datasets. On Tmall, IntEL-MSE is better than IntEL-BPR and IntEL-PL. It is because there is four-level ground truth \ud835\udf0b (three behaviors), and ranking on such diverse item lists is close to rating prediction. Therefore, IntEL-MSE, which directly optimizes the ensemble scores, performs better than IntEL-BPR and IntEL-PL, which optimize the comparison between rankings. On LifeData, IntEL-PL and IntEL-BPR perform better since LifeData has shorter sessions with fewer positive interactions (as in Table 3). So comparison-based BPR and P-L achieve better performance. Third, comparing different baselines, we find that supervised methods ( \ud835\udf06 Rank, ERA, and aWELv) outperform unsupervised RRA and Borda greatly on Tmall. It is because heterogeneous singlebehavior objective models (Single XXX) have diverse performance, making rank aggregation difficult for unsupervised methods. Lastly, aWELv and its variants perform well on LifeData but not on Tmall since session lists are generally longer (Table 3) for Tmall, and list-level weights of aWELv miss useful intra-list information. So item-level weights that consider item category intents are necessary. Nevertheless, aWELv is better than basic models in both datasets, which is consistent with the theory. Moreover, aWELv+Int/IntEL outperform aWELv on most metrics, indicating that user intents contributes to ranking ensemble learning.", "6.3 Further Analysis": "To further explore the performance of our ranking ensemble learning method, we conduct an ablation study, analysis of user intents, and hyper-parameters analysis on the best model for each dataset, i.e., IntEL-MSE for Tmall and IntEL-PL for LifeData . 6.3.1 Ablation Study. The main contributions of our proposed IntEL include adopting user intents for heterogeneous ranking ensemble and integration of basic-list scores, item categories, and user intents. We compare IntEL with five variants: Excluding one of the inputs: -Int (without intent), -I (without item categories), and -S (without basic-list scores). Replacing two main elements: -Cross , removing the intent-aware cross-attention layer; and -Self , replacing the self-attention layer with a direct connection. NDCG@3 on the general multi-level ranking list of variants and IntEL are shown in Figure 4. Ranking performance drops on all five variants, indicating all inputs and two attention layers contribute to the performance improvement of IntEL. Removing scores Intent-aware Ranking Ensemble for Personalized Recommendation SIGIR '23, July 23-27, 2023, Taipei, Taiwan. Table 4: Results of IntEL with three different loss functions and baseline methods on Tmall. Boldface shows the best result. Underline indicates the best baseline. Notation **/* demonstrates significantly better than the best baseline with p<0.05/0.01. Table 5: Results of IntEL with three different loss functions and baseline methods on LifeData. Boldface shows the best result. Underline indicates the best baseline. Notation **/* demonstrates significantly better than the best baseline with p<0.05/0.01. Table 6: Intent prediction and ranking ensemble performance comparison with different treatments on user intents. accuracy. Therefore, we compare IntEL with two variants: -Int , IntEL without user intents as input, and His.Avg. , predicting a user's current intents as her average historical session intents. and the self-attention layer both lead to considerable performance decreases on Tmall, showing that intra-list basic scores information is eseential, as long sessions are included in Tmall. Removing user intents leads to the most dramatic degradation on LifeData, which suggests it is important to adopt user intents for the multipleobjective ranking ensemble. Nevertheless, the ablation variants still outperform all basic lists (i.e., Single XXX), which aligns with our proof of loss reduction via EA ambiguity decomposition. 6.3.2 Influence of User Intent. Since intents are essential for ranking ensemble learning, we explore the influence of intent prediction Since Tmall has 1071 intents and LifeData has 12 intents, we utilize NDCG@10 and Macro-F1 as intent performance (I-perform) indicators, respectively. And ensemble results (E-NDCG@3) are evaluated by All-NDCG@3 for both datasets. The results are shown in Table 6. It indicates that better performance of ranking ensemble is achieved by adding intent prediction and improving prediction accuracy. Therefore, predicting user intents is helpful for ranking ensemble in recommendation. On the other, \ud835\udc3b\ud835\udc56\ud835\udc60.\ud835\udc34\ud835\udc63\ud835\udc54. works better than all baselines, providing an efficient and effective possible implementation in application. 6.3.3 Hyper-parameters Analysis. Since the construction of loss \ud835\udc59 \ud835\udc5f\ud835\udc52\ud835\udc50 (Eq.29) is essential for our method, we analyze the influence of hyper-parameters during model optimization. Two hyper-parameters are considered in the optimization loss \ud835\udc59 \ud835\udc5f\ud835\udc52\ud835\udc50 : \ud835\udefc , the weight for basic list ambiguity \ud835\udc34 ; \ud835\udefe , the weight for intent prediction loss \ud835\udc59 \ud835\udc56\ud835\udc5b\ud835\udc61 . All-NDCG@3 with different hyper-parameters on two datasets are SIGIR '23, July 23-27, 2023, Taipei, Taiwan. Jiayu Li, Peijie Sun, Zhefan Wang, Weizhi Ma, Yangkun Li, Min Zhang, Zhoutian Feng, and Daiyue Xue 0.430 (b) LifeData dataset 0 1e-07 1e-06 1e-05 0.0001 0.001 0.01 Diversity weight 0.410 0.415 0.420 0.425 All-NDCG@3 1e-4 5e-4 0.001 0.003 0.005 0.01 0.05 Intent loss weight (a) Tmall dataset 0 1e-07 1e-06 1e-05 0.0001 0.001 0.01 Diversity weight 0.43 0.432 0.434 0.436 0.438 0.44 All-NDCG@3 0.1 0.2 0.5 1 2 5 10 Intent loss weight Figure 5: Ranking ensemble results of IntEL with different hyper-parameters. shown in Figure 5. It illustrates that too large or small \ud835\udefc will both lead to ranking ensemble performance decrease. Especially when \ud835\udefc is too large, the model will focus on maximizing basic model ambiguity \ud835\udc34 to minimize \ud835\udc59 \ud835\udc5f\ud835\udc52\ud835\udc50 , while ensemble learning loss \ud835\udc59 \ud835\udc52\ud835\udc59 is less optimized. As for the intent loss weight \ud835\udefe , performance on Tmall shows fluctuation with \ud835\udefe , while performance on LifeData is relatively stable. It is because intent prediction difficulty differs on two datasets: Tmall contains 1071 types of intents, which is hard to predict accurately, so a proper intent loss weight is essential for predictor optimization, while LifeData has only 12 intents, which is easier to capture and model.", "7 CONCLUSION": "In this paper, we propose a novel ranking ensemble method IntEL for intent-aware single-objective ranking lists aggregation. To our knowledge, we are the first to generalize ranking ensemble learning with item-level weights on heterogeneous item lists. And we are also the first to integrate user intents into rank aggregation in recommendation. We generalize the ranking ensemble with itemlevel weights and prove its effectiveness with three representative loss functions via error-ambiguity decomposition theory. Based on the proof, we design an ensemble learning loss \ud835\udc59 \ud835\udc52\ud835\udc59 to minimize ranking ensemble loss \ud835\udc59 \ud835\udc52\ud835\udc5b\ud835\udc60 and maximize ambiguity \ud835\udc34 . Then we design an intent-aware ranking ensemble learning model, IntEL, to learn weights for heterogeneous lists' ensemble. In IntEL, a sequential intent predictor and a two-layer attention intent-aware ensemble module are adopted for learning the personalized and adaptive ensemble weights with user intents. Experiments on two large-scale datasets show that IntEL gains significant improvements on multiple optimization objectives simultaneously. This study still has some limitations. For basic list generation, we only applied one classical method, DeepFM, for different behaviors separately. However, multi-behavior methods are also possible models to generate multiple basic lists simultaneously, which may lead to different performance for IntEL. Also, a straight-forward method was adopted to predict intents and incorporate intent prediction loss. In the future, we will investigate the possibility of integrating more heterogeneous basic lists for other objectives in recommendation with IntEL. As we find that more accurate user intents will lead to better ranking ensemble performance, we will also try to design more sophisticated intent predictors to achieve better results.", "ACKNOWLEDGMENTS": "We sincerely thank our anonymous reviewers for their insightful feedback. This work is supported by the Natural Science Foundation of China (Grant No. U21B2026, 62002191) and Beijing Academy of Artificial Intelligence. Intent-aware Ranking Ensemble for Personalized Recommendation SIGIR '23, July 23-27, 2023, Taipei, Taiwan.", "REFERENCES": "[1] Micha\u0142 Ba\u0142chanowski and Urszula Boryczka. 2022. Aggregation of Rankings Using Metaheuristics in Recommendation Systems. Electronics 11, 3 (2022), 369. [2] Micha\u0142 Ba\u0142chanowski and Urszula Boryczka. 2022. Collaborative Rank Aggregation in Recommendation Systems. Procedia Computer Science 207 (2022), 22132222. [3] Avradeep Bhowmik and Joydeep Ghosh. 2017. Letor methods for unsupervised rank aggregation. In Proceedings of the 26th international conference on world wide web . 1331-1340. [4] Weijie Bian, Kailun Wu, Lejian Ren, Qi Pi, Yujing Zhang, Can Xiao, Xiang-Rong Sheng, Yong-Nan Zhu, Zhangming Chan, Na Mou, et al. 2022. CAN: Feature Co-Action Network for Click-Through Rate Prediction. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining . 57-65. [5] JC de Borda. 1784. M\u00e9moire sur les \u00e9lections au scrutin. Histoire de l'Academie Royale des Sciences pour 1781 (Paris, 1784) (1784). [6] Gavin Brown, Jeremy Wyatt, Rachel Harris, and Xin Yao. 2005. Diversity creation methods: a survey and categorisation. Information fusion 6, 1 (2005), 5-20. [7] Christopher Burges, Robert Ragno, and Quoc Le. 2006. Learning to rank with nonsmooth cost functions. Advances in neural information processing systems 19 (2006). [8] Pablo Castells, Neil Hurley, and Saul Vargas. 2022. Novelty and diversity in recommender systems. In Recommender systems handbook . Springer, 603-646. [9] Yukuo Cen, Jianwei Zhang, Xu Zou, Chang Zhou, Hongxia Yang, and Jie Tang. 2020. Controllable multi-interest framework for recommendation. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 2942-2951. [10] Tong Chen, Hongzhi Yin, Hongxu Chen, Rui Yan, Quoc Viet Hung Nguyen, and Xue Li. 2019. Air: Attentional intention-aware recommender systems. In 2019 IEEE 35th International Conference on Data Engineering (ICDE) . IEEE, 304-315. [11] Yongjun Chen, Zhiwei Liu, Jia Li, Julian McAuley, and Caiming Xiong. 2022. Intent Contrastive Learning for Sequential Recommendation. In Proceedings of the ACM Web Conference 2022 . 2172-2182. [12] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555 (2014). [13] Imre Csisz\u00e1r. 1975. I-divergence geometry of probability distributions and minimization problems. The annals of probability (1975), 146-158. [14] Quanyu Dai, Haoxuan Li, Peng Wu, Zhenhua Dong, Xiao-Hua Zhou, Rui Zhang, Rui Zhang, and Jie Sun. 2022. A generalized doubly robust learning framework for debiasing post-click conversion rate prediction. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 252-262. [15] Weiqiang Di. 2022. A multi-intent based multi-policy relay contrastive learning for sequential recommendation. PeerJ Computer Science 8 (2022), e1088. [16] Cynthia Dwork, Ravi Kumar, Moni Naor, and Dandapani Sivakumar. 2001. Rank aggregation methods for the web. In Proceedings of the 10th international conference on World Wide Web . 613-622. [17] Cynthia Dwork, Ravi Kumar, Moni Naor, and D Sivakumar. 2001. Rank aggregation revisited. [18] Ronald Fagin, Ravi Kumar, and Dandapani Sivakumar. 2003. Efficient similarity search and classification via rank aggregation. In Proceedings of the 2003 ACM SIGMOD international conference on Management of data . 301-312. [19] Mohamed Farah and Daniel Vanderpooten. 2007. An outranking approach for rank aggregation in information retrieval. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval . 591-598. [20] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. arXiv preprint arXiv:1703.04247 (2017). [21] Raivo Kolde, Sven Laur, Priit Adler, and Jaak Vilo. 2012. Robust rank aggregation for gene list integration and meta-analysis. Bioinformatics 28, 4 (2012), 573-580. [22] Anders Krogh and Jesper Vedelsby. 1994. Neural network ensembles, cross validation, and active learning. Advances in neural information processing systems 7 (1994). [23] Shangsong Liang, Ilya Markov, Zhaochun Ren, and Maarten de Rijke. 2018. Manifold learning for rank aggregation. In Proceedings of the 2018 World Wide Web Conference . 1735-1744. [24] Hongzhi Liu, Yingpeng Du, and Zhonghai Wu. 2022. Generalized Ambiguity Decomposition for Ranking Ensemble Learning. Journal of Machine Learning Research 23, 88 (2022), 1-36. [25] Zhaoyang Liu, Haokun Chen, Fei Sun, Xu Xie, Jinyang Gao, Bolin Ding, and Yanyan Shen. 2021. Intent preference decoupling for user representation on online recommender system. In Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence . 2575-2582. [26] Zhiwei Liu, Xiaohan Li, Ziwei Fan, Stephen Guo, Kannan Achan, and S Yu Philip. 2020. Basket recommendation with multi-intent translation graph neural network. In 2020 IEEE International Conference on Big Data (Big Data) . IEEE, 728-737. [27] Hui Luo, Jingbo Zhou, Zhifeng Bao, Shuangli Li, J Shane Culpepper, Haochao Ying, Hao Liu, and Hui Xiong. 2020. Spatial object recommendation with hints: When spatial granularity matters. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . 781-790. [28] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. 2018. Modeling task relationships in multi-task learning with multi-gate mixture-ofexperts. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1930-1939. [29] Joao Mendes-Moreira, Carlos Soares, Al\u00edpio M\u00e1rio Jorge, and Jorge Freire De Sousa. 2012. Ensemble approaches for regression: A survey. Acm computing surveys (csur) 45, 1 (2012), 1-40. [30] Samuel Oliveira, Victor Diniz, Anisio Lacerda, and Gisele L Pappa. 2016. Evolutionary rank aggregation for recommender systems. In 2016 IEEE Congress on Evolutionary Computation (CEC) . IEEE, 255-262. [31] Samuel EL Oliveira, Victor Diniz, Anisio Lacerda, Luiz Merschmanm, and Gisele L Pappa. 2020. Is rank aggregation effective in recommender systems? an experimental analysis. ACM Transactions on Intelligent Systems and Technology (TIST) 11, 2 (2020), 1-26. [32] Xiaofeng Pan, Ming Li, Jing Zhang, Keren Yu, Hong Wen, Luping Wang, Chengjun Mao, and Bo Cao. 2022. MetaCVR: Conversion Rate Prediction via Meta Learning in Small-Scale Recommendation Scenarios. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval . 2110-2114. [33] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2012. BPR: Bayesian personalized ranking from implicit feedback. arXiv preprint arXiv:1205.2618 (2012). [34] Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. 2017. Dynamic routing between capsules. Advances in neural information processing systems 30 (2017). [35] Omer Sagi and Lior Rokach. 2018. Ensemble learning: A survey. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 8, 4 (2018), e1249. [36] Qi Shen, Lingfei Wu, Yitong Pang, Yiming Zhang, Zhihua Wei, Fangli Xu, and Bo Long. 2021. Multi-behavior graph contextual aware network for session-based recommendation. arXiv preprint arXiv:2109.11903 (2021). [37] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer. In Proceedings of the 28th ACM international conference on information and knowledge management . 1441-1450. [38] Hongyan Tang, Junning Liu, Ming Zhao, and Xudong Gong. 2020. Progressive layered extraction (ple): A novel multi-task learning (mtl) model for personalized recommendations. In Fourteenth ACM Conference on Recommender Systems . 269278. [39] Aayushi Verma and Shikha Mehta. 2017. A comparative study of ensemble learning methods for classification in bioinformatics. In 2017 7th International Conference on Cloud Computing, Data Science & Engineering-Confluence . IEEE, 155-158. [40] Chenyang Wang, Weizhi Ma, Min Zhang, Chong Chen, Yiqun Liu, and Shaoping Ma. 2020. Toward dynamic user intention: Temporal evolutionary effects of item relations in sequential recommendation. ACM Transactions on Information Systems (TOIS) 39, 2 (2020), 1-33. [41] Chenyang Wang, Zhefan Wang, Yankai Liu, Yang Ge, Weizhi Ma, Min Zhang, Yiqun Liu, Junlan Feng, Chao Deng, and Shaoping Ma. 2022. Target Interest Distillation for Multi-Interest Recommendation. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management . 2007-2016. [42] Gang Wang, Jianshan Sun, Jian Ma, Kaiquan Xu, and Jibao Gu. 2014. Sentiment classification: The contribution of ensemble learning. Decision support systems 57 (2014), 77-93. [43] Shoujin Wang, Liang Hu, Yan Wang, Quan Z Sheng, Mehmet Orgun, and Longbing Cao. 2020. Intention nets: psychology-inspired user choice behavior modeling for next-basket prediction. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 34. 6259-6266. [44] Yifan Wang, Weizhi Ma, Min Zhang*, Yiqun Liu, and Shaoping Ma. 2022. A survey on the fairness of recommender systems. ACMJournal of the ACM (JACM) (2022). [45] Xu-Cheng Yin, Kaizhu Huang, Chun Yang, and Hong-Wei Hao. 2014. Convex ensemble learning with sparsity and diversity. Information Fusion 20 (2014), 49-59. [46] Enming Yuan, Wei Guo, Zhicheng He, Huifeng Guo, Chengkai Liu, and Ruiming Tang. 2022. Multi-Behavior Sequential Transformer Recommender. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1642-1652. [47] Qihua Zhang, Junning Liu, Yuzhuo Dai, Yiyan Qi, Yifan Yuan, Kunlun Zheng, Fan Huang, and Xianfeng Tan. 2022. Multi-Task Fusion via Reinforcement Learning for Long-Term User Satisfaction in Recommender Systems. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 4510-4520. [48] Jieming Zhu, Jinyang Liu, Shuai Yang, Qi Zhang, and Xiuqiang He. 2021. Open Benchmarking for Click-Through Rate Prediction. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management . 2759-2769."}
